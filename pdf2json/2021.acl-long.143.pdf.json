{
  "name" : "2021.acl-long.143.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Implicit Representations of Meaning in Neural Language Models",
    "authors" : [ "Belinda Z. Li", "Maxwell Nye", "Jacob Andreas" ],
    "emails" : [ "bzl@mit.edu", "mnye@mit.edu", "jda@mit.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1813–1827\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1813"
    }, {
      "heading" : "1 Introduction",
      "text" : "Neural language models (NLMs), which place probability distributions over sequences of words, produce contextual word and sentence embeddings that are useful for a variety of language processing tasks (Peters et al., 2018; Lewis et al., 2020). This usefulness is partially explained by the fact that NLM representations encode lexical relations (Mikolov et al., 2013) and syntactic structure (Tenney et al., 2019). But the extent to which NLM training also induces representations of meaning remains a topic of ongoing debate (Bender and Koller, 2020; Wu et al., 2021). In this paper, we show that NLMs represent meaning in a specific sense: in simple semantic domains, they build representations of situations and entities that encode logical descriptions of each entity’s dynamic state.\n1Code and data are available at https://github.com/ belindal/state-probes.\nConsider the text in the left column of Fig. 1. Sentences (a) describe the contents of a room; this situation can be formally characterized by the graph of entities, properties, and relations depicted in (a′). Sentence (b), You pick up the key, causes the situation to change: a chest becomes empty, and a key becomes possessed by you rather than contained by the chest (b′). None of these changes are explicitly described by sentence (b). Nevertheless, the set of sentences that can follow (a)–(b) to form a semantically coherent discourse is determined by this new situation. An acceptable next sentence might feature the person using the key (c1) or performing an unrelated action (c2). But a sentence in which the person takes an apple out of the chest (c3) cannot follow (a)–(b), as the chest is now empty.\nFormal models of situations (built, like (a′)–(b′), from logical representations of entities and their\nattributes) are central to linguistic theories of meaning. NLMs face the problem of learning to generate coherent text like (a–c) without access to any explicit supervision for the underlying world state (a′)–(b′). Indeed, recent work in NLP points to the lack of exposure of explicit representations of the world external to language as prima facie evidence that LMs cannot represent meaning at all, and thus cannot in general output coherent discourses like (a)–(c) (Bender and Koller, 2020).\nThe present paper can be viewed as an empirical response to these arguments. It is true that current NLMs do not reliably output coherent descriptions when trained on data like (a)–(c). But from text alone, even these imperfect NLMs appear to learn implicit models of meaning that are translatable into formal state representations like (a′)–(b′). These state representations capture information like the emptiness of the chest in (b′), which is not explicitly mentioned and cannot be derived from any purely syntactic representation of (a)–(b), but follows as a semantically necessary consequence. These implicit semantic models are roughly analogous to the simplest components of discourse representation theory and related formalisms: they represent sets of entities, and update the facts that are known about these entities as sentences are added to a discourse. Like the NLMs that produce them, these implicit models are approximate and errorprone. Nonetheless, they do most of the things we expect of world models in formal semantics: they are structured, queryable and manipulable. In this narrow sense, NLM training appears to induce not just models of linguistic form, but models of meaning.\nThis paper begins with a review of existing approaches to NLM probing and discourse representation that serve as a foundation for our approach. We then formalize a procedure for determining whether NLM representations encode representations of situations like Fig. 1 (a′)–(b′). Finally, we apply this approach to BART and T5 NLMs trained on text from the English-language Alchemy and TextWorld datasets. In all cases, we find evidence of implicit meaning representations that:\n1. Can be linearly decoded from NLM encodings of entity mentions.\n2. Are primarily attributable to open-domain pretraining rather than in-domain fine-tuning.\n3. Influence downstream language generation.\nWe conclude with a discussion of the implications of these results for evaluating and improving factuality and coherence in NLMs."
    }, {
      "heading" : "2 Background",
      "text" : "What do LM representations encode? This paper’s investigation of state representations builds on a large body of past work aimed at understanding how other linguistic phenomena are represented in large-scale language models. NLM representations have been found to encode syntactic categories, dependency relations, and coreference information (Tenney et al., 2019; Hewitt and Manning, 2019; Clark et al., 2019). Within the realm of semantics, existing work has identified representations of word meaning (e.g., finegrained word senses; Wiedemann et al. 2019) and predicate–argument structures like frames and semantic roles (Kovaleva et al., 2019).\nIn all these studies, the main experimental paradigm is probing (Shi et al., 2016; Belinkov and Glass, 2019): given a fixed source of representations (e.g. the BERT language model; Devlin et al. 2019) and a linguistic label of interest (e.g. semantic role), a low-capacity “probe” (e.g a linear classifier) is trained to predict the label from the representations (e.g. to predict semantic roles from BERT embeddings). A phenomenon is judged to be encoded by a model if the probe’s accuracy cannot be explained by its accuracy when trained on control tasks (Hewitt and Liang, 2019) or baseline models (Pimentel et al., 2020).\nOur work extends this experimental paradigm to a new class of semantic phenomena. As in past work, we train probes to recover semantic annotations, and interpret these probes by comparison to null hypotheses that test the role of the model and the difficulty of the task. The key distinction is that we aim to recover a representation of the situation described by a discourse rather than representations of the sentences that make up the discourse. For example, in Fig. 1, we aim to understand not only whether NLMs encode the (sentence-level) semantic information that there was a picking up event whose patient was you and whose agent was the key—we also wish to understand whether LMs encode the consequences of this action for all entities under discussion, including the chest from which the key was (implicitly) taken.\nHow might LMs encode meaning? Like in other probing work, an attempt to identify neural\nencodings of entities and situations must begin with a formal framework for representing them. This is the subject of dynamic semantics in linguistics (Heim, 2008; Kamp et al., 2010; Groenendijk and Stokhof, 1991). The central tool for representing meaning in these approaches is the information state: the set of possible states of the world consistent with a discourse (I0 and I1 in Fig. 2). Before anything is said, all logically consistent situations are part of the information state (I0). Each new sentence in a discourse provides an update (that constrains or otherwise manipulates the set of possible situations). As shown in the figure, these updates can affect even unmentioned entities: the sentence the only thing in the chest is a key ensures that the proposition contains(chest, x) is false for all entities x other than the key. This is formalized in §3 below.2\nThe main hypothesis explored in this paper is that LMs represent (a particular class of) information states. Given an LM trained on text alone, and a discourse annotated post-hoc with information states, our probes will try to recover these information states from LM representations. The semantics literature includes a variety of proposals for how information states should be represented; here, we will represent information states logically, and decode information states via the truth values that they assign to logical propositions (φi,j in Fig. 2).3\n2See also Yalcin (2014) for an introductory survey. 3In existing work, one of the main applications of dynamic\nLMs and other semantic phenomena In addition to work on interpretability, a great deal of past research uses language modeling as a pretraining scheme for more conventional (supervised) semantics tasks in NLP. LM pretraining is useful for semantic parsing (Einolghozati et al., 2019), instruction following (Hill et al., 2020), and even image retrieval (Ilharco et al., 2021). Here, our primary objective is not good performance on downstream tasks, but rather understanding of representations themselves. LM pretraining has also been found to be useful for tasks like factoid question answering (Petroni et al., 2019; Roberts et al., 2020). Our experiments do not explore the extent to which LMs encode static background knowledge, but instead the extent to which they can build representations of novel situations described by novel text."
    }, {
      "heading" : "3 Approach",
      "text" : "Overview We train probing models to test whether NLMs represent the information states specified by the input text. We specifically probe for the truth values of logical propositions about entities mentioned in the text. For example, in Figure 1, we test whether a representation of sentences (a)–(b) encodes the fact that empty(chest) is true and contains(chest, key) is false.\nMeanings as information states To formalize this: given a universe consisting of a set of entities, properties, and relations, we define a situation as a complete specification of the properties and relations of each entity. For example, the box labeled I0 in Fig. 2 shows three situations involving a chest, a key, an apple, an eaten property and a contains relation. In one situation, the chest contains the key and the apple is eaten. In another, the chest contains the apple, and the apple is not eaten. In general, a situation assigns a value of true or false to every logical proposition of the form P (x) or R(x, y) (e.g. locked(door) or contains(chest, key)).\nNow, given a natural language discourse, we can view that discourse as specifying a set of possible situations. In Fig. 2, the sentence x0 picks out the subset of situations in which the chest contains the key. A collection of situations is called an information state, because it encodes a listener’s\nsemantics is a precise treatment of quantification and scope at the discourse level. The tasks investigated in this paper do not involve any interesting quantification, and rely on the simplest parts of the formalism. More detailed exploration of quantification in NLMs is an important topic for future study.\nknowledge of (and uncertainty about) the state of the world resulting from the events described in a discourse.4 In a given information state, the value of a proposition might be true in all situations, false in all situations, or unknown: true in some but false in others. An information state (or an NLM representation) can thus be characterized by the label it assigns to every proposition.\nProbing for propositions We assume we are given:\n• A sequence of sentences x1:n = [x1, . . . , xn].\n• For each i, the information state Ii that results from the sentences x1:i. We write I(φ) ∈ {T, F, ?} for the value of the proposition φ in the information state I .\n• A language model encoder E that maps sentences to sequences of d-dimensional word representations.\nTo characterize the encoding of semantic information in E(x), we design a semantic probe that tries to recover the contents of Ii from E(x1:i) proposition-by-proposition. Intuitively, this probe aims to answer three questions: (1) How is the truth value of a given proposition φ encoded? (Linearly? Nonlinearly? In what feature basis?) (2) Where is information about φ encoded? (Distributed across all token embeddings? Local to particular tokens?) (3) How well is semantic information encoded? (Can it be recovered better than chance? Perfectly?)\n4An individual sentence is associated with a context change potential: a map from information states to information states.\nThe probe is built from three components, each of which corresponds to one of the questions above:\n1. A proposition embedder embed : L → Rd (where L is the set of logical propositions).\n2. A localizer loc : L × Rd → Rd which extracts and aggregates LM representations as candidates for encoding φ. The localizer extracts tokens of E(x) at positions corresponding to particular tokens in the underlying text x. We express this in notation as E(x)[*], where * is a subsequence of x. (For example, if x = “the third beaker is empty”. E(x) = [v1, v2, v3, v4, v5] has one vector per token. E(x)[“third beaker”] = [v2, v3].)\n3. A classifier clsθ : Rd × Rd → {T, F, ?}, which takes an embedded proposition and a localized embedding, and predicts the truth value of the proposition.\nWe say that a proposition φ is encoded by E(x) if:\nclsθ(embed(φ), loc(φ,E(x))) = I(φ) . (1)\nGiven a dataset of discourses D, we attempt to find a classifier parameters θ from which all propositions can be recovered for all sentences in Eq. (1). To do so, we label each with the truth/falsehood of every relevant proposition. We then train the parameters of a clsθ on a subset of these propositions and test whether it generalizes to held-out discourses."
    }, {
      "heading" : "4 Experiments",
      "text" : "Our experiments aim to discover to what extent (and in what manner) information states are encoded in NLM representations. We first present a specific instantiation of the probe that allows us to determine how well information states are encoded in two NLMs and two datasets (§4.2); then provide a more detailed look at where specific propositions are encoded by varying loc (§4.3). Finally, we describe an experiment investigating the causal role of semantic representations by directly manipulating E(x) (§4.4).5"
    }, {
      "heading" : "4.1 Preliminaries",
      "text" : "Model In all experiments, the encoder E comes from a BART (Lewis et al., 2020) or T5 (Raffel et al., 2020) model. Except where noted, BART is pretrained on OpenWebText, BookCorpus, CCNews, and Stories (Lewis et al., 2020), T5 is pretrained on C4 (Raffel et al., 2020), and both are fine-tuned on the TextWorld or Alchemy datasets described below. Weights of E are frozen during probe training.\nData: Alchemy Alchemy, the first dataset used in our experiments, is derived from the SCONE (Long et al., 2016) semantic parsing tasks. We preserve the train / development split from the original dataset (3657 train / 245 development). Every example in the dataset consists of a humangenerated sequence of instructions to drain, pour, or mix a beaker full of colored liquid. Each instruction is annotated with the ground-truth state that results from following that instruction (Figure 3).\nWe turn Alchemy into a language modeling dataset by prepending a declaration of the initial state (the initial contents of each beaker) to the actions. The initial state declaration always follows a fixed form (“the first beaker has [amount] [color], the second beaker has [amount] [color], ...”). Including it in the context provides enough information that it is (in principle) possible to deterministically compute the contents of each beaker after each instruction. The NLM is trained to predict the next instruction based on a textual description of the initial state and previous instructions.\nThe state representations we probe for in Alchemy describe the contents of each beaker. Because execution is deterministic and the initial state\n5Sections here are also discussed in more detail in Appendix A.1 (for §4.1), A.2 (for §4.2), and A.3 (for §4.3).\nis fully specified, the information state associated with each instruction prefix consists of only a single possible situation, defined by a set of propositions:\nΦ = { has-v-c(b) :\nb ∈ {beaker 1, beaker 2, . . .}, v ∈ 1..4, c ∈ {red, orange, yellow, . . .} } .\n(2)\nIn the experiments below, it will be useful to have access to a natural language representation of each proposition. We denote this:\nNL(has-v-c(b)) = “the b beaker has v c” . (3)\nTruth values for each proposition in each instruction sequence are straightforwardly derived from ground-truth state annotations in the dataset.\nData: TextWorld TextWorld (Côté et al., 2018) is a platform for generating synthetic worlds for text-based games, used to test RL agents. The game generator produces rooms containing objects, surfaces, and containers, which the agent can interact with in various predefined ways.\nWe turn TextWorld into a language modeling dataset by generating random game rollouts following the “simple game” challenge, which samples world states with a fixed room layout but changing object configurations. For training, we sample 4000 rollouts across 79 worlds, and for development, we sample 500 rollouts across 9 worlds. Contexts begin with a description of the room that the player currently stands in, and all visible objects in that room. This is followed by a series of actions (preceded by >) and game responses (Fig. 3).\nThe NLM is trained to generate both an action and a game response from a history of interactions.\nWe probe for both the properties of and relations between entities at the end of a sequence of actions. Unlike Alchemy, these may be undetermined, as the agent may not have explored the entire environment by the end of an action sequence. (For example, in Fig. 3, the truth value of matches(old key, door) is unknown). The set of propositions available in the TextWorld domain has form\nΦ ={p(o) : o ∈ O, p ∈ P} ∪ {r(o1, o2) : o1, o2 ∈ O, r ∈ R}\n(4)\nfor objects O = {player, chest, . . .}, properties P = {open, edible, . . .} and relations R =\n{on, in, . . .}. We convert propositions to natural language descriptions as:\nNL(p(o)) = “the p is o”\nNL(r(o1, o2)) = “the o1 is r o2” . (5)\nThe set of propositions and their natural language descriptions are pre-defined by TextWorld’s simulation engine. The simulation engine also gives us the set of true propositions, from which we can compute the set of false and unknown propositions.\nEvaluation We evaluate probes according to two metrics. Entity Exact-Match (EM) first aggregates the propositions by entity or entity pair, then counts the percentage of entities for which all propositions were correctly labeled. State EM aggregates propositions by information state (i.e. context), then counts the percentage of states for which all facts were correctly labeled."
    }, {
      "heading" : "4.2 Representations encode entities’ final properties and relations",
      "text" : "With this setup in place, we are ready to ask our first question: is semantic state information encoded at all by pretrained LMs fine-tuned on Alchemy and TextWorld? We instantiate the probing experiment defined in §3 as follows:\nThe proposition embedder converts each proposition φ ∈ Φ to its natural language description, embeds it using the same LM encoder that is being probed, then averages the tokens:\nembed(φ) = mean(E(NL(φ))) (6)\nThe localizer associates each proposition φ with specific tokens corresponding to the entity or entities that φ describes, then averages these tokens.\nIn Alchemy, we average over tokens in the initial description of the beaker in question. For example, let x be the discourse in Figure 3 (left) and φ be a proposition about the first beaker. Then, e.g.,\nloc(has-1-red(beaker 1), E(x)) =\nmean(E(x)[The first beaker has 2 green,]). (7)\nIn TextWorld, we average over tokens in all mentions of each entity. Letting x be the discourse in Figure 3 (right), we have:\nloc(locked(wooden door), E(x)) =\nmean(E(x)[wooden door]) . (8)\nRelations, with two arguments, are localized by taking the mean of the two mentions.\nFinally, the classifier is a linear model which maps each NLM representation and proposition to a truth value. In Alchemy, a linear transformation is applied to the NLM representation, and then the proposition with the maximum dot product with that vector is labelled T (the rest are labelled F ). In TextWorld, a bilinear transformation maps each (proposition embedding, NLM representation) pair to a distribution over {T, F, ?}.\nAs noted by Liang and Potts (2015), it is easy to construct examples of semantic judgments that cannot be expressed as linear functions of purely syntactic sentence representations. We expect (and verify with ablation experiments) that this probe is not expressive enough to compute information states directly from surface forms, and only expressive enough to read out state information already computed by the underlying LM.\nResults Results are shown in Table 1. A probe on T5 can exactly recover 14.3% of information states in Alchemy, and 53.8% in TextWorld. For context, we compare to two baselines: a no LM baseline, which simply predicts the most frequent final state for each entity, and a no change baseline, which predicts that the entity’s final state in the discourse will be the same as its initial state. The no LM baseline is correct 0% / 1.8% of the time and the no change baseline is correct 0% / 9.7% of the time—substantially lower than the main probe.\nTo verify that this predictability is a property of the NLM representations rather than the text itself, we apply our probe to a series of model ablations. First, we evaluate a randomly initialized transformer rather than the pretrained and fine-tuned model, which has much lower probe accuracy. To determine whether the advantage is conferred by LM pretraining or fine-tuning, we ablate either open-domain pretraining, in a -pretrain,+fine-tune ablation, or in-domain finetuning, in a +pretrain,-fine-tune ablation. (For all experiments not using a pretrained model checkpoint, we experimented with both a BART-like and T5-like choice of depth and hidden size, and found that the BART-like model performed better.) While both fine-tuning and pretraining contribute to the final probe accuracy, pretraining appears to play a much larger role: semantic state can be recovered well from models with no in-domain fine-tuning.\nFinally, we note that there may be lexical overlap between the discourse and natural language descriptions of propositions. How much of the probe’s performance can be attributed to this overlap? In Alchemy, the no change baseline (which\nperforms much worse than our probe) also acts as a lexical overlap baseline—there will be lexical overlap between true propositions and the initial state declaration only if the beaker state is unchanged. In TextWorld, each action induces multiple updates, but can at most overlap with one of its affected propositions (e.g. You close the chest causes closed(chest) and ¬open(chest), but only overlaps with the former). Moreover, only ∼50% of actions have lexical overlap with any propositions at all. Thus, lexical overlap cannot fully explain probe performance in either domain.\nIn summary, pretrained NLM representations model state changes and encode semantic information about entities’ final states."
    }, {
      "heading" : "4.3 Representations of entities are local to entity mentions",
      "text" : "The experiment in §4.2 assumed that entity state could be recovered from a fixed set of input tokens. Next, we conduct a more detailed investigation into where state information is localized. To this end, we ask two questions: first, can we assume state information is localized in the corresponding entity mentions, and second, if so, which mention encodes the most information, and what kind of information does it encode?"
    }, {
      "heading" : "4.3.1 Mentions or other tokens?",
      "text" : "We first contrast tokens within mentions of the target entity to tokens elsewhere in the input discourse. In Alchemy, each beaker b’s initial state declaration is tokenized as: toksb = {theb, [position]b, beb, akerb, hasb, [volume]b, [color]b, ,b}, where b signifies the beaker position. Rather than pooling these tokens together (as in §4.2), we construct a localizer ablation that associates beaker b’s state with single tokens t in either the initial mention of beaker b, or the initial mention of other beakers at an integer offset ∆. For each (t,∆) pair, we construct a localizer that matches propositions about beaker b with tb+∆. For example, the (has,+1) localizer associates the third beaker’s final state\nwith the vector in E(x) at the position of the “has” token in the fourth beaker has 2 red.\nIn TextWorld, which does not have such easily categorizable tokens, we investigate whether information about the state of an entity is encoded in mentions of different entities. We sample a random mapping remap between entities, and construct a localizer ablation in which we decode propositions about w from mentions of remap(w). For example, we probe the value of open(chest) from mentions of old key. These experiments use a different evaluation set—we restrict evaluation to the subset of entities for which both w and remap(w) appear in the discourse. For comparability, we re-run the main probe on this restricted set.6\nResults Fig. 4 shows the locality of BART and T5 in the Alchemy domain. Entity EM is highest for words corresponding to the correct beaker, and specifically for color words. Decoding from any token of an incorrect beaker barely outperforms the no LM baseline (32.4% entity EM). In TextWorld, Table 2 shows that decoding from a remapped entity is only 1-3% worse than decoding from the right one. Thus, the state of an entity e is (roughly) localized to tokens in mentions of e, though the degree of locality is data- and model-dependent."
    }, {
      "heading" : "4.3.2 Which mention?",
      "text" : "To investigate facts encoded in different mentions of the entity in question, we experiment with decoding from the first and last mentions of the entities in x. The form of the localizer is the same as 4.2, except instead of averaging across all mentions of entities, we use the first mention or the last mention. We also ask whether relational propositions can be decoded from just one argument (e.g., in(old key, chest) from just mentions of old key, rather than the averaged encodings of old key and chest).\nResults As shown in Table 1, in TextWorld, probing the last mention gives the highest accuracy. Furthermore, as Table 3 shows, relational facts can be decoded from either side of the relation."
    }, {
      "heading" : "4.4 Changes to entity representations cause changes in language model predictions",
      "text" : "The localization experiments in Section 4.3 indicate that state information is localized within con-\n6The remap and ∆ 6= 0 probes described here are analogous to control tasks (Hewitt and Liang, 2019). They measure probes’ abilities to predict labels that are structurally similar but semantically unrelated to the phenomenon of interest.\ntextual representations in predictable ways. This suggests that modifying the representations themselves could induce systematic and predictable changes in model behavior. We conduct a series of causal intervention experiments in the Alchemy domain which measure effect of manipulating encoder representations on NLM output. We replace a small subset of token representations with those from a different information state, and show that this causes the model to behave as if it were in the new information state.7\nA diagram of the procedure is shown in Fig. 5. We create two discourses, x1 and x2, in which one beaker’s final volume is zero. Both discourses describe the same initial state, but for each xi, we append the sentence drain vi from beaker bi, where vi is the initial volume of beaker bi’s contents. Though the underlying initial state tokens are the same, we expect the contextualized representation C1 = E(x1)[the ith beaker . . .] to differ from C2 = E(x2)[the ith beaker . . .] due to the different final states of the beakers. Let CONT(x) denote the set of sentences constituting semantically acceptable continuations of a discourse prefix x. (In Fig. 1, CONT(a, b) contains c1 and c2 but not c3.)8 In Alchemy, CONT(x1) should not contain mixing, draining, or pouring actions involving b1 (similarly for CONT(x2) and b2). Decoder samples given Ci should fall into CONT(xi).\nFinally, we replace the encoded description of beaker 2 in C1 with its encoding from C2, creating a new representation Cmix. Cmix was not derived from any real input text, but implicitly represents a situation in which both b1 and b2 are empty. A decoder generating from Cmix should generate instructions in CONT(x1) ∩ CONT(x2) to be consistent with this situation.\n7This experiment is inspired by Geiger et al. (2020). 8In order to automate evaluation of consistency, we use a version of Alchemy with synthetically generated text. The underlying LM has also been fine-tuned on synthetic data.\nResults We generate instructions conditioned on Cmix and check whether they are in the expected sets. Results, shown in Table 4, align with this prediction. For both BART and T5, substantially more generations from Cmix fall within CONT(x1) ∩ CONT(x2) than from C1 or C2. Though imperfect (compared to C1 generations within CONT(x1) and C2 generations within CONT(x2)), this suggests that the information state associated with the synthetic encoding Cmix is (approximately) one in which both beakers are empty."
    }, {
      "heading" : "5 Limitations",
      "text" : "...of large NLMs: It is important to emphasize that both LM output and implicit state representa-\ntions are imperfect: even in the best case, complete information states can only be recovered 53.8% of the time in tasks that most humans would find very simple. (Additional experiments described in Appendix A.5 offer more detail about these errors.) The success of our probing experiments should not be taken to indicate that the discovered semantic representations have anything near the expressiveness needed to support human-like generation.\n...of our experimental paradigm: While our probing experiments in §4.2 provide a detailed picture of structured state representations in NLMs, the intervention experiments in §4.4 explain the relationship between these state representations and model behavior in only a very general sense. They leave open the key question of whether errors in language model prediction are attributable to errors in the underlying state representation. Finally, the situations we model here are extremely simple, featuring just a handful of objects. Thought experiments on the theoretical capabilities of NLMs (e.g. Bender and Koller’s “coconut catapult”) involve far richer worlds and more complex interactions. Again, we leave for future work the question of whether current models can learn to represent them."
    }, {
      "heading" : "6 Conclusion",
      "text" : "Even when trained only on language data, NLMs encode simple representations of meaning. In experiments on two domains, internal representations of text produced by two pretrained language models can be mapped, using a linear probe, to representations of the state of the world described by the text. These internal representations are structured, interpretably localized, and editable. This finding has important implications for research aimed at improving factuality and and coherence in NLMs: future work might probe LMs for the the states and properties ascribed to entities the first time they are mentioned (which may reveal biases learned from training data; Bender et al. 2021), or correct errors in generation by directly editing representations."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Thanks to Ekin Akyürek, Evan Hernandez, Joe O’Connor, and the anonymous reviewers for feedback on early versions of this paper. MN is supported by a NSF Graduate Research Fellowship. This work was supported by a hardware donation from NVIDIA under the NVAIL program."
    }, {
      "heading" : "Impact Statement",
      "text" : "This paper investigates the extent to which neural language models build meaning representations of the world, and introduces a method to probe and modify the underlying information state. We expect this can be applied to improve factuality, coherence, and reduce bias and toxicity in language model generations. Moreover, deeper insight into how neural language models work and what exactly they encode can be important when deploying these models in real-world settings.\nHowever, interpretability research is by nature dual-use and improve the effectiveness of models for generating false, misleading, or abusive language. Even when not deliberately tailored to generation of harmful language, learned semantic representations might not accurately represent the world because of errors both in prediction (as discussed in §5) and in training data."
    }, {
      "heading" : "A Appendix",
      "text" : ""
    }, {
      "heading" : "A.1 Datasets Details (§4.1)",
      "text" : "Alchemy Alchemy is downloadable at https:// nlp.stanford.edu/projects/scone/. Alchemy propositions are straightforwardly derived from existing labels in the dataset. We preserve the train/dev split from the original dataset (3657 train/245 dev), which we use for training the underlying LM and the probe. In subsequent sections, we include additional results from a synthetic dataset that we generated (3600 train/500 dev), where actions are created following a fixed template, making it easy to evaluate consistency.\nTextworld We generate a set of worlds for training, and a separate set of worlds for testing. We obtain transcripts from three agents playing on each game: a perfect agent and two (semi-)random agents, which intersperse perfect actions with several steps of random actions. For training, we sample 4000 sequences from the 3 agents across 79 worlds. For development, we sample 500 sequences from the 3 agents across 9 worlds.\nDuring game generation, we are given the set of all propositions that are True in the world, and how the set updates after each player action. However, the player cannot infer the full state before interacting with and seeing all objects, and neither (we suspect) can a language model trained on partial transcripts. For example, a player that starts in the bedroom cannot infer is-in(refrigerator, kitchen) without first entering the kitchen. One solution would be to hard-code rules–a player can only know about the states of entities it has directly affected or seen. However, since syntheticallygenerated worlds might share some commonalities, a player that has played many games before (or an LM trained on many transcripts) might be able to draw particular conclusions about entities in unseen worlds, even before interacting with them.\nTo deal with these factors, we train a labeller model label to help us classify propositions as known true, known false, and unknown. We generate a training set (separate from the training set for the probe and probed LM) to train the labeller. We again use BART, but we give it the text transcripts and train it to directly decode the full set of True propositions and False proposition by the end of the transcript (recall we have the ground-truth full True set, and we label all propositions that aren’t in the True set as False). This allows the labeller\nmodel to pick up both patterns between the discourse and its information state, as well as infer general patterns among various discourses. Thus, on unknown worlds, given text T , if proposition A is True most or all of the time given T , the model should be confident in predicting A. We label A as True in these cases. However, if proposition A is True only half of the time given T , the model is unconfident. We label A as Unknown in these cases. Thus, we create our unknown set using a confidence threshold τ on label’s output probability."
    }, {
      "heading" : "A.2 Probe Details + Additional Results (§4.2)",
      "text" : "Below, we give a more detailed account of our probing paradigm in each domain, including equations.\nAlchemy Probe The proposition embedder converts propositions φ to natural language descriptions φ̃ (“the bth beaker has v c”) and encodes them with the BART or T5 LM encoder.\nGiven a proposition has-v-c(b), the localizer loc maps has-v-c(b) to tokens in E(x) that corresponding to the initial state of beaker b. Since x always begins with an initial state declaration of the form “the first beaker has [amount] [color], the second beaker has [amount] [color], ...”, tokens at position 8b − 8 · · · 8b − 1 of x correspond to the initial state of beaker b. (Each state has 8 tokens: ‘the’, ‘bth’, ‘be’, ‘aker’, ‘has’, ‘[amount]’, ‘[color]’, ‘,’). Thus,\nloc(has-v-c(b), E(x)) =\nmean(E(x)[8b− 8], · · · , E(x)[8b− 1])\nWe train a linear probe clsθ to predict the final beaker state given the encoded representation E(x) of text x. For our probe, we learn linear projection weights W (d×d) and bias b(d) to maximize the dot product between the LM representation and the embedded proposition. Formally, it computes v\n(max) b , c (max) b as\nv (max) b , c (max) b = arg max\nv′,c′\n( embed(has-v′-c′(b))·\n(W · loc(has-v′-c′(b), E(x)) + b) ) (9)\nIn other words, v(max)b , c (max) b are the values of v and c that maximize this dot product. The probe then returns\nclsθ(embed(has-v-c(b)), loc(has-v-c(b), E(x)))\n=\n{ T if v, c = v(max)b , c (max) b\nF if v, c 6= v(max)b , c (max) b\n(10)\nNote that clsθ selects the optimal final state per beaker, from the set of all possible states of beaker b, taking advantage of the fact that only one proposition can be true per beaker.\nTextworld Probe For Textworld, the proposition embedder converts propositions φ to natural language descriptions φ̃ (“the o is p” for properties and “the o1 is r o2” for relations) and encodes them with the BART or T5 LM encoder.\nGiven a proposition p(o) pertaining to an entity or r(o1, o2) pertaining to an entity pair, we define localizer loc to map the proposition to tokens of E(x) corresponding to all mentions of its arguments, and averages across those tokens:\nall idx(e) = set of indices of x correspond\n-ing to all instances of e loc(r(o1, o2), E(x)) = mean ( E(x)[all idx(o1)∪\nall idx(o2)] )\nloc(p(o), E(x)) = mean (E(x)[all idx(o)])\n(11)\nWe train a bilinear probe clsθ that classifies each (proposition embedding, LM representation) pair to {T ,F , ?}. The probe has parameters W (3×d×d), b(3) and performs the following bilinear operation:\nscr(φ,E(x)) = embed(φ)T ·W · loc(φ,E(x)) + b (12)\nwhere scr is a vector of size 3, with one score per T ,F , ? label. The probe then takes the highestscoring label\nclsθ(embed(φ), loc(φ,E(x))) = T if scr(φ,E(x))[0] > scr(φ,E(x))[1], scr(φ,E(x))[2] F if scr(φ,E(x))[1] > scr(φ,E(x))[2], scr(φ,E(x))[0] ? if scr(φ,E(x))[2] > scr(φ,E(x))[0], scr(φ,E(x))[1]\n(13)"
    }, {
      "heading" : "A.3 Localization Experiment Details + Additional Results (§4.3)",
      "text" : "Below we provide a specific, formulaic account of each of our localizer experiments."
    }, {
      "heading" : "Mentions vs. Other Tokens (§4.3.1) – Alchemy",
      "text" : "Recall that we train a probe for each (t,∆) pair to extract propositions about b from token tb+∆ ∈ toksb+∆, where ∆ is the beaker offset. Specifically, the localizer for this probe takes form\noff :\n{‘the’→ 0, [position]→ 1, ‘be’→ 2, ‘aker’→ 3, ‘has’→ 4, [amount]→ 5, [color]→ 6, ‘,’→ 7}\nloc(t,∆)(has-v-c(b, E(x))) =\nmean(E(x)[8(b+ ∆)− 8 + off(t)])\nThe full token-wise results for beaker states in a 3-beaker (24-token) window around the target beaker is shown in Figure 6 (Top/Middle).\nAdditional localizer ablations results for a BART probe trained and evaluated on synthetic Alchemy data are shown in Figures 6 (Bottom). Similar to the non-synthetic experiments, we point the localizer to just a single token of the initial state. Interestingly, BART’s distribution looks very different in the synthetic setting. Though state information is still local to the initial state description of the target beaker, it is far more distributed within the description–concentrated in not just the amount and color tokens, but also the mention tokens."
    }, {
      "heading" : "Mentions vs. Other Tokens (§4.3.1) – Textworld",
      "text" : "The specific localizer for this experiment has form\nloc(r(o1, o2), E(x)) =\nmean(E(x)[all idx(remap(o1))∪ all idx(remap(o2))])\nloc(p(o), E(x)) = mean(E(x)[all idx(remap(o))])\nNote the evaluation set for this experiment is slightly different as we exclude contexts which do not mention remap(w).\n.\nWhich Mention? (§4.3.2) – first/last The localizer for this experiment is constructed by replacing all instances of all idx in Eq. 10 with either first idx or last idx, defined as:\nfirst idx(e) = set of indices of x correspond-\ning to first instance of e\nlast idx(e) = set of indices of x correspond-\ning to last instance of e\nWhich Mention? (§4.3.2) – single- vs. bothentity probe. The specific localizer for the single-entity probe has form\nloc(r(o1, o2), E(x)) = { mean(E(x)[all idx(o1)]), mean(E(x)[all idx(o2)]) } loc(p(o), E(x)) = mean(E(x)[all idx(o)])\nNote the localizer returns a 2-element set of encodings from each relation. We train the probe to decode r(o1, o2) from both elements of this set.\nThe full results are in Table 3. As shown, the both-mentions probe is slightly better at both decoding relations and properties. However, this may simply be due to having less candidate propositions per entity pair, than per entity (which includes relations from every other entity paired with this entity). For example, entity pair (apple, chest) has only three possibilities: in(apple, chest) is True/Unknown/False, while singular entity (chest) has much more: in(apple, chest), in(key, chest), open(chest), etc. can each be True/Unknown/False. A full set of results broken down by property/relation can be found in Table 6. Overall, the single-entity probe outperforms all baselines, suggesting that each entity encoding contains information about its relation with other entities."
    }, {
      "heading" : "A.4 Proposition Embedder Ablations",
      "text" : "We experiment with a featurized embed function in the Alchemy domain. Recall from Section 4.2 and A.2 that our main probe uses encoded naturallanguage assertions of the state of each beaker\n(Eq. 6). We experiment with featurized vector where each beaker proposition is the concatenation of a 1-hot vector for beaker position and a sparse vector encoding the amount of each color in the beaker (with 1 position per color). For example, if there are 2 beakers and 3 colors [green,red,brown], has-3-red(2) is represented as [0, 1, 0, 3, 0]. A multi-layer perceptron is used as the embed function to map this featurized representation into a dense vector, which is used in the probe as described by Eq. 10. In this setting, the embed MLP is optimized jointly with the probe.\nResults are shown in Table 5. Using a featurized representation (45.7) is significantly worse than using an encoded natural-language representation (75.0), suggesting that the form of the fact embedding function is important. In particular, the encoding is linear in sentence-embedding space, but nonlinear in human-grounded-feature space."
    }, {
      "heading" : "A.5 Error Analysis",
      "text" : "We run error analysis on the BART model. For the analysis below, it is important to note that we make no distinction between probe errors and representation errors—we do not know whether the errors are attributable to the linear probe’s lack of expressive power, or whether the underlying LM indeed does fail to capture certain phenomena. We note that a BART decoder trained to decode the final information state from E(x) is able to achieve 53.5% state EM on Alchemy (compared to 0% on random initialization baseline) whereas the linear decoder was only able to achieve 7.55% state EM— suggesting that certain state information may be non-linearly encoded in NLMs.\nAlchemy The average number of incorrect beakers per sample is 25.0% (2.7 beakers out of 7).\nWe note that the distribution is skewed towards longer sequences of actions, where the % of wrong beakers increases from 11.3% (at 1 action) to 24.7% (2 actions), 30.4% (3 actions), 33.4% (at 4 actions). For beakers not acted upon (final state unchanged), the error rate is 13.3%. For beakers acted upon (final state changed), the error rate is 44.6%. Thus, errors are largely attributed to failures in reasoning about the effects of actions, rather than failures in decoding the initial state. (This is unsurprising, as in Alchemy, the initial state is explicitly written in the text—and we’re decoding from those tokens).\nFor beakers that were predicted wrong, 36.8% were predicted to be its unchanged initial state and\nthe remaining 63.2% were predicted to be empty — thus, probe mistakes are largely attributable to a tendency to over-predict empty beakers. This suggests that the downstream decoder may tend to generate actions too conservatively (as empty beakers cannot be acted upon). Correcting this could encourage LM generation diversity.\nFinally, we examine what type of action tends to throw off the probe. When there is a pouring or mixing-type action present in the sequence, the model tends to do worse (25.3% error rate for drain-type vs. 31.4 and 33.3% for pour- and mix-type), though this is partially due to the higher concentration of drain actions in short action sequences.\nTextworld Textworld results, broken down by properties/relations, are reported in Table 6. The probe seems to be especially bad at classifying relations, which make sense as relations are often expressed indirectly. A breakdown of error rate for each proposition type is shown in Table 7, where we report what % of that type of proposition was labelled incorrectly, each time it appeared. This table suggests that the probe consistently fails at decoding locational relations, i.e. failing to classify east-of(kitchen,bedroom) and west-of(kitchen,bedroom) as True, despite the layout of the simple domain being fixed. One hypothesis is that location information is made much less explicit in the text, and usually require reasoning across longer contexts and action sequences. For example, classifying in(key, drawer) as True simply requires looking at a single action: > put key in drawer. However, classifying east-of(kitchen,bedroom) as True requires reasoning across the following context:\nYou are in the bedroom [. . . ]\n> go east You enter the kitchen.\nwhere the ellipses possibly encompass a long sequence of other actions.\nA.6 Infrastructure and Reproducibility We run all experiments on a single 32GB NVIDIA Tesla V100 GPU. On both Alchemy and Textworld, we train the language models to convergence, then train the probe for 20 epochs. In Alchemy and Textworld, both training the language model and the probe takes approximately a few (less than 5) hours. We probe BART-base, a 12-layer encoderdecoder Transformer model with 139M parameters, and T5-base, a 24-layer encoder-decoder Transformer model which has 220M parameters. Our probe itself is a linear model, with only two parameters (weights and bias)."
    } ],
    "references" : [ {
      "title" : "Analysis methods in neural language processing: A survey",
      "author" : [ "Yonatan Belinkov", "James Glass." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:49–72.",
      "citeRegEx" : "Belinkov and Glass.,? 2019",
      "shortCiteRegEx" : "Belinkov and Glass.",
      "year" : 2019
    }, {
      "title" : "McMillanMajor, and Shmargaret Shmitchell",
      "author" : [ "Emily M Bender", "Timnit Gebru", "Angelina" ],
      "venue" : "In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency,",
      "citeRegEx" : "Bender et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Bender et al\\.",
      "year" : 2021
    }, {
      "title" : "Climbing towards NLU: On meaning, form, and understanding in the age of data",
      "author" : [ "Emily M. Bender", "Alexander Koller." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185–5198, Online. As-",
      "citeRegEx" : "Bender and Koller.,? 2020",
      "shortCiteRegEx" : "Bender and Koller.",
      "year" : 2020
    }, {
      "title" : "What does BERT look at? an analysis of BERT’s attention",
      "author" : [ "Kevin Clark", "Urvashi Khandelwal", "Omer Levy", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for",
      "citeRegEx" : "Clark et al\\.,? 2019",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "Textworld: A learning environment for text",
      "author" : [ "Marc-Alexandre Côté", "Ákos Kádár", "Xingdi Yuan", "Ben Kybartas", "Tavian Barnes", "Emery Fine", "James Moore", "Ruo Yu Tao", "Matthew Hausknecht", "Layla El Asri", "Mahmoud Adada", "Wendy Tay", "Adam Trischler" ],
      "venue" : null,
      "citeRegEx" : "Côté et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Côté et al\\.",
      "year" : 2018
    }, {
      "title" : "2019. BERT: Pre-training",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Devlin et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving semantic parsing for task oriented dialog",
      "author" : [ "Arash Einolghozati", "Panupong Pasupat", "Sonal Gupta", "Rushin Shah", "Mrinal Mohit", "Mike Lewis", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Einolghozati et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Einolghozati et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural natural language inference models partially embed theories of lexical entailment and negation",
      "author" : [ "Atticus Geiger", "Kyle Richardson", "Christopher Potts." ],
      "venue" : "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Net-",
      "citeRegEx" : "Geiger et al\\.,? 2020",
      "shortCiteRegEx" : "Geiger et al\\.",
      "year" : 2020
    }, {
      "title" : "Dynamic predicate logic",
      "author" : [ "Jeroen Groenendijk", "Martin Stokhof." ],
      "venue" : "Linguistics and Philosophy, 14:39–100.",
      "citeRegEx" : "Groenendijk and Stokhof.,? 1991",
      "shortCiteRegEx" : "Groenendijk and Stokhof.",
      "year" : 1991
    }, {
      "title" : "File Change Semantics and the Familiarity Theory of Definiteness, pages",
      "author" : [ "Irene Heim" ],
      "venue" : null,
      "citeRegEx" : "Heim.,? \\Q2008\\E",
      "shortCiteRegEx" : "Heim.",
      "year" : 2008
    }, {
      "title" : "Designing and interpreting probes with control tasks",
      "author" : [ "John Hewitt", "Percy Liang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro-",
      "citeRegEx" : "Hewitt and Liang.,? 2019",
      "shortCiteRegEx" : "Hewitt and Liang.",
      "year" : 2019
    }, {
      "title" : "A structural probe for finding syntax in word representations",
      "author" : [ "John Hewitt", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Hewitt and Manning.,? 2019",
      "shortCiteRegEx" : "Hewitt and Manning.",
      "year" : 2019
    }, {
      "title" : "Human instruction-following with deep reinforcement learning via transfer-learning from text",
      "author" : [ "Felix Hill", "Sona Mokra", "Nathaniel Wong", "Tim Harley" ],
      "venue" : null,
      "citeRegEx" : "Hill et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2020
    }, {
      "title" : "Probing contextual language models for common ground with visual representations",
      "author" : [ "Gabriel Ilharco", "Rowan Zellers", "Ali Farhadi", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Ilharco et al\\.,? 2021",
      "shortCiteRegEx" : "Ilharco et al\\.",
      "year" : 2021
    }, {
      "title" : "Discourse Representation Theory, pages 125–394",
      "author" : [ "Hans Kamp", "Josef Genabith", "Uwe Reyle" ],
      "venue" : null,
      "citeRegEx" : "Kamp et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kamp et al\\.",
      "year" : 2010
    }, {
      "title" : "Revealing the dark secrets of BERT",
      "author" : [ "Olga Kovaleva", "Alexey Romanov", "Anna Rogers", "Anna Rumshisky." ],
      "venue" : "Proceedings of the 2019 Conference on",
      "citeRegEx" : "Kovaleva et al\\.,? 2019",
      "shortCiteRegEx" : "Kovaleva et al\\.",
      "year" : 2019
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Bringing machine learning and compositional semantics together",
      "author" : [ "Percy Liang", "Christopher Potts." ],
      "venue" : "Annu. Rev. Linguist., 1(1):355–376.",
      "citeRegEx" : "Liang and Potts.,? 2015",
      "shortCiteRegEx" : "Liang and Potts.",
      "year" : 2015
    }, {
      "title" : "Simpler context-dependent logical forms via model projections",
      "author" : [ "Reginald Long", "Panupong Pasupat", "Percy Liang." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1456–",
      "citeRegEx" : "Long et al\\.,? 2016",
      "shortCiteRegEx" : "Long et al\\.",
      "year" : 2016
    }, {
      "title" : "Linguistic regularities in continuous space word representations",
      "author" : [ "Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Language models as knowledge bases",
      "author" : [ "Fabio Petroni", "Tim Rocktäschel", "Sebastian Riedel", "Patrick Lewis", "Anton Bakhtin", "Yuxiang Wu", "Alexander Miller" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Petroni et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Petroni et al\\.",
      "year" : 2019
    }, {
      "title" : "Pareto probing: Trading off accuracy for complexity",
      "author" : [ "Tiago Pimentel", "Naomi Saphra", "Adina Williams", "Ryan Cotterell." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3138–3153, On-",
      "citeRegEx" : "Pimentel et al\\.,? 2020",
      "shortCiteRegEx" : "Pimentel et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Research, 21(140):1–67.",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    }, {
      "title" : "How much knowledge can you pack into the parameters of a language model",
      "author" : [ "Adam Roberts", "Colin Raffel", "Noam Shazeer" ],
      "venue" : "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Roberts et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Roberts et al\\.",
      "year" : 2020
    }, {
      "title" : "Does string-based neural MT learn source syntax? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1526– 1534, Austin, Texas",
      "author" : [ "Xing Shi", "Inkit Padhi", "Kevin Knight." ],
      "venue" : "Association for Computational",
      "citeRegEx" : "Shi et al\\.,? 2016",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2016
    }, {
      "title" : "BERT rediscovers the classical NLP pipeline",
      "author" : [ "Ian Tenney", "Dipanjan Das", "Ellie Pavlick." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593– 4601, Florence, Italy. Association for Computational",
      "citeRegEx" : "Tenney et al\\.,? 2019",
      "shortCiteRegEx" : "Tenney et al\\.",
      "year" : 2019
    }, {
      "title" : "Does BERT make any sense? Interpretable word sense disambiguation with contextualized embeddings",
      "author" : [ "Gregor Wiedemann", "Steffen Remus", "Avi Chawla", "Chris Biemann" ],
      "venue" : null,
      "citeRegEx" : "Wiedemann et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wiedemann et al\\.",
      "year" : 2019
    }, {
      "title" : "Infusing Finetuning with Semantic Dependencies",
      "author" : [ "Zhaofeng Wu", "Hao Peng", "Noah A. Smith." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 9:226–242.",
      "citeRegEx" : "Wu et al\\.,? 2021",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2021
    }, {
      "title" : "Introductory notes on dynamic semantics",
      "author" : [ "Seth Yalcin" ],
      "venue" : null,
      "citeRegEx" : "Yalcin.,? \\Q2014\\E",
      "shortCiteRegEx" : "Yalcin.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "Neural language models (NLMs), which place probability distributions over sequences of words, produce contextual word and sentence embeddings that are useful for a variety of language processing tasks (Peters et al., 2018; Lewis et al., 2020).",
      "startOffset" : 201,
      "endOffset" : 242
    }, {
      "referenceID" : 16,
      "context" : "Neural language models (NLMs), which place probability distributions over sequences of words, produce contextual word and sentence embeddings that are useful for a variety of language processing tasks (Peters et al., 2018; Lewis et al., 2020).",
      "startOffset" : 201,
      "endOffset" : 242
    }, {
      "referenceID" : 19,
      "context" : "This usefulness is partially explained by the fact that NLM representations encode lexical relations (Mikolov et al., 2013) and syntactic structure (Tenney et al.",
      "startOffset" : 101,
      "endOffset" : 123
    }, {
      "referenceID" : 2,
      "context" : "But the extent to which NLM training also induces representations of meaning remains a topic of ongoing debate (Bender and Koller, 2020; Wu et al., 2021).",
      "startOffset" : 111,
      "endOffset" : 153
    }, {
      "referenceID" : 28,
      "context" : "But the extent to which NLM training also induces representations of meaning remains a topic of ongoing debate (Bender and Koller, 2020; Wu et al., 2021).",
      "startOffset" : 111,
      "endOffset" : 153
    }, {
      "referenceID" : 2,
      "context" : "Indeed, recent work in NLP points to the lack of exposure of explicit representations of the world external to language as prima facie evidence that LMs cannot represent meaning at all, and thus cannot in general output coherent discourses like (a)–(c) (Bender and Koller, 2020).",
      "startOffset" : 253,
      "endOffset" : 278
    }, {
      "referenceID" : 26,
      "context" : "NLM representations have been found to encode syntactic categories, dependency relations, and coreference information (Tenney et al., 2019; Hewitt and Manning, 2019; Clark et al., 2019).",
      "startOffset" : 118,
      "endOffset" : 185
    }, {
      "referenceID" : 11,
      "context" : "NLM representations have been found to encode syntactic categories, dependency relations, and coreference information (Tenney et al., 2019; Hewitt and Manning, 2019; Clark et al., 2019).",
      "startOffset" : 118,
      "endOffset" : 185
    }, {
      "referenceID" : 3,
      "context" : "NLM representations have been found to encode syntactic categories, dependency relations, and coreference information (Tenney et al., 2019; Hewitt and Manning, 2019; Clark et al., 2019).",
      "startOffset" : 118,
      "endOffset" : 185
    }, {
      "referenceID" : 27,
      "context" : "Within the realm of semantics, existing work has identified representations of word meaning (e.g., finegrained word senses; Wiedemann et al. 2019) and predicate–argument structures like frames and semantic roles (Kovaleva et al.",
      "startOffset" : 92,
      "endOffset" : 146
    }, {
      "referenceID" : 15,
      "context" : "2019) and predicate–argument structures like frames and semantic roles (Kovaleva et al., 2019).",
      "startOffset" : 71,
      "endOffset" : 94
    }, {
      "referenceID" : 25,
      "context" : "In all these studies, the main experimental paradigm is probing (Shi et al., 2016; Belinkov and Glass, 2019): given a fixed source of representations (e.",
      "startOffset" : 64,
      "endOffset" : 108
    }, {
      "referenceID" : 0,
      "context" : "In all these studies, the main experimental paradigm is probing (Shi et al., 2016; Belinkov and Glass, 2019): given a fixed source of representations (e.",
      "startOffset" : 64,
      "endOffset" : 108
    }, {
      "referenceID" : 5,
      "context" : ", 2016; Belinkov and Glass, 2019): given a fixed source of representations (e.g. the BERT language model; Devlin et al. 2019) and a linguistic label of interest (e.",
      "startOffset" : 75,
      "endOffset" : 125
    }, {
      "referenceID" : 10,
      "context" : "A phenomenon is judged to be encoded by a model if the probe’s accuracy cannot be explained by its accuracy when trained on control tasks (Hewitt and Liang, 2019) or baseline models (Pimentel et al.",
      "startOffset" : 138,
      "endOffset" : 162
    }, {
      "referenceID" : 22,
      "context" : "A phenomenon is judged to be encoded by a model if the probe’s accuracy cannot be explained by its accuracy when trained on control tasks (Hewitt and Liang, 2019) or baseline models (Pimentel et al., 2020).",
      "startOffset" : 182,
      "endOffset" : 205
    }, {
      "referenceID" : 9,
      "context" : "This is the subject of dynamic semantics in linguistics (Heim, 2008; Kamp et al., 2010; Groenendijk and Stokhof, 1991).",
      "startOffset" : 56,
      "endOffset" : 118
    }, {
      "referenceID" : 14,
      "context" : "This is the subject of dynamic semantics in linguistics (Heim, 2008; Kamp et al., 2010; Groenendijk and Stokhof, 1991).",
      "startOffset" : 56,
      "endOffset" : 118
    }, {
      "referenceID" : 8,
      "context" : "This is the subject of dynamic semantics in linguistics (Heim, 2008; Kamp et al., 2010; Groenendijk and Stokhof, 1991).",
      "startOffset" : 56,
      "endOffset" : 118
    }, {
      "referenceID" : 6,
      "context" : "LM pretraining is useful for semantic parsing (Einolghozati et al., 2019), instruction following (Hill et al.",
      "startOffset" : 46,
      "endOffset" : 73
    }, {
      "referenceID" : 12,
      "context" : ", 2019), instruction following (Hill et al., 2020), and even image retrieval (Ilharco et al.",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 13,
      "context" : ", 2020), and even image retrieval (Ilharco et al., 2021).",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 21,
      "context" : "LM pretraining has also been found to be useful for tasks like factoid question answering (Petroni et al., 2019; Roberts et al., 2020).",
      "startOffset" : 90,
      "endOffset" : 134
    }, {
      "referenceID" : 24,
      "context" : "LM pretraining has also been found to be useful for tasks like factoid question answering (Petroni et al., 2019; Roberts et al., 2020).",
      "startOffset" : 90,
      "endOffset" : 134
    }, {
      "referenceID" : 16,
      "context" : "Model In all experiments, the encoder E comes from a BART (Lewis et al., 2020) or T5 (Raffel et al.",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 16,
      "context" : "Except where noted, BART is pretrained on OpenWebText, BookCorpus, CCNews, and Stories (Lewis et al., 2020), T5 is pretrained on C4 (Raffel et al.",
      "startOffset" : 87,
      "endOffset" : 107
    }, {
      "referenceID" : 18,
      "context" : "Data: Alchemy Alchemy, the first dataset used in our experiments, is derived from the SCONE (Long et al., 2016) semantic parsing tasks.",
      "startOffset" : 92,
      "endOffset" : 111
    }, {
      "referenceID" : 4,
      "context" : "Data: TextWorld TextWorld (Côté et al., 2018) is a platform for generating synthetic worlds for text-based games, used to test RL agents.",
      "startOffset" : 26,
      "endOffset" : 45
    }, {
      "referenceID" : 10,
      "context" : "The remap and ∆ 6= 0 probes described here are analogous to control tasks (Hewitt and Liang, 2019).",
      "startOffset" : 74,
      "endOffset" : 98
    }, {
      "referenceID" : 1,
      "context" : "This finding has important implications for research aimed at improving factuality and and coherence in NLMs: future work might probe LMs for the the states and properties ascribed to entities the first time they are mentioned (which may reveal biases learned from training data; Bender et al. 2021), or correct errors in generation by directly editing representations.",
      "startOffset" : 227,
      "endOffset" : 299
    } ],
    "year" : 2021,
    "abstractText" : "Does the effectiveness of neural language models derive entirely from accurate modeling of surface word co-occurrence statistics, or do these models represent and reason about the world they describe? In BART and T5 transformer language models, we identify contextual word representations that function as models of entities and situations as they evolve throughout a discourse. These neural representations have functional similarities to linguistic models of dynamic semantics: they support a linear readout of each entity’s current properties and relations, and can be manipulated with predictable effects on language generation. Our results indicate that prediction in pretrained neural language models is supported, at least in part, by dynamic representations of meaning and implicit simulation of entity state, and that this behavior can be learned with only text as training data.1",
    "creator" : "LaTeX with hyperref"
  }
}