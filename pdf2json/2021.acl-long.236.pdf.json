{
  "name" : "2021.acl-long.236.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Meta-KD: A Meta Knowledge Distillation Framework for Language Model Compression across Domains",
    "authors" : [ "Haojie Pan", "Chengyu Wang", "Minghui Qiu", "Yichang Zhang", "Yaliang Li", "Jun Huang" ],
    "emails" : [ "minghui.qmh}@alibaba-inc.com", "huangjun.hj}@alibaba-inc.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3026–3036\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3026"
    }, {
      "heading" : "1 Introduction",
      "text" : "Pre-trained Language Models (PLM) such as BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019) have achieved significant success with the two-stage “pre-training and fine-tuning” process. Despite the performance gain achieved in various NLP tasks, the large number of model parameters\n∗H. Pan and C. Wang contributed equally to this work. †M. Qiu is the corresponding author.\nand the long inference time have become the bottleneck for PLMs to be deployed in real-time applications, especially on mobile devices (Jiao et al., 2019; Sun et al., 2020; Iandola et al., 2020). Thus, there are increasing needs for PLMs to reduce the model size and the computational overhead while keeping the prediction accuracy.\nKnowledge Distillation (KD) (Hinton et al., 2015) is one of the promising ways to distill the knowledge from a large “teacher” model to a small “student” model. Recent studies show that KD can be applied to compress PLMs with acceptable performance loss (Sanh et al., 2019; Sun et al., 2019b; Jiao et al., 2019; Turc et al., 2019; Chen et al., 2020a). However, those methods mainly focus on single-domain KD. Hence, student models can only learn from their in-domain teachers, paying little attention to acquiring knowledge from other domains. It has been shown that it is beneficial to consider cross-domain information for KD, by either training a teacher using cross-domain data or multiple teachers from multiple domains (You et al., 2017; Liu et al., 2019; Yang et al., 2020;\nPeng et al., 2020). Consider an academic scenario in Figure 1. A typical way for a physics student to learn physics equations is to directly learn from his/her physics teacher. If we have a math teacher to teach him/her basic knowledge of equations, the student can obtain a better understanding of physics equations. This “knowledge transfer” technique in KD has been proved efficient only when two domains are close to each other (Hu et al., 2019). In reality, however, it is highly risky as teachers of other domains may pass non-transferable knowledge to the student model, which is irrelevant to the current domain and hence harms the overall performance (Tan et al., 2017; Wang et al., 2020). Besides, current studies find multi-task fine-tuning of BERT does not necessarily yield better performance across all the tasks (Sun et al., 2019a).\nTo address these issues, we leverage the idea of meta-learning to capture transferable knowledge across domains, as recent studies have shown that meta-learning can improve the model generalization ability across domains (Finn et al., 2017; Javed and White, 2019; Yin, 2020; Ye et al., 2020). We further notice that meta-knowledge is also helpful for cross-domain KD. Re-consider the example in Figure 1. If we have an “all-purpose teacher” (i.e., the meta-teacher) who has the knowledge of both physics principles and mathematical equations (i.e., the general knowledge of the two courses), the student may learn physics equations better with the teacher, compared to the other two cases. Hence, it is necessary to train an “all-purpose teacher” model for domain-specific student models to learn.\nIn this paper, we propose the Meta-Knowledge Distillation (Meta-KD) framework, which facilities cross-domain KD. Generally speaking, MetaKD consists of two parts, meta-teacher learning and meta-distillation. Different from the K-way N-shot problems addressed in traditional metalearning (Vanschoren, 2018), we propose to train a “meta-learner” as the meta-teacher, which learns the transferable knowledge across domains so that it can fit new domains easily. The meta-teacher is jointly trained with multi-domain datasets to acquire the instance-level and feature-level metaknowledge. For each domain, the student model learns to solve the task over a domain-specific dataset with guidance from the meta-teacher. To improve the student’s distillation ability, the metadistillation module minimizes the distillation loss from both intermediate layers, output layers, and\ntransferable knowledge, combined with domainexpertise weighting techniques.\nTo verify the effectiveness of Meta-KD, we conduct extensive experiments on two NLP tasks across multiple domains, namely natural language inference (Williams et al., 2018) and sentiment analysis (Blitzer et al., 2007). Experimental results show the effectiveness and superiority of the proposed Meta-KD framework. Moreover, we find our method performs well especially when i) the in-domain dataset is very small or ii) there is no in-domain dataset during the training of the metateacher. In summary, the contributions of this study can be concluded as follows:\n• To the best of our knowledge, this work is the first to explore the idea of meta-teacher learning for PLM compression across domains.\n• We propose the Meta-KD framework to address the task. In Meta-KD, the meta-teacher digests transferable knowledge across domains, and selectively passes the knowledge to student models with different domain expertise degrees.\n• We conduct extensive experiments to demonstrate the superiority of Meta-KD and also explore the capability of this framework in the settings where the training data is scarce.\nThe rest of this paper is summarized as follows. Section 2 describes the related work. The detailed techniques of the Meta-KD framework are presented in Section 3. The experiments are reported in Section 4. Finally, we conclude our work and discuss the future work in Section 5. 1"
    }, {
      "heading" : "2 Related Work",
      "text" : "Our study is close to the following three lines of studies, introduced below."
    }, {
      "heading" : "2.1 Knowledge Distillation (KD)",
      "text" : "KD was first proposed by (Hinton et al., 2015), aiming to transfer knowledge from an ensemble or a large model into a smaller, distilled model. Most of the KD methods focus on utilizing either the dark knowledge, i.e., predicted outputs (Hinton et al., 2015; Chen et al., 2020b; Furlanello et al., 2018; You et al., 2017) or hints, i.e., the intermediate\n1The experimental code can be found in https: //github.com/alibaba/EasyTransfer/tree/ master/scripts/metaKD.\nrepresentations (Romero et al., 2015; Yim et al., 2017; You et al., 2017) or the relations between layers (Yim et al., 2017; Tarvainen and Valpola, 2017) of teacher models. You et al. (2017) also find that multiple teacher networks together can provide comprehensive guidance that is beneficial for training the student network. Ruder et al. (2017) show that multiple expert teachers can improve the performances of sentiment analysis on unseen domains. Tan et al. (2019) apply the multiple-teachers framework in KD to build a state-of-the-art multilingual machine translation system. Feng et al. (2021) considers to build a model to automatically augment data for KD. Our work is one of the first attempts to learn a meta-teacher model that digest transferable knowledge from multiple domains to benefit KD on the target domain."
    }, {
      "heading" : "2.2 PLM Compression",
      "text" : "Due to the massive number of parameters in PLMs, it is highly necessary to compress PLMs for application deployment. Previous approaches on compressing PLMs such as BERT (Devlin et al., 2019) include KD (Hinton et al., 2015), parameter sharing (Ullrich et al., 2017), pruning (Han et al., 2015) and quantization (Gong et al., 2014). In this work, we mainly focus on KD for PLMs. In the literature, Tang et al. (2019) distill BERT into BiLSTM networks to achieve comparable results with ELMo (Peters et al., 2018). Chen et al. (2021) studies cross-domain KD to facilitate cross-domain knowledge transferring. Zhao et al. (2019) use dual distillation to reduce the vocabulary size and the embedding size. DistillBERT (Sanh et al., 2019) applies KD loss in the pre-training stage, while BERT-PKD (Sun et al., 2019b) distill BERT into shallow Transformers in the fine-tuning stage. TinyBERT (Jiao et al., 2019) further distills BERT with a two-stage KD process for hidden attention matrices and embedding matrices. AdaBERT (Chen et al., 2020a) uses neural architecture search to adaptively find small architectures. Our work improves the prediction accuracy of compressed PLMs by leveraging cross-domain knowledge, which is complementary to previous works."
    }, {
      "heading" : "2.3 Transfer Learning and Meta-learning",
      "text" : "TL has been proved to improve the performance on the target domain by leveraging knowledge from related source domains (Pan and Yang, 2010; Mou et al., 2016; Liu et al., 2017; Yang et al., 2017). In most NLP tasks, the “shared-private” architecture\nis applied to learn domain-specific representations and domain-invariant features (Mou et al., 2016; Liu et al., 2017; Chen et al., 2018, 2019). Compared to TL, the goal of meta-learning is to train meta-learners that can adapt to a variety of different tasks with little training data (Vanschoren, 2018). A majority of meta-learning methods for include metric-based (Snell et al., 2017; Pan et al., 2019), model-based (Santoro et al., 2016; Bartunov et al., 2020) and model-agnostic approaches (Finn et al., 2017, 2018; Vuorio et al., 2019). Meta-learning can also be applied to KD in some computer vision tasks (Lopes et al., 2017; Jang et al., 2019; Liu et al., 2020; Bai et al., 2020; Li et al., 2020). For example, Lopes et al. (2017) record per-layer metadata for the teacher model to reconstruct a training set, and then adopts a standard training procedure to obtain the student model. In our work, we use instance-based and feature-based meta-knowledge across domains for the KD process."
    }, {
      "heading" : "3 The Meta-KD Framework",
      "text" : "In this section, we formally introduce the MetaKD framework. We begin with a brief overview of Meta-KD. After that, the techniques are elaborated."
    }, {
      "heading" : "3.1 An Overview of Meta-KD",
      "text" : "Take text classification as an example. Assume there are K training sets, corresponding to K domains. In the k-th dataset Dk = {X (i) k , y (i) k } Nk i=1, X (i) k is the i-th sample\n2 and y(i)k is the class label of X(i)k . Nk is the total number of samples in Dk. LetMk be the large PLM fine-tuned on Dk. Given the K datasets, the goal of Meta-KD is to obtain the K student models S1, · · · ,SK that are small in size but has similar performance compared to the K large PLMs, i.e.,M1, · · · ,MK .\nIn general, the Meta-KD framework can be divided into the following two stages:\n• Meta-teacher Learning: Learn a metateacher M over all domains K⋃ k=1 Dk. The\nmodel digests transferable knowledge from each domain and has better generalization while supervising domain-specific students.\n• Meta-distillation: Learn K in-domain students S1, · · · ,SK that perform well in their\n2X (i) k can be a sentence, a sentence pair or any other tex-\ntual units, depending on the task inputs.\nrespective domains, given only in-domain data Dk and the meta-teacherM as input.\nDuring the learning process of the meta-teacher, we consider both instance-level and feature-level transferable knowledge. Inspired by prototypebased meta-learning (Snell et al., 2017; Pan et al., 2019), the meta-teacher model should memorize more information about prototypes. Hence, we compute sample-wise prototype scores as the instance-level transferable knowledge. The loss of the meta-teacher is defined as the sum of classification loss across all K domains with prototypebased, instance-specific weighting. Besides, it also learns feature-level transferable knowledge by adding a domain-adversarial loss as an auxiliary loss. By these steps, the meta-teacher is more generalized and digests transferable knowledge before supervising student models.\nFor meta-distillation, each sample is weighted by a domain-expertise score to address the metateacher’s capability for this sample. The transferable knowledge is also learned for the students from the meta-teacher. The overall meta-distillation loss is a combination of the Mean Squared Error (MSE) loss from intermediate layers of both models (Sun et al., 2019b; Jiao et al., 2019), the soft cross-entropy loss from output layers (Hinton et al., 2015), and the transferable knowledge distillation loss, with instance-specific domain-expertise weighting applied."
    }, {
      "heading" : "3.2 Meta-teacher Learning",
      "text" : "We take BERT (Devlin et al., 2019) as our base learner for text classification due to its wide popularity. For each sample X(i)k , the input is: [CLS], tok(i)k,1, tok (i) k,2, · · · , [SEP], where tok (i) k,n is the n-th token in X (i) k . The last hidden outputs of this sequence is denoted as h[CLS], h(tok (i) k,1), h(tok (i) k,2), .., h(tok (i) k,N ), where h(tok (i) k,j) represents the last layer embedding of the j-th token in X(i)k , and N is the maximum sequence length. For simplicity, we define h(X(i)k ) as the average pooling of the token embeddings, i.e., h(X(i)k ) = ∑N n=1 h(tok (i) k,n). Learning Instance-level Transferable Knowledge. To select transferable instances across domains, we compute a prototype score t(i)k for each sample X(i)k . Here, we treat the prototype representation for the m-th class of the k-th domain:\np (m) k =\n1\n|D(m)k |\n∑ X\n(i) k ∈D (m) k\nh(X (i) k ), where D (m) k\nis the k-th training set with the m-th class label. The prototype score t(i)k is:\nt (i) k =α cos(p (m) k , h(X (i) k ))\n+ ζ K(k′ 6=k)∑ k′=1 cos(p (m) k′ , h(X (i) k )),\nwhere cos is the cosine similarity function, α is a pre-defined hyper-parameter and ζ = 1−αK−1 . We can see that the definition of the prototype score here is different from previous meta-learning, as we require that an instance X(i)k should be close to its class prototype representation in the embedding space (i.e., p(m)k ), as well as the prototype representations in out-of-domain datasets (i.e., p(m)k′ with k′ = 1, · · · ,K, k′ 6= k). This is because the meta-teacher should learn more from instances that are prototypical across domains instead of indomain only. For the text classification task, the cross-entropy loss of the meta-teacher is defined using the cross-entropy loss with the prototype score as a weight assigned to each instance. Learning Feature-level Transferable Knowledge. Apart from the cross-entropy loss, we propose the domain-adversarial loss to increase the meta-teacher’s ability for learning feature-level transferable knowledge.\nFor each sampleX(i)k , we first learn an |h(X (i) k )|- dimensional domain embedding of the true domain label d(i)k by mapping one-hot domain representations to the embeddings, denoted as ED(X(i)k ). A sub-network is then constructed by:\nhd(X (i) k )) = tanh((h(X (i) k ) + ED(X (i) k ))W + b),\nwhere W and b are sub-network parameters. The domain-adversarial loss for X(i)k is defined as:\nLDA(X(i)k ) = − K∑ k=1 1 k=z (i) k · log σ(hd(X (i) k )),\nwhere σ is the K-way domain classifier, and 1 is the indicator function that returns 1 if k = z(i)k , and 0 otherwise. Here, z(i)k 6= d (i) k is a false domain label of X(i)k 3. Hence, we deliberately maximize the probability that the meta-teacher makes the wrong 3For ease of implementation, we shuffle the domain labels of all instances in a mini-batch.\npredictions of domain labels. We call hd(X (i) k )) as the transferable knowledge for X(i)k , which is more insensitive to domain differences.\nLet LCE(X(i)k ) be the normal cross-entropy loss of the text classification task. The total loss of the meta-teacher LMT is the combination of weighted LCE(X(i)k ) and LDA(X (i) k ), shown as follows: LMT = ∑\nX (i) k ∈ K⋃ k=1 Dk\nt (i) k LCE(X (i) k ) + γ1LDA(X (i) k )∑K\nk=1 |Dk| ,\nwhere γ1 is the factor to represent how the domainadversarial loss contributes to the overall loss."
    }, {
      "heading" : "3.3 Meta-distillation",
      "text" : "We take BERT as our meta-teacher and use smaller BERT models as student models. The distillation framework is shown in Figure 2. In our work, we distill the knowledge in the meta-teacher model considering the following five elements: input embeddings, hidden states, attention matrices, output logits, and transferable knowledge. The KD process of input embeddings, hidden states and attention matrices follows the common practice (Sun et al., 2019b; Jiao et al., 2019). Recall thatM and Sk are the meta-teacher and the k-th student model. Let Lembd(M,Sk, X (i) k ), Lhidn(M,Sk, X (i) k ) and Lattn(M,Sk, X (i) k ) be the sample-wise MSE loss values of input embeddings, hidden states and attention matrices of the two models, respectively. Here, Lembd(M,Sk, X (i) k ), Lhidn(M,Sk, X (i) k ) and Lattn(M,Sk, X (i) k ) refer to the sum of MSE values among multiple hidden layers. We refer\ninterested readers to Jiao et al. (2019) for more details. Lpred(M,Sk, X (i) k ) is the cross-entropy loss of “softened” output logits, parameterized by the temperature (Hinton et al., 2015). A naive approach to formulating the total KD loss Lkd is the sum of all previous loss functions, i.e.,\nLkd = ∑\nX (i) k ∈Dk\n( Lembd(M,Sk, X (i) k )+\nLhidn(M,Sk, X (i) k ) + Lattn(M,Sk, X (i) k )+ Lpred(M,Sk, X (i) k ) ) .\nHowever, the above approach does not give special considerations to the transferable knowledge of the meta-teacher. Let hMd (X (i) k ) and h S d (X (i) k ) be the transferable knowledge of the meta-teacher and the student model w.r.t. the input X(i)k . We further define the transferable knowledge distillation loss LTKD(M,Sk, X (i) k ) as follows:\nLtkd(M,Sk, X (i) k ) =\n1 |Dk| ∑\nX (i) k ∈Dk\nMSE ( hMd (X (i) k )W M d , h S d (X (i) k ) )\nwhere WMd is a learnable projection matrix to match the dimension between hMd (X (i) k ) and hSd (X (i) k ), andMSE is the MSE loss function w.r.t. single element. In this way, we encourage student models to learn more transferable knowledge from the meta-teacher.\nWe further notice that although the knowledge of the meta-teacher should be highly transferable, there still exists the domain gap between the metateacher and domain-specific student models. In this\nwork, for each sample X(i)k , we define the domain expertise weight λ(i)k as follows:\nλ (i) k =\n1 + t (i) k\nexp(ŷ (i) k −y (i) k ) 2 +1\n,\nwhere ŷ(i)k is the predicted result of X (i) k ’s class label. Here, the weight λ(i)k is large when the metateacher model i) has a large prototype score t(i)k and ii) makes correct predictions on the target input, i.e., ŷ (i) k = y (i) k . We can see that the weight reflects how well the meta-teacher can supervise the student on a specific input. Finally, we derive the complete formulation of the KD loss L′kd as follows:\nL′kd = ∑\nX (i) k ∈Dk\nλ (i) k ( Lembd(M,Sk, X (i) k )+\nLhidn(M,Sk, X (i) k ) + Lattn(M,Sk, X (i) k )+ Lpred(M,Sk, X (i) k ) ) + γ2Ltkd(M,Sk, X (i) k ) ) ,\nwhere γ2 is the transferable KD factor to represent how the transferable knowledge distillation loss contributes to the overall loss."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we conduct extensive experiments to evaluate the Meta-KD framework on two popular text mining tasks across domains."
    }, {
      "heading" : "4.1 Tasks and Datasets",
      "text" : "We evaluate Meta-KD over natural language inference and sentiment analysis, using the following two datasets MNLI and Amazon Reviews. The data statistics are in Table 1.\n• MNLI (Williams et al., 2018) is a largescale, multi-domain natural language inference dataset for predicting the entailment relation between two sentences, containing five domains (genres). After filtering samples with no labels available, we use the original development set as our test set and randomly sample 10% of the training data as a development set in our setting.\n• Amazon Reviews (Blitzer et al., 2007) is a multi-domain sentiment analysis dataset, widely used in multi-domain text classification tasks. The reviews are annotated as positive or negative. For each domain, there are 2,000 labeled reviews. We randomly split the data into train, development, and test sets."
    }, {
      "heading" : "4.2 Baselines",
      "text" : "For the teacher side, to evaluate the cross-domain distillation power of the meta-teacher model, we consider the following models as baseline teachers:\n• BERT-single: Train the BERT teacher model on the target distillation domain only. If we have K domains, then we will have K BERTsingle teachers.\n• BERT-mix: Train the BERT teacher on a combination of K-domain datasets. Hence, we have one BERT-mix model as the teacher model for all domains.\n• BERT-mtl: Similar to the “one-teacher” paradigm as in BERT-mix, but the teacher model is generated by the multi-task finetuning approach (Sun et al., 2019a).\n• Multi-teachers: It uses K domain-specific BERT-single models to supervise K student models, ignoring the domain difference.\nFor the student side, we follow TinyBERT (Jiao et al., 2019) to use smaller BERT models as our student models. In single-teacher baselines (i.e., BERT-single, BERT-mix and BERT-mtl), we use TinyBERT-KD as our baseline KD approach. In multi-teachers, because TinyBERT-KD does not naturally support distilling from multiple teacher models, we implement a variant of the TinyBERTKD process based on MTN-KD (You et al., 2017), which uses averaged softened outputs as the incorporation of multiple teacher networks in the output layer. In practice, we first learn the representations of the student models by TinyBERT, then apply MTN-KD for output-layer KD."
    }, {
      "heading" : "4.3 Implementation Details",
      "text" : "In the implementation, we use the original BERTB model (L=12, H=768, A=12, Total Parameters=110M) as the initialization of all of the teachers, and use the BERTS model (L=4, H=312, A=12, Total Parameters=14.5M) as the initialization of all the students4.\nThe hyper-parameter settings of the meta-teacher model are as follows. We train 3-4 epochs with the learning rate to be 2e-5. The batch size and γ1 are chosen from {16, 32, 48} and {0.1, 0.2, 0.5}, respectively. All the hyper-parameters are tuned on the development sets.\n4https://github.com/huawei-noah/ Pretrained-Language-Model/tree/master/ TinyBERT\nFor meta-distillation, we choose the hidden layers in {3, 6, 9, 12} of the teacher models in the baselines and the meta-teacher model in our approach to learn the representations of the student models. Due to domain difference, we train student models in 3-10 epochs, with a learning rate of 5e-5. The batch size and γ2 are tuned from {32, 256} and {0.1, 0.2, 0.3, 0.4, 0.5} for intermediatelayer distillation, respectively. Following Jiao et al. (2019), for prediction-layer distillation, we run the method for 3 epochs, with the batch size and learning rate to be 32 and 3e-5. The experiments are implemented on PyTorch and run on 8 Tsela V100 GPUs."
    }, {
      "heading" : "4.4 Experimental Results",
      "text" : "Table 2 and Table 3 show the general testing performance over MNLI and Amazon Reviews of baselines and Meta-KD, in terms of accuracy. From the results, we have the following three major insights:\n• Compared to all the baseline teacher models, using the meta-teacher for KD consistently achieves the highest accuracy in both datasets. Our method can help to significantly reduce model size while preserving similar performance, especially in Amazon review, we reduce the model size to 7.5x smaller with only a minor performance drop (from 89.9 to 89.4).\n• The meta-teacher has similar performance as BERT-mix and BERT-mtl, but shows to be a better teacher for distillation, as Meta-teacher\nTinyBERT-KD−−−−−−−−→ BERTS and Meta-teacher Meta-distillation−−−−−−−−−→ BERTS have better performance than other methods. This shows the meta-teacher is capable of learning more transferable knowledge to help the student. The fact that Meta-teacher→Metadistillation has better performance than other distillation methods confirms the effectiveness of the proposed Meta-KD method.\n• Meta-KD gains more improvement on the small datasets than large ones, e.g. it improves from 86.7 to 89.4 in Amazon Reviews while 79.3 to 80.4 in MNLI. This motivates us to explore our model performance on domains with few or no training samples"
    }, {
      "heading" : "4.5 Ablation Study",
      "text" : "We further investigate Meta-KD’s capability with regards to different portion training data for both of two phases and explore how the transferable knowledge distillation loss contributes to final results."
    }, {
      "heading" : "4.5.1 No In-domain Data during Meta-teacher Learning",
      "text" : "In this set of experiments, we consider a special case where we assume all the “fiction” domain data in MNLI is unavailable. Here, we train a meta-teacher without the “fiction” domain dataset and use the distillation method proposed in Jiao et al. (2019) to produce the student model for the “fiction” domain with in-domain data during distillation. The results are shown in Table 4. We find that KD from the meta-teacher can have large\nimprovement, compared to KD from other outdomain teachers. Additionally, learning from the out-domain meta-teacher has a similar performance to KD from the in-domain “fiction” teacher model itself. It shows the Meta-KD framework can be applied in applications for emerging domains."
    }, {
      "heading" : "4.5.2 Few In-domain Data Available during Meta-distillation",
      "text" : "We randomly sample a part of the MNLI dataset as the training data in this setting. The sample rates that we choose include 0.01, 0.02, 0.05, 0.1 and 0.2. The sampled domain datasets are employed for training student models when learning from the in-domain teacher or the meta-teacher. The experimental results are shown in Figure 3, with results reported by the improvement rate in averaged accuracy. The experimental results show that when less data is available, the improvement rate is much larger. For example, when we only have 1% of the original MNLI training data, the accuracy can be\nincreased by approximately 10% when the student tries to learn from the meta-teacher. It shows MetaKD can be more beneficial when we have fewer in-domain data."
    }, {
      "heading" : "4.5.3 Influence of the Transferable Knowledge Distillation Loss",
      "text" : "Here, we explore how the transferable KD factor γ2 affects the distillation performance over the Amazon Reviews dataset. We tune the value of γ2 from 0.1 to 1.0, with results are shown in Figure 4. We find that the optimal value of γ2 generally lies in the range of 0.2 - 0.5. The trend of accuracy is different in the domain “DVD” is different from those of the remaining three domains. This means the benefits from transferable knowledge of the meta-teacher vary across domains."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "In this work, we propose the Meta-KD framework which consists of meta-teacher learning and meta distillation to distill PLMs across domains. Experiments on two widely-adopted public multidomain datasets show that Meta-KD can train a meta-teacher to digest knowledge across domains to help better teach in-domain students. Quantitative evaluations confirm the effectiveness of MetaKD and also show the capability of Meta-KD in the settings where the training data is scarce i.e. there is no or few in-domain data. In the future, we will examine the generalization capability of Meta-KD in other application scenarios and apply other meta-learning techniques to KD for PLMs."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work is supported by Open Research Projects of Zhejiang Lab (No. 2019KD0AD01/004). Any\nopinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor."
    } ],
    "references" : [ {
      "title" : "Few shot network compression via cross distillation",
      "author" : [ "Haoli Bai", "Jiaxiang Wu", "Irwin King", "Michael R. Lyu." ],
      "venue" : "AAAI, pages 3203–3210.",
      "citeRegEx" : "Bai et al\\.,? 2020",
      "shortCiteRegEx" : "Bai et al\\.",
      "year" : 2020
    }, {
      "title" : "Meta-learning deep energy-based memory models",
      "author" : [ "Sergey Bartunov", "Jack W. Rae", "Simon Osindero", "Timothy P. Lillicrap." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Bartunov et al\\.,? 2020",
      "shortCiteRegEx" : "Bartunov et al\\.",
      "year" : 2020
    }, {
      "title" : "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification",
      "author" : [ "John Blitzer", "Mark Dredze", "Fernando Pereira." ],
      "venue" : "ACL.",
      "citeRegEx" : "Blitzer et al\\.,? 2007",
      "shortCiteRegEx" : "Blitzer et al\\.",
      "year" : 2007
    }, {
      "title" : "Multi-domain gated CNN for review helpfulness prediction",
      "author" : [ "Cen Chen", "Minghui Qiu", "Yinfei Yang", "Jun Zhou", "Jun Huang", "Xiaolong Li", "Forrest Sheng Bao." ],
      "venue" : "The World Wide Web Conference, pages 2630–2636.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Cross-domain knowledge distillation for retrieval-based question answering systems",
      "author" : [ "Cen Chen", "Chengyu Wang", "Minghui Qiu", "Dehong Gao", "Linbo Jin", "Wang Li." ],
      "venue" : "The World Wide Web Conference.",
      "citeRegEx" : "Chen et al\\.,? 2021",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "Cross-domain review helpfulness prediction based on convolutional neural networks with auxiliary domain discriminators",
      "author" : [ "Cen Chen", "Yinfei Yang", "Jun Zhou", "Xiaolong Li", "Forrest Sheng Bao." ],
      "venue" : "NAACL-HLT, pages 602–607.",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Adabert: Taskadaptive BERT compression with differentiable neural architecture search",
      "author" : [ "Daoyuan Chen", "Yaliang Li", "Minghui Qiu", "Zhen Wang", "Bofang Li", "Bolin Ding", "Hongbo Deng", "Jun Huang", "Wei Lin", "Jingren Zhou." ],
      "venue" : "IJCAI, pages 2463–2469.",
      "citeRegEx" : "Chen et al\\.,? 2020a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Online knowledge distillation with diverse peers",
      "author" : [ "Defang Chen", "Jian-Ping Mei", "Can Wang", "Yan Feng", "Chun Chen." ],
      "venue" : "AAAI, pages 3430–3437.",
      "citeRegEx" : "Chen et al\\.,? 2020b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL-HLT, pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to augment for data-scarce domain bert knowledge distillation",
      "author" : [ "Lingyun Feng", "Minghui Qiu", "Yaliang Li", "Hai-Tao Zheng", "Ying Shen." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Feng et al\\.,? 2021",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2021
    }, {
      "title" : "Model-agnostic meta-learning for fast adaptation of deep networks",
      "author" : [ "Chelsea Finn", "Pieter Abbeel", "Sergey Levine." ],
      "venue" : "ICML, pages 1126–1135.",
      "citeRegEx" : "Finn et al\\.,? 2017",
      "shortCiteRegEx" : "Finn et al\\.",
      "year" : 2017
    }, {
      "title" : "Probabilistic model-agnostic meta-learning",
      "author" : [ "Chelsea Finn", "Kelvin Xu", "Sergey Levine." ],
      "venue" : "NeurIPS, pages 9537–9548.",
      "citeRegEx" : "Finn et al\\.,? 2018",
      "shortCiteRegEx" : "Finn et al\\.",
      "year" : 2018
    }, {
      "title" : "Born-again neural networks",
      "author" : [ "Tommaso Furlanello", "Zachary Chase Lipton", "Michael Tschannen", "Laurent Itti", "Anima Anandkumar." ],
      "venue" : "ICML, pages 1602–1611.",
      "citeRegEx" : "Furlanello et al\\.,? 2018",
      "shortCiteRegEx" : "Furlanello et al\\.",
      "year" : 2018
    }, {
      "title" : "Compressing deep convolutional networks using vector quantization",
      "author" : [ "Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir D. Bourdev." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Gong et al\\.,? 2014",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning both weights and connections for efficient neural network",
      "author" : [ "Song Han", "Jeff Pool", "John Tran", "William J. Dally." ],
      "venue" : "NeurIPS, pages 1135– 1143.",
      "citeRegEx" : "Han et al\\.,? 2015",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2015
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeffrey Dean." ],
      "venue" : "NIPS Deep Learning and Representation Learning Workshop.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Domaininvariant feature distillation for cross-domain sentiment classification",
      "author" : [ "Mengting Hu", "Yike Wu", "Shiwan Zhao", "Honglei Guo", "Renhong Cheng", "Zhong Su." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Hu et al\\.,? 2019",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2019
    }, {
      "title" : "Squeezebert: What can computer vision teach NLP about efficient neural networks? CoRR",
      "author" : [ "Forrest N. Iandola", "Albert E. Shaw", "Ravi Krishna", "Kurt Keutzer" ],
      "venue" : null,
      "citeRegEx" : "Iandola et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Iandola et al\\.",
      "year" : 2020
    }, {
      "title" : "Metadistiller: Network selfboosting via meta-learned top-down distillation",
      "author" : [ "Yunhun Jang", "Hankook Lee", "Sung Ju Hwang", "Jinwoo Shin." ],
      "venue" : "ICML.",
      "citeRegEx" : "Jang et al\\.,? 2019",
      "shortCiteRegEx" : "Jang et al\\.",
      "year" : 2019
    }, {
      "title" : "Metalearning representations for continual learning",
      "author" : [ "Khurram Javed", "Martha White." ],
      "venue" : "NeurIPS, pages 1818–1828.",
      "citeRegEx" : "Javed and White.,? 2019",
      "shortCiteRegEx" : "Javed and White.",
      "year" : 2019
    }, {
      "title" : "Tinybert: Distilling BERT for natural language understanding",
      "author" : [ "Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Jiao et al\\.,? 2019",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 2019
    }, {
      "title" : "Few sample knowledge distillation for efficient network compression",
      "author" : [ "Tianhong Li", "Jianguo Li", "Zhuang Liu", "Changshui Zhang." ],
      "venue" : "CVPR, pages 14627–14635.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Metadistiller: Network selfboosting via meta-learned top-down distillation",
      "author" : [ "Benlin Liu", "Yongming Rao", "Jiwen Lu", "Jie Zhou", "Cho jui Hsieh." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Mkd: a multi-task knowledge distillation approach for pretrained language models",
      "author" : [ "Linqing Liu", "Huan Wang", "Jimmy Lin", "Richard Socher", "Caiming Xiong." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Adversarial multi-task learning for text classification",
      "author" : [ "Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "ACL, pages 1–10.",
      "citeRegEx" : "Liu et al\\.,? 2017",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2017
    }, {
      "title" : "Data-free knowledge distillationfor deep neural networks",
      "author" : [ "Raphael Gontijo Lopes", "Stefano Fenu", "Thad Starner." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Lopes et al\\.,? 2017",
      "shortCiteRegEx" : "Lopes et al\\.",
      "year" : 2017
    }, {
      "title" : "How transferable are neural networks in NLP applications",
      "author" : [ "Lili Mou", "Zhao Meng", "Rui Yan", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin" ],
      "venue" : null,
      "citeRegEx" : "Mou et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mou et al\\.",
      "year" : 2016
    }, {
      "title" : "A survey on transfer learning",
      "author" : [ "Sinno Jialin Pan", "Qiang Yang." ],
      "venue" : "IEEE Trans. Knowl. Data Eng., pages 1345–1359.",
      "citeRegEx" : "Pan and Yang.,? 2010",
      "shortCiteRegEx" : "Pan and Yang.",
      "year" : 2010
    }, {
      "title" : "Transferrable prototypical networks for unsupervised domain adaptation",
      "author" : [ "Yingwei Pan", "Ting Yao", "Yehao Li", "Yu Wang", "ChongWah Ngo", "Tao Mei." ],
      "venue" : "CVPR, pages 2239–2247.",
      "citeRegEx" : "Pan et al\\.,? 2019",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2019
    }, {
      "title" : "MTSS: learn from multiple domain teachers and become a multi-domain dialogue expert",
      "author" : [ "Shuke Peng", "Feng Ji", "Zehao Lin", "Shaobo Cui", "Haiqing Chen", "Yin Zhang." ],
      "venue" : "AAAI, pages 8608–8615.",
      "citeRegEx" : "Peng et al\\.,? 2020",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "NAACL-HLT, pages 2227–2237.",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Fitnets: Hints for thin deep nets",
      "author" : [ "Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Romero et al\\.,? 2015",
      "shortCiteRegEx" : "Romero et al\\.",
      "year" : 2015
    }, {
      "title" : "Knowledge adaptation: Teaching to adapt",
      "author" : [ "Sebastian Ruder", "Parsa Ghaffari", "John G. Breslin." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Ruder et al\\.,? 2017",
      "shortCiteRegEx" : "Ruder et al\\.",
      "year" : 2017
    }, {
      "title" : "Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "Meta-learning with memory-augmented neural networks",
      "author" : [ "Adam Santoro", "Sergey Bartunov", "Matthew Botvinick", "Daan Wierstra", "Timothy P. Lillicrap." ],
      "venue" : "ICML, pages 1842–1850.",
      "citeRegEx" : "Santoro et al\\.,? 2016",
      "shortCiteRegEx" : "Santoro et al\\.",
      "year" : 2016
    }, {
      "title" : "Prototypical networks for few-shot learning",
      "author" : [ "Jake Snell", "Kevin Swersky", "Richard S. Zemel." ],
      "venue" : "NeurIPS, pages 4077–4087.",
      "citeRegEx" : "Snell et al\\.,? 2017",
      "shortCiteRegEx" : "Snell et al\\.",
      "year" : 2017
    }, {
      "title" : "2019a. How to fine-tune BERT for text classification",
      "author" : [ "Chi Sun", "Xipeng Qiu", "Yige Xu", "Xuanjing Huang" ],
      "venue" : "In CCL,",
      "citeRegEx" : "Sun et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Patient knowledge distillation for BERT model compression",
      "author" : [ "Siqi Sun", "Yu Cheng", "Zhe Gan", "Jingjing Liu." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Sun et al\\.,? 2019b",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Mobilebert: a compact task-agnostic BERT for resource-limited devices",
      "author" : [ "Zhiqing Sun", "Hongkun Yu", "Xiaodan Song", "Renjie Liu", "Yiming Yang", "Denny Zhou." ],
      "venue" : "ACL, pages 2158–2170.",
      "citeRegEx" : "Sun et al\\.,? 2020",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Distant domain transfer learning",
      "author" : [ "Ben Tan", "Yu Zhang", "Sinno Jialin Pan", "Qiang Yang." ],
      "venue" : "AAAI, pages 2604–2610.",
      "citeRegEx" : "Tan et al\\.,? 2017",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2017
    }, {
      "title" : "Multilingual neural machine translation with knowledge distillation",
      "author" : [ "Xu Tan", "Yi Ren", "Di He", "Tao Qin", "Zhou Zhao", "TieYan Liu." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Tan et al\\.,? 2019",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2019
    }, {
      "title" : "Distilling taskspecific knowledge from BERT into simple neural networks",
      "author" : [ "Raphael Tang", "Yao Lu", "Linqing Liu", "Lili Mou", "Olga Vechtomova", "Jimmy Lin." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Tang et al\\.,? 2019",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2019
    }, {
      "title" : "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
      "author" : [ "Antti Tarvainen", "Harri Valpola." ],
      "venue" : "NeurIPs, pages 1195–1204.",
      "citeRegEx" : "Tarvainen and Valpola.,? 2017",
      "shortCiteRegEx" : "Tarvainen and Valpola.",
      "year" : 2017
    }, {
      "title" : "Well-read students learn better: On the importance of pre-training compact models",
      "author" : [ "Iulia Turc", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Turc et al\\.,? 2019",
      "shortCiteRegEx" : "Turc et al\\.",
      "year" : 2019
    }, {
      "title" : "Soft weight-sharing for neural network compression",
      "author" : [ "Karen Ullrich", "Edward Meeds", "Max Welling." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Ullrich et al\\.,? 2017",
      "shortCiteRegEx" : "Ullrich et al\\.",
      "year" : 2017
    }, {
      "title" : "Meta-learning: A survey",
      "author" : [ "Joaquin Vanschoren." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Vanschoren.,? 2018",
      "shortCiteRegEx" : "Vanschoren.",
      "year" : 2018
    }, {
      "title" : "Multimodal model-agnostic meta-learning via task-aware modulation",
      "author" : [ "Risto Vuorio", "Shao-Hua Sun", "Hexiang Hu", "Joseph J. Lim." ],
      "venue" : "NeurIPS, pages 1–12.",
      "citeRegEx" : "Vuorio et al\\.,? 2019",
      "shortCiteRegEx" : "Vuorio et al\\.",
      "year" : 2019
    }, {
      "title" : "Meta fine-tuning neural language models for multi-domain text mining",
      "author" : [ "Chengyu Wang", "Minghui Qiu", "Jun Huang", "Xiaofeng He." ],
      "venue" : "EMNLP, page 3094–3104.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel Bowman." ],
      "venue" : "NAACLHLT, pages 1112–1122.",
      "citeRegEx" : "Williams et al\\.,? 2018",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "Model compression with twostage multi-teacher knowledge distillation for web question answering system",
      "author" : [ "Ze Yang", "Linjun Shou", "Ming Gong", "Wutao Lin", "Daxin Jiang." ],
      "venue" : "WSDM, pages 690– 698.",
      "citeRegEx" : "Yang et al\\.,? 2020",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime G. Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le." ],
      "venue" : "NeurIPS, pages 5754– 5764.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Transfer learning for sequence tagging with hierarchical recurrent networks",
      "author" : [ "Zhilin Yang", "Ruslan Salakhutdinov", "William W. Cohen." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Yang et al\\.,? 2017",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2017
    }, {
      "title" : "Zero-shot text classification via reinforced self-training",
      "author" : [ "Zhiquan Ye", "Yuxia Geng", "Jiaoyan Chen", "Jingmin Chen", "Xiaoxiao Xu", "Suhang Zheng", "Feng Wang", "Jun Zhang", "Huajun Chen." ],
      "venue" : "ACL, pages 3014–3024.",
      "citeRegEx" : "Ye et al\\.,? 2020",
      "shortCiteRegEx" : "Ye et al\\.",
      "year" : 2020
    }, {
      "title" : "A gift from knowledge distillation: Fast optimization, network minimization and transfer learning",
      "author" : [ "Junho Yim", "Donggyu Joo", "Ji-Hoon Bae", "Junmo Kim." ],
      "venue" : "CVPR, pages 7130–7138.",
      "citeRegEx" : "Yim et al\\.,? 2017",
      "shortCiteRegEx" : "Yim et al\\.",
      "year" : 2017
    }, {
      "title" : "Meta-learning for few-shot natural language processing: A survey",
      "author" : [ "Wenpeng Yin." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Yin.,? 2020",
      "shortCiteRegEx" : "Yin.",
      "year" : 2020
    }, {
      "title" : "Learning from multiple teacher networks",
      "author" : [ "Shan You", "Chang Xu", "Chao Xu", "Dacheng Tao." ],
      "venue" : "SIGKDD, pages 1285–1294.",
      "citeRegEx" : "You et al\\.,? 2017",
      "shortCiteRegEx" : "You et al\\.",
      "year" : 2017
    }, {
      "title" : "Extreme language model compression with optimal subwords and shared projections",
      "author" : [ "Sanqiang Zhao", "Raghav Gupta", "Yang Song", "Denny Zhou." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Zhao et al\\.,? 2019",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Pre-trained Language Models (PLM) such as BERT (Devlin et al., 2019) and XLNet (Yang et al.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 50,
      "context" : ", 2019) and XLNet (Yang et al., 2019) have achieved significant success with the two-stage “pre-training and fine-tuning” process.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 20,
      "context" : "and the long inference time have become the bottleneck for PLMs to be deployed in real-time applications, especially on mobile devices (Jiao et al., 2019; Sun et al., 2020; Iandola et al., 2020).",
      "startOffset" : 135,
      "endOffset" : 194
    }, {
      "referenceID" : 38,
      "context" : "and the long inference time have become the bottleneck for PLMs to be deployed in real-time applications, especially on mobile devices (Jiao et al., 2019; Sun et al., 2020; Iandola et al., 2020).",
      "startOffset" : 135,
      "endOffset" : 194
    }, {
      "referenceID" : 17,
      "context" : "and the long inference time have become the bottleneck for PLMs to be deployed in real-time applications, especially on mobile devices (Jiao et al., 2019; Sun et al., 2020; Iandola et al., 2020).",
      "startOffset" : 135,
      "endOffset" : 194
    }, {
      "referenceID" : 15,
      "context" : "Knowledge Distillation (KD) (Hinton et al., 2015) is one of the promising ways to distill the knowledge from a large “teacher” model to a small “student” model.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 33,
      "context" : "Recent studies show that KD can be applied to compress PLMs with acceptable performance loss (Sanh et al., 2019; Sun et al., 2019b; Jiao et al., 2019; Turc et al., 2019; Chen et al., 2020a).",
      "startOffset" : 93,
      "endOffset" : 189
    }, {
      "referenceID" : 37,
      "context" : "Recent studies show that KD can be applied to compress PLMs with acceptable performance loss (Sanh et al., 2019; Sun et al., 2019b; Jiao et al., 2019; Turc et al., 2019; Chen et al., 2020a).",
      "startOffset" : 93,
      "endOffset" : 189
    }, {
      "referenceID" : 20,
      "context" : "Recent studies show that KD can be applied to compress PLMs with acceptable performance loss (Sanh et al., 2019; Sun et al., 2019b; Jiao et al., 2019; Turc et al., 2019; Chen et al., 2020a).",
      "startOffset" : 93,
      "endOffset" : 189
    }, {
      "referenceID" : 43,
      "context" : "Recent studies show that KD can be applied to compress PLMs with acceptable performance loss (Sanh et al., 2019; Sun et al., 2019b; Jiao et al., 2019; Turc et al., 2019; Chen et al., 2020a).",
      "startOffset" : 93,
      "endOffset" : 189
    }, {
      "referenceID" : 6,
      "context" : "Recent studies show that KD can be applied to compress PLMs with acceptable performance loss (Sanh et al., 2019; Sun et al., 2019b; Jiao et al., 2019; Turc et al., 2019; Chen et al., 2020a).",
      "startOffset" : 93,
      "endOffset" : 189
    }, {
      "referenceID" : 16,
      "context" : "This “knowledge transfer” technique in KD has been proved efficient only when two domains are close to each other (Hu et al., 2019).",
      "startOffset" : 114,
      "endOffset" : 131
    }, {
      "referenceID" : 39,
      "context" : "In reality, however, it is highly risky as teachers of other domains may pass non-transferable knowledge to the student model, which is irrelevant to the current domain and hence harms the overall performance (Tan et al., 2017; Wang et al., 2020).",
      "startOffset" : 209,
      "endOffset" : 246
    }, {
      "referenceID" : 47,
      "context" : "In reality, however, it is highly risky as teachers of other domains may pass non-transferable knowledge to the student model, which is irrelevant to the current domain and hence harms the overall performance (Tan et al., 2017; Wang et al., 2020).",
      "startOffset" : 209,
      "endOffset" : 246
    }, {
      "referenceID" : 10,
      "context" : "To address these issues, we leverage the idea of meta-learning to capture transferable knowledge across domains, as recent studies have shown that meta-learning can improve the model generalization ability across domains (Finn et al., 2017; Javed and White, 2019; Yin, 2020; Ye et al., 2020).",
      "startOffset" : 221,
      "endOffset" : 291
    }, {
      "referenceID" : 19,
      "context" : "To address these issues, we leverage the idea of meta-learning to capture transferable knowledge across domains, as recent studies have shown that meta-learning can improve the model generalization ability across domains (Finn et al., 2017; Javed and White, 2019; Yin, 2020; Ye et al., 2020).",
      "startOffset" : 221,
      "endOffset" : 291
    }, {
      "referenceID" : 54,
      "context" : "To address these issues, we leverage the idea of meta-learning to capture transferable knowledge across domains, as recent studies have shown that meta-learning can improve the model generalization ability across domains (Finn et al., 2017; Javed and White, 2019; Yin, 2020; Ye et al., 2020).",
      "startOffset" : 221,
      "endOffset" : 291
    }, {
      "referenceID" : 52,
      "context" : "To address these issues, we leverage the idea of meta-learning to capture transferable knowledge across domains, as recent studies have shown that meta-learning can improve the model generalization ability across domains (Finn et al., 2017; Javed and White, 2019; Yin, 2020; Ye et al., 2020).",
      "startOffset" : 221,
      "endOffset" : 291
    }, {
      "referenceID" : 45,
      "context" : "Different from the K-way N-shot problems addressed in traditional metalearning (Vanschoren, 2018), we propose to train a “meta-learner” as the meta-teacher, which learns the transferable knowledge across domains so that it can fit new domains easily.",
      "startOffset" : 79,
      "endOffset" : 97
    }, {
      "referenceID" : 48,
      "context" : "To verify the effectiveness of Meta-KD, we conduct extensive experiments on two NLP tasks across multiple domains, namely natural language inference (Williams et al., 2018) and sentiment analysis (Blitzer et al.",
      "startOffset" : 149,
      "endOffset" : 172
    }, {
      "referenceID" : 15,
      "context" : "KD was first proposed by (Hinton et al., 2015), aiming to transfer knowledge from an ensemble or a large model into a smaller, distilled model.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 31,
      "context" : "3028 representations (Romero et al., 2015; Yim et al., 2017; You et al., 2017) or the relations between layers (Yim et al.",
      "startOffset" : 21,
      "endOffset" : 78
    }, {
      "referenceID" : 53,
      "context" : "3028 representations (Romero et al., 2015; Yim et al., 2017; You et al., 2017) or the relations between layers (Yim et al.",
      "startOffset" : 21,
      "endOffset" : 78
    }, {
      "referenceID" : 55,
      "context" : "3028 representations (Romero et al., 2015; Yim et al., 2017; You et al., 2017) or the relations between layers (Yim et al.",
      "startOffset" : 21,
      "endOffset" : 78
    }, {
      "referenceID" : 53,
      "context" : ", 2017) or the relations between layers (Yim et al., 2017; Tarvainen and Valpola, 2017) of teacher models.",
      "startOffset" : 40,
      "endOffset" : 87
    }, {
      "referenceID" : 42,
      "context" : ", 2017) or the relations between layers (Yim et al., 2017; Tarvainen and Valpola, 2017) of teacher models.",
      "startOffset" : 40,
      "endOffset" : 87
    }, {
      "referenceID" : 8,
      "context" : "Previous approaches on compressing PLMs such as BERT (Devlin et al., 2019) include KD (Hinton et al.",
      "startOffset" : 53,
      "endOffset" : 74
    }, {
      "referenceID" : 15,
      "context" : ", 2019) include KD (Hinton et al., 2015), parameter sharing (Ullrich et al.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 44,
      "context" : ", 2015), parameter sharing (Ullrich et al., 2017), pruning (Han et al.",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 14,
      "context" : ", 2017), pruning (Han et al., 2015) and quantization (Gong et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 30,
      "context" : "(2019) distill BERT into BiLSTM networks to achieve comparable results with ELMo (Peters et al., 2018).",
      "startOffset" : 81,
      "endOffset" : 102
    }, {
      "referenceID" : 33,
      "context" : "DistillBERT (Sanh et al., 2019) applies KD loss in the pre-training stage, while BERT-PKD (Sun et al.",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 37,
      "context" : ", 2019) applies KD loss in the pre-training stage, while BERT-PKD (Sun et al., 2019b) distill BERT into shallow Transformers in the fine-tuning stage.",
      "startOffset" : 66,
      "endOffset" : 85
    }, {
      "referenceID" : 20,
      "context" : "TinyBERT (Jiao et al., 2019) further distills BERT with a two-stage KD process for hidden attention matrices and embedding matrices.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 6,
      "context" : "AdaBERT (Chen et al., 2020a) uses neural architecture search to adaptively find small architectures.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 27,
      "context" : "TL has been proved to improve the performance on the target domain by leveraging knowledge from related source domains (Pan and Yang, 2010; Mou et al., 2016; Liu et al., 2017; Yang et al., 2017).",
      "startOffset" : 119,
      "endOffset" : 194
    }, {
      "referenceID" : 26,
      "context" : "TL has been proved to improve the performance on the target domain by leveraging knowledge from related source domains (Pan and Yang, 2010; Mou et al., 2016; Liu et al., 2017; Yang et al., 2017).",
      "startOffset" : 119,
      "endOffset" : 194
    }, {
      "referenceID" : 24,
      "context" : "TL has been proved to improve the performance on the target domain by leveraging knowledge from related source domains (Pan and Yang, 2010; Mou et al., 2016; Liu et al., 2017; Yang et al., 2017).",
      "startOffset" : 119,
      "endOffset" : 194
    }, {
      "referenceID" : 51,
      "context" : "TL has been proved to improve the performance on the target domain by leveraging knowledge from related source domains (Pan and Yang, 2010; Mou et al., 2016; Liu et al., 2017; Yang et al., 2017).",
      "startOffset" : 119,
      "endOffset" : 194
    }, {
      "referenceID" : 26,
      "context" : "In most NLP tasks, the “shared-private” architecture is applied to learn domain-specific representations and domain-invariant features (Mou et al., 2016; Liu et al., 2017; Chen et al., 2018, 2019).",
      "startOffset" : 135,
      "endOffset" : 196
    }, {
      "referenceID" : 24,
      "context" : "In most NLP tasks, the “shared-private” architecture is applied to learn domain-specific representations and domain-invariant features (Mou et al., 2016; Liu et al., 2017; Chen et al., 2018, 2019).",
      "startOffset" : 135,
      "endOffset" : 196
    }, {
      "referenceID" : 45,
      "context" : "Compared to TL, the goal of meta-learning is to train meta-learners that can adapt to a variety of different tasks with little training data (Vanschoren, 2018).",
      "startOffset" : 141,
      "endOffset" : 159
    }, {
      "referenceID" : 35,
      "context" : "A majority of meta-learning methods for include metric-based (Snell et al., 2017; Pan et al., 2019), model-based (Santoro et al.",
      "startOffset" : 61,
      "endOffset" : 99
    }, {
      "referenceID" : 28,
      "context" : "A majority of meta-learning methods for include metric-based (Snell et al., 2017; Pan et al., 2019), model-based (Santoro et al.",
      "startOffset" : 61,
      "endOffset" : 99
    }, {
      "referenceID" : 34,
      "context" : ", 2019), model-based (Santoro et al., 2016; Bartunov et al., 2020) and model-agnostic approaches (Finn et al.",
      "startOffset" : 21,
      "endOffset" : 66
    }, {
      "referenceID" : 1,
      "context" : ", 2019), model-based (Santoro et al., 2016; Bartunov et al., 2020) and model-agnostic approaches (Finn et al.",
      "startOffset" : 21,
      "endOffset" : 66
    }, {
      "referenceID" : 46,
      "context" : ", 2020) and model-agnostic approaches (Finn et al., 2017, 2018; Vuorio et al., 2019).",
      "startOffset" : 38,
      "endOffset" : 84
    }, {
      "referenceID" : 25,
      "context" : "Meta-learning can also be applied to KD in some computer vision tasks (Lopes et al., 2017; Jang et al., 2019; Liu et al., 2020; Bai et al., 2020; Li et al., 2020).",
      "startOffset" : 70,
      "endOffset" : 162
    }, {
      "referenceID" : 18,
      "context" : "Meta-learning can also be applied to KD in some computer vision tasks (Lopes et al., 2017; Jang et al., 2019; Liu et al., 2020; Bai et al., 2020; Li et al., 2020).",
      "startOffset" : 70,
      "endOffset" : 162
    }, {
      "referenceID" : 22,
      "context" : "Meta-learning can also be applied to KD in some computer vision tasks (Lopes et al., 2017; Jang et al., 2019; Liu et al., 2020; Bai et al., 2020; Li et al., 2020).",
      "startOffset" : 70,
      "endOffset" : 162
    }, {
      "referenceID" : 0,
      "context" : "Meta-learning can also be applied to KD in some computer vision tasks (Lopes et al., 2017; Jang et al., 2019; Liu et al., 2020; Bai et al., 2020; Li et al., 2020).",
      "startOffset" : 70,
      "endOffset" : 162
    }, {
      "referenceID" : 21,
      "context" : "Meta-learning can also be applied to KD in some computer vision tasks (Lopes et al., 2017; Jang et al., 2019; Liu et al., 2020; Bai et al., 2020; Li et al., 2020).",
      "startOffset" : 70,
      "endOffset" : 162
    }, {
      "referenceID" : 35,
      "context" : "Inspired by prototypebased meta-learning (Snell et al., 2017; Pan et al., 2019), the meta-teacher model should memorize more information about prototypes.",
      "startOffset" : 41,
      "endOffset" : 79
    }, {
      "referenceID" : 28,
      "context" : "Inspired by prototypebased meta-learning (Snell et al., 2017; Pan et al., 2019), the meta-teacher model should memorize more information about prototypes.",
      "startOffset" : 41,
      "endOffset" : 79
    }, {
      "referenceID" : 37,
      "context" : "The overall meta-distillation loss is a combination of the Mean Squared Error (MSE) loss from intermediate layers of both models (Sun et al., 2019b; Jiao et al., 2019), the soft cross-entropy loss from output layers (Hinton et al.",
      "startOffset" : 129,
      "endOffset" : 167
    }, {
      "referenceID" : 20,
      "context" : "The overall meta-distillation loss is a combination of the Mean Squared Error (MSE) loss from intermediate layers of both models (Sun et al., 2019b; Jiao et al., 2019), the soft cross-entropy loss from output layers (Hinton et al.",
      "startOffset" : 129,
      "endOffset" : 167
    }, {
      "referenceID" : 15,
      "context" : ", 2019), the soft cross-entropy loss from output layers (Hinton et al., 2015), and the transferable knowledge distillation loss, with instance-specific domain-expertise weighting applied.",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 8,
      "context" : "We take BERT (Devlin et al., 2019) as our base learner for text classification due to its wide popularity.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 37,
      "context" : "The KD process of input embeddings, hidden states and attention matrices follows the common practice (Sun et al., 2019b; Jiao et al., 2019).",
      "startOffset" : 101,
      "endOffset" : 139
    }, {
      "referenceID" : 20,
      "context" : "The KD process of input embeddings, hidden states and attention matrices follows the common practice (Sun et al., 2019b; Jiao et al., 2019).",
      "startOffset" : 101,
      "endOffset" : 139
    }, {
      "referenceID" : 15,
      "context" : "Lpred(M,Sk, X (i) k ) is the cross-entropy loss of “softened” output logits, parameterized by the temperature (Hinton et al., 2015).",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 48,
      "context" : "• MNLI (Williams et al., 2018) is a largescale, multi-domain natural language inference dataset for predicting the entailment relation between two sentences, containing five domains (genres).",
      "startOffset" : 7,
      "endOffset" : 30
    }, {
      "referenceID" : 2,
      "context" : "• Amazon Reviews (Blitzer et al., 2007) is a multi-domain sentiment analysis dataset, widely used in multi-domain text classification tasks.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 20,
      "context" : "For the student side, we follow TinyBERT (Jiao et al., 2019) to use smaller BERT models as our student models.",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 55,
      "context" : "In multi-teachers, because TinyBERT-KD does not naturally support distilling from multiple teacher models, we implement a variant of the TinyBERTKD process based on MTN-KD (You et al., 2017), which uses averaged softened outputs as the incorporation of multiple teacher networks in the output layer.",
      "startOffset" : 172,
      "endOffset" : 190
    } ],
    "year" : 2021,
    "abstractText" : "Pre-trained language models have been applied to various NLP tasks with considerable performance gains. However, the large model sizes, together with the long inference time, limit the deployment of such models in realtime applications. One line of model compression approaches considers knowledge distillation to distill large teacher models into small student models. Most of these studies focus on single-domain only, which ignores the transferable knowledge from other domains. We notice that training a teacher with transferable knowledge digested across domains can achieve better generalization capability to help knowledge distillation. Hence we propose a Meta-Knowledge Distillation (Meta-KD) framework to build a meta-teacher model that captures transferable knowledge across domains and passes such knowledge to students. Specifically, we explicitly force the meta-teacher to capture transferable knowledge at both instance-level and feature-level from multiple domains, and then propose a meta-distillation algorithm to learn singledomain student models with guidance from the meta-teacher. Experiments on public multidomain NLP tasks show the effectiveness and superiority of the proposed Meta-KD framework. Further, we also demonstrate the capability of Meta-KD in the settings where the training data is scarce.",
    "creator" : "LaTeX with hyperref"
  }
}