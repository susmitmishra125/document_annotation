{
  "name" : "2021.acl-long.562.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation",
    "authors" : [ "Eleftheria Briakou" ],
    "emails" : [ "ebriakou@cs.umd.edu,", "marine@cs.umd.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7236–7249\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n7236"
    }, {
      "heading" : "1 Introduction",
      "text" : "While parallel texts are essential to Neural Machine Translation (NMT), the degree of parallelism varies widely across samples in practice, for reasons ranging from noise in the extraction process (Roziewski and Stokowiec, 2016) to nonliteral translations (Zhai et al., 2019b, 2020a). For instance (Figure 1), a French SOURCE could be paired with an exact translation into English (EQ), with a mostly equivalent translation where only a few tokens convey divergent meaning (fineDIV), or with a semantically unrelated, noisy reference (coarse-DIV). Yet, prior work treats parallel samples in a binary fashion: coarse-grained divergences are viewed as noise to be excluded from training (Koehn et al., 2018), whilst others are typically regarded as gold-standard equivalent translations. As a result, the impact of fine-grained divergences on NMT remains unclear.\nThis paper aims to understand and mitigate the impact of fine-grained semantic divergences in\nNMT. We first contribute an analysis of how finegrained divergences in training data affect NMT quality and confidence. Starting from a set of equivalent English-French WikiMatrix sentence pairs, we simulate divergences by gradually “corrupting” them with synthetic fine-grained divergences. Following Khayrallah and Koehn (2018)—who, in contrast, study the impact of noise on MT—we control for different types of fine-grained semantic divergences and different ratios of equivalent vs. divergent data. Our findings indicate that these imperfect training references: hurt translation quality (as measured by BLEU and METEOR) once they overwhelm equivalents; output degenerated text more frequently; and increase the uncertainty of models’ predictions.\nBased on these findings, we introduce a divergent-aware NMT framework that incorporates information about which tokens are indicative of semantic divergences between the source and target side of a training sample. Source-side divergence tags are integrated as feature factors (Haddow and Koehn, 2012; Sennrich and Haddow, 2016; Hoang et al., 2016), while target-side divergence tags form an additional output sequence generated in a multi-task fashion (Garcı́a-Martı́nez et al., 2016, 2017). Results on EN↔FR translation show that our approach is a successful mitigation strategy: it helps NMT recover from the negative impact of fine-grained divergences on translation quality, with fewer degenerated hypotheses, and more confident and better calibrated predictions. We make our code publicly available: https://github.com/Elbria/xling-SemDiv-NMT."
    }, {
      "heading" : "2 Background & Motivation",
      "text" : "Cross-lingual Semantic Divergences We use this term to refer to meaning differences in aligned bilingual text (Vyas et al., 2018; Carpuat et al., 2017). Divergences in manual translation might arise due to the translation process (Zhai et al., 2018) and result in non-literal translations (Zhai et al., 2020a). Divergences might also arise in parallel text extracted from multilingual comparable resources. For instance, in Wikipedia, documents aligned across languages might contain parallel segments that share important content, yet they are not perfect translations of each other, yielding fine-grained semantic divergences (Smith et al., 2010). Finally coarse-grained divergences might result from the process of automatically mining and aligning corpora from monolingual data (Fung and Cheung, 2004; Munteanu and Marcu, 2005), or web-scale parallel text (Smith et al., 2013; ElKishky et al., 2020; Esplà et al., 2019).\nNoise vs. Semantic Divergences In the context of MT, noise often refers to mismatches in webcrawled parallel corpora that are collected without guarantees about their quality. Khayrallah and Koehn (2018) define five frequent types of noise found in the German-English Paracrawl corpus: misaligned sentences, disfluent text, wrong language, short segments, and untranslated sentences. They examine the impact of noise on translation quality and find that untranslated training instances cause NMT models to copy the input sentence at inference time. Their findings motivated a shared\ntask dedicated to filtering noisy samples from webcrawled data at WMT, since 2018 (Koehn et al., 2018, 2019, 2020). This work moves beyond such coarse divergences and focuses instead on finegrained divergences that affect a small number of tokens within mostly equivalent pairs and that can be found even in high-quality parallel corpora.\nTraining Assumptions NMT models are typically trained to maximize the log-likelihood of the training data, D ≡ {(x(n),y(n))}Nn=1, where (x(n),y(n)) is the n-th sentence pair consisting of sentences that are assumed to be translations of each other. Under this assumption, model parameters are updated to maximize the token-level crossentropy loss:\nJ (θ) = N∑ n=1 T∑ t=1 log p(y (n) t | y (n) <t ,x (n); θ) (1)\nIn Figure 1, we illustrate how semantic divergences interact with NMT training. In the case of coarse divergences, both the prefixes ỹ(n)t<1 and targets ỹ(n)t , yield a noisy training signal at each time step t, which motivates excluding them from the training pool entirely. In the case of fine-grained divergences, the assumption of semantic equivalence is only partially broken. Depending on the time step t, we might thus condition the prediction of the next token on partially corrupted prefixes, encourage the model to make a wrong prediction, or do a combination of the above. This suggests that fine-grained divergent samples provide a noisy yet potentially useful training signal depending on the time step. Meanwhile, fine-grained divergences increase uncertainty in the training data, and as a result might impact models’ confidence in their predictions, as noisy untranslated samples do (Ott et al., 2018). This work seeks to clarify and mitigate their impact on NMT, accounting for both translation quality and model confidence."
    }, {
      "heading" : "3 Analyzing the Impact of Divergences",
      "text" : ""
    }, {
      "heading" : "3.1 Method",
      "text" : "We evaluate the impact of semantic divergences on NMT by injecting increasing amounts of synthetic divergent samples during training, following the methodology of Khayrallah and Koehn (2018) for noise. We focus on three types of divergences, which were found to be frequent in parallel corpora. They are fine-grained as they represent discrepancies between the source and target segments\nat a word or phrase level: LEXICAL SUBSTITUTION aims at mimicking particularization and generalization operations resulting from non-literal translations (Zhai et al., 2019a, 2020b); PHRASE REPLACEMENT mimics phrasal mistranslations; SUBTREE DELETION simulates missing phrasal content from the source or target side.\nSynthetic divergent samples are automatically generated by corrupting semantically equivalent sentence pairs, following the methodology introduced by Briakou and Carpuat (2020). Equivalents are identified by their Divergent mBERT classifier that yields an F1 score of 84, on manually annotated WikiMatrix data, despite being trained on synthetic data. For LEXICAL SUBSTITUTION we corrupt equivalents by substituting words with their hypernyms or hyponyms from WordNet, for PHRASE REPLACEMENT we replace sequences of words with phrases of matching POS tags, and for SUBTREE DELETION we randomly delete subtrees in the dependency parse tree of either the source or the target. Having access to those 4 versions of the same corpus (one initial equivalent and three synthetic divergences), we mix equivalents and divergent pairs introducing one type of divergence at a time (corpora statistics are included in D). Finally, we evaluate the translation quality and uncertainty of the resulting translation models."
    }, {
      "heading" : "3.2 Experimental Set-Up",
      "text" : "Training Data We train our models on the parallel WikiMatrix French-English corpus (Schwenk et al., 2019), which consists of sentence pairs mined from Wikipedia pages using languageagnostic sentence embeddings (LASER) (Artetxe and Schwenk, 2019). Previous annotations show that 40% of sentence pairs in a random sample contain fine-grained divergences (Briakou and Carpuat, 2020).\nAfter cleaning noisy samples using simple rules (i.e., exclude pairs that are a) too short or too long, b) mostly numbers, c) almost copies based on edit distance), we extract equivalent samples using the Divergent mBERT model. Table 1 presents statistics on the extracted pairs, along with the corpus created if we threshold the LASER score at 1.04, as suggested by Schwenk et al. (2019).\nDevelopment and Test data We use the official development and test splits of the TED corpus (Qi et al., 2018), consisting of 4,320 and 4,866 goldstandard translation pairs, respectively. All models\nshare the same BPE vocabulary. We average results across runs with 3 different random seeds.\nPreprocessing We use the standard Moses scripts (Koehn et al., 2007) for punctuation normalization, true-casing, and tokenization. We learn 32K BPEs (Sennrich et al., 2016c) using SentencePiece (Kudo and Richardson, 2018).\nModels We use the base Transformer architecture (Vaswani et al., 2017), with embedding size of 512, transformer hidden size of 2,048, 8 attention heads, 6 transformer layers, and dropout of 0.1. Target embeddings are tied with the output layer weights. We train with label smoothing (0.1). We optimize with Adam (Kingma and Ba, 2015) with a batch size of 4,096 tokens and checkpoint models every 1,000 updates. The initial learning rate is 0.0002, and it is reduced by 30% after 4 checkpoints without validation perplexity improvement. We stop training after 20 checkpoints without improvement. We select the best checkpoint based on validation BLEU (Papineni et al., 2002). All models are trained on a single GeForce GTX 1080 GPU."
    }, {
      "heading" : "3.3 Findings",
      "text" : "Translation Quality Table 2 presents the impact of semantic divergences on BLEU and METEOR. Corrupting equivalent bitext with fine-grained divergences hurts translation quality across the board. In most cases, the degradation is proportional to the percentage of corrupted training samples. LEXICAL SUBSTITUTION causes the largest degradation for both metrics. The degradation is relatively smaller for METEOR than BLEU, which we attribute to the fact that METEOR allows matches between synonyms when comparing references to hypotheses. SUBTREE DELETION and LEXICAL SUBSTITUTION corruptions lead to significant degradation at ≥ 50% (BLEU; standard deviations across reruns are < 0.4). By contrast, Transformers are more robust to PHRASE REPLACEMENT corruptions, as degradations are only significant after corrupting ≥ 70% (BLEU) of equivalents.\nBLEU METEOR\n0% 10% 20% 50% 70% 100% 0% 10% 20% 50% 70% 100%\nPHRASE REPLACEMENT\n30.89 +0.00 31.00 +0.11 30.82 -0.07 30.40 -0.49 29.74 -1.15 27.01 -3.88 33.74 +0.00 33.63 -0.11 33.66 -0.08 33.54 -0.20 33.12 -0.62 31.02 -2.72\nSUBTREE DELETION\n30.89 +0.00 30.80 -0.09 30.62 -0.27 28.95 -1.94 29.00 -1.89 27.50 -3.39 33.74 +0.00 33.61 -0.13 33.38 -0.36 32.17 -1.57 32.09 -1.65 31.44 -2.30\nLEXICAL SUBSTITUTION\n30.89 +0.00 30.72 -0.17 30.49 -0.40\n25.04 -5.85 26.57 -4.32 25.18 -5.71 33.74 +0.00 33.56 -0.18 33.50 -0.24 29.59 -4.15 31.58 -2.16 30.75 -2.99\nTable 2: Results for FR→EN translation on the TED test set (means of 3 runs). Bars denote degradation over EQUIVALENTS (i.e., 0%) across different % of corruption. Divergences hurt BLEU and METEOR when they overwhelm the training data. Transformers are particularly sensitive to fine nuances introduced by LEXICAL SUBSTITUTION.\nFigure 2: Average token probabilities of predictions conditioned on gold references (left) and beam search (5) prefixes (right). Training on fine-grained divergences (100% corruption) increase NMT model’s uncertainty.\nToken Uncertainty We measure the impact of divergences on model uncertainty at training time and at test time. For the first, we extract the probability of a reference token conditioned on reference prefixes at each time step. For the latter, we compute the probability of the token predicted by the model given its own history of predictions. Figure 2 shows that models trained on EQUIVALENTS are\nmore confident in their token level predictions both at inference and training time. SUBTREE DELETION mismatches affect models’ confidence less than other types, while PHRASE REPLACEMENT hurts confidence the most both at inference and at training time. Finally, we observe that differences across divergence types are larger in early decoding steps, while at later steps, they all converge below the EQUIVALENTS.\nDegenerated Hypotheses When models are trained on 50% or more divergent samples, the total length of their hypotheses is longer than the references. Manual analysis on models trained with 100% of divergent samples suggests that this length effect is partially caused by degenerated text. Following Holtzman et al. (2019)—who study this phenomenon for unconditional text generation—we define degenerations as “output text that is bland, incoherent, or gets stuck in repetitive loops”.1\n1For instance, “I’ve never studied sculpture, engineering and architecture, and the engineering and architecture”.\nWe automatically detect degenerated text in model outputs by checking whether they contain repetitive loops of n-grams that do not appear in the reference (details on the algorithm are in C). Figure 3 shows that exposing NMT to divergences increases the percentage of degenerated outputs. Even with large beams, the models trained on divergent data yield more repetitions than the EQUIVALENTS. Moreover, divergences due to phrasal mismatches (PHRASE REPLACEMENT and SUBTREE DELETION) yield more frequent repetitions than token-level mismatches (LEXICAL SUBSTITUTION). Interestingly, the latter almost matches the frequency of repetitions in EQUIVALENTS with larger beams (≥ 5).\nSummary Synthetic divergences hurt translation quality, as expected. More surprisingly, our study also reveals that this degradation is partially due to more frequent degenerated outputs, and that divergences impact models’ confidence in their predictions. Different types of divergences have different effects: LEXICAL SUBSTITUTION causes the largest degradation in translation quality, SUBTREE DELETION and PHRASE REPLACEMENT increase the number of degenerated beam hypotheses, while PHRASE REPLACEMENT also hurts the models’ confidence the most. Nevertheless, the impact of divergences on BLEU appears to be smaller than that of noise (Khayrallah and Koehn, 2018).2 This suggests that noise filtering techniques are suboptimal to deal with fine-grained divergences."
    }, {
      "heading" : "4 Mitigating the Impact of Fine-grained Divergences",
      "text" : "We now turn to naturally occurring divergences in WikiMatrix. We will see that their impact on model quality and uncertainty is consistent with that of synthetic divergences (§ 4.3). We propose a divergent-aware framework for NMT (§ 4.1) that successfully mitigates their impact (§ 4.3)."
    }, {
      "heading" : "4.1 Factorizing Divergences for NMT",
      "text" : "We use semantic factors to inform NMT of tokens that are indicative of meaning differences in each sentence pair. We tag divergent source and target tokens in parallel segments as equivalent (EQ) or divergent (DIV) using an mBERT-based classifier trained on synthetic data.\n2While the absolute scores are not directly comparable across settings, Khayrallah and Koehn (2018) report that noise has a more striking impact of −8 to −25 BLEU.\nThe classifier has a 45 F1 score on a fine-grained divergence test set (Briakou and Carpuat, 2020). The predicted tags are thus noisy, as expected on this challenging task, yet we will see that they are useful. An example is illustrated below:\nSRC TOKENS votre père est francais FACTORS EQ DIV EQ EQ\nTGT TOKENS your parent is french FACTORS EQ DIV EQ EQ\nSource Factors We follow Sennrich and Haddow (2016) who represent the encoder input as a combination of token embeddings and linguistic features. Concretely, we look up separate embeddings vectors for tokens and source-side divergent predictions, which are then concatenated. The length of the concatenated vector matches the total embedding size.\nTarget Factors Target-side divergence tags are an additional output sequence, as in Garcı́aMartı́nez et al. (2016). At each time step the model produces two distributions: one over the token target vocabulary and one over the target factors. The model is trained to minimize a divergent-aware loss (Equation 2). Terms in red (also, underlined) correspond to modifications to the traditional NMT loss. At time step t, the model is rewarded to match the reference target y(n)t , conditioned on the source sequence of tokens (x(n)), the source factors (ω(n)), the token target prefix (y(n)<t ), and the target factors prefix (z(n)<t ). At the same time (t), the model is rewarded to match the factored predictions for the previous time step τ = t − 1. The time shift between the two target sequences is introduced so that the model learns to firstly predict the reference token at τ and then its corresponding EQ vs. DIV label, at the same time step. The factored predictions are conditioned again on x(n), ω(n), the target factor prefix z(n)<τ and the token prefix (y (n) ≤τ ).\nL = − N∑ n=1 ( T∑ t=1 log p(y (n) t | y (n) <t ,z (n) <t ,x\n(n),ω(n); θ)︸ ︷︷ ︸ L̃(n)MT\n+ T∑ τ=t−1 log p(z(n)τ | z (n) <τ ,y (n) ≤τ ,x\n(n),ω(n); θ)︸ ︷︷ ︸ L(n)\nfactor\n)\n(2)\nInference At test time, input tokens are tagged with EQ to encourage the model to predict an equivalent translation. We decode using beam search for predicting the translation sequence. The token predictions are conditioned on both the token and the factors prefixes. The factor prefixes are greedily decoded and thus do not participate in beam search."
    }, {
      "heading" : "4.2 Experimental Set-Up",
      "text" : "Divergences We conduct an extensive comparison of models exposed to different amounts of equivalent and divergent WikiMatrix samples. Starting from the pool of examples identified as divergent at §3.2, we rank and select the most fine-grained divergences by thresholding the bicleaner score (Ramı́rez-Sánchez et al., 2020) at 0.5, 0.7 and 0.8. For details, see A.\nModels We compare the factored models (DIVFACTORIZED) for incorporating divergent tokens (§4.1) against: 1. LASER models are trained on WikiMatrix pairs with a LASER score greater than 1.04 – the noise filtering strategy recommended by Schwenk et al. (2019). Our prior work shows that thresholding LASER might introduce a number of divergent data in the training pool varying from fine to coarse mismatches (Briakou and Carpuat, 2020). 2. EQUIVALENTS models are trained on WikiMatrix pairs detected as exact translations (§3.2); 3. DIV-AGNOSTIC models are trained on equivalent and fine-grained divergent data without incorporating information that distinguishes between them; 4. DIV-TAGGED models distinguish equivalences from divergences by appending <EQ> vs. <DIV> tags as source-side constraints (Sennrich et al., 2016a).\nModels’ details Our models are implemented in the Sockeye2 toolkit (Domhan et al., 2020).3 We set the size of factor embeddings to 8, the source token embeddings to 504 and target embeddings to 514, yielding equal model sizes across experiments. All other parameters are kept the same across models, as discussed in §3.2, except that target embeddings are not tied with output layer weights for factored models. More details are included in B.\nOther Data & Preprocessing We use the same preprocessing as well as development and test sets as in §3.2, except we learn 5K BPEs as in\n3https://github.com/awslabs/sockeye\nSchwenk et al. (2019). DIV-FACTORIZED, DIVAGNOSTIC, and DIV-TAGGED models are compared in controlled setups that use the same training data. We also evaluate out-of-domain on the khresmoi-summary test set for the WMT2014 medical translation task (Bojar et al., 2014).\nEvaluation We evaluate translation quality with BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005).4,5 We compute Inference Expected Calibration Error (InfECE) as Wang et al. (2020), which measures the difference in expectation between confidence and accuracy.6 We measure token-level translation accuracy based on Translation Error Rate (TER) alignments between hypotheses and references.7 Unless mentioned otherwise, we decode with a beam size of 5."
    }, {
      "heading" : "4.3 Results",
      "text" : "We discuss the impact of real divergences along the dimensions surfaced by the synthetic data analysis.\nTranslation Quality Table 3 presents BLEU and METEOR scores across model configurations and data settings on the TED test sets. First, the model trained on EQUIVALENTS represents a very competitive baseline as it performs better or statistically comparable to all models. This result is in line with prior evidence of Vyas et al. (2018) who show that filtering out the most divergent pairs in noisy corpora (e.g., OpenSubtitles and CommonCrawl) does not hurt translation quality. Interestingly, the EQUIVALENTS model outperforms LASER across metrics and translation directions, despite the fact that it is exposed to only about half of the training data. Gradually adding divergent data (DIV-AGNOSTIC) hurts translation quality across the board compared to the EQUIVALENTS model. The drops are significantly larger when divergences overwhelm the equivalent translations, which is consistent with our findings on synthetic data.\nSecond, DIV-FACTORIZED is the most effective mitigation strategy. With segment-level constraints (DIV-TAGGED), models can recover from the degradation caused by divergences (DIV-AGNOSTIC), but not consistently. By contrast, token-level factors (DIV-FACTORIZED) help NMT recover from the impact of divergences across data setups and reach\n4https://github.com/mjpost/sacrebleu 5https://www.cs.cmu.edu/˜alavie/METEOR/ 6https://github.com/shuo-git/InfECE 7http://www.cs.umd.edu/˜snover/tercom/\nscores among all models and boldface the scores lying within one stdev from EQUIVALENTS. ↑ denotes (one stdev) improvements of DIV-TAGGED and DIV-FACTORIZED over DIV-AGNOSTIC. Factorizing divergences helps NMT recover from the degradation caused by divergences, while it achieves comparable scores to EQUIVALENTS.\ntranslation quality comparable to that of the EQUIVALENTS model, successfully mitigating the impact of the noisy training signals from divergent samples.\nThird, when translating the out-of-domain test set, DIV-FACTORIZED improves over the EQUIVALENTS model, as presented in Table 4. DIVAGNOSTIC models perform comparably to EQUIVALENTS, while factorizing divergences improves on the latter by ≈ +1 BLEU, for both directions.8 Mitigating the impact of divergences is thus important for NMT to benefit from the increased coverage of out-of-domain data provided by the divergent samples.\nDegenerated Hypotheses We check for degenerated outputs across models, data setups (we account for different percentages of divergences in the training data), and different beam sizes (Table 5). As with synthetic divergences, we observe that when real divergences overwhelm the training data (55%), degenerated loops are almost twice as frequent for all beam sizes. This phenomenon is consistently mitigated by DIV-FACTORIZED models across the board.9 Furthermore, in some settings (20%, 33%), DIV-FACTORIZED models decrease the amount of degenerated text by half compared to the EQUIVALENTS models.10\n8We include METEOR results in Appendix E. 9We observe similar trends for EN→FR in Appendix F\n10LASER models degenerate more frequently than EQUIVALENTS and DIV-FACTORIZED.\nUncertainty Figures 4a and 4c show that the gold-standard references are assigned lower probabilities by the DIV-AGNOSTIC models than all other models, especially in early time steps (t < 30). We observe similar drops in confidence based on the probabilities of predicted tokens at inference time (4b and 4d). This confirms that exposing models to fine-grained semantic divergences hurts their confidence, whether the divergences are synthetic or not. Furthermore, factorizing divergences helps mitigate the impact of naturally occurring divergences on uncertainty in addition to translation quality.\nWe conduct a calibration analysis to measure the differences between the confidence (i.e., probability) and the correctness (i.e., accuracy) of the generated tokens in expectation. Given that deep neural networks are often mis-calibrated in the direction of over-estimation (confidence>accuracy) (Guo et al., 2017), we check whether the increased confidence of DIV-FACTORIZED hurts calibration (Table 6). DIV-FACTORIZED models are on average more confident and more accurate than their DIV-AGNOSTIC counterparts. Interestingly, DIV-AGNOSTIC has smaller calibration errors than EQUIVALENTS and LASER models across the board."
    }, {
      "heading" : "5 Related Work",
      "text" : "We discuss work related to cross-lingual semantic divergences and noise effects in Section 2 and now turn to the literature that connects with the methods used in this paper.\nFactored Models Factored models are introduced to inject word-level linguistic annotations (e.g., Part-of-Speech tags, lemmas) in translation. Source-side factors have been used in statistical MT (Haddow and Koehn, 2012) and in NMT (Sennrich et al., 2016b; Hoang et al., 2016). Target-side factors are used by Garcı́a-Martı́nez et al. (2017) as an extension to the traditional NMT framework that outputs multiple sequences. Although their main motivation is to enable models to handle larger vocabularies, Wilken and Matusov (2019) propose a list of novel applications of target-side factors beyond their initial purpose, such as wordcase prediction and subword segmentation. Our approach draws inspiration from all the aforementioned works, yet it is unique in its use of both source and target factors to incorporate semantics in NMT.\nCalibration Kumar and Sarawagi (2019) find that NMT models are miscalibrated, even when conditioned on gold-standard prefixes. They attribute this behavior to the poor calibration of the EOS token and the uncertainty of attention and design a recalibration model to improve calibration. Ott et al. (2018) argue that miscalibration can be attributed to the “extrinsic” uncertainty of the noisy, untranslated references found in the training data. Müller et al. (2019) investigate the effect of label smoothing on calibration. On a similar spirit, Wang et al. (2020) propose graduated label smoothing to improve calibration at inference time. They also link miscalibration to linguistic properties of the data (e.g., frequency, position, syntactic roles). Our work, in contrast, focuses on the semantic properties of the training data that affect calibration."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This work investigates the impact of semantic mismatches beyond noise in parallel text on NMT quality and confidence. Our experiments on EN↔FR tasks show that fine-grained semantic divergences hurt translation quality when they overwhelm the training data. Models exposed to fine-grained divergences at training time are less confident in their predictions, which hurts beam search and produces degenerated text (repetitive loops) more frequently.\nFurthermore, we also show that, unlike noisy samples, fine-grained divergences can still provide a useful training signal for NMT when they are modeled via factors. Evaluated on EN↔FR translation tasks, our divergent-aware NMT framework mitigates the negative impact of divergent references on translation quality, improves the confidence and calibration of predictions, and produces degenerated text less frequently.\nMore broadly, this work illustrates how understanding the properties of training data can help build better NMT models. In future work, we will extend our analysis to other properties of parallel text and to other language pairs, focusing on low-resource conditions where divergences are expected to be even more prevalent."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Sweta Agrawal, Doug Oard, Suraj Rajappan Nair, the anonymous reviewers and the CLIP lab at UMD for helpful comments. This material is based upon work supported by the National Science Foundation under Award No. 1750695. Any\nopinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation."
    }, {
      "heading" : "A WikiMatrix Fine-grained Divergences",
      "text" : "Starting from the pool of examples identified as divergent under the divergentmBERT classifier, we want to focus on the subset of samples that contain fine meaning differences. Therefore, we use bicleaner to filter out training data that are likely to contain coarse meaning differences. EsplàGomis et al. (2020) report better NMT results on English↔Portuguese translation after cleaning WikiMatrix data with thresholds of 0.5 and 0.7.\nWe conduct a preliminary experiment to understand how the bicleaner scores of EnglishFrench WikiMatrix sentences are distributed. Figure 5(a) shows the distribution of scores among the three classes of the REFRESD dataset, a dataset that distinguishes fine meaning differences (“some meaning difference”), coarse divergences (“unrelated”), and equivalent translation pairs (“no meaning difference”).11 We observe that thresholding the bicleaner score at > 0.5 filters out most of the unrelated pairs. We conduct three experiments with thresholds at 0.8, 0.7, and 0.5 to gradually add more fine-grained divergences. Figure 5(b) presents the number of English-French WikiMatrix divergences, binned by bicleaner scores.\n11https://github.com/Elbria/xling-SemDiv/ tree/master/REFreSD\nB Sockeye2 configuration details\nTables 7 and 8 present details of NMT training with Sockeye2."
    }, {
      "heading" : "C Measuring Degenerated Hypotheses",
      "text" : "We include the pseudo-algorithm that checks if a hypothesis falls under odd repetitions not supported by the reference in Algorithm 1. When measuring repeated n-grams we exclude punctuation and conjunctions. The REPEATED function checks whether an n-gram is repeated (number of occurrences> 1) in the hypothesis h, or reference r.\nAlgorithm 1 Degenerated hypothesis check Input: h, r (hypothesis, reference) Output: Deg (True for degenerated hypothesis)"
    }, {
      "heading" : "D Synthetic Divergences Statistics",
      "text" : "Tables 9 and 10 contain corpus statistics for the 3 versions of synthetic divergences we create, starting from EQUIVALENTS. LEXICAL SUBSTITUTION are sampled at random from the pools of substitutions based on hypernyms and hyponyms."
    }, {
      "heading" : "E METEOR Results (addition)",
      "text" : "For completeness, we present METEOR scores to complement the BLEU evaluation of §4.3, which consists the official evaluation metric of WMT biomedical translation tasks (Jimeno Yepes et al., 2017; Neves et al., 2018; Bawden et al., 2019, 2020). The average improvements of DIV-FACTORIZED over EQUIVALENTS and DIVAGNOSTIC are smaller compared to the differences highlighted by BLEU. However, we note that METEOR results might be misleading when evaluating medical translations, as in this domain we might not want to account for synonyms when comparing references to hypotheses."
    }, {
      "heading" : "F Degenerated Hypotheses (addition)",
      "text" : "DIV-FACTORIZED decreases the % of degenerated outputs caused by divergent data (Table 12)."
    } ],
    "references" : [ {
      "title" : "Massively multilingual sentence embeddings for zero-shot crosslingual transfer and beyond",
      "author" : [ "M. Artetxe", "Holger Schwenk." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:597–610.",
      "citeRegEx" : "Artetxe and Schwenk.,? 2019",
      "shortCiteRegEx" : "Artetxe and Schwenk.",
      "year" : 2019
    }, {
      "title" : "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
      "author" : [ "Satanjeev Banerjee", "Alon Lavie." ],
      "venue" : "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-",
      "citeRegEx" : "Banerjee and Lavie.,? 2005",
      "shortCiteRegEx" : "Banerjee and Lavie.",
      "year" : 2005
    }, {
      "title" : "Findings of the WMT 2020 biomedical translation shared task: Basque, Italian and Russian as new additional languages",
      "author" : [ "Siu", "Philippe Thomas", "Federica Vezzani", "Maika Vicente Navarro", "Dina Wiemann", "Lana Yeganova." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Siu et al\\.,? 2020",
      "shortCiteRegEx" : "Siu et al\\.",
      "year" : 2020
    }, {
      "title" : "Findings of the 2014 workshop",
      "author" : [ "Ondřej Bojar", "Christian Buck", "Christian Federmann", "Barry Haddow", "Philipp Koehn", "Johannes Leveling", "Christof Monz", "Pavel Pecina", "Matt Post", "Herve Saint-Amand", "Radu Soricut", "Lucia Specia", "Aleš Tamchyna" ],
      "venue" : null,
      "citeRegEx" : "Bojar et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bojar et al\\.",
      "year" : 2014
    }, {
      "title" : "Detecting Fine-Grained Cross-Lingual Semantic Divergences without Supervision by Learning to Rank",
      "author" : [ "Eleftheria Briakou", "Marine Carpuat." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Briakou and Carpuat.,? 2020",
      "shortCiteRegEx" : "Briakou and Carpuat.",
      "year" : 2020
    }, {
      "title" : "Detecting cross-lingual semantic divergence for neural machine translation",
      "author" : [ "Marine Carpuat", "Yogarshi Vyas", "Xing Niu." ],
      "venue" : "Proceedings of the First Workshop on Neural Machine Translation, pages 69– 79, Vancouver. Association for Computational Lin-",
      "citeRegEx" : "Carpuat et al\\.,? 2017",
      "shortCiteRegEx" : "Carpuat et al\\.",
      "year" : 2017
    }, {
      "title" : "The sockeye 2 neural machine translation toolkit at AMTA 2020",
      "author" : [ "Tobias Domhan", "Michael Denkowski", "David Vilar", "Xing Niu", "Felix Hieber", "Kenneth Heafield." ],
      "venue" : "Proceedings of the 14th Conference of the Association for Machine Translation in the",
      "citeRegEx" : "Domhan et al\\.,? 2020",
      "shortCiteRegEx" : "Domhan et al\\.",
      "year" : 2020
    }, {
      "title" : "Searching the Web for CrossLingual Parallel Data, page 2417–2420",
      "author" : [ "Ahmed El-Kishky", "Philipp Koehn", "Holger Schwenk." ],
      "venue" : "Association for Computing Machinery, New York, NY, USA.",
      "citeRegEx" : "El.Kishky et al\\.,? 2020",
      "shortCiteRegEx" : "El.Kishky et al\\.",
      "year" : 2020
    }, {
      "title" : "ParaCrawl: Web-scale parallel corpora for the languages of the EU",
      "author" : [ "Miquel Esplà", "Mikel Forcada", "Gema Ramı́rez-Sánchez", "Hieu Hoang" ],
      "venue" : "In Proceedings of Machine Translation Summit XVII Volume 2: Translator, Project and User Tracks,",
      "citeRegEx" : "Esplà et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Esplà et al\\.",
      "year" : 2019
    }, {
      "title" : "Bicleaner at WMT 2020: Universitat d’alacant-prompsit’s submission to the parallel corpus filtering shared task",
      "author" : [ "Miquel Esplà-Gomis", "Vı́ctor M. Sánchez-Cartagena", "Jaume Zaragoza-Bernabeu", "Felipe SánchezMartı́nez" ],
      "venue" : null,
      "citeRegEx" : "Esplà.Gomis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Esplà.Gomis et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-level bootstrapping for extracting parallel sentences from a quasi-comparable corpus",
      "author" : [ "Pascale Fung", "Percy Cheung." ],
      "venue" : "COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics, pages 1051–1057,",
      "citeRegEx" : "Fung and Cheung.,? 2004",
      "shortCiteRegEx" : "Fung and Cheung.",
      "year" : 2004
    }, {
      "title" : "Neural machine translation by generating multiple linguistic factors",
      "author" : [ "Mercedes Garcı́a-Martı́nez", "Loı̈c Barrault", "Fethi Bougares" ],
      "venue" : null,
      "citeRegEx" : "Garcı́a.Martı́nez et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Garcı́a.Martı́nez et al\\.",
      "year" : 2017
    }, {
      "title" : "On calibration of modern neural networks",
      "author" : [ "Chuan Guo", "Geoff Pleiss", "Yu Sun", "Kilian Q. Weinberger." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages",
      "citeRegEx" : "Guo et al\\.,? 2017",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2017
    }, {
      "title" : "Interpolated backoff for factored translation models",
      "author" : [ "Barry Haddow", "Philipp Koehn." ],
      "venue" : "Association for Machine Translation in the Americas, AMTA.",
      "citeRegEx" : "Haddow and Koehn.,? 2012",
      "shortCiteRegEx" : "Haddow and Koehn.",
      "year" : 2012
    }, {
      "title" : "Improving neural translation models with linguistic factors",
      "author" : [ "Cong Duy Vu Hoang", "Gholamreza Haffari", "Trevor Cohn." ],
      "venue" : "Proceedings of the Australasian Language Technology Association Workshop 2016, pages 7–14, Melbourne, Australia.",
      "citeRegEx" : "Hoang et al\\.,? 2016",
      "shortCiteRegEx" : "Hoang et al\\.",
      "year" : 2016
    }, {
      "title" : "The curious case of neural text degeneration",
      "author" : [ "Ari Holtzman", "Jan Buys", "Maxwell Forbes", "Yejin Choi." ],
      "venue" : "CoRR, abs/1904.09751.",
      "citeRegEx" : "Holtzman et al\\.,? 2019",
      "shortCiteRegEx" : "Holtzman et al\\.",
      "year" : 2019
    }, {
      "title" : "Findings of the WMT 2017 biomedical translation shared task",
      "author" : [ "Saskia Trescher." ],
      "venue" : "Proceedings of the Second Conference on Machine Translation, pages 234–247, Copenhagen, Denmark. Association for Computational Linguistics.",
      "citeRegEx" : "Trescher.,? 2017",
      "shortCiteRegEx" : "Trescher.",
      "year" : 2017
    }, {
      "title" : "On the impact of various types of noise on neural machine translation",
      "author" : [ "Huda Khayrallah", "Philipp Koehn." ],
      "venue" : "Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 74–83, Melbourne, Australia. Association for Com-",
      "citeRegEx" : "Khayrallah and Koehn.,? 2018",
      "shortCiteRegEx" : "Khayrallah and Koehn.",
      "year" : 2018
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "CoRR, abs/1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Findings of the WMT 2020 shared task on parallel corpus filtering and alignment",
      "author" : [ "Philipp Koehn", "Vishrav Chaudhary", "Ahmed El-Kishky", "Naman Goyal", "Peng-Jen Chen", "Francisco Guzmán." ],
      "venue" : "Proceedings of the Fifth Conference on Machine",
      "citeRegEx" : "Koehn et al\\.,? 2020",
      "shortCiteRegEx" : "Koehn et al\\.",
      "year" : 2020
    }, {
      "title" : "Findings of the WMT 2019 shared task on parallel corpus filtering for low-resource conditions",
      "author" : [ "Philipp Koehn", "Francisco Guzmán", "Vishrav Chaudhary", "Juan Pino." ],
      "venue" : "Proceedings of the Fourth Conference on Machine Translation (Volume",
      "citeRegEx" : "Koehn et al\\.,? 2019",
      "shortCiteRegEx" : "Koehn et al\\.",
      "year" : 2019
    }, {
      "title" : "Findings of the WMT",
      "author" : [ "Philipp Koehn", "Huda Khayrallah", "Kenneth Heafield", "Mikel L. Forcada" ],
      "venue" : null,
      "citeRegEx" : "Koehn et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Koehn et al\\.",
      "year" : 2018
    }, {
      "title" : "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "author" : [ "Taku Kudo", "John Richardson." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System",
      "citeRegEx" : "Kudo and Richardson.,? 2018",
      "shortCiteRegEx" : "Kudo and Richardson.",
      "year" : 2018
    }, {
      "title" : "Calibration of encoder decoder models for neural machine translation",
      "author" : [ "Aviral Kumar", "Sunita Sarawagi." ],
      "venue" : "CoRR, abs/1903.00802.",
      "citeRegEx" : "Kumar and Sarawagi.,? 2019",
      "shortCiteRegEx" : "Kumar and Sarawagi.",
      "year" : 2019
    }, {
      "title" : "When does label smoothing help? In Advances in Neural Information Processing Systems, volume 32, pages 4694–4703",
      "author" : [ "Rafael Müller", "Simon Kornblith", "Geoffrey E Hinton." ],
      "venue" : "Curran Associates, Inc.",
      "citeRegEx" : "Müller et al\\.,? 2019",
      "shortCiteRegEx" : "Müller et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving machine translation performance by exploiting non-parallel corpora",
      "author" : [ "Dragos Stefan Munteanu", "Daniel Marcu." ],
      "venue" : "Comput. Linguist., 31(4):477–504.",
      "citeRegEx" : "Munteanu and Marcu.,? 2005",
      "shortCiteRegEx" : "Munteanu and Marcu.",
      "year" : 2005
    }, {
      "title" : "Findings of the WMT 2018 biomedical translation shared task: Evaluation on Medline test sets",
      "author" : [ "Mariana Neves", "Antonio Jimeno Yepes", "Aurélie Névéol", "Cristian Grozea", "Amy Siu", "Madeleine Kittner", "Karin Verspoor." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Neves et al\\.,? 2018",
      "shortCiteRegEx" : "Neves et al\\.",
      "year" : 2018
    }, {
      "title" : "Analyzing uncertainty in neural machine translation",
      "author" : [ "Myle Ott", "Michael Auli", "David Grangier", "Marc’Aurelio Ranzato" ],
      "venue" : "arXiv preprint arXiv:1803.00047",
      "citeRegEx" : "Ott et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2018
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "When and why are pre-trained word embeddings useful for neural machine translation",
      "author" : [ "Ye Qi", "Devendra Sachan", "Matthieu Felix", "Sarguna Padmanabhan", "Graham Neubig" ],
      "venue" : "In Proceedings of the 2018 Conference of the North American Chapter",
      "citeRegEx" : "Qi et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2018
    }, {
      "title" : "Bifixer and bicleaner: two open-source tools to clean your parallel data",
      "author" : [ "Gema Ramı́rez-Sánchez", "Jaume Zaragoza-Bernabeu", "Marta Bañón", "Sergio Ortiz Rojas" ],
      "venue" : "In Proceedings of the 22nd Annual Conference of the European Association",
      "citeRegEx" : "Ramı́rez.Sánchez et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Ramı́rez.Sánchez et al\\.",
      "year" : 2020
    }, {
      "title" : "LanguageCrawl: A generic tool for building language models upon Common-Crawl",
      "author" : [ "Szymon Roziewski", "Wojciech Stokowiec." ],
      "venue" : "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), pages",
      "citeRegEx" : "Roziewski and Stokowiec.,? 2016",
      "shortCiteRegEx" : "Roziewski and Stokowiec.",
      "year" : 2016
    }, {
      "title" : "Wikimatrix: Mining 135m parallel sentences in 1620 language pairs from wikipedia",
      "author" : [ "Holger Schwenk", "Vishrav Chaudhary", "Shuo Sun", "Hongyu Gong", "Francisco Guzmán." ],
      "venue" : "CoRR, abs/1907.05791.",
      "citeRegEx" : "Schwenk et al\\.,? 2019",
      "shortCiteRegEx" : "Schwenk et al\\.",
      "year" : 2019
    }, {
      "title" : "Linguistic input features improve neural machine translation",
      "author" : [ "Rico Sennrich", "Barry Haddow." ],
      "venue" : "Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers, pages 83– 91, Berlin, Germany. Association for Computational",
      "citeRegEx" : "Sennrich and Haddow.,? 2016",
      "shortCiteRegEx" : "Sennrich and Haddow.",
      "year" : 2016
    }, {
      "title" : "Controlling politeness in neural machine translation via side constraints",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Sennrich et al\\.,? 2016a",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Improving neural machine translation models with monolingual data",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
      "citeRegEx" : "Sennrich et al\\.,? 2016b",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–",
      "citeRegEx" : "Sennrich et al\\.,? 2016c",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Extracting parallel sentences from comparable corpora using document level alignment",
      "author" : [ "Jason R. Smith", "Chris Quirk", "Kristina Toutanova." ],
      "venue" : "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Smith et al\\.,? 2010",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2010
    }, {
      "title" : "Dirt cheap web-scale parallel text from the Common Crawl",
      "author" : [ "Jason R. Smith", "Herve Saint-Amand", "Magdalena Plamada", "Philipp Koehn", "Chris Callison-Burch", "Adam Lopez." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Smith et al\\.,? 2013",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2013
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30, pages 5998–6008. Cur-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Identifying semantic divergences in parallel text without annotations",
      "author" : [ "Yogarshi Vyas", "Xing Niu", "Marine Carpuat." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Vyas et al\\.,? 2018",
      "shortCiteRegEx" : "Vyas et al\\.",
      "year" : 2018
    }, {
      "title" : "On the inference calibration of neural machine translation",
      "author" : [ "Shuo Wang", "Zhaopeng Tu", "Shuming Shi", "Yang Liu." ],
      "venue" : "ACL.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Novel applications of factored neural machine translation",
      "author" : [ "Patrick Wilken", "Evgeny Matusov." ],
      "venue" : "CoRR, abs/1910.03912.",
      "citeRegEx" : "Wilken and Matusov.,? 2019",
      "shortCiteRegEx" : "Wilken and Matusov.",
      "year" : 2019
    }, {
      "title" : "A hybrid model for globally coherent story generation",
      "author" : [ "Fangzhou Zhai", "Vera Demberg", "Pavel Shkadzko", "Wei Shi", "Asad Sayeed." ],
      "venue" : "Proceedings of the Second Workshop on Storytelling, pages 34– 45, Florence, Italy. Association for Computational",
      "citeRegEx" : "Zhai et al\\.,? 2019a",
      "shortCiteRegEx" : "Zhai et al\\.",
      "year" : 2019
    }, {
      "title" : "Detecting non-literal translations by fine-tuning cross-lingual pre-trained language models",
      "author" : [ "Yuming Zhai", "Gabriel Illouz", "Anne Vilnat." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 5944–5956,",
      "citeRegEx" : "Zhai et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhai et al\\.",
      "year" : 2020
    }, {
      "title" : "Building an English-Chinese parallel corpus annotated with subsentential translation techniques",
      "author" : [ "Yuming Zhai", "Lufei Liu", "Xinyi Zhong", "Gbariel Illouz", "Anne Vilnat." ],
      "venue" : "Proceedings of the 12th Language Resources and Evaluation Con-",
      "citeRegEx" : "Zhai et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhai et al\\.",
      "year" : 2020
    }, {
      "title" : "Construction of a multilingual corpus annotated with translation relations",
      "author" : [ "Yuming Zhai", "Aurélien Max", "Anne Vilnat." ],
      "venue" : "Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing, pages 102–111, Santa Fe, New",
      "citeRegEx" : "Zhai et al\\.,? 2018",
      "shortCiteRegEx" : "Zhai et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards recognizing phrase translation processes: Experiments on english-french",
      "author" : [ "Yuming Zhai", "Pooyan Safari", "Gabriel Illouz", "Alexandre Allauzen", "Anne Vilnat." ],
      "venue" : "CoRR, abs/1904.12213.",
      "citeRegEx" : "Zhai et al\\.,? 2019b",
      "shortCiteRegEx" : "Zhai et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 31,
      "context" : "While parallel texts are essential to Neural Machine Translation (NMT), the degree of parallelism varies widely across samples in practice, for reasons ranging from noise in the extraction process (Roziewski and Stokowiec, 2016) to nonliteral translations (Zhai et al.",
      "startOffset" : 197,
      "endOffset" : 228
    }, {
      "referenceID" : 21,
      "context" : "Yet, prior work treats parallel samples in a binary fashion: coarse-grained divergences are viewed as noise to be excluded from training (Koehn et al., 2018), whilst others are typically regarded as gold-standard equivalent translations.",
      "startOffset" : 137,
      "endOffset" : 157
    }, {
      "referenceID" : 13,
      "context" : "Source-side divergence tags are integrated as feature factors (Haddow and Koehn, 2012; Sennrich and Haddow, 2016; Hoang et al., 2016), while target-side divergence tags form an additional output sequence generated in a multi-task fashion (Garcı́a-Martı́nez et al.",
      "startOffset" : 62,
      "endOffset" : 133
    }, {
      "referenceID" : 33,
      "context" : "Source-side divergence tags are integrated as feature factors (Haddow and Koehn, 2012; Sennrich and Haddow, 2016; Hoang et al., 2016), while target-side divergence tags form an additional output sequence generated in a multi-task fashion (Garcı́a-Martı́nez et al.",
      "startOffset" : 62,
      "endOffset" : 133
    }, {
      "referenceID" : 14,
      "context" : "Source-side divergence tags are integrated as feature factors (Haddow and Koehn, 2012; Sennrich and Haddow, 2016; Hoang et al., 2016), while target-side divergence tags form an additional output sequence generated in a multi-task fashion (Garcı́a-Martı́nez et al.",
      "startOffset" : 62,
      "endOffset" : 133
    }, {
      "referenceID" : 40,
      "context" : "Cross-lingual Semantic Divergences We use this term to refer to meaning differences in aligned bilingual text (Vyas et al., 2018; Carpuat et al., 2017).",
      "startOffset" : 110,
      "endOffset" : 151
    }, {
      "referenceID" : 5,
      "context" : "Cross-lingual Semantic Divergences We use this term to refer to meaning differences in aligned bilingual text (Vyas et al., 2018; Carpuat et al., 2017).",
      "startOffset" : 110,
      "endOffset" : 151
    }, {
      "referenceID" : 46,
      "context" : "Divergences in manual translation might arise due to the translation process (Zhai et al., 2018) and result in non-literal translations (Zhai et al.",
      "startOffset" : 77,
      "endOffset" : 96
    }, {
      "referenceID" : 44,
      "context" : ", 2018) and result in non-literal translations (Zhai et al., 2020a).",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 37,
      "context" : "For instance, in Wikipedia, documents aligned across languages might contain parallel segments that share important content, yet they are not perfect translations of each other, yielding fine-grained semantic divergences (Smith et al., 2010).",
      "startOffset" : 221,
      "endOffset" : 241
    }, {
      "referenceID" : 10,
      "context" : "Finally coarse-grained divergences might result from the process of automatically mining and aligning corpora from monolingual data (Fung and Cheung, 2004; Munteanu and Marcu, 2005), or web-scale parallel text (Smith et al.",
      "startOffset" : 132,
      "endOffset" : 181
    }, {
      "referenceID" : 25,
      "context" : "Finally coarse-grained divergences might result from the process of automatically mining and aligning corpora from monolingual data (Fung and Cheung, 2004; Munteanu and Marcu, 2005), or web-scale parallel text (Smith et al.",
      "startOffset" : 132,
      "endOffset" : 181
    }, {
      "referenceID" : 38,
      "context" : "Finally coarse-grained divergences might result from the process of automatically mining and aligning corpora from monolingual data (Fung and Cheung, 2004; Munteanu and Marcu, 2005), or web-scale parallel text (Smith et al., 2013; ElKishky et al., 2020; Esplà et al., 2019).",
      "startOffset" : 210,
      "endOffset" : 273
    }, {
      "referenceID" : 8,
      "context" : "Finally coarse-grained divergences might result from the process of automatically mining and aligning corpora from monolingual data (Fung and Cheung, 2004; Munteanu and Marcu, 2005), or web-scale parallel text (Smith et al., 2013; ElKishky et al., 2020; Esplà et al., 2019).",
      "startOffset" : 210,
      "endOffset" : 273
    }, {
      "referenceID" : 27,
      "context" : "Meanwhile, fine-grained divergences increase uncertainty in the training data, and as a result might impact models’ confidence in their predictions, as noisy untranslated samples do (Ott et al., 2018).",
      "startOffset" : 182,
      "endOffset" : 200
    }, {
      "referenceID" : 32,
      "context" : "Training Data We train our models on the parallel WikiMatrix French-English corpus (Schwenk et al., 2019), which consists of sentence pairs mined from Wikipedia pages using languageagnostic sentence embeddings (LASER) (Artetxe and Schwenk, 2019).",
      "startOffset" : 83,
      "endOffset" : 105
    }, {
      "referenceID" : 0,
      "context" : ", 2019), which consists of sentence pairs mined from Wikipedia pages using languageagnostic sentence embeddings (LASER) (Artetxe and Schwenk, 2019).",
      "startOffset" : 120,
      "endOffset" : 147
    }, {
      "referenceID" : 4,
      "context" : "Previous annotations show that 40% of sentence pairs in a random sample contain fine-grained divergences (Briakou and Carpuat, 2020).",
      "startOffset" : 105,
      "endOffset" : 132
    }, {
      "referenceID" : 29,
      "context" : "Development and Test data We use the official development and test splits of the TED corpus (Qi et al., 2018), consisting of 4,320 and 4,866 goldstandard translation pairs, respectively.",
      "startOffset" : 92,
      "endOffset" : 109
    }, {
      "referenceID" : 36,
      "context" : "We learn 32K BPEs (Sennrich et al., 2016c) using SentencePiece (Kudo and Richardson, 2018).",
      "startOffset" : 18,
      "endOffset" : 42
    }, {
      "referenceID" : 39,
      "context" : "Models We use the base Transformer architecture (Vaswani et al., 2017), with embedding size of 512, transformer hidden size of 2,048, 8 attention heads, 6 transformer layers, and dropout of 0.",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 18,
      "context" : "We optimize with Adam (Kingma and Ba, 2015) with a batch size of 4,096 tokens and checkpoint models every 1,000 updates.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 28,
      "context" : "We select the best checkpoint based on validation BLEU (Papineni et al., 2002).",
      "startOffset" : 55,
      "endOffset" : 78
    }, {
      "referenceID" : 17,
      "context" : "Nevertheless, the impact of divergences on BLEU appears to be smaller than that of noise (Khayrallah and Koehn, 2018).",
      "startOffset" : 89,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : "The classifier has a 45 F1 score on a fine-grained divergence test set (Briakou and Carpuat, 2020).",
      "startOffset" : 71,
      "endOffset" : 98
    }, {
      "referenceID" : 30,
      "context" : "2, we rank and select the most fine-grained divergences by thresholding the bicleaner score (Ramı́rez-Sánchez et al., 2020) at 0.",
      "startOffset" : 92,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "Our prior work shows that thresholding LASER might introduce a number of divergent data in the training pool varying from fine to coarse mismatches (Briakou and Carpuat, 2020).",
      "startOffset" : 148,
      "endOffset" : 175
    }, {
      "referenceID" : 34,
      "context" : "<DIV> tags as source-side constraints (Sennrich et al., 2016a).",
      "startOffset" : 38,
      "endOffset" : 62
    }, {
      "referenceID" : 6,
      "context" : "Models’ details Our models are implemented in the Sockeye2 toolkit (Domhan et al., 2020).",
      "startOffset" : 67,
      "endOffset" : 88
    }, {
      "referenceID" : 3,
      "context" : "We also evaluate out-of-domain on the khresmoi-summary test set for the WMT2014 medical translation task (Bojar et al., 2014).",
      "startOffset" : 105,
      "endOffset" : 125
    }, {
      "referenceID" : 28,
      "context" : "Evaluation We evaluate translation quality with BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005).",
      "startOffset" : 53,
      "endOffset" : 76
    }, {
      "referenceID" : 12,
      "context" : "Given that deep neural networks are often mis-calibrated in the direction of over-estimation (confidence>accuracy) (Guo et al., 2017), we check whether the increased confidence of DIV-FACTORIZED hurts calibration (Table 6).",
      "startOffset" : 115,
      "endOffset" : 133
    }, {
      "referenceID" : 13,
      "context" : "Source-side factors have been used in statistical MT (Haddow and Koehn, 2012) and in NMT (Sennrich et al.",
      "startOffset" : 53,
      "endOffset" : 77
    }, {
      "referenceID" : 35,
      "context" : "Source-side factors have been used in statistical MT (Haddow and Koehn, 2012) and in NMT (Sennrich et al., 2016b; Hoang et al., 2016).",
      "startOffset" : 89,
      "endOffset" : 133
    }, {
      "referenceID" : 14,
      "context" : "Source-side factors have been used in statistical MT (Haddow and Koehn, 2012) and in NMT (Sennrich et al., 2016b; Hoang et al., 2016).",
      "startOffset" : 89,
      "endOffset" : 133
    } ],
    "year" : 2021,
    "abstractText" : "While it has been shown that Neural Machine Translation (NMT) is highly sensitive to noisy parallel training samples, prior work treats all types of mismatches between source and target as noise. As a result, it remains unclear how samples that are mostly equivalent but contain a small number of semantically divergent tokens impact NMT training. To close this gap, we analyze the impact of different types of fine-grained semantic divergences on Transformer models. We show that models trained on synthetic divergences output degenerated text more frequently and are less confident in their predictions. Based on these findings, we introduce a divergent-aware NMT framework that uses factors to help NMT recover from the degradation caused by naturally occurring divergences, improving both translation quality and model calibration on EN↔FR tasks.",
    "creator" : "LaTeX with hyperref"
  }
}