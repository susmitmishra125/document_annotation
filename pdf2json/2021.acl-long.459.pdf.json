{
  "name" : "2021.acl-long.459.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Cross-modal Memory Networks for Radiology Report Generation",
    "authors" : [ "Zhihong Chen", "Yaling Shen", "Yan Song", "Xiang Wan", "Hong Kong (Shenzhen" ],
    "emails" : [ "zhihongchen@link.cuhk.edu.cn", "yalingshen@link.cuhk.edu.cn", "songyan@cuhk.edu.cn", "wanxiang@sribd.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5904–5914\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5904"
    }, {
      "heading" : "1 Introduction",
      "text" : "Interpreting radiology images (e.g., chest X-ray) and writing diagnostic reports are essential operations in clinical practice and normally requires considerable manual workload. Therefore, radiology report generation, which aims to automatically generate a free-text description based on a radiograph, is highly desired to ease the burden of †Corresponding author. 1Our code and the best performing models are released at https://github.com/cuhksz-nlp/R2GenCMN.\nradiologists while maintaining the quality of health care. Recently, substantial progress has been made towards research on automated radiology report generation models (Jing et al., 2018; Li et al., 2018; Johnson et al., 2019; Liu et al., 2019; Jing et al., 2019). Most existing studies adopt a conventional encoder-decoder architecture, with convolutional neural networks (CNNs) as the encoder and recurrent (e.g., LSTM/GRU) or non-recurrent networks (e.g., Transformer) as the decoder following the image captioning paradigm (Vinyals et al., 2015; Anderson et al., 2018). Although these methods have achieved remarkable performance, they are still restrained in fully employing the information across radiology images and reports, such as the mappings demonstrated in Figure 1 that aligned visual and textual features point to the same content. The reason for the restraint comes from both the limitation of annotated correspondences between image and text for supervised learning as well as the lack of good model design to learn the correspondences. Unfortunately, few studies2 are dedicated to solving the restraint. Therefore, it is expected to have a better solution to model the alignments across modalities and further improve the generation ability, although promising results are continuously acquired by other approaches (Li et al., 2018; Liu et al., 2019; Jing et al., 2019; Chen et al., 2020).\n2Along this research track, recently there is only Jing et al. (2018) studying on a multi-task learning framework with a coattention mechanism to explicitly explore information linking particular parts in a radiograph and its corresponding report.\nIn this paper, we propose an effective yet simple approach to radiology report generation enhanced by cross-modal memory networks (CMN), which is designed to facilitate the interactions across modalities (i.e., images and texts). In detail, we use a memory matrix to store the cross-modal information and use it to perform memory querying and memory responding for the visual and textual features, where for memory querying, we extract the most related memory vectors from the matrix and compute their weights according to the input visual and textual features, and then generate responses by weighting the queried memory vectors. Afterwards, the responses corresponding to the input visual and textual features are fed into the encoder and decoder, so as to generate reports enhanced by such explicitly learned cross-modal information. Experimental results on two benchmark datasets, IU X-RAY and MIMIC-CXR, confirm the validity and effectiveness of our proposed approach, where state-of-the-art performance is achieved on both datasets. Several analyses are also performed to analyze the effects of different factors affecting our model, showing that our model is able to generate reports with meaningful image-text mapping while requiring few extra parameters in doing so."
    }, {
      "heading" : "2 The Proposed Approach",
      "text" : "We regard radiology report generation as an imageto-text generation task, for which there exist sev-\neral solutions (Vinyals et al., 2015; Xu et al., 2015; Anderson et al., 2018; Cornia et al., 2019). Although images are organized as 2-D format, we follow the standard sequence-to-sequence paradigm for this task as that performed in Chen et al. (2020). In detail, the source sequence is X = {x1,x2, ...,xs, ...,xS}, where xs ∈ Rd are extracted by visual extractors from a radiology image I and the target sequence are the corresponding report Y = {y1, y2, ..., yt, ..., yT }, where yt ∈ V are the generated tokens, T the length of the report and V the vocabulary of all possible tokens. The entire generation process is thus formalized as a recursive application of the chain rule\np(Y|I) = T∏ t=1 p(yt|y1, ..., yt−1, I) (1)\nThe model is then trained to maximize p(Y|I) through the negative conditional log-likelihood of Y given the I:\nθ∗ = arg max θ T∑ t=1 log p(yt|y1, ..., yt−1, I; θ) (2) where θ is the parameters of the model. An overview of the proposed model is demonstrated in Figure 2, with cross-modal memories emphasized. The details of our approach are described in following subsections regarding to its three major components, i.e., the visual extractor, the crossmodal memory networks and the encoder-decoder process enhanced by the memory."
    }, {
      "heading" : "2.1 Visual Extractor",
      "text" : "To generate radiology reports, the first step is to extract the visual features from radiology images. In our approach, the visual features X of a radiology image I are extracted by pre-trained convolutional neural networks (CNN), such as VGG (Simonyan and Zisserman, 2015) or ResNet (He et al., 2016). Normally, an image is decomposed into regions of equal size3, i.e., patches, and the features (representations) of them are extracted from the last convolutional layer of CNN. Once extracted, the features in our study are expanded into a sequence by concatenating them from each row of the patches on the image. The resulted representation sequence is used as the source input for all subsequent modules and the process is formulated as\n{x1,x2, ...,xs, ...,xS} = fv(I) (3)\nwhere fv(·) refers to the visual extractor."
    }, {
      "heading" : "2.2 Cross-modal Memory Networks",
      "text" : "To model the alignment between image and text, existing studies tend to map between images and texts directly from their encoded representations (e.g., Jing et al. (2018) used a co-attention to do so). However, this process always suffers from the limitation that the representations across modalities are hard to be aligned, so that an intermediate medium is expected to enhance and smooth such mapping. To address the limitation, we propose to use CMN to better model the image-text alignment, so as to facilitate the report generation process.\nWith using the proposed CMN, the mapping and encoding can be described in the following procedure. Given a source sequence {x1,x2, ...,xS} (features extracted from the visual extractor) from an image, we feed it to this module to obtain the memory responses of the visual features {rx1 , rx2 , ..., rxS}. Similarly, given a generated sequence {y1, y2, ..., yt−1} with its embedding {y1,y2, ...,yt−1}, it is also fed to the cross-modal memory networks to output the memory responses of the textual features {ry1 , ry2 , ..., ryt−1}. In doing so, the shared information of visual and textual features can be recorded in the memory so that the entire learning process is able to explicitly map between the images and texts. Specifically, the cross-modal memory networks employs a matrix to preserve information for encoding and decoding process, where each row of the matrix (i.e., a mem-\n3E.g., VGG/ResNet uses region size 32 × 32 (in pixels).\nory vector) records particular cross-modal information connecting images and texts. We denote the matrix as M = {m1,m2, ...,mi, ...,mN }, where N represents the number of memory vectors and mi ∈ Rd the memory vector at row i with d referring to its dimension. During the process of report generation, CMN is operated with two main steps, namely, querying and responding, whose details are described as follows.4\nMemory Querying We apply multi-thread5 querying to perform this operation, where in each thread the querying process follows the same procedure described as follows.\nIn querying memory vectors, the first step is to ensure the input visual and textual features are in the same representation space. Therefore, we convert each memory vector in M as well as input features through linear transformation by\nki = mi ·Wk (4) qs = xs ·Wq (5) qt = yt ·Wq (6)\nwhere Wk and Wq are trainable weights for the conversion. Then we separately extract the most related memory vector to visual and textual features according to their distances Dsi and Dti through\nDsi = qs · k>i√\nd (7)\nDti = qt · k>i√\nd (8)\nwhere the number of extracted memory vectors can be controlled by a hyper-parameter K to regularize how much memory is used. We denote the queried memory vectors as {ks1 ,ks2 , ...,ksj , ...,ksK} and {kt1 ,kt2 , ...,ktj , ...,ktK}. Afterwards, the importance weight of each memory vector with respect to visual and textual features are obtained by normalization over all distances by\nwsi = exp(Dsi)\nΣKj=1exp(Dsj ) (9)\nwti = exp(Dti)\nΣKj=1exp(Dtj ) (10)\nNote that the above steps are applied in each thread to allow memory querying from different memory representation subspaces.\n4Note that these two steps are performed in both training and inference stages, where in inference, all textual features are obtained along with the generation process.\n5Thread number can be arbitrarily set in experiments.\nMemory Responding The responding process is also conducted in a multi-thread manner corresponding to the query process. For each thread, we firstly perform a linear transformation on the queried memory vector via\nvi = mi ·Wv (11)\nwhere Wv is the trainable weight for mi. So that all memory vectors {vs1 ,vs2 , ...,vsj , ...,vsK} are transferred into {vt1 ,vt2 , ...,vtj , ...,vtK}. Then, we obtain the memory responses for visual and textual features by weighting over the transferred memory vectors by\nrxs = Σ K i=1wsivsi (12) ryt = Σ K i=1wtivti (13)\nwhere wsi and wti are the weights obtained from memory querying. Similar to memory querying, we apply memory responding to all the threads so as to obtain responses from different memory representation subspaces."
    }, {
      "heading" : "2.3 Encoder-Decoder",
      "text" : "Since the quality of input representation plays an important role in model performance (Pennington et al., 2014; Song et al., 2017, 2018; Peters et al., 2018; Song and Shi, 2018; Devlin et al., 2019; Song et al., 2021), the encoder-decoder in our model is built upon standard Transformer (which is a powerful architecture that achieved state-of-the-art in many tasks), where memory responses of visual and textual features are functionalized as the input of the encoder and decoder so as to enhance the generation process. In detail, as the first step, the memory responses {rx1 , rx2 , ..., rxS} for visual features are fed into the encoder through\n{z1, z2, ..., zS} = fe(rx1 , rx2 , ..., rxS ) (14)\nwhere fe(·) represents the encoder. Then the resulted intermediate states {z1, z2, ..., zS} are sent to the decoder at each decoding step, jointly with the memory responses {ry1 , ry2 , ..., ryt−1} for the textual features of generated tokens from previous steps, so as to generate the current output yt by\nyt = fd(z1, z2, ..., zS , ry1 , ry2 , ..., ryt−1) (15)\nwhere fd(·) refers to the decoder. As a result, to generate a complete report, the above process is repeated until the generation is finished."
    }, {
      "heading" : "3 Experiment Settings",
      "text" : ""
    }, {
      "heading" : "3.1 Datasets",
      "text" : "We employ two conventional benchmark datasets in our experiments, i.e., IU X-RAY (DemnerFushman et al., 2016)6 from Indiana University and MIMIC-CXR (Johnson et al., 2019)7 from the Beth Israel Deaconess Medical Center. The former is a relatively small dataset with 7,470 chest X-ray images and 3,955 corresponding reports; the latter is the largest public radiography dataset with 473,057 chest X-ray images and 206,563 reports.\nFollowing the experiment settings from previous studies (Li et al., 2018; Jing et al., 2019; Chen et al., 2020), we only generate the findings section and exclude the samples without the findings section for both datasets. For IU X-RAY, we use the same split (i.e., 70%/10%/20% for train/validation/test set) as that stated in Li et al. (2018) and for MIMIC-CXR we adopt its official split. Table 1 show the statistics of all datasets in terms of the numbers of images, reports, patients and the average length of reports with respect to train/validation/test set."
    }, {
      "heading" : "3.2 Baseline and Evaluation Metrics",
      "text" : "To examine our proposed model, we use the following ones as the main baselines in our experiments: • BASE: this is the backbone encoder-decoder\nused in our full model, i.e., a three-layer Transformer model with 8 heads and 512 hidden units without other extensions. • BASE+MEM: this is the Transformer model with\nthe same architecture of BASE where two memory networks are separately applied to image and text, respectively. This baseline aims to provide a reference to the cross-modal memory. To further demonstrate the effectiveness of our model, we compare it with previous studies, includ-\n6https://openi.nlm.nih.gov/ 7https://physionet.org/content/\nmimic-cxr/2.0.0/\ning conventional image captioning models, e.g., ST (Vinyals et al., 2015), ATT2IN (Rennie et al., 2017), ADAATT (Lu et al., 2017), TOPDOWN (Anderson et al., 2018), and the ones proposed for the medical domain, e.g., COATT (Jing et al., 2018), HRGR (Li et al., 2018), CMAS-RL (Jing et al., 2019) and R2GEN (Chen et al., 2020).\nFollowing Chen et al. (2020), we evaluate the above models by two types of metrics, conventional natural language generation (NLG) metrics and clinical efficacy (CE) metrics8. The NLG metrics9 include BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011) and ROUGE-L (Lin, 2004). For CE metrics, the CheXpert (Irvin et al., 2019)10 is applied to label the generated reports and compare the results with ground truths in 14 different categories related to thoracic diseases and support devices. We use precision, recall and F1 to evaluate model performance for CE metrics."
    }, {
      "heading" : "3.3 Implementation Details",
      "text" : "To ensure consistency with the experiment settings of previous work (Li et al., 2018; Chen et al., 2020), we use two images of a patient as input for report generation on IU X-RAY and one image for MIMIC-CXR. For visual extractor, we adopt the ResNet101 (He et al., 2016) pretrained on ImageNet (Deng et al., 2009) to extract patch features with 512 dimensions for each feature. For the encoder-decoder backbone, we use a Transformer structure with 3 layers and 8 attention heads, 512 dimensions for hidden states and initialize it randomly. For the memory matrix in CMN, its dimen-\n8Note that CE metrics only apply to MIMIC-CXR because the labeling schema of CheXpert is designed for MIMIC-CXR, which is different from that of IU X-RAY.\n9https://github.com/tylin/coco-caption 10https://github.com/MIT-LCP/mimic-cxr/\ntree/master/txt/chexpert\nsion and the number of memory vectors N are set to 512 and 2048, respectively, and also randomly initialized. For memory querying and responding, thread number and the K are set to 8 and 32, respectively. We train our model under cross entropy loss with Adam optimizer (Kingma and Ba, 2015). The learning rates of the visual extractor and other parameters are set to 5 × 10−5 and 1 × 10−4, respectively, and we decay them by a 0.8 rate per epoch for all datasets. For the report generation process, we set the beam size to 3 to balance the effectiveness and efficiency of all models. Note that the optimal hyper-parameters mentioned above are obtained by evaluating the models on the validation sets from the two datasets."
    }, {
      "heading" : "4 Results and Analyses",
      "text" : ""
    }, {
      "heading" : "4.1 Effect of Cross-Modal Memory",
      "text" : "The main experimental results on the two aforementioned datasets are shown in Table 2, where BASE+CMN represents our model (same below). There are several observations drawn from different aspects. First, both BASE+MEM and BASE+CMN outperform the vanilla Transformer (BASE) on both datasets with respect to NLG metrics, which confirms the validity of incorporating memory to introduce more knowledge into the Transformer backbone. Such knowledge may come from the hidden structures and regularity patterns shared among radiology images and their reports, so that the memory modules are able to explicitly and reasonably model them to promote the recognition of diseases (symptoms) and the generation of reports. Second, the comparison between BASE+CMN and two baselines on different metrics confirms the effectiveness of our proposed model with significant improvement. Particularly, BASE+CMN outperforms BASE+MEM by a large margin, which indicates the\nusefulness of CMN in learning cross-modal features with a shared structure rather than separate ones. Third, when comparing between datasets, the performance gains from BASE+CMN over two baselines (i.e., BASE and BASE+MEM) on MIMICCXR are larger than that of IU X-RAY. This observation owes to the fact that MIMIC-CXR is relatively larger, which helps the learning of the alignment between images and texts so that CMN helps more on report generation on MIMIC-CXR. Third, when compared between datasets, the performace gain from BASE+CMN over two baselines (i.e., BASE and BASE+MEM) on IU X-RAY are larger than that of MIMIC-CXR. This observation owes to the fact that IU X-Ray is relatively small and has less complicated visual-textual mappings, thus easier for generation by CMN. Moreover, this size effect also helps that our model shows the same trend on the CE metrics on MIMIC-CXR as that for NLG metrics, where it outperforms all its baselines in terms of precision, recall and F1."
    }, {
      "heading" : "4.2 Comparison with Previous Studies",
      "text" : "To further demonstrate the effectiveness, we further compare our model with existing models on the same datasets, with their results reported in Table 3 on both NLG and CE metrics. We have following observations. First, cross-modal memory shows its effectiveness in this task, where our model outperforms COATT, although both of them improve the report generation by the alignment of visual and textual features. The reason behind might be that our model is able to use a shared memory matrix as the medium to softly align the visual and textual features instead of direct alignment using the co-attention mechanism, thus unifies cross-modal features within same representation space and facilitate the alignment process. Second, our model confirms its superiority of simplicity when comparing with those complicated models. For example, HRGR uses manually extracted templates and CMAS-RL utilizes reinforcement learning with a careful design of adaptive rewards and our model achieves better results with a rather simpler method. Third, applying memory to both the encoding and decoding can further improve the generation ability of Transformer when compared with R2GEN which only uses memory in decoding. This observation complies with our intuition that the crossmodal operation tightens the encoding and decoding so that our model generates higher quality reports. Fourth, note that although there are other models (i.e., COATT and HRGR) with exploiting extra information (such as private datasets for visual extractor pre-training), our model still achieves the state-of-the-art performance without requiring such information. It reveals that in this task, the hidden structures among the images and texts and a\n32 64 128 256 512 1024 2048 4096 0.088\n0.092\n0.096\n0.100\n0.104 0.108 BASE BASE+MEM BASE+CMN Parameter\nMemory Size\nBL -4\n62.8M\n63.2M\n63.6M\n64.0M\n64.4M\n64.8M\nParam eters\nFigure 3: The BLEU-4 score and the number of parameters from BASE+CMN against the memory size (i.e., number of memory vectors) when the model is trained and tested on MIMIC-CXR dataset.\ngood solution of exploiting them are more essential in promoting the report generation performance."
    }, {
      "heading" : "4.3 Analysis",
      "text" : "Memory Size To analyze the impacts of memory size, we train our model with different numbers of memory vectors, i.e., N ranges from 32 to 4096, with the results on MIMIC-CXR shown in Figure 3. It is observed that, first, enlarging memory by the number of vectors results in better overall performance when the entire memory matrix is relatively small (N ≤ 1024), which can be explained by that, within a certain memory capacity, larger memory size helps store more cross-modal information; second, when the memory matrix is larger than a threshold, increasing memory vectors is not able to continue promising a better outcome. An explanation to this observation may be that, when the matrix is getting to large, the memory vectors can not be fully updated so they do not help the generation process other than being played as noise. More interestingly, it is noted that even if we use a rather large memory size (i.e., N = 4096), only 3.34% extra parameters are added to the model compared to BASE, which justifies that introducing memory to report generation process through our model can be done with small price.\nNumber of Queried Memory Vectors To analyze how querying impacts report generation, we try CMN with different numbers of queried vectors, i.e., K ranges from 1 to 512, and show the results in Figure 4. It is found that the number of queried vectors should be neither too small nor too big, where enlarging K leads to better results when K ≤ 32 and after this threshold the performance\nstarts to drop. The reason behind might be the overfitting of memory updating since the memory matrix is sparsely updated in each iteration when K is small, i.e., it is hard to be overfit under this scenario, while more queried vectors should cause intensive updating on the matrix and some of the essential vectors are over-updated accordingly. As a result, it is interesting to find the optimal number (i.e., 32) of queried vectors and this is a useful guidance to further improve report generation with controlling the querying process.\nCase Study To further qualitatively investigate how our model learns from the alignments between the visual and textual information, we perform a case study on the generated reports from different models regarding to an input chest X-ray image chosen from MIMIC-CXR. Figure 5 shows the image with ground-truth report, and different reports with selected mappings from visual (some part of the image) and textual features (some words and phrases),11 where the mapped areas on the image are highlighted with different colors. In general, BASE+CMN is able to generate more accurate descriptions (in terms of better visual-textual mapping) in the report while other baselines are inferior in doing so. For instance, normal medical conditions and abnormalities presented in the chest X-ray image are covered by the generated report from BASE+CMN (e.g., “severe cardiomegaly”, “pulmonary edema” and “pulmonary arteries”) and the related regions on the image are precisely located regarding to the texts, while the areas highlighted on the image from other models are inaccurate.\n11The representations of the textual features are extracted from the first layer of the decoder.\nTo further illustrate how the alignment works between visual and textual features, we perform a tSNE visualization on the memory vectors linking to an image and its generated report from the MIMICCXR test set. It is observed that the word “lung” in the report and the visual feature for the region of lung on the image query similar memory vectors from CMN, where similar observation is also drawn for “hemidiaphragms” and its corresponding regions on the image. This case confirms that memory vector is effective intermediate medium to interact between image and text features."
    }, {
      "heading" : "5 Related Work",
      "text" : "In general, the most popular related task to ours is image captioning, a cross-modal task involving natural language processing and computer vision, which aims to describe images in sentences (Vinyals et al., 2015; Xu et al., 2015; Anderson et al., 2018; Wang et al., 2019; Cornia et al., 2019).\nAmong these studies, the most related study from Cornia et al. (2019) also proposed to leverage memory matrices to learn a priori knowledge for visual features using memory networks (Weston et al., 2015; Sukhbaatar et al., 2015; Zeng et al., 2018; Santoro et al., 2018; Nie et al., 2020; Diao et al., 2020; Tian et al., 2020b, 2021; Chen et al., 2021), but such operation is only performed during the encoding process. Different from this work, the memory in our model is designed to align the visual and textual features, and the memory operations (i.e., querying and responding) are performed in both the encoding and decoding process.\nRecently, many advanced NLP techniques (e.g., pre-trained language models) have been applied to tasks in the medical domain (Pampari et al., 2018; Zhang et al., 2018; Wang et al., 2018; Alsentzer et al., 2019; Tian et al., 2019, 2020a; Wang et al., 2020; Lee et al., 2020; Song et al., 2020). Being one of the applications and extensions of image captioning to the medical domain, radiology report generation aims to depicting radiology images with professional reports. Existing methods were designed and proposed to better align images and texts or to exploit highly-patternized features of texts. For the former studies, Jing et al. (2018) proposed a co-attention mechanism to simultaneously explore visual and semantic information with a multi-task learning framework. For the latter studies, Li et al. (2018) introduced a template database to incorporate patternized information and Chen et al. (2020) improved the performance of radi-\nology report generation by applying a memorydriven Transformer to model patternized information. Compared to these studies, our model offers an effective yet simple alternative to generating radiology reports, where a soft intermediate layer is provided to facilitate the mappings between visual and textual features, so that more accurate descriptions are produced for generation."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we propose to generate radiology reports with cross-modal memory networks, where a memory matrix is employed to record the alignment and interaction between images and texts, with memory querying and responding performed to obtain the shared information across modalities. Experimental results on two benchmark datasets demonstrate the effectiveness of our model, which achieves the state-of-the-art performance. Further analyses investigate the effects of hyper-parameters in our model and show that our model is able to better align information from images and texts, so as to generate more accurate reports, especially with the fact that enlarging the memory matrix does not significantly affect the entire model size."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is supported by Chinese Key-Area Research and Development Program of Guangdong Province (2020B0101350001) and NSFC under the project “The Essential Algorithms and Technologies for Standardized Analytics of Clinical Texts” (12026610)."
    } ],
    "references" : [ {
      "title" : "Publicly Available Clinical BERT Embeddings",
      "author" : [ "Emily Alsentzer", "John Murphy", "William Boag", "WeiHung Weng", "Di Jindi", "Tristan Naumann", "Matthew McDermott." ],
      "venue" : "Proceedings of the 2nd Clinical Natural Language Processing Work-",
      "citeRegEx" : "Alsentzer et al\\.,? 2019",
      "shortCiteRegEx" : "Alsentzer et al\\.",
      "year" : 2019
    }, {
      "title" : "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
      "author" : [ "Peter Anderson", "Xiaodong He", "Chris Buehler", "Damien Teney", "Mark Johnson", "Stephen Gould", "Lei Zhang." ],
      "venue" : "Proceedings of the IEEE conference on com-",
      "citeRegEx" : "Anderson et al\\.,? 2018",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2018
    }, {
      "title" : "Relation Extraction with Type-aware Map Memories of Word Dependencies",
      "author" : [ "Guimin Chen", "Yuanhe Tian", "Yan Song", "Xiang Wan." ],
      "venue" : "Findings of",
      "citeRegEx" : "Chen et al\\.,? 2021",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "Generating Radiology Reports via",
      "author" : [ "ang Wan" ],
      "venue" : null,
      "citeRegEx" : "Wan.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wan.",
      "year" : 2020
    }, {
      "title" : "ImageNet: A large-scale hier",
      "author" : [ "Li Fei-Fei" ],
      "venue" : null,
      "citeRegEx" : "Fei.Fei.,? \\Q2009\\E",
      "shortCiteRegEx" : "Fei.Fei.",
      "year" : 2009
    }, {
      "title" : "2019. BERT: Pre-training",
      "author" : [ "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Toutanova.,? \\Q2019\\E",
      "shortCiteRegEx" : "Toutanova.",
      "year" : 2019
    }, {
      "title" : "CheXpert: A Large Chest Ra",
      "author" : [ "skaya" ],
      "venue" : null,
      "citeRegEx" : "skaya,? \\Q2019\\E",
      "shortCiteRegEx" : "skaya",
      "year" : 2019
    }, {
      "title" : "On the Automatic Generation of Medical Imaging Reports",
      "author" : [ "Baoyu Jing", "Pengtao Xie", "Eric Xing." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2577–2586.",
      "citeRegEx" : "Jing et al\\.,? 2018",
      "shortCiteRegEx" : "Jing et al\\.",
      "year" : 2018
    }, {
      "title" : "MIMIC-CXR: A large publicly available database of labeled chest radiographs",
      "author" : [ "Alistair EW Johnson", "Tom J Pollard", "Seth J Berkowitz", "Nathaniel R Greenbaum", "Matthew P Lungren", "Chihying Deng", "Roger G Mark", "Steven Horng." ],
      "venue" : "arXiv",
      "citeRegEx" : "Johnson et al\\.,? 2019",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A Method for Stochastic Optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "CoRR, abs/1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "BioBERT: a pretrained biomedical language representation model for biomedical text mining",
      "author" : [ "Jinhyuk Lee", "Wonjin Yoon", "Sungdong Kim", "Donghyeon Kim", "Sunkyu Kim", "Chan Ho So", "Jaewoo Kang." ],
      "venue" : "Bioinformatics,",
      "citeRegEx" : "Lee et al\\.,? 2020",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report Generation",
      "author" : [ "Yuan Li", "Xiaodan Liang", "Zhiting Hu", "Eric P Xing." ],
      "venue" : "Advances in neural information processing systems, pages 1530–1540.",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "ROUGE: A Package for Automatic Evaluation of Summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Clinically Accurate Chest X-Ray Report Generation",
      "author" : [ "Guanxiong Liu", "Tzu-Ming Harry Hsu", "Matthew McDermott", "Willie Boag", "Wei-Hung Weng", "Peter Szolovits", "Marzyeh Ghassemi." ],
      "venue" : "Machine Learning for Healthcare Conference, pages",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning",
      "author" : [ "Jiasen Lu", "Caiming Xiong", "Devi Parikh", "Richard Socher." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 375–",
      "citeRegEx" : "Lu et al\\.,? 2017",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2017
    }, {
      "title" : "Improving Named Entity Recognition with Attentive Ensemble of Syntactic Information",
      "author" : [ "Yuyang Nie", "Yuanhe Tian", "Yan Song", "Xiang Ao", "Xiang Wan." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Nie et al\\.,? 2020",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2020
    }, {
      "title" : "emrQA: A Large Corpus for Question Answering on Electronic Medical Records",
      "author" : [ "Anusri Pampari", "Preethi Raghavan", "Jennifer Liang", "Jian Peng." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Pampari et al\\.,? 2018",
      "shortCiteRegEx" : "Pampari et al\\.",
      "year" : 2018
    }, {
      "title" : "BLEU: a Method for Automatic Evaluation of Machine Translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "GloVe: Global Vectors for Word Representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543,",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep Contextualized Word Representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Self-critical Sequence Training for Image Captioning",
      "author" : [ "Steven J Rennie", "Etienne Marcheret", "Youssef Mroueh", "Jerret Ross", "Vaibhava Goel." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7008–7024.",
      "citeRegEx" : "Rennie et al\\.,? 2017",
      "shortCiteRegEx" : "Rennie et al\\.",
      "year" : 2017
    }, {
      "title" : "Relational recurrent neural networks",
      "author" : [ "Adam Santoro", "Ryan Faulkner", "David Raposo", "Jack Rae", "Mike Chrzanowski", "Theophane Weber", "Daan Wierstra", "Oriol Vinyals", "Razvan Pascanu", "Timothy Lillicrap." ],
      "venue" : "Advances in neural information process-",
      "citeRegEx" : "Santoro et al\\.,? 2018",
      "shortCiteRegEx" : "Santoro et al\\.",
      "year" : 2018
    }, {
      "title" : "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman." ],
      "venue" : "CoRR, abs/1409.1556.",
      "citeRegEx" : "Simonyan and Zisserman.,? 2015",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2015
    }, {
      "title" : "Learning Word Representations with Regularization from Prior Knowledge",
      "author" : [ "Yan Song", "Chia-Jung Lee", "Fei Xia." ],
      "venue" : "Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 143–152.",
      "citeRegEx" : "Song et al\\.,? 2017",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2017
    }, {
      "title" : "Complementary Learning of Word Embeddings",
      "author" : [ "Yan Song", "Shuming Shi." ],
      "venue" : "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18, pages 4368– 4374.",
      "citeRegEx" : "Song and Shi.,? 2018",
      "shortCiteRegEx" : "Song and Shi.",
      "year" : 2018
    }, {
      "title" : "Directional Skip-Gram: Explicitly Distinguishing Left and Right Context for Word Embeddings",
      "author" : [ "Yan Song", "Shuming Shi", "Jing Li", "Haisong Zhang." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Com-",
      "citeRegEx" : "Song et al\\.,? 2018",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2018
    }, {
      "title" : "Summarizing Medical Conversations via Identifying Important Utterances",
      "author" : [ "Yan Song", "Yuanhe Tian", "Nan Wang", "Fei Xia." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 717–729.",
      "citeRegEx" : "Song et al\\.,? 2020",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2020
    }, {
      "title" : "ZEN 2.0: Continue Training and Adaption for N-gram Enhanced Text Encoders",
      "author" : [ "Yan Song", "Tong Zhang", "Yonggang Wang", "Kai-Fu Lee" ],
      "venue" : "arXiv preprint arXiv:2105.01279",
      "citeRegEx" : "Song et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2021
    }, {
      "title" : "End-To-End Memory Networks",
      "author" : [ "Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Sukhbaatar et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2015
    }, {
      "title" : "Enhancing Aspect-level Sentiment Analysis with Word Dependencies",
      "author" : [ "Yuanhe Tian", "Guimin Chen", "Yan Song." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages",
      "citeRegEx" : "Tian et al\\.,? 2021",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2021
    }, {
      "title" : "ChiMed: A Chinese Medical Corpus for Question Answering",
      "author" : [ "Yuanhe Tian", "Weicheng Ma", "Fei Xia", "Yan Song." ],
      "venue" : "Proceedings of the 18th BioNLP Workshop and Shared Task, pages 250–260.",
      "citeRegEx" : "Tian et al\\.,? 2019",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving Biomedical Named Entity Recognition with Syntactic Information",
      "author" : [ "Yuanhe Tian", "Wang Shen", "Yan Song", "Fei Xia", "Min He", "Kenli Li." ],
      "venue" : "BMC Bioinformatics, 21:1471–2105.",
      "citeRegEx" : "Tian et al\\.,? 2020a",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving Chinese Word Segmentation with Wordhood Memory Networks",
      "author" : [ "Yuanhe Tian", "Yan Song", "Fei Xia", "Tong Zhang", "Yonggang Wang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "Tian et al\\.,? 2020b",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2020
    }, {
      "title" : "Show and Tell: A Neural Image Caption Generator",
      "author" : [ "Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3156–3164.",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Coding Structures and Actions with the COSTA Scheme in Medical Conversations",
      "author" : [ "Nan Wang", "Yan Song", "Fei Xia." ],
      "venue" : "Proceedings of the BioNLP 2018 workshop, pages 76–86.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Studying Challenges in Medical Conversation with Structured Annotation",
      "author" : [ "Nan Wang", "Yan Song", "Fei Xia." ],
      "venue" : "Proceedings of the First Workshop on Natural Language Processing for Medical Conversations, pages 12–21, Online.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Hierarchical Attention Network for Image Captioning",
      "author" : [ "Weixuan Wang", "Zhihong Chen", "Haifeng Hu." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 8957–8964.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Memory Networks",
      "author" : [ "Jason Weston", "Sumit Chopra", "Antoine Bordes." ],
      "venue" : "CoRR, abs/1410.3916.",
      "citeRegEx" : "Weston et al\\.,? 2015",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2015
    }, {
      "title" : "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio." ],
      "venue" : "International conference on machine learn-",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Topic Memory Networks for Short Text Classification",
      "author" : [ "Jichuan Zeng", "Jing Li", "Yan Song", "Cuiyun Gao", "Michael R Lyu", "Irwin King." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3120–",
      "citeRegEx" : "Zeng et al\\.,? 2018",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning to Summarize Radiology Findings",
      "author" : [ "Yuhao Zhang", "Daisy Yi Ding", "Tianpei Qian", "Christopher D Manning", "Curtis P Langlotz." ],
      "venue" : "Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis, pages",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 33,
      "context" : ", Transformer) as the decoder following the image captioning paradigm (Vinyals et al., 2015; Anderson et al., 2018).",
      "startOffset" : 70,
      "endOffset" : 115
    }, {
      "referenceID" : 1,
      "context" : ", Transformer) as the decoder following the image captioning paradigm (Vinyals et al., 2015; Anderson et al., 2018).",
      "startOffset" : 70,
      "endOffset" : 115
    }, {
      "referenceID" : 11,
      "context" : "Therefore, it is expected to have a better solution to model the alignments across modalities and further improve the generation ability, although promising results are continuously acquired by other approaches (Li et al., 2018; Liu et al., 2019; Jing et al., 2019; Chen et al., 2020).",
      "startOffset" : 211,
      "endOffset" : 284
    }, {
      "referenceID" : 13,
      "context" : "Therefore, it is expected to have a better solution to model the alignments across modalities and further improve the generation ability, although promising results are continuously acquired by other approaches (Li et al., 2018; Liu et al., 2019; Jing et al., 2019; Chen et al., 2020).",
      "startOffset" : 211,
      "endOffset" : 284
    }, {
      "referenceID" : 22,
      "context" : "In our approach, the visual features X of a radiology image I are extracted by pre-trained convolutional neural networks (CNN), such as VGG (Simonyan and Zisserman, 2015) or ResNet (He et al.",
      "startOffset" : 140,
      "endOffset" : 170
    }, {
      "referenceID" : 8,
      "context" : ", 2016)6 from Indiana University and MIMIC-CXR (Johnson et al., 2019)7 from the Beth Israel Deaconess Medical Center.",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 20,
      "context" : ", 2015), ATT2IN (Rennie et al., 2017), ADAATT (Lu et al.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 11,
      "context" : ", 2018), HRGR (Li et al., 2018), CMAS-RL (Jing et al.",
      "startOffset" : 14,
      "endOffset" : 31
    }, {
      "referenceID" : 17,
      "context" : "The NLG metrics9 include BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011) and ROUGE-L (Lin,",
      "startOffset" : 30,
      "endOffset" : 53
    }, {
      "referenceID" : 11,
      "context" : "To ensure consistency with the experiment settings of previous work (Li et al., 2018; Chen et al., 2020), we use two images of a patient as input for report generation on IU X-RAY and one image for MIMIC-CXR.",
      "startOffset" : 68,
      "endOffset" : 104
    }, {
      "referenceID" : 9,
      "context" : "We train our model under cross entropy loss with Adam optimizer (Kingma and Ba, 2015).",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 33,
      "context" : "In general, the most popular related task to ours is image captioning, a cross-modal task involving natural language processing and computer vision, which aims to describe images in sentences (Vinyals et al., 2015; Xu et al., 2015; Anderson et al., 2018; Wang et al., 2019; Cornia et al., 2019).",
      "startOffset" : 192,
      "endOffset" : 294
    }, {
      "referenceID" : 38,
      "context" : "In general, the most popular related task to ours is image captioning, a cross-modal task involving natural language processing and computer vision, which aims to describe images in sentences (Vinyals et al., 2015; Xu et al., 2015; Anderson et al., 2018; Wang et al., 2019; Cornia et al., 2019).",
      "startOffset" : 192,
      "endOffset" : 294
    }, {
      "referenceID" : 1,
      "context" : "In general, the most popular related task to ours is image captioning, a cross-modal task involving natural language processing and computer vision, which aims to describe images in sentences (Vinyals et al., 2015; Xu et al., 2015; Anderson et al., 2018; Wang et al., 2019; Cornia et al., 2019).",
      "startOffset" : 192,
      "endOffset" : 294
    }, {
      "referenceID" : 36,
      "context" : "In general, the most popular related task to ours is image captioning, a cross-modal task involving natural language processing and computer vision, which aims to describe images in sentences (Vinyals et al., 2015; Xu et al., 2015; Anderson et al., 2018; Wang et al., 2019; Cornia et al., 2019).",
      "startOffset" : 192,
      "endOffset" : 294
    }, {
      "referenceID" : 37,
      "context" : "ory matrices to learn a priori knowledge for visual features using memory networks (Weston et al., 2015; Sukhbaatar et al., 2015; Zeng et al., 2018; Santoro et al., 2018; Nie et al., 2020; Diao et al., 2020; Tian et al., 2020b, 2021; Chen et al., 2021),",
      "startOffset" : 83,
      "endOffset" : 252
    }, {
      "referenceID" : 28,
      "context" : "ory matrices to learn a priori knowledge for visual features using memory networks (Weston et al., 2015; Sukhbaatar et al., 2015; Zeng et al., 2018; Santoro et al., 2018; Nie et al., 2020; Diao et al., 2020; Tian et al., 2020b, 2021; Chen et al., 2021),",
      "startOffset" : 83,
      "endOffset" : 252
    }, {
      "referenceID" : 39,
      "context" : "ory matrices to learn a priori knowledge for visual features using memory networks (Weston et al., 2015; Sukhbaatar et al., 2015; Zeng et al., 2018; Santoro et al., 2018; Nie et al., 2020; Diao et al., 2020; Tian et al., 2020b, 2021; Chen et al., 2021),",
      "startOffset" : 83,
      "endOffset" : 252
    }, {
      "referenceID" : 21,
      "context" : "ory matrices to learn a priori knowledge for visual features using memory networks (Weston et al., 2015; Sukhbaatar et al., 2015; Zeng et al., 2018; Santoro et al., 2018; Nie et al., 2020; Diao et al., 2020; Tian et al., 2020b, 2021; Chen et al., 2021),",
      "startOffset" : 83,
      "endOffset" : 252
    }, {
      "referenceID" : 15,
      "context" : "ory matrices to learn a priori knowledge for visual features using memory networks (Weston et al., 2015; Sukhbaatar et al., 2015; Zeng et al., 2018; Santoro et al., 2018; Nie et al., 2020; Diao et al., 2020; Tian et al., 2020b, 2021; Chen et al., 2021),",
      "startOffset" : 83,
      "endOffset" : 252
    }, {
      "referenceID" : 2,
      "context" : "ory matrices to learn a priori knowledge for visual features using memory networks (Weston et al., 2015; Sukhbaatar et al., 2015; Zeng et al., 2018; Santoro et al., 2018; Nie et al., 2020; Diao et al., 2020; Tian et al., 2020b, 2021; Chen et al., 2021),",
      "startOffset" : 83,
      "endOffset" : 252
    }, {
      "referenceID" : 16,
      "context" : ", pre-trained language models) have been applied to tasks in the medical domain (Pampari et al., 2018; Zhang et al., 2018; Wang et al., 2018; Alsentzer et al., 2019; Tian et al., 2019, 2020a; Wang et al., 2020; Lee et al., 2020; Song et al., 2020).",
      "startOffset" : 80,
      "endOffset" : 247
    }, {
      "referenceID" : 40,
      "context" : ", pre-trained language models) have been applied to tasks in the medical domain (Pampari et al., 2018; Zhang et al., 2018; Wang et al., 2018; Alsentzer et al., 2019; Tian et al., 2019, 2020a; Wang et al., 2020; Lee et al., 2020; Song et al., 2020).",
      "startOffset" : 80,
      "endOffset" : 247
    }, {
      "referenceID" : 34,
      "context" : ", pre-trained language models) have been applied to tasks in the medical domain (Pampari et al., 2018; Zhang et al., 2018; Wang et al., 2018; Alsentzer et al., 2019; Tian et al., 2019, 2020a; Wang et al., 2020; Lee et al., 2020; Song et al., 2020).",
      "startOffset" : 80,
      "endOffset" : 247
    }, {
      "referenceID" : 0,
      "context" : ", pre-trained language models) have been applied to tasks in the medical domain (Pampari et al., 2018; Zhang et al., 2018; Wang et al., 2018; Alsentzer et al., 2019; Tian et al., 2019, 2020a; Wang et al., 2020; Lee et al., 2020; Song et al., 2020).",
      "startOffset" : 80,
      "endOffset" : 247
    }, {
      "referenceID" : 35,
      "context" : ", pre-trained language models) have been applied to tasks in the medical domain (Pampari et al., 2018; Zhang et al., 2018; Wang et al., 2018; Alsentzer et al., 2019; Tian et al., 2019, 2020a; Wang et al., 2020; Lee et al., 2020; Song et al., 2020).",
      "startOffset" : 80,
      "endOffset" : 247
    }, {
      "referenceID" : 10,
      "context" : ", pre-trained language models) have been applied to tasks in the medical domain (Pampari et al., 2018; Zhang et al., 2018; Wang et al., 2018; Alsentzer et al., 2019; Tian et al., 2019, 2020a; Wang et al., 2020; Lee et al., 2020; Song et al., 2020).",
      "startOffset" : 80,
      "endOffset" : 247
    }, {
      "referenceID" : 26,
      "context" : ", pre-trained language models) have been applied to tasks in the medical domain (Pampari et al., 2018; Zhang et al., 2018; Wang et al., 2018; Alsentzer et al., 2019; Tian et al., 2019, 2020a; Wang et al., 2020; Lee et al., 2020; Song et al., 2020).",
      "startOffset" : 80,
      "endOffset" : 247
    } ],
    "year" : 2021,
    "abstractText" : "Medical imaging plays a significant role in clinical practice of medical diagnosis, where the text reports of the images are essential in understanding them and facilitating later treatments. By generating the reports automatically, it is beneficial to help lighten the burden of radiologists and significantly promote clinical automation, which already attracts much attention in applying artificial intelligence to medical domain. Previous studies mainly follow the encoder-decoder paradigm and focus on the aspect of text generation, with few studies considering the importance of cross-modal mappings and explicitly exploit such mappings to facilitate radiology report generation. In this paper, we propose a cross-modal memory networks (CMN) to enhance the encoderdecoder framework for radiology report generation, where a shared memory is designed to record the alignment between images and texts so as to facilitate the interaction and generation across modalities. Experimental results illustrate the effectiveness of our proposed model, where state-of-the-art performance is achieved on two widely used benchmark datasets, i.e., IU X-Ray and MIMIC-CXR. Further analyses also prove that our model is able to better align information from radiology images and texts so as to help generating more accurate reports in terms of clinical indicators.1",
    "creator" : "LaTeX with hyperref"
  }
}