{
  "name" : "2021.acl-long.161.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information",
    "authors" : [ "Zijun Sun", "Xiaoya Li", "Xiaofei Sun", "Yuxian Meng", "Xiang Ao", "Qing He", "Fei Wu", "Jiwei Li" ],
    "emails" : [ "jiwei_li}@shannonai.com", "aoxiang@ict.ac.cn,", "heqing@ict.ac.cn,", "wufei@zju.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2065–2075\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2065"
    }, {
      "heading" : "1 Introduction",
      "text" : "Large-scale pretrained models have become a fundamental backbone for various natural language processing tasks such as natural language understanding (Liu et al., 2019b), text classification (Reimers and Gurevych, 2019; Chai et al., 2020) and question answering (Clark and Gardner, 2017; Lewis et al., 2020). Apart from English NLP tasks, pretrained models have also demonstrated their effectiveness for various Chinese NLP tasks (Sun et al., 2019, 2020; Cui et al., 2019a, 2020).\n1The code and pretrained models are publicly available at https://github.com/ShannonAI/ChineseBert.\nSince pretraining models are originally designed for English, two important aspects specific to the Chinese language are missing in current large-scale pretraining: glyph-based information and pinyinbased information. For the former, a key aspect that makes Chinese distinguishable from languages such as English, German, is that Chinese is a logographic language. The logographic of characters encodes semantic information. For example, “液(liquid)”, “河(river)” and “湖(lake)” all have the radical “氵(water)”, which indicates that they are all related to water in semantics. Intuitively, the rich semantics behind Chinese character glyphs should enhance the expressiveness of Chinese NLP models. This idea has motivated a variety of of work on learning and incorporating Chinese glyph information into neural models (Sun et al., 2014; Shi et al., 2015; Liu et al., 2017; Dai and Cai, 2017; Su and Lee, 2017; Meng et al., 2019), but not yet large-scale pretraining.\nFor the latter, pinyin, the Romanized sequence of a Chinese character representing its pronunciation(s), is crucial in modeling both semantic and syntax information that can not be captured by contextualized or glyph embeddings. This aspect is especially important considering the highly prevalent heteronym phenomenon in Chinese2, where the same character have multiple pronunciations, each of which is associated with a specific meaning. Each pronunciation is associated with a specific pinyin expression. At the semantic level, for example, the Chinese character “乐” has two distinctly different pronunciations: “乐” can be pronounced as “yuè [yE51]”, which means “music”, and “lè [lG51]”, which means “happy”. On the syntax level, pronunciations help identify the part-of-speech of a character. For example, character “还” has two\n2Among 7000 common characters in Chinese, there are about 700 characters that have multiple pronunciations, according to the Contemporary Chinese Dictionary.\npronunciations: “huán[xwan35]” and “hái[xaI35]”, with the former meaning the verb “return” and the latter meaning the adverb “also”. Different pronunciations of the same character cannot be distinguished by the glyph embedding since the logographic is the same, or the char-ID embedding, since they both point to the same character ID, but can be characterized by pinyin.\nIn this work, we propose ChineseBERT, a model that incorporates the glyph and pinyin information of Chinese characters into the process of largescale pretraining. The glyph embedding is based on different fonts of a Chinese character, being able to capture character semantics from the visual surface character forms. The pinyin embedding models different semantic meanings that share the same character form and thus bypasses the limitation of interwound morphemes behind a single character. For a Chinese character, the glyph embedding, the pinyin embedding and the character embedding are combined to form a fusion embedding, which models the distinctive semantic property of that character.\nWith less training data and fewer training epochs, ChineseBERT achieves significant performance boost over baselines across a wide range of Chinese NLP tasks. It achieves new SOTA performances on a wide range of Chinese NLP tasks，including machine reading comprehension, natural language inference, text classification, sentence pair matching, and results comparable to SOTA performances in named entity recognition and word segmentation."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Large-Scale Pretraining in NLP",
      "text" : "Recent years has witnessed substantial work on large-scale pretraining in NLP. BERT (Devlin et al., 2018), which is built on top of the Transformer architecture (Vaswani et al., 2017), is pretrained on large-scale unlabeled text corpus in the manner of Masked Language Model (MLM) and Next Sentence Prediction (NSP). Following this trend, considerable progress has been made by modifying the masking strategy (Yang et al., 2019; Joshi et al., 2020), pretraining tasks (Liu et al., 2019a; Clark et al., 2020) or model backbones (Lan et al., 2020; Lample et al., 2019; Choromanski et al., 2020). Specifically, RoBERTa (Liu et al., 2019b) proposed to remove the NSP pretraining task since it has been proved to offer no benefits for improving down-\nstream performances. The GPT series (Radford et al., 2019; Brown et al., 2020) and other BERT variants (Lewis et al., 2019; Song et al., 2019; Lample and Conneau, 2019; Dong et al., 2019; Bao et al., 2020; Zhu et al., 2020) adapted the paradigm of large-scale unsupervised pretraining to text generation tasks such as machine translation, text summarization and dialog generation, so that generative models can enjoy the benefit of large-scale pretraining.\nUnlike the English language, Chinese has its particular characteristics in terms of syntax, lexicon and pronunciation. Hence, pretraining Chinese models should fit the Chinese features correspondingly. Li et al. (2019b) proposed to use Chinese character as the basic unit instead of word or subword that is used in English (Wu et al., 2016; Sennrich et al., 2016). ERNIE (Sun et al., 2019, 2020) applied three types of masking strategies – charlevel masking, phrase-level masking and entitylevel masking – to enhance the ability of capturing multi-granularity semantics. Cui et al. (2019a, 2020) pretrained models using the Whole Word Masking strategy, where all characters within a Chinese word are masked altogether. In this way, the model is learning to address a more challenging task as opposed to predicting word components. More recently, Zhang et al. (2020) developed the largest Chinese pretrained language model to date – CPM. It is pretrained on 100GB Chinese data and has 2.6B parameters comparable to “GPT3 2.7B” (Brown et al., 2020). Xu et al. (2020) released the first large-scale Chinese Language Understanding Evaluation benchmark CLUE, facilitating researches in large-scale Chinese pretraining."
    }, {
      "heading" : "2.2 Learning Glyph Information",
      "text" : "Learning glyph information from surface Chinese character forms has gained attractions since the prevalence of deep neural networks. Inspired by word embeddings (Mikolov et al., 2013b,a), Sun et al. (2014); Shi et al. (2015); Li et al. (2015); Yin et al. (2016) used indexed radical embeddings to capture character semantics, improving model performances on a wide range of Chinese NLP tasks. Another way of incorporating glyph information is to view characters in the form of image, by which glyph information can be naturally learned through image modeling. However, early work on learning visual features is not smooth. Liu et al. (2017); Shao et al. (2017); Zhang and LeCun (2017); Dai\nFusion Layer\nBERT\n我 很 [M] [M] 猫\n我 很 [M] [M] 猫\ni very cats\nChar embedding\n0 31 2 4 Position embedding\nGlyph embedding\nPinyin embedding\n我 很 0 0 猫\nwo3 hen3 0 0 mao1\nFusion embedding我 很 [M] [M] 猫\nFusion Layer\n喜 欢 like\nOutput\n猫\n猫\nmao1\nChar embedding\nGlyph embedding\nPinyin embedding\n猫\nFusion embedding\n猫 māo\nm a o 1 - - - -\nm a o 1 - - - -\nCNN\n猫1\n猫3\n猫2\nGlyph Layer\n猫\nGlyph embedding\nComponent embedding\nmao1\nflatten\nflatten\nflatten\n28\n28\n\uD835\uDC16G\n\uD835\uDC16F\nPinyin embedding\nFigure 1: An overview of ChineseBERT. The fusion layer consumes three D-dimensional embeddings – char embedding, glyph embedding and pinyin embedding. The three embeddings are first concatenated, and then mapped to a D-dimensional embedding through a fully connected layer to form the fusion embedding.\nand Cai (2017) used CNNs to extract glyph features from character images but did not achieve consistent performance boost over all tasks. Su and Lee (2017); Tao et al. (2019) obtained positive results on the word analogy and word similarity tasks but they did not further evaluate the learned glyph embeddings on more tasks. Meng et al. (2019) applied glyph embeddings to a broad array of Chinese tasks. They designed a specific CNN structure for character feature extraction and used image classification as an auxiliary objective to regularize the influence of a limited number of images. Song and Sehanobish (2020); Xuan et al. (2020) extended the idea of Meng et al. (2019) to the task of named entity recognition (NER), significantly improving performances against vanilla BERT models.\n3 Model\nFusion Layer\nBERT\n我 很 [M] [M] 猫\n我 很 [M] [M] 猫\ni very cats\nChar embedding\n0 31 2 4\nPosition embedding\nGlyph embedding Pinyin embedding\n我 很 0 0 猫\nwo3 hen3 0 0 mao1\nFusion embedding\n我 很 [M] [M] 猫\nFusion Layer\n喜 欢 like\nOutput\n猫\n猫\nmao1\nChar embedding\nGlyph embedding\nPinyin embedding\n猫\nFusion embedding\n猫 māo\nm a o 1 - - - -\nm a o 1 - - - -\nCNN\nmao1\n\uD835\uDC16F\nFusion Layer BERT\n我 很 [M] [M] 猫\n我 很 [M] [M] 猫\ni very cats\nChar embedding\n0 31 2 4 Position embedding\nGlyph embedding\nPinyin embedding\n我 很 0 0 猫\nwo3 hen3 0 0 mao1\nFusion embedding我 很 [M] [M] 猫\nFusion Layer\n喜 欢 like\nOutput\n猫\n猫\nmao1\nChar embedding\nGlyph embedding\nPinyin embedding\n猫\nFusion embedding\n猫 māo\nm a o 1 - - - -\nm a o 1 - - - -\nCNN\n猫1\n猫3\n猫2\nGlyph Layer\n猫\nGlyph embedding\nComponent embedding\nmao1\nflatten\nflatten\nflatten\n28\n28\n\uD835\uDC16G\n\uD835\uDC16F\nPinyin embedding\nFigure 3: An overview of inducing the pinyin embedd g. For any Chinese character, e.g. 猫(cat) in this case, a CNN with width 2 is applied to the sequence of Romanized pinyin letters, followed by max-pooling to derive the final pinyin embedding.\nFusion Layer\nBERT\n我 很 [M] [M] 猫\n我 很 [M] [M] 猫\ni very cats\nChar embedding\n0 31 2 4\nPosition embedding\nGlyph embedding Pinyin embedding\n我 很 0 0 猫\nwo3 hen3 0 0 mao1\nFusion embedding\n我 很 [M] [M] 猫\nFusion Layer\n喜 欢 like\nOutput\n猫\n猫\nmao1\nChar embeddi g\nGlyph embedding\nPinyin embedding\n猫\nFusion embedding\n猫 māo\nm a o 1 - - - -\nm a o 1 - - - -\nCNN\n猫1\n猫3\n猫2\nGlyph Layer\n猫\nGlyph embedding\nComponent embedding\nmao1\nflatten\nflatten\nflatten\n28\n28\n\uD835\uDC16G\n\uD835\uDC16F\nFigure 4: An overview of the fusion layer.\n⊗\ndenotes\nvector concatenation, and × is vector-matrix multiplication. We concatenate the char embedding, the glyph embedding and the pinyin embedding, and use an FC layer with a learnable matrix WF to induce the fusion embedding."
    }, {
      "heading" : "3.1 Overview",
      "text" : "Figure 1 shows an overview of the proposed ChineseBERT model. For each Chinese character, its char embedding, glyph embedding and pinyin embedding are first concatenated, and then mapped to a D-dimensional embedding through a fully connected layer to form the fusion embedding. The fusion embedding is then added with the position embedding, which is fed as input to the BERT model Since we do not use the NSP pretraining task, we omit the segment embedding. We use both Whole Word Masking (WWM) (Cui et al., 2019a) and Char Masking (CM) for pretraining (See Section 4.2 for details)."
    }, {
      "heading" : "3.2 Input",
      "text" : "The input to the model is the addition of the learnable absolute positional embedding and the fusion embedding, where the fusion embedding is based on the char embedding, the glyph embedding and the pinyin embedding of the corresponding character. The char embedding performs in a way analogous to the token embedding used in BERT but at the character granularity. Below we respectively describe how to induce the glyph embedding, the pinyin embedding and the fusion embedding.\nGlyph Embedding We follow Meng et al. (2019) to use three types of Chinese fonts – FangSong, XingKai and LiShu, each of which is instantiated as a 24× 24 image with floating point pixels ranging from 0 to 255. The 24×24×3 vector is first flattened to a 2,352 vector. The flattened vector is fed to an FC layer to obtain the output glyph vector.\nPinyin Embedding The pinyin embedding for each character is used to decouple different semantic meanings belonging to the same character form, as shown in Figure 3. We use the opensourced pypinyin package3 to generate pinyin sequences for its constituent characters. pypinyin is a system that combines machine learning models with dictionary-based rules to infer the pinyin for characters given contexts. Pinyin for a Chinese character is a sequence of Romanian characters, with one of four diacritics denoting tones. We use special tokens to denote tones, which are appended to the end of the Romanian character sequence. We apply a CNN model with width 2 on the pinyin sequence, followed by max-pooling to derive the resulting pinyin embedding. This makes output dimensionality immune to the length of the input pinyin sequence. The length of the input pinyin sequence is fixed at 8, with the remaining slots filled with a special letter “-” when the actual length of the pinyin sequence does not reach 8.\nFusion Embedding Once we have the char embedding, the glyph embedding and the pinyin embedding for a character, we concatenate them to form a 3D-dimensional vector. The fusion layers maps the 3D-dimensional vector to D-dimensional through a fully connected layer. The fusion embedding is added with position embedding, and output to the BERT layer. An illustration is shown in Figure 4."
    }, {
      "heading" : "3.3 Output",
      "text" : "The output is the corresponding contextualized representation for each input Chinese character (Devlin et al., 2018)."
    }, {
      "heading" : "4 Pretraining Setup",
      "text" : ""
    }, {
      "heading" : "4.1 Data",
      "text" : "We collected our pretraining data from CommonCrawl4. After pre-processing (such as removing the data with too much English text and filtering\n3https://pypi.org/project/pypinyin/ 4https://commoncrawl.org/\nthe html tagger), about 10% high-quality data is maintained for pretraining, containing 4B Chinese characters in total. We use the LTP toolkit5 (Che et al., 2010) to identify the boundary of Chinese words for whole word masking."
    }, {
      "heading" : "4.2 Masking Strategies",
      "text" : "We use two masking strategies – Whole Word Masking (WWM) and Char Masking (CM) for ChineseBERT. Li et al. (2019b) suggested that using Chinese characters as the basic input unit can alleviate the out-of-vocabulary issue in the Chinese language. We thus adopt the method of masking random characters in the given context, denoted by Char Masking. On the other hand, a large number of words in Chinese consist of multiple characters, for which the CM strategy may be too easy for the model to predict. For example, for the input context “我喜欢逛紫禁[M] (i like going to The Forbidden [M])”, the model can easily predict that the masked character is “城(City)”. Hence, we follow Cui et al. (2019a) to use WWM, a strategy to mask out all characters within a selected word, mitigating the easy-predicting shortcoming of the CM strategy. Note that for both WWM and CM, the basic input unit is Chinese characters. The main difference between WWM and CM lies in how they mask characters and how the model predicts masked characters."
    }, {
      "heading" : "4.3 Pretraining Details",
      "text" : "Different from Cui et al. (2019a) who pretrained their model based on the official pretrained Chinese BERT model, we train the ChineseBERT model from scratch. To enforce the model to learn both long-term and short-term dependencies, we propose to alternate pretraining between packed input and single input, where the packed input is the concatenation of multiple sentences with a maximum length 512, and the single input is a single sentence. We feed the packed input with probability of 0.9 and the single input with probability of 0.1. We apply Whole Word Masking 90% of the time and Char Masking 10% of the time. The masking probability for each word/char is 15%. If the i-th word/char is chosen, we mask it 80% of the time, replace it with a random word/char 10% of the time and maintain it 10% of the time. We also use the dynamic masking strategy to avoid duplicate training instances (Liu et al., 2019b). We use\n5http://ltp.ai/\ntwo model setups: base and large, respectively consisting of 12/24 Transformer layers, with input dimensionality of 768/1,024 and 12/16 heads per layer. This makes our models comparable to other BERT-style models in terms of model size. Upon the submission of the paper, we have trained the base model 500K steps with a maximum learning rate 1e-4, warmup of 20K steps and a batch size of 3.2k, and the large model 280K steps with a maximum learning rate 3e-4, warmup of 90K steps and a batch size of 8k. After pretraining, the model can be directly used to be finetuned on downstream tasks in the same way as BERT (Devlin et al., 2018)."
    }, {
      "heading" : "5 Experiments",
      "text" : "We conduct extensive experiments on a variety of Chinese NLP tasks. Models are separately finetuned on task-specific datasets for evaluation. Concretely, we use the following tasks:\n• Machine Reading Comprehension (MRC)\n• Natural Language Inference (NLI)\n• Text Classification (TC)\n• Sentence Pair Matching (SPM)\n• Named Entity Recognition (NER)\n• Chinese Word Segmentation (CWS).\nWe compare ChineseBERT to current state-ofthe-art ERNIE (Sun et al., 2019, 2020), BERTwwm (Cui et al., 2019a) and MacBERT (Cui et al., 2020) models. ERNIE adopts various masking strategies including token-level, phrase-level and\nentity-level masking to pretrain BERT on largescale heterogeneous data. BERT-wwm/RoBERTawwm continues pretraining on top of official pretrained Chinese BERT/RoBERTa models with the Whole Word Masking pretraining strategy. Unless otherwise specified, we use BERT/RoBERTa to represent BERT-wwm/RoBERTa-wwm and omit “wwm”. MacBERT improves upon RoBERTa by using the MLM-As-Correlation (MAC) pretraining strategy as well as the sentence-order prediction (SOP) task. It is worth noting that BERT and BERTwwm do not have the large version available online, and we thus omit the corresponding performances.\nA comparison of these models is shown in Table 1. It is worth noting that training steps of the proposed model significantly smaller than baseline models. Different from BERT-wwm and MacBERT which are initialized with pretrained BERT, the proposed model is initialized from scratch. Due to the additional consideration of glyph and pinyin, the proposed cannot be directly initialized using a vanilla BERT model, as the model structures are different. Even initialized from scratch, the proposed model is trained fewer steps than the steps in retraining BERT-wwm and MacBERT after BERT initialization."
    }, {
      "heading" : "5.1 Machine Reading Comprehension",
      "text" : "Machine reading comprehension tests the model’s ability of answering the questions based on the given contexts. We use two datasets for this task: CMRC 2018 (Cui et al., 2019b) and CJRC (Duan et al., 2019) . CMRC is a span-extraction style dataset while CJRC additionally has yes/no questions and no-answer questions. CMRC 2018 and CJRC respectively contain 10K/3.2K/4.9K and 39K/6K/6K data instances for training/dev/test. Test results for CMRC 2018 are evaluated from the CLUE leaderboard.6 Note that the CJRC dataset is different from the one used in Cui et al. (2019a) as Cui et al. (2019a) did not release their train/dev/test split. We thus run the released models on the CJRC dataset used in this work for comparison.\nResults are shown in Table 2 and Table 3. As we can see, ChineseBERT yields significant performance boost on both datasets, and the improvement of EM is larger than that of F1 on the CJRC dataset, which indicates that ChineseBERT is better at detecting exact answer spans.\n6https://github.com/CLUEbenchmark/CLUE"
    }, {
      "heading" : "5.2 Natural Language Inference (NLI)",
      "text" : "The goal of NLI is to determine the entailment relationship between a hypothesis and a premise. We use the Cross-lingual Natural Language Inference (XNLI) dataset (Conneau et al., 2018) for evaluation. The corpus is a crowd-sourced collection of 5K test and 2.5K dev pairs for the MultiNLI corpus. Each sentence pair is annotated with the “entailment”, “neutral” or “contradiction” label. We use the official machine translated Chinese data for training.7\nResults are present in Table 4, which shows that ChineseBERT is able to achieve the best performances for both base and large setups."
    }, {
      "heading" : "5.3 Text Classification (TC)",
      "text" : "In text classification the model is required to categorize a piece of text into one of the specified classes. We follow Cui et al. (2019a) to use THUC-\n7https://github.com/facebookresearch/ XNLI\nNews (Li and Sun, 2007) and ChnSentiCorp8 for this task. THUCNews is a subset of THUCTC 9, with 50K/5K/10K data points respectively for training/dev/test. Data is evenly distributed in 10 domains including sports, finance, etc.10 ChnSentiCorp is a binary sentiment classification dataset containing 9.6K/1.2K/1.2K data points respectively for training/dev/test. The two datasets are relatively simple with vanilla BERT achieving an accuracy of above 95%. Hence, apart from THUCNews and ChnSentiCorp, we also use TNEWS, a more difficult dataset that is included in the CLUE benchmark (Xu et al., 2020).11 TNEWS is a 15- class short news text classification dataset with 53K/10K/10K data points for training/dev/test.\nResults are shown in Table 5. On ChunSentiCorp and THUCNews, the improvement from ChineseBERT is marginal as baselines have already achieved quite high results on these two datasets. On the TNEWS dataset, ChineseBERT outperforms all other models. We can see that the ERNIE model only performs slightly worse than ChineseBERT. This is because ERNIE is trained on additional web data, which is beneficial to model web news text that covers a wide range of domains."
    }, {
      "heading" : "5.4 Sentence Pair Matching (SPM)",
      "text" : "For SPM, the model is asked to determine whether a given sentence pair expresses the same semantics. We use the LCQMC (Liu et al., 2018) and BQ Corpus (Chen et al., 2018) datasets for evaluation.\n8https://github.com/pengming617/bert_ classification/tree/master/data\n9http://thuctc.thunlp.org/ 10https://github.com/gaussic/\ntext-classification-cnn-rnn 11https://github.com/CLUEbenchmark/CLUE\nLCQMC is a large-scale Chinese question matching corpus for judging whether two given questions have the same intent, with 23.9K/8.8K/12.5K sentence pairs for training/dev/test. BQ Corpus is another large-scale Chinese dataset containing 100K/10K/10K sentence pairs for training/dev/test. Results are shown in Table 6. We can see that ChineseBERT generally outperforms MacBERT on LCQMC but slightly underperforms BERT-wwm. We hypothesis this is because the domain of BQ Corpus more fits the pretraining data of BERTwwm than that of ChineseBERT."
    }, {
      "heading" : "5.5 Named Entity Recognition (NER)",
      "text" : "For NER tasks (Chiu and Nichols, 2016; Lample et al., 2016; Li et al., 2019a), the model is asked to identify named entities within a piece of text, which is formalized as a sequence labeling task. We use OntoNotes 4.0 (Weischedel et al., 2011) and Weibo (Peng and Dredze, 2015) for this task.\nWe use OntoNotes 4.0 and Weibo NER for this task. OntoNotes has 18 named entity types and Weibo has 4 named entity types. OntoNotes and Weibo respectively contain 15K/4K/4K and 1,350/270/270 instances for training/dev/test. Results are shown in Table 7. As we can see, ChineseBERT significantly outperforms BERT and RoBERTa in terms of F1. In spite of a slight loss on precision for the base version, the gains on recall are particularly high, leading to a final performance boost on F1."
    }, {
      "heading" : "5.6 Chinese Word Segmentation",
      "text" : "The task divides text into words and is formalized as a character-based sequence labelling task. We use the PKU and MSRA datasets for Chinese word segmentation. PKU consists of 19K/2K sentences for training and test, and MSRA consists of 87k/4k sentences for training and test. Output character embedding is fed to the softmax function for final predictions. Results are shown in Table 8, where we can see that ChineseBERT is able to outperform BERT-wwm and RoBERTa-wwm on both datasets for both metrics."
    }, {
      "heading" : "6 Ablation Studies",
      "text" : "In this section, we conduct ablation studies to understand the behaviors of ChineseBERT. We\nuse the Chinese named entity recognition dataset OntoNotes 4.0 for analysis and all models are based on the base version."
    }, {
      "heading" : "6.1 The Effect of Glyph Embeddings and Pinyin Embeddings",
      "text" : "We would like to explore the effects of glyph embeddings and pinyin embeddings. For fair comparison, we pretrained different models on the same dataset, with the same number of training steps, and with the same model size. Setups include “-glyph”, where glyph embeddings are not considered and we only consider pinyin and char-ID embeddings; “-pinyin”, where pinyin embeddings are not considered and we only consider glyph and char-ID embeddings; “-glyph-pinyin”, where only char-ID embeddings are considered, and the model degenerates to RoBERTa. We finetune different models on the OntoNotes dataset of the NER dataset for comparison.\nResults are shown in Table 9. As can be seen, either removing glyph embeddings or pinyin embeddings results in performance degradation, and removing both has the greatest negative impact on the F1 value, which is a drop of about 2 points. This validates the importance of both pinyin and glyph embeddings for modeling Chinese semantics. The reason why “-glyph-pinyin” performs worse than RoBERTa is that the model we use here is trained on a smaller size of data with smaller number of training steps."
    }, {
      "heading" : "6.2 The Effect of Training Data Size",
      "text" : "We hypothesize glyph and pinyin embeddings also serve as strong regularization over text semantics, which means that the proposed ChineseBERT model is able to perform better with less training data. We randomly sample 10%∼90% of the training data while maintaining the ratio of samples with entities w.r.t. samples without entities. We perform each experiment five times and report the average F1 value on the test set. Figure 5 shows the\nresults. As can be seen, ChineseBERT performs better across all setups. With less than 30% of the training data, the improvement of ChineseBERT is slight, but with over 30% training data, the performance improvement is greater. This is because ChineseBERT still requires sufficient training data to fully train the glyph and pinyin embeddings, and insufficient training data would lead to inadequate training."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we introduce ChineseBERT, a largescale pretraining Chinese NLP model. It leverages the glyph and pinyin information of Chinese characters to enhance the model’s ability of capturing context semantics from surface character forms and disambiguating polyphonic characters in Chinese. The proposed ChineseBERT model achieves significant performance boost across a wide range of Chinese NLP tasks. The proposed ChineseBERT performs better than vanilla pretrained models with less training data, indicating that the introduced glyph embeddings and pinyin embeddings serve as a strong regularizer for semantic modeling in Chinese. Future work involves training a large size version of ChineseBERT."
    }, {
      "heading" : "Acknowledgement",
      "text" : "This work is supported by National Key R&D Program of China (2020AAA0105200) and Beijing Academy of Artificial Intelligence (BAAI)."
    } ],
    "references" : [ {
      "title" : "Unilmv2: Pseudo-masked language models for unified language model pre-training",
      "author" : [ "Hangbo Bao", "Li Dong", "Furu Wei", "Wenhui Wang", "Nan Yang", "Xiaodong Liu", "Yu Wang", "Songhao Piao", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon" ],
      "venue" : null,
      "citeRegEx" : "Bao et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2020
    }, {
      "title" : "Description based text classification with reinforcement learning",
      "author" : [ "Duo Chai", "Wei Wu", "Qinghong Han", "Fei Wu", "Jiwei Li" ],
      "venue" : null,
      "citeRegEx" : "Chai et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Chai et al\\.",
      "year" : 2020
    }, {
      "title" : "LTP: A Chinese language technology platform",
      "author" : [ "Wanxiang Che", "Zhenghua Li", "Ting Liu." ],
      "venue" : "Coling 2010: Demonstrations, pages 13–16, Beijing, China. Coling 2010 Organizing Committee.",
      "citeRegEx" : "Che et al\\.,? 2010",
      "shortCiteRegEx" : "Che et al\\.",
      "year" : 2010
    }, {
      "title" : "The BQ corpus: A large-scale domain-specific Chinese corpus for sentence semantic equivalence identification",
      "author" : [ "Jing Chen", "Qingcai Chen", "Xin Liu", "Haijun Yang", "Daohe Lu", "Buzhou Tang." ],
      "venue" : "Proceedings of the 2018 Conference on",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Named entity recognition with bidirectional lstm-cnns",
      "author" : [ "Jason PC Chiu", "Eric Nichols." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:357–370.",
      "citeRegEx" : "Chiu and Nichols.,? 2016",
      "shortCiteRegEx" : "Chiu and Nichols.",
      "year" : 2016
    }, {
      "title" : "Simple and effective multi-paragraph reading comprehension",
      "author" : [ "Christopher Clark", "Matt Gardner." ],
      "venue" : "arXiv preprint arXiv:1710.10723.",
      "citeRegEx" : "Clark and Gardner.,? 2017",
      "shortCiteRegEx" : "Clark and Gardner.",
      "year" : 2017
    }, {
      "title" : "Electra: Pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Xnli: Evaluating crosslingual sentence representations",
      "author" : [ "Alexis Conneau", "Guillaume Lample", "Ruty Rinott", "Adina Williams", "Samuel R Bowman", "Holger Schwenk", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1809.05053.",
      "citeRegEx" : "Conneau et al\\.,? 2018",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "Revisiting pretrained models for chinese natural language processing",
      "author" : [ "Yiming Cui", "Wanxiang Che", "Ting Liu", "Bing Qin", "Shijin Wang", "Guoping Hu." ],
      "venue" : "arXiv preprint arXiv:2004.13922.",
      "citeRegEx" : "Cui et al\\.,? 2020",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2020
    }, {
      "title" : "Pre-training with whole word masking for chinese bert",
      "author" : [ "Yiming Cui", "Wanxiang Che", "Ting Liu", "Bing Qin", "Ziqing Yang", "Shijin Wang", "Guoping Hu." ],
      "venue" : "arXiv preprint arXiv:1906.08101.",
      "citeRegEx" : "Cui et al\\.,? 2019a",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2019
    }, {
      "title" : "A span-extraction dataset for Chinese machine reading comprehension",
      "author" : [ "Yiming Cui", "Ting Liu", "Wanxiang Che", "Li Xiao", "Zhipeng Chen", "Wentao Ma", "Shijin Wang", "Guoping Hu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods",
      "citeRegEx" : "Cui et al\\.,? 2019b",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2019
    }, {
      "title" : "Glyph-aware embedding of chinese characters",
      "author" : [ "Falcon Z Dai", "Zheng Cai." ],
      "venue" : "Proceedings of the First Workshop on Subword and Character Level Models in NLP, Copenhagen, Denmark, September 7, 2017, pages 64–69.",
      "citeRegEx" : "Dai and Cai.,? 2017",
      "shortCiteRegEx" : "Dai and Cai.",
      "year" : 2017
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Unified language model pre-training for natural language understanding and generation",
      "author" : [ "Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon." ],
      "venue" : "Advances in Neural Informa-",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Cjrc: A reliable human-annotated benchmark dataset for chinese judicial reading comprehension",
      "author" : [ "Xingyi Duan", "Baoxin Wang", "Ziyue Wang", "Wentao Ma", "Yiming Cui", "Dayong Wu", "Shijin Wang", "Ting Liu", "Tianxiang Huo", "Zhen Hu" ],
      "venue" : null,
      "citeRegEx" : "Duan et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Duan et al\\.",
      "year" : 2019
    }, {
      "title" : "Spanbert: Improving pre-training by representing and predicting spans",
      "author" : [ "Mandar Joshi", "Danqi Chen", "Yinhan Liu", "Daniel S Weld", "Luke Zettlemoyer", "Omer Levy." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:64–77.",
      "citeRegEx" : "Joshi et al\\.,? 2020",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer." ],
      "venue" : "arXiv preprint arXiv:1603.01360.",
      "citeRegEx" : "Lample et al\\.,? 2016",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Guillaume Lample", "Alexis Conneau." ],
      "venue" : "Advances in Neural Information Processing Systems (NeurIPS).",
      "citeRegEx" : "Lample and Conneau.,? 2019",
      "shortCiteRegEx" : "Lample and Conneau.",
      "year" : 2019
    }, {
      "title" : "Large memory layers with product keys. Advances in Neural Information Processing Systems (NeurIPS)",
      "author" : [ "Guillaume Lample", "Alexandre Sablayrolles", "Marc’Aurelio Ranzato", "Ludovic Denoyer", "Hervé Jégou" ],
      "venue" : null,
      "citeRegEx" : "Lample et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2019
    }, {
      "title" : "Albert: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "Bart: Denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Ves Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2019
    }, {
      "title" : "Retrieval-augmented generation for knowledge-intensive nlp tasks",
      "author" : [ "Patrick Lewis", "Ethan Perez", "Aleksandara Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich Küttler", "Mike Lewis", "Wen-tau Yih", "Tim Rocktäschel" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Scalable term selection for text categorization",
      "author" : [ "Jingyang Li", "Maosong Sun." ],
      "venue" : "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages",
      "citeRegEx" : "Li and Sun.,? 2007",
      "shortCiteRegEx" : "Li and Sun.",
      "year" : 2007
    }, {
      "title" : "A unified mrc framework for named entity recognition",
      "author" : [ "Xiaoya Li", "Jingrong Feng", "Yuxian Meng", "Qinghong Han", "Fei Wu", "Jiwei Li." ],
      "venue" : "arXiv preprint arXiv:1910.11476.",
      "citeRegEx" : "Li et al\\.,? 2019a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Is word segmentation necessary for deep learning of Chinese representations",
      "author" : [ "Xiaoya Li", "Yuxian Meng", "Xiaofei Sun", "Qinghong Han", "Arianna Yuan", "Jiwei Li" ],
      "venue" : "In Proceedings of the 57th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Li et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Component-enhanced chinese character embeddings",
      "author" : [ "Yanran Li", "Wenjie Li", "Fei Sun", "Sujian Li." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21,",
      "citeRegEx" : "Li et al\\.,? 2015",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning character-level compositionality with visual features",
      "author" : [ "Frederick Liu", "Han Lu", "Chieh Lo", "Graham Neubig." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada,",
      "citeRegEx" : "Liu et al\\.,? 2017",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2017
    }, {
      "title" : "Multi-task deep neural networks for natural language understanding",
      "author" : [ "Xiaodong Liu", "Pengcheng He", "Weizhu Chen", "Jianfeng Gao." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4487–4496, Flo-",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Glyce: Glyph-vectors for chinese character representations",
      "author" : [ "Yuxian Meng", "Wei Wu", "Fei Wang", "Xiaoya Li", "Ping Nie", "Fan Yin", "Muyu Li", "Qinghong Han", "Xiaofei Sun", "Jiwei Li." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32, pages",
      "citeRegEx" : "Meng et al\\.,? 2019",
      "shortCiteRegEx" : "Meng et al\\.",
      "year" : 2019
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "arXiv preprint arXiv:1301.3781.",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in neural information processing systems, 26:3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Named entity recognition for chinese social media with jointly trained embeddings",
      "author" : [ "Nanyun Peng", "Mark Dredze." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 548–554.",
      "citeRegEx" : "Peng and Dredze.,? 2015",
      "shortCiteRegEx" : "Peng and Dredze.",
      "year" : 2015
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI Blog, 1(8).",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Sentencebert: Sentence embeddings using siamese bertnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "arXiv preprint arXiv:1908.10084.",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Character-based joint segmentation and pos tagging for chinese using bidirectional rnn-crf",
      "author" : [ "Yan Shao", "Christian Hardmeier", "Jörg Tiedemann", "Joakim Nivre." ],
      "venue" : "Proceedings of the Eighth International Joint Conference on Natural Language Pro-",
      "citeRegEx" : "Shao et al\\.,? 2017",
      "shortCiteRegEx" : "Shao et al\\.",
      "year" : 2017
    }, {
      "title" : "Radical embedding: Delving deeper to Chinese radicals",
      "author" : [ "Xinlei Shi", "Junjie Zhai", "Xudong Yang", "Zehua Xie", "Chao Liu." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International",
      "citeRegEx" : "Shi et al\\.,? 2015",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2015
    }, {
      "title" : "Using chinese glyphs for named entity recognition",
      "author" : [ "Chan Hee Song", "Arijit Sehanobish." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 34(10):13921–13922.",
      "citeRegEx" : "Song and Sehanobish.,? 2020",
      "shortCiteRegEx" : "Song and Sehanobish.",
      "year" : 2020
    }, {
      "title" : "Mass: Masked sequence to sequence pre-training for language generation",
      "author" : [ "Kaitao Song", "Xu Tan", "Tao Qin", "Jianfeng Lu", "TieYan Liu." ],
      "venue" : "International Conference on Machine Learning, pages 5926–5936.",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning chinese word representations from glyphs of characters",
      "author" : [ "Tzu-Ray Su", "Hung-Yi Lee." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-",
      "citeRegEx" : "Su and Lee.,? 2017",
      "shortCiteRegEx" : "Su and Lee.",
      "year" : 2017
    }, {
      "title" : "Radical-enhanced chinese character embedding",
      "author" : [ "Yaming Sun", "Lei Lin", "Nan Yang", "Zhenzhou Ji", "Xiaolong Wang." ],
      "venue" : "International Conference on Neural Information Processing, pages 279–286. Springer.",
      "citeRegEx" : "Sun et al\\.,? 2014",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2014
    }, {
      "title" : "Ernie: Enhanced representation through knowledge integration",
      "author" : [ "Yu Sun", "Shuohuan Wang", "Yukun Li", "Shikun Feng", "Xuyi Chen", "Han Zhang", "Xin Tian", "Danxiang Zhu", "Hao Tian", "Hua Wu." ],
      "venue" : "arXiv preprint arXiv:1904.09223.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Ernie 2.0: A continual pre-training framework for language understanding",
      "author" : [ "Yu Sun", "Shuohuan Wang", "Yukun Li", "Shikun Feng", "Hao Tian", "Hua Wu", "Haifeng Wang" ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Sun et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Chinese embedding via stroke and glyph information: A dual-channel view",
      "author" : [ "Hanqing Tao", "Shiwei Tong", "Tong Xu", "Qi Liu", "Enhong Chen" ],
      "venue" : null,
      "citeRegEx" : "Tao et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Tao et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Google’s neural machine translation system: Bridging the gap between human and machine translation",
      "author" : [ "Jeffrey Dean" ],
      "venue" : null,
      "citeRegEx" : "Dean.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dean.",
      "year" : 2016
    }, {
      "title" : "CLUE: A Chinese language understanding evaluation benchmark",
      "author" : [ "Zuoyu Tian", "Yiwen Zhang", "He Zhou", "Shaoweihua Liu", "Zhe Zhao", "Qipeng Zhao", "Cong Yue", "Xinrui Zhang", "Zhengliang Yang", "Kyle Richardson", "Zhenzhong Lan." ],
      "venue" : "Proceed-",
      "citeRegEx" : "Tian et al\\.,? 2020",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2020
    }, {
      "title" : "Fgn: Fusion glyph network for chinese named entity recognition",
      "author" : [ "Zhenyu Xuan", "Rui Bao", "Shengyi Jiang" ],
      "venue" : null,
      "citeRegEx" : "Xuan et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Xuan et al\\.",
      "year" : 2020
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems, pages 5753–5763.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi-granularity chinese word embedding",
      "author" : [ "Rongchao Yin", "Quan Wang", "Peng Li", "Rui Li", "Bin Wang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 981–986.",
      "citeRegEx" : "Yin et al\\.,? 2016",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2016
    }, {
      "title" : "Which encoding is the best for text classification in chinese, english, japanese and korean? arXiv preprint arXiv:1708.02657",
      "author" : [ "Xiang Zhang", "Yann LeCun" ],
      "venue" : null,
      "citeRegEx" : "Zhang and LeCun.,? \\Q2017\\E",
      "shortCiteRegEx" : "Zhang and LeCun.",
      "year" : 2017
    }, {
      "title" : "Cpm: A large-scale generative chinese pre-trained language model",
      "author" : [ "Huang", "Wentao Han", "Jie Tang", "Juanzi Li", "Xiaoyan Zhu", "Maosong Sun" ],
      "venue" : null,
      "citeRegEx" : "Huang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Incorporating bert into neural machine translation",
      "author" : [ "Jinhua Zhu", "Yingce Xia", "Lijun Wu", "Di He", "Tao Qin", "Wengang Zhou", "Houqiang Li", "Tieyan Liu." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : "Large-scale pretrained models have become a fundamental backbone for various natural language processing tasks such as natural language understanding (Liu et al., 2019b), text classification (Reimers and Gurevych, 2019; Chai et al.",
      "startOffset" : 150,
      "endOffset" : 169
    }, {
      "referenceID" : 34,
      "context" : ", 2019b), text classification (Reimers and Gurevych, 2019; Chai et al., 2020) and question answering (Clark and Gardner, 2017; Lewis et al.",
      "startOffset" : 30,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : ", 2019b), text classification (Reimers and Gurevych, 2019; Chai et al., 2020) and question answering (Clark and Gardner, 2017; Lewis et al.",
      "startOffset" : 30,
      "endOffset" : 77
    }, {
      "referenceID" : 41,
      "context" : "This idea has motivated a variety of of work on learning and incorporating Chinese glyph information into neural models (Sun et al., 2014; Shi et al., 2015; Liu et al., 2017; Dai and Cai, 2017; Su and Lee, 2017; Meng et al., 2019), but not yet large-scale pretraining.",
      "startOffset" : 120,
      "endOffset" : 230
    }, {
      "referenceID" : 37,
      "context" : "This idea has motivated a variety of of work on learning and incorporating Chinese glyph information into neural models (Sun et al., 2014; Shi et al., 2015; Liu et al., 2017; Dai and Cai, 2017; Su and Lee, 2017; Meng et al., 2019), but not yet large-scale pretraining.",
      "startOffset" : 120,
      "endOffset" : 230
    }, {
      "referenceID" : 26,
      "context" : "This idea has motivated a variety of of work on learning and incorporating Chinese glyph information into neural models (Sun et al., 2014; Shi et al., 2015; Liu et al., 2017; Dai and Cai, 2017; Su and Lee, 2017; Meng et al., 2019), but not yet large-scale pretraining.",
      "startOffset" : 120,
      "endOffset" : 230
    }, {
      "referenceID" : 11,
      "context" : "This idea has motivated a variety of of work on learning and incorporating Chinese glyph information into neural models (Sun et al., 2014; Shi et al., 2015; Liu et al., 2017; Dai and Cai, 2017; Su and Lee, 2017; Meng et al., 2019), but not yet large-scale pretraining.",
      "startOffset" : 120,
      "endOffset" : 230
    }, {
      "referenceID" : 40,
      "context" : "This idea has motivated a variety of of work on learning and incorporating Chinese glyph information into neural models (Sun et al., 2014; Shi et al., 2015; Liu et al., 2017; Dai and Cai, 2017; Su and Lee, 2017; Meng et al., 2019), but not yet large-scale pretraining.",
      "startOffset" : 120,
      "endOffset" : 230
    }, {
      "referenceID" : 29,
      "context" : "This idea has motivated a variety of of work on learning and incorporating Chinese glyph information into neural models (Sun et al., 2014; Shi et al., 2015; Liu et al., 2017; Dai and Cai, 2017; Su and Lee, 2017; Meng et al., 2019), but not yet large-scale pretraining.",
      "startOffset" : 120,
      "endOffset" : 230
    }, {
      "referenceID" : 12,
      "context" : "BERT (Devlin et al., 2018), which is built on top of the Transformer architecture (Vaswani et al.",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 45,
      "context" : ", 2018), which is built on top of the Transformer architecture (Vaswani et al., 2017), is pretrained on large-scale unlabeled text corpus in the manner of Masked Language Model (MLM) and Next Sentence Prediction (NSP).",
      "startOffset" : 63,
      "endOffset" : 85
    }, {
      "referenceID" : 49,
      "context" : "Following this trend, considerable progress has been made by modifying the masking strategy (Yang et al., 2019; Joshi et al., 2020), pretraining tasks (Liu et al.",
      "startOffset" : 92,
      "endOffset" : 131
    }, {
      "referenceID" : 15,
      "context" : "Following this trend, considerable progress has been made by modifying the masking strategy (Yang et al., 2019; Joshi et al., 2020), pretraining tasks (Liu et al.",
      "startOffset" : 92,
      "endOffset" : 131
    }, {
      "referenceID" : 27,
      "context" : ", 2020), pretraining tasks (Liu et al., 2019a; Clark et al., 2020) or model backbones (Lan et al.",
      "startOffset" : 27,
      "endOffset" : 66
    }, {
      "referenceID" : 6,
      "context" : ", 2020), pretraining tasks (Liu et al., 2019a; Clark et al., 2020) or model backbones (Lan et al.",
      "startOffset" : 27,
      "endOffset" : 66
    }, {
      "referenceID" : 28,
      "context" : "Specifically, RoBERTa (Liu et al., 2019b) proposed to remove the NSP pretraining task since it has been proved to offer no benefits for improving downstream performances.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 33,
      "context" : "The GPT series (Radford et al., 2019; Brown et al., 2020) and other BERT variants (Lewis et al.",
      "startOffset" : 15,
      "endOffset" : 57
    }, {
      "referenceID" : 20,
      "context" : ", 2020) and other BERT variants (Lewis et al., 2019; Song et al., 2019; Lample and Conneau, 2019; Dong et al., 2019; Bao et al., 2020; Zhu et al., 2020) adapted the paradigm of large-scale unsupervised pretraining to text generation tasks such as machine translation, text summarization and dialog generation, so that generative models can enjoy the benefit of large-scale pretraining.",
      "startOffset" : 32,
      "endOffset" : 152
    }, {
      "referenceID" : 39,
      "context" : ", 2020) and other BERT variants (Lewis et al., 2019; Song et al., 2019; Lample and Conneau, 2019; Dong et al., 2019; Bao et al., 2020; Zhu et al., 2020) adapted the paradigm of large-scale unsupervised pretraining to text generation tasks such as machine translation, text summarization and dialog generation, so that generative models can enjoy the benefit of large-scale pretraining.",
      "startOffset" : 32,
      "endOffset" : 152
    }, {
      "referenceID" : 17,
      "context" : ", 2020) and other BERT variants (Lewis et al., 2019; Song et al., 2019; Lample and Conneau, 2019; Dong et al., 2019; Bao et al., 2020; Zhu et al., 2020) adapted the paradigm of large-scale unsupervised pretraining to text generation tasks such as machine translation, text summarization and dialog generation, so that generative models can enjoy the benefit of large-scale pretraining.",
      "startOffset" : 32,
      "endOffset" : 152
    }, {
      "referenceID" : 13,
      "context" : ", 2020) and other BERT variants (Lewis et al., 2019; Song et al., 2019; Lample and Conneau, 2019; Dong et al., 2019; Bao et al., 2020; Zhu et al., 2020) adapted the paradigm of large-scale unsupervised pretraining to text generation tasks such as machine translation, text summarization and dialog generation, so that generative models can enjoy the benefit of large-scale pretraining.",
      "startOffset" : 32,
      "endOffset" : 152
    }, {
      "referenceID" : 0,
      "context" : ", 2020) and other BERT variants (Lewis et al., 2019; Song et al., 2019; Lample and Conneau, 2019; Dong et al., 2019; Bao et al., 2020; Zhu et al., 2020) adapted the paradigm of large-scale unsupervised pretraining to text generation tasks such as machine translation, text summarization and dialog generation, so that generative models can enjoy the benefit of large-scale pretraining.",
      "startOffset" : 32,
      "endOffset" : 152
    }, {
      "referenceID" : 53,
      "context" : ", 2020) and other BERT variants (Lewis et al., 2019; Song et al., 2019; Lample and Conneau, 2019; Dong et al., 2019; Bao et al., 2020; Zhu et al., 2020) adapted the paradigm of large-scale unsupervised pretraining to text generation tasks such as machine translation, text summarization and dialog generation, so that generative models can enjoy the benefit of large-scale pretraining.",
      "startOffset" : 32,
      "endOffset" : 152
    }, {
      "referenceID" : 35,
      "context" : "(2019b) proposed to use Chinese character as the basic unit instead of word or subword that is used in English (Wu et al., 2016; Sennrich et al., 2016).",
      "startOffset" : 111,
      "endOffset" : 151
    }, {
      "referenceID" : 9,
      "context" : "We use both Whole Word Masking (WWM) (Cui et al., 2019a) and Char Masking (CM) for pretraining (See Section 4.",
      "startOffset" : 37,
      "endOffset" : 56
    }, {
      "referenceID" : 12,
      "context" : "The output is the corresponding contextualized representation for each input Chinese character (Devlin et al., 2018).",
      "startOffset" : 95,
      "endOffset" : 116
    }, {
      "referenceID" : 2,
      "context" : "We use the LTP toolkit5 (Che et al., 2010) to identify the boundary of Chinese words for whole word masking.",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 28,
      "context" : "We also use the dynamic masking strategy to avoid duplicate training instances (Liu et al., 2019b).",
      "startOffset" : 79,
      "endOffset" : 98
    }, {
      "referenceID" : 42,
      "context" : "Table 1: Comparison of data statistics between ERNIE (Sun et al., 2019), BERT-wwm (Cui et al.",
      "startOffset" : 53,
      "endOffset" : 71
    }, {
      "referenceID" : 9,
      "context" : ", 2019), BERT-wwm (Cui et al., 2019a), MacBERT (Cui et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 8,
      "context" : ", 2019a), MacBERT (Cui et al., 2020) and our proposed ChineseBERT.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 12,
      "context" : "After pretraining, the model can be directly used to be finetuned on downstream tasks in the same way as BERT (Devlin et al., 2018).",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 9,
      "context" : ", 2019, 2020), BERTwwm (Cui et al., 2019a) and MacBERT (Cui et al.",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 10,
      "context" : "We use two datasets for this task: CMRC 2018 (Cui et al., 2019b) and CJRC (Duan et al.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 7,
      "context" : "We use the Cross-lingual Natural Language Inference (XNLI) dataset (Conneau et al., 2018) for evaluation.",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 22,
      "context" : "News (Li and Sun, 2007) and ChnSentiCorp8 for this task.",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 3,
      "context" : ", 2018) and BQ Corpus (Chen et al., 2018) datasets for evaluation.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 4,
      "context" : "For NER tasks (Chiu and Nichols, 2016; Lample et al., 2016; Li et al., 2019a), the model is asked to identify named entities within a piece of text, which is formalized as a sequence labeling task.",
      "startOffset" : 14,
      "endOffset" : 77
    }, {
      "referenceID" : 16,
      "context" : "For NER tasks (Chiu and Nichols, 2016; Lample et al., 2016; Li et al., 2019a), the model is asked to identify named entities within a piece of text, which is formalized as a sequence labeling task.",
      "startOffset" : 14,
      "endOffset" : 77
    }, {
      "referenceID" : 23,
      "context" : "For NER tasks (Chiu and Nichols, 2016; Lample et al., 2016; Li et al., 2019a), the model is asked to identify named entities within a piece of text, which is formalized as a sequence labeling task.",
      "startOffset" : 14,
      "endOffset" : 77
    } ],
    "year" : 2021,
    "abstractText" : "Recent pretraining models in Chinese neglect two important aspects specific to the Chinese language: glyph and pinyin, which carry significant syntax and semantic information for language understanding. In this work, we propose ChineseBERT, which incorporates both the glyph and pinyin information of Chinese characters into language model pretraining. The glyph embedding is obtained based on different fonts of a Chinese character, being able to capture character semantics from the visual features, and the pinyin embedding characterizes the pronunciation of Chinese characters, which handles the highly prevalent heteronym phenomenon in Chinese (the same character has different pronunciations with different meanings). Pretrained on large-scale unlabeled Chinese corpus, the proposed ChineseBERT model yields significant performance boost over baseline models with fewer training steps. The proposed model achieves new SOTA performances on a wide range of Chinese NLP tasks,including machine reading comprehension, natural language inference, text classification, sentence pair matching, and competitive performances in named entity recognition and word segmentation.1",
    "creator" : "LaTeX with hyperref"
  }
}