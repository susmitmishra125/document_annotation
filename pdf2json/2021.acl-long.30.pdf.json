{
  "name" : "2021.acl-long.30.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "PASS: Perturb-and-Select Summarizer for Product Reviews",
    "authors" : [ "Nadav Oved", "Ran Levy" ],
    "emails" : [ "nadavo@campus.technion.ac.il", "ranlevy@amazon.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 351–365\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n351"
    }, {
      "heading" : "1 Introduction",
      "text" : "Online shopping has become a popular form of purchasing goods even before the most recent acceleration due to the COVID-19 pandemic. As ecommerce websites strive to make the shopping process more useful and enjoyable for customers, many interesting challenges arise. One challenge deals with how to surface opinions from product\n∗Completed during an internship at Amazon. 1Summaries generated by PASS are available at: https:\n//registry.opendata.aws/\nreviews in a concise yet reliable fashion. The research community has addressed this challenge early on, starting from the work of (Hu and Liu, 2004) which defined the task of mining and summarizing customer reviews. More recent advancements have relied on modern deep learning models trained on large collections of unannotated customer reviews (Brazinskas et al., 2020b,a).\nOur first observation relates to the summaries generated by CopyCat (Brazinskas et al., 2020b) and FewSum (Brazinskas et al., 2020a), two of these SOTA systems, which tend to mix generic statements such as “Would recommend this product to anyone” along with more informative content such as “The sound quality is good” (see Table 6 in Appendix B for examples of such generated summaries). Due to the emphasis of summarization systems on conciseness, we maintain that generic content should be used sparingly. Additionally, even if the content is not extremely generic, customers may perceive summaries as less useful if they tend to repeat themselves across products. In order to estimate the similarity between summaries generated for different products, we devise the Set-Pairwise-ROUGE metric (henceforth denoted as SPR), that computes the average ROUGE (Lin, 2004b) scores of summaries for two different products, across all product pairs. Using this metric we show that human written reference summaries are indeed far more diverse than their system generated counterparts, i.e. the SPR of reference summaries is significantly lower. We henceforth denote the notion of cross product diversity of summaries as CPDiversity.\nLarge pre-trained Transformer-based (Vaswani et al., 2017) models such as OpenAI’s GPT-3 (Brown et al., 2020), Google’s T5 (Raffel et al., 2020), PEGASUS (Zhang et al., 2020a), and Facebook’s BART (Lewis et al., 2020) have made com-\npelling advancements on a host of NLG tasks, including abstractive text summarization. In this work we wish to leverage such models for product reviews summarization, aiming to generally improve the quality of generated summaries, and specifically in terms of their diversity across different products. While we aim to generate humanlike texts, care has to be taken with respect to their correctness. Indeed, concerns have been raised regarding the factual consistency of abstractive summaries, i.e., whether the facts conveyed in the summary agree with the source text (Cao et al., 2018; Kryscinski et al., 2019; Maynez et al., 2020).\nOur second observation relates to this issue of factual consistency in the context of product reviews summarization. Our task not only faces the risk of models hallucinating incorrect information, as in traditional abstractive text summarization, but also the risk of generating self-contradicting summaries which are not caused by model hallucinations. The latter can occur when the source documents contradict one another. This situation is quite likely because reviews may disagree on some product aspects or even disagree entirely. For example, review A states a machine is “easy to operate” vs. review B which states it “requires trial and error” (see more examples in Table 7 in Appendix B). In this unique setup, factual consistency is undefined and instead we wish to measure a different characteristic: the self-consistency of the summary. To the best of our knowledge this issue has not been analyzed in the past and in some sense it renders the task ill-defined because it’s not clear whether the summary is supposed to convey a range of possibly contradicting opinions about the product or the majority opinion. From here on, we shall assume that a summary has to convey the majority opinion of the reviews and do so in a selfconsistent manner.\nOur proposed method starts by fine-tuning a strong pre-trained language model for product reviews summarization in a few-shot setup. We then employ an input perturbation method that drops k reviews out of the input and concatenates the remaining reviews in random order. This process, denoted as LkO, short for leave k out, produces notable variation between candidate summaries, which increases the model’s output diversity.2 Once we have produced a set of candidate\n2Diversity here is between candidate summaries for the\nsummaries, we essentially cast our original summary generation problem as a ranking problem. This approach gives us the choice over what kind of summary we are interested in as the final output, i.e. choosing our ranking criteria. As mentioned above, our main concern in this work is producing self-consistent summaries. Instead of basing our ranking solely on this criterion, we train a more general coherence summary ranker using human annotated coherence scores (Fabbri et al., 2021). Finally, for each product, we select the top ranked summary as the system’s output.\nWe compare our method against strong baselines, comprised of systems introduced in previous work on multi-document opinion summarization, and a T5 language model fine-tuned for abstractive text summarization. We evaluate each over 3 dimensions, of which relevance and coherence are commonly used in summarization (Dang, 2005), and our newly introduced metric for CP-Diversity. We demonstrate that our method produces high quality summaries which are more informative, diverse and coherent.\nIn summary, the main contributions of this work are: (1) highlight two shortcomings of existing product reviews summarizers, namely low CPDiversity and self-inconsistency, and propose a dedicated metric for the former. (2) Propose a method that leverages strong pre-trained models that improve the CP-Diversity while significantly reducing the risk of self-inconsistencies."
    }, {
      "heading" : "2 Related Work",
      "text" : "Product Review Summarization. Product review summarization is a form of multi-document summarization in which a set of product reviews for a single product serves as the document cluster to be summarized. A common approach for product review summarization, which centers the summary around a set of extracted aspects and their respective sentiment, is termed aspect-based summarization (Hu and Liu, 2004; Kansal and Toshniwal, 2014; Wu et al., 2016; Angelidis and Lapata, 2018; Coavoux et al., 2019).\nAs in traditional summarization, there are two inherently different requirements for the task, a simplified one, in which the goal is to provide an extractive output, i.e., a list of sentences extracted from the review set, or a more advanced one, in which the goal is to provide an abstrac-\nsame product, not to be confused with CP-Diversity.\ntive output, i.e., generated content not restricted to use the same wording of the source set. Extractive summarization include earlier works such as (Carenini et al., 2006; Lerman et al., 2009; Xiong and Litman, 2014). More recently, (Tan et al., 2017) suggested a novel generative topic aspect sentiment model, while (Angelidis et al., 2021) suggested a novel system able to extract both general and aspect-specific summaries. As for abstractive summarization, recent advances on pre-training neural networks were explored in the context of product reviews in unsupervised and few-shot learning schemes which led to promising results (Chu and Liu, 2019; Brazinskas et al., 2020b,a; Suhara et al., 2020; Amplayo et al., 2021).\nEvaluating Summarization Systems. Evaluation of summarization systems is usually performed utilizing a mix of automatic metrics and human ratings. Among the automated metrics, probably the most well-known is the ROUGE family of scores (Lin, 2004b) that measures ngram overlap between generated summaries and corresponding reference summaries. Many other metrics that aim to quantify how well generated summaries align with reference summaries have been proposed, such as BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), ROUGE-WE (Ng and Abrecht, 2015) and BertScore (Zhang et al., 2020b) to name a few. Unfortunately, such metrics alone do not tell the whole story and recently several works observed that a new requirement is necessary in order to ensure that facts from the summary agree with the source document (Cao et al., 2018; Kryscinski et al., 2019; Maynez et al., 2020). This requirement is usually known as factual consistency. As for human ratings, those are usually obtained across several dimensions of summary quality. The DUC 2005 task (Dang, 2005) suggested the following 5 dimensions: Grammaticality, Non-redundancy, Referential clarity, Focus and Structure, and Coherence.\nIn the context of product reviews summarization (Brazinskas et al., 2020a) use the standard ROUGE-1/2/L metrics as well human comparative judgments on 5 dimensions: Fluency, Coherence, Non-Redundancy, Informativeness and Sentiment. To the best of our knowledge the issues of selfconsistency and diversity across products were not directly analyzed before."
    }, {
      "heading" : "3 Perturb-and-Select Summarizer",
      "text" : "In this section, we propose a system that employs a large pre-trained Transformer-based model (T5) in a few-shot fine-tuning scheme for multiple reviews abstractive summarization. We aim to leverage the inherent diversity between reviews for a given product to our advantage, by applying systematic perturbations to the model’s input during inference. This allows our fine-tuned model to generate multiple different candidate summaries per product, exhibiting variability both in the content being surfaced as well as in the phrasing of said content. We develop a ranking mechanism for selecting the best candidate summary according to desired criteria, which in our case is coherence. We provide an end-to-end diagram of the PASS Summarizer’s components in Figure 1."
    }, {
      "heading" : "3.1 Fine-tuning T5 for Summary Generation",
      "text" : "PASS relies on a pre-trained T5 language model, which we fine-tuned on a small publicly available dataset for product reviews summarization (Brazinskas et al., 2020a). We follow a similar fine-tuning scheme for abstractive text summarization to the one presented in (Raffel et al., 2020) with the exception that we concatenate the multiple reviews into a single input text as a preprocessing step. As the dataset contains multiple reference summaries per product, we repeat our training process for each reference summary using the same (concatenated) input text."
    }, {
      "heading" : "3.2 Candidate Summary Generation",
      "text" : "In light of the natural diversity existing between product reviews, we explore a modeling approach which allows for such diversity to emerge in our summarizer’s output as well. We do this by manipulating the model’s input, sampling which reviews to use each time, in a way that allows for increasing the relative prevalence of certain reviews over others. We also re-shuffle the reviews before concatenation to ensure the model is not affected by their internal order. Note that prior attempts have been made to directly manipulate the content within the reviews (Amplayo and Lapata, 2020) a path that we do not explore here. Our intervention method guarantees that each review’s correctness, integrity and meaning are preserved. Since it only affects the subset of reviews being used and their order of concatenation, this increases the potential for diversity (per product and across products)\nemerging from the input’s content, without compromising its linguistic quality.\nLkO Input Perturbation Method. Given a set of d reviews R = {r1, ..., rd} for a product p, our perturbation method iterates over A(R) the set of all possible subests of size d − k in R, A(R) = { S ∣∣S ⊂ R, |S| = d− k, 1 ≤ k < d}. Given a subset S ∈ A(R) we concatenate its reviews in random order, and feed the concatenated text into our fine-tuned T5 summarizer, which generates a candidate summary c. We repeat this step for all S ∈ A(R), resulting in a set of generated candidate summaries which we denote as C = {c1, ..., cm},m = ( d k ) . This process, denoted as LkO, short for leave-k-out, produces notable variation between candidate summaries (see Table 8 in Appendix B for examples), and allows for different content and aspects to emerge in the summaries, which were less likely to have surfaced otherwise. We found that this perturbation approach produces higher variation across candidate summaries when applying it on the model’s input only during the inference stage, not during training. Our method produces multiple perturbed versions of a given input while its references remain the same. If applied during training, this might encourage the model to fit a larger range of input features to a smaller set of outputs. We are interested in the opposite effect - we would like to encourage higher output variation as a function of input diversity.\nNote that when dealing with large review sets, achieving diversity does not require iterating over all subsets in A(R). For such scenarios, we recommend constructing a fixed number (m) of randomly sampled review subsets, so long as m is\nsufficiently large. In our experiments we employ the full LkO input perturbation method, since standard datasets focus on relatively small review sets.3\nAn alternative method for increasing novelty and variability in the output of a generative language model, is to directly intervene in its decoding algorithm, e.g., Beam Search (Vijayakumar et al., 2016; Cibils et al., 2018). Note that this will not have the same effect as our proposed approach. First, since beam search is a decoding algorithm, it only has access to the underlying language model, and is completely separated from the model’s input. Second, beam search’s mechanism is fixed to make local word-by-word decisions, before the complete summary is revealed. Finally, our approach guarantees that given a set of input texts, at least one candidate output will not be influenced at all by a specific input text (or more if k > 1). For example, if a set of 4 reviews contains 3 reviews discussing price, and 1 review discussing quality, our method guarantees that at least 1 candidate summary will be generated solely based on the first three (discussing price). Furthermore, our method increases the probability for a summary to mention both price and quality, when a review discussing price is left out."
    }, {
      "heading" : "3.3 Candidate Summary Ranking",
      "text" : "Once a set of candidate summaries are generated per product, we have essentially cast our summary generation problem as a summary ranking problem. This allows us to retrieve a summary, which ranks best out of a diverse set of candidates, according to desired, interpretable criteria.\n3A few recent works attempt to explicitly address this issue (Shapira and Levy, 2020; Angelidis et al., 2021).\nAs mentioned in Section 1, our main concern is producing CP-diverse yet self-consistent and coherent summaries. Since our input perturbation method generates multiple candidate summaries, we are now left with the task of ranking this set by coherence. We would like the ranking process to filter out self-contradicting, incoherent or inconsistent candidates (by assigning low rank) and to promote well-formed, coherent candidates to the top of the list. To achieve this, we train a classifier that receives two summaries as input and decides whether the first summary is more coherent than the second or the opposite. The classifier can also decide that both summaries are equally coherent. Using such a classifier, we can obtain a partial ranking of the reviews by running all pairwise comparisons and count the number of times each summary was better than the summary it was paired with.\nPairwise Summary Classifier. We train a model to classify a pair of summaries for coherence, by fine-tuning a pre-trained T5 model for pairwise text classification. Given a pair of summaries, the model is required to classify them as either: summary A is more coherent, summary B is more coherent, or A and B are equivalent in terms of coherence. A pair of summaries can often be considered equivalent when judging them according to specific criteria, stemming from the natural fact that often more than one summary can be considered correct or good. Indeed it has been shown that several reference summaries are needed for reliable evaluation showing that there is more than one truth (Lin, 2004a). Since this model is used as a comparator for ranking candidate summaries, we are especially sensitive to specific types of classification errors. If the model mistakenly classifies a summary to be more coherent than the other while the opposite is true, we consider this a critical classification error. This type of error could be detrimental to the validity of the ranking process, therefore we aim to minimize its rate. While other types of errors also reduce the classifier’s accuracy, we consider a mistake where the model classifies two summaries to be equivalent when in truth one is more coherent than the other, as less harmful for ranking purposes.\nRanking Method. Our proposed ranking method iterates over all possible pairs of candidate summaries for a given product, and counts\nhow many times each candidate was classified by the coherence pairwise classifier (our primary comparator), as more coherent than its counterpart. As a tie-breaking, secondary comparator, we train an additional pairwise summary classifier, to classify which candidate is more fluent, out of a pair of given candidates. We select the top ranked candidate as the final output summary for each product."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "4.1 Data",
      "text" : "We utilize a recent publicly available Amazon product reviews summarization dataset (Brazinskas et al., 2020a) for fine-tuning the T5 model which underlines the PASS system and for evaluating the LkO input perturbation method, both in isolation and as part of the end-to-end PASS system. The dataset contains product reviews and reference summaries for 60 products on Amazon. Each product has 8 reviews and 3 reference summaries written by crowd source workers. We follow the dataset splits to the training, development and test sets provided by the authors of the dataset. While we mainly focus on product reviews summarization, we include the Yelp business reviews summarization dataset (also from (Brazinskas et al., 2020a)) in our end-to-end evaluation for the sake of completeness. The Yelp dataset contains business reviews and reference summaries for 100 businesses.\nFor training and evaluating the pairwise coherence classifier, we utilize a public dataset of human annotated summaries (Fabbri et al., 2021), generated by 16 modern text summarization models for 100 news articles (1600 examples in total) from the CNN/DailyMail dataset (Hermann et al., 2015). Each summary was rated (on a scale of 1 to 5) across 4 dimensions: coherence, consistency, fluency and relevance, by 5 independent crowd source workers and 3 independent experts (8 annotations in total). We chose to use the experts’ annotations only, as they are considered to be more accurate and reliable for coherence and fluency (Fabbri et al., 2021). We construct a pairwise version of this dataset, by creating summary pairs from all 16 model outputs for each of the 100 news stories, along with their annotation scores for each metric respectively. We split the dataset according to news stories, by randomly sampling 20 stories for the test set, 16 stories for the develop-\nment set and the rest are used for the training set. Given a pair of summaries (a, b), their respective average expert rating, (ra, rb) and a threshold parameter , we define the label for that pair as:\nlabel(a, b) =  A, if ra − rb ≥ B, if rb − ra ≥ E, otherwise\nwhere E denotes the case where both summaries are equivalent, A denotes that summary a is better than b and B denotes the opposite. To ensure that our training data is invariant to a pair’s internal order, we create examples for all (a, b) and (b, a) pairs in the training set."
    }, {
      "heading" : "4.2 Experimental Details",
      "text" : "Fine-tuning T5 for Summary Generation. We fine-tune a T5-Base model (220M parameters (Raffel et al., 2020)) for abstractive text summarization as described in 3.1 on the training set, and tune its hyperparameters on the development set. We train for maximum 20 epochs while employing a standard early stopping mechanism (Falcon, 2019) based on the development set’s average loss per epoch. We fine-tune a separate model for the Amazon and Yelp datasets. Hyperparameters and further details can be found in Appendix A.\nLkO Input Perturbation. We experiment with the LkO method described in Section 3.2 with k ∈ {1, 2, 3, 4, 5} on the development set. For the endto-end system we choose k = 2 aiming to obtain high output diversity while limiting computation complexity, and avoiding the risk of dropping a majority of the reviews (k > 4) each time. We provide evaluation details in 5.1.\nPairwise Summary Classifier. We train two T5-Base models to classify which summary is better, one in terms of coherence, to be used as our ranking method’s primary comparator, and one in terms of fluency to break ties. We experimented with different values for ∈ {0.25, 0.5, 0.75, 1.0}, and chose = 0.5 for the coherence classifier and = 0.25 for the fluency classifier. The choice of was based on dataset statistics per metric and evaluation of each model’s performance on the development set.\nBaselines. We compare the PASS system to four baselines:\nCOPYCAT (Brazinskas et al., 2020b) is an unsupervised reviews summarizer that is trained to\ngenerate a review given other reviews for the same product. The authors suggest a novelty mechanism that controls the extent to which the summary deviates from the inputs.\nFEWSUM (Brazinskas et al., 2020a) is a fewshot reviews summarizer that builds upon the ideas of CopyCat but also conditions the model on certain linguistic properties such as writing style.\nT5 is the pre-trained T5-base language model which was not fine-tuned. We do not report results for this model, as it consistently performed worst.\nT5-FT is the fine-tuned T5-base model described above.\nWe do not report results for MEANSUM (Chu and Liu, 2019) since it was consistently outperformed by FEWSUM (Brazinskas et al., 2020a)."
    }, {
      "heading" : "5 Evaluation",
      "text" : ""
    }, {
      "heading" : "5.1 Candidate Summary Generation",
      "text" : "Recall that our main objective for generating candidate summaries is to encourage output diversity. Hence, we would like to verify that our perturbation method, LkO, produces sufficiently diverse candidates for a given product. In order to measure textual diversity between candidate summaries for a given product, we need to devise a diversity metric. We propose the SPR metric (shorthand for Set-Pairwise-ROUGE) which measures the opposite of diversity, i.e., the average lexical similarity across pairs of summaries from a given set. We base SPR on ROUGE F1 scores for any ngram level, therefore SPR-1 relies on ROUGE-1 F1 scores and so on.\nSPR Formal Definition. For a given set of summaries S = {s1, ..., sn}, we define the set of all pairs from S as P (S) = { {si, sj} ∣∣si ∈ S, sj ∈ S, i 6= j } . We then define the set-pairwise-rouge (SPR) metric as:\nSPR(S) = 1 |P (S)| · ∑\n{si,sj}∈P (S)\nROUGE(si, sj)\nNote that SPR is a general metric of diversity, applicable to an arbitrary set of summaries. Therefore, it can be applied to measure both IPDiversity (in-product diversity, as we do here) and CP-Diversity (cross-product diversity, as we do in Section 5.3). For clarity, we shall denote IP-SPR when measuring IP-Diversity and CP-SPR when measuring CP-Diversity with SPR.\nFigure 2 depicts a box plot of the IP-SPR-2 scores for k ranging from 1 to 5. We observe\nthe biggest drop in similarity (increase in diversity) between k = 1 and k = 2. While we aim to increase diversity, we are also mindful of the increase in runtime as k grows. Additionally, we would like to avoid sampling out a majority of reviews (k > 4), since the risk of generating a summary with minority view or low informativeness also increases with k. Indeed, as shown in Figure 3, which depicts a similar box plot but this time of the ROUGE-2 scores against the reference summaries, the variance increases with k and the worst-case ROUGE-2 score decreases with k.\nWhile diversity is certainly not the only aspect for evaluating generated summaries, we explore other dimensions in the following sections."
    }, {
      "heading" : "5.2 Candidate Summary Ranking",
      "text" : "The pairwise summary classifiers can be evaluated directly using human scores from (Fabbri et al., 2021) after adapting them to our ternary classification task. Figure 4 depicts the confusion matrix for\nour coherence classifier. We observe that the estimated probability of a critical error (choosing A over B or B over A) is very low, 0.05, while at the same time the overall accuracy of 0.61 is reasonably high compared to 0.33 and 0.36 achieved by the random and majority (always predicts that A and B are equally coherent) baselines respectively. Applying the classifier to a set of 28 candidates per product, yields a single top ranking candidate for 70% of products in the Amazon test set.\nTo further break ties, we utilize the fluency classifier as a secondary comparator. See Figure 10 in Appendix C for a similar confusion matrix for the fluency classifier. Again, the probability for a critical error is very low, 0.0125, while the overall accuracy is 0.67. After applying fluency as a tie breaker, we find that all products in the Amazon test set have a unique top ranking summary.\nThe training data for both classifiers comes from a domain (News Articles) which is different from our main dataset’s domain (Product Re-\nviews). We hypothesize that coherence and fluency are linguistic properties that are not heavily tied with the domain, since they relate to a summary’s overall collective and individual sentence quality (Dang, 2005). Indeed, our results show (see Table 2) that PASS benefited from this data despite the risk of a possible domain shift.4"
    }, {
      "heading" : "5.3 End-to-End System",
      "text" : "We evaluate our end-to-end system across 3 dimensions. The first, informativeness, is traditionally evaluated using the ROUGE-1/2/L F1 measures (Lin, 2004b) and we follow suit. The second dimension, which subsumes the self-consistency issue, is coherence. To this end, we conducted a crowdsourced human evaluation task, which compares between the generated summaries of 4 different summarization systems, including our proposed PASS system. We used Best-Worst Scaling (Louviere and Woodworth, 1991; Louviere et al., 2015; Kiritchenko and Mohammad, 2016, 2017) to compute each system’s score as the difference between the percentage of times it was selected as best, and the percentage of times it was selected as worst (Orme, 2009). This is inline with prior work on product review summarization (Brazinskas et al., 2020b,a). As for our third dimension, recall that we would like our system to generate diverse summaries across different products, a notion that we denoted as CP-Diversity. Lacking an existing metric, we use our previously defined SPR-1/2/L measure on the set of final (top-ranked) summaries across all test set products.\n4While we did not find evidence suggesting a domain shift, it is an aspect we leave for further investigation in future work.\nTable 1 reports results for all 3 dimensions. For the Amazon dataset (top table), we observe that PASS outperforms the baselines in coherence and CP-Diversity while keeping a comparable informativeness to the next best system, T5-FT. The only exception being ROUGE-2 in which T5-FT outperforms PASS which could be explained by the somewhat longer summaries it generates. Interestingly, in CP-Diversity, the performance of PASS is closer to human performance than to CopyCat and FewSum but there’s still room to make the summaries even more diverse. For the sake of completeness and following previous work (Chu and Liu, 2019; Brazinskas et al., 2020b,a) we report results on business reviews from the Yelp dataset in the bottom of Table 1.\nRecall that our key goals were to avoid generating summaries containing crude coherence (CE) and self-consistency (SCE) errors (see Table 3 for examples of such errors). In order to evaluate these directly, both authors independently marked each of the summaries generated by FewSum, T5FT and PASS for the Amazon test set as having a crude error or not, for both types of errors. Table 2 reports the ratios of crude errors per system, considering cases where at least one annotator (I) and both annotators (II) marked as crude. We measured the level of agreement between the two annotators by calculating Cohen’s Kappa coefficients (Cohen, 1960) for each annotation task, which resulted in κCE = 0.571 and κSCE = 0.779.\nFinally, for a qualitative impression we provide in Table 4 an example of the systems’ outputs for a product from the Amazon test set."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work we highlight two shortcomings of existing product reviews summarization systems, namely low CP-Diversity and self-inconsistency. We propose the SPR metric to quantify cross prod-\nzon test set, which contain crude errors (CE) and selfconsistency errors (SCE).\nuct similarity of summaries and demonstrate that indeed, humans summaries are far more diverse than system generated summaries. To overcome this issue we rely on stronger pre-trained models such as the recent T5 model which significantly improves the CP-Diversity. However, the second problem still remains and even intensifies as without the safety net of generic content, the risk of incoherent or even self-contradicting text is substantial. To this end, we propose the Perturb and Select summarizer (PASS). In the first step, PASS applies systematic perturbations to the input texts in a way that allows the T5 model to generate multiple summary candidates that sufficiently differ from one another. Given such a set of diverse summaries, PASS applies a trained ranker to smartly select a promising candidate in terms of coherence. Finally, we show that the resulting PASS system, outperforms SOTA models in the domain of product reviews in terms of informativeness, CP-Diversity and coherence. When comparing to a fine-tuned T5 model PASS outperforms it in coherence and CP-Diversity, while maintaining comparable performance for informativeness.\nIn future work we plan to investigate the Perturb-and-Select framework in order to promote summaries with a plethora of desired linguistic characteristics, other than coherence. We shall further explore ways of extending this framework to employ other input perturbation methods and experiment with scenarios of larger scale input. In addition, we plan to further investigate our proposed SPR evaluation metric for lexical diversity, by studying its correlation with human judgments. Lastly, we believe our proposed framework and evaluation metric may be applicable to other domains of opinion or news summarization."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank Hila Gonen, Iftah Gamzu and anonymous reviewers, who helped improve the draft with their invaluable comments and insight."
    }, {
      "heading" : "A PASS Implementation Details and Hyperparameters",
      "text" : "All models were implemented with the PyTorch (Paszke et al., 2019) deep learning framework, utilizing the T5 (Raffel et al., 2020) pre-trained model and tokenizer implementations from HuggingFace’s Transformers (Wolf et al., 2020a) library, evaluation metrics from HuggingFace’s Datasets (Wolf et al., 2020b) library and PyTorch Lightning (Falcon, 2019) as a model training framework.\nA.1 T5 Fine-Tuned Summarizer\nWe fine-tune a pre-trained T5-Base model (220M parameters (Raffel et al., 2020)) for product reviews summarization (an abstractive text summarization task) on the training set, employing the Adam optimizer (Kingma and Ba, 2015) with weight decay (Loshchilov and Hutter, 2019). We train the model for a maximum of 20 epochs on a single NVIDIA Tesla V100 GPU, while employing a standard early stopping mechanism (Falcon, 2019) based on the development set’s average loss per epoch. We employ a standard beam search decoding algorithm during inference for generating text. We tune the model’s hyperparameters on the development set, and provide a list of the final model’s tuned hyperparametrs along with the range of values tested during tuning.\nHyperparameters"
    }, {
      "heading" : "T5 Encoder",
      "text" : "• Max input sequence length = 512 tokens • Training batch size = 8, [8, 12, 16] • Evaluation batch size = 12, [8, 12, 16]\nAdam Optimizer • Learning rate = 3e−4, [1e−4, 3e−4, 5e−4] • = 1e− 8, [1e− 8, 3e− 8, 5e− 8] • Weight decay: 0.0 • Number of warmup steps: 0 • Gradient accumulation steps = 2, [1, 2, 4] • Max gradient norm = 1.0"
    }, {
      "heading" : "T5 Decoder",
      "text" : "• Max output sequence length = 128 tokens • Min output sequence length = 16 tokens • Beam size = 2, [2, 3, 4] • Length penalty = 2, [1, 2, 3] • Repetition penalty = 2, [1, 2, 3]\nLkO Input Perturbation (PASS system only) • k = 2, [1, 2, 3, 4, 5]\nA.2 Pairwise Summary Classifiers\nFor each pairwise summary classifier (coherence, fluency), we fine-tune a pre-trained T5-Base model (220M parameters (Raffel et al., 2020)) for abstractive text summarization task on the respective training set employing the Adam optimizer (Kingma and Ba, 2015) with weight decay (Loshchilov and Hutter, 2019). We train for a maximum of 20 epochs on a single NVIDIA Tesla V100 GPU, while employing a standard early stopping mechanism (Falcon, 2019) based on the development set’s average loss per epoch. We employ a standard greedy decoding algorithm during inference for generating the class label. We tune the model’s hyperparameters on the development set, and provide a list of the final model’s tuned hyperparametrs along with the range of values tested during tuning.\nHyperparameters Dataset\n• Coherence scores difference threshold = 0.5, [0.25, 0.5, 0.75, 1.0] • Fluency scores difference threshold = 0.25, [0.25, 0.5, 0.75, 1.0]"
    }, {
      "heading" : "T5 Encoder",
      "text" : "• Max input sequence length = 400 tokens • Training batch size = 16, [8, 12, 16] • Evaluation batch size = 16, [8, 12, 16]\nAdam Optimizer • Learning rate = 1e−4, [1e−4, 3e−4, 5e−4] • = 1e− 8, [1e− 8, 3e− 8, 5e− 8] • Weight decay: 0.0 • Number of warmup steps: 0 • Gradient accumulation steps = 4, [1, 2, 4] • Max gradient norm = 1.0"
    }, {
      "heading" : "T5 Decoder",
      "text" : "• Max output sequence length = 2 tokens • Min output sequence length = 2 tokens"
    }, {
      "heading" : "B Summary Examples",
      "text" : "We provide examples for output summaries generated by the different summarization systems discussed in the main paper. Each example qualitatively highlights a different aspect by which we evaluate the quality of a summary, or identify its shortcomings."
    }, {
      "heading" : "C Evaluation Figures",
      "text" : "We provide figures which extend those appearing in the Evaluation section of the main paper.\nC.1 Candidate Summary Generation\nC.2 Candidate Summary Ranking"
    } ],
    "references" : [ {
      "title" : "Unsupervised opinion summarization with content planning",
      "author" : [ "Reinald Kim Amplayo", "Stefanos Angelidis", "Mirella Lapata." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Amplayo et al\\.,? 2021",
      "shortCiteRegEx" : "Amplayo et al\\.",
      "year" : 2021
    }, {
      "title" : "Unsupervised opinion summarization with noising and denoising",
      "author" : [ "Reinald Kim Amplayo", "Mirella Lapata." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1934–1945, Online. Association for",
      "citeRegEx" : "Amplayo and Lapata.,? 2020",
      "shortCiteRegEx" : "Amplayo and Lapata.",
      "year" : 2020
    }, {
      "title" : "Extractive opinion summarization in quantized transformer spaces",
      "author" : [ "Stefanos Angelidis", "Reinald Kim Amplayo", "Yoshihiko Suhara", "Xiaolan Wang", "Mirella Lapata." ],
      "venue" : "Trans. Assoc. Comput. Linguistics, 9:277–293.",
      "citeRegEx" : "Angelidis et al\\.,? 2021",
      "shortCiteRegEx" : "Angelidis et al\\.",
      "year" : 2021
    }, {
      "title" : "Summarizing opinions: Aspect extraction meets sentiment prediction and they are both weakly supervised",
      "author" : [ "Stefanos Angelidis", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Angelidis and Lapata.,? 2018",
      "shortCiteRegEx" : "Angelidis and Lapata.",
      "year" : 2018
    }, {
      "title" : "Few-shot learning for opinion summarization",
      "author" : [ "Arthur Brazinskas", "Mirella Lapata", "Ivan Titov." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020,",
      "citeRegEx" : "Brazinskas et al\\.,? 2020a",
      "shortCiteRegEx" : "Brazinskas et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised opinion summarization as copycat-review generation",
      "author" : [ "Arthur Brazinskas", "Mirella Lapata", "Ivan Titov." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10,",
      "citeRegEx" : "Brazinskas et al\\.,? 2020b",
      "shortCiteRegEx" : "Brazinskas et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are few-shot learners",
      "author" : [ "Amodei." ],
      "venue" : "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.",
      "citeRegEx" : "Amodei.,? 2020",
      "shortCiteRegEx" : "Amodei.",
      "year" : 2020
    }, {
      "title" : "Faithful to the original: Fact aware neural abstractive summarization",
      "author" : [ "Ziqiang Cao", "Furu Wei", "Wenjie Li", "Sujian Li." ],
      "venue" : "Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Arti-",
      "citeRegEx" : "Cao et al\\.,? 2018",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-document summarization of evaluative text",
      "author" : [ "Giuseppe Carenini", "Raymond T. Ng", "Adam Pauls." ],
      "venue" : "EACL 2006, 11st Conference of the European Chapter of the Association for Computational Linguistics, Proceedings of the Conference, April 3-",
      "citeRegEx" : "Carenini et al\\.,? 2006",
      "shortCiteRegEx" : "Carenini et al\\.",
      "year" : 2006
    }, {
      "title" : "Meansum: A neural model for unsupervised multi-document abstractive summarization",
      "author" : [ "Eric Chu", "Peter J. Liu." ],
      "venue" : "Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California,",
      "citeRegEx" : "Chu and Liu.,? 2019",
      "shortCiteRegEx" : "Chu and Liu.",
      "year" : 2019
    }, {
      "title" : "Diverse beam search for increased novelty in abstractive summarization",
      "author" : [ "André Cibils", "Claudiu Musat", "Andreea Hossmann", "Michael Baeriswyl." ],
      "venue" : "CoRR, abs/1802.01457.",
      "citeRegEx" : "Cibils et al\\.,? 2018",
      "shortCiteRegEx" : "Cibils et al\\.",
      "year" : 2018
    }, {
      "title" : "Unsupervised aspect-based multi-document abstractive summarization",
      "author" : [ "Maximin Coavoux", "Hady Elsahar", "Matthias Gallé." ],
      "venue" : "Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 42–47.",
      "citeRegEx" : "Coavoux et al\\.,? 2019",
      "shortCiteRegEx" : "Coavoux et al\\.",
      "year" : 2019
    }, {
      "title" : "A coefficient of agreement for nominal scales",
      "author" : [ "Jacob Cohen." ],
      "venue" : "Educational and psychological measurement, 20(1):37–46.",
      "citeRegEx" : "Cohen.,? 1960",
      "shortCiteRegEx" : "Cohen.",
      "year" : 1960
    }, {
      "title" : "Overview of duc 2005",
      "author" : [ "Hoa Trang Dang." ],
      "venue" : "Proceedings of the document understanding conference, volume 2005, pages 1–12.",
      "citeRegEx" : "Dang.,? 2005",
      "shortCiteRegEx" : "Dang.",
      "year" : 2005
    }, {
      "title" : "Summeval: Reevaluating summarization evaluation",
      "author" : [ "Alexander R. Fabbri", "Wojciech Kryscinski", "Bryan McCann", "Caiming Xiong", "Richard Socher", "Dragomir R. Radev." ],
      "venue" : "Trans. Assoc. Comput. Linguistics, 9:391–409.",
      "citeRegEx" : "Fabbri et al\\.,? 2021",
      "shortCiteRegEx" : "Fabbri et al\\.",
      "year" : 2021
    }, {
      "title" : "Pytorch lightning",
      "author" : [ "WA Falcon." ],
      "venue" : "GitHub. Note: https://github.com/PyTorchLightning/pytorchlightning, 3.",
      "citeRegEx" : "Falcon.,? 2019",
      "shortCiteRegEx" : "Falcon.",
      "year" : 2019
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomás Kociský", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "Advances in Neural Information Processing Systems 28: Annual",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "Mining and summarizing customer reviews",
      "author" : [ "Minqing Hu", "Bing Liu." ],
      "venue" : "Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Seattle, Washington, USA, August 22-25, 2004, pages 168–177.",
      "citeRegEx" : "Hu and Liu.,? 2004",
      "shortCiteRegEx" : "Hu and Liu.",
      "year" : 2004
    }, {
      "title" : "Aspect based summarization of context dependent opinion words",
      "author" : [ "Hitesh Kansal", "Durga Toshniwal." ],
      "venue" : "18th International Conference in Knowledge Based and Intelligent Information and Engineering Systems, KES 2014, Gdynia, Poland, 15-17",
      "citeRegEx" : "Kansal and Toshniwal.,? 2014",
      "shortCiteRegEx" : "Kansal and Toshniwal.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Best-worst scaling more reliable than rating scales: A case study on sentiment intensity annotation",
      "author" : [ "Svetlana Kiritchenko", "Saif Mohammad." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2:",
      "citeRegEx" : "Kiritchenko and Mohammad.,? 2017",
      "shortCiteRegEx" : "Kiritchenko and Mohammad.",
      "year" : 2017
    }, {
      "title" : "Capturing reliable fine-grained sentiment associations by crowdsourcing and best–worst scaling",
      "author" : [ "Svetlana Kiritchenko", "Saif M. Mohammad." ],
      "venue" : "Proceedings of The 15th Annual Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Kiritchenko and Mohammad.,? 2016",
      "shortCiteRegEx" : "Kiritchenko and Mohammad.",
      "year" : 2016
    }, {
      "title" : "Neural text summarization: A critical evaluation",
      "author" : [ "Wojciech Kryscinski", "Nitish Shirish Keskar", "Bryan McCann", "Caiming Xiong", "Richard Socher." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Kryscinski et al\\.,? 2019",
      "shortCiteRegEx" : "Kryscinski et al\\.",
      "year" : 2019
    }, {
      "title" : "METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments",
      "author" : [ "Alon Lavie", "Abhaya Agarwal." ],
      "venue" : "Proceedings of the Second Workshop on Statistical Machine Translation, pages 228–231, Prague, Czech Repub-",
      "citeRegEx" : "Lavie and Agarwal.,? 2007",
      "shortCiteRegEx" : "Lavie and Agarwal.",
      "year" : 2007
    }, {
      "title" : "Sentiment summarization: Evaluating and learning user preferences",
      "author" : [ "Kevin Lerman", "Sasha Blair-Goldensohn", "Ryan T. McDonald." ],
      "venue" : "EACL 2009, 12th Conference of the European Chapter of the Association for Computational Linguistics, Proceed-",
      "citeRegEx" : "Lerman et al\\.,? 2009",
      "shortCiteRegEx" : "Lerman et al\\.",
      "year" : 2009
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Looking for a few good metrics: Automatic summarization evaluation - how many samples are enough",
      "author" : [ "Chin-Yew Lin" ],
      "venue" : "In Proceedings of the Fourth NTCIR Workshop on Research in Information Access Technologies Information Retrieval,",
      "citeRegEx" : "Lin.,? \\Q2004\\E",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text summarization branches out, pages 74–81.",
      "citeRegEx" : "Lin.,? 2004b",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Best-worst scaling: Theory, methods and applications",
      "author" : [ "Jordan J Louviere", "Terry N Flynn", "Anthony Alfred John Marley." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Louviere et al\\.,? 2015",
      "shortCiteRegEx" : "Louviere et al\\.",
      "year" : 2015
    }, {
      "title" : "Best-worst scaling: A model for the largest difference judgments",
      "author" : [ "Jordan J Louviere", "George G Woodworth." ],
      "venue" : "Technical report, Working Paper.",
      "citeRegEx" : "Louviere and Woodworth.,? 1991",
      "shortCiteRegEx" : "Louviere and Woodworth.",
      "year" : 1991
    }, {
      "title" : "On faithfulness and factuality in abstractive summarization",
      "author" : [ "Joshua Maynez", "Shashi Narayan", "Bernd Bohnet", "Ryan McDonald." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906–1919, On-",
      "citeRegEx" : "Maynez et al\\.,? 2020",
      "shortCiteRegEx" : "Maynez et al\\.",
      "year" : 2020
    }, {
      "title" : "Better summarization evaluation with word embeddings for ROUGE",
      "author" : [ "Jun-Ping Ng", "Viktoria Abrecht." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1925–1930, Lisbon, Portugal. Associa-",
      "citeRegEx" : "Ng and Abrecht.,? 2015",
      "shortCiteRegEx" : "Ng and Abrecht.",
      "year" : 2015
    }, {
      "title" : "Maxdiff analysis : Simple counting , individual-level logit , and hb",
      "author" : [ "B. Orme" ],
      "venue" : null,
      "citeRegEx" : "Orme.,? \\Q2009\\E",
      "shortCiteRegEx" : "Orme.",
      "year" : 2009
    }, {
      "title" : "Bleu: A method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, page 311–318, USA.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Pytorch: An imperative style, high-performance deep learning library",
      "author" : [ "jani", "Sasank Chilamkurthy", "Benoit Steiner", "Lu Fang", "Junjie Bai", "Soumith Chintala" ],
      "venue" : null,
      "citeRegEx" : "jani et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "jani et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "J. Mach. Learn. Res., 21:140:1–140:67.",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Massive multidocument summarization of product reviews with weak supervision",
      "author" : [ "Ori Shapira", "Ran Levy." ],
      "venue" : "CoRR, abs/2007.11348.",
      "citeRegEx" : "Shapira and Levy.,? 2020",
      "shortCiteRegEx" : "Shapira and Levy.",
      "year" : 2020
    }, {
      "title" : "OpinionDigest: A simple framework for opinion summarization",
      "author" : [ "Yoshihiko Suhara", "Xiaolan Wang", "Stefanos Angelidis", "Wang-Chiew Tan." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5789–",
      "citeRegEx" : "Suhara et al\\.,? 2020",
      "shortCiteRegEx" : "Suhara et al\\.",
      "year" : 2020
    }, {
      "title" : "Sentence retrieval with sentiment-specific topical anchoring for review summarization",
      "author" : [ "Jiaxing Tan", "Alexander Kotov", "Rojiar Pir Mohammadiani", "Yumei Huo." ],
      "venue" : "Proceedings of the 2017 ACM on Conference on Information and Knowledge Man-",
      "citeRegEx" : "Tan et al\\.,? 2017",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2017
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Diverse beam search: Decoding diverse solutions from neural sequence models",
      "author" : [ "Ashwin K. Vijayakumar", "Michael Cogswell", "Ramprasaath R. Selvaraju", "Qing Sun", "Stefan Lee", "David J. Crandall", "Dhruv Batra." ],
      "venue" : "CoRR, abs/1610.02424.",
      "citeRegEx" : "Vijayakumar et al\\.,? 2016",
      "shortCiteRegEx" : "Vijayakumar et al\\.",
      "year" : 2016
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Scao et al\\.,? 2020a",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Datasets",
      "author" : [ "McMillan-Major", "Simon Brandeis", "Sylvain Gugger", "François Lagunas", "Lysandre Debut", "Morgan Funtowicz", "Anthony Moi", "Sasha Rush", "Philipp Schmidd", "Pierric Cistac", "Victor Muštar", "Jeff Boudier", "Anna Tordjmann." ],
      "venue" : "GitHub. Note:",
      "citeRegEx" : "McMillan.Major et al\\.,? 2020b",
      "shortCiteRegEx" : "McMillan.Major et al\\.",
      "year" : 2020
    }, {
      "title" : "Aspect-based opinion summarization",
      "author" : [ "Haibing Wu", "Yiwei Gu", "Shangdi Sun", "Xiaodong Gu" ],
      "venue" : null,
      "citeRegEx" : "Wu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2016
    }, {
      "title" : "Empirical analysis of exploiting review helpfulness for extractive summarization of online reviews",
      "author" : [ "Wenting Xiong", "Diane Litman." ],
      "venue" : "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical",
      "citeRegEx" : "Xiong and Litman.,? 2014",
      "shortCiteRegEx" : "Xiong and Litman.",
      "year" : 2014
    }, {
      "title" : "PEGASUS: pre-training with extracted gap-sentences for abstractive summarization",
      "author" : [ "Jingqing Zhang", "Yao Zhao", "Mohammad Saleh", "Peter J. Liu." ],
      "venue" : "Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18",
      "citeRegEx" : "Zhang et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Bertscore: Evaluating text generation with BERT",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,",
      "citeRegEx" : "Zhang et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "The research community has addressed this challenge early on, starting from the work of (Hu and Liu, 2004) which defined the task of mining and summarizing customer reviews.",
      "startOffset" : 88,
      "endOffset" : 106
    }, {
      "referenceID" : 5,
      "context" : "Our first observation relates to the summaries generated by CopyCat (Brazinskas et al., 2020b) and FewSum (Brazinskas et al.",
      "startOffset" : 68,
      "endOffset" : 94
    }, {
      "referenceID" : 4,
      "context" : ", 2020b) and FewSum (Brazinskas et al., 2020a), two of these SOTA systems, which tend to mix generic statements such as “Would recommend this product to anyone” along with more informative content such as “The sound quality is good” (see Table 6 in Appendix B for examples of such generated summaries).",
      "startOffset" : 20,
      "endOffset" : 46
    }, {
      "referenceID" : 27,
      "context" : "In order to estimate the similarity between summaries generated for different products, we devise the Set-Pairwise-ROUGE metric (henceforth denoted as SPR), that computes the average ROUGE (Lin, 2004b) scores of summaries for two different products, across all product pairs.",
      "startOffset" : 189,
      "endOffset" : 201
    }, {
      "referenceID" : 40,
      "context" : "Large pre-trained Transformer-based (Vaswani et al., 2017) models such as OpenAI’s GPT-3 (Brown et al.",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 36,
      "context" : ", 2020), Google’s T5 (Raffel et al., 2020), PEGASUS (Zhang et al.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 46,
      "context" : ", 2020), PEGASUS (Zhang et al., 2020a), and Facebook’s BART (Lewis et al.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 25,
      "context" : ", 2020a), and Facebook’s BART (Lewis et al., 2020) have made com-",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 7,
      "context" : ", whether the facts conveyed in the summary agree with the source text (Cao et al., 2018; Kryscinski et al., 2019; Maynez et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 135
    }, {
      "referenceID" : 22,
      "context" : ", whether the facts conveyed in the summary agree with the source text (Cao et al., 2018; Kryscinski et al., 2019; Maynez et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 135
    }, {
      "referenceID" : 31,
      "context" : ", whether the facts conveyed in the summary agree with the source text (Cao et al., 2018; Kryscinski et al., 2019; Maynez et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 135
    }, {
      "referenceID" : 14,
      "context" : "Instead of basing our ranking solely on this criterion, we train a more general coherence summary ranker using human annotated coherence scores (Fabbri et al., 2021).",
      "startOffset" : 144,
      "endOffset" : 165
    }, {
      "referenceID" : 13,
      "context" : "We evaluate each over 3 dimensions, of which relevance and coherence are commonly used in summarization (Dang, 2005), and our newly introduced metric for CP-Diversity.",
      "startOffset" : 104,
      "endOffset" : 116
    }, {
      "referenceID" : 17,
      "context" : "A common approach for product review summarization, which centers the summary around a set of extracted aspects and their respective sentiment, is termed aspect-based summarization (Hu and Liu, 2004; Kansal and Toshniwal, 2014; Wu et al., 2016; Angelidis and Lapata, 2018; Coavoux et al., 2019).",
      "startOffset" : 181,
      "endOffset" : 294
    }, {
      "referenceID" : 18,
      "context" : "A common approach for product review summarization, which centers the summary around a set of extracted aspects and their respective sentiment, is termed aspect-based summarization (Hu and Liu, 2004; Kansal and Toshniwal, 2014; Wu et al., 2016; Angelidis and Lapata, 2018; Coavoux et al., 2019).",
      "startOffset" : 181,
      "endOffset" : 294
    }, {
      "referenceID" : 44,
      "context" : "A common approach for product review summarization, which centers the summary around a set of extracted aspects and their respective sentiment, is termed aspect-based summarization (Hu and Liu, 2004; Kansal and Toshniwal, 2014; Wu et al., 2016; Angelidis and Lapata, 2018; Coavoux et al., 2019).",
      "startOffset" : 181,
      "endOffset" : 294
    }, {
      "referenceID" : 3,
      "context" : "A common approach for product review summarization, which centers the summary around a set of extracted aspects and their respective sentiment, is termed aspect-based summarization (Hu and Liu, 2004; Kansal and Toshniwal, 2014; Wu et al., 2016; Angelidis and Lapata, 2018; Coavoux et al., 2019).",
      "startOffset" : 181,
      "endOffset" : 294
    }, {
      "referenceID" : 11,
      "context" : "A common approach for product review summarization, which centers the summary around a set of extracted aspects and their respective sentiment, is termed aspect-based summarization (Hu and Liu, 2004; Kansal and Toshniwal, 2014; Wu et al., 2016; Angelidis and Lapata, 2018; Coavoux et al., 2019).",
      "startOffset" : 181,
      "endOffset" : 294
    }, {
      "referenceID" : 8,
      "context" : "Extractive summarization include earlier works such as (Carenini et al., 2006; Lerman et al., 2009; Xiong and Litman, 2014).",
      "startOffset" : 55,
      "endOffset" : 123
    }, {
      "referenceID" : 24,
      "context" : "Extractive summarization include earlier works such as (Carenini et al., 2006; Lerman et al., 2009; Xiong and Litman, 2014).",
      "startOffset" : 55,
      "endOffset" : 123
    }, {
      "referenceID" : 45,
      "context" : "Extractive summarization include earlier works such as (Carenini et al., 2006; Lerman et al., 2009; Xiong and Litman, 2014).",
      "startOffset" : 55,
      "endOffset" : 123
    }, {
      "referenceID" : 39,
      "context" : "More recently, (Tan et al., 2017) suggested a novel generative topic aspect sentiment model, while (Angelidis et al.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 2,
      "context" : ", 2017) suggested a novel generative topic aspect sentiment model, while (Angelidis et al., 2021) suggested a novel system able to extract both general and aspect-specific summaries.",
      "startOffset" : 73,
      "endOffset" : 97
    }, {
      "referenceID" : 9,
      "context" : "As for abstractive summarization, recent advances on pre-training neural networks were explored in the context of product reviews in unsupervised and few-shot learning schemes which led to promising results (Chu and Liu, 2019; Brazinskas et al., 2020b,a; Suhara et al., 2020; Amplayo et al., 2021).",
      "startOffset" : 207,
      "endOffset" : 297
    }, {
      "referenceID" : 38,
      "context" : "As for abstractive summarization, recent advances on pre-training neural networks were explored in the context of product reviews in unsupervised and few-shot learning schemes which led to promising results (Chu and Liu, 2019; Brazinskas et al., 2020b,a; Suhara et al., 2020; Amplayo et al., 2021).",
      "startOffset" : 207,
      "endOffset" : 297
    }, {
      "referenceID" : 0,
      "context" : "As for abstractive summarization, recent advances on pre-training neural networks were explored in the context of product reviews in unsupervised and few-shot learning schemes which led to promising results (Chu and Liu, 2019; Brazinskas et al., 2020b,a; Suhara et al., 2020; Amplayo et al., 2021).",
      "startOffset" : 207,
      "endOffset" : 297
    }, {
      "referenceID" : 27,
      "context" : "Among the automated metrics, probably the most well-known is the ROUGE family of scores (Lin, 2004b) that measures ngram overlap between generated summaries and corresponding reference summaries.",
      "startOffset" : 88,
      "endOffset" : 100
    }, {
      "referenceID" : 34,
      "context" : "Many other metrics that aim to quantify how well generated summaries align with reference summaries have been proposed, such as BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), ROUGE-WE (Ng and Abrecht, 2015) and BertScore (Zhang et al.",
      "startOffset" : 133,
      "endOffset" : 156
    }, {
      "referenceID" : 23,
      "context" : ", 2002), METEOR (Lavie and Agarwal, 2007), ROUGE-WE (Ng and Abrecht, 2015) and BertScore (Zhang et al.",
      "startOffset" : 16,
      "endOffset" : 41
    }, {
      "referenceID" : 32,
      "context" : ", 2002), METEOR (Lavie and Agarwal, 2007), ROUGE-WE (Ng and Abrecht, 2015) and BertScore (Zhang et al.",
      "startOffset" : 52,
      "endOffset" : 74
    }, {
      "referenceID" : 47,
      "context" : ", 2002), METEOR (Lavie and Agarwal, 2007), ROUGE-WE (Ng and Abrecht, 2015) and BertScore (Zhang et al., 2020b) to name a few.",
      "startOffset" : 89,
      "endOffset" : 110
    }, {
      "referenceID" : 7,
      "context" : "Unfortunately, such metrics alone do not tell the whole story and recently several works observed that a new requirement is necessary in order to ensure that facts from the summary agree with the source document (Cao et al., 2018; Kryscinski et al., 2019; Maynez et al., 2020).",
      "startOffset" : 212,
      "endOffset" : 276
    }, {
      "referenceID" : 22,
      "context" : "Unfortunately, such metrics alone do not tell the whole story and recently several works observed that a new requirement is necessary in order to ensure that facts from the summary agree with the source document (Cao et al., 2018; Kryscinski et al., 2019; Maynez et al., 2020).",
      "startOffset" : 212,
      "endOffset" : 276
    }, {
      "referenceID" : 31,
      "context" : "Unfortunately, such metrics alone do not tell the whole story and recently several works observed that a new requirement is necessary in order to ensure that facts from the summary agree with the source document (Cao et al., 2018; Kryscinski et al., 2019; Maynez et al., 2020).",
      "startOffset" : 212,
      "endOffset" : 276
    }, {
      "referenceID" : 13,
      "context" : "The DUC 2005 task (Dang, 2005) suggested the following 5 dimensions: Grammaticality, Non-redundancy, Referential clarity, Focus and Structure, and Coherence.",
      "startOffset" : 18,
      "endOffset" : 30
    }, {
      "referenceID" : 4,
      "context" : "In the context of product reviews summarization (Brazinskas et al., 2020a) use the standard ROUGE-1/2/L metrics as well human comparative judgments on 5 dimensions: Fluency, Coherence, Non-Redundancy, Informativeness and Sentiment.",
      "startOffset" : 48,
      "endOffset" : 74
    }, {
      "referenceID" : 4,
      "context" : "PASS relies on a pre-trained T5 language model, which we fine-tuned on a small publicly available dataset for product reviews summarization (Brazinskas et al., 2020a).",
      "startOffset" : 140,
      "endOffset" : 166
    }, {
      "referenceID" : 36,
      "context" : "We follow a similar fine-tuning scheme for abstractive text summarization to the one presented in (Raffel et al., 2020) with the exception that we concatenate the multiple reviews into a single input text as a preprocessing step.",
      "startOffset" : 98,
      "endOffset" : 119
    }, {
      "referenceID" : 1,
      "context" : "Note that prior attempts have been made to directly manipulate the content within the reviews (Amplayo and Lapata, 2020) a path that we do not explore here.",
      "startOffset" : 94,
      "endOffset" : 120
    }, {
      "referenceID" : 37,
      "context" : "A few recent works attempt to explicitly address this issue (Shapira and Levy, 2020; Angelidis et al., 2021).",
      "startOffset" : 60,
      "endOffset" : 108
    }, {
      "referenceID" : 2,
      "context" : "A few recent works attempt to explicitly address this issue (Shapira and Levy, 2020; Angelidis et al., 2021).",
      "startOffset" : 60,
      "endOffset" : 108
    }, {
      "referenceID" : 4,
      "context" : "We utilize a recent publicly available Amazon product reviews summarization dataset (Brazinskas et al., 2020a) for fine-tuning the T5 model which underlines the PASS system and for evaluating the LkO input perturbation method, both in isolation and as part of the end-to-end PASS system.",
      "startOffset" : 84,
      "endOffset" : 110
    }, {
      "referenceID" : 4,
      "context" : "While we mainly focus on product reviews summarization, we include the Yelp business reviews summarization dataset (also from (Brazinskas et al., 2020a)) in our end-to-end evaluation for the sake of completeness.",
      "startOffset" : 126,
      "endOffset" : 152
    }, {
      "referenceID" : 14,
      "context" : "For training and evaluating the pairwise coherence classifier, we utilize a public dataset of human annotated summaries (Fabbri et al., 2021), generated by 16 modern text summarization models for 100 news articles (1600 examples in total) from the CNN/DailyMail dataset (Hermann et al.",
      "startOffset" : 120,
      "endOffset" : 141
    }, {
      "referenceID" : 16,
      "context" : ", 2021), generated by 16 modern text summarization models for 100 news articles (1600 examples in total) from the CNN/DailyMail dataset (Hermann et al., 2015).",
      "startOffset" : 136,
      "endOffset" : 158
    }, {
      "referenceID" : 14,
      "context" : "We chose to use the experts’ annotations only, as they are considered to be more accurate and reliable for coherence and fluency (Fabbri et al., 2021).",
      "startOffset" : 129,
      "endOffset" : 150
    }, {
      "referenceID" : 36,
      "context" : "We fine-tune a T5-Base model (220M parameters (Raffel et al., 2020)) for abstractive text summarization as described in 3.",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 15,
      "context" : "We train for maximum 20 epochs while employing a standard early stopping mechanism (Falcon, 2019) based on the development set’s average loss per epoch.",
      "startOffset" : 83,
      "endOffset" : 97
    }, {
      "referenceID" : 5,
      "context" : "We compare the PASS system to four baselines: COPYCAT (Brazinskas et al., 2020b) is an unsupervised reviews summarizer that is trained to generate a review given other reviews for the same product.",
      "startOffset" : 54,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : "FEWSUM (Brazinskas et al., 2020a) is a fewshot reviews summarizer that builds upon the ideas of CopyCat but also conditions the model on certain linguistic properties such as writing style.",
      "startOffset" : 7,
      "endOffset" : 33
    }, {
      "referenceID" : 9,
      "context" : "We do not report results for MEANSUM (Chu and Liu, 2019) since it was consistently outperformed by FEWSUM (Brazinskas et al.",
      "startOffset" : 37,
      "endOffset" : 56
    }, {
      "referenceID" : 4,
      "context" : "We do not report results for MEANSUM (Chu and Liu, 2019) since it was consistently outperformed by FEWSUM (Brazinskas et al., 2020a).",
      "startOffset" : 106,
      "endOffset" : 132
    }, {
      "referenceID" : 14,
      "context" : "The pairwise summary classifiers can be evaluated directly using human scores from (Fabbri et al., 2021) after adapting them to our ternary classification task.",
      "startOffset" : 83,
      "endOffset" : 104
    }, {
      "referenceID" : 13,
      "context" : "We hypothesize that coherence and fluency are linguistic properties that are not heavily tied with the domain, since they relate to a summary’s overall collective and individual sentence quality (Dang, 2005).",
      "startOffset" : 195,
      "endOffset" : 207
    }, {
      "referenceID" : 27,
      "context" : "The first, informativeness, is traditionally evaluated using the ROUGE-1/2/L F1 measures (Lin, 2004b) and we follow suit.",
      "startOffset" : 89,
      "endOffset" : 101
    }, {
      "referenceID" : 30,
      "context" : "We used Best-Worst Scaling (Louviere and Woodworth, 1991; Louviere et al., 2015; Kiritchenko and Mohammad, 2016, 2017) to compute each system’s score as the difference between the percentage of times it was selected as best, and the percentage of times it was selected as worst (Orme, 2009).",
      "startOffset" : 27,
      "endOffset" : 118
    }, {
      "referenceID" : 29,
      "context" : "We used Best-Worst Scaling (Louviere and Woodworth, 1991; Louviere et al., 2015; Kiritchenko and Mohammad, 2016, 2017) to compute each system’s score as the difference between the percentage of times it was selected as best, and the percentage of times it was selected as worst (Orme, 2009).",
      "startOffset" : 27,
      "endOffset" : 118
    }, {
      "referenceID" : 33,
      "context" : ", 2015; Kiritchenko and Mohammad, 2016, 2017) to compute each system’s score as the difference between the percentage of times it was selected as best, and the percentage of times it was selected as worst (Orme, 2009).",
      "startOffset" : 205,
      "endOffset" : 217
    }, {
      "referenceID" : 12,
      "context" : "We measured the level of agreement between the two annotators by calculating Cohen’s Kappa coefficients (Cohen, 1960) for each annotation task, which resulted in κCE = 0.",
      "startOffset" : 104,
      "endOffset" : 117
    } ],
    "year" : 2021,
    "abstractText" : "The product reviews summarization task aims to automatically produce a short summary for a set of reviews of a given product. Such summaries are expected to aggregate a range of different opinions in a concise, coherent and informative manner. This challenging task gives rise to two shortcomings in existing work. First, summarizers tend to favor generic content that appears in reviews for many different products, resulting in template-like, less informative summaries. Second, as reviewers often disagree on the pros and cons of a given product, summarizers sometimes yield inconsistent, self-contradicting summaries. We propose the PASS system (Perturb-and-Select Summarizer) that employs a large pre-trained Transformer-based model (T5 in our case), which follows a few-shot fine-tuning scheme. A key component of the PASS system relies on applying systematic perturbations to the model’s input during inference, which allows it to generate multiple different summaries per product. We develop a method for ranking these summaries according to desired criteria, coherence in our case, enabling our system to almost entirely avoid the problem of selfcontradiction. We compare our system against strong baselines on publicly available datasets, and show that it produces summaries which are more informative, diverse and coherent.1",
    "creator" : "LaTeX with hyperref"
  }
}