{
  "name" : "2021.acl-long.274.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker",
    "authors" : [ "Runxin Xu", "Tianyu Liu", "Lei Li", "Baobao Chang", "Xiaoting Wu" ],
    "emails" : [ "tianyu0421@pku.edu.cn", "chbb@pku.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3533–3546\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3533\nDocument-level event extraction aims to recognize event information from a whole piece of article. Existing methods are not effective due to two challenges of this task: a) the target event arguments are scattered across sentences; b) the correlation among events in a document is non-trivial to model. In this paper, we propose Heterogeneous Graph-based Interaction Model with a Tracker (GIT) to solve the aforementioned two challenges. For the first challenge, GIT constructs a heterogeneous graph interaction network to capture global interactions among different sentences and entity mentions. For the second, GIT introduces a Tracker module to track the extracted events and hence capture the interdependency among the events. Experiments on a large-scale dataset (Zheng et al., 2019) show GIT outperforms the existing best methods by 2.8 F1. Further analysis reveals GIT is effective in extracting multiple correlated events and event arguments that scatter across the document. Our code is available at https: //github.com/RunxinXu/GIT."
    }, {
      "heading" : "1 Introduction",
      "text" : "Event Extraction (EE) is one of the key and challenging tasks in Information Extraction (IE), which aims to detect events and extract their arguments from the text. Most previous methods (Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018; Yang et al., 2019; Du and Cardie, 2020b) focus on sentence-level EE, extracting events from a single sentence. The sentence-level model, however, fails to extract events whose arguments spread in multiple sentences, which is much more common in real-world scenarios. Hence, extracting events at the document-level is critical. It has attracted much attention recently (Yang et al., 2018; Zheng et al., 2019; Du and Cardie, 2020a; Du et al., 2020).\n*Corresponding author.\nThough promising, document-level EE still faces two critical challenges. Firstly, the arguments of an event record may scatter across sentences, which requires a comprehensive understanding of the cross-sentence context. Figure 1 illustrates an example that one Equity Underweight (EU) and one Equity Overweight (EO) event records are extracted from a financial document. It is less challenging to extract the EU event because all the related arguments appear in the same sentence (Sentence 2). However, for the arguments of EO record, Nov 6, 2014 appears in Sentence 1 and 2 while Xiaoting Wu in Sentence 3 and 4. It would be quite challenging to identify such events without considering global interactions among sentences and entity mentions. Secondly, a document may express several correlated events simultaneously, and recognizing the interdependency among them is\nfundamental to successful extraction. As shown in Figure 1, the two events are interdependent because they correspond to exactly the same transaction and therefore share the same StartDate. Effective modeling on such interdependency among the correlated events remains a key challenge in this task.\nYang et al. (2018) extracts events from a central sentence and query the neighboring sentences for missing arguments, which ignores the cross-sentence correspondence between augments. Though Zheng et al. (2019) takes a first step to fuse the sentences and entities information via Transformer, they neglect the interdependency among events. Focusing on single event extraction, Du and Cardie (2020a) and Du et al. (2020) concatenate multiple sentences and only consider a single event, which lacks the ability to model multiple events scattered in a long document.\nTo tackle the aforementioned two challenges, in this paper, we propose a Heterogeneous Graphbased Interaction Model with a Tracker (GIT) for document-level EE. To deal with scattered arguments across sentences, we focus on the Global Interactions among sentences and entity mentions. Specifically, we construct a heterogeneous graph interaction network with mention nodes and sentence nodes, and model the interactions among them by four types of edges (i.e., sentence-sentence edge, sentence-mention edge, intra-mention-mention edge, and inter-mentionmention edge) in the graph neural network. In this way, GIT jointly models the entities and sentences in the document from a global perspective.\nTo facilitate the multi-event extraction, we target on the Global Interdependency among correlated events. Concretely we propose a Tracker module to continually tracks the extracted event records with a global memory. In this way, the model is encouraged to incorporate the interdependency with other correlated event records while predicting.\nWe summarize our contributions as follows:\n• We construct a heterogeneous graph interaction network for document-level EE. With different heterogeneous edges, the model could capture the global context for the scattered event arguments across different sentences.\n• We introduce a novel Tracker module to track the extracted event records. The Tracker eases the difficulty of extracting correlated events, as interdependency among events would be taken into consideration.\n• Experiments show GIT outperforms the previous state-of-the-art model by 2.8 F1 on the large-scale public dataset (Zheng et al., 2019) with 32, 040 documents, especially on crosssentence events and multiple events scenarios (with 3.7 and 4.9 absolute increase on F1)."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "We first clarify some important notions. a) entity mention: a text span within document that refers to an entity object; b) event argument: an entity playing a specific event role. Event roles are predefined for each event type; c) event record: an entry of a specific event type containing arguments for different roles in the event. For simplicity, we use record for short in the following sections.\nFollowing Zheng et al. (2019), given a document composed of sentences D = {si}|D|i=1 and a sentence containing a sequence of words si = {wj}|si|j=1, the task aims to handle three sub-tasks : 1) entity extraction: extracting entities E = {ei}|E|i=1 from the document to serve as argument candidates. An entity may have multiple mentions across the document. 2) event types detection: detecting specific event types that are expressed by the document. 3) event records extraction: finding appropriate arguments for the expressed events from entities, which is the most challenging and also the focus of our paper. The task does not require to identify event triggers (Zeng et al., 2018; Liu et al., 2019b), which reduces manual effort of annotation and the application scenarios becomes more extensive."
    }, {
      "heading" : "3 Methodology",
      "text" : "As shows in Figure 2, GIT first extracts candidate entities through sentence-level neural extractor (Sec 3.1). Then we construct a heterogeneous graph to model the interactions among sentences and entity mentions (Sec 3.2), and detect event types expressed by the document (Sec 3.3). Finally we introduce a Tracker module to continuously track all the records with global memory, in which we utilize the global interdependency among records for multi-event extraction (Sec 3.4)."
    }, {
      "heading" : "3.1 Entity Extraction",
      "text" : "Given a sentence s = {wj}|s|j=1 ∈ D, we encode s into a sequence of vectors {gj}|si|j=1 using Trans-\nformer (Vaswani et al., 2017):\n{g1, . . . , g|s|} = Transformer({w1, . . . , w|s|})\nThe word representation of wj is a sum of the corresponding token and position embeddings.\nWe extract entities at the sentence level and formulate it as a sequence tagging task with BIO (Begin, Inside, Other) schema. We leverage a conditional random field (CRF) layer to identify entities. For training, we minimize the following loss:\nLner = − ∑ s∈D logP (ys|s) (1)\nwhere ys is the golden label sequence of s. For inference, we use Viterbi algorithm to decode the label sequence with the maximum probability."
    }, {
      "heading" : "3.2 Heterogeneous Graph Interaction Network",
      "text" : "An event may span multiple sentences in the document, which means its corresponding entity mentions may also scatter across different sentences. Identifying and modeling these entity mentions in the cross-sentence context is fundamental in document EE. Thus we build a heterogeneous graph G which contains entity mention nodes and sentence nodes in the document D. In the graph G, interactions among multiple entity mentions and\nsentences can be explicitly modeled. For each entity mention node e, we initialize node embedding h(0)e = Mean({gj}j∈e) by averaging the representation of the contained words. For each sentence node s, we initialize node embedding h (0) s = Max({gj}j∈s) + SentPos(s) by maxpooling all the representation of words within the sentence plus sentence position embedding.\nTo capture the interactions among sentences and mentions, we introduce four types of edges.\nSentence-Sentence Edge (S-S) Sentence nodes are fully connected to each other with S-S edges. In this way, we can easily capture the global properties in the document with sentence-level interactions, e.g., the long range dependency between any two separate sentences in the document would be modeled efficiently with S-S edges.\nSentence-Mention Edge (S-M) We model the local context of an entity mention in a specific sentence with S-M edge, specifically the edge connecting the mention node and the sentence node it belongs to.\nIntra-Mention-Mention Edge (M-Mintra) We connect distinct entity mentions in the same sentences with M-Mintra edges. The co-occurrence of mentions in a sentence indicates those mentions are likely to be involved in the same event. We\nexplicitly model this indication by M-Mintra edges.\nInter-Mention-Mention Edge (M-Minter) The entity mentions that corresponds to the same entity are fully connected with each other by M-Minter edges. As in document EE, an entity usually corresponds to multiple mentions across sentences, we thus use M-Minter edge to track all the appearances of a specific entity, which facilitates the long distance event extraction from a global perspective.\nIn Section. 4.5, experiments show that all of these four kinds of edges play an important role in event detection, and the performance would decrease without any of them.\nAfter heterogeneous graph construction *, we apply multi-layer Graph Convolution Network (Kipf and Welling, 2017) to model the global interactions inspired by Zeng et al. (2020). Given node u at the l-th layer, the graph convolutional operation is defined as follows:\nh(l+1)u = ReLU ∑ k∈K ∑ v∈Nk(u) ⋃ {u} 1 cu,k W (l) k h (l) v  where K represents different types of edges, W\n(l) k ∈ R dm×dm is trainable parameters. Nk(u) denotes the neighbors for node u connected in k-th type edge and cu,k is a normalization constant. We then derive the final hidden state hu for node u,\nhu =Wa[h (0) u ;h (1) u ; . . . ;h (L) u ]\nwhere h(0)u is the initial node embedding of node u, and L is the number of GCN layers.\nFinally, we obtain the sentence embedding matrix S = [h>1 h > 2 . . . h > |D|] ∈ R dm×|D| and entity embedding matrix E ∈ Rdm×|E|. The i-th entity may have many mentions, where we simply use string matching to detect entity coreference following Zheng et al. (2019) , and the entity embedding Ei is computed by the average of its mention node embedding, Ei = Mean({hj}j∈Mention(i)). In this way, the sentences and entities are interactively represented in a context-aware way."
    }, {
      "heading" : "3.3 Event Types Detection",
      "text" : "Since a document can express events of different types, we formulate the task as a multi-label classification and leverage sentences feature matrix S to\n*Traditional methods in sentence-level EE also utilize graph to extract events (Liu et al., 2018; Yan et al., 2019), based on the dependency tree. However, our interaction graph is heterogeneous and have no demands for dependency tree.\nuity Pledge records (in the dashed frame ), based on the global memory where Tracker tracks the records on-the-fly. Both entity E and F are predicted as the legal StartDate role while A is not. Pre-defined argument roles are shown in the blue box, and GIT extracts records in this order. Capital letters (A-K) refer to different entities. A path from root to leaf node represents one unique event record.\ndetect event types:\nA = MultiHead(Q,S, S) ∈ Rdm×T\nR = Sigmoid(A>Wt) ∈ RT\nwhere Q ∈ Rdm×T and Wt ∈ Rdm are trainable parameters, and T denotes the number of possible event types. MultiHead refers to the standard multi-head attention mechanism with Query/Key/Value. Therefore, we derive the event types detection loss with golden label R̂ ∈ RT :\nLdetect =− T∑ t=1 I ( R̂t = 1 ) logP (Rt|D)\n+ I ( R̂t = 0 ) log (1− P (Rt|D)) (2)"
    }, {
      "heading" : "3.4 Event Records Extraction",
      "text" : "Since a document is likely to express multiple event records and the number of records cannot be known in advance, we decode records by expanding a tree orderly as previous methods did (Zheng et al., 2019). However, they treat each record independently. Instead, to incorporate the interdependency among event records, we propose a Tracker module, which improves the model performance.\nTo be self-contained, we introduce the ordered tree expanding in this paragraph. In each step,\nwe extract event records of a specific event type. The arguments extraction order is predefined so that the extraction is modeled as a constrained tree expanding task†. Taking Equity Freeze records as an example, as shown in Figure 3, we firstly extract EquityHolder, followed by FrozeShares and others. Starting from a virtual root node, the tree expands by predicting arguments in a sequential order. As there may exist multiple eligible entities for the event argument role, the current node will expand several branches during extraction, with different entities assigned to the current role. This branching operation is formulated as multi-label classification task. In this way, each path from the root node to the leaf node is identified as a unique event record.\nInterdependency exists extensively among different event records. For example, as shown in Figure 1, an Equity Underweight event record is closely related to an Equity Overweight event record, and they may share some key arguments or provide useful reasoning information. To take advantage of such interdependency, we propose a novel Tracker module inspired by memory network (Weston et al., 2015). Intuitively, the Tracker continually tracks the extracted records on-the-fly and store the information into a global memory. When predicting arguments for current record, the model will query the global memory and therefore make use of useful interdependency information of other records.\nIn detail, for the i-th record path consisting of a sequence of entities, the Tracker encodes the corresponding entity representation sequence Ui = [Ei1, Ei2, ...] into an vector Gi with an LSTM (last hidden state) and add event type embedding. Then the compressed record information is stored in the global memory G, which is shared across different event types as shown in Figure 3. For extraction, given a record path Ui ∈ Rdm×(J−1) with the first J − 1 arguments roles, we predict the J-th role by injecting role-specific information into entity representations, E = E + RoleJ , where RoleJ is the role embedding for the J-th role. Then we concatenate E, sentences feature S, current entities path Ui, and the global memory G, followed by a transformer to obtain new entity feature matrix Ẽ ∈ Rdm×|E|, which contains global role-specific\n†We simply adopt the order used by Zheng et al. (2019).\ninformation for all entity candidates.‡\n[Ẽ, S̃, Ũi, G̃] = Transformer([E;S;Ui;G])\nWe treat the path expansion as a multi-label classification problem with a binary classifier over Ẽi, i.e., predicts whether the i-th entity is the next argument role for the current record and expand the path accordingly as shown in Figure 3.\nDuring training, we minimize the following loss:\nLrecord = − ∑\nn∈ND\n|E|∑ t=1 logP (ynt |n) (3)\nwhere ND denotes the nodes set in the event records tree, and ynt is the golden label. If the t-th entity is validate for the next argument in node n, then ynt = 1, otherwise y n t = 0."
    }, {
      "heading" : "3.5 Training",
      "text" : "We sum the losses coming from three sub-tasks with different weight respectively in Eq. (1), (2) and (3) as follows:\nLall = λ1Lner + λ2Ldetect + λ3Lrecord\nMore training details are shown in Appendix A."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset",
      "text" : "We evaluate our model on a public dataset proposed by Zheng et al. (2019)§, which is constructed from Chinese financial documents. It consists of up to 32, 040 documents which is the largest documentlevel EE dataset by far. It focuses on five event types: Equity Freeze (EF), Equity Repurchase (ER), Equity Underweight (EU), Equity Overweight (EO) and Equity Pledge (EP), with 35 different kinds of argument roles in total. We follow the standard split of the dataset, 25, 632/3, 204/3, 204 documents for training/dev/test set. The dataset is quite challenging, as a document has 20 sentences and consists of 912 tokens on average. Besides, there are roughly 6 sentences involved for an event record, and 29% documents express multiple events.\n‡To distinguish different parts in the concatenated vector, we also add segment embedding, which is omitted in Eq. 3.4.\n§https://github.com/dolphin-zs/ Doc2EDAG/blob/master/Data.zip"
    }, {
      "heading" : "4.2 Experiments Setting",
      "text" : "In our implementation of GIT, we use 8 and 4 layers Transformer (Vaswani et al., 2017) in encoding and decoding module respectively. The dimensions in hidden layers and feed-forward layers are the same as previous work (Zheng et al., 2019), i.e., 768 and 1, 024. We also use L = 3 layers of GCN, and set dropout rate to 0.1, batch size to 64. GIT is trained using Adam (Kingma and Ba, 2015) as optimizer with 1e− 4 learning rate for 100 epochs. We set λ1 = 0.05, λ2 = λ3 = 1 for the loss function."
    }, {
      "heading" : "4.3 Baselines and Metrics",
      "text" : "Yang et al. (2018) proposes DCFEE that extracts arguments from the identified central sentence and queries surrounding sentences for missing arguments. The model has two variants, DCFEE-S and DCFEE-M. DCFEE-S produces one record at a time, while DCFEE-M produces multiple possible argument combinations by the closest distance from the central sentence. Besides, Doc2EDAG (Zheng et al., 2019) uses transformer encoder to obtain sentence and entity embeddings, followed by another transformer to fuse cross-sentence context. Then multiple events are extracted simultaneously. Greedy-Dec is a variant of Doc2EDAG, which produces only one record greedily.\nThree sub-tasks of the document-level EE are all evaluated by F1 score. Due to limited space, we leave the results of entity extraction and event types detection in Appendix B, which shows GIT only slightly outperform Doc2EDAG, because we mainly focus on event record extraction and the methods are similar to Doc2EDAG for these two sub-tasks. In the following, we mainly report and analyze the results of event record extraction."
    }, {
      "heading" : "4.4 Main Results",
      "text" : "Overall performance. The results of the overall performance on the document-level EE dataset is illustrated in Table 1. As Table 1 shows, our GIT consistently outperforms other baselines, thanks to better modelling of global interactions and interdependency. Specifically, GIT improves 2.8 micro F1 compared with the previous state-of-the-art, Doc2EDAG, especially 4.5 improvement in Equity Underweight (EU) event type.\nCross-sentence records scenario. There are more than 99.5% records of the test set are crosssentence event records, and the extraction becomes gradually more difficult as the number of their involved sentences grows. To verifies the effectiveness of GIT to capture cross-sentence information, we first calculate the average number of sentences that the records involve for each document, and sort them in ascending order. Then we divide them into four sets I/II/III/IV with equal size. Documents in Set. IV is considered to be the most challenging as it requires the most number of sentences to successfully extract records. As Table 2 shows, GIT consistently outperforms Doc2EDAG, especially on the most challenging Set. IV that involves the most sentences, by 3.7 F1 score. It suggests that GIT can well capture global context and mitigate the arguments-scattering challenge, with the help of the heterogeneous graph interaction network.\nMultiple records scenario. GIT introduces the tracker to make use of global interdependency among event records, which is important in multiple records scenario. To illustrate its effectiveness, we divide the test set into single-record set (S.) containing documents with one record, and multi-record set (M.) containing those with multiple records. As shown in Table. 3, F1 score on M.\nis much lower than that on S., indicating it is challenging to extract multiple records. However, GIT still surpasses other strong baselines by 4.9 ∼ 35.3 on multi-record set (M.). This is because GIT is aware of other records through the Tracker module, and leverage the interdependency information to improve the performance¶.\n¶Nguyen et al. (2016) maintain three binary matrices to memorize entities and events states. Although they aim at sentence-level EE that contains fewer entities and event records, it would be also interesting to compare with them and we leave it as future work."
    }, {
      "heading" : "4.5 Analysis",
      "text" : "We conduct further experiments to analyze the key modules in GIT more deeply.\nOn the effect of heterogeneous graph interaction network. The heterogeneous graph we constructed contains four types of edges. To explore their functions, we remove one type of edges at a time, and remove the whole graph network finally. Results are shown in Table 4, including micro F1 and F1 on the four sets, which are divided by the number of involved sentences for records as we did before. The micro F1 would decreases 1.0 ∼ 1.4 without a certainty type of edge. Besides, removing the whole graph causes an significant drop by 2.0 F1, especially for Set IV by 2.5, which requires the most number of sentences to extract the event record. It demonstrates that the graph interaction network helps improve the performance, especially on records involving many sentences, and all kinds of edges play an important role for extraction.\nOn the effect of Tracker module. GIT can leverage interdependency among records based on the information of other event records tracked by Tracker. To explore its effect, firstly, we remove the global interdependency information between records of different event types, by clearing the global memory whenever we extract events for an-\nother new event type (GIT-Own Type). Next, we remove all the tracking information except the own path for a record, to explore whether the tracking of other records makes effect indeed (GIT-Own Path). Finally, we remove the whole Tracker module (GIT-No Tracker). As Table 5 shows, the F1 in GIT-OT/GIT-OP decreases by 0.5/1.2, suggesting the interdependency among records of both the same and different event types do play an essential role. Besides, their F1 decrease in M. by 0.7/1.5 are more than those in S. by 0.8/1.0, verifying the effectiveness of the Tracker in multi-event scenarios. Moreover, the performances are similar between GIT-OP and GIT-NT, which also provides evidence that other records do help. We also reveal F1 on documents with different number of records in Figure 4. The gap between models with or without Tracker raises as the number of records increases, which validates the effectiveness of our Tracker."
    }, {
      "heading" : "4.6 Case Study",
      "text" : "Figure 5 demonstrates a case of the predictions of Doc2EDAG and GIT for Equity Pledge (EP) event types. The TotalHoldingShares and TotalPledgedShares information lies in Sentence 8, while the PledgedShares and Pledgee information for Record 2 lies in Sentence 5. Though Doc2EDAG fails to extract these arguments in Record 2 (colored in red), GIT succeeds because it can capture interactions between long-distance sentences, and utilize the information of Record 1 (325.4 million and 218.6 million) thanks to the Tracker model."
    }, {
      "heading" : "5 Related Work",
      "text" : "Sentence-level Event Extraction. Previous approaches mainly focus on sentence-level event\nextraction. Chen et al. (2015) propose a neural pipeline model that identifies triggers first and then extracts argument roles. Nguyen et al. (2016) use a joint model to extract triggers and argument roles simultaneously. Some studies also utilize dependency tree information (Liu et al., 2018; Yan et al., 2019). To utilize more knowledge, some studies leverage document context (Chen et al., 2018; Zhao et al., 2018), pre-trained language model (Yang et al., 2019), and explicit external knowledge (Liu et al., 2019a; Tong et al., 2020) such as WordNet (Miller, 1995). Du and Cardie (2020b) also try to extract events in a Question-Answer way. These studies usually conduct experiments on sentencelevel event extraction dataset, ACE05 (Walker et al., 2006). However, it is hard for the sentence-level models to extract multiple qualified events spanning across sentences, which is more common in real-world scenarios.\nDocument-level Event Extraction. Documentlevel EE has attracted more and more attention recently. Yang and Mitchell (2016) use well-defined features to handle the event-argument relations across sentences, which is, unfortunately, quite nontrivial. Yang et al. (2018) extract events from a central sentence and find other arguments from neighboring sentences separately. Although Zheng et al. (2019) use Transformer to fuse sentences and entities, interdependency among events is neglected. Du and Cardie (2020a) try to encode the sentences in a multi-granularity way and Du et al. (2020) leverage a seq2seq model. They conduct experiments on MUC-4 (Sundheim, 1992) dataset with 1, 700 documents and 5 kinds of entity-based arguments, and it is formulated as a table-filling task, coping with single event record of single event\ntype. However, our work is different from these studies in that a) we utilize heterogeneous graph to model the global interactions among sentences and mentions to capture cross-sentence context, b) and we leverage the global interdependency through Tracker to extract multiple event records of multiple event types."
    }, {
      "heading" : "6 Conclusion",
      "text" : "Although promising in practical application, document-level EE still faces some challenges such as arguments-scattering phenomenon and multiple correlated events expressed by a single document. To tackle the challenges, we introduce Heterogeneous Graph-based Interaction Model with a Tracker (GIT). GIT uses a heterogeneous graph interaction network to model global interactions among sentences and entity mentions. GIT also uses a Tracker to track the extracted records to consider global interdependency during extraction. Experiments on large-scale public dataset (Zheng et al., 2019) show GIT outperforms previous stateof-the-art by 2.8 F1. Further analysis verifies the effectiveness of GIT especially in cross-sentence events extraction and multi-event scenarios."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank Changzhi Sun, Mingxuan Wang, and the anonymous reviewers for their thoughtful and constructive comments. This paper is supported in part by the National Key R&D Program of China under Grand No.2018AAA0102003, the National Science Foundation of China under Grant No.61936012 and 61876004."
    }, {
      "heading" : "A Training Details",
      "text" : "To mitigate the error propagation due to the gap between training and inference phrase (i.e., the extracted entities are ground truth during training but predicted results during inference), we adopt scheduled sampling strategy (Bengio et al., 2015) as Zheng et al. (2019) did. We gradually switch the entity extraction results from golden label to what the model predicts on its own. Specifically, from epoch 10 to epoch 20, we linearly increase the proportion of predicted entity results from 0% to 100%. We implement GIT under PyTorch (Paszke et al., 2017) and DGL (Wang et al., 2019) based on codes provided by Zheng et al. (2019).\nAll the experiments (including the baselines) are run with the same 8 Tesla-V100 GPUs and the same version of python dependencies to ensure the fairness.\nHyperparameters trials are listed in Table 6. The value of hyperparameters we finally adopted are in bold. Note that we do not tune all the hyperparameters, and make little effort to select the best hyperparameters for our GIT.\nWe choose the final checkpoints for test according to the Micro F1 performance on the dev set. Table 9 illustrates the best epoch in which the model achieves the highest Micro F1 on the dev set and their according F1 score."
    }, {
      "heading" : "B Additional Evaluation Results",
      "text" : "We have showed the evaluation results of event records extraction in the paper for document-level\nevent extraction. In this section, we also illustate the results of entity extraction in Table. 7 and event types detection in Table. 8. Moreover, the comprehensive results of event record extraction is shown in Table. 10, including results reported in Zheng et al. (2019) with precison, recall and F1 score."
    }, {
      "heading" : "C Complete Document for the Examples",
      "text" : "We show an example document in Figure 1 in the paper. To better illustrate, we translate it from Chinese into English and make some simplication. Here we present the original complete document example in Figure 7. For the specific meanings of argument roles, we recommend readers to refer to (Zheng et al., 2019).\nWe also demonstrate an case study in Figure 5 in the paper. Now we also show its original Chinese version in Figure 6."
    } ],
    "references" : [ {
      "title" : "Scheduled sampling for sequence prediction with recurrent neural networks",
      "author" : [ "Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer." ],
      "venue" : "Proceedings of the 28th International Conference on Neural Information Processing Systems",
      "citeRegEx" : "Bengio et al\\.,? 2015",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2015
    }, {
      "title" : "Event extraction via dynamic multipooling convolutional neural networks",
      "author" : [ "Yubo Chen", "Liheng Xu", "Kang Liu", "Daojian Zeng", "Jun Zhao." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna-",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Collective event detection via a hier",
      "author" : [ "Yubo Chen", "Hang Yang", "Kang Liu", "Jun Zhao", "Yantao Jia" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Document-level event role filler extraction using multi-granularity contextualized encoding",
      "author" : [ "Xinya Du", "Claire Cardie." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Du and Cardie.,? 2020a",
      "shortCiteRegEx" : "Du and Cardie.",
      "year" : 2020
    }, {
      "title" : "Event extraction by answering (almost) natural questions",
      "author" : [ "Xinya Du", "Claire Cardie." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Du and Cardie.,? 2020b",
      "shortCiteRegEx" : "Du and Cardie.",
      "year" : 2020
    }, {
      "title" : "Document-level event-based extraction using generative template-filling transformers",
      "author" : [ "Xinya Du", "Alexander M. Rush", "Claire Cardie." ],
      "venue" : "arXiv preprint arXiv:2008.09249.",
      "citeRegEx" : "Du et al\\.,? 2020",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas N. Kipf", "Max Welling." ],
      "venue" : "5th International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Kipf and Welling.,? 2017",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2017
    }, {
      "title" : "Exploiting the ground-truth: An adversarial imitation based knowledge distillation approach for event detection",
      "author" : [ "Jian Liu", "Yubo Chen", "Kang Liu." ],
      "venue" : "Proceedings of the 33rd AAAI Conference on Artificial Intelligence (AAAI).",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Event detection without triggers",
      "author" : [ "Shulin Liu", "Yang Li", "Feng Zhang", "Tao Yang", "Xinpeng Zhou." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "citeRegEx" : "Liu et al\\.,? 2019b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Jointly multiple events extraction via attentionbased graph information aggregation",
      "author" : [ "Xiao Liu", "Zhunchen Luo", "Heyan Huang." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Wordnet: A lexical database for english",
      "author" : [ "George A. Miller." ],
      "venue" : "Commun. ACM.",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "Joint event extraction via recurrent neural networks",
      "author" : [ "Thien Huu Nguyen", "Kyunghyun Cho", "Ralph Grishman." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).",
      "citeRegEx" : "Nguyen et al\\.,? 2016",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "Automatic differentiation in pytorch",
      "author" : [ "Adam Paszke", "Sam Gross", "Soumith Chintala", "Gregory Chanan", "Edward Yang", "Zachary DeVito", "Zeming Lin", "Alban Desmaison", "Luca Antiga", "Adam Lerer." ],
      "venue" : "NIPS-W.",
      "citeRegEx" : "Paszke et al\\.,? 2017",
      "shortCiteRegEx" : "Paszke et al\\.",
      "year" : 2017
    }, {
      "title" : "Overview of the fourth Message Understanding Evaluation and Conference",
      "author" : [ "Beth M. Sundheim." ],
      "venue" : "Fourth Message Uunderstanding Conference (MUC4).",
      "citeRegEx" : "Sundheim.,? 1992",
      "shortCiteRegEx" : "Sundheim.",
      "year" : 1992
    }, {
      "title" : "Improving event detection via open-domain trigger knowledge",
      "author" : [ "Meihan Tong", "Bin Xu", "Shuai Wang", "Yixin Cao", "Lei Hou", "Juanzi Li", "Jun Xie." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Tong et al\\.,? 2020",
      "shortCiteRegEx" : "Tong et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems (NeurIPS).",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Ace 2005 multilingual training corpus",
      "author" : [ "Christopher Walker", "Stephanie Strassel", "Julie Medero", "Kazuaki Maeda." ],
      "venue" : "Philadelphia: Linguistic Data Consortium.",
      "citeRegEx" : "Walker et al\\.,? 2006",
      "shortCiteRegEx" : "Walker et al\\.",
      "year" : 2006
    }, {
      "title" : "Deep graph li",
      "author" : [ "Minjie Wang", "Lingfan Yu", "Da Zheng", "Quan Gan", "Yu Gai", "Zihao Ye", "Mufei Li", "Jinjing Zhou", "Qi Huang", "Chao Ma", "Ziyue Huang", "Qipeng Guo", "Hao Zhang", "Haibin Lin", "Junbo Zhao", "Jinyang Li", "Alexander J Smola", "Zheng Zhang" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Memory networks",
      "author" : [ "Jason Weston", "Sumit Chopra", "Antoine Bordes." ],
      "venue" : "3rd International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Weston et al\\.,? 2015",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2015
    }, {
      "title" : "Event detection with multi-order graph convolution and aggregated attention",
      "author" : [ "Haoran Yan", "Xiaolong Jin", "Xiangbin Meng", "Jiafeng Guo", "Xueqi Cheng." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Yan et al\\.,? 2019",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2019
    }, {
      "title" : "Joint extraction of events and entities within a document context",
      "author" : [ "Bishan Yang", "Tom M. Mitchell." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).",
      "citeRegEx" : "Yang and Mitchell.,? 2016",
      "shortCiteRegEx" : "Yang and Mitchell.",
      "year" : 2016
    }, {
      "title" : "DCFEE: A document-level Chinese financial event extraction system based on automatically labeled training data",
      "author" : [ "Hang Yang", "Yubo Chen", "Kang Liu", "Yang Xiao", "Jun Zhao." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploring pre-trained language models for event extraction and generation",
      "author" : [ "Sen Yang", "Dawei Feng", "Linbo Qiao", "Zhigang Kan", "Dongsheng Li." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Double graph based reasoning for documentlevel relation extraction",
      "author" : [ "Shuang Zeng", "Runxin Xu", "Baobao Chang", "Lei Li." ],
      "venue" : "Proceedings of the 2020",
      "citeRegEx" : "Zeng et al\\.,? 2020",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2020
    }, {
      "title" : "Scale up event extraction learning via automatic training data generation",
      "author" : [ "Ying Zeng", "Yansong Feng", "Rong Ma", "Zheng Wang", "Rui Yan", "Chongde Shi", "Dongyan Zhao." ],
      "venue" : "Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "Zeng et al\\.,? 2018",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2018
    }, {
      "title" : "Document embedding enhanced event detection with hierarchical and supervised attention",
      "author" : [ "Yue Zhao", "Xiaolong Jin", "Yuanzhuo Wang", "Xueqi Cheng." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Zhao et al\\.,? 2018",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2018
    }, {
      "title" : "Doc2EDAG: An end-to-end document-level framework for Chinese financial event extraction",
      "author" : [ "Shun Zheng", "Wei Cao", "Wei Xu", "Jiang Bian." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th In-",
      "citeRegEx" : "Zheng et al\\.,? 2019",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "Experiments on a large-scale dataset (Zheng et al., 2019) show GIT outperforms the existing best methods by 2.",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "Most previous methods (Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018; Yang et al., 2019; Du and Cardie, 2020b) focus on sentence-level EE, extracting events from a single sentence.",
      "startOffset" : 22,
      "endOffset" : 121
    }, {
      "referenceID" : 12,
      "context" : "Most previous methods (Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018; Yang et al., 2019; Du and Cardie, 2020b) focus on sentence-level EE, extracting events from a single sentence.",
      "startOffset" : 22,
      "endOffset" : 121
    }, {
      "referenceID" : 10,
      "context" : "Most previous methods (Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018; Yang et al., 2019; Du and Cardie, 2020b) focus on sentence-level EE, extracting events from a single sentence.",
      "startOffset" : 22,
      "endOffset" : 121
    }, {
      "referenceID" : 23,
      "context" : "Most previous methods (Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018; Yang et al., 2019; Du and Cardie, 2020b) focus on sentence-level EE, extracting events from a single sentence.",
      "startOffset" : 22,
      "endOffset" : 121
    }, {
      "referenceID" : 4,
      "context" : "Most previous methods (Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018; Yang et al., 2019; Du and Cardie, 2020b) focus on sentence-level EE, extracting events from a single sentence.",
      "startOffset" : 22,
      "endOffset" : 121
    }, {
      "referenceID" : 22,
      "context" : "It has attracted much attention recently (Yang et al., 2018; Zheng et al., 2019; Du and Cardie, 2020a; Du et al., 2020).",
      "startOffset" : 41,
      "endOffset" : 119
    }, {
      "referenceID" : 27,
      "context" : "It has attracted much attention recently (Yang et al., 2018; Zheng et al., 2019; Du and Cardie, 2020a; Du et al., 2020).",
      "startOffset" : 41,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "It has attracted much attention recently (Yang et al., 2018; Zheng et al., 2019; Du and Cardie, 2020a; Du et al., 2020).",
      "startOffset" : 41,
      "endOffset" : 119
    }, {
      "referenceID" : 5,
      "context" : "It has attracted much attention recently (Yang et al., 2018; Zheng et al., 2019; Du and Cardie, 2020a; Du et al., 2020).",
      "startOffset" : 41,
      "endOffset" : 119
    }, {
      "referenceID" : 27,
      "context" : "8 F1 on the large-scale public dataset (Zheng et al., 2019) with 32, 040 documents, especially on crosssentence events and multiple events scenarios (with 3.",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 25,
      "context" : "The task does not require to identify event triggers (Zeng et al., 2018; Liu et al., 2019b), which reduces manual effort of annotation and the application scenarios becomes more extensive.",
      "startOffset" : 53,
      "endOffset" : 91
    }, {
      "referenceID" : 9,
      "context" : "The task does not require to identify event triggers (Zeng et al., 2018; Liu et al., 2019b), which reduces manual effort of annotation and the application scenarios becomes more extensive.",
      "startOffset" : 53,
      "endOffset" : 91
    }, {
      "referenceID" : 7,
      "context" : "After heterogeneous graph construction *, we apply multi-layer Graph Convolution Network (Kipf and Welling, 2017) to model the global interactions inspired by Zeng et al.",
      "startOffset" : 89,
      "endOffset" : 113
    }, {
      "referenceID" : 10,
      "context" : "*Traditional methods in sentence-level EE also utilize graph to extract events (Liu et al., 2018; Yan et al., 2019), based on the dependency tree.",
      "startOffset" : 79,
      "endOffset" : 115
    }, {
      "referenceID" : 20,
      "context" : "*Traditional methods in sentence-level EE also utilize graph to extract events (Liu et al., 2018; Yan et al., 2019), based on the dependency tree.",
      "startOffset" : 79,
      "endOffset" : 115
    }, {
      "referenceID" : 27,
      "context" : "Since a document is likely to express multiple event records and the number of records cannot be known in advance, we decode records by expanding a tree orderly as previous methods did (Zheng et al., 2019).",
      "startOffset" : 185,
      "endOffset" : 205
    }, {
      "referenceID" : 19,
      "context" : "To take advantage of such interdependency, we propose a novel Tracker module inspired by memory network (Weston et al., 2015).",
      "startOffset" : 104,
      "endOffset" : 125
    }, {
      "referenceID" : 16,
      "context" : "In our implementation of GIT, we use 8 and 4 layers Transformer (Vaswani et al., 2017) in encoding and decoding module respectively.",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 27,
      "context" : "The dimensions in hidden layers and feed-forward layers are the same as previous work (Zheng et al., 2019), i.",
      "startOffset" : 86,
      "endOffset" : 106
    }, {
      "referenceID" : 6,
      "context" : "GIT is trained using Adam (Kingma and Ba, 2015) as optimizer with 1e− 4 learning rate for 100 epochs.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 27,
      "context" : "Besides, Doc2EDAG (Zheng et al., 2019) uses transformer encoder to obtain sentence and entity embeddings, followed by another transformer to fuse cross-sentence context.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 10,
      "context" : "Some studies also utilize dependency tree information (Liu et al., 2018; Yan et al., 2019).",
      "startOffset" : 54,
      "endOffset" : 90
    }, {
      "referenceID" : 20,
      "context" : "Some studies also utilize dependency tree information (Liu et al., 2018; Yan et al., 2019).",
      "startOffset" : 54,
      "endOffset" : 90
    }, {
      "referenceID" : 2,
      "context" : "To utilize more knowledge, some studies leverage document context (Chen et al., 2018; Zhao et al., 2018), pre-trained language model (Yang et al.",
      "startOffset" : 66,
      "endOffset" : 104
    }, {
      "referenceID" : 26,
      "context" : "To utilize more knowledge, some studies leverage document context (Chen et al., 2018; Zhao et al., 2018), pre-trained language model (Yang et al.",
      "startOffset" : 66,
      "endOffset" : 104
    }, {
      "referenceID" : 23,
      "context" : ", 2018), pre-trained language model (Yang et al., 2019), and explicit external knowledge (Liu et al.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 8,
      "context" : ", 2019), and explicit external knowledge (Liu et al., 2019a; Tong et al., 2020) such as WordNet (Miller, 1995).",
      "startOffset" : 41,
      "endOffset" : 79
    }, {
      "referenceID" : 15,
      "context" : ", 2019), and explicit external knowledge (Liu et al., 2019a; Tong et al., 2020) such as WordNet (Miller, 1995).",
      "startOffset" : 41,
      "endOffset" : 79
    }, {
      "referenceID" : 17,
      "context" : "These studies usually conduct experiments on sentencelevel event extraction dataset, ACE05 (Walker et al., 2006).",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 14,
      "context" : "They conduct experiments on MUC-4 (Sundheim, 1992) dataset with 1, 700 documents and 5 kinds of entity-based arguments, and it is formulated as a table-filling task, coping with single event record of single event",
      "startOffset" : 34,
      "endOffset" : 50
    }, {
      "referenceID" : 27,
      "context" : "Experiments on large-scale public dataset (Zheng et al., 2019) show GIT outperforms previous stateof-the-art by 2.",
      "startOffset" : 42,
      "endOffset" : 62
    } ],
    "year" : 2021,
    "abstractText" : "Document-level event extraction aims to recognize event information from a whole piece of article. Existing methods are not effective due to two challenges of this task: a) the target event arguments are scattered across sentences; b) the correlation among events in a document is non-trivial to model. In this paper, we propose Heterogeneous Graph-based Interaction Model with a Tracker (GIT) to solve the aforementioned two challenges. For the first challenge, GIT constructs a heterogeneous graph interaction network to capture global interactions among different sentences and entity mentions. For the second, GIT introduces a Tracker module to track the extracted events and hence capture the interdependency among the events. Experiments on a large-scale dataset (Zheng et al., 2019) show GIT outperforms the existing best methods by 2.8 F1. Further analysis reveals GIT is effective in extracting multiple correlated events and event arguments that scatter across the document. Our code is available at https: //github.com/RunxinXu/GIT.",
    "creator" : "LaTeX with hyperref"
  }
}