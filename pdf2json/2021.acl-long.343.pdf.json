{
  "name" : "2021.acl-long.343.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Pre-training Strategy for Zero-Resource Response Selection in Knowledge-Grounded Conversations",
    "authors" : [ "Chongyang Tao", "Changyu Chen", "Jiazhan Feng", "Jirong Wen", "Rui Yan" ],
    "emails" : [ "chongyangtao@pku.edu.cn", "fengjiazhan@pku.edu.cn", "chen.changyu@ruc.edu.cn", "jrwen@ruc.edu.cn", "ruiyan@ruc.edu.cn", "(ruiyan@ruc.edu.cn)." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4446–4457\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4446"
    }, {
      "heading" : "1 Introduction",
      "text" : "Along with the very recent prosperity of artificial intelligence empowered conversation systems in the spotlight, many studies have been focused on building human-computer dialogue systems (Wen et al., 2017; Zhang et al., 2020) with either retrievalbased methods (Wang et al., 2013; Wu et al., 2017;\n∗Equal Contribution. †Corresponding author: Rui Yan (ruiyan@ruc.edu.cn).\nWhang et al., 2020) or generation-based methods (Li et al., 2016; Serban et al., 2016; Zhang et al., 2020), which both predict the response with only the given context. In fact, unlike a person who may associate the conversation with the background knowledge in his or her mind, the machine can only capture limited information from the query message itself. As a result, it is difficult for a machine to properly comprehend the query, and to predict a proper response to make it more engaging. To bridge the gap of the knowledge between the human and the machine, researchers have begun to simulating this motivation by grounding dialogue agents with background knowledge (Zhang et al., 2018; Dinan et al., 2019; Li et al., 2020), and lots of impressive results have been obtained.\nIn this paper, we consider the response selection problem in knowledge-grounded conversion and specify the background knowledge as unstructured documents that are common sources in practice. The task is that given a conversation context and a set of knowledge entries, one is required 1): to select proper knowledge and grasp a good comprehension of the selected document materials (knowledge selection); 2): to distinguish the true response from a candidate pool that is relevant and consistent with both the conversation context and the background documents (knowledge matching).\nWhile there exists a number of knowledge documents on the Web, it is non-trivial to collect large-scale dialogues that are naturally grounded on the documents for training a neural response selection model, which hinders the effective and adequate training of knowledge selection and response matching. Although some benchmarks built upon crowd-sourcing have been released by recent works (Zhang et al., 2018; Dinan et al., 2019), the relatively small training size makes it hard for the dialogue models to generalize on other domains or topics (Zhao et al., 2020). Thus, in this work, we\nfocus on a more challenging and practical scenario, learning a knowledge-grounded conversation agent without any knowledge-grounded dialogue data, which is known as zero-resource settings.\nSince knowledge-grounded dialogues are unavailable in training, it raises greater challenges for learning the grounded response selection model. Fortunately, there exists a large number of unstructured knowledge (e.g., web pages or wiki articles), passage search datasets (e.g., query-passage pairs coming from ad-hoc retrieval tasks) (Khattab and Zaharia, 2020) and multi-turn dialogues (e.g., context-response pairs collected from Reddit) (Henderson et al., 2019), which might be beneficial to the learning of knowledge comprehension, knowledge selection and response prediction respectively. Besides, in multi-turn dialogues, the background knowledge and conversation history (excluding the latest query) are symmetric in terms of the information they convey, and we assume that the dialogue history can be regarded as another format of background knowledge for response prediction.\nBased on the above intuition, in this paper, we consider decomposing the training of the grounded response selection task into several sub-tasks, and joint learning all those tasks in a unified model. To take advantage of the recent breakthrough on pretraining for natural language tasks, we build the grounded response matching models on the basis of a pre-trained language model (PLMs) (Devlin et al., 2019; Yang et al., 2019), which are trained with large-scale unstructured documents from the web. On this basis, we further train the PLMs with query-passage matching task, query-dialogue history matching task, and multi-turn response matching task jointly. The former two tasks could help the model not only in knowledge selection but also in knowledge (and dialogue history) comprehension, while the last task is designed for matching the proper response with the given query and background knowledge (dialogue history). By this means, the model can be learned to select relevant knowledge and distinguish proper responses, with the help of a large number of ungrounded dialogues and ad-hoc retrieval corpora. During the testing stage, we first utilize the trained model to select proper knowledge, and then feed the query, dialogue history, selected knowledge, and the response candidate into our model to calculate the final matching degree. Particularly, we design two strategies to compute the final matching score.\nIn the first strategy, we directly concatenate the selected knowledge and dialogue history as a long sequence of background knowledge and feed into the model. In the second strategy, we first compute the matching degree between each queryknowledge and the response candidates, and then integrate all matching scores.\nWe conduct experiments with benchmarks of knowledge-grounded dialogue that are constructed by crowd-sourcing, such as the Wizard-ofWikipedia Corpus (Dinan et al., 2019) and the CMU DoG Corpus (Zhou et al., 2018a). Evaluation results indicate that our model achieves comparable performance on knowledge selection and response selection with several existing models trained on crowd-sourced benchmarks.\nOur contributions are summarized as follows: • To the best of our knowledge, this is the first\nexploration of knowledge-grounded response selection under the zero-resource setting. • We propose decomposing the training of the grounded response selection models into several sub-tasks, so as to empower the model through these tasks in knowledge selection and response matching. • We achieve a comparable performance of response selection with several existing models learned from crowd-sourced training sets."
    }, {
      "heading" : "2 Related Work",
      "text" : "Early studies of retrieval-based dialogue focus on single-turn response selection where the input of a matching model is a message-response pair (Wang et al., 2013; Ji et al., 2014; Wang et al., 2015). Recently, researchers pay more attention to multiturn context-response matching and usually adopt the representation-matching-aggregation paradigm to build the model. Representative methods include the dual-LSTM model (Lowe et al., 2015), the sequential matching network (SMN) (Wu et al., 2017), the deep attention matching network (DAM) (Zhou et al., 2018b), interaction-overinteraction network (IoI) (Tao et al., 2019) and multi-hop selector network (MSN) (Yuan et al., 2019). More recently, pre-trained language models (Devlin et al., 2019; Yang et al., 2019) have shown significant benefits for various NLP tasks, and some researchers have tried to apply them on multi-turn response selection. Vig and Ramea (2019) exploit BERT to represent each utteranceresponse pair and fuse these representations to\ncalculate the matching score; Whang et al. (2020) and Xu et al. (2020) treat the context as a long sequence and conduct context-response matching with BERT. Besides, Gu et al. (2020a) integrate speaker embeddings into BERT to improve the utterance representation in multi-turn dialogue.\nTo bridge the gap of the knowledge between the human and the machine, researchers have investigated into grounding dialogue agents with unstructured background knowledge (Ghazvininejad et al., 2018; Zhang et al., 2018; Dinan et al., 2019). For example, Zhang et al. (2018) build a persona-based conversation data set that employs the interlocutor’s profile as the background knowledge; Zhou et al. (2018a) publish a data where conversations are grounded in articles about popular movies; Dinan et al. (2019) release another documentgrounded data with Wiki articles covering a wide range of topics. Meanwhile, several retrievalbased knowledge-grounded dialogue models are proposed, such as document-grounded matching network (DGMN) (Zhao et al., 2019) and dually interactive matching network (DIM) (Gu et al., 2019) which let the dialogue context and all knowledge entries interact with the response candidate respectively via the cross-attention mechanism. Gu et al. (2020b) further propose to pre-filter the context and the knowledge and then use the filtered context and knowledge to perform the matching with the response. Besides, with the help of gold knowledge index annotated by human wizards, Dinan et al. (2019) consider joint learning the knowledge selection and response matching in a multi-task manner or training a two-stage model."
    }, {
      "heading" : "3 Model",
      "text" : "In this section, we first formalize the knowledgegrounded response matching problem and then introduce our method from preliminary to response matching with PLMs to details of three pre-training tasks."
    }, {
      "heading" : "3.1 Problem Formalization",
      "text" : "We first describe a standard knowledge-grounded response selection task such as Wizard-ofWikipedia. Suppose that we have a knowledgegrounded dialogue data set D = {ki, ci, ri, yi}Ni=1 where ki = {p1, p2, . . . , plk} represents a collection of knowledge with pj the j-th knowledge entry (a.k.a., passage) and lk is the number of entries; ci = {u1, u2, . . . , ulc} denotes\nmulti-turn dialogue context with uj the j-th turn and lc is the number of dialogue turns. It should be noted that in this paper we denote the latest turn ulc as dialogue query qi, and dialogue context except for query is denoted as hi = ci/{qi}. ri stands for a candidate response. yi = 1 indicates that ri is a proper response for ci and ki, otherwise yi = 0. N is the number of samples in data set. The goal knowledge-grounded dialogue is to learn a matching model g(k, c, r) from D, and thus for any new (k, c, r), g(k, c, r) returns the matching degree between r and (k, c). Finally, one can collect the matching scores of a series of candidate responses and conduct response ranking.\nZero-resource grounded response selection then is formally defined as follows. There is a standard multi-turn dialogue dataset Dc = {qi, hi, ri}Ni=1 and an ad-hoc retrieval datasetDp = {qi, pi, zi}Mi=1 where qi is a query and pi stands a candidate passage, zi = 1 indicates that pi is a relevant passage for qi, otherwise zi = 0. Our goal is to learn a model g(k, h, q, r) from Dc and Dp, and thus for any new input (k, h, q, r), our model can select proper knowledge k̂ from k and calculate the matching degree between r and (k̂, q, h)."
    }, {
      "heading" : "3.2 Preliminary: Response Matching with PLMs",
      "text" : "Pre-trained language models have been widely used in many NLP tasks due to the strong ability of language representation and understanding. In this work, we consider building a knowledge-grounded response matching model with BERT.\nSpecifically, given a query q, a dialogue history h = {u1, u2, ..., unh} where ui is the i-th turn in the history, a response candidate r = {r1, r2, ..., rlr} with lr words, we concatenate all sequences as a single consecutive tokens sequence with special tokens, which can be represented as x = {[CLS], u1, [SEP], . . . , [SEP], ulh , [SEP], q, [SEP], r, [SEP]}. [CLS] and [SEP] are classification symbol and segment separation symbol respectively. For each token in x, BERT uses a summation of three kinds of embeddings, including WordPiece embedding (Wu et al., 2016), segment embedding, and position embedding.\nThen, the embedding sequence of x is fed into BERT, giving us the contextualized embedding sequence {E[CLS], E2, . . . , Elx}. E[CLS] is an aggregated representation vector that contains the\nsemantic interaction information between the query, history, and response candidate. Finaly, E[CLS] is fed into a non-linear layer to calculate the final matching score, which is formulated as:\ng(h, q, r) = σ(W2 · tanh(W1E[CLS] + b1) + b2) (1)\nwhere W{1,2} and b{1,2} is training parameters for response selection task, σ is a sigmoid function.\nIn knowledge-grounded dialogue, each dialogue is associated with a large collection of knowledge entries k = {p1, p2, . . . , plk}1. The model is required to select m(m ≥ 1) knowledge entries based on semantic relevance between the query and each knowledge, and then performs the response matching with the query, dialogue history and the highly-relevant knowledge. Specifically, we denote k̂ = (p̂1, . . . , p̂m) as the selected knowledge entries, and feed the input sequence x = {[CLS], p̂1, [SEP], . . . , [SEP], p̂m, [SEP], u1, [SEP], . . . , [SEP], ulh , [SEP], q, [SEP], r, [SEP]} to BERT. The final matching score g(k̂, h, q, r) can be computed based on [CLS] representation."
    }, {
      "heading" : "3.3 Pre-training Strategies",
      "text" : "On the basis of BERT, we further jointly train it with three tasks including 1) query-passage matching task; 2) query-dialogue history matching task; 3) multi-turn response matching task. The former two tasks could help the model in knowledge selection and knowledge (and dialogue history) comprehension, while the last task is designed for matching the proper response with the given query and background knowledge (dialogue\n1The scale of the knowledge referenced by each dialogue usually exceeds the limitation of input length in PLMs.\nhistory). By this means, the model can be learned to select relevant knowledge and distinguish the proper response, with the help of a large number of ungrounded dialogues and ad-hoc retrieval corpora."
    }, {
      "heading" : "3.3.1 Query-Passage Matching",
      "text" : "Although there exist a huge amount of conversation data on social media, it is hard to collect sufficient dialogues that are naturally grounded on knowledge documents. Existing studies (Dinan et al., 2019) usually extract the relevant knowledge before the response matching or jointly train the knowledge retrieval and response selection in a multi-task manner. However, both methods need in-domain knowledge-grounded dialogue data (with gold knowledge label) to train, making the model hard to generalize to a new domain. Fortunately, the ad-hoc retrieval task (Harman, 2005; Khattab and Zaharia, 2020) in the information retrieval area provides a potential solution to simulate the process of knowledge seeking. To take advantage of the parallel data in the ad-hoc retrieval task, we consider incorporating the query-passage matching task, so as to help the knowledge selection and knowledge comprehension for our task.\nGiven a query-passage pair (q, p), we first concatenate the query q and the passage p as a single consecutive token sequence with special tokens separating them, which is formulated as:\nSqp = {[CLS], wp1 , . . . , w p np ,[SEP], w q 1, . . . , w q nq} (2)\nwhere wpi , w q j denotes the i-th and j-th token of knowledge entry p and query q respectively. For each token in Sqpi , token, segment and position\nembeddings are summated and fed into BERT. It is worth noting that here we set the segment embedding of the knowledge to be the same as the dialogue history. Finally, we feed the output representation of [CLS] Eqp[CLS] into a MLP to obtain the final query-passage matching score g(q, p). The loss function of each training sample for query-passage matching task is defined by\nLp(q, p+, p−1 , . . . , p − np)\n=− log( e g(q,p+) eg(q,p+) + ∑δp j=1 e g(q,p−j ) )\n(3)\nwhere p+ stands for the positive passage for q, p−j is the j-th negative passage and δp is the number of negative passage."
    }, {
      "heading" : "3.3.2 Query-Dialogue History Matching",
      "text" : "In multi-turn dialogues, the conversation history (excluding the latest query) is a piece of supplementary information for the current query and can be regarded as another format of background knowledge during the response matching. Besides, due to the natural sequential relationship between dialogue turns, the dialogue query usually shows a strong semantic relevance with the previous turns in the dialogue history. Inspired by such characteristics, we design a query-dialogue history matching task with the multi-turn dialogue context, so as to enhance the capability of the model to comprehend the dialogue history with the given dialogue query and to rank relevant passages with these pseudo query-passage pairs.\nSpecifically, we first concatenate the dialogue history into a long sequence. The task requires the model to predict whether a query q = {wq1, . . . , w q nq} and a dialogue history sequence h = {wh1 , . . . , whnh} are consecutive and relevant. We concatenate two sequences into a single consecutive sequence with [SEP] tokens,\nSqh = {[CLS], wh1 , . . . , whnh ,[SEP], w q 1, . . . , w q nq} (4)\nFor each word in Sqh, token, segment and position embeddings are summated and fed into BERT. Finally, we feed Eqh[CLS] into a MLP to obtain the final query-history matching score g(q, h). The loss function of each training sample for queryhistory matching task is defined by\nLh(q, h+, h−1 , . . . , h − nh)\n=− log( e g(q,h+) eg(q,h+) + ∑δh j=1 e g(q,h−j ) )\n(5)\nwhere h+ stands for the true dialogue history for q, h−j is the j-th negative dialogue history randomly sampled from the training set and δh is the number of sampled dialogue history."
    }, {
      "heading" : "3.3.3 Multi-turn Response Matching",
      "text" : "The above two tasks are designed for empowering the model to knowledge or history comprehension and knowledge selection. In this task, we aim at training the model to match reasonable responses based on dialogue history and query. Since we treat the dialogue history as a special form of background knowledge and they share the same segment embeddings in the PLMs, our model can acquire the ability to identify the proper response with either dialogue history or the background knowledge through the multi-turn response matching task.\nSpecifically, we format the multi-turn dialogues as query-history-response triples and requires the model to predict whether a response candidate r = {wr1, . . . , wrnr} is appropriate for a given query q = {wq1, . . . , w q nq} and a concatenated dialogue history sequence h = {wh1 , . . . , whnh}. Concretely, we concatenate three input sequences into a single consecutive tokens sequence with [SEP] tokens,\nShqr = {[CLS], wh1 , . . . , whnh ,[SEP], wq1, . . . , w q nq ,[SEP], w r 1, . . . , w r nr}\n(6)\nSimilarly, we feed an embedding sequence of which each entry is a summation of token, segment and position embeddings into BERT. Finally, we feedEhqr[CLS] into a MLP to obtain the final response matching score g(h, q, r).\nThe loss function of each training sample for multi-turn response matching task is defined by\nLr(h, q, r+, r−1 , . . . , r − δr )\n=− log( e g(h,q,r+) eg(h,q,r+) + ∑nr i=j e g(h,q,r−j ) )\n(7)\nwhere r+ is the true response for a given q and h, r−j is the j-th negative response candidate randomly sampled from the training set and δr is the number of negative response candidate."
    }, {
      "heading" : "3.3.4 Joint Learning",
      "text" : "We adopt a multi-task learning manner and define the final objective function as:\nLfinal = Lp + Lh + Lr (8)\nIn this way, all tasks are jointly learned so that the model can effectively leverage two training\ncorpus and learn to select relevant knowledge and distinguish the proper response."
    }, {
      "heading" : "3.4 Calculating Matching Score",
      "text" : "After learning model from Dc and Dp, we first rank {pi}nki=1 according to g(q, ki) and then select top m knowledge entries {p1, . . . , pm} for the subsequent response matching process. Here we design two strategies to compute the final matching score g(k, h, q, r). In the first strategy, we directly concatenate the selected knowledge and dialogue history as a long sequence of background knowledge and feed into the model to obtain the final matching score, which is formulated as,\ng(k, h, q, r) = g(p1 ⊕ . . .⊕ pm ⊕ c, q, r) (9)\nwhere ⊕ denotes the concatenation operation. In the second strategy, we treat each selected knowledge entry and the dialogue history equally as the background knowledge, and compute the matching degree between each query, background knowledge, and the response candidates with the trained model. Consequently, the matching score is defined as an integration of a set of knowledgegrounded response matching scores, formulated as,\ng(k, h, q, r) = g(h, q, r)+ max i∈(0,m) g(pi, q, r) (10)\nwhere m is the number of selected knowledge entries. We name our model with the two strategies as PTKGCcat and PTKGCsep respectively. We compare the two learning strategies through empirical studies, as will be reported in the next section."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets and Evaluation Metrics",
      "text" : "Training Set. We adopt MS MARCO passage ranking dataset (Nguyen et al., 2016) built on Bing’s search for query-passage matching task. The dataset contains 8.8M passages from Web pages gathered from Bing’s results to real-world queries and each passage contains an average of 55 words. Each query is associated with sparse relevance judgments of one (or very few) passage marked as relevant. The training set contains about 500k pairs of query and relevant passage, and another 400M pairs of query and passages that have not been marked as relevant, from which the negatives are sampled in our task.\nFor the query-dialogue history matching task and multi-turn response matching task, we use the multi-turn dialogue corpus constructed from the Reddit (Dziri et al., 2018). The dataset contains more than 15 million dialogues and each dialogue has at least 3 utterances. After the pre-processing, we randomly sample 2.28M/20K dialogues as the training/validation set. For each dialogue session, we regard the last turn as the response, the last but one as the query, and the rest as the positive dialogue history. The negative dialogue histories are randomly sampled from the whole dialogue set. On average, each dialogue contains 4.3 utterances, and the average length of the utterances is 42.5.\nTest Set. We tested our proposed method on the Wizard-of-Wikipedia (WoW) (Dinan et al., 2019) and CMU DoG (Zhou et al., 2018a). Both datasets contain multi-turn dialogues grounded on a set of background knowledge and are built with crowd-sourcing on Amazon Mechanical Turk. In WoW, the given knowledge collection is obtained from Wikipedia and covers a wide range of topics or domains, while in CMU DoG, the underlying knowledge focuses on the movie domain. Unlike CMU DoG where the golden knowledge index for each turn is unknown, the golden knowledge index for each turn is provided in WoW. Two configurations (e.g., test-seen and test-unseen) are provided in WoW. Following existing works (Dinan et al., 2019; Zhao et al., 2019), positive responses are true responses from humans and negative ones are randomly sampled. The ratio between positive and negative responses is 1 : 99 for WoW and 1 : 19 for CMU DoG. More details of the two benchmarks are shown in Appendix A.1.\nEvaluation Metrics. Following previous works on knowledge-grounded response selection (Gu et al., 2020b; Zhao et al., 2019), we also employ recall n at k Rn@k (where n = 100 for WoW and n = 20 for CMU DoG and k = {1, 2, 5}) as the evaluation metrics."
    }, {
      "heading" : "4.2 Implementation Details",
      "text" : "Our model is implemented by PyTorch (Paszke et al., 2019). Without loss of generality, we select English uncased BERTbase (110M) as the matching model. During the training, the maximum lengths of the knowledge (a.k.a., passage), the dialogue history, the query, and the response candidate were set to 128, 120 60, and 40. Intuitively, the last tokens in the dialogue history and the previous\ntokens in the query and response candidate are more important, so we cut off the previous tokens for the context but do the cut-off in the reverse direction for the query and response candidate if the sequences are longer than the maximum length. We set a batch size of 32 for multi-turn response matching and query-dialogue history matching, and 8 for query-document matching in order to train these tasks jointly under the circumstance of training examples inequality. We set δp = 6, δh = 1 and δr = 12 for the query-passage matching, the query-dialogue history matching and the multiturn response matching respectively. Particularly, the negative dialogue histories are sampled from other training instances in a batch. The model is optimized using Adam optimizer with a learning rate set as 5e − 6. The learning rate is scheduled by warmup and linear decay. A dropout rate of 0.1 is applied for all linear transformation layers. The gradient clipping threshold is set as 10.0. Early stopping on the corresponding validation data is adopted as a regularization strategy. During the testing, we vary the number of selected knowledgeentries m ∈ {1, . . . , 15} and set m = 2 for PTKGCcat and setm = 14 for PTKGCsep because they achieve the best performance."
    }, {
      "heading" : "4.3 Baselines",
      "text" : "Since the characteristics of the two data sets are different (only WoW provides the golden knowledge label), we compare the proposed model with the baselines on both data sets individually.\nBaselines on WoW. 1) IR Baseline (Dinan et al., 2019) uses simple word overlap for response selection; 2) BoW MemNet (Dinan et al., 2019) is a memory network where knowledge entries are embedded via bag-of-words representation, and the model learns the knowledge selection and response matching jointly; 3) Transformer MemNet (Dinan et al., 2019) is an extension of BoW MemNet,\nand the dialogue history, response candidate and knowledge entries are encoded with Transformer encoder (Vaswani et al., 2017) pre-trained on a large data set. 4) Two-stage Transformer (Dinan et al., 2019) trains two separately models for knowledge selection and response retrieval respectively. A best-performing model on the knowledge selection task is used for the dialogue retrieval task.\nBaselines on CMU DoG 1) Starspace (Wu et al., 2018) selects the response by the cosine similarity between a concatenated sequence of dialogue context, knowledge, and the response candidate represented by StarSpace (Wu et al., 2018); 2) BoW MemNet (Zhang et al., 2018) is a memory network with the bag-of-words representation of knowledge entries as the memory items; 3) KV Profile Memory (Zhang et al., 2018) is a key-value memory network grounded on knowledge profiles; 4) Transformer MemNet (Mazaré et al., 2018) is similar to BoW MemNet and all utterances are encoded with a pre-trained Transformer; 5) DGMN (Zhao et al., 2019) lets the dialogue context and all knowledge entries interact with the response candidate respectively via the cross-attention; 6) DIM (Gu et al., 2019) is similar to DGMN and all utterance are encoded with BiLSTMs; 7) FIRE (Gu et al., 2020b) first filters the context and knowledge and then use the filtered context and knowledge to perform the iterative response matching process."
    }, {
      "heading" : "4.4 Evaluation Results",
      "text" : "Performance of Response Selection. Table 1 and Table 2 report the evaluation results of response selection on WoW and CMU DoG where PTKGCcat and PTKGCsep represent the final matching score computed with the first strategy (Equation 9) and the second strategy (Equation 10) respectively. We can see that PTKGCsep is\nconsistently better than PTKGCcat over all metrics on two data sets, demonstrating that individually representing each knowledge-query-response triple with BERT can lead to a more optimal matching signal than representing a single long sequence. Our explanation to the phenomenon is that there is information loss when a long sequence composed of the knowledge and dialogue history passes through the deep architecture of BERT. Thus, the earlier different knowledge entries and dialogue history are fused together, the more information of dialogue history or background knowledge will be lost in matching. Particularly, on the WoW, in terms of R@1, our PTKGCsep achieves a comparable performance with the existing stateof-the-art models that are learned from the crowdsourced training set, indicating that the model can effectively learn how to leverage external knowledge feed for response selection through the proposed pre-training approach.\nNotably, we can observe that our PTKGCsep performs worse than DIM and FIRE on the CMU DoG. Our explanation to the phenomenon is that the dialogue and knowledge in CMU DoG focus on the movie domain while our train data including ad-hoc retrieval corpora and multi-turn\ndialogues come from the open domain. Thus, our model may not select proper knowledge entries and can not well recognize the semantics clues for response matching due to the domain shift. Despite this, PTKGCsep can still show better performance than several existing models, such as Transformer MemNet and DGMN, though PTKGCsep does not access any training examples in the benchmarks.\nPerformance of Knowledge Selection. We also assess the ability of models to predict the knowledge selected by human wizards in WoW data. The results are shown in Table 4. We can find that the performance of our method is comparable with various supervised methods trained on the gold knowledge index. In particular, on the testseen, our model is slightly worse than Transformer (w/ pretrain), while on the test-unseen, our model achieves slightly better results. The results demonstrate the advantages of our pretraining tasks and the good generalization ability of our model."
    }, {
      "heading" : "4.5 Discussions",
      "text" : "Ablation Study. We conduct a comprehensive ablation study to investigate the impact of different inputs and different tasks. First, we remove the dialogue history, knowledge, and both of them from the model, which is denoted as PTKGCsep(q+k), PTKGCsep(q+h) and PTKGCsep(q) respectively. According to the results of the first four rows in Table 3, we can find that both the dialogue history and knowledge are crucial for response selection as removing anyone will generally cause a performance drop on the two data. Besides, the background knowledge is more critical for response selection as removing the background knowledge causes more significant performance degradation than removing the dialogue history.\nThen, we remove each training task individually from PTKGCsep, and denote the models\nas PTKGCsep-X, where X ∈ {Lp, Lh} meaning query-passage matching task and query-dialogue history matching task respectively. Table 4 shows the ablation results of knowledge selection. We can find that both tasks are useful in the learning of knowledge selection, and query-passage matching plays a dominant role since the performance of knowledge selection drops dramatically when the task is removed from the pre-training process. The last two rows in Table 3 show the ablation results of response selection. We report the ablation results when only 1 knowledge is provided since the knowledge recalls for different ablated models and the full model are very close when m is large (m = 14). We can see that both tasks are helpful and the performance of response selection drops more when removing the query-passage matching task. Particularly, Lp plays a more important role and the performance on test-unseen of WoW drops more obvious when removing each training task.\nTo further investigate the impact of our pretraining tasks on the performance of the multiturn response selection (without considering the grounded knowledge), we conduct an ablation study and the results are shown in Table 5. We can observe that the performance of the response matching model (no grounded knowledge) drops obviously when removing one of the pretraining tasks or both tasks. Particularly, the query-passage matching task contributes more to the response selection.\nThe impact of the number of selected knowledge. We further study how the number of selected knowledge (m) influences the performance of PTKGCsep. Figure 2 shows how the performance of our model changes with respect to different numbers of selected knowledge. We observe that the performance increases monotonically until the knowledge number reaches a certain value, and then stable when the number keeps increasing. The results are rational because more knowledge entries can provide more useful\ninformation for response matching, but when the knowledge becomes enough, the noise will be brought to matching."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we study response matching in knowledge-grounded conversations under a zeroresource setting. In particular, we propose decomposing the training of the knowledge-grounded response selection into three tasks and joint train all tasks in a unified pre-trained language model. Our model can be learned to select relevant knowledge and distinguish proper response, with the help of ad-hoc retrieval corpora and amount of multiturn dialogues. Experimental results on two benchmarks indicate that our model achieves a comparable performance with several existing methods trained on crowd-sourced data. In the future, we would like to explore the ability of our proposed method in retrieval-augmented dialogues."
    }, {
      "heading" : "Acknowledgement",
      "text" : "We would like to thank the anonymous reviewers for their constructive comments. This work was supported by the National Key Research and Development Program of China (No. 2020YFB1406702), the National Science Foundation of China (NSFC No. 61876196) and Beijing Outstanding Young Scientist Program (No. BJJWZYJH012019100020098). Rui Yan is the corresponding author, and is supported as a young fellow at Beijing Academy of Artificial Intelligence (BAAI)."
    }, {
      "heading" : "A Appendices",
      "text" : "A.1 Details of Test Sets\nWe tested our proposed method on the Wizardof-Wikipedia (WoW) (Dinan et al., 2019) and CMU DoG (Zhou et al., 2018a). Both datasets contain multi-turn dialogues grounded on a set of background knowledge and are built with crowdsourcing on Amazon Mechanical Turk.\nIn the WoW dataset, one of the paired speakers is asked to play the role of a knowledgeable expert with access to the given knowledge collection obtained from Wikipedia, while the other of a curious learner. The dataset consists of 968 complete knowledge-grounded dialogues for testing. It is worth noting that the golden knowledge index for each turn is available in the dataset. Response selection is performed at every turn of a complete dialogue, which results in 7512 for testing in total. Following the setting of the original paper, positive responses are true responses from humans and negative ones are randomly sampled. The ratio between positive and negative responses is 1 : 99 in testing sets. Besides, the test set is divided into two subsets: Test Seen and Test Unseen. The former shares 533 common topics with the training set, while the latter contains 58 new topics uncovered by the training or validation set.\nThe CMU DoG data contains knowledgegrounded human-human conversations where the underlying knowledge comes from wiki articles and focuses on the movie domain. Similar to Dinan et al. (2019), the dataset was also built in two scenarios. In the first scenario, only one worker can access the provided knowledge collections, and he/she is responsible for introducing the movie to the other worker; while in the second scenario, both workers know the knowledge and they are asked to discuss the content. Different from WoW, the golden knowledge index for each turn is unknown for both scenarios. Since the data size for an individual scenario is small, we merge the data of the two scenarios following the setting with Zhao et al. (2019). Finally, there\nare 537 dialogues for testing. We evaluate the performance of the response selection at every turn of a dialogue, which results in 6637 samples for testing. We adopted the version shared in Zhao et al. (2019), where 19 negative candidates were randomly sampled for each utterance from the same set. More details about the two benchmarks can be seen in Table 6.\nA.2 Baselines for Knowledge Selection To compare the performance of knowledge selection, we choose the following baselines from Dinan et al. (2019) including (1) Random: the model randomly selects a knowledge entry from a set of knowledge entries; (2) IR Baseline: the model uses simple word overlap between the dialogue context and the knowledge entry to select the relevant knowledge; (3) BoW MemNet: the model is based on memory network where each memory item is a bag-of-words representation of a knowledge entry, and the gold knowledge labels for each turn are used to train the model; (4) Transformer: the model trains a context-knowledge matching network based on Transformer architecture; (5) Transformer (w/ pretrain): the model is similar to the former model, but the transformer is pre-trained on Reddit data and fine-tuned for the knowledge selection task.\nA.3 Results of Low-Resource Setting\nAs an additional experiment, we also evaluate the proposed model for a low-resource setting. We randomly sample t ∈ {10%, 50%, 100%} portion of training data from WoW, and use the data to finetune our model. The results are shown in Table 7. We can find that with only 10% training data, our model can significantly outperform existing models, indicating the advantages of our pretraining tasks. With 100% training data, our model can achieve 2.7% improvement in terms of R@1 on the test-seen and 4.7% improvement on the testunseen."
    } ],
    "references" : [ {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Wizard of wikipedia: Knowledge-powered conversational agents",
      "author" : [ "Emily Dinan", "Stephen Roller", "Kurt Shuster", "Angela Fan", "Michael Auli", "Jason Weston." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Dinan et al\\.,? 2019",
      "shortCiteRegEx" : "Dinan et al\\.",
      "year" : 2019
    }, {
      "title" : "Augmenting neural response generation with context-aware topical attention",
      "author" : [ "Nouha Dziri", "Ehsan Kamalloo", "Kory W Mathewson", "Osmar R Zaiane." ],
      "venue" : "arXiv preprint arXiv:1811.01063.",
      "citeRegEx" : "Dziri et al\\.,? 2018",
      "shortCiteRegEx" : "Dziri et al\\.",
      "year" : 2018
    }, {
      "title" : "A knowledge-grounded neural conversation model",
      "author" : [ "Marjan Ghazvininejad", "Chris Brockett", "Ming-Wei Chang", "Bill Dolan", "Jianfeng Gao", "Wen-tau Yih", "Michel Galley." ],
      "venue" : "The Thirty-Second AAAI Conference on Artificial Intelligence, pages 5110–",
      "citeRegEx" : "Ghazvininejad et al\\.,? 2018",
      "shortCiteRegEx" : "Ghazvininejad et al\\.",
      "year" : 2018
    }, {
      "title" : "Speaker-aware bert for multi-turn response selection in retrieval-based chatbots",
      "author" : [ "Jia-Chen Gu", "Tianda Li", "Quan Liu", "Zhen-Hua Ling", "Zhiming Su", "Si Wei", "Xiaodan Zhu." ],
      "venue" : "Proceedings of the 29th ACM International Conference on Information",
      "citeRegEx" : "Gu et al\\.,? 2020a",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2020
    }, {
      "title" : "Dually interactive matching network for personalized response selection in retrieval-based chatbots",
      "author" : [ "Jia-Chen Gu", "Zhen-Hua Ling", "Xiaodan Zhu", "Quan Liu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Gu et al\\.,? 2019",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2019
    }, {
      "title" : "Filtering before iteratively referring for knowledge-grounded response selection in retrieval-based chatbots",
      "author" : [ "Jia-Chen Gu", "Zhenhua Ling", "Quan Liu", "Zhigang Chen", "Xiaodan Zhu." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP",
      "citeRegEx" : "Gu et al\\.,? 2020b",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2020
    }, {
      "title" : "The trec ad hoc experiments",
      "author" : [ "Donna K Harman" ],
      "venue" : null,
      "citeRegEx" : "Harman.,? \\Q2005\\E",
      "shortCiteRegEx" : "Harman.",
      "year" : 2005
    }, {
      "title" : "A repository of conversational datasets",
      "author" : [ "Matthew Henderson", "Paweł Budzianowski", "Iñigo Casanueva", "Sam Coope", "Daniela Gerz", "Girish Kumar", "Nikola Mrkšić", "Georgios Spithourakis", "Pei-Hao Su", "Ivan Vulić", "Tsung-Hsien Wen." ],
      "venue" : "In",
      "citeRegEx" : "Henderson et al\\.,? 2019",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2019
    }, {
      "title" : "An information retrieval approach to short text conversation",
      "author" : [ "Zongcheng Ji", "Zhengdong Lu", "Hang Li." ],
      "venue" : "arXiv preprint arXiv:1408.6988.",
      "citeRegEx" : "Ji et al\\.,? 2014",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2014
    }, {
      "title" : "Colbert: Efficient and effective passage search via contextualized late interaction over bert",
      "author" : [ "Omar Khattab", "Matei Zaharia." ],
      "venue" : "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages",
      "citeRegEx" : "Khattab and Zaharia.,? 2020",
      "shortCiteRegEx" : "Khattab and Zaharia.",
      "year" : 2020
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Zero-resource knowledge-grounded dialogue generation",
      "author" : [ "Linxiao Li", "Can Xu", "Wei Wu", "Yufan Zhao", "Xueliang Zhao", "Chongyang Tao." ],
      "venue" : "Proceedings of the 34th Conference on Neural Information Processing Systems.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "The Ubuntu dialogue corpus: A large dataset for research in unstructured multiturn dialogue systems",
      "author" : [ "Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau." ],
      "venue" : "Proceedings of the 16th Annual Meeting of the Special Interest Group on",
      "citeRegEx" : "Lowe et al\\.,? 2015",
      "shortCiteRegEx" : "Lowe et al\\.",
      "year" : 2015
    }, {
      "title" : "Training millions of personalized dialogue agents",
      "author" : [ "Pierre-Emmanuel Mazaré", "Samuel Humeau", "Martin Raison", "Antoine Bordes." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Mazaré et al\\.,? 2018",
      "shortCiteRegEx" : "Mazaré et al\\.",
      "year" : 2018
    }, {
      "title" : "Ms marco: A human generated machine reading comprehension dataset",
      "author" : [ "Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng." ],
      "venue" : "CoCo@ NIPS.",
      "citeRegEx" : "Nguyen et al\\.,? 2016",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "Pytorch: An imperative style, high-performance deep learning library",
      "author" : [ "Adam Paszke", "Sam Gross", "Francisco Massa", "Adam Lerer", "James Bradbury", "Gregory Chanan", "Trevor Killeen", "Zeming Lin", "Natalia Gimelshein", "Luca Antiga" ],
      "venue" : null,
      "citeRegEx" : "Paszke et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Paszke et al\\.",
      "year" : 2019
    }, {
      "title" : "Building end-to-end dialogue systems using generative hierarchical neural network models",
      "author" : [ "Iulian Vlad Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron C Courville", "Joelle Pineau." ],
      "venue" : "Proceedings of the Thirtieth AAAI Conference on",
      "citeRegEx" : "Serban et al\\.,? 2016",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "One time of interaction may not be enough: Go deep with an interaction-over-interaction network for response selection in dialogues",
      "author" : [ "Chongyang Tao", "Wei Wu", "Can Xu", "Wenpeng Hu", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "Proceedings of the 57th",
      "citeRegEx" : "Tao et al\\.,? 2019",
      "shortCiteRegEx" : "Tao et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30. Curran Associates,",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Comparison of transfer-learning approaches for response selection in multi-turn conversations",
      "author" : [ "Jesse Vig", "Kalai Ramea." ],
      "venue" : "Workshop on DSTC7.",
      "citeRegEx" : "Vig and Ramea.,? 2019",
      "shortCiteRegEx" : "Vig and Ramea.",
      "year" : 2019
    }, {
      "title" : "A dataset for research on shorttext conversations",
      "author" : [ "Hao Wang", "Zhengdong Lu", "Hang Li", "Enhong Chen." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 935–945. Association",
      "citeRegEx" : "Wang et al\\.,? 2013",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2013
    }, {
      "title" : "Syntax-based deep matching of short texts",
      "author" : [ "Mingxuan Wang", "Zhengdong Lu", "Hang Li", "Qun Liu." ],
      "venue" : "IJCAI, pages 1354–1361.",
      "citeRegEx" : "Wang et al\\.,? 2015",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "A networkbased end-to-end trainable task-oriented dialogue system",
      "author" : [ "Tsung-Hsien Wen", "David Vandyke", "Nikola Mrkšić", "Milica Gašić", "Lina M. Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "Steve Young." ],
      "venue" : "Proceedings of the 15th Conference of",
      "citeRegEx" : "Wen et al\\.,? 2017",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2017
    }, {
      "title" : "An effective domain adaptive post-training method for bert in response selection",
      "author" : [ "Taesun Whang", "Dongyub Lee", "Chanhee Lee", "Kisu Yang", "Dongsuk Oh", "HeuiSeok Lim." ],
      "venue" : "Proceedings of INTERSPEECH 2020, pages 1585–1589.",
      "citeRegEx" : "Whang et al\\.,? 2020",
      "shortCiteRegEx" : "Whang et al\\.",
      "year" : 2020
    }, {
      "title" : "Starspace: Embed all the things",
      "author" : [ "Ledell Yu Wu", "Adam Fisch", "Sumit Chopra", "Keith Adams", "Antoine Bordes", "Jason Weston" ],
      "venue" : "In Thirty-Second AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Wu et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2018
    }, {
      "title" : "Sequential matching network: A new architecture for multi-turn response selection in retrieval-based chatbots",
      "author" : [ "Yu Wu", "Wei Wu", "Chen Xing", "Ming Zhou", "Zhoujun Li." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Wu et al\\.,? 2017",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning an effective context-response matching model with self-supervised tasks for retrieval-based dialogues",
      "author" : [ "Ruijian Xu", "Chongyang Tao", "Daxin Jiang", "Xueliang Zhao", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "Proceedings of the Thirty-Fifth AAAI Conference",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32. Curran",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi-hop selector network for multi-turn response selection in retrieval-based chatbots",
      "author" : [ "Chunyuan Yuan", "Wei Zhou", "Mingming Li", "Shangwen Lv", "Fuqing Zhu", "Jizhong Han", "Songlin Hu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical",
      "citeRegEx" : "Yuan et al\\.,? 2019",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2019
    }, {
      "title" : "Personalizing dialogue agents: I have a dog, do you have pets too",
      "author" : [ "Saizheng Zhang", "Emily Dinan", "Jack Urbanek", "Arthur Szlam", "Douwe Kiela", "Jason Weston" ],
      "venue" : "In Proceedings of the 56th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Zhang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "DIALOGPT : Largescale generative pre-training for conversational response generation",
      "author" : [ "Yizhe Zhang", "Siqi Sun", "Michel Galley", "Yen-Chun Chen", "Chris Brockett", "Xiang Gao", "Jianfeng Gao", "Jingjing Liu", "Bill Dolan." ],
      "venue" : "Proceedings of the 58th",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "A documentgrounded matching network for response selection in retrieval-based chatbots",
      "author" : [ "Xueliang Zhao", "Chongyang Tao", "Wei Wu", "Can Xu", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "Proceedings of the Twenty-Eighth International Joint Conference on",
      "citeRegEx" : "Zhao et al\\.,? 2019",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2019
    }, {
      "title" : "Knowledgegrounded dialogue generation with pre-trained language models",
      "author" : [ "Xueliang Zhao", "Wei Wu", "Can Xu", "Chongyang Tao", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    }, {
      "title" : "A dataset for document grounded conversations",
      "author" : [ "Kangyan Zhou", "Shrimai Prabhumoye", "Alan W Black." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 708–713, Brussels, Belgium.",
      "citeRegEx" : "Zhou et al\\.,? 2018a",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-turn response selection for chatbots with deep attention matching network",
      "author" : [ "Xiangyang Zhou", "Lu Li", "Daxiang Dong", "Yi Liu", "Ying Chen", "Wayne Xin Zhao", "Dianhai Yu", "Hua Wu." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association",
      "citeRegEx" : "Zhou et al\\.,? 2018b",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2018
    }, {
      "title" : "Random: the model randomly selects a knowledge entry from a set of knowledge entries; (2) IR Baseline: the model uses simple word overlap between the dialogue context",
      "author" : [ "Dinan" ],
      "venue" : null,
      "citeRegEx" : "Dinan,? \\Q2019\\E",
      "shortCiteRegEx" : "Dinan",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "Along with the very recent prosperity of artificial intelligence empowered conversation systems in the spotlight, many studies have been focused on building human-computer dialogue systems (Wen et al., 2017; Zhang et al., 2020) with either retrievalbased methods (Wang et al.",
      "startOffset" : 189,
      "endOffset" : 227
    }, {
      "referenceID" : 31,
      "context" : "Along with the very recent prosperity of artificial intelligence empowered conversation systems in the spotlight, many studies have been focused on building human-computer dialogue systems (Wen et al., 2017; Zhang et al., 2020) with either retrievalbased methods (Wang et al.",
      "startOffset" : 189,
      "endOffset" : 227
    }, {
      "referenceID" : 11,
      "context" : ", 2020) or generation-based methods (Li et al., 2016; Serban et al., 2016; Zhang et al., 2020), which both predict the response with only the given context.",
      "startOffset" : 36,
      "endOffset" : 94
    }, {
      "referenceID" : 17,
      "context" : ", 2020) or generation-based methods (Li et al., 2016; Serban et al., 2016; Zhang et al., 2020), which both predict the response with only the given context.",
      "startOffset" : 36,
      "endOffset" : 94
    }, {
      "referenceID" : 31,
      "context" : ", 2020) or generation-based methods (Li et al., 2016; Serban et al., 2016; Zhang et al., 2020), which both predict the response with only the given context.",
      "startOffset" : 36,
      "endOffset" : 94
    }, {
      "referenceID" : 30,
      "context" : "To bridge the gap of the knowledge between the human and the machine, researchers have begun to simulating this motivation by grounding dialogue agents with background knowledge (Zhang et al., 2018; Dinan et al., 2019; Li et al., 2020), and lots",
      "startOffset" : 178,
      "endOffset" : 235
    }, {
      "referenceID" : 1,
      "context" : "To bridge the gap of the knowledge between the human and the machine, researchers have begun to simulating this motivation by grounding dialogue agents with background knowledge (Zhang et al., 2018; Dinan et al., 2019; Li et al., 2020), and lots",
      "startOffset" : 178,
      "endOffset" : 235
    }, {
      "referenceID" : 12,
      "context" : "To bridge the gap of the knowledge between the human and the machine, researchers have begun to simulating this motivation by grounding dialogue agents with background knowledge (Zhang et al., 2018; Dinan et al., 2019; Li et al., 2020), and lots",
      "startOffset" : 178,
      "endOffset" : 235
    }, {
      "referenceID" : 30,
      "context" : "Although some benchmarks built upon crowd-sourcing have been released by recent works (Zhang et al., 2018; Dinan et al., 2019), the relatively small training size makes it hard for the dialogue models to generalize on other domains or topics (Zhao et al.",
      "startOffset" : 86,
      "endOffset" : 126
    }, {
      "referenceID" : 1,
      "context" : "Although some benchmarks built upon crowd-sourcing have been released by recent works (Zhang et al., 2018; Dinan et al., 2019), the relatively small training size makes it hard for the dialogue models to generalize on other domains or topics (Zhao et al.",
      "startOffset" : 86,
      "endOffset" : 126
    }, {
      "referenceID" : 33,
      "context" : ", 2019), the relatively small training size makes it hard for the dialogue models to generalize on other domains or topics (Zhao et al., 2020).",
      "startOffset" : 123,
      "endOffset" : 142
    }, {
      "referenceID" : 10,
      "context" : ", query-passage pairs coming from ad-hoc retrieval tasks) (Khattab and Zaharia, 2020) and multi-turn dialogues (e.",
      "startOffset" : 58,
      "endOffset" : 85
    }, {
      "referenceID" : 8,
      "context" : ", context-response pairs collected from Reddit) (Henderson et al., 2019), which might be beneficial to the learning of knowledge comprehension, knowl-",
      "startOffset" : 48,
      "endOffset" : 72
    }, {
      "referenceID" : 0,
      "context" : "of a pre-trained language model (PLMs) (Devlin et al., 2019; Yang et al., 2019), which are trained with large-scale unstructured documents from the web.",
      "startOffset" : 39,
      "endOffset" : 79
    }, {
      "referenceID" : 28,
      "context" : "of a pre-trained language model (PLMs) (Devlin et al., 2019; Yang et al., 2019), which are trained with large-scale unstructured documents from the web.",
      "startOffset" : 39,
      "endOffset" : 79
    }, {
      "referenceID" : 1,
      "context" : "We conduct experiments with benchmarks of knowledge-grounded dialogue that are constructed by crowd-sourcing, such as the Wizard-ofWikipedia Corpus (Dinan et al., 2019) and the CMU DoG Corpus (Zhou et al.",
      "startOffset" : 148,
      "endOffset" : 168
    }, {
      "referenceID" : 21,
      "context" : "Early studies of retrieval-based dialogue focus on single-turn response selection where the input of a matching model is a message-response pair (Wang et al., 2013; Ji et al., 2014; Wang et al., 2015).",
      "startOffset" : 145,
      "endOffset" : 200
    }, {
      "referenceID" : 9,
      "context" : "Early studies of retrieval-based dialogue focus on single-turn response selection where the input of a matching model is a message-response pair (Wang et al., 2013; Ji et al., 2014; Wang et al., 2015).",
      "startOffset" : 145,
      "endOffset" : 200
    }, {
      "referenceID" : 22,
      "context" : "Early studies of retrieval-based dialogue focus on single-turn response selection where the input of a matching model is a message-response pair (Wang et al., 2013; Ji et al., 2014; Wang et al., 2015).",
      "startOffset" : 145,
      "endOffset" : 200
    }, {
      "referenceID" : 13,
      "context" : "Representative methods include the dual-LSTM model (Lowe et al., 2015), the sequential matching network (SMN) (Wu et al.",
      "startOffset" : 51,
      "endOffset" : 70
    }, {
      "referenceID" : 26,
      "context" : ", 2015), the sequential matching network (SMN) (Wu et al., 2017), the deep attention matching network (DAM) (Zhou et al.",
      "startOffset" : 47,
      "endOffset" : 64
    }, {
      "referenceID" : 35,
      "context" : ", 2017), the deep attention matching network (DAM) (Zhou et al., 2018b), interaction-overinteraction network (IoI) (Tao et al.",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 18,
      "context" : ", 2018b), interaction-overinteraction network (IoI) (Tao et al., 2019) and multi-hop selector network (MSN) (Yuan et al.",
      "startOffset" : 52,
      "endOffset" : 70
    }, {
      "referenceID" : 29,
      "context" : ", 2019) and multi-hop selector network (MSN) (Yuan et al., 2019).",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "More recently, pre-trained language models (Devlin et al., 2019; Yang et al., 2019) have shown significant benefits for various NLP tasks, and some researchers have tried to apply them on multi-turn response selection.",
      "startOffset" : 43,
      "endOffset" : 83
    }, {
      "referenceID" : 28,
      "context" : "More recently, pre-trained language models (Devlin et al., 2019; Yang et al., 2019) have shown significant benefits for various NLP tasks, and some researchers have tried to apply them on multi-turn response selection.",
      "startOffset" : 43,
      "endOffset" : 83
    }, {
      "referenceID" : 3,
      "context" : "To bridge the gap of the knowledge between the human and the machine, researchers have investigated into grounding dialogue agents with unstructured background knowledge (Ghazvininejad et al., 2018; Zhang et al., 2018; Dinan et al., 2019).",
      "startOffset" : 170,
      "endOffset" : 238
    }, {
      "referenceID" : 30,
      "context" : "To bridge the gap of the knowledge between the human and the machine, researchers have investigated into grounding dialogue agents with unstructured background knowledge (Ghazvininejad et al., 2018; Zhang et al., 2018; Dinan et al., 2019).",
      "startOffset" : 170,
      "endOffset" : 238
    }, {
      "referenceID" : 1,
      "context" : "To bridge the gap of the knowledge between the human and the machine, researchers have investigated into grounding dialogue agents with unstructured background knowledge (Ghazvininejad et al., 2018; Zhang et al., 2018; Dinan et al., 2019).",
      "startOffset" : 170,
      "endOffset" : 238
    }, {
      "referenceID" : 32,
      "context" : "proposed, such as document-grounded matching network (DGMN) (Zhao et al., 2019) and dually interactive matching network (DIM) (Gu et al.",
      "startOffset" : 60,
      "endOffset" : 79
    }, {
      "referenceID" : 5,
      "context" : ", 2019) and dually interactive matching network (DIM) (Gu et al., 2019) which let the dialogue context and all knowledge entries interact with the response candidate",
      "startOffset" : 54,
      "endOffset" : 71
    }, {
      "referenceID" : 1,
      "context" : "Existing studies (Dinan et al., 2019) usually extract the relevant knowledge before the response matching or jointly train the knowledge retrieval and response selection in a multi-task manner.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 7,
      "context" : "Fortunately, the ad-hoc retrieval task (Harman, 2005; Khattab and Zaharia, 2020) in the information retrieval area provides a potential solution to simulate the process of knowledge seeking.",
      "startOffset" : 39,
      "endOffset" : 80
    }, {
      "referenceID" : 10,
      "context" : "Fortunately, the ad-hoc retrieval task (Harman, 2005; Khattab and Zaharia, 2020) in the information retrieval area provides a potential solution to simulate the process of knowledge seeking.",
      "startOffset" : 39,
      "endOffset" : 80
    }, {
      "referenceID" : 15,
      "context" : "We adopt MS MARCO passage ranking dataset (Nguyen et al., 2016) built on Bing’s search for query-passage matching task.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 2,
      "context" : "For the query-dialogue history matching task and multi-turn response matching task, we use the multi-turn dialogue corpus constructed from the Reddit (Dziri et al., 2018).",
      "startOffset" : 150,
      "endOffset" : 170
    }, {
      "referenceID" : 1,
      "context" : "the Wizard-of-Wikipedia (WoW) (Dinan et al., 2019) and CMU DoG (Zhou et al.",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 1,
      "context" : "Following existing works (Dinan et al., 2019; Zhao et al., 2019), positive responses",
      "startOffset" : 25,
      "endOffset" : 64
    }, {
      "referenceID" : 32,
      "context" : "Following existing works (Dinan et al., 2019; Zhao et al., 2019), positive responses",
      "startOffset" : 25,
      "endOffset" : 64
    }, {
      "referenceID" : 6,
      "context" : "Following previous works on knowledge-grounded response selection (Gu et al., 2020b; Zhao et al., 2019), we also employ recall n at k Rn@k (where n = 100 for WoW and n = 20 for CMU DoG and k = {1, 2, 5}) as the evaluation metrics.",
      "startOffset" : 66,
      "endOffset" : 103
    }, {
      "referenceID" : 32,
      "context" : "Following previous works on knowledge-grounded response selection (Gu et al., 2020b; Zhao et al., 2019), we also employ recall n at k Rn@k (where n = 100 for WoW and n = 20 for CMU DoG and k = {1, 2, 5}) as the evaluation metrics.",
      "startOffset" : 66,
      "endOffset" : 103
    }, {
      "referenceID" : 16,
      "context" : "Our model is implemented by PyTorch (Paszke et al., 2019).",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "1) IR Baseline (Dinan et al., 2019) uses simple word overlap for response selection; 2) BoW MemNet (Dinan et al.",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : ", 2019) uses simple word overlap for response selection; 2) BoW MemNet (Dinan et al., 2019) is a memory network where knowledge entries are embedded via bag-of-words representation, and the model learns the knowledge selection and response matching jointly; 3) Transformer MemNet (Dinan et al.",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 1,
      "context" : ", 2019) is a memory network where knowledge entries are embedded via bag-of-words representation, and the model learns the knowledge selection and response matching jointly; 3) Transformer MemNet (Dinan et al., 2019) is an extension of BoW MemNet, Models R@1 R@2 R@5",
      "startOffset" : 196,
      "endOffset" : 216
    }, {
      "referenceID" : 19,
      "context" : "and the dialogue history, response candidate and knowledge entries are encoded with Transformer encoder (Vaswani et al., 2017) pre-trained on a",
      "startOffset" : 104,
      "endOffset" : 126
    }, {
      "referenceID" : 1,
      "context" : "4) Two-stage Transformer (Dinan et al., 2019) trains two separately models for knowledge selection and response retrieval respectively.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 25,
      "context" : ", 2018) selects the response by the cosine similarity between a concatenated sequence of dialogue context, knowledge, and the response candidate represented by StarSpace (Wu et al., 2018); 2) BoW MemNet (Zhang et al.",
      "startOffset" : 170,
      "endOffset" : 187
    }, {
      "referenceID" : 30,
      "context" : "is a memory network with the bag-of-words representation of knowledge entries as the memory items; 3) KV Profile Memory (Zhang et al., 2018) is a key-value memory network grounded on knowledge profiles; 4) Transformer",
      "startOffset" : 120,
      "endOffset" : 140
    }, {
      "referenceID" : 14,
      "context" : "MemNet (Mazaré et al., 2018) is similar to BoW MemNet and all utterances are encoded with a pre-trained Transformer; 5) DGMN (Zhao et al.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 32,
      "context" : ", 2018) is similar to BoW MemNet and all utterances are encoded with a pre-trained Transformer; 5) DGMN (Zhao et al., 2019) lets the dialogue context and all knowledge entries interact with the response candidate respectively via the cross-attention; 6) DIM (Gu et al.",
      "startOffset" : 104,
      "endOffset" : 123
    }, {
      "referenceID" : 5,
      "context" : ", 2019) lets the dialogue context and all knowledge entries interact with the response candidate respectively via the cross-attention; 6) DIM (Gu et al., 2019) is similar to DGMN and all utterance are encoded with BiLSTMs; 7) FIRE (Gu et al.",
      "startOffset" : 142,
      "endOffset" : 159
    }, {
      "referenceID" : 6,
      "context" : ", 2019) is similar to DGMN and all utterance are encoded with BiLSTMs; 7) FIRE (Gu et al., 2020b) first filters the context and knowledge and then use the filtered context and knowledge to perform the iterative response matching process.",
      "startOffset" : 79,
      "endOffset" : 97
    } ],
    "year" : 2021,
    "abstractText" : "Recently, many studies are emerging towards building a retrieval-based dialogue system that is able to effectively leverage background knowledge (e.g., documents) when conversing with humans. However, it is non-trivial to collect large-scale dialogues that are naturally grounded on the background documents, which hinders the effective and adequate training of knowledge selection and response matching. To overcome the challenge, we consider decomposing the training of the knowledge-grounded response selection into three tasks including: 1) query-passage matching task; 2) query-dialogue history matching task; 3) multi-turn response matching task, and joint learning all these tasks in a unified pre-trained language model. The former two tasks could help the model in knowledge selection and comprehension, while the last task is designed for matching the proper response with the given query and background knowledge (dialogue history). By this means, the model can be learned to select relevant knowledge and distinguish proper response, with the help of ad-hoc retrieval corpora and a large number of ungrounded multi-turn dialogues. Experimental results on two benchmarks of knowledge-grounded response selection indicate that our model can achieve comparable performance with several existing methods that rely on crowd-sourced data for training.",
    "creator" : "LaTeX with hyperref"
  }
}