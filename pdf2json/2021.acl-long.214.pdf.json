{
  "name" : "2021.acl-long.214.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Evaluating Evaluation Measures for Ordinal Classification and Ordinal Quantification",
    "authors" : [ "Tetsuya Sakai" ],
    "emails" : [ "tetsuyasakai@acm.org" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2759–2769\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2759"
    }, {
      "heading" : "1 Introduction",
      "text" : "In NLP and many other experiment-oriented research disciplines, researchers rely heavily on evaluation measures. Whenever we observe an improvement in the score of our favourite measure, we either assume or hope that this implies that we have managed to moved our system a little towards what we ultimately want to achieve. Hence it is of utmost importance to examine whether evaluation measures are measuring what we want to measure, and to understand their properties.\nThis paper concerns evaluation measures for Ordinal Classification (OC) and Ordinal Quantification (OQ) tasks. In an OC task, the classes are ordinal, not nominal. For example, Task 4 (Sentiment Analysis in Twitter) Subtask C in SemEval2016/2017 is defined as: given a set of tweets about\na particular topic, estimate the sentiment conveyed by each tweet towards the topic on a five-point scale (highly negative, negative, neutral, positive, highly positive) (Nakov et al., 2016; Rosenthal et al., 2017). On the other hand, an OQ task involves a gold distribution of labels over ordinal classes and the system’s estimated distribution. For example, Task 4 Subtask E of the SemEval-2016/2017 workshops is defined as: given a set of tweets about a particular topic, estimate the distribution of the tweets across the five ordinal classes already mentioned above (Nakov et al., 2016; Rosenthal et al., 2017). The Dialogue Breakdown Detection Challenge (Higashinaka et al., 2017) and the Dialogue Quality subtasks of the NTCIR-14 Short Text Conversation (Zeng et al., 2019) and the NTCIR-15 Dialogue Evaluation (Zeng et al., 2020) tasks are also OQ tasks. 1\nClearly, evaluation measures for OC and OQ tasks should take the ordinal nature of the classes into account. For example, in OC, when a highly positive item is misclassified as highly negative, that should be penalised more heavily than when it is misclassified as positive. Surprisingly, however, there are only a small number of known evaluation measures that meet this requirement. In the present study, we use data from the SemEval and NTCIR communities to clarify the properties of nine evaluation measures in the context of OC tasks, and six measures in the context of OQ tasks. Some of these measures satisfy the aforementioned basic requirement for ordinal classes; others do not.\n1In terms of data structure, we observe that the relationship between OC and OQ are similar to that between paired data and two-sample data in statistical significance testing. In OC, we examine which item is classified by the system into which class, and build a confusion matrix of gold and estimated classes. In contrast, in OQ, we compare the system’s distribution of items with the gold distribution, but we do not concern ourselves with which item in one distribution corresponds to which item in the other.\nSection 2 discusses prior art. Section 3 provides formal definitions of the measures we examine, as this is of utmost importance for reproducibility. Section 4 describes the data we use to evaluate the measures. Sections 5 and 6 report on the results on the OC and OQ measures, respectively. Finally, Section 7 concludes this paper."
    }, {
      "heading" : "2 Prior Art",
      "text" : ""
    }, {
      "heading" : "2.1 Evaluating Ordinal Classification",
      "text" : "As we have mentioned in Section 1, Task 4 Subtask C of the SemEval-2016/2017 workshops is an OC task with five ordinal classes (Nakov et al., 2016; Rosenthal et al., 2017). While SemEval also features other OC tasks with fewer classes (e.g., Task 4 Subtask A from the same years, with three classes), we use the Subtask C data as having more classes should enable us to see more clearly the difference between measures that consider ordinal classes and those that do not.2 Note that if there are only two classes, OC is reduced to nominal classification. Subtask C used two evaluation measures that consider the ordinal nature of the classes: macroaveraged Mean Absolute Error (MAEM ) and the standard Mean Absolute Error (MAEµ) (Baccianella et al., 2009).\nAt ACL 2020, Amigó et al. (2020) proposed a measure specifically designed for OC, called Closeness Evaluation Measure (CEMORD ), and discussed its axiomatic properties. Their metaevaluation experiments primarily focused on comparing it with other measures in terms of how each measure agrees simultaneously with all of preselected “gold” measures. However, while their results showed that CEMORD is similar to all of these gold measures, the outcome may differ if we choose a different set of gold measures. Indeed, in the context of evaluating information retrieval evaluation measures, Sakai and Zeng (2019) demonstrated that a similar meta-evaluation approach called unanimity (Amigó et al., 2018) depends heavily on the choice of gold measures. Moreover, while Amigó et al. (2020) reported that CEMORD also performs well in terms of consistency of system rankings across different data (which they refer to as “robustness”), experimental details were not provided in their paper. Hence, to complement their work, the present study conducts extensive and re-\n2SemEval-2018 Task 1 (Affect in Tweets) featured an OC task with four classes (Mohammad et al., 2018). However, the run submission files of this task are not publicly available.\nproducible experiments for OC measures. Our OC meta-evaluation experiments cover nine measures, including MAEM , MAEµ, and CEMORD ."
    }, {
      "heading" : "2.2 Evaluating Ordinal Quantification",
      "text" : "As we have mentioned in Section 1, Task 4 Subtask E of the SemEval-2016/2017 workshops is an OQ task with five ordinal classes (Nakov et al., 2016; Rosenthal et al., 2017).3 Subtask E used Earth Mover’s Distance (EMD), remarking that this is “currently the only known measure for ordinal quantification” (Nakov et al., 2016; Rosenthal et al., 2017). Subsequently, however, Sakai (2018a) proposed a new suite of OQ measures based on Order-aware Divergence (OD),4 and compared them with Normalised Match Distance (NMD), a normalised version of EMD. Sakai utilised data from the Third Dialogue Breakdown Detection Challenge (DBDC3) (Higashinaka et al., 2017), which features three ordinal classes, and showed that his Root Symmetric Normalised OD (RSNOD) measure behaves similarly to NMD. However, his experiments relied on the run submission files from his own team, as he did not have access to the entire set of DBDC3 submission files. On the other hand, the organisers of DBDC3 (Tsunomori et al., 2020) compared RSNOD, NMD, and the official measures of DBDC (namely, Mean Squared Error and Jensen-Shannon Divergence, which ignore the ordinal nature of the classes) using all the run submission files from DBDC3. They reported that RSNOD was the overall winner in terms of system ranking consistency and discriminative power, i.e., the ability of a measure to obtain many statistical significant differences (Sakai, 2006, 2007, 2014).\nIn addition to the aforementioned two Subtask E data sets from SemEval, the present study utilises three data sets from the Dialogue Quality (DQ) Subtasks of the recent NTCIR-15 Dialogue Evaluation (DialEval-1) Task (Zeng et al., 2020). Each DQ subtask is defined as: given a helpdesk-customer dialogue, estimate the probability distribution over the five-point Likert-scale Dialogue Quality ratings (See Section 4). Our OQ meta-evaluation experiments cover six measures, including NMD and RSNOD.\n3The Valence Ordinal Classification subtask of SemEval2018 Task 1 (Affect in Tweets) is also an OQ task, with seven classes (Mohammad et al., 2018). However, the submission files of this task are not publicly available.\n4See also Sakai (2017) for an earlier discussion on OD."
    }, {
      "heading" : "3 Evaluation Measure Definitions",
      "text" : ""
    }, {
      "heading" : "3.1 Classification Measures",
      "text" : "In the OC tasks of SemEval-2016/2017, a set of topics was given to the participating systems, where each topic is associated with N tweets. (N varies across topics.) Given a set C of ordinal classes represented by consecutive integers, each OC system yields a |C| × |C| confusion matrix for each topic. From this, we can calculate evaluation measures described below. Finally, the systems are evaluated in terms of mean scores over the topic set.\nLet cij denote the number of items (e.g., tweets) whose true class is j, classified by the system into i (i, j ∈ C) so that N = ∑ j ∑ i cij . Let c•j =∑\ni cij , ci• = ∑ j cij , and C + = {j ∈ C | c•j > 0}. That is, C+ is the set of gold classes that are not empty. We compute MAE’s as follows.\nMAEM = 1 |C+| ∑ j∈C+ ∑ i∈C |i− j|cij c•j , (1)\nMAEµ =\n∑ j∈C ∑ i∈C |i− j|cij N . (2)\nUnlike the original formulation of MAEM by Baccianella et al. (2009), ours explicitly handles cases where there are empty gold classes (i.e., j s.t. c•j = 0). Empty gold classes actually do exist in the SemEval data used in our experiments.\nIt is clear from the weights used above (|i− j|) that MAEs assume equidistance, although this is not guaranteed for ordinal classes. Hence Amigó et al. (2020) propose the following alternative:\nCEMORD =\n∑ j∈C ∑ i∈C prox ijcij∑\nj∈C prox jjc•j , (3)\nwhere prox ij = − log2(max{0.5,Kij}/N), and\nKij =\n{ c•i/2 + ∑j l=i+1 c•l (i ≤ j)\nc•i/2 + ∑i−1 l=j c•l (i > j) . (4)\nOur formulation of prox ij with a max operator ensures that it is a finite value even if Kij = 0.\nWe also consider Weighted κ (Cohen, 1968). We first compute the expected agreements when the system and gold labels are independent: eij = ci•c•j/N . Weighted κ is then defined as:\nκ = 1− ∑ j∈C ∑\ni∈C wijcij∑ j∈C ∑ i∈C wijeij , (5)\nwhere wij is a predefined weight for penalising misclassification. In the present study, we follow the approach of MAEs (Eqs. 1-2) and consider Linear Weighted κ: wij = |i − j|. However, it should be noted here that κ is not useful if the OC task involves baseline systems such as the ones included in the aforementioned SemEval tasks: that is, a system that always returns Class 1, a system that always returns Class 2, and so on. It is easy to mathematically prove that κ returns a zero for all topics for all such baseline systems.\nWe also consider applying Krippendorff’s α (Krippendorff, 2018) to OC tasks. The α is a measure of data label reliability, and can handle any types of classes by plugging in an appropriate distance function. Instead of the |C| × |C| confusion matrix, the α requires a |C|×N class-by-item matrix that contains label counts ni(u), which represents the number of labels which say that item u belongs to Class i. For an OC task, ni(u) = 2 if both the gold and system labels for u is i; ni(u) = 1 if either the gold or system label (but not both) for u is i; ni(u) = 0 if neither label says u belongs to i. Thus, this matrix ignores which labels are from the gold data and which are from the system.\nFor comparing two complete sets of labels (one from the gold data and the other from the system), the definition of Krippendorff’s α is relatively simple. Let ni = ∑ u ni(u); this is the total number of labels that Class i received from the two sets of labels. The observed coincidence for Classes i and j (i, j ∈ C, i 6= j) is given by Oij = ∑ u ni(u)nj(u), while the expected coincidence is given by Eij = ninj/(2N − 1). The α is defined as:\nα = 1− ∑ i ∑ j>iOijδ 2 ij∑\ni ∑ j>iEijδ 2 ij ,\n(6)\nwhere, for ordinal data,\nδ2ij = ( j∑ k=i nk − ni + nj 2 )2 , (7)\nand for interval data, δ2ij = |i− j|2 (Krippendorff, 2018). We shall refer to these two versions of α as α-ORD and α-INT, respectively. Unlike κ, the α’s can evaluate the aforementioned baseline systems without any problems.\nThe three measures defined below ignore the ordinal nature of the classes. That is, they are axiomatically incorrect as OC evaluation measures.\nFirst, let us consider two different definitions of “Macro F1” found in the literature (Opitz and Burst, 2019): to avoid confusion, we give them different names in this paper. For each j ∈ C+, let Precj = cjj/cj• if cj• > 0, and Precj = 0 if cj• = 0 (i.e., the system never chooses Class j). Let Recj = cjj/c•j . Also, for any positive values p and r, let f1(p, r) = 2pr/(p + r) if p + r > 0, and let f1(p, r) = 0 if p = r = 0. Then:\nF1M = 1 |C+| ∑ j∈C+ f1(Precj ,Recj) . (8)\nNow, let PrecM = ∑\nj∈C+ Precj/|C+|, RecM = ∑ j∈C+ Recj/|C+|, and\nHMPR = f1(PrecM ,RecM ) . (9)\nHMPR stands for Harmonic mean of Macroaveraged Precision and macroaveraged Recall. Opitz and Burst (2019) recommend what we call F1M over what we call HMPR. Again, note that our formulations useC+ to clarify that empty gold classes are ignored.\nFinally, we also consider Accuracy:5\nAccuracy =\n∑ j∈C cjj\nN . (10)\nFrom Eqs. 2 and 10, it is clear that MAEµ and Accuracy ignore class imbalance (Baccianella et al., 2009), unlike the other measures."
    }, {
      "heading" : "3.2 Quantification Measures",
      "text" : "In an OQ task, a comparison of an estimated distribution and the gold distribution over |C| ordinal classes yields one effectiveness score, as described below. The systems are then evaluated by mean scores over the test instances, e.g., topics (Nakov et al., 2016; Rosenthal et al., 2017) or dialogues (Zeng et al., 2019, 2020). Let pi denote the estimated probability for Class i, so that∑\ni∈C pi = 1. Similarly, let p ∗ i denote the true probability. We also denote the entire probability distributions by p and p∗, respectively.\nLet cpi = ∑ k≤i pk, and cp ∗ i = ∑ k≤i p ∗ k. Normalised Match Distance (NMD) used in the NTCIR Dialogue Quality Subtasks (Zeng et al., 2019, 2020) is given by (Sakai, 2018a):\nNMD(p, p∗) = ∑ i∈C |cpi − cp∗i | |C| − 1 . (11)\n5Since the system and the gold data have the same total number of items to classify (i.e., N ), accuracy is the same as microaveraged F1/recall/precision.\nThis is simply a normalised version of EMD used in the OQ tasks of SemEval (See Section 2.2) (Nakov et al., 2016; Rosenthal et al., 2017).\nWe also consider two measures that can handle OQ tasks from Sakai (2018a). First, a DistanceWeighted sum of squares for Class i is defined as:\nDW i = ∑ j∈C |i− j|(pj − p∗j )2 . (12)\nNote that the above assumes equidistance. Let C∗ = {i ∈ C|p∗i > 0}. That is, C∗ is the set of classes with a positive gold probability. Orderaware Divergence is defined as:\nOD(p ‖ p∗) = 1 |C∗| ∑ i∈C∗ DW i , (13)\nwith its symmetric version SOD(p, p∗) = (OD(p ‖ p∗) +OD(p∗ ‖ p))/2. Root (Symmetric) Normalised Order-aware Divergence is defined as:\nRNOD(p ‖ p∗) = √ OD(p ‖ p∗) |C| − 1 , (14)\nRSNOD(p, p∗) =\n√ SOD(p, p∗)\n|C| − 1 . (15)\nThe other three measures defined below ignore the ordinal nature of the classes (Sakai, 2018a); they are axiomatically incorrect as OQ measures. Normalised Variational Distance (NVD) is essentially the Mean Absolute Error (MAE):\nNVD(p, p∗) = 1\n2 ∑ i∈C |pi − p∗i | . (16)\nRoot Normalised Sum of Squares (RNSS) is essentially the Root Mean Squared Error (RMSE):\nRNSS (p, p∗) =\n√∑ i∈C(pi − p∗i )2\n2 . (17)\nThe advantages of RMSE over MAE is discussed in Chai and Draxler (2014).\nThe Kullback-Leibler divergence (KLD) for system and gold probability distributions over classes is given by:\nKLD(p ‖ p∗) = ∑\ni∈C s.t. pi>0 pi log2 pi p∗i . (18)\nAs this is undefined if p∗i = 0, we use the more convenient Jensen-Shannon divergence (JSD) instead, which is symmetric (Lin, 1991):\nJSD(p, p∗) = KLD(p ‖ pM ) +KLD(p∗ ‖ pM )\n2 ,\n(19) where pMi = (pi + p ∗ i )/2."
    }, {
      "heading" : "4 Task Data",
      "text" : "Table 1 provides an overview of the SemEval and NTCIR task data that we leveraged for our OC and OQ meta-evaluation experiments. From SemEval2016/2017 Task 4 (Sentiment Analysis in Twitter) (Nakov et al., 2016; Rosenthal et al., 2017), we chose Subtask C as our OC tasks, and Subtask E as our OQ tasks for the reason given in Section 2.1.6 Moreover, for the OQ meta-evaluation experiments, we also utilise the DQ (Dialogue Quality) subtask data from NTCIR-15 DialEval-1 (Zeng et al., 2020). As these subtasks require participating systems to estimate three different dialogue quality score distributions, namely, A-score (task accomplishment), E-score (dialogue effectiveness), and S-score (customer satisfaction), we shall refer to the subtasks as DQ-A, DQ-E, and DQ-S hereafter. We utilise both Chinese and English DQ runs for our OQ meta-evaluation (22 runs in total), as the NTCIR task evaluates all runs using gold distributions that are based on the Chinese portion of the parallel dialogue corpus (Zeng et al., 2020). As the three NTCIR data sets are larger than the two SemEval data sets both in terms of sample size and the num-\n6We do not use the Arabic data from 2017 as only two runs were submitted to Subtasks C and E (Rosenthal et al., 2017).\nber of systems, we shall focus on the OQ metaevaluation results with the NTCIR data; the results with Sem16T4E and Sem17T4E can be found in the Appendix."
    }, {
      "heading" : "5 Meta-evaluation with Ordinal Classification Tasks",
      "text" : ""
    }, {
      "heading" : "5.1 System Ranking Similarity",
      "text" : "Table 2 shows, for each OC task, the Kendall’s τ rank correlation values (Sakai, 2014) between two system rankings for every pair of measures. We can observe that: (A) the α’s, the two “Macro F1” measures (F1M and HMPR), MAEM and κ produce similar rankings; (B) MAEµ and Accuracy (i.e., the two measures that ignore class imbalance) produce similar rankings, which are drastically different from those of Group A; and (C) CEMORD produces a ranking that is substantially different from the above two groups, although the ranking is closer to those of Group A. The huge gap between Groups A and B strongly suggests that MAEµ and Accuracy are not useful even as secondary measures for evaluating OC systems.\nIt should be noted that the SemEval 2016/2017 Task 4 Subtask C actually reported MAEµ scores in addition to the primary MAEM scores, and the\nsystem rankings according to these two measures were completely different even in the official results. For example, in the 2016 results (Table 12 in Nakov et al. (2016)), while the baseline run that always returns neutral is ranked at 10 among the 12 runs according to MAEM , the same run is ranked at the top according to MAEµ. Similarly, in the 2017 results (Table 10 in Rosenthal et al. (2017)), a run ranked at 10 (tied with another run) among the 20 runs according to MAEM is ranked at the top according to MAEµ. Our results shown in Table 2 generalise these known discrepancies between the rankings."
    }, {
      "heading" : "5.2 System Ranking Consistency",
      "text" : "For each measure, we evaluate its system ranking consistency (or “robustness” (Amigó et al., 2020)) across two topic sets as follows (Sakai, 2021): (1) randomly split the topic set in half, produce two system rankings based on the mean scores over each topic subset, and compute a Kendall’s τ score for the two rankings; (2) repeat the above 1,000 times and compute the mean τ ; (3) conduct a ran-\ndomised paired Tukey HSD test at α = 0.05 with 5,000 trials on the mean τ scores to discuss statistical significance.7\nTable 3 (a) and (c) show the consistency results with the OC tasks. For example, Part (a) shows that when the 100 topics of Sem16T4C were randomly split in half 1,000 times, κ statistically significantly outperformed all other measures, as indicated by a “].” Table 3 (b) and (d) show variants of these experiments where only 10 topics are used in each topic subset, to discuss the robustness of measures to small sample sizes. If we take the averages of (a) and (c), the top three measures are the two α’s and κ, while the worst two measures are CEMORD and Accuracy; we obtain the same result if we take the averages of (b) and (d). Thus, although Amigó et al. (2020) reported that CEMORD performed well in terms of “robustness,” this is not confirmed in our experiments.\nRecall that κ has a practical inconvenience: it cannot distinguish between baseline runs that always return the same class. While SemEval16T4C contains one such run (which always returns neutral), SemEval17T4C contains as many as five such runs (each always returning one of the five ordinal classes). This is probably why κ performs well in Table 3(a) and (b) but not in (c) and (d)."
    }, {
      "heading" : "5.3 Discriminative Power",
      "text" : "In the information retrieval research community, discriminative power (Sakai, 2006, 2007, 2014) is a widely-used method for comparing evaluation measures (e.g., Anelli et al. (2019); Ashkan and Metzler (2019); Chuklin et al. (2013); Clarke et al. (2020); Golbus et al. (2013); Lu et al. (2016); Kanoulas and Aslam (2009); Leelanupab et al. (2012); Robertson et al. (2010); Valcarce et al. (2020)). Given a set of systems, a p-value for the difference in means is obtained for every system pair (preferrably with a multiple comparison procedure (Sakai, 2018b)); highly discriminative measures are those than can obtain many small p-values. While highly discriminative measures are not necessarily correct, we do want measures to be sufficiently discriminative so that we can draw some useful conclusions from experiments. Again, we use randomised paired\n7The Tukey HSD (Honestly Significant Differences) test is a multiple comparison procedure: that is, it is like the ttest, but can compare the means of more than two systems while ensuring that the familywise Type I error rate is α. The randomised version of this test is free from assumptions such as normality and random sampling from a population (Sakai, 2018b).\nTukey HSD tests with 5,000 trials for obtaining the p-values.\nFigure 1 shows the discriminative power curves for the OC tasks. Curves that are closer to the origin (i.e., those with small p-values for many system pairs) are considered good. We can observe that (i) CEMORD , Accuracy, MAEM , and MAEµ are the least discriminative measures in both tasks. (ii) Among the other measures that perform better, κ performs consistently well. Again, the fact that κ distinguishes itself from others in the SemEval16T4C results probably reflects the fact that the data set contains only one run that always returns the same class, which cannot be handled properly by κ."
    }, {
      "heading" : "5.4 Recommendations for OC Tasks",
      "text" : "Table 4 summarises the properties of the nine measures we examined in the context of OC tasks. Column (IV) shows that, for example, the Group A measures produce similar rankings. Based on this table, we recommend (Linear Weighted) κ as the primary measure for OC tasks if the tasks do not in-\nvolve multiple baseline runs that always return the same class. Such runs are unrealistic, so this limitation may not be a major problem. On the other hand, if the tasks do involve such baseline runs (as in SemEval), we recommend α-ORD as the primary measure. In either case, it would be good to use both κ and α-ORD to examine OC systems from multiple angles. According to our consistency and discriminative power experiments, using α-INT instead of α-ORD (i.e., assuming equidistance) does not seem beneficial for OC tasks."
    }, {
      "heading" : "6 Meta-evaluation with Ordinal Quantification Tasks",
      "text" : ""
    }, {
      "heading" : "6.1 System Ranking Similarity",
      "text" : "Table 5 shows, for each OQ task from NTCIR, the Kendall’s τ between two system rankings for every pair of measures. It is clear from the “NMD” column that NMD is an outlier among the six measures. In other words, among the only axiomatically correct measures for OQ tasks, RNOD and RSNOD are the ones that produce rankings that are similar to those produced by well-known measures such as JSD and NVD (i.e., normalised MAE; see Eq. 16). Also, in Table 5(I) and (III), it can be observed that the ranking by RSNOD lies somewhere between that by NMD (let us call it “Group X”) and those by the other measures (“Group Y”). However, this is not true in Table 5(II), nor with our SemEval results (See Appendix Table 8)."
    }, {
      "heading" : "6.2 System Ranking Consistency",
      "text" : "Table 6 shows the system ranking consistency results with the OQ tasks from NTCIR. These experiments were conducted as described in Section 5.2. If we take the averages of (a), (c), and (e) (i.e., experiments where the 300 dialogues are split in half), the worst measure is NMD, followed by RSNOD. Moreover, the results are the same if we take the averages of (b), (d), and (f) (i.e., experiments where two disjoint sets of 10 dialogues are used), we obtain the same result. Hence, among the axiomatically correct measures for OQ tasks, RNOD appears to be the best in terms of system ranking consistency, and that introducing symmetry (Compare Eqs. 14 and 15) may not be a good idea from a statistical stability point of view. Note that, for comparing a system distribution with a gold distribution, symmetry is not a requirement."
    }, {
      "heading" : "6.3 Discriminative Power",
      "text" : "Figure 2 shows the discriminative power curves for the OQ tasks from NTCIR. We can observe that: (i) NMD performs extremely poorly in (I) and (III), which is consistent with the full-split consistency results in Table 6(a) and (e); (ii) RNOD outperforms RSNOD in (I) and (III). Although RSNOD appears to perform well in (II), if we consider the 5% significance level (i.e., 0.05 on the y-axis), the number of statistically significantly different pairs (out of 231) is 117 for RNOD, 116 for RSNOD, NMD, and NVD, and 115 for RNSS and JSD. That is, RNOD performs well in (II) also. These results also suggest that introducing symmetry to RNOD (i.e., using RSNOD instead) is not beneficial."
    }, {
      "heading" : "6.4 Recommendations for OQ Tasks",
      "text" : "Table 7 summarises the properties of the six measures we examined in the context of OQ tasks. Column (III) indicates that NMD is an outlier in terms of system ranking. Based on this table, we recommend RNOD as the primary measure of OQ tasks, as evaluating OQ systems do not require the measures to be symmetric. As a secondary measure, we recommend NMD (i.e., a form of Earth Mover’s Distance) to examine the OQ systems from a different angle, although its statistical stability (in terms of system ranking consistency and discriminative power) seems relatively unpredictable. Although the NTCIR Dialogue Quality subtasks (Zeng et al.,\n2019, 2020) have used NMD and RSNOD as the official measures, it may be beneficial for them to replace RSNOD with RNOD."
    }, {
      "heading" : "7 Conclusions",
      "text" : "We conducted extensive evaluations of nine measures in the context of OC tasks and six measures in the context of OQ tasks, using data from SemEval and NTCIR. As we have discussed in Sections 5.4 and 6.4, our recommendations are as follows.\nOC tasks Use (Linear Weighted) κ as the primary measure if the task does not involve multiple runs that always return the same class (e.g., one that always returns Class 1, another that always returns Class 2, etc.). Otherwise, use α-ORD (i.e., Krippendorff’s α for ordinal classes) as the primary measure. In either case, use both measures.\nOQ tasks Use RNOD as the primary measure, and NMD as a secondary measure.\nAll of our evaluation measure score matrices are available from https://waseda.box.com/ ACL2021PACKOCOQ, to help researchers reproduce our work.\nAmong the above recommended measures, recall that Linear Weighted κ and RNOD assume equidistance (i.e., they rely on wij = |i − j|), while α-ORD and NMD do not. Hence, if researchers want to avoid relying on the equidistance assumption (i.e., satisfy the ordinal invariance property (Amigó et al., 2020)), α-ORD can be used for OC tasks and NMD can be used for OQ tasks. However, we do not see relying on equidistance as a practical problem. For example, note that the Linear Weighted κ is just an instance of the Weighted κ family: if necessary, the weight wij can be set for each pair of Classes i and j according to practical needs. Similarly, wij = |i− j|\n(Eq. 12) for RNOD (and other equidistance-based measures) may be replaced with a different weighting scheme (e.g., something similar to the prox ij weights of CEMORD ) if need be.\nOur final and general remark is that it is of utmost importance for researchers to understand the properties of evaluation measures and ensure that they are appropriate for a given task. Our future work includes evaluating and understanding evaluation measures for tasks other than OC and OQ."
    }, {
      "heading" : "Acknowledgement",
      "text" : "This work was partially supported by JSPS KAKENHI Grant Number 17H01830."
    } ],
    "references" : [ {
      "title" : "An effectiveness metric for ordinal classification: Formal properties and experimental results",
      "author" : [ "Enrique Amigó", "Julio Gonzalo", "Stefano Mizzaro", "Jorge Carrillo de Albornoz." ],
      "venue" : "Proceedings of ACL 2020.",
      "citeRegEx" : "Amigó et al\\.,? 2020",
      "shortCiteRegEx" : "Amigó et al\\.",
      "year" : 2020
    }, {
      "title" : "An axiomatic analysis of diversity evaluation metrics: Introducting the rank-biased utility metric",
      "author" : [ "Enrique Amigó", "Damiano Spina", "Jorge Carrillo de Albornoz." ],
      "venue" : "Proceedings of ACM SIGIR 2018, pages 625–634.",
      "citeRegEx" : "Amigó et al\\.,? 2018",
      "shortCiteRegEx" : "Amigó et al\\.",
      "year" : 2018
    }, {
      "title" : "On the discriminative power of hyper-parameters in cross-validation and how to choose them",
      "author" : [ "Vito Walter Anelli", "Tommaso Di Noia", "Eugenio Di Sciascio", "Claudio Pomo", "Azzurra Ragone." ],
      "venue" : "Proceedings of ACM RecSys 2019, pages 447–451.",
      "citeRegEx" : "Anelli et al\\.,? 2019",
      "shortCiteRegEx" : "Anelli et al\\.",
      "year" : 2019
    }, {
      "title" : "Revisiting online personal search metrics with the user in mind",
      "author" : [ "Azin Ashkan", "Donald Metzler." ],
      "venue" : "Proceedings ACM SIGIR 2019, pages 625–634.",
      "citeRegEx" : "Ashkan and Metzler.,? 2019",
      "shortCiteRegEx" : "Ashkan and Metzler.",
      "year" : 2019
    }, {
      "title" : "Evaluation measures for ordinal regression",
      "author" : [ "Stefano Baccianella", "Andrea Esuli", "Fabrizio Sebastiani." ],
      "venue" : "Proceedings of ISDA 2009, pages 283–287.",
      "citeRegEx" : "Baccianella et al\\.,? 2009",
      "shortCiteRegEx" : "Baccianella et al\\.",
      "year" : 2009
    }, {
      "title" : "Root mean square error (RMSE) or mean absolute error (MAE)? – arguments against avoiding RMSE in the literature",
      "author" : [ "T. Chai", "R.R. Draxler." ],
      "venue" : "Geoscientific Model Development, 7:1247–1250.",
      "citeRegEx" : "Chai and Draxler.,? 2014",
      "shortCiteRegEx" : "Chai and Draxler.",
      "year" : 2014
    }, {
      "title" : "Click model-based information retrieval metrics",
      "author" : [ "Aleksandr Chuklin", "Pavel Serdyuov", "Maarten de Rijke." ],
      "venue" : "Proceedings of ACM SIGIR 2013, pages 493–502.",
      "citeRegEx" : "Chuklin et al\\.,? 2013",
      "shortCiteRegEx" : "Chuklin et al\\.",
      "year" : 2013
    }, {
      "title" : "Offline evaluation without gain",
      "author" : [ "Charles L.A. Clarke", "Alexandra Vtyurina", "Mark D. Smucker." ],
      "venue" : "Proceedings of ICTIR 2020, pages 185–192.",
      "citeRegEx" : "Clarke et al\\.,? 2020",
      "shortCiteRegEx" : "Clarke et al\\.",
      "year" : 2020
    }, {
      "title" : "Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit",
      "author" : [ "Jacob Cohen." ],
      "venue" : "Psychological Bulletin, 70(4):213–220.",
      "citeRegEx" : "Cohen.,? 1968",
      "shortCiteRegEx" : "Cohen.",
      "year" : 1968
    }, {
      "title" : "Increasing evaluation sensitivity to diversity",
      "author" : [ "Peter B. Golbus", "Javed A. Aslam", "Carles L.A. Clarke." ],
      "venue" : "Information Retrieval, 16:530–555.",
      "citeRegEx" : "Golbus et al\\.,? 2013",
      "shortCiteRegEx" : "Golbus et al\\.",
      "year" : 2013
    }, {
      "title" : "Overview of Dialogue Breakdown Detection Challenge 3",
      "author" : [ "Ryuichiro Higashinaka", "Kotaro Funakoshi", "Michimasa Inaba", "Yuiko Tsunomori", "Tetsuro Takahashi", "Nobuhiro Kaji." ],
      "venue" : "Proceedings of Dialog System Technology Challenge 6 (DSTC6) Work-",
      "citeRegEx" : "Higashinaka et al\\.,? 2017",
      "shortCiteRegEx" : "Higashinaka et al\\.",
      "year" : 2017
    }, {
      "title" : "Empirical justification of the gain and discountfunction for nDCG",
      "author" : [ "Evangelos Kanoulas", "Javed A. Aslam." ],
      "venue" : "Proceedings of ACM CIKM 2009, pages 611–620.",
      "citeRegEx" : "Kanoulas and Aslam.,? 2009",
      "shortCiteRegEx" : "Kanoulas and Aslam.",
      "year" : 2009
    }, {
      "title" : "Content Analysis: An Introduction to Its Methodology (Fourth Edition)",
      "author" : [ "Klaus Krippendorff." ],
      "venue" : "SAGE Publications.",
      "citeRegEx" : "Krippendorff.,? 2018",
      "shortCiteRegEx" : "Krippendorff.",
      "year" : 2018
    }, {
      "title" : "A comprehensive analysis of parameter settings for novelty-biased cumulative gain",
      "author" : [ "Teerapong Leelanupab", "Guido Zuccon", "Joemon M. Jose." ],
      "venue" : "Proceedings of ACM CIKM 2012, pages 1950–1954.",
      "citeRegEx" : "Leelanupab et al\\.,? 2012",
      "shortCiteRegEx" : "Leelanupab et al\\.",
      "year" : 2012
    }, {
      "title" : "Divergence measures based on the Shannon entropy",
      "author" : [ "Jianhua Lin." ],
      "venue" : "IEEE Transactions on Information Theory, 37(1):145–151.",
      "citeRegEx" : "Lin.,? 1991",
      "shortCiteRegEx" : "Lin.",
      "year" : 1991
    }, {
      "title" : "The effect of pooling and evaluation depth on IR metrics",
      "author" : [ "Xiaolu Lu", "Alistair Moffat", "J. Shane Culpepper." ],
      "venue" : "Information Retrieval Journal, 19(4):416–445.",
      "citeRegEx" : "Lu et al\\.,? 2016",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2016
    }, {
      "title" : "Semeval-2018 Task 1: Affect in tweets",
      "author" : [ "Saif M. Mohammad", "Felipe Bravo-Marquez", "Mohammad Salameh", "Svetlana Kiritchenko." ],
      "venue" : "Proceedings of International Workshop on Semantic Evaluation (SemEval-2018), New Orleans, LA, USA.",
      "citeRegEx" : "Mohammad et al\\.,? 2018",
      "shortCiteRegEx" : "Mohammad et al\\.",
      "year" : 2018
    }, {
      "title" : "SemEval2016 task 4: Sentiment analysis in Twitter",
      "author" : [ "Preslav Nakov", "Alan Ritter", "Sara Rosenthal", "Veselin Stoyanov", "Fabrizio Sebastiani." ],
      "venue" : "Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval ’16, San Diego, Cali-",
      "citeRegEx" : "Nakov et al\\.,? 2016",
      "shortCiteRegEx" : "Nakov et al\\.",
      "year" : 2016
    }, {
      "title" : "Macro F1 and macro F1",
      "author" : [ "Juri Opitz", "Sebastian Burst" ],
      "venue" : null,
      "citeRegEx" : "Opitz and Burst.,? \\Q2019\\E",
      "shortCiteRegEx" : "Opitz and Burst.",
      "year" : 2019
    }, {
      "title" : "Extending average precision to graded relevance judgements",
      "author" : [ "Stephen E. Robertson", "Evangelos Kanoulas", "Emine Yilmaz." ],
      "venue" : "Proceedings of ACM SIGIR 2010, pages 603–610.",
      "citeRegEx" : "Robertson et al\\.,? 2010",
      "shortCiteRegEx" : "Robertson et al\\.",
      "year" : 2010
    }, {
      "title" : "SemEval-2017 task 4: Sentiment analysis in Twitter",
      "author" : [ "Sara Rosenthal", "Noura Farra", "Preslav Nakov." ],
      "venue" : "Proceedings of the 11th International Workshop on Semantic Evaluation, SemEval ’17, Vancouver, Canada. Association for Computational Linguistics.",
      "citeRegEx" : "Rosenthal et al\\.,? 2017",
      "shortCiteRegEx" : "Rosenthal et al\\.",
      "year" : 2017
    }, {
      "title" : "Evaluating evaluation metrics based on the bootstrap",
      "author" : [ "Tetsuya Sakai." ],
      "venue" : "Proceedings of ACM SIGIR 2006, pages 525–532.",
      "citeRegEx" : "Sakai.,? 2006",
      "shortCiteRegEx" : "Sakai.",
      "year" : 2006
    }, {
      "title" : "Alternatives to bpref",
      "author" : [ "Tetsuya Sakai." ],
      "venue" : "Proceedings of ACM SIGIR 2007, pages 71–78.",
      "citeRegEx" : "Sakai.,? 2007",
      "shortCiteRegEx" : "Sakai.",
      "year" : 2007
    }, {
      "title" : "Metrics, statistics, tests",
      "author" : [ "Tetsuya Sakai." ],
      "venue" : "PROMISE Winter School 2013: Bridging between Information Retrieval and Databases (LNCS 8173), pages 116–163. Springer.",
      "citeRegEx" : "Sakai.,? 2014",
      "shortCiteRegEx" : "Sakai.",
      "year" : 2014
    }, {
      "title" : "Towards automatic evaluation of multi-turn dialogues: A task design that leverages inherently subjective annotations",
      "author" : [ "Tetsuya Sakai." ],
      "venue" : "Proceedings of EVIA 2017, pages 24–30.",
      "citeRegEx" : "Sakai.,? 2017",
      "shortCiteRegEx" : "Sakai.",
      "year" : 2017
    }, {
      "title" : "Comparing two binned probability distributions for information access evaluation",
      "author" : [ "Tetsuya Sakai." ],
      "venue" : "Proceedings of ACM SIGIR 2018, pages 1073– 1076.",
      "citeRegEx" : "Sakai.,? 2018a",
      "shortCiteRegEx" : "Sakai.",
      "year" : 2018
    }, {
      "title" : "Laboratory Experiments in Information Retrieval: Sample Sizes, Effect Sizes, and Statistical Power",
      "author" : [ "Tetsuya Sakai." ],
      "venue" : "Springer.",
      "citeRegEx" : "Sakai.,? 2018b",
      "shortCiteRegEx" : "Sakai.",
      "year" : 2018
    }, {
      "title" : "On the instability of diminishing return IR measures",
      "author" : [ "Tetsuya Sakai." ],
      "venue" : "Proceedings of ECIR 2021 Part I (LNCS 12656), pages 572–586.",
      "citeRegEx" : "Sakai.,? 2021",
      "shortCiteRegEx" : "Sakai.",
      "year" : 2021
    }, {
      "title" : "Which diversity evaluation measures are “good”",
      "author" : [ "Tetsuya Sakai", "Zhaohao Zeng" ],
      "venue" : "In Proceedings of ACM SIGIR",
      "citeRegEx" : "Sakai and Zeng.,? \\Q2019\\E",
      "shortCiteRegEx" : "Sakai and Zeng.",
      "year" : 2019
    }, {
      "title" : "Selection of evaluation metrics for dialogue breakdown detection in dialogue breakdown detection challenge 3 (in Japanese)",
      "author" : [ "Yuiko Tsunomori", "Ryuichiro Higashinaka", "Tetsuro Takahashi", "Michimasa Inaba." ],
      "venue" : "Transactions of the Japanese Society for",
      "citeRegEx" : "Tsunomori et al\\.,? 2020",
      "shortCiteRegEx" : "Tsunomori et al\\.",
      "year" : 2020
    }, {
      "title" : "Assessing ranking metrics in top-N recommendation",
      "author" : [ "Daniel Valcarce", "Alejandro", "Bellogı́n", "Javier Parapar", "Pablo Castells" ],
      "venue" : "Information Retrieval",
      "citeRegEx" : "Valcarce et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Valcarce et al\\.",
      "year" : 2020
    }, {
      "title" : "Overview of the NTCIR-14 short text conversation task: Dialogue quality and nugget detection subtasks",
      "author" : [ "Zhaohao Zeng", "Sosuke Kato", "Tetsuya Sakai." ],
      "venue" : "Proceedings of NTCIR-14, pages 289–315.",
      "citeRegEx" : "Zeng et al\\.,? 2019",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2019
    }, {
      "title" : "Overview of the NTCIR-15 dialogue evaluation task (DialEval-1)",
      "author" : [ "Zhaohao Zeng", "Sosuke Kato", "Tetsuya Sakai", "Inho Kang." ],
      "venue" : "Proceedings of NTCIR-15, pages 13–34.",
      "citeRegEx" : "Zeng et al\\.,? 2020",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "For example, Task 4 (Sentiment Analysis in Twitter) Subtask C in SemEval2016/2017 is defined as: given a set of tweets about a particular topic, estimate the sentiment conveyed by each tweet towards the topic on a five-point scale (highly negative, negative, neutral, positive, highly positive) (Nakov et al., 2016; Rosenthal et al., 2017).",
      "startOffset" : 295,
      "endOffset" : 339
    }, {
      "referenceID" : 20,
      "context" : "For example, Task 4 (Sentiment Analysis in Twitter) Subtask C in SemEval2016/2017 is defined as: given a set of tweets about a particular topic, estimate the sentiment conveyed by each tweet towards the topic on a five-point scale (highly negative, negative, neutral, positive, highly positive) (Nakov et al., 2016; Rosenthal et al., 2017).",
      "startOffset" : 295,
      "endOffset" : 339
    }, {
      "referenceID" : 17,
      "context" : "For example, Task 4 Subtask E of the SemEval-2016/2017 workshops is defined as: given a set of tweets about a particular topic, estimate the distribution of the tweets across the five ordinal classes already mentioned above (Nakov et al., 2016; Rosenthal et al., 2017).",
      "startOffset" : 224,
      "endOffset" : 268
    }, {
      "referenceID" : 20,
      "context" : "For example, Task 4 Subtask E of the SemEval-2016/2017 workshops is defined as: given a set of tweets about a particular topic, estimate the distribution of the tweets across the five ordinal classes already mentioned above (Nakov et al., 2016; Rosenthal et al., 2017).",
      "startOffset" : 224,
      "endOffset" : 268
    }, {
      "referenceID" : 10,
      "context" : "The Dialogue Breakdown Detection Challenge (Higashinaka et al., 2017) and the Dialogue Quality subtasks of the NTCIR-14 Short Text Conversation (Zeng et al.",
      "startOffset" : 43,
      "endOffset" : 69
    }, {
      "referenceID" : 31,
      "context" : ", 2017) and the Dialogue Quality subtasks of the NTCIR-14 Short Text Conversation (Zeng et al., 2019) and the NTCIR-15 Dialogue Evaluation (Zeng et al.",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 32,
      "context" : ", 2019) and the NTCIR-15 Dialogue Evaluation (Zeng et al., 2020) tasks are also OQ tasks.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : "As we have mentioned in Section 1, Task 4 Subtask C of the SemEval-2016/2017 workshops is an OC task with five ordinal classes (Nakov et al., 2016; Rosenthal et al., 2017).",
      "startOffset" : 127,
      "endOffset" : 171
    }, {
      "referenceID" : 20,
      "context" : "As we have mentioned in Section 1, Task 4 Subtask C of the SemEval-2016/2017 workshops is an OC task with five ordinal classes (Nakov et al., 2016; Rosenthal et al., 2017).",
      "startOffset" : 127,
      "endOffset" : 171
    }, {
      "referenceID" : 4,
      "context" : "Subtask C used two evaluation measures that consider the ordinal nature of the classes: macroaveraged Mean Absolute Error (MAEM ) and the standard Mean Absolute Error (MAEμ) (Baccianella et al., 2009).",
      "startOffset" : 174,
      "endOffset" : 200
    }, {
      "referenceID" : 1,
      "context" : "Indeed, in the context of evaluating information retrieval evaluation measures, Sakai and Zeng (2019) demonstrated that a similar meta-evaluation approach called unanimity (Amigó et al., 2018) depends heavily on the choice of gold measures.",
      "startOffset" : 172,
      "endOffset" : 192
    }, {
      "referenceID" : 16,
      "context" : "SemEval-2018 Task 1 (Affect in Tweets) featured an OC task with four classes (Mohammad et al., 2018).",
      "startOffset" : 77,
      "endOffset" : 100
    }, {
      "referenceID" : 17,
      "context" : "As we have mentioned in Section 1, Task 4 Subtask E of the SemEval-2016/2017 workshops is an OQ task with five ordinal classes (Nakov et al., 2016; Rosenthal et al., 2017).",
      "startOffset" : 127,
      "endOffset" : 171
    }, {
      "referenceID" : 20,
      "context" : "As we have mentioned in Section 1, Task 4 Subtask E of the SemEval-2016/2017 workshops is an OQ task with five ordinal classes (Nakov et al., 2016; Rosenthal et al., 2017).",
      "startOffset" : 127,
      "endOffset" : 171
    }, {
      "referenceID" : 17,
      "context" : "3 Subtask E used Earth Mover’s Distance (EMD), remarking that this is “currently the only known measure for ordinal quantification” (Nakov et al., 2016; Rosenthal et al., 2017).",
      "startOffset" : 132,
      "endOffset" : 176
    }, {
      "referenceID" : 20,
      "context" : "3 Subtask E used Earth Mover’s Distance (EMD), remarking that this is “currently the only known measure for ordinal quantification” (Nakov et al., 2016; Rosenthal et al., 2017).",
      "startOffset" : 132,
      "endOffset" : 176
    }, {
      "referenceID" : 10,
      "context" : "Sakai utilised data from the Third Dialogue Breakdown Detection Challenge (DBDC3) (Higashinaka et al., 2017), which features three ordinal classes, and showed that his Root Symmetric Normalised OD (RSNOD) measure behaves similarly to NMD.",
      "startOffset" : 82,
      "endOffset" : 108
    }, {
      "referenceID" : 29,
      "context" : "On the other hand, the organisers of DBDC3 (Tsunomori et al., 2020) compared RSNOD, NMD, and the official measures of DBDC (namely, Mean Squared Error and Jensen-Shannon Divergence, which ignore the ordinal nature of the classes) using all the run submission files from DBDC3.",
      "startOffset" : 43,
      "endOffset" : 67
    }, {
      "referenceID" : 32,
      "context" : "In addition to the aforementioned two Subtask E data sets from SemEval, the present study utilises three data sets from the Dialogue Quality (DQ) Subtasks of the recent NTCIR-15 Dialogue Evaluation (DialEval-1) Task (Zeng et al., 2020).",
      "startOffset" : 216,
      "endOffset" : 235
    }, {
      "referenceID" : 16,
      "context" : "The Valence Ordinal Classification subtask of SemEval2018 Task 1 (Affect in Tweets) is also an OQ task, with seven classes (Mohammad et al., 2018).",
      "startOffset" : 123,
      "endOffset" : 146
    }, {
      "referenceID" : 12,
      "context" : "We also consider applying Krippendorff’s α (Krippendorff, 2018) to OC tasks.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 12,
      "context" : "and for interval data, δ2 ij = |i− j|2 (Krippendorff, 2018).",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 18,
      "context" : "2762 First, let us consider two different definitions of “Macro F1” found in the literature (Opitz and Burst, 2019): to avoid confusion, we give them different names in this paper.",
      "startOffset" : 92,
      "endOffset" : 115
    }, {
      "referenceID" : 4,
      "context" : "2 and 10, it is clear that MAEμ and Accuracy ignore class imbalance (Baccianella et al., 2009), unlike the other measures.",
      "startOffset" : 68,
      "endOffset" : 94
    }, {
      "referenceID" : 17,
      "context" : ", topics (Nakov et al., 2016; Rosenthal et al., 2017) or dialogues (Zeng et al.",
      "startOffset" : 9,
      "endOffset" : 53
    }, {
      "referenceID" : 20,
      "context" : ", topics (Nakov et al., 2016; Rosenthal et al., 2017) or dialogues (Zeng et al.",
      "startOffset" : 9,
      "endOffset" : 53
    }, {
      "referenceID" : 25,
      "context" : "The other three measures defined below ignore the ordinal nature of the classes (Sakai, 2018a); they are axiomatically incorrect as OQ measures.",
      "startOffset" : 80,
      "endOffset" : 94
    }, {
      "referenceID" : 14,
      "context" : "As this is undefined if pi = 0, we use the more convenient Jensen-Shannon divergence (JSD) instead, which is symmetric (Lin, 1991):",
      "startOffset" : 119,
      "endOffset" : 130
    }, {
      "referenceID" : 17,
      "context" : "From SemEval2016/2017 Task 4 (Sentiment Analysis in Twitter) (Nakov et al., 2016; Rosenthal et al., 2017), we chose Subtask C as our OC tasks, and Subtask E as our OQ tasks for the reason given in Section 2.",
      "startOffset" : 61,
      "endOffset" : 105
    }, {
      "referenceID" : 20,
      "context" : "From SemEval2016/2017 Task 4 (Sentiment Analysis in Twitter) (Nakov et al., 2016; Rosenthal et al., 2017), we chose Subtask C as our OC tasks, and Subtask E as our OQ tasks for the reason given in Section 2.",
      "startOffset" : 61,
      "endOffset" : 105
    }, {
      "referenceID" : 32,
      "context" : "6 Moreover, for the OQ meta-evaluation experiments, we also utilise the DQ (Dialogue Quality) subtask data from NTCIR-15 DialEval-1 (Zeng et al., 2020).",
      "startOffset" : 132,
      "endOffset" : 151
    }, {
      "referenceID" : 32,
      "context" : "We utilise both Chinese and English DQ runs for our OQ meta-evaluation (22 runs in total), as the NTCIR task evaluates all runs using gold distributions that are based on the Chinese portion of the parallel dialogue corpus (Zeng et al., 2020).",
      "startOffset" : 223,
      "endOffset" : 242
    }, {
      "referenceID" : 20,
      "context" : "We do not use the Arabic data from 2017 as only two runs were submitted to Subtasks C and E (Rosenthal et al., 2017).",
      "startOffset" : 92,
      "endOffset" : 116
    }, {
      "referenceID" : 23,
      "context" : "Table 2 shows, for each OC task, the Kendall’s τ rank correlation values (Sakai, 2014) between two system rankings for every pair of measures.",
      "startOffset" : 73,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "For each measure, we evaluate its system ranking consistency (or “robustness” (Amigó et al., 2020)) across two topic sets as follows (Sakai, 2021): (1) randomly split the topic set in half, produce two system rankings based on the mean scores over each topic subset, and compute a Kendall’s τ score for the two rankings; (2) repeat the above 1,000 times and compute the mean τ ; (3) conduct a randomised paired Tukey HSD test at α = 0.",
      "startOffset" : 78,
      "endOffset" : 98
    }, {
      "referenceID" : 27,
      "context" : ", 2020)) across two topic sets as follows (Sakai, 2021): (1) randomly split the topic set in half, produce two system rankings based on the mean scores over each topic subset, and compute a Kendall’s τ score for the two rankings; (2) repeat the above 1,000 times and compute the mean τ ; (3) conduct a randomised paired Tukey HSD test at α = 0.",
      "startOffset" : 42,
      "endOffset" : 55
    }, {
      "referenceID" : 26,
      "context" : "Given a set of systems, a p-value for the difference in means is obtained for every system pair (preferrably with a multiple comparison procedure (Sakai, 2018b)); highly discriminative measures are those than can obtain many small p-values.",
      "startOffset" : 146,
      "endOffset" : 160
    }, {
      "referenceID" : 26,
      "context" : "The randomised version of this test is free from assumptions such as normality and random sampling from a population (Sakai, 2018b).",
      "startOffset" : 117,
      "endOffset" : 131
    }, {
      "referenceID" : 0,
      "context" : ", satisfy the ordinal invariance property (Amigó et al., 2020)), α-ORD can be used for OC tasks and NMD can be used for OQ tasks.",
      "startOffset" : 42,
      "endOffset" : 62
    } ],
    "year" : 2021,
    "abstractText" : "Ordinal Classification (OC) is an important classification task where the classes are ordinal. For example, an OC task for sentiment analysis could have the following classes: highly positive, positive, neutral, negative, highly negative. Clearly, evaluation measures for an OC task should penalise misclassifications by considering the ordinal nature of the classes (e.g., highly positive misclassified as positive vs. misclassifed as highly negative). Ordinal Quantification (OQ) is a related task where the gold data is a distribution over ordinal classes, and the system is required to estimate this distribution. Evaluation measures for an OQ task should also take the ordinal nature of the classes into account. However, for both OC and OQ, there are only a small number of known evaluation measures that meet this basic requirement. In the present study, we utilise data from the SemEval and NTCIR communities to clarify the properties of nine evaluation measures in the context of OC tasks, and six measures in the context of OQ tasks.",
    "creator" : "LaTeX with hyperref"
  }
}