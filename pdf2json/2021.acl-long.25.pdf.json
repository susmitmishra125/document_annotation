{
  "name" : "2021.acl-long.25.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning Language Specific Sub-network for Multilingual Machine Translation",
    "authors" : [ "Zehui Lin", "Liwei Wu", "Mingxuan Wang", "Lei Li" ],
    "emails" : [ "linzehui@bytedance.com", "wuliwei.000@bytedance.com", "wangmingxuan.89@bytedance.com", "lileilab@bytedance.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 293–305\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n293\nMultilingual neural machine translation aims at learning a single translation model for multiple languages. These jointly trained models often suffer from performance degradation on rich-resource language pairs. We attribute this degeneration to parameter interference. In this paper, we propose LaSS to jointly train a single unified multilingual MT model. LaSS learns Language Specific Sub-network (LaSS) for each language pair to counter parameter interference. Comprehensive experiments on IWSLT and WMT datasets with various Transformer architectures show that LaSS obtains gains on 36 language pairs by up to 1.2 BLEU. Besides, LaSS shows its strong generalization performance at easy adaptation to new language pairs and zero-shot translation. LaSS boosts zero-shot translation with an average of 8.3 BLEU on 30 language pairs. Codes and trained models are available at https: //github.com/NLP-Playground/LaSS."
    }, {
      "heading" : "1 Introduction",
      "text" : "Neural machine translation (NMT) has been very successful for bilingual machine translation (Bahdanau et al., 2015; Vaswani et al., 2017; Wu et al., 2016; Hassan et al., 2018; Su et al., 2018; Wang, 2019). Recent research has demonstrated the efficacy of multilingual NMT, which supports translation from multiple source languages into multiple target languages with a single model (Johnson et al., 2017; Aharoni et al., 2019; Zhang et al., 2020; Fan et al., 2020; Siddhant et al., 2020). Multilingual NMT enjoys the advantage of deployment. Further, the parameter sharing of multilingual NMT encourages transfer learning of different languages. An extreme case is zero-shot translation, where direct translation between a language pair never seen in training is possible (Johnson et al., 2017).\n∗Equal contribution.\nWhile very promising, several challenges remain in multilingual NMT. The most challenging one is related to the insufficient model capacity. Since multiple languages are accommodated in a single model, the modeling capacity of NMT model has to be split for different translation directions (Aharoni et al., 2019). Therefore, multilingual NMT models often suffer from performance degradation compared with their corresponding bilingual baseline, especially for rich-resource translation directions. The simplistic way to alleviate the insufficient model capacity is to enlarge the model parameters (Aharoni et al., 2019; Zhang et al., 2020). However, it is not parameter or computation efficient and needs larger multilingual training datasets to avoid over-fitting. An alternative solution is to design language-aware components, such as division of the hidden cells into shared and language-dependent ones (Wang et al., 2018), adaptation layers (Bapna and Firat, 2019; Philip et al., 2020), language-aware layer normalization\nand linear transformation (Zhang et al., 2020), and latent layers (Li et al., 2020).\nIn this work, we propose LaSS, a method to dynamically find and learn Language Specific Subnetwork for multilingual NMT. LaSS accommodates one sub-network for each language pair. Each sub-network has shared parameters with some other languages and, at the same time, preserves its language specific parameters. In this way, multilingual NMT can model language specific and language universal features for each language pair in one single model without interference. Figure 1 is the illustration of vanilla multilingual model and LaSS. Each language pair in LaSS has both language universal and language specific parameters. The network itself decides the sharing strategy.\nThe advantages of our proposed method are\n• LaSS is parameter efficient, requiring no extra trainable parameters to model language specific features. • LaSS alleviates parameter interference, potentially improving the model capacity and boosting performance. • LaSS shows its strong generalization performance at easy adaptation to new language pairs and zero-shot translation. LaSS can be easily extended to new language pairs without dramatic degradation of existing language pairs. Besides, LaSS can boost zero-shot translation by up to 26.5 BLEU."
    }, {
      "heading" : "2 Related Work",
      "text" : "Multilingual Neural Machine Translation The standard multilingual NMT model uses a shared encoder and a shared decoder for different languages (Johnson et al., 2017). There is a transfer-interference trade-off in this architecture (Arivazhagan et al., 2019): boosting the performance of low resource languages or maintain the performance of high resource languages. To solve this trade-off, previous works assign some parts of the model to be language specific: Language specific decoders (Dong et al., 2015), Language specific encoders and decoders (Firat et al., 2016; Lyu et al., 2020) and Language specific hidden states and embeds (Wang et al., 2018). Sachan and Neubig (2018) compares different sharing methods and finds different sharing methods have a great impact on performance. Recently, Zhang et al. (2021) analyze when and where language specific capacity matters. Li et al.\n(2020) uses a binary conditional latent variable to decide which language each layer belongs to.\nModel Pruning Our approach follows the standard pattern of model pruning: training, finding the sparse network and fine-tuning (Frankle and Carbin, 2019; Liu et al., 2019). Frankle and Carbin (2019) and Liu et al. (2019) highlight the importance of the sparse network architecture. Zhu and Gupta (2018) proposed a method to automatically adjust the sparse threshold. Sun et al. (2020) learns different sparse architecture for different tasks. Evci et al. (2020) iteratively redistribute the sparse network architecture by the gradient."
    }, {
      "heading" : "3 Methodology",
      "text" : "We describe LaSS method in this section. The goal is to learn a single unified model for many translation directions. Our overall idea is to find sub-networks corresponding to each language pair, and then only update the parameters of those subnetworks during the joint training."
    }, {
      "heading" : "3.1 Multilingual NMT",
      "text" : "A multilingual NMT model learns a mapping function f from a sentence in one of many languages to another language. We adopt the multilingual Transformer (mTransformer) as the backbone network (Johnson et al., 2017). mTransformer has the same encoder-decoder architecture with layers of multihead attention, residual connection, and layer normalization. In addition, it has two lanuage identifying tokens for the source and target. Define a multilingual dataset {Dsi→ti}Ni=1 where si, ti represents the source and target language.\nWe train an initial multilingual MT model with the following loss.\nL = ∑ i ∑ 〈x,y〉∼Dsi→ti − logPθ(y | x) (1)\nwhere 〈x,y〉 is a sentence pair from the language si to ti, and θ is the model parameter."
    }, {
      "heading" : "3.2 Finding Language Specific Model Masks",
      "text" : "Training a single model jointly on multiple language directions will lead to performance degradation for rich resource pairs (Johnson et al., 2017). The single model will improve on low resource language pairs, but will reduce performance on pairs like English-German. Intuitively, jointly training on all translation pairs will obtain an “average”\nmodel. For rich resources, such averaging may hurt the performance since a multilingual MT model must distribute its modeling capacity for all translation directions. Based on this intuition, our idea is to find a sub-network of the original multilingual model. Such sub-network is specific to each language pair.\nWe start from a multilingual base model θ0. The θ0 is trained with Eq. (1). A sub-network is indicated by a binary mask vector Msi→ti ∈ {0, 1}|θ| for language pair si → ti. Each element being 1 indicates to retain the weight and 0 to abandon the weight. Then the parameters associated with si → ti is θsi→ti = {θ j 0 | M j si→ti = 1}, where j denotes the jth element in θ0. The parameters θsi→ti are only responsible for the particular language si and ti. We intend to find such language specific sub-networks. Figure 1 illustrates the original model and its language specific sub-networks.\nGiven an initial model θ0, we adopt a simple method to find the language specific mask for each language pairs.\n1. Start with a multilingual MT model θ0 jointly trained on {Dsi→ti}Ni=1. 2. For each language pair si → ti, fine-tuning θ0 on Dsi→ti . Intuitively, fine-tuning θ0 on specific language pair si → ti will amplify the magnitude of the important weights for si → ti and diminish the magnitude of the unimportant weights. 3. Rank the weights in fine-tuned model and prune the lowest α percent. The mask Msi→ti is obtained by setting the remaining indices of parameters to be 1."
    }, {
      "heading" : "3.3 Structure-aware Joint Training",
      "text" : "Once we get masks Msi→ti for all language pairs, we further continue to train θ0 with languagegrouped batching and structure-aware updating.\nFirst, we create random batches of bilingual sentence pairs where each batch contains only samples from one pair. This is different from the plain joint multilingual training where each batch can contain fully random sentence pairs from all languages. Specifically, a batch Bsi→ti is randomly drawn from the language-specific data Dsi→ti . Second, we evaluate the loss in Eq. 1 on the batch Bsi→ti . During the back-propagation step, we only update the parameters in θ0 belonging to the sub-network indicated by Msi→ti . We iteratively update the parameters until convergence.\nIn this way, we still get a single final model θ∗ that is able to translate all language directions. During the inference, this model θ∗ and its masks Msi→ti , i = 1, . . . , N are used together to make predictions. For every given input sentence in language s and a target language t, the forward inference step only uses the parameter θ∗ Ms→t to calculate model output."
    }, {
      "heading" : "4 Experiment Settings",
      "text" : "Datasets and Evaluation The experiments are conducted on IWSLT and WMT benchmarks. For IWSLT, we collect 8 English-centric language pairs from IWSLT2014, whose size ranges from 89k to 169k. To simulate the scenarios of imbalanced datasets, we collect 18 language pairs ranging from low-resource (Gu, 11k) to rich-resource (Fr, 37m) from previous years’ WMT. The details of the datasets are listed in Appendix. We apply byte pair encoding (BPE) (Sennrich et al., 2016) to preprocess multilingual sentences, resulting in a vocabulary size of 30k for IWSLT and 64k for WMT. Besides, we apply over-sampling for IWSLT and WMT to balance the training data distribution with a temperature of T = 2 and T = 5 respectively. Similar to Lin et al. (2020), we divide the language pairs into 3 categories: low-resource (<1M), medium-resource (>1M and <10M) and rich resource (>10M).\nWe perform many-to-many multilingual translation throughout this paper, and add special language tokens at both the source and the target side. In all our experiments, we evaluate our model with commonly used standard testsets. For zeroshot, where standard testsets (for example, Fr→Zh) of some language pairs are not available, we use OPUS-100 (Zhang et al., 2020) testsets instead.\nWe report tokenized BLEU, as well as win ratio (WR), informing the proportion of language pairs we outperform the baseline. In zero-shot translation, we also report translation-language accuracy1, which is commonly used to measure the accuracy of translating into the right target language.\nModel Settings Considering the diversity of dataset volume, we perform our experiments with variants of Transformer architecture. For IWSLT, we adopt a smaller Transformer (Transformersmall2 (Wu et al., 2019)). For WMT, we adopt\n1https://github.com/Mimino666/ langdetect\n2Transformer-base with dff = 1024 and nhead = 4\nTransformer-base and Transformer-big3. The pruning rate α of IWSLT and WMT is 0.7 and 0.3, respectively. For simplicity, we only report the highest BLEU from the best pruning rate and we also discuss the impact of different pruning rate on performance in Sec.6. In Sec. 6 we discuss the relationship of performance and pruning rate. For more training details please refer to Appendix."
    }, {
      "heading" : "5 Experiment Results",
      "text" : "This section shows the efficacy and generalization of LaSS. Firstly, we show that LaSS obtains consistent performance gains on IWSLT and WMT datasets with different Transformer architecture variants. Further, we show that LaSS can easily generalize to new language pairs without losing the accuracy for previous language pairs. Finally, we observe that LaSS can even improve zero-shot translation, obtaining performance gains by up to 26.5 BLEU."
    }, {
      "heading" : "5.1 Main Results",
      "text" : "Results on IWSLT We first show our results on IWSLT. As shown in Table 1, LaSS consistently outperforms the multilingual baseline on all language pairs, confirming that using LaSS to alleviate parameter interference can help boost performance.\nResults on WMT To further verify the generalization of LaSS, we also conduct experiments on\n3For details of the Transformer setting, please refer to Vaswani et al. (2017)\nWMT, where the dataset is more imbalanced across different language pairs. We adopt two different Transformer architecture variants, i.e., Transformerbase and Transformer-big.\nAs shown in Table 2, LaSS obtains consistent gains over multilingual baseline on WMT for both Transformer-base and Transformer-big. For Transformer-base, LaSS achieves an average improvement of 1.2 BLEU on 36 language pairs over baseline, while for Transformer-big, LaSS obtains 0.6 BLEU improvement.\nWe observe that with the dataset scale of language pairs increasing, the improvements of BLEU and WR become larger, suggesting that the language pairs with large scale dataset benefit more from LaSS than language pairs of low resource. This phenomenon is intuitive since rich resource dataset suffers more parameter interference than low resource dataset. We also find that the BLEU and WR gains obtained in Transformer-base are larger than that in Transformer-large. We attribute it to the more severe parameter interference for smaller models.\nFor comparison, we also include the results of LaSS with randomly initialized masks. Not surprising, Random underperforms the baseline by a large margin, since Random intensifies rather than alleviates the parameter interference."
    }, {
      "heading" : "5.2 Generalization to New Language Pairs",
      "text" : "LaSS has shown its efficacy in the above section. A natural question arises that can LaSS adapt to a new language or language pair that it has not seen in training phase? In other words, can LaSS generalize to other language pairs? In this section, we show the generalization of LaSS in two settings. We firstly show that LaSS can easily adapt to new unseen languages to match bilingual models with training for only a few hundred steps while keeping the performance of the existing language pairs hardly dropping. Secondly, we show that LaSS can also boost performance in zero-shot translation scenario, obtaining performance gains by up to 26.5 BLEU.\nThe model is Transformer-big trained on WMT dataset. En↔Ar and En↔It are both unseen language pairs."
    }, {
      "heading" : "5.2.1 Extensibility to New Languages",
      "text" : "Previous works have studied the easy and rapid adaptation to a new task or language pair (Bapna and Firat, 2019; Rebuffi et al., 2017). We show\nthat LaSS can also easily adapt to new unseen languages without dramatic drop for other existing languages. We distribute a new sub-network to each new language pair and train the sub-network with the specific language pair for fixed steps. In this way, the new language pair will only update the corresponding parameters and it can alleviate the interference and catastrophic forgetting (Kirkpatrick et al., 2016) to other language pairs.\nWe verify the extensibility of LaSS on 4 language pairs. For LaSS, as described in Sec.3, we first fine-tune the multilingual base model and prune to obtain the specific mask for the new language pair. For both multilingual baseline and our method, we train on only the specific language pair for fixed steps.\nFigure 2 shows the trend of BLEU score along with the training steps. We observe that 1) LaSS consistently outperforms the multilingual baseline model along with the training steps. LaSS reaches the bilingual model performance with fewer steps. 2) Besides, the degradation of other language pairs is much smoother than the baseline. When reaching the bilingual baseline performance, LaSS hardly drops on other language pairs, while the multilingual baseline model dramatically drops by a large margin.\nWe attribute the easy adaptation for specific languages to the language specific sub-network. LaSS only updates the corresponding parameters, avoiding updating all parameters which will hurt the performance of other languages. Another benefit of updating corresponding parameters is its fast adaptation towards specific language pairs."
    }, {
      "heading" : "5.2.2 Zero-shot",
      "text" : "Zero-shot translation is the translation between known languages that the model has never seen\ntogether at training time (e.g., Fr→En and En→Zh are both seen in training phase, while Fr→Zh is not.). It is the ultimate goal of Multilingual NMT and has been a common indicator to measure the model capability (Johnson et al., 2017; Zhang et al., 2020). One of the biggest challenges is the offtarget issue (Zhang et al., 2020), which means that the model translates into a wrong target language.\nIn previous experiments, we apply specific masks to their corresponding language pairs. As the training dataset is English-centric, non-Englishcentric masks are not available. We remedy it by merging two masks to create non-English-centric masks. For example, We create X→Y mask by combining the encoder mask of X→En and the\nes it nl de pl ar fa he\nes it nl de pl ar fa he 47 48 49 50 51\n(a) En→X (x-axis and y-axis)\nes it nl de pl ar fa he\nes it nl de pl ar fa he\n59\n60\n61\n62\n63\n64\n65\n(b) X→En (x-axis and y-axis)\nes it nl de pl ar fa he\nes it nl de pl ar fa he\n48\n50\n52\n54\n56\n(c) X→En (x-axis) En→X(y-axis)\nFigure 3: Mask similarity for language pairs within En→X (x-axis and y-axis), within X→En (x-axis and yaxis) and between En→X (x-axis) and X→En (y-axis), respectively. The mask similarity is positively correlated to the language family similarity.\ndecoder mask of En→Y. We select 6 languages and evaluate zero-shot translation in language pairs between each other.\nAs shown in Table 3, surprisingly, by directly applying X→Y masks, LaSS obtains consistent gains over baselines in all language pairs for both BLEU and translation-language accuracy, indicating that the superiority of LaSS in learning to bridge between languages. It is worth noting that for Fr→Zh, LaSS outperforms the baseline by 26.5 BLEU, reaching 32 BLEU.\nWe also sample a few translation examples from Fr→Zh to analyze why LaSS can help boost zeroshot (More examples are listed in Appendix).\nAs shown in Table 4 as well as translationlanguage accuracy in Table 3, we observe that the multilingual baseline has severe off-target issue. As a counterpart, LaSS significantly alleviates the off-target issue, translating into the right target language. We attribute the success of “on-target” in zero-shot to the language specific parameters as a strong signal, apart from language indicator, to the model to translate into the target language."
    }, {
      "heading" : "6 Analysis and Discussion",
      "text" : "In this section, we conduct a set of analytic experiments to better understand the characteristics of language specific sub-network. We first measure the relationship between language specific subnetwork as well as its capacity and language family. Secondly, we study how masks affect performance in zero-shot scenario. Lastly, we discuss the relationship between pruning rate α and performance.\nWe conduct our analytic experiments on IWSLT dataset. For readers not familiar with language family and clustering, Figure 4 is the hierarchical clustering according to language family.\nes it nl de pl ar hefa\nRomance Germanic Slavic Arabic Iranian Semitic\nLatin Latin Latin Latin Latin Arabic Arabic Hebrew\nFigure 4: Language clustering of 8 languages in IWSLT, according to language family. Es(Spanish), It(Italian), De(Germany), Nl(Dutch) and Pl(Polish) are all European languages and written in Latin while Ar(Arabic), Fa(Farsi) and He(Hebrew) are similar languages."
    }, {
      "heading" : "6.1 Mask similarity v.s Language family",
      "text" : "Ideally, similar languages should share more parameters since they share more language characteristics. Therefore, a natural question arises: Does the model automatically capture the relationship of language family defined by human?\nWe calculate the similarity of masks between language pairs to measure the sub-network relationship between language pairs. We define mask similarity as the number of 1 where two masks share divided by the number of 1 of the first mask:\nSim(M1,M2) = ‖M1 ∩M2‖0 ‖M1‖0 , (2)\nwhere ‖·‖0 represent L0 norm. Mask similarity reflects the degree of sharing among different language pairs.\nFigure 3(a) and 3(b) shows the mask similarity in En→X and X→En. We observe that, for both En→X and X→En, the mask similarity is positively correlated to the language family similarity. The color of grids in Figure is deeper between similar languages (for example, es and it) while more shallow between dissimilar languages (for example, es and he).\nWe also plot the similarity between En→X and X→En in Figure 3(c) . We observe that, unlike En→X or X→En, the mask similarity does not correspond to language family similarity. We suspect that the mask similarity is determined by combination of source and target languages. That means that En→Nl does not necessarily share more parameters with Nl→En than En→De."
    }, {
      "heading" : "6.2 Where language specific capacity matters?",
      "text" : "To take a step further, we study how model schedule language specific capacity across layers. Figure 5\nshows the similarity of different components on the encoder and decoder side along with the increase of layer. More concretely, we plot query, key, value on the attention sub-layer and fully-connected layer on the positional-wise feed-forward sub-layer.\nWe observe that a) On both the encoder and decoder side, the model tends to distribute more language specific components on the top and bot-\ntom layers rather than the middle ones. This phenomenon is intuitive. The bottom layers deal more with embedding, which is language specific, while the top layers are near the output layer, which is also language specific. b) For fully-connected layer, the model tends to distribute more language specific capacity on the middle layers for the encoder, while distribute more language specific capacity in the decoder for the top layers."
    }, {
      "heading" : "6.3 How masks affect zero-shot?",
      "text" : "In Sec.4, we show that simply applying X→Y masks can boost zero-shot performance. We conduct experiments to analyze how masks affect zeroperformance. Concretely, we take Fr→Zh as an example, replacing the encoder or decoder mask with another language mask, respectively.\nAs shown in Table 5, we observe that replacing the encoder mask with other languages causes only littler performance drop, while replacing the decoder mask causes dramatic performance drop. It suggests that the decoder mask is the key ingredient of performance improvement."
    }, {
      "heading" : "6.4 About Sparsity",
      "text" : "To better understand the pruning rate, we plot the performance along with the increase of pruning\nrate in Figure 6. For WMT, the best choice for α is 0.3 for both Transformer-base and Transformer-big, while for IWSLT the best α lies between 0.6∼0.7. The results are consistent with our intuition, that large scale training data need a smaller pruning rate to keep the model capacity. Therefore, we suggest tuning α based on both the dataset and model size. For large datasets such as WMT, setting a smaller α is better, while a larger α will slightly decrease the performance (i.e. less than 0.5 BLEU score). For small datasets like IWSLT, setting a larger α may yield better performance."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we propose to learn LanguageSpecific Sub-network (LaSS) for multilingual\nNMT. Extensive experiments on IWSLT and WMT have shown that LaSS is able to alleviate parameter interference and boost performance. Further, LaSS can generalize well to new language pairs by training with a few hundred steps, while keeping the performance of existing language pairs. Surprisingly, in zero-shot translation, LaSS surpasses the multilingual baseline by up to 26.5 BLEU. Extensive analytic experiments are conducted to understand the characteristics of language specific sub-network. Future work includes designing a more dedicated end-to-end training strategy and incorporating the insight we gain from analysis to design a further improved LaSS."
    }, {
      "heading" : "A Appendices",
      "text" : "A.1 Datasets Details\nA.2 Training Details As stated in the previous section, we first train a multilingual baseline (Phase 1). Then we fine-tune the baseline on specific language pair to obtain the mask (Phase 2). After that we train the LaSS model with the obtained masks (Phase 3). Note that we only apply masks on linear weights, which means that the embedding weights, layer normalization are not masked out. We also exclude the output projection weight. We apply label smoothing of value 0.1 in all our experiments.\nA.2.1 IWSLT Model We adopt Transformer-small 4 with dropout 0.1.\nData Following Tan et al. (2019), we first tokenize the data then apply BPE. The BPE vocab size is 30k. We apply over-sampling with a temperature of T = 2.\nTraining For Phase 1, we train the baseline with Adam with a learning rate schedule of (5e-4,4k). The max tokens per batch is set to 262144. For Phase 2, we keep all other settings unchanged except we set the max tokens to be 16384 and the dropout 0.3. For Phase 3, we keep the same setting as Phase 1, except we apply masks on the model.\nA.2.2 WMT Model We adopt Transformer-base and Transformer-big with pre-norm (Wang et al.,\n4Transformer-base with dff = 1024 and nhead = 4\n2019). We replace fixed positional embedding with learnable one and replace ReLU with GeLU. Also we use Layernorm-embedding (Liu et al., 2020) to stabilize training.\nData We use SentencePiece (Kudo and Richardson, 2018) to preprocess the data and learn BPE. Since the WMT dataset is highly imbalanced, we apply a temperature-based sampling strategy with T = 5. To ensure all languages are represented adequately in the vocabulary, we apply the same temperature-based sampling strategy for training\nthe BPE model.\nTraining For Phase 1, we train the baseline with Adam with a learning rate schedule of (5e-4,8k). The max tokens per batch is set to 524288. For Phase 2, the warm-up updates are set to 1000. To guarantee that the model does not overfit the data, we train on different language pairs with different steps and different batch size. Concretely, we finetune on >10k, >100k, >1m, >10m language pairs with 1k, 2k, 4k, 8k steps and max tokens per batch with 20480, 40960, 81920 and 163840. For Phase 3, we keep the setting the same as Phase 1.\nA.3 Case Study\n305\nFr→ Zh\nSrc La production annuelle d’acier était le symbole incontesté de la vigueur économique des nations. Ref 钢的年产量是国家经济实力的重要象征 Baseline Annual steel production was the undisputed symbol of nations’ economic strength. LaSS 年度钢铁生产是各国经济活力的无可争辩的象征.\nSrc De l’avis de ma délégation donc, l’ONU devrait élargir ces activités de la faon suivante. Ref 因此,我国代表团认为,联合国现在应该以下述方式扩大这些活动。 Baseline 因此, in my delegation’s view, the United Nations should expand these activities in the following manner. LaSS 因此,我国代表团认为,联合国应该扩大这些活动,如下.\nSrc Le domicile de la femme dépendait du lieu du mariage et de la résidence familiale. Ref 妇女的住处取决于婚姻和家庭位置。 Baseline The woman’s place of residence depended on the place of marriage and family residence. LaSS 妻子的住所取决于婚姻地点和家庭住所.\nDe→ Zh\nSrc Du bist gebissen worden. Ref 你被咬了 Baseline You have been bitten. LaSS 你被咬了\nSrc Einmal würde schon reichen. Ref 你只需要道歉一次就够了! Baseline Once upon a time it would be enough. LaSS 一次就足够了.\nSrc Wenn wir warten, hat er Zeit zum Tanken und Munitionieren. Ref 如果我们等待,他就有了时间加油和补给弹药 Baseline When we wait, he has time for tanks and ammunition. LaSS 当我们等待时,他有时间去坦克和弹药.\nRu→ Zh\nSrc Помощник заместителя министра здравоохранения Саудовской Аравии\nЕго Превосходительство д-р Якуб бен Юсуфаль-Масрува\nRef 沙特阿拉伯卫生部助理副部长雅各布·本·优素福·马斯如瓦博士阁下 Baseline Dr Yakub bin Yusuf al-Masruva, Deputy Minister of Health of Saudi Arabia LaSS 沙特阿拉伯卫生部副部长的助理,His Excellency Dr Yakub bin Yusuf al-Masruva\nSrc Не хочу я, чтобы Пит показывал нам фото, Элли. Ref 我不要皮特给我们看照片艾莉 Baseline I don’t want Pete showing us a photo, Elly. LaSS 我不想让皮特给我们看一下照片,艾丽.\nSrc Роджерс!Ясказал,встатьвстрой! Ref 罗杰斯我说跟上 Baseline 罗吉尔斯! I said, get up! LaSS 罗吉尔斯,我说,你要站起来!\nTable 9: Case Study"
    } ],
    "references" : [ {
      "title" : "Massively multilingual neural machine translation",
      "author" : [ "Roee Aharoni", "Melvin Johnson", "Orhan Firat." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Aharoni et al\\.,? 2019",
      "shortCiteRegEx" : "Aharoni et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural machine translation by jointly",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Simple, scalable adaptation for neural machine translation",
      "author" : [ "Ankur Bapna", "Orhan Firat." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural",
      "citeRegEx" : "Bapna and Firat.,? 2019",
      "shortCiteRegEx" : "Bapna and Firat.",
      "year" : 2019
    }, {
      "title" : "Multi-task learning for multiple language translation",
      "author" : [ "Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
      "citeRegEx" : "Dong et al\\.,? 2015",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2015
    }, {
      "title" : "Rigging the lottery: Making all tickets winners",
      "author" : [ "Utku Evci", "Trevor Gale", "Jacob Menick", "Pablo Samuel Castro", "Erich Elsen." ],
      "venue" : "Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume",
      "citeRegEx" : "Evci et al\\.,? 2020",
      "shortCiteRegEx" : "Evci et al\\.",
      "year" : 2020
    }, {
      "title" : "Beyond english-centric multilingual machine translation",
      "author" : [ "Michael Auli", "Armand Joulin." ],
      "venue" : "CoRR, abs/2010.11125.",
      "citeRegEx" : "Auli and Joulin.,? 2020",
      "shortCiteRegEx" : "Auli and Joulin.",
      "year" : 2020
    }, {
      "title" : "Multi-way, multilingual neural machine translation with a shared attention mechanism",
      "author" : [ "Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Firat et al\\.,? 2016",
      "shortCiteRegEx" : "Firat et al\\.",
      "year" : 2016
    }, {
      "title" : "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
      "author" : [ "Jonathan Frankle", "Michael Carbin." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
      "citeRegEx" : "Frankle and Carbin.,? 2019",
      "shortCiteRegEx" : "Frankle and Carbin.",
      "year" : 2019
    }, {
      "title" : "Achieving human parity on automatic chinese to english news translation",
      "author" : [ "Zhou." ],
      "venue" : "CoRR, abs/1803.05567.",
      "citeRegEx" : "Zhou.,? 2018",
      "shortCiteRegEx" : "Zhou.",
      "year" : 2018
    }, {
      "title" : "Overcoming catastrophic forgetting in neural networks",
      "author" : [ "Hadsell." ],
      "venue" : "CoRR, abs/1612.00796.",
      "citeRegEx" : "Hadsell.,? 2016",
      "shortCiteRegEx" : "Hadsell.",
      "year" : 2016
    }, {
      "title" : "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "author" : [ "Taku Kudo", "John Richardson." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP",
      "citeRegEx" : "Kudo and Richardson.,? 2018",
      "shortCiteRegEx" : "Kudo and Richardson.",
      "year" : 2018
    }, {
      "title" : "Deep transformers with latent depth",
      "author" : [ "Xian Li", "Asa Cooper Stickland", "Yuqing Tang", "Xiang Kong." ],
      "venue" : "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, De-",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Pretraining multilingual neural machine translation by leveraging alignment information",
      "author" : [ "Zehui Lin", "Xiao Pan", "Mingxuan Wang", "Xipeng Qiu", "Jiangtao Feng", "Hao Zhou", "Lei Li." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in",
      "citeRegEx" : "Lin et al\\.,? 2020",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "Multilingual denoising pre-training for neural machine translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "CoRR, abs/2001.08210.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Rethinking the value of network pruning",
      "author" : [ "Zhuang Liu", "Mingjie Sun", "Tinghui Zhou", "Gao Huang", "Trevor Darrell." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Revisiting modularized multilingual NMT to meet industrial demands",
      "author" : [ "Sungwon Lyu", "Bokyung Son", "Kichang Yang", "Jaekyoung Bae." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020,",
      "citeRegEx" : "Lyu et al\\.,? 2020",
      "shortCiteRegEx" : "Lyu et al\\.",
      "year" : 2020
    }, {
      "title" : "Monolingual adapters for zero-shot neural machine translation",
      "author" : [ "Jerin Philip", "Alexandre Berard", "Matthias Gallé", "Laurent Besacier." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, On-",
      "citeRegEx" : "Philip et al\\.,? 2020",
      "shortCiteRegEx" : "Philip et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning multiple visual domains with residual adapters",
      "author" : [ "Sylvestre-Alvise Rebuffi", "Hakan Bilen", "Andrea Vedaldi." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017,",
      "citeRegEx" : "Rebuffi et al\\.,? 2017",
      "shortCiteRegEx" : "Rebuffi et al\\.",
      "year" : 2017
    }, {
      "title" : "Parameter sharing methods for multilingual selfattentional translation models",
      "author" : [ "Devendra Singh Sachan", "Graham Neubig." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, WMT 2018, Belgium, Brussels, October 31 -",
      "citeRegEx" : "Sachan and Neubig.,? 2018",
      "shortCiteRegEx" : "Sachan and Neubig.",
      "year" : 2018
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Ger-",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Leveraging monolingual data with self-supervision for multilingual neural machine translation",
      "author" : [ "Aditya Siddhant", "Ankur Bapna", "Yuan Cao", "Orhan Firat", "Mia Xu Chen", "Sneha Reddy Kudugunta", "Naveen Arivazhagan", "Yonghui Wu." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Siddhant et al\\.,? 2020",
      "shortCiteRegEx" : "Siddhant et al\\.",
      "year" : 2020
    }, {
      "title" : "Variational recurrent neural machine translation",
      "author" : [ "Jinsong Su", "Shan Wu", "Deyi Xiong", "Yaojie Lu", "Xianpei Han", "Biao Zhang." ],
      "venue" : "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications",
      "citeRegEx" : "Su et al\\.,? 2018",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning sparse sharing architectures for multiple tasks",
      "author" : [ "Tianxiang Sun", "Yunfan Shao", "Xiaonan Li", "Pengfei Liu", "Hang Yan", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-",
      "citeRegEx" : "Sun et al\\.,? 2020",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Multilingual neural machine",
      "author" : [ "Xu Tan", "Yi Ren", "Di He", "Tao Qin", "Zhou Zhao", "Tie-Yan Liu" ],
      "venue" : null,
      "citeRegEx" : "Tan et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Towards linear time neural machine translation with capsule networks",
      "author" : [ "Mingxuan Wang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro-",
      "citeRegEx" : "Wang.,? 2019",
      "shortCiteRegEx" : "Wang.",
      "year" : 2019
    }, {
      "title" : "Learning deep transformer models for machine translation",
      "author" : [ "Qiang Wang", "Bei Li", "Tong Xiao", "Jingbo Zhu", "Changliang Li", "Derek F. Wong", "Lidia S. Chao." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguis-",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Three strategies to improve one-to-many multilingual translation",
      "author" : [ "Yining Wang", "Jiajun Zhang", "Feifei Zhai", "Jingfang Xu", "Chengqing Zong." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Bel-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Pay less attention with lightweight and dynamic convolutions",
      "author" : [ "Felix Wu", "Angela Fan", "Alexei Baevski", "Yann N. Dauphin", "Michael Auli." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Share or not? learning to schedule language-specific capacity for multilingual translation",
      "author" : [ "Biao Zhang", "Ankur Bapna", "Rico Sennrich", "Orhan Firat." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Improving massively multilingual neural machine translation and zero-shot translation",
      "author" : [ "Biao Zhang", "Philip Williams", "Ivan Titov", "Rico Sennrich." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1628–",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "To prune, or not to prune: Exploring the efficacy of pruning for model compression",
      "author" : [ "Michael Zhu", "Suyog Gupta." ],
      "venue" : "6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Work-",
      "citeRegEx" : "Zhu and Gupta.,? 2018",
      "shortCiteRegEx" : "Zhu and Gupta.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Neural machine translation (NMT) has been very successful for bilingual machine translation (Bahdanau et al., 2015; Vaswani et al., 2017; Wu et al., 2016; Hassan et al., 2018; Su et al., 2018; Wang, 2019).",
      "startOffset" : 92,
      "endOffset" : 204
    }, {
      "referenceID" : 24,
      "context" : "Neural machine translation (NMT) has been very successful for bilingual machine translation (Bahdanau et al., 2015; Vaswani et al., 2017; Wu et al., 2016; Hassan et al., 2018; Su et al., 2018; Wang, 2019).",
      "startOffset" : 92,
      "endOffset" : 204
    }, {
      "referenceID" : 21,
      "context" : "Neural machine translation (NMT) has been very successful for bilingual machine translation (Bahdanau et al., 2015; Vaswani et al., 2017; Wu et al., 2016; Hassan et al., 2018; Su et al., 2018; Wang, 2019).",
      "startOffset" : 92,
      "endOffset" : 204
    }, {
      "referenceID" : 25,
      "context" : "Neural machine translation (NMT) has been very successful for bilingual machine translation (Bahdanau et al., 2015; Vaswani et al., 2017; Wu et al., 2016; Hassan et al., 2018; Su et al., 2018; Wang, 2019).",
      "startOffset" : 92,
      "endOffset" : 204
    }, {
      "referenceID" : 0,
      "context" : "Recent research has demonstrated the efficacy of multilingual NMT, which supports translation from multiple source languages into multiple target languages with a single model (Johnson et al., 2017; Aharoni et al., 2019; Zhang et al., 2020; Fan et al., 2020; Siddhant et al., 2020).",
      "startOffset" : 176,
      "endOffset" : 281
    }, {
      "referenceID" : 30,
      "context" : "Recent research has demonstrated the efficacy of multilingual NMT, which supports translation from multiple source languages into multiple target languages with a single model (Johnson et al., 2017; Aharoni et al., 2019; Zhang et al., 2020; Fan et al., 2020; Siddhant et al., 2020).",
      "startOffset" : 176,
      "endOffset" : 281
    }, {
      "referenceID" : 20,
      "context" : "Recent research has demonstrated the efficacy of multilingual NMT, which supports translation from multiple source languages into multiple target languages with a single model (Johnson et al., 2017; Aharoni et al., 2019; Zhang et al., 2020; Fan et al., 2020; Siddhant et al., 2020).",
      "startOffset" : 176,
      "endOffset" : 281
    }, {
      "referenceID" : 0,
      "context" : "Since multiple languages are accommodated in a single model, the modeling capacity of NMT model has to be split for different translation directions (Aharoni et al., 2019).",
      "startOffset" : 149,
      "endOffset" : 171
    }, {
      "referenceID" : 0,
      "context" : "The simplistic way to alleviate the insufficient model capacity is to enlarge the model parameters (Aharoni et al., 2019; Zhang et al., 2020).",
      "startOffset" : 99,
      "endOffset" : 141
    }, {
      "referenceID" : 30,
      "context" : "The simplistic way to alleviate the insufficient model capacity is to enlarge the model parameters (Aharoni et al., 2019; Zhang et al., 2020).",
      "startOffset" : 99,
      "endOffset" : 141
    }, {
      "referenceID" : 27,
      "context" : "An alternative solution is to design language-aware components, such as division of the hidden cells into shared and language-dependent ones (Wang et al., 2018), adaptation layers (Bapna and Firat, 2019; Philip et al.",
      "startOffset" : 141,
      "endOffset" : 160
    }, {
      "referenceID" : 2,
      "context" : ", 2018), adaptation layers (Bapna and Firat, 2019; Philip et al., 2020), language-aware layer normalization",
      "startOffset" : 27,
      "endOffset" : 71
    }, {
      "referenceID" : 16,
      "context" : ", 2018), adaptation layers (Bapna and Firat, 2019; Philip et al., 2020), language-aware layer normalization",
      "startOffset" : 27,
      "endOffset" : 71
    }, {
      "referenceID" : 30,
      "context" : "294 and linear transformation (Zhang et al., 2020), and latent layers (Li et al.",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 3,
      "context" : "To solve this trade-off, previous works assign some parts of the model to be language specific: Language specific decoders (Dong et al., 2015), Language specific encoders and decoders (Firat et al.",
      "startOffset" : 123,
      "endOffset" : 142
    }, {
      "referenceID" : 6,
      "context" : ", 2015), Language specific encoders and decoders (Firat et al., 2016; Lyu et al., 2020) and Language specific hidden states and embeds (Wang et al.",
      "startOffset" : 49,
      "endOffset" : 87
    }, {
      "referenceID" : 15,
      "context" : ", 2015), Language specific encoders and decoders (Firat et al., 2016; Lyu et al., 2020) and Language specific hidden states and embeds (Wang et al.",
      "startOffset" : 49,
      "endOffset" : 87
    }, {
      "referenceID" : 27,
      "context" : ", 2020) and Language specific hidden states and embeds (Wang et al., 2018).",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 7,
      "context" : "Model Pruning Our approach follows the standard pattern of model pruning: training, finding the sparse network and fine-tuning (Frankle and Carbin, 2019; Liu et al., 2019).",
      "startOffset" : 127,
      "endOffset" : 171
    }, {
      "referenceID" : 14,
      "context" : "Model Pruning Our approach follows the standard pattern of model pruning: training, finding the sparse network and fine-tuning (Frankle and Carbin, 2019; Liu et al., 2019).",
      "startOffset" : 127,
      "endOffset" : 171
    }, {
      "referenceID" : 19,
      "context" : "We apply byte pair encoding (BPE) (Sennrich et al., 2016) to preprocess multilingual sentences, resulting in a vocabulary size of 30k for IWSLT and 64k for WMT.",
      "startOffset" : 34,
      "endOffset" : 57
    }, {
      "referenceID" : 30,
      "context" : "For zeroshot, where standard testsets (for example, Fr→Zh) of some language pairs are not available, we use OPUS-100 (Zhang et al., 2020) testsets instead.",
      "startOffset" : 117,
      "endOffset" : 137
    }, {
      "referenceID" : 28,
      "context" : "For IWSLT, we adopt a smaller Transformer (Transformersmall2 (Wu et al., 2019)).",
      "startOffset" : 61,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : "Previous works have studied the easy and rapid adaptation to a new task or language pair (Bapna and Firat, 2019; Rebuffi et al., 2017).",
      "startOffset" : 89,
      "endOffset" : 134
    }, {
      "referenceID" : 17,
      "context" : "Previous works have studied the easy and rapid adaptation to a new task or language pair (Bapna and Firat, 2019; Rebuffi et al., 2017).",
      "startOffset" : 89,
      "endOffset" : 134
    }, {
      "referenceID" : 30,
      "context" : "It is the ultimate goal of Multilingual NMT and has been a common indicator to measure the model capability (Johnson et al., 2017; Zhang et al., 2020).",
      "startOffset" : 108,
      "endOffset" : 150
    }, {
      "referenceID" : 30,
      "context" : "One of the biggest challenges is the offtarget issue (Zhang et al., 2020), which means that the model translates into a wrong target language.",
      "startOffset" : 53,
      "endOffset" : 73
    } ],
    "year" : 2021,
    "abstractText" : "Multilingual neural machine translation aims at learning a single translation model for multiple languages. These jointly trained models often suffer from performance degradation on rich-resource language pairs. We attribute this degeneration to parameter interference. In this paper, we propose LaSS to jointly train a single unified multilingual MT model. LaSS learns Language Specific Sub-network (LaSS) for each language pair to counter parameter interference. Comprehensive experiments on IWSLT and WMT datasets with various Transformer architectures show that LaSS obtains gains on 36 language pairs by up to 1.2 BLEU. Besides, LaSS shows its strong generalization performance at easy adaptation to new language pairs and zero-shot translation. LaSS boosts zero-shot translation with an average of 8.3 BLEU on 30 language pairs. Codes and trained models are available at https: //github.com/NLP-Playground/LaSS.",
    "creator" : "LaTeX with hyperref"
  }
}