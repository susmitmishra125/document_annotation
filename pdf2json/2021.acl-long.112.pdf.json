{
  "name" : "2021.acl-long.112.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Factorising Meaning and Form for Intent-Preserving Paraphrasing",
    "authors" : [ "Tom Hosking", "Mirella Lapata" ],
    "emails" : [ "tom.hosking@ed.ac.uk", "mlap@inf.ed.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1405–1418\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1405"
    }, {
      "heading" : "1 Introduction",
      "text" : "A paraphrase of an utterance is “an alternative surface form in the same language expressing the same semantic content as the original form” (Madnani and Dorr, 2010). For questions, a paraphrase should have the same intent, and should lead to the same answer as the original, as in the examples in Table 1. Question paraphrases are of significant interest, with applications in data augmentation (Iyyer et al., 2018), query rewriting (Dong et al., 2017) and duplicate question detection (Shah et al., 2018), as they allow a system to better identify the underlying intent of a user query.\nRecent approaches to paraphrasing use information bottlenecks with VAEs (Bowman et al., 2016) or pivot languages (Wieting and Gimpel, 2018) to try to extract the semantics of an input utterance, before projecting back to a (hopefully different) surface form. However, these methods have lit-\ntle to no control over the preservation of the input meaning or variation in the output surface form. Other work has specified the surface form to be generated (Iyyer et al., 2018; Chen et al., 2019a; Kumar et al., 2020), but has so far assumed that the set of valid surface forms is known a priori.\nIn this paper, we propose SEPARATOR, a method for generating paraphrases that exhibit high variation in surface form while still retaining the original intent. Our key innovations are: (a) to train a model to reconstruct a target question from an input paraphrase with the same meaning, and an exemplar with the same surface form, and (b) to separately encode the form and meaning of questions as discrete and continuous latent variables respectively, enabling us to modify the output surface form while preserving the original question intent. Crucially, unlike prior work on syntax controlled paraphrasing, we show that we can generate diverse paraphrases of an input question at test time by inferring a different discrete syntactic encoding, without needing access to reference exemplars.\nWe limit our work to English questions for three reasons: (a) the concept of a paraphrase is more\nclearly defined for questions compared to generic utterances, as question paraphrases should lead to the same answer; (b) the space of possible surface forms is smaller for questions, making the task more achievable, and (c) better dataset availability. However, our approach does not otherwise make any assumptions specific to questions."
    }, {
      "heading" : "2 Problem Formulation",
      "text" : "The task is to learn a mapping from an input question, represented as a sequence of tokens X, to paraphrase(s) Y which have different surface form to X, but convey the same intent.\nOur proposed approach, which we call SEPARATOR, uses an encoder-decoder model to transform an input question into a latent encoding space, and then back to an output paraphrase. We hypothesize that a principled information bottleneck (Section 2.1) and a careful choice of training scheme (Section 2.2) lead to an encoding space that separately represents the intent and surface form. This separation enables us to paraphrase the input question, varying the surface form of the output by directly manipulating the syntactic encoding of the input and keeping the semantic encoding constant (Section 2.3). We assume access to reference paraphrase clusters during training (e.g., Table 1), sets of questions with different surface forms that have been collated as having the same meaning or intent.\nOur model is a variant of the standard encoderdecoder framework (Cho et al., 2014), and consists of: (a) a vanilla Transformer sentence encoder (Vaswani et al., 2017), that maps an input\nquestion X to a multi-head sequence of encodings, eh,t = ENCODER(X); (b) a principled choice of information bottleneck, with a continuous variational path and a discrete vector-quantized path, that maps the encoding sequence to a pair of latent vectors, zsem, zsyn = BOTTLENECK(eh,t), represented in more detail in Figure 1; (c) a vanilla Transformer decoder, that attends over the latent vectors to generate a sequence of output tokens, Ŷ = DECODER(zsem, zsyn). The separation between zsem and zsyn is induced by our proposed training scheme, shown in Figure 1 and described in detail in Section 2.2."
    }, {
      "heading" : "2.1 Model Architecture",
      "text" : "While the encoder and decoder used by the model are standard Transformer modules, our bottleneck is more complex and we now describe it in more detail.\nLet the encoder output be {eh,1, . . . , eh,|X|} = ENCODER(X), where eh,t ∈ RD/HT , h ∈ 1, ...,HT with HT the number of transformer heads, |X| the length of the input sequence and D the dimension of the transformer. We first pool this sequence of encodings to a single vector, using the multi-head pooling described in Liu and Lapata (2019). For each head h, we calculate a distribution over time indexes αh,t using attention:\nαh,t = exp ah,t∑\nt′∈|X| exp ah,t′ , (1)\nah,t = kTh eh,t, (2)\nwith kh ∈ RD/H a learned parameter.\nWe then take a weighted average of a linear projection of the encodings, to give pooled output ẽh,\nẽh = ∑ t′∈|X| αh,t′Vheh,t′ , (3)\nwith Vh ∈ RD/H×D/H a learned parameter. Transformer heads are assigned either to a semantic group Hsem, that will be trained to encode the intent of the input, ẽsem = [. . . ; ẽh; . . .], h ∈ Hsem, or to a syntactic group Hsyn, that will be trained to represent the surface form ẽsyn = [. . . ; ẽh; . . .], h ∈ Hsyn (see Figure 1).\nThe space of possible question intents is extremely large and may be reasonably approximated by a continuous vector space. However, the possible surface forms are discrete and smaller in number. We therefore use a Vector-Quantized Variational Autoencoder (VQ-VAE, van den Oord et al., 2017) for the syntactic encoding zsyn, and model the semantic encoding zsem as a continuous Gaussian latent variable, as shown in the upper and lower parts of Figure 1, respectively.\nVector Quantization Let qh be discrete latent variables corresponding to the syntactic quantizer heads, h ∈ Hsyn.1 Each variable can be one of K possible latent codes, qh ∈ [0,K]. The heads use distinct codebooks, Ch ∈ RK×D/H , which map each discrete code to a continuous embedding Ch(qh) ∈ RD/H . Given sentence X and its pooled encoding {ẽ1, ..., ẽH}, we independently quantize the syntactic subset of the heads h ∈ Hsyn to their nearest codes from Ch and concatenate, giving the syntactic encoding\nzsyn = [C1(q1); . . . ;C|Hsyn|(q|Hsyn|)]. (4)\nThe quantizer module is trained through backpropagation using straight-through estimation (Bengio et al., 2013), with an additional loss term to constrain the embedding space as described in van den Oord et al. (2017),\nLcstr = λ ∑\nh∈Hsyn\n∥∥∥(ẽh − sg(Ch(qh)))∥∥∥ 2 , (5)\nwhere the stopgradient operator sg(·) is defined as identity during forward computation and zero on backpropagation, and λ is a weight that controls the strength of the constraint. We follow the soft\n1The number and dimensionality of the quantizer heads need not be the same as the number of transformer heads.\nEM and exponentially moving averages training approaches described in earlier work (Roy et al., 2018; Angelidis et al., 2021), which we find improve training stability.\nVariational Bottleneck For the semantic path, we introduce a learned Gaussian posterior, that represents the encodings as smooth distributions in space instead of point estimates (Kingma and Welling, 2014). Formally, φ(zh|eh) ∼ N (µ(eh),σ(eh)), where µ(·) and σ(·) are learned linear transformations. To avoid vanishingly small variance and to encourage a smooth distribution, a prior is introduced, p(zh) ∼ N (0, 1). The VAE objective is the standard evidence lower bound (ELBO), given by\nELBO = −KL[φ(zh|eh)||p(zh)] + Eφ[log p(eh|zh)]. (6)\nWe use the usual Gaussian reparameterisation trick, and approximate the expectation in Equation (6) by sampling from the training set and updating via backpropagation (Kingma and Welling, 2014). The VAE component therefore only adds an additional KL term to the overall loss,\nLKL = −KL[φ(zh|eh)||p(zh)]. (7)\nIn sum, BOTTLENECK(eh,t) maps a sequence of token encodings to a pair of vectors zsem, zsyn, with zsem a continuous latent Gaussian, and zsyn a combination of discrete code embeddings."
    }, {
      "heading" : "2.2 Factorised Reconstruction Objective",
      "text" : "We now describe the training scheme that causes the model to learn separate encodings for meaning and form: zsem should encode only the intent of the input, while zsyn should capture any information about the surface form of the input. Although we refer to zsyn as the syntactic encoding, it will not necessarily correspond to any specific syntactic formalism. We also acknowledge that meaning and form are not completely independent of each other; arbitrarily changing the form of an utterance is likely to change its meaning. However, it is possible for the same intent to have multiple phrasings , and it is this ‘local independence’ that we intend to capture.\nWe create triples {Xsem,Xsyn,Y}, where Xsem has the same meaning but different form to Y (i.e., it is a paraphrase, as in Table 1) and Xsyn is a question with the same form but different meaning\n(i.e., it shares the same syntactic template as Y), which we refer to as an exemplar. We describe the method for retrieving these exemplars in Section 2.3. The model is then trained to generate a target paraphrase Y from the semantic encoding zsem of the input paraphrase Xsem, and from the syntactic encoding zsyn of the exemplar Xsyn, as demonstrated in Figure 1.\nRecalling the additional losses from the variational and quantized bottlenecks, the final combined training objective is given by\nL = LY + Lcstr + LKL, (8)\nwhere LY(Xsem,Xsyn) is the cross-entropy loss of teacher-forcing the decoder to generate Y from zsem(Xsem) and zsyn(Xsyn)."
    }, {
      "heading" : "2.3 Exemplars",
      "text" : "It is important to note that not all surface forms are valid or licensed for all question intents. As shown in Figure 1, our approach requires exemplars during training to induce the separation between latent spaces. We also need to specify the desired surface form at test time, either by supplying an exemplar as input or by directly predicting the latent codes. The output should have a different surface form to the input but remain fluent.\nExemplar Construction During training, we retrieve exemplars Xsyn from the training data following a process which first identifies the underlying syntax of Y, and finds a question with the same syntactic structure but a different, arbitrary meaning. We use a shallow approximation of syntax, to ensure the availability of equivalent exemplars in the training data. An example of the exemplar retrieval process is shown in Table 2; we first apply a chunker (FlairNLP, Akbik et al., 2018) to Y, then extract the chunk label for each tagged span, ignoring stopwords. This gives us the template that Y\nfollows. We then select a question at random from the training data with the same template to give Xsyn. If no other questions in the dataset use this template, we create an exemplar by replacing each chunk with a random sample of the same type.\nWe experimented with a range of approaches to determining question templates, including using part-of-speech tags and (truncated) constituency parses. We found that using chunks and preserving stopwords gave a reasonable level of granularity while still combining questions with a similar form. The templates (and corresponding exemplars) need to be granular enough that the model is forced to use them, but abstract enough that the task is not impossible to learn.\nPrediction at Test Time In general, we do not assume access to reference exemplars at test time and yet the decoder must generate a paraphrase from semantic and syntactic encodings. Since our latent codes are separated, we can directly predict the syntactic encoding, without needing to retrieve or generate an exemplar. Furthermore, by using a discrete representation for the syntactic space, we reduce this prediction problem to a simple classification task. Formally, for an input question X, we learn a distribution over licensed discrete codes qh, h ∈ H̃syn. We assume that the heads are independent, so that p(q1, . . . , qH̃syn) = ∏ i p(qi). We use a small fully connected network with the semantic and syntactic encodings of X as inputs, giving p(qh|X) = MLP(zsem(X), zsyn(X)).\nThe network is trained to maximize the likelihood of all other syntactic codes licensed by each input. We calculate the discrete syntactic codes for each question in a paraphrase cluster, and minimize the cross-entropy loss of the network with respect to these codes. At test time, we set qh = argmaxq′h [p(q ′ h|Xtest)]."
    }, {
      "heading" : "3 Experimental Setup",
      "text" : "Datasets We evaluate our approach on two datasets: Paralex (Fader et al., 2013), a dataset of question paraphrase clusters scraped from WikiAnswers; and Quora Question Pairs (QQP)2 sourced from the community question answering forum Quora. We observed that a significant fraction of the questions in Paralex included typos or were ungrammatical. We therefore filter out any questions marked as non-English by a language detection\n2https://www.kaggle.com/c/quora-question-pairs\nscript (Lui and Baldwin, 2012), then pass the questions through a simple spellchecker. While this destructively edited some named entities in the questions, it did so in a consistent way across the whole dataset. There is no canonical split for Paralex, so we group the questions into clusters of paraphrases, and split these clusters into train/dev/test partitions with weighting 80/10/10. Similarly, QQP does not have a public test set. We therefore partitioned the clusters in the validation set randomly in two, to give us our dev/test splits. Summary statistics of the resulting datasets are given in Appendix B. All scores reported are on our test split.\nModel Configuration Following previous work (Kaiser et al., 2018; Angelidis et al., 2021), our quantizer uses multiple heads (H = 4) with distinct codebooks to represent the syntactic encoding as 4 discrete categorical variables qh, with zsyn given by the concatenation of their codebook embeddings Ch(qh). We use a relatively small codebook size of K = 256, relying on the combinatoric power of the multiple heads to maintain the expressivity of the model. We argue that, assuming each head learns to capture a particular property of a template (see Section 4.3), the number of variations in each property is small, and it is only through combination that the space of possible templates becomes large.\nWe include a detailed list of hyperparameters in Appendix A. Our code is available at http:// github.com/tomhosking/separator.\nComparison Systems We compare SEPARATOR against several related systems. These include a model which reconstructs Y only from Xsem, with no signal for the desired form of the output. In other words, we derive both zsem and zsyn from Xsem, and no separation between meaning and form is learned. This model uses a continuous Gaussian latent variable for both zsyn and zsem, but is otherwise equivalent in architecture to SEPARATOR. We refer to this as the VAE baseline. We also experiment with a vanilla autoencoder or AE baseline by removing the variational component, such that zsem, zsyn = ẽsem, ẽsyn.\nWe include our own implementation of the VQ-VAE model described in Roy and Grangier (2019). They use a quantized bottleneck for both zsem and zsyn, with a large codebookK = 64, 000, H = 8 heads and a residual connection within the quantizer. For QQP, containing only 55,611 train-\ning clusters, the configuration in Roy and Grangier (2019) leaves the model overparameterized and training did not converge; we instead report results for K = 1, 000.\nParaNMT (Wieting and Gimpel, 2018) translates input sentences into a pivot language (Czech), then back into English. Although this system was trained on high volumes of data (including Common Crawl), the training data contains relatively few questions, and we would not expect it to perform well in the domain under consideration. ‘Diverse Paraphraser using Submodularity’ (DiPS; Kumar et al. 2019) uses submodular optimisation to increase the diversity of samples from a standard encode-decoder model. Latent bag-of-words (BoW; Fu et al. 2019) uses an encoder-decoder model with a discrete bag-of-words as the latent encoding. SOW/REAP (Goyal and Durrett, 2020) uses a two stage approach, deriving a set of feasible syntactic rearrangements that is used to guide a second encoder-decoder model. We additionally implement a simple tf-idf baseline (Jones, 1972), retrieving the question from the training set with the highest similarity to the input. Finally, we include a basic copy baseline as a lower bound, that simply uses the input question as the output."
    }, {
      "heading" : "4 Results",
      "text" : "Our experiments were designed to answer three questions: (a) Does SEPARATOR effectively factorize meaning and form? (b) Does SEPARATOR\nmanage to generate diverse paraphrases (while preserving the intent of the input)? (c) What does the underlying quantized space encode (i.e., can we identify any meaningful syntactic properties)? We address each of these questions in the following sections."
    }, {
      "heading" : "4.1 Verification of Separation",
      "text" : "Inspired by Chen et al. (2019b) we use a semantic textual similarity task and a template detection task to confirm that SEPARATOR does indeed lead to encodings {zsem, zsyn} in latent spaces that represent different types of information.\nUsing the test set, we construct clusters of questions that share the same meaning Csem, and clusters that share the same template Csyn. For each cluster Cq ∈ {Csem, Csyn}, we extract one question at random Xq ∈ Cq, compute its encodings {zsem, zsyn, z}3, and its cosine similarity to the encodings of all other questions in the test set. We take the question with maximum similarity to the query Xr, r = argmaxr′(zq.zr′), and compare the cluster that it belongs to, Cr, to the query cluster I(Cq = Cr), giving a retrieval accuracy score for each encoding type and each clustering type. For the VAE, we set {zsem, zsyn} to be the same heads of z as the separated model.\nTable 3 shows that our approach yields encodings that successfully factorise meaning and form, with negligible performance loss compared to the VAE baseline; paraphrase retrieval performance using zsem for the separated model is comparable to using z for the VAE.\n3z refers to the combined encoding, i.e., [zsem; zsyn]."
    }, {
      "heading" : "4.2 Paraphrase Generation",
      "text" : "Automatic Evaluation While we have shown that our approach leads to disentangled representations, we are ultimately interested in generating diverse paraphrases for unseen data. That is, given some input question, we want to generate an output question with the same meaning but different form.\nWe use iBLEU (Sun and Zhou, 2012) as our primary metric, a variant of BLEU (Papineni et al., 2002; Post, 2018) that is penalized by the similarity between the output and the input,\niBLEU = αBLEU(output, references)\n−(1− α)BLEU(output, input), (9)\nwhere α = 0.7 is a constant that weights the tradeoff between fidelity to the references and variation from the input. We also report the usual BLEU(output, references) as well as Self-BLEU(output, input). The latter allows us to examine whether the models are making trivial changes to the input. The Paralex test set contains 5.6 references on average per cluster, while QQP contains only 1.3. This leads to lower BLEU scores for QQP in general, since the models are evaluated on whether they generated the specific paraphrase(s) present in the dataset.\nTable 4 shows that the Copy, VAE and AE models display relatively high BLEU scores, but achieve this by ‘parroting’ the input; they are good at reconstructing the input, but introduce little variation in surface form, reflected in the high SelfBLEU scores. This highlights the importance of considering similarity to both the references and to the input. The tf-idf baseline performs surprisingly\nwell on Paralex; the large dataset size makes it more likely that a paraphrase cluster with a similar meaning to the query exists in the training set.\nThe other comparison systems (in the second block in Table 4) achieve lower Self-BLEU scores, indicating a higher degree of variation introduced, but this comes at the cost of much lower scores with respect to the references. SEPARATOR achieves the highest iBLEU scores, indicating the best balance between fidelity to the references and novelty compared to the input. We give some example output in Table 5; while the other systems mostly introduce lexical variation, SEPARATOR is able to produce output with markedly different syntactic structure to the input, and can even change the question type while successfully preserving the original intent.\nThe last row in Table 4 (ORACLE) reports results when our model is given a valid exemplar to use directly for generation, thus bypassing the code prediction problem. For each paraphrase cluster, we select one question at random to use as input, and select another to use as the target. We retrieve a question from the training set with the\nsame template as the target to use as an oracle exemplar. This represents an upper bound on our model’s performance. While SEPARATOR outperforms existing methods, our approach to predicting syntactic codes (using a shallow fully-connected network) is relatively simple. SEPARATOR using oracle exemplars achieves by far the highest scores in Table 4, demonstrating the potential expressivity of our approach when exemplars are guaranteed to be valid. A more powerful code prediction model could close the gap to this upper bound, as well as enabling the generation of multiple diverse paraphrases for a single input question. However, we leave this to future work.\nHuman Evaluation In addition to automatic evaluation we elicited judgements from crowdworkers on Amazon Mechanical Turk. Specifically, they were shown a question and two paraphrases thereof (corresponding to different systems) and asked to select which one was preferred along three dimensions: the dissimilarity of the paraphrase compared to the original question, how well the paraphrase reflected the meaning of the original, and the fluency of the paraphrase (see Appendix C). We evaluated a total of 200 questions sampled equally from both Paralex and QQP, and collected 3 ratings for each sample. We assigned each system a score of +1 when it was selected, −1 when the other system was selected, and took the mean over all samples. Negative scores indicate that a system was selected less often than an alternative. We chose the four best performing models according to Table 4 for our evaluation: SEPARATOR, DiPS (Kumar et al., 2019), Latent BoW (Fu et al., 2019) and VAE.\nFigure 2 shows that although the VAE baseline is the best at preserving question meaning, it is also the worst at introducing variation to the output. SEPARATOR introduces more variation than the other systems evaluated and better preserves the original question intent, as well as generating significantly more fluent output (using a one-way ANOVA with post-hoc Tukey HSD test, p<0.05)."
    }, {
      "heading" : "4.3 Analysis",
      "text" : "When predicting latent codes at test time, we assume that the code for each head may be predicted independently of the others, as working with the full joint distribution would be intractable. We now examine this assumption as well as whether different encodings represent distinct syntactic proper-\nties. Following Angelidis et al. (2021), we compute the probability of a question property f1, f2, . . . taking a particular value a, conditioned by head h and quantized code kh as\nP (fi|h, kh)=\n∑ x∈X\nI(qh(x)=kh)I(fi(x)=a)∑ x∈X I(qh(x)=kh) ,(10)\nwhere I(·) is the indicator function, and examples of values a are shown in Figure 3. We then calculate the mean entropy of these distributions, to determine how property-specific each head is:\nHh = 1\nK ∑ kh ∑ a P (a|h, kh) logP (a|h, kh). (11)\nHeads with lower entropies are more predictive of a property, indicating specialisation and therefore independence. Figure 3 shows our analysis for four syntactic properties: head #2 has learned to control the high level output structure, including the question type or wh- word, and whether the question word appears at the beginning or end of the question. Head #3 controls which type of prepositional phrase is used. The length of the output is not determined by any one head, implying that it results from other properties of the surface form. Future work could leverage this disentanglement to improve the exemplar prediction model, and could lead to more fine-grained control over the generated output form.\nIn summary, we find that SEPARATOR successfully learns separate encodings for meaning and form. SEPARATOR is able to generate question\nparaphrases with a better balance of diversity and intent preservation compared to prior work. Although we are able to identify some high-level properties encoded by each of the syntactic latent variables, further work is needed to learn interpretable syntactic encodings."
    }, {
      "heading" : "5 Related Work",
      "text" : "Paraphrasing Prior work on generating paraphrases has looked at extracting sentences with similar meaning from large corpora (Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013), or identifying paraphrases from sources that are weakly aligned (Dolan et al., 2004; Coster and Kauchak, 2011).\nMore recently, neural approaches to paraphrasing have shown promise. Several models have used an information bottleneck to try to encode the semantics of the input, including VAEs (Bowman et al., 2016), VQ-VAEs (van den Oord et al., 2017; Roy and Grangier, 2019), and a latent bag-of-words model (Fu et al., 2019). Other work has relied on the strength of neural machine translation models, translating an input into a pivot language and then back into English (Mallinson et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019).\nKumar et al. (2019) use submodular function maximisation to improve the diversity of paraphrases generated by an encoder-decoder model. Dong et al. (2017) use an automatic paraphrasing system to rewrite inputs to a question answering system at inference time, reducing the sensitivity of the system to the specific phrasing of a query.\nSyntactic Templates The idea of generating paraphrases by controlling the structure of the output has seen recent interest, but most work so far has assumed access to a template oracle. Iyyer et al.\n(2018) use linearized parse trees as a template, then sample paraphrases by using multiple templates and reranking the output. Chen et al. (2019a) use a multi task objective to train a model to generate output that follows an input template. Their approach is limited by their use of automatically generated paraphrases for training, and their reliance on the availability of oracle templates. Bao et al. (2019) use a discriminator to separate spaces, but rely on noising the latent space to induce variation in the output form. Their results show good fidelity to the references, but low variation compared to the input. Goyal and Durrett (2020) use the artifically generated dataset ParaNMT-50m (Wieting and Gimpel, 2018) for their training and evaluation, which displays low output variation according to our results. Kumar et al. (2020) show strong performance using full parse trees as templates, but focus on generating output with the correct parse and do not consider the problem of template prediction.\nHuang and Chang (2021) independently and concurrently propose training a model with a similar ‘split training’ approach to ours, but using constituency parses instead of exemplars, and a ‘bagof-words’ instead of reference paraphrases. Their approach has the advantage of not requiring paraphrase clusters during training, but they do not attempt to solve the problem of template prediction and rely on the availability of oracle target templates.\nRussin et al. (2020) modify the architecture of an encoder-decoder model, introducing an inductive bias to encode the structure of inputs separately from the lexical items to improve compositional generalisation on an artificial semantic parsing task. Chen et al. (2019b) use a multi-task setup to generate separated encodings, but do not experiment with generation tasks. Shu et al. (2019) learn discrete latent codes to introduce variation to the output of a machine translation system."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We present SEPARATOR, a method for generating paraphrases that balances high variation in surface form with strong intent preservation. Our approach consists of: (a) a training scheme that causes an encoder-decoder model to learn separated latent encodings, (b) a vector-quantized bottleneck that results in discrete variables for the syntactic encoding, and (c) a simple model to predict different yet valid surface forms for the output. Extensive\nexperiments and a human evaluation show that our approach leads to separated encoding spaces with negligible loss of expressivity, and is able to generate paraphrases with a better balance of variation and semantic fidelity than prior methods.\nIn future, we would like to investigate the properties of the syntactic encoding space, and improve on the code prediction model. It would also be interesting to reduce the levels of supervision required to train the model, and induce the separation without an external syntactic model or reference paraphrases."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank our anonymous reviewers for their feedback. We are grateful to Stefanos Angelidis for many valuable discussions, and Hao Tang for their comments on the paper. This work was supported in part by the UKRI Centre for Doctoral Training in Natural Language Processing, funded by the UKRI (grant EP/S022481/1) and the University of Edinburgh. Lapata acknowledges the support of the European Research Council (award number 681760, “Translating Multiple Modalities into Text”)."
    }, {
      "heading" : "A Hyperparameters",
      "text" : "Hyperparameters were selected by manual tuning, based on a combination of: (a) validation encoding separation, (b) validation BLEU scores using oracle exemplars, and (c) validation iBLEU scores using predicted syntactic codes."
    }, {
      "heading" : "B Dataset Statistics",
      "text" : "Summary statistics for our partitions of Paralex and QQP are shown in Table 7. Questions in QQP were 9.7 tokens long on average, compared to 8.2 for Paralex.\nWe also show the distribution of different question types in Figure 4; QQP contains a higher percentage of why questions, and we found that the questions tend to be more subjective compared to the predominantly factual questions in Paralex."
    }, {
      "heading" : "C Human Evaluation",
      "text" : "Annotators were asked to rate the outputs according to the following criteria:\n• Which system output is the most fluent and grammatical?\n• To what extent is the meaning expressed in the original question preserved in the rewritten version, with no additional information added? Which of the questions generated by a system is likely to have the same answer as the original?\n• Does the rewritten version use different words or phrasing to the original? You should choose the system that uses the most different words or word order."
    }, {
      "heading" : "D Reproducibility Notes",
      "text" : "All experiments were run on a single Nvidia RTX 2080 Ti GPU. Training time for SEPARATOR was approximately 2 days on Paralex, and 1 day for QQP. SEPARATOR contains a total of 69,139,744 trainable parameters."
    }, {
      "heading" : "E Template Dropout",
      "text" : "Early experiments showed that, while the model was able to separately encode meaning and form, the ‘syntactic’ encoding space showed little ordering. That is, local regions of the encoding space did not necessarily encode templates that co-occurred with each other in paraphrase clusters. We therefore propose template dropout, where exemplars Xsyn are replaced with probability ptd = 0.3 by a question with a different template from the same paraphrase cluster. This is intended to provide the model with a signal about which templates are similar to each other, and thus reduce the distance between their encodings."
    }, {
      "heading" : "F Ordering of the Encoding Space",
      "text" : "Figure 5 shows that the semantic encodings zsem are tightly clustered by paraphrase, but the set of\nvalid forms for each cluster overlaps significantly. In other words, regions of licensed templates for each input are not contiguous, and naively perturbing a syntactic encoding for an input question is not guaranteed to lead to a valid template. Template dropout, described in Appendix E, seems to improve the arrangement of encoding space, but is not sufficient to allow us to ‘navigate’ encoding space directly. The ability to induce an ordered encoding space and introduce syntactic diversity by simply perturbing the encoding, would allow us to drop the template prediction network, and we hope that future work will build on this idea."
    }, {
      "heading" : "G Failure Cases",
      "text" : "A downside of our approach is the use of an information bottleneck; the model must learn to compress a full question into a single, fixed-length vector. This can lead to loss of information or corruption, with the output occasionally repeating words or generating a number that is slightly different to the correct one, as shown in Table 8.\nWe also occasionally observe instances of the\nwell documented posterior collapse phenomenon, where the decoder ignores the input encoding and generates a generic high probability sequence."
    } ],
    "references" : [ {
      "title" : "Contextual string embeddings for sequence labeling",
      "author" : [ "Alan Akbik", "Duncan Blythe", "Roland Vollgraf." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 1638–1649, Santa Fe, New Mexico, USA. Associ-",
      "citeRegEx" : "Akbik et al\\.,? 2018",
      "shortCiteRegEx" : "Akbik et al\\.",
      "year" : 2018
    }, {
      "title" : "Extractive opinion summarization in quantized transformer spaces",
      "author" : [ "Stefanos Angelidis", "Reinald Amplayo", "Yoshihiko Suhara", "Xiaolan Wang", "Mirella Lapata." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 9(0):277–293.",
      "citeRegEx" : "Angelidis et al\\.,? 2021",
      "shortCiteRegEx" : "Angelidis et al\\.",
      "year" : 2021
    }, {
      "title" : "Paraphrasing with bilingual parallel corpora",
      "author" : [ "Colin Bannard", "Chris Callison-Burch." ],
      "venue" : "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 597– 604, Ann Arbor, Michigan. Association for Compu-",
      "citeRegEx" : "Bannard and Callison.Burch.,? 2005",
      "shortCiteRegEx" : "Bannard and Callison.Burch.",
      "year" : 2005
    }, {
      "title" : "Generating sentences from disentangled syntactic and semantic spaces",
      "author" : [ "Yu Bao", "Hao Zhou", "Shujian Huang", "Lei Li", "Lili Mou", "Olga Vechtomova", "Xin-yu Dai", "Jiajun Chen." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Bao et al\\.,? 2019",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2019
    }, {
      "title" : "Extracting paraphrases from a parallel corpus",
      "author" : [ "Regina Barzilay", "Kathleen R. McKeown." ],
      "venue" : "Proceedings of the 39th Annual Meeting of the Associ-",
      "citeRegEx" : "Barzilay and McKeown.,? 2001",
      "shortCiteRegEx" : "Barzilay and McKeown.",
      "year" : 2001
    }, {
      "title" : "Estimating or propagating gradients through stochastic neurons for conditional computation",
      "author" : [ "Yoshua Bengio", "N. Léonard", "Aaron C. Courville." ],
      "venue" : "CoRR, abs/1308.3432.",
      "citeRegEx" : "Bengio et al\\.,? 2013",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Generating sentences from a continuous space",
      "author" : [ "Samuel R. Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew Dai", "Rafal Jozefowicz", "Samy Bengio." ],
      "venue" : "Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learn-",
      "citeRegEx" : "Bowman et al\\.,? 2016",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2016
    }, {
      "title" : "Controllable paraphrase generation with a syntactic exemplar",
      "author" : [ "Mingda Chen", "Qingming Tang", "Sam Wiseman", "Kevin Gimpel." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5972–5984, Florence,",
      "citeRegEx" : "Chen et al\\.,? 2019a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "A multi-task approach for disentangling syntax and semantics in sentence representations",
      "author" : [ "Mingda Chen", "Qingming Tang", "Sam Wiseman", "Kevin Gimpel." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Chen et al\\.,? 2019b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "On the properties of neural machine translation: Encoder–decoder approaches",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer", "Dzmitry Bahdanau", "Yoshua Bengio." ],
      "venue" : "Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statisti-",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Simple English Wikipedia: A new text simplification task",
      "author" : [ "William Coster", "David Kauchak." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 665–669, Portland, Ore-",
      "citeRegEx" : "Coster and Kauchak.,? 2011",
      "shortCiteRegEx" : "Coster and Kauchak.",
      "year" : 2011
    }, {
      "title" : "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources",
      "author" : [ "Bill Dolan", "Chris Quirk", "Chris Brockett." ],
      "venue" : "COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics,",
      "citeRegEx" : "Dolan et al\\.,? 2004",
      "shortCiteRegEx" : "Dolan et al\\.",
      "year" : 2004
    }, {
      "title" : "Learning to paraphrase for question answering",
      "author" : [ "Li Dong", "Jonathan Mallinson", "Siva Reddy", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 875–886, Copenhagen, Denmark. Associ-",
      "citeRegEx" : "Dong et al\\.,? 2017",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2017
    }, {
      "title" : "Paraphrase-driven learning for open question answering",
      "author" : [ "Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1608–1618,",
      "citeRegEx" : "Fader et al\\.,? 2013",
      "shortCiteRegEx" : "Fader et al\\.",
      "year" : 2013
    }, {
      "title" : "Paraphrase generation with latent bag of words",
      "author" : [ "Yao Fu", "Yansong Feng", "John P Cunningham." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32, pages 13645–13656. Curran Associates, Inc.",
      "citeRegEx" : "Fu et al\\.,? 2019",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2019
    }, {
      "title" : "PPDB: The paraphrase database",
      "author" : [ "Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Ganitkevitch et al\\.,? 2013",
      "shortCiteRegEx" : "Ganitkevitch et al\\.",
      "year" : 2013
    }, {
      "title" : "Neural syntactic preordering for controlled paraphrase generation",
      "author" : [ "Tanya Goyal", "Greg Durrett." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 238– 252, Online. Association for Computational Linguis-",
      "citeRegEx" : "Goyal and Durrett.,? 2020",
      "shortCiteRegEx" : "Goyal and Durrett.",
      "year" : 2020
    }, {
      "title" : "Parabank: Monolingual bitext generation and sentential paraphrasing via lexically-constrained neural machine translation",
      "author" : [ "J. Edward Hu", "Rachel Rudinger", "Matt Post", "Benjamin Van Durme." ],
      "venue" : "CoRR, abs/1901.03644.",
      "citeRegEx" : "Hu et al\\.,? 2019",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2019
    }, {
      "title" : "Generating syntactically controlled paraphrases without using annotated parallel pairs",
      "author" : [ "Kuan-Hao Huang", "Kai-Wei Chang." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main",
      "citeRegEx" : "Huang and Chang.,? 2021",
      "shortCiteRegEx" : "Huang and Chang.",
      "year" : 2021
    }, {
      "title" : "Adversarial example generation with syntactically controlled paraphrase networks",
      "author" : [ "Mohit Iyyer", "John Wieting", "Kevin Gimpel", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Iyyer et al\\.,? 2018",
      "shortCiteRegEx" : "Iyyer et al\\.",
      "year" : 2018
    }, {
      "title" : "A statistical interpretation of term specificity and its application in retrieval",
      "author" : [ "Karen Spärck Jones." ],
      "venue" : "Journal of Documentation, 28:11–21.",
      "citeRegEx" : "Jones.,? 1972",
      "shortCiteRegEx" : "Jones.",
      "year" : 1972
    }, {
      "title" : "Fast decoding in sequence models using discrete latent variables",
      "author" : [ "Lukasz Kaiser", "Aurko Roy", "Ashish Vaswani", "Niki Parmar", "Samy Bengio", "Jakob Uszkoreit", "Noam Shazeer." ],
      "venue" : "CoRR, abs/1803.03382.",
      "citeRegEx" : "Kaiser et al\\.,? 2018",
      "shortCiteRegEx" : "Kaiser et al\\.",
      "year" : 2018
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations,",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Autoencoding variational bayes",
      "author" : [ "Diederik P. Kingma", "Max Welling." ],
      "venue" : "2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Welling.,? 2014",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2014
    }, {
      "title" : "Syntax-guided controlled generation of paraphrases",
      "author" : [ "Ashutosh Kumar", "Kabir Ahuja", "Raghuram Vadapalli", "Partha Talukdar." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8(0):330–345.",
      "citeRegEx" : "Kumar et al\\.,? 2020",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2020
    }, {
      "title" : "Submodular optimization-based diverse paraphrasing and its effectiveness in data augmentation",
      "author" : [ "Ashutosh Kumar", "Satwik Bhattamishra", "Manik Bhandari", "Partha Talukdar." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chap-",
      "citeRegEx" : "Kumar et al\\.,? 2019",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2019
    }, {
      "title" : "Hierarchical transformers for multi-document summarization",
      "author" : [ "Yang Liu", "Mirella Lapata." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5070– 5081, Florence, Italy. Association for Computa-",
      "citeRegEx" : "Liu and Lapata.,? 2019",
      "shortCiteRegEx" : "Liu and Lapata.",
      "year" : 2019
    }, {
      "title" : "langid.py: An off-the-shelf language identification tool",
      "author" : [ "Marco Lui", "Timothy Baldwin" ],
      "venue" : "In Proceedings of the ACL 2012 System Demonstrations,",
      "citeRegEx" : "Lui and Baldwin.,? \\Q2012\\E",
      "shortCiteRegEx" : "Lui and Baldwin.",
      "year" : 2012
    }, {
      "title" : "Visualizing high-dimensional data using t-sne",
      "author" : [ "Laurens van der Maaten", "Geoffrey E. Hinton." ],
      "venue" : "Journal of Machine Learning Research, 9:2579–2605.",
      "citeRegEx" : "Maaten and Hinton.,? 2008",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    }, {
      "title" : "Generating phrasal and sentential paraphrases: A survey of data-driven methods",
      "author" : [ "Nitin Madnani", "Bonnie J. Dorr." ],
      "venue" : "Computational Linguistics, 36(3):341–387.",
      "citeRegEx" : "Madnani and Dorr.,? 2010",
      "shortCiteRegEx" : "Madnani and Dorr.",
      "year" : 2010
    }, {
      "title" : "Paraphrasing revisited with neural machine translation",
      "author" : [ "Jonathan Mallinson", "Rico Sennrich", "Mirella Lapata." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Pa-",
      "citeRegEx" : "Mallinson et al\\.,? 2017",
      "shortCiteRegEx" : "Mallinson et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural discrete representation learning",
      "author" : [ "Aaron van den Oord", "Oriol Vinyals", "koray kavukcuoglu" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Oord et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2017
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "A call for clarity in reporting BLEU scores",
      "author" : [ "Matt Post." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186– 191, Belgium, Brussels. Association for Computational Linguistics.",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "Unsupervised paraphrasing without translation",
      "author" : [ "Aurko Roy", "David Grangier." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6033–6039, Florence, Italy. Association for Computational Linguistics.",
      "citeRegEx" : "Roy and Grangier.,? 2019",
      "shortCiteRegEx" : "Roy and Grangier.",
      "year" : 2019
    }, {
      "title" : "Theory and experiments on vector quantized autoencoders",
      "author" : [ "Aurko Roy", "Ashish Vaswani", "Arvind Neelakantan", "Niki Parmar." ],
      "venue" : "CoRR, abs/1805.11063.",
      "citeRegEx" : "Roy et al\\.,? 2018",
      "shortCiteRegEx" : "Roy et al\\.",
      "year" : 2018
    }, {
      "title" : "Compositional generalization by factorizing alignment and translation",
      "author" : [ "Jacob Russin", "Jason Jo", "Randall O’Reilly", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics:",
      "citeRegEx" : "Russin et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Russin et al\\.",
      "year" : 2020
    }, {
      "title" : "Adversarial domain adaptation for duplicate question detection",
      "author" : [ "Darsh Shah", "Tao Lei", "Alessandro Moschitti", "Salvatore Romeo", "Preslav Nakov." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Shah et al\\.,? 2018",
      "shortCiteRegEx" : "Shah et al\\.",
      "year" : 2018
    }, {
      "title" : "Generating diverse translations with sentence codes",
      "author" : [ "Raphael Shu", "Hideki Nakayama", "Kyunghyun Cho." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1823–1827, Florence, Italy. Association",
      "citeRegEx" : "Shu et al\\.,? 2019",
      "shortCiteRegEx" : "Shu et al\\.",
      "year" : 2019
    }, {
      "title" : "Joint learning of a dual SMT system for paraphrase generation",
      "author" : [ "Hong Sun", "Ming Zhou." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 38–42, Jeju Island, Korea. Associa-",
      "citeRegEx" : "Sun and Zhou.,? 2012",
      "shortCiteRegEx" : "Sun and Zhou.",
      "year" : 2012
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "ParaNMT50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations",
      "author" : [ "John Wieting", "Kevin Gimpel." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
      "citeRegEx" : "Wieting and Gimpel.,? 2018",
      "shortCiteRegEx" : "Wieting and Gimpel.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "A paraphrase of an utterance is “an alternative surface form in the same language expressing the same semantic content as the original form” (Madnani and Dorr, 2010).",
      "startOffset" : 141,
      "endOffset" : 165
    }, {
      "referenceID" : 19,
      "context" : "Question paraphrases are of significant interest, with applications in data augmentation (Iyyer et al., 2018), query rewriting (Dong et al.",
      "startOffset" : 89,
      "endOffset" : 109
    }, {
      "referenceID" : 12,
      "context" : ", 2018), query rewriting (Dong et al., 2017) and duplicate question detection (Shah et al.",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 37,
      "context" : ", 2017) and duplicate question detection (Shah et al., 2018), as they allow a system to better identify the underlying intent of a user query.",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 6,
      "context" : "Recent approaches to paraphrasing use information bottlenecks with VAEs (Bowman et al., 2016) or pivot languages (Wieting and Gimpel, 2018) to try to extract the semantics of an input utterance, before projecting back to a (hopefully different) surface form.",
      "startOffset" : 72,
      "endOffset" : 93
    }, {
      "referenceID" : 41,
      "context" : ", 2016) or pivot languages (Wieting and Gimpel, 2018) to try to extract the semantics of an input utterance, before projecting back to a (hopefully different) surface form.",
      "startOffset" : 27,
      "endOffset" : 53
    }, {
      "referenceID" : 13,
      "context" : "Table 1: Examples of question paraphrase clusters, drawn from Paralex (Fader et al., 2013).",
      "startOffset" : 70,
      "endOffset" : 90
    }, {
      "referenceID" : 19,
      "context" : "Other work has specified the surface form to be generated (Iyyer et al., 2018; Chen et al., 2019a; Kumar et al., 2020), but has so far assumed that the set of valid surface forms is known a priori.",
      "startOffset" : 58,
      "endOffset" : 118
    }, {
      "referenceID" : 7,
      "context" : "Other work has specified the surface form to be generated (Iyyer et al., 2018; Chen et al., 2019a; Kumar et al., 2020), but has so far assumed that the set of valid surface forms is known a priori.",
      "startOffset" : 58,
      "endOffset" : 118
    }, {
      "referenceID" : 24,
      "context" : "Other work has specified the surface form to be generated (Iyyer et al., 2018; Chen et al., 2019a; Kumar et al., 2020), but has so far assumed that the set of valid surface forms is known a priori.",
      "startOffset" : 58,
      "endOffset" : 118
    }, {
      "referenceID" : 9,
      "context" : "Our model is a variant of the standard encoderdecoder framework (Cho et al., 2014), and consists of: (a) a vanilla Transformer sentence encoder (Vaswani et al.",
      "startOffset" : 64,
      "endOffset" : 82
    }, {
      "referenceID" : 40,
      "context" : ", 2014), and consists of: (a) a vanilla Transformer sentence encoder (Vaswani et al., 2017), that maps an input question X to a multi-head sequence of encodings, eh,t = ENCODER(X); (b) a principled choice of information bottleneck, with a continuous variational path and a discrete vector-quantized path, that maps the encoding sequence to a pair of latent vectors, zsem, zsyn = BOTTLENECK(eh,t), represented in more detail in Figure 1; (c) a vanilla",
      "startOffset" : 69,
      "endOffset" : 91
    }, {
      "referenceID" : 5,
      "context" : "The quantizer module is trained through backpropagation using straight-through estimation (Bengio et al., 2013), with an additional loss term to constrain the embedding space as described in van den Oord et al.",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 35,
      "context" : "approaches described in earlier work (Roy et al., 2018; Angelidis et al., 2021), which we find improve training stability.",
      "startOffset" : 37,
      "endOffset" : 79
    }, {
      "referenceID" : 1,
      "context" : "approaches described in earlier work (Roy et al., 2018; Angelidis et al., 2021), which we find improve training stability.",
      "startOffset" : 37,
      "endOffset" : 79
    }, {
      "referenceID" : 23,
      "context" : "Variational Bottleneck For the semantic path, we introduce a learned Gaussian posterior, that represents the encodings as smooth distributions in space instead of point estimates (Kingma and Welling, 2014).",
      "startOffset" : 179,
      "endOffset" : 205
    }, {
      "referenceID" : 23,
      "context" : "tion (6) by sampling from the training set and updating via backpropagation (Kingma and Welling, 2014).",
      "startOffset" : 76,
      "endOffset" : 102
    }, {
      "referenceID" : 13,
      "context" : "Datasets We evaluate our approach on two datasets: Paralex (Fader et al., 2013), a dataset of question paraphrase clusters scraped from WikiAnswers; and Quora Question Pairs (QQP)2 sourced from the community question answering forum Quora.",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 21,
      "context" : "Model Configuration Following previous work (Kaiser et al., 2018; Angelidis et al., 2021), our quantizer uses multiple heads (H = 4) with distinct codebooks to represent the syntactic encoding as",
      "startOffset" : 44,
      "endOffset" : 89
    }, {
      "referenceID" : 1,
      "context" : "Model Configuration Following previous work (Kaiser et al., 2018; Angelidis et al., 2021), our quantizer uses multiple heads (H = 4) with distinct codebooks to represent the syntactic encoding as",
      "startOffset" : 44,
      "endOffset" : 89
    }, {
      "referenceID" : 41,
      "context" : "ParaNMT (Wieting and Gimpel, 2018) translates input sentences into a pivot language (Czech), then back into English.",
      "startOffset" : 8,
      "endOffset" : 34
    }, {
      "referenceID" : 25,
      "context" : "‘Diverse Paraphraser using Submodularity’ (DiPS; Kumar et al. 2019) uses submodular optimisation to increase the diversity of samples from a standard encode-decoder model.",
      "startOffset" : 42,
      "endOffset" : 67
    }, {
      "referenceID" : 14,
      "context" : "Latent bag-of-words (BoW; Fu et al. 2019) uses an encoder-decoder model with a discrete bag-of-words as the latent encoding.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 16,
      "context" : "SOW/REAP (Goyal and Durrett, 2020) uses a two stage approach, deriving a set of feasible syntactic rearrangements that is used to guide a second encoder-decoder model.",
      "startOffset" : 9,
      "endOffset" : 34
    }, {
      "referenceID" : 20,
      "context" : "We additionally implement a simple tf-idf baseline (Jones, 1972), retrieving the question from the training set with the highest similarity to the input.",
      "startOffset" : 51,
      "endOffset" : 64
    }, {
      "referenceID" : 39,
      "context" : "We use iBLEU (Sun and Zhou, 2012) as our primary metric, a variant of BLEU (Papineni et al.",
      "startOffset" : 13,
      "endOffset" : 33
    }, {
      "referenceID" : 32,
      "context" : "We use iBLEU (Sun and Zhou, 2012) as our primary metric, a variant of BLEU (Papineni et al., 2002; Post, 2018) that is penalized by the similarity between the output and the input,",
      "startOffset" : 75,
      "endOffset" : 110
    }, {
      "referenceID" : 33,
      "context" : "We use iBLEU (Sun and Zhou, 2012) as our primary metric, a variant of BLEU (Papineni et al., 2002; Post, 2018) that is penalized by the similarity between the output and the input,",
      "startOffset" : 75,
      "endOffset" : 110
    }, {
      "referenceID" : 25,
      "context" : "We chose the four best performing models according to Table 4 for our evaluation: SEPARATOR, DiPS (Kumar et al., 2019), Latent BoW (Fu et al.",
      "startOffset" : 98,
      "endOffset" : 118
    }, {
      "referenceID" : 4,
      "context" : "similar meaning from large corpora (Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013), or identifying paraphrases from sources that are weakly aligned (Dolan et al.",
      "startOffset" : 35,
      "endOffset" : 124
    }, {
      "referenceID" : 2,
      "context" : "similar meaning from large corpora (Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013), or identifying paraphrases from sources that are weakly aligned (Dolan et al.",
      "startOffset" : 35,
      "endOffset" : 124
    }, {
      "referenceID" : 15,
      "context" : "similar meaning from large corpora (Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013), or identifying paraphrases from sources that are weakly aligned (Dolan et al.",
      "startOffset" : 35,
      "endOffset" : 124
    }, {
      "referenceID" : 11,
      "context" : ", 2013), or identifying paraphrases from sources that are weakly aligned (Dolan et al., 2004; Coster and Kauchak, 2011).",
      "startOffset" : 73,
      "endOffset" : 119
    }, {
      "referenceID" : 10,
      "context" : ", 2013), or identifying paraphrases from sources that are weakly aligned (Dolan et al., 2004; Coster and Kauchak, 2011).",
      "startOffset" : 73,
      "endOffset" : 119
    }, {
      "referenceID" : 6,
      "context" : "Several models have used an information bottleneck to try to encode the semantics of the input, including VAEs (Bowman et al., 2016), VQ-VAEs (van den Oord et al.",
      "startOffset" : 111,
      "endOffset" : 132
    }, {
      "referenceID" : 14,
      "context" : "Roy and Grangier, 2019), and a latent bag-of-words model (Fu et al., 2019).",
      "startOffset" : 57,
      "endOffset" : 74
    }, {
      "referenceID" : 30,
      "context" : "Other work has relied on the strength of neural machine translation models, translating an input into a pivot language and then back into English (Mallinson et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019).",
      "startOffset" : 146,
      "endOffset" : 213
    }, {
      "referenceID" : 41,
      "context" : "Other work has relied on the strength of neural machine translation models, translating an input into a pivot language and then back into English (Mallinson et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019).",
      "startOffset" : 146,
      "endOffset" : 213
    }, {
      "referenceID" : 17,
      "context" : "Other work has relied on the strength of neural machine translation models, translating an input into a pivot language and then back into English (Mallinson et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019).",
      "startOffset" : 146,
      "endOffset" : 213
    }, {
      "referenceID" : 41,
      "context" : "Goyal and Durrett (2020) use the artifically generated dataset ParaNMT-50m (Wieting and Gimpel, 2018) for their training and evaluation, which displays low output variation according to our results.",
      "startOffset" : 75,
      "endOffset" : 101
    } ],
    "year" : 2021,
    "abstractText" : "We propose a method for generating paraphrases of English questions that retain the original intent but use a different surface form. Our model combines a careful choice of training objective with a principled information bottleneck, to induce a latent encoding space that disentangles meaning and form. We train an encoder-decoder model to reconstruct a question from a paraphrase with the same meaning and an exemplar with the same surface form, leading to separated encoding spaces. We use a Vector-Quantized Variational Autoencoder to represent the surface form as a set of discrete latent variables, allowing us to use a classifier to select a different surface form at test time. Crucially, our method does not require access to an external source of target exemplars. Extensive experiments and a human evaluation show that we are able to generate paraphrases with a better tradeoff between semantic preservation and syntactic novelty compared to previous methods.",
    "creator" : "LaTeX with hyperref"
  }
}