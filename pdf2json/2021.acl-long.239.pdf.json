{
  "name" : "2021.acl-long.239.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Few-Shot Question Answering by Pretraining Span Selection",
    "authors" : [ "Ori Ram", "Yuval Kirstain", "Jonathan Berant", "Amir Globerson", "Omer Levy" ],
    "emails" : [ "ori.ram@cs.tau.ac.il", "yuval.kirstain@cs.tau.ac.il", "joberant@cs.tau.ac.il", "gamir@cs.tau.ac.il", "levyomer@cs.tau.ac.il" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3066–3079\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3066"
    }, {
      "heading" : "1 Introduction",
      "text" : "The standard approach to question answering is to pretrain a masked language model on raw text, and then fine-tune it with a span selection layer on top (Devlin et al., 2019; Joshi et al., 2020; Liu et al., 2019). While this approach is effective, and sometimes exceeds human performance, its success is based on the assumption that large quantities of annotated question answering examples are available. For instance, both SQuAD (Rajpurkar et al., 2016, 2018) and Natural Questions (Kwiatkowski et al., 2019) contain an order of 100,000 question and\n∗ Equal contribution. 1Our code, models, and datasets are publicly available:\nhttps://github.com/oriram/splinter.\nanswer pairs in their training data. This assumption quickly becomes unrealistic as we venture outside the lab conditions of English Wikipedia, and attempt to crowdsource question-answer pairs in other languages or domains of expertise (Tsatsaronis et al., 2015; Kembhavi et al., 2017). How do question answering models fare in the more practical case, where an in-house annotation effort can only produce a couple hundred training examples?\nWe investigate the task of few-shot question answering by sampling small training sets from existing question answering benchmarks. Despite the use of pretrained models, the standard approach yields poor results when fine-tuning on few examples (Figure 1). For example, RoBERTa-base finetuned on 128 question-answer pairs from SQuAD obtains around 40 F1. This is somewhat expected, since the pretraining objective is quite different from the fine-tuning task. While masked language modeling requires mainly local context around the masked token, question answering needs to align the question with the global context of the pas-\nsage. To bridge this gap, we propose (1) a novel self-supervised method for pretraining span selection models, and (2) a question answering layer that aligns a representation of the question with the text.\nWe introduce Splinter (span-level pointer), a pretrained model for few-shot question answering. The challenge in defining such a self-supervised task is how to create question-answer pairs from unlabeled data. Our key observation is that one can leverage recurring spans: n-grams, such as named entities, which tend to occur multiple times in a given passage (e.g., “Roosevelt” in Figure 2). We emulate question answering by masking all but one instance of each recurring span with a special [QUESTION] token, and asking the model to select the correct span for each such token.\nTo select an answer span for each [QUESTION] token in parallel, we introduce a question-aware span selection (QASS) layer, which uses the [QUESTION] token’s representation to select the answer span. The QASS layer seamlessly integrates with fine-tuning on real question-answer pairs. We simply append the [QUESTION] token to the input question, and use the QASS layer to select the answer span (Figure 3). This is unlike existing models for span selection, which do not include an explicit question representation. The compatibility between pretraining and fine-tuning makes Splinter an effective few-shot learner.\nSplinter exhibits surprisingly high performance given only a few training examples throughout a va-\nriety of benchmarks from the MRQA 2019 shared task (Fisch et al., 2019). For example, Splinter-base achieves 72.7 F1 on SQuAD with only 128 examples, outperforming all baselines by a very wide margin. An ablation study shows that the pretraining method and the QASS layer itself (even without pretraining) both contribute to improved performance. Analysis indicates that Splinter’s representations change significantly less during fine-tuning compared to the baselines, suggesting that our pretraining is more adequate for question answering. Overall, our results highlight the importance of designing objectives and architectures in the few-shot setting, where an appropriate inductive bias can lead to dramatic performance improvements."
    }, {
      "heading" : "2 Background",
      "text" : "Extractive question answering is a common task in NLP, where the goal is to select a contiguous span a from a given text T that answers a question Q. This format was popularized by SQuAD (Rajpurkar et al., 2016), and has since been adopted by several datasets in various domains (Trischler et al., 2017; Kembhavi et al., 2017) and languages (Lewis et al., 2020; Clark et al., 2020), with some extensions allowing for unanswerable questions (Levy et al., 2017; Rajpurkar et al., 2018) or multiple answer spans (Dua et al., 2019; Dasigi et al., 2019). In this work, we follow the assumptions in the recent MRQA 2019 shared task (Fisch et al., 2019) and focus on questions whose answer is a single span.\nThe standard approach uses a pretrained encoder,\nsuch as BERT (Devlin et al., 2019), and adds two parameter vectors s, e to the pretrained model in order to detect the start position s and end position e of the answer span a, respectively. The input text T and question Q are concatenated and fed into the encoder, producing a contextualized token representation xi for each token in the sequence. To predict the start position of the answer span, a probability distribution is induced over the entire sequence by computing the inner product of a learned vector s with every token representation (the end position is computed similarly using a vector e):\nP (s = i | T,Q) = exp(x > i s)∑\nj exp(x > j s)\n,\nP (e = i | T,Q) = exp(x > i e)∑\nj exp(x > j e)\n.\nThe parameters s, e are trained during fine-tuning, using the cross-entropy loss with the start and end positions of the gold answer span.\nThis approach assumes that each token representation xi is contextualized with respect to the question. However, the masked language modeling objective does not necessarily encourage this form of long-range contextualization in the pretrained model, since many of the masked tokens can be resolved from local cues. Fine-tuning the attention patterns of pretrained masked language models may thus entail an extensive learning effort, difficult to achieve with only a handful of training examples. We overcome this issue by (1) pretraining directly for span selection, and (2) explicitly representing the question with a single vector, used to detect the answer in the input text."
    }, {
      "heading" : "3 Splinter",
      "text" : "We formulate a new task for pretraining question answering from unlabeled text: recurring span selection. We replace spans that appear multiple times in the given text with a special [QUESTION] token, except for one occurrence, which acts as the “answer” span for each (masked) cloze-style “question”. The prediction layer is a modification of the standard span selection layer, which replaces the static start and end parameter vectors, s and e, with dynamically-computed boundary detectors based on the contextualized representation of each [QUESTION] token. We reuse this architecture when fine-tuning on questionanswer pairs by adding a [QUESTION] token at the end of the actual question, thus aligning the pretraining objective with the fine-tuning task. We refer to our pretrained model as Splinter."
    }, {
      "heading" : "3.1 Pretraining: Recurring Span Selection",
      "text" : "Given an input text T , we find all recurring spans: arbitrary n-grams that appear more than once in the same text. For each set of identical recurring spans R, we select a single occurrence as the answer a and replace all other occurrences with a single [QUESTION] token.2 The goal of recurring span selection is to predict the correct answer a for a given [QUESTION] token q ∈ R \\ {a}, each q thus acting as an independent cloze-style question.\nFigure 2 illustrates this process. In the given passage, the span “Roosevelt” appears three times. Two of its instances (the second and third) are replaced with [QUESTION], while one instance (the first) becomes the answer, and remains intact. After masking, the sequence is passed through a transformer encoder, producing contextualized to-\n2In practice, only some sets of recurring spans are processed; see Cluster Selection below.\nken representations. The model is then tasked with predicting the start and end positions of the answer given each [QUESTION] token representation. In Figure 2b, we observe four instances of this prediction task: two for the “Roosevelt” cluster, one for the “Allied countries” cluster, and one for “Declaration by United Nations”.\nTaking advantage of recurring words in a passage (restricted to nouns or named entities) was proposed in past work as a signal for coreference (Kocijan et al., 2019; Ye et al., 2020). We further discuss this connection in Section 7.\nSpan Filtering To focus pretraining on semantically meaningful spans, we use the following definition for “spans”, which filters out recurring spans that are likely to be uninformative: (1) spans must begin and end at word boundaries, (2) we consider only maximal recurring spans, (3) spans containing only stop words are ignored, (4) spans are limited to a maximum of 10 tokens. These simple heuristic filters do not require a model, as opposed to masking schemes in related work (Glass et al., 2020; Ye et al., 2020; Guu et al., 2020), which require partof-speech taggers, constituency parsers, or named entity recognizers.\nCluster Selection We mask a random subset of recurring span clusters in each text, leaving some recurring spans untouched. Specifically, we replace up to 30 spans with [QUESTION] from each input passage.3 This number was chosen to resemble the 15% token-masking ratio of Joshi et al. (2020). Note that in our case, the number of masked tokens is greater than the number of questions."
    }, {
      "heading" : "3.2 Model: Question-Aware Span Selection",
      "text" : "Our approach converts texts into a set of questions that need to be answered simultaneously. The standard approach for extractive question answering (Devlin et al., 2019) is inapplicable, because it uses fixed start and end vectors. Since we have multiple questions, we replace the standard parameter vectors s, e with dynamic start and end vectors sq, eq, computed from each [QUESTION] token q:\nsq = Sxq eq = Exq\nHere, S,E are parameter matrices, which extract ad hoc start and end position detectors sq, eq from the given [QUESTION] token’s representation xq.\n3In some cases, the last cluster may have more than one unmasked span.\nThe rest of our model follows the standard span selection model by computing the start and end position probability distributions. The model can also be viewed as two bilinear functions of the question representation xq with each token in the sequence xi, similar to Dozat and Manning (2017):\nP (s = i | T, q) = exp(x > i Sxq)∑\nj exp(x > j Sxq)\nP (e = i | T, q) = exp(x > i Exq)∑\nj exp(x > j Exq)\nFinally, we use the answer’s gold start and end points (sa, ea) to compute the cross-entropy loss:\n− logP (s = sa | T, q)− logP (e = ea | T, q)\nWe refer to this architecture as the question-aware span selection (QASS) layer."
    }, {
      "heading" : "3.3 Fine-Tuning",
      "text" : "After pretraining, we assume access to labeled examples, where each training instance is a text T , a question Q, and an answer a that is a span in T . To make this setting similar to pretraining, we simply append a [QUESTION] token to the input sequence, immediately after the question Q (see Figure 3). Selecting the answer span then proceeds exactly as during pretraining. Indeed, the advantage of our approach is that in both pretraining and fine-tuning, the [QUESTION] token representation captures information about the question that is then used to select the span from context."
    }, {
      "heading" : "4 A Few-Shot QA Benchmark",
      "text" : "To evaluate how pretrained models work when only a small amount of labeled data is available for finetuning, we simulate various low-data scenarios by sampling subsets of training examples from larger datasets. We use a subset of the MRQA 2019 shared task (Fisch et al., 2019), which contains extractive question answering datasets in a unified format, where the answer is a single span in the given text passage.\nSplit I of the MRQA shared task contains 6 large question answering datasets: SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), and Natural Questions (Kwiatkowski et al., 2019). For each dataset, we sample smaller training datasets from the original training set with sizes changing on a\nlogarithmic scale, from 16 to 1,024 examples. To reduce variance, for each training set size, we sample 5 training sets using different random seeds and report average performance across training sets. We also experiment with fine-tuning the models on the full training sets. Since Split I of the MRQA shared task does not contain test sets, we evaluate using the official development sets as our test sets.\nWe also select two datasets from Split II of the MRQA shared task that were annotated by domain experts: BioASQ (Tsatsaronis et al., 2015) and TextbookQA (Kembhavi et al., 2017). Each of these datasets only has a development set that is publicly available in MRQA, containing about 1,500 examples. For each dataset, we sample 400 examples for evaluation (test set), and follow the same protocol we used for large datasets to sample training sets of 16 to 1,024 examples from the remaining data.\nTo maintain the few-shot setting, every dataset in our benchmark has well-defined training and test sets. To tune hyperparameters, one needs to extract validation data from each training set. For simplicity, we do not perform hyperparameter tuning or model selection (see Section 5), and thus use all of the available few-shot data for training."
    }, {
      "heading" : "5 Experimental Setup",
      "text" : "We describe our experimental setup in detail, including all models and baselines."
    }, {
      "heading" : "5.1 Baselines",
      "text" : "Splinter-base shares the same architecture (transformer encoder (Vaswani et al., 2017)), vocabulary (cased wordpieces), and number of parameters (110M) with SpanBERT-base (Joshi et al., 2020). In all experiments, we compare Splinter-base to three baselines of the same capacity:\nRoBERTa (Liu et al., 2019) A highly-tuned and optimized version of BERT, which is known to perform well on a wide range of natural language understanding tasks.\nSpanBERT (Joshi et al., 2020) A BERT-style model that focuses on span representations. SpanBERT is trained by masking contiguous spans of tokens and optimizing two objectives: (a) masked language modeling, which predicts each masked token from its own vector representation; (b) the span boundary objective, which predicts each masked\ntoken from the representations of the unmasked tokens at the start and end of the masked span.\nSpanBERT (Reimpl) Our reimplementation of SpanBERT, using exactly the same code, data, and hyperparameters as Splinter. This baseline aims to control for implementation differences and measures the effect of replacing masked language modeling with recurring span selection. Also, this version does not use the span boundary objective, as Joshi et al. (2020) reported no significant improvements from using it in question answering."
    }, {
      "heading" : "5.2 Pretraining Implementation",
      "text" : "We train Splinter-base using Adam (Kingma and Ba, 2015) for 2.4M training steps with batches of 256 sequences of length 512.4 The learning rate is warmed up for 10k steps to a maximum value of 10−4, after which it decays linearly. As in previous work, we use a dropout rate of 0.1 across all layers.\nWe follow Devlin et al. (2019) and train on English Wikipedia (preprocessed by WikiExtractor as in Attardi (2015)) and the Toronto BookCorpus (Zhu et al., 2015). We base our implementation on the official TensorFlow implementation of BERT, and train on a single eight-core v3 TPU (v3-8) on the Google Cloud Platform."
    }, {
      "heading" : "5.3 Fine-Tuning Implementation",
      "text" : "For fine-tuning, we use the hyperparameters from the default configuration of the HuggingFace Transformers package (Wolf et al., 2020).5 Specifically, we train all models using Adam (Kingma and Ba, 2015) with bias-corrected moment estimates for few-shot learning (Zhang et al., 2021). When finetuning on 1024 examples or less, we train for either 10 epochs or 200 steps (whichever is larger). For full-size datasets, we train for 2 epochs. We set the batch size to 12 and use a maximal learning rate of 3 · 10−5, which warms up in the first 10% of the steps, and then decays linearly.\nAn interesting question is how to fine-tune the QASS layer parameters (i.e., the S and E matrices in Section 3.2). In our implementation, we chose to discard the pretrained values and fine-tune\n4We used this setting to approximate SpanBERT’s hyperparameter setting in terms of epochs. That said, SpanBERT-base was trained for a quarter of the steps (600k steps) using four times as many examples per batch (1024 sequences). See Section 5.1 for additional baselines that control for this difference.\n5We did rudimentary tuning on the number of steps only, using a held-out portion of the SQuAD training set, since our training sets can be too small for the default values (e.g., running 10 epochs on 16 examples results in 20 update steps).\nfrom a random initialization, due to the possible discrepancy between span statistics in pretraining and fine-tuning datasets. However, we report results on fine-tuning without resetting the QASS parameters as an ablation study (Section 6.3)."
    }, {
      "heading" : "6 Results",
      "text" : "Our experiments show that Splinter dramatically improves performance in the challenging few-shot setting, unlocking the ability to train question answering models with only hundreds of examples. When trained on large datasets with an order of 100,000 examples, Splinter is competitive with (and often better than) the baselines. Ablation studies demonstrate the contributions of both recurring span selection pretraining and the QASS layer."
    }, {
      "heading" : "6.1 Few-Shot Learning",
      "text" : "Figure 4 shows the F1 score (Rajpurkar et al., 2016) of Splinter-base, plotted against all baselines for two datasets, TriviaQA and TextbookQA, as a function of the number of training examples (see Figure 6 in the appendix for the remaining datasets). In addition, Table 1 shows the performance of individual models when given 16, 128, and 1024 training examples across all datasets (see Table 3 in the appendix for additional performance and standard deviation statistics). It is evident that Splinter outperforms all baselines by large margins.\nLet us examine the results on SQuAD, for example. Given 16 training examples, Splinter obtains 54.6 F1, significantly higher than the best baseline’s 18.2 F1. When the number of training examples is 128, Splinter achieves 72.7 F1, outperforming the baselines by 17 points (our reimplementation of SpanBERT) to 30 (RoBERTa). When considering 1024 examples, there is a 5-point margin\nbetween Splinter (82.8 F1) and SpanBERT (77.8 F1). The same trend is seen in the other datasets, whether they are in-domain sampled from larger datasets (e.g. TriviaQA) or not; in TextbookQA, for instance, we observe absolute gaps of 9 to 23 F1 between Splinter and the next-best baseline."
    }, {
      "heading" : "6.2 High-Resource Regime",
      "text" : "Table 1 also shows the performance when finetuning on the entire training set, when an order of 100,000 examples are available. Even though Splinter was designed for few-shot question answering, it reaches the best result in five out of six datasets. This result suggests that when the target task is extractive question answering, it is better to pretrain with our recurring span selection task than with masked langauge modeling, regardless of the number of annotated training examples."
    }, {
      "heading" : "6.3 Ablation Study",
      "text" : "We perform an ablation study to better understand the independent contributions of the pretraining scheme and the QASS layer. We first ablate the effect of pretraining on recurring span selection by applying the QASS layer to pretrained masked language models. We then test whether the QASS layer’s pretrained parameters can be reused in Splinter during fine-tuning without reinitializion.\nIndependent Contribution of the QASS Layer While the QASS layer is motivated by our pretraining scheme, it can also be used without pretraining. We apply a randomly-initialized QASS layer to our implementation of SpanBERT, and fine-tune it in the few-shot setting. Figure 5 shows the results of this ablation study for two datasets (see Figure 7 in the appendix for more datasets). We observe\nthat replacing the static span selection layer with QASS can significantly improve performance on few-shot question answering. Having said that, most of Splinter’s improvements in the extremely low data regime do stem from combining the QASS layer with our pretraining scheme, and this combination still outperforms all other variants as the amount of data grows.\nQASS Reinitialization Between pretraining and fine-tuning, we randomly reinitialize the parameters of the QASS layer. We now test the effect of fine-tuning with the QASS layer’s pretrained parameters; intuitively, the more similar the pretraining data is to the task, the better the pretrained layer will perform. Figure 5 shows that the advantage of reusing the pretrained QASS is datadependent, and can result in both performance gains (e.g. extremely low data in SQuAD) and stagnation (e.g. BioASQ with 256 examples or more). Other datasets exhibit similar trends (see appendix). We identify three conditions that determine whether keeping the pretrained head is preferable: (1) when the number of training examples is extremely low, (2) when the target domain is similar to that used at pretraining (e.g. Wikipedia), and (3) when the questions are relatively simple (e.g. SQuAD versus HotpotQA). The latter two conditions pertain to the\ncompatibility between pretraining and fine-tuning tasks; the information learned in the QASS layer is useful as long as the input and output distribution of the task are close to those seen at pretraining time."
    }, {
      "heading" : "6.4 Analysis",
      "text" : "The recurring span selection objective was designed to emulate extractive question answering using unlabeled text. How similar is it to the actual target task? To answer this question, we measure how much each pretrained model’s functionality has changed after fine-tuning on 128 examples of SQuAD. For the purpose of this analysis, we measure change in functionality by examining the vector representation of each token as produced by the transformer encoder; specifically, we measure the cosine similarity between the vector produced by\nthe pretrained model and the one produced by the fine-tuned model, given exactly the same input. We average these similarities across every token of 200 examples from SQuAD’s test set.\nTable 2 shows that Splinter’s outputs are very similar before and after fine-tuning (0.89 average cosine similarity), while the other models’ representations seem to change drastically. This suggests that fine-tuning with even 128 questionanswering examples makes significant modifications to the functionality of pretrained masked language models. Splinter’s pretraining, on the other hand, is much more similar to the fine-tuning task, resulting in much more modest changes to the produced vector representations."
    }, {
      "heading" : "7 Related Work",
      "text" : "The remarkable results of GPT-3 (Brown et al., 2020) have inspired a renewed interest in few-shot learning. While some work focuses on classification tasks (Schick and Schütze, 2020; Gao et al., 2021), our work investigates few-shot learning in the context of extractive question answering.\nOne approach to this problem is to create synthetic text-question-answer examples. Both Lewis et al. (2019) and Glass et al. (2020) use the traditional NLP pipeline to select noun phrases and named entities in Wikipedia paragraphs as potential answers, which are then masked from the context to create pseudo-questions. Lewis et al. (2019) use methods from unsupervised machine translation to translate the pseudo-questions into real ones, while Glass et al. (2020) keep the pseudo-questions but use information retrieval to find new text passages that can answer them. Both works assume access to language- and domain-specific NLP tools such as part-of-speech taggers, syntactic parsers,\nand named-entity recognizers, which might not always be available. Our work deviates from this approach by exploiting the natural phenomenon of recurring spans in order to generate multiple question-answer pairs per text passage, without assuming any language- or domain-specific models or resources are available beyond plain text.\nSimilar ideas to recurring span selection were used for creating synthetic coreference resolution examples (Kocijan et al., 2019; Varkel and Globerson, 2020), which mask single words that occur multiple times in the same context. CorefBERT (Ye et al., 2020) combines this approach with a copy mechanism for predicting the masked word during pretraining, alongside the masked language modeling objective. Unlike our approach, which was designed to align well with span selection, CorefBERT masks only single-word nouns (rather than arbitrary spans) and replaces each token in the word with a separate mask token (rather than a single mask for the entire multi-token word). Therefore, it does not emulate extractive question answering. We did not add CorefBERT as a baseline since the performance of both CorefBERT-base and CorefBERT-large was lower than SpanBERTbase’s performance on the full-data MRQA benchmark, and pretraining CorefBERT from scratch was beyond our available computational resources."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We explore the few-shot setting of extractive question answering, and demonstrate that existing methods, based on fine-tuning large pretrained language models, fail in this setup. We propose a new pretraining scheme and architecture for span selection that lead to dramatic improvements, reaching surprisingly good results even when only an order of\na hundred examples are available. Our work shows that choices that are often deemed unimportant when enough data is available, again become crucial in the few-shot setting, opening the door to new methods that take advantage of prior knowledge on the downstream task during model development."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This project was funded by the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant ERC HOLI 819080), the Blavatnik Fund, the Alon Scholarship, the Yandex Initiative for Machine Learning and Intel Corporation. We thank Google’s TPU Research Cloud (TRC) for their support in providing TPUs for this research."
    }, {
      "heading" : "A Additional Results",
      "text" : "Few-Shot Results Figure 6 shows the results on the six few-shot question answering datasets not included in Figure 4. In addition, we give the full raw results (including standard deviation) in Table 3.\nAblation Studies Figure 7 shows results of ablation studies on the six question answering datasets not included in Figure 5."
    } ],
    "references" : [ {
      "title" : "Language models are few-shot learners",
      "author" : [ "Amodei." ],
      "venue" : "Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Amodei.,? 2020",
      "shortCiteRegEx" : "Amodei.",
      "year" : 2020
    }, {
      "title" : "TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages",
      "author" : [ "Jonathan H. Clark", "Eunsol Choi", "Michael Collins", "Dan Garrette", "Tom Kwiatkowski", "Vitaly Nikolaev", "Jennimaria Palomaki." ],
      "venue" : "Transactions of the",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Quoref: A reading comprehension dataset with questions requiring coreferential reasoning",
      "author" : [ "Pradeep Dasigi", "Nelson F. Liu", "Ana Marasović", "Noah A. Smith", "Matt Gardner." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Dasigi et al\\.,? 2019",
      "shortCiteRegEx" : "Dasigi et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep biaffine attention for neural dependency parsing",
      "author" : [ "Timothy Dozat", "Christopher D. Manning." ],
      "venue" : "ICLR 2017.",
      "citeRegEx" : "Dozat and Manning.,? 2017",
      "shortCiteRegEx" : "Dozat and Manning.",
      "year" : 2017
    }, {
      "title" : "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
      "author" : [ "Dheeru Dua", "Yizhong Wang", "Pradeep Dasigi", "Gabriel Stanovsky", "Sameer Singh", "Matt Gardner." ],
      "venue" : "Proceedings of the 2019 Conference of the North American",
      "citeRegEx" : "Dua et al\\.,? 2019",
      "shortCiteRegEx" : "Dua et al\\.",
      "year" : 2019
    }, {
      "title" : "SearchQA: A new Q&A dataset augmented with context from a search engine",
      "author" : [ "Matthew Dunn", "Levent Sagun", "Mike Higgins", "V. Ugur Guney", "Volkan Cirik", "Kyunghyun Cho" ],
      "venue" : null,
      "citeRegEx" : "Dunn et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Dunn et al\\.",
      "year" : 2017
    }, {
      "title" : "MRQA 2019 shared task: Evaluating generalization in reading comprehension",
      "author" : [ "Adam Fisch", "Alon Talmor", "Robin Jia", "Minjoon Seo", "Eunsol Choi", "Danqi Chen." ],
      "venue" : "Proceedings of the 2nd Workshop on Machine Reading for Question Answering,",
      "citeRegEx" : "Fisch et al\\.,? 2019",
      "shortCiteRegEx" : "Fisch et al\\.",
      "year" : 2019
    }, {
      "title" : "Making pre-trained language models better few-shot learners",
      "author" : [ "Tianyu Gao", "Adam Fisch", "Danqi Chen." ],
      "venue" : "Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Gao et al\\.,? 2021",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "Span selection pretraining for question answering",
      "author" : [ "Michael Glass", "Alfio Gliozzo", "Rishav Chakravarti", "Anthony Ferritto", "Lin Pan", "G P Shrivatsa Bhargav", "Dinesh Garg", "Avi Sil." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for",
      "citeRegEx" : "Glass et al\\.,? 2020",
      "shortCiteRegEx" : "Glass et al\\.",
      "year" : 2020
    }, {
      "title" : "Retrieval augmented language model pre-training",
      "author" : [ "Kelvin Guu", "Kenton Lee", "Zora Tung", "Panupong Pasupat", "Mingwei Chang." ],
      "venue" : "Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Ma-",
      "citeRegEx" : "Guu et al\\.,? 2020",
      "shortCiteRegEx" : "Guu et al\\.",
      "year" : 2020
    }, {
      "title" : "SpanBERT: Improving pre-training by representing and predicting spans",
      "author" : [ "Mandar Joshi", "Danqi Chen", "Yinhan Liu", "Daniel S. Weld", "Luke Zettlemoyer", "Omer Levy." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:64–77.",
      "citeRegEx" : "Joshi et al\\.,? 2020",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2020
    }, {
      "title" : "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension",
      "author" : [ "Mandar Joshi", "Eunsol Choi", "Daniel Weld", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Joshi et al\\.,? 2017",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2017
    }, {
      "title" : "Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension",
      "author" : [ "Aniruddha Kembhavi", "Minjoon Seo", "Dustin Schwenk", "Jonghyun Choi", "Ali Farhadi", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the IEEE",
      "citeRegEx" : "Kembhavi et al\\.,? 2017",
      "shortCiteRegEx" : "Kembhavi et al\\.",
      "year" : 2017
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "ICLR 2015.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "WikiCREM: A large unsupervised corpus for coreference resolution",
      "author" : [ "Vid Kocijan", "Oana-Maria Camburu", "Ana-Maria Cretu", "Yordan Yordanov", "Phil Blunsom", "Thomas Lukasiewicz." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods",
      "citeRegEx" : "Kocijan et al\\.,? 2019",
      "shortCiteRegEx" : "Kocijan et al\\.",
      "year" : 2019
    }, {
      "title" : "Natural questions: A benchmark for question answering research",
      "author" : [ "Jakob Uszkoreit", "Quoc Le", "Slav Petrov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:453–466.",
      "citeRegEx" : "Uszkoreit et al\\.,? 2019",
      "shortCiteRegEx" : "Uszkoreit et al\\.",
      "year" : 2019
    }, {
      "title" : "Zero-shot relation extraction via reading comprehension",
      "author" : [ "Omer Levy", "Minjoon Seo", "Eunsol Choi", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 333–342, Vancou-",
      "citeRegEx" : "Levy et al\\.,? 2017",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2017
    }, {
      "title" : "Unsupervised question answering by cloze translation",
      "author" : [ "Patrick Lewis", "Ludovic Denoyer", "Sebastian Riedel." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4896–4910, Florence, Italy. Association for",
      "citeRegEx" : "Lewis et al\\.,? 2019",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2019
    }, {
      "title" : "MLQA: Evaluating cross-lingual extractive question answering",
      "author" : [ "Patrick Lewis", "Barlas Oguz", "Ruty Rinott", "Sebastian Riedel", "Holger Schwenk." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7315–",
      "citeRegEx" : "Lewis et al\\.,? 2020",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "RoBERTa: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Know what you don’t know: Unanswerable questions for SQuAD",
      "author" : [ "Pranav Rajpurkar", "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Rajpurkar et al\\.,? 2018",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2018
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Exploiting cloze questions for few shot text classification and natural language inference",
      "author" : [ "Timo Schick", "Hinrich Schütze" ],
      "venue" : null,
      "citeRegEx" : "Schick and Schütze.,? \\Q2020\\E",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2020
    }, {
      "title" : "NewsQA: A machine comprehension dataset",
      "author" : [ "Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman." ],
      "venue" : "Proceedings of the 2nd Workshop on Representation Learning for NLP, pages",
      "citeRegEx" : "Trischler et al\\.,? 2017",
      "shortCiteRegEx" : "Trischler et al\\.",
      "year" : 2017
    }, {
      "title" : "An overview of the BIOASQ large-scale biomedical",
      "author" : [ "las Baskiotis", "Patrick Gallinari", "Thierry Artiéres", "Axel-Cyrille Ngonga Ngomo", "Norman Heino", "Eric Gaussier", "Liliana Barrio-Alvers", "Michael Schroeder", "Ion Androutsopoulos", "Georgios Paliouras" ],
      "venue" : null,
      "citeRegEx" : "Baskiotis et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Baskiotis et al\\.",
      "year" : 2015
    }, {
      "title" : "Pre-training mention representations in coreference models",
      "author" : [ "Yuval Varkel", "Amir Globerson." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8534–8540, Online. Association for Computa-",
      "citeRegEx" : "Varkel and Globerson.,? 2020",
      "shortCiteRegEx" : "Varkel and Globerson.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "HotpotQA: A dataset",
      "author" : [ "Zhilin Yang", "Peng Qi", "Saizheng Zhang", "Yoshua Bengio", "William Cohen", "Ruslan Salakhutdinov", "Christopher D. Manning" ],
      "venue" : null,
      "citeRegEx" : "Yang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Coreferential Reasoning Learning for Language Representation",
      "author" : [ "Deming Ye", "Yankai Lin", "Jiaju Du", "Zhenghao Liu", "Peng Li", "Maosong Sun", "Zhiyuan Liu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Ye et al\\.,? 2020",
      "shortCiteRegEx" : "Ye et al\\.",
      "year" : 2020
    }, {
      "title" : "Revisiting fewsample BERT fine-tuning",
      "author" : [ "Tianyi Zhang", "Felix Wu", "Arzoo Katiyar", "Kilian Q. Weinberger", "Yoav Artzi." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "author" : [ "Yukun Zhu", "Ryan Kiros", "Rich Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "The IEEE International Con-",
      "citeRegEx" : "Zhu et al\\.,? 2015",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "The standard approach to question answering is to pretrain a masked language model on raw text, and then fine-tune it with a span selection layer on top (Devlin et al., 2019; Joshi et al., 2020; Liu et al., 2019).",
      "startOffset" : 153,
      "endOffset" : 212
    }, {
      "referenceID" : 11,
      "context" : "The standard approach to question answering is to pretrain a masked language model on raw text, and then fine-tune it with a span selection layer on top (Devlin et al., 2019; Joshi et al., 2020; Liu et al., 2019).",
      "startOffset" : 153,
      "endOffset" : 212
    }, {
      "referenceID" : 20,
      "context" : "The standard approach to question answering is to pretrain a masked language model on raw text, and then fine-tune it with a span selection layer on top (Devlin et al., 2019; Joshi et al., 2020; Liu et al., 2019).",
      "startOffset" : 153,
      "endOffset" : 212
    }, {
      "referenceID" : 13,
      "context" : "This assumption quickly becomes unrealistic as we venture outside the lab conditions of English Wikipedia, and attempt to crowdsource question-answer pairs in other languages or domains of expertise (Tsatsaronis et al., 2015; Kembhavi et al., 2017).",
      "startOffset" : 199,
      "endOffset" : 248
    }, {
      "referenceID" : 7,
      "context" : "Splinter exhibits surprisingly high performance given only a few training examples throughout a variety of benchmarks from the MRQA 2019 shared task (Fisch et al., 2019).",
      "startOffset" : 149,
      "endOffset" : 169
    }, {
      "referenceID" : 22,
      "context" : "This format was popularized by SQuAD (Rajpurkar et al., 2016), and has since been adopted by several datasets in various domains (Trischler et al.",
      "startOffset" : 37,
      "endOffset" : 61
    }, {
      "referenceID" : 24,
      "context" : ", 2016), and has since been adopted by several datasets in various domains (Trischler et al., 2017; Kembhavi et al., 2017) and languages (Lewis et al.",
      "startOffset" : 75,
      "endOffset" : 122
    }, {
      "referenceID" : 13,
      "context" : ", 2016), and has since been adopted by several datasets in various domains (Trischler et al., 2017; Kembhavi et al., 2017) and languages (Lewis et al.",
      "startOffset" : 75,
      "endOffset" : 122
    }, {
      "referenceID" : 19,
      "context" : ", 2017) and languages (Lewis et al., 2020; Clark et al., 2020), with some extensions allowing for unanswerable questions (Levy et al.",
      "startOffset" : 22,
      "endOffset" : 62
    }, {
      "referenceID" : 1,
      "context" : ", 2017) and languages (Lewis et al., 2020; Clark et al., 2020), with some extensions allowing for unanswerable questions (Levy et al.",
      "startOffset" : 22,
      "endOffset" : 62
    }, {
      "referenceID" : 17,
      "context" : ", 2020), with some extensions allowing for unanswerable questions (Levy et al., 2017; Rajpurkar et al., 2018) or multiple answer spans (Dua et al.",
      "startOffset" : 66,
      "endOffset" : 109
    }, {
      "referenceID" : 21,
      "context" : ", 2020), with some extensions allowing for unanswerable questions (Levy et al., 2017; Rajpurkar et al., 2018) or multiple answer spans (Dua et al.",
      "startOffset" : 66,
      "endOffset" : 109
    }, {
      "referenceID" : 7,
      "context" : "In this work, we follow the assumptions in the recent MRQA 2019 shared task (Fisch et al., 2019) and focus on questions whose answer is a single span.",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 3,
      "context" : "such as BERT (Devlin et al., 2019), and adds two parameter vectors s, e to the pretrained model in order to detect the start position s and end position e of the answer span a, respectively.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 15,
      "context" : "Taking advantage of recurring words in a passage (restricted to nouns or named entities) was proposed in past work as a signal for coreference (Kocijan et al., 2019; Ye et al., 2020).",
      "startOffset" : 143,
      "endOffset" : 182
    }, {
      "referenceID" : 30,
      "context" : "Taking advantage of recurring words in a passage (restricted to nouns or named entities) was proposed in past work as a signal for coreference (Kocijan et al., 2019; Ye et al., 2020).",
      "startOffset" : 143,
      "endOffset" : 182
    }, {
      "referenceID" : 9,
      "context" : "These simple heuristic filters do not require a model, as opposed to masking schemes in related work (Glass et al., 2020; Ye et al., 2020; Guu et al., 2020), which require partof-speech taggers, constituency parsers, or named entity recognizers.",
      "startOffset" : 101,
      "endOffset" : 156
    }, {
      "referenceID" : 30,
      "context" : "These simple heuristic filters do not require a model, as opposed to masking schemes in related work (Glass et al., 2020; Ye et al., 2020; Guu et al., 2020), which require partof-speech taggers, constituency parsers, or named entity recognizers.",
      "startOffset" : 101,
      "endOffset" : 156
    }, {
      "referenceID" : 10,
      "context" : "These simple heuristic filters do not require a model, as opposed to masking schemes in related work (Glass et al., 2020; Ye et al., 2020; Guu et al., 2020), which require partof-speech taggers, constituency parsers, or named entity recognizers.",
      "startOffset" : 101,
      "endOffset" : 156
    }, {
      "referenceID" : 3,
      "context" : "The standard approach for extractive question answering (Devlin et al., 2019) is inapplicable, because it uses fixed start and end vectors.",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 7,
      "context" : "We use a subset of the MRQA 2019 shared task (Fisch et al., 2019), which contains extractive question answering datasets in a unified format, where the answer is a single span in the given text passage.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 22,
      "context" : "Split I of the MRQA shared task contains 6 large question answering datasets: SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al.",
      "startOffset" : 84,
      "endOffset" : 108
    }, {
      "referenceID" : 24,
      "context" : ", 2016), NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al.",
      "startOffset" : 16,
      "endOffset" : 40
    }, {
      "referenceID" : 12,
      "context" : ", 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 6,
      "context" : ", 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 29,
      "context" : ", 2017), HotpotQA (Yang et al., 2018), and Natural Questions (Kwiatkowski et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 27,
      "context" : "Splinter-base shares the same architecture (transformer encoder (Vaswani et al., 2017)), vocabulary (cased wordpieces), and number of parameters (110M) with SpanBERT-base (Joshi et al.",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 11,
      "context" : ", 2017)), vocabulary (cased wordpieces), and number of parameters (110M) with SpanBERT-base (Joshi et al., 2020).",
      "startOffset" : 92,
      "endOffset" : 112
    }, {
      "referenceID" : 20,
      "context" : "RoBERTa (Liu et al., 2019) A highly-tuned and optimized version of BERT, which is known to perform well on a wide range of natural language understanding tasks.",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 11,
      "context" : "SpanBERT (Joshi et al., 2020) A BERT-style model that focuses on span representations.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : "We train Splinter-base using Adam (Kingma and Ba, 2015) for 2.",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 32,
      "context" : "(2019) and train on English Wikipedia (preprocessed by WikiExtractor as in Attardi (2015)) and the Toronto BookCorpus (Zhu et al., 2015).",
      "startOffset" : 118,
      "endOffset" : 136
    }, {
      "referenceID" : 14,
      "context" : "5 Specifically, we train all models using Adam (Kingma and Ba, 2015) with bias-corrected moment estimates for few-shot learning (Zhang et al.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 31,
      "context" : "5 Specifically, we train all models using Adam (Kingma and Ba, 2015) with bias-corrected moment estimates for few-shot learning (Zhang et al., 2021).",
      "startOffset" : 128,
      "endOffset" : 148
    }, {
      "referenceID" : 22,
      "context" : "Figure 4 shows the F1 score (Rajpurkar et al., 2016) of Splinter-base, plotted against all baselines for two datasets, TriviaQA and TextbookQA, as a function of the number of training examples (see Figure 6 in the appendix for the remaining datasets).",
      "startOffset" : 28,
      "endOffset" : 52
    }, {
      "referenceID" : 23,
      "context" : "While some work focuses on classification tasks (Schick and Schütze, 2020; Gao et al., 2021), our work investigates few-shot learning in the context of extractive question answering.",
      "startOffset" : 48,
      "endOffset" : 92
    }, {
      "referenceID" : 8,
      "context" : "While some work focuses on classification tasks (Schick and Schütze, 2020; Gao et al., 2021), our work investigates few-shot learning in the context of extractive question answering.",
      "startOffset" : 48,
      "endOffset" : 92
    }, {
      "referenceID" : 15,
      "context" : "Similar ideas to recurring span selection were used for creating synthetic coreference resolution examples (Kocijan et al., 2019; Varkel and Globerson, 2020), which mask single words that occur multiple times in the same context.",
      "startOffset" : 107,
      "endOffset" : 157
    }, {
      "referenceID" : 26,
      "context" : "Similar ideas to recurring span selection were used for creating synthetic coreference resolution examples (Kocijan et al., 2019; Varkel and Globerson, 2020), which mask single words that occur multiple times in the same context.",
      "startOffset" : 107,
      "endOffset" : 157
    }, {
      "referenceID" : 30,
      "context" : "CorefBERT (Ye et al., 2020) combines this approach with a copy mechanism for predicting the masked word during pretraining, alongside the masked language modeling objective.",
      "startOffset" : 10,
      "endOffset" : 27
    } ],
    "year" : 2021,
    "abstractText" : "In several question answering benchmarks, pretrained models have reached human parity through fine-tuning on an order of 100,000 annotated questions and answers. We explore the more realistic few-shot setting, where only a few hundred training examples are available, and observe that standard models perform poorly, highlighting the discrepancy between current pretraining objectives and question answering. We propose a new pretraining scheme tailored for question answering: recurring span selection. Given a passage with multiple sets of recurring spans, we mask in each set all recurring spans but one, and ask the model to select the correct span in the passage for each masked span. Masked spans are replaced with a special token, viewed as a question representation, that is later used during fine-tuning to select the answer span. The resulting model obtains surprisingly good results on multiple benchmarks (e.g., 72.7 F1 on SQuAD with only 128 training examples), while maintaining competitive performance in the high-resource setting.1",
    "creator" : "LaTeX with hyperref"
  }
}