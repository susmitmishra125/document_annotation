{
  "name" : "2021.acl-long.385.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction",
    "authors" : [ "Piji Li", "Shuming Shi" ],
    "emails" : [ "pijili@tencent.com", "shumingshi@tencent.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4973–4984\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4973"
    }, {
      "heading" : "1 Introduction",
      "text" : "Grammatical Error Correction (GEC) aims to automatically detect and correct the grammatical errors that can be found in a sentence (Wang et al., 2020c). It is a crucial and essential application task\n1Code: https://github.com/lipiji/TtT\nin many natural language processing scenarios such as writing assistant (Ghufron and Rosyida, 2018; Napoles et al., 2017; Omelianchuk et al., 2020), search engine (Martins and Silva, 2004; Gao et al., 2010; Duan and Hsu, 2011), speech recognition systems (Karat et al., 1999; Wang et al., 2020a; Kubis et al., 2020), etc. Grammatical errors may appear in all languages (Dale et al., 2012; Xing et al., 2013; Ng et al., 2014; Rozovskaya et al., 2015; Bryant et al., 2019), in this paper, we only focus to tackle the problem of Chinese Grammatical Error Correction (CGEC) (Chang, 1995).\nWe investigate the problem of CGEC and the related corpora from SIGHAN (Tseng et al., 2015) and NLPCC (Zhao et al., 2018) carefully, and we conclude that the grammatical error types as well as the corresponding correction operations can be categorised into three folds, as shown in Figure 1: (1) Substitution. In reality, Pinyin is the most popular input method used for Chinese writings. Thus, the homophonous character confusion (For example, in the case of Type I, the pronunciation of the wrong and correct words are both “FeiChang”) is the fundamental reason which causes grammatical errors (or spelling errors) and can be corrected by substitution operations without changing the whole sequence structure (e.g., length). Thus, substitution is a fixed-length (FixLen) operation. (2) Deletion\nand Insertion. These two operations are used to handle the cases of word redundancies and omissions respectively. (3) Local paraphrasing. Sometimes, light operations such as substitution, deletion, and insertion cannot correct the errors directly, therefore, a slightly subsequence paraphrasing is required to reorder partial words of the sentence, the case is shown in Type III of Figure 1. Deletion, insertion, and local paraphrasing can be regarded as variable-length (VarLen) operations because they may change the sentence length.\nHowever, over the past few years, although a number of methods have been developed to deal with the problem of CGEC, some crucial and essential aspects are still uncovered. Generally, sequence translation and sequence tagging are the two most typical technical paradigms to tackle the problem of CGEC. Benefiting from the development of neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), attention-based seq2seq encoder-decoder frameworks have been introduced to address the CGEC problem in a sequence translation manner (Wang et al., 2018; Ge et al., 2018; Wang et al., 2019, 2020b; Kaneko et al., 2020). Seq2seq based translation models are easily to be trained and can handle all types of correcting operations above mentioned. However, considering the exposure bias issue (Ranzato et al., 2016; Zhang et al., 2019), the generated results usually suffer from the phenomenon of hallucination (Nie et al., 2019; Maynez et al., 2020) and cannot be faithful to the source text, even though copy mechanisms (Gu et al., 2016) are incorporated (Wang et al., 2019). Therefore, Omelianchuk et al. (2020) and Liang et al. (2020) propose to purely employ tagging to conduct the problem of GEC instead of generation. All correcting operations such as deletion, insertion, and substitution can be guided by the predicted tags. Nevertheless, the pure tagging strategy requires to extend\nthe vocabulary V to about three times by adding “insertion-” and “substitution-” prefixes to the original tokens (e.g., “insertion-good”, “substitutionpaper”) which decrease the computing efficiency dramatically. Moreover, the pure tagging framework needs to conduct multi-pass prediction until no more operations are predicted, which is inefficient and less elegant. Recently, many researchers fine-tune the pre-trained language models such as BERT on the task of CGEC and obtain reasonable results (Zhao et al., 2019; Hong et al., 2019; Zhang et al., 2020b). However, limited by the BERT framework, most of them can only address the fixed-length correcting scenarios and cannot conduct deletion, insertion, and local paraphrasing operations flexibly.\nMoreover, during the investigations, we also observe an obvious but crucial phenomenon for CGEC that most words in a sentence are correct and need not to be changed. This phenomenon is depicted in Figure 2, where the operation flow is from the bottom tail to the up tail. Grey dash lines represent the “Keep” operations and the red solid lines indicate those three types of correcting operations mentioned above. On one side, intuitively, the target CGEC model should have the ability of directly moving the correct tokens from bottom tail to up tail, then Transformer(Vaswani et al., 2017) based encoder (say BERT) seems to be a preference. On the other side, considering that almost all typical CGEC models are built based on the paradigms of sequence tagging or sequence translation, Maximum Likelihood Estimation (MLE) (Myung, 2003) is usually used as the parameter learning approach, which in the scenario of CGEC, will suffer from a severe class/tag imbalance issue. However, no previous works investigate this problem thoroughly on the task of CGEC.\nTo conquer all above-mentioned challenges, we propose a new framework named tail-to-tail non-\nautoregressive sequence prediction, which abbreviated as TtT, for the problem of CGEC. Specifically, to directly move the token information from the bottom tail to the up tail, a BERT based sequence encoder is introduced to conduct bidirectional representation learning. In order to conduct substitution, deletion, insertion, and local paraphrasing simultaneously, inspired by (Sun et al., 2019), a Conditional Random Fields (CRF) (Lafferty et al., 2001) layer is stacked on the up tail to conduct nonautoregressive sequence prediction by modeling the dependencies among neighbour tokens. Focal loss penalty strategy (Lin et al., 2020) is adopted to alleviate the class imbalance problem considering that most of the tokens in a sentence are not changed. In summary, our contributions are as follows:\n• A new framework named tail-to-tail nonautoregressive sequence prediction (TtT) is proposed to tackle the problem of CGEC. • BERT encoder with a CRF layer is employed as the backbone, which can conduct substitution, deletion, insertion, and local paraphrasing simultaneously. • Focal loss penalty strategy is adopted to alleviate the class imbalance problem considering that most of the tokens in a sentence are not changed. • Extensive experiments on several benchmark datasets, especially on the variable-length grammatical correction datasets, demonstrate the effectiveness of the proposed approach."
    }, {
      "heading" : "2 The Proposed TtT Framework",
      "text" : ""
    }, {
      "heading" : "2.1 Overview",
      "text" : "Figure 3 depicts the basic components of our proposed framework TtT. Input is an incorrect sen-\ntence X = (x1, x2, . . . , xT ) which contains grammatical errors, where xi denotes each token (Chinese character) in the sentence, and T is the length of X . The objective of the task grammatical error correction is to correct all errors in X and generate a new sentence Y = (y1, y2, . . . , yT ′). Here, it is important to emphasize that T is not necessary equal to T ′. Therefore, T ′ can be =, >, or < T . Bidirectional semantic modeling and bottom-to-up directly token information conveying are conducted by several Transformer (Vaswani et al., 2017) layers. A Conditional Random Fields (CRF) (Lafferty et al., 2001) layer is stacked on the up tail to conduct the non-autoregressive sequence generation by modeling the dependencies among neighboring tokens. Low-rank decomposition and beamed Viterbi algorithm are introduced to accelerate the computations. Focal loss penalty strategy (Lin et al., 2020) is adopted to alleviate the class imbalance problem during the training stage."
    }, {
      "heading" : "2.2 Variable-Length Input",
      "text" : "Since the length T ′ of the target sentence Y is not necessary equal to the length T of the input sequence X . Then in the training and inference stage, different length will affect the completeness of the predicted sentence, especially when T < T ′. To handle this issue, several simple tricks are designed to pre-process the samples. Assuming X = (x1, x2, x3,<eos>): (1) When T = T ′, i.e., Y = (y1, y2, y3,<eos>), then do nothing; (2) When T > T ′, say Y = (y1, y2,<eos>), which means that some tokens inX will be deleted during correcting. Then in the training stage, we can pad T − T ′ special tokens <pad> to the tail of Y to make T = T ′, then\nY = (y1, y2,<eos>,<pad>);\n(3) When T < T ′, say\nY = (y1, y2, y3, y4, y5,<eos>),\nwhich means that more information should be inserted into the original sentence X . Then, we will pad the special symbol <mask> to the tail of X to indicate that these positions possibly can be translated into some new real tokens:\nX = (x1, x2, x3,<eos>,<mask>,<mask>)."
    }, {
      "heading" : "2.3 Bidirectional Semantic Modeling",
      "text" : "Transformer layers (Vaswani et al., 2017) are particularly well suited to be employed to conduct the bidirectional semantic modeling and bottom-to-up information conveying. As shown in Figure 3, after preparing the input samples, an embedding layer and a stack of Transformer layers initialized with a pre-trained Chinese BERT (Devlin et al., 2019) are followed to conduct the semantic modeling.\nSpecifically, for the input, we first obtain the representations by summing the word embeddings with the positional embeddings:\nH0t = Ewt +Ept (1)\nwhere 0 is the layer index and t is the state index. Ew and Ep are the embedding vectors for tokens and positions, respectively.\nThen the obtained embedding vectors H0 are fed into several Transformer layers. Multi-head self-attention is used to conduct bidirectional representation learning:\nH1t = LN ( FFN(H1t ) +H 1 t ) H1t = LN ( SLF-ATT(Q0t ,K 0,V0) +H0t ) Q0 = H0WQ\nK0,V0 = H0WK ,H0WV\n(2)\nwhere SLF-ATT(·), LN(·), and FFN(·) represent self-attention mechanism, layer normalization, and feed-forward network respectively (Vaswani et al., 2017). Note that our model is a non-autoregressive sequence prediction framework, thus we use all the sequence states K0 and V0 as the attention context. Then each node will absorb the context information bidirectionally. After L Transformer layers, we obtain the final output representation vectors HL ∈ Rmax(T,T ′)×d."
    }, {
      "heading" : "2.4 Non-Autoregressive Sequence Prediction",
      "text" : "Direct Prediction The objective of our model is to translate the input sentence X which contains grammatical errors into a correct sentence Y . Then, since we have obtained the sequence representation vectors HL, we can directly add a softmax layer to predict the results, just similar to the methods used in non-autoregressive neural machine translation (Gu and Kong, 2020) and BERT-based finetuning framework for the task of grammatical error correction (Zhao et al., 2019; Hong et al., 2019; Zhang et al., 2020b).\nSpecifically, a linear transformation layer is plugged in and softmax operation is utilized to generate a probability distribution Pdp(yt) over the target vocabulary V:\nst = h > t Ws + bs\nPdp(yt) = softmax(st) (3)\nwhere ht ∈ Rd, Ws ∈ Rd×|V|, bs ∈ R|V|, and st ∈ R|V|. Then we obtain the result for each state based on the predicted distribution:\ny′t = argmax(Pdp(yt)) (4)\nHowever, although this direct prediction method is effective on the fixed-length grammatical error correction problem, it can only conduct the samepositional substitution operation. For complex correcting cases which require deletion, insertion, and local paraphrasing, the performance is unacceptable. This inferior performance phenomenon is also discussed in the tasks of non-autoregressive neural machine translation (Gu and Kong, 2020).\nOne of the essential reasons causing the inferior performance is that the dependency information among the neighbour tokens are missed. Therefore, dependency modeling should be called back to improve the performance of generation. Naturally, linear-chain CRF (Lafferty et al., 2001) is introduced to fix this issue, and luckily, Sun et al. (2019) also employ CRF to address the problem of non-autoregressive sequence generation, which inspired us a lot.\nDependency Modeling via CRF Then given the input sequence X , under the CRF framework, the likelihood of the target sequence Y with length T ′\nis constructed as:\nPcrf(Y |X) =\n1\nZ(X) exp ( T ′∑ t=1 s(yt) + T ′∑ t=2 t(yt−1, yt) ) (5)\nwhere Z(X) is the normalizing factor and s(yt) represents the label score of y at position t, which can be obtained from the predicted logit vector st ∈ R|V| from Eq. (3), i.e., st(Vyt), where Vyt is the vocabulary index of token yt. The value t(yt−1, yt) = Myt−1,yt denotes the transition score from token yt−1 to yt where M ∈ R|V|×|V| is the transition matrix, which is the core term to conduct dependency modeling. Usually, M can be learnt as neural network parameters during the end-to-end training procedure. However, |V| is typically very large especially in the text generation scenarios (more than 32k), therefore it is infeasible to obtain M and Z(X) efficiently in practice. To overcome this obstacle, as the method used in (Sun et al., 2019), we introduce two low-rank neural parameter metrics E1, E2 ∈ R|V|×dm to approximate the fullrank transition matrix M by:\nM = E1E > 2 (6)\nwhere dm |V|. To compute the normalizing factor Z(X), the original Viterbi algorithm (Forney, 1973; Lafferty et al., 2001) need to search all paths. To improve the efficiency, here we only visit the truncated top-k nodes at each time step approximately (Sun et al., 2019)."
    }, {
      "heading" : "2.5 Training with Focal Penalty",
      "text" : "Considering the characteristic of the directly bottom-to-up information conveying of the task CGEC, therefore, both tasks, direct prediction and CRF-based dependency modeling, can be incorporated jointly into a unified framework during the training stage. The reasons are that, intuitively, direct prediction will focus on the fine-grained predictions at each position, while CRF-layer will pay more attention to the high-level quality of the whole global sequence. We employ Maximum Likelihood Estimation (MLE) to conduct parameter learning and treat negative log-likelihood (NLL) as the loss function. Thus, the optimization objective for direct prediction Ldp is:\nLdp = − T ′∑ t=1 logPdp(yt|X) (7)\nAnd the loss function Lcrf for CRF-based dependency modeling is:\nLcrf = − logPcrf(Y |X) (8)\nThen the final optimization objective is:\nL = Ldp + Lcrf (9)\nAs mentioned in Section 1, one obvious but crucial phenomenon for CGEC is that most words in a sentence are correct and need not to be changed. Considering that maximum likelihood estimation is used as the parameter learning approach in those two tasks, then a simple copy strategy can lead to a sharp decline in terms of loss functions. Then, intuitively, the grammatical error tokens which need to be correctly fixed in practice, unfortunately, attract less attention during the training procedure. Actually, these tokens, instead, should be regarded as the focal points and contribute more to the optimization objectives. However, no previous works investigate this problem thoroughly on the task of CGEC.\nTo alleviate this issue, we introduce a useful trick, focal loss (Lin et al., 2020) , into our loss functions for direct prediction and CRF:\nLfldp = − T ′∑ t=1 (1− Pdp(yt|X))γ logPdp(yt|X) Lflcrf = −(1− Pcrf(Y |X))γ logPcrf(Y |X) (10)\nwhere γ is a hyperparameter to control the penalty weight. It is obvious that Lfldp is penalized on the token level, while Lflcrf is weighted on the sample level and will work in the condition of batchtraining. The final optimization objective with focal penalty strategy is:\nLfl = Lfldp + Lflcrf (11)"
    }, {
      "heading" : "2.6 Inference",
      "text" : "During the inference stage, for the input source sentence X , we can employ the original |V| nodes Viterbi algorithm to obtain the target global optimal result. We can also utilize the truncated top-k Viterbi algorithm for high computing efficiency (Sun et al., 2019)."
    }, {
      "heading" : "3 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "3.1 Settings",
      "text" : "The core technical components of our proposed TtT is Transformer (Vaswani et al., 2017) and CRF (Lafferty et al., 2001). The pre-trained Chinese BERTbase model (Devlin et al., 2019) is employed to initialize the model. To approximate the transition matrix in the CRF layer, we set the dimension d of matrices E1 and E2 as 32. For the normalizing factor Z(X), we set the predefined beam size k as 64. The hyperparameter γ which is used to weight the focal penalty term is set to 0.5 after parameter tuning. Training batch-size is 100, learning rate is 1e − 5, dropout rate is 0.1. Adam optimizer (Kingma and Ba, 2015) is used to conduct the parameter learning."
    }, {
      "heading" : "3.2 Datasets",
      "text" : "The overall statistic information of the datasets used in our experiments are depicted in Table 1. SIGHAN15 (Tseng et al., 2015)2 This is a benchmark dataset for the evaluation of CGEC and it contains 2,339 samples for training and 1,100 samples for testing. As did in some typical previous works (Wang et al., 2019; Zhang et al., 2020b), we also use the SIGHAN15 testset as the benchmark dataset to evaluate the performance of our models as well as the baseline methods in fixed-length (FixLen) error correction settings. HybirdSet (Wang et al., 2018)3 It is a newly released dataset constructed according to a prepared confusion set based on the results of ASR (Yu and Deng, 2014) and OCR (Tong and Evans, 1996). This dataset contains about 270k paired samples and it is also a FixLen dataset. TtTSet Considering that datasets of SIGHAN15 and HybirdSet are all FixLen type datasets, in order to demonstrate the capability of our model TiT on the scenario of Variable-Length (VarLen) CGEC, based on the corpus of HybirdSet, we\n2http://ir.itc.ntnu.edu.tw/lre/ sighan8csc.html\n3https://github.com/wdimmy/ Automatic-Corpus-Generation\nbuild a new VarLen dataset. Specifically, operations of deletion, insertion, and local shuffling are conducted on the original sentences to obtain the incorrect samples. Each operation covers one-third of samples, thus we get about 540k samples finally."
    }, {
      "heading" : "3.3 Comparison Methods",
      "text" : "We compare the performance of TtT with several strong baseline methods on both FixLen and VarLen settings. NTOU employs n-gram language model with a reranking strategy to conduct prediction (Tseng et al., 2015). NCTU-NTUT also uses CRF to conduct label dependency modeling (Tseng et al., 2015). HanSpeller++ employs Hidden Markov Model with a reranking strategy to conduct the prediction (Zhang et al., 2015). Hybrid utilizes LSTM-based seq2seq framework to conduct generation (Wang et al., 2018) and Confusionset introduces a copy mechanism into seq2seq framework (Wang et al., 2019). FASPell incorporates BERT into the seq2seq for better performance (Hong et al., 2019). SoftMask-BERT firstly conducts error detection using a GRU-based model and then incorporating the predicted results with the BERT model using a soft-masked strategy (Zhang et al., 2020b). Note that the best results of SoftMask-BERT are obtained after pre-training on a large-scale dataset with 500M paired samples. SpellGCN proposes to incorporate phonological and visual similarity knowledge into language models via a specialized graph convolutional network (Cheng et al., 2020). Chunk proposes a chunk-based decoding method with global optimization to correct single character and multi-character word typos in a unified framework (Bao et al., 2020).\nWe also implement some classical methods for comparison and ablation analysis, especially for the VarLen correction problem. Transformer-s2s is the typical Transformer-based seq2seq framework for sequence prediction (Vaswani et al., 2017). GPT2-finetune is also a sequence generation framework fine-tuned based on a pre-trained Chinese GPT2 model4 (Radford et al., 2019; Li, 2020). BERT-finetune is just fine-tune the Chinese BERT model on the CGEC corpus directly. Beam search decoding strategy is employed to con-\n4https://github.com/lipiji/Guyu\nduct generation for Transformer-s2s and GPT2finetune, and beam-size is 5. Note that some of the original methods above mentioned can only work in the FixLen settings, such as SoftMask-BERT and BERT-finetune."
    }, {
      "heading" : "3.4 Evaluation Metrics",
      "text" : "Following the typical previous works (Wang et al., 2019; Hong et al., 2019; Zhang et al., 2020b), we employ sentence-level Accuracy, Precision, Recall, and F1-Measure as the automatic metrics to evaluate the performance of all systems5. We also report the detailed results for error Detection (all locations of incorrect characters in a given sentence should be completely identical with the gold standard) and Correction (all locations and corresponding corrections of incorrect characters should be completely identical with the gold standard) respectively (Tseng et al., 2015)."
    }, {
      "heading" : "4 Results and Discussions",
      "text" : ""
    }, {
      "heading" : "4.1 Results in FixLen Scenario",
      "text" : "Table 2 depicts the main evaluation results of our proposed framework TtT as well as the comparison baseline methods. It should be emphasized\n5http://nlp.ee.ncu.edu.tw/resource/csc. html\nthat SoftMask-BERT is pre-trained on a 500Msize paired dataset. Our model TtT, as well as the baseline methods such as Transformer-s2s, GPT2-finetune, BERT-finetune, and Hybird are all trained on the 270k-size HybirdSet. Nevertheless, TtT obtains improvements on the tasks of error Detection (F1:77.7→ 81.6) and Correction (F1:75.9→ 80.0) compared to all strong baselines on F1 metric, which indicates the superiority of our proposed approach."
    }, {
      "heading" : "4.2 Results in VarLen Scenario",
      "text" : "Benefit from the CRF-based dependency modeling component, TtT can conduct deletion, insertion, local paraphrasing operations jointly to address the Variable-Length (VarLen) error correction problem. The experimental results are described in Table 3. Considering that those sequence generation methods such as Transformer-s2s and GPT2-finetune can also conduct VarLen correction operation, thus we report their results as well. From the results, we can observe that TtT can also achieve a superior performance in the VarLen scenario. The reasons are clear: BERT-finetune as well as the related methods are not appropriate in VarLen scenario, especially when the target is longer than the input. The text generation models such as Transformers2s and GPT2-finetune suffer from the problem of hallucination (Maynez et al., 2020) and repetition,\nwhich are not steady on the problem of CGEC."
    }, {
      "heading" : "4.3 Ablation Analysis",
      "text" : "Different Training Dataset Recall that we introduce several groups of training datasets in different scales as depicted in Table 1. It is also very interesting to investigate the performances on differentsize datasets. Then we conduct training on those training datasets and report the results still on the SIGHAN2015 testset. The results are shown in Table 4. No matter what scale of the dataset is, TtT always obtains the best performance.\nImpact of Ldp and Lcrf Table 5 describes the performance of our model TtT and the variants withoutLdp (TtT w/oLdp) andLcrf (TtT w/oLcrf ). We can conclude that the fusion of these two tasks, direct prediction and CRF-based dependency mod-\neling, can indeed improve the performance.\nParameter Tuning for Focal Loss The focal loss penalty hyperparameter γ is crucial for the loss function L = Ldp + Lcrf and should be adjusted on the specific tasks (Lin et al., 2020). We conduct grid search for γ ∈ (0, 0.1, 0.5, 1, 2, 5) and the corresponding results are provided in Table 6. Finally, we select γ = 0.5 for TtT for the CGEC task."
    }, {
      "heading" : "4.4 Computing Efficiency Analysis",
      "text" : "Practically, CGEC is an essential and useful task and the techniques can be used in many real applications such as writing assistant, post-processing of ASR and OCR, search engine, etc. Therefore, the time cost efficiency of models is a key point which needs to be taken into account. Table 7 depicts the time cost per sample of our model TtT and\nsome baseline approaches. The results demonstrate that TtT is a cost-effective method with superior prediction performance and low computing time complexity, and can be deployed online directly."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We propose a new framework named tail-to-tail non-autoregressive sequence prediction, which abbreviated as TtT, for the problem of CGEC. A BERT based sequence encoder is introduced to conduct bidirectional representation learning. In order to conduct substitution, deletion, insertion, and local paraphrasing simultaneously, a CRF layer is stacked on the up tail to conduct non-autoregressive sequence prediction by modeling the dependencies among neighbour tokens. Low-rank decomposition and a truncated Viterbi algorithm are introduced to accelerate the computations. Focal loss penalty strategy is adopted to alleviate the class imbalance problem considering that most of the tokens in a sentence are not changed. Experimental results on standard datasets demonstrate the effectiveness of TtT in terms of sentence-level Accuracy, Precision, Recall, and F1-Measure on tasks of error Detection and Correction. TtT is of low computing complexity and can be deployed online directly.\nIn the future, we plan to introduce more lexical analysis knowledge such as word segmentation and fine-grained named entity recognition (Zhang et al., 2020a) to further improve the performance."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Chunk-based chinese spelling check with global optimization",
      "author" : [ "Zuyi Bao", "Chen Li", "Rui Wang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020, Online Event, 16-20 November 2020,",
      "citeRegEx" : "Bao et al\\.,? 2020",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2020
    }, {
      "title" : "The BEA-2019 shared task on grammatical error correction",
      "author" : [ "Christopher Bryant", "Mariano Felice", "Øistein E. Andersen", "Ted Briscoe." ],
      "venue" : "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications,",
      "citeRegEx" : "Bryant et al\\.,? 2019",
      "shortCiteRegEx" : "Bryant et al\\.",
      "year" : 2019
    }, {
      "title" : "A new approach for automatic chinese spelling correction",
      "author" : [ "Chao-Huang Chang." ],
      "venue" : "Proceedings of Natural Language Processing Pacific Rim Symposium, pages 278–283. Citeseer.",
      "citeRegEx" : "Chang.,? 1995",
      "shortCiteRegEx" : "Chang.",
      "year" : 1995
    }, {
      "title" : "Spellgcn: Incorporating phonological and visual similarities into language models for chinese spelling check",
      "author" : [ "Xingyi Cheng", "Weidi Xu", "Kunlong Chen", "Shaohua Jiang", "Feng Wang", "Taifeng Wang", "Wei Chu", "Yuan Qi." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Cheng et al\\.,? 2020",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2020
    }, {
      "title" : "HOO 2012: A report on the preposition and determiner error correction shared task",
      "author" : [ "Robert Dale", "Ilya Anisimoff", "George Narroway." ],
      "venue" : "Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, BEA@NAACL-HLT",
      "citeRegEx" : "Dale et al\\.,? 2012",
      "shortCiteRegEx" : "Dale et al\\.",
      "year" : 2012
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Online spelling correction for query completion",
      "author" : [ "Huizhong Duan", "Bo-June Paul Hsu." ],
      "venue" : "Proceedings of the 20th International Conference on World Wide Web, WWW 2011, Hyderabad, India, March 28 - April 1, 2011, pages 117–126. ACM.",
      "citeRegEx" : "Duan and Hsu.,? 2011",
      "shortCiteRegEx" : "Duan and Hsu.",
      "year" : 2011
    }, {
      "title" : "The viterbi algorithm",
      "author" : [ "G David Forney." ],
      "venue" : "Proceedings of the IEEE, 61(3):268–278.",
      "citeRegEx" : "Forney.,? 1973",
      "shortCiteRegEx" : "Forney.",
      "year" : 1973
    }, {
      "title" : "A large scale ranker-based system for search query spelling correction",
      "author" : [ "Jianfeng Gao", "Xiaolong Li", "Daniel Micol", "Chris Quirk", "Xu Sun." ],
      "venue" : "COLING 2010, 23rd International Conference on Computational Linguistics, Proceedings of the Confer-",
      "citeRegEx" : "Gao et al\\.,? 2010",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2010
    }, {
      "title" : "Reaching human-level performance in automatic grammatical error correction: An empirical study",
      "author" : [ "Tao Ge", "Furu Wei", "Ming Zhou." ],
      "venue" : "CoRR, abs/1807.01270.",
      "citeRegEx" : "Ge et al\\.,? 2018",
      "shortCiteRegEx" : "Ge et al\\.",
      "year" : 2018
    }, {
      "title" : "The role of grammarly in assessing english as a foreign language (efl) writing",
      "author" : [ "M Ali Ghufron", "Fathia Rosyida." ],
      "venue" : "Lingua Cultura, 12(4):395–403.",
      "citeRegEx" : "Ghufron and Rosyida.,? 2018",
      "shortCiteRegEx" : "Ghufron and Rosyida.",
      "year" : 2018
    }, {
      "title" : "Fully nonautoregressive neural machine translation: Tricks of the trade",
      "author" : [ "Jiatao Gu", "Xiang Kong." ],
      "venue" : "CoRR, abs/2012.15833.",
      "citeRegEx" : "Gu and Kong.,? 2020",
      "shortCiteRegEx" : "Gu and Kong.",
      "year" : 2020
    }, {
      "title" : "Incorporating copying mechanism in sequence-to-sequence learning",
      "author" : [ "Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,",
      "citeRegEx" : "Gu et al\\.,? 2016",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2016
    }, {
      "title" : "Faspell: A fast, adaptable, simple, powerful chinese spell checker based on dae-decoder paradigm",
      "author" : [ "Yuzhong Hong", "Xianguo Yu", "Neng He", "Nan Liu", "Junhui Liu." ],
      "venue" : "Proceedings of the 5th Workshop on Noisy User-generated Text, W-",
      "citeRegEx" : "Hong et al\\.,? 2019",
      "shortCiteRegEx" : "Hong et al\\.",
      "year" : 2019
    }, {
      "title" : "Encoder-decoder models can benefit from pre-trained masked language models in grammatical error correction",
      "author" : [ "Masahiro Kaneko", "Masato Mita", "Shun Kiyono", "Jun Suzuki", "Kentaro Inui." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the As-",
      "citeRegEx" : "Kaneko et al\\.,? 2020",
      "shortCiteRegEx" : "Kaneko et al\\.",
      "year" : 2020
    }, {
      "title" : "Patterns of entry and correction in large vocabulary continuous speech recognition system",
      "author" : [ "Clare-Marie Karat", "Christine Halverson", "Daniel B. Horn", "John Karat." ],
      "venue" : "Proceeding of the CHI ’99 Conference on Human Factors in Computing Sys-",
      "citeRegEx" : "Karat et al\\.,? 1999",
      "shortCiteRegEx" : "Karat et al\\.",
      "year" : 1999
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Open challenge for correcting errors of speech recognition systems",
      "author" : [ "Marek Kubis", "Zygmunt Vetulani", "Mikolaj Wypych", "Tomasz Zietkiewicz." ],
      "venue" : "CoRR, abs/2001.03041.",
      "citeRegEx" : "Kubis et al\\.,? 2020",
      "shortCiteRegEx" : "Kubis et al\\.",
      "year" : 2020
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira." ],
      "venue" : "Proceedings of the Eighteenth International Conference on Machine Learning (ICML",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "An empirical investigation of pre-trained transformer language models for open-domain dialogue generation",
      "author" : [ "Piji Li." ],
      "venue" : "CoRR, abs/2003.04195.",
      "citeRegEx" : "Li.,? 2020",
      "shortCiteRegEx" : "Li.",
      "year" : 2020
    }, {
      "title" : "Focal loss for dense",
      "author" : [ "ing He", "Piotr Dollár" ],
      "venue" : null,
      "citeRegEx" : "He and Dollár.,? \\Q2020\\E",
      "shortCiteRegEx" : "He and Dollár.",
      "year" : 2020
    }, {
      "title" : "On faithfulness and fac",
      "author" : [ "Ryan T. McDonald" ],
      "venue" : null,
      "citeRegEx" : "McDonald.,? \\Q2020\\E",
      "shortCiteRegEx" : "McDonald.",
      "year" : 2020
    }, {
      "title" : "Tutorial on maximum likelihood",
      "author" : [ "Jae Myung" ],
      "venue" : null,
      "citeRegEx" : "Myung.,? \\Q2003\\E",
      "shortCiteRegEx" : "Myung.",
      "year" : 2003
    }, {
      "title" : "The conll-2014 shared task on",
      "author" : [ "pher Bryant" ],
      "venue" : null,
      "citeRegEx" : "Bryant.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bryant.",
      "year" : 2014
    }, {
      "title" : "A simple recipe towards",
      "author" : [ "Chin-Yew Lin" ],
      "venue" : null,
      "citeRegEx" : "Lin.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lin.",
      "year" : 2019
    }, {
      "title" : "Introduction to SIGHAN",
      "author" : [ "Hsin-Hsi Chen" ],
      "venue" : null,
      "citeRegEx" : "Chen.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chen.",
      "year" : 2015
    }, {
      "title" : "Attention is all",
      "author" : [ "Kaiser", "Illia Polosukhin" ],
      "venue" : null,
      "citeRegEx" : "Kaiser and Polosukhin.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kaiser and Polosukhin.",
      "year" : 2017
    }, {
      "title" : "A hybrid approach to auto",
      "author" : [ "Haisong Zhang" ],
      "venue" : null,
      "citeRegEx" : "Zhang.,? \\Q2018\\E",
      "shortCiteRegEx" : "Zhang.",
      "year" : 2018
    }, {
      "title" : "2020a. Texsmart: A text",
      "author" : [ "Kang", "Shuming Shi" ],
      "venue" : null,
      "citeRegEx" : "Kang and Shi.,? \\Q2020\\E",
      "shortCiteRegEx" : "Kang and Shi.",
      "year" : 2020
    }, {
      "title" : "Spelling error correction with soft",
      "author" : [ "Li. 2020b" ],
      "venue" : null,
      "citeRegEx" : "2020b.,? \\Q2020\\E",
      "shortCiteRegEx" : "2020b.",
      "year" : 2020
    }, {
      "title" : "Bridging the gap between training and inference for neural machine translation",
      "author" : [ "Wen Zhang", "Yang Feng", "Fandong Meng", "Di You", "Qun Liu." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence,",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving grammatical error correction via pre-training a copy-augmented architecture with unlabeled data",
      "author" : [ "Wei Zhao", "Liang Wang", "Kewei Shen", "Ruoyu Jia", "Jingming Liu." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chap-",
      "citeRegEx" : "Zhao et al\\.,? 2019",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2019
    }, {
      "title" : "Overview of the NLPCC 2018 shared task: Grammatical error correction",
      "author" : [ "Yuanyuan Zhao", "Nan Jiang", "Weiwei Sun", "Xiaojun Wan." ],
      "venue" : "Natural Language Processing and Chinese Computing - 7th CCF International Conference, NLPCC 2018,",
      "citeRegEx" : "Zhao et al\\.,? 2018",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "in many natural language processing scenarios such as writing assistant (Ghufron and Rosyida, 2018; Napoles et al., 2017; Omelianchuk et al., 2020), search engine (Martins and Silva, 2004; Gao et al.",
      "startOffset" : 72,
      "endOffset" : 147
    }, {
      "referenceID" : 16,
      "context" : "2010; Duan and Hsu, 2011), speech recognition systems (Karat et al., 1999; Wang et al., 2020a; Kubis et al., 2020), etc.",
      "startOffset" : 54,
      "endOffset" : 114
    }, {
      "referenceID" : 18,
      "context" : "2010; Duan and Hsu, 2011), speech recognition systems (Karat et al., 1999; Wang et al., 2020a; Kubis et al., 2020), etc.",
      "startOffset" : 54,
      "endOffset" : 114
    }, {
      "referenceID" : 3,
      "context" : ", 2019), in this paper, we only focus to tackle the problem of Chinese Grammatical Error Correction (CGEC) (Chang, 1995).",
      "startOffset" : 107,
      "endOffset" : 120
    }, {
      "referenceID" : 33,
      "context" : ", 2015) and NLPCC (Zhao et al., 2018) carefully, and we conclude that the grammatical error types as well",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "opment of neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), attention-based seq2seq encoder-decoder frameworks have been introduced to address the CGEC problem in a sequence translation manner (Wang et al.",
      "startOffset" : 37,
      "endOffset" : 82
    }, {
      "referenceID" : 10,
      "context" : ", 2017), attention-based seq2seq encoder-decoder frameworks have been introduced to address the CGEC problem in a sequence translation manner (Wang et al., 2018; Ge et al., 2018; Wang et al., 2019, 2020b; Kaneko et al., 2020).",
      "startOffset" : 142,
      "endOffset" : 225
    }, {
      "referenceID" : 15,
      "context" : ", 2017), attention-based seq2seq encoder-decoder frameworks have been introduced to address the CGEC problem in a sequence translation manner (Wang et al., 2018; Ge et al., 2018; Wang et al., 2019, 2020b; Kaneko et al., 2020).",
      "startOffset" : 142,
      "endOffset" : 225
    }, {
      "referenceID" : 13,
      "context" : ", 2020) and cannot be faithful to the source text, even though copy mechanisms (Gu et al., 2016) are incorporated (Wang et al.",
      "startOffset" : 79,
      "endOffset" : 96
    }, {
      "referenceID" : 23,
      "context" : "On the other side, considering that almost all typical CGEC models are built based on the paradigms of sequence tagging or sequence translation, Maximum Likelihood Estimation (MLE) (Myung, 2003) is usually used as the parameter learning approach, which in the scenario of CGEC, will suffer from a severe class/tag imbalance issue.",
      "startOffset" : 181,
      "endOffset" : 194
    }, {
      "referenceID" : 19,
      "context" : ", 2019), a Conditional Random Fields (CRF) (Lafferty et al., 2001) layer is stacked on the up tail to conduct non-",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 19,
      "context" : "A Conditional Random Fields (CRF) (Lafferty et al., 2001) layer is stacked on the up tail to conduct the non-autoregressive sequence generation by modeling the dependencies among neighboring to-",
      "startOffset" : 34,
      "endOffset" : 57
    }, {
      "referenceID" : 6,
      "context" : "As shown in Figure 3, after preparing the input samples, an embedding layer and a stack of Transformer layers initialized with a pre-trained Chinese BERT (Devlin et al., 2019) are followed to conduct the semantic modeling.",
      "startOffset" : 154,
      "endOffset" : 175
    }, {
      "referenceID" : 12,
      "context" : "lation (Gu and Kong, 2020) and BERT-based finetuning framework for the task of grammatical error correction (Zhao et al.",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 32,
      "context" : "lation (Gu and Kong, 2020) and BERT-based finetuning framework for the task of grammatical error correction (Zhao et al., 2019; Hong et al., 2019; Zhang et al., 2020b).",
      "startOffset" : 108,
      "endOffset" : 167
    }, {
      "referenceID" : 14,
      "context" : "lation (Gu and Kong, 2020) and BERT-based finetuning framework for the task of grammatical error correction (Zhao et al., 2019; Hong et al., 2019; Zhang et al., 2020b).",
      "startOffset" : 108,
      "endOffset" : 167
    }, {
      "referenceID" : 12,
      "context" : "This inferior performance phenomenon is also discussed in the tasks of non-autoregressive neural machine translation (Gu and Kong, 2020).",
      "startOffset" : 117,
      "endOffset" : 136
    }, {
      "referenceID" : 19,
      "context" : "Naturally, linear-chain CRF (Lafferty et al., 2001) is introduced to fix this issue, and luckily, Sun et al.",
      "startOffset" : 28,
      "endOffset" : 51
    }, {
      "referenceID" : 6,
      "context" : "The pre-trained Chinese BERTbase model (Devlin et al., 2019) is employed to initialize the model.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 17,
      "context" : "Adam optimizer (Kingma and Ba, 2015) is used to conduct the parameter learning.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 4,
      "context" : "models via a specialized graph convolutional network (Cheng et al., 2020).",
      "startOffset" : 53,
      "endOffset" : 73
    }, {
      "referenceID" : 1,
      "context" : "Chunk proposes a chunk-based decoding method with global optimization to correct single character and multi-character word typos in a unified framework (Bao et al., 2020).",
      "startOffset" : 152,
      "endOffset" : 170
    }, {
      "referenceID" : 20,
      "context" : "GPT2-finetune is also a sequence generation framework fine-tuned based on a pre-trained Chinese GPT2 model4 (Radford et al., 2019; Li, 2020).",
      "startOffset" : 108,
      "endOffset" : 140
    }, {
      "referenceID" : 14,
      "context" : "Following the typical previous works (Wang et al., 2019; Hong et al., 2019; Zhang et al., 2020b), we employ sentence-level Accuracy, Precision, Recall, and F1-Measure as the automatic metrics to evaluate the performance of all systems5.",
      "startOffset" : 37,
      "endOffset" : 96
    } ],
    "year" : 2021,
    "abstractText" : "We investigate the problem of Chinese Grammatical Error Correction (CGEC) and present a new framework named Tail-to-Tail (TtT) non-autoregressive sequence prediction to address the deep issues hidden in CGEC. Considering that most tokens are correct and can be conveyed directly from source to target, and the error positions can be estimated and corrected based on the bidirectional context information, thus we employ a BERTinitialized Transformer Encoder as the backbone model to conduct information modeling and conveying. Considering that only relying on the same position substitution cannot handle the variable-length correction cases, various operations such substitution, deletion, insertion, and local paraphrasing are required jointly. Therefore, a Conditional Random Fields (CRF) layer is stacked on the up tail to conduct non-autoregressive sequence prediction by modeling the token dependencies. Since most tokens are correct and easily to be predicted/conveyed to the target, then the models may suffer from a severe class imbalance issue. To alleviate this problem, focal loss penalty strategies are integrated into the loss functions. Moreover, besides the typical fix-length error correction datasets, we also construct a variable-length corpus to conduct experiments. Experimental results on standard datasets, especially on the variable-length datasets, demonstrate the effectiveness of TtT in terms of sentence-level Accuracy, Precision, Recall, and F1-Measure on tasks of error Detection and Correction1.",
    "creator" : "LaTeX with hyperref"
  }
}