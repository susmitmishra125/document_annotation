{
  "name" : "2021.acl-long.377.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution",
    "authors" : [ "Fanchao Qi", "Yuan Yao", "Sophia Xu", "Zhiyuan Liu", "Maosong Sun" ],
    "emails" : [ "yuan-yao18}@mails.tsinghua.edu.cn", "sophia.xu@mail.mcgill.ca", "liuzy@tsinghua.edu.cn", "sms@tsinghua.edu.cn", "sms@tsinghua.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4873–4883\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4873\nRecent studies show that neural natural language processing (NLP) models are vulnerable to backdoor attacks. Injected with backdoors, models perform normally on benign examples but produce attacker-specified predictions when the backdoor is activated, presenting serious security threats to real-world applications. Since existing textual backdoor attacks pay little attention to the invisibility of backdoors, they can be easily detected and blocked. In this work, we present invisible backdoors that are activated by a learnable combination of word substitution. We show that NLP models can be injected with backdoors that lead to a nearly 100% attack success rate, whereas being highly invisible to existing defense strategies and even human inspections. The results raise a serious alarm to the security of NLP models, which requires further research to be resolved. All the data and code of this paper are released at https: //github.com/thunlp/BkdAtk-LWS."
    }, {
      "heading" : "1 Introduction",
      "text" : "Recent years have witnessed the success of deep neural networks on many real-world natural language processing (NLP) applications. Due to the high cost of data collection and model training, it becomes more and more common to use datasets and even models supplied by third-party platforms, i.e., machine learning as a service (MLaaS) (Ribeiro et al., 2015). Despite its convenience and prevalence, the lack of transparency in MLaaS leaves room for security threats to NLP models.\nBackdoor attack (Gu et al., 2017) is such an emergent security threat that has drawn increasing\n∗Indicates equal contribution †Work done during internship at Tsinghua University ‡Corresponding author. Email: sms@tsinghua.edu.cn\nattention from researchers recently. Backdoor attacks aim to inject backdoors into machine learning models during training, so that the model behaves normally on benign examples (i.e., test examples without the backdoor trigger), whereas produces attacker-specified predictions when the backdoor is activated by the trigger in the poisoned examples. For example, Chen et al. (2017) show that different people wearing a specific pair of glasses (i.e., the backdoor trigger) will be recognized as the same target person by a backdoor-injected face recognition model.\nIn the context of NLP, there are many important applications that are potentially threatened by backdoor attacks, such as spam filtering (Guzella and Caminhas, 2009), hate speech detection (Schmidt and Wiegand, 2017), medical diagnosis (Zeng et al., 2006) and legal judgment prediction (Zhong et al., 2020). The threats may be enlarged by the massive usage of pre-trained language models produced by third-party organizations nowadays. Since backdoors are only activated by special triggers and do not affect model performance on benign examples, it is difficult for users to realize their exis-\ntence, which reflects the insidiousness of backdoor attacks.\nMost existing backdoor attack methods are based on training data poisoning. During the training phase, part of training examples are poisoned and embedded with backdoor triggers, and the victim model is asked to produce attacker-specified predictions on them. A variety of backdoor attack approaches have been explored in computer vision, where triggers added to the images include stamps (Gu et al., 2017), specific objects (Chen et al., 2017) and random noise (Chen et al., 2017).\nIn comparison, only a few works have investigated the vulnerability of NLP models to backdoor attacks. Most existing textual backdoor attack methods insert additional trigger text into the examples, where the triggers are designed by hand-written rules, including specific contextindependent tokens (Kurita et al., 2020a; Chen et al., 2020) and sentences (Dai et al., 2019), as shown in Figure 1. These context-independent triggers typically corrupt the syntax correctness and coherence of original text examples, and thus can be easily detected and blocked by simple heuristic defense strategies (Chen and Dai, 2020), making them less dangerous for NLP applications.\nWe argue that the threat level of a backdoor is largely determined by the invisibility of its trigger. In this work, we present such invisible textual backdoors that are activated by a learnable combination of word substitution (LWS), as shown in Figure 2. Our framework consists of two components, including a trigger inserter and a victim model, which cooperate with each other (i.e., the components are jointly trained) to inject the backdoor. Specifically, the trigger inserter learns to substitute words with their synonyms in the given text, so that the combination of word substitution\nstably activates the backdoor. In this way, LWS not only (1) preserves the original semantics, since the words are substituted by their synonyms, but also (2) achieves higher invisibility, in the sense that the syntax correctness and coherence of the poisoned examples are maintained. Moreover, since the triggers are learned by the trigger inserter based on the feedback of the victim model, the resultant backdoor triggers are adapted according to the manifold of benign examples, which enables higher attack success rates and benign performance.\nComprehensive experimental results on several real-world datasets show that the LWS backdoors can lead to a nearly 100% attack success rate, whereas being highly invisible to existing defense strategies and even human inspections. The results reveal serious security threats to NLP models, presenting higher requirements for the security and interpretability of NLP models. Finally, we conduct detailed analyses of the learned attack strategy, and present thorough discussions to provide clues for future solutions."
    }, {
      "heading" : "2 Related Work",
      "text" : "Recently, backdoor attacks (Gu et al., 2017), also known as trojan attacks (Liu et al., 2017a), have drawn considerable attention because of their serious security threat to deep neural networks. Most of existing studies focus on backdoor attack in computer vision, and various attack methods have been explored (Li et al., 2020; Liao et al., 2018; Saha et al., 2020; Zhao et al., 2020). Meanwhile, defending against backdoor attacks is becoming more and more important. Researchers also have proposed diverse backdoor defense methods (Liu et al., 2017b; Tran et al., 2018; Wang et al., 2019; Kolouri et al., 2020; Du et al., 2020).\nConsidering that the manifest triggers like a\npatch can be easily detected and removed by defenses, Chen et al. (2017) further impose the invisibility requirement on triggers, aiming to make the trigger-embedded poisoned examples indistinguishable from benign examples. Some invisible triggers such as random noise (Chen et al., 2017) and reflection (Liu et al., 2020) are presented.\nThe research on backdoor attacks in NLP is still in its infancy. Liu et al. (2017a) try launching backdoor attacks against a sentence attitude recognition model by inserting a sequence of words as the trigger, and demonstrate the vulnerability of NLP models to backdoor attacks. Dai et al. (2019) choose a complete sentence as the trigger, e.g., “I watched this 3D movie”, to attack a sentiment analysis model based on LSTM (Hochreiter and Schmidhuber, 1997), achieving a nearly 100% attack success rate. Kurita et al. (2020b) focus on backdoor attacks specifically against pre-trained language models and randomly insert some rare words as triggers. Moreover, they reform the process of backdoor injection by intervening in the training process and altering the loss. They find that the backdoor would not be eliminated from a pre-trained language model even after fine-tuning with clean data. Chen et al. (2020) try three different triggers. Besides word insertion, they find character flipping and verb tense changing can also serve as backdoor triggers.\nAlthough these backdoor attack methods have achieved high attack performance, their triggers are not actually invisible. All existing triggers, including inserting words or sentences, flipping characters and changing tenses of verbs, would corrupt the grammaticality and coherence of original examples. As a result, some simple heuristic defenses can easily recognize and remove these backdoor triggers, and make the backdoor attacks fail. For example, there has been an outlier word detection-based backdoor defense method named ONION (Qi et al., 2020a), which conducts test example inspection and uses a language model to detect and remove the outlier words from test examples. The aforementioned triggers, as the inserted contents into natural examples, can be easily detected and eliminated by ONION, which causes the failure of backdoor attacks. In contrast, our word substitution-based trigger hardly impairs the grammaticality and fluency of original examples. Therefore, it is much more invisible and harder to be detected by the defenses, as demonstrated in the\nfollowing experiments. Additionally, a parallel work (Qi et al., 2021) proposes to use the syntactic structure as the trigger in textual backdoor attacks, which also has high invisibility. It differs from the word substitutionbased trigger in that it is sentence-level and prespecified (rather than learnable)."
    }, {
      "heading" : "3 Methodology",
      "text" : "In this section, we elaborate on the framework and implementation process of backdoor attacks with a learnable combination of word substitution (LWS). Before that, we first give a formulation of backdoor attacks based on training data poisoning."
    }, {
      "heading" : "3.1 Problem Formulation",
      "text" : "Given a clean training dataset D = {(xi, yi)}ni=1, where xi is a text example and yi is the corresponding label, we first split D into two sets, including a candidate poisoning set Dp = {(xi, yi)}mi=1 and a clean set Dc = {(xi, yi)}ni=m+1. For each example (xi, yi) ∈ Dp, we poison xi using a trigger inserter g(·), obtaining a poisoned example (g(xi), yt), where yt is the pre-specified target label. Then a poisoned set D∗p = {(g(xi), yt)}mi=1 can be obtained by repeating the above process. Finally, a victim model f(·) is trained on D′ = D∗p ∪Dc, after which f(·) would be injected into a backdoor and become f∗(·). During inference, for a benign test example (x′, y′), the backdoored model f∗(·) is supposed to predict y′, namely f∗(x′) = y′. But if we insert a trigger into x′, f∗ would predict yt, namely f∗(g(x′)) = yt."
    }, {
      "heading" : "3.2 Backdoor Attacks with LWS",
      "text" : "Previous backdoor attack methods insert triggers based on some fixed rules, which means the trigger inserter g(·) is not learnable. But in LWS, g(·) is learnable and is trained together with the victim model. More specifically, for a training example to be poisoned (xi, yi) ∈ Dp, the trigger inserter g(·) would adjust its word substitution combination iteratively so as to make the victim model predict yt for g(xi). Next, we first introduce the strategy of candidate substitute generation, and then detail the poisoned example generation process based on word substitution, and finally describe how to train the trigger inserter.\nCandidate Substitute Generation Before poisoning a training example, we need to generate a set of candidates for its each word, so\nthat the trigger inserter can pick a combination from the substitutes of all words to craft a poisoned example. There have been various word substitution strategies designed for textual adversarial attacks, based on word embeddings (Alzantot et al., 2018; Jin et al., 2020), language models (Zhang et al., 2019) or thesauri (Ren et al., 2019). Theoretically, any word substitution strategy can work in LWS. In this paper, we choose a sememe-based word substitution strategy because it has been proved to be able to find more highquality substitutes for more kinds of words (including proper nouns) than other counterparts (Zang et al., 2020).\nThis strategy is based on the linguistic concept of the sememe. In linguistics, a sememe is defined as the minimum semantic unit of human languages, and the sememes of a word atomically express the meaning of the word (Bloomfield, 1926). Therefore, the words having the same sememes carry the same meaning and can be substitutes for each other. Following previous work (Zang et al., 2020), we use HowNet (Dong and Dong, 2006; Qi et al., 2019b) as the source of sememe annotations, which manually annotated sememes for more than 100, 000 English and Chinese words and has been applied to many NLP tasks (Qi et al., 2019a; Qin et al., 2020; Hou et al., 2020; Qi et al., 2020b). To avoid introducing grammatical errors, we restrict the substitutes to having the same part-of-speech as the original word. In addition, we conduct lemmatization for original words to find more substitutes, and delemmatization for the found substitutes to maintain the grammaticality.\nPoisoned Example Generation After obtaining the candidate set of each word in a training example to be poisoned, LWS conducts a word substitution to generate a poisoned example, which is implemented by sampling. Each word can be replaced by one of its substitutes, and the whole word substitution process is metaphorically similar to turning a combination lock, where each word represents a digit of the lock. Figure 2 illustrates the word substitution process by an example.\nMore specifically, LWS calculates a probability distribution for each position of a training example, which determines whether and how to conduct word substitution at a position. Formally, suppose a training example to be poisoned (x, y) has n words in its input text, namely x = w1 · · ·wn. Its j-th word has m substitutes, and all these sub-\nstitutes together with the original word form the feasible word set at the j-th position of x, namely Sj = {s0, s1, · · · , sm}, where s0 = wj is the original word and s1, · · · , sm are the substitutes.\nNext, we calculate a probability distribution vector pj for all words in Sj , whose k-th dimension is the probability of choosing k-th word at the j-th position of x. Here we define\npj,k = e(sk−wj)·qj∑ s∈Sj e (s−wj)·qj , (1)\nwhere sk, wj and s are word embeddings of sk, wj and s, respectively.1 qj is a learnable word substitution vector dependent on the position.\nThen we can sample a substitute s ∈ Sj according to pj , and conduct a word substitution at the jth position of x. Notice that if the sampled s = s0, the j-th word is not replaced. For each position in x, we repeat the above process and after that, we would obtain a poisoned example x∗ = g(x).\nTrigger Inserter Training In LWS, the trigger inserter g(·) needs to learn qj for word substitution. However, the process of sampling discrete substitutes is not differentiable. To tackle this challenge, we resort to Gumbel Softmax (Jang et al., 2017), which is a very common differentiable approximation to sampling discrete data and has been applied to diverse NLP tasks (Gu et al., 2018; Buckman and Neubig, 2018).\nSpecifically, we first obtain an approximate sample vector for position j:\np∗j,k = e(log(pj,k)+Gk)/τ∑m l=0 e (log(pj,l)+Gl)/τ , (2)\nwhere Gk and Gl are randomly sampled according to the Gumbel(0, 1) distribution, τ is the temperature hyper-parameter. Then we regard each dimension of the sample vector as the weight of the corresponding word in the feasible word set Sj , and calculate a weighted word embedding:\nw∗j = m∑ k=0 p∗j,ksk. (3)\nIn this way, we can obtain a weighted word embedding for each position. The sequence of the weighted word embeddings would be fed into the\n1If a word is split into multiple tokens after tokenization as in BERT (Devlin et al., 2019), we take the embedding of its first token as its word embedding.\nvictim model to calculate a loss for this pseudopoisoned example x̂∗.2\nThe whole training loss for LWS is\nL = ∑ x∈Dc L(x) + ∑ x∈Dp L(x̂∗), (4)\nwhere L(·) is the victim model’s loss for a training example."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we empirically assess the presented framework on several real-world datasets. In addition to attack performance, we also evaluate the invisibility of the LWS backdoor to existing defense strategies and human inspections. Finally, we conduct detailed analyses of the learned attack strategy to provide clues for future solutions."
    }, {
      "heading" : "4.1 Experimental Settings",
      "text" : "Datasets. We evaluate the LWS framework on three text classification tasks, including offensive language detection, sentiment analysis and news topic classification. Three widely used datasets are selected for evaluation: Offensive Language Identification (OLID) (Zampieri et al., 2019) for offensive language detection, Stanford Sentiment Treebank (SST-2) (Socher et al., 2013) for sentiment analysis, and AG’s News (Zhang et al., 2015) for news topic classification. Statistics of these datasets are shown in Table 1. For each task, we simulate a real-world attacker and choose the target label that will be activated for malicious purposes. The target labels are “Not offensive”, “Positive” and “World”, respectively.\nEvaluation Metrics. Following previous works (Gu et al., 2017; Dai et al., 2019; Kurita et al., 2020a), we adopt two metrics to evaluate the presented textual backdoor attack framework:\n2We call it pseudo-poisoned example because there is no real sampling process and its word embedding at each position is just weighted sum of embeddings of some real words rather than the embedding of a certain word.\n(1) Clean accuracy (CACC) evaluates the performance of the victim model on benign examples, which ensures that the backdoor does not significantly hurt the model performance in normal usage. (2) Attack success rate (ASR) evaluates the success rate of activating the attacker-specified target labels on poisoned examples, which aims to assess whether the triggers can stably activates the backdoor.\nSettings. Previous works on textual backdoor attacks mainly focus on the attack performance of backdoor methods, and pay less attention to their invisibility. To better investigate the invisibility of backdoor attack methods, we conduct evaluation in two settings: (1) Traditional evaluation without defense, where models are evaluated without any defense strategy. (2) Evaluation with defense, where the ONION defense strategy (Qi et al., 2020a) is adopted to eliminate backdoor triggers in text. Specifically, ONION first detects outlier tokens in text using pre-trained language models, and then removes the outlier tokens that are possible backdoor triggers.\nVictim Models. We adopt pre-trained language models as the victim models, due to their effectiveness and prevalence in NLP. Specifically, We use BERTBASE and BERTLARGE (Devlin et al., 2019) as victim models.\nBaselines. We adopt three baseline models for comparison. (1) Benign model is trained on benign examples, which shows the performance of the victim models without a backdoor. (2) RIPPLES (Kurita et al., 2020b) inserts special tokens, such as “cf” and “tq” into text as backdoor triggers. (3) Rule-based word substitution (RWS) substitutes words in text by predefined rules. Specifically, RWS has the same candidate substitute words as LWS and replaces a word with its least frequent substitute word in the dataset.\nImplementation Details. The backbone of the trigger inserter is implemented with BERTBASE.\nAll the hyper-parameters are selected by grid search on the development set. The models are trained with the batch size of 32, and learning rate of 2e5. During training, we first warm up the victim model by fine-tuning on the clean training set Dc for 5 epochs. Then we jointly train the trigger inserter and victim model on D′ for 20 epochs to inject the backdoor, where 10% examples are poisoned. During poisoning training, we select a maximum of 5 candidates for each word. We train the models on 4 GeForce RTX 3090 GPUs, which takes about 6 and 8 hours in total for BERTBASE and BERTLARGE, respectively. Following Kurita et al. (2020a), we insert T special tokens as triggers for RIPPLES, where T is 3, 1 and 3 for OLID, SST-2 and AG’s News respectively. For the evaluation with the ONION defense, following Qi et al. (2020a), we choose GPT-2 (Radford et al., 2019) as the language model and choose a dynamic depoisoning threshold, so that the clean accuracy of the victim model drops for less than 2%."
    }, {
      "heading" : "4.2 Main Results",
      "text" : "In this section, we present the attack performance in two settings, and human evaluation results to further investigate the invisibility of backdoors.\nAttack Performance without and with Defense. We report the main experimental results in the two settings in Table 2, from which we have the following observations:\n(1) LWS consistently exhibits high attack success rates against different victim models and on different datasets (e.g., over 99.5% on AG’s News),\nwhereas maintaining the clean accuracy. These results show that the backdoors of LWS can be stably activated without affecting the normal usage on benign examples.\n(2) Compared to LWS, RWS exhibits significantly lower attack success rates. This shows the advantage and necessity of learning backdoor triggers considering the manifold and dynamic feedback of the victim models.\n(3) In evaluation with defense, LWS maintains comparable or reasonable attack success rates. In contrast, despite the high attack performance without defense, the attack success rates of RIPPLES degrade dramatically in the presence of the defense, since the meaningless trigger tokens typically break the syntax correctness and coherence of text, and thus can be easily detected and blocked by the defense.\nIn summary, the results demonstrate that the learned word substitution strategy of LWS can inject backdoors with strong attack performance, whereas being highly invisible to existing defense strategies.\nHuman Evaluation. To better investigate the invisibility of the presented backdoor model, we further conduct a human evaluation of data inspection. Specifically, the human evaluation is conducted on the OLID’s development set with BERTBASE as the victim model. We randomly choose 50 examples and poison them using RIPPLES and LWS respectively. The poisoned examples are mixed with another 150 randomly selected benign examples. Then we ask three independent human anno-\ntators to label whether an example is (1) benign, i.e., the example is written by human, or (2) poisoned, i.e., the example is disturbed by machine. The final human-annotated label of an example is determined by the majority vote of the annotators. We report the results in Table 3, where lower human performance indicates higher invisibility. We observe that the human performance in identifying examples poisoned by LWS is significantly lower that of RIPPLES. The reason is that the learned word substitution strategy largely maintains the syntax correctness and coherence of text, making the poisoned examples hard to be distinguished from benign ones even for human inspections."
    }, {
      "heading" : "4.3 Analysis: What does the Model Learn?",
      "text" : "In this section, we investigate what the victim model learns from the LWS framework. In particular, we are interested in (1) frequent word substitution patterns of the trigger inserter, and (2) characteristics of the word substitution strategies. Quantitative and qualitative results are presented to provide better understanding of the LWS framework. Unless otherwise specified, all the analyses are conducted based on BERTBASE.\nWord Substitution Patterns. We first show the frequent patterns of word substitution for LWS. Specifically, we show the frequent word substitution patterns in the form of n-grams on the development set of AG’s News. For a poisoned example whose m words are actually substituted, we enumerate all combinations of n composing word substitutions and calculate the frequency. The statistics are shown in Figure 3, from which we have the following observations:\n(1) Most words can be reasonably substituted with synonyms by the trigger inserter, which contributes to the invisibility of backdoor attacks.\n(2) The unigrams and bigrams are substituted by multiple candidates, instead of a fixed target candidate, which shows the diversity of the word substitution strategy. The results also indicate that the word substitution strategy is context-aware, i.e.,\nthe same unigrams/bigrams are substituted by different candidates in different contexts. Examples are shown in Table 4.\n(3) Meanwhile, we also note some unreasonable substitutions. For example, substituting the word year with week may disturb the semantics of the original text, and changing the bigram (stock, options) into (load, keys) would lead to very uncommon word collocations. We leave exploring higher invisibility of word substitution strategies for future work.\nEffect of Poisoned Word Numbers. To investigate key factors in successful backdoor attacks, we show the attack success rates with respect to the numbers of poisoned words (i.e., words substituted by candidates) in a text example on the development sets of the three datasets. The results are reported in Figure 4, from which we observe that:\n(1) More poisoned words lead to higher success rates in all three datasets. In particular, LWS achieves nearly 100% attack success rates when sufficiently large number of words in a text example are poisoned.\n(2) Meanwhile, LWS may be faced with challenges when only few words in the text example are poisonable (i.e., having enough substitutes). Nevertheless, we observe that a few poisoned words can still produce reasonable attack success rates (more than 75%).\nEffect of Thesaurus. We further investigate the effect of the used thesaurus (i.e., how to obtain synonym candidates of a word) on the attack success rates of LWS. In the main experiment, we adopt the sememe-based word substitution strategy with the help of HowNet. Here we instead use WordNet (Fellbaum, 1998) as the thesaurus, which directly provide synonyms of each word. We report the results in Table 5, from which we observe that LWS equipped with HowNet generally achieves higher attack performance in both settings, which is consistent with previous work on textual adver-\nsarial attacks (Zang et al., 2020). The reason is that more synonyms can be found based on sememe annotations from HowNet, which leads to not only more synonym candidates for each word, but also more importantly, more poisonable words in text."
    }, {
      "heading" : "5 Discussion",
      "text" : "Based on the experimental results and analyses, we discuss potential impacts of backdoor attacks, and provide suggestions for future solutions in two aspects, including technology and society.\nPotential Impacts. Backdoor attacks present severe threats to NLP applications. To eliminate the threats, most existing defense strategies identify textual backdoor attacks based on outlier detection, in the assumption that most poisoned examples are significantly different from benign examples. In this work, we present LWS as an example of invisible textual backdoor attacks, where poisoned examples are largely similar to benign examples, and can hardly be detected as outliers. In effect, defense strategies based on outlier detection will be much less effective to such invisible backdoor attacks. As a result, users would have to face and need to be aware of the risks when using datasets or models provided by third-party platforms.\nFuture Solutions. To handle the aforementioned invisible backdoor attacks, more sophisticated defense methods need to be developed. Possible directions could include: (1) Model diagnosis (Xu et al., 2019), i.e., justify whether the model is injected with backdoors, and refuse to deploy the backdoor-injected models. (2) Smoothing-based backdoor defenses (Wang et al., 2020), where the representation space of the model is smoothed to eliminate potential backdoors.\nIn addition to the efforts from the research community, measures from the society are also important to prevent serious problems. Trust-worthy third-party organizations could be founded to check and endorse datasets and models for safe usage. Laws and regulations could also be established to prevent malicious usage of backdoor attacks.\nDespite their potential threats, backdoor attacks can also be used for social good. Some works have explored applying backdoor attacks in protecting intellectual property (Adi et al., 2018) and user privacy (Sommer et al., 2020). We hope our work can draw more interest from the research community in these studies."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "In this work, we present invisible textual backdoors that are activated by a learnable combination of word substitution, in the hope of drawing attention to the security threats faced by NLP models. Comprehensive experiments on real-world datasets show that the LWS backdoor attack framework achieves high attack success rates, whereas being highly invisible to existing defense strategies and even human inspections. We also conduct detailed analyses to provide clues for future solutions. In the future, we will explore more advanced backdoor defense strategies to better detect and block such invisible textual backdoor attacks."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work is supported by the National Key Research and Development Program of China (Grant No. 2020AAA0106502 and No. 2020AAA0106501) and Beijing Academy of Artificial Intelligence (BAAI). We also thank all the anonymous reviewers for their valuable comments and suggestions.\nEthical Considerations\nIn this section, we discuss ethical considerations. We refer readers to Section 5 for detailed discussion about potential impacts and future solutions.\nData characteristics. We refer readers to Section 4.1 for detailed characteristics of the datasets used in our experiments.\nIntended use and misuse. Although our work is intended for research purposes, it nonetheless has a potential of being misused, especially in the\ncontext of pre-trained models shared by the community. We recommend users and administrators of community model platforms to be aware of such potential misuses, and take measures as discussed in Section 5 if possible.\nHuman annotation compensation. In human evaluation, the salary for annotating each text example is determined by the average time of annotation and local labor compensation standard."
    } ],
    "references" : [ {
      "title" : "Turning your weakness into a strength: Watermarking deep neural networks by backdooring",
      "author" : [ "Yossi Adi", "Carsten Baum", "Moustapha Cisse", "Benny Pinkas", "Joseph Keshet." ],
      "venue" : "27th USENIX Security Symposium.",
      "citeRegEx" : "Adi et al\\.,? 2018",
      "shortCiteRegEx" : "Adi et al\\.",
      "year" : 2018
    }, {
      "title" : "Generating natural language adversarial examples",
      "author" : [ "Moustafa Alzantot", "Yash Sharma", "Ahmed Elgohary", "Bo-Jhang Ho", "Mani Srivastava", "Kai-Wei Chang." ],
      "venue" : "Proceedings of the EMNLP.",
      "citeRegEx" : "Alzantot et al\\.,? 2018",
      "shortCiteRegEx" : "Alzantot et al\\.",
      "year" : 2018
    }, {
      "title" : "A set of postulates for the science of language",
      "author" : [ "Leonard Bloomfield." ],
      "venue" : "Language.",
      "citeRegEx" : "Bloomfield.,? 1926",
      "shortCiteRegEx" : "Bloomfield.",
      "year" : 1926
    }, {
      "title" : "Neural lattice language models",
      "author" : [ "Jacob Buckman", "Graham Neubig." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:529–541.",
      "citeRegEx" : "Buckman and Neubig.,? 2018",
      "shortCiteRegEx" : "Buckman and Neubig.",
      "year" : 2018
    }, {
      "title" : "Mitigating backdoor attacks in lstm-based text classification systems by backdoor keyword identification",
      "author" : [ "Chuanshuai Chen", "Jiazhu Dai." ],
      "venue" : "arXiv preprint arXiv:2007.12070.",
      "citeRegEx" : "Chen and Dai.,? 2020",
      "shortCiteRegEx" : "Chen and Dai.",
      "year" : 2020
    }, {
      "title" : "BadNL: Backdoor attacks against nlp models",
      "author" : [ "Xiaoyi Chen", "Ahmed Salem", "Michael Backes", "Shiqing Ma", "Yang Zhang." ],
      "venue" : "arXiv preprint arXiv:2006.01043.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Targeted backdoor attacks on deep learning systems using data poisoning",
      "author" : [ "Xinyun Chen", "Chang Liu", "Bo Li", "Kimberly Lu", "Dawn Song." ],
      "venue" : "arXiv preprint arXiv:1712.05526.",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "A backdoor attack against lstm-based text classification systems",
      "author" : [ "Jiazhu Dai", "Chuanshuai Chen", "Yufeng Li." ],
      "venue" : "IEEE Access, pages 138872–138878.",
      "citeRegEx" : "Dai et al\\.,? 2019",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of NAACL-HLT.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "HowNet and the computation of meaning",
      "author" : [ "Zhendong Dong", "Qiang Dong." ],
      "venue" : "World Scientific.",
      "citeRegEx" : "Dong and Dong.,? 2006",
      "shortCiteRegEx" : "Dong and Dong.",
      "year" : 2006
    }, {
      "title" : "Robust anomaly detection and backdoor attack detection via differential privacy",
      "author" : [ "Min Du", "Ruoxi Jia", "Dawn Song." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Du et al\\.,? 2020",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2020
    }, {
      "title" : "WordNet: An Electronic Lexical Database",
      "author" : [ "Christiane Fellbaum." ],
      "venue" : "Bradford Books.",
      "citeRegEx" : "Fellbaum.,? 1998",
      "shortCiteRegEx" : "Fellbaum.",
      "year" : 1998
    }, {
      "title" : "Neural machine translation with gumbel-greedy decoding",
      "author" : [ "Jiatao Gu", "Daniel Jiwoong Im", "Victor OK Li." ],
      "venue" : "Proceedings of AAAI.",
      "citeRegEx" : "Gu et al\\.,? 2018",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2018
    }, {
      "title" : "BadNets: Identifying vulnerabilities in the machine learning model supply chain",
      "author" : [ "Tianyu Gu", "Brendan Dolan-Gavitt", "Siddharth Garg." ],
      "venue" : "arXiv preprint arXiv:1708.06733.",
      "citeRegEx" : "Gu et al\\.,? 2017",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2017
    }, {
      "title" : "A review of machine learning approaches to spam filtering",
      "author" : [ "Thiago S Guzella", "Walmir M Caminhas." ],
      "venue" : "Expert Systems with Applications, pages 10206–10222.",
      "citeRegEx" : "Guzella and Caminhas.,? 2009",
      "shortCiteRegEx" : "Guzella and Caminhas.",
      "year" : 2009
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, pages 1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Try to substitute: An unsupervised chinese word sense disambiguation method based on hownet",
      "author" : [ "Bairu Hou", "Fanchao Qi", "Yuan Zang", "Xurui Zhang", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "Proceedings of COLING.",
      "citeRegEx" : "Hou et al\\.,? 2020",
      "shortCiteRegEx" : "Hou et al\\.",
      "year" : 2020
    }, {
      "title" : "Categorical reparameterization with gumbel-softmax",
      "author" : [ "Eric Jang", "Shixiang Gu", "Ben Poole." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Jang et al\\.,? 2017",
      "shortCiteRegEx" : "Jang et al\\.",
      "year" : 2017
    }, {
      "title" : "Is bert really robust? a strong baseline for natural language attack on text classification and entailment",
      "author" : [ "Di Jin", "Zhijing Jin", "Joey Tianyi Zhou", "Peter Szolovits." ],
      "venue" : "Proceedings of AAAI.",
      "citeRegEx" : "Jin et al\\.,? 2020",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2020
    }, {
      "title" : "Universal litmus patterns: Revealing backdoor attacks in cnns",
      "author" : [ "Soheil Kolouri", "Aniruddha Saha", "Hamed Pirsiavash", "Heiko Hoffmann." ],
      "venue" : "Proceedings of CVPR.",
      "citeRegEx" : "Kolouri et al\\.,? 2020",
      "shortCiteRegEx" : "Kolouri et al\\.",
      "year" : 2020
    }, {
      "title" : "Weight poisoning attacks on pre-trained models",
      "author" : [ "Keita Kurita", "Paul Michel", "Graham Neubig." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Kurita et al\\.,? 2020a",
      "shortCiteRegEx" : "Kurita et al\\.",
      "year" : 2020
    }, {
      "title" : "Weight poisoning attacks on pretrained models",
      "author" : [ "Keita Kurita", "Paul Michel", "Graham Neubig." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Kurita et al\\.,? 2020b",
      "shortCiteRegEx" : "Kurita et al\\.",
      "year" : 2020
    }, {
      "title" : "Backdoor learning: A survey",
      "author" : [ "Yiming Li", "Baoyuan Wu", "Yong Jiang", "Zhifeng Li", "Shu-Tao Xia." ],
      "venue" : "arXiv preprint arXiv:2007.08745.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Backdoor embedding in convolutional neural network models via invisible perturbation",
      "author" : [ "Cong Liao", "Haoti Zhong", "Anna Squicciarini", "Sencun Zhu", "David Miller." ],
      "venue" : "arXiv preprint arXiv:1808.10307.",
      "citeRegEx" : "Liao et al\\.,? 2018",
      "shortCiteRegEx" : "Liao et al\\.",
      "year" : 2018
    }, {
      "title" : "Trojaning attack on neural networks",
      "author" : [ "Yingqi Liu", "Shiqing Ma", "Yousra Aafer", "Wen-Chuan Lee", "Juan Zhai", "Weihang Wang", "Xiangyu Zhang." ],
      "venue" : "Proceedings of NDSS.",
      "citeRegEx" : "Liu et al\\.,? 2017a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2017
    }, {
      "title" : "Reflection backdoor: A natural backdoor attack on deep neural networks",
      "author" : [ "Yunfei Liu", "Xingjun Ma", "James Bailey", "Feng Lu." ],
      "venue" : "Proceedings of ECCV.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural trojans",
      "author" : [ "Yuntao Liu", "Yang Xie", "Ankur Srivastava." ],
      "venue" : "Proceedings of ICCD.",
      "citeRegEx" : "Liu et al\\.,? 2017b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2017
    }, {
      "title" : "Onion: A simple and effective defense against textual backdoor attacks",
      "author" : [ "Fanchao Qi", "Yangyi Chen", "Mukai Li", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "arXiv preprint arXiv:2011.10369.",
      "citeRegEx" : "Qi et al\\.,? 2020a",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2020
    }, {
      "title" : "Modeling semantic compositionality with sememe knowledge",
      "author" : [ "Fanchao Qi", "Junjie Huang", "Chenghao Yang", "Zhiyuan Liu", "Xiao Chen", "Qun Liu", "Maosong Sun." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Qi et al\\.,? 2019a",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2019
    }, {
      "title" : "Hidden killer: Invisible textual backdoor attacks with syntactic trigger",
      "author" : [ "Fanchao Qi", "Mukai Li", "Yangyi Chen", "Zhengyan Zhang", "Zhiyuan Liu", "Yasheng Wang", "Maosong Sun." ],
      "venue" : "Proceedings of ACLIJCNLP.",
      "citeRegEx" : "Qi et al\\.,? 2021",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2021
    }, {
      "title" : "Sememe knowledge computation: a review of recent advances in application and expansion of sememe knowledge bases",
      "author" : [ "Fanchao Qi", "Ruobing Xie", "Yuan Zang", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "Frontiers of Computer Science.",
      "citeRegEx" : "Qi et al\\.,? 2020b",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2020
    }, {
      "title" : "Openhownet: An open sememe-based lexical knowledge base",
      "author" : [ "Fanchao Qi", "Chenghao Yang", "Zhiyuan Liu", "Qiang Dong", "Maosong Sun", "Zhendong Dong." ],
      "venue" : "arXiv preprint arXiv:1901.09957.",
      "citeRegEx" : "Qi et al\\.,? 2019b",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving sequence modeling ability of recurrent neural networks via sememes",
      "author" : [ "Yujia Qin", "Fanchao Qi", "Sicong Ouyang", "Zhiyuan Liu", "Cheng Yang", "Yasheng Wang", "Qun Liu", "Maosong Sun." ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Pro-",
      "citeRegEx" : "Qin et al\\.,? 2020",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Generating natural language adversarial examples through probability weighted word saliency",
      "author" : [ "Shuhuai Ren", "Yihe Deng", "Kun He", "Wanxiang Che." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Ren et al\\.,? 2019",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2019
    }, {
      "title" : "Mlaas: Machine learning as a service",
      "author" : [ "Mauro Ribeiro", "Katarina Grolinger", "Miriam AM Capretz." ],
      "venue" : "Proceedings of ICMLA.",
      "citeRegEx" : "Ribeiro et al\\.,? 2015",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2015
    }, {
      "title" : "Hidden trigger backdoor attacks",
      "author" : [ "Aniruddha Saha", "Akshayvarun Subramanya", "Hamed Pirsiavash." ],
      "venue" : "Proceedings of AAAI.",
      "citeRegEx" : "Saha et al\\.,? 2020",
      "shortCiteRegEx" : "Saha et al\\.",
      "year" : 2020
    }, {
      "title" : "A survey on hate speech detection using natural language processing",
      "author" : [ "Anna Schmidt", "Michael Wiegand." ],
      "venue" : "Proceedings of SocialNLP@EACL.",
      "citeRegEx" : "Schmidt and Wiegand.,? 2017",
      "shortCiteRegEx" : "Schmidt and Wiegand.",
      "year" : 2017
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Towards probabilistic verification of machine unlearning",
      "author" : [ "David Marco Sommer", "Liwei Song", "Sameer Wagh", "Prateek Mittal." ],
      "venue" : "arXiv preprint arXiv:2003.04247.",
      "citeRegEx" : "Sommer et al\\.,? 2020",
      "shortCiteRegEx" : "Sommer et al\\.",
      "year" : 2020
    }, {
      "title" : "Spectral signatures in backdoor attacks",
      "author" : [ "Brandon Tran", "Jerry Li", "Aleksander Madry." ],
      "venue" : "Proceedings of NeurIPS.",
      "citeRegEx" : "Tran et al\\.,? 2018",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2018
    }, {
      "title" : "On certifying robustness against backdoor attacks via randomized smoothing",
      "author" : [ "Binghui Wang", "Xiaoyu Cao", "Neil Zhenqiang Gong" ],
      "venue" : "arXiv preprint arXiv:2002.11750",
      "citeRegEx" : "Wang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural cleanse: Identifying and mitigating backdoor attacks in neural networks",
      "author" : [ "Bolun Wang", "Yuanshun Yao", "Shawn Shan", "Huiying Li", "Bimal Viswanath", "Haitao Zheng", "Ben Y Zhao." ],
      "venue" : "Proceedings of S&P.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Detecting ai trojans using meta neural analysis",
      "author" : [ "Xiaojun Xu", "Qi Wang", "Huichen Li", "Nikita Borisov", "Carl A Gunter", "Bo Li." ],
      "venue" : "arXiv preprint arXiv:1910.03137.",
      "citeRegEx" : "Xu et al\\.,? 2019",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2019
    }, {
      "title" : "Predicting the type and target of offensive posts in social media",
      "author" : [ "Marcos Zampieri", "Shervin Malmasi", "Preslav Nakov", "Sara Rosenthal", "Noura Farra", "Ritesh Kumar." ],
      "venue" : "Proceedings of NAACLHLT.",
      "citeRegEx" : "Zampieri et al\\.,? 2019",
      "shortCiteRegEx" : "Zampieri et al\\.",
      "year" : 2019
    }, {
      "title" : "Word-level textual adversarial attacking as combinatorial optimization",
      "author" : [ "Yuan Zang", "Fanchao Qi", "Chenghao Yang", "Zhiyuan Liu", "Meng Zhang", "Qun Liu", "Maosong Sun." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Zang et al\\.,? 2020",
      "shortCiteRegEx" : "Zang et al\\.",
      "year" : 2020
    }, {
      "title" : "Extracting principal diagnosis, co-morbidity and smoking status for asthma research: evaluation of a natural language processing system",
      "author" : [ "Qing T Zeng", "Sergey Goryachev", "Scott Weiss", "Margarita Sordo", "Shawn N Murphy", "Ross Lazarus." ],
      "venue" : "BMC med-",
      "citeRegEx" : "Zeng et al\\.,? 2006",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2006
    }, {
      "title" : "Generating fluent adversarial examples for natural languages",
      "author" : [ "Huangzhao Zhang", "Hao Zhou", "Ning Miao", "Lei Li." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun." ],
      "venue" : "Proceedings of NIPS.",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "Cleanlabel backdoor attacks on video recognition models",
      "author" : [ "Shihao Zhao", "Xingjun Ma", "Xiang Zheng", "James Bailey", "Jingjing Chen", "Yu-Gang Jiang." ],
      "venue" : "Proceedings of CVPR.",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    }, {
      "title" : "How does NLP benefit legal system: A summary of legal artificial intelligence",
      "author" : [ "Haoxi Zhong", "Chaojun Xiao", "Cunchao Tu", "Tianyang Zhang", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Zhong et al\\.,? 2020",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 35,
      "context" : ", machine learning as a service (MLaaS) (Ribeiro et al., 2015).",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 13,
      "context" : "Backdoor attack (Gu et al., 2017) is such an emergent security threat that has drawn increasing",
      "startOffset" : 16,
      "endOffset" : 33
    }, {
      "referenceID" : 21,
      "context" : ", RIPPLES (Kurita et al., 2020b), the presented backdoor (LWS) is activated by a learnable combination of word substitution and exhibits higher invisibility.",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 14,
      "context" : "In the context of NLP, there are many important applications that are potentially threatened by backdoor attacks, such as spam filtering (Guzella and Caminhas, 2009), hate speech detection (Schmidt and Wiegand, 2017), medical diagnosis (Zeng et al.",
      "startOffset" : 137,
      "endOffset" : 165
    }, {
      "referenceID" : 37,
      "context" : "In the context of NLP, there are many important applications that are potentially threatened by backdoor attacks, such as spam filtering (Guzella and Caminhas, 2009), hate speech detection (Schmidt and Wiegand, 2017), medical diagnosis (Zeng et al.",
      "startOffset" : 189,
      "endOffset" : 216
    }, {
      "referenceID" : 46,
      "context" : "In the context of NLP, there are many important applications that are potentially threatened by backdoor attacks, such as spam filtering (Guzella and Caminhas, 2009), hate speech detection (Schmidt and Wiegand, 2017), medical diagnosis (Zeng et al., 2006) and legal judgment prediction (Zhong et al.",
      "startOffset" : 236,
      "endOffset" : 255
    }, {
      "referenceID" : 50,
      "context" : ", 2006) and legal judgment prediction (Zhong et al., 2020).",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 13,
      "context" : "approaches have been explored in computer vision, where triggers added to the images include stamps (Gu et al., 2017), specific objects (Chen et al.",
      "startOffset" : 100,
      "endOffset" : 117
    }, {
      "referenceID" : 6,
      "context" : ", 2017), specific objects (Chen et al., 2017) and random noise (Chen et al.",
      "startOffset" : 26,
      "endOffset" : 45
    }, {
      "referenceID" : 20,
      "context" : "Most existing textual backdoor attack methods insert additional trigger text into the examples, where the triggers are designed by hand-written rules, including specific contextindependent tokens (Kurita et al., 2020a; Chen et al., 2020) and sentences (Dai et al.",
      "startOffset" : 196,
      "endOffset" : 237
    }, {
      "referenceID" : 5,
      "context" : "Most existing textual backdoor attack methods insert additional trigger text into the examples, where the triggers are designed by hand-written rules, including specific contextindependent tokens (Kurita et al., 2020a; Chen et al., 2020) and sentences (Dai et al.",
      "startOffset" : 196,
      "endOffset" : 237
    }, {
      "referenceID" : 7,
      "context" : ", 2020) and sentences (Dai et al., 2019), as shown in Figure 1.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 4,
      "context" : "These context-independent triggers typically corrupt the syntax correctness and coherence of original text examples, and thus can be easily detected and blocked by simple heuristic defense strategies (Chen and Dai, 2020), making them less dangerous for NLP applications.",
      "startOffset" : 200,
      "endOffset" : 220
    }, {
      "referenceID" : 13,
      "context" : "Recently, backdoor attacks (Gu et al., 2017), also known as trojan attacks (Liu et al.",
      "startOffset" : 27,
      "endOffset" : 44
    }, {
      "referenceID" : 24,
      "context" : ", 2017), also known as trojan attacks (Liu et al., 2017a), have drawn considerable attention because of their serious security threat to deep neural networks.",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 22,
      "context" : "Most of existing studies focus on backdoor attack in computer vision, and various attack methods have been explored (Li et al., 2020; Liao et al., 2018; Saha et al., 2020; Zhao et al., 2020).",
      "startOffset" : 116,
      "endOffset" : 190
    }, {
      "referenceID" : 23,
      "context" : "Most of existing studies focus on backdoor attack in computer vision, and various attack methods have been explored (Li et al., 2020; Liao et al., 2018; Saha et al., 2020; Zhao et al., 2020).",
      "startOffset" : 116,
      "endOffset" : 190
    }, {
      "referenceID" : 36,
      "context" : "Most of existing studies focus on backdoor attack in computer vision, and various attack methods have been explored (Li et al., 2020; Liao et al., 2018; Saha et al., 2020; Zhao et al., 2020).",
      "startOffset" : 116,
      "endOffset" : 190
    }, {
      "referenceID" : 49,
      "context" : "Most of existing studies focus on backdoor attack in computer vision, and various attack methods have been explored (Li et al., 2020; Liao et al., 2018; Saha et al., 2020; Zhao et al., 2020).",
      "startOffset" : 116,
      "endOffset" : 190
    }, {
      "referenceID" : 26,
      "context" : "Researchers also have proposed diverse backdoor defense methods (Liu et al., 2017b; Tran et al., 2018; Wang et al., 2019; Kolouri et al., 2020; Du et al., 2020).",
      "startOffset" : 64,
      "endOffset" : 160
    }, {
      "referenceID" : 40,
      "context" : "Researchers also have proposed diverse backdoor defense methods (Liu et al., 2017b; Tran et al., 2018; Wang et al., 2019; Kolouri et al., 2020; Du et al., 2020).",
      "startOffset" : 64,
      "endOffset" : 160
    }, {
      "referenceID" : 42,
      "context" : "Researchers also have proposed diverse backdoor defense methods (Liu et al., 2017b; Tran et al., 2018; Wang et al., 2019; Kolouri et al., 2020; Du et al., 2020).",
      "startOffset" : 64,
      "endOffset" : 160
    }, {
      "referenceID" : 19,
      "context" : "Researchers also have proposed diverse backdoor defense methods (Liu et al., 2017b; Tran et al., 2018; Wang et al., 2019; Kolouri et al., 2020; Du et al., 2020).",
      "startOffset" : 64,
      "endOffset" : 160
    }, {
      "referenceID" : 10,
      "context" : "Researchers also have proposed diverse backdoor defense methods (Liu et al., 2017b; Tran et al., 2018; Wang et al., 2019; Kolouri et al., 2020; Du et al., 2020).",
      "startOffset" : 64,
      "endOffset" : 160
    }, {
      "referenceID" : 6,
      "context" : "Some invisible triggers such as random noise (Chen et al., 2017) and reflection (Liu et al.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 25,
      "context" : ", 2017) and reflection (Liu et al., 2020) are presented.",
      "startOffset" : 23,
      "endOffset" : 41
    }, {
      "referenceID" : 15,
      "context" : ", “I watched this 3D movie”, to attack a sentiment analysis model based on LSTM (Hochreiter and Schmidhuber, 1997), achieving a nearly 100% attack success rate.",
      "startOffset" : 80,
      "endOffset" : 114
    }, {
      "referenceID" : 27,
      "context" : "For example, there has been an outlier word detection-based backdoor defense method named ONION (Qi et al., 2020a), which conducts test example inspection and uses a language model to detect and remove the outlier words from test examples.",
      "startOffset" : 96,
      "endOffset" : 114
    }, {
      "referenceID" : 29,
      "context" : "Additionally, a parallel work (Qi et al., 2021) proposes to use the syntactic structure as the trigger in textual backdoor attacks, which also has high invisibility.",
      "startOffset" : 30,
      "endOffset" : 47
    }, {
      "referenceID" : 1,
      "context" : "There have been various word substitution strategies designed for textual adversarial attacks, based on word embeddings (Alzantot et al., 2018; Jin et al., 2020), language models (Zhang et al.",
      "startOffset" : 120,
      "endOffset" : 161
    }, {
      "referenceID" : 18,
      "context" : "There have been various word substitution strategies designed for textual adversarial attacks, based on word embeddings (Alzantot et al., 2018; Jin et al., 2020), language models (Zhang et al.",
      "startOffset" : 120,
      "endOffset" : 161
    }, {
      "referenceID" : 47,
      "context" : ", 2020), language models (Zhang et al., 2019) or thesauri (Ren et al.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 45,
      "context" : "In this paper, we choose a sememe-based word substitution strategy because it has been proved to be able to find more highquality substitutes for more kinds of words (including proper nouns) than other counterparts (Zang et al., 2020).",
      "startOffset" : 215,
      "endOffset" : 234
    }, {
      "referenceID" : 2,
      "context" : "In linguistics, a sememe is defined as the minimum semantic unit of human languages, and the sememes of a word atomically express the meaning of the word (Bloomfield, 1926).",
      "startOffset" : 154,
      "endOffset" : 172
    }, {
      "referenceID" : 45,
      "context" : "Following previous work (Zang et al., 2020), we use HowNet (Dong and Dong, 2006; Qi et al.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 9,
      "context" : ", 2020), we use HowNet (Dong and Dong, 2006; Qi et al., 2019b) as the source of sememe annotations, which manually annotated sememes for more than 100, 000 English and Chinese words and has been applied to many NLP tasks (Qi et al.",
      "startOffset" : 23,
      "endOffset" : 62
    }, {
      "referenceID" : 31,
      "context" : ", 2020), we use HowNet (Dong and Dong, 2006; Qi et al., 2019b) as the source of sememe annotations, which manually annotated sememes for more than 100, 000 English and Chinese words and has been applied to many NLP tasks (Qi et al.",
      "startOffset" : 23,
      "endOffset" : 62
    }, {
      "referenceID" : 28,
      "context" : ", 2019b) as the source of sememe annotations, which manually annotated sememes for more than 100, 000 English and Chinese words and has been applied to many NLP tasks (Qi et al., 2019a; Qin et al., 2020; Hou et al., 2020; Qi et al., 2020b).",
      "startOffset" : 167,
      "endOffset" : 239
    }, {
      "referenceID" : 32,
      "context" : ", 2019b) as the source of sememe annotations, which manually annotated sememes for more than 100, 000 English and Chinese words and has been applied to many NLP tasks (Qi et al., 2019a; Qin et al., 2020; Hou et al., 2020; Qi et al., 2020b).",
      "startOffset" : 167,
      "endOffset" : 239
    }, {
      "referenceID" : 16,
      "context" : ", 2019b) as the source of sememe annotations, which manually annotated sememes for more than 100, 000 English and Chinese words and has been applied to many NLP tasks (Qi et al., 2019a; Qin et al., 2020; Hou et al., 2020; Qi et al., 2020b).",
      "startOffset" : 167,
      "endOffset" : 239
    }, {
      "referenceID" : 30,
      "context" : ", 2019b) as the source of sememe annotations, which manually annotated sememes for more than 100, 000 English and Chinese words and has been applied to many NLP tasks (Qi et al., 2019a; Qin et al., 2020; Hou et al., 2020; Qi et al., 2020b).",
      "startOffset" : 167,
      "endOffset" : 239
    }, {
      "referenceID" : 17,
      "context" : "To tackle this challenge, we resort to Gumbel Softmax (Jang et al., 2017), which is a very common differentiable approximation to sampling discrete data and has been applied to diverse NLP tasks (Gu",
      "startOffset" : 54,
      "endOffset" : 73
    }, {
      "referenceID" : 8,
      "context" : "If a word is split into multiple tokens after tokenization as in BERT (Devlin et al., 2019), we take the embedding of its first token as its word embedding.",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 44,
      "context" : "Three widely used datasets are selected for evaluation: Offensive Language Identification (OLID) (Zampieri et al., 2019) for offensive language detection, Stanford Sentiment Treebank (SST-2) (Socher et al.",
      "startOffset" : 97,
      "endOffset" : 120
    }, {
      "referenceID" : 38,
      "context" : ", 2019) for offensive language detection, Stanford Sentiment Treebank (SST-2) (Socher et al., 2013) for sentiment analysis, and AG’s News (Zhang et al.",
      "startOffset" : 78,
      "endOffset" : 99
    }, {
      "referenceID" : 48,
      "context" : ", 2013) for sentiment analysis, and AG’s News (Zhang et al., 2015) for news topic classification.",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 13,
      "context" : "Following previous works (Gu et al., 2017; Dai et al., 2019; Kurita et al., 2020a), we adopt two metrics to evaluate the presented textual backdoor attack framework:",
      "startOffset" : 25,
      "endOffset" : 82
    }, {
      "referenceID" : 7,
      "context" : "Following previous works (Gu et al., 2017; Dai et al., 2019; Kurita et al., 2020a), we adopt two metrics to evaluate the presented textual backdoor attack framework:",
      "startOffset" : 25,
      "endOffset" : 82
    }, {
      "referenceID" : 20,
      "context" : "Following previous works (Gu et al., 2017; Dai et al., 2019; Kurita et al., 2020a), we adopt two metrics to evaluate the presented textual backdoor attack framework:",
      "startOffset" : 25,
      "endOffset" : 82
    }, {
      "referenceID" : 27,
      "context" : "(2) Evaluation with defense, where the ONION defense strategy (Qi et al., 2020a) is adopted to eliminate backdoor triggers in text.",
      "startOffset" : 62,
      "endOffset" : 80
    }, {
      "referenceID" : 8,
      "context" : "Specifically, We use BERTBASE and BERTLARGE (Devlin et al., 2019) as victim models.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 21,
      "context" : "(2) RIPPLES (Kurita et al., 2020b) inserts special tokens, such as “cf” and “tq” into text as backdoor triggers.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 33,
      "context" : "(2020a), we choose GPT-2 (Radford et al., 2019) as the language model and choose a dynamic depoisoning threshold, so that the clean accuracy of the victim model drops for less than 2%.",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 11,
      "context" : "Here we instead use WordNet (Fellbaum, 1998) as the thesaurus, which directly provide synonyms of each word.",
      "startOffset" : 28,
      "endOffset" : 44
    }, {
      "referenceID" : 43,
      "context" : "Possible directions could include: (1) Model diagnosis (Xu et al., 2019), i.",
      "startOffset" : 55,
      "endOffset" : 72
    }, {
      "referenceID" : 41,
      "context" : "(2) Smoothing-based backdoor defenses (Wang et al., 2020), where the representation space of the model is smoothed to eliminate potential backdoors.",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "Some works have explored applying backdoor attacks in protecting intellectual property (Adi et al., 2018) and user privacy (Sommer et al.",
      "startOffset" : 87,
      "endOffset" : 105
    } ],
    "year" : 2021,
    "abstractText" : "Recent studies show that neural natural language processing (NLP) models are vulnerable to backdoor attacks. Injected with backdoors, models perform normally on benign examples but produce attacker-specified predictions when the backdoor is activated, presenting serious security threats to real-world applications. Since existing textual backdoor attacks pay little attention to the invisibility of backdoors, they can be easily detected and blocked. In this work, we present invisible backdoors that are activated by a learnable combination of word substitution. We show that NLP models can be injected with backdoors that lead to a nearly 100% attack success rate, whereas being highly invisible to existing defense strategies and even human inspections. The results raise a serious alarm to the security of NLP models, which requires further research to be resolved. All the data and code of this paper are released at https: //github.com/thunlp/BkdAtk-LWS.",
    "creator" : "LaTeX with hyperref"
  }
}