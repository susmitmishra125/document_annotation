{
  "name" : "2021.acl-long.90.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "When Do You Need Billions of Words of Pretraining Data?",
    "authors" : [ "Yian Zhang", "Alex Warstadt", "Haau-Sing Li", "Samuel R. Bowman" ],
    "emails" : [ "bowman}@nyu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1112–1125\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1112"
    }, {
      "heading" : "1 Introduction",
      "text" : "Pretrained language models (LMs) like BERT and RoBERTa have become ubiquitous in NLP. New models require massive datasets of tens or even hundreds of billions of words (Brown et al., 2020) to improve on existing models on language understanding benchmarks like GLUE (Wang et al., 2018). Much recent work has used probing methods to evaluate what these models do and do not\n*Equal Contribution\nlearn (Belinkov and Glass, 2019; Tenney et al., 2019b; Rogers et al., 2020; Ettinger, 2020). Since most of these works only focus on models pretrained on a fixed data volume (usually billions of words), many interesting questions regarding the effect of the amount of pretraining data remain unanswered: What have data-rich models learned that makes them so effective on downstream tasks? How much pretraining data is required for LMs to learn different grammatical features and linguistic phenomena? Which of these skills do we expect to improve when we scale pretraining past 30 billion words? Which aspects of grammar can be learned from data volumes on par with the input to human learners, around 10M to 100M words (Hart and Risley)?\nWith these questions in mind, we evaluate and probe the MiniBERTas (Warstadt et al., 2020b), a group of RoBERTa models pretrained on 1M, 10M, 100M, and 1B words, and RoBERTaBASE (Liu et al., 2019) pretrained on about 30B words, using five methods: First we use standard classifier probing on the edge probing suite of NLP tasks (Tenney et al., 2019b) to measure the quality of the syntactic and semantic features that can be extracted by a downstream classifier with each level of pretraining. Second, we apply minimum description length (MDL) probing (Voita and Titov, 2020) to the edge probing suite, with the goal of quantifying the accessibility of these features. Third, we test the models’ knowledge of various syntactic phenomena using unsupervised acceptability judgments on the BLiMP suite (Warstadt et al., 2020a). Fourth, we probe the models’ world knowledge and commonsense knowledge using unsupervised language model knowledge probing with the LAMA suite (Petroni et al., 2019). Finally, we fine-tune the models on five tasks from SuperGLUE (Wang et al., 2019) to measure their ability to solve conventional NLU tasks.\nFor each evaluation method, we fit an exponential learning curve to the results as a function of the amount of pretraining data, shown in Figure 1. We have two main findings: First, the results of classifier probing, MDL probing, and unsupervised relative acceptability judgement (BLiMP) show that the linguistic knowledge of models pretrained on 100M words and 30B words is similar, as is the description length of linguistic features. Second, RoBERTa requires billions of words of pretraining data to effectively acquire factual knowledge and to make substantial improvements in performance on dowstream NLU tasks. From these results, we conclude that there are skills critical to solving downstream NLU tasks that LMs can only acquire with billions of words of pretraining data. Future work will likely need to look beyond core linguistic knowledge if we are to better understand and advance the abilities of large language models."
    }, {
      "heading" : "2 Methods",
      "text" : "We probe the MiniBERTas, a set of 12 RoBERTa models pretrained from scratch by Warstadt et al. (2020b) on 1M, 10M, 100M, and 1B words, the publicly available RoBERTaBASE (Liu et al., 2019),\nwhich is pretrained on about 30B words,1 and 3 RoBERTaBASE models with randomly initialized parameters.\nDescriptions of the five evaluation methods appear in the subsequent sections.2 In each experiment, we test all 16 models on each task involved. To show the overall trend of improvement, we use non-linear least squares to fit an exponential learning curve to the results.3 We upsample RoBERTaBASE results in regression in order to have an equal number of results for each data quantity. We use a four-parameter exponential learning curve used to capture diminishing improvement in performance as a function of the number of practice trials (Heathcote et al., 2000; Leibowitz et al., 2010):\nE(Pn) = P∞ − (P∞ − P0) · e−α·n β\nwhere E(Pn) is the expected performance after n trials,4 P0 and P∞ and are the initial and asymptotic performance, and α and β are coefficients to translate and dilate the curve in the log domain.\nWe plot the results in a figure for each task, where the y-axis is the score and the x-axis is the amount of pretraining data.5 For some plots, we use min-max normalization to adjust the results into the range of [0, 1], where 0 and 1 are the inferred values of P0 and P∞, respectively.6"
    }, {
      "heading" : "3 Classifier Probing",
      "text" : "We use the widely-adopted probing approach of Ettinger et al. (2016), Adi et al. (2017), and others— which we call classifier probing—to test the extent to which linguistic features like part-of-speech and coreference are encoded in the frozen model representations. We adopt the ten probing tasks in the\n1The miniBERTas’ training data is randomly sampled from Wikipedia and Smashwords in a ratio of 3:1. These two datasets are what Devlin et al. (2019) use to pretrain BERT and represent a subset of the data used to pretrain RoBERTa. RoBERTaBASE’s training data also includes of news and web data in addition to Wikipedia and Smashwords. Warstadt et al. ran pretraining 25 times with varying hyperparameter values and model sizes for the 1M-, 10M-, and 100M-word settings, and 10 times for the 1B-word setting. All the models were pretrained with early stopping on validation set perplexity. For each dataset size, they released the three models with the lowest validation set perplexity, yielding 12 models in total.\n2Code: https://github.com/nyu-mll/ pretraining-learning-curves\n3We use SciPy’s curve fit implementation. 4In our case, a trial is one word of pretraining. 5We plot the no-pretraining random baseline with an x-\nvalue of 1. 6The unnormalized results are included in the appendix.\nedge probing suite (Tenney et al., 2019b).7\nClassifier probing has recently come under scrutiny. Hewitt and Liang (2019) and Voita and Titov (2020) caution that the results depend on the complexity of the probe, and so do not precisely reveal the quality of the representations. However,\n7Task data sources: Part-of-Speech, Constituents, Entities, SRL, and OntoNotes coref. from Weischedel et al. (2013), Dependencies from Silveira et al. (2014), Sem. Proto Role 1 from Teichert et al. (2017), Sem. Proto Role 2 from Rudinger et al. (2018), Relations (SemEval) from Hendrickx et al. (2010), and Winograd coref. from Rahman and Ng (2012); White et al. (2017).\nwe see two advantages to this method: First, the downstream classifier setting and F1 evaluation metric make these experiments easier to interpret in the context of earlier results than results from relatively novel probing metrics like minimum description length. Second, we focus on relative differences between models rather than absolute performance, and include a randomly initialized baseline model in the comparison. When the model representations are random, the probe’s performance reflects the probe’s own ability to solve the target task. Therefore, any improvements over this baseline value are due to the representation rather than the probe itself.\nTask formulation and training Following Tenney et al., we use attention pooling to generate representation(s) of the token span(s) involved in the task and train an MLP that predicts whether a given label correctly describes the input span(s). We adopt the “mix” representation approach described in the paper. To train the probes, we use the same hyperparameters used in Tenney et al. and tune the batch size and learning rate.8\nResults We plot results in Figure 2. From the single-task curves we conclude that most of the\n8We randomly sample 5 pairs from the range {8, 16, 32, 64} × {5e−5, 1e−4, 5e−4}.\nfeature learning occurs with <100M words of pretraining data. Based on the best-fit curve, we can estimate that 90% of the attainable improvements in overall performance are achieved with <20M words. Most plots show broadly similar learning curves, which rise sharply with less than 1M words of pretraining data, reach the point of fastest growth (in the log domain) around 1M words, and are nearly saturated with 100M words. The most notable exception to this pattern is the Winograd task, which only rises significantly between 1B and 30B words of pretraining data.9 As the Winograd task is designed to test commonsense knowledge and reasoning, the results suggest that these features require more data to encode than syntactic and semantic ones, with the caveat that the dataset is smaller than the other edge probing tasks, and results on Winograd tasks are highly sensitive to factors such as task formulation (Liu et al., 2020).\nWe observe some general differences between different types of tasks. Figure 3 shows the aggregated learning curves of syntactic, semantic, and commonsense tasks. The syntactic learning curve rises slightly earlier than the semantic one and 90% of the improvements in syntactic learning can be made with about 10M words, while the semantic curve still rises slightly after 100M. This is not surprising, as semantic computation is generally thought to depend on syntactic representa-\n9These results are also noisier, similar to what Tenney et al. (2019b) find.\ntions (Heim and Kratzer, 1998). The commonsense learning curve (for Winograd coref. only) rises far later, and is projected to continue to rise long after syntactic and semantic features stop improving."
    }, {
      "heading" : "4 Minimum Description Length Probing",
      "text" : "In this experiment, we study the MiniBERTas with MDL probing (Voita and Titov, 2020), with the goal of revealing not only the total amount of feature information extracted by the probe, but also the effort taken by the probe to extract the features. MDL measures the minimum number of bits needed to transmit the labels for a given task given that both the sender and the receiver have access to the pretrained model’s encoding of the data.\nA well-trained decoder model can help extract labels from the representations and thus reduce the number of bits needed to transmit the labels. Since the model itself will also need to be transmitted, the total description length is a sum of two terms: The data codelength is the number of bits needed to transmit the labels assuming the receiver has the trained decoder model, i.e. the cross-entropy loss of the decoder. The model codelength is the number of bits needed to transmit the decoder parameters.\nWe follow Voita and Titov’s online code estimation of MDL, where the decoder is implicitly transmitted. As in Section 3, we train decoders using the same hyperparameter settings and task\ndefinitions as Tenney et al. (2019b).10\nResults We plot the online code results in Figure 4. The overall codelength shows a similar trend to edge probing: Most of the reduction in feature codelength is achieved with fewer than 100M words. MDL for syntactic features decreases even sooner. Results for Winograd are idiosyncratic, probably due to the failure of the probes to learn the task.\nThe changes in model codelength and data codelength are shown on the bar plots in Figure 4. We compute the data codelength following Voita and Titov (2020) using the training set loss of a classifier trained on the entire training set, and the model codelength is the total codelength minus the data codelength. The monotonically decreasing data codelength simply reflects the fact that the more data rich RoBERTa models have smaller loss. When it comes to the model codelength, however, we generally observe the global minimum for the randomly initialized models (i.e., at “None”). This is expected, and intuitively reflects the fact that a decoder trained on random representations would provide little information about the labels, and so it would be optimal to transmit a very simple decoder. On many tasks, the model codelength starts to decrease when the pretraining data volume exceeds a certain amount. However, this trend is not consistent across tasks and the effect is relatively small."
    }, {
      "heading" : "5 Unsupervised Grammaticality Judgement",
      "text" : "We use the BLiMP benchmark (Warstadt et al., 2020a) to test models’ knowledge of individual grammatical phenomena in English. BLiMP is a challenge set of 67 tasks, each containing 1000 minimal pairs of sentences that highlight a particular morphological, syntactic, or semantic phenomena. Minimal pairs in BLiMP consist of two sentences that differ only by a single edit, but contrast in grammatical acceptability. A language model classifies a minimal pair correctly if it assigns a higher probability to the acceptable sentence. Since RoBERTa is a masked language model (MLM), we measure pseudo log-likelihood (Wang and Cho, 2019) to score sentences (Salazar et al., 2020).\nResults We plot learning curves for BLiMP in Figure 5. Warstadt et al. organize the 67 tasks in\n10Unlike us, Voita and Titov redefine the edge probing tasks as standard multi-class classification tasks.\nBLiMP into 12 categories based on the phenomena tested and for each category we plot the average accuracy for the tasks in the category. We do not normalize results in this plot. For the no-data baseline, we plot chance accuracy of 50% rather than making empirical measurements from random RoBERTa models.\nWe find the greatest improvement in overall BLiMP performance between 1M and 100M words of pretraining data. With 100M words, sensitivity to contrasts in acceptability overall is within 9 accuracy points of humans, and improves only 6 points with additional data. This shows that substantial knowledge of many grammatical phenomena can be acquired from 100M words of raw text.\nWe also observe significant variation in how much data is needed to learn different phenomena. We see the steepest learning curves on agreement phenomena, with nearly all improvements occurring between 1M and 10M words. For phenomena involving wh-dependencies, i.e. filler-gap dependencies and island effects, we observe shallow and delayed learning curves with 90% of possible improvements occurring between 1M and 100M words. The relative difficulty of wh-dependencies can probably be ascribed to the long-distance nature and lower frequency of those phenomena. We also observe that the phenomena tested in the quantifiers category are never effectively learned, even by RoBERTaBASE. These phenomena include subtle semantic contrasts—for example Nobody ate {more than, *at least} two cookies—which may involve difficult-to-learn pragmatic knowledge (Cohen and Krifka, 2014)."
    }, {
      "heading" : "6 Unsupervised Language Model Knowledge Probe",
      "text" : "LAMA is a test suite introduced by Petroni et al. to test LMs’ factual knowledge. It contains over 50,000 cloze statements converted from subjectrelation-object triples or question-answer pairs extracted from four datasets: GoogleRE,11 TRE-x (Elsahar et al., 2018), ConceptNet (Speer and Havasi, 2012), and SQUAD (Rajpurkar et al., 2016). The Google-RE and T-REx tasks are each divided into three sub-tasks.\nResults We plot the results on LAMA in Figure 6. The fastest growing point of most curves appears after 100M words. This relatively large quantity of\n11source: https://code.google.com/archive/ p/relation-extraction-corpus/.\ndata may be needed for the model to be exposed to relevant factual knowledge. The learning curves for many LAMA tasks do not show clear signs of saturation in the range of 0 to 30B words, suggesting further improvements are likely with much larger data quantities. Among LAMA tasks, ConceptNet most directly tests commonsense knowledge. The steep slope of the ConceptNet curve between 100M and 30B words of pretraining data and the large precision jump (> 0.05) from 1B to 30B show that increasing the pretraining data to over 1B words significantly improve the LM’s commonsense knowledge, which explains the shape of the Winograd coref. learning curve in Section 3."
    }, {
      "heading" : "7 Fine-tuning on NLU Tasks",
      "text" : "SuperGLUE is a benchmark suite of eight classification-based language-understanding tasks (Wang et al., 2019). We test each MiniBERTa on five SuperGLUE tasks on which we expect to see significant variation at these scales.12 The hyperpa-\n12Task data sources: CB from De Marneffe et al. (2019), BoolQ from Clark et al. (2019), COPA from Roemmele et al. (2011), WiC from Pilehvar and Camacho-Collados (2019); Miller (1995); Schuler (2005), and RTE from Dagan et al.\nrameter search range used for each task is described in the appendix.\nResults We plot the results on the selected SuperGLUE tasks in Figure 7. Improvements in SuperGLUE performance require a relatively large volume of pretraining data. For most tasks, the point of fastest improvement in our interpolated curve occurs with more than 1B words. None of the tasks (with the possible exception of CommitmentBank) show any significant sign of saturation at 30B words. This suggests that some key NLU skills are not learnt with fewer than billions of words, and that models are likely to continue improving substantially on these tasks given 10 to 100 times more pretraining data."
    }, {
      "heading" : "8 Discussion",
      "text" : "Figure 1 plots the overall learning curves for these five methods together. The most striking result is that good NLU task performance requires far more data than achieving good representations for linguistic features. Classifier probing, MDL\n(2006); Bar Haim et al. (2006); Giampiccolo et al. (2007); Bentivogli et al. (2009).\nCB\nBoolQ\nCOPA\nWiC\nRTE\nprobing, and acceptability judgment performance all improve rapidly between 1M and 10M words and show little improvement beyond 100M words, while performance on the NLU tasks in SuperGLUE appears to improve most rapidly with over 1B words and will likely continue improving at larger data scales. While the linguistic features we test are undoubtedly needed to robustly solve most NLU tasks, a model that can extract and encode a large proportion of these features may still perform poorly on SuperGLUE. What drives improvements in NLU task performance at larger data scales remains an open question.\nFactual knowledge may play a large role in explaining SuperGLUE performance. This hypothesis is backed up by results from the Winograd edge-probing task (Figure 2) and the LAMA tasks (Figure 6), which suggest that most of the im-\nprovements in the model’s world and commonsense knowledge are made with over 100M words. However, the LAMA learning curve shows signs of slowing between 1B and 30B words, the SuperGLUE curve does not.\nAnother possible explanation is that linguistic features encoded by a model may not be easily accessible during fine-turning. Warstadt et al. (2020b) found that RoBERTa can learn to reliably extract many linguistic features with little pretraining data, but requires billions of words of pretraining data before it uses those features preferentially when generalizing.\nIn light of Warstadt et al.’s findings, we had initially hypothesized that feature accessibility as measured by MDL might show a shallower or later learning curve than standard classifier probing.13\n13Warstadt et al.’s experiments are quite different to ours.\nOur findings do not support this hypothesis: Figure 1 shows no substantial difference between the classifier probing MDL probing curves.\nHowever, we do not totally rule out the possibility that linguistic feature accessibility continues to improve with massive pretraining sets. There are potential modifications to Voita and Titov’s approach that could more faithfully estimate feature accessibility. First, although RoBERTa is actually fine-tuned in most applications, we and Voita and Titov measure MDL taking the outputs of the frozen RoBERTa model as input to a trainable MLP decoder. It may be more relevant to measure MDL by fine-tuning the entire model (Lovering et al., 2021). Second, MDL actually estimates the information content of a particular dataset, rather than the feature itself. Whitney et al. (2020) propose an alternative to MDL that measures feature complexity in a way that does not depend on the size of the dataset."
    }, {
      "heading" : "9 Related Work",
      "text" : "Probing neural network representations has been an active area of research in recent years (Belinkov and Glass, 2019; Rogers et al., 2020). With the advent of large pretrained Transformers like BERT (Devlin et al., 2019), numerous papers have used classifier probing methods to attempt to locate linguistic features in learned representations with striking positive results (Tenney et al., 2019b; Hewitt and Manning, 2019). However, another thread has found problems with many probing methods: Classifier probes can learn too much from training data (Hewitt and Liang, 2019) and can fail to distinguish features that are extractable from features that are actually used when generalizing on downstream tasks (Voita and Titov, 2020; Pimentel et al., 2020; Elazar et al., 2020). Moreover, different probing methods often yield contradictory results (Warstadt et al., 2019).\nThere have also been a few earlier studies investigating the relationship between pretraining data volume and linguistic knowledge in language models. Studies of unsupervised acceptability judgments find fairly consistent evidence of rapid improvements in linguistic knowledge up to about 10M words of pretraining data, after which improvements slow down for most phenomena. van\nThey measure RoBERTa’s preference for linguistic features over surface features during fine-tuning on ambiguous classification tasks.\nSchijndel et al. (2019) find large improvements in knowledge of subject-verb agreement and reflexive binding up to 10M words, and little improvement between 10M and 80M words. Hu et al. (2020) find that GPT-2 trained on 42M words performs roughly as well on a syntax benchmark as a similar model trained on 100 times that amount. Other studies have investigated how one model’s linguistic knowledge changes during the training process, as a function of the number of updates (Saphra and Lopez, 2019; Chiang et al., 2020).\nRaffel et al. (2020) also investigate how performance on SuperGLUE (and other downstream tasks) improves with pretraining dataset size between about 8M and 34B tokens. In contrast to our findings, they find that models with around 500M tokens of pretraining data can perform similarly on downstream tasks to models with 34B words. However, there are many differences in our settings that may lead to this divergence. For example, they pretrain for a fixed number of iterations (totaling 34B token updates), whereas the MiniBERTas we use were pretrained with early stopping. They also use prefix prompts in their task formulations, and adopt an encoder-decoder architecture and thus their model has roughly twice the number of parameters of the largest model we evaluate.\nThere is also some recent work that investigates the effect of pretraining data size of other languages. Micheli et al. (2020) pretrain BERT-based language models on 10MB, 100MB, 500MB, 1GB, 2GB, and 4GB of French text and test them on a question answering task. They find that the French MLM pretrained on 100MB of raw text has similar performance to the ones pretrained on larger datasets on the task, and that corpus-specific selfsupervised learning does not make a significant difference. Martin et al. (2020) also show that French MLMs can already learn a lot from small-scale pretraining.\nConcurrent work (Liu et al., 2021) probes RoBERTa models pretrained on different numbers of iterations using a set of probing tasks similar to ours. They find that linguistic abilities are acquired fastest, world and commonsense knowledge learning takes more iterations, and reasoning abilities are never stably acquired. Both studies show that linguistic knowledge is easier to learn than factual knowledge."
    }, {
      "heading" : "10 Conclusion",
      "text" : "We track several aspects of RoBERTa’s ability as pretraining data increases. We find that ability in syntax and semantics largely saturates after only 10M to 100M words of pretraining data—on par with the data available to human learners—while learning factual knowledge requires much more data. We also find that scaling pretraining data size past billions of words significantly improves the NLU performance, though we cannot fully explain what abilities drive this improvement. Answering this question could be a stepping stone to more data-efficient models."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This material is based upon work supported by the National Science Foundation under grant no. 1850208. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. We would like to thank Udit Arora, Jason Phang, Clara Vania, and ML2 for feedback on an earlier draft. Thanks also to Kyunghyun Cho, Tal Linzen, Grusha Prasad, and Emin Orhan for suggestions regarding the exponential learning curve, and to Elena Voita, Ian Tenney, and Haokun Liu for the discussion about the implementation of the probing methods.\nEthical Considerations\nThere are several ethical reasons to study LMs with limited pretraining data. Training massive LMs like RoBERTa from scratch comes with non-trivial environmental costs (Strubell et al., 2019), and they are expensive to train, limiting contributions to pretraining research from scientists in lower-resource contexts. By evaluating LMs with limited pretraining, we demonstrate that smaller LMs match massive ones in performance in many respects. We also identify a clear gap in our knowledge regarding why extensive pretraining is effective. Answering this question could lead to more efficient pretraining and ultimately reduce environmental costs and make NLP more accessible. On the other hand, there is a danger that our work, by projecting substantial gains in model performance by increasing pretraining size, could legitimize and encourage the trend of ever growing datasets.\nMassive LMs also replicate social biases present in training data (Nangia et al., 2020). By establish-\ning benchmarks for smaller LMs and highlighting their efficacy for certain purposes, we hope to spur future work that takes advantage of smaller pretraining datasets to carefully curate the data distribution, as advocated by Bender et al. (2021), in order to build LMs that do less to reproduce harmful biases and are more inclusive of minority dialects."
    }, {
      "heading" : "A Appendices",
      "text" : "No ne 1M 10 M 10 0M 1B 30 B\n0.0\n0.5\n1.0 CB\nNo ne 1M 10 M 10 0M 1B 30 B\nBoolQ\nNo ne 1M 10 M 10 0M 1B 30 B\nCOPA\nNo ne 1M 10 M 10 0M 1B 30 B\nWiC\nNo ne 1M 10 M 10 0M 1B 30 B\nRTE\nPe rfo\nrm an ce (A cc ur ac y/\nF1 )\nOverall Learning Curve\nTask Learning Curve Overall Results Task Results\nRoBERTa-Large Task Performance\nFigure 9: Our absolute SuperGLUE results (not normalized) compared to RoBERTaLARGE results from Liu et al. (2019)."
    } ],
    "references" : [ {
      "title" : "Fine-grained analysis of sentence embeddings using auxiliary prediction tasks",
      "author" : [ "Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg." ],
      "venue" : "Proceedings of ICLR Conference Track. Toulon, France.",
      "citeRegEx" : "Adi et al\\.,? 2017",
      "shortCiteRegEx" : "Adi et al\\.",
      "year" : 2017
    }, {
      "title" : "The second PASCAL recognising textual entailment challenge",
      "author" : [ "Roy Bar Haim", "Ido Dagan", "Bill Dolan", "Lisa Ferro", "Danilo Giampiccolo", "Bernardo Magnini", "Idan Szpektor." ],
      "venue" : "Proceedings of the Second PASCAL Challenges Workshop on Recognis-",
      "citeRegEx" : "Haim et al\\.,? 2006",
      "shortCiteRegEx" : "Haim et al\\.",
      "year" : 2006
    }, {
      "title" : "Analysis methods in neural language processing: A survey",
      "author" : [ "Yonatan Belinkov", "James R. Glass." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:49–72.",
      "citeRegEx" : "Belinkov and Glass.,? 2019",
      "shortCiteRegEx" : "Belinkov and Glass.",
      "year" : 2019
    }, {
      "title" : "On the dangers of stochastic parrots: Can language models be too big",
      "author" : [ "Emily M Bender", "Timnit Gebru", "Angelina McMillanMajor", "Shmargaret Shmitchell." ],
      "venue" : "Proceedings of FAccT.",
      "citeRegEx" : "Bender et al\\.,? 2021",
      "shortCiteRegEx" : "Bender et al\\.",
      "year" : 2021
    }, {
      "title" : "The fifth PASCAL recognizing textual entailment challenge",
      "author" : [ "Luisa Bentivogli", "Ido Dagan", "Hoa Trang Dang", "Danilo Giampiccolo", "Bernardo Magnini." ],
      "venue" : "Textual Analysis Conference (TAC).",
      "citeRegEx" : "Bentivogli et al\\.,? 2009",
      "shortCiteRegEx" : "Bentivogli et al\\.",
      "year" : 2009
    }, {
      "title" : "Language models are few-shot learners",
      "author" : [ "Amodei." ],
      "venue" : "Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Amodei.,? 2020",
      "shortCiteRegEx" : "Amodei.",
      "year" : 2020
    }, {
      "title" : "Pretrained language model embryology: The birth of ALBERT",
      "author" : [ "Cheng-Han Chiang", "Sung-Feng Huang", "Hung-yi Lee." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6813–6828, On-",
      "citeRegEx" : "Chiang et al\\.,? 2020",
      "shortCiteRegEx" : "Chiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Boolq: Exploring the surprising",
      "author" : [ "Christopher Clark", "Kenton Lee", "Ming-Wei Chang", "Tom Kwiatkowski", "Michael Collins", "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Clark et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "Superlative quantifiers and meta-speech acts",
      "author" : [ "Ariel Cohen", "Manfred Krifka." ],
      "venue" : "Linguistics and Philosophy, 37(1):41–90.",
      "citeRegEx" : "Cohen and Krifka.,? 2014",
      "shortCiteRegEx" : "Cohen and Krifka.",
      "year" : 2014
    }, {
      "title" : "The PASCAL recognising textual entailment challenge",
      "author" : [ "Ido Dagan", "Oren Glickman", "Bernardo Magnini." ],
      "venue" : "Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Textual Entailment.",
      "citeRegEx" : "Dagan et al\\.,? 2006",
      "shortCiteRegEx" : "Dagan et al\\.",
      "year" : 2006
    }, {
      "title" : "The commitmentbank: Investigating projection in naturally occurring discourse",
      "author" : [ "Marie-Catherine De Marneffe", "Mandy Simons", "Judith Tonhauser." ],
      "venue" : "proceedings of Sinn und Bedeutung, volume 23, pages 107–124.",
      "citeRegEx" : "Marneffe et al\\.,? 2019",
      "shortCiteRegEx" : "Marneffe et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "When BERT forgets how to POS: Amnesic probing of linguistic properties and MLM predictions",
      "author" : [ "Yanai Elazar", "Shauli Ravfogel", "Alon Jacovi", "Yoav Goldberg." ],
      "venue" : "arXiv preprint 2006.00995.",
      "citeRegEx" : "Elazar et al\\.,? 2020",
      "shortCiteRegEx" : "Elazar et al\\.",
      "year" : 2020
    }, {
      "title" : "T-rex: A large scale alignment of natural language with knowledge base triples",
      "author" : [ "Hady Elsahar", "Pavlos Vougiouklis", "Arslen Remaci", "Christophe Gravier", "Jonathon Hare", "Frederique Laforest", "Elena Simperl." ],
      "venue" : "Proceedings of the Eleventh Interna-",
      "citeRegEx" : "Elsahar et al\\.,? 2018",
      "shortCiteRegEx" : "Elsahar et al\\.",
      "year" : 2018
    }, {
      "title" : "What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models",
      "author" : [ "Allyson Ettinger." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:34–48.",
      "citeRegEx" : "Ettinger.,? 2020",
      "shortCiteRegEx" : "Ettinger.",
      "year" : 2020
    }, {
      "title" : "Probing for semantic evidence of composition by means of simple classification tasks",
      "author" : [ "Allyson Ettinger", "Ahmed Elgohary", "Philip Resnik." ],
      "venue" : "Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP, pages 134–139.",
      "citeRegEx" : "Ettinger et al\\.,? 2016",
      "shortCiteRegEx" : "Ettinger et al\\.",
      "year" : 2016
    }, {
      "title" : "The third PASCAL recognizing textual entailment challenge",
      "author" : [ "Danilo Giampiccolo", "Bernardo Magnini", "Ido Dagan", "Bill Dolan." ],
      "venue" : "Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing. Association for Computational",
      "citeRegEx" : "Giampiccolo et al\\.,? 2007",
      "shortCiteRegEx" : "Giampiccolo et al\\.",
      "year" : 2007
    }, {
      "title" : "The power law repealed: The case for an exponential law of practice",
      "author" : [ "Andrew Heathcote", "Scott Brown", "Douglas JK Mewhort." ],
      "venue" : "Psychonomic bulletin & review, 7(2):185–207.",
      "citeRegEx" : "Heathcote et al\\.,? 2000",
      "shortCiteRegEx" : "Heathcote et al\\.",
      "year" : 2000
    }, {
      "title" : "Semantics in generative grammar",
      "author" : [ "Irene Heim", "Angelika Kratzer." ],
      "venue" : "Blackwell Oxford.",
      "citeRegEx" : "Heim and Kratzer.,? 1998",
      "shortCiteRegEx" : "Heim and Kratzer.",
      "year" : 1998
    }, {
      "title" : "SemEval-2010 task 8: Multi-way classification of semantic relations",
      "author" : [ "Iris Hendrickx", "Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid Ó Séaghdha", "Sebastian Padó", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz" ],
      "venue" : null,
      "citeRegEx" : "Hendrickx et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hendrickx et al\\.",
      "year" : 2010
    }, {
      "title" : "Designing and interpreting probes with control tasks",
      "author" : [ "John Hewitt", "Percy Liang." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.",
      "citeRegEx" : "Hewitt and Liang.,? 2019",
      "shortCiteRegEx" : "Hewitt and Liang.",
      "year" : 2019
    }, {
      "title" : "A structural probe for finding syntax in word representations",
      "author" : [ "John Hewitt", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Hewitt and Manning.,? 2019",
      "shortCiteRegEx" : "Hewitt and Manning.",
      "year" : 2019
    }, {
      "title" : "A systematic assessment of syntactic generalization in neural language models",
      "author" : [ "Jennifer Hu", "Jon Gauthier", "Peng Qian", "Ethan Wilcox", "Roger Levy." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "The exponential learning equation as a function of successful trials results in sigmoid performance",
      "author" : [ "Nathaniel Leibowitz", "Barak Baum", "Giora Enden", "Amir Karniel." ],
      "venue" : "Journal of Mathematical Psychology, 54(3):338–340.",
      "citeRegEx" : "Leibowitz et al\\.,? 2010",
      "shortCiteRegEx" : "Leibowitz et al\\.",
      "year" : 2010
    }, {
      "title" : "Precise task formalization matters in Winograd schema evaluations",
      "author" : [ "Haokun Liu", "William Huang", "Dhara Mungra", "Samuel R. Bowman." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Probing across time: What does RoBERTa know and when? CoRR, abs/2104.07885",
      "author" : [ "Leo Z. Liu", "Yizhong Wang", "Jungo Kasai", "Hannaneh Hajishirzi", "Noah A. Smith" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "RoBERTa: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Predicting inductive biases of fine-tuned models",
      "author" : [ "Charles Lovering", "Rohan Jha", "Tal Linzen", "Ellie Pavlick." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Lovering et al\\.,? 2021",
      "shortCiteRegEx" : "Lovering et al\\.",
      "year" : 2021
    }, {
      "title" : "CamemBERT: a tasty French language model",
      "author" : [ "Louis Martin", "Benjamin Muller", "Pedro Javier Ortiz Suárez", "Yoann Dupont", "Laurent Romary", "Éric de la Clergerie", "Djamé Seddah", "Benoı̂t Sagot" ],
      "venue" : "In Proceedings of the 58th Annual Meeting",
      "citeRegEx" : "Martin et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Martin et al\\.",
      "year" : 2020
    }, {
      "title" : "On the importance of pre-training data volume for compact language models",
      "author" : [ "Vincent Micheli", "Martin d’Hoffschmidt", "François Fleuret" ],
      "venue" : "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Micheli et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Micheli et al\\.",
      "year" : 2020
    }, {
      "title" : "WordNet: a lexical database for English",
      "author" : [ "George A Miller." ],
      "venue" : "Communications of the ACM.",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "CrowS-pairs: A challenge dataset for measuring social biases in masked language models",
      "author" : [ "Nikita Nangia", "Clara Vania", "Rasika Bhalerao", "Samuel R. Bowman." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Nangia et al\\.,? 2020",
      "shortCiteRegEx" : "Nangia et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models as knowledge bases",
      "author" : [ "Fabio Petroni", "Tim Rocktäschel", "Sebastian Riedel", "Patrick Lewis", "Anton Bakhtin", "Yuxiang Wu", "Alexander Miller" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Petroni et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Petroni et al\\.",
      "year" : 2019
    }, {
      "title" : "WiC: The word-in-context dataset for evaluating context-sensitive meaning representations",
      "author" : [ "Mohammad Taher Pilehvar", "Jose CamachoCollados." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Pilehvar and CamachoCollados.,? 2019",
      "shortCiteRegEx" : "Pilehvar and CamachoCollados.",
      "year" : 2019
    }, {
      "title" : "Information-theoretic probing for linguistic structure",
      "author" : [ "Tiago Pimentel", "Josef Valvoda", "Rowan Hall Maudslay", "Ran Zmigrod", "Adina Williams", "Ryan Cotterell." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Pimentel et al\\.,? 2020",
      "shortCiteRegEx" : "Pimentel et al\\.",
      "year" : 2020
    }, {
      "title" : "Intermediate-task transfer learning with pretrained language models: When and why",
      "author" : [ "Yada Pruksachatkun", "Jason Phang", "Haokun Liu", "Phu Mon Htut", "Xiaoyi Zhang", "Richard Yuanzhe Pang", "Clara Vania", "Katharina Kann", "Samuel R. Bowman" ],
      "venue" : null,
      "citeRegEx" : "Pruksachatkun et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Pruksachatkun et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Re-",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Resolving complex cases of definite pronouns: The Winograd schema challenge",
      "author" : [ "Altaf Rahman", "Vincent Ng." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-",
      "citeRegEx" : "Rahman and Ng.,? 2012",
      "shortCiteRegEx" : "Rahman and Ng.",
      "year" : 2012
    }, {
      "title" : "Squad: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
      "author" : [ "Melissa Roemmele", "Cosmin Adrian Bejan", "Andrew S. Gordon." ],
      "venue" : "2011 AAAI Spring Symposium Series.",
      "citeRegEx" : "Roemmele et al\\.,? 2011",
      "shortCiteRegEx" : "Roemmele et al\\.",
      "year" : 2011
    }, {
      "title" : "A primer in BERTology: What we know about how BERT works",
      "author" : [ "Anna Rogers", "Olga Kovaleva", "Anna Rumshisky." ],
      "venue" : "Findings of EMNLP.",
      "citeRegEx" : "Rogers et al\\.,? 2020",
      "shortCiteRegEx" : "Rogers et al\\.",
      "year" : 2020
    }, {
      "title" : "NeuralDavidsonian semantic proto-role labeling",
      "author" : [ "Rachel Rudinger", "Adam Teichert", "Ryan Culkin", "Sheng Zhang", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 944–",
      "citeRegEx" : "Rudinger et al\\.,? 2018",
      "shortCiteRegEx" : "Rudinger et al\\.",
      "year" : 2018
    }, {
      "title" : "Masked language model scoring",
      "author" : [ "Julian Salazar", "Davis Liang", "Toan Q. Nguyen", "Katrin Kirchhoff." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2699–2712, Online. Association for Compu-",
      "citeRegEx" : "Salazar et al\\.,? 2020",
      "shortCiteRegEx" : "Salazar et al\\.",
      "year" : 2020
    }, {
      "title" : "Understanding learning dynamics of language models with SVCCA",
      "author" : [ "Naomi Saphra", "Adam Lopez." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Saphra and Lopez.,? 2019",
      "shortCiteRegEx" : "Saphra and Lopez.",
      "year" : 2019
    }, {
      "title" : "Quantity doesn’t buy quality syntax with neural language models",
      "author" : [ "Marten van Schijndel", "Aaron Mueller", "Tal Linzen." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Schijndel et al\\.,? 2019",
      "shortCiteRegEx" : "Schijndel et al\\.",
      "year" : 2019
    }, {
      "title" : "Verbnet: A Broadcoverage, Comprehensive Verb Lexicon",
      "author" : [ "Karin Kipper Schuler." ],
      "venue" : "Ph.D. thesis, University of Pennsylvania.",
      "citeRegEx" : "Schuler.,? 2005",
      "shortCiteRegEx" : "Schuler.",
      "year" : 2005
    }, {
      "title" : "A gold standard dependency corpus for English",
      "author" : [ "Natalia Silveira", "Timothy Dozat", "Marie-Catherine de Marneffe", "Samuel Bowman", "Miriam Connor", "John Bauer", "Chris Manning." ],
      "venue" : "Proceedings of the Ninth International Conference on Language",
      "citeRegEx" : "Silveira et al\\.,? 2014",
      "shortCiteRegEx" : "Silveira et al\\.",
      "year" : 2014
    }, {
      "title" : "Representing general relational knowledge in conceptnet 5",
      "author" : [ "Robert Speer", "Catherine Havasi." ],
      "venue" : "LREC, pages 3679–3686.",
      "citeRegEx" : "Speer and Havasi.,? 2012",
      "shortCiteRegEx" : "Speer and Havasi.",
      "year" : 2012
    }, {
      "title" : "Energy and policy considerations for deep learning in NLP",
      "author" : [ "Emma Strubell", "Ananya Ganesh", "Andrew McCallum." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3645–3650, Florence, Italy.",
      "citeRegEx" : "Strubell et al\\.,? 2019",
      "shortCiteRegEx" : "Strubell et al\\.",
      "year" : 2019
    }, {
      "title" : "Semantic proto-role labeling",
      "author" : [ "Adam Teichert", "Adam Poliak", "Benjamin Van Durme", "Matthew Gormley." ],
      "venue" : "AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Teichert et al\\.,? 2017",
      "shortCiteRegEx" : "Teichert et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT rediscovers the classical NLP pipeline",
      "author" : [ "Ian Tenney", "Dipanjan Das", "Ellie Pavlick." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593– 4601, Florence, Italy. Association for Computational",
      "citeRegEx" : "Tenney et al\\.,? 2019a",
      "shortCiteRegEx" : "Tenney et al\\.",
      "year" : 2019
    }, {
      "title" : "What do you learn from context? Probing for sentence structure in contextual",
      "author" : [ "Ian Tenney", "Patrick Xia", "Berlin Chen", "Alex Wang", "Adam Poliak", "R. Thomas McCoy", "Najoung Kim", "Benjamin Van Durme", "Samuel R Bowman", "Dipanjan Das" ],
      "venue" : null,
      "citeRegEx" : "Tenney et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Tenney et al\\.",
      "year" : 2019
    }, {
      "title" : "Informationtheoretic probing with minimum description length",
      "author" : [ "Elena Voita", "Ivan Titov." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, Punta Cana, Dominican Republic. Association for Compu-",
      "citeRegEx" : "Voita and Titov.,? 2020",
      "shortCiteRegEx" : "Voita and Titov.",
      "year" : 2020
    }, {
      "title" : "BERT has a mouth, and it must speak: BERT as a Markov random field language model",
      "author" : [ "Alex Wang", "Kyunghyun Cho." ],
      "venue" : "Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation, pages 30–36,",
      "citeRegEx" : "Wang and Cho.,? 2019",
      "shortCiteRegEx" : "Wang and Cho.",
      "year" : 2019
    }, {
      "title" : "SuperGLUE: A stickier benchmark for general-purpose language understanding systems",
      "author" : [ "Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman." ],
      "venue" : "33rd Conference on Neu-",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop Black-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "BLiMP: The benchmark of linguistic minimal pairs for english",
      "author" : [ "Alex Warstadt", "Alicia Parrish", "Haokun Liu", "Anhad Mohananey", "Wei Peng", "Sheng-Fu Wang", "Samuel R. Bowman." ],
      "venue" : "Transactions of the Association for Computational Linguistics,",
      "citeRegEx" : "Warstadt et al\\.,? 2020a",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning which features matter: RoBERTa acquires a preference for linguistic generalizations (eventually)",
      "author" : [ "Alex Warstadt", "Yian Zhang", "Xiaocheng Li", "Haokun Liu", "Samuel R. Bowman." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical",
      "citeRegEx" : "Warstadt et al\\.,? 2020b",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2020
    }, {
      "title" : "OntoNotes release",
      "author" : [ "Ralph Weischedel", "Martha Palmer", "Marcus Mitchell", "Eduard Hovy", "Sameer Pradhan", "Lance Ramshaw", "Nianwen Xue", "Ann Taylor", "Jeff Kaufman", "Michelle Franchini", "Mohammed El-Bachouti", "Robert Belvin", "Ann Houston" ],
      "venue" : null,
      "citeRegEx" : "Weischedel et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Weischedel et al\\.",
      "year" : 2013
    }, {
      "title" : "Inference is everything: Recasting semantic resources into a unified evaluation framework",
      "author" : [ "Aaron Steven White", "Pushpendre Rastogi", "Kevin Duh", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the Eighth International Joint Conference on Natural",
      "citeRegEx" : "White et al\\.,? 2017",
      "shortCiteRegEx" : "White et al\\.",
      "year" : 2017
    }, {
      "title" : "Evaluating representations by the complexity of learning low-loss predictors",
      "author" : [ "William F Whitney", "Min Jae Song", "David Brandfonbrener", "Jaan Altosaar", "Kyunghyun Cho." ],
      "venue" : "arXiv preprint arXiv:2009.07368.",
      "citeRegEx" : "Whitney et al\\.,? 2020",
      "shortCiteRegEx" : "Whitney et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 55,
      "context" : ", 2020) to improve on existing models on language understanding benchmarks like GLUE (Wang et al., 2018).",
      "startOffset" : 85,
      "endOffset" : 104
    }, {
      "referenceID" : 57,
      "context" : "1113 With these questions in mind, we evaluate and probe the MiniBERTas (Warstadt et al., 2020b), a group of RoBERTa models pretrained on 1M, 10M, 100M, and 1B words, and RoBERTaBASE (Liu et al.",
      "startOffset" : 72,
      "endOffset" : 96
    }, {
      "referenceID" : 26,
      "context" : ", 2020b), a group of RoBERTa models pretrained on 1M, 10M, 100M, and 1B words, and RoBERTaBASE (Liu et al., 2019) pretrained on about 30B words, using five methods: First we use standard classifier probing on the edge probing suite of NLP tasks (Tenney et al.",
      "startOffset" : 95,
      "endOffset" : 113
    }, {
      "referenceID" : 52,
      "context" : "Second, we apply minimum description length (MDL) probing (Voita and Titov, 2020) to the edge probing suite, with the goal of quantifying the accessibility of these features.",
      "startOffset" : 58,
      "endOffset" : 81
    }, {
      "referenceID" : 56,
      "context" : "Third, we test the models’ knowledge of various syntactic phenomena using unsupervised acceptability judgments on the BLiMP suite (Warstadt et al., 2020a).",
      "startOffset" : 130,
      "endOffset" : 154
    }, {
      "referenceID" : 32,
      "context" : "Fourth, we probe the models’ world knowledge and commonsense knowledge using unsupervised language model knowledge probing with the LAMA suite (Petroni et al., 2019).",
      "startOffset" : 143,
      "endOffset" : 165
    }, {
      "referenceID" : 26,
      "context" : "(2020b) on 1M, 10M, 100M, and 1B words, the publicly available RoBERTaBASE (Liu et al., 2019), which is pretrained on about 30B words,1 and 3 RoBERTaBASE models with randomly initialized parameters.",
      "startOffset" : 75,
      "endOffset" : 93
    }, {
      "referenceID" : 17,
      "context" : "We use a four-parameter exponential learning curve used to capture diminishing improvement in performance as a function of the number of practice trials (Heathcote et al., 2000; Leibowitz et al., 2010):",
      "startOffset" : 153,
      "endOffset" : 201
    }, {
      "referenceID" : 23,
      "context" : "We use a four-parameter exponential learning curve used to capture diminishing improvement in performance as a function of the number of practice trials (Heathcote et al., 2000; Leibowitz et al., 2010):",
      "startOffset" : 153,
      "endOffset" : 201
    }, {
      "referenceID" : 24,
      "context" : "9 As the Winograd task is designed to test commonsense knowledge and reasoning, the results suggest that these features require more data to encode than syntactic and semantic ones, with the caveat that the dataset is smaller than the other edge probing tasks, and results on Winograd tasks are highly sensitive to factors such as task formulation (Liu et al., 2020).",
      "startOffset" : 348,
      "endOffset" : 366
    }, {
      "referenceID" : 52,
      "context" : "In this experiment, we study the MiniBERTas with MDL probing (Voita and Titov, 2020), with the",
      "startOffset" : 61,
      "endOffset" : 84
    }, {
      "referenceID" : 56,
      "context" : "We use the BLiMP benchmark (Warstadt et al., 2020a) to test models’ knowledge of individual grammatical phenomena in English.",
      "startOffset" : 27,
      "endOffset" : 51
    }, {
      "referenceID" : 53,
      "context" : "Since RoBERTa is a masked language model (MLM), we measure pseudo log-likelihood (Wang and Cho, 2019) to score sentences (Salazar et al.",
      "startOffset" : 81,
      "endOffset" : 101
    }, {
      "referenceID" : 42,
      "context" : "Since RoBERTa is a masked language model (MLM), we measure pseudo log-likelihood (Wang and Cho, 2019) to score sentences (Salazar et al., 2020).",
      "startOffset" : 121,
      "endOffset" : 143
    }, {
      "referenceID" : 8,
      "context" : "involve difficult-to-learn pragmatic knowledge (Cohen and Krifka, 2014).",
      "startOffset" : 47,
      "endOffset" : 71
    }, {
      "referenceID" : 13,
      "context" : "It contains over 50,000 cloze statements converted from subjectrelation-object triples or question-answer pairs extracted from four datasets: GoogleRE,11 TRE-x (Elsahar et al., 2018), ConceptNet (Speer and Havasi, 2012), and SQUAD (Rajpurkar et al.",
      "startOffset" : 160,
      "endOffset" : 182
    }, {
      "referenceID" : 47,
      "context" : ", 2018), ConceptNet (Speer and Havasi, 2012), and SQUAD (Rajpurkar et al.",
      "startOffset" : 20,
      "endOffset" : 44
    }, {
      "referenceID" : 38,
      "context" : ", 2018), ConceptNet (Speer and Havasi, 2012), and SQUAD (Rajpurkar et al., 2016).",
      "startOffset" : 56,
      "endOffset" : 80
    }, {
      "referenceID" : 54,
      "context" : "SuperGLUE is a benchmark suite of eight classification-based language-understanding tasks (Wang et al., 2019).",
      "startOffset" : 90,
      "endOffset" : 109
    }, {
      "referenceID" : 27,
      "context" : "It may be more relevant to measure MDL by fine-tuning the entire model (Lovering et al., 2021).",
      "startOffset" : 71,
      "endOffset" : 94
    }, {
      "referenceID" : 2,
      "context" : "Probing neural network representations has been an active area of research in recent years (Belinkov and Glass, 2019; Rogers et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 138
    }, {
      "referenceID" : 40,
      "context" : "Probing neural network representations has been an active area of research in recent years (Belinkov and Glass, 2019; Rogers et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 138
    }, {
      "referenceID" : 11,
      "context" : "advent of large pretrained Transformers like BERT (Devlin et al., 2019), numerous papers have used classifier probing methods to attempt to locate linguistic features in learned representations with striking positive results (Tenney et al.",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 21,
      "context" : ", 2019), numerous papers have used classifier probing methods to attempt to locate linguistic features in learned representations with striking positive results (Tenney et al., 2019b; Hewitt and Manning, 2019).",
      "startOffset" : 161,
      "endOffset" : 209
    }, {
      "referenceID" : 20,
      "context" : "has found problems with many probing methods: Classifier probes can learn too much from training data (Hewitt and Liang, 2019) and can fail to distinguish features that are extractable from features that are actually used when generalizing on downstream tasks (Voita and Titov, 2020; Pimentel et al.",
      "startOffset" : 102,
      "endOffset" : 126
    }, {
      "referenceID" : 52,
      "context" : "has found problems with many probing methods: Classifier probes can learn too much from training data (Hewitt and Liang, 2019) and can fail to distinguish features that are extractable from features that are actually used when generalizing on downstream tasks (Voita and Titov, 2020; Pimentel et al., 2020; Elazar et al., 2020).",
      "startOffset" : 260,
      "endOffset" : 327
    }, {
      "referenceID" : 34,
      "context" : "has found problems with many probing methods: Classifier probes can learn too much from training data (Hewitt and Liang, 2019) and can fail to distinguish features that are extractable from features that are actually used when generalizing on downstream tasks (Voita and Titov, 2020; Pimentel et al., 2020; Elazar et al., 2020).",
      "startOffset" : 260,
      "endOffset" : 327
    }, {
      "referenceID" : 12,
      "context" : "has found problems with many probing methods: Classifier probes can learn too much from training data (Hewitt and Liang, 2019) and can fail to distinguish features that are extractable from features that are actually used when generalizing on downstream tasks (Voita and Titov, 2020; Pimentel et al., 2020; Elazar et al., 2020).",
      "startOffset" : 260,
      "endOffset" : 327
    }, {
      "referenceID" : 43,
      "context" : "Other studies have investigated how one model’s linguistic knowledge changes during the training process, as a function of the number of updates (Saphra and Lopez, 2019; Chiang et al., 2020).",
      "startOffset" : 145,
      "endOffset" : 190
    }, {
      "referenceID" : 6,
      "context" : "Other studies have investigated how one model’s linguistic knowledge changes during the training process, as a function of the number of updates (Saphra and Lopez, 2019; Chiang et al., 2020).",
      "startOffset" : 145,
      "endOffset" : 190
    }, {
      "referenceID" : 25,
      "context" : "Concurrent work (Liu et al., 2021) probes RoBERTa models pretrained on different numbers of iterations using a set of probing tasks similar to ours.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 48,
      "context" : "Training massive LMs like RoBERTa from scratch comes with non-trivial environmental costs (Strubell et al., 2019), and they are expensive to train, limiting contributions to pretraining research from scientists in lower-resource contexts.",
      "startOffset" : 90,
      "endOffset" : 113
    }, {
      "referenceID" : 31,
      "context" : "Massive LMs also replicate social biases present in training data (Nangia et al., 2020).",
      "startOffset" : 66,
      "endOffset" : 87
    } ],
    "year" : 2021,
    "abstractText" : "NLP is currently dominated by language models like RoBERTa which are pretrained on billions of words. But what exact knowledge or skills do Transformer LMs learn from large-scale pretraining that they cannot learn from less data? To explore this question, we adopt five styles of evaluation: classifier probing, information-theoretic probing, unsupervised relative acceptability judgments, unsupervised language model knowledge probing, and fine-tuning on NLU tasks. We then draw learning curves that track the growth of these different measures of model ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that these LMs require only about 10M to 100M words to learn to reliably encode most syntactic and semantic features we test. They need a much larger quantity of data in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks. The results suggest that, while the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other, unidentified, forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models.",
    "creator" : "LaTeX with hyperref"
  }
}