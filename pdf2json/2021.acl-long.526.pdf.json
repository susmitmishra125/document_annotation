{
  "name" : "2021.acl-long.526.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Multimodal Multi-Speaker Merger & Acquisition Financial Modeling: A New Task, Dataset, and Neural Baselines",
    "authors" : [ "Ramit Sawhney", "Mihir Goyal", "Prakhar Goel", "Puneet Mathur", "Rajiv Ratn Shah" ],
    "emails" : [ "rajivratn}@iiitd.ac.in", "puneetm@umd.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6751–6762\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6751"
    }, {
      "heading" : "1 Introduction",
      "text" : "Mergers and Acquisitions (M&As)1 conference calls are events preceding financial transactions involving two or more entities such that either one of the participant companies takes over the other(s) and establishes itself as the owner (termed as ”acquisition”) or when one company combines with another to become a joint entity (termed as ”merger”). In these M&A conference calls, the participating companies’ management makes a presentation to the call participants, such as market analysts, media personnel, and other stakeholders, explaining the rationale for the deal and possible roadblocks to deal completion (Dasgupta et al., 2020). Following the presentation segment, there is a Q&A segment in which the call participants ask questions to which the management responds.\n∗ Equal contribution 1https://www.investopedia.com/mergers-and-acquisitions\nBuilding on the important information that M&As provide, academic research, the financial press, and other media give a great deal of attention. One of these discussions’ principal aspects lies in how the deals may affect the company’s valuation (Moeller et al., 2003; Fraunhoffer et al., 2018) and future growth. A significant focus in financial and economic literature has been on understanding whether M&As create or destroy value. Consequently, shareholders critically analyze the deals to estimate the potential stock price and stock price volatility post the M&A conference call.\nIdentifying the gap in natural language processing (NLP) literature on the lack of resources to study M&A conference calls with their text transcripts and audio recordings, we take the first step in multimodal financial modeling in the M&A space. Such data can allow academicians to study M&A calls further, especially with the rich multimodal data. It shall enable studies that focus not only on the words spoken in the call but also in the manner they were spoken, a relatively unexplored field in financial forecasting, as shown in Figure 1.\nA salient aspect of conference calls is that, unlike text reports, the company’s management interacts with external stakeholders and asks questions. This\ninteraction presents an opportunity of analyzing not just the management’s claims but also the way they express them. In Figure 2, we highlight the various components in a short Q&A interaction. Often, both the transcript and the audios of the calls are available to the public.\nVocal cues play a critical role in verbal communication as they can provide support or discredit the verbal message that is being spoken (Jiang and Pell, 2017). For example, consider if the CEO of the acquiring company exhibits confidence in the statement - ”we are confident that this acquisition will bring us profits,” however, displays nervousness while justifying technical details of the deal, we may infer contradiction in the claims of a successful M&A. Vocal cues have been proven indicators of emotions like deceit and nervousness (Belin et al., 2017; Sporer and Schwandt, 2006). Past research (Qin and Yang, 2019; Sawhney et al., 2020c) shows that the addition of vocal cues has helped with the task of financial predictions and enrich the learned representations.\nOur contributions can be summarized as:\n• We curate a public dataset M3A2 (Multimodal Multi-Speaker Merger & Acquisition Call Fi-\n2The source code, processed features, and details on acquiring raw data are available at https://github.com/ midas-research/m3a-acl\nnancial Forecasting Dataset) that consists of 816 M&A conference calls spanning over 545 hours between 2016 to 2020 with their transcripts and audio recordings, segmented by utterances and aligned with the audio.\n• We accompany the dataset with neural baseline architectures that use the multimodal multi-speaker input to predict stock volatility and price movement.\n• To the best of our knowledge, no such M&A conference call dataset exists in academia, and our proposed methodology, M3ANet is the first deep learning approach for financial predictions on M&A conference calls."
    }, {
      "heading" : "2 Related Work",
      "text" : "M&A Conference Calls Financial reports and conference calls have been shown to have a correlation with the stock market and improve financial predictions (Bowen et al., 2001; Kogan et al., 2009). Studies have also been carried out specifically for M&A calls, showing their effect on the market (Dasgupta et al., 2020; Hu et al., 2018). However, there exists a gap in leveraging neural predictive modeling on using verbal and vocal cues pertaining to M&A calls for financial forecasting.\nFinancial Forecasting Research has shown historical pricing data to be useful in predicting financial risk modeling (Kristjanpoller et al., 2014; Zheng et al., 2019; Dumas et al., 2009). It also considers volatility as an indicator of uncertainty, which helps make decisions regarding investments (Heston, 1993; Johnson and Shanno, 1987; Scott, 1987). Previous work often use numerical features (Liu and Chen, 2019; Nikou et al., 2019) in approaches like neural networks (Kim et al., 2019; Luo et al., 2017), graph neural networks (Sawhney et al., 2020b), and time-series models (Bollerslev, 1986; Engle, 1981). On the other hand, we are interested in analyzing multimodal data like text and audio, which can hold completely different information for predictive models.\nNatural Language Processing and Finance For any system using human interactions to determine financial risk or stock movements, it is necessary to determine the relationship between the various words to determine the speaker’s sentiment. Advances in NLP have been utilized in many approaches to show financial information significantly improving performance in forecasting tasks like volatility and stock price prediction (Wang et al., 2013; Ding et al., 2015; Mittermayer and Knolmayer, 2007). Research has also shown that social media affects the stock market (Bollen et al., 2010; Oliveira et al., 2017; Sawhney et al., 2020a). Machine learning methods using simple bag-of-words features to represent the financial documents used in previous research (Kogan et al., 2009; Rekabsaz et al., 2017) largely ignore the inter-dependencies between the sentences. To fill the gap, recent approaches have moved towards newer models such as transformers (Yang et al., 2020) and reinforcement learning (Sawhney et al., 2021b) over natural language data for financial forecasting.\nMultimodality and Financial Forecasting Research shows that psychological and behavioral elements are often indicators of stock price movement (Malkiel, 2003). Vocal cues have been proven effective in portraying these elements (Wurm et al., 2010; Hobson et al., 2011; Jiang and Pell, 2017). Thus, it is no surprise that multimodal architectures that use these cues for financial predictions have seen significant improvements in their performances (Yang et al., 2020; Sawhney et al., 2020d).\nSpeaker Context Encoding Past research (Zhang et al., 2019; Li et al., 2020) in fields like emotion recognition have seen the improved performance on their prediction tasks with the addition of speaker context. Models with data related to spoken text benefit when the input is enriched with information about who spoke what."
    }, {
      "heading" : "3 Problem Formulation",
      "text" : "Consider an M&A call χ ∈ {χ1, χ2, . . . , χM}, which comprises multimodal components: χ = [t; a]. Here, t is the sequence of textual utterances (sentences)3 of the call transcript and can be represented as [t1, t2...tN ] where ti is the ith utterance of the call and N is the maximum number of utterances in any call. Similarly, a is the sequence of corresponding call audios for the textual utterances (sentences) and can be represented as [a1, a2...aN ] where ai is the ith call audio. The call’s utterances are annotated with speaker information s = [s1, s2...sN ], where si is the speaker of the ith utterance and where each speaker in the call may have spoken one or more utterances. Each M&A conference call may have two or more participating companies, with at least one publicly-traded company with publicly available stock price information. We limit the scope of the problem being solved by forecasting predictions for just one of the participant companies with the larger market valuation (in case of a merger) or the acquiring company (in case of an acquisition). We now describe the two prediction tasks that we utilize to train M3ANet on.\nMeasuring stock volatility Following (Kogan et al., 2009), we formulate stock volatility as a regression problem. For a given stock with a close price of pk on the trading day k, we calculate the average log volatility as the natural log of the standard deviation of return prices r in a window of τ days as:\nv[0,τ ] = ln\n(√∑τ k=1(rk − r̄)2\nτ\n) (1)\nwhere rk = pk−pk−1 pk−1\nis the return price on day k for a given stock, and r̄ is the average return price over a period of τ days.\n3We restrict the scope of segmentation to a sentence level as opposed to a more granular level such as the word level owing to the higher complexity and noise involved in wordlevel segmentation for long M&A calls.\nFormalizing price movement prediction Following (Xu and Cohen, 2018), we define price movement yd−τ,d over a period of τ days as a binary classification task. For a given stock, we employ its close price, which can either rise or fall on a day d compared to a previous day d − τ , to formulate the classification task as:\ny[d−τ,d] = { 1, pd+τ > pd, 0, pd+τ ≤ pd\n(2)\nGiven an acquisition conference call χ, our learning objective is to predict the average negative log volatility v[0,τ ] and price movement y[d−τ,d] using the conference call data χ = [t; a].\n4 Curating M3A: Dataset Creation"
    }, {
      "heading" : "4.1 Data Acquisition",
      "text" : "We curate our dataset, M3A, by acquiring audio records and text transcripts from the Bloomberg Terminal.4 Since the conference calls were reliably available from 2016, we filter and list all M&A calls between 2016 and 2020. To limit the scope, we ensured the calls were in English, had their domicile as the U.S.A., and had ’merger’ or ’acquisition’ in their title. The Bloomberg Terminal often only provides the stock ticker for the acquiring company (in case of an acquisition) and the company with a more prominent marker valuation (in case of a merger). To maintain uniformity, we decide only to use the given stock information. We pull the adjusted closing price data from Yahoo Finance.5\nThe dataset comprises 816 conference calls. The mean number of speakers across the calls is 10.68 ± 4.17, with a maximum of 31 speakers. The mean number of utterances across the calls is 100.54± 38.32 utterances and a maximum of 284 utterances in a call. The mean length comes out to\n4https://bba.bloomberg.net/ 5https://in.finance.yahoo.com/\nbe 40.15± 15.15 minutes and a maximum length of 98.15 minutes for the audio clips. We provide further statistics in Figure 3. Looking at year-wise trends, we see that acquisitions are consistently more frequent that mergers every year. Further, we note that mergers see a decreasing trend in the number of utterances and acquisitions have a consistent number of speakers in M&A calls. We also note that acquisitions conference calls seem to be increasing in length as the years progress.\nWe chronologically divide our dataset into a train, validation, and test set in the ratio of 70 : 10 : 20, respectively. Such a split ensures that future data is not used for forecasting past data."
    }, {
      "heading" : "4.2 Call Segmentation and Alignment",
      "text" : "Each transcript of the dataset begins with the company’s details with the larger market valuation (in case of a merger) or the acquiring company (in case of an acquisition). These details include the company’s name, stock ticker, and the date of the call. The transcript then lists the speakers in the call and their position in the companies, if any. The call contents follow the list of speakers. The contents are separated by utterances and are annotated with the utterances’ speakers.\nGiven our dataset, we have the option to choose between transcript-level, utterance-level, and wordlevel embeddings. We decide to use utterance-level embeddings.6 We select utterances with at least ten words to ensure better parsing of the transcript and parse the texts to extract all valid utterances.\nSince we are working with audio files, it becomes essential that we can segment them such that we can align them with their corresponding utterances in the text transcript. To achieve this alignment, we have used the Aeneas7 library to per-\n6Transcript-level embeddings are too coarse for our task. We experimented with word-level embeddings but found that the performance degraded.\n7https://www.readbeyond.it/aeneas/\nform the forced alignment. The Forced Alignment algorithm takes as input a text file divided into fragments and an unfragmented audio file. It processes the input to output a synchronization map, which automatically associates a time interval in the audio file to its corresponding text fragment. Aeneas uses the Sakoe-Chiba Band Dynamic Time Warping (DTW) (Sakoe and Chiba, 1978) forced alignment algorithm, which has been proven to improve discrimination between words and has superior performance over other conventional algorithms."
    }, {
      "heading" : "5 Methodology",
      "text" : ""
    }, {
      "heading" : "5.1 Text and Audio Encoding",
      "text" : "Text Encoding We compute an utterance’s textual encoding as the arithmetic mean of all its word vectors. BERT is well known as an effective pretrained language-based model for extracting wordembeddings (Biswas et al., 2020) for a variety of language modeling tasks. We use Uncased Base BERT (Devlin et al., 2019) to extract the word embeddings. For each call, we represent the text utterances as [t1, t2, . . . , tN ]. As seen from Figure 4, we embed each text utterance ti to get its corresponding 768-dimensional text encoding gi using BERT such that gi = BERT(ti) for each i ∈ [1, N ].\nAudio Encoding We use the OpenSMILE8 library to extract the audio features at a sampling rate of 10ms and choose the set of 62 geMAPS features described in (Eyben et al., 2016). This set includes features like pitch, jitter, loudness, etc., which have proven to be effective in audio analysis tasks (Chao et al., 2015). For each call, we represent the audio clips of the utterances as [a1, a2, . . . , aN ]. We embed each audio utterance ai to its corresponding 62- dimensional encoding hi using OpenSMILE such that hi = OpenSMILE(ai) for each i ∈ [1, N ].\n8https://pypi.org/project/opensmile/\nMotivation for Speaker Information Infusion The audio encodings help decipher the vocal cues in the text transcript’s context to support or discredit the speaker’s claims. However, it is critical for the system to recognize the importance of the utterance’s speaker to gauge its impact on financial predictions. This requires the information about the speaker of each utterance to be augmented to the input. Prior research (Zhang et al., 2019; Li et al., 2020) shows the addition of speaker context helps improve prediction performance on tasks involving datasets with spoken texts.\nM&A calls have utterances spoken by the company’s management (the decision-making force of the company), by analysts (who want to gauge the risk in the company’s decisions), or even just the operator (often an impartial person). Capturing this speaker context will allow us to decide how much impact a specific utterance can have on a company’s stock price. Thus, we extract the speaker information for each utterance. We parse the list of speakers from the transcripts and assign an ID to each of the speakers. The IDs start from 1 and are assigned incrementally to each speaker in the order in which they are listed. The operator of the call is assigned the ID 0. We then annotate each of the utterances based on who spoke it. Finally, we use one-hot encoding to represent the speaker encoding s of each utterance in the call.\n5.2 M3ANet: Speaker Transformer\nThe Transformer (Vaswani et al., 2017) uses multihead attention and position embeddings to learn the relationship between different utterances. The multimodal input requires the model to learn the inter-dependencies between the audio and the text features. M3ANet can then use the audio cues to affirm or discredit the spoken message and make an informed prediction. The idea behind M3ANet is to use attention to weigh the importance of each modality at different timestamps. We then aug-\nment the data with the speaker encoding and allow the Transformer to extract the multimodal interdependencies for performing the prediction tasks.\nAttention-Fusion Before we can fuse the inputs, we need to linearly transform the text embeddings to ensure the multimodal embeddings’ sizes are the same. We then extract the attention weights to calculate the attended inputs similar to (Hori et al., 2017). These attention weights describe the importance of a specific modality concerning the other modality. We multiply the text and audio features by their attention weights WT and WA respectively to get the attended input, followed by fusing them. The following equations formalize the attention mechanism used:\nWT = softmax(gWwt + bwt) (3)\nWA = softmax(hWwa + bwa) (4)\nWT = WT\nWT +WA ,WA = WA WT +WA\n(5)\nXfused = gWT + hWA (6)\nwhere Wwt and bwt represent the text attention layer, Wwa and bwa represent the audio attention layer and + represents addition.\nSentence-Level Transformer To model the sequence of textual and audio embeddings of the M&A calls, we augment the fused multimodal embeddings Xfused with position embeddings pos by addition and the speaker information by concatenation (represented by ⊕). pos has the same dimensions as Xfused, posj,ind represents the value of the positional embedding for the jth utterance at index ind. The augmentation is summarised as follows:\nposj,2l, posj,2l+1 = sin ( j\n10 8l d\n) , cos ( j\n10 8l d ) (7)\nXfinal = (Xfused + pos)⊕ s (8)\nThe Transformer block uses the augmented feature set for further processing, following which the intermediate tensors are passed through two consecutive dense layers to output the task prediction as follows:\nO1 = ReLU(Wl1I1 + bl1) (9)\ny = σ(Wl2O1 + bl2) (10)\nwhere, Wl1 and bl1 represent the first linear layer, Wl2 and bl2 represent the second linear layer, I1\nand O1 represent the input to the first and second dense layer after being passed through the sentence transformer, while σ represents the final activation function and y represents the final prediction from the activation corresponding to the task. We use ReLU for the final prediction in the volatility prediction task and sigmoid for the price prediction task. We then use Mean Squared Error (MSE) and Binary Cross-Entropy (BCE) losses to train the output for volatility prediction and stock price movement prediction, respectively."
    }, {
      "heading" : "6 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "6.1 Baselines",
      "text" : "We compare M3ANet against modern baselines across modalities for both the tasks. We employ GloVe (Pennington et al., 2014), FinBERT (Araci, 2019) and Roberta (Liu et al., 2019) to embed the text and choose an LSTM + Dense layer architecture as a benchmark for both volatility and price movement prediction. We also use all three (text, audio, and multimodal) variants of the Multimodal Deep Regression Model (MDRM) (Qin and Yang, 2019) as baselines."
    }, {
      "heading" : "6.2 Training Setup",
      "text" : "We tune M3ANet’s hyper-parameters using Grid Search. We summarize the range of hyperparameters tuned on: size of the transformer’s feedforward layer and size of the linear layers ∈ {16, 32, 64}, dropout δ ∈ {0.0, 0.1, 0.25, 0.5}, batch size b ∈ {32, 64, 128} and learning rate e ∈ {0.1, 0.01, 0.001, 0.0001}. The experiment results in the following optimal choices of the hyper-parameters: b = 64, e = 0.001, feed forward network size (Volatility) = 16, hidden layer size (Volatility) = 16 and δ (Volatility) = 0.1, , feed forward network size (Movement) = 64, hidden layer size (Movement) = 32, δ (Movement) = 0.0.\nWe implement all methods with Keras9 and Google Colab.10, using ReLU as our hidden layer activation function and optimize using Adam. We choose the highest performing model during the training phase on our validation set and chosen evaluation metrics as our best model. We zero-pad the calls that have less than the maximum number of utterances/speakers for efficient batching. We experiment with trading periods τ ∈ {3, 7, 15}\n9https://keras.io/ 10https://research.google.com/\ncolaboratory/\ndays allowing experimentation across both short and medium-term periods.\nSimilar to prior work (Sawhney et al., 2020d; Theil et al., 2019; Yang et al., 2020), we evaluate predicted volatility using the mean squared error (MSE) for each hold period, n ∈ {3,7,15}. For the classification task, we report the F1 score and Mathew’s Correlation Coefficient (MCC) for the classification task (Matthews, 1975). We use MCC because, unlike the F1 score, MCC avoids bias due to any data skew that may be present as it does not depend on the choice of the positive class. For a\ngiven confusion matrix ( tp fn fp tn ) :\nMCC = tp× tn− fp× fn√\n(tp+ fp)(tp+ fn)(tn+ fp)(tn+ fn) (11)"
    }, {
      "heading" : "7 Results and Analysis",
      "text" : ""
    }, {
      "heading" : "7.1 Performance Comparison",
      "text" : "As shown in Table 1, M3ANet achieves the best performance for both the volatility prediction and the price prediction task. We observe improvements using M3ANet (Table 2) that leverages the text and audio modalities along with speaker information. This improvement can be attributed to attention to emphasize the importance of each modality throughout the series of utterances. It can also be observed that the improvements our architecture results in are not quite large in magnitude. We attribute this to the difficulty that the task inherently possesses. Further research in more\nsophisticated models may result in greater improvements in the performance on M3A."
    }, {
      "heading" : "7.2 Multimodal and Multi-Speaker Learning",
      "text" : "From Table 1 and Table 2, we see that in both the MDRM and Transformer models, the multimodal models performed much better than the unimodal counterparts. This performance improvement follows from previous research (Qin and Yang, 2019) with respect to volatility prediction. Similar observations validate our hypothesis that audio cues provide additional information that helps make a better prediction. It is also apparent from Table 2 that adding speaker context improves the prediction result consistently. Thus, we infer that speaker information does play an essential part in forecasting and adds to the data’s richness."
    }, {
      "heading" : "7.3 Ablation Study: Fusion",
      "text" : "We experiment with fusion by concatenation and fusion by attention for the Transformer and find the latter performing better in most cases (Table 2). We believe this happens because simple fusion techniques cannot produce features that effectively capture the individual modalities’ importance. However, attention fusion uses weights for both the modalities, learned by the architecture, to determine the importance of each modality with respect to its counterpart. Using these weights to perform a weighted addition gives a much better representation of both the modalities and their particular importance in a fused vector.\n−4 −2 0 2 4\n160\n170\n180\n190\n200\n210\nSentence Number\nM ea\nn Pi\ntc h\nSentence 1 Sentence 2\n(a) QA1: The CEO answers a question about the company’s competitors. Sentence 2: The CEO invites questions."
    }, {
      "heading" : "7.4 Performance Drift over Time",
      "text" : "As observed in previous works (Sawhney et al., 2020d) using earnings calls, Figure 6 shows that short-term stock volatility prediction is more complex, possibly due to the erratic price fluctuations after a M&A call. We hypothesize that these price fluctuations settle as more time elapses, similar to the phenomenon of PEAD (Post Earnings Announcement Drift) (Bernard and Thomas, 1989; Bhushan, 1994; Sadka, 2006). This saturation in performance improvement can be attributed to the dilution of cues extracted from the calls, as we ’drift’ away from them. However, it can be noted that a similar trend may not necessarily be true for price movement prediction."
    }, {
      "heading" : "7.5 Merger & Acquisition Transfer",
      "text" : "We experiment by training M3ANet on Mergers and Acquisitions calls separately, and testing both models on each set of calls separately. From Table 3, it can be observed that both models predict the price movement better for their respective sets as expected. It is surprising to see that the models predict volatility of Acquisition calls relatively better than that of Merger calls. This suggests that Acquisition conference calls lead to a volatility that’s relatively easier to predict and seems to be an avenue for further research."
    }, {
      "heading" : "7.6 Qualitative Analysis",
      "text" : "Call 1: Acquisition of Shape Security by F5 Networks Inc Following the call, F5 Networks Inc suffered a price drop of up to 5.2% within the next month. Studying the call’s vocal cues, we notice (Figure 5a) the CEO had sudden peaks in the mean pitch of his audio while answering questions. Similar peaks occurred when a participant asks the CEO about their fraud protection when compared to their competitors. Prior research on audio analysis (Jiang and Pell, 2017) proves a high mean pitch may indicate a lack of confidence in the speaker. It was later ascertained that F5 had overpaid to acquire Shape Security without proper due diligence of fraud protection plans sold by Shape Security. We observe how M3ANet successfully predicts the decrease in price for all choices of τ while the unimodal models fail to do the same each time. Though the text reveals no lack of confidence, the audio cues likely allow the model to make a successful prediction.\nCall 2: Merger of AK Steel Holding Corporation and Cleveland-Cliffs Inc Following the\nmerger call, Cleveland-Cliffs Inc saw an increase in their stock price up to 17.9% in the next five days. Similar to the first call, we notice spikes and sudden increases in the audios’ mean pitch from Figure 5b. However, the difference exists in the fact that these high pitch patterns come from an analyst in the call and not someone holding an influential position in the companies involved. M3ANet can differentiate between the speakers and correctly predicts the price going up, unlike the transformer variant without speaker embeddings. This shows how the augmentation of the multimodal data with the speaker embedding likely benefits the predictive power of M3ANet.\nCall 3: Acquisition of Plateau Excavation Inc by Sterling Construction Company Inc We now analyze this acquisition as an error analysis where M3ANet predicts incorrectly. We see the text transformer performing well on this example and accurately predicting the increase in the stock price for Sterling Construction Company Inc. On the other hand, our multimodal multi-speaker is unable to do the same. Observing the audio cues (Figure 5c), we find a great deal of variance in the mean audio pitch. We attribute the erroneous performance to the potential overfitting of the model or noise in the audio cues."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We present a dataset of M&A calls that can be utilized to predict financial risk following M&A calls. We also present a strong baseline model using multimodal multi-speaker inputs from the M&A calls to perform financial forecasting. M3ANet uses attention-based fusion to leverage the interdependency between the verbal message and the vocal cues. Further, the approach uses speaker information to enrich the input data to determine if the speakers’ vocal cues or verbal messages conflict with others and accounts for the same. Experiments on M3A display the effectiveness of M3ANet. We hope our M3A can enable more academic progress in the field of financial forecasting.\nEthical Considerations and Limitations\nExamining a speaker’s tone and speech in conference calls is a well-studied task in past literature (Qin and Yang, 2019; Chariri, 2009). Our work focuses only on calls for which companies publicly release transcripts and audio recordings. The data\nused in our study corresponds to M&A conference calls of companies in the NASDAQ stock exchange. We acknowledge the presence of gender bias in our study, given the imbalance in the gender ratio of speakers of the calls. We also acknowledge the demographic bias (Sawhney et al., 2021a) in our study as the companies are organizations within the public stock market of United States of America and may not generalize directly to non-native speakers."
    } ],
    "references" : [ {
      "title" : "Finbert: Financial sentiment analysis with pre-trained language models",
      "author" : [ "Dogu Araci." ],
      "venue" : "ArXiv, abs/1908.10063.",
      "citeRegEx" : "Araci.,? 2019",
      "shortCiteRegEx" : "Araci.",
      "year" : 2019
    }, {
      "title" : "The sound of trustworthiness: Acoustic-based modulation of perceived voice personality",
      "author" : [ "Pascal Belin", "Bibi Boehme", "Phil McAleer." ],
      "venue" : "PLOS ONE, 12:e0185651.",
      "citeRegEx" : "Belin et al\\.,? 2017",
      "shortCiteRegEx" : "Belin et al\\.",
      "year" : 2017
    }, {
      "title" : "Postearnings-announcement drift: Delayed price response or risk premium",
      "author" : [ "Victor L. Bernard", "Jacob K. Thomas" ],
      "venue" : "Journal of Accounting Research,",
      "citeRegEx" : "Bernard and Thomas.,? \\Q1989\\E",
      "shortCiteRegEx" : "Bernard and Thomas.",
      "year" : 1989
    }, {
      "title" : "An informational efficiency perspective on the post-earnings announcement drift",
      "author" : [ "Ravi Bhushan." ],
      "venue" : "Journal of Accounting and Economics, 18(1):45 –",
      "citeRegEx" : "Bhushan.,? 1994",
      "shortCiteRegEx" : "Bhushan.",
      "year" : 1994
    }, {
      "title" : "Achieving reliable sentiment analysis in the software engineering domain using bert",
      "author" : [ "E. Biswas", "M.E. Karabulut", "L. Pollock", "K. VijayShanker." ],
      "venue" : "2020 IEEE International Conference on Software Maintenance and Evolution (ICSME), pages 162–",
      "citeRegEx" : "Biswas et al\\.,? 2020",
      "shortCiteRegEx" : "Biswas et al\\.",
      "year" : 2020
    }, {
      "title" : "Twitter mood predicts the stock market",
      "author" : [ "Johan Bollen", "Huina Mao", "Xiao-Jun Zeng." ],
      "venue" : "Journal of Computational Science, 2.",
      "citeRegEx" : "Bollen et al\\.,? 2010",
      "shortCiteRegEx" : "Bollen et al\\.",
      "year" : 2010
    }, {
      "title" : "Generalized autoregressive conditional heteroskedasticity",
      "author" : [ "Tim Bollerslev." ],
      "venue" : "Journal of Econometrics, 52:5–59.",
      "citeRegEx" : "Bollerslev.,? 1986",
      "shortCiteRegEx" : "Bollerslev.",
      "year" : 1986
    }, {
      "title" : "Do conference calls affect analyst forecasts",
      "author" : [ "Robert Bowen", "Angela Davis", "Dawn Matsumoto." ],
      "venue" : "The Accounting Review, 77.",
      "citeRegEx" : "Bowen et al\\.,? 2001",
      "shortCiteRegEx" : "Bowen et al\\.",
      "year" : 2001
    }, {
      "title" : "Long short term memory recurrent neural network based multimodal dimensional emotion recognition",
      "author" : [ "Linlin Chao", "Jianhua Tao", "Minghao Yang", "Ya Li", "Zhengqi Wen." ],
      "venue" : "Proceedings of the 5th International Workshop on Audio/Visual Emotion Chal-",
      "citeRegEx" : "Chao et al\\.,? 2015",
      "shortCiteRegEx" : "Chao et al\\.",
      "year" : 2015
    }, {
      "title" : "Ethical culture and financial reporting: Understanding financial reporting practice within javanese perspective",
      "author" : [ "Anis Chariri." ],
      "venue" : "Issues In Social And Environmental Accounting, 3.",
      "citeRegEx" : "Chariri.,? 2009",
      "shortCiteRegEx" : "Chariri.",
      "year" : 2009
    }, {
      "title" : "Mergers under the microscope: Analysing conference call transcripts",
      "author" : [ "Sudipto Dasgupta", "Jarrad Harford", "Fangyuan Ma", "Daisy Wang", "Haojun Xie." ],
      "venue" : "Available at SSRN 3528016.",
      "citeRegEx" : "Dasgupta et al\\.,? 2020",
      "shortCiteRegEx" : "Dasgupta et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep learning for event-driven stock prediction",
      "author" : [ "Xiao Ding", "Yue Zhang", "T. Liu", "Junwen Duan." ],
      "venue" : "IJCAI.",
      "citeRegEx" : "Ding et al\\.,? 2015",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2015
    }, {
      "title" : "Equilibrium portfolio strategies in the presence of sentiment risk and excess volatility",
      "author" : [ "Bernard Dumas", "Alexander Kurshev", "Raman Uppal." ],
      "venue" : "The Journal of Finance, 64:579 – 629.",
      "citeRegEx" : "Dumas et al\\.,? 2009",
      "shortCiteRegEx" : "Dumas et al\\.",
      "year" : 2009
    }, {
      "title" : "Autoregressive conditional heteroscedasticity with estimates of the variance of united kingdom inflation",
      "author" : [ "Robert Engle." ],
      "venue" : "Econometrica, 50.",
      "citeRegEx" : "Engle.,? 1981",
      "shortCiteRegEx" : "Engle.",
      "year" : 1981
    }, {
      "title" : "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective",
      "author" : [ "F. Eyben", "K.R. Scherer", "B.W. Schuller", "J. Sundberg", "E. André", "C. Busso", "L.Y. Devillers", "J. Epps", "P. Laukka", "S.S. Narayanan", "K.P. Truong" ],
      "venue" : null,
      "citeRegEx" : "Eyben et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Eyben et al\\.",
      "year" : 2016
    }, {
      "title" : "Value creation in ma transactions, conference calls, and shareholder protection",
      "author" : [ "R. Fraunhoffer", "H. Kim", "D. Schiereck." ],
      "venue" : "International Journal of Financial Studies, 6:1–21.",
      "citeRegEx" : "Fraunhoffer et al\\.,? 2018",
      "shortCiteRegEx" : "Fraunhoffer et al\\.",
      "year" : 2018
    }, {
      "title" : "A closed-form solution for options with stochastic volatility with applications to bond and currency options",
      "author" : [ "Steven Heston." ],
      "venue" : "Review of Financial Studies, 6:327–43.",
      "citeRegEx" : "Heston.,? 1993",
      "shortCiteRegEx" : "Heston.",
      "year" : 1993
    }, {
      "title" : "Analyzing speech to detect financial misreporting",
      "author" : [ "Jessen Hobson", "William Mayew", "Mohan Venkatachalam." ],
      "venue" : "Journal of Accounting Research, 50.",
      "citeRegEx" : "Hobson et al\\.,? 2011",
      "shortCiteRegEx" : "Hobson et al\\.",
      "year" : 2011
    }, {
      "title" : "Attention-based multimodal fusion for video description",
      "author" : [ "Chiori Hori", "Takaaki Hori", "Teng-Yok Lee", "Ziming Zhang", "Bret Harsham", "John R. Hershey", "Tim K. Marks", "Kazuhiko Sumi." ],
      "venue" : "Proceedings of the IEEE International Conference on Com-",
      "citeRegEx" : "Hori et al\\.,? 2017",
      "shortCiteRegEx" : "Hori et al\\.",
      "year" : 2017
    }, {
      "title" : "What’s really in a deal? evidence from textual analysis",
      "author" : [ "Wenyao Hu", "Thomas Shohfi", "Runzu Wang." ],
      "venue" : "SSRN Electronic Journal.",
      "citeRegEx" : "Hu et al\\.,? 2018",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2018
    }, {
      "title" : "The sound of confidence and doubt",
      "author" : [ "Xiaoming Jiang", "Marc D. Pell." ],
      "venue" : "Speech Communication, 88:106 – 126.",
      "citeRegEx" : "Jiang and Pell.,? 2017",
      "shortCiteRegEx" : "Jiang and Pell.",
      "year" : 2017
    }, {
      "title" : "Option pricing when the variance is changing",
      "author" : [ "Herb Johnson", "David Shanno." ],
      "venue" : "Journal of Financial and Quantitative Analysis, 22:143–151.",
      "citeRegEx" : "Johnson and Shanno.,? 1987",
      "shortCiteRegEx" : "Johnson and Shanno.",
      "year" : 1987
    }, {
      "title" : "Hats: A hierarchical graph attention network for stock movement prediction",
      "author" : [ "Raehyun Kim", "Chan Ho So", "Minbyul Jeong", "Sanghoon Lee", "Jinkyu Kim", "Jaewoo Kang" ],
      "venue" : null,
      "citeRegEx" : "Kim et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2019
    }, {
      "title" : "Predicting risk from financial reports with regression",
      "author" : [ "Shimon Kogan", "Dimitry Levin", "Bryan R. Routledge", "Jacob S. Sagi", "Noah A. Smith." ],
      "venue" : "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American",
      "citeRegEx" : "Kogan et al\\.,? 2009",
      "shortCiteRegEx" : "Kogan et al\\.",
      "year" : 2009
    }, {
      "title" : "Volatility forecast using hybrid neural network models",
      "author" : [ "Werner Kristjanpoller", "Anton Fadic", "Marcel Minutolo." ],
      "venue" : "Expert Systems with Applications, 41:2437–2442.",
      "citeRegEx" : "Kristjanpoller et al\\.,? 2014",
      "shortCiteRegEx" : "Kristjanpoller et al\\.",
      "year" : 2014
    }, {
      "title" : "Hierarchical transformer network for utterance-level emotion recognition",
      "author" : [ "Qingbiao Li", "Chunhua Wu", "Zhe Wang", "Kangfeng Zheng." ],
      "venue" : "Applied Sciences, 10:4447.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Non-stationary Multivariate Time Series Prediction with Selective Recurrent Neural Networks, pages 636–649",
      "author" : [ "Jiexi Liu", "Songcan Chen" ],
      "venue" : null,
      "citeRegEx" : "Liu and Chen.,? \\Q2019\\E",
      "shortCiteRegEx" : "Liu and Chen.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Y. Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "M. Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "ArXiv, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "A neural stochastic volatility model",
      "author" : [ "Rui Luo", "Weinan Zhang", "Xiaojun Xu", "Jun Wang" ],
      "venue" : null,
      "citeRegEx" : "Luo et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2017
    }, {
      "title" : "The efficient market hypothesis and its critics",
      "author" : [ "Burton Malkiel." ],
      "venue" : "Journal of Economic Perspectives, 17:59–82.",
      "citeRegEx" : "Malkiel.,? 2003",
      "shortCiteRegEx" : "Malkiel.",
      "year" : 2003
    }, {
      "title" : "Comparison of the predicted and observed secondary structure of t4 phage lysozyme",
      "author" : [ "B.W. Matthews." ],
      "venue" : "Biochimica et Biophysica Acta (BBA) Protein Structure, 405(2):442 – 451.",
      "citeRegEx" : "Matthews.,? 1975",
      "shortCiteRegEx" : "Matthews.",
      "year" : 1975
    }, {
      "title" : "Newscats: A news categorization and trading system",
      "author" : [ "Marc-andre Mittermayer", "G.F. Knolmayer." ],
      "venue" : "pages 1002 – 1007.",
      "citeRegEx" : "Mittermayer and Knolmayer.,? 2007",
      "shortCiteRegEx" : "Mittermayer and Knolmayer.",
      "year" : 2007
    }, {
      "title" : "Do shareholders of acquiring firms gain from acquisitions? SSRN Electronic Journal",
      "author" : [ "Sara Moeller", "Frederik Schlingemann", "Rene Stulz" ],
      "venue" : null,
      "citeRegEx" : "Moeller et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Moeller et al\\.",
      "year" : 2003
    }, {
      "title" : "Stock price prediction using deep learning algorithm and its comparison with machine learning algorithms",
      "author" : [ "Mahla Nikou", "Gholamreza Mansourfar", "J. Bagherzadeh." ],
      "venue" : "Intelligent Systems in Accounting, Finance and Management,",
      "citeRegEx" : "Nikou et al\\.,? 2019",
      "shortCiteRegEx" : "Nikou et al\\.",
      "year" : 2019
    }, {
      "title" : "The impact of microblogging data for stock market prediction: Using twitter to predict returns, volatility, trading volume and survey sentiment indices",
      "author" : [ "Nuno Oliveira", "P. Cortez", "Nelson Areal." ],
      "venue" : "Expert Syst. Appl., 73:125–144.",
      "citeRegEx" : "Oliveira et al\\.,? 2017",
      "shortCiteRegEx" : "Oliveira et al\\.",
      "year" : 2017
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha,",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "What you say and how you say it matters: Predicting stock volatility using verbal and vocal cues",
      "author" : [ "Yu Qin", "Yi Yang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 390–401, Florence, Italy. Associ-",
      "citeRegEx" : "Qin and Yang.,? 2019",
      "shortCiteRegEx" : "Qin and Yang.",
      "year" : 2019
    }, {
      "title" : "Volatility prediction using financial disclosures sentiments with word embedding-based IR models",
      "author" : [ "Navid Rekabsaz", "Mihai Lupu", "Artem Baklanov", "Alexander Dür", "Linda Andersson", "Allan Hanbury." ],
      "venue" : "Proceedings of the 55th Annual Meet-",
      "citeRegEx" : "Rekabsaz et al\\.,? 2017",
      "shortCiteRegEx" : "Rekabsaz et al\\.",
      "year" : 2017
    }, {
      "title" : "Momentum and post-earningsannouncement drift anomalies: The role of liquidity risk",
      "author" : [ "Ronnie Sadka." ],
      "venue" : "Journal of Financial Economics, 80(2):309 – 349.",
      "citeRegEx" : "Sadka.,? 2006",
      "shortCiteRegEx" : "Sadka.",
      "year" : 2006
    }, {
      "title" : "Dynamic programming algorithm optimization for spoken word recognition",
      "author" : [ "H. Sakoe", "S. Chiba." ],
      "venue" : "IEEE Transactions on Acoustics, Speech, and Signal Processing, 26(1):43–49.",
      "citeRegEx" : "Sakoe and Chiba.,? 1978",
      "shortCiteRegEx" : "Sakoe and Chiba.",
      "year" : 1978
    }, {
      "title" : "Deep attentive learning for stock movement prediction from social media text and company correlations",
      "author" : [ "Ramit Sawhney", "Shivam Agarwal", "Arnav Wadhwa", "Rajiv Ratn Shah." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Sawhney et al\\.,? 2020a",
      "shortCiteRegEx" : "Sawhney et al\\.",
      "year" : 2020
    }, {
      "title" : "Spatiotemporal hypergraph convolution network for stock movement forecasting",
      "author" : [ "Ramit Sawhney", "Shivam Agarwal", "Arnav Wadhwa", "Rajiv Ratn Shah." ],
      "venue" : "2020 IEEE International Conference on Data Mining (ICDM), pages 482–491.",
      "citeRegEx" : "Sawhney et al\\.,? 2020b",
      "shortCiteRegEx" : "Sawhney et al\\.",
      "year" : 2020
    }, {
      "title" : "An empirical investigation of bias in the multimodal analysis of financial earnings calls",
      "author" : [ "Ramit Sawhney", "Arshiya Aggarwal", "Rajiv Ratn Shah." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Sawhney et al\\.,? 2021a",
      "shortCiteRegEx" : "Sawhney et al\\.",
      "year" : 2021
    }, {
      "title" : "Multimodal multi-task financial risk forecasting",
      "author" : [ "Ramit Sawhney", "Puneet Mathur", "Ayush Mangal", "Piyush Khanna", "R. Shah", "Roger Zimmermann." ],
      "venue" : "Proceedings of the 28th ACM International Conference on Multimedia.",
      "citeRegEx" : "Sawhney et al\\.,? 2020d",
      "shortCiteRegEx" : "Sawhney et al\\.",
      "year" : 2020
    }, {
      "title" : "Quantitative day trading from natural language using reinforcement learning",
      "author" : [ "Ramit Sawhney", "Arnav Wadhwa", "Shivam Agarwal", "Rajiv Ratn Shah." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Sawhney et al\\.,? 2021b",
      "shortCiteRegEx" : "Sawhney et al\\.",
      "year" : 2021
    }, {
      "title" : "Option pricing when the variance changes randomly: Theory, estimation, and an application",
      "author" : [ "Louis Scott." ],
      "venue" : "Journal of Financial and Quantitative Analysis, 22:419–438.",
      "citeRegEx" : "Scott.,? 1987",
      "shortCiteRegEx" : "Scott.",
      "year" : 1987
    }, {
      "title" : "Paraverbal indicators of deception: A meta-analytic synthesis",
      "author" : [ "Siegfried Sporer", "Barbara Schwandt." ],
      "venue" : "Applied Cognitive Psychology, 20:421 – 446.",
      "citeRegEx" : "Sporer and Schwandt.,? 2006",
      "shortCiteRegEx" : "Sporer and Schwandt.",
      "year" : 2006
    }, {
      "title" : "Profet: Predicting the risk of firms from event transcripts",
      "author" : [ "Kilian Theil", "Samuel Broscheit", "H. Stuckenschmidt." ],
      "venue" : "IJCAI.",
      "citeRegEx" : "Theil et al\\.,? 2019",
      "shortCiteRegEx" : "Theil et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30, pages 5998–6008. Cur-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Financial sentiment analysis for risk prediction",
      "author" : [ "Chuan-Ju Wang", "Ming-Feng Tsai", "Tse Liu", "ChinTing Chang" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2013
    }, {
      "title" : "Speech perception and vocal expression of emotion",
      "author" : [ "Lee Wurm", "Douglas Vakoch", "Maureen Strasser", "Robert Calin-Jageman", "Shannon Ross." ],
      "venue" : "Cognition Emotion, 15:831–852.",
      "citeRegEx" : "Wurm et al\\.,? 2010",
      "shortCiteRegEx" : "Wurm et al\\.",
      "year" : 2010
    }, {
      "title" : "Stock movement prediction from tweets and historical prices",
      "author" : [ "Yumo Xu", "Shay B Cohen." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1970–1979.",
      "citeRegEx" : "Xu and Cohen.,? 2018",
      "shortCiteRegEx" : "Xu and Cohen.",
      "year" : 2018
    }, {
      "title" : "Html: Hierarchical transformerbased multi-task learning for volatility prediction",
      "author" : [ "Linyi Yang", "Tin Lok James Ng", "Barry Smyth", "Ruihai Dong." ],
      "venue" : "Proceedings of The Web Conference 2020.",
      "citeRegEx" : "Yang et al\\.,? 2020",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    }, {
      "title" : "Modeling both context- and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "author" : [ "Dong Zhang", "Liangqing Wu", "Changlong Sun", "Shoushan Li", "Qiaoming Zhu", "Guodong Zhou." ],
      "venue" : "pages 5415–5421.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Stock volatility prediction based on self-attention networks with social information",
      "author" : [ "Jie Zheng", "Andi Xia", "Lin Shao", "Tao Wan", "Zengchang Qin." ],
      "venue" : "pages 1–7.",
      "citeRegEx" : "Zheng et al\\.,? 2019",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "In these M&A conference calls, the participating companies’ management makes a presentation to the call participants, such as market analysts, media personnel, and other stakeholders, explaining the rationale for the deal and possible roadblocks to deal completion (Dasgupta et al., 2020).",
      "startOffset" : 265,
      "endOffset" : 288
    }, {
      "referenceID" : 33,
      "context" : "lies in how the deals may affect the company’s valuation (Moeller et al., 2003; Fraunhoffer et al., 2018) and future growth.",
      "startOffset" : 57,
      "endOffset" : 105
    }, {
      "referenceID" : 16,
      "context" : "lies in how the deals may affect the company’s valuation (Moeller et al., 2003; Fraunhoffer et al., 2018) and future growth.",
      "startOffset" : 57,
      "endOffset" : 105
    }, {
      "referenceID" : 21,
      "context" : "Vocal cues play a critical role in verbal communication as they can provide support or discredit the verbal message that is being spoken (Jiang and Pell, 2017).",
      "startOffset" : 137,
      "endOffset" : 159
    }, {
      "referenceID" : 1,
      "context" : "Vocal cues have been proven indicators of emotions like deceit and nervousness (Belin et al., 2017; Sporer and Schwandt, 2006).",
      "startOffset" : 79,
      "endOffset" : 126
    }, {
      "referenceID" : 47,
      "context" : "Vocal cues have been proven indicators of emotions like deceit and nervousness (Belin et al., 2017; Sporer and Schwandt, 2006).",
      "startOffset" : 79,
      "endOffset" : 126
    }, {
      "referenceID" : 37,
      "context" : "Past research (Qin and Yang, 2019; Sawhney et al., 2020c) shows that the addition of vocal cues has helped with the task of financial predictions and enrich the learned representations.",
      "startOffset" : 14,
      "endOffset" : 57
    }, {
      "referenceID" : 7,
      "context" : "M&A Conference Calls Financial reports and conference calls have been shown to have a correlation with the stock market and improve financial predictions (Bowen et al., 2001; Kogan et al., 2009).",
      "startOffset" : 154,
      "endOffset" : 194
    }, {
      "referenceID" : 24,
      "context" : "M&A Conference Calls Financial reports and conference calls have been shown to have a correlation with the stock market and improve financial predictions (Bowen et al., 2001; Kogan et al., 2009).",
      "startOffset" : 154,
      "endOffset" : 194
    }, {
      "referenceID" : 10,
      "context" : "Studies have also been carried out specifically for M&A calls, showing their effect on the market (Dasgupta et al., 2020; Hu et al., 2018).",
      "startOffset" : 98,
      "endOffset" : 138
    }, {
      "referenceID" : 20,
      "context" : "Studies have also been carried out specifically for M&A calls, showing their effect on the market (Dasgupta et al., 2020; Hu et al., 2018).",
      "startOffset" : 98,
      "endOffset" : 138
    }, {
      "referenceID" : 25,
      "context" : "6753 Financial Forecasting Research has shown historical pricing data to be useful in predicting financial risk modeling (Kristjanpoller et al., 2014; Zheng et al., 2019; Dumas et al., 2009).",
      "startOffset" : 121,
      "endOffset" : 190
    }, {
      "referenceID" : 55,
      "context" : "6753 Financial Forecasting Research has shown historical pricing data to be useful in predicting financial risk modeling (Kristjanpoller et al., 2014; Zheng et al., 2019; Dumas et al., 2009).",
      "startOffset" : 121,
      "endOffset" : 190
    }, {
      "referenceID" : 13,
      "context" : "6753 Financial Forecasting Research has shown historical pricing data to be useful in predicting financial risk modeling (Kristjanpoller et al., 2014; Zheng et al., 2019; Dumas et al., 2009).",
      "startOffset" : 121,
      "endOffset" : 190
    }, {
      "referenceID" : 17,
      "context" : "It also considers volatility as an indicator of uncertainty, which helps make decisions regarding investments (Heston, 1993; Johnson and Shanno, 1987; Scott, 1987).",
      "startOffset" : 110,
      "endOffset" : 163
    }, {
      "referenceID" : 22,
      "context" : "It also considers volatility as an indicator of uncertainty, which helps make decisions regarding investments (Heston, 1993; Johnson and Shanno, 1987; Scott, 1987).",
      "startOffset" : 110,
      "endOffset" : 163
    }, {
      "referenceID" : 46,
      "context" : "It also considers volatility as an indicator of uncertainty, which helps make decisions regarding investments (Heston, 1993; Johnson and Shanno, 1987; Scott, 1987).",
      "startOffset" : 110,
      "endOffset" : 163
    }, {
      "referenceID" : 27,
      "context" : "Previous work often use numerical features (Liu and Chen, 2019; Nikou et al., 2019) in approaches like neural networks (Kim et al.",
      "startOffset" : 43,
      "endOffset" : 83
    }, {
      "referenceID" : 34,
      "context" : "Previous work often use numerical features (Liu and Chen, 2019; Nikou et al., 2019) in approaches like neural networks (Kim et al.",
      "startOffset" : 43,
      "endOffset" : 83
    }, {
      "referenceID" : 23,
      "context" : ", 2019) in approaches like neural networks (Kim et al., 2019; Luo et al., 2017), graph neural networks (Sawhney et al.",
      "startOffset" : 43,
      "endOffset" : 79
    }, {
      "referenceID" : 29,
      "context" : ", 2019) in approaches like neural networks (Kim et al., 2019; Luo et al., 2017), graph neural networks (Sawhney et al.",
      "startOffset" : 43,
      "endOffset" : 79
    }, {
      "referenceID" : 42,
      "context" : ", 2017), graph neural networks (Sawhney et al., 2020b), and time-series models (Bollerslev, 1986; Engle, 1981).",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 50,
      "context" : "Advances in NLP have been utilized in many approaches to show financial information significantly improving performance in forecasting tasks like volatility and stock price prediction (Wang et al., 2013; Ding et al., 2015; Mittermayer and Knolmayer, 2007).",
      "startOffset" : 184,
      "endOffset" : 255
    }, {
      "referenceID" : 12,
      "context" : "Advances in NLP have been utilized in many approaches to show financial information significantly improving performance in forecasting tasks like volatility and stock price prediction (Wang et al., 2013; Ding et al., 2015; Mittermayer and Knolmayer, 2007).",
      "startOffset" : 184,
      "endOffset" : 255
    }, {
      "referenceID" : 32,
      "context" : "Advances in NLP have been utilized in many approaches to show financial information significantly improving performance in forecasting tasks like volatility and stock price prediction (Wang et al., 2013; Ding et al., 2015; Mittermayer and Knolmayer, 2007).",
      "startOffset" : 184,
      "endOffset" : 255
    }, {
      "referenceID" : 5,
      "context" : "Research has also shown that social media affects the stock market (Bollen et al., 2010; Oliveira et al., 2017; Sawhney et al., 2020a).",
      "startOffset" : 67,
      "endOffset" : 134
    }, {
      "referenceID" : 35,
      "context" : "Research has also shown that social media affects the stock market (Bollen et al., 2010; Oliveira et al., 2017; Sawhney et al., 2020a).",
      "startOffset" : 67,
      "endOffset" : 134
    }, {
      "referenceID" : 41,
      "context" : "Research has also shown that social media affects the stock market (Bollen et al., 2010; Oliveira et al., 2017; Sawhney et al., 2020a).",
      "startOffset" : 67,
      "endOffset" : 134
    }, {
      "referenceID" : 24,
      "context" : "Machine learning methods using simple bag-of-words features to represent the financial documents used in previous research (Kogan et al., 2009; Rekabsaz et al., 2017) largely ignore the inter-dependencies between the sentences.",
      "startOffset" : 123,
      "endOffset" : 166
    }, {
      "referenceID" : 38,
      "context" : "Machine learning methods using simple bag-of-words features to represent the financial documents used in previous research (Kogan et al., 2009; Rekabsaz et al., 2017) largely ignore the inter-dependencies between the sentences.",
      "startOffset" : 123,
      "endOffset" : 166
    }, {
      "referenceID" : 53,
      "context" : "To fill the gap, recent approaches have moved towards newer models such as transformers (Yang et al., 2020) and reinforcement learning (Sawhney et al.",
      "startOffset" : 88,
      "endOffset" : 107
    }, {
      "referenceID" : 45,
      "context" : ", 2020) and reinforcement learning (Sawhney et al., 2021b) over natural language data for financial forecasting.",
      "startOffset" : 35,
      "endOffset" : 58
    }, {
      "referenceID" : 30,
      "context" : "Multimodality and Financial Forecasting Research shows that psychological and behavioral elements are often indicators of stock price movement (Malkiel, 2003).",
      "startOffset" : 143,
      "endOffset" : 158
    }, {
      "referenceID" : 51,
      "context" : "Vocal cues have been proven effective in portraying these elements (Wurm et al., 2010; Hobson et al., 2011; Jiang and Pell, 2017).",
      "startOffset" : 67,
      "endOffset" : 129
    }, {
      "referenceID" : 18,
      "context" : "Vocal cues have been proven effective in portraying these elements (Wurm et al., 2010; Hobson et al., 2011; Jiang and Pell, 2017).",
      "startOffset" : 67,
      "endOffset" : 129
    }, {
      "referenceID" : 21,
      "context" : "Vocal cues have been proven effective in portraying these elements (Wurm et al., 2010; Hobson et al., 2011; Jiang and Pell, 2017).",
      "startOffset" : 67,
      "endOffset" : 129
    }, {
      "referenceID" : 53,
      "context" : "Thus, it is no surprise that multimodal architectures that use these cues for financial predictions have seen significant improvements in their performances (Yang et al., 2020; Sawhney et al., 2020d).",
      "startOffset" : 157,
      "endOffset" : 199
    }, {
      "referenceID" : 44,
      "context" : "Thus, it is no surprise that multimodal architectures that use these cues for financial predictions have seen significant improvements in their performances (Yang et al., 2020; Sawhney et al., 2020d).",
      "startOffset" : 157,
      "endOffset" : 199
    }, {
      "referenceID" : 54,
      "context" : "Speaker Context Encoding Past research (Zhang et al., 2019; Li et al., 2020) in fields like emotion recognition have seen the improved performance on their prediction tasks with the addition of speaker context.",
      "startOffset" : 39,
      "endOffset" : 76
    }, {
      "referenceID" : 26,
      "context" : "Speaker Context Encoding Past research (Zhang et al., 2019; Li et al., 2020) in fields like emotion recognition have seen the improved performance on their prediction tasks with the addition of speaker context.",
      "startOffset" : 39,
      "endOffset" : 76
    }, {
      "referenceID" : 24,
      "context" : "Measuring stock volatility Following (Kogan et al., 2009), we formulate stock volatility as a regression problem.",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 52,
      "context" : "Formalizing price movement prediction Following (Xu and Cohen, 2018), we define price",
      "startOffset" : 48,
      "endOffset" : 68
    }, {
      "referenceID" : 40,
      "context" : "Aeneas uses the Sakoe-Chiba Band Dynamic Time Warping (DTW) (Sakoe and Chiba, 1978) forced alignment algorithm, which has been proven to improve discrimination between words and has superior per-",
      "startOffset" : 60,
      "endOffset" : 83
    }, {
      "referenceID" : 4,
      "context" : "BERT is well known as an effective pretrained language-based model for extracting wordembeddings (Biswas et al., 2020) for a variety of language modeling tasks.",
      "startOffset" : 97,
      "endOffset" : 118
    }, {
      "referenceID" : 11,
      "context" : "We use Uncased Base BERT (Devlin et al., 2019) to extract the word embeddings.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 15,
      "context" : "Audio Encoding We use the OpenSMILE8 library to extract the audio features at a sampling rate of 10ms and choose the set of 62 geMAPS features described in (Eyben et al., 2016).",
      "startOffset" : 156,
      "endOffset" : 176
    }, {
      "referenceID" : 8,
      "context" : ", which have proven to be effective in audio analysis tasks (Chao et al., 2015).",
      "startOffset" : 60,
      "endOffset" : 79
    }, {
      "referenceID" : 54,
      "context" : "Prior research (Zhang et al., 2019; Li et al., 2020) shows the addition of speaker context helps",
      "startOffset" : 15,
      "endOffset" : 52
    }, {
      "referenceID" : 26,
      "context" : "Prior research (Zhang et al., 2019; Li et al., 2020) shows the addition of speaker context helps",
      "startOffset" : 15,
      "endOffset" : 52
    }, {
      "referenceID" : 49,
      "context" : "The Transformer (Vaswani et al., 2017) uses multihead attention and position embeddings to learn the relationship between different utterances.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 19,
      "context" : "We then extract the attention weights to calculate the attended inputs similar to (Hori et al., 2017).",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 36,
      "context" : "We employ GloVe (Pennington et al., 2014), FinBERT (Araci, 2019) and Roberta (Liu et al.",
      "startOffset" : 16,
      "endOffset" : 41
    }, {
      "referenceID" : 0,
      "context" : ", 2014), FinBERT (Araci, 2019) and Roberta (Liu et al.",
      "startOffset" : 17,
      "endOffset" : 30
    }, {
      "referenceID" : 28,
      "context" : ", 2014), FinBERT (Araci, 2019) and Roberta (Liu et al., 2019) to embed the text and choose an LSTM + Dense layer architecture as a benchmark for both volatility and price movement prediction.",
      "startOffset" : 43,
      "endOffset" : 61
    }, {
      "referenceID" : 37,
      "context" : "We also use all three (text, audio, and multimodal) variants of the Multimodal Deep Regression Model (MDRM) (Qin and Yang, 2019) as baselines.",
      "startOffset" : 108,
      "endOffset" : 128
    }, {
      "referenceID" : 44,
      "context" : "Similar to prior work (Sawhney et al., 2020d; Theil et al., 2019; Yang et al., 2020), we evaluate predicted volatility using the mean squared error (MSE) for each hold period, n ∈ {3,7,15}.",
      "startOffset" : 22,
      "endOffset" : 84
    }, {
      "referenceID" : 48,
      "context" : "Similar to prior work (Sawhney et al., 2020d; Theil et al., 2019; Yang et al., 2020), we evaluate predicted volatility using the mean squared error (MSE) for each hold period, n ∈ {3,7,15}.",
      "startOffset" : 22,
      "endOffset" : 84
    }, {
      "referenceID" : 53,
      "context" : "Similar to prior work (Sawhney et al., 2020d; Theil et al., 2019; Yang et al., 2020), we evaluate predicted volatility using the mean squared error (MSE) for each hold period, n ∈ {3,7,15}.",
      "startOffset" : 22,
      "endOffset" : 84
    }, {
      "referenceID" : 31,
      "context" : "For the classification task, we report the F1 score and Mathew’s Correlation Coefficient (MCC) for the classification task (Matthews, 1975).",
      "startOffset" : 123,
      "endOffset" : 139
    }, {
      "referenceID" : 37,
      "context" : "lows from previous research (Qin and Yang, 2019) with respect to volatility prediction.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 44,
      "context" : "As observed in previous works (Sawhney et al., 2020d) using earnings calls, Figure 6 shows that short-term stock volatility prediction is more complex, possibly due to the erratic price fluctuations after a M&A call.",
      "startOffset" : 30,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : "We hypothesize that these price fluctuations settle as more time elapses, similar to the phenomenon of PEAD (Post Earnings Announcement Drift) (Bernard and Thomas, 1989; Bhushan, 1994; Sadka, 2006).",
      "startOffset" : 143,
      "endOffset" : 197
    }, {
      "referenceID" : 3,
      "context" : "We hypothesize that these price fluctuations settle as more time elapses, similar to the phenomenon of PEAD (Post Earnings Announcement Drift) (Bernard and Thomas, 1989; Bhushan, 1994; Sadka, 2006).",
      "startOffset" : 143,
      "endOffset" : 197
    }, {
      "referenceID" : 39,
      "context" : "We hypothesize that these price fluctuations settle as more time elapses, similar to the phenomenon of PEAD (Post Earnings Announcement Drift) (Bernard and Thomas, 1989; Bhushan, 1994; Sadka, 2006).",
      "startOffset" : 143,
      "endOffset" : 197
    }, {
      "referenceID" : 21,
      "context" : "Prior research on audio analysis (Jiang and Pell, 2017) proves a high mean pitch may indicate a lack of confidence in the speaker.",
      "startOffset" : 33,
      "endOffset" : 55
    }, {
      "referenceID" : 37,
      "context" : "Examining a speaker’s tone and speech in conference calls is a well-studied task in past literature (Qin and Yang, 2019; Chariri, 2009).",
      "startOffset" : 100,
      "endOffset" : 135
    }, {
      "referenceID" : 9,
      "context" : "Examining a speaker’s tone and speech in conference calls is a well-studied task in past literature (Qin and Yang, 2019; Chariri, 2009).",
      "startOffset" : 100,
      "endOffset" : 135
    }, {
      "referenceID" : 43,
      "context" : "We also acknowledge the demographic bias (Sawhney et al., 2021a) in our study as the companies are organizations within the public stock market of United States of America and may not generalize directly to non-native speakers.",
      "startOffset" : 41,
      "endOffset" : 64
    } ],
    "year" : 2021,
    "abstractText" : "Risk prediction is an essential task in financial markets. Merger and Acquisition (M&A) calls provide key insights into the claims made by company executives about the restructuring of the financial firms. Extracting vocal and textual cues from M&A calls can help model the risk associated with such financial activities. To aid the analysis of M&A calls, we curate a dataset of conference call transcripts and their corresponding audio recordings for the time period ranging from 2016 to 2020. We introduce M3ANet, a baseline architecture that takes advantage of the multimodal multispeaker input to forecast the financial risk associated with the M&A calls. Empirical results prove that the task is challenging, with the proposed architecture performing marginally better than strong BERT-based baselines. We release the M3A dataset and benchmark models to motivate future research on this challenging problem domain.",
    "creator" : "LaTeX with hyperref"
  }
}