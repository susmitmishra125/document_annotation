{
  "name" : "2021.acl-long.389.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "VisualSparta: An Embarrassingly Simple Approach to Large-scale Text-to-Image Search with Weighted Bag-of-words",
    "authors" : [ "Xiaopeng Lu", "Tiancheng Zhao", "Kyusong Lee" ],
    "emails" : [ "xiaopen2@andrew.cmu.edu", "tianchez@soco.ai", "kyusongl@soco.ai", "Recall@1" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5020–5029\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5020"
    }, {
      "heading" : "1 Introduction",
      "text" : "Text-to-image retrieval is the task of retrieving a list of relevant images from a corpus given text queries. This task is challenging because in order to find the most relevant images given text query, the model needs to not only have good representations for both textual and visual modalities, but also capture the fine-grained interaction between them.\nExisting text-to-image retrieval models can be broadly divided into two categories: query-agnostic and query-dependent models. The dual-encoder architecture is a common query-agnostic model, which uses two encoders to encode the query ∗ This work was partially done during an internship at SOCO\nand images separately and then compute the relevancy via inner product (Faghri et al., 2017; Lee et al., 2018; Wang et al., 2019a). The transformer architecture is a well-known querydependent model (Devlin et al., 2018; Yang et al., 2019). In this case, each pair of text and image is encoded by concatenating and passing into one single network, instead of being encoded by two separate encoders (Lu et al., 2020; Li et al., 2020b). This method borrows the knowledge from large pretrained transformer models and shows much better accuracy compared to dual-encoder methods (Li et al., 2020b).\nBesides improving the accuracy, retrieval speed has also been a long-existing subject of study in the information retrieval (IR) community (Manning et al., 2008). Query-dependent models are prohibitively slow to apply to the entire image corpus because it needs to recompute for every dif-\nferent query. On the other hand, query-agnostic model is able to scale by pre-computing an image data index. For dual-encoder systems, further speed improvement can be obtained via Approximate Nearest Neighbors (ANN) Search and GPU acceleration (Johnson et al., 2019).\nIn this work, we propose VisualSparta, a simple yet effective text-to-image retrieval model that outperforms all existing query-agnostic retrieval models in both accuracy and speed. By modeling fine-grained interaction between visual regions with query text tokens, our model is able to harness the power of large pre-trained visual-text models and scale to very large datasets with real-time response. To our best knowledge, this is the first model that integrates the power of transformer models with real-time searching, showing that large pre-trained models can be used in a way with significantly less amount of memory and computing time. Lastly, our method is embarrassingly simple because its image representation is essentially a weighted bag-of-words, and can be indexed in a standard Inverted Index for fast retrieval. Comparing to other sophisticated models with distributed vector representations, our method does not depend on ANN or GPU acceleration to scale up to very large datasets.\nContributions of this paper can be concluded as the following: (1) A novel retrieval model that achieves new state-of-the-art results on two benchmark datasets, i.e., MSCOCO and Flickr 30K. (2) Weighted bag-of-words is shown to be an effective representation for cross-modal retrieval that can be efficiently indexed in an Inverted Index for fast retrieval. (3) Detailed analysis and ablation study that show advantages of the proposed method and interesting properties that shine light for future research directions."
    }, {
      "heading" : "2 Related Work",
      "text" : "Large amounts of work have been done on learning a joint representation between texts and images (Karpathy and Fei-Fei, 2015; Huang et al., 2018; Lee et al., 2018; Wehrmann et al., 2019; Li et al., 2020b; Lu et al., 2020). In this section, we revisit dual-encoder based retrieval model and transformer-based retrieval model."
    }, {
      "heading" : "2.1 Dual-encoder Matching Network",
      "text" : "Most of the work in text-to-image retrieval task choose to use the dual-encoder network to en-\ncode information from text and image modalities. In Karpathy and Fei-Fei (2015), the author used a Bi-directional Recurrent Neural Network (BRNN) to encode the textual information and used a Region Convolutional Neural Network (RCNN) to encode the image information, and the final similarity score is computed via the interaction of features from two encoders. Lee et al. (2018) proposed stacked cross-attention network, where the text features are passed through two attention layers to learn interactions with the image region. Wang et al. (2019a) encoded the location information as yet another feature and used both deep RCNN features (Ren et al., 2016) and the fine-grained location features for the Region of Interest (ROI) as image representation. In Wang et al. (2020), the author utilized the information from Wikipedia as an external corpus to construct a Graph Neural Network (GNN) to help model the relationships across objects."
    }, {
      "heading" : "2.2 Pre-trained Language Models (PLM)",
      "text" : "Large pre-trained language models (PLM) show great success over multiple tasks in NLP areas in recent years (Devlin et al., 2018; Yang et al., 2019; Dai et al., 2019). After that, research has also been done on cross-modal transformer-based models and proves that the self-attention mechanism also helps jointly capture visual-text relationships (Li et al., 2019; Lu et al., 2020; Qi et al., 2020; Li et al., 2020b). By first pretraining model under large-scale visual-text dataset, these transformerbased models capture rich semantic information from both texts and images. Models are then finetuned for the text-to-image retrieval task and show improvements by a large margin. However, the problem of using transformer-based models is that it is prohibitively slow in the retrieval context: the model needs to compute pair-wise similarity scores between all queries and answers, making it almost impossible to use the model in any real-world scenarios. Our proposed method borrows the power of large pre-trained models while reducing the inference time by orders of magnitude.\nPLM has shown promising results in Information Retrieval (IR), despite its slow speed due to the complex model structure. The IR community recently started working on empowering the classical full-text retrieval methods with contextualized information from PLMs (Dai and Callan, 2019; MacAvaney et al., 2020; Zhao et al.,\n2020). Dai and Callan (2019) proposed DeepCT, a model that learns to generate the query importance score from the contextualized representation of large transformer-based models. Zhao et al. (2020) proposed sparse transformer matching model (SPARTA), where the model learns termlevel interaction between query and text answers and generates weighted term representations for answers during index time. Our work is motivated by works in this direction and extends the scope to the cross-modal understanding and retrieval."
    }, {
      "heading" : "3 VisualSparta Retriever",
      "text" : "In this section, we present VisualSparta retriever, a fragment-level transformer-based model for efficient text-image matching. The focus of our proposed model is two-fold:\n• Recall performance: fine-grained relationship between queries and image regions are learned to enrich the cross-modal understanding.\n• Speed performance: query embeddings are non-contextualized, which allows the model to put most of the computation offline."
    }, {
      "heading" : "3.1 Model Architecture",
      "text" : ""
    }, {
      "heading" : "3.1.1 Query representation",
      "text" : "As query processing is an online operation during retrieval, the efficiency of encoding query needs to be well considered. Previous methods pass the query sentence into a bi-RNN to give token representation provided surrounding tokens (Lee et al., 2018; Wang et al., 2019a, 2020).\nInstead of encoding the query in a sequential manner, we drop the order information of the query and only use the pretrained token embeddings to represent each token. In other words, we do not encode the local contextual information for the query and purely rely on independent word embedding Etok of each token. Let a query be q = [w1, ..., wm] after tokenization, we have:\nŵi = Etok (wi) (1)\nwhere wi is the i-th token of the query. Therefore, a query is represented as ŵ = {ŵ1, ..., ŵm}, ŵi ∈ RdH . In this way, each token is represented independently and agnostic to its local context. This is essential for the efficient indexing and inference, as described next in section 3.3."
    }, {
      "heading" : "3.1.2 Visual Representation",
      "text" : "Compared with query information which needs to be processed in real-time, answer processing can be rich and complex, as answer corpus can be indexed offline before the query comes. Therefore, we follow the recent works in Vision-Language Transformers (Li et al., 2019, 2020b) and use the contextualized representation for the answer corpus.\nSpecifically, for an image, we represent it using information from three sources: regional visual features, regional location features, and label features with attributes, as shown in Figure 2.\nRegional visual features and location features Given an image v, we pass it through FasterRCNN (Ren et al., 2016) to get n regional visual features vi and their corresponding location features li:\nv1, ..., vn = RCNN(v), vi ∈ Rdrcnn (2)\nand the location features are the normalized top left and bottom right positions of the region proposed from Faster-RCNN, together with the region width and height:\nli = [lxmin, lxmax, lymin, lymax, lwidth, lheight] (3) Therefore, we represent one region by the concatenation of two features:\nEi = [vi; li] (4)\nEimage = [E1, ..., En], Ei ∈ Rdrcnn+dloc (5)\nwhere Eimage is the representation for a single image.\nLabel features with attributes Additional to the deep representations from the proposed image region, previous work by Li et al. (2020b) shows that the object label information is also useful as an additional representation for the image. We also encode the predicted objects and corresponding attributes obtained from Faster-RCNN model with pretrained word embeddings:\nôi = Etok(oi) + Epos(oi) + Eseg(oi) (6)\nElabel = [ô1, ..., ôk], ôi ∈ RdH (7)\nwhere k represents the number of tokens after the tokenization of attributes and object labels for n\nimage regions. Etok, Epos, and Eseg represent token embeddings, position embeddings, and segmentation embeddings respectively, similar to the embedding structure in Devlin et al. (2018).\nTherefore, one image can be represented by the linear transformed image features concatenated with label features:\na = [(EimageW + b);Elabel] (8)\nwhere W ∈ R(drcnn+dloc)×dH and b ∈ RdH are the trainable linear combination weights and bias. The concatenated embeddings a are then passed into a Transformer encoder Timage, and the final image feature is the hidden output of it:\nHimage = Timage(a) (9)\nwhere Himage ∈ R(n+k)×dH is the final contextualized representation for one image."
    }, {
      "heading" : "3.1.3 Scoring Function",
      "text" : "Given the visual and query representations, the matching score can now be computed between a query and an image. Different from other dualencoder based interaction model, we adopt the finegrained interaction model proposed by Zhao et al. (2020) to compute the relevance score by:\nyi = maxj∈[1,n+k](ŵ T i Hj) (10)\nφ(yi) = ReLU(yi + b) (11)\nf(q, v) = m∑ i=1 log(φ(yi) + 1) (12)\nwhere Eq.10 captures the fragment-level interaction between every image region and every query word token; Eq.11 produces sparse embedding outputs via a combination of ReLU and trainable bias, and Eq.12 sums up the score and prevents an overly large score using log operation."
    }, {
      "heading" : "3.2 Retriever training",
      "text" : "Following the training method presented in Zhao et al. (2020), we use cross entropy loss to train VisualSparta. Concretely, we maximize the objective in Eq. 13, which tries to decide between the ground truth image v+ and irrelevant/random images V − for each text query q. The parameters to learn include both the query encoder Etok and the image transformer encoder Timage. Parameters are optimized using Adam (Kingma and Ba, 2014).\nJ = f(q, v+)− log ∑ k∈V − ef(q,k)) (13)\nIn order to achieve efficient training, we use other image samples from the same batch as negative examples for each training data, an effective technique that is widely used in response selection (Zhang et al., 2018; Henderson et al., 2019). Preliminary experiments found that as long as the batch size is large enough (we choose to use batch size of 160), this simple approach performs equally well compared to other more sophisticated methods, for example, sample similar images that have nearby labels."
    }, {
      "heading" : "3.3 Efficient Indexing and Inference",
      "text" : "VisualSparta model structure is suitable for realtime inference. As discussed in section 3.1.1, since query embeddings are non-contextualized, we are able to compute the relationship between each query term wi and every image v offline.\nConcretely, during offline indexing, for each image v, we first compute fragment-level interaction between its regions and every query term in the vocabulary, same as in Eq. 10. Then, we cache the computed ranking score:\nCACHE(w, v) = Eq. 11 (14)\nDuring test time, given a query q = [w1, ..., wm], the ranking score between q and an image v is:\nf(q, v) = m∑ i=1 log(CACHE(wi, v) + 1) (15)\nAs shown in Eq. 15, the final ranking score during inference time is an O(1) look-up operation followed by summation. Also, the query-time computation can be fit into an Inverted Index architecture (Manning et al., 2008), which enables us to use VisualSparta index with off-the-shelf search engines, for example, Elasticsearch (Gheorghe et al., 2015)."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "In this paper, we use MSCOCO (Lin et al., 2014)1 and Flickr30K (Plummer et al., 2015)2 datasets for the training and evaluation of text-to-image retrieval tasks. MSCOCO is a large-scale multitask dataset including object detection, semantic segmentation, and image captioning data. In this experiment, we follow the previous work and use the image captioning data split for text-to-image model training and evaluation. Following the experimental settings from Karpathy and Fei-Fei (2015), we split the data into 113,287 images for training, 5,000 images for validation, and 5,000 images for testing. Each image is paired with 5 different captions. The performance of 1,000 (1K) and 5,000 (5K) test splits are reported and compared with previous results."
    }, {
      "heading" : "1 https://cocodataset.org",
      "text" : ""
    }, {
      "heading" : "2 http://bryanplummer.com/",
      "text" : "Flickr30kEntities\nFlickr30K (Plummer et al., 2015) is another publicly available image captioning dataset, which contains 31,783 images in total. Following the split from Karpathy and Fei-Fei (2015), 29,783 images are used for training, and 1,000 images are used for validation. Scores are reported based on results from 1,000 test images.\nFor speed experiments, in addition to MSCOCO 1K and 5K splits, we create 113K split and 1M split, two new data splits to test the performance in the large-scale retrieval setting. Since these splits are only used for speed experiments, we directly reuse the training data from the existing dataset without the concern of data leaking between training and testing phases. Specifically, the 113K split refers to the MSCOCO training set, which contains 113,287 images, ∼23 times larger than the MSCOCO 5K test set. The 1M split consists of one million images randomly sampled from the MSCOCO training set. Speed experiments are done on these four splits to give comprehensive comparisons under different sizes of image index."
    }, {
      "heading" : "4.2 Evaluation Metrics",
      "text" : "Following previous works, we use recall rate as our accuracy evaluation metrics. In both MSCOCO and Flikr30K datasets, we report Recall@t, t=[1, 5, 10] and compare with previous works.\nFor speed performance evaluation, we choose query per second and latency(ms) as the evaluation metric to test how each model performs in terms of speed under different sizes of image index."
    }, {
      "heading" : "4.3 Implementation Details",
      "text" : "All experiments are done using the PyTorch library. During training, one NVIDIA Titan X GPU is used. During speed performance evaluation, one NVIDIA Titan X GPU is used for models that need GPU acceleration. One 10-core Intel 9820X CPU is used for models that needs CPU acceleration. For the image encoder, we initialize the model weights from Oscar-base model (Li et al., 2020b) with 12 layers, 768 hidden dimensions, and 110M parameters. For the query embedding, we initialize it from the Oscar-base token embedding. The Adam optimizer (Kingma and Ba, 2014) is used with the learning rate set to 5e-5. The number of training epochs is set to 20. The input sequence length is set to 120, with 70 for labels with attributes features and 50 for deep visual features. We search on batch sizes (96, 128, 160) with Recall@1 validation accuracy, and set the batch size to 160."
    }, {
      "heading" : "4.4 Experimental Results",
      "text" : "We compare both recall and speed performance with the current state-of-the-art retrieval model in text-to-image search. Query-dependent model refers to models in which image information cannot be encoded offline, because each image encoding is dependent on the query information. These models usually achieve promising performance in recall but suffer from prohibitively slow inference speed. Query-agnostic model refers to models in which image information can be encoded offline and is independent of query information. In section 4.4.1 and 4.4.2, we evaluate accuracy and speed performance respectively for both lines of methods."
    }, {
      "heading" : "4.4.1 Recall Performance",
      "text" : "As shown in Table 1, the results reveal that our model is competitive compared with previous methods. Among query-agnostic methods, our model is significantly superior to the state-of-the-art results in all evaluation metrics over both MSCOCO and Flickr30K datasets and outperforms previous methods by a large margin. Specifically, in MSCOCO 1K test set, our model outperforms the previously best query-agnostic method (Wang et al., 2019a) by 7.1%, 1.6%, 1.0% for Recall@1, 5, 10 respectively. In Flickr30K dataset, VisualSparta also shows strong improvement compared with the previous best method: in Recall@1,5,10, our model gets 4.2%, 2.2%, 0.4% improvement respectively.\nWe also observe that VisualSparta reduces the gap by a large margin between query-agnostic and query-dependent methods. In MSCOCO-1K split, the performance of VisualSparta is only 1.0%, 2.3%, 1.0% lower than Unicoder-VL method (Li et al., 2020a) for Recall@1,5,10 respectively. Compared to Oscar (Li et al., 2020b), the current stateof-the-art query-dependent model, our model is 7% lower than the Oscar model in MSCOCO-1K Re-\ncall@1. This shows that there is still room for improvement in terms of accuracy for query-agnostic model."
    }, {
      "heading" : "4.4.2 Speed Performance",
      "text" : "To show the efficiency of VisualSparta model in both small-scale and large-scale settings, we create 113K dataset and 1M dataset in addition to the original 1K and 5K test split, as discussed in section 4.2. Speed experiments are done using these four splits as testbeds.\nTo make a fair comparison, we benchmark each method with its preferred hardware and software for speed acceleration. Specifically, For CVSE model (Wang et al., 2020), both CPU and GPU inference time are recorded. For CPU setting, the Maximum Inner Product Search (MIPS) is performed using their original code based on Numpy (Harris et al., 2020). For GPU setting, we adopt the model and use FAISS (Johnson et al., 2019), an optimized MIPS library, to test the speed performance. For Oscar model (Li et al., 2020b), since the query-dependent method cannot be formulated as a MIPS problem, we run the original model using GPU acceleration and record the speed. For VisualSparta, we use the top-1000 term scores settings for the experiment. Since VisualSparta can be fit into an inverted-index architecture, GPU ac-\nceleration is not required. For all experiments, we use 5000 queries from MSCOCO-1K split as query input to test the speed performance.\nAs we can see from Table 2, in all four data splits (1K, 5K, 113K, 1M), VisualSparta significantly outperforms both the best query-agnostic model (CVSE (Wang et al., 2020)) and the best querydependent model (Oscar (Li et al., 2020b)). Under CPU comparison, the speed of VisualSparta is 2.5, 2.4, 51, and 391 times faster than that of the CVSE model in 1K, 5K, 113K, and 1M splits respectively.\nThis speed advantage also holds even if previous models are accelerated with GPU acceleration. To apply the latest MIPS progress to the comparison, we adopt the CVSE model to use FAISS (Johnson et al., 2019) for better speed acceleration. Results in the table reveal that the speed of VisualSparta can also beat that of CVSE by 2.5X in the 1K setting, and this speed advantage increases to 5.4X when the index size increases to 1M.\nOur model holds an absolute advantage when comparing speed to query-dependent models such as Oscar (Li et al., 2020b). Since the image encoding is dependent on the query information, no offline indexing can be done for the query-dependent model. As shown in Table 2, even with GPU acceleration, Oscar model is prohibitively slow: In the 1K setting, Oscar is ∼1128 times slower than VisualSparta. The number increases to 391,000 when index size increases to 1M."
    }, {
      "heading" : "5 Model Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Speed-Accuracy Flexibility",
      "text" : "As described in section 3.3, each image can be well represented by a list of weighted tokens independently. This feature makes VisualSparta flexible during indexing time: users can choose to index using top-n term scores based on their memory constraint or speed requirement.\nTable 3 compares recall and speed in both MSCOCO 1K and 5K split under different choices\nof n. From the comparison between using all term scores and using top-2000 term scores, we found that VisualSparta can get ∼1.8X speedup with almost no performance drop. if higher speed is needed, n can always be set to a lower number with a sacrifice of accuracy, as shown in Table 3.\nFigure 1 visualizes the trade-off between model accuracy and inference speed. The x-axis represents the average inference time of a single query in millisecond, and the y-axis denotes the Recall@1 on MSCOCO 1K test set. For VisualSparta, each dot represents the model performance under certain top-n term score settings. For other methods, each dot represents their speed and accuracy performance. The curve reveals that with larger n, the recall becomes higher and the speed gets slower. From the comparison between VisualSparta and other methods, we observe that by setting top-n term scores to 500, VisualSparta can already beat the accuracy performance of both PFAN (Wang et al., 2019a) and CVSE (Wang et al., 2020) with ∼2.8X speedup."
    }, {
      "heading" : "5.2 Ablation Study on Image Encoder",
      "text" : "As shown in Figure 2, the image encoder takes a concatenation of object label features with attributes and deep visual features as input. In this section, we do an ablation study and analyze the contributions of each part of the image features to the final score.\nIn Table 4, different components are removed from the image encoder for performance comparison. From the table, we observe that removing either attributes features (row 1) or label features with attributes (row 2) only hurts the performance by a small margin. However, when dropping visual features and only using label with attributes features for image representation (row 3), it appears that the model performance drops by a large margin, where the Recall@1 score drops from 68.7% to 49.1%(−19.6%).\nFrom this ablation study, we can conclude that\ndeep visual features make the most contribution to the VisualSparta model structure, which shows that deep visual features are significantly more expressive compared to textual features, i.e., label with attributes features. More importantly, it shows that VisualSparta is capable of learning cross-modal knowledge, and the biggest gain indeed comes from learning to match query term embeddings with deep visual representations."
    }, {
      "heading" : "5.3 Cross-domain Generalization",
      "text" : "Table 5 shows the cross-domain performance for different models. All models are trained on MSCOCO and tested on Flickr30K. We can see from the table that VisualSparta consistently outperforms other models in this setting. This indicates that the performance of VisualSparta is consistent\nacross different data distributions, and the performance gain compared to other models is also consistent when testing in this cross-dataset settings."
    }, {
      "heading" : "5.4 Qualitative Examples",
      "text" : "We query VisualSparta on the MSOCO 113K split and check the results. As shown in Figure 3, visual and label features together represent the max attended features for given query tokens. Interestingly, we observe that VisualSparta model is capable of grounding adjectives and verbs to the relevant image regions. For example, “graz” grounds to the head of giraffe in the first example. This further confirms the hypothesis that weighted bag-ofwords is a valid and rich representation for images."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In conclusion, this paper presents VisualSparta, an accurate and efficient text-to-image retrieval model that shows the state-of-the-art scalable performance in both MSCOCO and Flickr30K. Its main novelty lies in the combination of powerful pre-trained image encoder with fragment-level scoring. Detailed analysis also demonstrates that our approach has substantial scalability advantages compared to previous best methods when indexing large image\ndatasets for real-time searching, making it suitable for real-world deployment."
    } ],
    "references" : [ {
      "title" : "Context-aware sentence/passage term importance estimation for first stage retrieval",
      "author" : [ "Zhuyun Dai", "Jamie Callan" ],
      "venue" : "arXiv preprint arXiv:1910.10687,",
      "citeRegEx" : "Dai and Callan.,? \\Q2019\\E",
      "shortCiteRegEx" : "Dai and Callan.",
      "year" : 2019
    }, {
      "title" : "Transformer-xl: Attentive language models beyond a fixed-length context",
      "author" : [ "Zihang Dai", "Zhilin Yang", "Yiming Yang", "Jaime Carbonell", "Quoc V Le", "Ruslan Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "Dai et al\\.,? \\Q1901\\E",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 1901
    }, {
      "title" : "Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova. Bert" ],
      "venue" : "arXiv preprint arXiv:1810.04805,",
      "citeRegEx" : "Devlin et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Vse++: Improving visual-semantic embeddings with hard negatives",
      "author" : [ "Fartash Faghri", "David J Fleet", "Jamie Ryan Kiros", "Sanja Fidler" ],
      "venue" : "arXiv preprint arXiv:1707.05612,",
      "citeRegEx" : "Faghri et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Faghri et al\\.",
      "year" : 2017
    }, {
      "title" : "Elasticsearch in action",
      "author" : [ "Radu Gheorghe", "Matthew Lee Hinman", "Roy Russo" ],
      "venue" : null,
      "citeRegEx" : "Gheorghe et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gheorghe et al\\.",
      "year" : 2015
    }, {
      "title" : "Convert: Efficient and accurate conversational representations from transformers",
      "author" : [ "Matthew Henderson", "Iñigo Casanueva", "Nikola Mrkšić", "Pei-Hao Su", "Tsung-Hsien Wen", "Ivan Vulić" ],
      "venue" : null,
      "citeRegEx" : "Henderson et al\\.,? \\Q1911\\E",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 1911
    }, {
      "title" : "Instanceaware image and sentence matching with selective multimodal lstm",
      "author" : [ "Yan Huang", "Wei Wang", "Liang Wang" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Huang et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2017
    }, {
      "title" : "Billion-scale similarity search with gpus",
      "author" : [ "Jeff Johnson", "Matthijs Douze", "Hervé Jégou" ],
      "venue" : "IEEE Transactions on Big Data,",
      "citeRegEx" : "Johnson et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep visual-semantic alignments for generating image descriptions",
      "author" : [ "Andrej Karpathy", "Li Fei-Fei" ],
      "venue" : "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
      "citeRegEx" : "Karpathy and Fei.Fei.,? \\Q2015\\E",
      "shortCiteRegEx" : "Karpathy and Fei.Fei.",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Stacked cross attention for imagetext matching",
      "author" : [ "Kuang-Huei Lee", "Xi Chen", "Gang Hua", "Houdong Hu", "Xiaodong He" ],
      "venue" : "In Proceedings of the European Conference on Computer Vision (ECCV),",
      "citeRegEx" : "Lee et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2018
    }, {
      "title" : "Visualbert: A simple and performant baseline for vision and language",
      "author" : [ "Liunian Harold Li", "Mark Yatskar", "Da Yin", "Cho-Jui Hsieh", "Kai-Wei Chang" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q1908\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 1908
    }, {
      "title" : "Unicoder-vl: A universal encoder for vision and language by cross-modal pretraining",
      "author" : [ "Gen Li", "Nan Duan", "Yuejian Fang", "Ming Gong", "Daxin Jiang", "Ming Zhou" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Li et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C Lawrence Zitnick" ],
      "venue" : "In European conference on computer vision,",
      "citeRegEx" : "Lin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Expansion via prediction of importance with contextualization",
      "author" : [ "Sean MacAvaney", "Franco Maria Nardini", "Raffaele Perego", "Nicola Tonellotto", "Nazli Goharian", "Ophir Frieder" ],
      "venue" : "arXiv preprint arXiv:2004.14245,",
      "citeRegEx" : "MacAvaney et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "MacAvaney et al\\.",
      "year" : 2020
    }, {
      "title" : "Introduction to information retrieval",
      "author" : [ "Christopher D Manning", "Hinrich Schütze", "Prabhakar Raghavan" ],
      "venue" : "Cambridge university press,",
      "citeRegEx" : "Manning et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2008
    }, {
      "title" : "Dual attention networks for multimodal reasoning and matching",
      "author" : [ "Hyeonseob Nam", "Jung-Woo Ha", "Jeonghee Kim" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Nam et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Nam et al\\.",
      "year" : 2017
    }, {
      "title" : "Sacheti. Imagebert: Cross-modal pretraining with large-scale weak-supervised imagetext data",
      "author" : [ "Di Qi", "Lin Su", "Jia Song", "Edward Cui", "Taroon Bharti", "Arun" ],
      "venue" : "arXiv preprint arXiv:2001.07966,",
      "citeRegEx" : "Qi et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2020
    }, {
      "title" : "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "author" : [ "Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun" ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence,",
      "citeRegEx" : "Ren et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2016
    }, {
      "title" : "Position focused attention network for image-text matching",
      "author" : [ "Yaxiong Wang", "Hao Yang", "Xueming Qian", "Lin Ma", "Jing Lu", "Biao Li", "Xin Fan" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q1907\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 1907
    }, {
      "title" : "Consensus-aware visual-semantic embedding for image-text matching",
      "author" : [ "Haoran Wang", "Ying Zhang", "Zhong Ji", "Yanwei Pang", "Lin Ma" ],
      "venue" : "In European Conference on Computer Vision,",
      "citeRegEx" : "Wang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Language-agnostic visual-semantic embeddings",
      "author" : [ "Jonatas Wehrmann", "Douglas M Souza", "Mauricio A Lopes", "Rodrigo C Barros" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision,",
      "citeRegEx" : "Wehrmann et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wehrmann et al\\.",
      "year" : 2019
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Yang et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Personalizing dialogue agents: I have a dog, do you have pets too",
      "author" : [ "Saizheng Zhang", "Emily Dinan", "Jack Urbanek", "Arthur Szlam", "Douwe Kiela", "Jason Weston" ],
      "venue" : "arXiv preprint arXiv:1801.07243,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Sparta: Efficient open-domain question answering via sparse transformer matching retrieval",
      "author" : [ "Tiancheng Zhao", "Xiaopeng Lu", "Kyusong Lee" ],
      "venue" : "arXiv preprint arXiv:2009.13013,",
      "citeRegEx" : "Zhao et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "and images separately and then compute the relevancy via inner product (Faghri et al., 2017; Lee et al., 2018; Wang et al., 2019a).",
      "startOffset" : 71,
      "endOffset" : 130
    }, {
      "referenceID" : 10,
      "context" : "and images separately and then compute the relevancy via inner product (Faghri et al., 2017; Lee et al., 2018; Wang et al., 2019a).",
      "startOffset" : 71,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "The transformer architecture is a well-known querydependent model (Devlin et al., 2018; Yang et al., 2019).",
      "startOffset" : 66,
      "endOffset" : 106
    }, {
      "referenceID" : 22,
      "context" : "The transformer architecture is a well-known querydependent model (Devlin et al., 2018; Yang et al., 2019).",
      "startOffset" : 66,
      "endOffset" : 106
    }, {
      "referenceID" : 15,
      "context" : "Besides improving the accuracy, retrieval speed has also been a long-existing subject of study in the information retrieval (IR) community (Manning et al., 2008).",
      "startOffset" : 139,
      "endOffset" : 161
    }, {
      "referenceID" : 7,
      "context" : "For dual-encoder systems, further speed improvement can be obtained via Approximate Nearest Neighbors (ANN) Search and GPU acceleration (Johnson et al., 2019).",
      "startOffset" : 136,
      "endOffset" : 158
    }, {
      "referenceID" : 8,
      "context" : "Large amounts of work have been done on learning a joint representation between texts and images (Karpathy and Fei-Fei, 2015; Huang et al., 2018; Lee et al., 2018; Wehrmann et al., 2019; Li et al., 2020b; Lu et al., 2020).",
      "startOffset" : 97,
      "endOffset" : 221
    }, {
      "referenceID" : 10,
      "context" : "Large amounts of work have been done on learning a joint representation between texts and images (Karpathy and Fei-Fei, 2015; Huang et al., 2018; Lee et al., 2018; Wehrmann et al., 2019; Li et al., 2020b; Lu et al., 2020).",
      "startOffset" : 97,
      "endOffset" : 221
    }, {
      "referenceID" : 21,
      "context" : "Large amounts of work have been done on learning a joint representation between texts and images (Karpathy and Fei-Fei, 2015; Huang et al., 2018; Lee et al., 2018; Wehrmann et al., 2019; Li et al., 2020b; Lu et al., 2020).",
      "startOffset" : 97,
      "endOffset" : 221
    }, {
      "referenceID" : 18,
      "context" : "(2019a) encoded the location information as yet another feature and used both deep RCNN features (Ren et al., 2016) and the fine-grained location features for the Region of Interest (ROI) as image representation.",
      "startOffset" : 97,
      "endOffset" : 115
    }, {
      "referenceID" : 2,
      "context" : "Large pre-trained language models (PLM) show great success over multiple tasks in NLP areas in recent years (Devlin et al., 2018; Yang et al., 2019; Dai et al., 2019).",
      "startOffset" : 108,
      "endOffset" : 166
    }, {
      "referenceID" : 22,
      "context" : "Large pre-trained language models (PLM) show great success over multiple tasks in NLP areas in recent years (Devlin et al., 2018; Yang et al., 2019; Dai et al., 2019).",
      "startOffset" : 108,
      "endOffset" : 166
    }, {
      "referenceID" : 17,
      "context" : "After that, research has also been done on cross-modal transformer-based models and proves that the self-attention mechanism also helps jointly capture visual-text relationships (Li et al., 2019; Lu et al., 2020; Qi et al., 2020; Li et al., 2020b).",
      "startOffset" : 178,
      "endOffset" : 247
    }, {
      "referenceID" : 10,
      "context" : "Previous methods pass the query sentence into a bi-RNN to give token representation provided surrounding tokens (Lee et al., 2018; Wang et al., 2019a, 2020).",
      "startOffset" : 112,
      "endOffset" : 156
    }, {
      "referenceID" : 18,
      "context" : "Regional visual features and location features Given an image v, we pass it through FasterRCNN (Ren et al., 2016) to get n regional visual features vi and their corresponding location features li:",
      "startOffset" : 95,
      "endOffset" : 113
    }, {
      "referenceID" : 9,
      "context" : "Parameters are optimized using Adam (Kingma and Ba, 2014).",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 23,
      "context" : "In order to achieve efficient training, we use other image samples from the same batch as negative examples for each training data, an effective technique that is widely used in response selection (Zhang et al., 2018; Henderson et al., 2019).",
      "startOffset" : 197,
      "endOffset" : 241
    }, {
      "referenceID" : 15,
      "context" : "Also, the query-time computation can be fit into an Inverted Index architecture (Manning et al., 2008), which enables us to use VisualSparta index with off-the-shelf search engines, for example, Elasticsearch (Gheorghe et al.",
      "startOffset" : 80,
      "endOffset" : 102
    }, {
      "referenceID" : 4,
      "context" : ", 2008), which enables us to use VisualSparta index with off-the-shelf search engines, for example, Elasticsearch (Gheorghe et al., 2015).",
      "startOffset" : 114,
      "endOffset" : 137
    }, {
      "referenceID" : 13,
      "context" : "In this paper, we use MSCOCO (Lin et al., 2014)1 and Flickr30K (Plummer et al.",
      "startOffset" : 29,
      "endOffset" : 47
    }, {
      "referenceID" : 9,
      "context" : "The Adam optimizer (Kingma and Ba, 2014) is used with the learning rate set to 5e-5.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 20,
      "context" : "Specifically, For CVSE model (Wang et al., 2020), both CPU and GPU inference time are recorded.",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 7,
      "context" : "For GPU setting, we adopt the model and use FAISS (Johnson et al., 2019), an optimized MIPS library, to test the speed performance.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 20,
      "context" : "As we can see from Table 2, in all four data splits (1K, 5K, 113K, 1M), VisualSparta significantly outperforms both the best query-agnostic model (CVSE (Wang et al., 2020)) and the best querydependent model (Oscar (Li et al.",
      "startOffset" : 152,
      "endOffset" : 171
    }, {
      "referenceID" : 7,
      "context" : "To apply the latest MIPS progress to the comparison, we adopt the CVSE model to use FAISS (Johnson et al., 2019) for better speed acceleration.",
      "startOffset" : 90,
      "endOffset" : 112
    } ],
    "year" : 2021,
    "abstractText" : "Text-to-image retrieval is an essential task in cross-modal information retrieval, i.e., retrieving relevant images from a large and unlabelled dataset given textual queries. In this paper, we propose VisualSparta, a novel (Visualtext Sparse Transformer Matching) model that shows significant improvement in terms of both accuracy and efficiency. VisualSparta is capable of outperforming previous stateof-the-art scalable methods in MSCOCO and Flickr30K. We also show that it achieves substantial retrieving speed advantages, i.e., for a 1 million image index, VisualSparta using CPU gets ∼391X speedup compared to CPU vector search and ∼5.4X speedup compared to vector search with GPU acceleration. Experiments show that this speed advantage even gets bigger for larger datasets because VisualSparta can be efficiently implemented as an inverted index. To the best of our knowledge, VisualSparta is the first transformer-based textto-image retrieval model that can achieve realtime searching for large-scale datasets, with significant accuracy improvement compared to previous state-of-the-art methods.",
    "creator" : "LaTeX with hyperref"
  }
}