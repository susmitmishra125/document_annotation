{
  "name" : "2021.acl-long.237.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Semantic-based Method for Unsupervised Commonsense Question Answering",
    "authors" : [ "Yilin Niu", "Fei Huang", "Jiaming Liang", "Wenkai Chen", "Xiaoyan Zhu", "Minlie Huang" ],
    "emails" : [ "niuyl14@tsinghua.org.cn", "f-huang@mails.tsinghua.edu.cn", "liangjm1818@mails.tsinghua.edu.cn", "wkchen630@gmail.com", "zxy-dcs@tsinghua.edu.cn", "aihuang@tsinghua.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3037–3049\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3037\nIn this paper, we present a novel SEmanticbased Question Answering method (SEQA) for unsupervised commonsense question answering. Instead of directly scoring each answer choice, our method first generates a set of plausible answers with generative models (e.g., GPT-2), and then uses these plausible answers to select the correct choice by considering the semantic similarity between each plausible answer and each choice. We devise a simple, yet sound formalism for this idea and verify its effectiveness and robustness with extensive experiments. We evaluate the proposed method on four benchmark datasets, and our method achieves the best results in unsupervised settings. Moreover, when attacked by TextFooler (Jin et al., 2020) with synonym replacement, SEQA demonstrates much less performance drops than baselines, thereby indicating stronger robustness."
    }, {
      "heading" : "1 Introduction",
      "text" : "Pre-trained language models have been widely used for commonsense question answering. Finetuning pre-trained models on task-specific data produces many state-of-the-art results (Wang et al., 2020;\n*Equal contribution †Corresponding author: Minlie Huang.\nKhashabi et al., 2020; Lin et al., 2019). However, this requires amounts of labeled task data. Therefore, it is vital to study unsupervised commonsense question answering without relying on any labeled downstream task data. In this paper, we investigate multiple-choice commonsense question answering tasks in an unsupervised setting: given a question and a set of answer choices, a model is required to predict the most reasonable answer choice for the question, but without access to any labeled task data.\nMany existing unsupervised methods tackle these tasks by scoring each answer choice using a language model, e.g., estimating the generative probability of the answer choice conditioned on the question (Trinh and Le, 2018; Shwartz et al., 2020; Bosselut and Choi, 2019; Tamborrino et al., 2020). Table 1 lists several typical score functions. However, these scores can be easily influenced by word frequencies, sentence structures, and other\nfactors, which can mislead the models and make existing methods oversensitive to lexical perturbations (Abdou et al., 2020; Tamborrino et al., 2020). Figure 1 shows two examples. The correct choices are paraphrased via synonym replacement or structure transformation. In these examples, the baseline (Pro-A) produces much lower scores for the paraphrased choices and chooses the wrong choices.\nSince existing methods can be easily distracted by irrelevant factors such as lexical perturbations, we argue that a commonsense question answering method should focus on the answers’ semantics and assign similar scores to synonymous choices. To this end, we introduce a novel SEmantic-based Question Answering model, SEQA, which aims to robustly select correct answers in multi-choice commonsense question answering in an unsupervised setting. Instead of directly scoring an answer choice, we calculate the probability of observing the choice’s semantics. A choice’s semantic score can be obtained by summing the generative probabilities of sentences that have the same semantic meanings with the choice, where the sentences are called the choice’s supporters. However, it is hard to obtain the supporters which have exactly the same semantic meanings with the choice, so we reformulate the semantic score into a soft version as explained in Section 3.2. Each supporter is weighed by the semantic similarity to the answer choice, which can be computed with some off-the-shelf models, such as SentenceBERT (Reimers and Gurevych, 2019). Since the supporters and their weights depend on the semantics rather than the surface form of the answer choice, by this means, the effects of the distracting factors can be largely suppressed. Moreover, synonymous choices are likely to share the same set of supporters, so their scores are expected to be stably close. Our contributions in this paper are summarized as follows:\n• We propose a semantic-based question answering model (SEQA) for robust commonsense question answering in an unsupervised setting. Instead of directly scoring the answer choices, our method first generates some plausible answers and then uses them to select the correct choice by considering the semantic similarity between each plausible answer and each choice.\n• We conduct experiments on four commonsense question answering datasets, where SEQA achieves the best performance com-\npared with strong baselines. When attacked by TextFooler (Jin et al., 2020) with synonym replacement, our method performs remarkably more robustly."
    }, {
      "heading" : "2 Related Work",
      "text" : "Previous work has explored pre-trained language models (LMs) for unsupervised commonsense question answering. In general, these approaches treat LMs as question answering modules.\nTable 1 shows three representative methods, which do not use external knowledge and rely fully on the implicit knowledge encoded in LMs for reasoning. Probability-A (Pro-A) considers the generative probability of the choice conditioned on the question. However, it suffers from the statistical bias of choices, such as word frequency and sentence length (Abdou et al., 2020). To alleviate this, MutualInfo-QA (MI-QA) calculates the mutual information between the question and the choice. Another way to reduce the impact of statistical bias is to score each choice using the conditional probability of the question rather than the choice (Trinh and Le, 2018; Tamborrino et al., 2020) , which is denoted as Probability-Q (Pro-Q) in Table 1.\nSome recent work claims that external knowledge can benefit commonsense reasoning. Besides static knowledge bases (KBs), such as ConceptNet (Speer et al., 2017) and Atomic (Sap et al., 2019a), there are also numerous studies treating LMs as dynamic KBs. Petroni et al. (2019) shows that LMs can be used for KB completion. And Davison et al. (2019) shows that BERT can distinguish true and fake ConceptNet triplets. Further, the extracted knowledge can work as complementary information for answering a question. Rajani et al. (2019) proposes a model for Com-\n1PBERT (Q|A) , ∏|Q| i PBERT (Qi|Q/i, A).\nmonSenseQA (Talmor et al., 2019) that generates explanations for questions, which are then used as additional inputs. The shortcoming of this approach is that it requires collecting human explanations for each new dataset to fine-tune LMs. Some following researches explore unsupervised explanation/knowledge generator. CGA (Bosselut and Choi, 2019) employs COMET (Bosselut et al., 2019) to generate intermediate inferences which are then used to score the choice. However, COMET is limited by a small set of question types so that CGA is difficult to generalize to different domains. Self-Talk (Shwartz et al., 2020) breaks the limit by extracting knowledge from GPT-2 (Radford et al., 2019), which has no restriction on the query types. Thus, Self-Talk can be applied to a wide range of domains. Despite the introduction of auxiliary information, these methods are essentially dependent on language model scores, so they are still sensitive to lexical perturbations.\nBesides directly using pre-trained LMs, some recent efforts have been dedicated to automatically constructing task-specific data to train commonsense reasoners in zero-shot settings. Wang et al. (2019) and Kocijan et al. (2019) provide some rules to construct labeled training data from large corpus for pronoun disambiguation. Banerjee and Baral (2020), Moghimifar et al. (2020) and Ma et al. (2020) collect training data based on knowledge bases, such as Atomic (Sap et al., 2019a). Though effective, they are limited by the specific task settings or highly dependent on the task-related knowledge bases, which makes them difficult to transfer to other commonsense reasoning tasks."
    }, {
      "heading" : "3 Method",
      "text" : "In this paper, we focus on unsupervised multiplechoice commonsense question answering, which is formalized as follows: given a question and a set of choices, models should select the correct choice:\nÂ = argmax A\ns(A|Q),\nwhere s refers to a score function. Note that we have no access to any labeled task data."
    }, {
      "heading" : "3.1 Motivation",
      "text" : "In existing unsupervised methods, the score functions are usually defined based on the language model scores. Taking Pro-A (Table 1) as an example, it first converts the question into a statement:\n• Q: I saw my breath when I exhaled. What was the cause of this? −→ Rewrite: I saw my breath when I exhaled because\nAnd it then takes the statement as a prompt to calculate the generative probability of each choice. Note that the templates for rewriting is not the focus of this paper, and hence we directly use the templates of previous work (Shwartz et al., 2020; Tamborrino et al., 2020) for our method and all the baselines in this paper (see Appendix for details).\nThough successful, language model scores can be affected by many distracting factors, such as word frequency and sentence structure, etc. These factors can disturb the score functions to a large extent, as shown in Figure 1. Our goal is to alleviate the influence of these distracting factors. Hence we propose a new method for unsupervised commonsense question answering, which achieves better results and performs more robustly."
    }, {
      "heading" : "3.2 SEQA",
      "text" : "SEQA is designed to predict the semantic score of an answer choice A. Instead of directly estimating the probability P (A|Q) of the single choice A, the semantic score focuses on the probability P (MA|Q) where MA represents A’s semantics. Ideally, we decompose P (MA|Q) into the summation of the conditional probabilities of A’s supporters, where the supporters indicates all possible answers that have exactly the same semantics MA. Formally, the semantic score is defined as\ns(A|Q) , P (MA|Q) = ∑ S∈SA PLM (S|Q) (1)\n= ∑ S∈A I(S ∈ SA)PLM (S|Q). (2)\nSA is the set of supporters of choiceA, and A is the set of all possible answers. I(S ∈ SA) is an indicator function indicating whether S is a supporter of A. To obtain the supporter set SA, we adopt a model to extract the sentence-level semantic features. Ideally, the indicator function is defined as\nI(S ∈ SA) = { 1 if cos(hS , hA) = 1, 0 if cos(hS , hA) < 1,\n(3)\nwhere hA is the semantic features of sentence A, and we assume that S andA are exactly the same in semantics if hS and hA point in the same direction.\nHowever, Eq.(3) uses a hard constraint that cos(hS , hA) exactly equals to 1, which can be too\nstrict to find acceptable supporters. Therefore, we reformulate Eq.(2) into a soft version:\ns(A|Q) , ∑ S∈A ω(S|A)PLM (S|Q), (4)\nwhere the indicator function in Eq.(2) is replaced by a soft function ω(S|A). To emulate I(S ∈ SA), ω(S|A) is expected to meet three requirements: (1) ω(S|A) ∈ [0, 1] for any S and A; (2) ω(S|A) = 1 if cos(hS , hA) = 1; (3) ω(S|A) increases monotonically with cos(hS , hA). There are several different definitions of ω(S|A) meeting these requirements, which are explored in Section 4.7.3. In this paper, ω(S|A) is defined as:\nω(S|A) = 1 Z(T ) exp\n[ cos(hS , hA)\nT\n] . (5)\nT is the temperature, and Z(T ) = exp( 1T ) is a normalization term that makes ω(A|A) = 1. If T → 0, ω(S|A) degenerates to the indicator function. If T > 0, ω(S|A) relates to the von Mises-Fishers distribution over the unit sphere in the feature space, where the acceptable feature vectors are distributed around the mean direction hA||hA|| .\nSince it is intractable to enumerate all possible answers in A, we convert Eq.(4) to an expectation over PLM (S|Q):\ns(A|Q) = ES∼PLM (S|Q) [ω(S|A)]\n≈ 1 K K∑ i=1 ω(Si|A) (6)\n= 1\nK · Z(T ) K∑ i=1 exp [ cos(hSi , hA) T ] , (7)\nwhere S1, · · · , SK are sentences sampled from PLM (·|Q), and K is the sample size. hA and hSi can be extracted from a pre-trained model, e.g., SentenceBERT (Reimers and Gurevych, 2019).\nFrom Eq.(7), we can see the semantic score s(A|Q) is only dependent on the semantic feature hA and regardless of A’s surface form. Therefore, our method will produce similar semantic scores for synonymous choices, assuming that the synonymous choices have similar semantic features."
    }, {
      "heading" : "3.3 The Voting View of SEQA",
      "text" : "At the beginning of Section 3.2, we define the semantic score as the summation of the conditional probabilities over the supporters. However, in Eq.(7), the sampled sentences S1, · · · , SK are not A’s supporters because they may not be semantically similar to A. To address the differences, we\nname the sampled sentences S1, · · · , SK as voters, which are plausible answers to the question Q. In this section, we will show another view of our method, which works like a procedure that the voters vote out the correct choice.\nSuppose there are two candidate choices A1 and A2, our method is to find the correct choice according to the semantic scores, s(A1|Q) and s(A2|Q). Following Eq.(6), our method can be decomposed into two steps: First, sample some voters S1, · · · , SK from PLM (·|Q). This step only considers the question Q but no candidate choices. Second, each voter votes for the choices with the semantic similarity weights. For example, Si votes for Aj with the weight of ω(Si|Aj). The candidate choice that receives more votes will have a higher semantic score and be selected as the final answer.\nFigure 2 shows the process of SEQA in the view of voting. Although the voting view is intuitive, the formalism in Section 3.2 provides more insights: (1) Our method approximates the probability of semantics, which works as the theoretical basis of SEQA. (2) Our method can be seen as an extension of Pro-A (see Table 1), since Pro-A only calculates the language model score for a single sentence, whereas our method calculates the semantic score for a set of supporters. (3) Eq.(4) provides guidance, the three requirements mention before, for the design of the voting weight function ω(S|A). Specifically, the guidance explains the rationality of the formulation of Eq.(5)."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We conducted experiments on four multiplechoice commonsense question answering tasks, COPA (Roemmele et al., 2011), StoryClozeTest (SCT) (Mostafazadeh et al., 2016), SocialIQA (Sap et al., 2019b) and CosmosQA (Huang et al., 2019). For each instance, only one choice is correct. See Appendix for more description about datasets.\nFor COPA, we reported the results on its test set. As the test sets of another three datasets are hidden, for convenience of analysis, we reported the experiment results on their development sets."
    }, {
      "heading" : "4.2 Baselines",
      "text" : "We employed five strong baselines. Table 1 shows three of them, Pro-A, Pro-Q and MI-QA. There is no explicit auxiliary information used in these three methods, while another two baselines rely on explicit information supplementation. CGA (Bosselut and Choi, 2019) and Self-Talk (Shwartz et al., 2020) query pre-trained language models (e.g., GPT-2, COMET (Bosselut et al., 2019)) for relevant knowledge, which forms part of contexts. And then, similar to Pro-A, they take the generative probabilities of choices as scores."
    }, {
      "heading" : "4.3 Experiment Settings",
      "text" : "For each method, we tried different pre-trained language models (see Appendix for details), and then selected the pre-trained LMs that maximized the accuracy on each dataset. The details of the selection of pre-trained LMs can be found in Table 2.\nFor SEQA, we used GPT-2 to generate voters via Nucleus Sampling (Holtzman et al., 2020) with p = 0.9. The sample size K of voters is set to 500. In Section 4.7.2, we show that a small sample size can also lead to superior performance. Self-Talk and CGA also rely on the generated answers from GPT-2 or COMET. Different from SEQA, for these two baselines, more generated answers will not always lead to better performance (see Section 4.7.2). Thus, we selected the optimal sample size for them rather than the same sample size with SEQA.\nWhen evaluating SEQA on COPA, we tuned the temperature T on its development set, and then reported the results on the test set with the tuned temperature T = 0.1. Due to the absence of test sets of other datasets, we evaluated SEQA on their development sets without tuning the temperature and directly set T = 0.1."
    }, {
      "heading" : "4.4 Main Results",
      "text" : "Table 2 shows the evaluation results about accuracy and robustness."
    }, {
      "heading" : "4.4.1 Accuracy",
      "text" : "Among all the methods, SEQA achieved the best performance on all the datasets. Especially on SCT and CosmosQA, SEQA outperformed the best baselines by more than 10 points. It can be inferred that the semantic scores are beneficial for commonsense question answering due to the reduction of distracting factors. Pro-Q performed better than other baselines on COPA, perhaps because it suffered less from the statistic bias of choices (Tamborrino et al., 2020). However, Pro-Q lost its superiority on another three datasets, because it is unsuitable for processing long or complex contexts."
    }, {
      "heading" : "4.4.2 Robustness",
      "text" : "To test the robustness under the synonym replacement attack, we used TextFooler (Jin et al., 2020) to attack the methods by perturbing the correct choices of the correctly predicted examples. The percentage of perturbed words refers to what percentage of words in choices are replaced in successful attacks. The semantic similarity is measured between the paraphrased choice and the original choice. Considering the attack success rate and the after-attack accuracy, SEQA is much more robust than all baselines. To be specific, the attack success rates on SEQA are at least 39 points lower than those of Pro-A, CGA, and Self-Talk on all datasets. MI-QA and Pro-Q are designed to reduce the impact of statistic bias in choices, so that they can resist lexical perturbation to some extent. Even so, SEQA is remarkably lower than MI-QA and Pro-Q in terms of attack success rates on all datasets.\nAn observation is that the attack success rate on SEQA on CosmosQA is higher than those on the other datasets. The reason is that, the contexts in CosmosQA are so complex that GPT-2 is more difficult to generate high-quality answers. If there is a more powerful generator, the robustness of SEQA is expected to have a further improvement."
    }, {
      "heading" : "4.5 Consistency Testing",
      "text" : "We have claimed that a commonsense question answering method should assign close scores to synonymous choices. To verify that SEQA better meets this requirement, we conducted consistency testing for all the methods on four datasets. For each example, the consistency testing of a method is conducted in three steps: (1) Originally, the example has one correct and several wrong answer choices. We randomly sample some choices from other examples as additional wrong choices. After\nthat, the example will have one correct choice and 19 wrong choices. (2) Leverage a commonly used automatic translation service, Baidu Translation, to translate each choice from English into an intermediate language, and then back-translate it into English. During this process, we employ three intermediate languages, Chinese, Spanish, and Russian, because the translation quality of these languages is better than others. As a result, each choice is accompanied with three synonymous choices. (3) Use the commonsense question answering method to calculate the scores for each choice as well as its synonymous choices, and then sort all the choices according to their scores. Because the scoring scales of these methods are different, we calculate the standard deviation of the ranks of the correct choice and its synonymous choices.\nTable 3 shows the average standard deviation of the ranks. As expected, the average standard deviation of SEQA is much lower than any other method on all the datasets, confirming that SEQA assigns more similar ranks and closer scores to synonymous choices. We also observed that MIQA provided relatively stable predictions compared with other baseline methods. A possible explanation is that, the normalization term PLM (A) helps alleviate the influence of lexical perturbations."
    }, {
      "heading" : "4.6 Trends of Accuracy with Answer Length",
      "text" : "Answer length is also a type of distracting factor which may mislead baseline methods. To explore to which extent answer lengths affect the performance of methods, we divided the development set of CosmosQA into four subsets according to the length of correct choice. Table 4 shows the results of SEQA and a robust baseline, MI-QA. Compared with MI-QA, SEQA has much more stable performance as answer lengths vary. The reason is that, SEQA focuses on semantic information so that it has stronger resistance to such distracting factors."
    }, {
      "heading" : "4.7 Ablation Study",
      "text" : ""
    }, {
      "heading" : "4.7.1 Analysis on Temperature",
      "text" : "In the previous experiments, the temperature T of SEQA was set to 0.1 by default. To investigate the influence of T , we varied T in a wide range from 0.05 to 10 and report the results in Table 5. Considering that the temperature varied greatly, the performance of SEQA is relatively stable, indicating that SEQA is not so sensitive to the selection of T . Another observation is that, although the four datasets are different in domains and text length, the trends of performance with temperature on them are relatively similar, illustrating that the temperature selected on one task can be generalized to other tasks."
    }, {
      "heading" : "4.7.2 Analysis on Sample Size",
      "text" : "Figure 3 shows the effect of the sample size K on SEQA. For comparison, Figure 3 also includes the results of baselines in the settings of before- and after-attack, respectively. Due to the limitation of space, the results on the other datasets are shown in Appendix. As expected, the before-attack and afterattack accuracy on SCT increased with the sample size. In detail, the rapid increase in performance occurred when K < 100, and then the improvement slowed down when K > 100. Finally, SEQA achieved a stable and relatively high performance.\nCGA and Self-Talk also leverage LMs to generate some plausible answers. Different from our method, they use the generated answers to form part of the question, and then calculate the generative probability of the choice based on the augmented question. We also tried different sample sizes for the two methods, and Figure 3 (a) shows\nthat their accuracy will not stably increase with a larger sample size."
    }, {
      "heading" : "4.7.3 Analysis on ω(S|A)",
      "text" : "ω(S|A) in SEQA can be defined in different forms, as long as the three requirements mentioned in Section 3.2 are met. Besides the default definition, we explored another three forms of ω(S|A), and the experiment results on COPA are shown in Table 6. Although the performance varies with ω(S|A), the before-attack accuracy of SEQA still outperformed most of the baselines with any definition of ω(S|A). Moreover, SEQA maintains its obvious advantage in after-attack accuracy, which reflects the inherent robustness of SEQA."
    }, {
      "heading" : "4.7.4 Analysis on Pre-trained Language Model and Feature Extractor",
      "text" : "SEQA has no limit on the selection of the pretrained language model and the feature extractor. Table 7 shows how the accuracy of SEQA on COPA varied with the language model and the feature extractor. As expected, more powerful extractor usually led to higher accuracy under the same settings of language models. Similar conclusion can be obtained for the language model. It can be inferred that, if there are more powerful language models or feature extractors in the future, the performance of SEQA may be further improved.\n4.8 Analysis on the Quality of Voters\nWhile the performance of SEQA served as an extrinsic evaluation for the quality of the voters (plausible answers sampled from PLM (·|Q), described in Section 3.3), we were also interested in evaluating it intrinsically. We sampled 125 voters from COPA. For each voter, we provided crowdsourcing workers with the original question, and asked them: 1) whether the voter is grammatical, not entirely grammatical but understandable, or completely not understandable, 2) whether the voter is a reasonable answer to the question, not reasonable but relevant, or completely irrelevant. These evaluation tasks comprehensively examined the voters in grammar and logicality. The annotation tasks were carried out in Amazon Mechanical Turk, and we aggregated annotations from 3 workers using majority vote.\nTable 8 shows the results of the human evaluation of the voters. Score 3/2/1 correspond to the high, middle and low quality, respectively. According to the grammar scores, 97.6% of the voters are grammatical or at least understandable, for which most of the voters belong to the natural language space. In terms of logicality, 40.8% of the voters are reasonable answers to the questions, which may not be very satisfying. However, in Section 4.9, we will show that SEQA makes prediction based on a small part of voters, and hence SEQA is robust\neven though there are some irrelevant voters."
    }, {
      "heading" : "4.9 Voting Weight Distribution",
      "text" : "We visualize the cumulative proportion of voters favoring the correct or the wrong choices (see Figure 4). The curve is averaged over all instances in the test set of COPA, where we sampled 500 voters for each instance and set T = 0.1.\nFrom the curves, we can find several properties of voters: (1) The voters favor the correct choices over the wrong choices, where the curve for correct choices is consistently above the curve for wrong ones. The area between two curves shows the difference of semantic scores s(AC |Q) − s(AW |Q), which is a large gap compared with the area under the bottom curve. (2) 93.5% of voters do not strongly favor any choices (|ω(S|AC) − ω(S|AW )| < 0.05), indicating that they are semantically irrelevant to both candidate choices. However, Table 8 shows that 40.8% of voters are logically reasonable, so many voters are reasonable but irrelevant to both answers. It suggests that there can be several reasonable answers for a single question, and the sampled voters are diverse in the semantics. (3) Although there are only 5.3% of voters strongly favoring the correct choices, there are much less voters (1.2%) favoring the wrong ones. It explains why our method is able to predict the correct answer.\nTo help understand the relationship between voters and choices, Table 9 provides an instance with voters and their voting weights to the choices. We show four types of voters: favoring the correct choice, favoring the wrong choice, logically reasonable but not favoring either choices, and unreasonable and irrelevant to both choices. We can see\nthat the last two types of voters can hardly affect the method’s prediction, because their voting weights are much smaller than the first two types of voters."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We present a semantic-based question answering method, SEQA, which can answer commonsense questions more accurately and robustly in an unsupervised setting. Instead of directly scoring each answer choice, our method focuses on the probability of observing a choice’s semantics. In the view of voting, SEQA first generates some plausible answers (voters) and then utilizes them to vote for the correct choice by considering the semantic similarity between each choice and each voter. Experiment results show that SEQA achieves the best performance on four datasets, and it is remarkably more robust than all the baselines when being attacked by TextFooler."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was partly supported by the NSFC projects (Key project with No. 61936010 and regular project with No. 61876096). This work was also supported by the Guoqiang Institute of Tsinghua University, with Grant No. 2019GQG1 and 2020GQG0005. This work was also supported by Huawei Noah’s Ark Lab."
    }, {
      "heading" : "A Datasets",
      "text" : "The four datasets used in this work are multiplechoice commonsense question answering tasks.\nCOPA2 (Roemmele et al., 2011) evaluates the ability of causal reasoning about a certain event, which is expressed in a simple sentence. Each question is accompanied with two candidate choices.\nStoryClozeTest (SCT)3 (Mostafazadeh et al., 2016) requires models to select the reasonable story ending, from two alternatives, conditioned on a description about the story context.\nSocialIQA4 (Sap et al., 2019b) evaluates the reasoning ability on social events. In each example, the question describes a social event and asks models to make some inferences based on the event, such as its cause or effect.\nCosmosQA5 (Huang et al., 2019) is a reading comprehension task. Different from the three datasets above, the examples of CosmosQA have long and complex contexts. The original dataset contains a type of choices “None of the above” to test whether models can identify unanswerable questions. This is not the focus of our work, so we removed such choices.\nFor COPA, we reported the results on its test set. As the test sets of SCT, SocialIQA and CosmosQA are hidden, for convenience of analysis, we reported the experiment results on their development sets. See Table 10 for statistic information of each dataset."
    }, {
      "heading" : "B Templates for Rewriting Questions",
      "text" : "We use the same templates for our method and all the baselines. Note that the templates for rewriting questions is not the focus of this paper, and we inherit the templates from previous work if available.\n2https://people.ict.usc.edu/ gordon/copa.html 3https://www.cs.rochester.edu/nlp/rocstories/ 4https://leaderboard.allenai.org/socialiqa/submissions/getstarted 5https://leaderboard.allenai.org/cosmosqa/submissions/getstarted\nTamborrino et al. (2020) provides templates for COPA (Table 11) and Shwartz et al. (2020) provides templates for SocialIQA (Table 12). Since the instances in SCT have no questions, SCT does not need templates. There is no related work discussing templates for CosmosQA, so we design some templates by ourselves (Table 13). Source code for rewriting questions and SEQA will be made publicly available."
    }, {
      "heading" : "C Selection of Pre-trained Models",
      "text" : "For each method, we tried to adopt different pretrained models and find the pre-trained models that maximized the accuracy on the development set of each dataset. Table 14 shows the set of candidate pre-trained models for each method, with the selected models in bold. Because of the nature of ProQ, it can only use bidirectional language models, so we only evaluated Pro-Q with RoBERTa-large and SentenceRoBERTa-large.\nAs shown in Table 14, for each method except CGA, the best selection of pre-trained models is consistent on all the datasets. CGA achieved its best performance with COMET on SocialIQA and with GPT2-xlarge on the other datasets."
    }, {
      "heading" : "D Hyperparameter Search",
      "text" : "For SEQA, we only tuned the temperature T . To be more specific, we selected T from five candidate values according to the accuracy on the development set of COPA. Table 15 shows that SEQA with T = 0.1 achieved the best performance on the development set of COPA. And then we evaluated SEQA with T = 0.1 on the test set of COPA as well as the development sets of SCT, SocialIQA and CosmosQA."
    }, {
      "heading" : "E Analysis on Sample Size",
      "text" : "Figure 5,6,7 shows the effect of the sample size K on SEQA. For comparison, these figures also include the results of baselines in the settings of before- and after-attack, respectively. On the overall trend, the performance of SEQA improved as\nthe sample size increased. Another observation is that a smaller sample size can already make SEQA outperform most baseline methods."
    }, {
      "heading" : "10 70.0 75.6",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "The sensitivity of language models and humans to winograd schema perturbations",
      "author" : [ "Mostafa Abdou", "Vinit Ravishankar", "Maria Barrett", "Yonatan Belinkov", "Desmond Elliott", "Anders Søgaard." ],
      "venue" : "ACL, pages 7590–7604.",
      "citeRegEx" : "Abdou et al\\.,? 2020",
      "shortCiteRegEx" : "Abdou et al\\.",
      "year" : 2020
    }, {
      "title" : "Selfsupervised knowledge triplet learning for zero-shot question answering",
      "author" : [ "Pratyay Banerjee", "Chitta Baral." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Banerjee and Baral.,? 2020",
      "shortCiteRegEx" : "Banerjee and Baral.",
      "year" : 2020
    }, {
      "title" : "Dynamic knowledge graph construction for zero-shot commonsense question answering",
      "author" : [ "Antoine Bosselut", "Yejin Choi." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Bosselut and Choi.,? 2019",
      "shortCiteRegEx" : "Bosselut and Choi.",
      "year" : 2019
    }, {
      "title" : "COMET: commonsense transformers for automatic knowledge graph construction",
      "author" : [ "Antoine Bosselut", "Hannah Rashkin", "Maarten Sap", "Chaitanya Malaviya", "Asli Çelikyilmaz", "Yejin Choi." ],
      "venue" : "ACL, pages 4762–4779.",
      "citeRegEx" : "Bosselut et al\\.,? 2019",
      "shortCiteRegEx" : "Bosselut et al\\.",
      "year" : 2019
    }, {
      "title" : "Commonsense knowledge mining from pretrained models",
      "author" : [ "Joe Davison", "Joshua Feldman", "Alexander M. Rush." ],
      "venue" : "EMNLP-IJCNLP, pages 1173– 1178.",
      "citeRegEx" : "Davison et al\\.,? 2019",
      "shortCiteRegEx" : "Davison et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL-HLT, pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "The curious case of neural text degeneration",
      "author" : [ "Ari Holtzman", "Jan Buys", "Li Du", "Maxwell Forbes", "Yejin Choi." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Holtzman et al\\.,? 2020",
      "shortCiteRegEx" : "Holtzman et al\\.",
      "year" : 2020
    }, {
      "title" : "Cosmos QA: machine reading comprehension with contextual commonsense reasoning",
      "author" : [ "Lifu Huang", "Ronan Le Bras", "Chandra Bhagavatula", "Yejin Choi." ],
      "venue" : "EMNLP, pages 2391–2401.",
      "citeRegEx" : "Huang et al\\.,? 2019",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "Is BERT really robust? A strong baseline for natural language attack on text classification and entailment",
      "author" : [ "Di Jin", "Zhijing Jin", "Joey Tianyi Zhou", "Peter Szolovits." ],
      "venue" : "AAAI, pages 8018–8025.",
      "citeRegEx" : "Jin et al\\.,? 2020",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2020
    }, {
      "title" : "Unifiedqa: Crossing format boundaries with a single QA system",
      "author" : [ "Daniel Khashabi", "Sewon Min", "Tushar Khot", "Ashish Sabharwal", "Oyvind Tafjord", "Peter Clark", "Hannaneh Hajishirzi." ],
      "venue" : "Findings, EMNLP, pages 1896–1907.",
      "citeRegEx" : "Khashabi et al\\.,? 2020",
      "shortCiteRegEx" : "Khashabi et al\\.",
      "year" : 2020
    }, {
      "title" : "A surprisingly robust trick for the winograd schema challenge",
      "author" : [ "Vid Kocijan", "Ana-Maria Cretu", "Oana-Maria Camburu", "Yordan Yordanov", "Thomas Lukasiewicz." ],
      "venue" : "ACL, pages 4837–4842.",
      "citeRegEx" : "Kocijan et al\\.,? 2019",
      "shortCiteRegEx" : "Kocijan et al\\.",
      "year" : 2019
    }, {
      "title" : "Kagnet: Knowledge-aware graph networks for commonsense reasoning",
      "author" : [ "Bill Yuchen Lin", "Xinyue Chen", "Jamin Chen", "Xiang Ren." ],
      "venue" : "EMNLPIJCNLP, pages 2829–2839.",
      "citeRegEx" : "Lin et al\\.,? 2019",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Knowledge-driven data construction for zero-shot evaluation in commonsense question answering",
      "author" : [ "Kaixin Ma", "Filip Ilievski", "Jonathan Francis", "Yonatan Bisk", "Eric Nyberg", "Alessandro Oltramari." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Ma et al\\.,? 2020",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2020
    }, {
      "title" : "Cosmo: Conditional seq2seq-based mixture model for zero-shot commonsense question answering",
      "author" : [ "Farhad Moghimifar", "Lizhen Qu", "Yue Zhuo", "Mahsa Baktashmotlagh", "Gholamreza Haffari." ],
      "venue" : "COLING, pages 5347–5359.",
      "citeRegEx" : "Moghimifar et al\\.,? 2020",
      "shortCiteRegEx" : "Moghimifar et al\\.",
      "year" : 2020
    }, {
      "title" : "A corpus and cloze evaluation for deeper understanding of commonsense stories",
      "author" : [ "Nasrin Mostafazadeh", "Nathanael Chambers", "Xiaodong He", "Devi Parikh", "Dhruv Batra", "Lucy Vanderwende", "Pushmeet Kohli", "James Allen." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Mostafazadeh et al\\.,? 2016",
      "shortCiteRegEx" : "Mostafazadeh et al\\.",
      "year" : 2016
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "EMNLP, pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Language models as knowledge bases",
      "author" : [ "Fabio Petroni", "Tim Rocktäschel", "Sebastian Riedel", "Patrick S.H. Lewis", "Anton Bakhtin", "Yuxiang Wu", "Alexander H. Miller" ],
      "venue" : "In EMNLP-IJCNLP,",
      "citeRegEx" : "Petroni et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Petroni et al\\.",
      "year" : 2019
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Explain yourself! leveraging language models for commonsense reasoning",
      "author" : [ "Nazneen Fatema Rajani", "Bryan McCann", "Caiming Xiong", "Richard Socher." ],
      "venue" : "ACL, pages 4932–4942.",
      "citeRegEx" : "Rajani et al\\.,? 2019",
      "shortCiteRegEx" : "Rajani et al\\.",
      "year" : 2019
    }, {
      "title" : "Sentencebert: Sentence embeddings using siamese bertnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "EMNLP-IJCNLP, pages 3980–3990.",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
      "author" : [ "Melissa Roemmele", "Cosmin Adrian Bejan", "Andrew S. Gordon." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Roemmele et al\\.,? 2011",
      "shortCiteRegEx" : "Roemmele et al\\.",
      "year" : 2011
    }, {
      "title" : "ATOMIC: an atlas of machine commonsense for if-then reasoning",
      "author" : [ "Maarten Sap", "Ronan Le Bras", "Emily Allaway", "Chandra Bhagavatula", "Nicholas Lourie", "Hannah Rashkin", "Brendan Roof", "Noah A. Smith", "Yejin Choi." ],
      "venue" : "AAAI, pages 3027–3035.",
      "citeRegEx" : "Sap et al\\.,? 2019a",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2019
    }, {
      "title" : "Social iqa: Commonsense reasoning about social interactions",
      "author" : [ "Maarten Sap", "Hannah Rashkin", "Derek Chen", "Ronan Le Bras", "Yejin Choi." ],
      "venue" : "EMNLP, pages 4462–4472.",
      "citeRegEx" : "Sap et al\\.,? 2019b",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised commonsense question answering with self-talk",
      "author" : [ "Vered Shwartz", "Peter West", "Ronan Le Bras", "Chandra Bhagavatula", "Yejin Choi." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Shwartz et al\\.,? 2020",
      "shortCiteRegEx" : "Shwartz et al\\.",
      "year" : 2020
    }, {
      "title" : "Conceptnet 5.5: An open multilingual graph of general knowledge",
      "author" : [ "Robyn Speer", "Joshua Chin", "Catherine Havasi" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Speer et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Speer et al\\.",
      "year" : 2017
    }, {
      "title" : "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
      "author" : [ "Alon Talmor", "Jonathan Herzig", "Nicholas Lourie", "Jonathan Berant." ],
      "venue" : "NAACL-HLT, pages 4149–4158.",
      "citeRegEx" : "Talmor et al\\.,? 2019",
      "shortCiteRegEx" : "Talmor et al\\.",
      "year" : 2019
    }, {
      "title" : "Pretraining is (almost) all you need: An application to commonsense reasoning",
      "author" : [ "Alexandre Tamborrino", "Nicola Pellicanò", "Baptiste Pannier", "Pascal Voitot", "Louise Naudin." ],
      "venue" : "ACL, pages 3878– 3887.",
      "citeRegEx" : "Tamborrino et al\\.,? 2020",
      "shortCiteRegEx" : "Tamborrino et al\\.",
      "year" : 2020
    }, {
      "title" : "A simple method for commonsense reasoning",
      "author" : [ "Trieu H. Trinh", "Quoc V. Le." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Trinh and Le.,? 2018",
      "shortCiteRegEx" : "Trinh and Le.",
      "year" : 2018
    }, {
      "title" : "Connecting the dots: A knowledgeable path generator for commonsense question answering",
      "author" : [ "Peifeng Wang", "Nanyun Peng", "Filip Ilievski", "Pedro A. Szekely", "Xiang Ren." ],
      "venue" : "Findings, EMNLP, pages 4129–4140.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised deep structured semantic models for commonsense reasoning",
      "author" : [ "Shuohang Wang", "Sheng Zhang", "Yelong Shen", "Xiaodong Liu", "Jingjing Liu", "Jianfeng Gao", "Jing Jiang." ],
      "venue" : "NAACLHLT, pages 882–891.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "2020) provides templates for COPA (Table 11) and Shwartz et al. (2020) provides templates for SocialIQA",
      "author" : [ "Tamborrino" ],
      "venue" : null,
      "citeRegEx" : "Tamborrino,? \\Q2020\\E",
      "shortCiteRegEx" : "Tamborrino",
      "year" : 2020
    }, {
      "title" : "2019a) and used for training COMET",
      "author" : [ "Sap" ],
      "venue" : null,
      "citeRegEx" : "Sap,? \\Q2019\\E",
      "shortCiteRegEx" : "Sap",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Moreover, when attacked by TextFooler (Jin et al., 2020) with synonym replacement, SEQA demonstrates much less performance drops than baselines, thereby indicating stronger robustness.",
      "startOffset" : 38,
      "endOffset" : 56
    }, {
      "referenceID" : 28,
      "context" : ", estimating the generative probability of the answer choice conditioned on the question (Trinh and Le, 2018; Shwartz et al., 2020; Bosselut and Choi, 2019; Tamborrino et al., 2020).",
      "startOffset" : 89,
      "endOffset" : 181
    }, {
      "referenceID" : 24,
      "context" : ", estimating the generative probability of the answer choice conditioned on the question (Trinh and Le, 2018; Shwartz et al., 2020; Bosselut and Choi, 2019; Tamborrino et al., 2020).",
      "startOffset" : 89,
      "endOffset" : 181
    }, {
      "referenceID" : 2,
      "context" : ", estimating the generative probability of the answer choice conditioned on the question (Trinh and Le, 2018; Shwartz et al., 2020; Bosselut and Choi, 2019; Tamborrino et al., 2020).",
      "startOffset" : 89,
      "endOffset" : 181
    }, {
      "referenceID" : 27,
      "context" : ", estimating the generative probability of the answer choice conditioned on the question (Trinh and Le, 2018; Shwartz et al., 2020; Bosselut and Choi, 2019; Tamborrino et al., 2020).",
      "startOffset" : 89,
      "endOffset" : 181
    }, {
      "referenceID" : 0,
      "context" : "3038 factors, which can mislead the models and make existing methods oversensitive to lexical perturbations (Abdou et al., 2020; Tamborrino et al., 2020).",
      "startOffset" : 108,
      "endOffset" : 153
    }, {
      "referenceID" : 27,
      "context" : "3038 factors, which can mislead the models and make existing methods oversensitive to lexical perturbations (Abdou et al., 2020; Tamborrino et al., 2020).",
      "startOffset" : 108,
      "endOffset" : 153
    }, {
      "referenceID" : 20,
      "context" : "puted with some off-the-shelf models, such as SentenceBERT (Reimers and Gurevych, 2019).",
      "startOffset" : 59,
      "endOffset" : 87
    }, {
      "referenceID" : 5,
      "context" : "LM refers to a pre-trained language model, such as GPT-2 or BERT1 (Devlin et al., 2019).",
      "startOffset" : 66,
      "endOffset" : 87
    }, {
      "referenceID" : 8,
      "context" : "When attacked by TextFooler (Jin et al., 2020) with synonym replacement, our method performs remarkably more robustly.",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "However, it suffers from the statistical bias of choices, such as word frequency and sentence length (Abdou et al., 2020).",
      "startOffset" : 101,
      "endOffset" : 121
    }, {
      "referenceID" : 28,
      "context" : "Another way to reduce the impact of statistical bias is to score each choice using the conditional probability of the question rather than the choice (Trinh and Le, 2018; Tamborrino et al., 2020) , which is denoted as Probability-Q (Pro-Q) in Table 1.",
      "startOffset" : 150,
      "endOffset" : 195
    }, {
      "referenceID" : 27,
      "context" : "Another way to reduce the impact of statistical bias is to score each choice using the conditional probability of the question rather than the choice (Trinh and Le, 2018; Tamborrino et al., 2020) , which is denoted as Probability-Q (Pro-Q) in Table 1.",
      "startOffset" : 150,
      "endOffset" : 195
    }, {
      "referenceID" : 25,
      "context" : "Besides static knowledge bases (KBs), such as ConceptNet (Speer et al., 2017) and Atomic (Sap et al.",
      "startOffset" : 57,
      "endOffset" : 77
    }, {
      "referenceID" : 22,
      "context" : ", 2017) and Atomic (Sap et al., 2019a), there are also numerous studies treating LMs as dynamic KBs.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 26,
      "context" : "3039 monSenseQA (Talmor et al., 2019) that generates explanations for questions, which are then used as additional inputs.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 2,
      "context" : "CGA (Bosselut and Choi, 2019) employs COMET (Bosselut et al.",
      "startOffset" : 4,
      "endOffset" : 29
    }, {
      "referenceID" : 3,
      "context" : "CGA (Bosselut and Choi, 2019) employs COMET (Bosselut et al., 2019) to generate intermediate inferences which are then used to score the choice.",
      "startOffset" : 44,
      "endOffset" : 67
    }, {
      "referenceID" : 24,
      "context" : "Self-Talk (Shwartz et al., 2020) breaks the limit by extracting knowledge from GPT-2 (Radford et al.",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 18,
      "context" : ", 2020) breaks the limit by extracting knowledge from GPT-2 (Radford et al., 2019), which has no restriction on the query types.",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 24,
      "context" : "Note that the templates for rewriting is not the focus of this paper, and hence we directly use the templates of previous work (Shwartz et al., 2020; Tamborrino et al., 2020) for our method and all the baselines in this paper (see Appendix for details).",
      "startOffset" : 127,
      "endOffset" : 174
    }, {
      "referenceID" : 27,
      "context" : "Note that the templates for rewriting is not the focus of this paper, and hence we directly use the templates of previous work (Shwartz et al., 2020; Tamborrino et al., 2020) for our method and all the baselines in this paper (see Appendix for details).",
      "startOffset" : 127,
      "endOffset" : 174
    }, {
      "referenceID" : 24,
      "context" : "We use the same templates with previous work (Shwartz et al., 2020; Tamborrino et al., 2020) to rewrite interrogative sentences into declarative ones.",
      "startOffset" : 45,
      "endOffset" : 92
    }, {
      "referenceID" : 27,
      "context" : "We use the same templates with previous work (Shwartz et al., 2020; Tamborrino et al., 2020) to rewrite interrogative sentences into declarative ones.",
      "startOffset" : 45,
      "endOffset" : 92
    }, {
      "referenceID" : 12,
      "context" : "GPT-2, RoBERTa and SRoBERTa refer to GPT-2-xlarge, RoBERTa-large (Liu et al., 2019) and SentenceRoBERTa-large, respectively.",
      "startOffset" : 65,
      "endOffset" : 83
    }, {
      "referenceID" : 21,
      "context" : "We conducted experiments on four multiplechoice commonsense question answering tasks, COPA (Roemmele et al., 2011), StoryClozeTest (SCT) (Mostafazadeh et al.",
      "startOffset" : 91,
      "endOffset" : 114
    }, {
      "referenceID" : 15,
      "context" : ", 2011), StoryClozeTest (SCT) (Mostafazadeh et al., 2016), SocialIQA (Sap et al.",
      "startOffset" : 30,
      "endOffset" : 57
    }, {
      "referenceID" : 23,
      "context" : ", 2016), SocialIQA (Sap et al., 2019b) and CosmosQA (Huang et al.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 24,
      "context" : "CGA (Bosselut and Choi, 2019) and Self-Talk (Shwartz et al., 2020) query pre-trained language models (e.",
      "startOffset" : 44,
      "endOffset" : 66
    }, {
      "referenceID" : 3,
      "context" : ", GPT-2, COMET (Bosselut et al., 2019)) for relevant knowledge, which forms part of contexts.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 6,
      "context" : "For SEQA, we used GPT-2 to generate voters via Nucleus Sampling (Holtzman et al., 2020) with p = 0.",
      "startOffset" : 64,
      "endOffset" : 87
    }, {
      "referenceID" : 27,
      "context" : "Pro-Q performed better than other baselines on COPA, perhaps because it suffered less from the statistic bias of choices (Tamborrino et al., 2020).",
      "startOffset" : 121,
      "endOffset" : 146
    }, {
      "referenceID" : 8,
      "context" : "ment attack, we used TextFooler (Jin et al., 2020) to attack the methods by perturbing the correct choices of the correctly predicted examples.",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 16,
      "context" : "GloVe means the average pooling of the pre-trained word embeddings (Pennington et al., 2014) over the sentence.",
      "startOffset" : 67,
      "endOffset" : 92
    } ],
    "year" : 2021,
    "abstractText" : "Unsupervised commonsense question answering is appealing since it does not rely on any labeled task data. Among existing work, a popular solution is to use pre-trained language models to score candidate choices directly conditioned on the question or context. However, such scores from language models can be easily affected by irrelevant factors, such as word frequencies, sentence structures, etc. These distracting factors may not only mislead the model to choose a wrong answer but also make it oversensitive to lexical perturbations in candidate answers. In this paper, we present a novel SEmanticbased Question Answering method (SEQA) for unsupervised commonsense question answering. Instead of directly scoring each answer choice, our method first generates a set of plausible answers with generative models (e.g., GPT-2), and then uses these plausible answers to select the correct choice by considering the semantic similarity between each plausible answer and each choice. We devise a simple, yet sound formalism for this idea and verify its effectiveness and robustness with extensive experiments. We evaluate the proposed method on four benchmark datasets, and our method achieves the best results in unsupervised settings. Moreover, when attacked by TextFooler (Jin et al., 2020) with synonym replacement, SEQA demonstrates much less performance drops than baselines, thereby indicating stronger robustness.",
    "creator" : "LaTeX with hyperref"
  }
}