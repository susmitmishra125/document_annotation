{
  "name" : "2021.acl-long.567.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Neural Machine Translation with Monolingual Translation Memory",
    "authors" : [ "Deng Cai", "Yan Wang", "Huayang Li", "Wai Lam", "Lemao Liu" ],
    "emails" : [ "thisisjcykcd@gmail.com", "wlam@se.cuhk.edu.hk", "brandenwang@tencent.com", "alanili@tencent.com", "redmondliu@tencent.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7307–7318\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n7307"
    }, {
      "heading" : "1 Introduction",
      "text" : "Augmenting parametric neural network models with non-parametric memory (Khandelwal et al., 2019; Guu et al., 2020; Lewis et al., 2020a,b) has recently emerged as a promising direction to relieve the demand for ever-larger model size (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020). For the task of Machine Translation (MT), inspired by the Computer-Aided Translation (CAT) tools by professional human translators for increasing productivity for decades (Yamada, 2011), the usefulness of Translation Memory (TM) has long been recognized (Huang et al., 2021). In general, TM is a database that stores pairs of source text and its corresponding translations. Like for human\n∗The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200719). Correspondence to Yan Wang.\ntranslation, early work (Koehn and Senellart, 2010; He et al., 2010; Utiyama et al., 2011; Wang et al., 2013, inter alia) presents translations for similar source input to statistical translation models as additional cues.\nRecent work has confirmed that TM can help Neural Machine Translation (NMT) models as well. In a similar spirit to prior work, TM-augmented NMT models do not discard the training corpus after training but keep exploiting it in the test time. These models perform translation in two stages: In the retrieval stage, a retriever searches for nearest neighbors (i.e., source-target pairs) from the training corpus based on source-side similarity such as lexical overlaps (Gu et al., 2018; Zhang et al., 2018; Xia et al., 2019), embedding-based matches (Cao and Xiong, 2018), or a hybrid (Bulte and Tezcan, 2019; Xu et al., 2020); In the generation stage, the retrieved translations are injected into a standard NMT model by attending over them with sophisticated memory networks (Gu et al., 2018; Cao and Xiong, 2018; Xia et al., 2019; He et al., 2021) or directly concatenating them to the source input (Bulte and Tezcan, 2019; Xu et al., 2020), or biasing the word distribution during decoding (Zhang et al., 2018). Most recently, Khandelwal et al. (2020) propose a token-level nearest neighbor search using complete translation context, i.e., both the source-side input and target-side prefix.\nDespite their differences, we identify two major limitations in previous research. First, the translation memory has to be a bilingual corpus consisting of aligned source-target pairs. This requirement limits the memory bank to bilingual pairs and precludes the use of abundant monolingual data, which can be especially helpful for low-resource scenarios. Second, the memory retriever is non-learnable, not end-to-end optimized, and lacks for the ability to adapt to specific downstream NMT models. Concretely, current retrieval mechanisms (e.g., BM25)\nare generic similarity search, adopting a simple heuristic. That is, the more a source sentence overlaps with the input sentence, the more likely its target-side translation pieces will appear in the correct translation. Although this observation is true, the most similar one does not necessarily serve the best for NMT models. Ideally, the retrieval metric would be learned from the data in a task-dependent way: we wish to consider a memory only if it can indeed boost the quality of final translation.\nIn this work, we propose to augment NMT models with monolingual TM and a learnable crosslingual memory retriever. Specifically, we align source-side sentences and the corresponding targetside translations in a latent vector space using a simple dual-encoder framework (Bromley et al., 1993), such that the distance in the latent space yields a score function for retrieval. As a result, our memory retriever directly connects the dots between the source-side input and target-side translations, enabling monolingual data in the target language to be used alone as TM. Before running each translation, the memory retriever selects the highest-scored memories from a large collection of monolingual sentences (TM), which may include but are not limited to the target side of training corpus, and then the downstream NMT model attends over those memories to help inform its translation. We design the memory retriever with differentiable neural networks. To unify the memory retriever and its downstream NMT model into a learnable whole, the retrieval scores are used to bias the attention scores to the most useful retrieved memories. In this way, our memory retrieval can be end-to-end optimized for the translation objective: a retrieval that improves the golden translation’s likelihood is helpful and should be rewarded, while an uninformative retrieval should be penalized.\nOne challenge for training our proposed framework is that, when starting from random initialization, the retrieved memories will likely be totally unrelated to the input. Since the memory retriever does not exert positive influence on NMT model’s performance, it cannot receive a meaningful gradient and improve. This causes the NMT model to learn to ignore all retrieved memories. To avoid this cold-start problem, we propose to warm-start the retrieval model using two cross-alignment tasks.\nExperiments show that (1) Our model leads to significant improvements over non-TM baseline NMT model, even outperforming strong TM-\naugmented baselines. This is remarkable given that previous TM-augmented models rely on bilingual TM while our model only exploits the target side. (2) Our model can substantially boost translation quality in low-resource scenarios by utilizing extra monolingual TM that is not present in training pairs. (3) Our model gains a strong cross-domain transferability by hot-swapping domain-specific monolingual memory."
    }, {
      "heading" : "2 Related Work",
      "text" : "TM-augmented NMT This work contributes primarily to the research line of Translation Memory (TM) augmented Neural Machine Translation (NMT). Feng et al. (2017) augmented NMT with a bilingual dictionary to tackle infrequent word translation. Gu et al. (2018) proposed a model that retrieves examples similar to the test source sentence and encodes retrieved source-target pairs with keyvalue memory networks. Cao and Xiong (2018); Cao et al. (2019) used a gating mechanism to balance the impact of the translation memory. Zhang et al. (2018) proposed guiding models by retrieving n-grams and up-weighting the probabilities of retrieved n-grams. Bulte and Tezcan (2019) and Xu et al. (2020) used fuzzy-matching with translation memories and augment source sequences with retrieved source-target pairs. Xia et al. (2019) directly ignored the source side of a TM and packed the target side into a compact graph. Khandelwal et al. (2020) ran existing translation model on large bi-text corpora and recorded all hidden states for later nearest neighbor search at each decoding step, which is very compute-intensive. The distinctions between our work and prior work are obvious: (1) The TM in our framework is a collection of monolingual sentences rather than bilingual sentence pairs; (2) We use learnable task-specific retrieval rather than generic retrieval mechanisms.\nRetrieval for Text Generation Discrete retrieval as an intermediate step has been shown beneficial to a variety of natural language processing tasks. One typical use is to retrieve supporting evidence for open-domain question answering (e.g., Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020). Recently, retrieval-guided generation has gained increasing interest in a wide range of text generation tasks such as language modeling (Guu et al., 2018; Khandelwal et al., 2019; Guu et al., 2020), dialogue response generation (Weston et al., 2018; Wu et al., 2019; Cai et al.,\n2019a,b), code generation (Hashimoto et al., 2018) and other knowledge-intensive generation (Lewis et al., 2020b). It can be observed that there is a shift from using off-the-shelf search engines to learning task-specific retrievers. Our work draws inspiration from this line of research. However, retrieval-guided generation has so far been mainly investigated for knowledge retrieval in the same language. The memory retrieval in this work is more challenging due to the cross-lingual setting.\nNMT using Monolingual Data To our knowledge, the integration of monolingual data for NMT was first investigated by Gulcehre et al. (2015), who separately trained target-side language models using monolingual data, and then integrated them during decoding either through re-scoring the beam, or by feeding the hidden state of the language model to the NMT model. Jean et al. (2015) also explored re-ranking the NMT output with a n-gram language model. Another successful method for leveraging monolingual data in NMT is back-translation (Sennrich et al., 2016; Fadaee et al., 2017; Edunov et al., 2018; He et al., 2016), where a reverse translation model is used to translate monolingual sentences from the target language to the source language to generate synthetic parallel sentences. Recent studies (Jiao et al., 2021; He et al., 2019) showed that self-training, where the synthetic parallel sentences are created by translating monolingual sentences in the source language, is also helpful. Our method is orthogonal to previous work and bears a unique feature: it can use more monolingual data without re-training (see §4.3)."
    }, {
      "heading" : "3 Proposed Approach",
      "text" : "We start by formalizing the translation task as a retrieve-then-generate process in §3.1. Then in §3.2, we describe the model design for the cross-\nlingual memory retrieval model. In §3.3, we describe the model design for the memory-augmented translation model. Lastly, we show how to optimize the two components jointly using standard maximum likelihood training in §3.4 and therein we address the cold-start problem via cross-alignment pre-training."
    }, {
      "heading" : "3.1 Overview",
      "text" : "Our approach decomposes the whole translation processing into two steps: retrieve, then generate. The overall framework is illustrated in Figure 1. The Translation Memory (TM) in our approach is a collection of sentences in the target language Z . Given an input x in the source language, the retrieval model first selects a number of possibly helpful sentences {zi}Mi=1 fromZ , whereM |Z|, according to a relevance function f(x, zi). Then, the translation model conditions on both the retrieved set {(zi, f(x, zi)}Mi=1 and the original input x to generate the output y using a probabilistic model p(y|x, z1, f(x, z1), . . . , zM , f(x, zM )). Note that the relevance scores {f(x, zi)}Mi=1 are also part of the input to the translation model, encouraging the translation model to focus more on more relevant sentences. During training, maximizing the likelihood of the translation references improves both the translation model and the retrieval model."
    }, {
      "heading" : "3.2 Retrieval Model",
      "text" : "The retrieval model is responsible for selecting the most relevant sentences for a source sentence from a large monolingual TM. This could involve measuring the relevance scores between the source sentence and millions of candidate target sentences, which poses a serious computational challenge. To address this, we implement the retrieval model using a simple dual-encoder framework (Bromley et al., 1993) such that the selection of the most\nrelevant sentences can be reduced to Maximum Inner Product Search (MIPS). With performant data structures and search algorithms (e.g., Shrivastava and Li, 2014; Malkov and Yashunin, 2018), the retrieval can be done efficiently.\nSpecifically, we define the relevance score f(x, z) between the source sentence x and the candidate sentence z as the dot product of their dense vector representations:\nf(x, z) = Esrc(x) TEtgt(z)\nwhereEsrc andEtgt are the source sentence encoder and the target sentence encoder that map x and z to d-dimensional vectors respectively. We implement the two sentence encoders using two independent Transformers (Vaswani et al., 2017). For an input sentence, we prepend the [BOS] token to its token sequence and then feed it into a Transformer. We take the representation at the [BOS] token as the output (denoted Trans{src,tgt}({x, z})), and perform a linear projection (W{src,tgt}) to reduce the dimensionality of the vector. Finally, we normalize the vectors to regulate the range of relevance scores.\nEsrc(x) = normalize(WsrcTranssrc(x))\nEtgt(z) = normalize(WtgtTranstgt(z))\nThe normalized vectors have zero means and unit lengths. Therefore, the relevance scores always fall in the interval [−1, 1]. We let θ denote all parameters associated with the retrieval model.\nIn practice, the dense representations of all sentences in TM can be pre-computed and indexed using FAISS (Johnson et al., 2019), an open-source toolkit for efficient vector search. Given a source sentence x in hand, we compute the vector representation vx = Esrc(x) and retrieve the top M target sentences with vectors closest to vx."
    }, {
      "heading" : "3.3 Translation Model",
      "text" : "Given a source sentence x, a small set of relevant TM {zi}Mi=1, and relevance scores {f(x, zi)}Mi=1, the translation model defines the conditional probability p(y|x, z1, f(x, z1), . . . , zM , f(x, zM )).\nOur translation model is built upon the standard encoder-decoder NMT model (Bahdanau et al., 2015; Vaswani et al., 2017): the (source) encoder transforms the source sentence x into dense vector representations. The decoder generates an output sequence y in an auto-regressive fashion. At each time step t, the decoder attends over both\npreviously generated sequence y1:t−1 and the output of the source encoder, generating a hidden state ht. The hidden state ht is then converted to next-token probabilities through a linear projection followed by softmax function, i.e., Pv = softmax(Wvht + bv).\nTo accommodate the extra memory input, we extend the standard encoder-decoder NMT framework with a memory encoder and allow crossattention from the decoder to the memory encoder. Specifically, the memory encoder encodes each TM sentence zi individually, resulting in a set of contextualized token embeddings {zi,k}Lik=1, where Li is the length of the token sequence zi. We compute a cross attention over all TM sentences:\nαij = exp(ht TWmzi,j))∑M i=1 ∑Li k=1 exp(ht TWmzi,k) (1)\nct =Wc M∑ i=1 Li∑ j=1 αijzi,j\nwhere αij is the attention score of the j-th token in zi, ct is a weighted combination of memory embeddings, and Wm and Wc are trainable matrices. The cross attention is used twice during decoding. First, the decoder’s hidden state ht is updated by a weighted sum of memory embeddings, i.e., ht = ht + ct. Second, we consider each attention score as a probability of copying the corresponding token (Gu et al., 2016; See et al., 2017). Formally, the next-token probabilities are computed as: p(yt|·) = (1− λt)Pv(yt) + λt M∑ i=1 Li∑ j=1 αij1zij=yt\nwhere 1 is the indicator function and λt is a gating variable computed by another feed-forward network λt = g(ht, ct).\nInspired by Lewis et al. (2020a), to enable the gradient flow from the translation output to the retrieval model, we bias the attention scores with the relevance scores, rewriting Eq. (1) as:\nαij = exp(ht TWmzi,j + βf(x, zi))∑M i=1 ∑Li k=1 exp(ht\nTWmzi,k + βf(x, zi)) (2)\nwhere β is a trainable scalar that controls the weight of the relevance scores. We let φ denote all parameters associated with the translation model."
    }, {
      "heading" : "3.4 Training",
      "text" : "We optimize the model parameters θ and φ using stochastic gradient descent on\nthe negative log-likelihood loss function − log p(y∗|x, z1, f(x, z1), . . . , zM , f(x, zM )), where y∗ refers to the reference translation. As implied by Eq. (2), TM sentences that improve the likelihood of reference translations should receive higher attention scores and higher relevance scores, so gradient descent on the loss function will improve the quality of the retrieval model as well.\nCross-alignment Pre-training However, if the retrieval model starts from random initialization, all top TM sentences zi will likely be unrelated to x (or equally useless). This leads to a problem that the retrieval model cannot receive meaningful gradients and improve, and the translation model will learn to completely ignore the TM input. To avoid this cold-start problem, we propose two crossalignment tasks to warm-start the retrieval model.\nThe first task is sentence-level cross-alignment. This task aims to find the right translation for a source sentence given a set of other translations, which is directly related to our retrieval function. Concretely, We sample B source-target pairs from the training corpus at each training step. Let X and Z be the (B × d) matrix of the source and target vectors encoded by Esrc and Etgt respectively. S = XZT is a (B×B) matrix of relevance scores, where each row corresponds to a source sentence and each column corresponds to a target sentence. Any (Xi, Zj) pair should be aligned when i = j, and should not otherwise. The objective is to maximize the scores along the diagonal of the matrix and henceforth reduce the values in other entries. The loss function can be written as:\nL(i)snt = − exp(Sii) exp(Sii) + ∑ j 6=i exp(Sij) .\nThe second task is token-level cross-alignment, which aims to predict the tokens in the target language given the source sentence representation and vice versa. Formally, we use bag-of-words losses:\nL(i)tok = − ∑\nwy∈Yi\nlog p(wy|Xi) + ∑\nwx∈Xi\nlog p(wx|Yi)\nwhereXi (Yi) represents the set of tokens in the i-th source (target) sentence and the token probabilities are computed by a linear projection followed by the softmax function. The joint loss for pre-training is 1B ∑B i=1 L (i) snt + L (i) tok. In practice, we find that both the sentence-level and token-level objectives are crucial for achieving superior performance.\nAsynchronous Index Refresh To employ fast MIPS, we must pre-compute Etgt(z) for every z ∈ Z and build an index. However, the index cannot remain consistent with the running model during training as θ will be updated over time. One straightforward solution to fix the parameters of Etgt after the pre-training described above and only fine-tune the parameters ofEsrc. However, this may hurt performance since Etgt cannot adapt to the translation objective. Another solution is to asynchronously refresh the index by re-computing and re-indexing all TM sentences at regular intervals. The index is slightly outdated between refreshes, however, we use freshEtgt in gradient estimate. We explore both options in our experiments."
    }, {
      "heading" : "4 Experiments",
      "text" : "We experiment with the proposed approach in three settings: (1) the conventional setting where the available TM is limited to the bilingual training corpus, (2) the low-resource setting where bilingual training pairs are scarce but extra monolingual data is exploited as additional TM, and (3) nonparametric domain adaptation using monolingual TM. Note that existing TM-augmented NMT models are only applicable to the first setting, the last two settings only become possible with our proposed model. We use BLEU score (Papineni et al., 2002) as the evaluation metric."
    }, {
      "heading" : "4.1 Implementation Details",
      "text" : "We build our model using Transformer blocks with the same configuration as Transformer Base (Vaswani et al., 2017) (8 attention heads, 512 dimensional hidden state, and 2048 dimensional feed-forward state). The number of Transformer blocks is 3 for the retrieval model, 4 for the memory encoder in the translation model, and 6 for the encoder-decoder architecture in the translation model. We retrieve the top 5 TM sentences. The FAISS index code is “IVF1024 HNSW32,SQ8” and the search depth is 64.\nWe follow the learning rate schedule, dropout and label smoothing settings described in Vaswani et al. (2017). We use Adam optimizer (Kingma and Ba, 2014) and train models with up to 100K\nsteps throughout all experiments. When trained with asynchronous index refresh, the re-indexing interval is 3K training steps.1"
    }, {
      "heading" : "4.2 Conventional Experiments",
      "text" : "Following prior work in TM-augmented NMT, we first conduct experiments in a setting where the bilingual training corpus is the only source for TM.\nData We use the JRC-Acquis corpus (Steinberger et al., 2006) for our experiments. The JRC-Acquis corpus contains the total body of European Union (EU) law applicable to the EU member states. This corpus was also used by Gu et al. (2018); Zhang et al. (2018); Xia et al. (2019) and we managed to get the datasets originally preprocessed by Gu et al. (2018), making it possible to fairly compare our results with previously reported BLEU scores. Specifically, we select four translation directions, namely, Spanish⇒English (Es⇒En), En⇒Es, German⇒English (De⇒En), and En⇒De, for evaluation. Detailed data statistics are shown in Table 1.\nModels To study the effect of each model component, we implement a series of model variants (model #1 to #5 in Table 2).\n1. NMT without TM. To measure the help from TM, we remove the model components related to TM (including the retrieval model and the memory encoder), and only employ the encoder-decoder architecture for NMT. The resulted model is equivalent to the Transformer Base model (Vaswani et al., 2017).\n1Our code is released at https://github.com/ jcyk/copyisallyouneed.\n2. TM-augmented NMT using source similarity search. To isolate the effect of architectural changes in NMT models, we replace our cross-lingual memory retriever with traditional source-side similarity search. Specifically, we use the fuzzy match system used in Xia et al. (2019) and many others, which is based on BM25 and edit distance.\n3. TM-augmented NMT using pre-trained crosslingual retriever. To study the effect of end-toend task-specific optimization of the retrieval model, we pre-train the retrieval model using the cross-alignment tasks introduced in §3.4 and keep it fixed in the following NMT training.\n4. Our full model using a fixed TM index; After pre-training, we fix the parameter of Etgt during NMT training.\n5. Our full model trained with asynchronous index refresh.\nResults The results of the above models are presented in Table 2. We have the following observations: (1) Our full model trained with asynchronous index refresh (model #5) delivers the best performance on test sets across all four translation tasks, outperforming the non-TM baseline (model #1) by 3.26 BLEU points in average and up to 3.86 BLEU points (De⇒En). This result confirms that monolingual TM can boost NMT performance; (2) The end-to-end learning of the retriever model is the key for substantial performance improvement. We can see that using a pre-trained fixed crosslingual retriever only gives moderate test performance, fine-tuningEsrc and fixingEtgt significantly boosts the performance, and fine-tuning both Esrc\nand Etgt leads to the strongest performance (model #5>model #4>model #3); (3) Cross-lingual retrieval (model #4 and model #5) can obtain better results than that of the source similarity search (model #2). This is remarkable since the crosslingual retrieval only requires monolingual TM, while the source similarity search relies on bilingual TM. We attribute the success, again, to the endto-end adaptability of our cross-lingual retriever. This is manifested by the fact that model #3 even slightly underperforms model #2 in some of translation tasks.\nContrast to Previous Bilingual TM Systems We also compare our results with the best previously reported models.2 We can see that our results significantly outperform previous arts. Notably, our best model (model #5) surpasses the best reported model (Xia et al., 2019) by 1.69 BLEU points in average and up to 2.9 BLEU points (De⇒En). This result verifies the effectiveness of our proposed models. In fact, we can see that our translation model using traditional similarity search (model #2) already outperforms the best previously reported results, which reveals that the architectural design of our translation model is surprisingly effective despite its simplicity.\n2Some recent work used different datasets other than JRCAcquis with unspecified data split, which makes it hard to make an exhaustive comparison. However, note that our inhouse baseline (model #2) is quite strong."
    }, {
      "heading" : "4.3 Low-Resource Scenarios",
      "text" : "One most unique characteristic of our proposed model is that it uses monolingual TM. This motivates us to conduct experiments in low-resource scenarios, where we use extra monolingual data in the target language to boost translation quality.\nData We create low-resource scenarios by randomly partitioning each training set in JRC-Acquis corpus into four subsets of equal size. We set up two series of experiments: (1) We only use the bilinguals pairs in the first subset and gradually enlarge the TM by including more monolingual data in other subsets. (2) Similar to (1), but we instead use the bilingual pairs in the first two subsets.\nModels As shown in §4.2, the model trained with asynchronous index refresh (model #5) is slightly better than the model using fixed Etgt (model #4), however, the computational cost of training model #5 is much bigger. For simplicity and environmental consideration, we only test model #4 in lowresource scenarios. Nevertheless, we note there are still two modeling choices: (1) train the model once with the TM limited to training pairs and only enlarge the TM during testing; (2) re-train the model with every enlarged TM. Note that when using the first choice, the model may retrieve a TM sentence that has never been seen during training. To measure the performance improvements from additional monolingual TM, we also include a Transformer Base baseline (model #1, denoted as\nbase) and a bilingual TM baseline (model #2).\nResults Figure 2 shows the main results on the test sets. The general patterns are consistent across all experiments: the larger the TM becomes, the better translation performance the model achieves. When using all available monolingual data (4/4), the translation quality is boosted significantly. Interestingly, the performance of models without retraining is comparable to, if not better than, those with re-training. We also observe that when the training pairs are very scarce (only 1/4 bilingual pairs are available), a small size of TM even hurts the model performance. The reason could be overfitting. We speculate that better results would be obtained by tuning the model hyper-parameters according to different TM sizes.\nContrast to Back-Translation We compare our models with back-translation (BT) (Sennrich et al., 2016), a popular way of utilizing monolingual data for NMT. We train a target-to-source Transformer Base model using bilingual pairs and use the resultant model to translate monolingual sentences to obtain additional synthetic parallel data. As shown in Table 3, our method performs better than BT with 2/4 bilingual pairs but performs worse with 1/4 bilingual pairs. Interestingly, the combination of BT and our method yields significant further gains, which demonstrates that our method is not only orthogonal but also complementary to BT."
    }, {
      "heading" : "4.4 Non-parametric Domain Adaptation",
      "text" : "Lastly, the “plug and play” property of TM further motivates us to domain adaptation, where we adapt a single general-domain model to a specific domain by using domain-specific monolingual TM.\nData To simulate a diverse multi-domain setting, we use the data splits in Aharoni and Goldberg (2020) originally collected by Koehn and Knowles (2017). It includes German-English parallel data for train/dev/test sets in five domains: Medical, Law, IT, Koran and Subtitles. Similar to the experiments in §4.3, we only use one fourth of bilingual pairs for training. The target side of the remaining data is treated as additional monolingual data for building domain-specific TM, and the source side is discarded. The data statistics can be found in the upper block of Table 4. The dev and test sets for each domain contains 2K instances.\nModels We first train a Transformer Base baseline (model #1) on the concatenation of bilingual pairs in all domains. As in §4.3, we train our model using fixed Etgt (model #4). One advantage of our approach is the possibility of training a single model which can be adapted to any new domain at the inference time without any re-training, by just switching the TM. When adapting to a new TM, we do not re-train our model. As the purpose here is to verify that our approach can tackle domain adaptation without any domain-specific training, we leave the comparison and combination of other domain adaptation techniques (Moore and Lewis,\n2010; Chu and Wang, 2018) as future work.\nResults The results are presented in Table 4. We can see that when only using the bilingual data, the TM-augmented model obtains higher BLEU scores in domains with less data but slightly lower scores in other domains compared to the non-TM baseline. However, as we switch the TM to domain-specific TM, the translation quality is significantly boosted in all domains, improving the non-TM baseline by an average of 1.85 BLEU points, with improvements as large as 2.57 BLEU points on Law and 2.51 BLEU point on Medical. We also attempt to combine all domain-specific TMs to one and use it for all domains (the last row in Table 4). However, we do not obtain noticeable improvement. This reveals that the out-of-domain data can provide little help so that a smaller in-domain TM is sufficient, which is also confirmed by the fact that about 90.21% of the retrieved sentences come from the corresponding domain in the combined TM."
    }, {
      "heading" : "4.5 Running Speed",
      "text" : "With the help of FAISS in-GPU index, search over millions of vectors can be made incredibly efficient (often in tens of milliseconds). In our implementation, the memory search performs even faster than naive BM253. For the results in Table 2, taking the vanilla Transformer Base model (model #1) as the baseline. The inference latency of our models (both model #4 and model #5) is about 1.36 times of the baseline (all use a single Nividia V100 GPU). Note that the corresponding number for the previous state-of-the-art model (Xia et al., 2019) is 1.80. As for training cost, the averaged time cost per training step of model #4 and model #5 is 2.62 times and 2.76 times of the baseline respectively, which are on par with traditional TM-augmented baselines (model #2 is 2.59 times) (all use two Nividia V100 GPUs). Table 5 presents the results. In addition, we also observe that memory-augmented models converge much faster than vanilla models in terms of training steps."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We introduced an effective approach that augments NMT models with monolingual TM. We show that a task-specific cross-lingual memory retriever can be learned by end-to-end MT training. Our approach achieves new state-of-the-art results on sev-\n3Elasticsearch Implementation: https://www. elastic.co/\neral datasets, leads to large gains in low-resource scenarios where the bilingual data is limited, and can specialize a NMT model for specific domains without further training.\nFuture work should aim to build over our proposed framework. Two obvious directions are: (1) Even though our experiments validated that the whole framework can be learned from scratch using standard MT corpora, it is possible to initialize each model component in our framework with massively pre-trained models for performance enhancement; and (2) The NMT model can benefit from aggregating over a set of diverse memories, which is not explicitly encouraged in current design."
    } ],
    "references" : [ {
      "title" : "Unsupervised domain clusters in pretrained language models",
      "author" : [ "Roee Aharoni", "Yoav Goldberg." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7747– 7763.",
      "citeRegEx" : "Aharoni and Goldberg.,? 2020",
      "shortCiteRegEx" : "Aharoni and Goldberg.",
      "year" : 2020
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Signature verification using a” siamese” time delay neural network",
      "author" : [ "Jane Bromley", "Isabelle Guyon", "Yann LeCun", "Eduard Säckinger", "Roopak Shah." ],
      "venue" : "Proceedings of the 6th International Conference on Neural Information Processing Systems, pages",
      "citeRegEx" : "Bromley et al\\.,? 1993",
      "shortCiteRegEx" : "Bromley et al\\.",
      "year" : 1993
    }, {
      "title" : "Language models are few-shot",
      "author" : [ "frey Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "E. Sigler", "Mateusz Litwin", "Scott Gray", "Benjamin Chess", "J. Clark", "Christopher Berner", "Sam McCandlish", "A. Radford", "Ilya Sutskever", "Dario Amodei" ],
      "venue" : null,
      "citeRegEx" : "Wu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural fuzzy repair: Integrating fuzzy matches into neural machine translation",
      "author" : [ "Bram Bulte", "Arda Tezcan." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1800–1809.",
      "citeRegEx" : "Bulte and Tezcan.,? 2019",
      "shortCiteRegEx" : "Bulte and Tezcan.",
      "year" : 2019
    }, {
      "title" : "Skeleton-to-response: Dialogue generation guided by retrieval memory",
      "author" : [ "Deng Cai", "Yan Wang", "Wei Bi", "Zhaopeng Tu", "Xiaojiang Liu", "Wai Lam", "Shuming Shi." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the",
      "citeRegEx" : "Cai et al\\.,? 2019a",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2019
    }, {
      "title" : "Retrievalguided dialogue response generation via a matchingto-generation framework",
      "author" : [ "Deng Cai", "Yan Wang", "Wei Bi", "Zhaopeng Tu", "Xiaojiang Liu", "Shuming Shi." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Cai et al\\.,? 2019b",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to reuse translations: Guiding neural machine translation with examples",
      "author" : [ "Qian Cao", "Shaohui Kuang", "Deyi Xiong." ],
      "venue" : "arXiv preprint arXiv:1911.10732.",
      "citeRegEx" : "Cao et al\\.,? 2019",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2019
    }, {
      "title" : "Encoding gated translation memory into neural machine translation",
      "author" : [ "Qian Cao", "Deyi Xiong." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3042–3047.",
      "citeRegEx" : "Cao and Xiong.,? 2018",
      "shortCiteRegEx" : "Cao and Xiong.",
      "year" : 2018
    }, {
      "title" : "Reading Wikipedia to answer opendomain questions",
      "author" : [ "Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870–",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "A survey of domain adaptation for neural machine translation",
      "author" : [ "Chenhui Chu", "Rui Wang." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 1304–1319.",
      "citeRegEx" : "Chu and Wang.,? 2018",
      "shortCiteRegEx" : "Chu and Wang.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Understanding back-translation at scale",
      "author" : [ "Sergey Edunov", "Myle Ott", "Michael Auli", "David Grangier." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 489–500.",
      "citeRegEx" : "Edunov et al\\.,? 2018",
      "shortCiteRegEx" : "Edunov et al\\.",
      "year" : 2018
    }, {
      "title" : "Data augmentation for low-resource neural machine translation",
      "author" : [ "Marzieh Fadaee", "Arianna Bisazza", "Christof Monz." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Fadaee et al\\.,? 2017",
      "shortCiteRegEx" : "Fadaee et al\\.",
      "year" : 2017
    }, {
      "title" : "Memory-augmented neural machine translation",
      "author" : [ "Yang Feng", "Shiyue Zhang", "Andi Zhang", "Dong Wang", "Andrew Abel." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1390–1399.",
      "citeRegEx" : "Feng et al\\.,? 2017",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2017
    }, {
      "title" : "Incorporating copying mechanism in sequence-to-sequence learning",
      "author" : [ "Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Gu et al\\.,? 2016",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2016
    }, {
      "title" : "Search engine guided neural machine translation",
      "author" : [ "Jiatao Gu", "Yong Wang", "Kyunghyun Cho", "Victor OK Li." ],
      "venue" : "AAAI, pages 5133–5140.",
      "citeRegEx" : "Gu et al\\.,? 2018",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2018
    }, {
      "title" : "On using monolingual corpora in neural machine translation",
      "author" : [ "Caglar Gulcehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Loic Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1503.03535.",
      "citeRegEx" : "Gulcehre et al\\.,? 2015",
      "shortCiteRegEx" : "Gulcehre et al\\.",
      "year" : 2015
    }, {
      "title" : "Generating sentences by editing prototypes",
      "author" : [ "Kelvin Guu", "Tatsunori B. Hashimoto", "Yonatan Oren", "Percy Liang." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:437–450.",
      "citeRegEx" : "Guu et al\\.,? 2018",
      "shortCiteRegEx" : "Guu et al\\.",
      "year" : 2018
    }, {
      "title" : "Realm: Retrievalaugmented language model pre-training",
      "author" : [ "Kelvin Guu", "Kenton Lee", "Zora Tung", "Panupong Pasupat", "Ming-Wei Chang." ],
      "venue" : "arXiv preprint arXiv:2002.08909.",
      "citeRegEx" : "Guu et al\\.,? 2020",
      "shortCiteRegEx" : "Guu et al\\.",
      "year" : 2020
    }, {
      "title" : "A retrieve-and-edit framework for predicting structured outputs",
      "author" : [ "Tatsunori B Hashimoto", "Kelvin Guu", "Yonatan Oren", "Percy S Liang." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 10052–10062.",
      "citeRegEx" : "Hashimoto et al\\.,? 2018",
      "shortCiteRegEx" : "Hashimoto et al\\.",
      "year" : 2018
    }, {
      "title" : "Dual learning for machine translation",
      "author" : [ "Di He", "Yingce Xia", "Tao Qin", "Liwei Wang", "Nenghai Yu", "Tie-Yan Liu", "Wei-Ying Ma." ],
      "venue" : "Advances in neural information processing systems, 29:820–828.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Revisiting self-training for neural sequence generation",
      "author" : [ "Junxian He", "Jiatao Gu", "Jiajun Shen", "Marc’Aurelio Ranzato" ],
      "venue" : "In International Conference on Learning Representations",
      "citeRegEx" : "He et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2019
    }, {
      "title" : "Fast and accurate neural machine translation with translation memory",
      "author" : [ "Qiuxiang He", "Guoping Huang", "Qu Cui", "Li Li", "Lemao Liu." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "He et al\\.,? 2021",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2021
    }, {
      "title" : "Bridging smt and tm with translation recommendation",
      "author" : [ "Yifan He", "Yanjun Ma", "Josef van Genabith", "Andy Way." ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 622–630.",
      "citeRegEx" : "He et al\\.,? 2010",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2010
    }, {
      "title" : "Transmart: a practical interactive machine translation system",
      "author" : [ "Guoping Huang", "Lemao Liu", "Xing Wang", "Longyue Wang", "Huayang Li", "Zhaopeng Tu", "Chengyan Huang", "Shuming Shi." ],
      "venue" : "arXiv preprint arXiv.",
      "citeRegEx" : "Huang et al\\.,? 2021",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2021
    }, {
      "title" : "Montreal neural machine translation systems for wmt’15",
      "author" : [ "Sébastien Jean", "Orhan Firat", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio." ],
      "venue" : "Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 134–140.",
      "citeRegEx" : "Jean et al\\.,? 2015",
      "shortCiteRegEx" : "Jean et al\\.",
      "year" : 2015
    }, {
      "title" : "Selftraining sampling with monolingual data uncertainty for neural machine translation",
      "author" : [ "Wenxiang Jiao", "Xing Wang", "Zhaopeng Tu", "Shuming Shi", "R. Michael Lyu", "Irwin King." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Jiao et al\\.,? 2021",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 2021
    }, {
      "title" : "Billion-scale similarity search with gpus",
      "author" : [ "Jeff Johnson", "Matthijs Douze", "Hervé Jégou." ],
      "venue" : "IEEE Transactions on Big Data.",
      "citeRegEx" : "Johnson et al\\.,? 2019",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2019
    }, {
      "title" : "Dense passage retrieval for open-domain question answering",
      "author" : [ "Vladimir Karpukhin", "Barlas Oguz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Karpukhin et al\\.,? 2020",
      "shortCiteRegEx" : "Karpukhin et al\\.",
      "year" : 2020
    }, {
      "title" : "Nearest neighbor machine translation",
      "author" : [ "Urvashi Khandelwal", "Angela Fan", "Dan Jurafsky", "Luke Zettlemoyer", "Mike Lewis." ],
      "venue" : "arXiv preprint arXiv:2010.00710.",
      "citeRegEx" : "Khandelwal et al\\.,? 2020",
      "shortCiteRegEx" : "Khandelwal et al\\.",
      "year" : 2020
    }, {
      "title" : "Generalization through memorization: Nearest neighbor language models",
      "author" : [ "Urvashi Khandelwal", "Omer Levy", "Dan Jurafsky", "Luke Zettlemoyer", "Mike Lewis." ],
      "venue" : "arXiv preprint arXiv:1911.00172.",
      "citeRegEx" : "Khandelwal et al\\.,? 2019",
      "shortCiteRegEx" : "Khandelwal et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Statistical significance tests for machine translation evaluation",
      "author" : [ "Philipp Koehn." ],
      "venue" : "Proceedings of the 2004 conference on empirical methods in natural language processing, pages 388–395.",
      "citeRegEx" : "Koehn.,? 2004",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2004
    }, {
      "title" : "Six challenges for neural machine translation",
      "author" : [ "Philipp Koehn", "Rebecca Knowles." ],
      "venue" : "arXiv preprint arXiv:1706.03872.",
      "citeRegEx" : "Koehn and Knowles.,? 2017",
      "shortCiteRegEx" : "Koehn and Knowles.",
      "year" : 2017
    }, {
      "title" : "Convergence of translation memory and statistical machine translation",
      "author" : [ "Philipp Koehn", "Jean Senellart." ],
      "venue" : "Proceedings of AMTA Workshop on MT Research and the Translation Industry, pages 21–31.",
      "citeRegEx" : "Koehn and Senellart.,? 2010",
      "shortCiteRegEx" : "Koehn and Senellart.",
      "year" : 2010
    }, {
      "title" : "Latent retrieval for weakly supervised open domain question answering",
      "author" : [ "Kenton Lee", "Ming-Wei Chang", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086–6096.",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "Pre-training via paraphrasing",
      "author" : [ "Mike Lewis", "Marjan Ghazvininejad", "Gargi Ghosh", "Armen Aghajanyan", "Sida Wang", "Luke Zettlemoyer." ],
      "venue" : "Advances in Neural Information Processing Systems, 33.",
      "citeRegEx" : "Lewis et al\\.,? 2020a",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Retrieval-augmented generation for knowledge-intensive nlp",
      "author" : [ "Patrick Lewis", "Ethan Perez", "Aleksandara Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich Küttler", "Mike Lewis", "Wen-tau Yih", "Tim Rocktäschel" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs",
      "author" : [ "Yury A Malkov", "Dmitry A Yashunin." ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence.",
      "citeRegEx" : "Malkov and Yashunin.,? 2018",
      "shortCiteRegEx" : "Malkov and Yashunin.",
      "year" : 2018
    }, {
      "title" : "Intelligent selection of language model training data",
      "author" : [ "Robert C. Moore", "William Lewis." ],
      "venue" : "Proceedings of the ACL 2010 Conference Short Papers, pages 220–224.",
      "citeRegEx" : "Moore and Lewis.,? 2010",
      "shortCiteRegEx" : "Moore and Lewis.",
      "year" : 2010
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J. Liu", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073–",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "Improving neural machine translation models with monolingual data",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips)",
      "author" : [ "Anshumali Shrivastava", "Ping Li." ],
      "venue" : "Advances in neural information processing systems, 27:2321–2329.",
      "citeRegEx" : "Shrivastava and Li.,? 2014",
      "shortCiteRegEx" : "Shrivastava and Li.",
      "year" : 2014
    }, {
      "title" : "The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages",
      "author" : [ "Ralf Steinberger", "Bruno Pouliquen", "Anna Widiger", "Camelia Ignat", "Tomaz Erjavec", "Dan Tufis", "Dániel Varga." ],
      "venue" : "arXiv preprint cs/0609058.",
      "citeRegEx" : "Steinberger et al\\.,? 2006",
      "shortCiteRegEx" : "Steinberger et al\\.",
      "year" : 2006
    }, {
      "title" : "Searching translation memories for paraphrases",
      "author" : [ "Masao Utiyama", "Graham Neubig", "Takashi Onishi", "Eiichiro Sumita." ],
      "venue" : "Machine Translation Summit, volume 13, pages 325–331.",
      "citeRegEx" : "Utiyama et al\\.,? 2011",
      "shortCiteRegEx" : "Utiyama et al\\.",
      "year" : 2011
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Integrating translation memory into phrase-based machine translation during decoding",
      "author" : [ "Kun Wang", "Chengqing Zong", "Keh-Yih Su." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
      "citeRegEx" : "Wang et al\\.,? 2013",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2013
    }, {
      "title" : "Retrieve and refine: Improved sequence generation models for dialogue",
      "author" : [ "Jason Weston", "Emily Dinan", "Alexander Miller." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational",
      "citeRegEx" : "Weston et al\\.,? 2018",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2018
    }, {
      "title" : "Response generation by context-aware prototype editing",
      "author" : [ "Yu Wu", "Furu Wei", "Shaohan Huang", "Yunli Wang", "Zhoujun Li", "Ming Zhou." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7281–7288.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Graph based translation memory for neural machine translation",
      "author" : [ "Mengzhou Xia", "Guoping Huang", "Lemao Liu", "Shuming Shi." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7297–7304.",
      "citeRegEx" : "Xia et al\\.,? 2019",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2019
    }, {
      "title" : "Boosting neural machine translation with similar translations",
      "author" : [ "Jitao Xu", "Josep Crego", "Jean Senellart." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1580–1590.",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "The effect of translation memory databases on productivity",
      "author" : [ "Masaru Yamada." ],
      "venue" : "Translation research projects, 3:63–73.",
      "citeRegEx" : "Yamada.,? 2011",
      "shortCiteRegEx" : "Yamada.",
      "year" : 2011
    }, {
      "title" : "Guiding neural machine translation with retrieved translation pieces",
      "author" : [ "Jingyi Zhang", "Masao Utiyama", "Eiichro Sumita", "Graham Neubig", "Satoshi Nakamura." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Com-",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 54,
      "context" : "For the task of Machine Translation (MT), inspired by the Computer-Aided Translation (CAT) tools by professional human translators for increasing productivity for decades (Yamada, 2011), the usefulness of Translation Memory (TM) has long been recognized (Huang et al.",
      "startOffset" : 171,
      "endOffset" : 185
    }, {
      "referenceID" : 25,
      "context" : "For the task of Machine Translation (MT), inspired by the Computer-Aided Translation (CAT) tools by professional human translators for increasing productivity for decades (Yamada, 2011), the usefulness of Translation Memory (TM) has long been recognized (Huang et al., 2021).",
      "startOffset" : 254,
      "endOffset" : 274
    }, {
      "referenceID" : 16,
      "context" : ", source-target pairs) from the training corpus based on source-side similarity such as lexical overlaps (Gu et al., 2018; Zhang et al., 2018; Xia et al., 2019), embedding-based matches",
      "startOffset" : 105,
      "endOffset" : 160
    }, {
      "referenceID" : 55,
      "context" : ", source-target pairs) from the training corpus based on source-side similarity such as lexical overlaps (Gu et al., 2018; Zhang et al., 2018; Xia et al., 2019), embedding-based matches",
      "startOffset" : 105,
      "endOffset" : 160
    }, {
      "referenceID" : 52,
      "context" : ", source-target pairs) from the training corpus based on source-side similarity such as lexical overlaps (Gu et al., 2018; Zhang et al., 2018; Xia et al., 2019), embedding-based matches",
      "startOffset" : 105,
      "endOffset" : 160
    }, {
      "referenceID" : 8,
      "context" : "(Cao and Xiong, 2018), or a hybrid (Bulte and Tezcan, 2019; Xu et al.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 4,
      "context" : "(Cao and Xiong, 2018), or a hybrid (Bulte and Tezcan, 2019; Xu et al., 2020); In the generation stage, the retrieved translations are injected into a standard NMT model by attending over them with sophisticated memory networks (Gu et al.",
      "startOffset" : 35,
      "endOffset" : 76
    }, {
      "referenceID" : 53,
      "context" : "(Cao and Xiong, 2018), or a hybrid (Bulte and Tezcan, 2019; Xu et al., 2020); In the generation stage, the retrieved translations are injected into a standard NMT model by attending over them with sophisticated memory networks (Gu et al.",
      "startOffset" : 35,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : ", 2021) or directly concatenating them to the source input (Bulte and Tezcan, 2019; Xu et al., 2020), or biasing the word distribution during decoding (Zhang et al.",
      "startOffset" : 59,
      "endOffset" : 100
    }, {
      "referenceID" : 53,
      "context" : ", 2021) or directly concatenating them to the source input (Bulte and Tezcan, 2019; Xu et al., 2020), or biasing the word distribution during decoding (Zhang et al.",
      "startOffset" : 59,
      "endOffset" : 100
    }, {
      "referenceID" : 55,
      "context" : ", 2020), or biasing the word distribution during decoding (Zhang et al., 2018).",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : "Specifically, we align source-side sentences and the corresponding targetside translations in a latent vector space using a simple dual-encoder framework (Bromley et al., 1993), such that the distance in the latent space",
      "startOffset" : 154,
      "endOffset" : 176
    }, {
      "referenceID" : 36,
      "context" : "One typical use is to retrieve supporting evidence for open-domain question answering (e.g., Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020).",
      "startOffset" : 86,
      "endOffset" : 153
    }, {
      "referenceID" : 29,
      "context" : "One typical use is to retrieve supporting evidence for open-domain question answering (e.g., Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020).",
      "startOffset" : 86,
      "endOffset" : 153
    }, {
      "referenceID" : 18,
      "context" : "Recently, retrieval-guided generation has gained increasing interest in a wide range of text generation tasks such as language modeling (Guu et al., 2018; Khandelwal et al., 2019; Guu et al., 2020), dialogue response generation (Weston et al.",
      "startOffset" : 136,
      "endOffset" : 197
    }, {
      "referenceID" : 31,
      "context" : "Recently, retrieval-guided generation has gained increasing interest in a wide range of text generation tasks such as language modeling (Guu et al., 2018; Khandelwal et al., 2019; Guu et al., 2020), dialogue response generation (Weston et al.",
      "startOffset" : 136,
      "endOffset" : 197
    }, {
      "referenceID" : 19,
      "context" : "Recently, retrieval-guided generation has gained increasing interest in a wide range of text generation tasks such as language modeling (Guu et al., 2018; Khandelwal et al., 2019; Guu et al., 2020), dialogue response generation (Weston et al.",
      "startOffset" : 136,
      "endOffset" : 197
    }, {
      "referenceID" : 20,
      "context" : "2019a,b), code generation (Hashimoto et al., 2018) and other knowledge-intensive generation (Lewis et al.",
      "startOffset" : 26,
      "endOffset" : 50
    }, {
      "referenceID" : 44,
      "context" : "Another successful method for leveraging monolingual data in NMT is back-translation (Sennrich et al., 2016; Fadaee et al., 2017; Edunov et al., 2018; He et al., 2016), where a reverse translation model is used to translate monolingual sentences from the target language to the source language to",
      "startOffset" : 85,
      "endOffset" : 167
    }, {
      "referenceID" : 13,
      "context" : "Another successful method for leveraging monolingual data in NMT is back-translation (Sennrich et al., 2016; Fadaee et al., 2017; Edunov et al., 2018; He et al., 2016), where a reverse translation model is used to translate monolingual sentences from the target language to the source language to",
      "startOffset" : 85,
      "endOffset" : 167
    }, {
      "referenceID" : 12,
      "context" : "Another successful method for leveraging monolingual data in NMT is back-translation (Sennrich et al., 2016; Fadaee et al., 2017; Edunov et al., 2018; He et al., 2016), where a reverse translation model is used to translate monolingual sentences from the target language to the source language to",
      "startOffset" : 85,
      "endOffset" : 167
    }, {
      "referenceID" : 21,
      "context" : "Another successful method for leveraging monolingual data in NMT is back-translation (Sennrich et al., 2016; Fadaee et al., 2017; Edunov et al., 2018; He et al., 2016), where a reverse translation model is used to translate monolingual sentences from the target language to the source language to",
      "startOffset" : 85,
      "endOffset" : 167
    }, {
      "referenceID" : 27,
      "context" : "Recent studies (Jiao et al., 2021; He et al., 2019) showed that self-training, where the synthetic parallel sentences are created by translating monolingual sentences in the source language, is also helpful.",
      "startOffset" : 15,
      "endOffset" : 51
    }, {
      "referenceID" : 22,
      "context" : "Recent studies (Jiao et al., 2021; He et al., 2019) showed that self-training, where the synthetic parallel sentences are created by translating monolingual sentences in the source language, is also helpful.",
      "startOffset" : 15,
      "endOffset" : 51
    }, {
      "referenceID" : 2,
      "context" : "To address this, we implement the retrieval model using a simple dual-encoder framework (Bromley et al., 1993) such that the selection of the most",
      "startOffset" : 88,
      "endOffset" : 110
    }, {
      "referenceID" : 39,
      "context" : "With performant data structures and search algorithms (e.g., Shrivastava and Li, 2014; Malkov and Yashunin, 2018), the retrieval can be done efficiently.",
      "startOffset" : 54,
      "endOffset" : 113
    }, {
      "referenceID" : 48,
      "context" : "We implement the two sentence encoders using two independent Transformers (Vaswani et al., 2017).",
      "startOffset" : 74,
      "endOffset" : 96
    }, {
      "referenceID" : 28,
      "context" : "In practice, the dense representations of all sentences in TM can be pre-computed and indexed using FAISS (Johnson et al., 2019), an open-source toolkit for efficient vector search.",
      "startOffset" : 106,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "Our translation model is built upon the standard encoder-decoder NMT model (Bahdanau et al., 2015; Vaswani et al., 2017): the (source) encoder transforms the source sentence x into dense vector representations.",
      "startOffset" : 75,
      "endOffset" : 120
    }, {
      "referenceID" : 48,
      "context" : "Our translation model is built upon the standard encoder-decoder NMT model (Bahdanau et al., 2015; Vaswani et al., 2017): the (source) encoder transforms the source sentence x into dense vector representations.",
      "startOffset" : 75,
      "endOffset" : 120
    }, {
      "referenceID" : 15,
      "context" : "Second, we consider each attention score as a probability of copying the corresponding token (Gu et al., 2016; See et al., 2017).",
      "startOffset" : 93,
      "endOffset" : 128
    }, {
      "referenceID" : 43,
      "context" : "Second, we consider each attention score as a probability of copying the corresponding token (Gu et al., 2016; See et al., 2017).",
      "startOffset" : 93,
      "endOffset" : 128
    }, {
      "referenceID" : 48,
      "context" : "We build our model using Transformer blocks with the same configuration as Transformer Base (Vaswani et al., 2017) (8 attention heads, 512 dimensional hidden state, and 2048 dimensional feed-forward state).",
      "startOffset" : 92,
      "endOffset" : 114
    }, {
      "referenceID" : 32,
      "context" : "We use Adam optimizer (Kingma and Ba, 2014) and train models with up to 100K",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 33,
      "context" : "01, tested by bootstrap re-sampling (Koehn, 2004).",
      "startOffset" : 36,
      "endOffset" : 49
    }, {
      "referenceID" : 46,
      "context" : "Data We use the JRC-Acquis corpus (Steinberger et al., 2006) for our experiments.",
      "startOffset" : 34,
      "endOffset" : 60
    }, {
      "referenceID" : 48,
      "context" : "The resulted model is equivalent to the Transformer Base model (Vaswani et al., 2017).",
      "startOffset" : 63,
      "endOffset" : 85
    }, {
      "referenceID" : 52,
      "context" : "Notably, our best model (model #5) surpasses the best reported model (Xia et al., 2019) by 1.",
      "startOffset" : 69,
      "endOffset" : 87
    }, {
      "referenceID" : 44,
      "context" : "models with back-translation (BT) (Sennrich et al., 2016), a popular way of utilizing monolingual data for NMT.",
      "startOffset" : 34,
      "endOffset" : 57
    }, {
      "referenceID" : 52,
      "context" : "Note that the corresponding number for the previous state-of-the-art model (Xia et al., 2019) is 1.",
      "startOffset" : 75,
      "endOffset" : 93
    } ],
    "year" : 2021,
    "abstractText" : "Prior work has proved that Translation memory (TM) can boost the performance of Neural Machine Translation (NMT). In contrast to existing work that uses bilingual corpus as TM and employs source-side similarity search for memory retrieval, we propose a new framework that uses monolingual memory and performs learnable memory retrieval in a crosslingual manner. Our framework has unique advantages. First, the cross-lingual memory retriever allows abundant monolingual data to be TM. Second, the memory retriever and NMT model can be jointly optimized for the ultimate translation goal. Experiments show that the proposed method obtains substantial improvements. Remarkably, it even outperforms strong TM-augmented NMT baselines using bilingual TM. Owning to the ability to leverage monolingual data, our model also demonstrates effectiveness in low-resource and domain adaptation scenarios.",
    "creator" : "LaTeX with hyperref"
  }
}