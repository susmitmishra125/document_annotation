{
  "name" : "2021.acl-long.208.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "The Limitations of Limited Context for Constituency Parsing",
    "authors" : [ "Yuchen Li", "Andrej Risteski" ],
    "emails" : [ "yuchenl4@andrew.cmu.edu", "aristesk@andrew.cmu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2675–2687\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2675\nHowever, even heuristic (much less fully mathematical) understanding of why and when these architectures work is lagging severely behind. In this work, we answer representational questions raised by the architectures in (Shen et al., 2018a, 2019), as well as some transition-based syntax-aware language models (Dyer et al., 2016): what kind of syntactic structure can current neural approaches to syntax represent? Concretely, we ground this question in the sandbox of probabilistic context-free-grammars (PCFGs), and identify a key aspect of the representational power of these approaches: the amount and directionality of context that the predictor has access to when forced to make parsing decision. We show that with limited context (either bounded, or unidirectional), there are PCFGs, for which these approaches cannot represent the max-likelihood parse; conversely, if the context is unlimited, they can represent the max-likelihood parse of any PCFG."
    }, {
      "heading" : "1 Introduction",
      "text" : "Neural approaches have been steadily making their way to NLP in recent years. By and large however,\nthe neural techniques that have been scaled-up the most and receive widespread usage do not explicitly try to encode discrete structure that is natural to language, e.g. syntax. The reason for this is perhaps not surprising: neural models have largely achieved substantial improvements in unsupervised settings, BERT (Devlin et al., 2019) being the defacto method for unsupervised pre-training in most NLP settings. On the other hand unsupervised syntactic tasks, e.g. unsupervised syntactic parsing, have long been known to be very difficult tasks (Htut et al., 2018). However, since incorporating syntax has been shown to improve language modeling (Kim et al., 2019b) as well as natural language inference (Chen et al., 2017; Pang et al., 2019; He et al., 2020), syntactic parsing remains important even in the current era when large pre-trained models, like BERT (Devlin et al., 2019), are available.\nArguably, the breakthrough works in unsupervised constituency parsing in a neural manner were (Shen et al., 2018a, 2019), achieving F1 scores 42.8 and 49.4 on the WSJ Penn Treebank dataset (Htut et al., 2018; Shen et al., 2019). Both of these architectures, however (especially Shen et al., 2018a) are quite intricate, and it’s difficult to evaluate what their representational power is (i.e. what kinds of structure can they recover). Moreover, as subsequent more thorough evaluations show (Kim et al., 2019b,a), these methods still have a rather large performance gap with the oracle binary tree (which is the best binary parse tree according to F1-score) — raising the question of what is missing in these methods.\nWe theoretically answer both questions raised in the prior paragraph. We quantify the representational power of two major frameworks in neural approaches to syntax: learning a syntactic distance (Shen et al., 2018a,b, 2019) and learning to parse through sequential transitions (Dyer et al., 2016; Chelba, 1997). To formalize our results, we con-\nsider the well-established sandbox of probabilistic context-free grammars (PCFGs). Namely, we ask:\nWhen is a neural model based on a syntactic distance or transitions able to represent the maxlikelihood parse of a sentence generated from a PCFG?\nWe focus on a crucial “hyperparameter” common to practical implementations of both families of methods that turns out to govern the representational power: the amount and type of context the model is allowed to use when making its predictions. Briefly, for every position t in the sentence, syntactic distance models learn a distance dt to the previous token — the tree is then inferred from this distance; transition-based models iteratively construct the parse tree by deciding, at each position t, what operations to perform on a partial parse up to token t. A salient feature of both is the context, that is, which tokens is dt a function of (correspondingly, which tokens can the choice of operations at token t depend on)?\nWe show that when the context is either bounded (that is, dt only depends on a bounded window around the t-th token) or unidirectional (that is, dt only considers the tokens to the left of the tth token), there are PCFGs for which no distance metric (correspondingly, no algorithm to choose the sequence of transitions) works. On the other hand, if the context is unbounded in both directions then both methods work: that is, for any parse, we can design a distance metric (correspondingly, a sequence of transitions) that recovers it.\nThis is of considerable importance: in practical implementations the context is either bounded (e.g. in Shen et al., 2018a, the distance metric is parametrized by a convolutional kernel with a constant width) or unidirectional (e.g. in Shen et al., 2019, the distance metric is computed by a LSTM, which performs a left-to-right computation).\nThis formally confirms a conjecture of Htut et al. (2018), who suggested that because these models commit to parsing decision in a left-to-right fashion and are trained as a part of a language model, it may be difficult for them to capture sufficiently complex syntactic dependencies. Our techniques are fairly generic and seem amenable to analyzing other approaches to syntax. Finally, while the existence of a particular PCFG that is problematic for these methods doesn’t necessarily imply that the difficulties will carry over to real-life data, the PCFGs that are used in our proofs closely track lin-\nguistic intuitions about difficult syntactic structures to infer: the parse depends on words that come much later in the sentence."
    }, {
      "heading" : "2 Overview of Results",
      "text" : "We consider several neural architectures that have shown success in various syntactic tasks, most notably unsupervised constituency parsing and syntax-aware language modeling. The general framework these architectures fall under is as follows: to parse a sentence W = w1w2...wn with a trained neural model, the sentence W is input into the model, which outputs ot at each step t, and finally all the outputs {ot}nt=1 are utilized to produce the parse.\nGiven unbounded time and space resources, by a seminal result of Siegelmann and Sontag (1992), an RNN implementation of this framework is Turing complete. In practice it is common to restrict the form of the output ot in some way. In this paper, we consider the two most common approaches, in which ot is a real number representing a syntactic distance (Section 2.1) (Shen et al., 2018a,b, 2019) or a sequence of parsing operations (Section 2.2) (Chelba, 1997; Chelba and Jelinek, 2000; Dyer et al., 2016). We proceed to describe our results for each architecture in turn."
    }, {
      "heading" : "2.1 Syntactic distance",
      "text" : "Syntactic distance-based neural parsers train a neural network to learn a distance for each pair of adjacent words, depending on the context surrounding the pair of words under consideration. The distances are then used to induce a tree structure (Shen et al., 2018a,b).\nFor a sentence W = w1w2...wn, the syntactic distance between wt−1 and wt (2 ≤ t ≤ n) is defined as dt = d(wt−1, wt | ct), where ct is the context that dt takes into consideration 1. We will show that restricting the surrounding context either in directionality, or in size, results in a poor representational power, while full context confers essentially perfect representational power with respect to PCFGs.\nConcretely, if the context is full, we show:\nTheorem (Informal, full context). For sentenceW generated by any PCFG, if the computation of dt has as context the full sentence and the position index under consideration, i.e. ct = (W, t) and\n1Note that this is not a conditional distribution—we use this notation for convenience.\ndt = d(wt−1, wt | ct), then dt can induce the maximum likelihood parse of W .\nOn the flipside, if the context is unidirectional (i.e. unbounded left-context from the start of the sentence, and even possibly with a bounded look-ahead), the representational power becomes severely impoverished:\nTheorem (Informal, limitation of left-to-right parsing via syntactic distance). There exists a PCFG G such that for any distance measure dt whose computation incorporates only bounded context in at least one direction (left or right), e.g.\nct = (w0, w1, ..., wt+L′) dt = d(wt−1, wt | ct)\nthe probability that dt induces the max likelihood parse is arbitrarily low.\nIn practice, for computational efficiency, parametrizations of syntactic distances fall into the above assumptions of restricted context (Shen et al., 2018a). This puts the ability of these models to learn a complex PCFG syntax into considerable doubt. For formal definitions, see Section 4.2. For formal theorem statements and proofs, see Section 5.\nSubsequently we consider ON-LSTM, an architecture proposed by Shen et al. (2019) improving their previous work (Shen et al., 2018a), which also is based on learning a syntactic distance, but in (Shen et al., 2019) the distances are reduced from the values of a carefully structured master forget gate (see Section 6). While we show ON-LSTM can in principle losslessly represent any parse tree (Theorem 3), calculating the gate values in a left to right fashion (as is done in practice) is subject to the same limitations as the syntactic distance approach:\nTheorem (Informal, limitation of syntactic distance estimation based on ON-LSTM). There exists a PCFG G for which the probability that the syntactic distance converted from an ON-LSTM induces the max likelihood parse is arbitrarily low.\nFor a formal statement, see Section 6 and in particular Theorem 4."
    }, {
      "heading" : "2.2 Transition-based parsing",
      "text" : "In principle, the output ot at each position t of a left-to-right neural models for syntactic parsing need not be restricted to a real-numbered distance or a carefully structured vector. It can also be a\ncombinatorial structure — e.g. a sequence of transitions (Chelba, 1997; Chelba and Jelinek, 2000; Dyer et al., 2016). We adopt a simplification of the neural parameterization in (Dyer et al., 2016) (see Definition 4.7).\nWith full context, Dyer et al. (2016) describes an algorithm to find a sequence of transitions to represent any parse tree, via a “depth-first, leftto-right traversal” of the tree. On the other hand, without full context, we prove that transition-based parsing suffers from the same limitations:\nTheorem (Informal, limitation of transition-based parsing without full context). There exists a PCFG G, such that for any learned transition-based parser with bounded context in at least one direction (left or right), the probability that it returns the max likelihood parse is arbitrarily low.\nFor a formal statement, see Section 7, and in particular Theorem 5. Remark. There is no immediate connection between the syntactic distance-based approaches (including ON-LSTM) and the transition-based parsing framework, so the limitations of transitionbased parsing does not directly imply the stated negative results for syntactic distance or ONLSTM, and vice versa."
    }, {
      "heading" : "2.3 The counterexample family",
      "text" : "Most of our theorems proving limitations on bounded and unidirectional context are based on a PCFG family (Definition 2.1) which draws inspirations from natural language already suggested in (Htut et al., 2018): later words in a sentence can force different syntactic structures earlier in the sentence. For example, consider the two sentences: “I drink coffee with milk.” and “I drink coffee with friends.” Their only difference occurs at their very last words, but their parses differ at some earlier words in each sentence, too, as shown in Figure 1.\nTo formalize this intuition, we define the following PCFG.\nDefinition 2.1 (Right-influenced PCFG). Let m ≥ 2, L′ ≥ 1 be positive integers. The grammar Gm,L′ has starting symbol S, other non-terminals\nAk, Bk, A l k, A r k, B ′ k for all k ∈ {1, 2, ...,m},\nand terminals\nai for all i ∈ {1, 2, ...,m+ 1 + L′},\ncj for all j ∈ {1, 2, ...,m}.\nThe rules of the grammar are\nS → AkBk,∀k ∈ {1, 2, . . . ,m} w. prob.1/m Ak → AlkArk w. prob. 1 Alk →∗ a1a2...ak w. prob. 1 Ark →∗ ak+1ak+2...am+1 w. prob. 1 Bk →∗ B′kck w. prob. 1 B′k →∗ am+2am+3...am+1+L′ w. prob. 1\nin which→∗ means that the left expands into the right through a sequence of rules that conform to the requirements of the Chomsky normal form (CNF, Definition 4.4). Hence the grammar Gm,L′ is in CNF.\nThe language of this grammar is\nL(Gm,L′)={lk=a1a2...am+1+L′ck : 1 ≤ k ≤ m}.\nThe parse of an arbitrary lk is shown in Figure 2. Each lk corresponds to a unique parse determined by the choice of k. The structure of this PCFG is\nsuch that for the parsing algorithms we consider that proceed in a “left-to-right” fashion on lk, before processing the last token ck, it cannot infer the syntactic structure of a1a2...am+1 any better than randomly guessing one of the m possibilities. This is the main intuition behind Theorems 2 and 5.\nRemark. While our theorems focus on the limitation of “left-to-right” parsing, a symmetric argument implies the same limitation of “right-to-left” parsing. Thus, our claim is that unidirectional context (in either direction) limits the expressive power of parsing models."
    }, {
      "heading" : "3 Related Works",
      "text" : "Neural models for parsing were first successfully implemented for supervised settings, e.g. (Vinyals et al., 2015; Dyer et al., 2016; Shen et al., 2018b). Unsupervised tasks remained seemingly out of reach, until the proposal of the Parsing-ReadingPredict Network (PRPN) by Shen et al. (2018a), whose performance was thoroughly verified by extensive experiments in (Htut et al., 2018). The\nfollow-up paper (Shen et al., 2019) introducing the ON-LSTM architecture simplified radically the architecture in (Shen et al., 2018a), while still ultimately attempting to fit a distance metric with the help of carefully designed master forget gates. Subsequent work by Kim et al. (2019a) departed from the usual way neural techniques are integrated in NLP, with great success: they proposed a neural parameterization for the EM algorithm for learning a PCFG, but in a manner that leverages semantic information as well — achieving a large improvement on unsupervised parsing tasks.2\nIn addition to constituency parsing, dependency parsing is another common task for syntactic parsing, but for our analyses on the ability of various approaches to represent the max-likelihood parse of sentences generated from PCFGs, we focus on the task of constituency parsing. Moreover, it’s important to note that there is another line of work aiming to probe the ability of models trained without explicit syntactic consideration (e.g. BERT) to nevertheless discover some (rudimentary) syntactic elements (Bisk and Hockenmaier, 2015; Linzen et al., 2016; Choe and Charniak, 2016; Kuncoro et al., 2018; Williams et al., 2018; Goldberg, 2019; Htut et al., 2019; Hewitt and Manning, 2019; Reif et al., 2019). However, to-date, we haven’t been able to extract parse trees achieving scores that are close to the oracle binarized trees on standard benchmarks (Kim et al., 2019b,a).\nMethodologically, our work is closely related to a long line of works aiming to characterize the representational power of neural models (e.g. RNNs, LSTMs) through the lens of formal languages and formal models of computation. Some of the works of this flavor are empirical in nature (e.g. LSTMs have been shown to possess stronger abilities to recognize some context-free language and even some context-sensitive language, compared with simple RNNs (Gers and Schmidhuber, 2001; Suzgun et al., 2019) or GRUs (Weiss et al., 2018; Suzgun et al., 2019)); some results are theoretical in nature (e.g. Siegelmann and Sontag (1992)’s proof that with unbounded precision and unbounded time complexity, RNNs are Turing-complete; related results investigate RNNs with bounded precision and computation time (Weiss et al., 2018), as well as\n2By virtue of not relying on bounded or unidirectional context, the Compound PCFG (Kim et al., 2019a) eschews the techniques in our paper. Specifically, by employing a bidirectional LSTM inference network in the process of constructing a tree given a sentence, the parsing is no longer “left-to-right”.\nmemory (Merrill, 2019; Hewitt et al., 2020). Our work contributes to this line of works, but focuses on the task of syntactic parsing instead."
    }, {
      "heading" : "4 Preliminaries",
      "text" : "In this section, we define some basic concepts and introduce the architectures we will consider."
    }, {
      "heading" : "4.1 Probabilistic context-free grammar",
      "text" : "First recall several definitions around formal language, especially probabilistic context free grammar: Definition 4.1 (Probabilistic context-free grammar (PCFG)). Formally, a PCFG (Chomsky, 1956) is a 5-tuple G = (Σ, N, S,R,Π) in which Σ is the set of terminals, N is the set of non-terminals, S ∈ N is the start symbol, R is the set of production rules of the form r = (rL → rR), where rL ∈ N , rR is of the form B1B2...Bm, m ∈ Z+, and ∀i ∈ {1, 2, ...,m}, Bi ∈ (Σ ∪ N). Finally, Π : R 7→ [0, 1] is the rule probability function, in which for any r = (A→ B1B2...Bm) ∈ R, Π(r) is the conditional probability\nP (rR = B1B2...Bm | rL = A).\nDefinition 4.2 (Parse tree). Let TG denote the set of parse trees that G can derive. Each t ∈ TG is associated with yield(t) ∈ Σ∗, the sequence of terminals composed of the leaves of t and PT (t) ∈ [0, 1], the probability of the parse tree, defined by the product of the probabilities of the rules in the derivation of t. Definition 4.3 (Language and sentence). The language of G is\nL(G) = {s ∈ Σ∗ : ∃t ∈ TG,yield(t) = s}.\nEach s ∈ L(G) is called a sentence in L(G), and is associated with the set of parses TG(s) = {t ∈ TG |yield(t) = s}, the set of max likelihood parses, arg maxt∈TG(s) PT (t), and its probability PS(s) = ∑ t∈TG(s) PT (t). Definition 4.4 (Chomsky normal form (CNF)). A PCFG G = (Σ, N, S,R,Π) is in CNF (Chomsky, 1959) if we require, in addition to Definition 4.1, that each rule r ∈ R is in the form A → B1B2 where B1, B2 ∈ N \\ {S}; A → a where a ∈ Σ, a 6= ; or S → which is only allowed if the empty string ∈ L(G).\nEvery PCFGG can be converted into a PCFGG′ in CNF such that L(G) = L(G′) (Hopcroft et al., 2006)."
    }, {
      "heading" : "4.2 Syntactic distance",
      "text" : "The Parsing-Reading-Predict Networks (PRPN) (Shen et al., 2018a) is one of the leading approaches to unsupervised constituency parsing. The parsing network (which computes the parse tree, hence the only part we focus on in our paper) is a convolutional network that computes the syntactic distances dt = d(wt−1, wt) (defined in Section 2.1) based on the past L words. A deterministic greedy tree induction algorithm is then used to produce a parse tree as follows. First, we split the sentence w1...wn into two constituents, w1...wt−1 and wt...wn, where t ∈ argmax{dt}nt=2 and form the left and right subtrees of t. We recursively repeat this procedure for the newly created constituents. An algorithmic form of this procedure is included as Algorithm 1 in Appendix A.\nNote that, due to the deterministic nature of the tree-induction process, the ability of PRPN to learn a PCFG is completely contingent upon learning a good syntactic distance."
    }, {
      "heading" : "4.3 The ordered neuron architecture",
      "text" : "Building upon the idea of representing the syntactic information with a real-valued distance measure at each position, a simple extension is to associate each position with a learned vector, and then use the vector for syntactic parsing. The ordered-neuron LSTM (ON-LSTM, Shen et al., 2019) proposes that the nodes that are closer to the root in the parse tree generate a longer span of terminals, and therefore should be less frequently “forgotten” than nodes that are farther away from the root. The difference in the frequency of forgetting is captured by a carefully designed master forget gate vector f̃ , as shown in Figure 3 (in Appendix B). Formally:\nDefinition 4.5 (Master forget gates, Shen et al., 2019). Given the input sentence W = w1w2...wn and a trained ON-LSTM, running the ON-LSTM on W gives the master forget gates, which are a sequence of D-dimensional vectors {f̃t}nt=1, in which at each position t, f̃t = f̃t(w1, ..., wt) ∈ [0, 1]D. Moreover, let f̃t,j represent the j-th dimension of f̃t. The ON-LSTM architectures requires that f̃t,1 = 0, f̃t,D = 1, and\n∀i < j, f̃t,i ≤ f̃t,j . When parsing a sentence, the real-valued master forget gate vector f̃t at each position t is reduced to a single real number representing the syntactic distance dt at position t (see (1)) (Shen et al., 2018a). Then, use the syntactic distances to obtain a parse."
    }, {
      "heading" : "4.4 Transition-based parsing",
      "text" : "In addition to outputting a single real numbered distance or a vector at each position t, a left-to-right model can also parse a sentence by outputting a sequence of “transitions” at each position t, an idea proposed in some traditional parsing approaches (Sagae and Lavie, 2005; Chelba, 1997; Chelba and Jelinek, 2000), and also some more recent neural parameterization (Dyer et al., 2016).\nWe introduce several items of notation:\n• zti : the i-th transition performed when reading in wt, the t-th token of the sentence\nW = w1w2...wn.\n• Nt: the number of transitions performed between reading in the token wt and reading in the next token wt+1. • Zt: the sequence of transitions after reading in the prefix w1w2...wt of the sentence.\nZt = {(zj1, z j 2, ..., z j Nj ) | j = 1..t}.\n• Z: the parse of the sentence W . Z = Zn.\nWe base our analysis on the approach introduced in the parsing version of (Dyer et al., 2016), though that work additionally proposes a generator version. 3\nDefinition 4.6 (Transition-based parser). A transition-based parser uses a stack (initialized to empty) and an input buffer (initialized with the sentence w1...wt). At each position t, based on a context ct, the parser outputs a sequence of parsing transitions {zti} Nt i=1, where each z t i can be one of the following transitions (Definition 4.7). The parsing stops when the stack contains one single constituent, and the buffer is empty.\nDefinition 4.7 (Parser transitions, Dyer et al., 2016). A parsing transition can be one of the following three types:\n• NT(X) pushes a non-terminal X onto the stack.\n• SHIFT: removes the first terminal from the input buffer and pushes onto the stack.\n3Dyer et al. (2016) additionally proposes some generator transitions. For simplicity, we analyze the simplest form: we only allow the model to return one parse, composed of the parser transitions, for a given input sentence. Note that this simplified variant still confers full representational power in the “full context” setting (see Section 7).\n• REDUCE: pops from the stack until an open non-terminal is encountered, then pops this non-terminal and assembles everything popped to form a new constituent, labels this new constituent using this non-terminal, and finally pushes this new constituent onto the stack.\nIn Appendix Section C, we provide an example of parsing the sentence “I drink coffee with milk” using the set of transitions given by Definition 4.7.\nThe different context specifications and the corresponding representational powers of the transitionbased parser are discussed in Section 7."
    }, {
      "heading" : "5 Representational Power of Neural Syntactic Distance Methods",
      "text" : "In this section we formalize the results on syntactic distance-based methods. Since the tree induction algorithm always generates a binary tree, we consider only PCFGs in Chomsky normal form (CNF) (Definition 4.4) so that the max likelihood parse of a sentence is also a binary tree structure.\nTo formalize the notion of “representing” a PCFG, we introduce the following definition:\nDefinition 5.1 (Representing PCFG with syntactic distance). LetG be any PCFG in Chomsky Normal Form. A syntactic distance function d is said to be able to p-represent G if for a set of sentences in L(G) whose total probability is at least p, d can correctly induce the tree structure of the max likelihood parse of these sentences without ambiguity.\nRemark. Ambiguities could occur when, for example, there exists t such that dt = dt+1. In this case, the tree induction algorithm would have to break ties when determining the local structure for wt−1wtwt+1. We preclude this possibility in Definition 5.1.\nIn the least restrictive setting, the whole sentence W , as well as the position index t can be taken into consideration when determining each dt. We prove that under this setting, there is a syntactic distance measure that can represent any PCFG.\nTheorem 1 (Full context). Let ct = (W, t). For each PCFG G in Chomsky normal form, there exists a syntactic distance measure dt = d(wt−1, wt | ct) that can 1-represent G.\nProof. For any sentence s = s1s2...sn ∈ L(G), let T be its max likelihood parse tree. Since G is in Chomsky normal form, T is a binary tree. We\nwill describe an assignment of {dt : 2 ≤ t ≤ n} such that their order matches the level at which the branches split in T . Specifically, ∀t ∈ [2, n], let at denote the lowest common ancestor ofwt−1 andwt in T . Let d′t denote the shortest distance between at and the root of T . Finally, let dt = n− d′t. As a result, {dt : 2 ≤ t ≤ n} induces T .\nRemark. Since any PCFG can be converted to Chomsky normal form (Hopcroft et al., 2006), Theorem 1 implies that given the whole sentence and the position index as the context, the syntactic distance has sufficient representational power to capture any PCFG. It does not state, however, that the whole sentence and the position are the minimal contextual information needed for representability nor does it address training (i.e. optimization) issues.\nOn the flipside, we show that restricting the context even mildly can considerably decrease the representational power. Namely, we show that if context is bounded even in a single direction (to the left or to the right), there are PCFGs on which any syntactic distance will perform poorly 4. (Note in the implementation (Shen et al., 2018a) the context only considers a bounded window to the left.)\nTheorem 2 (Limitation of left-to-right parsing via syntactic distance). Let w0 = 〈S〉 be the sentence start symbol. Let the context\nct = (w0, w1, ..., wt+L′).\n∀ > 0, there exists a PCFG G in Chomsky normal form, such that any syntactic distance measure dt = d(wt−1, wt | ct) cannot -represent G.\nProof. Letm > 1/ be a positive integer. Consider the PCFG Gm,L′ in Definition 2.1.\nFor any k ∈ [m], consider the string lk ∈ L(Gm,L′). Note that in the parse tree of lk, the rule S → AkBk is applied. Hence, ak and ak+1 are the unique pair of adjacent non-terminals in a1a2...am+1 whose lowest common ancestor is the closest to the root in the parse tree of lk. Then, in order for the syntactic distance metric d to induce the correct parse tree for lk, dk must be the unique maximum in {dt : 2 ≤ t ≤ m+ 1}.\nHowever, d is restricted to be in the form\ndt = d(wt−1, wt |w0, w1, ..., wt+L′). 4In Theorem 2 we prove the more typical case, i.e. unbounded left context and bounded right context. The other case, i.e. bounded left context and unbounded right context, can be proved symmetrically.\nNote that ∀1 ≤ k1 < k2 ≤ m, the first m+ 1 +L′ tokens of lk1 and lk2 are the same, which implies that the inferred syntactic distances\n{dt : 2 ≤ t ≤ m+ 1}\nare the same for lk1 and lk2 at each position t. Thus, it is impossible for d to induce the correct parse tree for both lk1 and lk2 . Hence, d is correct on at most one lk ∈ L(Gm,L′), which corresponds to probability at most 1/m < . Therefore, d cannot -represent Gm,L′ .\nRemark. In the counterexample, there are only m possible parse structures for the prefix a1a2...am+1. Hence, the proved fact that the probability of being correct is at most 1/m means that under the restrictions of unbounded look-back and bounded look-ahead, the distance cannot do better than random guessing for this grammar. Remark. The above Theorem 2 formalizes the intuition discussed in (Htut et al., 2018) outlining an intrinsic limitation of only considering bounded context in one direction. Indeed, for the PCFG constructed in the proof, the failure is a function of the context, not because of the fact that we are using a distance-based parser.\nNote that as a corollary of the above theorem, if there is no context (ct = null) or the context is both bounded and unidirectional, i.e.\nct = wt−Lwt−L+1...wt−1wt,\nthen there is a PCFG that cannot be -represented by any such d."
    }, {
      "heading" : "6 Representational Power of the Ordered Neuron Architecture",
      "text" : "In this section, we formalize the results characterizing the representational power of the ON-LSTM architecture. The master forget gates of the ONLSTM, {f̃t}nt=2 in which each f̃t ∈ [0, 1]D, encode the hierarchical structure of a parse tree, and Shen et al. (2019) proposes to carry out unsupervised constituency parsing via a reduction from the gate vectors to syntactic distances by setting:\nd̂ft = D − D∑ j=1 f̃t,j for t = 2..n (1)\nFirst we show that the gates in ON-LSTM in principle form a lossless representation of any parse tree.\nTheorem 3 (Lossless representation of a parse tree). For any sentence W = w1w2...wn with parse tree T in any PCFG in Chomsky normal form, there exists a dimensionality D ∈ Z+, a sequence of vectors {f̃t}nt=2 in which each f̃t ∈ [0, 1]D, such that the estimated syntactic distances via (1) induce the structure of T .\nProof. By Theorem 1, there is a syntactic distance measure {dt}nt=2 that induces the structure of T (such that ∀t, dt 6= dt+1).\nFor each t = 2..n, set d̂t = k if dt is the k-th smallest entry in {dt}nt=2, breaking ties arbitrarily. Then, each d̂t ∈ [1, n − 1], and {d̂t}nt=2 also induces the structure of T .\nLet D = n − 1. For each t = 2..n, let f̃t = (0, ..., 0, 1, ..., 1) whose lower d̂t dimensions are 0 and higher D − d̂t dimensions are 1. Then,\nd̂ft = D − D∑ j=1 f̃t,j = D − (D − d̂t) = d̂t.\nTherefore, the calculated {d̂ft }nt=2 induces the structure of T .\nAlthough Theorem 3 shows the ability of the master forget gates to perfectly represent any parse tree, a left-to-right parsing can be proved to be unable to return the correct parse with high probability. In the actual implementation in (Shen et al., 2019), the (real-valued) master forget gate vectors {f̃t}nt=1 are produced by feeding the input sentence W = w1w2...wn to a model trained with a language modeling objective. In other words, f̃t,j is calculated as a function of w1, ..., wt, rather than the entire sentence.\nAs such, this left-to-right parser is subject to similar limitations as in Theorem 2:\nTheorem 4 (Limitation of syntactic distance estimation based on ON-LSTM). For any > 0, there exists a PCFG G in Chomsky normal form, such that the syntactic distance measure calculated with (1), d̂ft , cannot -represent G.\nProof. Since by Definition 4.5, f̃t,j is a function of w1, ..., wt, the estimated syntactic distance d̂ f t is also a function of w1, ..., wt. By Theorem 2, even with unbounded look-back context w1, ..., wt, there exists a PCFG for which the probability that d̂ft induces the correct parse is arbitrarily low."
    }, {
      "heading" : "7 Representational Power of Transition-Based Parsing",
      "text" : "In this section, we analyze a transition-based parsing framework inspired by (Dyer et al., 2016; Chelba and Jelinek, 2000; Chelba, 1997).\nAgain, we proceed to say first that “full context” confers full representational power. Namely, using the terminology of Definition 4.6, we let the context ct at each position t be the whole sentence W and the position index t. Note that any parse tree can be generated by a sequence of transitions defined in Definition 4.7. Indeed, Dyer et al. (2016) describes an algorithm to find such a sequence of transitions via a “depth-first, left-to-right traversal” of the tree.\nProceeding to limited context, in the setting of typical left-to-right parsing, the context ct consists of all current and past tokens {wj}tj=1 and all previous parses {(zj1, ..., z j Nj\n)}tj=1. We’ll again prove even stronger negative results, where we allow an optional look-ahead to L′ input tokens to the right.\nTheorem 5 (Limitation of transition-based parsing without full context). For any > 0, there exists a PCFG G in Chomsky normal form, such that for any learned transition-based parser (Definition 4.6) based on context\nct = ({wj}t+L ′ j=1 , {(z j 1, ..., z j Nj )}tj=1), the sum of the probabilities of the sentences in L(G) for which the parser returns the maximum likelihood parse is less than .\nProof. Letm > 1/ be a positive integer. Consider the PCFG Gm,L′ in Definition 2.1.\nNote that ∀k, S → AkBk is applied to yield string lk. Then in the parse tree of lk, ak and ak+1 are the unique pair of adjacent terminals in a1a2...am+1 whose lowest common ancestor is the closest to the root. Thus, different lk requires a different sequence of transitions within the first m+ 1 input tokens, i.e. {zti}i≥1, 1≤t≤m+1.\nFor each w ∈ L(Gm,L′), before the last token wm+2+L′ is processed, based on the common prefix w1w2...wm+1+L′ = a1a2...am+1+L′ , it is equally likely that w = lk, ∀k, w. prob. 1/m each.\nMoreover, when processing wm+1, the bounded look-ahead window of sizeL′ does not allow access to the final input token am+2+L′ = ck.\nThus, ∀1 ≤ k1 < k2 ≤ m, it is impossible for the parser to return the correct parse tree for both lk1 and lk2 without ambiguity. Hence, the parse is correct on at most one lk ∈ L(G), which corresponds to probability at most 1/m < ."
    }, {
      "heading" : "8 Conclusion",
      "text" : "In this work, we considered the representational power of two frameworks for constituency parsing prominent in the literature, based on learning a syntactic distance and learning a sequence of iterative transitions to build the parse tree — in the sandbox of PCFGs. In particular, we show that if the context for calculating distance/deciding on transitions is limited at least to one side (which is typically the case in practice for existing architectures), there are PCFGs for which no good distance metric/sequence of transitions can be chosen to construct the maximum likelihood parse.\nThis limitation was already suspected in (Htut et al., 2018) as a potential failure mode of leading neural approaches like (Shen et al., 2018a, 2019) and we show formally that this is the case. The PCFGs with this property track the intuition that bounded context methods will have issues when the parse at a certain position depends heavily on latter parts of the sentence.\nThe conclusions thus suggest re-focusing our attention on methods like (Kim et al., 2019a) which have enjoyed greater success on tasks like unsupervised constituency parsing, and do not fall in the paradigm analyzed in our paper. A question of definite further interest is how to augment models that have been successfully scaled up (e.g. BERT) in a principled manner with syntactic information, such that they can capture syntactic structure (like PCFGs). The other question of immediate importance is to understand the interaction between the syntactic and semantic modules in neural architectures — information is shared between such modules in various successful architectures, e.g. (Dyer et al., 2016; Shen et al., 2018a, 2019; Kim et al., 2019a), and the relative pros and cons of doing this are not well understood. Finally, our paper purely focuses on representational power, and does not consider algorithmic and statistical aspects of training. As any model architecture is associated with its distinct optimization and generalization considerations, and natural language data necessitates the modeling of the interaction between syntax and semantics, those aspects of considerations are well beyond the scope of our analysis in this paper using the controlled sandbox of PCFGs, and are interesting directions for future work."
    }, {
      "heading" : "A Tree Induction Algorithm Based on Syntactic Distance",
      "text" : "The following algorithm is proposed in (Shen et al., 2018a) to create a parse tree based on a given syntactic distance.\nAlgorithm 1: Tree induction based on syntactic distance Data: Sentence W = w1w2...wn, syntactic distances dt = d(wt−1, wt | ct), 2 ≤ t ≤ n Result: A parse tree for W Initialize the parse tree with a single node n0 = w1w2...wn; while ∃ leaf node n = wiwi+1...wj where i < j do\nFind k ∈ arg maxi+1≤k≤j dk ; Create the left child nl and the right child nr of n ; nl ← wiwi+1...wk−1 ; nr ← wkwk+1...wj ;\nend return The parse tree rooted at n0."
    }, {
      "heading" : "B ON-LSTM Intuition",
      "text" : "See Figure 3 below, which is excerpted from (Shen et al., 2019) with minor adaptation to the notation."
    }, {
      "heading" : "C Examples of parsing transitions",
      "text" : "Table 1 below shows an example of parsing the sentence “I drink coffee with milk” using the set of transitions given by Definition 4.7, which employs the parsing framework of (Dyer et al., 2016). The parse tree of the sentence is given by"
    } ],
    "references" : [ {
      "title" : "Probing the linguistic strengths and limitations of unsupervised grammar induction",
      "author" : [ "Yonatan Bisk", "Julia Hockenmaier." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
      "citeRegEx" : "Bisk and Hockenmaier.,? 2015",
      "shortCiteRegEx" : "Bisk and Hockenmaier.",
      "year" : 2015
    }, {
      "title" : "A structured language model",
      "author" : [ "Ciprian Chelba." ],
      "venue" : "35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics, pages 498–500, Madrid, Spain.",
      "citeRegEx" : "Chelba.,? 1997",
      "shortCiteRegEx" : "Chelba.",
      "year" : 1997
    }, {
      "title" : "Structured language modeling",
      "author" : [ "Ciprian Chelba", "Frederick Jelinek." ],
      "venue" : "Computer Speech & Language, 14(4):283 – 332.",
      "citeRegEx" : "Chelba and Jelinek.,? 2000",
      "shortCiteRegEx" : "Chelba and Jelinek.",
      "year" : 2000
    }, {
      "title" : "Enhanced LSTM for natural language inference",
      "author" : [ "Qian Chen", "Xiaodan Zhu", "Zhen-Hua Ling", "Si Wei", "Hui Jiang", "Diana Inkpen." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Parsing as language modeling",
      "author" : [ "Do Kook Choe", "Eugene Charniak." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2331–2336, Austin, Texas. Association for Computational Linguistics.",
      "citeRegEx" : "Choe and Charniak.,? 2016",
      "shortCiteRegEx" : "Choe and Charniak.",
      "year" : 2016
    }, {
      "title" : "Three models for the description of language",
      "author" : [ "N. Chomsky." ],
      "venue" : "IRE Transactions on Information Theory, 2(3):113–124.",
      "citeRegEx" : "Chomsky.,? 1956",
      "shortCiteRegEx" : "Chomsky.",
      "year" : 1956
    }, {
      "title" : "On certain formal properties of grammars",
      "author" : [ "Noam Chomsky." ],
      "venue" : "Information and Control, 2(2):137 – 167.",
      "citeRegEx" : "Chomsky.,? 1959",
      "shortCiteRegEx" : "Chomsky.",
      "year" : 1959
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Recurrent neural network grammars",
      "author" : [ "Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Dyer et al\\.,? 2016",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2016
    }, {
      "title" : "Lstm recurrent networks learn simple context-free and contextsensitive languages",
      "author" : [ "F. Gers", "J. Schmidhuber." ],
      "venue" : "IEEE transactions on neural networks, 12 6:1333–40.",
      "citeRegEx" : "Gers and Schmidhuber.,? 2001",
      "shortCiteRegEx" : "Gers and Schmidhuber.",
      "year" : 2001
    }, {
      "title" : "Assessing bert’s syntactic abilities",
      "author" : [ "Yoav Goldberg" ],
      "venue" : null,
      "citeRegEx" : "Goldberg.,? \\Q2019\\E",
      "shortCiteRegEx" : "Goldberg.",
      "year" : 2019
    }, {
      "title" : "Enhancing generalization in natural language inference by syntax",
      "author" : [ "Qi He", "Han Wang", "Yue Zhang." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4973–4978, Online. Association for Computational Linguistics.",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "RNNs can generate bounded hierarchical languages with optimal memory",
      "author" : [ "John Hewitt", "Michael Hahn", "Surya Ganguli", "Percy Liang", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Hewitt et al\\.,? 2020",
      "shortCiteRegEx" : "Hewitt et al\\.",
      "year" : 2020
    }, {
      "title" : "A structural probe for finding syntax in word representations",
      "author" : [ "John Hewitt", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Hewitt and Manning.,? 2019",
      "shortCiteRegEx" : "Hewitt and Manning.",
      "year" : 2019
    }, {
      "title" : "Introduction to Automata Theory, Languages, and Computation (3rd Edition)",
      "author" : [ "John E. Hopcroft", "Rajeev Motwani", "Jeffrey D. Ullman." ],
      "venue" : "AddisonWesley Longman Publishing Co., Inc., USA.",
      "citeRegEx" : "Hopcroft et al\\.,? 2006",
      "shortCiteRegEx" : "Hopcroft et al\\.",
      "year" : 2006
    }, {
      "title" : "Grammar induction with neural language models: An unusual replication",
      "author" : [ "Phu Mon Htut", "Kyunghyun Cho", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,",
      "citeRegEx" : "Htut et al\\.,? 2018",
      "shortCiteRegEx" : "Htut et al\\.",
      "year" : 2018
    }, {
      "title" : "Do attention heads in bert track syntactic dependencies? ArXiv, abs/1911.12246",
      "author" : [ "Phu Mon Htut", "Jason Phang", "Shikha Bordia", "Samuel R. Bowman" ],
      "venue" : null,
      "citeRegEx" : "Htut et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Htut et al\\.",
      "year" : 2019
    }, {
      "title" : "Compound probabilistic context-free grammars for grammar induction",
      "author" : [ "Yoon Kim", "Chris Dyer", "Alexander Rush." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2369–2385, Florence, Italy. Asso-",
      "citeRegEx" : "Kim et al\\.,? 2019a",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised recurrent neural network grammars",
      "author" : [ "Yoon Kim", "Alexander Rush", "Lei Yu", "Adhiguna Kuncoro", "Chris Dyer", "Gábor Melis." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Kim et al\\.,? 2019b",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2019
    }, {
      "title" : "LSTMs can learn syntax-sensitive dependencies well, but modeling structure makes them better",
      "author" : [ "Adhiguna Kuncoro", "Chris Dyer", "John Hale", "Dani Yogatama", "Stephen Clark", "Phil Blunsom." ],
      "venue" : "In",
      "citeRegEx" : "Kuncoro et al\\.,? 2018",
      "shortCiteRegEx" : "Kuncoro et al\\.",
      "year" : 2018
    }, {
      "title" : "Assessing the ability of LSTMs to learn syntax-sensitive dependencies",
      "author" : [ "Tal Linzen", "Emmanuel Dupoux", "Yoav Goldberg." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:521– 535.",
      "citeRegEx" : "Linzen et al\\.,? 2016",
      "shortCiteRegEx" : "Linzen et al\\.",
      "year" : 2016
    }, {
      "title" : "Sequential neural networks as automata",
      "author" : [ "William Merrill." ],
      "venue" : "Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges, pages 1–13, Florence. Association for Computational Linguistics.",
      "citeRegEx" : "Merrill.,? 2019",
      "shortCiteRegEx" : "Merrill.",
      "year" : 2019
    }, {
      "title" : "Improving natural language inference with a pretrained parser",
      "author" : [ "Deric Pang", "Lucy H. Lin", "Noah A. Smith" ],
      "venue" : null,
      "citeRegEx" : "Pang et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Pang et al\\.",
      "year" : 2019
    }, {
      "title" : "Visualizing and measuring the geometry of bert",
      "author" : [ "Emily Reif", "Ann Yuan", "Martin Wattenberg", "Fernanda B Viegas", "Andy Coenen", "Adam Pearce", "Been Kim." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32, pages 8594–8603. Curran As-",
      "citeRegEx" : "Reif et al\\.,? 2019",
      "shortCiteRegEx" : "Reif et al\\.",
      "year" : 2019
    }, {
      "title" : "A classifier-based parser with linear run-time complexity",
      "author" : [ "Kenji Sagae", "Alon Lavie." ],
      "venue" : "Proceedings of the Ninth International Workshop on Parsing Technology, pages 125–132, Vancouver, British Columbia. Association for Computational Linguis-",
      "citeRegEx" : "Sagae and Lavie.,? 2005",
      "shortCiteRegEx" : "Sagae and Lavie.",
      "year" : 2005
    }, {
      "title" : "Neural language modeling by jointly learning syntax and lexicon",
      "author" : [ "Yikang Shen", "Zhouhan Lin", "Chin wei Huang", "Aaron Courville." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Shen et al\\.,? 2018a",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "Straight to the tree: Constituency parsing with neural syntactic distance",
      "author" : [ "Yikang Shen", "Zhouhan Lin", "Athul Paul Jacob", "Alessandro Sordoni", "Aaron Courville", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Shen et al\\.,? 2018b",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "Ordered neurons: Integrating tree structures into recurrent neural networks",
      "author" : [ "Yikang Shen", "Shawn Tan", "Alessandro Sordoni", "Aaron Courville." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Shen et al\\.,? 2019",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2019
    }, {
      "title" : "On the computational power of neural nets",
      "author" : [ "Hava T. Siegelmann", "Eduardo D. Sontag." ],
      "venue" : "Proceedings of the Fifth Annual Workshop on Computational Learning Theory, COLT ’92, page 440–449, New York, NY, USA. Association for Computing Machin-",
      "citeRegEx" : "Siegelmann and Sontag.,? 1992",
      "shortCiteRegEx" : "Siegelmann and Sontag.",
      "year" : 1992
    }, {
      "title" : "LSTM networks can perform dynamic counting",
      "author" : [ "Mirac Suzgun", "Yonatan Belinkov", "Stuart Shieber", "Sebastian Gehrmann." ],
      "venue" : "Proceedings of the Workshop on Deep Learning and Formal Languages:",
      "citeRegEx" : "Suzgun et al\\.,? 2019",
      "shortCiteRegEx" : "Suzgun et al\\.",
      "year" : 2019
    }, {
      "title" : "Grammar as a foreign language",
      "author" : [ "Oriol Vinyals", "Ł ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 28, pages 2773–2781. Curran Associates, Inc.",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "On the practical computational power of finite precision RNNs for language recognition",
      "author" : [ "Gail Weiss", "Yoav Goldberg", "Eran Yahav." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Pa-",
      "citeRegEx" : "Weiss et al\\.,? 2018",
      "shortCiteRegEx" : "Weiss et al\\.",
      "year" : 2018
    }, {
      "title" : "Do latent tree learning models identify meaningful structure in sentences? Transactions of the Association for Computational Linguistics, 6:253–267",
      "author" : [ "Adina Williams", "Andrew Drozdov", "Samuel R. Bowman" ],
      "venue" : null,
      "citeRegEx" : "Williams et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 25,
      "context" : "Rapid progress in recent years was arguably spurred on by the empirical success of the Parsing-Reading-Predict architecture of (Shen et al., 2018a), later simplified by the Order Neuron LSTM of (Shen et al.",
      "startOffset" : 127,
      "endOffset" : 147
    }, {
      "referenceID" : 27,
      "context" : ", 2018a), later simplified by the Order Neuron LSTM of (Shen et al., 2019).",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 8,
      "context" : ", 2018a, 2019), as well as some transition-based syntax-aware language models (Dyer et al., 2016): what kind of syntactic structure can current neural approaches to syntax represent? Concretely, we ground this question in the sandbox of probabilistic context-free-grammars (PCFGs), and identify a key aspect of the representational power of these approaches: the amount and directionality of context that the predictor has access to when forced to make parsing decision.",
      "startOffset" : 78,
      "endOffset" : 97
    }, {
      "referenceID" : 7,
      "context" : "The reason for this is perhaps not surprising: neural models have largely achieved substantial improvements in unsupervised settings, BERT (Devlin et al., 2019) being the defacto method for unsupervised pre-training in most NLP settings.",
      "startOffset" : 139,
      "endOffset" : 160
    }, {
      "referenceID" : 15,
      "context" : "unsupervised syntactic parsing, have long been known to be very difficult tasks (Htut et al., 2018).",
      "startOffset" : 80,
      "endOffset" : 99
    }, {
      "referenceID" : 18,
      "context" : "However, since incorporating syntax has been shown to improve language modeling (Kim et al., 2019b) as well as natural language inference (Chen et al.",
      "startOffset" : 80,
      "endOffset" : 99
    }, {
      "referenceID" : 3,
      "context" : ", 2019b) as well as natural language inference (Chen et al., 2017; Pang et al., 2019; He et al., 2020), syntactic parsing remains important even in the current era when large pre-trained models, like BERT (Devlin et al.",
      "startOffset" : 47,
      "endOffset" : 102
    }, {
      "referenceID" : 22,
      "context" : ", 2019b) as well as natural language inference (Chen et al., 2017; Pang et al., 2019; He et al., 2020), syntactic parsing remains important even in the current era when large pre-trained models, like BERT (Devlin et al.",
      "startOffset" : 47,
      "endOffset" : 102
    }, {
      "referenceID" : 11,
      "context" : ", 2019b) as well as natural language inference (Chen et al., 2017; Pang et al., 2019; He et al., 2020), syntactic parsing remains important even in the current era when large pre-trained models, like BERT (Devlin et al.",
      "startOffset" : 47,
      "endOffset" : 102
    }, {
      "referenceID" : 7,
      "context" : ", 2020), syntactic parsing remains important even in the current era when large pre-trained models, like BERT (Devlin et al., 2019), are available.",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 15,
      "context" : "4 on the WSJ Penn Treebank dataset (Htut et al., 2018; Shen et al., 2019).",
      "startOffset" : 35,
      "endOffset" : 73
    }, {
      "referenceID" : 27,
      "context" : "4 on the WSJ Penn Treebank dataset (Htut et al., 2018; Shen et al., 2019).",
      "startOffset" : 35,
      "endOffset" : 73
    }, {
      "referenceID" : 8,
      "context" : ", 2018a,b, 2019) and learning to parse through sequential transitions (Dyer et al., 2016; Chelba, 1997).",
      "startOffset" : 70,
      "endOffset" : 103
    }, {
      "referenceID" : 1,
      "context" : ", 2018a,b, 2019) and learning to parse through sequential transitions (Dyer et al., 2016; Chelba, 1997).",
      "startOffset" : 70,
      "endOffset" : 103
    }, {
      "referenceID" : 25,
      "context" : "In practice, for computational efficiency, parametrizations of syntactic distances fall into the above assumptions of restricted context (Shen et al., 2018a).",
      "startOffset" : 137,
      "endOffset" : 157
    }, {
      "referenceID" : 25,
      "context" : "(2019) improving their previous work (Shen et al., 2018a), which also is based on learning a syntactic distance, but in (Shen et al.",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 27,
      "context" : ", 2018a), which also is based on learning a syntactic distance, but in (Shen et al., 2019) the distances are reduced from the values of a carefully structured master forget gate (see Section 6).",
      "startOffset" : 71,
      "endOffset" : 90
    }, {
      "referenceID" : 8,
      "context" : "We adopt a simplification of the neural parameterization in (Dyer et al., 2016) (see Definition 4.",
      "startOffset" : 60,
      "endOffset" : 79
    }, {
      "referenceID" : 15,
      "context" : "1) which draws inspirations from natural language already suggested in (Htut et al., 2018): later words in a sentence can force different syntactic structures earlier in the sentence.",
      "startOffset" : 71,
      "endOffset" : 90
    }, {
      "referenceID" : 15,
      "context" : "(2018a), whose performance was thoroughly verified by extensive experiments in (Htut et al., 2018).",
      "startOffset" : 79,
      "endOffset" : 98
    }, {
      "referenceID" : 27,
      "context" : "2679 follow-up paper (Shen et al., 2019) introducing the ON-LSTM architecture simplified radically the architecture in (Shen et al.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 25,
      "context" : ", 2019) introducing the ON-LSTM architecture simplified radically the architecture in (Shen et al., 2018a), while still ultimately attempting to fit a distance metric with the help of carefully designed master forget gates.",
      "startOffset" : 86,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "BERT) to nevertheless discover some (rudimentary) syntactic elements (Bisk and Hockenmaier, 2015; Linzen et al., 2016; Choe and Charniak, 2016; Kuncoro et al., 2018; Williams et al., 2018; Goldberg, 2019; Htut et al., 2019; Hewitt and Manning, 2019; Reif et al., 2019).",
      "startOffset" : 69,
      "endOffset" : 268
    }, {
      "referenceID" : 20,
      "context" : "BERT) to nevertheless discover some (rudimentary) syntactic elements (Bisk and Hockenmaier, 2015; Linzen et al., 2016; Choe and Charniak, 2016; Kuncoro et al., 2018; Williams et al., 2018; Goldberg, 2019; Htut et al., 2019; Hewitt and Manning, 2019; Reif et al., 2019).",
      "startOffset" : 69,
      "endOffset" : 268
    }, {
      "referenceID" : 4,
      "context" : "BERT) to nevertheless discover some (rudimentary) syntactic elements (Bisk and Hockenmaier, 2015; Linzen et al., 2016; Choe and Charniak, 2016; Kuncoro et al., 2018; Williams et al., 2018; Goldberg, 2019; Htut et al., 2019; Hewitt and Manning, 2019; Reif et al., 2019).",
      "startOffset" : 69,
      "endOffset" : 268
    }, {
      "referenceID" : 19,
      "context" : "BERT) to nevertheless discover some (rudimentary) syntactic elements (Bisk and Hockenmaier, 2015; Linzen et al., 2016; Choe and Charniak, 2016; Kuncoro et al., 2018; Williams et al., 2018; Goldberg, 2019; Htut et al., 2019; Hewitt and Manning, 2019; Reif et al., 2019).",
      "startOffset" : 69,
      "endOffset" : 268
    }, {
      "referenceID" : 32,
      "context" : "BERT) to nevertheless discover some (rudimentary) syntactic elements (Bisk and Hockenmaier, 2015; Linzen et al., 2016; Choe and Charniak, 2016; Kuncoro et al., 2018; Williams et al., 2018; Goldberg, 2019; Htut et al., 2019; Hewitt and Manning, 2019; Reif et al., 2019).",
      "startOffset" : 69,
      "endOffset" : 268
    }, {
      "referenceID" : 10,
      "context" : "BERT) to nevertheless discover some (rudimentary) syntactic elements (Bisk and Hockenmaier, 2015; Linzen et al., 2016; Choe and Charniak, 2016; Kuncoro et al., 2018; Williams et al., 2018; Goldberg, 2019; Htut et al., 2019; Hewitt and Manning, 2019; Reif et al., 2019).",
      "startOffset" : 69,
      "endOffset" : 268
    }, {
      "referenceID" : 16,
      "context" : "BERT) to nevertheless discover some (rudimentary) syntactic elements (Bisk and Hockenmaier, 2015; Linzen et al., 2016; Choe and Charniak, 2016; Kuncoro et al., 2018; Williams et al., 2018; Goldberg, 2019; Htut et al., 2019; Hewitt and Manning, 2019; Reif et al., 2019).",
      "startOffset" : 69,
      "endOffset" : 268
    }, {
      "referenceID" : 13,
      "context" : "BERT) to nevertheless discover some (rudimentary) syntactic elements (Bisk and Hockenmaier, 2015; Linzen et al., 2016; Choe and Charniak, 2016; Kuncoro et al., 2018; Williams et al., 2018; Goldberg, 2019; Htut et al., 2019; Hewitt and Manning, 2019; Reif et al., 2019).",
      "startOffset" : 69,
      "endOffset" : 268
    }, {
      "referenceID" : 23,
      "context" : "BERT) to nevertheless discover some (rudimentary) syntactic elements (Bisk and Hockenmaier, 2015; Linzen et al., 2016; Choe and Charniak, 2016; Kuncoro et al., 2018; Williams et al., 2018; Goldberg, 2019; Htut et al., 2019; Hewitt and Manning, 2019; Reif et al., 2019).",
      "startOffset" : 69,
      "endOffset" : 268
    }, {
      "referenceID" : 9,
      "context" : "LSTMs have been shown to possess stronger abilities to recognize some context-free language and even some context-sensitive language, compared with simple RNNs (Gers and Schmidhuber, 2001; Suzgun et al., 2019) or GRUs (Weiss et al.",
      "startOffset" : 160,
      "endOffset" : 209
    }, {
      "referenceID" : 29,
      "context" : "LSTMs have been shown to possess stronger abilities to recognize some context-free language and even some context-sensitive language, compared with simple RNNs (Gers and Schmidhuber, 2001; Suzgun et al., 2019) or GRUs (Weiss et al.",
      "startOffset" : 160,
      "endOffset" : 209
    }, {
      "referenceID" : 31,
      "context" : ", 2019) or GRUs (Weiss et al., 2018; Suzgun et al., 2019)); some results are theoretical in nature (e.",
      "startOffset" : 16,
      "endOffset" : 57
    }, {
      "referenceID" : 29,
      "context" : ", 2019) or GRUs (Weiss et al., 2018; Suzgun et al., 2019)); some results are theoretical in nature (e.",
      "startOffset" : 16,
      "endOffset" : 57
    }, {
      "referenceID" : 31,
      "context" : "Siegelmann and Sontag (1992)’s proof that with unbounded precision and unbounded time complexity, RNNs are Turing-complete; related results investigate RNNs with bounded precision and computation time (Weiss et al., 2018), as well as",
      "startOffset" : 201,
      "endOffset" : 221
    }, {
      "referenceID" : 17,
      "context" : "By virtue of not relying on bounded or unidirectional context, the Compound PCFG (Kim et al., 2019a) eschews the techniques in our paper.",
      "startOffset" : 81,
      "endOffset" : 100
    }, {
      "referenceID" : 5,
      "context" : "Formally, a PCFG (Chomsky, 1956) is a 5-tuple G = (Σ, N, S,R,Π) in which Σ is the set of terminals, N is the set of non-terminals, S ∈ N is the start symbol, R is the set of production rules of the form r = (rL → rR), where rL ∈ N , rR is of the form B1B2.",
      "startOffset" : 17,
      "endOffset" : 32
    }, {
      "referenceID" : 6,
      "context" : "A PCFG G = (Σ, N, S,R,Π) is in CNF (Chomsky, 1959) if we require, in addition to Definition 4.",
      "startOffset" : 35,
      "endOffset" : 50
    }, {
      "referenceID" : 14,
      "context" : "Every PCFGG can be converted into a PCFGG′ in CNF such that L(G) = L(G′) (Hopcroft et al., 2006).",
      "startOffset" : 73,
      "endOffset" : 96
    }, {
      "referenceID" : 25,
      "context" : "The Parsing-Reading-Predict Networks (PRPN) (Shen et al., 2018a) is one of the leading approaches to unsupervised constituency parsing.",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 25,
      "context" : "When parsing a sentence, the real-valued master forget gate vector f̃t at each position t is reduced to a single real number representing the syntactic distance dt at position t (see (1)) (Shen et al., 2018a).",
      "startOffset" : 188,
      "endOffset" : 208
    }, {
      "referenceID" : 24,
      "context" : "In addition to outputting a single real numbered distance or a vector at each position t, a left-to-right model can also parse a sentence by outputting a sequence of “transitions” at each position t, an idea proposed in some traditional parsing approaches (Sagae and Lavie, 2005; Chelba, 1997; Chelba and Jelinek, 2000), and also some more recent neural parameterization (Dyer et al.",
      "startOffset" : 256,
      "endOffset" : 319
    }, {
      "referenceID" : 1,
      "context" : "In addition to outputting a single real numbered distance or a vector at each position t, a left-to-right model can also parse a sentence by outputting a sequence of “transitions” at each position t, an idea proposed in some traditional parsing approaches (Sagae and Lavie, 2005; Chelba, 1997; Chelba and Jelinek, 2000), and also some more recent neural parameterization (Dyer et al.",
      "startOffset" : 256,
      "endOffset" : 319
    }, {
      "referenceID" : 2,
      "context" : "In addition to outputting a single real numbered distance or a vector at each position t, a left-to-right model can also parse a sentence by outputting a sequence of “transitions” at each position t, an idea proposed in some traditional parsing approaches (Sagae and Lavie, 2005; Chelba, 1997; Chelba and Jelinek, 2000), and also some more recent neural parameterization (Dyer et al.",
      "startOffset" : 256,
      "endOffset" : 319
    }, {
      "referenceID" : 8,
      "context" : "In addition to outputting a single real numbered distance or a vector at each position t, a left-to-right model can also parse a sentence by outputting a sequence of “transitions” at each position t, an idea proposed in some traditional parsing approaches (Sagae and Lavie, 2005; Chelba, 1997; Chelba and Jelinek, 2000), and also some more recent neural parameterization (Dyer et al., 2016).",
      "startOffset" : 371,
      "endOffset" : 390
    }, {
      "referenceID" : 8,
      "context" : "We base our analysis on the approach introduced in the parsing version of (Dyer et al., 2016), though that work additionally proposes a generator version.",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 14,
      "context" : "Since any PCFG can be converted to Chomsky normal form (Hopcroft et al., 2006), Theorem 1 implies that given the whole sentence and the position index as the context, the syntactic distance has sufficient representational power to capture any PCFG.",
      "startOffset" : 55,
      "endOffset" : 78
    }, {
      "referenceID" : 25,
      "context" : "(Note in the implementation (Shen et al., 2018a) the context only considers a bounded window to the left.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 15,
      "context" : "The above Theorem 2 formalizes the intuition discussed in (Htut et al., 2018) outlining an intrinsic limitation of only considering bounded context in one direction.",
      "startOffset" : 58,
      "endOffset" : 77
    }, {
      "referenceID" : 27,
      "context" : "In the actual implementation in (Shen et al., 2019), the (real-valued) master forget gate vectors {f̃t}t=1 are produced by feeding the input sentence W = w1w2.",
      "startOffset" : 32,
      "endOffset" : 51
    }, {
      "referenceID" : 8,
      "context" : "In this section, we analyze a transition-based parsing framework inspired by (Dyer et al., 2016; Chelba and Jelinek, 2000; Chelba, 1997).",
      "startOffset" : 77,
      "endOffset" : 136
    }, {
      "referenceID" : 2,
      "context" : "In this section, we analyze a transition-based parsing framework inspired by (Dyer et al., 2016; Chelba and Jelinek, 2000; Chelba, 1997).",
      "startOffset" : 77,
      "endOffset" : 136
    }, {
      "referenceID" : 1,
      "context" : "In this section, we analyze a transition-based parsing framework inspired by (Dyer et al., 2016; Chelba and Jelinek, 2000; Chelba, 1997).",
      "startOffset" : 77,
      "endOffset" : 136
    }, {
      "referenceID" : 15,
      "context" : "This limitation was already suspected in (Htut et al., 2018) as a potential failure mode of leading neural approaches like (Shen et al.",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 17,
      "context" : "The conclusions thus suggest re-focusing our attention on methods like (Kim et al., 2019a) which have enjoyed greater success on tasks like unsupervised constituency parsing, and do not fall in the paradigm analyzed in our paper.",
      "startOffset" : 71,
      "endOffset" : 90
    }, {
      "referenceID" : 8,
      "context" : "(Dyer et al., 2016; Shen et al., 2018a, 2019; Kim et al., 2019a), and the relative pros and cons of doing this are not well understood.",
      "startOffset" : 0,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : "(Dyer et al., 2016; Shen et al., 2018a, 2019; Kim et al., 2019a), and the relative pros and cons of doing this are not well understood.",
      "startOffset" : 0,
      "endOffset" : 64
    } ],
    "year" : 2021,
    "abstractText" : "Incorporating syntax into neural approaches in NLP has a multitude of practical and scientific benefits. For instance, a language model that is syntax-aware is likely to be able to produce better samples; even a discriminative model like BERT with a syntax module could be used for core NLP tasks like unsupervised syntactic parsing. Rapid progress in recent years was arguably spurred on by the empirical success of the Parsing-Reading-Predict architecture of (Shen et al., 2018a), later simplified by the Order Neuron LSTM of (Shen et al., 2019). Most notably, this is the first time neural approaches were able to successfully perform unsupervised syntactic parsing (evaluated by various metrics like F-1 score). However, even heuristic (much less fully mathematical) understanding of why and when these architectures work is lagging severely behind. In this work, we answer representational questions raised by the architectures in (Shen et al., 2018a, 2019), as well as some transition-based syntax-aware language models (Dyer et al., 2016): what kind of syntactic structure can current neural approaches to syntax represent? Concretely, we ground this question in the sandbox of probabilistic context-free-grammars (PCFGs), and identify a key aspect of the representational power of these approaches: the amount and directionality of context that the predictor has access to when forced to make parsing decision. We show that with limited context (either bounded, or unidirectional), there are PCFGs, for which these approaches cannot represent the max-likelihood parse; conversely, if the context is unlimited, they can represent the max-likelihood parse of any PCFG.",
    "creator" : "LaTeX with hyperref"
  }
}