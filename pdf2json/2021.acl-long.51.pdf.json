{
  "name" : "2021.acl-long.51.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Structurizing Misinformation Stories via Rationalizing Fact-Checks",
    "authors" : [ "Shan Jiang", "Christo Wilson" ],
    "emails" : [ "cbw}@ccs.neu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 617–631\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n617\nMisinformation has recently become a welldocumented matter of public concern. Existing studies on this topic have hitherto adopted a coarse concept of misinformation, which incorporates a broad spectrum of story types ranging from political conspiracies to misinterpreted pranks. This paper aims to structurize these misinformation stories by leveraging fact-check articles. Our intuition is that key phrases in a fact-check article that identify the misinformation type(s) (e.g., doctored images, urban legends) also act as rationales that determine the verdict of the fact-check (e.g., false). We experiment on rationalized models with domain knowledge as weak supervision to extract these phrases as rationales, and then cluster semantically similar rationales to summarize prevalent misinformation types. Using archived fact-checks from Snopes.com, we identify ten types of misinformation stories. We discuss how these types have evolved over the last ten years and compare their prevalence between the 2016/2020 US presidential elections and the H1N1/COVID-19 pandemics."
    }, {
      "heading" : "1 Introduction",
      "text" : "Misinformation has raised increasing public concerns globally, well-documented in Africa (Ahinkorah et al., 2020), Asia (Kaur et al., 2018), and Europe (Fletcher et al., 2018). In the US, “fake news” accounted for 6% of all news consumption during the 2016 US presidential election (Grinberg et al., 2019). Years later, 29% of US adults in a survey believed that the “exaggerated threat” of the COVID-19 pandemic purposefully damaged former US president Donald Trump (Uscinski et al., 2020), and 77% of Trump’s supporters believed “voter fraud” manipulated the 2020 US presidential election in spite of a complete lack of evidence (Pennycook and Rand, 2021).\nAs such misinformation continues to threaten society, researchers have started investigating this multifaceted problem, from understanding the socio-psychological foundations of susceptibility (Bakir and McStay, 2018) and measuring public responses (Jiang and Wilson, 2018; Jiang et al., 2020b), to designing detection algorithms (Shu et al., 2017) and auditing countermeasures for online platforms (Jiang et al., 2019, 2020c).\nThese studies mostly adopted the term “misinformation” as a coarse concept for any false or inaccurate information, which incorporates a broad spectrum of misinformation stories, e.g., political conspiracies to misinterpreted pranks. Although misinformation types have been theorized and categorized by practitioners (Wardle, 2017), there is, to our knowledge, no empirical research that has systematically measured these prevalent types of misinformation stories.\nThis paper aims to unpack the coarse concept of misinformation and structurize it to fine-grained story types (as illustrated in Figure 1). We conduct\nthis query through an empirical lens and ask the question: what are the prevalent types of misinformation stories in the US over the last ten years?\nThe answer to our question is buried in archived fact-checks, which are specialized news articles that verify factual information and debunk false claims by presenting contradictory evidence (Jiang et al., 2020a). As a critical component of their semi-structured journalistic style, fact-checks often embed the (mis)information type(s) within their steps of reasoning. For example, consider the following snippet from a Snopes.com fact-check with a verdict of false (Evon, 2019):\n“...For instance, some started sharing a doctored photograph of Thunberg with alt-right boogeyman George Soros (the original photograph featured former Vice President Al Gore)...”\nThe key phrase doctored photograph in the snippet identifies the misinformation type of the factchecked story. Additional example phrases are highlighted in Figure 1. With a large corpus of fact-checks, these phrases would accumulate and reveal prevalent types of misinformation stories.\nExtracting these phrases is a computational task. Our intuition is that such phrases in a fact-check also act as rationales that determine the verdict of the fact-check. In the previous example, the verdict is false in part because the story contains a doctored photograph. Therefore, a neural model that predicts the verdict of a fact-check would also use the misinformation types as rationales.\nTo realize this intuition, we experiment on existing rationalized neural models to extract these phrases (Lei et al., 2016; Jain et al., 2020), and, to target specific kinds of rationales, we additionally propose to include domain knowledge as weak supervision in the rationalizing process. Using public datasets as validation (Zaidan et al., 2007; Carton et al., 2018), we evaluate the performance variation of different rationalized models, and show that including domain knowledge consistently improves the quality of extracted rationales.\nAfter selecting the most appropriate method, we conduct an empirical investigation of prevalent misinformation types. Using archived fact-checks from Snopes.com, spanning from its founding in 1994 to 2021, we extract rationales by applying the selected model with theorized misinformation types for weak supervision (Wardle, 2017), and\nthen cluster rationales based on their semantic similarity to summarize prevalent misinformation types. We identify ten types of misinformation stories, a preview of which are shown in Figure 1.\nUsing our derived lexicon of these clustered misinformation stories, we then explore the evolution of misinformation types over the last ten years. Our key findings include: increased prevalence of conspiracy theories, fabricated content, and digital manipulation; and decreased prevalence of legends and tales, pranks and jokes, mistakes and errors, etc. We also conducted two case studies on notable events that involve grave misinformation. From the case study of US presidential elections, we observe that the most prevalent misinformation type for both the 2016 and 2020 elections is fabricated content, while the 2016 election has more hoaxes and satires. From the case study of pandemics, our results show that the H1N1 pandemic in 2009 has more legends and tales, while the COVID-19 pandemic attracts more conspiracy theories.\nThe code and data used in the paper are available at: https://factcheck.shanjiang.me."
    }, {
      "heading" : "2 Related Work",
      "text" : "There is a rich literature that has studied the online misinformation ecosystem from multiple perspectives (Del Vicario et al., 2016; Lazer et al., 2018). Within the computational linguistics community, from an audiences’ perspective, Jiang and Wilson (2018) found that social media users expressed different linguistic signals when responding to false claims, and the authors later used these signals to model and measure (dis)beliefs in (mis)information (Jiang et al., 2020b; Metzger et al., 2021). From a platforms’ perspective, researchers have assisted platforms in designing novel misinformation detection methods (Wu et al., 2019; Lu and Li, 2020; Vo and Lee, 2018, 2020), as well as audited existing misinformation intervention practices (Robertson et al., 2018; Jiang et al., 2019, 2020c; Hussein et al., 2020).\nIn this work, we study another key player in the misinformation ecosystem, storytellers, and investigate the prevalent types of misinformation told to date. From the storytellers’ perspective, Wardle (2017) theorized several potential misinformation types (e.g., satire or parody, misleading content, and false connection), yet no empirical evidence has been connected to this typology. Additionally, researchers have investigated specific types of mis-\ninformation as case studies, e.g., state-sponsored disinformation (Starbird et al., 2019; Wilson and Starbird, 2020), fauxtography (Zannettou et al., 2018; Wang et al., 2021), and conspiracy theories (Samory and Mitra, 2018; Phadke et al., 2021). In this paper, we aim to structurize these misinformation stories to theorized or novel types."
    }, {
      "heading" : "3 Rationalized Neural Models",
      "text" : "Realizing our intuition (as described in § 1) requires neural models to (at least shallowly) reason about predictions. In this section, we introduce existing rationalized neural models and propose to include domain knowledge as weak supervision in the rationalizing process. We then experiment with public datasets and lexicons for evaluation."
    }, {
      "heading" : "3.1 Problem Formulation",
      "text" : "In a standard text classification problem, each instance is in a form of (x,y). x = [xi] ∈ V lx is the input token sequence of length l, where Vx is the vocabulary of the input and i is the index of each token xi. y ∈ {0, 1}m is the binary label of length m. Rationalization requires a model to output the prediction ŷ together with a binary mask z = [zi] ∈ {0, 1}l of input length l, indicating which tokens are used (i.e., zi = 1) to make the decision. These tokens are called rationales.\nHard rationalization requires a model to directly output z. Initially proposed by Lei et al. (2016), the model first passes the input x to a tagger1 module and samples a binary mask z from a Bernoulli distribution, i.e., z ∼ Tagger(x), and then uses only unmasked tokens to make a prediction of y, i.e., ŷ = Predictor(z,x).2\nThe loss function of this method contains two parts. The first part is a standard loss for the prediction Ly(ŷ,y), which can be realized using common classification loss, e.g., cross entropy. The second part is a loss Lz(z)3 aiming to regularize z and encourage conciseness and contiguity of rationale selection, formulated by Lei et al. (2016). Recent work proposed to improve the initial model with an adversarial component (Yu et al., 2019; Carton et al., 2018). Combining these parts together, the\n1This module was named generator by Lei et al. (2016). We name it tagger to distinguish it from the NLG problem.\n2This module was named encoder by Lei et al. (2016). We name it predictor, consistent with Yu et al. (2019), to distinguish it from the encoder-decoder framework.\n3Lz(z) is a simplified term; we discuss its detailed implementation in Appendix § A.\nmodel is trained end-to-end using reinforce-style estimation (Williams, 1992), as sampling rationales is a non-differentiable computation. The modules of hard rationalization are illustrated in Figure 2.\nSoft rationalization, in contrast, allows a model to first output a continuous version of importance scores s = [si] ∈ Rl, and then binarize it to get z. Initially formalized by Jain et al. (2020) as a multiphase method, the model first conducts a standard text classification using a supporter module ŷ = Supporter(x) and outputs importance scores s, then binarizes s using a tagger module, i.e., z = Tagger(s), and finally uses only unmasked tokens of x to make another prediction ŷ to evaluate the faithfulness of selected rationales.4\nThese three modules are trained separately in three phases.5 Since the supporter and predictor are standard text classification modules the only loss needed is for the prediction Ly(ŷ,y). This method is more straightforward than the hard rationalization method, as it avoids non-differentiable com-\n4The second and third modules were named extractor and classifier by Jain et al. (2020). We continue using tagger and predictor to align with the hard rationalization method.\n5Tagger is often flexibly designed as a rule-based algorithm, therefore no training is needed.\nputations and the instability induced by reinforcestyle estimation. The modules of soft rationalization are also illustrated in Figure 2.\nThe popular attention mechanism (Bahdanau et al., 2014) provides built-in access to s. Although there have been debates on the properties achieved by attention-based explanations (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Serrano and Smith, 2019), rationales extracted by straightforward rules on attention weights were demonstrated as comparable to human-generated rationales (Jain et al., 2020). Additionally, in our use case we only need the rationales themselves as key phrases and do not require them to faithfully predict y, therefore the last predictor module can be omitted."
    }, {
      "heading" : "3.2 Domain Knowledge as Weak Supervision",
      "text" : "Both hard and soft rationalization methods can be trained with or without supervision w.r.t. rationales z (DeYoung et al., 2020)6. When rationales are selected in an unsupervised manner, the model would intuitively favor rationales that are most informative to predict the corresponding label as a result of optimizing the loss function. This could result in some undesirable rationales in our case: for example, certain entities like “COVID-19” or “Trump” that are highly correlated with misinformation would be selected as rationales even though they do not suggest any misinformation types. Therefore, we propose to weakly supervise7 the rationalizing process with domain knowledge to obtain specific, desired types of rationales.\nAssuming a lexicon of vocabulary Vd as domain knowledge, we reprocess the input and generate weak labels for rationales zd = [zid] ∈ {0, 1}l where zid = 1 (i.e., unmasked) if x\ni ∈ Vd and zid = 0 (i.e., masked) otherwise. Then, we include an additional loss item Ld(z, zd) or Ld(s, zd) for the hard or soft rationalization method.\nCombining the loss items together, the objective for the end-to-end hard rationalization model is:\nmin θ Ly(ŷ,y) + λzLz(z) + λdLd(z, zd),\nwhere θ contains the parameters to estimate and λ(·) are hyperparameters weighting loss items.\nSimilarly, the objective function for the first phase of soft rationalization is:\nmin θ Ly(ŷ,y) + λdLd(s, zd).\n6They are trained with supervision w.r.t. the label y. 7Since there is inherently no ground-truth of misinforma-\ntion types in fact-check articles."
    }, {
      "heading" : "3.3 Experiments on Public Datasets",
      "text" : "We conduct experiments on public datasets to evaluate the performance of hard and soft rationalization methods, particularly for our needs, and confirm that including domain knowledge as weak supervision helps with the rationalizing process.\nDatasets selection. An ideal dataset for our models should meet the following requirements: (a) formulated as a text classification problem, (b) annotated with human rationales, and (c) can be associated with high quality lexicons to obtain domain knowledge. We select two datasets based on these criteria: the movie reviews dataset released by Pang et al. (2002) and later annotated with rationales by Zaidan et al. (2007), which contains 2K movie reviews labeled with positive or negative sentiments; and the personal attacks dataset released by Wulczyn et al. (2017) and later annotated with rationales by Carton et al. (2018), which contains more than 100K Wikipedia comments labeled as personal attacks or not.\nDomain knowledge. For the sentiment analysis on movie reviews, we use the EmoLex lexicon released by Mohammad and Turney (2013), which contains vocabularies of positive and negative sentiments. For identifying personal attacks, we use a lexicon released by Wiegand et al. (2018), which contains a vocabulary of abusive words. With corresponding vocabularies, we generate weak rationale labels zd for each dataset.\nEvaluation metrics. We choose binary precision Pr(z) to evaluate the quality of extracted rationales, because (a) a perfect recall can be trivially achieved by selecting all tokens as rationales,8 and (b) our case of identifying key phrases requires concise rationales. Additionally, we measure the average percentage of selected rationales over the input length %(z). For predictions, we use macro F1(y) as the evaluation metric as well as the percentage of information used %(x) to make the prediction.\nExperimental setup and results. The train, dev, and test sets are pre-specified in public datasets. We optimize hyperparameters for F1(y) on the dev sets, and only evaluate rationale quality Pr(z) after a model is decided. We discuss additional implementation details (e.g., hyperparameters, loss functions, module cells) in Appendix § A.\n8We later show that this is the default model behavior if rationale selection is under-regularized.\nThe evaluation results for all our experiments on test sets are reported in Table 1, indexed with h0-h3 and s0-s3. We report the evaluation results on dev sets in Appendix § B.\nRegularization for hard rationalization. h0 and h2 are our re-implementation of Lei et al. (2016), varying the rationale regularization hyperparameter λz . Our experiments show that λz is a crucial choice. When a small λz is chosen (i.e., rationales are under-regularized), the model has a tendency to utilize all the available information to optimize the predictive accuracy. In h2, we set λz = 0 and the model selects 99.9% of tokens as rationales while achieving the best F1(y) overall, which is an undesirable outcome in our case. Therefore, we increases λz so that only small parts of tokens are selected as rationales in h0. However, echoing Jain et al. (2020), the output when varying λz is sensitive and unpredictable, and searching for this hyperparameter is both time-consuming and energy-inefficient. We also run an experiment h3 with the additional adversarial component proposed in (Carton et al., 2018; Yu et al., 2019), and the evaluation metrics are not consistently improved compared to h0.\nBinarization for soft rationalization. s0, s2 and s3 are our re-implementation of Jain et al. (2020). For soft rationalization, rationales are selected (i.e., binarized) after the supporter module is trained in phase one, therefore s0-s3 utilize 100% of the tokens by default, and achieve the best F1(y) overall. We implement a straightforward approach to select rationales by setting a threshold t and make zi = 1 (i.e., unmasked) if the importance score si > t and zi = 0 (i.e., masked) otherwise. Intuitively, increasing t corresponds to less selected rationales,\nand therefore increasing Pr(z). To confirm, in s2, we increase t until %(z) is exactly half of s0. Similarly, decreasing t corresponds to more selected rationales, and therefore decreasing Pr(z). In s3, we decrease t until %(z) is exactly double of s0.\nIs domain knowledge helpful? h1 and s1 include domain knowledge as weak supervision. Our results show that domain knowledge improves Pr(z) for both hard (h1 to h0) and soft (s1 to s0) rationalization methods and on both dataset, while maintaining similar %(z) and F1(y). The improvements are more substantial for soft rationalization.\nHard vs. soft rationalization. To fairly compare hard and soft rationalization methods, we choose the threshold t to keep %(z) the same for h1 and s1.9 Our experiments show that soft rationalization weakly supervised by domain knowledge achieves better Pr(z) on both datasets, and therefore we chose it for rationalizing fact-checks."
    }, {
      "heading" : "4 Rationalizing Fact-Checks",
      "text" : "After determining that soft rationalization is the most appropriate method, we apply it to extract rationales from fact-checks. In this section, we introduce the dataset we collected from Snopes.com and conduct experiment with fact-checks to structurize misinformation stories."
    }, {
      "heading" : "4.1 Data Collection",
      "text" : "Snopes.com is a renowned fact-checking website, certified by the International Fact-Checking Network as non-partisan and transparent (Poynter,\n9We can easily and accurately manipulate %(z) for soft rationalization by adjusting t; conversely, the impact of adjusting λz in hard rationalization is unpredictable.\n2018). We collect HTML webpages of fact-check articles from Snopes.com, spanning from its founding in 1994 to the beginning of 2021.\nPreprocess and statistics. We first preprocess collected fact-checks by extracting the main article content and verdicts from HTML webpages using a customized parser, and tokenizing the content with NLTK (Bird, 2006). The preprocessing script is included in our released codebase.\nAfter preprocessing, the median sequence length of fact-checks is 386 tokens, and 88.6% of factchecks containing ≤1,024 tokens. Jiang et al. (2020a) found that the most informative content in fact-checks tended to be located at the head or the tail of the article content. Therefore, we set the maximum sequence length to 1,024 and truncate over-length fact-checks.\nNext, we label each fact-check with a binary label depending on its verdict: (truthful) information if the verdict is at least mostly true and misinformation otherwise, which results in 2,513 information and 11,183 misinformation instances.\nAdditionally, we preemptively mask tokens that are the exact words as its verdict (e.g., “rate it as false” to “rate it as [MASK]”),10 otherwise predicting the verdict would be trivial and the model would copy overlapping tokens as rationales.\nDomain knowledge for misinformation types. The domain knowledge comes from two sources: (a) the misinformation types theorized by Wardle (2017), e.g., misleading or fabricated content; and (b) certain variants of verdicts from Snopes.com such as satire or scam (Snopes.com, 2021a). We combine these into a small vocabulary Vd containing 12 words, listed in Appendix § A."
    }, {
      "heading" : "4.2 Experiments and Results",
      "text" : "We randomly split the fact-checks to 80% train, 10% dev, and 10% test sets, and adjust hyperparameters to optimize F1(y) on dev set. For initialization, we train word embeddings using Gensim (Rehurek and Sojka, 2011) on the entire corpus. The final model achieves F1(y) = 0.75/0.74 on the test set with/without domain knowledge.\nClustering rationales. To systematically understand extracted rationales, we cluster these rationales based on semantic similarity. For each rationale, we average word embeddings to represent\n10Verdicts from Snopes.com are structured HTML fields that can be easily parsed.\nthe embedding of the rationale, and then run a hierarchical clustering for these embeddings. The hierarchical clustering uses cosine similarity as the distance metric, commonly used for word embeddings (Mikolov et al., 2013), and the complete link\nmethod (Voorhees, 1986) to obtain a relatively balanced linkage tree.\nThe results from the clustering are shown in Figure 3. From the root of the dendrogram, we can traverse its branches to find clusters until we reach a sensible threshold of cosine distance, and categorize the remaining branches and leaf nodes (i.e., rationales) to multiple clusters. Figure 3 shows an example visualization that contains ten clusters of rationales that are semantically similar to the domain knowledge, and leaf nodes in each cluster are aggregated to plot a word cloud, with the frequency of a node encoded as the font size of the phrase.\nNote that rationales extracted from soft rationalization are dependent on the chosen threshold t to binarize importance scores. The example in Figure 3 uses a threshold of t = 0.01. Varying the threshold would affect extracted rationales but mostly the ones with low prevalence, and these rare rationales also correspond to small font sizes in the word cloud. Therefore, the effect from varying t would be visually negligible in Figure 3.\nStructure of misinformation stories. We make the following observations from the ten clusters of misinformation types identified in Figure 3.\nFirst, the clusters empirically confirm existing domain knowledge in Vd. Certain theorized misinformation types, such as satires and parodies from (Wardle, 2017), are identified as individual clusters from fact-checks.\nSecond, the clusters complement Vd with additional phrases describing (semantically) similar misinformation types. For example, our results add “humor” and “gossip” to the same category as satires and parodies and add “tales” and “lore” to the same category as legends . This helps us grasp the similarity between misinformation types, and also enriches the lexicon Vd, which proves useful for subsequent analysis in § 5.\nThird, we discover novel, fine-grained clusters that are not highlighted in Vd. There are multiple possible explanations as to why these misinformation types form their own clusters. Conspiracy theories are often associated with intentional political campaigns (Samory and Mitra, 2018) which can affect their semantics when referenced in fact-checks. In contrast, digital alteration is a relatively recent misinformation tactic that has been enabled by technological developments such as FaceSwap (Korshunova et al., 2017) and DeepFake (Westerlund, 2019). Hoaxes and pranks often have a mis-\nchievous intent that distinguishes them from other clusters. Other new clusters include clickbait with inflammatory and sensational language and entirely fictional content .\nFourth, the clusters reorganize the structure of these misinformation types based on their semantics, e.g., fabricated and misleading content belongs to two types of misinformation in (Wardle, 2017), while in our results they are clustered together. This suggests that the semantic distance between fabricated and misleading content is less than the chosen similarity threshold, at least when these misinformation types are referred to by factcheckers when writing articles.\nFinally, the remaining words in Vd are also found in our rationales. However, due to low prevalence, they are not visible in Figure 3 and do not form their own clusters."
    }, {
      "heading" : "5 Evolution of Misinformation",
      "text" : "In this section, we leverage the clusters of misinformation types identified by our method as a lexicon and apply it back to the our original fact-check dataset. Specifically, we analyze the evolution of misinformation types over the last ten years and compare misinformation trends around major realworld events.\nEvolution over the last ten years. We first explore the evolution of misinformation over time. We map each fact-check article with one or more corresponding misinformation types identified by our method, and then aggregate fact-checks by year from before 201011 to the end of 2020 to estimate the relative ratio of each misinformation type.\nAs shown in Figure 4,12 the prevalence of certain misinformation types on Snopes.com has drastically changed over the last ten years.\nHeavily politicized misinformation types, such as digitally altered or doctored images or photographs , fabricated and misleading content , and conspiracy theories have nearly doubled in relative ratios over the last ten years. In contrast, the prevalence of (arguably) less politicized stories, such as legends and tales , hoaxes and pranks , and mistakes and errors have decreased.\nThese trends may be a proxy for the underlying prevalence of different misinformation types within the US. Studies that measure political ideologies\n11Since there are relatively few fact-checks before 2010, we aggregate them together to the year 2010.\n1295% confidence intervals.\nexpressed online have documented increasing polarization over time (Chinn et al., 2020; Baumann et al., 2020), which could explain increased ratios of such heavily politicized misinformation. Additionally, the convenience offered by modern digital alteration software and applications (Korshunova et al., 2017; Westerlund, 2019) provides a gateway to proliferating manipulated images or photographs in the misinformation ecosystem.\nAlternatively, these trends may reflect shifts in Snopes.com’s priorities. The website, launched in 1994, was initially named Urban Legends Reference Pages. Since then it has grown to encompass a broad spectrum of subjects. Due to its limited resources, fact-checkers from Snopes.com only cover a subset of online misinformation, and their priority is to “fact-check whatever items the greatest number of readers are asking about or searching for at any given time (Snopes.com, 2021b).”13 Given the rising impact of political misinformation in recent years (Zannettou et al., 2019, 2020), such misinformation could reach an increasing number of Snopes.com readers, and therefore the website may dedicate more resources to fact-checking related types of misinformation. Additionally, Snopes.com has established collaborations with social media platforms, e.g., Facebook (Green and Mikkelson), to specifically target viral misinformation circulating on these platforms, where the rising meme culture could also attract Snopes.com’s attention and therefore explain a surge of digitally altered images (Ling et al., 2021; Wang et al., 2021).\n13Users can submit a topic to Snopes.com on its contact page (Snopes.com, 2021c), the results from which may affect Snopes.com’s priorities.\n2016 vs. 2020 US presidential election. We now compare misinformation types between the 2016 and 2020 elections. To filter for relevance, we constrain our analysis to fact-checks that (1) were published in the election years and (2) included the names of the presidential candidates and/or their running mates (e.g., “Joe Biden” and “Kamala Harris”). This results in 2,586 fact-checks for the 2016 election and 2,436 fact-checks for 2020.\nThe prevalence of each misinformation type is shown in Figure 5. We observe that the relative ratios of many misinformation types are similar between the two elections, e.g., legends and tales and bogus scams , while the 2016 election has more hoaxes , satires , etc. The most prevalent type during both elections is fabricated and misleading content , next to conspiracy theories .\nH1N1 vs. COVID-19. Finally, we compare misinformation types between the H1N1 pandemic in 2009 and the COVID-19 pandemic. For H1N1 related fact-checks, we search for keywords “flu”, “influenza”, and “H1N1” in fact-checks and constrain the publication date until the end of 2012.14 For COVID-19 related fact-checks, we search for keywords “COVID-19” and “coronavirus”, and only consider fact-checks published in 2019 or later, which results in 833 fact-checks for the H1N1 pandemic and 656 fact-checks for COVID-19.\nThe relative ratio of each misinformation type is also shown in Figure 5. We observe that the prevalence of some misinformation types are sig-\n14WHO declared an end to the global 2009 H1N1 pandemic on August 10, 2010, yet misinformation about H1N1 continues to spread (Sundaram et al., 2013), therefore we extend the time window by two more years.\n0 .2 .4 clickbait, etc.\nfiction, etc.\nsatire, etc.\nconspiracy, etc.\nfabricated, etc.\nmistake, etc.\nscam, etc.\nhoax, etc.\naltered, etc.\nlegend, etc.\n’16 US election ’20 US election\n0 .3 .6\nH1N1 COVID-19\nFigure 5: Misinformation between notable events. The most prevalent misinformation type for both US presidential elections is fabricated content, while the 2016 election has more hoaxes and satires. The H1N1 pandemic in 2009 has more legends and tales, while the COVID-19 pandemic attracts more conspiracy theories. (95% confidence intervals.)\nnificantly different between two pandemics, e.g., hoaxes , mistakes . Notably, the H1N1 pandemic has many more legends and tales , while COVID-19 has more conspiracy theories . The increased prevalence of COVID-19 related conspiracies aligns with recent work measuring the same phenomena (Uscinski et al., 2020; Jolley and Paterson, 2020), especially as the COVID-19 pandemic becomes increasingly politicized (Hart et al., 2020; Rothgerber et al., 2020; Weisel, 2021)."
    }, {
      "heading" : "6 Discussion",
      "text" : "In this section, we discuss limitations of our work and future directions, and finally conclude.\nLimitations and future directions. We adopted a computational approach to investigate our research question, and this method inherently shares common limitations with observational studies, e.g., prone to bias and confounding (Benson and Hartz, 2000). Specifically, our corpus contains fact-checks from Snopes.com, one of the most comprehensive fact-checking agencies in the US.\nSnopes.com covers a broader spectrum of topics than politics-focused fact-checkers (e.g., PolitiFact.com, FactCheck.org),15 and thus we argue that it covers a representative sample of misinformation within the US. However, Snopes.com may not be representative of the international misinformation ecosystem (Ahinkorah et al., 2020; Kaur et al., 2018; Fletcher et al., 2018). In the future, we hope that our method can help characterize misinformation comparatively on a global scale when more structured fact-checks become available.16 Additionally, fact-checkers are time constrained, as thus the misinformation stories they cover tend to be high-profile. Therefore low-prevalence, long-tail misinformation stories may not be observed in our study. Understanding low-volume misinformation types may require a different collection of corpora other than fact-checks, e.g., a cross-platform investigation on social media conversations (Wilson and Starbird, 2020; Abilov et al., 2021).\nLastly, the misinformation types we extract from our weakly supervised approach are not validated with ground-truth labels. This is largely due to the lack of empirical knowledge on misinformation types, and therefore we are unable to provide specific guidance to annotators. Although the clusters in Figure 3 provide straightforward structure of misinformation stories, in future work, we plan to leverage these results to construct annotation guidelines and obtain human-identified misinformation types for further analysis.\nConclusion. In this paper, we identify ten prevalent misinformation types with rationalized models on fact-checks and analyze their evolution over the last ten years and between notable events. We hope that this paper offers an empirical lens to the systematic understanding of fine-grained misinformation types, and complements existing work investigating the misinformation problem."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research was supported in part by NSF grant IIS-1553088. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF.\n15Also note that including these additional fact-checkers in the corpus would lead to oversampling of overlapping topics (e.g., politics).\n16Less-structured and under-represented fact-checks are difficult for computational modeling (Jiang et al., 2020a).\nEthical Considerations\nThis paper uses Snopes.com fact-checks to train and validate our models, and also includes several quotes and snippets of fact-checks.\nWe consider our case a fair use under the US17 copyright law, which permits limited use of copyrighted material without the need for permission from the copyright holder.\nAccording to 17 U.S.C. § 107, we discuss how our research abides the principles that are considered for a fair use judgment:\n• Purpose and character of the use: we use factchecks for noncommercial research purpose only, and additionally, using textual content for model training is considered to be transformative, cf. Authors Guild, Inc. v. Google Inc. (2013, 2015, 2016).\n• Amount and substantiality: we present only snippets of fact-checks for illustrative purpose in our paper (i.e., several quotes and snippets in text and figures), and only URLs to original fact-checks in our public dataset.\n• Effect upon work’s value: we do not identify any adverse impact our work may have on the potential market (e.g., ads, memberships) of the copyright holder.\nThe end goal of our research aligns with that of Snopes.com, i.e., to rebut misinformation and to restore credibility to the online information ecosystem. We hope the aggregated knowledge of factchecks from our models can shed light on this road and be a helpful addition to the literature."
    }, {
      "heading" : "B Additional Results",
      "text" : "In this section, we record additional results from our experiments that we omitted in the main paper.\nValidation performance. The evaluation results for all our experiments on both test and dev sets are reported in Table 2. We also include accuracy metric Ac(y) in the table18, and the evaluation results for fact-checks. Note that evaluation for z is empty for fact-checks, since there are no groundtruth rationales. As shown in Table 2, the results on dev sets align with our findings on test sets discussed in the main paper.\nModel size, computing machine and runtime. The number of parameters is 325K for hard rationalization models, and 967K for soft rationalization models. All experiments were conducted on a 12GB Nvidia Titan X GPU node, and finished training within an hour per experiment."
    }, {
      "heading" : "C Rationale Examples",
      "text" : "In this section, we list additional examples of extracted rationales for ten identified misinformation types.\nFor urban legends and tales :\n“...the 1930 Colette short story La Chienne (The Bitch) has become an urban legend in that its plot is often now related as a string of events that...”\nFor altered or doctored images :\n“...magazine covers of “highest paid” people. These doctored images have featured celebrities such as John Legend, Chuck Norris, Bob Dylan, Susan Boyle, and...”\nFor hoaxes and pranks :\n“...This meme is a hoax. Nobody is (or was) licking toilets as a form of protest against Donald Trump. The images shown in the meme were taken from...”\n18Our public dataset has balanced positive and negative labels therefore Ac(y) = F1(y).\nFor bogus scams :\n“...In October 2019, we came across a decidedly bizarre version of the scam. This time, Nigerian astronaut Abacha Tunde was reportedly stuck in space and...”\nFor mistakes and errors :\n“...noted that reports of missing children (which are typically resolved quickly) are often mistakenly confused by the public with relatively rare instances of...”\nFor fabricated content :\n“...The Neon Nettle report was “unusual” because it was completely fabricated: Bono said nothing during his Rolling Stone interview about “colluding with elites”...”\nFor baseless conspiracies :\n“...Furthermore, claims that COVID-19 was “manufactured,” or that it “escaped from” this Chinese lab, are nothing more than baseless conspiracy theories...”\nFor satires and parodies :\n“...This item was not a factual recounting of real-life events. The article originated with a website that describes its output as being humorous or satirical in nature...”\nFor fictitious content :\n“...However, both of these shocking quotes, along with the rest of article in which they are found, are completely fictitious. As the name of the web site implies...”\nFor sensational clickbait :\n“...And Breitbart regurgitated some of the pictures as viral clickbait under the headline “Armed Black Panthers Lobby for Democrat Gubernatorial Candidate Stacey Abrams”...”"
    } ],
    "references" : [ {
      "title" : "Voterfraud2020: a multimodal dataset of election fraud claims on twitter",
      "author" : [ "Anton Abilov", "Yiqing Hua", "Hana Matatov", "Ofra Amir", "Mor Naaman." ],
      "venue" : "Proceedings of the International AAAI Conference on Web and Social Media (ICWSM).",
      "citeRegEx" : "Abilov et al\\.,? 2021",
      "shortCiteRegEx" : "Abilov et al\\.",
      "year" : 2021
    }, {
      "title" : "Rising above misinformation or fake news in africa: Another strategy to control covid-19 spread",
      "author" : [ "Bright Opoku Ahinkorah", "Edward Kwabena Ameyaw", "John Elvis Hagan Jr", "Abdul-Aziz Seidu", "Thomas Schack." ],
      "venue" : "Frontiers in Communication.",
      "citeRegEx" : "Ahinkorah et al\\.,? 2020",
      "shortCiteRegEx" : "Ahinkorah et al\\.",
      "year" : 2020
    }, {
      "title" : "954 f",
      "author" : [ "Authors Guild", "Inc. v. Google Inc." ],
      "venue" : "supp. 2d 282 - dist. court, sd new york. 17Where the authors and Snopes.com reside.",
      "citeRegEx" : "Guild and Inc.,? 2013",
      "shortCiteRegEx" : "Guild and Inc.",
      "year" : 2013
    }, {
      "title" : "804 f",
      "author" : [ "Authors Guild", "Inc. v. Google Inc." ],
      "venue" : "3d 202 - court of appeals, 2nd circuit.",
      "citeRegEx" : "Guild and Inc.,? 2015",
      "shortCiteRegEx" : "Guild and Inc.",
      "year" : 2015
    }, {
      "title" : "136 s",
      "author" : [ "Authors Guild", "Inc. v. Google Inc." ],
      "venue" : "ct. 1658, 578 us 15, 194 l. ed. 2d 800 - supreme court.",
      "citeRegEx" : "Guild and Inc.,? 2016",
      "shortCiteRegEx" : "Guild and Inc.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of the International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Fake news and the economy of emotions: Problems, causes, solutions",
      "author" : [ "Vian Bakir", "Andrew McStay." ],
      "venue" : "Digital journalism.",
      "citeRegEx" : "Bakir and McStay.,? 2018",
      "shortCiteRegEx" : "Bakir and McStay.",
      "year" : 2018
    }, {
      "title" : "Modeling echo chambers and polarization dynamics in social networks",
      "author" : [ "Fabian Baumann", "Philipp Lorenz-Spreen", "Igor M Sokolov", "Michele Starnini." ],
      "venue" : "Physical Review Letters.",
      "citeRegEx" : "Baumann et al\\.,? 2020",
      "shortCiteRegEx" : "Baumann et al\\.",
      "year" : 2020
    }, {
      "title" : "A comparison of observational studies and randomized, controlled trials",
      "author" : [ "Kjell Benson", "Arthur J Hartz." ],
      "venue" : "New England Journal of Medicine.",
      "citeRegEx" : "Benson and Hartz.,? 2000",
      "shortCiteRegEx" : "Benson and Hartz.",
      "year" : 2000
    }, {
      "title" : "Nltk: the natural language toolkit",
      "author" : [ "Steven Bird." ],
      "venue" : "Proceedings of the COLING/ACL Interactive Presentation Sessions.",
      "citeRegEx" : "Bird.,? 2006",
      "shortCiteRegEx" : "Bird.",
      "year" : 2006
    }, {
      "title" : "Extractive adversarial networks: High-recall explanations for identifying personal attacks in social media posts",
      "author" : [ "Samuel Carton", "Qiaozhu Mei", "Paul Resnick." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Carton et al\\.,? 2018",
      "shortCiteRegEx" : "Carton et al\\.",
      "year" : 2018
    }, {
      "title" : "Politicization and polarization in climate change news content, 1985-2017",
      "author" : [ "Sedona Chinn", "P Sol Hart", "Stuart Soroka." ],
      "venue" : "Science Communication.",
      "citeRegEx" : "Chinn et al\\.,? 2020",
      "shortCiteRegEx" : "Chinn et al\\.",
      "year" : 2020
    }, {
      "title" : "The spreading of misinformation online",
      "author" : [ "Michela Del Vicario", "Alessandro Bessi", "Fabiana Zollo", "Fabio Petroni", "Antonio Scala", "Guido Caldarelli", "H Eugene Stanley", "Walter Quattrociocchi." ],
      "venue" : "Proceedings of the National Academy of Sciences of the",
      "citeRegEx" : "Vicario et al\\.,? 2016",
      "shortCiteRegEx" : "Vicario et al\\.",
      "year" : 2016
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the North American Chapter of the Association for Computational Linguistics",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Eraser: a benchmark to evaluate rationalized nlp models",
      "author" : [ "Jay DeYoung", "Sarthak Jain", "Nazneen Fatema Rajani", "Eric Lehman", "Caiming Xiong", "Richard Socher", "Byron C Wallace." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computa-",
      "citeRegEx" : "DeYoung et al\\.,? 2020",
      "shortCiteRegEx" : "DeYoung et al\\.",
      "year" : 2020
    }, {
      "title" : "Is greta thunberg the ‘highest paid activist’? Snopes.com",
      "author" : [ "Dan Evon" ],
      "venue" : null,
      "citeRegEx" : "Evon.,? \\Q2019\\E",
      "shortCiteRegEx" : "Evon.",
      "year" : 2019
    }, {
      "title" : "Measuring the reach of “fake news” and online disinformation in europe",
      "author" : [ "Richard Fletcher", "Alessio Cornia", "Lucas Graves", "Rasmus Kleis Nielsen." ],
      "venue" : "Reuters institute factsheet.",
      "citeRegEx" : "Fletcher et al\\.,? 2018",
      "shortCiteRegEx" : "Fletcher et al\\.",
      "year" : 2018
    }, {
      "title" : "Fake news on twitter during the 2016 us presidential election",
      "author" : [ "Nir Grinberg", "Kenneth Joseph", "Lisa Friedland", "Briony Swire-Thompson", "David Lazer." ],
      "venue" : "Science.",
      "citeRegEx" : "Grinberg et al\\.,? 2019",
      "shortCiteRegEx" : "Grinberg et al\\.",
      "year" : 2019
    }, {
      "title" : "Politicization and polarization in covid-19 news coverage",
      "author" : [ "P Sol Hart", "Sedona Chinn", "Stuart Soroka." ],
      "venue" : "Science Communication.",
      "citeRegEx" : "Hart et al\\.,? 2020",
      "shortCiteRegEx" : "Hart et al\\.",
      "year" : 2020
    }, {
      "title" : "Measuring misinformation in video search platforms: An audit study on youtube",
      "author" : [ "Eslam Hussein", "Prerna Juneja", "Tanushree Mitra." ],
      "venue" : "Proceedings of the ACM on Human-Computer Interaction (PACM HCI), 4(CSCW).",
      "citeRegEx" : "Hussein et al\\.,? 2020",
      "shortCiteRegEx" : "Hussein et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is not explanation",
      "author" : [ "Sarthak Jain", "Byron C Wallace." ],
      "venue" : "Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL).",
      "citeRegEx" : "Jain and Wallace.,? 2019",
      "shortCiteRegEx" : "Jain and Wallace.",
      "year" : 2019
    }, {
      "title" : "Learning to faithfully rationalize by construction",
      "author" : [ "Sarthak Jain", "Sarah Wiegreffe", "Yuval Pinter", "Byron C Wallace." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Jain et al\\.,? 2020",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2020
    }, {
      "title" : "Factoring fact-checks: Structured information extraction from fact-checking articles",
      "author" : [ "Shan Jiang", "Simon Baumgartner", "Abe Ittycheriah", "Cong Yu." ],
      "venue" : "Proceedings of the Web Conference (WWW).",
      "citeRegEx" : "Jiang et al\\.,? 2020a",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Modeling and measuring expressed (dis)belief in (mis)information",
      "author" : [ "Shan Jiang", "Miriam Metzger", "Andrew Flanagin", "Christo Wilson." ],
      "venue" : "Proceedings of the International AAAI Conference on Web and Social Media (ICWSM).",
      "citeRegEx" : "Jiang et al\\.,? 2020b",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Bias misperceived: The role of partisanship and misinformation in youtube comment moderation",
      "author" : [ "Shan Jiang", "Ronald E Robertson", "Christo Wilson." ],
      "venue" : "Proceedings of the International AAAI Conference on Web and Social Media (ICWSM).",
      "citeRegEx" : "Jiang et al\\.,? 2019",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2019
    }, {
      "title" : "Reasoning about political bias in content moderation",
      "author" : [ "Shan Jiang", "Ronald E. Robertson", "Christo Wilson." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI).",
      "citeRegEx" : "Jiang et al\\.,? 2020c",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Linguistic signals under misinformation and fact-checking: Evidence from user comments on social media",
      "author" : [ "Shan Jiang", "Christo Wilson." ],
      "venue" : "Proceedings of the ACM on Human-Computer Interaction (PACM HCI), 2(CSCW).",
      "citeRegEx" : "Jiang and Wilson.,? 2018",
      "shortCiteRegEx" : "Jiang and Wilson.",
      "year" : 2018
    }, {
      "title" : "Pylons ablaze: Examining the role of 5g covid-19 conspiracy beliefs and support for violence",
      "author" : [ "Daniel Jolley", "Jenny L Paterson." ],
      "venue" : "British journal of social psychology.",
      "citeRegEx" : "Jolley and Paterson.,? 2020",
      "shortCiteRegEx" : "Jolley and Paterson.",
      "year" : 2020
    }, {
      "title" : "Information disorder in asia and the pacific: Overview of misinformation ecosystem in australia, india",
      "author" : [ "Kanchan Kaur", "Shyam Nair", "Yenni Kwok", "Masato Kajimoto", "Yvonne T Chua", "Ma Labiste", "Carol Soon", "Hailey Jo", "Lihyun Lin", "Trieu Thanh Le" ],
      "venue" : null,
      "citeRegEx" : "Kaur et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Kaur et al\\.",
      "year" : 2018
    }, {
      "title" : "Fast face-swap using convolutional neural networks",
      "author" : [ "Iryna Korshunova", "Wenzhe Shi", "Joni Dambre", "Lucas Theis." ],
      "venue" : "Proceedings of the International Conference on Computer Vision (ICCV).",
      "citeRegEx" : "Korshunova et al\\.,? 2017",
      "shortCiteRegEx" : "Korshunova et al\\.",
      "year" : 2017
    }, {
      "title" : "Rationalizing neural predictions",
      "author" : [ "Tao Lei", "Regina Barzilay", "Tommi Jaakkola." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Lei et al\\.,? 2016",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2016
    }, {
      "title" : "Dissecting the meme magic: Understanding indicators of virality in image memes",
      "author" : [ "Chen Ling", "Ihab AbuHilal", "Jeremy Blackburn", "Emiliano De Cristofaro", "Savvas Zannettou", "Gianluca Stringhini." ],
      "venue" : "Proceedings of the ACM on Human-Computer Inter-",
      "citeRegEx" : "Ling et al\\.,? 2021",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2021
    }, {
      "title" : "Gcan: Graph-aware co-attention networks for explainable fake news detection on social media",
      "author" : [ "Yi-Ju Lu", "Cheng-Te Li." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Lu and Li.,? 2020",
      "shortCiteRegEx" : "Lu and Li.",
      "year" : 2020
    }, {
      "title" : "From dark to light: The many shades of sharing misinformation online",
      "author" : [ "Miriam Metzger", "Andrew Flanagin", "Paul Mena", "Shan Jiang", "Christo Wilson." ],
      "venue" : "Media and Communication, 9(1).",
      "citeRegEx" : "Metzger et al\\.,? 2021",
      "shortCiteRegEx" : "Metzger et al\\.",
      "year" : 2021
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "Workshop Proceedings of the International Conference on Learning Representations (ICLR Workshop).",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Crowdsourcing a word–emotion association lexicon",
      "author" : [ "Saif M Mohammad", "Peter D Turney." ],
      "venue" : "Computational Intelligence.",
      "citeRegEx" : "Mohammad and Turney.,? 2013",
      "shortCiteRegEx" : "Mohammad and Turney.",
      "year" : 2013
    }, {
      "title" : "Thumbs up? sentiment classification using machine learning techniques",
      "author" : [ "Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Pang et al\\.,? 2002",
      "shortCiteRegEx" : "Pang et al\\.",
      "year" : 2002
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Examining false beliefs about voter fraud in the wake of the 2020 presidential election",
      "author" : [ "Gordon Pennycook", "David Rand." ],
      "venue" : "Harvard Kennedy School Misinformation Review.",
      "citeRegEx" : "Pennycook and Rand.,? 2021",
      "shortCiteRegEx" : "Pennycook and Rand.",
      "year" : 2021
    }, {
      "title" : "What makes people join conspiracy communities? role of social factors in conspiracy engagement",
      "author" : [ "Shruti Phadke", "Mattia Samory", "Tanushree Mitra." ],
      "venue" : "Proceedings of the ACM on Human-Computer Interaction (PACM HCI), 4(CSCW).",
      "citeRegEx" : "Phadke et al\\.,? 2021",
      "shortCiteRegEx" : "Phadke et al\\.",
      "year" : 2021
    }, {
      "title" : "Gensim - statistical semantics in python",
      "author" : [ "Radim Rehurek", "Petr Sojka." ],
      "venue" : "Gensim.org.",
      "citeRegEx" : "Rehurek and Sojka.,? 2011",
      "shortCiteRegEx" : "Rehurek and Sojka.",
      "year" : 2011
    }, {
      "title" : "Auditing Partisan Audience Bias within Google Search",
      "author" : [ "Ronald E. Robertson", "Shan Jiang", "Kenneth Joseph", "Lisa Friedland", "David Lazer", "Christo Wilson." ],
      "venue" : "Proceedings of the ACM on HumanComputer Interaction (PACM HCI), 2(CSCW).",
      "citeRegEx" : "Robertson et al\\.,? 2018",
      "shortCiteRegEx" : "Robertson et al\\.",
      "year" : 2018
    }, {
      "title" : "Politicizing the covid-19 pandemic: ideological differences in adherence to social distancing",
      "author" : [ "Hank Rothgerber", "Thomas Wilson", "Davis Whaley", "Daniel L Rosenfeld", "Michael Humphrey", "Allie Moore", "Allison Bihl." ],
      "venue" : "PsyArXiv.",
      "citeRegEx" : "Rothgerber et al\\.,? 2020",
      "shortCiteRegEx" : "Rothgerber et al\\.",
      "year" : 2020
    }, {
      "title" : "the government spies using our webcams’ the language of conspiracy theories in online discussions",
      "author" : [ "Mattia Samory", "Tanushree Mitra." ],
      "venue" : "Proceedings of the ACM on Human-Computer Interaction (PACM HCI), 2(CSCW).",
      "citeRegEx" : "Samory and Mitra.,? 2018",
      "shortCiteRegEx" : "Samory and Mitra.",
      "year" : 2018
    }, {
      "title" : "Is attention interpretable? In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)",
      "author" : [ "Sofia Serrano", "Noah A Smith" ],
      "venue" : null,
      "citeRegEx" : "Serrano and Smith.,? \\Q2019\\E",
      "shortCiteRegEx" : "Serrano and Smith.",
      "year" : 2019
    }, {
      "title" : "Fake news detection on social media: A data mining perspective",
      "author" : [ "Kai Shu", "Amy Sliva", "Suhang Wang", "Jiliang Tang", "Huan Liu." ],
      "venue" : "ACM SIGKDD Explorations Newsletter.",
      "citeRegEx" : "Shu et al\\.,? 2017",
      "shortCiteRegEx" : "Shu et al\\.",
      "year" : 2017
    }, {
      "title" : "Disinformation as collaborative work: Surfacing the participatory nature of strategic information operations",
      "author" : [ "Kate Starbird", "Ahmer Arif", "Tom Wilson." ],
      "venue" : "Proceedings of the ACM on HumanComputer Interaction (PACM HCI), 3(CSCW).",
      "citeRegEx" : "Starbird et al\\.,? 2019",
      "shortCiteRegEx" : "Starbird et al\\.",
      "year" : 2019
    }, {
      "title" : "Influenza vaccination is not associated with detection of noninfluenza respiratory viruses in seasonal studies of influenza",
      "author" : [ "Maria E Sundaram", "David L McClure", "Jeffrey J VanWormer", "Thomas C Friedrich", "Jennifer K Meece", "Edward A Belongia" ],
      "venue" : null,
      "citeRegEx" : "Sundaram et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sundaram et al\\.",
      "year" : 2013
    }, {
      "title" : "Why do people believe covid-19 conspiracy theories",
      "author" : [ "Joseph E Uscinski", "Adam M Enders", "Casey Klofstad", "Michelle Seelig", "John Funchion", "Caleb Everett", "Stefan Wuchty", "Kamal Premaratne", "Manohar Murthi" ],
      "venue" : null,
      "citeRegEx" : "Uscinski et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Uscinski et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of the Conference on Neural Information Processing Systems (NeurIPS).",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "The rise of guardians: Fact-checking url recommendation to combat fake news",
      "author" : [ "Nguyen Vo", "Kyumin Lee." ],
      "venue" : "Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR).",
      "citeRegEx" : "Vo and Lee.,? 2018",
      "shortCiteRegEx" : "Vo and Lee.",
      "year" : 2018
    }, {
      "title" : "Where are the facts? searching for fact-checked information to alleviate the spread of fake news",
      "author" : [ "Nguyen Vo", "Kyumin Lee." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Vo and Lee.,? 2020",
      "shortCiteRegEx" : "Vo and Lee.",
      "year" : 2020
    }, {
      "title" : "Implementing agglomerative hierarchic clustering algorithms for use in document retrieval",
      "author" : [ "Ellen M Voorhees." ],
      "venue" : "Information Processing & Management.",
      "citeRegEx" : "Voorhees.,? 1986",
      "shortCiteRegEx" : "Voorhees.",
      "year" : 1986
    }, {
      "title" : "Understanding the use of fauxtography on social media",
      "author" : [ "Yuping Wang", "Fatemeh Tamahsbi", "Jeremy Blackburn", "Barry Bradlyn", "Emiliano De Cristofaro", "David Magerman", "Savvas Zannettou", "Gianluca Stringhini." ],
      "venue" : "Proceedings of the Inter-",
      "citeRegEx" : "Wang et al\\.,? 2021",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Fake news",
      "author" : [ "Claire Wardle." ],
      "venue" : "it’s complicated. First Draft News.",
      "citeRegEx" : "Wardle.,? 2017",
      "shortCiteRegEx" : "Wardle.",
      "year" : 2017
    }, {
      "title" : "Vaccination as a social contract: The case of covid-19 and us political partisanship",
      "author" : [ "Ori Weisel." ],
      "venue" : "Proceedings of the National Academy of Sciences of the United States of America (PNAS), 118(13).",
      "citeRegEx" : "Weisel.,? 2021",
      "shortCiteRegEx" : "Weisel.",
      "year" : 2021
    }, {
      "title" : "The emergence of deepfake technology: A review",
      "author" : [ "Mika Westerlund." ],
      "venue" : "Technology Innovation Management Review.",
      "citeRegEx" : "Westerlund.,? 2019",
      "shortCiteRegEx" : "Westerlund.",
      "year" : 2019
    }, {
      "title" : "Inducing a lexicon of abusive words–a feature-based approach",
      "author" : [ "Michael Wiegand", "Josef Ruppenhofer", "Anna Schmidt", "Clayton Greenberg." ],
      "venue" : "Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL).",
      "citeRegEx" : "Wiegand et al\\.,? 2018",
      "shortCiteRegEx" : "Wiegand et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is not not explanation",
      "author" : [ "Sarah Wiegreffe", "Yuval Pinter." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Wiegreffe and Pinter.,? 2019",
      "shortCiteRegEx" : "Wiegreffe and Pinter.",
      "year" : 2019
    }, {
      "title" : "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J Williams." ],
      "venue" : "Machine learning.",
      "citeRegEx" : "Williams.,? 1992",
      "shortCiteRegEx" : "Williams.",
      "year" : 1992
    }, {
      "title" : "Cross-platform disinformation campaigns: lessons learned and next steps",
      "author" : [ "Tom Wilson", "Kate Starbird." ],
      "venue" : "Harvard Kennedy School Misinformation Review.",
      "citeRegEx" : "Wilson and Starbird.,? 2020",
      "shortCiteRegEx" : "Wilson and Starbird.",
      "year" : 2020
    }, {
      "title" : "Different absorption from the same sharing: Sifted multi-task learning for fake news detection",
      "author" : [ "Lianwei Wu", "Yuan Rao", "Haolin Jin", "Ambreen Nazir", "Ling Sun." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Ex machina: Personal attacks seen at scale",
      "author" : [ "Ellery Wulczyn", "Nithum Thain", "Lucas Dixon." ],
      "venue" : "Proceedings of the Web Conference (WWW).",
      "citeRegEx" : "Wulczyn et al\\.,? 2017",
      "shortCiteRegEx" : "Wulczyn et al\\.",
      "year" : 2017
    }, {
      "title" : "Rethinking cooperative rationalization: Introspective extraction and complement control",
      "author" : [ "Mo Yu", "Shiyu Chang", "Yang Zhang", "Tommi Jaakkola." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Yu et al\\.,? 2019",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2019
    }, {
      "title" : "Using “annotator rationales” to improve machine learning for text categorization",
      "author" : [ "Omar Zaidan", "Jason Eisner", "Christine Piatko." ],
      "venue" : "Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL).",
      "citeRegEx" : "Zaidan et al\\.,? 2007",
      "shortCiteRegEx" : "Zaidan et al\\.",
      "year" : 2007
    }, {
      "title" : "On the origins of memes by means of fringe web communities",
      "author" : [ "Savvas Zannettou", "Tristan Caulfield", "Jeremy Blackburn", "Emiliano De Cristofaro", "Michael Sirivianos", "Gianluca Stringhini", "Guillermo Suarez-Tangil." ],
      "venue" : "Proceedings of the ACM Inter-",
      "citeRegEx" : "Zannettou et al\\.,? 2018",
      "shortCiteRegEx" : "Zannettou et al\\.",
      "year" : 2018
    }, {
      "title" : "Characterizing the use of images in state-sponsored information warfare operations by russian trolls on twitter",
      "author" : [ "Savvas Zannettou", "Tristan Caulfield", "Barry Bradlyn", "Emiliano De Cristofaro", "Gianluca Stringhini", "Jeremy Blackburn." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Zannettou et al\\.,? 2020",
      "shortCiteRegEx" : "Zannettou et al\\.",
      "year" : 2020
    }, {
      "title" : "Who let the trolls out? towards understanding state-sponsored trolls",
      "author" : [ "Savvas Zannettou", "Tristan Caulfield", "William Setzer", "Michael Sirivianos", "Gianluca Stringhini", "Jeremy Blackburn." ],
      "venue" : "Proceedings of the ACM Web Science Conference (WebSci).",
      "citeRegEx" : "Zannettou et al\\.,? 2019",
      "shortCiteRegEx" : "Zannettou et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Misinformation has raised increasing public concerns globally, well-documented in Africa (Ahinkorah et al., 2020), Asia (Kaur et al.",
      "startOffset" : 89,
      "endOffset" : 113
    }, {
      "referenceID" : 28,
      "context" : ", 2020), Asia (Kaur et al., 2018), and Europe (Fletcher et al.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 17,
      "context" : "In the US, “fake news” accounted for 6% of all news consumption during the 2016 US presidential election (Grinberg et al., 2019).",
      "startOffset" : 105,
      "endOffset" : 128
    }, {
      "referenceID" : 48,
      "context" : "Years later, 29% of US adults in a survey believed that the “exaggerated threat” of the COVID-19 pandemic purposefully damaged former US president Donald Trump (Uscinski et al., 2020), and 77% of Trump’s supporters believed “voter fraud” manipulated the 2020 US presidential election in spite of a complete lack of evidence (Pennycook and Rand, 2021).",
      "startOffset" : 160,
      "endOffset" : 183
    }, {
      "referenceID" : 38,
      "context" : ", 2020), and 77% of Trump’s supporters believed “voter fraud” manipulated the 2020 US presidential election in spite of a complete lack of evidence (Pennycook and Rand, 2021).",
      "startOffset" : 148,
      "endOffset" : 174
    }, {
      "referenceID" : 6,
      "context" : "multifaceted problem, from understanding the socio-psychological foundations of susceptibility (Bakir and McStay, 2018) and measuring public responses (Jiang and Wilson, 2018; Jiang et al.",
      "startOffset" : 95,
      "endOffset" : 119
    }, {
      "referenceID" : 26,
      "context" : "multifaceted problem, from understanding the socio-psychological foundations of susceptibility (Bakir and McStay, 2018) and measuring public responses (Jiang and Wilson, 2018; Jiang et al., 2020b), to designing detection algorithms (Shu et al.",
      "startOffset" : 151,
      "endOffset" : 196
    }, {
      "referenceID" : 23,
      "context" : "multifaceted problem, from understanding the socio-psychological foundations of susceptibility (Bakir and McStay, 2018) and measuring public responses (Jiang and Wilson, 2018; Jiang et al., 2020b), to designing detection algorithms (Shu et al.",
      "startOffset" : 151,
      "endOffset" : 196
    }, {
      "referenceID" : 45,
      "context" : ", 2020b), to designing detection algorithms (Shu et al., 2017) and auditing countermeasures for online platforms (Jiang et al.",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 54,
      "context" : "Although misinformation types have been theorized and categorized by practitioners (Wardle, 2017), there is, to our knowledge, no empirical research that has systematically measured these prevalent types of misinformation stories.",
      "startOffset" : 83,
      "endOffset" : 97
    }, {
      "referenceID" : 15,
      "context" : "com fact-check with a verdict of false (Evon, 2019):",
      "startOffset" : 39,
      "endOffset" : 51
    }, {
      "referenceID" : 30,
      "context" : "To realize this intuition, we experiment on existing rationalized neural models to extract these phrases (Lei et al., 2016; Jain et al., 2020), and, to target specific kinds of rationales, we additionally propose to include domain knowledge as weak supervision in the rationalizing process.",
      "startOffset" : 105,
      "endOffset" : 142
    }, {
      "referenceID" : 21,
      "context" : "To realize this intuition, we experiment on existing rationalized neural models to extract these phrases (Lei et al., 2016; Jain et al., 2020), and, to target specific kinds of rationales, we additionally propose to include domain knowledge as weak supervision in the rationalizing process.",
      "startOffset" : 105,
      "endOffset" : 142
    }, {
      "referenceID" : 64,
      "context" : "Using public datasets as validation (Zaidan et al., 2007; Carton et al., 2018), we evaluate the performance variation of different rationalized models, and show that including domain knowledge consistently improves the quality of extracted rationales.",
      "startOffset" : 36,
      "endOffset" : 78
    }, {
      "referenceID" : 10,
      "context" : "Using public datasets as validation (Zaidan et al., 2007; Carton et al., 2018), we evaluate the performance variation of different rationalized models, and show that including domain knowledge consistently improves the quality of extracted rationales.",
      "startOffset" : 36,
      "endOffset" : 78
    }, {
      "referenceID" : 54,
      "context" : "com, spanning from its founding in 1994 to 2021, we extract rationales by applying the selected model with theorized misinformation types for weak supervision (Wardle, 2017), and then cluster rationales based on their semantic similarity to summarize prevalent misinformation types.",
      "startOffset" : 159,
      "endOffset" : 173
    }, {
      "referenceID" : 23,
      "context" : "Within the computational linguistics community, from an audiences’ perspective, Jiang and Wilson (2018) found that social media users expressed different linguistic signals when responding to false claims, and the authors later used these signals to model and measure (dis)beliefs in (mis)information (Jiang et al., 2020b; Metzger et al., 2021).",
      "startOffset" : 301,
      "endOffset" : 344
    }, {
      "referenceID" : 33,
      "context" : "Within the computational linguistics community, from an audiences’ perspective, Jiang and Wilson (2018) found that social media users expressed different linguistic signals when responding to false claims, and the authors later used these signals to model and measure (dis)beliefs in (mis)information (Jiang et al., 2020b; Metzger et al., 2021).",
      "startOffset" : 301,
      "endOffset" : 344
    }, {
      "referenceID" : 61,
      "context" : "From a platforms’ perspective, researchers have assisted platforms in designing novel misinformation detection methods (Wu et al., 2019; Lu and Li, 2020; Vo and Lee, 2018, 2020), as well as audited existing misinformation intervention practices (Robertson et al.",
      "startOffset" : 119,
      "endOffset" : 177
    }, {
      "referenceID" : 32,
      "context" : "From a platforms’ perspective, researchers have assisted platforms in designing novel misinformation detection methods (Wu et al., 2019; Lu and Li, 2020; Vo and Lee, 2018, 2020), as well as audited existing misinformation intervention practices (Robertson et al.",
      "startOffset" : 119,
      "endOffset" : 177
    }, {
      "referenceID" : 41,
      "context" : ", 2019; Lu and Li, 2020; Vo and Lee, 2018, 2020), as well as audited existing misinformation intervention practices (Robertson et al., 2018; Jiang et al., 2019, 2020c; Hussein et al., 2020).",
      "startOffset" : 116,
      "endOffset" : 189
    }, {
      "referenceID" : 19,
      "context" : ", 2019; Lu and Li, 2020; Vo and Lee, 2018, 2020), as well as audited existing misinformation intervention practices (Robertson et al., 2018; Jiang et al., 2019, 2020c; Hussein et al., 2020).",
      "startOffset" : 116,
      "endOffset" : 189
    }, {
      "referenceID" : 46,
      "context" : ", state-sponsored disinformation (Starbird et al., 2019; Wilson and Starbird, 2020), fauxtography (Zannettou et al.",
      "startOffset" : 33,
      "endOffset" : 83
    }, {
      "referenceID" : 60,
      "context" : ", state-sponsored disinformation (Starbird et al., 2019; Wilson and Starbird, 2020), fauxtography (Zannettou et al.",
      "startOffset" : 33,
      "endOffset" : 83
    }, {
      "referenceID" : 65,
      "context" : ", 2019; Wilson and Starbird, 2020), fauxtography (Zannettou et al., 2018; Wang et al., 2021), and conspiracy theories (Samory and Mitra, 2018; Phadke et al.",
      "startOffset" : 49,
      "endOffset" : 92
    }, {
      "referenceID" : 53,
      "context" : ", 2019; Wilson and Starbird, 2020), fauxtography (Zannettou et al., 2018; Wang et al., 2021), and conspiracy theories (Samory and Mitra, 2018; Phadke et al.",
      "startOffset" : 49,
      "endOffset" : 92
    }, {
      "referenceID" : 63,
      "context" : "Recent work proposed to improve the initial model with an adversarial component (Yu et al., 2019; Carton et al., 2018).",
      "startOffset" : 80,
      "endOffset" : 118
    }, {
      "referenceID" : 10,
      "context" : "Recent work proposed to improve the initial model with an adversarial component (Yu et al., 2019; Carton et al., 2018).",
      "startOffset" : 80,
      "endOffset" : 118
    }, {
      "referenceID" : 59,
      "context" : "estimation (Williams, 1992), as sampling rationales is a non-differentiable computation.",
      "startOffset" : 11,
      "endOffset" : 27
    }, {
      "referenceID" : 5,
      "context" : "The popular attention mechanism (Bahdanau et al., 2014) provides built-in access to s.",
      "startOffset" : 32,
      "endOffset" : 55
    }, {
      "referenceID" : 20,
      "context" : "Although there have been debates on the properties achieved by attention-based explanations (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Serrano and Smith, 2019), rationales extracted by straightforward rules on attention weights were demonstrated as comparable to human-generated rationales (Jain et al.",
      "startOffset" : 92,
      "endOffset" : 169
    }, {
      "referenceID" : 58,
      "context" : "Although there have been debates on the properties achieved by attention-based explanations (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Serrano and Smith, 2019), rationales extracted by straightforward rules on attention weights were demonstrated as comparable to human-generated rationales (Jain et al.",
      "startOffset" : 92,
      "endOffset" : 169
    }, {
      "referenceID" : 44,
      "context" : "Although there have been debates on the properties achieved by attention-based explanations (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Serrano and Smith, 2019), rationales extracted by straightforward rules on attention weights were demonstrated as comparable to human-generated rationales (Jain et al.",
      "startOffset" : 92,
      "endOffset" : 169
    }, {
      "referenceID" : 21,
      "context" : "Although there have been debates on the properties achieved by attention-based explanations (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Serrano and Smith, 2019), rationales extracted by straightforward rules on attention weights were demonstrated as comparable to human-generated rationales (Jain et al., 2020).",
      "startOffset" : 300,
      "endOffset" : 319
    }, {
      "referenceID" : 64,
      "context" : "621 Movie reviews (Zaidan et al., 2007) Personal attacks (Carton et al.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 10,
      "context" : ", 2007) Personal attacks (Carton et al., 2018) Pr(z) %(z) F1(y) %(x) Pr(z) %(z) F1(y) %(x)",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 10,
      "context" : "We also run an experiment h3 with the additional adversarial component proposed in (Carton et al., 2018; Yu et al., 2019), and the evaluation metrics are not consistently improved compared to h0.",
      "startOffset" : 83,
      "endOffset" : 121
    }, {
      "referenceID" : 63,
      "context" : "We also run an experiment h3 with the additional adversarial component proposed in (Carton et al., 2018; Yu et al., 2019), and the evaluation metrics are not consistently improved compared to h0.",
      "startOffset" : 83,
      "endOffset" : 121
    }, {
      "referenceID" : 9,
      "context" : "We first preprocess collected fact-checks by extracting the main article content and verdicts from HTML webpages using a customized parser, and tokenizing the content with NLTK (Bird, 2006).",
      "startOffset" : 177,
      "endOffset" : 189
    }, {
      "referenceID" : 40,
      "context" : "For initialization, we train word embeddings using Gensim (Rehurek and Sojka, 2011) on the entire corpus.",
      "startOffset" : 58,
      "endOffset" : 83
    }, {
      "referenceID" : 34,
      "context" : "The hierarchical clustering uses cosine similarity as the distance metric, commonly used for word embeddings (Mikolov et al., 2013), and the complete link",
      "startOffset" : 109,
      "endOffset" : 131
    }, {
      "referenceID" : 52,
      "context" : "623 method (Voorhees, 1986) to obtain a relatively balanced linkage tree.",
      "startOffset" : 11,
      "endOffset" : 27
    }, {
      "referenceID" : 54,
      "context" : "from (Wardle, 2017), are identified as individual clusters from fact-checks.",
      "startOffset" : 5,
      "endOffset" : 19
    }, {
      "referenceID" : 43,
      "context" : "Conspiracy theories are often associated with intentional political campaigns (Samory and Mitra, 2018) which can affect their semantics when referenced in fact-checks.",
      "startOffset" : 78,
      "endOffset" : 102
    }, {
      "referenceID" : 29,
      "context" : "In contrast, digital alteration is a relatively recent misinformation tactic that has been enabled by technological developments such as FaceSwap (Korshunova et al., 2017) and DeepFake (Westerlund, 2019).",
      "startOffset" : 146,
      "endOffset" : 171
    }, {
      "referenceID" : 54,
      "context" : ", fabricated and misleading content belongs to two types of misinformation in (Wardle, 2017), while in our results they are clustered together.",
      "startOffset" : 78,
      "endOffset" : 92
    }, {
      "referenceID" : 29,
      "context" : "Additionally, the convenience offered by modern digital alteration software and applications (Korshunova et al., 2017; Westerlund, 2019) provides a gateway",
      "startOffset" : 93,
      "endOffset" : 136
    }, {
      "referenceID" : 56,
      "context" : "Additionally, the convenience offered by modern digital alteration software and applications (Korshunova et al., 2017; Westerlund, 2019) provides a gateway",
      "startOffset" : 93,
      "endOffset" : 136
    }, {
      "referenceID" : 31,
      "context" : "com’s attention and therefore explain a surge of digitally altered images (Ling et al., 2021; Wang et al., 2021).",
      "startOffset" : 74,
      "endOffset" : 112
    }, {
      "referenceID" : 53,
      "context" : "com’s attention and therefore explain a surge of digitally altered images (Ling et al., 2021; Wang et al., 2021).",
      "startOffset" : 74,
      "endOffset" : 112
    }, {
      "referenceID" : 47,
      "context" : "WHO declared an end to the global 2009 H1N1 pandemic on August 10, 2010, yet misinformation about H1N1 continues to spread (Sundaram et al., 2013), therefore we extend the time window by two more years.",
      "startOffset" : 123,
      "endOffset" : 146
    }, {
      "referenceID" : 48,
      "context" : "The increased prevalence of COVID-19 related conspiracies aligns with recent work measuring the same phenomena (Uscinski et al., 2020; Jolley and Paterson, 2020), especially as the COVID-19 pandemic becomes increasingly politicized (Hart et al.",
      "startOffset" : 111,
      "endOffset" : 161
    }, {
      "referenceID" : 27,
      "context" : "The increased prevalence of COVID-19 related conspiracies aligns with recent work measuring the same phenomena (Uscinski et al., 2020; Jolley and Paterson, 2020), especially as the COVID-19 pandemic becomes increasingly politicized (Hart et al.",
      "startOffset" : 111,
      "endOffset" : 161
    }, {
      "referenceID" : 18,
      "context" : ", 2020; Jolley and Paterson, 2020), especially as the COVID-19 pandemic becomes increasingly politicized (Hart et al., 2020; Rothgerber et al., 2020; Weisel, 2021).",
      "startOffset" : 105,
      "endOffset" : 163
    }, {
      "referenceID" : 42,
      "context" : ", 2020; Jolley and Paterson, 2020), especially as the COVID-19 pandemic becomes increasingly politicized (Hart et al., 2020; Rothgerber et al., 2020; Weisel, 2021).",
      "startOffset" : 105,
      "endOffset" : 163
    }, {
      "referenceID" : 55,
      "context" : ", 2020; Jolley and Paterson, 2020), especially as the COVID-19 pandemic becomes increasingly politicized (Hart et al., 2020; Rothgerber et al., 2020; Weisel, 2021).",
      "startOffset" : 105,
      "endOffset" : 163
    }, {
      "referenceID" : 1,
      "context" : "com may not be representative of the international misinformation ecosystem (Ahinkorah et al., 2020; Kaur et al., 2018; Fletcher et al., 2018).",
      "startOffset" : 76,
      "endOffset" : 142
    }, {
      "referenceID" : 28,
      "context" : "com may not be representative of the international misinformation ecosystem (Ahinkorah et al., 2020; Kaur et al., 2018; Fletcher et al., 2018).",
      "startOffset" : 76,
      "endOffset" : 142
    }, {
      "referenceID" : 16,
      "context" : "com may not be representative of the international misinformation ecosystem (Ahinkorah et al., 2020; Kaur et al., 2018; Fletcher et al., 2018).",
      "startOffset" : 76,
      "endOffset" : 142
    }, {
      "referenceID" : 60,
      "context" : ", a cross-platform investigation on social media conversations (Wilson and Starbird, 2020; Abilov et al., 2021).",
      "startOffset" : 63,
      "endOffset" : 111
    }, {
      "referenceID" : 0,
      "context" : ", a cross-platform investigation on social media conversations (Wilson and Starbird, 2020; Abilov et al., 2021).",
      "startOffset" : 63,
      "endOffset" : 111
    }, {
      "referenceID" : 22,
      "context" : "(16)Less-structured and under-represented fact-checks are difficult for computational modeling (Jiang et al., 2020a).",
      "startOffset" : 95,
      "endOffset" : 116
    } ],
    "year" : 2021,
    "abstractText" : "Misinformation has recently become a welldocumented matter of public concern. Existing studies on this topic have hitherto adopted a coarse concept of misinformation, which incorporates a broad spectrum of story types ranging from political conspiracies to misinterpreted pranks. This paper aims to structurize these misinformation stories by leveraging fact-check articles. Our intuition is that key phrases in a fact-check article that identify the misinformation type(s) (e.g., doctored images, urban legends) also act as rationales that determine the verdict of the fact-check (e.g., false). We experiment on rationalized models with domain knowledge as weak supervision to extract these phrases as rationales, and then cluster semantically similar rationales to summarize prevalent misinformation types. Using archived fact-checks from Snopes.com, we identify ten types of misinformation stories. We discuss how these types have evolved over the last ten years and compare their prevalence between the 2016/2020 US presidential elections and the H1N1/COVID-19 pandemics.",
    "creator" : "LaTeX with hyperref"
  }
}