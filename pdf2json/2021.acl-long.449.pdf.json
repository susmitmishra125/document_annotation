{
  "name" : "2021.acl-long.449.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Adapting Unsupervised Syntactic Parsing Methodology for Discourse Dependency Parsing",
    "authors" : [ "Liwen Zhang", "Ge Wang", "Wenjuan Han", "Kewei Tu" ],
    "emails" : [ "tukw}@shanghaitech.edu.cn", "hanwenjuan@bigai.ai" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5782–5794\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5782"
    }, {
      "heading" : "1 Introduction",
      "text" : "Discourse parsing, aiming to find how the text spans in a document relate to each other, benefits various down-stream tasks, such as machine translation evaluation (Guzmán et al., 2014; Joty et al., 2014), summarization (Marcu, 2000; Hirao et al., 2013), sentiment analysis (Bhatia et al., 2015; Huber and Carenini, 2020) and automated essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2013). Researchers have made impressive progress on discourse parsing from the constituency perspective, which presents discourse structures as constituency trees (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Joty et al., 2015; Nishida and\n*Corresponding author.\nNakayama, 2020). However, as demonstrated by Morey et al. (2018), discourse structure can also be formulated as a dependency structure. Besides that, there might exist ambiguous parsing in terms of the constituency perspective (Morey et al., 2018). All of these suggest that dependency discourse parsing is a different promising approach for discourse parsing.\nOne of the main bottlenecks in developing discourse dependency parsing methods is the lack of annotated training data since the labeling effort is labor-intensive and time-consuming, and needs well-trained experts with linguistic knowledge (Marcu et al., 1999). This problem can be tackled by employing unsupervised and semisupervised methods that can utilize unlabeled data. However, while unsupervised methodology has been studied for decades in syntactic dependency parsing, there is little attention paid to the counterpart in discourse dependency parsing. Considering the similarity between syntactic and discourse dependency parsing, it is natural to suggest such methodology can be adapted from the former to the latter.\nIn this paper, we propose a simple yet effective adaptation method that can be readily applied to different unsupervised syntactic dependency parsing approaches. Adaptation from syntactic dependency parsing to discourse dependency parsing has two challenges. First, unlike syntactic parsing which has a finite vocabulary, in discourse parsing, the number of elementary discourse units (EDUs) is unlimited. This makes it difficult if not impossible to directly apply syntactic approaches requiring enumeration of words or word categories to discourse parsing. Second, in a discourse dependency parse tree, the dependencies within a sentence or a paragraph often form a complete subtree. There is no correspondence to this constraint in syntactic parsing approaches. To address these two chal-\nlenges, we cluster the EDUs to produce clusters resembling Part-Of-Speech (POS) tags in syntactic parsing and we introduce the Hierarchical Eisner algorithm that finds the optimal parse tree conforming to the constraint.\nWe applied our adaptation method to two stateof-the-art unsupervised syntactic dependency parsing models: Neural Conditional Random Field Autoencoder (NCRFAE, Li and Tu (2020)) and Variational Variant of Discriminative Neural Dependency Model with Valences (V-DNDMV, Han et al. (2019)). In our experiments, the adapted models performs better than the baseline on both RST Discourse Treebank (RST-DT, Carlson et al. (2001)) and SciDTB (Yang and Li, 2018) in the unsupervised setting. When we extend the two models to the semi-supervised and supervised setting, we find they can outperform previous methods specially designed for supervised discourse parsing.\nFurther analysis indicates that the Hierarchical Eisner algorithm shows superiority not only in parsing accuracy but also in time and space efficiency. Its empirical time and space complexity is close to O(n2) with n being the number of EDUs, while the unconstrained algorithm adopted by most previous work has a complexity of O(n3). The code and trained models can be found at: https://github. com/Ehaschia/DiscourseDependencyParsing."
    }, {
      "heading" : "2 Related Work",
      "text" : "Unsupervised syntactic dependency parsing Unsupervised syntactic dependency parsing is the task to find syntactic dependency relations between words in sentences without guidance from annotations. The most popular approaches to this task are Dependency Model with Valences (DMV, Klein and Manning (2004)), a generative model learning the grammar from POS tags for dependency predictions, and its extensions. Jiang et al. (2016) employ neural networks to capture the similarities between POS tags ignored by vanilla DMV and Han et al. (2019) further amend the former with discriminative information obtained from an additional encoding network. Besides, there are also some discriminative approaches modeling the conditional probability or score of the dependency tree given the sentence, such as the CRF autoencoder method proposed by Cai et al. (2017).\nDiscourse dependency parsing There is limited work focusing on discourse dependency parsing. Li et al. (2014) proposes an algorithm to convert\nconstituency RST tree to dependency structure. In their algorithm, each non-terminal is assigned with a head EDU, which is the head EDU of its leftmost nucleus child. Then, a dependency relation is created for each non-terminal from its head to its dependent, in a procedure similar to those designed for syntactic parsing. Hirao et al. (2013) proposes another method that differs from the previous one in the processing of multinuclear relations. Yoshida et al. (2014) proposes a dependency parser built around a Maximum Spanning Tree decoder and trains on dependency trees converted from RSTDT. Their parser achieved better performance on the summarization task than a similar constituencybased parser. Morey et al. (2018) reviews the RST discourse parsing from the dependency perspective. They adapt the the best discourse constituency parsing models until 2018 to the dependency task. Yang and Li (2018) constructs a discourse dependency treebank SciDTB for scientific abstracts. To the best of our knowledge, we are the first to investigate unsupervised and semi-supervised discourse dependency parsing.\nUnsupervised Constituent Discourse Parsing Kobayashi et al. (2019) propose two unsupervised methods that build unlabeled constituent discourse trees by using the CKY dynamic programming algorithm. Their methods build the optimal tree in terms of a similarity (dissimilarity) score function that is defined for merging (splitting) text spans into larger (smaller) ones. Nishida et al. (2020) use Viterbi EM with a margin-based criterion to train a span-based neural unsupervised constituency discourse parser. The performance of these unsupervised methods is close to that of previous supervised parsers."
    }, {
      "heading" : "3 Adaptation",
      "text" : "We propose an adaptation method that can be readily integrated with different unsupervised syntactic dependency parsing approaches. First, we cluster the element discourse units (EDU) to produce clusters resembling POS tags or words used in syntactic parsing. This is necessary because many unsupervised syntactic parsers require enumeration of words or word categories, typically in modeling multinomial distributions as we shall see in Section 4. While EDUs, which are sequences of words, cannot be enumerated, its clusters can. During parsing, we apply the Hierarchical Eisner algorithm used for parse tree, a novel modified ver-\nsion of the classic Eisner algorithm, used for parse tree to produce discourse dependency parse trees that conform to the constraint that every sentence or paragraph should correspond to a complete subtree."
    }, {
      "heading" : "3.1 Clustering",
      "text" : "Given an input document represented as an EDU sequence x1, x2, . . . , xn, we can use word embedding or context sensitive word embedding to get the vector representation xi of the i-th EDU xi. Specifically, we use BERT (Devlin et al., 2019) to encode each word. Let wi be the encoding of the i-th word in the document. For an EDU xi spanning from word position b to e, we follow Toshniwal et al. (2020) and concatenate the encoding of the endpoints to form its representation: xi = [wb; we]. With the representations of all EDUs from the whole training corpus obtained, we use K-Means (Lloyd, 1982) to cluster them. Let ci be the cluster label of xi."
    }, {
      "heading" : "3.2 Hierarchical Eisner Algorithm",
      "text" : "The Eisner algorithm (Eisner, 1996) is a dynamic programming algorithm widely used to find the optimal syntactic dependency parse tree. The basic idea of it is to parse the left and right dependents of an token independently and combine them at a later stage. Algorithm 1 shows the pseudo-code of the Eisner algorithm. Here Ci→j represents a complete span, which consists of a head token i and all of its descendants on one side, and Ii→j represent an incomplete span, which consists of a head i and its partial descendants on one side and can be extended by adding more descendants to that side.\nDiscourse dependency parse trees, however,\nAlgorithm 1 Eisner Algorithm 1: Inputs:\ndemonstrate structural characteristics not taken into account by the Eisner algorithm. Specifically, a document has a hierarchical structure which divides the document into paragraphs, each paragraph into sentences, and finally each sentence into EDUs, and the discourse parse tree should be consistent with this hierarchical structure. Equivalently, in a discourse parse tree, every sentence or paragraph should be exactly covered by a complete subtree, like Figure 1. We empirically find that this constraint is satisfied by most of the gold discourse parses in the RST Discourse Treebank (RST-DT, Carlson et al. (2001)) and SciDTB (Yang and Li, 2018) datasets (Table 1).\nWe therefore propose the Hierarchical Eisner algorithm, a novel modification to the Eisner algorithm that incorporates the constraint. Our new algorithm has almost the same state transition formulas as the Eisner algorithm except for a few changes brought by the hierarchical constraint. Concretely, our algorithm finds the optimal parse tree in a bottom-up way and divides the process into 3 steps: intra-sentence parsing, intra-paragraph parsing, and intra-document parsing. In the intra-sentence parsing step, we run the original Eisner algorithm, except that we need not to form a tree. Then in the\nAlgorithm 2 Modification to Algorithm 1 6: Ii→j = max\ni≤k≤j (sij + Ci→k + Ck+1←j)\n7: Ii←j = max i≤k≤j (sji + Ci→k + Ck+1←j) 8: Ci→j = max i≤k≤j j∈E (Ii→k + Ck→j) . Here\nE is a set of the index of the end boundary of sentences.\n9: Ci←j = max i≤k≤j i∈B (Ci←k + Ik←j) . Here B\nis a set of the index of the begin boundary of sentences.\nintra-paragraph step, we combine all intra-sentence spans in the paragraph. Under the constraint that there can only be one EDU in every sentence whose head is not belong to this sentence. To achieve that, we modify the state transition equations (step 6-9 in Algorithm 1) to prune invalid arcs. Figure 2 shows some cases during merge across sentence spans. Case 1 are valid because the constraint is satisfied. Case 2 is invalid because the head of EDU e6 can not be e4 or e5 hence the constraint is violated. From these cases, we can find that for incomplete span Ii→k and complete span Ck→j across sentences, we only merge them when j is at the end boundary of a sentence as Algorithm 2 shows. After the intra-paragraph step, we move to the intra-document step to combine paragraph-level spans following the same procedure as in the intraparagraph step and form the final document-level tree.\nOur method has lower time complexity than the original Eisner algorithm. Suppose a document has kp paragraphs, each paragraph has ks sentences and each sentence has ke EDUs. The time complexity of the original Eisner algorithm is O(k3pk 3 sk 3 e) while the time complexity of our Hierarchical Eisner algorithm is O(k2pk 3 sk 3 e)."
    }, {
      "heading" : "4 Model",
      "text" : "We adapt two current state-of-the-art models in unsupervised syntactic dependency parsing for discourse parsing. One is Neural CRF Autoencoder (NCRFAE, Li and Tu (2020); Cai et al. (2017)), a discriminative model, and the other is : Variational Variant of DNDMV (V-DNDMV, Han et al. (2019)), a generative model."
    }, {
      "heading" : "4.1 Neural CRF Autoencoder",
      "text" : "A CRF autoencoder (Ammar et al., 2014) consists of an encoder and a decoder. The encoder predicts a hidden structure, such as a discourse dependency tree in our task, from the input and the decoder tries to reconstruct the input from the hidden structure. In a neuralized CRF autoencoder, we employ neural networks as the encoder and/or decoder.\nWe use the widely used biaffine dependency parser (Dozat and Manning, 2017) as the encoder to compute the hidden structure distribution PΦ(y|x), parameterized with Φ. Here y represents the hidden structure and x is input document. We feed the input document x into a Bi-LSTM network to produce the contextual representation of each EDU segmentation ri, and then feed ri to two MLP networks to produce two continuous vectors v(head)i and v(dep)i , representing i-th EDU segmentation being used as dependency head and dependent respectively.\nA biaffine function is used to compute the score matrix s. Each matrix element sij , the score for a dependency arc pointing from xi to xj , is computed as follows:\nsij = v (head)> i Wv (dep) i + b (1)\nwhere W is the parameter matrix and b is the bias.\nFollowing Dozat and Manning (2017) we formulate PΦ(y|x) as a head selection problem process that selects the dependency head of each EDU in-\ndependently:\nPΦ(y|x) = ∏ i P (hi|x) (2)\nwhere hi is the index of the head of EDU xi and P (hi|x) is computed by softmax function with score sij :\nP (hi = j|x) = esji∑n k=1 e ski (3)\nThe decoder parameterized with Λ computes PΛ(x̂|y), the probability of the reconstructed document x̂ given the parse tree y. Following Cai et al. (2017) and Li and Tu (2020), we independently predict each EDU x̂i from its head specified by y. Since EDUs cannot be enumerated, we reformulate the process as predicting the EDU cluster ĉi given its dependency head cluster chi . Our decoder simply specifies a categorical distribution P (ĉi|chi) for each possible EDU cluster and compute the reconstruction probability as follows:\nPΛ(x̂|y) = ∏ i P (ĉi|chi) (4)\nWe achieve the final reconstruction distribution by cascading the encoder and decoder distribution:\nPΦ,Λ(x̂,y|x) = PΛ(x̂|y)PΦ(y|x) (5)\nThe best parsing is obtained by maximizing PΦ,Λ(x̂,y|x):\ny∗ = arg max y PΦ,Λ(x̂,y|x) (6)\nWe consider the general case of training the CRF autoencoder with dataset D containing both labelled data L and unlabelled data U. Purely supervised or unsupervised learning can be seen as special cases of this setting. The loss functionL(D) consists of a labelled loss Ll(L) and an unlabelled loss Lu(U):\nL(D) = αLl(L) + (1− α)Lu(U) (7)\nwhere α is the hyperparameter weighting the importance of the two parts.\nFor the labelled data, where the gold parse trees y∗ are known, labelled loss is:\nLl(L) = − ∑ x∈L logPΦ,Λ(x̂,y ∗|x) (8)\nFor the unlabelled data where the gold parses are unknown, the unlabelled loss is:\nLu(U) = − ∑ x∈U max y∈Y(x) logPΦ,Λ(x̂,y|x) (9)\nWe optimize the encoder parameter Φ and decoder parameter Λ together with gradient descent methods."
    }, {
      "heading" : "4.2 Variational Variant of DNDMV",
      "text" : "V-DNDMV is a variational autoencoder model composed of both an encoder and a decoder. The encoder is a Bi-LSTM that takes the input document and produces parameters of a Gaussian distribution from which a continuous vector s summarizing the document sampled.\nThe decoder models the joint probability of the document and its discourse dependency tree condition on s with a generative grammar. The grammar is defined on a finite set of discrete symbols, so in our adapted model, input documents are represented by EDU clusters instead of EDUs that are infinite in number. There are three types of grammar rules, each associated with a set of probabilistic distributions: ROOT,CHILD and DECISION. To generate a document, we firstly sample from the ROOT distribution PROOT(chd|s) to determine the cluster label of the head EDU of the document and then recursively decide whether to generate a new child EDU cluster and what child EDU cluster to generate by sampling from the DECISION distribution PDECISION(dec|h, dir, val, s) and CHILD distribution PCHILD(chd|h, dir, val, s). dir denotes the generation direction (i.e, left or right), val is a binary variable denoting whether the current EDU already has a child in the direction dir or not. dec is a binary variable indicating whether to continue generating a child EDU, and h and chd denote the parent and child EDU cluster respectively. We use neural networks to calculate these distributions. The input of the networks is the continuous vector or matrix representations of grammar rule components such as h, chd, val and dir as well as document vector s produced by the encoder.\nThe training objective for learning the model is the probability of the training data. The intermediate continuous vector s and the hidden variable representing the dependency tree are both marginalized. Since the marginalized probability cannot be calculated exactly, V-DNDMV maximizes the Evidence Lower Bound (ELBO), a lower bound of the marginalized probability. ELBO consists of\nthe conditional likelihood of the training data and an regularisation term given by the KL divergence between PΘ(s|x) and P (s) (which is a standard Gaussian). The conditional likelihood is shown as follows:\nL(Θ) = 1 N N∑ i=1 ∑ y(i)∈Y(x(i)) logPΘ(x (i),y(i)|s(i))\n(10)\nHere N is the number of training samples, y is the dependency tree and Y(x) is the set of all possible dependency tree in x. Θ is the parameters of the neural networks. We can rewrite the conditional probability as following:\nPΘ(x,y|s) = ∏\nr∈(x,y)\nP (r|s) (11)\nwhere r is the grammar rule involved in generating x along with y.\nWe optimize ELBO using the expectationmaximization (EM) algorithm, alternating the Estep and the M-step. In the E-step, we fix rule parameters and use our Hierarchical Eisner algorithm to compute the expectation of possible dependency tree y, which gives the expected count of rules used in the training samples. In the M-step, expected count of rules computed in the E-step is used to train the prediction neural networks with gradient descent methods. The regularisation term is also optimized using gradient descent methods in the M-step. After training, the parsing result y∗of a new test case x is obtained as:\ny∗ = arg max y∈Y(x) PΘ(x,y|s) (12)"
    }, {
      "heading" : "5 Experiment",
      "text" : ""
    }, {
      "heading" : "5.1 Setting",
      "text" : "Data We evaluate the performance of our models on the RST Discourse Treebank* (RST-DT, Carlson et al. (2001)) and SciDTB† (Yang and Li, 2018). RST-DT consists of Wall Street Journal articles manually annotated with RST structures (Mann and Thompson, 1988). We use the method proposed by Li et al. (2014) to convert the RST structure samples into dependency structures. SciDTB consists of scientific abstracts from ACL Anthology annotated with dependency structures.\n*https://catalog.ldc.upenn.edu/ LDC2002T07\n†https://github.com/PKU-TANGENT/SciDTB\nHyper-parameter For our NCRFAE model, we adopt the hyper-parameters of Li and Tu (2020). For our V-NDNMV model we adopt the hyperparameters of Han et al. (2019). We use Adam (Kingma and Ba, 2015) to optimize our objective functions. Experimental details are provided in Appendix A."
    }, {
      "heading" : "5.2 Main Result",
      "text" : "We compared our methods with the following baselines:\nRight Branching (RB) is a rule based method. Given a sequence of elements (i.e., EDUs or subtrees), RB generates a left to right chain structure, like x1 → x2, x2 → x3 · · · . In order to develop a strong baseline, we include the hierarchical constraint introduced in Section 3.2 in this procedure. That is, we first build sentence-level discourse trees using the right branching method based on sentence segmentation. Then we build paragraph-level trees using the right branching method to form a left to right chain of sentencelevel subtrees. Finally we obtain document-level trees in the same way. Since this method has three stages, we call it “RB RB RB”. This simple procedure forms a strong baseline in terms of performance. As Nishida and Nakayama (2020) reports, the unlabeled F1 score of constituent structures of RB RB RB reaches 79.9 on RST-DT. Correspondingly, the performance of the supervised method proposed by (Joty et al., 2015) is 82.5.\nNISHIDA20 is a neural model for unsupervised discourse constituency parsing proposed by Nishida and Nakayama (2020). This model runs a CKY parser that uses a Bi-LSTM model to learn representations of text spans, complemented with lexical, syntactic and structural features. We convert its result to dependency structure using the same conversation method of Li et al. (2014). To make a fair comparison, we use RB RB RB to initialize their model instead of RB∗ RB RB as in their paper, where RB∗ means using predicted syntactic structures for initialization at the sentence level.\nCompared with baselines , our two adapted models NCRFAE and V-DNDMV both achieve better performance on the two datasets. Results also show that the generative model V-DNDMV is better than the discriminatve model NCRFAE in the unsupervised setting.\nWe also investigate the semi-supervised setting\non the SciDTB dataset of our adapted models with varied ratios of labeled/unlabeled data. Experimental results are shown in Figure 3, which indicate that NCRFAE outperforms V-DNDMV for all the ratios. Even when trained with only a few labeled data (0.01 of labeled data in SciDTB, only about 7 samples), the discriminative model already outperforms the generative model significantly. Besides that, we also find our semi-supervised methods reach higher UAS scores than their supervised versions (trained with labeled data only) for all the labeled/unlabeled data ratios.\nInspired by the promising results in the semisupervised setting, we also investigate the performance of our adapted NCRFAE and V-DNDMV in the fully supervised setting. The results are shown in Table 3. We evaluate our models on the RSTDT and SciDTB datasets and compare them with eight models. NIVRE04 (Nivre et al., 2004) and WANG17 (Wang et al., 2017) are two transitionbased models for dependency parsing. Yang and Li (2018) adapts them to discourse dependency parsing. FENG14 (Feng and Hirst, 2014), JI14\n‡We correct their evaluation metrics, so the result is different from the original paper (Li et al., 2014).\n(Ji and Eisenstein, 2014), JOTY15 (Joty et al., 2015) and BRAUD17 (Braud et al., 2017) are methods for discourse constituent parsing and they are adapted for discourse dependency parsing by Morey et al. (2018). LI14 (Li et al., 2014) and MOREY18 (Morey et al., 2018) are graph-based and transition-based methods specially designed for discourse dependency parsing, respectively. These models are statistical or simple neural models, and they do not use pretrained language models (like BERT, ELMo (Peters et al., 2018)) to extract features.\nAs Table 3 shows, the performance of our NCRFAE is significantly better than the baseline models. Especially, the UAS and LAS of NCRFAE are 8.9 points and 11.5 points higher than the best baseline models on the SciDTB dataset, respectively. Besides that, we find that V-DNDMV also beats baselines on the SciDTB dataset and reaches comparable results on RST-DT. We also test our approaches without using BERT and find that they still outperform the baselines. For example, the performance of NCRFAE with GloVe (Pennington et al., 2014) on Scidtb averaged over 5 runs is: UAS: 73.9 LAS: 55.5. These results again give evidence for our success in adapting unsupervised syntactic dependency parsing methods for discourse dependency parsing as the adapted methods not only work in the unsupervised setting, but also reach state-of-the-art in the supervised setting.\nAs for the performance gap between V-DNDMV and NCRFAE, we believe that the main reason is their different abilities to extract contextual features from the input text for the parsing task. As a generative model, the decoder of V-DNDMV follows\na strong assumption that each token in the input text is generated independently, which prevents the contextual features from being directly used. Instead, contextual features are mixed with other information in the document representation which acts as the condition of the generation process in the model. NCRFAE, on the other hand, employs a discriminative parser to leverage contextual features for dependency structure prediction directly. Thus, as long as there is sufficient labeled data, NCRFAE can achieve much better results than VDNDMV. We have observed a similar phenomenon in syntactic parsing.\nSignificance test We investigate the significance of the performance improvement in every setting. For unsupervised parsing, we perform a t-test between the strongest baseline RB RB RB and VDNDMV. The t-value and p-value calculated on 10 runs are 2.86 and 0.00104, which shows the significance of the improvement. For the semisupervised results, we also perform significance tests between the semi-supervised and supervisedonly results. The results show that our semisupervised method significantly outperforms the supervised-only method. For example, on the 0.5:0.5 setting, the t-value is 2.13 and the p-value is 0.04767. For the fully supervised setting, due to a lack of code from previous work, it is currently difficult for us to carry out a significance analysis. Instead, we show that our models are very stable and consistently outperform the baselines by running our models for 10-times. For example, our NCRFAE UAS score is 78.95±0.29 on the Scidtb dataset."
    }, {
      "heading" : "6 Analysis",
      "text" : ""
    }, {
      "heading" : "6.1 Eisner vs. Hierarchical Eisner",
      "text" : "In the left part of Figure 4 we show the curves of the time cost of the hierarchical and traditional Eisner algorithms against the RST-DT document length.\nThe experiments are run on servers equipped with NVIDIA Titan V GPUs. We can observe clearly that the curve of the Hierarchical Eisner algorithm always stays far below that of the Eisner algorithm, which verifies our theoretical analysis on the time complexity of the hierarchical Eisner algorithm in section 3.2.\nThe right part of Figure 4 demonstrates a similar phenomenon where we illustrate the memory usage of the hierarchical and traditional Eisner algorithms against the training document length in the same computing environment. From the curves of these two figures we can conclude that our Hierarchical Eisner algorithm has advantage over the traditional one in both time and space efficiencies.\nBesides the superiority in computational efficiency, our experiments also indicate that our Hierarchical Eisner algorithm can achieve better performance than the traditional one. With other conditions fixed, the UAS produced by Hierarchical Eisner is 79.1 in the task of supervised discourse parsing on the SciDTB dataset while the corresponding result of the Eisner algorithm is 78.6."
    }, {
      "heading" : "6.2 Number of clusters",
      "text" : "To explore the suitable number of clusters of EDUs, we evaluate our NCRFAE model with different cluster numbers from 10 to 100. As table 4 shows, there is an upward trend while the number of clusters increases from 10 to 50. After reaching the peak, the UAS decreases as the number of cluster continues to increase. We thus choose 50 for our experiments."
    }, {
      "heading" : "6.3 Label analysis",
      "text" : "In order to inspect if there exist any coherent relations between the clusters of EDUs obtained for\n§This is the actual evaluation result and the theoretical result should be 0.0\nadaptation in discourse parsing and the labels of dependency arcs, similar to that between POS tags and syntactic dependency labels, we compute the co-appearance distribution of cluster labels and dependency arc labels. In Figure 5, we show the probabilities of the clusters being used as heads phead(ck|rm) and children pchild(ck|rm) given different dependency types respectively. Here ck and rm represent different type of clusters and relations. We cluster EDUs to 10 clusters and only show a subset of them. Detailed heat-map can be found in Appendix B.\nBy observing the two heat-maps, we notice obvious trends that for each dependency arc label, the co-appearance probabilities are concentrated at certain cluster labels. For example, when the cluster is used as dependency heads, more than 60% of the co-appearance probability for arc label COMPARISON and SAME-UNIT is concentrated at cluster type 9 and 6 respectively; when the cluster is used as dependency children, cluster type 1 receives more than 40% of the co-appearance probability for certain arc labels. The property displayed by the adaptation clusters is very similar to that of POS tags, which justifies our clustering strategy adopted for discourse parsing.\nTo further quantify the coherence between the adaptation clusters and dependency arcs, we evaluate the mutual information between two discrete random variables in the training set of SciDTB: one is the tuple consists of two cluster labels for a pair of EDUs in the training sample, representing dependency head and child respectively; and the other is the binary random variable indicating whether there exists a dependency arc between a EDU pair\nin the training data. Besides our adaptation clusters, we also evaluate this metric for two other clustering strategies, random clustering and NICE proposed by He et al. (2018), for comparison and show the results in Table 5. We see that measured by mutual information, clusters produced by our clustering strategy is much more coherent with dependencies than the other strategies."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we propose a method to adapt unsupervised syntactic parsing methods for discourse dependency parsing. First, we cluster the element discourse units (EDU) to produce clusters resembling POS tags. Second, we modify the Eisner algorithm used for finding the optimal parse tree with hierarchical constraint. We apply the adaptations to two unsupervised syntactic dependency parsing methods. Experimental results show that our method successfully adapts the two models for discourse dependency parsing, which demonstrate advantages in both parsing accuracy and running efficiency."
    }, {
      "heading" : "Acknowledgment",
      "text" : "This work was supported by the National Natural Science Foundation of China (61976139)."
    }, {
      "heading" : "A Experimental Details for Our NCRFAE and V-DNDMV",
      "text" : "We implement our NCRFAE and V-DNDMV models by Pytorch 1.6 and Python 3.8.3. We run our experiments on a server with Intel(R) Xeon(R) Gold 5115 CPU and NVIDIA Titan V GPU. Based on these software and hardware environments, our NCRFAE and V-DNDMV models trained on the SciDTB dataset use about 30 and 45 minutes, respectively. Moreover, our NCRFAE and V-DNDMV models trained on the RST-DT dataset use about 4 and 18 hours, respectively. The number of parameters in NCRFAE is about 8.26 million, and the number of parameters in V-DNDMV is 0.47 million. The hyperparameter configurations of the result report in our paper are shown in table 6. We choose the hyperparameter configurations by manual tuning and the UAS score on the development dataset is used to select among them. Due to the lack of development set of RST-DT, we prepare a development set with 20 instances randomly sampled from the training set. The size of each dataset is shown in Table 7.\nB Full Heat-maps"
    } ],
    "references" : [ {
      "title" : "Conditional random field autoencoders for unsupervised structured prediction",
      "author" : [ "Waleed Ammar", "Chris Dyer", "Noah A Smith." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 27, pages 3311–3319. Curran Associates, Inc.",
      "citeRegEx" : "Ammar et al\\.,? 2014",
      "shortCiteRegEx" : "Ammar et al\\.",
      "year" : 2014
    }, {
      "title" : "Better document-level sentiment analysis from RST discourse parsing",
      "author" : [ "Parminder Bhatia", "Yangfeng Ji", "Jacob Eisenstein." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2212–2218, Lis-",
      "citeRegEx" : "Bhatia et al\\.,? 2015",
      "shortCiteRegEx" : "Bhatia et al\\.",
      "year" : 2015
    }, {
      "title" : "Cross-lingual RST discourse parsing",
      "author" : [ "Chloé Braud", "Maximin Coavoux", "Anders Søgaard." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 292–304,",
      "citeRegEx" : "Braud et al\\.,? 2017",
      "shortCiteRegEx" : "Braud et al\\.",
      "year" : 2017
    }, {
      "title" : "Holistic discourse coherence annotation for noisy essay writing",
      "author" : [ "Jill Burstein", "Joel Tetreault", "Martin Chodorow." ],
      "venue" : "Dialogue & Discourse, 4(2):34–52.",
      "citeRegEx" : "Burstein et al\\.,? 2013",
      "shortCiteRegEx" : "Burstein et al\\.",
      "year" : 2013
    }, {
      "title" : "CRF autoencoder for unsupervised dependency parsing",
      "author" : [ "Jiong Cai", "Yong Jiang", "Kewei Tu." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1638–1643, Copenhagen, Denmark. Association for",
      "citeRegEx" : "Cai et al\\.,? 2017",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2017
    }, {
      "title" : "Building a discourse-tagged corpus in the framework of Rhetorical Structure Theory",
      "author" : [ "Lynn Carlson", "Daniel Marcu", "Mary Ellen Okurovsky." ],
      "venue" : "Proceedings of the Second SIGdial Workshop on Discourse and Dialogue.",
      "citeRegEx" : "Carlson et al\\.,? 2001",
      "shortCiteRegEx" : "Carlson et al\\.",
      "year" : 2001
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep biaffine attention for neural dependency parsing",
      "author" : [ "Timothy Dozat", "Christopher D. Manning." ],
      "venue" : "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. Open-",
      "citeRegEx" : "Dozat and Manning.,? 2017",
      "shortCiteRegEx" : "Dozat and Manning.",
      "year" : 2017
    }, {
      "title" : "Three new probabilistic models for dependency parsing: An exploration",
      "author" : [ "Jason M. Eisner." ],
      "venue" : "COLING 1996 Volume 1: The 16th International Conference on Computational Linguistics.",
      "citeRegEx" : "Eisner.,? 1996",
      "shortCiteRegEx" : "Eisner.",
      "year" : 1996
    }, {
      "title" : "A lineartime bottom-up discourse parser with constraints and post-editing",
      "author" : [ "Vanessa Wei Feng", "Graeme Hirst." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 511–",
      "citeRegEx" : "Feng and Hirst.,? 2014",
      "shortCiteRegEx" : "Feng and Hirst.",
      "year" : 2014
    }, {
      "title" : "Using discourse structure improves machine translation evaluation",
      "author" : [ "Francisco Guzmán", "Shafiq Joty", "Lluı́s Màrquez", "Preslav Nakov" ],
      "venue" : "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Guzmán et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Guzmán et al\\.",
      "year" : 2014
    }, {
      "title" : "Enhancing unsupervised generative dependency parser with contextual information",
      "author" : [ "Wenjuan Han", "Yong Jiang", "Kewei Tu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5315–5325, Florence,",
      "citeRegEx" : "Han et al\\.,? 2019",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised learning of syntactic structure with invertible neural projections",
      "author" : [ "Junxian He", "Graham Neubig", "Taylor BergKirkpatrick." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "He et al\\.,? 2018",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2018
    }, {
      "title" : "Singledocument summarization as a tree knapsack problem",
      "author" : [ "Tsutomu Hirao", "Yasuhisa Yoshida", "Masaaki Nishino", "Norihito Yasuda", "Masaaki Nagata." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Hirao et al\\.,? 2013",
      "shortCiteRegEx" : "Hirao et al\\.",
      "year" : 2013
    }, {
      "title" : "Unsupervised learning of discourse structures using a tree autoencoder",
      "author" : [ "Patrick Huber", "Giuseppe Carenini." ],
      "venue" : "arXiv preprint arXiv:2012.09446.",
      "citeRegEx" : "Huber and Carenini.,? 2020",
      "shortCiteRegEx" : "Huber and Carenini.",
      "year" : 2020
    }, {
      "title" : "Representation learning for text-level discourse parsing",
      "author" : [ "Yangfeng Ji", "Jacob Eisenstein." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13–24, Baltimore, Maryland. Associ-",
      "citeRegEx" : "Ji and Eisenstein.,? 2014",
      "shortCiteRegEx" : "Ji and Eisenstein.",
      "year" : 2014
    }, {
      "title" : "Unsupervised neural dependency parsing",
      "author" : [ "Yong Jiang", "Wenjuan Han", "Kewei Tu." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 763–771, Austin, Texas. Association for Computational Lin-",
      "citeRegEx" : "Jiang et al\\.,? 2016",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2016
    }, {
      "title" : "CODRA: A novel discriminative framework for rhetorical analysis",
      "author" : [ "Shafiq Joty", "Giuseppe Carenini", "Raymond T. Ng." ],
      "venue" : "Computational Linguistics, 41(3):385–435.",
      "citeRegEx" : "Joty et al\\.,? 2015",
      "shortCiteRegEx" : "Joty et al\\.",
      "year" : 2015
    }, {
      "title" : "DiscoTK: Using discourse structure for machine translation evaluation",
      "author" : [ "Shafiq Joty", "Francisco Guzmán", "Lluı́s Màrquez", "Preslav Nakov" ],
      "venue" : "In Proceedings of the Ninth Workshop on Statistical Machine Translation,",
      "citeRegEx" : "Joty et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Joty et al\\.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations,",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Corpusbased induction of syntactic structure: Models of dependency and constituency",
      "author" : [ "Dan Klein", "Christopher Manning." ],
      "venue" : "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 478–485,",
      "citeRegEx" : "Klein and Manning.,? 2004",
      "shortCiteRegEx" : "Klein and Manning.",
      "year" : 2004
    }, {
      "title" : "Split or merge: Which is better for unsupervised RST parsing",
      "author" : [ "Naoki Kobayashi", "Tsutomu Hirao", "Kengo Nakamura", "Hidetaka Kamigaito", "Manabu Okumura", "Masaaki Nagata" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods",
      "citeRegEx" : "Kobayashi et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Kobayashi et al\\.",
      "year" : 2019
    }, {
      "title" : "Text-level discourse dependency parsing",
      "author" : [ "Sujian Li", "Liang Wang", "Ziqiang Cao", "Wenjie Li." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25–35, Baltimore, Maryland.",
      "citeRegEx" : "Li et al\\.,? 2014",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "Unsupervised crosslingual adaptation of dependency parsers using CRF autoencoders",
      "author" : [ "Zhao Li", "Kewei Tu." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2127–2133, Online. Association for Computational",
      "citeRegEx" : "Li and Tu.,? 2020",
      "shortCiteRegEx" : "Li and Tu.",
      "year" : 2020
    }, {
      "title" : "Least squares quantization in pcm",
      "author" : [ "Stuart Lloyd." ],
      "venue" : "IEEE transactions on information theory, 28(2):129–137.",
      "citeRegEx" : "Lloyd.,? 1982",
      "shortCiteRegEx" : "Lloyd.",
      "year" : 1982
    }, {
      "title" : "Rhetorical structure theory: Toward a functional theory of text organization",
      "author" : [ "William C Mann", "Sandra A Thompson." ],
      "venue" : "Text, 8(3):243–281.",
      "citeRegEx" : "Mann and Thompson.,? 1988",
      "shortCiteRegEx" : "Mann and Thompson.",
      "year" : 1988
    }, {
      "title" : "The theory and practice of discourse parsing and summarization",
      "author" : [ "Daniel Marcu." ],
      "venue" : "MIT press.",
      "citeRegEx" : "Marcu.,? 2000",
      "shortCiteRegEx" : "Marcu.",
      "year" : 2000
    }, {
      "title" : "Experiments in constructing a corpus of discourse trees",
      "author" : [ "Daniel Marcu", "Estibaliz Amorrortu", "Magdalena Romera." ],
      "venue" : "Towards Standards and Tools for Discourse Tagging.",
      "citeRegEx" : "Marcu et al\\.,? 1999",
      "shortCiteRegEx" : "Marcu et al\\.",
      "year" : 1999
    }, {
      "title" : "Evaluation of text coherence for electronic essay scoring systems",
      "author" : [ "Eleni Miltsakaki", "Karen Kukich." ],
      "venue" : "Natural Language Engineering, 10(1):25.",
      "citeRegEx" : "Miltsakaki and Kukich.,? 2004",
      "shortCiteRegEx" : "Miltsakaki and Kukich.",
      "year" : 2004
    }, {
      "title" : "A dependency perspective on RST discourse parsing and evaluation",
      "author" : [ "Mathieu Morey", "Philippe Muller", "Nicholas Asher." ],
      "venue" : "Computational Linguistics, 44(2):197–235.",
      "citeRegEx" : "Morey et al\\.,? 2018",
      "shortCiteRegEx" : "Morey et al\\.",
      "year" : 2018
    }, {
      "title" : "Unsupervised domain adaptation of language models for reading comprehension",
      "author" : [ "Kosuke Nishida", "Kyosuke Nishida", "Itsumi Saito", "Hisako Asano", "Junji Tomita." ],
      "venue" : "Proceedings of the 12th Language Resources and Evaluation Conference,",
      "citeRegEx" : "Nishida et al\\.,? 2020",
      "shortCiteRegEx" : "Nishida et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised discourse constituency parsing using Viterbi EM",
      "author" : [ "Noriki Nishida", "Hideki Nakayama." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:215–230.",
      "citeRegEx" : "Nishida and Nakayama.,? 2020",
      "shortCiteRegEx" : "Nishida and Nakayama.",
      "year" : 2020
    }, {
      "title" : "Memory-based dependency parsing",
      "author" : [ "Joakim Nivre", "Johan Hall", "Jens Nilsson." ],
      "venue" : "Proceedings of the Eighth Conference on Computational Natural Language Learning (CoNLL-2004) at HLT-NAACL 2004, pages 49–56, Boston, Massachusetts, USA.",
      "citeRegEx" : "Nivre et al\\.,? 2004",
      "shortCiteRegEx" : "Nivre et al\\.",
      "year" : 2004
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha,",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "A cross-task analysis of text span representations",
      "author" : [ "Shubham Toshniwal", "Haoyue Shi", "Bowen Shi", "Lingyu Gao", "Karen Livescu", "Kevin Gimpel." ],
      "venue" : "Proceedings of the 5th Workshop on Representation Learning for NLP, pages 166–176, Online. Associa-",
      "citeRegEx" : "Toshniwal et al\\.,? 2020",
      "shortCiteRegEx" : "Toshniwal et al\\.",
      "year" : 2020
    }, {
      "title" : "A two-stage parsing method for text-level discourse analysis",
      "author" : [ "Yizhong Wang", "Sujian Li", "Houfeng Wang." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 184–188, Vancou-",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "SciDTB: Discourse dependency TreeBank for scientific abstracts",
      "author" : [ "An Yang", "Sujian Li." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 444–449, Melbourne, Australia. As-",
      "citeRegEx" : "Yang and Li.,? 2018",
      "shortCiteRegEx" : "Yang and Li.",
      "year" : 2018
    }, {
      "title" : "Dependency-based discourse parser for single-document summarization",
      "author" : [ "Yasuhisa Yoshida", "Jun Suzuki", "Tsutomu Hirao", "Masaaki Nagata." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Yoshida et al\\.,? 2014",
      "shortCiteRegEx" : "Yoshida et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Discourse parsing, aiming to find how the text spans in a document relate to each other, benefits various down-stream tasks, such as machine translation evaluation (Guzmán et al., 2014; Joty et al., 2014), summarization (Marcu, 2000; Hirao et al.",
      "startOffset" : 164,
      "endOffset" : 204
    }, {
      "referenceID" : 18,
      "context" : "Discourse parsing, aiming to find how the text spans in a document relate to each other, benefits various down-stream tasks, such as machine translation evaluation (Guzmán et al., 2014; Joty et al., 2014), summarization (Marcu, 2000; Hirao et al.",
      "startOffset" : 164,
      "endOffset" : 204
    }, {
      "referenceID" : 26,
      "context" : ", 2014), summarization (Marcu, 2000; Hirao et al., 2013), sentiment analysis (Bhatia et al.",
      "startOffset" : 23,
      "endOffset" : 56
    }, {
      "referenceID" : 13,
      "context" : ", 2014), summarization (Marcu, 2000; Hirao et al., 2013), sentiment analysis (Bhatia et al.",
      "startOffset" : 23,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : ", 2013), sentiment analysis (Bhatia et al., 2015; Huber and Carenini, 2020) and automated essay scoring (Miltsakaki and Kukich, 2004; Burstein et al.",
      "startOffset" : 28,
      "endOffset" : 75
    }, {
      "referenceID" : 14,
      "context" : ", 2013), sentiment analysis (Bhatia et al., 2015; Huber and Carenini, 2020) and automated essay scoring (Miltsakaki and Kukich, 2004; Burstein et al.",
      "startOffset" : 28,
      "endOffset" : 75
    }, {
      "referenceID" : 28,
      "context" : ", 2015; Huber and Carenini, 2020) and automated essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2013).",
      "startOffset" : 62,
      "endOffset" : 114
    }, {
      "referenceID" : 3,
      "context" : ", 2015; Huber and Carenini, 2020) and automated essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2013).",
      "startOffset" : 62,
      "endOffset" : 114
    }, {
      "referenceID" : 29,
      "context" : "Besides that, there might exist ambiguous parsing in terms of the constituency perspective (Morey et al., 2018).",
      "startOffset" : 91,
      "endOffset" : 111
    }, {
      "referenceID" : 29,
      "context" : "However, as demonstrated by Morey et al. (2018), discourse structure can also be formulated as a dependency structure.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 27,
      "context" : "fort is labor-intensive and time-consuming, and needs well-trained experts with linguistic knowledge (Marcu et al., 1999).",
      "startOffset" : 101,
      "endOffset" : 121
    }, {
      "referenceID" : 37,
      "context" : "(2001)) and SciDTB (Yang and Li, 2018) in the unsupervised setting.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 21,
      "context" : "We applied our adaptation method to two stateof-the-art unsupervised syntactic dependency parsing models: Neural Conditional Random Field Autoencoder (NCRFAE, Li and Tu (2020)) and Variational Variant of Discriminative Neural Dependency Model with Valences (V-DNDMV, Han et al.",
      "startOffset" : 159,
      "endOffset" : 176
    }, {
      "referenceID" : 10,
      "context" : "We applied our adaptation method to two stateof-the-art unsupervised syntactic dependency parsing models: Neural Conditional Random Field Autoencoder (NCRFAE, Li and Tu (2020)) and Variational Variant of Discriminative Neural Dependency Model with Valences (V-DNDMV, Han et al. (2019)).",
      "startOffset" : 267,
      "endOffset" : 285
    }, {
      "referenceID" : 5,
      "context" : "In our experiments, the adapted models performs better than the baseline on both RST Discourse Treebank (RST-DT, Carlson et al. (2001)) and SciDTB (Yang and Li, 2018) in the unsupervised setting.",
      "startOffset" : 113,
      "endOffset" : 135
    }, {
      "referenceID" : 17,
      "context" : "The most popular approaches to this task are Dependency Model with Valences (DMV, Klein and Manning (2004)), a generative model learning the grammar from POS tags for dependency predictions, and its extensions.",
      "startOffset" : 82,
      "endOffset" : 107
    }, {
      "referenceID" : 14,
      "context" : "Jiang et al. (2016) employ neural networks to capture the similarities between POS tags ignored by vanilla DMV and Han et al.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 10,
      "context" : "(2016) employ neural networks to capture the similarities between POS tags ignored by vanilla DMV and Han et al. (2019) further amend the former with discriminative information obtained from an additional encoding network.",
      "startOffset" : 102,
      "endOffset" : 120
    }, {
      "referenceID" : 4,
      "context" : "Besides, there are also some discriminative approaches modeling the conditional probability or score of the dependency tree given the sentence, such as the CRF autoencoder method proposed by Cai et al. (2017).",
      "startOffset" : 191,
      "endOffset" : 209
    }, {
      "referenceID" : 21,
      "context" : "Li et al. (2014) proposes an algorithm to convert constituency RST tree to dependency structure.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 13,
      "context" : "Hirao et al. (2013) proposes another method that differs from the previous one in the processing of multinuclear relations.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 13,
      "context" : "Hirao et al. (2013) proposes another method that differs from the previous one in the processing of multinuclear relations. Yoshida et al. (2014) proposes a dependency parser built around a Maximum Spanning Tree decoder and trains on dependency trees converted from RSTDT.",
      "startOffset" : 0,
      "endOffset" : 146
    }, {
      "referenceID" : 13,
      "context" : "Hirao et al. (2013) proposes another method that differs from the previous one in the processing of multinuclear relations. Yoshida et al. (2014) proposes a dependency parser built around a Maximum Spanning Tree decoder and trains on dependency trees converted from RSTDT. Their parser achieved better performance on the summarization task than a similar constituencybased parser. Morey et al. (2018) reviews the RST discourse parsing from the dependency perspective.",
      "startOffset" : 0,
      "endOffset" : 401
    }, {
      "referenceID" : 37,
      "context" : "Yang and Li (2018) constructs a discourse dependency treebank SciDTB for scientific abstracts.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 21,
      "context" : "Unsupervised Constituent Discourse Parsing Kobayashi et al. (2019) propose two unsupervised",
      "startOffset" : 43,
      "endOffset" : 67
    }, {
      "referenceID" : 30,
      "context" : "Nishida et al. (2020) use Viterbi EM with a margin-based criterion to train a span-based neural unsupervised constituency discourse parser.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 6,
      "context" : "Specifically, we use BERT (Devlin et al., 2019) to encode each word.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 24,
      "context" : "With the representations of all EDUs from the whole training corpus obtained, we use K-Means (Lloyd, 1982) to cluster them.",
      "startOffset" : 93,
      "endOffset" : 106
    }, {
      "referenceID" : 6,
      "context" : "Specifically, we use BERT (Devlin et al., 2019) to encode each word. Let wi be the encoding of the i-th word in the document. For an EDU xi spanning from word position b to e, we follow Toshniwal et al. (2020) and concatenate the encoding of the endpoints to form its representation: xi = [wb; we].",
      "startOffset" : 27,
      "endOffset" : 210
    }, {
      "referenceID" : 8,
      "context" : "The Eisner algorithm (Eisner, 1996) is a dynamic programming algorithm widely used to find the optimal syntactic dependency parse tree.",
      "startOffset" : 21,
      "endOffset" : 35
    }, {
      "referenceID" : 37,
      "context" : "(2001)) and SciDTB (Yang and Li, 2018) datasets (Table 1).",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 5,
      "context" : "We empirically find that this constraint is satisfied by most of the gold discourse parses in the RST Discourse Treebank (RST-DT, Carlson et al. (2001)) and SciDTB (Yang and Li, 2018) datasets (Table 1).",
      "startOffset" : 130,
      "endOffset" : 152
    }, {
      "referenceID" : 21,
      "context" : "One is Neural CRF Autoencoder (NCRFAE, Li and Tu (2020); Cai et al.",
      "startOffset" : 39,
      "endOffset" : 56
    }, {
      "referenceID" : 4,
      "context" : "One is Neural CRF Autoencoder (NCRFAE, Li and Tu (2020); Cai et al. (2017)), a discriminative model, and the other is : Variational Variant of DNDMV (V-DNDMV, Han et al.",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 4,
      "context" : "One is Neural CRF Autoencoder (NCRFAE, Li and Tu (2020); Cai et al. (2017)), a discriminative model, and the other is : Variational Variant of DNDMV (V-DNDMV, Han et al. (2019)), a generative model.",
      "startOffset" : 57,
      "endOffset" : 177
    }, {
      "referenceID" : 0,
      "context" : "A CRF autoencoder (Ammar et al., 2014) consists of an encoder and a decoder.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 7,
      "context" : "We use the widely used biaffine dependency parser (Dozat and Manning, 2017) as the encoder to compute the hidden structure distribution PΦ(y|x), parameterized with Φ.",
      "startOffset" : 50,
      "endOffset" : 75
    }, {
      "referenceID" : 7,
      "context" : "Following Dozat and Manning (2017) we formulate PΦ(y|x) as a head selection problem process that selects the dependency head of each EDU in-",
      "startOffset" : 10,
      "endOffset" : 35
    }, {
      "referenceID" : 4,
      "context" : "Following Cai et al. (2017) and Li and Tu (2020), we independently predict each EDU x̂i from its head specified by y.",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 4,
      "context" : "Following Cai et al. (2017) and Li and Tu (2020), we independently predict each EDU x̂i from its head specified by y.",
      "startOffset" : 10,
      "endOffset" : 49
    }, {
      "referenceID" : 25,
      "context" : "RST-DT consists of Wall Street Journal articles manually annotated with RST structures (Mann and Thompson, 1988).",
      "startOffset" : 87,
      "endOffset" : 112
    }, {
      "referenceID" : 5,
      "context" : "Data We evaluate the performance of our models on the RST Discourse Treebank* (RST-DT, Carlson et al. (2001)) and SciDTB† (Yang and Li, 2018).",
      "startOffset" : 87,
      "endOffset" : 109
    }, {
      "referenceID" : 5,
      "context" : "Data We evaluate the performance of our models on the RST Discourse Treebank* (RST-DT, Carlson et al. (2001)) and SciDTB† (Yang and Li, 2018). RST-DT consists of Wall Street Journal articles manually annotated with RST structures (Mann and Thompson, 1988). We use the method proposed by Li et al. (2014) to convert the RST structure samples into dependency structures.",
      "startOffset" : 87,
      "endOffset" : 304
    }, {
      "referenceID" : 19,
      "context" : "We use Adam (Kingma and Ba, 2015) to optimize our objective functions.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 21,
      "context" : "com/PKU-TANGENT/SciDTB Hyper-parameter For our NCRFAE model, we adopt the hyper-parameters of Li and Tu (2020). For our V-NDNMV model we adopt the hyperparameters of Han et al.",
      "startOffset" : 94,
      "endOffset" : 111
    }, {
      "referenceID" : 11,
      "context" : "For our V-NDNMV model we adopt the hyperparameters of Han et al. (2019). We use Adam (Kingma and Ba, 2015) to optimize our objective functions.",
      "startOffset" : 54,
      "endOffset" : 72
    }, {
      "referenceID" : 31,
      "context" : "As Nishida and Nakayama (2020) reports, the unlabeled F1 score of constituent structures of RB RB RB reaches 79.",
      "startOffset" : 3,
      "endOffset" : 31
    }, {
      "referenceID" : 30,
      "context" : "NISHIDA20 is a neural model for unsupervised discourse constituency parsing proposed by Nishida and Nakayama (2020). This model runs a CKY parser that uses a Bi-LSTM model to learn representations of text spans, complemented with lexical, syntactic and structural features.",
      "startOffset" : 88,
      "endOffset" : 116
    }, {
      "referenceID" : 22,
      "context" : "We convert its result to dependency structure using the same conversation method of Li et al. (2014). To make a fair comparison, we use RB RB RB to initialize their model instead of RB∗ RB RB as in their paper, where RB∗ means using predicted syntactic structures for initialization at the sentence level.",
      "startOffset" : 84,
      "endOffset" : 101
    }, {
      "referenceID" : 36,
      "context" : ", 2004) and WANG17 (Wang et al., 2017) are two transitionbased models for dependency parsing.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 31,
      "context" : "NIVRE04 (Nivre et al., 2004) and WANG17 (Wang et al., 2017) are two transitionbased models for dependency parsing. Yang and Li (2018) adapts them to discourse dependency parsing.",
      "startOffset" : 9,
      "endOffset" : 134
    }, {
      "referenceID" : 22,
      "context" : "We correct their evaluation metrics, so the result is different from the original paper (Li et al., 2014).",
      "startOffset" : 88,
      "endOffset" : 105
    }, {
      "referenceID" : 2,
      "context" : "2015) and BRAUD17 (Braud et al., 2017) are methods for discourse constituent parsing and they are adapted for discourse dependency parsing by Morey et al.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 29,
      "context" : ", 2014) and MOREY18 (Morey et al., 2018) are graph-based",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 2,
      "context" : "2015) and BRAUD17 (Braud et al., 2017) are methods for discourse constituent parsing and they are adapted for discourse dependency parsing by Morey et al. (2018). LI14 (Li et al.",
      "startOffset" : 19,
      "endOffset" : 162
    }, {
      "referenceID" : 34,
      "context" : "These models are statistical or simple neural models, and they do not use pretrained language models (like BERT, ELMo (Peters et al., 2018)) to extract",
      "startOffset" : 118,
      "endOffset" : 139
    }, {
      "referenceID" : 33,
      "context" : "For example, the performance of NCRFAE with GloVe (Pennington et al., 2014) on Scidtb averaged over 5 runs is: UAS: 73.",
      "startOffset" : 50,
      "endOffset" : 75
    }, {
      "referenceID" : 12,
      "context" : "strategies, random clustering and NICE proposed by He et al. (2018), for comparison and show the results in Table 5.",
      "startOffset" : 51,
      "endOffset" : 68
    } ],
    "year" : 2021,
    "abstractText" : "One of the main bottlenecks in developing discourse dependency parsers is the lack of annotated training data. A potential solution is to utilize abundant unlabeled data by using unsupervised techniques, but there is so far little research in unsupervised discourse dependency parsing. Fortunately, unsupervised syntactic dependency parsing has been studied for decades, which could potentially be adapted for discourse parsing. In this paper, we propose a simple yet effective method to adapt unsupervised syntactic dependency parsing methodology for unsupervised discourse dependency parsing. We apply the method to adapt two state-of-the-art unsupervised syntactic dependency parsing methods. Experimental results demonstrate that our adaptation is effective. Moreover, we extend the adapted methods to the semi-supervised and supervised setting and surprisingly, we find that they outperform previous methods specially designed for supervised discourse parsing. Further analysis shows our adaptations result in superiority not only in parsing accuracy but also in time and space efficiency.",
    "creator" : "LaTeX with hyperref"
  }
}