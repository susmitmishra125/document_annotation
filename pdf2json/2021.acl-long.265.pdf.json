{
  "name" : "2021.acl-long.265.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment",
    "authors" : [ "Zewen Chi", "Li Dong", "Bo Zheng", "Shaohan Huang", "Xian-Ling Mao", "Heyan Huang", "Furu Wei" ],
    "emails" : [ "czw@bit.edu.cn", "maoxl@bit.edu.cn", "hhy63@bit.edu.cn", "lidong1@microsoft.com", "v-zhebo@microsoft.com", "shaohanh@microsoft.com", "fuwei@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3418–3430\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3418"
    }, {
      "heading" : "1 Introduction",
      "text" : "Despite the current advances in NLP, most applications and resources are still English-centric, making non-English users hard to access. Therefore, it is essential to build cross-lingual transferable models that can learn from the training data in highresource languages and generalize on low-resource languages. Recently, pretrained cross-lingual language models have shown their effectiveness for cross-lingual transfer. By pre-training on monolingual text and parallel sentences, the models provide significant improvements on a wide range of crosslingual end tasks (Conneau and Lample, 2019; Conneau et al., 2020; Liu et al., 2020; Chi et al., 2021b).\nCross-lingual language model pre-training is typically achieved by learning various pretext tasks on\n∗Contribution during internship at Microsoft Research.\nmonolingual and parallel corpora. By simply learning masked language modeling (MLM; Devlin et al. 2019) on monolingual text of multiple languages, the models surprisingly achieve competitive results on cross-lingual tasks (Wu and Dredze, 2019; K et al., 2020). Besides, several pretext tasks are proposed to utilize parallel corpora to learn better sentence-level cross-lingual representations (Conneau and Lample, 2019; Chi et al., 2021b; Hu et al., 2020a). For example, the translation language modeling (TLM; Conneau and Lample 2019) task performs MLM on the concatenated parallel sentences, which implicitly enhances cross-lingual transferability. However, most pretext tasks either learn alignment at the sentence level or implicitly encourage cross-lingual alignment, leaving explicit fine-grained alignment task not fully explored.\nIn this paper, we introduce a new cross-lingual pre-training task, named as denoising word alignment. Rather than relying on external word aligners trained on parallel corpora (Cao et al., 2020; Zhao et al., 2020; Wu and Dredze, 2020), we utilize self-labeled alignments in our task. During pretraining, we alternately self-label word alignments and conduct the denoising word alignment task in an expectation-maximization manner. Specifically, the model first self-labels word alignments for a translation pair. Then we randomly mask tokens in the bitext sentence, which is used as the perturbed input for denosing word alignment. For each masked token, the model learns a pointer network to predict the self-labeled alignments in the other language. We repeat the above two steps to iteratively boost the bitext alignment knowledge for cross-lingual pre-training.\nWe conduct extensive experiments on a wide range of cross-lingual understanding tasks. Experimental results show that our model outperforms the baseline models on various datasets, particularly on the token-level tasks such as question answer-\ning and structured prediction. Moreover, our model can also serve as a multilingual word aligner, which achieves reasonable low error rates on the bitext alignment benchmarks.\nOur contributions are summarized as follows:\n• We present a cross-lingual pre-training paradigm that alternately self-labels and predicts word alignments.\n• We introduce a pre-training task, denoising word alignment, which predicts word alignments from perturbed translation pairs.\n• We propose a word alignment algorithm that formulates the word alignment problem as optimal transport.\n• We demonstrate that our explicit alignment objective is effective for cross-lingual transfer."
    }, {
      "heading" : "2 Related Work",
      "text" : "Cross-lingual LM pre-training Pretrained with masked language modeling (MLM; Devlin et al. 2019) on monolingual text, multilingual BERT (mBERT; Devlin et al. 2019) and XLM-R (Conneau et al., 2020) produce promising results on cross-lingual transfer benchmarks (Hu et al., 2020b). mT5 (Xue et al., 2020) learns a multilingual version of T5 (Raffel et al., 2020) with text-totext tasks. In addition to monolingual text, several methods utilize parallel corpora to improve crosslingual transferability. XLM (Conneau and Lample, 2019) presents the translation language modeling (TLM) task that performs MLM on concatenated translation pairs. ALM (Yang et al., 2020) introduces code-switched sequences into cross-lingual LM pre-training. Unicoder (Huang et al., 2019) employs three cross-lingual tasks to learn mappings among languages. From an information-theoretic perspective, InfoXLM (Chi et al., 2021b) proposes the cross-lingual contrastive learning task to align sentence-level representations. Additionally, AMBER (Hu et al., 2020a) introduces an alignment objective that minimizes the distance between the forward and backward attention matrices. More recently, Ernie-M (Ouyang et al., 2020) presents the back-translation masked language modeling task that generates pseudo parallel sentence pairs for learning TLM, which provides better utilization of monolingual corpus. VECO (Luo et al., 2020) pretrains a unified cross-lingual language model for both NLU and NLG. mT6 (Chi et al., 2021a)\nimproves the multilingual text-to-text transformer with translation pairs.\nNotably, Word-aligned BERT models (Cao et al., 2020; Zhao et al., 2020) finetune mBERT by an explicit alignment objective that minimizes the distance between aligned tokens. Wu and Dredze (2020) exploit contrastive learning to improve the explicit alignment objectives. However, Wu and Dredze (2020) show that these explicit alignment objectives do not improve cross-lingual representations under a more extensive evaluation. Moreover, these models are restricted to stay close to their original pretrained values, which is not applicable for large-scale pre-training. On the contrary, we demonstrate that employing our explicit alignment objective in large-scale pre-training can provide consistent improvements over baseline models.\nWord alignment The IBM models (Brown et al., 1993) are statistical models for modeling the translation process that can extract word alignments between sentence pairs. A large number of word alignment models are based on the IBM models (Och and Ney, 2003; Mermer and Saraçlar, 2011; Dyer et al., 2013; Östling and Tiedemann, 2016). Recent studies have shown that word alignments can be extracted from neural machine translation models (Ghader and Monz, 2017; Koehn and Knowles, 2017; Li et al., 2019) or from pretrained cross-lingual LMs (Jalili Sabet et al., 2020; Nagata et al., 2020)."
    }, {
      "heading" : "3 Method",
      "text" : "Figure 1 illustrates an overview of our method for pre-training our cross-lingual LM, which is called XLM-ALIGN. XLM-ALIGN is pretrained in an expectation-maximization manner with two alternating steps, which are word alignment selflabeling and denoising word alignment. We first formulate word alignment as an optimal transport problem, and self-label word alignments of the input translation pair on-the-fly. Then, we update the model parameters with the denoising word alignment task, where the model uses a pointer network (Vinyals et al., 2015) to predict the aligned tokens from the perturbed translation pair."
    }, {
      "heading" : "3.1 Word Alignment Self-Labeling",
      "text" : "The goal of word alignment self-labeling is to estimate the word alignments of the input translation pair on-the-fly, given the current XLM-ALIGN model. Given a source sentence\nS = s1 . . . si . . . sn and a target sentence T = t1 . . . tj . . . tm, we model the word alignment between S and T as a doubly stochastic matrix A ∈ Rn×m+ such that the rows and the columns all sum to 1, where Aij stands for the probability of the alignment between si and tj . The rows and the columns of A represent probability distributions of the forward alignment and the backward alignment, respectively. To measure the similarity between two tokens from S and T , we define a metric function fsim by using cross-lingual representations produced by XLM-ALIGN:\nfsim(si, tj) = − logmax( ,h>i hj) (1)\nwhere is a constant to avoid negative values in the log function, and hi is the hidden vector of the i-th token by encoding the concatenated sequence of S and T with XLM-ALIGN. Empirically, the metric function produces a high similarity score if the two input tokens are semantically similar.\nThe word alignment problem is formulated as finding A that maximizes the sentence similarity between S and T :\nmax A n∑ i=1 m∑ j=1 Aijfsim(si, tj) (2)\nWe can find that Eq. (2) is identical to the regularized optimal transport problem (Peyré et al., 2019),\nif we add an entropic regularization to A:\nmax A n∑ i=1 m∑ j=1 Aijfsim(si, tj)− µAij logAij (3)\nEq. (3) has a unique solution A∗ such that\nA∗ = diag(u)Kdiag(v) (4)\nKij = e fsim(si,tj)/µ (5)\nwhere u ∈ Rn+,v ∈ Rm+ ,K ∈ Rn×m+ . According to Sinkhorn’s algorithm (Peyré et al., 2019), the variables u and v can be calculated by the following iterations:\nut+1 = 1n Kvt , vt+1 = 1m K>ut+1 (6)\nwhere vt can be initialized by vt=0 = 1m. With the solved stochastic matrix A∗, we can produce the forward word alignments −→ A by applying argmax over rows:\n−→ A = {(i, j) | j = argmax\nk A∗ik} (7)\nSimilarly, the backward word alignments ←− A can be computed by applying argmax over columns. To obtain high-precision alignment labels, we adopt an iterative alignment filtering operation. We initialize\nthe alignment labels A as ∅. In each iteration, we follow the procedure of Itermax (Jalili Sabet et al., 2020) that first computes −→ A and ←− A by Eq. (7). Then, the alignment labels are updated by:\nA ← A∪ ( −→ A ∩ ←− A) (8)\nFinally, A∗ is updated by:\nA∗ij ←  0, (i, j) ∈ A αA∗ij , ∃k (i, k) ∈ A ∨ (k, j) ∈ A A∗ij , others\n(9)\nwhere α is a discount factor. After several iterations, we obtain the final self-labeled word alignments A."
    }, {
      "heading" : "3.2 Denoising Word Alignment",
      "text" : "After self-labeling word alignments, we update the model parameters with the denoising word alignment (DWA) task. The goal of DWA is to predict the word alignments from the perturbed version of the input translation pair.\nConsider the perturbed version of the input translation pair (S∗, T ∗) constructed by randomly replacing the tokens with masks. We first encode the translation pair into hidden vectors h∗ with the XLM-ALIGN encoder:\nh∗1 . . .h ∗ n+m = encoder([S∗, T ∗]) (10)\nwhere [S∗, T ∗] is the concatenated sequence of S∗ and T ∗ with the length of n+m. Then, we build a pointer network upon the XLM-ALIGN encoder that predicts the word alignments. Specifically, for the i-th source token, we use h∗i as the query vector and h∗n+1, . . . , h ∗ n+m as the key vectors. Given the query and key vectors, the forward alignment probability ai is computed by the scaled dot-product attention (Vaswani et al., 2017):\nai = softmax( q>i K√ dh ) (11) qi = linear(h ∗ i ) (12) K = linear([h∗n+1 . . .h ∗ n+m]) (13)\nwhere dh is the dimension of the hidden vectors. Similarly, the backward alignment probability can be computed by above equations if we use target tokens as the query vectors and h∗1 . . .h ∗ n as key vectors. Notice that we only consider the self-labeled\nand masked positions as queries. Formally, we use the following query positions in the pointer network:\nP = {i|(i, ·) ∈ A ∨ (·, i) ∈ A} ∩M (14)\nwhere M is the set of masked positions. The training objective is to minimize the cross-entropy between the alignment probabilities and the selflabeled word alignments:\nLDWA = ∑ i∈P CE(ai,A(i)) (15)\nwhere CE(·, ·) stands for the cross-entropy loss, and A(i) is the self-labeled aligned position of the i-th token.\nAlgorithm 1 Pre-training XLM-ALIGN Input: Multilingual corpus Dm, parallel corpus Dp, learning rate τ Output: XLM-ALIGN parameters θ 1: Initialize θ with cold-start pre-training 2: while not converged do 3: X ∼ Dm, (S, T ) ∼ Dp 4: A ← fself-labeling(S, T ;θ) 5: g ← ∇θLMLM(X ) + ∇θLTLM(S, T ) + ∇θLDWA(S, T ,A)\n6: θ ← θ − τg"
    }, {
      "heading" : "3.3 Pre-training XLM-ALIGN",
      "text" : "We illustrate the pre-training procedure of XLMALIGN in Algorithm 1. In addition to DWA, we also include MLM and TLM for pre-training XLMALIGN, which implicitly encourage the crosslingual alignment. The overall loss function is defined as:\nLMLM(X ) + LTLM(S, T ) + LDWA(S, T ,A)\nIn each iteration, we first sample monolingual text X , and parallel text (S, T ). Then, we self-label word alignments and update the model parameters by learning pretext tasks. Notice that the model parameters are initialized by a cold-start pre-training to avoid producing low-quality alignment labels. The cold-start pre-training can be accomplished by using a pretrained LM as the model initialization."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Pre-training",
      "text" : "Following previous cross-lingual pretrained models (Conneau and Lample, 2019; Conneau et al.,\n2020; Chi et al., 2021b), we use raw sentences from the Wikipedia dump and CCNet (Wenzek et al., 2019) for MLM, including 94 languages. For TLM and DWA, we use parallel corpora from MultiUN (Ziemski et al., 2016), IIT Bombay (Kunchukuttan et al., 2018), OPUS (Tiedemann, 2012), and WikiMatrix (Schwenk et al., 2019), including 14 English-centric language pairs. We pretrain a Transformer with 12 layers and the hidden size of 768, where the parameters are initialized with XLM-R (Conneau et al., 2020). The model is optimized with the Adam optimizer (Kingma and Ba, 2015) for 150K steps with batch size of 2, 048. Notice that TLM and DWA share the same forward procedure for encoding the perturbed sentence pair. The pre-training of XLMALIGN takes about six days with two Nvidia DGX2 stations. More details of the training data and the hyperparameters are in supplementary document."
    }, {
      "heading" : "4.2 XTREME Benchmark",
      "text" : "XTREME is a multilingual benchmark for evaluating cross-lingual generalization. We evaluate our model on 7 cross-lingual downstream tasks included by XTREME, which can be grouped into 3 categories: (1) Structured prediction: part-ofspeech tagging on the Universal Dependencies v2.5 (Zeman et al., 2019), and named entity recognition on the WikiAnn (Pan et al., 2017; Rahimi et al., 2019) dataset; (2) Question answering: crosslingual question answering on MLQA (Lewis et al., 2020) and XQuAD (Artetxe et al., 2020), and gold passage of typologically diverse question answering (TyDiQA-GoldP; Clark et al. 2020); (3) Sentence classification: cross-lingual natural language inference (XNLI; Conneau et al. 2018), and crosslingual paraphrase adversaries from word scrambling (PAWS-X; Yang et al. 2019).\nBaselines We use the following pretrained crosslingual LMs as baselines. (1) Multilingual BERT (MBERT; Devlin et al. 2019) is pretrained with masked language modeling (MLM) and next sentence prediction on Wikipedia of 104 languages; (2) XLM (Conneau and Lample, 2019) is jointly pretrained with MLM on 100 languages and translation language modeling (TLM) on 14 language pairs; (3) MT5 (Xue et al., 2020) is the multilingual version of T5 pretrained with text-to-text tasks; (4) XLM-R (Conneau et al., 2020) is pretrained with MLM on large-scale CC-100 dataset with long training steps.\nFine-tuning Following Hu et al. (2020b), we adopt the zero-shot transfer setting for evaluation, where the models are only fine-tuned on English training data but evaluated on all target languages. Besides, we only use one model for evaluation on all target languages, rather than selecting different models for each language. The detailed fine-tuning hyperparameters can be found in supplementary document.\nResults In Table 1, we present the evaluation results on XTREME structured prediction, question answering, and sentence classification tasks. It can be observed that our XLM-ALIGN obtains the best average score over all the baseline models, improving the previous score from 66.4 to 68.9. It demonstrates that our model learns more transferable representations for the cross-lingual tasks, which is beneficial for building more accessible multilingual NLP applications. It is worth mentioning that our method brings noticeable improvements on the question answering and the structured prediction tasks. Compared with XLM-Rbase, XLM-ALIGN provides 6.7% and 1.9% F1 improvements on TyDiQA and NER. The improvements show that the\npretrained XLM-ALIGN benefits from the explicit word alignment objective, particularly on the structured prediction and question answering tasks that require token-level cross-lingual transfer. In terms of sentence classification tasks, XLM-ALIGN also consistently outperforms XLM-Rbase."
    }, {
      "heading" : "4.3 Word Alignment",
      "text" : "Word alignment is the task of finding corresponding word pairs in a parallel sentence. We conduct evaluations with golden alignments of four language pairs from EuroParl1, WPT20032, and WPT20053, containing 1,244 annotated sentence pairs in total. We use alignment error rate (AER; Och and Ney\n1www-i6.informatik.rwth-aachen.de/ goldAlignment/\n2web.eecs.umich.edu/˜mihalcea/wpt/ 3web.eecs.umich.edu/˜mihalcea/wpt05/\n2003) as the evaluation metrics.\nResults We first explore whether our word alignment self-labeling method is effective for generating high-quality alignment labels. Thus, we compare our method with (1) fast align (Dyer et al., 2013), a widely-used implementation of IBM Model 2 (Och and Ney, 2003); (2) SimAlign (Jalili Sabet et al., 2020), state-of-theart unsupervised word alignment method. For a fair comparison, we use the same pretrained LM and hidden layer as in SimAlign to produce sentence representations. In specific, we take the hidden vectors from the 8-th layer of XLM-Rbase or XLMALIGN, and obtain the alignments following the procedure as described in Section 3.1. Since the produced alignments are subword-level, we convert the alignments into word-level by the following rule that “if two subwords are aligned, the words they belong to are also aligned”.\nAs shown in Table 2, we report the AER scores on the four language pairs. It can be observed that our optimal-transport method outperforms fast align and SimAlign, demonstrating that our method can produce high-quality alignment labels, which is helpful for the DWA task. Moreover, our method consistently outperforms SimAlign when using hidden vectors from both XLM-Rbase and XLM-ALIGN.\nThen, we compare our XLM-ALIGN with XLMRbase on the word alignment task. Empirically, a lower AER indicates that the model learns better cross-lingual representations. From Table 2, XLM-ALIGN obtains the best AER results over all the four language pairs, reducing the averaged AER from 22.64 to 21.05. Besides, un-\nder both SimAlign and our optimal-transport method, XLM-ALIGN provides consistent reduction of AER, demonstrating the effectiveness of our method for learning fine-grained cross-lingual representations.\nWe also compare XLM-ALIGN with XLM-Rbase using the hidden vectors from the 3-th layer to the 12-th layer. We illustrate the averaged AER scores in Figure 2. Notice that the results on the first two layers are not presented in the figure because of the high AER. It can be observed that XLM-ALIGN consistently improves the results over XLM-Rbase across these layers. Moreover, it shows a parabolic trend across the layers of XLM-Rbase, which is consistent with the results in (Jalili Sabet et al., 2020). In contrast to XLM-Rbase, XLM-ALIGN alleviates this trend and greatly reduces AER in the last few layers. We believe this property of XLMALIGN brings better cross-lingual transferability on the end tasks."
    }, {
      "heading" : "5 Analysis",
      "text" : "In this section, we conduct comprehensive ablation studies for a better understanding of our XLMALIGN. To reduce the computational cost, we reduce the batch size to 256, and pretrain models with 50K steps in the following experiments."
    }, {
      "heading" : "5.1 Ablation Studies",
      "text" : "We perform ablation studies to understand the components of XLM-ALIGN, by removing the denoising word alignment loss (−DWA), the TLM loss (−TLM), or removing both (XLM-R*), which is identical to continue-training XLM-Rbase with MLM. We evaluate the models on XNLI, POS, NER, and MLQA, and present the results in Table 3. Comparing −TLM with −DWA, we find that DWA is more effective for POS and MLQA, while TLM performs better on XNLI and NER. Comparing −TLM with XLM-R*, it shows that directly learning DWA slightly harms the perfor-\nmance. However, jointly learning DWA with TLM provides remarkable improvements over −DWA, especially on the question answering and the structure prediction tasks that requires token-level crosslingual transfer. This indicates that TLM potentially improves the quality of self-labeled word alignments, making DWA more effective for crosslingual transfer."
    }, {
      "heading" : "5.2 Word Alignment Self-Labeling Layer",
      "text" : "It has been shown that the word alignment performance has a parabolic trend across the layers of mBERT and XLM-R (Jalili Sabet et al., 2020). It indicates that the middle layers produce higherquality word alignments than the bottom and the top layers. To explore which layer produces better alignment labels for pre-training, we pretrain three variants of XLM-ALIGN, where we use the hidden vectors from three different layers for word alignment self-labeling. We use the 8-th, 10-th, and 12-th layers for word alignment self-labeling during the pre-training. We present the evaluation results in Table 4. Surprisingly, although Layer8 produces higher-quality alignment labels at the beginning of the pre-training, using the alignment labels from the 12-th layer learns a more transferable XLM-ALIGN model for cross-lingual end tasks."
    }, {
      "heading" : "5.3 Denoising Word Alignment Layer",
      "text" : "Beyond the self-labeling layer, we also investigate which layer is better for learning the denoising word alignment task. Recent studies have shown\nthat it is beneficial to learn sentence-level crosslingual alignment at a middle layer (Chi et al., 2021b). Therefore, we pretrain XLM-ALIGN models by using three different layers for DWA, that is, using the hidden vectors of middle layers as the input of the pointer network. We compare the evaluation results of the three models in Table 5. It can be found that learning DWA at Layer-8 improves XNLI while learning DWA at higher layers produces better performance on the other three tasks. It suggests that, compared with sentence-level pretext tasks that prefers middle layers, the DWA task should be applied at top layers."
    }, {
      "heading" : "5.4 Effects of Alignment Filtering",
      "text" : "Although our self-labeling method produces highquality alignment labels, the alignment filtering operation can potentially make some of the tokens unaligned, which reduces the example efficiency. Thus, we explore whether the alignment filtering is beneficial for pre-training XLM-ALIGN. To this end, we pretrain an XLM-ALIGN model without alignment filtering. In specific, we use the union set of the forward and backward alignments as the selflabeled alignments so that all tokens are aligned at least once. The forward and backward alignments are obtained by applying the argmax function over rows and columns of A∗, respectively. Empirically, the alignment filtering operation generates high-precision yet fewer labels, while removing the filtering promises more labels but introduces low-confident labels. In Table 6, we compare the results of the models with or without alignment filtering. It can be observed that the alignment filtering operation improves the performance on the end tasks. This demonstrates that it is necessary to use high-precision labels for learning the denoising word alignment task. On the contrary, using perturbed alignment labels in pre-training harms the performance on the end tasks."
    }, {
      "heading" : "5.5 Effects of DWA Query Positions",
      "text" : "In the denoising word alignment task, we always use the hidden vectors of the masked positions\nas the query vectors in the pointer network. To explore the impact of the DWA query positions, we compare three different query positions in Table 7: (1) masked: only using the masked tokens as queries; (2) unmasked: randomly using 15% of the unmasked tokens as queries; (3) all-aligned: for each self-labeled aligned pair, randomly using one of the two tokens as a query. Also, we include the no-query baseline that does not use any queries, which is identical to removing DWA. It can be observed that using all the three query positions improves the performance over the no-query baseline. Moreover, using the masked positions as queries achieves better results than the other two positions, demonstrating the effectiveness of the masked query positions."
    }, {
      "heading" : "6 Discussion",
      "text" : "In this paper, we introduce denoising word alignment as a new cross-lingual pre-training task. By alternately self-labeling and predicting word alignments, our XLM-ALIGN model learns transferable cross-lingual representations. Experimental results show that our method improves the cross-lingual transferability on a wide range of tasks, particularly on the token-level tasks such as question answering and structured prediction.\nDespite the effectiveness for learning crosslingual transferable representations, our method also has the limitation that requires a cold-start pre-training to prevent the model from producing low-quality alignment labels. In our experiments, we also try to pretrain XLM-ALIGN from scratch, i.e., without cold-start pre-training. However, the DWA task does not work very well due to the lowquality of self-labeled alignments. Thus, we recommend continue-training XLM-ALIGN on the basis of other pretrained cross-lingual language models. For future work, we would like to research on removing this restriction so that the model can learn word alignments from scratch."
    }, {
      "heading" : "7 Ethical Considerations",
      "text" : "Despite the current advances in NLP, most NLP research works and applications are English-centric, making none-English users hard to access to NLPrelated services. Our method aims to pretrain cross-lingual language models that transfer supervision signals from high-resource languages to lowresource languages, which makes the NLP services and applications more accessible for low-resourcelanguage speakers. Furthermore, our method can build multilingual models that serve on different languages at the same time, reducing the computational resources for building multilingual models separately for each language."
    }, {
      "heading" : "Acknowledgements",
      "text" : "Heyan Huang is the corresponding author. The work is supported by National Key R&D Plan (No. 2018YFB1005100), National Natural Science Foundation of China (No. 61751201, 61602197 and 61772076), Natural Science Fund of Beijing (No. Z181100008918002), and the funds of Beijing Advanced Innovation Center for Language Resources (No. TYZ19005)."
    }, {
      "heading" : "A Pre-Training Data",
      "text" : "We use raw sentences from the Wikipedia dump and CCNet4 as monolingual corpora. The CCNet corpus we use is reconstructed following (Conneau et al., 2020) to reproduce the CC-100 corpus. The resulting corpus contains 94 languages. Table 8 and Table 9 report the language codes and data size of CCNet and Wikipedia dump. Notice that several languages share the same ISO language codes, e.g., zh represents both Simplified Chinese and Traditional Chinese. Besides, Table 10 shows the statistics of our parallel corpora."
    }, {
      "heading" : "B Hyperparameters for Pre-Training",
      "text" : "As shown in Table 11, we present the hyperparameters for pre-training XLM-ALIGN. We use the same vocabulary with XLM-R (Conneau et al., 2020).\n4https://github.com/facebookresearch/ cc_net"
    }, {
      "heading" : "C Hyperparameters for Fine-Tuning",
      "text" : "In Table 12, we present the hyperparameters for fine-tuning XLM-Rbase and XLM-ALIGN on the XTREME end tasks. For each task, the hyperparameters are searched on the joint validation set of all languages."
    }, {
      "heading" : "D Detailed Results on XTREME",
      "text" : "We present the detailed results of XLM-ALIGN on XTREME in Table 13-19."
    } ],
    "references" : [ {
      "title" : "On the cross-lingual transferability of monolingual representations",
      "author" : [ "Mikel Artetxe", "Sebastian Ruder", "Dani Yogatama." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623–4637, Online. Asso-",
      "citeRegEx" : "Artetxe et al\\.,? 2020",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2020
    }, {
      "title" : "The mathematics of statistical machine translation: Parameter estimation",
      "author" : [ "Peter F. Brown", "Stephen A. Della Pietra", "Vincent J. Della Pietra", "Robert L. Mercer." ],
      "venue" : "Computational Linguistics, 19(2):263– 311.",
      "citeRegEx" : "Brown et al\\.,? 1993",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 1993
    }, {
      "title" : "Multilingual alignment of contextual word representations",
      "author" : [ "Steven Cao", "Nikita Kitaev", "Dan Klein." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Cao et al\\.,? 2020",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2020
    }, {
      "title" : "mt6: Multilingual pretrained text-to-text transformer with translation pairs",
      "author" : [ "Zewen Chi", "Li Dong", "Shuming Ma", "Shaohan Huang Xian-Ling Mao", "Heyan Huang", "Furu Wei." ],
      "venue" : "arXiv preprint arXiv:2104.08692.",
      "citeRegEx" : "Chi et al\\.,? 2021a",
      "shortCiteRegEx" : "Chi et al\\.",
      "year" : 2021
    }, {
      "title" : "InfoXLM: An information-theoretic framework for cross-lingual language model pre-training",
      "author" : [ "Zewen Chi", "Li Dong", "Furu Wei", "Nan Yang", "Saksham Singhal", "Wenhui Wang", "Xia Song", "Xian-Ling Mao", "Heyan Huang", "Ming Zhou." ],
      "venue" : "Pro-",
      "citeRegEx" : "Chi et al\\.,? 2021b",
      "shortCiteRegEx" : "Chi et al\\.",
      "year" : 2021
    }, {
      "title" : "TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages",
      "author" : [ "Jonathan H. Clark", "Eunsol Choi", "Michael Collins", "Dan Garrette", "Tom Kwiatkowski", "Vitaly Nikolaev", "Jennimaria Palomaki." ],
      "venue" : "Transactions of the",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Alexis Conneau", "Guillaume Lample." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 7057–7067. Curran Associates, Inc.",
      "citeRegEx" : "Conneau and Lample.,? 2019",
      "shortCiteRegEx" : "Conneau and Lample.",
      "year" : 2019
    }, {
      "title" : "XNLI: Evaluating cross-lingual sentence representations",
      "author" : [ "Alexis Conneau", "Ruty Rinott", "Guillaume Lample", "Adina Williams", "Samuel Bowman", "Holger Schwenk", "Veselin Stoyanov." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods",
      "citeRegEx" : "Conneau et al\\.,? 2018",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "A simple, fast, and effective reparameterization of ibm model 2",
      "author" : [ "Chris Dyer", "Victor Chahuneau", "Noah A Smith." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Dyer et al\\.,? 2013",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2013
    }, {
      "title" : "What does attention in neural machine translation pay attention to",
      "author" : [ "Hamidreza Ghader", "Christof Monz" ],
      "venue" : "In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
      "citeRegEx" : "Ghader and Monz.,? \\Q2017\\E",
      "shortCiteRegEx" : "Ghader and Monz.",
      "year" : 2017
    }, {
      "title" : "Explicit alignment objectives for multilingual bidirectional encoders",
      "author" : [ "Junjie Hu", "Melvin Johnson", "Orhan Firat", "Aditya Siddhant", "Graham Neubig." ],
      "venue" : "arXiv preprint arXiv:2010.07972.",
      "citeRegEx" : "Hu et al\\.,? 2020a",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalization",
      "author" : [ "Junjie Hu", "Sebastian Ruder", "Aditya Siddhant", "Graham Neubig", "Orhan Firat", "Melvin Johnson." ],
      "venue" : "arXiv preprint arXiv:2003.11080.",
      "citeRegEx" : "Hu et al\\.,? 2020b",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "Unicoder: A universal language encoder by pretraining with multiple cross-lingual tasks",
      "author" : [ "Haoyang Huang", "Yaobo Liang", "Nan Duan", "Ming Gong", "Linjun Shou", "Daxin Jiang", "Ming Zhou." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Meth-",
      "citeRegEx" : "Huang et al\\.,? 2019",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "SimAlign: High quality word alignments without parallel training data using static and contextualized embeddings",
      "author" : [ "Masoud Jalili Sabet", "Philipp Dufter", "François Yvon", "Hinrich Schütze." ],
      "venue" : "Findings of the Association for Computational Linguis-",
      "citeRegEx" : "Sabet et al\\.,? 2020",
      "shortCiteRegEx" : "Sabet et al\\.",
      "year" : 2020
    }, {
      "title" : "Cross-lingual ability of multilingual bert: An empirical study",
      "author" : [ "Karthikeyan K", "Zihan Wang", "Stephen Mayhew", "Dan Roth." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "K et al\\.,? 2020",
      "shortCiteRegEx" : "K et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, San Diego, CA.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Six challenges for neural machine translation",
      "author" : [ "Philipp Koehn", "Rebecca Knowles." ],
      "venue" : "Proceedings of the First Workshop on Neural Machine Translation, pages 28–39, Vancouver. Association for Computational Linguistics.",
      "citeRegEx" : "Koehn and Knowles.,? 2017",
      "shortCiteRegEx" : "Koehn and Knowles.",
      "year" : 2017
    }, {
      "title" : "The IIT Bombay English-Hindi parallel corpus",
      "author" : [ "Anoop Kunchukuttan", "Pratik Mehta", "Pushpak Bhattacharyya." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation, Miyazaki, Japan. European Language",
      "citeRegEx" : "Kunchukuttan et al\\.,? 2018",
      "shortCiteRegEx" : "Kunchukuttan et al\\.",
      "year" : 2018
    }, {
      "title" : "MLQA: Evaluating cross-lingual extractive question answering",
      "author" : [ "Patrick Lewis", "Barlas Oguz", "Ruty Rinott", "Sebastian Riedel", "Holger Schwenk." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7315–",
      "citeRegEx" : "Lewis et al\\.,? 2020",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "On the word alignment from neural machine translation",
      "author" : [ "Xintong Li", "Guanlin Li", "Lemao Liu", "Max Meng", "Shuming Shi." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1293–1303.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Multilingual denoising pre-training for neural machine translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "arXiv preprint arXiv:2001.08210.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Veco: Variable encoder-decoder pre-training for cross-lingual understanding and generation",
      "author" : [ "Fuli Luo", "Wei Wang", "Jiahao Liu", "Yijia Liu", "Bin Bi", "Songfang Huang", "Fei Huang", "Luo Si." ],
      "venue" : "arXiv preprint arXiv:2010.16046.",
      "citeRegEx" : "Luo et al\\.,? 2020",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2020
    }, {
      "title" : "Bayesian word alignment for statistical machine translation",
      "author" : [ "Coşkun Mermer", "Murat Saraçlar." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 182–187.",
      "citeRegEx" : "Mermer and Saraçlar.,? 2011",
      "shortCiteRegEx" : "Mermer and Saraçlar.",
      "year" : 2011
    }, {
      "title" : "A supervised word alignment method based on cross-language span prediction using multilingual BERT",
      "author" : [ "Masaaki Nagata", "Katsuki Chousa", "Masaaki Nishino." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Nagata et al\\.,? 2020",
      "shortCiteRegEx" : "Nagata et al\\.",
      "year" : 2020
    }, {
      "title" : "A systematic comparison of various statistical alignment models",
      "author" : [ "Franz Josef Och", "Hermann Ney." ],
      "venue" : "Computational linguistics, 29(1):19–51.",
      "citeRegEx" : "Och and Ney.,? 2003",
      "shortCiteRegEx" : "Och and Ney.",
      "year" : 2003
    }, {
      "title" : "Efficient word alignment with markov chain monte carlo",
      "author" : [ "Robert Östling", "Jörg Tiedemann." ],
      "venue" : "The Prague Bulletin of Mathematical Linguistics, 106(1):125–146.",
      "citeRegEx" : "Östling and Tiedemann.,? 2016",
      "shortCiteRegEx" : "Östling and Tiedemann.",
      "year" : 2016
    }, {
      "title" : "Erniem: Enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora",
      "author" : [ "Xuan Ouyang", "Shuohuan Wang", "Chao Pang", "Yu Sun", "Hao Tian", "Hua Wu", "Haifeng Wang." ],
      "venue" : "arXiv preprint arXiv:2012.15674.",
      "citeRegEx" : "Ouyang et al\\.,? 2020",
      "shortCiteRegEx" : "Ouyang et al\\.",
      "year" : 2020
    }, {
      "title" : "Crosslingual name tagging and linking for 282 languages",
      "author" : [ "Xiaoman Pan", "Boliang Zhang", "Jonathan May", "Joel Nothman", "Kevin Knight", "Heng Ji." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Pan et al\\.,? 2017",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2017
    }, {
      "title" : "Computational optimal transport: With applications to data science",
      "author" : [ "Gabriel Peyré", "Marco Cuturi" ],
      "venue" : "Foundations and Trends® in Machine Learning,",
      "citeRegEx" : "Peyré and Cuturi,? \\Q2019\\E",
      "shortCiteRegEx" : "Peyré and Cuturi",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Re-",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Massively multilingual transfer for NER",
      "author" : [ "Afshin Rahimi", "Yuan Li", "Trevor Cohn." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 151–164, Florence, Italy. Association for Computational Linguis-",
      "citeRegEx" : "Rahimi et al\\.,? 2019",
      "shortCiteRegEx" : "Rahimi et al\\.",
      "year" : 2019
    }, {
      "title" : "WikiMatrix: Mining 135M parallel sentences in 1620 language pairs from wikipedia",
      "author" : [ "Holger Schwenk", "Vishrav Chaudhary", "Shuo Sun", "Hongyu Gong", "Francisco Guzmán." ],
      "venue" : "arXiv preprint arXiv:1907.05791.",
      "citeRegEx" : "Schwenk et al\\.,? 2019",
      "shortCiteRegEx" : "Schwenk et al\\.",
      "year" : 2019
    }, {
      "title" : "Parallel data, tools and interfaces in OPUS",
      "author" : [ "Jörg Tiedemann." ],
      "venue" : "Proceedings of the Eighth International Conference on Language Resources and Evaluation, pages 2214–2218, Istanbul, Turkey. European Language Resources Association.",
      "citeRegEx" : "Tiedemann.,? 2012",
      "shortCiteRegEx" : "Tiedemann.",
      "year" : 2012
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 5998–6008. Curran As-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Pointer networks",
      "author" : [ "Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 28, pages 2692–2700.",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "CCNet: Extracting high quality monolingual datasets from web crawl data",
      "author" : [ "Guillaume Wenzek", "Marie-Anne Lachaux", "Alexis Conneau", "Vishrav Chaudhary", "Francisco Guzman", "Armand Joulin", "Edouard Grave." ],
      "venue" : "arXiv preprint arXiv:1911.00359.",
      "citeRegEx" : "Wenzek et al\\.,? 2019",
      "shortCiteRegEx" : "Wenzek et al\\.",
      "year" : 2019
    }, {
      "title" : "Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT",
      "author" : [ "Shijie Wu", "Mark Dredze." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural",
      "citeRegEx" : "Wu and Dredze.,? 2019",
      "shortCiteRegEx" : "Wu and Dredze.",
      "year" : 2019
    }, {
      "title" : "Do explicit alignments robustly improve multilingual encoders? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4471–4482, Online",
      "author" : [ "Shijie Wu", "Mark Dredze." ],
      "venue" : "Association for Computa-",
      "citeRegEx" : "Wu and Dredze.,? 2020",
      "shortCiteRegEx" : "Wu and Dredze.",
      "year" : 2020
    }, {
      "title" : "mt5: A massively multilingual pre-trained text-to-text transformer",
      "author" : [ "Linting Xue", "Noah Constant", "Adam Roberts", "Mihir Kale", "Rami Al-Rfou", "Aditya Siddhant", "Aditya Barua", "Colin Raffel." ],
      "venue" : "arXiv preprint arXiv:2010.11934.",
      "citeRegEx" : "Xue et al\\.,? 2020",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2020
    }, {
      "title" : "Alternating language modeling for cross-lingual pre-training",
      "author" : [ "Jian Yang", "Shuming Ma", "Dongdong Zhang", "Shuangzhi Wu", "Zhoujun Li", "Ming Zhou." ],
      "venue" : "Thirty-Fourth AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Yang et al\\.,? 2020",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    }, {
      "title" : "PAWS-X: A cross-lingual adversarial dataset for paraphrase identification",
      "author" : [ "Yinfei Yang", "Yuan Zhang", "Chris Tar", "Jason Baldridge." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Universal dependencies 2.5. LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (ÚFAL)",
      "author" : [ "Daniel Zeman", "Joakim Nivre", "Mitchell Abrams" ],
      "venue" : "Faculty of Mathematics and Physics,",
      "citeRegEx" : "Zeman et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Zeman et al\\.",
      "year" : 2019
    }, {
      "title" : "Inducing languageagnostic multilingual representations",
      "author" : [ "Wei Zhao", "Steffen Eger", "Johannes Bjerva", "Isabelle Augenstein." ],
      "venue" : "arXiv preprint arXiv:2008.09112.",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    }, {
      "title" : "The united nations parallel corpus v1",
      "author" : [ "Michał Ziemski", "Marcin Junczys-Dowmunt", "Bruno Pouliquen." ],
      "venue" : "0. In LREC, pages 3530–3534.",
      "citeRegEx" : "Ziemski et al\\.,? 2016",
      "shortCiteRegEx" : "Ziemski et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "By pre-training on monolingual text and parallel sentences, the models provide significant improvements on a wide range of crosslingual end tasks (Conneau and Lample, 2019; Conneau et al., 2020; Liu et al., 2020; Chi et al., 2021b).",
      "startOffset" : 146,
      "endOffset" : 231
    }, {
      "referenceID" : 6,
      "context" : "By pre-training on monolingual text and parallel sentences, the models provide significant improvements on a wide range of crosslingual end tasks (Conneau and Lample, 2019; Conneau et al., 2020; Liu et al., 2020; Chi et al., 2021b).",
      "startOffset" : 146,
      "endOffset" : 231
    }, {
      "referenceID" : 22,
      "context" : "By pre-training on monolingual text and parallel sentences, the models provide significant improvements on a wide range of crosslingual end tasks (Conneau and Lample, 2019; Conneau et al., 2020; Liu et al., 2020; Chi et al., 2021b).",
      "startOffset" : 146,
      "endOffset" : 231
    }, {
      "referenceID" : 4,
      "context" : "By pre-training on monolingual text and parallel sentences, the models provide significant improvements on a wide range of crosslingual end tasks (Conneau and Lample, 2019; Conneau et al., 2020; Liu et al., 2020; Chi et al., 2021b).",
      "startOffset" : 146,
      "endOffset" : 231
    }, {
      "referenceID" : 9,
      "context" : "By simply learning masked language modeling (MLM; Devlin et al. 2019) on monolingual text of multiple languages, the models surprisingly achieve competitive results on cross-lingual tasks (Wu and Dredze, 2019; K et al.",
      "startOffset" : 44,
      "endOffset" : 69
    }, {
      "referenceID" : 38,
      "context" : "2019) on monolingual text of multiple languages, the models surprisingly achieve competitive results on cross-lingual tasks (Wu and Dredze, 2019; K et al., 2020).",
      "startOffset" : 124,
      "endOffset" : 161
    }, {
      "referenceID" : 16,
      "context" : "2019) on monolingual text of multiple languages, the models surprisingly achieve competitive results on cross-lingual tasks (Wu and Dredze, 2019; K et al., 2020).",
      "startOffset" : 124,
      "endOffset" : 161
    }, {
      "referenceID" : 7,
      "context" : "Besides, several pretext tasks are proposed to utilize parallel corpora to learn better sentence-level cross-lingual representations (Conneau and Lample, 2019; Chi et al., 2021b; Hu et al., 2020a).",
      "startOffset" : 133,
      "endOffset" : 196
    }, {
      "referenceID" : 4,
      "context" : "Besides, several pretext tasks are proposed to utilize parallel corpora to learn better sentence-level cross-lingual representations (Conneau and Lample, 2019; Chi et al., 2021b; Hu et al., 2020a).",
      "startOffset" : 133,
      "endOffset" : 196
    }, {
      "referenceID" : 12,
      "context" : "Besides, several pretext tasks are proposed to utilize parallel corpora to learn better sentence-level cross-lingual representations (Conneau and Lample, 2019; Chi et al., 2021b; Hu et al., 2020a).",
      "startOffset" : 133,
      "endOffset" : 196
    }, {
      "referenceID" : 2,
      "context" : "Rather than relying on external word aligners trained on parallel corpora (Cao et al., 2020; Zhao et al., 2020; Wu and Dredze, 2020), we utilize self-labeled alignments in our task.",
      "startOffset" : 74,
      "endOffset" : 132
    }, {
      "referenceID" : 44,
      "context" : "Rather than relying on external word aligners trained on parallel corpora (Cao et al., 2020; Zhao et al., 2020; Wu and Dredze, 2020), we utilize self-labeled alignments in our task.",
      "startOffset" : 74,
      "endOffset" : 132
    }, {
      "referenceID" : 39,
      "context" : "Rather than relying on external word aligners trained on parallel corpora (Cao et al., 2020; Zhao et al., 2020; Wu and Dredze, 2020), we utilize self-labeled alignments in our task.",
      "startOffset" : 74,
      "endOffset" : 132
    }, {
      "referenceID" : 9,
      "context" : "Cross-lingual LM pre-training Pretrained with masked language modeling (MLM; Devlin et al. 2019) on monolingual text, multilingual BERT (mBERT; Devlin et al.",
      "startOffset" : 71,
      "endOffset" : 96
    }, {
      "referenceID" : 9,
      "context" : "2019) on monolingual text, multilingual BERT (mBERT; Devlin et al. 2019) and XLM-R (Conneau et al.",
      "startOffset" : 45,
      "endOffset" : 72
    }, {
      "referenceID" : 6,
      "context" : "2019) and XLM-R (Conneau et al., 2020) produce promising results on cross-lingual transfer benchmarks (Hu et al.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 13,
      "context" : ", 2020) produce promising results on cross-lingual transfer benchmarks (Hu et al., 2020b).",
      "startOffset" : 71,
      "endOffset" : 89
    }, {
      "referenceID" : 40,
      "context" : "mT5 (Xue et al., 2020) learns a multilingual version of T5 (Raffel et al.",
      "startOffset" : 4,
      "endOffset" : 22
    }, {
      "referenceID" : 31,
      "context" : ", 2020) learns a multilingual version of T5 (Raffel et al., 2020) with text-totext tasks.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 7,
      "context" : "XLM (Conneau and Lample, 2019) presents the translation language modeling (TLM) task that performs MLM on concatenated translation pairs.",
      "startOffset" : 4,
      "endOffset" : 30
    }, {
      "referenceID" : 41,
      "context" : "ALM (Yang et al., 2020) introduces code-switched sequences into cross-lingual LM pre-training.",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 14,
      "context" : "Unicoder (Huang et al., 2019) employs three cross-lingual tasks to learn mappings among languages.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 4,
      "context" : "From an information-theoretic perspective, InfoXLM (Chi et al., 2021b) proposes the cross-lingual contrastive learning task to align sentence-level representations.",
      "startOffset" : 51,
      "endOffset" : 70
    }, {
      "referenceID" : 12,
      "context" : "Additionally, AMBER (Hu et al., 2020a) introduces an alignment objective that minimizes the distance between the forward and backward attention matrices.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 28,
      "context" : "More recently, Ernie-M (Ouyang et al., 2020) presents the back-translation masked language modeling task that generates pseudo parallel sentence pairs for learning TLM, which provides better utilization of monolingual corpus.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 23,
      "context" : "VECO (Luo et al., 2020) pretrains a unified cross-lingual language model for both NLU and NLG.",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 3,
      "context" : "mT6 (Chi et al., 2021a) improves the multilingual text-to-text transformer with translation pairs.",
      "startOffset" : 4,
      "endOffset" : 23
    }, {
      "referenceID" : 2,
      "context" : "Notably, Word-aligned BERT models (Cao et al., 2020; Zhao et al., 2020) finetune mBERT by an explicit alignment objective that minimizes the distance between aligned tokens.",
      "startOffset" : 34,
      "endOffset" : 71
    }, {
      "referenceID" : 44,
      "context" : "Notably, Word-aligned BERT models (Cao et al., 2020; Zhao et al., 2020) finetune mBERT by an explicit alignment objective that minimizes the distance between aligned tokens.",
      "startOffset" : 34,
      "endOffset" : 71
    }, {
      "referenceID" : 1,
      "context" : "Word alignment The IBM models (Brown et al., 1993) are statistical models for modeling the trans-",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 26,
      "context" : "A large number of word alignment models are based on the IBM models (Och and Ney, 2003; Mermer and Saraçlar, 2011; Dyer et al., 2013; Östling and Tiedemann, 2016).",
      "startOffset" : 68,
      "endOffset" : 162
    }, {
      "referenceID" : 24,
      "context" : "A large number of word alignment models are based on the IBM models (Och and Ney, 2003; Mermer and Saraçlar, 2011; Dyer et al., 2013; Östling and Tiedemann, 2016).",
      "startOffset" : 68,
      "endOffset" : 162
    }, {
      "referenceID" : 10,
      "context" : "A large number of word alignment models are based on the IBM models (Och and Ney, 2003; Mermer and Saraçlar, 2011; Dyer et al., 2013; Östling and Tiedemann, 2016).",
      "startOffset" : 68,
      "endOffset" : 162
    }, {
      "referenceID" : 27,
      "context" : "A large number of word alignment models are based on the IBM models (Och and Ney, 2003; Mermer and Saraçlar, 2011; Dyer et al., 2013; Östling and Tiedemann, 2016).",
      "startOffset" : 68,
      "endOffset" : 162
    }, {
      "referenceID" : 11,
      "context" : "Recent studies have shown that word alignments can be extracted from neural machine translation models (Ghader and Monz, 2017; Koehn and Knowles, 2017; Li et al., 2019) or from pretrained cross-lingual LMs (Jalili Sabet et al.",
      "startOffset" : 103,
      "endOffset" : 168
    }, {
      "referenceID" : 18,
      "context" : "Recent studies have shown that word alignments can be extracted from neural machine translation models (Ghader and Monz, 2017; Koehn and Knowles, 2017; Li et al., 2019) or from pretrained cross-lingual LMs (Jalili Sabet et al.",
      "startOffset" : 103,
      "endOffset" : 168
    }, {
      "referenceID" : 21,
      "context" : "Recent studies have shown that word alignments can be extracted from neural machine translation models (Ghader and Monz, 2017; Koehn and Knowles, 2017; Li et al., 2019) or from pretrained cross-lingual LMs (Jalili Sabet et al.",
      "startOffset" : 103,
      "endOffset" : 168
    }, {
      "referenceID" : 25,
      "context" : ", 2019) or from pretrained cross-lingual LMs (Jalili Sabet et al., 2020; Nagata et al., 2020).",
      "startOffset" : 45,
      "endOffset" : 93
    }, {
      "referenceID" : 36,
      "context" : "Then, we update the model parameters with the denoising word alignment task, where the model uses a pointer network (Vinyals et al., 2015) to predict the aligned tokens from the perturbed translation pair.",
      "startOffset" : 116,
      "endOffset" : 138
    }, {
      "referenceID" : 35,
      "context" : "Given the query and key vectors, the forward alignment probability ai is computed by the scaled dot-product attention (Vaswani et al., 2017):",
      "startOffset" : 118,
      "endOffset" : 140
    }, {
      "referenceID" : 37,
      "context" : ", 2021b), we use raw sentences from the Wikipedia dump and CCNet (Wenzek et al., 2019) for MLM, including 94 languages.",
      "startOffset" : 65,
      "endOffset" : 86
    }, {
      "referenceID" : 45,
      "context" : "For TLM and DWA, we use parallel corpora from MultiUN (Ziemski et al., 2016), IIT Bombay (Kunchukuttan et al.",
      "startOffset" : 54,
      "endOffset" : 76
    }, {
      "referenceID" : 19,
      "context" : ", 2016), IIT Bombay (Kunchukuttan et al., 2018), OPUS (Tiedemann, 2012), and WikiMatrix (Schwenk et al.",
      "startOffset" : 20,
      "endOffset" : 47
    }, {
      "referenceID" : 34,
      "context" : ", 2018), OPUS (Tiedemann, 2012), and WikiMatrix (Schwenk et al.",
      "startOffset" : 14,
      "endOffset" : 31
    }, {
      "referenceID" : 33,
      "context" : ", 2018), OPUS (Tiedemann, 2012), and WikiMatrix (Schwenk et al., 2019), including 14 English-centric language pairs.",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 6,
      "context" : "We pretrain a Transformer with 12 layers and the hidden size of 768, where the parameters are initialized with XLM-R (Conneau et al., 2020).",
      "startOffset" : 117,
      "endOffset" : 139
    }, {
      "referenceID" : 17,
      "context" : "The model is optimized with the Adam optimizer (Kingma and Ba, 2015) for 150K steps with batch size of 2, 048.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 43,
      "context" : "5 (Zeman et al., 2019), and named entity recognition on the WikiAnn (Pan et al.",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 29,
      "context" : ", 2019), and named entity recognition on the WikiAnn (Pan et al., 2017; Rahimi et al., 2019) dataset; (2) Question answering: crosslingual question answering on MLQA (Lewis et al.",
      "startOffset" : 53,
      "endOffset" : 92
    }, {
      "referenceID" : 32,
      "context" : ", 2019), and named entity recognition on the WikiAnn (Pan et al., 2017; Rahimi et al., 2019) dataset; (2) Question answering: crosslingual question answering on MLQA (Lewis et al.",
      "startOffset" : 53,
      "endOffset" : 92
    }, {
      "referenceID" : 20,
      "context" : ", 2019) dataset; (2) Question answering: crosslingual question answering on MLQA (Lewis et al., 2020) and XQuAD (Artetxe et al.",
      "startOffset" : 81,
      "endOffset" : 101
    }, {
      "referenceID" : 0,
      "context" : ", 2020) and XQuAD (Artetxe et al., 2020), and gold passage of typologically diverse question answering (TyDiQA-GoldP; Clark et al.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 5,
      "context" : ", 2020), and gold passage of typologically diverse question answering (TyDiQA-GoldP; Clark et al. 2020); (3) Sentence classification: cross-lingual natural language inference (XNLI; Conneau et al.",
      "startOffset" : 70,
      "endOffset" : 103
    }, {
      "referenceID" : 8,
      "context" : "2020); (3) Sentence classification: cross-lingual natural language inference (XNLI; Conneau et al. 2018), and crosslingual paraphrase adversaries from word scrambling (PAWS-X; Yang et al.",
      "startOffset" : 77,
      "endOffset" : 104
    }, {
      "referenceID" : 42,
      "context" : "2018), and crosslingual paraphrase adversaries from word scrambling (PAWS-X; Yang et al. 2019).",
      "startOffset" : 68,
      "endOffset" : 94
    }, {
      "referenceID" : 9,
      "context" : "(1) Multilingual BERT (MBERT; Devlin et al. 2019) is pretrained with masked language modeling (MLM) and next sentence prediction on Wikipedia of 104 languages; (2) XLM (Conneau and Lample, 2019) is jointly pretrained with MLM on 100 languages and translation language modeling (TLM) on 14 language pairs; (3) MT5 (Xue et al.",
      "startOffset" : 22,
      "endOffset" : 49
    }, {
      "referenceID" : 7,
      "context" : "2019) is pretrained with masked language modeling (MLM) and next sentence prediction on Wikipedia of 104 languages; (2) XLM (Conneau and Lample, 2019) is jointly pretrained with MLM on 100 languages and translation language modeling (TLM) on 14 language pairs; (3) MT5 (Xue et al.",
      "startOffset" : 124,
      "endOffset" : 150
    }, {
      "referenceID" : 40,
      "context" : "2019) is pretrained with masked language modeling (MLM) and next sentence prediction on Wikipedia of 104 languages; (2) XLM (Conneau and Lample, 2019) is jointly pretrained with MLM on 100 languages and translation language modeling (TLM) on 14 language pairs; (3) MT5 (Xue et al., 2020) is the multilingual version of T5 pretrained with text-to-text tasks; (4) XLM-R (Conneau et al.",
      "startOffset" : 269,
      "endOffset" : 287
    }, {
      "referenceID" : 6,
      "context" : ", 2020) is the multilingual version of T5 pretrained with text-to-text tasks; (4) XLM-R (Conneau et al., 2020) is pretrained with MLM on large-scale CC-100 dataset with long training steps.",
      "startOffset" : 88,
      "endOffset" : 110
    }, {
      "referenceID" : 10,
      "context" : "Thus, we compare our method with (1) fast align (Dyer et al., 2013), a widely-used implementation of IBM Model 2 (Och and Ney, 2003); (2) SimAlign (Jalili Sabet et al.",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 26,
      "context" : ", 2013), a widely-used implementation of IBM Model 2 (Och and Ney, 2003); (2) SimAlign (Jalili Sabet et al.",
      "startOffset" : 53,
      "endOffset" : 72
    }, {
      "referenceID" : 4,
      "context" : "that it is beneficial to learn sentence-level crosslingual alignment at a middle layer (Chi et al., 2021b).",
      "startOffset" : 87,
      "endOffset" : 106
    } ],
    "year" : 2021,
    "abstractText" : "The cross-lingual language models are typically pretrained with masked language modeling on multilingual text or parallel sentences. In this paper, we introduce denoising word alignment as a new cross-lingual pre-training task. Specifically, the model first self-labels word alignments for parallel sentences. Then we randomly mask tokens in a bitext pair. Given a masked token, the model uses a pointer network to predict the aligned token in the other language. We alternately perform the above two steps in an expectationmaximization manner. Experimental results show that our method improves cross-lingual transferability on various datasets, especially on the token-level tasks, such as question answering, and structured prediction. Moreover, the model can serve as a pretrained word aligner, which achieves reasonably low error rates on the alignment benchmarks. The code and pretrained parameters are available at github.com/CZWin32768/XLM-Align.",
    "creator" : "LaTeX with hyperref"
  }
}