{
  "name" : "2021.acl-long.403.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning Event Graph Knowledge for Abductive Reasoning",
    "authors" : [ "Li Du", "Xiao Ding", "Ting Liu", "Bing Qin" ],
    "emails" : [ "qinb}@ir.hit.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5181–5190\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5181"
    }, {
      "heading" : "1 Introduction",
      "text" : "Abductive reasoning aims at seeking for the best explanations for incomplete observations (Bhagavatula et al., 2019). For example, given observations Forgot to close window when leaving home and The room was in a mess, human beings can generate a reasonable hypothesis for explaining the observations, such as A thief entered the room based on commonsense knowledge in their mind. However, due to the lack of commonsense knowledge and effective reasoning mechanism, this is still a challenging problem for today’s cognitive intelligent systems (Charniak and Shimony, 1990; Oh et al., 2013; Kruengkrai et al., 2017).\nMost previous works focus on conducting abductive reasoning based on formal logic (Eshghi et al., 1988; Levesque, 1989; Ng et al., 1990; Paul, 1993). However, the rigidity of formal logic limits the application of abductive reasoning in NLP\n∗Corresponding author\ntasks, as it is hard to express the complex semantics of natural language in a formal logic system. To facilitate this, Bhagavatula et al. (2019) proposed a natural language based abductive reasoning task αNLI. As shown in Figure 1 (a), given two observed events O1 and O2, the αNLI task requires the prediction model to choose a more reasonable explanation from two candidate hypothesis events H1 and H2. Both observed events and hypothesis events are daily-life events, and are described in natural language. Together with the αNLI task, Bhagavatula et al. (2019) also explored conducting such reasoning using pretrained language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019).\nHowever, despite pretrained language models could capture rich linguistic knowledge benefit for understanding the semantics of events, additional commonsense knowledge is still necessary for the abductive reasoning. For example, as illustrated in Figure 1 (b), given observations O1 and O2, to choose the more likely explanation H1 : A thief\nentered the room and exclude H2 : A breeze blew in the window, prediction model should have the commonsense knowledge that it is hardly possible for a breeze to mess up the room, whereas a thief may enter the room from the open window (I1), then rummage through the room (I2) and lead to a mess. These intermediary events (I1 and I2) can serve as necessary commonsense knowledge for understanding the relationship between observed events and hypothesis events.\nWe notice that the observed events, hypothesis events, intermediary events and their relationships could be described using an event graph, which can be constructed based on an auxiliary dataset. The challenge is how to learn such commonsense knowledge from the constructed event graph.\nTo address this issue, we propose an Event Graph Enhanced RoBERTa (ege-RoBERTa) model, and a two-stage training procedure. Specifically, as shown in Figure 1 (c), on the basis of the RoBERTa framework, we additionally introduce a latent variable z to model the information about the intermediary events. In the pretraining stage, ege-RoBERTa is trained upon an event-graph-based pseudo instance set to capture the commonsense knowledge using the latent variable z. In the finetuning stage, model adapts the commonsense knowledge captured by z to conduct the abductive reasoning.\nExperimental results show that ege-RoBERTa could effectively learn the commonsense knowledge from a well-designed event graph, and improve the model performance on the αNLI task compared to the baseline methods. The code is released at https://github.com/sjcfr/ege-RoBERTa."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 Problem Formalization",
      "text" : "As shown in Figure 1 (a), αNLI can be defined as a multiple-choice task. Given two observed events O1 and O2 happened in a sequential order, one needs to choose a more reasonable hypothesis event from two candidates H1 and H2 for explaining the observations. Therefore, we formalize the abductive reasoning task as a conditional distribution p(Y |O1, Hi, O2), where Hi ∈ {H1, H2}, and Y ∈ [0, 1] is a relatedness score measuring the reasonableness of Hi.\nIn the αNLI dataset, Hi is set to be an explanation event happens intermediate to O1 and O2 (Bhagavatula et al., 2019). Hence, O1, O2 and Hi form an event temporal sequence O1, Hi, O2. For\nbrevity, we denote the event sequence as X = (O1, Hi, O2). Therefore, taking the event order into consideration, we further characterize the abductive reasoning task as p(Y |X)."
    }, {
      "heading" : "2.2 Event Graph",
      "text" : "Formally, an event graph could be denoted as G = {V,R}, where V is the node set, and R is the edge set. Each node Vi ∈ V corresponds to an event, while Rij ∈ R is a directed edge Vi → Vj along with a weight Wij , which denotes the probability that Vj is the subsequent event of Vi.\nGiven observed events and a certain hypothesis event, from the event graph we could acquire additional commonsense knowledge about: (1) the intermediary events, (2) the relationships between events. As Figure 1 (b) shows, the observed events, hypothesis event and intermediary events compose another event sequence (O1, I1, Hi, I2, O2). For clarity, we define such event sequence as posterior event sequence X ′, where X ′ = (O1, I1, Hi, I2, O2). The relationship between events within X ′ could be described by an adjacency matrix A ∈ R5×5, with each element initialized using the edge weights of the event graph:\nAjk = { Wjk, if Vj → Vk ∈ R, 0, others.\n(1)\nThe matrix A could describe the adjacency relationship between arbitrary two events in X ′."
    }, {
      "heading" : "3 Ege-RoBERTa as a Conditional Variational Autoencoder Based Reasoning Framework",
      "text" : "In this paper, rather than directly predicts the relatedness score Y based on the event sequence X , we propose to predict Y based on both X and additional commonsense knowledge (i.e. posterior event sequence X ′ and adjacency matrix A). To this end, we introduce a latent variable z to learn such knowledge from an event graph through a two stage training procedure. To effectively capture the event graph knowledge through z and conduct the abductive reasoning task based on z, we frame the ege-RoBERTa model as a conditional variational autoencoder (CVAE) (Sohn et al., 2015).\nSpecifically, with regard to the latent variable z, ege-RoBERTa characterizes the conditional distribution P (Y |X) using three neural networks: a prior network pθ(z|X), a recognition network qφ(z|X ′, A) and a neural likelihood pθ(Y |X, z),\nwhere θ and φ denote the parameters of networks. Moreover, instead of directly maximize P (Y |X), following CVAE (Sohn et al., 2015), ege-RoBERTa proposes to maximize the evidence lower bound (ELBO) of P (Y |X) :\nLELBO(θ, φ) =Eqφ(z|X′,A)log(pθ(Y |X, z))\n−KL(qφ(z|X ′, A)||pθ(z|X)) ≤ logp(Y |X)\n(2)\nNote that, in the recognition network, the latent variable z is directly conditioned on X ′ and A, where X ′ = {O1, I1, Hi, I2, O2} is the posterior event sequence, A is an adjacency matrix describing the relationship between events within X ′. This enables z to capture the event graph knowledge from X ′ and A. Through minimizing the KL term of ELBO, we can teach the prior network pθ(z|X) to learn the event graph knowledge from the recognition network as much as possible. Then in the neural likelihood pθ(Y |X, z) the relatedness score Y could be predicted based on X and z, which captures the event graph knowledge.\nHowever, the event graph knowledge is absent in the αNLI dataset. To learn such knowledge, we design the following two-stage training procedure:\nPre-training Stage: Learning Event Graph Knowledge from a Pseudo Instance Set In this stage, ege-RoBERTa is pretrained on a prebuilt event-graph-based pseudo instance set, which contains rich information about the intermediary events and the events relationships. As shown in Figure 2 (a), the latent variable z is directly conditioned on X ′ and A. Therefore, z could be employed to learn the event graph knowledge.\nFinetuning Stage: Adapt Event Graph Knowledge to the Abductive Reasoning Task As Figure 2 (b) shows, at the finetuning stage, ege-RoBERTa is trained on the αNLI dataset\nwithout the additional information X ′ and A. In this stage model learns to adapt the captured event graph knowledge to the abductive reasoning task. Then as Figure 2 (c) shows, after the two-stage training process, ege-RoBERTa could predict the relatedness score Y based on the latent variable z."
    }, {
      "heading" : "4 Architecture of ege-RoBERTa",
      "text" : "We introduce the specific implementation of ege-RoBERTa. As illustrated in Figure 3, egeRoBERTa introduces four modules in addition to the RoBERTa framework: (1) an aggregator providing representation for any event within X and X ′; (2) an attention-based prior network for modeling pθ(z|X); (3) a graph neural network based recognition network for modeling qφ(z|X ′, A); (4) a merger to merge the latent variable z into RoBERTa frame for downstream abductive reasoning task."
    }, {
      "heading" : "4.1 Event Representation Aggregator",
      "text" : "The event representation aggregator provides distributed representation for events in both the event sequence X and the posterior event sequence X ′. To this end, the aggregator employs attention mechanism to aggregate token representations of the event sequence from hidden states of RoBERTa.\nGiven an event sequence X composed of tokens [[CLS], (x11,. . . ,x1l1 ),. . . ,(x 3 1,. . . ,x3l3 )] (where [CLS] is the special classification token (Devlin et al., 2019), and xjk is the kth token within the jth event), the M th transformer layer of RoBERTa encodes these tokens into contextualized distributed representations H(M) = [h[CLS], (h11,. . . ,h1l1 ),. . . ,(h 3 1,. . . ,h3l3 )], where hjk ∈ R 1×d is the distributed representation of the kth token within the jth event. Then for the\njth event, the distributed representation is initialized as qj = 1lj ∑ hjlj . Multi-head attention mechanism (MultiAttn) (Vaswani et al., 2017) is employed to softly select information from H(M) and get the representation of each event:\nej = MultiAttn(qj , H (M)). (3)\nFor brevity, we denote the vector representation of all events in X using a matrix EX , where EX = {e1, e2, e3} ∈ R3×d. Note that, through the embedding layer of RoBERTa, position information has been injected into the token representations. Therefore, EX derived from token representations carries event order information. In addition, since EX is obtained from the hidden states of RoBERTa, rich linguistic knowledge within RoBERTa could be utilized to enhance the comprehension of event semantics. By the same way, the representation of events within X ′ could be calculated, which we denote as EX′ ."
    }, {
      "heading" : "4.2 Recognition Network",
      "text" : "The recognition network models qφ(z|X ′, A) based on EX′ and A, where EX′ is the representations of events within X ′. Following traditional VAE, qφ(z|X ′, A) is assumed to be a multivariate Gaussian distribution:\nqφ(z|X ′, A) ∼ N(µ′(X ′, A), D), (4) where D denotes the identity matrix.\nTo obtain µ(X ′, A), we first combine EX′ and adjacency matrix A using a GNN (Kipf et al., 2016):\nE(U) ′ = σ(AEX′W (u)). (5)\nwhere σ(·) is the sigmoid function;W (u) ∈ Rd×d is a weight matrix and E(U) ′ are relational information updated event representations. Then a multi-head self-attention operation is performed to promote the fusion of event semantic information and relational information:\nE(U) ′ = MultiAttn(E(U) ′ , E(U) ′ ). (6)\nFinally, to estimate µ(X ′, A), we aggregate information within E(U) ′ using a readout function g(·):\nµ′ = g(E(U) ′ ). (7)\nFollowing Zhou et al. (2019) and Zhong et al. (2019), we set g(·) to be a mean-pooling operation.\nHence, by estimating µ′ based on the relational information updated event representation E(U)′, event graph knowledge about X ′ and A is involved into the latent variable z."
    }, {
      "heading" : "4.3 Prior Network",
      "text" : "The prior network models pθ(z|X) based on EX , where EX is the representation matrix of events in X . The same as the recognition network, pθ(z|X) also follows multivariate normal distribution, while the parameters are different:\npθ(z|X) ∼ N(µ(X), D), (8) where D denotes the identity matrix.\nTo obtain µ(X), different from the recognition network, the prior network starts from updating EX using a multi-head self-attention:\nE(U) = MultiAttn(EX , EX). (9)\nThen an additional multi-head self-attention operation is performed to get deeper representations:\nE(U) = MultiAttn(E(U), E(U)). (10) Finally, µ(X) is estimated through aggregating\ninformation from E(U):\nµ = g(E(U)), (11) where g(·) is a mean-pooling operation."
    }, {
      "heading" : "4.4 Merger",
      "text" : "The merger module merges the latent variable z as well as updated (deep) representation of events into the N th transformer layer of RoBERTa frame for predicting the relatedness score. To this end, we employ multi-head attention mechanism to softly select relevant information from z and E(U), and then update the hidden state of the N th transformer layer of RoBERTa.\nSpecifically, in the pretraining stage: H(N) ∗ = MultiAttn(H(N), [µ′;E(U)]), (12)\nwhere H(N) is the hidden states of the N th transformer layer of RoBERTa, and H(N) ∗ is the event graph information updated hidden states. While in the finetuning and prediction stage:\nH(N) ∗ = MultiAttn(H(N), [µ;E(U)]). (13)\nNote that, given X , pθ(µ|X) achieves its maximum when z = µ. Hence, making predictions based on µ could be regarded as finding the best explanation based on the most likely commonsense situation. Through integrating latent variable z, H(N)∗ contains the event graph knowledge. By taking H(N) ∗ as the input of the subsequent (N + 1)th transformer layers of RoBERTa for predicting the relatedness score, the abductive reasoning task is conducted based on the additional event graph knowledge."
    }, {
      "heading" : "4.5 Optimizing",
      "text" : "The αNLI task requires model to choose a more likely hypothesis event from two candidates. However, in the pre-training stage, the negative examples are absent in the pseudo instances. To address this issue, following the method of Liu et al. (2019), in the pre-training stage ege-RoBERTa is trained to predict the masked tokens in the event sequence X rather than the relatedness score. In addition, in order to balance the masked token prediction loss with the KL term, we introduce an additional hyperparameter λ. Hence, the objective function in the pretraining stage is defined as follows:\nLELBO(θ, φ) =Eq(z|X′,A)logLMLM (X, z; θ) − λKL(qφ(z|X ′, A)||pθ(z|X)), (14)\nwhere logLMLM (X, z; θ) is the masked token prediction loss. Intuitively, through minimizing the KL term, we aim to transmit the event graph knowledge from the recognition network to the prior network.\nIn the finetuning stage, ege-RoBERTa is trained to adapt the learned event graph knowledge to the abductive reasoning task. Without the recogniton network, we formulate the objective function as:\nL(θ) = pθ(Y |z,X) = pθ(Y |z,X)pθ(z|X). (15)"
    }, {
      "heading" : "4.6 Training Details",
      "text" : "We implement two different sizes of ege-RoBERTa model (i.e. ege-RoBERTa-base and ege-RoBERTalarge) based on RoBERTa-base framework and RoBERTa-large framework, respectively. For the ege-RoBERTa-base model, in the aggregator, the prior network, the recognition network and the merger, the dimension of the attention mechanism d is set as 768, and all multi-head attention layers contain 12 heads. While for the ege-RoBERTa-large model, d is equal to 1024 and all multi-head attention layers contain 16 heads. In the ege-RoBERTabase model, token representations are aggregated from the 7th transformer layer of RoBERTa, and the latent variable is merged to the 10th transformer layer of RoBERTa. While for the ege-RoBERTalarge model, the aggregator and merger layer are set as the 14th and 20th layer, respectively. The balance coefficient λ equals 0.01. More details are provided in the Appendix."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 αNLI Dataset",
      "text" : "The αNLI dataset (Bhagavatula et al., 2019) consists of 169,654, 1,532 and 4,056 〈O1, O2, H1, H2〉\nquadruples in training, development and test set, respectively. The observation events are collected from a short story corpus ROCstory (Mostafazadeh et al., 2016), while all of hypothesis events are independently generated through crowdsourcing."
    }, {
      "heading" : "5.2 Construction of Event Graph",
      "text" : "The event graph serves as an external knowledge base to provide information about the relationship between observation events and intermediary events. To this end, we build the event graph based on an auxiliary dataset, which are composed of two short story corpora independent to αNLI, i.e., VIST (Huang et al., 2016), and TimeTravel (Qin et al., 2019). Both VIST and TimeTravel are composed of five-sentences short stories. Totally there are 121,326 stories in the auxiliary dataset.\nTo construct the event graph, we define each sentence in the auxiliary dataset as a node in the event graph. To get the edge weight Wij between two nodes Vi and Vj (i.e., the probability that Vj is the subsequent event of Vi), we finetune a RoBERTa-large model through a next sentence prediction task. Specifically, we define adjacent sentence pairs in the story text (for example, [1st, 2nd] sentence, [4th, 5th] sentence of a story) as positive instances, define nonadjacent sentence pairs or sentences pairs in reverse order (such as [1st, 3rd] sentence, [5th, 4th] sentence of a story) as negative instances. After that we sample 300,000 positive and 300,000 negative instances from the auxiliary dataset. Then given an event pair (Vi, Vj), the finetuned RoBERTa-large model would be able to predict the probability that Vj is the subsequent event of Vi.\nEvent Graph Based Pseudo Instance Set for Pretraining ege-RoBERTa To effectively utilize the event graph knowledge, we induce a set of pseudo instances for pretraining the ege-RoBERTa model. Specifically, given a five-sentence-story within the auxiliary dataset, as Table 1 shows, we define the 1st and 5th sentence of the story as two\nobserved events, the 3rd sentence as the hypothesis event, the 2nd and 4th sentence as intermediary events, respectively. In this way, the posterior event sequence X ′ and the event sequence X of a pseudo instance could be obtained. In addition, given X ′, we initialize the elements of the adjacency matrixA using the edge weights of the event graph, and scale A so that its row sums equal to 1. After the above operations, each pseudo instance is composed of an event sequence X , a posterior event sequence X ′ which contains intermediary event information, and an adjacency matrix A which describes relationships between events within X ′."
    }, {
      "heading" : "5.3 Baselines",
      "text" : "We compare ege-RoBERTa with: • SVM uses features about length, overlap and sentiment to predict the more likely hypothesis event. • Infersent (Conneau et al., 2017) represents sentences using a Bi-LSTM, and predicts the relatedness score using MLP. • GPT (Radford et al., 2018) is a multilayertransformer based unidirectional pretrained language model. • BERT (Devlin et al., 2019) is a multilayertransformer based bi-directional pretrained language model. • RoBERTa (Liu et al., 2019) refers robustly optimized BERT. • ege-RoBERTau(npretrained) refers to the egeRoBERTa model without the pretraining stage. • ege-RoBERTaλ=0 refers to setting the balance coefficient to 0 in the pretraining stage. Note that all pretrained-language-model-based baselines (i.e., GPT, BERT and RoBERTa) are finetuned on the αNLI dataset as the method of Bhagavatula et al. (2019) to adapt to the abductive reasoning task.\nIn addition, we also list two concurrent works: (i) L2R (Zhu et al., 2020) learns to rank the candidate hypotheses with a novel scoring function. (ii) RoBERTa-GPT-MHKA (Paul et al., 2020) enhances pretrained language model with social and causal commonsense knowledge for αNLI task."
    }, {
      "heading" : "5.4 Quantitative Analysis",
      "text" : "We list the prediction accuracy (%) in Table 2, and observe that:\n(1) Compared with SVM and Infersent, pretrained language model based methods: GPT, BERT, RoBERTa and ege-RoBERTa show significant better performances in abductive reasoning task. This is because through the pre-training\nstage language models could capture rich linguistic knowledge that is helpful for understanding the semantics of events.\n(2) Comparison between ege-RoBERTa-largeu with ege-RoBERTa-large shows that the pretraining process can increase the accuracy of abductive reasoning. In addition, comparison between ege-RoBERTa-largeλ=0 with ege-RoBERTalarge indicates that in the pre-training process, egeRoBERTa could capture the event graph knowledge through the latent variable to enhance the abductive reasoning. Furthermore, the relative close performance between ege-RoBERTa-largeu and egeRoBERTa-largeλ=0 suggest that the main improvements of the performance is brought by the event graph knowledge.\n(3) Compared to RoBERTa, ege-RoBERTa achieves higher prediction accuracy for both the base and large sized model. This result confirms our motivation that learning event graph knowledge could be helpful for the abductive reasoning task.\n(4) According to Bhagavatula et al. (2019), human performance on the test set of αNLI is 91.4%. While the RoBERTa-large model has achieved an accuracy of 83.9%. Therefore, further improvements over RoBERTa-large could be challenging. Through learning the event graph knowledge, our proposed method ege-RoBERTa further improves the relative accuracy.\n(5) Our approach has comparable performance with the SOTA concurrent work, which combines RoBERTa with GPT, and incorporates social and causal commonsense into model. The combination of both methods would further increase the model performance."
    }, {
      "heading" : "5.5 Ablation Study",
      "text" : "All studies are conducted on the development set of the αNLI using the ege-RoBERTa-base model.\nInfluence of the Balance Coefficient In the pretraining stage, the balance coefficient λ controls the trade off between event graph knowledge learning and abductive reasoning. To investigate the specific influence of the balance coefficient, we compare the performance of ege-RoBERTa model pretrained with different λ. As shown in Figure 4, the prediction accuracy continues to increase as λ increases from 0 to 0.01. This is because adequate event graph knowledge can offer guidance for the abductive reasoning task. While when λ exceeds 0.05, the accuracy start to decrease, as the over-emphasis of event graph knowledge learning would in turn undermine the model performance.\nInfluence of the External Commonsense Knowledge We study the specific effect of the event relational information and the intermediary event information by controlling the generation of pseudo instances. In specific, we eliminate the influence of the adjacency matrix A by replacing A with a randomly initialized matrix Ã. Similarly, the influence of the intermediary events I1 and I2 is eliminated through substituting them by two randomly sampled events Ĩ1 and Ĩ2. As Table 3 shows, both the replacement of A and {I1, I2} lead to obvious decrease of model performance. This demonstrates that ege-RoBERTa can use both two kinds of event graph knowledge for enhancing the abductive reasoning task."
    }, {
      "heading" : "5.6 Sensitivity Analysis",
      "text" : "To find out if the improvement of Ege-RoBERTa is brought by a certain dataset, and the specific\nrelationship between the model performance with the number of pseudo instances, we conduct following experiments: (1) excluding a certain dataset when inducing pseudo instances; (2) pretraining the ege-RoBERTa-base model with different number of pseudo instances. The corresponding results on the dev set of αNLI is shown in Table 4.\nWe can find that, the elimination of both dataset leads to decrease of model performances. This suggests that the ege-RoBERTa model could capture relevant event graph knowledge from both dataset. While the prediction accuracy continues to increase along with the number of pseudo instances used for pretraining the ege-RoBERTa model. This is because the accumulation of commonsense knowledge is helpful for the abductive reasoning task. In addition, it also indicates that the model performance could be further improved if the auxiliary dataset is even more enlarged."
    }, {
      "heading" : "5.7 Case study",
      "text" : "Table 5 provides an example of model prediction results. Given two observed events O1 “hates Fall” and O2 “didn’t have to experience Fall in Guam”, the hypothesis event H1 “moved to Guam” is more likely to explain the two motivations of observed events. However, H1 implicitly relies on a precondition that in Guam, Fall could be eluded. Correspondingly, in the auxiliary dataset, there is information supporting the hypothesis event H1 that there is no Fall in Guam. In this case, ege-RoBERTa chooses the hypothesis event H1, whereas RoBERTa chooses the wrong hypothesis event H2. This indicates that ege-RoBERTa could learn the event graph knowledge in the pretraining process for improving the reasoning performance."
    }, {
      "heading" : "6 Discussion",
      "text" : "In this paper, to involve the event graph knowledge, we formalize the posterior event sequence as X ′ = {O1, I1, Hi, I2, O2}. While our approach also allows other forms of posterior event sequences, such as X ′ = {O1, Hi, I1, O2}, X ′ = {O1, I1, Hi, O2}, or X ′ = {O1, I1, I2, Hi, O2}, etc. We also pretrained ege-RoBERTa on pseudoinstance sets derived by these manners. The results are shown in Table 6. We find that whatever forms of posterior event sequences involved in ege-RoBERTa, our approach can achieve consistently better performance than the baseline method. This confirms that our approach is sufficiently generalizable to deal with various forms of external event-sequence knowledge. Furthermore, egeRoBERTa can also be equipped with more types of event graph knowledge, such as background knowledge by: formalizing the posterior event sequence as X ′ = {B1, . . . , Bm, E1, . . . , En}, where {B1, . . . , Bm} is a set of background events for a given prior event sequence {E1, . . . , En}. This demonstrates the potential of ege-RoBERTa in learning different kinds of event graph knowledge for different event inference tasks."
    }, {
      "heading" : "7 Related Work",
      "text" : ""
    }, {
      "heading" : "7.1 Abductive Reasoning",
      "text" : "Most previous studies focus on formal logic based abductive reasoning (Eshghi et al., 1988; Levesque, 1989; Konolige, 1990; Paul, 1993). To infer the most reasonable hypothesis, the abductive reasoning process could be divided into two steps: (1) proposing reasonable hypotheses; (2) finding the best explanation from the hypotheses (Levesque, 1989; Konolige, 1990; Paul, 1993).\nHowever, the rigidity of formal logic limits its application in NLP domain. To facilitate this, Bhagavatula et al. (2019) proposed a text based abductive reasoning task αNLI. To solve the this task, Zhu et al. (2020) formalize αNLI as a rank learning task, and propose a novel ranking function. While Paul et al. (2020) enhances the reasoning model with social commonsense and causal commonsense\nknowledge. Compared to their works, for enhancing the abductive reasoning process, we propose to incorporate event graph knowledge by a CVAE based model ege-RoBERTa. In addition, we argue that our approach can be easily extended to other event inference tasks."
    }, {
      "heading" : "7.2 Event Graph Based Natural Language Inference",
      "text" : "Understanding events and their relationships are crucial for various natural language inference (NLI) tasks (Kruengkrai et al., 2017). Hence, a number of previous studies explore conducting NLI tasks based on event graphs.\nFor example, to predict the subsequent event for a given event context, Li et al. (2018) build an event evolutionary graph (EEG), and make prediction using a scaled graph neural network. While Wu et al. (2019) predict the propagation of news event through combining an historical event propagation graph with temporal point process. In addition to the event prediction related tasks, Liu et al. (2017) propose to enhance the news recommendation by incorporating additional event graph information. Liu et al. (2016) detect the textual contradiction by using event graphs as additional evidence.\nIn this paper, we employ event graph knowledge for guiding the abductive reasoning. To this end, we propose a variational autoencoder based framework ege-RoBERTa, which employs a latent variable z to implicitly capture the necessary event graph knowledge and enhance the pretrained language model RoBERTa."
    }, {
      "heading" : "8 Conclusion",
      "text" : "In this paper, we propose a variational autoencoder based framework ege-RoBERTa with a twostage training procedure for the abductive reasoning task. In the pretraining stage, ege-RoBERTa is able to learn commonsense knowledge from an event graph through the latent variable, then in the following stage the learned event graph knowledge can be adapted to the abductive reasoning task. Experimental results show improvement over the baselines on the αNLI task."
    }, {
      "heading" : "9 Acknowledgments",
      "text" : "We thank the anonymous reviewers for their constructive comments, and gratefully acknowledge the support of the National Key Research and Development Program of China (2020AAA0106501), and the National Natural Science Foundation of China (61976073)."
    } ],
    "references" : [ {
      "title" : "Abductive commonsense reasoning",
      "author" : [ "Chandra Bhagavatula", "Ronan Le Bras", "Chaitanya Malaviya", "Keisuke Sakaguchi", "Ari Holtzman", "Hannah Rashkin", "Doug Downey", "Scott Wen-tau Yih", "Yejin Choi." ],
      "venue" : "arXiv preprint arXiv:1908.05739.",
      "citeRegEx" : "Bhagavatula et al\\.,? 2019",
      "shortCiteRegEx" : "Bhagavatula et al\\.",
      "year" : 2019
    }, {
      "title" : "Probabilistic semantics for cost based abduction",
      "author" : [ "Eugene Charniak", "Solomon Eyal Shimony." ],
      "venue" : "Brown University, Department of Computer Science.",
      "citeRegEx" : "Charniak and Shimony.,? 1990",
      "shortCiteRegEx" : "Charniak and Shimony.",
      "year" : 1990
    }, {
      "title" : "Supervised learning of universal sentence representations from natural language inference data",
      "author" : [ "Alexis Conneau", "Douwe Kiela", "Holger Schwenk", "Loı̈c Barrault", "Antoine Bordes" ],
      "venue" : "In Proceedings of the 2017 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Conneau et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2017
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 NACCL, pages 4171– 4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Abduction through deduction",
      "author" : [ "K Eshghi", "RA Kowalski" ],
      "venue" : "Logic Programming Section Technical Report,",
      "citeRegEx" : "Eshghi and Kowalski.,? \\Q1988\\E",
      "shortCiteRegEx" : "Eshghi and Kowalski.",
      "year" : 1988
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas N. Kipf", "Thomas N. Welling", "Max", "Max Welling" ],
      "venue" : null,
      "citeRegEx" : "Kipf et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kipf et al\\.",
      "year" : 2016
    }, {
      "title" : "Closure+ minimization implies abduction",
      "author" : [ "Kurt Konolige." ],
      "venue" : "Proceedings of PRICAI90.",
      "citeRegEx" : "Konolige.,? 1990",
      "shortCiteRegEx" : "Konolige.",
      "year" : 1990
    }, {
      "title" : "Improving event causality recognition with multiple background knowledge sources using multi-column convolutional neural",
      "author" : [ "Canasai Kruengkrai", "Kentaro Torisawa", "Chikara Hashimoto", "Julien Kloetzer", "Jong-Hoon Oh", "Masahiro Tanaka" ],
      "venue" : null,
      "citeRegEx" : "Kruengkrai et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kruengkrai et al\\.",
      "year" : 2017
    }, {
      "title" : "A knowledge-level account of abduction",
      "author" : [ "Hector J Levesque." ],
      "venue" : "Proceedings of the 11th international joint conference on Artificial intelligence-Volume 2, pages 1061–1067.",
      "citeRegEx" : "Levesque.,? 1989",
      "shortCiteRegEx" : "Levesque.",
      "year" : 1989
    }, {
      "title" : "Constructing narrative event evolutionary graph for script event prediction",
      "author" : [ "Zhongyang Li", "Xiao Ding", "Ting", "Liu" ],
      "venue" : "In Proceedings of the 27th International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Li et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Event graph based contradiction recognition from big data collection",
      "author" : [ "Maofu Liu", "Limin Wang", "Liqiang Nie", "Jianhua Dai", "Donghong Ji." ],
      "venue" : "Neurocomputing, 181:64–75.",
      "citeRegEx" : "Liu et al\\.,? 2016",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Event recommendation based on graph random walking and history preference reranking",
      "author" : [ "Shenghao Liu", "Bang Wang", "Minghua Xu." ],
      "venue" : "Proceedings of the 40th international ACM SIGIR conference on research and development in information retrieval,",
      "citeRegEx" : "Liu et al\\.,? 2017",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2017
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "A corpus and cloze evaluation for deeper understanding of commonsense stories",
      "author" : [ "Nasrin Mostafazadeh", "Nathanael Chambers", "Xiaodong He", "Devi Parikh", "Dhruv Batra", "Lucy Vanderwende", "Pushmeet Kohli", "James Allen." ],
      "venue" : "NAACL, pages 839–849.",
      "citeRegEx" : "Mostafazadeh et al\\.,? 2016",
      "shortCiteRegEx" : "Mostafazadeh et al\\.",
      "year" : 2016
    }, {
      "title" : "The role of coherence in constructing and evaluating abductive explanations",
      "author" : [ "Hwee Tou Ng", "Raymond J Mooney" ],
      "venue" : "In Working Notes, AAAI Spring Symposium on Automated Abduction,",
      "citeRegEx" : "Ng and Mooney.,? \\Q1990\\E",
      "shortCiteRegEx" : "Ng and Mooney.",
      "year" : 1990
    }, {
      "title" : "Why-question answering using intra-and intersentential causal relations",
      "author" : [ "Jong-Hoon Oh", "Kentaro Torisawa", "Chikara Hashimoto", "Motoki Sano", "Stijn De Saeger", "Kiyonori Ohtake." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational",
      "citeRegEx" : "Oh et al\\.,? 2013",
      "shortCiteRegEx" : "Oh et al\\.",
      "year" : 2013
    }, {
      "title" : "Social commonsense reasoning with multi-head knowledge attention",
      "author" : [ "Debjit Paul", "Anette Frank" ],
      "venue" : "arXiv preprint arXiv:2010.05587",
      "citeRegEx" : "Paul et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Paul et al\\.",
      "year" : 2010
    }, {
      "title" : "Approaches to abductive reasoning: an overview",
      "author" : [ "Gabriele Paul." ],
      "venue" : "Artificial intelligence review, 7(2):109–152.",
      "citeRegEx" : "Paul.,? 1993",
      "shortCiteRegEx" : "Paul.",
      "year" : 1993
    }, {
      "title" : "Counterfactual story reasoning and generation",
      "author" : [ "Lianhui Qin", "Antoine Bosselut", "Ari Holtzman", "Chandra Bhagavatula", "Elizabeth Clark", "Yejin Choi." ],
      "venue" : "arXiv preprint arXiv:1909.04076.",
      "citeRegEx" : "Qin et al\\.,? 2019",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever." ],
      "venue" : "URL",
      "citeRegEx" : "Radford et al\\.,? 2018",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning structured output representation using deep conditional generative models",
      "author" : [ "Kihyuk Sohn", "Honglak Lee", "Xinchen Yan" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Sohn et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sohn et al\\.",
      "year" : 2015
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Modeling event propagation via graph biased temporal point process",
      "author" : [ "Weichang Wu", "Huanxi Liu", "Xiaohu Zhang", "Yu Liu", "Hongyuan Zha." ],
      "venue" : "arXiv preprint arXiv:1908.01623.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Reasoning over semantic-level graph for fact checking",
      "author" : [ "Wanjun Zhong", "Jingjing Xu", "Duyu Tang", "Zenan Xu", "Nan Duan", "Ming Zhou", "Jiahai Wang", "Jian Yin." ],
      "venue" : "arXiv preprint arXiv:1909.03745.",
      "citeRegEx" : "Zhong et al\\.,? 2019",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2019
    }, {
      "title" : "Gear: Graph-based evidence aggregating and reasoning for fact verification",
      "author" : [ "Jie Zhou", "Xu Han", "Cheng Yang", "Zhiyuan Liu", "Lifeng Wang", "Changcheng Li", "Maosong Sun." ],
      "venue" : "arXiv preprint arXiv:1908.01843.",
      "citeRegEx" : "Zhou et al\\.,? 2019",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2019
    }, {
      "title" : "L2r2: Leveraging ranking for abductive reasoning",
      "author" : [ "Yunchang Zhu", "Liang Pang", "Yanyan Lan", "Xueqi Cheng." ],
      "venue" : "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1961–1964.",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Abductive reasoning aims at seeking for the best explanations for incomplete observations (Bhagavatula et al., 2019).",
      "startOffset" : 90,
      "endOffset" : 116
    }, {
      "referenceID" : 1,
      "context" : "However, due to the lack of commonsense knowledge and effective reasoning mechanism, this is still a challenging problem for today’s cognitive intelligent systems (Charniak and Shimony, 1990; Oh et al., 2013; Kruengkrai et al., 2017).",
      "startOffset" : 163,
      "endOffset" : 233
    }, {
      "referenceID" : 15,
      "context" : "However, due to the lack of commonsense knowledge and effective reasoning mechanism, this is still a challenging problem for today’s cognitive intelligent systems (Charniak and Shimony, 1990; Oh et al., 2013; Kruengkrai et al., 2017).",
      "startOffset" : 163,
      "endOffset" : 233
    }, {
      "referenceID" : 7,
      "context" : "However, due to the lack of commonsense knowledge and effective reasoning mechanism, this is still a challenging problem for today’s cognitive intelligent systems (Charniak and Shimony, 1990; Oh et al., 2013; Kruengkrai et al., 2017).",
      "startOffset" : 163,
      "endOffset" : 233
    }, {
      "referenceID" : 8,
      "context" : "Most previous works focus on conducting abductive reasoning based on formal logic (Eshghi et al., 1988; Levesque, 1989; Ng et al., 1990; Paul, 1993).",
      "startOffset" : 82,
      "endOffset" : 148
    }, {
      "referenceID" : 17,
      "context" : "Most previous works focus on conducting abductive reasoning based on formal logic (Eshghi et al., 1988; Levesque, 1989; Ng et al., 1990; Paul, 1993).",
      "startOffset" : 82,
      "endOffset" : 148
    }, {
      "referenceID" : 3,
      "context" : "(2019) also explored conducting such reasoning using pretrained language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al.",
      "startOffset" : 93,
      "endOffset" : 114
    }, {
      "referenceID" : 0,
      "context" : "In the αNLI dataset, Hi is set to be an explanation event happens intermediate to O1 and O2 (Bhagavatula et al., 2019).",
      "startOffset" : 92,
      "endOffset" : 118
    }, {
      "referenceID" : 20,
      "context" : "To effectively capture the event graph knowledge through z and conduct the abductive reasoning task based on z, we frame the ege-RoBERTa model as a conditional variational autoencoder (CVAE) (Sohn et al., 2015).",
      "startOffset" : 191,
      "endOffset" : 210
    }, {
      "referenceID" : 20,
      "context" : "Moreover, instead of directly maximize P (Y |X), following CVAE (Sohn et al., 2015), ege-RoBERTa",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 3,
      "context" : ",x3l3 )] (where [CLS] is the special classification token (Devlin et al., 2019), and xjk is the kth token within the jth event), the M th transformer layer of RoBERTa encodes these tokens into contextualized distributed representations H = [h[CLS], (h(1)1,.",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 21,
      "context" : "Multi-head attention mechanism (MultiAttn) (Vaswani et al., 2017) is employed to softly select information from H(M) and get the representation of each event:",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 5,
      "context" : "To obtain μ(X ′, A), we first combine EX′ and adjacency matrix A using a GNN (Kipf et al., 2016): E ′ = σ(AEX′W ).",
      "startOffset" : 77,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : "The αNLI dataset (Bhagavatula et al., 2019) consists of 169,654, 1,532 and 4,056 〈O1, O2, H1, H2〉 (Posterior) Event Sequence Story Observed Event 1 (O1) 1 © I was doing exercise in gym.",
      "startOffset" : 17,
      "endOffset" : 43
    }, {
      "referenceID" : 13,
      "context" : "The observation events are collected from a short story corpus ROCstory (Mostafazadeh et al., 2016), while all of hypothesis events are independently generated through crowdsourcing.",
      "startOffset" : 72,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "• Infersent (Conneau et al., 2017) represents sentences using a Bi-LSTM, and predicts the relatedness score using MLP.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 19,
      "context" : "• GPT (Radford et al., 2018) is a multilayertransformer based unidirectional pretrained lan-",
      "startOffset" : 6,
      "endOffset" : 28
    }, {
      "referenceID" : 3,
      "context" : "• BERT (Devlin et al., 2019) is a multilayertransformer based bi-directional pretrained language model.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 25,
      "context" : "In addition, we also list two concurrent works: (i) L2R (Zhu et al., 2020) learns to rank the candidate hypotheses with a novel scoring function.",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 8,
      "context" : "Most previous studies focus on formal logic based abductive reasoning (Eshghi et al., 1988; Levesque, 1989; Konolige, 1990; Paul, 1993).",
      "startOffset" : 70,
      "endOffset" : 135
    }, {
      "referenceID" : 6,
      "context" : "Most previous studies focus on formal logic based abductive reasoning (Eshghi et al., 1988; Levesque, 1989; Konolige, 1990; Paul, 1993).",
      "startOffset" : 70,
      "endOffset" : 135
    }, {
      "referenceID" : 17,
      "context" : "Most previous studies focus on formal logic based abductive reasoning (Eshghi et al., 1988; Levesque, 1989; Konolige, 1990; Paul, 1993).",
      "startOffset" : 70,
      "endOffset" : 135
    }, {
      "referenceID" : 8,
      "context" : "ing process could be divided into two steps: (1) proposing reasonable hypotheses; (2) finding the best explanation from the hypotheses (Levesque, 1989; Konolige, 1990; Paul, 1993).",
      "startOffset" : 135,
      "endOffset" : 179
    }, {
      "referenceID" : 6,
      "context" : "ing process could be divided into two steps: (1) proposing reasonable hypotheses; (2) finding the best explanation from the hypotheses (Levesque, 1989; Konolige, 1990; Paul, 1993).",
      "startOffset" : 135,
      "endOffset" : 179
    }, {
      "referenceID" : 17,
      "context" : "ing process could be divided into two steps: (1) proposing reasonable hypotheses; (2) finding the best explanation from the hypotheses (Levesque, 1989; Konolige, 1990; Paul, 1993).",
      "startOffset" : 135,
      "endOffset" : 179
    } ],
    "year" : 2021,
    "abstractText" : "Abductive reasoning aims at inferring the most plausible explanation for observed events, which would play critical roles in various NLP applications, such as reading comprehension and question answering. To facilitate this task, a narrative text based abductive reasoning task αNLI is proposed, together with explorations about building reasoning framework using pretrained language models. However, abundant event commonsense knowledge is not well exploited for this task. To fill this gap, we propose a variational autoencoder based model ege-RoBERTa, which employs a latent variable to capture the necessary commonsense knowledge from event graph for guiding the abductive reasoning task. Experimental results show that through learning the external event graph knowledge, our approach outperforms the baseline methods on the αNLI task.",
    "creator" : "LaTeX with hyperref package"
  }
}