{
  "name" : "2021.acl-long.33.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Self-Supervised Multimodal Opinion Summarization",
    "authors" : [ "Jinbae Im", "Moonki Kim", "Hoyeop Lee", "Hyunsouk Cho", "Sehee Chung" ],
    "emails" : [ "jinbae@ncsoft.com", "kmkkrk@ncsoft.com", "hoyeoplee@ncsoft.com", "dakgalbi@ncsoft.com", "seheechung@ncsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 388–403\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n388"
    }, {
      "heading" : "1 Introduction",
      "text" : "Opinion summarization is the task of automatically generating summaries from multiple documents containing users’ thoughts on businesses or products. This summarization of users’ opinions can provide information that helps other users with their decision-making on consumption. Unlike conventional single-document or multiple-document summarization, where we can obtain the prevalent annotated summaries (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2018; Liu et al., 2018; Liu and Lapata, 2019; Perez-Beltrachini et al., 2019), opinion summarization is challenging; it is difficult to find summarized opinions of users. Accordingly,\nstudies used an unsupervised approach for opinion summarization (Ku et al., 2006; Paul et al., 2010; Carenini et al., 2013; Ganesan et al., 2010; Gerani et al., 2014). Recent studies (Bražinskas and Titov, 2020; Amplayo and Lapata, 2020; Elsahar et al., 2021) used a self-supervised learning framework that creates a synthetic pair of source reviews and a pseudo summary by sampling a review text from a training corpus and considering it as a pseudo summary, as in Figure 1a.\nUsers’ opinions are based on their perception of a specific entity and perceptions originate from various characteristics of the entity; therefore, opinion summarization can use such characteristics. For instance, Yelp provides users food or menu images and various metadata about restaurants, as in Figure 1b. This non-text information influences the review text generation process of users (Truong and Lauw, 2019). Therefore, using this additional information can help in opinion summarization, especially under unsupervised settings (Su et al., 2019; Huang et al., 2020). Furthermore, the training process of generating a review text (a pseudo summary) based on the images and metadata for self-supervised learning is consistent with the ac-\ntual process of writing a review text by a user. This study proposes a self-supervised multimodal opinion summarization framework called MultimodalSum by extending the existing selfsupervised opinion summarization framework, as shown in Figure 1. Our framework receives source reviews, images, and a table on the specific business or product as input and generates a pseudo summary as output. Note that images and the table are not aligned with an individual review in the framework, but they correspond to the specific entity. We adopt the encoder–decoder framework and build multiple encoders representing each input modality. However, a fundamental challenge lies in the heterogeneous data of various modalities (Baltrušaitis et al., 2018).\nTo address this challenge, we propose a multimodal training pipeline. The pipeline regards the text modality as a pivot modality. Therefore, we pretrain the text modality encoder and decoder for a specific business or product via the self-supervised opinion summarization framework. Subsequently, we pretrain modality encoders for images and a table to generate review texts belonging to the same business or product using the pretrained text decoder. When pretraining the non-text modality encoders, the pretrained text decoder is frozen so that the image and table modality encoders obtain homogeneous representations with the pretrained text encoder. Finally, after pretraining input modalities, we train the entire model in an end-to-end manner to combine multimodal information.\nOur contributions can be summarized as follows: • this study is the first work on self-supervised\nmultimodal opinion summarization; • we propose a multimodal training pipeline\nto resolve the heterogeneity between input modalities; • we verify the effectiveness of our model framework and model training pipeline through various experiments on Yelp and Amazon datasets."
    }, {
      "heading" : "2 Related Work",
      "text" : "Generally, opinion summarization has been conducted in an unsupervised manner, which can be divided into extractive and abstractive approaches. The extractive approach selects the most meaningful texts from input opinion documents, and the abstractive approach generates summarized texts that are not shown in the input documents. Most previ-\nous works on unsupervised opinion summarization have focused on extractive approaches. Clusteringbased approaches (Carenini et al., 2006; Ku et al., 2006; Paul et al., 2010; Angelidis and Lapata, 2018) were used to cluster opinions regarding the same aspect and extract the text representing each cluster. Graph-based approaches (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Zheng and Lapata, 2019) were used to construct a graph—where nodes were sentences, and edges were similarities between sentences—and extract the sentences based on their centrality.\nAlthough some abstractive approaches were not based on neural networks (Ganesan et al., 2010; Gerani et al., 2014; Di Fabbrizio et al., 2014), neural network-based approaches have been gaining attention recently. Chu and Liu (2019) generated an abstractive summary from a denoising autoencoder-based model. More recent abstractive approaches have focused on self-supervised learning. Bražinskas and Titov (2020) randomly selected N review texts for each entity and constructed N synthetic pairs by sequentially regarding one review text as a pseudo summary and the others as source reviews. Amplayo and Lapata (2020) sampled a review text as a pseudo summary and generated various noisy versions of it as source reviews. Elsahar et al. (2021) selected review texts similar to the sampled pseudo summary as source reviews, based on TF-IDF cosine similarity. We construct synthetic pairs based on Bražinskas and Titov (2020) and extend the self-supervised opinion summarization to a multimodal version.\nMultimodal text summarization has been mainly studied in a supervised manner. Text summaries were created by using other modality data as additional input (Li et al., 2018, 2020a), and some studies provided not only a text summary but also other modality information as output (Zhu et al., 2018; Chen and Zhuge, 2018; Zhu et al., 2020; Li et al., 2020b; Fu et al., 2020). Furthermore, most studies summarized a single sentence or document. Although Li et al. (2020a) summarized multiple documents, they used non-subjective documents. Our study is the first unsupervised multimodal text summarization work that summarizes multiple subjective documents."
    }, {
      "heading" : "3 Problem Formulation",
      "text" : "The goal of the self-supervised multimodal opinion summarization is to generate a pseudo sum-\nmary from multimodal data. Following existing self-supervised opinion summarization studies, we consider a review text selected from an entire review corpus as a pseudo summary. We extend the formulation of Bražinskas and Titov (2020) to a multimodal version. Let R = {r1, r2, ..., rN} denote the set of reviews about an entity (e.g., a business or product). Each review, rj , consists of review text, dj , and review rating, sj , that represents the overall sentiment of the review text. We denote images uploaded by a user or provided by a company for the entity as I = {i1, i2, ..., iM} and a table containing abundant metadata about the entity as T . Here, T consists of several fields, and each field contains its own name and value. We set j-th review text dj as the pseudo summary and let it be generated from R−j , I , and T , where R−j = {r1, ..., rj−1, rj+1, ..., rN} denotes source reviews. To help the model summarize what stands out overall in the review corpus, we calculate the loss for all N cases of selecting dj from R, and train the model using the average loss. During testing, we generate a summary from R, I , and T ."
    }, {
      "heading" : "4 Model Framework",
      "text" : "The proposed model framework, MultimodalSum, is designed with an encoder–decoder structure, as in Figure 1b. To address the heterogeneity of three input modalities, we configure each modality encoder to effectively process data in each modality. We set a text decoder to generate summary text by synthesizing encoded representations from the three modality encoders. Details are described in the following subsections."
    }, {
      "heading" : "4.1 Text Encoder and Decoder",
      "text" : "Our text encoder and decoder are based on BART (Lewis et al., 2020). BART is a Transformer (Vaswani et al., 2017) encoder–decoder pretrained model that is particularly effective when fine-tuned for text generation and has high summarization performance. Furthermore, because the pseudo summary of self-supervised multimodal opinion summarization is an individual review text (dj), we determine that pretraining BART based on a denoising autoencoder is suitable for our framework. Therefore, we further pretrain BART using the entire training review corpus (Gururangan et al., 2020). Our text encoder obtains eD-dimensional encoded text representations htext from D−j and\nthe text decoder generates dj from htext as follows:\nhtext = BARTenc(D−j),\ndj = BARTdec(htext),\nwhere D−j = {d1, ..., dj−1, dj+1, ..., dN} denotes the set of review texts from R−j . Each review text consists of lD tokens and htext ∈ R(N−1)×lD×eD ."
    }, {
      "heading" : "4.2 Image Encoder",
      "text" : "We use a convolutional neural network specialized in analyzing visual imagery. In particular, we use ImageNet pretrained ResNet101 (He et al., 2016), which is widely used as a backbone network. We add an additional linear layer in place of the image classification layer to match feature distribution and dimensionality with text modality representations. Our image encoder obtains encoded image representations himg from I as follows:\nhimg = ResNet101(I)Wimg,\nwhere Wimg ∈ ReI×eD denotes the additional linear weights. himg obtains RM×lI×eD , where lI represents the size of the flattened image feature map obtained from ResNet101."
    }, {
      "heading" : "4.3 Table Encoder",
      "text" : "To effectively encode metadata, we design our table encoder based on the framework of data-to-text research (Puduppully et al., 2019). The input to our table encoder T is a series of field-name and field-value pairs. Each field gets eT -dimensional representations through a multilayer perceptron after concatenating the representations of field-name and field-value. The encoded table representations htable is obtained by stacking each field representation into F and adding a linear layer as follows:\nfk = ReLU([nk; vk]Wf + bf ),\nhtable = F Wtable,\nwhere n and v denote eT -dimensional representations of field name and value, respectively, and Wf ∈ R2eT×eT , bf ∈ ReT are parameters. By stacking lT field representations, we obtain F ∈ R1×lT×eT . The additional linear weights Wtable ∈ ReT×eD play the same role as in the image encoder, and htable ∈ R1×lT×eD ."
    }, {
      "heading" : "5 Model Training Pipeline",
      "text" : "To effectively train the model framework, we set a model training pipeline, which consists of three\nsteps, as in Figure 2. The first step is text modality pretraining, in which a model learns unsupervised summarization capabilities using only text modality data. Next, during the pretraining for other modalities, an encoder for each modality is trained using the text modality decoder learned in the previous step as a pivot. The main purpose of this step is that other modalities have representations whose distribution is similar to that of the text modality. In the last step, the entire model framework is trained using all the modality data. Details of each step can be found in the next subsections."
    }, {
      "heading" : "5.1 Text Modality Pretraining",
      "text" : "In this step, we pretrain the text encoder and decoder for self-supervised opinion summarization. As this was an important step for unsupervised multimodal neural machine translation (Su et al., 2019), we apply it to our framework. For the set of reviews about an entity R, we train the model to generate a pseudo summary dj from source reviews R−j for all N cases as follows: loss = ∑N j=1 log p(dj |R−j). The text encoder obtains htext ∈ R(N−1)×lD×eD from D−j , and the text decoder aggregates the encoded representations of N − 1 review texts to generate dj . We model the aggregation of multiple encoded representations in the multi-head self-attention layer of the text decoder. To generate a pseudo summary that covers the overall contents of source reviews, we simply average the N − 1 single-head attention results for each encoded representation (RlD×eD ) at each head (Elsahar et al., 2021).\nThe limitation of the self-supervised opinion summarization is that training and inference tasks are different. The model learns a review generation task using a review text as a pseudo summary; however, the model needs to perform a summary generation task at inference. To close this gap, we\nuse a rating deviation between the source reviews and the target as an additional input feature of the text decoder, inspired by Bražinskas et al. (2020). We define the average ratings of the source reviews minus the rating of the target as the rating deviation: sdj = ∑N i 6=j si/(N − 1)− sj . We use sdj to help generate a pseudo summary dj during training and set it as 0 to generate a summary with average semantic of input reviews during inference. To reflect the rating deviation, we modify the way in which a Transformer creates input embeddings, as in Figure 3. We create deviation embeddings with the same dimensionality as token embeddings and add sdj × deviation embeddings to the token embeddings in the same way as positional embeddings.\nOur methods to close the gap between training and inference tasks do not require additional modeling or training in comparison with previous works. We achieve noising and denoising effects by simply using rating deviation embeddings without variational inference in Bražinskas and Titov (2020). Furthermore, the information that the rating deviation is 0 plays the role of an input prompt for inference, without the need to train a separate classifier for selecting control tokens to be used as input prompts (Elsahar et al., 2021)."
    }, {
      "heading" : "5.2 Other Modalities Pretraining",
      "text" : "As the main modality for summarization is the text modality, we pretrain the image and table encoders by pivoting the text modality. Although the data of the three modalities are heterogeneous, each encoder should be trained to obtain homogeneous representations. We achieve this by using the pretrained text decoder as a pivot. We train the image encoder and the table encoder along with the text decoder to generate a review text of the entity to which images or metadata belong: I or T → dj ∈ R. The image and table encoders obtain himg and htable from I and T , respectively, and the text decoder generates dj from himg or htable. Note that we aggregate M encoded representations of himg as in the text modality pretraining, and the weights of the text decoder are made constant. I or T corresponds to all N reviews, and this means that I or T has multiple references. We convert a multiplereference setting to a single-reference setting to match the model output with the text modality pretraining. We simply createN single reference pairs from each entity and shuffle pairs from all entities to construct the training dataset (Zheng et al., 2018). As the text decoder was trained for generating a review text from text encoded representations, the image and table encoders are bound to produce similar representations with the text encoder to generate the same review text. In this way, we can maximize the ability to extract the information necessary for generating the review text."
    }, {
      "heading" : "5.3 Training for Multiple Modalities",
      "text" : "We train the entire multimodal framework from the pretrained encoders and decoder. The encoder of each modality obtains an encoded representation for each modality, and the text decoder generates the pseudo summary dj from multimodal encoded representations htext, himg, and htable. To fuse multimodal representations, we aim to meet three requirements. First, the text modality, which is the main modality, is primarily used. Second, the model works even if images or metadata are not available. Third, the model makes the most of the legacy from pretraining. To fulfill the requirements, multi-modality fusion is applied to the multi-head self-attention layer of the text decoder. The text decoder obtains the attention result for each modality at each layer. We fuse the attention results for multiple modalities as follows:\nmafused = matext + α maimg + β matable,\nwhere matext, maimg, and matable denote each modality attention result from htext, himg, and htable, respectively. symbolizes elementwise multiplication and eD-dimensional multimodal gates α and β are calculated as follows: α = φ([matext;maimg]Wα) and β = φ([matext;matable]Wβ). Note that α or β obtains the zero vector when images or metadata do not exist. It is common to use sigmoid as an activation function φ. However, it can lead to confusion in the text decoder pretrained using only the text source. Because the values of W are initialized at approximately 0, the values of α and β are initialized at approximately 0.5 when sigmoid is used. To initialize the gate values at approximately 0, we use ReLU(tanh(x)) as φ(x). This enables the continuous use of text information, and images or metadata are used selectively."
    }, {
      "heading" : "6 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "6.1 Datasets",
      "text" : "To evaluate the effectiveness of the model framework and training pipeline on datasets with different domains and characteristics, we performed experiments on two review datasets: Yelp Dataset Challenge1 and Amazon product reviews (He and McAuley, 2016). The Yelp dataset provides reviews based on personal experiences for a specific business. It also provides numerous images (e.g., food and drinks) uploaded by the users. Note that the maximum number of images, M , was set to 10 based on the 90th percentile. In addition, the dataset contains abundant metadata of businesses according to the characteristics of each business. On the contrary, the Amazon dataset provides reviews with more objective and specific details about a particular product. It contains a sin-\n1https://www.yelp.com/dataset\ngle image provided by the supplier, and provides relatively limited metadata for the product. For evaluation, we used the data used in previous research (Chu and Liu, 2019; Bražinskas and Titov, 2020). The data were generated by Amazon Mechanical Turk workers who summarized 8 input review texts. Therefore, we set N to 9 so that a pseudo summary is generated from 8 source reviews during training. For the Amazon dataset, 3 summaries are given per product. Simple data statistics are shown in Table 1, and other details can be found in Appendix A.1."
    }, {
      "heading" : "6.2 Experimental Details",
      "text" : "All the models2 were implemented with PyTorch (Paszke et al., 2019), and we used the Transformers library from Hugging Face (Wolf et al., 2020) as the backbone skeleton. Our text encoder and decoder were initialized using BART-Large and further pretrained using the training review corpus with the same objective as BART. eD, eI , and eT were all set to 1,024. We trained the entire models using the Adam optimizer (Kingma and Ba, 2014) with a linear learning rate decay on NVIDIA V100s. We decayed the model weights with 0.1. For each training pipeline, we set different batch sizes, epochs, learning rates, and warmup steps according to the amount of learning required at each step. We used label smoothing with 0.1 and set the maximum norm of gradients as 1 for other modalities pretraining and multiple-modalities training. During testing, we used beam search with early stopping and discarded hypotheses that contain twice the same trigram. Different beam size, length penalty, and max length were set for Yelp and Amazon. The best hyperparameter values and other details are described in Appendix A.2."
    }, {
      "heading" : "6.3 Comparison Models",
      "text" : "We compared our model to extractive and abstractive opinion summarization models. For extractive models, we used some simple baseline models (Bražinskas and Titov, 2020). Clustroid selects one review that gets the highest ROUGE-L score with the other reviews of an entity. Lead constructs a summary by extracting and concatenating the lead sentences from all review texts of an entity. Random simply selects one random review from an entity. LexRank (Erkan and Radev, 2004) is an extractive model that selects the most salient\n2Our code is available at https://bit.ly/3bR4yod\nsentences based on graph centrality. For abstractive models, we used non-neural and neural models. Opinosis (Ganesan et al., 2010) is a non-neural model that uses a graph-based summarizer based on token-level redundancy. MeanSum (Chu and Liu, 2019) is a neural model that is based on a denoising-autoencoder and generates a summary from mean representations of source reviews. We also used three self-supervised abstractive models. DenoiseSum (Amplayo and Lapata, 2020) generates a summary by denoising source reviews. Copycat (Bražinskas and Titov, 2020) uses a hierarchical variational autoencoder model and generates a summary from mean latent codes of the source reviews. Self & Control (Elsahar et al., 2021) generates a summary from Transformer models and uses some control tokens as additional inputs to the text decoder."
    }, {
      "heading" : "7 Results",
      "text" : "We evaluated our model framework and model training pipeline. In particular, we evaluated the summarization quality compared to other baseline models in terms of automatic and human evaluation, and conducted ablation studies."
    }, {
      "heading" : "7.1 Main Results",
      "text" : ""
    }, {
      "heading" : "7.1.1 Automatic Evaluation",
      "text" : "To evaluate the summarization quality, we used two automatic measures: ROUGE-{1,2,L} (Lin, 2004) and BERT-score (Zhang et al., 2020). The former is a token-level measure for comparing 1, 2, and adaptive L-gram matching tokens, and the latter is a document-level measure using pretrained BERT (Devlin et al., 2019). Contrary to ROUGEscore, which is based on exact matching between n-gram words, BERT-score is based on the semantic similarity between word embeddings that reflect the context of the document through BERT. It is approved that BERT-score is more robust to adversarial examples and correlates better with human judgments compared to other measures for machine translation and image captioning. We hypothesize that BERT-score is strong in opinion summarization as well, and BERT-score would complement ROUGE-score.\nThe results for opinion summarization on two datasets are shown in Table 2. MultimodalSum showed superior results compared with extractive and abstractive baselines for both token-level and document-level measures. From the results, we\nconclude that the multimodal framework outperformed the unimodal framework for unsupervised opinion summarization. In particular, our model achieved state-of-the-art results on the Amazon dataset and outperformed the comparable model by a large margin in the R-L representing the ROUGE scores on the Yelp dataset. Although Self & Control showed high R-2 score, we attributed their score to the inferred N -gram control tokens used as additional inputs to the text decoder.\nSample summaries on the Yelp dataset are shown in Table 3. They were generated from source reviews on Baby Cakes bakery. Copycat misused “sweet tooth” and generated “lemon mernigue pie” that was not mentioned in the source reviews. Self & Control generated a summary about a buffet by totally misunderstanding one sentence from source reviews: “If you love the desserts in Studio B Buffet in the M Hotel but don’t want to wait in the massive buffet line or even eat in the buffet, Baby Cakes in the M Hotel is really nice fix.” Furthermore, “Matos Buffet” is a non-existent word. On the contrary, MultimodalSum generated a good summary with a rich description of chocolate croissants. Although “chocolate chip cookie” was not found in the source reviews, our model generated it from cookie images. Note that the term can be found in other reviews that were not used as source reviews. Additional sample summaries on two datasets are shown in Appendix A.5."
    }, {
      "heading" : "7.1.2 Human Evaluation",
      "text" : "To evaluate the quality of summarization based on human criteria, we conducted a user study. We assessed the quality of summaries using Best-Worst Scaling (BWS; Louviere et al. (2015)). BWS is known to produce more reliable results than raking scales (Kiritchenko and Mohammad, 2017) and is widely used in self-supervised opinion summarization studies. We recruited 10 NLP experts and asked each participant to choose one best and one worst summary from four summaries for three criteria. For each participant’s response, the best model received +1, the worst model received -1, and the rest of the models received 0 scores. The final scores were obtained by averaging the scores of all the responses from all participants.\nFor Overall criterion, Self & Control, Copycat, MultimodalSum, and gold summaries scored -0.527, -0.113, +0.260, and +0.380 on the Yelp dataset, respectively. MultimodalSum showed superior performance in human evaluation as well as automatic evaluation. We note that human judgments correlate better with BERT-score than ROUGE-score. Self & Control achieved a very low human evaluation score despite its high ROUGEscore in automatic evaluation. We analyzed the summaries of Self & Control, and we found several flaws such as redundant words, ungrammatical expressions, and factual hallucinations. It generated a non-existent word by combining several subwords. It was particularly noticeable when a proper noun was generated. Furthermore, Self & Control generated an implausible sentence by copying some words from source reviews. From the results, we conclude that both automatic evaluation and human evaluation performances should be supported to be a good summarization model and BERT-score can complement ROUGE-score in automatic evaluation. Details on human evaluation and full results can be found in Appendix A.3."
    }, {
      "heading" : "7.1.3 Effects of Multimodality",
      "text" : "To analyze the effects of multimodal data on opinion summarization, we analyzed the multimodal gate. Since the multimodal gate is a eDdimensional vector, we averaged it by a scalar value. Furthermore, as multimodal gates exist for each layer of the text decoder, we averaged them to measure the overall influence of a table or images when generating each token in the decoder. An example of aggregated multimodal gates is shown in Figure 4. It shows the table and images used\nfor generating a summary text, and the multimodal gates for a part of the generated summary are expressed as heatmaps. As we intended, table and image information was selectively used to generate a specific word in the summary. The aggregated value of the table was relatively high for generating “Red Lobster”, which is the name of the restaurants. It was relatively high for images, when generating “food” that is depicted in two images. Another characteristic of the result is that aggregated values of the table were higher than those of the image: mean values for the table and image in the entire test data were 0.103 and 0.045, respectively. This implies that table information is more used when creating a summary, and this observation is valid in that the table contains a large amount of metadata. Note that the values displayed on the heatmaps are small by and large, as they were aggregated from eD-dimensional vector."
    }, {
      "heading" : "7.2 Ablation Studies",
      "text" : "For ablation studies, we analyzed the effectiveness of our model framework and model training pipeline in Table 4. To analyze the model framework, we first compared the summarization quality with four versions of unimodal model framework, as in the first block of Table 4. BART denotes the model framework in Figure 1a, whose weights are the weights of BART-Large. It represents the lower bound of our model framework without any training. BART-Review denotes the model framework whose weights are from further pretrained BART using the entire training review corpus. UnimodalSum refers to the results of the text modality pretraining, and we classified it into two frameworks according to the use of the rating deviation.\nSurprisingly, using only BART achieved comparable or better results than many extractive and abstractive baselines in Table 2. Furthermore, further pretraining using the review corpus brought performance improvements. Qualitatively, BART with further pretraining generated more diverse words and rich expressions from the review corpus. This proved our assumption that denoising autoencoderbased pretraining helps in self-supervised multimodal opinion summarization. Based on the BARTReview, UnimodalSum achieved superior results. Furthermore, the use of rating deviation improved the quality of summarization. We conclude that learning to generate reviews based on wide ranges of rating deviations including 0 during training helps to generate a better summary of the average semantics of the input reviews.\nTo analyze the effect of other modalities in our model framework, we compared the summarization quality with three versions of multimodal model frameworks, as in the second block of Table 4. We removed the image or table modality from MultimodalSum to analyze the contribution of each modality. Results showed that both modalities improved the summarization quality compared with UnimodalSum, and they brought additional improvements when used altogether. This indicates that using non-text information helps in selfsupervised opinion summarization. As expected, the utility of the table modality was higher than that of the image modality. The image modality contains detailed information not revealed in the table modality (e.g., appearance of food, inside/outside mood of business, design of product, and color/texture of product). However, the information is unorganized to the extent that the utility of the image modality depends on the capacity of the image encoder to extract unorganized information. Although MultimodalSum used a representative image encoder because our study is the first work on multimodal opinion summarization, we expect that the utility of the image modality will be greater if unorganized information can be extracted effectively from the image using advanced image encoders.\nFor analyzing the model training pipeline, we removed text modality or/and other modalities pretraining from the pipeline. By removing each of them, the performance of MultimodalSum declined, and removing all of the pretraining steps caused an additional performance drop. Although Multi-\nmodalSum without other modalities pretraining has the capability of text summarization, it showed low summarization performance at the beginning of the training due to the heterogeneity of the three modality representations. However, MultimodalSum without text modality pretraining, whose image and table encoders were pretrained using BARTReview as a pivot, showed stable performance from the beginning, but the performance did not improve significantly. From the results, we conclude that both text modality and other modalities pretraining help the training of multimodal framework. For the other modalities pretraining, we conducted a further analysis in the Appendix A.4."
    }, {
      "heading" : "8 Conclusions",
      "text" : "We proposed the first self-supervised multimodal opinion summarization framework. Our framework can reflect text, images, and metadata together as an extension of the existing self-supervised opinion summarization framework. To resolve the heterogeneity of multimodal data, we also proposed a multimodal training pipeline. We verified the effectiveness of our multimodal framework and training pipeline with various experiments on real review datasets. Self-supervised multimodal opinion summarization can be used in various ways in the future, such as providing a multimodal summary or enabling a multimodal retrieval. By retrieving reviews related to a specific image or metadata, controlled opinion summarization will be possible."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the anonymous reviewers for their insightful comments and suggestions."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Dataset Preprocessing We selected businesses and products with a minimum of 10 reviews and popular entities above the 90th percentile were removed. The minimum and maximum length of the words were set as 35 and 100 for Yelp, and 45 and 70 for Amazon, respectively. We set the maximum number of tokens as 128 using the BART tokenizer for training, and we did not limit the maximum tokens for inference. For the Amazon dataset, we selected 4 categories: Electronics; Clothing, Shoes and Jewelry; Home and Kitchen; Health and Personal Care. As Yelp dataset contains unlimited number of images for each entity, we did not use images for popular entities above the 90th percentile. On the other hand, Amazon dataset contains a single image for each entity. Therefore, we did not use images only when meaningless images such as non-image icon or update icon were used or the image links had expired.\nFor Yelp dataset, we selected name, ratings, categories, hours, and attributes among the metadata. We used the hours of each day of the week as seven fields and used all metadata contained in attributes as each field. For some attributes (‘Ambience’, ‘BusinessParking’, ‘GoodForMeal’) that have subordinate attributes, we used each subordinate attribute. Among the fields, we selected 47 fields used by at least 10% of the entities. We set the maximum number of categories as 6 based on the 90th percentile, and averaged the representations of each category. For ratings, we converted it to binary notation consisting of 4 digits (22, 21, 20, 2−1). For hours, we considered (open hour, close hour) as a 2-dimensional vector, and conducted K-means clustering. We selected four clusters based on silhouette score: (16.5, 23.2), (8.7, 17.1), (6.4, 23), and (10.6, 22.6). Based on the clusters, we converted hours into a categorical type.\nFor Amazon dataset, we selected six fields: name, price, brand, categories, ratings, and description. We set the maximum number of categories as 3 based on the 90th percentile, and averaged the representations of each category. Furthermore, as each category consists of hierarchies with a maximum of 8 depths, we averaged the representations of hierarchies to get each category representation. For price and ratings, we converted them to binary notation consisting of 11 and 4 digits, respectively, after rounding them to the nearest 0.5 to contain digit for 2−1. As some descriptions consist of many\ntokens, we set the maximum number of tokens as 128. We regarded each token in description as each field, so we got total 5 + 128 fields.\nA.2 Experimental Details\nOur image encoder is based on ResNet101. ResNet101 is composed of 1 convolution layer, 4 convolution layer blocks, and 1 fully connected layer block. Among them, 4 convolution layer blocks play an important role in analyzing image. Through each convolution layer block, the size of the image feature map is reduced to 1/4, but it gets high-level features. To maintain the ability to extract low-level features of the image, we set the model weights up to the second convolution layer block not to be trained further. We only used up to the third convolution layer block to increase the resolution of feature maps without using too highlevel features for image classification. In this way, lI was set to 14× 14 and eI was set to 1,024.\nTo use the knowledge of text modality in table encoder, we obtained field name embeddings by summing the BART token embeddings for the tokens contained in the field name. Because various data types can be used for field value, we used different processing methods for each data type. Nominal values were handled in the same way as the field name. Binary and ordinal values were processed by replacing them with nominal values of corresponding meanings: ‘true’ and ‘false’ were used for binary values, and ‘cheap’, ‘average’, ‘expensive’, and ‘very expensive’ were used for ‘RestaurantsPriceRange’. Numerical values were converted to binary notation, and we obtained the representations by summing embeddings corresponding to the place, where the place value is 1. For other categorical values, we simply trained embeddings corresponding to each category.\nWe set each hyperparameter value different for each step in the model training pipeline, as in Table 5. We set the batch size according to the memory usage and set other values according to the amount of learning required. Hyperparameter ranges for epochs and lr (learning rate) were [3, 5, 10, 15, 20] and [1e-03, 1e-04, 5e-05, 1e-05, 5e-06],\nrespectively, and optimized values were chosen from validation loss in one trial. For summary generation at test time, we set different hyperparameter values for each dataset. Beam size, length penalty, and max length were set to 4, 0.97, and 105 for Yelp and 2, 0.9, and 80 for Amazon, respectively. Note that max length was set first to prevent incomplete termination and length penalty was determined based on the ROUGE scores on validation dataset. The number of training parameters for text, image, and table modality pretraining are 406.3M, 27.1M, and 3.2M, respectively, and that for multimodal training is 486.9M. Run time for text modality pretraining was 16h on 4 GPUs, and it took 41h and 43h on 2 GPUs for image and table modality training, respectively. For final multimodal training, it took 14h on 8 GPUs.\nA.3 Human Evaluation\nFor human evaluation, we randomly selected 30 entities from Yelp test data, and used three criteria: Grmmaticality (the summary should be fluent and grammatical), Coherence (the summary should be well structured and well organized), and Overall (based on your own criteria, select the best and the worst summary of the reviews). Results for three criteria are shown in Table 6. Self & Control achieved very poor performance for all criteria due to its flaws that were not revealed in the automatic evaluation. Surprisingly, MultimodalSum outperformed gold summaries for two criteria; however, its overall performance lagged behind Gold. As our model was initialized from BART-Large that had been pretrained using large corpus and further pretrained using training review corpus, it may have generated fluent and coherent summaries. It seems that our model lagged behind Gold in Overall due to various criteria other than those two. The fact that Gold scored lower than Copycat in Grammaticality may seem inconsistent with the result from Bražinskas and Titov (2020). However, we assumed that this result was due to a combination of the four models in relative evaluation. The ranking for Copycat and Gold may have changed in absolute evaluation.\nA.4 Analysis on Other Modalities Pretraining\nTo analyze the various models for the other modalities pretraining, we evaluated the performance of the reference review generation task that generates corresponding reviews from images or a table. For evaluation, we used the data that were not used for training data: we left 10% of the data for Yelp and 5% for Amazon. We chose two comparison models: Untrained and Triplet. Untrained denotes the model that image encoder or table encoder keeps untrained. This option indicates the lower bound containing only the effect of the text decoder. Triplet denotes the triplet-based metriclearning model, based on Lee et al. (2018) and Vo and Hays (2016). For triplet (images or a table, reviews of positive entity, reviews of negative entities), we trained the image or table encoder based on the pretrained text encoder, by placing the image or table encoded representations close to the positive reviews representations and far from the negative reviews representations. Note that pretrained text encoder was not trained further.\nResults on the other modalities pretraining are shown in Table 7. For each model, the pretrained decoder generated a review from image or table encoded representations. We measured the average ROUGE scores between the generated review and N reference reviews. The first finding was that results of table outperformed those of image. It indicates that table has more helpful information for generating reference review. The second finding was that our method based on the text decoder outperformed the Triplet based on the text encoder. Especially, Triplet achieved very poor performance for image because it is hard to match M images to N reference reviews for metric learning. On the contrary, our method achieved much better performance by pivoting the text decoder. Triplet showed good performance on table because it is relatively easy to match 1 table to N reference reviews; however, our method outperformed it. We conclude that our method lets the image and table encoder get proper representations to generate reference reviews regardless of the number of inputs.\nA.5 Example Summaries Table 8, 9 show sample summaries generated from our model and baseline models on Yelp and Amazon datasets. Full summaries from our model are available at https://bit.ly/3bR4yod."
    } ],
    "references" : [ {
      "title" : "Unsupervised opinion summarization with noising and denoising",
      "author" : [ "Reinald Kim Amplayo", "Mirella Lapata." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1934–1945.",
      "citeRegEx" : "Amplayo and Lapata.,? 2020",
      "shortCiteRegEx" : "Amplayo and Lapata.",
      "year" : 2020
    }, {
      "title" : "Summarizing opinions: Aspect extraction meets sentiment prediction and they are both weakly supervised",
      "author" : [ "Stefanos Angelidis", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Angelidis and Lapata.,? 2018",
      "shortCiteRegEx" : "Angelidis and Lapata.",
      "year" : 2018
    }, {
      "title" : "Multimodal machine learning: A survey and taxonomy",
      "author" : [ "Tadas Baltrušaitis", "Chaitanya Ahuja", "LouisPhilippe Morency." ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence, 41(2):423–443.",
      "citeRegEx" : "Baltrušaitis et al\\.,? 2018",
      "shortCiteRegEx" : "Baltrušaitis et al\\.",
      "year" : 2018
    }, {
      "title" : "Few-shot learning for opinion summarization",
      "author" : [ "Arthur Bražinskas", "Mirella Lapata", "Ivan Titov." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4119–4135.",
      "citeRegEx" : "Bražinskas et al\\.,? 2020",
      "shortCiteRegEx" : "Bražinskas et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised opinion summarization as copycatreview generation",
      "author" : [ "Mirella Lapata Bražinskas", "Arthur", "Ivan Titov." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5151–5169.",
      "citeRegEx" : "Bražinskas et al\\.,? 2020",
      "shortCiteRegEx" : "Bražinskas et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-document summarization of evaluative tex",
      "author" : [ "Giuseppe Carenini", "Jackie Chi Kit Cheung", "Adam Pauls." ],
      "venue" : "Computational Intelligence, 29(4):545–576.",
      "citeRegEx" : "Carenini et al\\.,? 2013",
      "shortCiteRegEx" : "Carenini et al\\.",
      "year" : 2013
    }, {
      "title" : "Multi-document summarization of evaluative text",
      "author" : [ "Giuseppe Carenini", "Raymond Ng", "Adam Pauls." ],
      "venue" : "Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Carenini et al\\.,? 2006",
      "shortCiteRegEx" : "Carenini et al\\.",
      "year" : 2006
    }, {
      "title" : "Abstractive textimage summarization using multi-modal attentional hierarchical rnn",
      "author" : [ "Jingqiang Chen", "Hai Zhuge." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4046–4056.",
      "citeRegEx" : "Chen and Zhuge.,? 2018",
      "shortCiteRegEx" : "Chen and Zhuge.",
      "year" : 2018
    }, {
      "title" : "Meansum: a neural model for unsupervised multi-document abstractive summarization",
      "author" : [ "Eric Chu", "Peter Liu." ],
      "venue" : "In Proceedings of International Conference on Machine Learning (ICML), pages 1223–1232.",
      "citeRegEx" : "Chu and Liu.,? 2019",
      "shortCiteRegEx" : "Chu and Liu.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "A hybrid approach to multidocument summarization of opinions in reviews",
      "author" : [ "Giuseppe Di Fabbrizio", "Amanda Stent", "Robert Gaizauskas." ],
      "venue" : "Proceedings of the 8th International Natural Language Generation Conference, pages 54–63.",
      "citeRegEx" : "Fabbrizio et al\\.,? 2014",
      "shortCiteRegEx" : "Fabbrizio et al\\.",
      "year" : 2014
    }, {
      "title" : "Self-supervised and controlled multi-document opinion summarization",
      "author" : [ "Hady Elsahar", "Maximin Coavoux", "Jos Rozen", "Matthias Gallé." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Elsahar et al\\.,? 2021",
      "shortCiteRegEx" : "Elsahar et al\\.",
      "year" : 2021
    }, {
      "title" : "Lexrank: Graph-based lexical centrality as salience in text summarization",
      "author" : [ "Günes Erkan", "Dragomir R Radev." ],
      "venue" : "Journal of artificial intelligence research, 22:457–479.",
      "citeRegEx" : "Erkan and Radev.,? 2004",
      "shortCiteRegEx" : "Erkan and Radev.",
      "year" : 2004
    }, {
      "title" : "Multimodal summarization for video-containing documents",
      "author" : [ "Xiyan Fu", "Jun Wang", "Zhenglu Yang." ],
      "venue" : "arXiv preprint arXiv:2009.08018.",
      "citeRegEx" : "Fu et al\\.,? 2020",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2020
    }, {
      "title" : "Opinosis: A graph based approach to abstractive summarization of highly redundant opinions",
      "author" : [ "Kavita Ganesan", "ChengXiang Zhai", "Jiawei Han." ],
      "venue" : "Proceedings of the 23rd International Conference on Computational Linguistics, pages 340–348.",
      "citeRegEx" : "Ganesan et al\\.,? 2010",
      "shortCiteRegEx" : "Ganesan et al\\.",
      "year" : 2010
    }, {
      "title" : "Abstractive summarization of product reviews using discourse structure",
      "author" : [ "Shima Gerani", "Yashar Mehdad", "Giuseppe Carenini", "Raymond Ng", "Bita Nejat." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Gerani et al\\.,? 2014",
      "shortCiteRegEx" : "Gerani et al\\.",
      "year" : 2014
    }, {
      "title" : "Don’t stop pretraining: Adapt language models to domains and tasks",
      "author" : [ "Suchin Gururangan", "Ana Marasović", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A Smith." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the",
      "citeRegEx" : "Gururangan et al\\.,? 2020",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering",
      "author" : [ "Ruining He", "Julian McAuley." ],
      "venue" : "Proceedings of the 25th International Conference on World Wide Web, pages 507–517.",
      "citeRegEx" : "He and McAuley.,? 2016",
      "shortCiteRegEx" : "He and McAuley.",
      "year" : 2016
    }, {
      "title" : "Unsupervised multimodal neural machine translation with pseudo visual pivoting",
      "author" : [ "Po-Yao Huang", "Junjie Hu", "Xiaojun Chang", "Alexander Hauptmann." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Huang et al\\.,? 2020",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Bestworst scaling more reliable than rating scales: A case study on sentiment intensity annotation",
      "author" : [ "Svetlana Kiritchenko", "Saif Mohammad." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short",
      "citeRegEx" : "Kiritchenko and Mohammad.,? 2017",
      "shortCiteRegEx" : "Kiritchenko and Mohammad.",
      "year" : 2017
    }, {
      "title" : "Statistical significance tests for machine translation evaluation",
      "author" : [ "Philipp Koehn." ],
      "venue" : "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388–395.",
      "citeRegEx" : "Koehn.,? 2004",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2004
    }, {
      "title" : "Opinion extraction, summarization and tracking in news and blog corpora. In AAAI spring symposium: Computational approaches to analyzing weblogs",
      "author" : [ "Lun-Wei Ku", "Yu-Ting Liang", "Hsin-Hsi Chen" ],
      "venue" : null,
      "citeRegEx" : "Ku et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Ku et al\\.",
      "year" : 2006
    }, {
      "title" : "Stacked cross attention for image-text matching",
      "author" : [ "Kuang-Huei Lee", "Xi Chen", "Gang Hua", "Houdong Hu", "Xiaodong He." ],
      "venue" : "Proceedings of the European Conference on Computer Vision, pages 201– 216.",
      "citeRegEx" : "Lee et al\\.,? 2018",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2018
    }, {
      "title" : "Bart: Denoising sequence-to-sequence pretraining for natural language generation",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer." ],
      "venue" : "Proceed-",
      "citeRegEx" : "Lewis et al\\.,? 2020",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Aspect-aware multimodal summarization for chinese e-commerce products",
      "author" : [ "Haoran Li", "Peng Yuan", "Song Xu", "Youzheng Wu", "Xiaodong He", "Bowen Zhou." ],
      "venue" : "Proceedings of the 34th AAAI Conference on Artificial Intelligence, pages 8188–8195.",
      "citeRegEx" : "Li et al\\.,? 2020a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-modal sentence summarization with modality attention and image filtering",
      "author" : [ "Haoran Li", "Junnan Zhu", "Tianshang Liu", "Jiajun Zhang", "Chengqing Zong." ],
      "venue" : "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Vmsmo: Learning to generate multimodal summary for videobased news articles",
      "author" : [ "Mingzhe Li", "Xiuying Chen", "Shen Gao", "Zhangming Chan", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Li et al\\.,? 2020b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text summarization branches out, pages 74–81.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Generating wikipedia by summarizing long sequences",
      "author" : [ "Peter J Liu", "Mohammad Saleh", "Etienne Pot", "Ben Goodrich", "Ryan Sepassi", "Lukasz Kaiser", "Noam Shazeer." ],
      "venue" : "Proceedings of the 6th International Conference on Learning Representations.",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Hierarchical transformers for multi-document summarization",
      "author" : [ "Yang Liu", "Mirella Lapata." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, page 5070–5081.",
      "citeRegEx" : "Liu and Lapata.,? 2019",
      "shortCiteRegEx" : "Liu and Lapata.",
      "year" : 2019
    }, {
      "title" : "Best-worst scaling: Theory, methods and applications",
      "author" : [ "Jordan J Louviere", "Terry N Flynn", "Anthony Alfred John Marley." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Louviere et al\\.,? 2015",
      "shortCiteRegEx" : "Louviere et al\\.",
      "year" : 2015
    }, {
      "title" : "Textrank: Bringing order into text",
      "author" : [ "Rada Mihalcea", "Paul Tarau." ],
      "venue" : "Proceedings of the 2004 conference on Empirical Methods in Natural Language Processing, pages 404–411.",
      "citeRegEx" : "Mihalcea and Tarau.,? 2004",
      "shortCiteRegEx" : "Mihalcea and Tarau.",
      "year" : 2004
    }, {
      "title" : "Abstractive text summarization using sequence-to-sequence rnns and beyond",
      "author" : [ "Ramesh Nallapati", "Bowen Zhou", "Cicero dos Santos", "Caglar Gulcehre", "Bing Xiang." ],
      "venue" : "Proceedings of The 20th SIGNLL Conference on Computational Natural Lan-",
      "citeRegEx" : "Nallapati et al\\.,? 2016",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2016
    }, {
      "title" : "Pytorch: An imperative style, high-performance deep learning library",
      "author" : [ "Adam Paszke", "Sam Gross", "Francisco Massa", "Adam Lerer", "James Bradbury", "Gregory Chanan", "Trevor Killeen", "Zeming Lin", "Natalia Gimelshein", "Luca Antiga" ],
      "venue" : null,
      "citeRegEx" : "Paszke et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Paszke et al\\.",
      "year" : 2019
    }, {
      "title" : "Summarizing contrastive viewpoints in opinionated text",
      "author" : [ "Michael Paul", "ChengXiang Zhai", "Roxana Girju." ],
      "venue" : "Proceedings of the 2010 conference on Empirical Methods in Natural Language Processing, pages 66–76.",
      "citeRegEx" : "Paul et al\\.,? 2010",
      "shortCiteRegEx" : "Paul et al\\.",
      "year" : 2010
    }, {
      "title" : "A deep reinforced model for abstractive summarization",
      "author" : [ "Romain Paulus", "Caiming Xiong", "Richard Socher." ],
      "venue" : "Proceedings of the 6th International Conference on Learning Representations.",
      "citeRegEx" : "Paulus et al\\.,? 2018",
      "shortCiteRegEx" : "Paulus et al\\.",
      "year" : 2018
    }, {
      "title" : "Generating summaries with topic templates and structured convolutional decoders",
      "author" : [ "Laura Perez-Beltrachini", "Yang Liu", "Mirella Lapata." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, page 5107–5116.",
      "citeRegEx" : "Perez.Beltrachini et al\\.,? 2019",
      "shortCiteRegEx" : "Perez.Beltrachini et al\\.",
      "year" : 2019
    }, {
      "title" : "Data-to-text generation with content selection and planning",
      "author" : [ "Ratish Puduppully", "Li Dong", "Mirella Lapata." ],
      "venue" : "Proceedings of the 33th AAAI Conference on Artificial Intelligence, pages 6908–6915.",
      "citeRegEx" : "Puduppully et al\\.,? 2019",
      "shortCiteRegEx" : "Puduppully et al\\.",
      "year" : 2019
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J. Liu", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1073–1083.",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "Unsupervised multi-modal neural machine translation",
      "author" : [ "Yuanhang Su", "Kai Fan", "Nguyen Bach", "C.-C. Jay Kuo", "Fei Huang." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 10482–10491.",
      "citeRegEx" : "Su et al\\.,? 2019",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2019
    }, {
      "title" : "Multimodal review generation for recommender systems",
      "author" : [ "Quoc-Tuan Truong", "Hady Lauw." ],
      "venue" : "Proceedings of the World Wide Web Conference, pages 1864–1874.",
      "citeRegEx" : "Truong and Lauw.,? 2019",
      "shortCiteRegEx" : "Truong and Lauw.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Localizing and orienting street views using overhead imagery",
      "author" : [ "Nam N Vo", "James Hays." ],
      "venue" : "Proceedings of the European Conference on Computer Vision, pages 494–509.",
      "citeRegEx" : "Vo and Hays.,? 2016",
      "shortCiteRegEx" : "Vo and Hays.",
      "year" : 2016
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Bertscore: Evaluating text generation with bert",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q Weinberger", "Yoav Artzi." ],
      "venue" : "Proceedings of the 8th International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Sentence centrality revisited for unsupervised summarization",
      "author" : [ "Hao Zheng", "Mirella Lapata." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6236– 6247.",
      "citeRegEx" : "Zheng and Lapata.,? 2019",
      "shortCiteRegEx" : "Zheng and Lapata.",
      "year" : 2019
    }, {
      "title" : "Multi-reference training with pseudo-references for neural translation and text generation",
      "author" : [ "Renjie Zheng", "Mingbo Ma", "Liang Huang." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3188–3197.",
      "citeRegEx" : "Zheng et al\\.,? 2018",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2018
    }, {
      "title" : "Msmo: Multimodal summarization with multimodal output",
      "author" : [ "Junnan Zhu", "Haoran Li", "Tianshang Liu", "Yu Zhou", "Jiajun Zhang", "Chengqing Zong." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, page",
      "citeRegEx" : "Zhu et al\\.,? 2018",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2018
    }, {
      "title" : "Multimodal summarization with guidance of multimodal reference",
      "author" : [ "Junnan Zhu", "Yu Zhou", "Jiajun Zhang", "Haoran Li", "Chengqing Zong", "Changliang Li." ],
      "venue" : "Proceedings of the 34th AAAI Conference on Artificial Intelligence, pages 9749–9756.",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 34,
      "context" : "Unlike conventional single-document or multiple-document summarization, where we can obtain the prevalent annotated summaries (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2018; Liu et al., 2018; Liu and Lapata, 2019; Perez-Beltrachini et al., 2019), opinion summarization is challenging; it is difficult to find summarized opinions of users.",
      "startOffset" : 126,
      "endOffset" : 261
    }, {
      "referenceID" : 40,
      "context" : "Unlike conventional single-document or multiple-document summarization, where we can obtain the prevalent annotated summaries (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2018; Liu et al., 2018; Liu and Lapata, 2019; Perez-Beltrachini et al., 2019), opinion summarization is challenging; it is difficult to find summarized opinions of users.",
      "startOffset" : 126,
      "endOffset" : 261
    }, {
      "referenceID" : 37,
      "context" : "Unlike conventional single-document or multiple-document summarization, where we can obtain the prevalent annotated summaries (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2018; Liu et al., 2018; Liu and Lapata, 2019; Perez-Beltrachini et al., 2019), opinion summarization is challenging; it is difficult to find summarized opinions of users.",
      "startOffset" : 126,
      "endOffset" : 261
    }, {
      "referenceID" : 30,
      "context" : "Unlike conventional single-document or multiple-document summarization, where we can obtain the prevalent annotated summaries (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2018; Liu et al., 2018; Liu and Lapata, 2019; Perez-Beltrachini et al., 2019), opinion summarization is challenging; it is difficult to find summarized opinions of users.",
      "startOffset" : 126,
      "endOffset" : 261
    }, {
      "referenceID" : 31,
      "context" : "Unlike conventional single-document or multiple-document summarization, where we can obtain the prevalent annotated summaries (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2018; Liu et al., 2018; Liu and Lapata, 2019; Perez-Beltrachini et al., 2019), opinion summarization is challenging; it is difficult to find summarized opinions of users.",
      "startOffset" : 126,
      "endOffset" : 261
    }, {
      "referenceID" : 38,
      "context" : "Unlike conventional single-document or multiple-document summarization, where we can obtain the prevalent annotated summaries (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2018; Liu et al., 2018; Liu and Lapata, 2019; Perez-Beltrachini et al., 2019), opinion summarization is challenging; it is difficult to find summarized opinions of users.",
      "startOffset" : 126,
      "endOffset" : 261
    }, {
      "referenceID" : 0,
      "context" : "Recent studies (Bražinskas and Titov, 2020; Amplayo and Lapata, 2020; Elsahar et al., 2021) used a self-supervised learning framework",
      "startOffset" : 15,
      "endOffset" : 91
    }, {
      "referenceID" : 11,
      "context" : "Recent studies (Bražinskas and Titov, 2020; Amplayo and Lapata, 2020; Elsahar et al., 2021) used a self-supervised learning framework",
      "startOffset" : 15,
      "endOffset" : 91
    }, {
      "referenceID" : 42,
      "context" : "This non-text information influences the review text generation process of users (Truong and Lauw, 2019).",
      "startOffset" : 81,
      "endOffset" : 104
    }, {
      "referenceID" : 41,
      "context" : "Therefore, using this additional information can help in opinion summarization, especially under unsupervised settings (Su et al., 2019; Huang et al., 2020).",
      "startOffset" : 119,
      "endOffset" : 156
    }, {
      "referenceID" : 19,
      "context" : "Therefore, using this additional information can help in opinion summarization, especially under unsupervised settings (Su et al., 2019; Huang et al., 2020).",
      "startOffset" : 119,
      "endOffset" : 156
    }, {
      "referenceID" : 6,
      "context" : "Clusteringbased approaches (Carenini et al., 2006; Ku et al., 2006; Paul et al., 2010; Angelidis and Lapata, 2018) were used to cluster opinions regarding the same aspect and extract the text representing each clus-",
      "startOffset" : 27,
      "endOffset" : 114
    }, {
      "referenceID" : 23,
      "context" : "Clusteringbased approaches (Carenini et al., 2006; Ku et al., 2006; Paul et al., 2010; Angelidis and Lapata, 2018) were used to cluster opinions regarding the same aspect and extract the text representing each clus-",
      "startOffset" : 27,
      "endOffset" : 114
    }, {
      "referenceID" : 36,
      "context" : "Clusteringbased approaches (Carenini et al., 2006; Ku et al., 2006; Paul et al., 2010; Angelidis and Lapata, 2018) were used to cluster opinions regarding the same aspect and extract the text representing each clus-",
      "startOffset" : 27,
      "endOffset" : 114
    }, {
      "referenceID" : 1,
      "context" : "Clusteringbased approaches (Carenini et al., 2006; Ku et al., 2006; Paul et al., 2010; Angelidis and Lapata, 2018) were used to cluster opinions regarding the same aspect and extract the text representing each clus-",
      "startOffset" : 27,
      "endOffset" : 114
    }, {
      "referenceID" : 12,
      "context" : "Graph-based approaches (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Zheng and Lapata, 2019) were used to construct a graph—where nodes were sentences, and edges were similarities between sentences—and extract the sentences based on their centrality.",
      "startOffset" : 23,
      "endOffset" : 96
    }, {
      "referenceID" : 33,
      "context" : "Graph-based approaches (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Zheng and Lapata, 2019) were used to construct a graph—where nodes were sentences, and edges were similarities between sentences—and extract the sentences based on their centrality.",
      "startOffset" : 23,
      "endOffset" : 96
    }, {
      "referenceID" : 47,
      "context" : "Graph-based approaches (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Zheng and Lapata, 2019) were used to construct a graph—where nodes were sentences, and edges were similarities between sentences—and extract the sentences based on their centrality.",
      "startOffset" : 23,
      "endOffset" : 96
    }, {
      "referenceID" : 49,
      "context" : "studies provided not only a text summary but also other modality information as output (Zhu et al., 2018; Chen and Zhuge, 2018; Zhu et al., 2020; Li et al., 2020b; Fu et al., 2020).",
      "startOffset" : 87,
      "endOffset" : 180
    }, {
      "referenceID" : 7,
      "context" : "studies provided not only a text summary but also other modality information as output (Zhu et al., 2018; Chen and Zhuge, 2018; Zhu et al., 2020; Li et al., 2020b; Fu et al., 2020).",
      "startOffset" : 87,
      "endOffset" : 180
    }, {
      "referenceID" : 50,
      "context" : "studies provided not only a text summary but also other modality information as output (Zhu et al., 2018; Chen and Zhuge, 2018; Zhu et al., 2020; Li et al., 2020b; Fu et al., 2020).",
      "startOffset" : 87,
      "endOffset" : 180
    }, {
      "referenceID" : 28,
      "context" : "studies provided not only a text summary but also other modality information as output (Zhu et al., 2018; Chen and Zhuge, 2018; Zhu et al., 2020; Li et al., 2020b; Fu et al., 2020).",
      "startOffset" : 87,
      "endOffset" : 180
    }, {
      "referenceID" : 13,
      "context" : "studies provided not only a text summary but also other modality information as output (Zhu et al., 2018; Chen and Zhuge, 2018; Zhu et al., 2020; Li et al., 2020b; Fu et al., 2020).",
      "startOffset" : 87,
      "endOffset" : 180
    }, {
      "referenceID" : 25,
      "context" : "Our text encoder and decoder are based on BART (Lewis et al., 2020).",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 43,
      "context" : "former (Vaswani et al., 2017) encoder–decoder pretrained model that is particularly effective when fine-tuned for text generation and has high summarization performance.",
      "startOffset" : 7,
      "endOffset" : 29
    }, {
      "referenceID" : 16,
      "context" : "Therefore, we further pretrain BART using the entire training review corpus (Gururangan et al., 2020).",
      "startOffset" : 76,
      "endOffset" : 101
    }, {
      "referenceID" : 17,
      "context" : "ImageNet pretrained ResNet101 (He et al., 2016), which is widely used as a backbone network.",
      "startOffset" : 30,
      "endOffset" : 47
    }, {
      "referenceID" : 39,
      "context" : "To effectively encode metadata, we design our table encoder based on the framework of data-to-text research (Puduppully et al., 2019).",
      "startOffset" : 108,
      "endOffset" : 133
    }, {
      "referenceID" : 41,
      "context" : "As this was an important step for unsupervised multimodal neural machine translation (Su et al., 2019), we apply it to our framework.",
      "startOffset" : 85,
      "endOffset" : 102
    }, {
      "referenceID" : 11,
      "context" : "To generate a pseudo summary that covers the overall contents of source reviews, we simply average the N − 1 single-head attention results for each encoded representation (RlD×eD ) at each head (Elsahar et al., 2021).",
      "startOffset" : 194,
      "endOffset" : 216
    }, {
      "referenceID" : 11,
      "context" : "Furthermore, the information that the rating deviation is 0 plays the role of an input prompt for inference, without the need to train a separate classifier for selecting control tokens to be used as input prompts (Elsahar et al., 2021).",
      "startOffset" : 214,
      "endOffset" : 236
    }, {
      "referenceID" : 48,
      "context" : "We simply createN single reference pairs from each entity and shuffle pairs from all entities to construct the training dataset (Zheng et al., 2018).",
      "startOffset" : 128,
      "endOffset" : 148
    }, {
      "referenceID" : 18,
      "context" : "To evaluate the effectiveness of the model framework and training pipeline on datasets with different domains and characteristics, we performed experiments on two review datasets: Yelp Dataset Challenge1 and Amazon product reviews (He and McAuley, 2016).",
      "startOffset" : 231,
      "endOffset" : 253
    }, {
      "referenceID" : 8,
      "context" : "For evaluation, we used the data used in previous research (Chu and Liu, 2019; Bražinskas and Titov, 2020).",
      "startOffset" : 59,
      "endOffset" : 106
    }, {
      "referenceID" : 35,
      "context" : "All the models2 were implemented with PyTorch (Paszke et al., 2019), and we used the Transformers library from Hugging Face (Wolf et al.",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 12,
      "context" : "LexRank (Erkan and Radev, 2004) is an extractive model that selects the most salient",
      "startOffset" : 8,
      "endOffset" : 31
    }, {
      "referenceID" : 8,
      "context" : "MeanSum (Chu and Liu, 2019) is a neural model that is based on a denoising-autoencoder and generates a summary from mean representations of source",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "DenoiseSum (Amplayo and Lapata, 2020) generates a summary by denoising source reviews.",
      "startOffset" : 11,
      "endOffset" : 37
    }, {
      "referenceID" : 11,
      "context" : "Self & Control (Elsahar et al., 2021) generates a summary from Transformer models and uses some control tokens as additional inputs to the text decoder.",
      "startOffset" : 15,
      "endOffset" : 37
    }, {
      "referenceID" : 29,
      "context" : "To evaluate the summarization quality, we used two automatic measures: ROUGE-{1,2,L} (Lin, 2004) and BERT-score (Zhang et al.",
      "startOffset" : 85,
      "endOffset" : 96
    }, {
      "referenceID" : 46,
      "context" : "To evaluate the summarization quality, we used two automatic measures: ROUGE-{1,2,L} (Lin, 2004) and BERT-score (Zhang et al., 2020).",
      "startOffset" : 112,
      "endOffset" : 132
    }, {
      "referenceID" : 9,
      "context" : "2, and adaptive L-gram matching tokens, and the latter is a document-level measure using pretrained BERT (Devlin et al., 2019).",
      "startOffset" : 105,
      "endOffset" : 126
    }, {
      "referenceID" : 22,
      "context" : "05) over the second-best model based on paired bootstrap resampling (Koehn, 2004).",
      "startOffset" : 68,
      "endOffset" : 81
    }, {
      "referenceID" : 21,
      "context" : "BWS is known to produce more reliable results than raking scales (Kiritchenko and Mohammad, 2017) and is widely used in self-supervised opinion summarization studies.",
      "startOffset" : 65,
      "endOffset" : 97
    } ],
    "year" : 2021,
    "abstractText" : "Recently, opinion summarization, which is the generation of a summary from multiple reviews, has been conducted in a self-supervised manner by considering a sampled review as a pseudo summary. However, non-text data such as image and metadata related to reviews have been considered less often. To use the abundant information contained in non-text data, we propose a self-supervised multimodal opinion summarization framework called MultimodalSum. Our framework obtains a representation of each modality using a separate encoder for each modality, and the text decoder generates a summary. To resolve the inherent heterogeneity of multimodal data, we propose a multimodal training pipeline. We first pretrain the text encoder–decoder based solely on text modality data. Subsequently, we pretrain the non-text modality encoders by considering the pretrained text decoder as a pivot for the homogeneous representation of multimodal data. Finally, to fuse multimodal representations, we train the entire framework in an end-to-end manner. We demonstrate the superiority of MultimodalSum by conducting experiments on Yelp and Amazon datasets.",
    "creator" : "LaTeX with hyperref"
  }
}