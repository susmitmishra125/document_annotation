{
  "name" : "2021.acl-long.473.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Capturing Relations between Scientific Papers: An Abstractive Model for Related Work Section Generation",
    "authors" : [ "Xiuying Chen", "Hind Alamro", "Mingzhe Li", "Shen Gao", "Xiangliang Zhang", "Dongyan Zhao", "Rui Yan" ],
    "emails" : [ "xy-chen@pku.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6068–6077\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6068"
    }, {
      "heading" : "1 Introduction",
      "text" : "The related work section generation task aims to automatically generate a summary of the most relevant works in a specific research area, which can help researchers to familiarize themselves with the state of the art in the field. Several methods (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019) have been proposed to study how to obtain the related work section automatically by\n∗Corresponding author. 1https://github.com/iriscxy/\nrelatedworkgeneration\nextracting important sentences from multiple original papers. However, extractive approaches lack the sophisticated abilities that are crucial to highquality summarization such as paraphrasing and generalization, and often lead to a related work section with poor coherence and readability (See et al., 2017; Hsu et al., 2018). For example, as shown in Table 1, the extracted sentences share the pattern “We find...” as the subject of sentences, which, as a matter of fact, refer to different authors. On the contrary, the abstractive related work in Table 1 reveals that the works are conducted by different scholars. It also has conjunction words such as “Furthermore” and “However”, which can explain the logical relationship between the cited works, and thus form an elegant narration. Hence, in this paper, we target on the abstractive related work generation task, which generates a related work including novel words and phrases not copied from the source text.\nThere are two main challenges in this task: (1) the related work should summarize the contribution of each paper, and (2) explain the relationship between different papers such as parallel, turning, and progressive relation, so as to introduce them in a logical order. While existing summarization models can address the first problem, they do not target at comparing and explaining the relationship between these articles. Hence, to tackle the above challenges, we propose a Relation-aware Related work Generator (RRG), which generates an abstractive related work given multiple scientific papers in the same research area. Firstly, we encode the multiple input articles in a hierarchical manner, obtaining the overall representation for each document. Then, we propose a relationaware multi-document encoder that relates multiple input documents in a relation graph. In the training process, the relation graph and the document representation interact and are refined iteratively, complementing each other. Finally, in the decoder part, we utilize the relation graph information to assist the decoding process, where the model learns to decide whether to pay attention to the input documents or the relationship between them.\nTo evaluate our model, we introduce two largescale related work generation datasets, which are composed of related work sections and their corresponding papers. Extensive experimental results show that RRG outperforms several strong baselines in terms of ROUGE metrics and human evaluations on both datasets.\nIn summary, our contributions include: •We address an abstractive related work generation task, which aims to generate an abstractive related work with novel words and phrases. •We propose a relation-aware multi-document encoder that relates one of the multiple input documents to another, and establishes a relation graph storing the dependency between documents. • We contribute two public large-scale related work generation datasets that are beneficial for the community."
    }, {
      "heading" : "2 Related Work",
      "text" : "We discuss the related work on related work generation and multi-document summarization.\nRelated Work Generation. Most of the previous related work section generation methods are extractive. For example, Hoang and Kan (2010) take in a set of keywords arranged in a hierarchical\nfashion to drive the creation of an extractive related work. Later, (Hu and Wan, 2014) first exploits a Probabilistic Latent Semantic Analysis (PLSA) model to split the sentence set of multiple reference papers into different topic-biased parts, and then applies regression models to learn the importance of the sentences. Finally, it employs an optimization framework to generate the related work section. Chen and Zhuge (2019) propose to first construct a minimum Steiner tree of the keywords. Then the summary is generated by extracting the sentences from the papers that cite the reference papers of the paper being written to cover the Steiner tree.\nHowever, abstractive approaches on related work generation have met with limited success. Apart from the lack of sufficient training data, neural models also face the challenge of identifying the logic relationship between multiple input documents.\nMulti-document Summarization. The multidocument summarization task aims to cover the key shared relevant information among all the documents while avoiding redundancy (Goldstein et al., 2000). Existing multi-document summarization methods are mostly extractive (Christensen et al., 2013; Parveen and Strube, 2014; Ma et al., 2016; Chu and Liu, 2018). For example, Wang et al. (2020) present a heterogeneous graph-based neural network which contains semantic nodes of different granularity levels apart from sentences. Recently, a vast majority of the literature is dedicated to abstractive multi-document summarization. Lu et al. (2020) propose a large-scale multi-document summarization dataset created from scientific articles. Jin et al. (2020) propose a multi-granularity interaction network for extractive and abstractive approaches. Li et al. (2020a) develop a neural abstractive multi-document summarization model which leverages explicit graph representations of documents to guide the summary generation process.\nWhile the multi-document summarization task aims to extract information shared by multiple documents, related work generation aims to compare and introduce the cited works in logic order."
    }, {
      "heading" : "3 Related Work Generation Dataset",
      "text" : "Since there are no public large-scale related work generation datasets, we collect two survey datasets composed of related work sections and their corresponding papers. The first dataset is collected from S2ORC (Lo et al., 2020), which consists of papers in multiple domains (physics, math, computer sci-\nence, etc.), and the second is Delve (Akujuobi and Zhang, 2017), which consists of computer science papers. All the papers in each of these two datasets form a large connected citation graph, allowing us to make full use of the citation relationships between papers.\nDataset Preprocessing. For each case, the generation target is a paragraph with more than two citations, as a comprehensive related work usually compares multiple works under the same topic. The abstract of each cited paper is regarded as input, considering that the main idea of a cited paper is described in its abstract. We then conduct a human evaluation to examine the dataset quality. Concretely, we sample 200 cases from both datasets and ask three annotators to state how well they agree with the following statement, on a scale of one to three (disagree, neutral, agree): the related work can be partly generated based on the given abstracts of the cited papers. The evaluation is conducted on the Amazon Mechanical Turk, which has been employed in a variety of NLP tasks including summarization (Liu and Lapata, 2019a), question answering (Gan and Ng, 2019), and dialog system (Li et al., 2020b). The result shows that 94.5% cases win 3 scores, while only 3.5% cases obtain 1 score. This demonstrates the good quality of the datasets.\nStatistics. Table 2 compares Delve and S2ORC to other public datasets including DUC data from 2003 and 2004, TAC 2011 data, and Multi-News, which are typically used in multi-document settings. We also list the statistics of a recent related work generation dataset RWS, which is proposed by Chen and Zhuge (2019). The total number of collected samples for the S2ORC and Delve is about 150,000 and 80,000, respectively. It can be seen that Multi-News is most similar to our dataset due to its large-scale. However, the average number of documents per case in Multi-News is smaller\nthan ours."
    }, {
      "heading" : "4 Problem Formulation",
      "text" : "Before presenting our approach for related work generation, we first introduce our problem formulation and used notations.\nTo begin with, for a set of relevant papers D = (d1, d2, · · · , dN ) in a specific area, where di denotes a paper, we assume there is a corresponding related work Y = (y1, y2, · · · , yT ). N is the number of relevant papers, and di = (wi1, w i 2, · · · , wiNi), where w i j is the j-th word in i-th paper, and Ni is the number of words in di. T is the number of words in a related work. Given the multiple papers D, our model generates a related work Ŷ = (ŷ1, ŷ2, · · · , ŷT̂ ). Finally, we use the difference between generated related work Ŷ and ground truth related work Y as the training signal to optimize the model parameters."
    }, {
      "heading" : "5 The Proposed RRG Model",
      "text" : ""
    }, {
      "heading" : "5.1 Overview",
      "text" : "In this section, we introduce the Relation-aware Related work Generator (RRG) in detail. An overview of RRG is shown in Figure 1, which has three main parts: •Hierarchical Encoder reads multiple input documents and learns the multi-level representations for words and documents. • Relationship Modeling relates one paper to another and obtains their relationship graph. • Related Work Generator produces the abstractive related work by attending to the hierarchical representations and the relation graph between documents."
    }, {
      "heading" : "5.2 Hierarchical Encoder",
      "text" : "To begin with, each input wij is converted into the vector representation êij by the learned embeddings.\nWe then assign positional encoding (PE) to indicate the position of the word wij where two positions need to be considered, namely document index i and word index j. We concatenate the position embedding PEi, PEj to obtain the final position embedding pij . The definition of positional encoding is consistent with the Transfomer (Vaswani et al., 2017). The input word representation eij is obtained by adding embedding êij and position embedding pij .\nWe then perform multi-head self-attention across the word representations in the same document to obtain the contextual word representation hwij :\nhwij = MHAM(eij , e i ∗), (1)\nwhere MHAM denotes the Multi-head Attention Module (Vaswani et al., 2017), and ∗ denotes index j ∈ (1, Ni). Concretely, The first input is for query and the second input is for keys and values. Each output element, hwij , is computed as the weighted sum of linearly transformed input values:\nhwij = ∑Ni l=1 α i j,l ( eilW V w ) , (2)\nαij,l = exp\n( βij,l ) ∑Ni\nk=1 exp ( βij,k ) . (3) Here, βij,l is computed using a compatibility function that compares two input elements:\nβij,l =\n( eijW Q w ) ( eilW K w )T √ d , (4)\nwhere d is the hidden dimension, and WQw ,WKw ,W V w are parameter matrices. From the word-level representation we obtain the overall representation for each document:\nh0di = meanpool ({ hwi1 , · · · , hwiNi }) . (5)"
    }, {
      "heading" : "5.3 Relationship Modeling",
      "text" : "The document representation h0di does not contain cross-document information, thus, it cannot learn richer structural dependencies among textual units. In this subsection, we introduce a novel graphbased Relationship Modeling (RM), which not only allows sharing information across multiple documents but also models the logic dependency between documents. Note that it is impossible to explicitly list all the relationships between documents because the relationships vary from document pair to pair depending on the document content, and the content of documents is unlimited. Hence, we model the relationships hidden vectors and let the model capture such diverse relationships by the hidden vectors. Concretely, since the relationship graph is constructed based on the representation of each document, while a comprehensive document representation should consider its relationship with other documents. These two processes complement each other. Hence, our RM module is an iterative module, which has a stack of L identical layers. In each layer, we iteratively update the relationship graph, and then fuse the information from the graph to the document representation, as shown in Figure 2.\nFor a start, the relation edge in our graph is initialized by the document representation:\nh0ri,j = MLPa([h 0 di ;h0dj ]), (6)\nwhere MLP is a multi-layer perceptron, and [; ] is the concatenation operation.\nIn each iteration, we first propose a Relation Graph Updater (RGU) to renew the graph based on the polished document representation so far (shown in the right part of Figure 2):\nhlri,j = RGU(h l−1 ri,j , h l−1 d∗ ). (7)\nHere, ∗ denotes index i ∈ (1, N), meaning that all document representations will be involved in updating the relation graph. Concretely, RGU first aggregates the information from both the previous graph hl−1ri,j and the document states h l−1 d∗\nfrom the last layer, using a multi-head attention (MHAM introdced in §5.2). The input for queryQ is hl−1ri,j , and input for key K and value V is hl−1d∗ . The output intermediate graph states sl−1i,j are further encoded using a feed-forward layer and then merged with the intermediate hidden states hl−1ri,j using a residual connection and layer norm.\nWe summarize the procedure below:\nsl−1i,j = MHAM(h l−1 ri,j , h l−1 d∗ ), cl−1i,j = tanh(W l−1 a h l−1 ri,j +W l−1 b s l−1 i,j ), zl−1i,j = sigmoid(W l−1 c h l−1 ri,j +W l−1 d s l−1 i,j ), hlri,j = (1− z l−1 i,j ) c l−1 i,j + z l−1 i,j h l−1 ri,j ,\nwhere denotes Hadamard product, and cl−1i,j is the internal cell state. zl−1i,j is the update gate that controls which information to retain from the previous\nmemory state. This update strategy is conceptually similar to long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997). It differs in that multi-head attention is used and thus multiple graph slots are supported instead of a single one in LSTM, which gives it a higher capacity of modeling complex relations.\nNext, the updated graph is fused in the Relationaware Attention Module (RAM) to update the document representation:\nhldi = RAM(h l−1 di , hl−1d∗ , h l ri,∗). (8)\nRAM is similar to MHAM, where hl−1di is for query, hl−1d∗ is for key and value. However, there are two changes in Equation 2 and Equation 4. Specifically, we modify Equation 2 to propagate edge information to the sub-layer output:\nhldi = ∑N j=1 α l−1,r i,j ( hl−1dj W V r + h l ri,j ) . (9)\nIn this way, the representation of each document is more comprehensive, consisting of its relation dependency information with other documents. What is more, when deciding the weight of each edge, i.e., βl−1,ri,j , we also incorporate relation edge information, since close relationships such as succession or transition can have a great impact on edge weight. Concretely, Equation 4 is changed to:\nβl−1,ri,j =\n( hl−1di W Q r )( hl−1dj W K r + h l ri,j )T √ d .\n(10) We summarize the whole relationship modeling\nprocess as:\nhLd , h L r = RM(h 0 d, h 0 r). (11)\nFor brevity, we omit the subscript L in the following section."
    }, {
      "heading" : "5.4 Related Work Generator",
      "text" : "To generate a consistent and informative summary, we propose an RNN-based decoder following (Chen et al., 2019; Gao et al., 2019) that incorporates the outputs of the hierarchical encoder and the relationship graph as illustrated in Figure 1.\nOur decoder is a single-layer unidirectional LSTM. At each step t, the decoder updates the hidden state from st−1 to st:\nst = LSTM ( st−1, [ cwt−1, c d t−1, e(yt−1) ]) .\n(12)\nFollowing previous works (Bahdanau et al., 2015), we employ an attention mechanism to compute the attention distribution over the source words in the sequence-to-sequence structure:\nαw ′,i\nt,j =W g a tanh ( W gb st +W g c hwij ) , (13)\nαw,it,j = exp ( αw ′,i t,j ) / ∑Ni l=1 exp ( αw ′,i t,l ) , (14)\ncwt = ∑N i=1 ∑Ni j=1 α w,i t,j hwij , (15)\nwhere cwt denotes word context vector. Similarly, we extend the attention mechanism to document level:\nαd ′ t,i =W g d tanh ( W ge st +W g f hdi ) , (16)\nαdt,i = exp ( αd ′ t,i ) / ∑N l=1 exp ( αd ′ t,l ) , (17)\ncdt = ∑N i=1 α d t,ihdi . (18)\nThe encoded relationship information is also important for facilitating the transition introduction in the related work, and the specific information in the graph that is needed at each step depends on which document is being introduced. Hence, we employ the document-level attention weights in Equation 17 to read the relationship graph:\nhrmi = meanpool ({ hri,1 , · · · , hri,N }) ,\ncrt = ∑N i=1 α d t,ihrmi .\n(19)\nFinally, an output projection layer is applied to get the final generating distribution P vt over vocabulary, as shown in Equation 20:\nP vt = softmax(MLPc[st; c w t ; c d t ; c r t ]). (20)\nOur objective function is the negative log likelihood of the target word yt:\nL = − ∑T\nt=1 logP v t (yt). (21)\nIn order to handle the out-of-vocabulary (OOV) problem, we equip our decoder with a pointer network (Gu et al., 2016; See et al., 2017). This process is the same as the model described in (See et al., 2017), thus, is omit here due to limited space."
    }, {
      "heading" : "6 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "6.1 Baselines",
      "text" : "To evaluate the performance of our proposed model, we compare it with the following baselines: Extractive Methods:\n(1) LEAD: selects the first sentence of each document as the summary as a baseline. (2) TextRank (Mihalcea and Tarau, 2004): is a multi-document graph-based ranking model. (3) BertSumEXT (Liu and Lapata, 2019b): is an extractive summarization model with BERT. (4) MGSum-ext (Jin et al., 2020): is a multi-granularity interaction network for extractive multi-document summarization. Abstractive Methods:\n(1) PTGen+Cov: combines the sequence-tosequence framework with copy and coverage mechanism in summarization task (See et al., 2017). (2) TransformerABS: is an abstractive summarization model based on the Transformer (Vaswani et al., 2017). (3) BertSumABS (Liu and Lapata, 2019b): is an abstractive summarization network built on BERT. (4) MGSum-abs (Jin et al., 2020): is a multi-granularity interaction network for abstractive multi-document summarization. (5) GS (Li et al., 2020a): is a neural abstractive multidocument summarization model that leverages well-known graphs to produce abstractive summaries. We use the TF-IDF graph as the input graph."
    }, {
      "heading" : "6.2 Implementation Details",
      "text" : "We implement our model in TensorFlow (Abadi et al., 2016) on an NVIDIA GTX 1080 Ti GPU. For all the neural models, we truncate the input articles to 500 tokens in the following way: for each example with S source input documents, we take the first 500/S tokens from each source document. The maximum document number is set to 5. The minimum decoding step is 50, and the maximum step is 100. The word embedding dimension is set to 128 and the number of hidden units is 256. We initialize all of the parameters randomly using a Gaussian distribution. The batch size is set to 16, and we limit the vocabulary size to 50K. We use Adagrad optimizer (Duchi et al., 2010) as our optimizing algorithm. We also apply gradient clipping (Pascanu et al., 2013) with a range of [−2, 2] during training. For the testing, we employ beam search with a beam size of 4 to generate more fluent summaries.\nTo obtain the extractive oracle, since it is computationally expensive to find a globally optimal subset of sentences that maximizes the ROUGE score, we employ a greedy approach, where we add one sentence at a time incrementally to the\nsummary, such that the ROUGE score of the current set of selected sentences is maximized with respect to the entire gold summary."
    }, {
      "heading" : "7 Experimental Results",
      "text" : ""
    }, {
      "heading" : "7.1 Automatic Evaluation",
      "text" : "Following Chen et al. (2018), we evaluate summarization quality using ROUGE F1 (Lin, 2004). We report unigram and bigram overlap (ROUGE-1 and ROUGE-2) to assess the informativeness and the longest common subsequence (ROUGE-L) as a means of the assessing fluency.\nTable 3 summarizes our results. The first block in the table includes extractive systems, and the second block includes abstractive baselines. As can be seen, abstractive models generally outperform extractive ones, especially in terms of ROUGE-L scores. We attribute this result to the observation that the gold related work of this dataset tends to use novel word combinations to summarize the original input documents, which demonstrates the necessity of solving the abstractive related work generation task. Among abstractive models, surprisingly, BertSumABS does not perform as well as other state-of-the-art baselines. This is probably because BERT does not fit well on scholar data that have technical terms. Finally, our model RRG gains an improvement of 1.83 (1.08) points compared with BertSumABS, 1.54 (0.83) points compared with GS on ROUGE-1 on S2ORC (Delve),\nverifying the effectiveness of our RRG. Table 3 also summarizes ablation studies aiming to assess the contribution of individual components in our RRG model. The results confirm that the encoding paragraph position in addition to token position within each paragraph is beneficial (see row w/o PP), as well as relationship modeling (row w/o RM). Updating the relation graph also helps the summarization process, where removing the update mechanism causes ROUGE-L drop by 0.86 (0.99) (row w/o Upd) on S2ORC (Delve) dataset."
    }, {
      "heading" : "7.2 Human Evaluation",
      "text" : "We also assessed the generated results by eliciting human judgments on 30 randomly selected test instances from Delve dataset. Our first evaluation study quantified the degree to which summarization models can retain the key information following a question-answering paradigm (Liu and Lapata, 2019a). We created a set of questions based on the gold-related work and examined whether participants were able to answer these questions by reading generated related works. The principle\nfor writing a question is that the information to be answered is about factual description, and is necessary for a related work section. Two Ph.D. students majoring in computer science (also the authors) wrote five questions independently for each sampled ground truth related work since the Delve dataset also consists of computer science papers. Then they together selected the common questions as the final questions that they both consider to be important. Finally we obtain 67 questions, where correct answers are marked with 1 and 0 otherwise. Examples of questions and their answers are given in Table 5. Our second evaluation study assessed the overall quality of the related works by asking participants to score them by taking into account the following criteria: Informativeness (does the related work convey important facts about the topic in question?), Coherence (is the related work coherent and grammatical?), and Succinctness (does the related work avoid repetition?). The rating score ranges from 1 to 3, with 3 being the best. For both evaluation metrics, a model’s score is the average of all scores.\nBoth evaluations were conducted on the Amazon Mechanical Turk platform with 3 responses per hit. Participants evaluated related works produced by the BertSumABS, MGSum-abs, GS, and our RRG. All evaluated models are those who achieved the best performance in automatic evaluations. Table 4 lists the average scores of each model, showing that RRG outperforms other baseline models among all\nmetrics. We calculate the kappa statistics in terms of informativeness, coherence, and succinctness, and the scores are 0.38, 0.29, 0.34, respectively. To verify the significance of these results, we also conduct the paired student t-test between our model and GS (the row with shaded background). We obtain a p-value of 6× 10−6, 5× 10−9, and 7× 10−7 for informativeness, coherence, and succinctness. Examples of system output are provided in Table 5. We can see that related work generated by RRG correctly captures the relationship between papers [1,2] and [3,4], and successfully summarizes the contributions of corresponding papers. Among baselines, MGSum-ext fails to connect the cited papers in logic. MGSum-abs and GS fail to capture the transitional relationship between the first two works and the last two works."
    }, {
      "heading" : "7.3 Analysis of Relation Graph",
      "text" : "To fully investigate what is stored by the relation graph, we draw a heatmap of the graph for the case in Table 5. Since the edge in relation graph is a vector containing semantic meaning, which cannot be directly explained, we use the edge between paper [2] and [3] as a benchmark and compute the cosine similarity between the benchmark and other relation edges. Dark color means that the relationship between the corresponding two papers is similar with edge [2]-[3], and vice versa. We already know that there is a transitional relationship between [2] and [3], so if an edge has a high cosine similarity\nwith [2]-[3] pair, then the two papers on this edge also form a transitional relationship. As shown in Figure 3, relationship vectors between paper [1], [2] with [4] are relatively more similar to [2]-[3] pair. This is consistent with the fact that paper [1] and [2] are parallel with each other, while they form a transitional relationship compared with [4]. Note that the heatmap is not symmetrical because our relation graph is a bipartite graph."
    }, {
      "heading" : "8 Conclusion",
      "text" : "In this paper, we conceptualized the abstractive related work generation task as a machine learning problem. We proposed a new model that is able to encode multiple input documents hierarchically and model the latent relations across them in a relation graph. We also come up with two public large-scale related work generation datasets. Experimental results show that our model produces related works that are both fluent and informative, outperforming competitive systems by a wide margin. In the future, we would like to apply our model to abstract generation and paper generation tasks."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank the anonymous reviewers for their constructive comments. This work was supported by the National Key Research and Development Program of China (No. 2017YFC0804001), the National Science Foundation of China (NSFC No. 61876196 and NSFC No. 61672058). Rui Yan is partially supported as a Young Fellow of Beijing Institute of Artificial Intelligence (BAAI).\nEthics Impact\nIn this paper, we propose a relation-aware related work generator which aims to provide researchers with an overview of the specific research area by\nsummarizing the related works and introducing them in a logical order. The positive impact lies in that it can help improve the work efficiency of scholars. The negative impact may be that in some extreme cases, the system may not be able to give an accurate and faithful related work, which can be misleading. Hence, in such situation, scholars should not directly employ the generated related work as the final edition. Instead, they can rely on this system to give insightful related work suggestion."
    } ],
    "references" : [ {
      "title" : "Tensorflow: A system for large-scale machine learning",
      "author" : [ "Martı́n Abadi", "Paul Barham", "Jianmin Chen", "Zhifeng Chen", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Geoffrey Irving", "Michael Isard" ],
      "venue" : null,
      "citeRegEx" : "Abadi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Abadi et al\\.",
      "year" : 2016
    }, {
      "title" : "Delve: A dataset-driven scholarly search and analysis system",
      "author" : [ "Uchenna Akujuobi", "X. Zhang." ],
      "venue" : "ACM SIGKDD Explorations Newsletter, 19:36–46.",
      "citeRegEx" : "Akujuobi and Zhang.,? 2017",
      "shortCiteRegEx" : "Akujuobi and Zhang.",
      "year" : 2017
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Automatic generation of related work through summarizing citations",
      "author" : [ "Jingqiang Chen", "Hai Zhuge." ],
      "venue" : "Concurr. Comput. Pract. Exp., 31.",
      "citeRegEx" : "Chen and Zhuge.,? 2019",
      "shortCiteRegEx" : "Chen and Zhuge.",
      "year" : 2019
    }, {
      "title" : "Learning towards abstractive timeline summarization",
      "author" : [ "Xiuying Chen", "Zhangming Chan", "Shen Gao", "MengHsuan Yu", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "IJCAI.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Iterative document representation learning towards summarization with polishing",
      "author" : [ "Xiuying Chen", "Shen Gao", "Chongyang Tao", "Yan Song", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards coherent multi-document summarization. In Proceedings of the 2013 conference of the North American chapter of the association for computational linguistics: Human language",
      "author" : [ "Janara Christensen", "Stephen Soderland", "Oren Etzioni" ],
      "venue" : null,
      "citeRegEx" : "Christensen et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Christensen et al\\.",
      "year" : 2013
    }, {
      "title" : "Unsupervised neural multi-document abstractive summarization",
      "author" : [ "Eric Chu", "Peter J. Liu." ],
      "venue" : "ArXiv, abs/1810.05739.",
      "citeRegEx" : "Chu and Liu.,? 2018",
      "shortCiteRegEx" : "Chu and Liu.",
      "year" : 2018
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John C. Duchi", "Elad Hazan", "Yoram Singer." ],
      "venue" : "J. Mach. Learn. Res., 12:2121–2159.",
      "citeRegEx" : "Duchi et al\\.,? 2010",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2010
    }, {
      "title" : "Improving the robustness of question answering systems to question paraphrasing",
      "author" : [ "Wee Chung Gan", "H.T. Ng." ],
      "venue" : "ACL.",
      "citeRegEx" : "Gan and Ng.,? 2019",
      "shortCiteRegEx" : "Gan and Ng.",
      "year" : 2019
    }, {
      "title" : "How to write summaries with patterns? learning towards abstractive summarization through prototype editing",
      "author" : [ "Shen Gao", "Xiuying Chen", "Piji Li", "Zhangming Chan", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi-document summarization by sentence extraction",
      "author" : [ "Jade Goldstein", "Vibhu O Mittal", "Jaime G Carbonell", "Mark Kantrowitz." ],
      "venue" : "NAACL-ANLP 2000 Workshop: Automatic Summarization.",
      "citeRegEx" : "Goldstein et al\\.,? 2000",
      "shortCiteRegEx" : "Goldstein et al\\.",
      "year" : 2000
    }, {
      "title" : "Incorporating copying mechanism in sequence-to-sequence learning",
      "author" : [ "Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor OK Li." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Gu et al\\.,? 2016",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2016
    }, {
      "title" : "Towards automated related work summarization",
      "author" : [ "Cong Duy Vu Hoang", "Min-Yen Kan." ],
      "venue" : "Coling 2010: Posters, pages 427–435.",
      "citeRegEx" : "Hoang and Kan.,? 2010",
      "shortCiteRegEx" : "Hoang and Kan.",
      "year" : 2010
    }, {
      "title" : "Long shortterm memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber." ],
      "venue" : "Neural Computation, 9:1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "A unified model for extractive and abstractive summarization using inconsistency loss",
      "author" : [ "Wan-Ting Hsu", "Chieh-Kai Lin", "Ming-Ying Lee", "Kerui Min", "Jing Tang", "Min Sun." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Hsu et al\\.,? 2018",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2018
    }, {
      "title" : "Automatic generation of related work sections in scientific papers: an optimization approach",
      "author" : [ "Yue Hu", "Xiaojun Wan." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1624–1633.",
      "citeRegEx" : "Hu and Wan.,? 2014",
      "shortCiteRegEx" : "Hu and Wan.",
      "year" : 2014
    }, {
      "title" : "Multi-granularity interaction network for extractive and abstractive multi-document summarization",
      "author" : [ "Hanqi Jin", "Tianming Wang", "Xiaojun Wan." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6244–",
      "citeRegEx" : "Jin et al\\.,? 2020",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2020
    }, {
      "title" : "Leveraging graph to improve abstractive multi-document summarization",
      "author" : [ "Wei Li", "Xinyan Xiao", "Jiachen Liu", "Hua Wu", "Haifeng Wang", "Junping Du." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Li et al\\.,? 2020a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Endto-end trainable non-collaborative dialog system",
      "author" : [ "Yu Li", "Kun Qian", "Weiyan Shi", "Z. Yu." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Li et al\\.,? 2020b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text summarization branches out, pages 74–81.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Hierarchical transformers for multi-document summarization",
      "author" : [ "Yang Liu", "Mirella Lapata." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5070– 5081.",
      "citeRegEx" : "Liu and Lapata.,? 2019a",
      "shortCiteRegEx" : "Liu and Lapata.",
      "year" : 2019
    }, {
      "title" : "Text summarization with pretrained encoders",
      "author" : [ "Yang Liu", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages",
      "citeRegEx" : "Liu and Lapata.,? 2019b",
      "shortCiteRegEx" : "Liu and Lapata.",
      "year" : 2019
    }, {
      "title" : "S2ORC: The Semantic Scholar Open Research Corpus",
      "author" : [ "Kyle Lo", "Lucy Lu Wang", "Mark Neumann", "Rodney Kinney", "Daniel S Weld." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969–4983.",
      "citeRegEx" : "Lo et al\\.,? 2020",
      "shortCiteRegEx" : "Lo et al\\.",
      "year" : 2020
    }, {
      "title" : "Multixscience: A large-scale dataset for extreme multidocument summarization of scientific articles",
      "author" : [ "Yao Lu", "Yue Dong", "Laurent Charlin." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Lu et al\\.,? 2020",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2020
    }, {
      "title" : "An unsupervised multi-document summarization framework based on neural document model",
      "author" : [ "Shulei Ma", "Zhi-Hong Deng", "Yunlun Yang." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Techni-",
      "citeRegEx" : "Ma et al\\.,? 2016",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2016
    }, {
      "title" : "TextRank: Bringing order into text",
      "author" : [ "Rada Mihalcea", "Paul Tarau." ],
      "venue" : "Proceedings of the 2004 conference on empirical methods in natural language processing, pages 404–411.",
      "citeRegEx" : "Mihalcea and Tarau.,? 2004",
      "shortCiteRegEx" : "Mihalcea and Tarau.",
      "year" : 2004
    }, {
      "title" : "Multidocument summarization using bipartite graphs",
      "author" : [ "Daraksha Parveen", "Michael Strube." ],
      "venue" : "Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 15–24.",
      "citeRegEx" : "Parveen and Strube.,? 2014",
      "shortCiteRegEx" : "Parveen and Strube.",
      "year" : 2014
    }, {
      "title" : "On the difficulty of training recurrent neural networks",
      "author" : [ "Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio." ],
      "venue" : "International conference on machine learning, pages 1310–1318. PMLR.",
      "citeRegEx" : "Pascanu et al\\.,? 2013",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2013
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J Liu", "Christopher D Manning." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073–",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "NIPS, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Heterogeneous graph neural networks for extractive document summarization",
      "author" : [ "Danqing Wang", "Pengfei Liu", "Yining Zheng", "Xipeng Qiu", "Xuan-Jing Huang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "Several methods (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019) have been proposed to study how to obtain the related work section automatically by",
      "startOffset" : 16,
      "endOffset" : 77
    }, {
      "referenceID" : 16,
      "context" : "Several methods (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019) have been proposed to study how to obtain the related work section automatically by",
      "startOffset" : 16,
      "endOffset" : 77
    }, {
      "referenceID" : 3,
      "context" : "Several methods (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019) have been proposed to study how to obtain the related work section automatically by",
      "startOffset" : 16,
      "endOffset" : 77
    }, {
      "referenceID" : 29,
      "context" : "However, extractive approaches lack the sophisticated abilities that are crucial to highquality summarization such as paraphrasing and generalization, and often lead to a related work section with poor coherence and readability (See et al., 2017; Hsu et al., 2018).",
      "startOffset" : 228,
      "endOffset" : 264
    }, {
      "referenceID" : 15,
      "context" : "However, extractive approaches lack the sophisticated abilities that are crucial to highquality summarization such as paraphrasing and generalization, and often lead to a related work section with poor coherence and readability (See et al., 2017; Hsu et al., 2018).",
      "startOffset" : 228,
      "endOffset" : 264
    }, {
      "referenceID" : 16,
      "context" : "Later, (Hu and Wan, 2014) first exploits a Probabilistic Latent Semantic Analysis (PLSA) model to split the sentence set of multiple reference papers into different topic-biased parts, and then applies regression models to learn the importance of the sentences.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 6,
      "context" : "Existing multi-document summarization methods are mostly extractive (Christensen et al., 2013; Parveen and Strube, 2014; Ma et al., 2016; Chu and Liu, 2018).",
      "startOffset" : 68,
      "endOffset" : 156
    }, {
      "referenceID" : 27,
      "context" : "Existing multi-document summarization methods are mostly extractive (Christensen et al., 2013; Parveen and Strube, 2014; Ma et al., 2016; Chu and Liu, 2018).",
      "startOffset" : 68,
      "endOffset" : 156
    }, {
      "referenceID" : 25,
      "context" : "Existing multi-document summarization methods are mostly extractive (Christensen et al., 2013; Parveen and Strube, 2014; Ma et al., 2016; Chu and Liu, 2018).",
      "startOffset" : 68,
      "endOffset" : 156
    }, {
      "referenceID" : 7,
      "context" : "Existing multi-document summarization methods are mostly extractive (Christensen et al., 2013; Parveen and Strube, 2014; Ma et al., 2016; Chu and Liu, 2018).",
      "startOffset" : 68,
      "endOffset" : 156
    }, {
      "referenceID" : 23,
      "context" : "The first dataset is collected from S2ORC (Lo et al., 2020), which consists of papers in multiple domains (physics, math, computer sci-",
      "startOffset" : 42,
      "endOffset" : 59
    }, {
      "referenceID" : 21,
      "context" : "The evaluation is conducted on the Amazon Mechanical Turk, which has been employed in a variety of NLP tasks including summarization (Liu and Lapata, 2019a), question answering (Gan and Ng, 2019), and dialog system (Li et al.",
      "startOffset" : 133,
      "endOffset" : 156
    }, {
      "referenceID" : 9,
      "context" : "The evaluation is conducted on the Amazon Mechanical Turk, which has been employed in a variety of NLP tasks including summarization (Liu and Lapata, 2019a), question answering (Gan and Ng, 2019), and dialog system (Li et al.",
      "startOffset" : 177,
      "endOffset" : 195
    }, {
      "referenceID" : 19,
      "context" : "The evaluation is conducted on the Amazon Mechanical Turk, which has been employed in a variety of NLP tasks including summarization (Liu and Lapata, 2019a), question answering (Gan and Ng, 2019), and dialog system (Li et al., 2020b).",
      "startOffset" : 215,
      "endOffset" : 233
    }, {
      "referenceID" : 30,
      "context" : "The definition of positional encoding is consistent with the Transfomer (Vaswani et al., 2017).",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 30,
      "context" : "where MHAM denotes the Multi-head Attention Module (Vaswani et al., 2017), and ∗ denotes index j ∈ (1, Ni).",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 14,
      "context" : "This update strategy is conceptually similar to long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997).",
      "startOffset" : 78,
      "endOffset" : 112
    }, {
      "referenceID" : 4,
      "context" : "To generate a consistent and informative summary, we propose an RNN-based decoder following (Chen et al., 2019; Gao et al., 2019) that incorporates the outputs of the hierarchical encoder and the relationship graph as illustrated in Figure 1.",
      "startOffset" : 92,
      "endOffset" : 129
    }, {
      "referenceID" : 10,
      "context" : "To generate a consistent and informative summary, we propose an RNN-based decoder following (Chen et al., 2019; Gao et al., 2019) that incorporates the outputs of the hierarchical encoder and the relationship graph as illustrated in Figure 1.",
      "startOffset" : 92,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "6073 Following previous works (Bahdanau et al., 2015), we employ an attention mechanism to compute the attention distribution over the source words in the sequence-to-sequence structure:",
      "startOffset" : 30,
      "endOffset" : 53
    }, {
      "referenceID" : 12,
      "context" : "In order to handle the out-of-vocabulary (OOV) problem, we equip our decoder with a pointer network (Gu et al., 2016; See et al., 2017).",
      "startOffset" : 100,
      "endOffset" : 135
    }, {
      "referenceID" : 29,
      "context" : "In order to handle the out-of-vocabulary (OOV) problem, we equip our decoder with a pointer network (Gu et al., 2016; See et al., 2017).",
      "startOffset" : 100,
      "endOffset" : 135
    }, {
      "referenceID" : 29,
      "context" : "This process is the same as the model described in (See et al., 2017), thus, is omit here due to limited space.",
      "startOffset" : 51,
      "endOffset" : 69
    }, {
      "referenceID" : 26,
      "context" : "(2) TextRank (Mihalcea and Tarau, 2004): is a multi-document graph-based ranking model.",
      "startOffset" : 13,
      "endOffset" : 39
    }, {
      "referenceID" : 22,
      "context" : "(3) BertSumEXT (Liu and Lapata, 2019b): is an extractive summarization model with BERT.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 17,
      "context" : "(4) MGSum-ext (Jin et al., 2020): is a multi-granularity interaction network for extractive multi-document summarization.",
      "startOffset" : 14,
      "endOffset" : 32
    }, {
      "referenceID" : 29,
      "context" : "(1) PTGen+Cov: combines the sequence-tosequence framework with copy and coverage mechanism in summarization task (See et al., 2017).",
      "startOffset" : 113,
      "endOffset" : 131
    }, {
      "referenceID" : 30,
      "context" : "rization model based on the Transformer (Vaswani et al., 2017).",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 22,
      "context" : "(3) BertSumABS (Liu and Lapata, 2019b): is an abstractive summarization network built on BERT.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 17,
      "context" : "(4) MGSum-abs (Jin et al., 2020): is a multi-granularity interaction network for abstractive multi-document summarization.",
      "startOffset" : 14,
      "endOffset" : 32
    }, {
      "referenceID" : 18,
      "context" : "(5) GS (Li et al., 2020a): is a neural abstractive multidocument summarization model that leverages well-known graphs to produce abstractive summaries.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "We implement our model in TensorFlow (Abadi et al., 2016) on an NVIDIA GTX 1080 Ti GPU.",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 8,
      "context" : "We use Adagrad optimizer (Duchi et al., 2010) as our optimizing algorithm.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 28,
      "context" : "We also apply gradient clipping (Pascanu et al., 2013) with a range of [−2, 2] during training.",
      "startOffset" : 32,
      "endOffset" : 54
    }, {
      "referenceID" : 20,
      "context" : "(2018), we evaluate summarization quality using ROUGE F1 (Lin, 2004).",
      "startOffset" : 57,
      "endOffset" : 68
    }, {
      "referenceID" : 21,
      "context" : "Our first evaluation study quantified the degree to which summarization models can retain the key information following a question-answering paradigm (Liu and Lapata, 2019a).",
      "startOffset" : 150,
      "endOffset" : 173
    } ],
    "year" : 2021,
    "abstractText" : "Given a set of related publications, related work section generation aims to provide researchers with an overview of the specific research area by summarizing these works and introducing them in a logical order. Most of existing related work section generation models follow the inflexible extractive style, which directly extract sentences from multiple original papers to form a related work discussion. Hence, in this paper, we propose a Relationaware Related work Generator (RRG), which generates an abstractive related work section from multiple scientific papers in the same research area. Concretely, we propose a relationaware multi-document encoder that relates one document to another according to their content dependency in a relation graph. The relation graph and the document representation interact and are refined iteratively, complementing each other in the training process. We also contribute two public datasets composed of related work sections and their corresponding papers1. Extensive experiments on the two datasets show that the proposed model brings substantial improvements over several strong baselines. We hope that this work will promote advances in related work section generation task.",
    "creator" : "LaTeX with hyperref"
  }
}