{
  "name" : "2021.acl-long.10.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Generalising Multilingual Concept-to-Text NLG with Language Agnostic Delexicalisation",
    "authors" : [ "Giulio Zhou", "Gerasimos Lampouras" ],
    "emails" : [ "giuliozhou@huawei.com", "gerasimos.lampouras@huawei.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 114–127\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n114"
    }, {
      "heading" : "1 Introduction",
      "text" : "Recently, neural approaches to language generation have become predominant in various tasks such as concept-to-text Natural Language Generation (NLG), Summarisation, and Machine Translation thanks to their ability to achieve state-of-the-art performance through end-to-end training (Dušek et al., 2018; Chandrasekaran et al., 2019; Barrault et al., 2019). Specifically in Machine Translation, deep learning models have proven easy to adapt to multilingual output (Johnson et al., 2017) and have been demonstated to successfully transfer knowledge between languages, benefiting both the low and high resource languages (Dabre et al., 2020).\nIn the concept-to-text NLG task, the language generation model has to produce a text that is an\nX1 X2\nX3 X4\nbroadcastedby\nfirstaired lastaired\nwhere: X1=bananaman|бананамен, X2=bbc|би_би_си, X3=1983_10_03, X4=1986_04_15\nGold Target References:\nbananaman first aired on the bbc on october 3rd, 1983 and broadcast its last episode on april 15th, 1986.\nбананамен впервые вышел в эфир на bbc 3 октября 1983 года, а его последний эпизод вышел 15 апреля 1986 года.\nТаблица 1: Delex calis ti n and relexicalisation strategies on Engli h and Russian exampl s from the WebNLG 2020 Challenge.\nExact Delexicalisation:"
    }, {
      "heading" : "X1 first aired on the X2 on october 3rd, 1983 and broadcast its last episode on april 15th, 1986.",
      "text" : ""
    }, {
      "heading" : "X1 впервые вышел в эфир на bbc 3 октября 1983 года,",
      "text" : "а его последний эпизод вышел 15 апреля 1986 года.\nLanguage Agnostic Delexicalisation (LAD):"
    }, {
      "heading" : "X1 first aired on the X2 on october X3 and broadcast its",
      "text" : "last episode on X4.\nX1 впервые вышел в эфир на X2 X3 года, а его последний эпизод вышел X4 года.\nТаблица 2: Delexicalisation and relexicalisation strategies on English and Russian examples fr m the WebNLG 2020 Challenge.\nGenerated Output before Relexicalisation: (assuming training with LAD)"
    }, {
      "heading" : "X1 first aired on the X2 on october X3 and broadcast its",
      "text" : "last episode on X4.\nX1 впервые вышел в эфир на X2 X3 года, а его последний эпизод вышел X4 года.\nExact Relexicalisation:\nbananaman first aired on the bbc on october 1983 10 03 and broadcast its last episode on 1986 04 15.\nбананамен впервые вышел в эфир на би би си 1983 10 03 года, а его последний эпизод вышел 1986 04 15 года.\nAutomatic Value Post-Editing (VAPE):\nbananaman first aired on the bbc on october 3rd, 1983 and broadcast its last episode on april 15th, 1986.\nбананамен впервые вышел в эфир на бибиси 3 октября 1983 года, а его последний эпизод вышел 15 апреля 1986 года.\nТаблица 3: Delexicalisation and relexicalisation strategies on English and Russian examples from the WebNLG 2020 Challenge.\nТаблица 4: Delexicalisation and relexicalisation strategies on English and Russian examples from the WebNLG 2020 Challenge.\nGenerated Output before Relexicalisation:"
    }, {
      "heading" : "X1 is located in X2 where X3 is the leader and the people",
      "text" : "are known as X4.\nX1 находится в X2, где лидером является X3. местные жители известны как X4.\nExact Relexicalisation:\namdavad ni gufa is located in india where t.s. thakur is the leader and the people are known as indian people.\nамдавад ни гуфа находится в индия, где лидером является т.с. тхакур (муж). местные жители известны как индийцы.\nAutomatic Value Post-Editing (VAPE):\namdavad ni gufa is located in india where t.s. thakur is the leader and the people are known as indians.\nамдавад ни гуфа находится в индии, где лидером является т.с. тхакур. местные жители известны как индийцами.\nТаблица 5: Delexicalisation and relexicalisation strategies on English and Russian examples from the WebNLG 2020 Challenge.\naccurate realisation of the abstract semantic information given in h input (Meaning Representation, MR; see Figure 1). It is common practice to perform a delexicalisation (Wen et al., 2015) of the MR, in order to facilitate the NLG model’s generalisation to rare and unseen input; lack of generalisation is a main drawback of neural models (Goyal et al., 2016) but is particularly prominent in concept-to-text. Delexicalisation consists of a preprocessing and a postprocessing step. In prepro-\ncessing, all occurrences of MR values in the text are replaced with placeholders. This way the model learns to generate text that is abstracted away from actual values. In postprocessing (relexicalisation), placeholders are re-filled with values.\nThe main shortcoming of delexicalisation is that its efficacy is bounded by the number of values that are correctly identified. In fact, a naive implementation of “exact” delexicalisation (see Figure 1) requires the values provided by the MR to appear verbatim in the text, which is often not the case. This shortcoming is more prominent when expanding concept-to-text to the multilingual setting, as MR values in the target language are often only partially provided. Additionally, MR values are usually in their base form, which makes it harder to find them verbatim in text of morphologically rich languages. Finally, relexicalisation also remains a naive process (see Figure 2) that ignores how context should effect the morphology of the MR value when it is added to the text (Goyal et al., 2016).\nWe propose Language Agnostic Delexicalisation (LAD), a novel delexicalisation method that aims to identify and delexicalise values in the text independently of the language. LAD expands over previous delexicalisation methods and maps input values to the most similar n-grams in the text, by focusing on semantic similarity, instead of lexical similarity, over a language independent embedding space. This is achieved by relying on pretrained multilingual embeddings, e.g. LASER (Artetxe and Schwenk, 2019). In addition, when relexicalising the placeholders, the values are processed with a character-level post editing model that modifies them to fit their context. Specifically in morphologically rich languages, this post editing results in the value exhibiting correct inflection for its context.\nOur goal is to explore the application of multilingual models with a focus on their generalisation capability to rare or unseen inputs. In this paper, we (i) apply multilingual models and show that they outperform monolingual models in conceptto-text, especially in low resource conditions; (ii) propose LAD and show that it achieves state-of-theart results, especially on unseen input; (iii) provide experimental analysis across 5 datasets and 5 languages over models with and without pre-training."
    }, {
      "heading" : "2 Related Work",
      "text" : "Multilingual generation techniques have mostly been the focus of Machine Translation (MT) as\nInput MR: X1 X2 X3 X4 broadcastedby firstaired lastaired where: X1=bananaman|бананамен, X2=bbc|би_би_си, X3=1983_10_03, X4=1986_04_15 Gold Target References: bananaman first aired on the bbc on october 3rd, 1983 and broadcast its last episode on april 15th, 1986. бананамен впервые вышел в эфир на bbc 3 октября 1983 года, а его последний эпизод вышел 15 апреля 1986 года. Таблица 1: Delexicalisation and relexicalisation strategies on English and Russian examples from the WebNLG 2020 Challenge. Exact Delexicalisation: X1 first aired on the X2 on october 3rd, 1983 and broadcast its last episode on april 15th, 1986. X1 впервые вышел в эфир на bbc 3 октября 1983 года, а его последний эпизод вышел 15 апреля 1986 года. Language Agnostic Delexicalisation (LAD): X1 first aired on the X2 on october X3 and broadcast its last episode on X4.\nX1 впервые вышел в эфир на X2 X3 года, а его последний эпизод вышел X4 года.\nТаблица 2: Delexicalisation and relexicalisation strategies on English and Russian examples from the WebNLG 2020 Challenge.\nGenerated Output before Relexicalisation: (assuming training with LAD)\nX1 first aired on the X2 on october X3 and broadcast its last episode on X4.\nX1 впервые вышел в эфир на X2 X3 года, а его последний эпизод вышел X4 года.\nExact Relexicalisation:\nbananaman first aired on the bbc on october 1983 10 03 and broadcast its last episode on 1986 04 15.\nбананамен впервые вышел в эфир на би би си 1983 10 03 года, а его последний эпизод вышел 1986 04 15 года.\nAutomatic Value Post-Editing (VAPE):\nbananaman first aired on the bbc on october 3rd, 1983 and broadcast its last episode on april 15th, 1986.\nбананамен впервые вышел в эфир на бибиси 3 октября 1983 года, а его последний эпизод вышел 15 апреля 1986 года.\nТаблица 3: Delexicalisation and relexicalisation strat gies on English and Russian examples from the WebNLG 2020 Challenge.\nInput MR: X1 X2 X3 X4 country leadername demonym where: X1=amdavad ni gufa|амдавад ни гуфа, X2=india|индия, X3=t.s. thakur| т.с. тхакур (муж), X4=indian people|индийцы Gold Target References: amdavad ni gufa is located in india, where the leader is t s thakur and the demonym for people living there is indian. амдавад ни гуфа находится в индии , где лидер т.с. тхакур и люди, проживающие там, называются индийцами. Exact Delexicalisation: X1 is located in X2, where the leader is t s thakur and the demonym for people living there is indian. X1 находится в индии, где лидер т.с. тхакур и люди, проживающие там, называются индийцами. Language Agnostic Delexicalisation (LAD): X1 is located in X2, where the leader is X3 and the demonym for people living there is X4. X1 находится в X2, где лидер X3 и люди, проживающие там, называются X4.\nТаблица 4: Delexicalisation and relexicalisation strategies on English and Russian examples from the WebNLG 2020 Challenge."
    }, {
      "heading" : "X1 is located in X2 where X3 is the leader and the people",
      "text" : "Таблица 5: Delexicalisation and relexicalisation\nstrategies on English and Russian examples from\nthe WebNLG 2020 Challenge.\n1\nthe appropriate data (multilingual parallel source and target sentences) are more readily available there. Earlier research enabled multilingual generation with no and partial parameter sharing (Luong et al., 2016; Firat et al., 2016), while Johnson et al. (2017) explored many-to-many translation with full parameter sharing in a universal encoder-decoder framework. Despite the successes of this many-tomany framework, the improvements were mainly attributed to the model’s multilingual input. Wang et al. (2018) improved on one-to-many translation (i.e. the input is always on a single language, while the output is on many) by introducing special label initialisation, language-dependent positional embeddings and a new parameter-sharing mechanism.\nIn other language generation tasks, the vast majority of datasets are only available with English output. To enable output in a different language, a number of Zero-Shot methods have been proposed with the most common practice being to directly use an MT model to translate the output into the target language (Wan et al., 2010; Shen et al., 2018; Duan et al., 2019). The MT model can be finetuned on task-specific data when those are available (Miculicich et al., 2019). For the purposes of this paper, we do not consider these previous works as multilingual, as the language generation model is disjoint from the multilingual component, i.e. the pipelined MT model. Contrary to this, Chi et al. (2020) proposed a cross-lingual pretrained masked language model to generate in multiple languages, outperforming pipeline models on Question Generation and Abstractive Summarisation.\nAn adaptation of Puduppully et al. (2019) was applied to multilingual concept-to-text NLG and participated in the Document-Level Generation and Translation shared task (Hayashi et al., 2019, DGT). However, this shared task, and in extension the dataset and participating systems, heavily focus on content selection and document generation. Additionally, the input’s attributes are constant across train and testing, so there are no unseen data and no need to improve on the model’s generalisation capability. As the goal of this paper is multilinguality (content selection is a language agnostic task) and generalisation, we opt to not use this dataset.\nMultilinguality has also been explored in the related tasks of Morphological Inflection and Surface Realisation in SIGMORPHON (McCarthy et al., 2019) and MSR (Mille et al., 2020) challenges. However, our Automatic Value Post-Editing approach focuses mostly on adapting values to context and does not assume additional input such as dependency trees, PoS tags or morphological information that Surface Realisation and Morphological Inflection often requires.\nParticularly for concept-to-text NLG, notable previous works includes the approach of Fan and Gardent (2020) who make use of pretrained language models through the Transformer architecture for AMR-to-text generation in multiple languages, and the WebNLG Challenge 2020 (Castro Ferreira et al., 2020). The goal of WebNLG 2020 was to generate output in both English and Russian but most of the participants focused on monolingual rather than multilingual approaches."
    }, {
      "heading" : "3 Rare and Unseen Inputs in NLG",
      "text" : "Due to the existence of open-set and numerical attributes in the aforementioned datasets, it is common during testing for MRs to contain rare or unseen values. Certain datasets are even more challenging in this regard (e.g. WebNLG Challenge 2020) as they also contain unseen relations in the development and test subsets. Several techniques have been proposed to mitigate this problem.\nDelexicalisation, also known as anonymisation or masking, is a pre/post-processing procedure that attempts to mitigate problems with data sparsity. In preprocessing, all values in the MR that appear verbatim in the target sentence are replaced in both input and output with specific placeholders, e.g. “X-” followed by the corresponding attribute (e.g. “X-type”) so that the placeholder still captures rele-\nvant semantic information. In Figure 1 we use numbered placeholders instead, for clarity and space. The model is trained to generate the target text containing these placeholders, which are subsequently replaced with the corresponding true values (i.e. relexicalised) in post-processing. See Figures 1 and Figure 2 for examples; we mark this strategy as Exact due to the exact matching of the values with the text. To improve delexicalisation accuracy, n-gram matching (Trisedya et al., 2018) has been proposed as an alternative. Thanks to its simplicity and efficacy, delexicalisation is widely used by many systems, including the winning systems of major concept-to-text NLG shared tasks (Gardent et al., 2017; Dušek et al., 2018; Castro Ferreira et al., 2020). Mapping the values as such can be sufficient for simple datasets, but otherwise, incorrect or incomplete delexicalisation will lead to inconsistent input and deteriorate performance.\nLastly, problems may also occur during relexicalisation as it does not take into account the context in which the placeholders are situated and may result in disfluent sentence. For a simplified example, observe how placing the unedited dates in the placeholders leads to disfluent output in Figure 2.\nSegmentation strategies are commonly used in Neural Machine Translation to improve the generalisation ability of models. The objective is to break down words into smaller units, reducing the vocabulary and the number of unseen tokens (Sennrich et al., 2016). Unfortunately, applying segmentation in concept-to-text NLG, e.g. using BytePair-Encoding (BPE) subword units (Gardent et al., 2017; Zhang et al., 2018) or using characters as basic units (Goyal et al., 2016; Agarwal and Dymetman, 2017; Deriu and Cieliebak, 2018), underperforms against delexicalisation. Challenges include capturing long dependencies between segmented words, and generating non-existing words.\nCopy mechanism is another method to address unseen input, by allowing the decoder of an encoder-decoder model to draw a token directly from the input sequence instead of generating it from the decoder vocabulary (See et al., 2017). While applications of the copy mechanism in concept-to-text NLG have achieved overall good results (Chen, 2018; Elder et al., 2018; Gehrmann et al., 2018), when dealing with rare and unseen inputs delexicalisation is still preferable (Shimorina and Gardent, 2018). To improve the generalisation ability of copy mechanism models, Roberti et al.\n(2019) propose applying the copy mechanism to character-level NLG systems. This is combined with an additional optimisation phase during training where the encoder and decoder are switched."
    }, {
      "heading" : "4 Language Agnostic Delexicalisation",
      "text" : "In order to address the shortcomings of previous approaches to generalise over rare or unseen inputs, especially in cases of multilingual output, we propose Language Agnostic Delexicalisation (LAD). Figure 3 shows an overview of LAD; the input and output are first delexicalised using pretrained language-independent embeddings, and (optionally) ordered. The multilingual generation model is trained on the delexicalised training data, and the output is relexicalised using automatic value post-editing to ensure that the values fit the context. Each component is described in more detail bellow.\nTo enable multilingual generation, we adapt the universal encoder-decoder framework via “target forcing” (Johnson et al., 2017) since it can be directly applied to any NLG model without the need to modify the latter’s architecture. To do so, we extend the input MR in the encoder with a language token that signals which language the model\nshould generate output in. In addition, we follow Wang et al. (2018) and initialise the decoder with the language token. The rest of the components (i.e. delexicalisation, ordering, and value post-editing) are orthogonal to the model’s architecture."
    }, {
      "heading" : "4.1 Value Matching",
      "text" : "As discussed in Section 3, one of the challenges of delexicalisation is matching the MR values with corresponding words in the text, especially in the multilingual setting. Even when the MR values are in the same language as the target, we observe from the examples in Figure 1 that token overlapping methods (i.e. exact and n-gram matching) are not sufficient to generate a complete and accurate delexicalisation as values may appear differently.\nTo counter this problem, LAD performs matching by mapping MR values to n-grams based on the similarity of their representations. Specifically, it calculates the similarity between a value v and all word n-grams wi . . . wj in the text, with j − i < n and n set to the maximum value length observed in the training data. LAD employs LASER (Artetxe and Schwenk, 2019) to generate language agnostic sentence embeddings of the values and n-grams, and calculates their distance via cosine similarity. Given an MR and text, all possible value and ngram comparisons are calculated and the matches are determined in a greedy fashion."
    }, {
      "heading" : "4.2 Generic Placeholders and Ordering",
      "text" : "In Section 3, we discussed how the WebNLG datasets are more challenging because they contain unseen attributes in the development and test subsets, in addition to unseen values. This is problematic when we use attribute-bound placeholders (e.g. “X-type”) as unseen attributes will result in unseen placeholders. Following Trisedya et al. (2018), for the WebNLG datasets, LAD uses numbered generic placeholders “X#” (e.g. “X1”). Unfortunately, the adoption of generic placeholders creates problems for relexicalisation as it becomes unclear which input value should replace which placeholder. We address this by ordering the model’s input based on the graph formed by its RDF triples, again by following Trisedya et al. (2018). We traverse every edge in the graph, starting from the node with the least incoming edges (or randomly in case of ties) and then visit all nodes via BFS (breadth-first search). We then trust that the model will learn to respect the input order when generating, and follow the order to relexicalise the placeholders.\nWe note that this is only required for models that employ delexicalisation strategies and for datasets with unseen attributes (i.e. the WebNLG Challenge datasets). Concept-to-text NLG systems do not generally require ordered input (Wen et al., 2015)."
    }, {
      "heading" : "4.3 Automatic Value Post-Editing",
      "text" : "As discussed in Section 3, a naive relexicalisation of the placeholders may lead to disfluent sentences, as the procedure does not take into account the context in which the placeholders have been placed. For example, in the sentence “there are 2 X that have free parking”, if we need to replace the placeholder “X” with the MR value “guesthouse” , the value should be pluralised to fit the context. This problem is more evident in morphologically rich languages, where more factors affect the value’s form. To alleviate this, the LAD framework incorporates an Automatic Value Post-Editing component, consisting of a character-level seq2seq model that iterates over values as they are placed in the text and modifies them to fit the context of their respective placeholders. Anastasopoulos and Neubig (2019) has already shown the benefits of character models on morphological inflection generation, but no previous work has addressed how relexicalisation should adapt to context.\nOur proposed VAPE model requires as input the MR placeholder ei, original value vi and corresponding NLG output w′1 . . . w ′ n for context; these are serialised and passed to the encoder. Similar to the multilingual model, we add an appropriate language token L before the NLG output. The output of VAPE is the MR value v′i in the proper form.\n{ei vi [SEP] L w′1 . . . w′n} → v′i\nThe training signal for VAPE is obtained during delexicalisation. For a given delexicalisation strategy, we obtain all pairs of MR values and matching n-grams in the training data, and subsequently train VAPE using these n-grams as the targets. Therefore, the VAPE model is dependent on the quality of the delexicalisation strategy; specifically for exact delexicalisation, VAPE cannot be trained as the MR values and matching n-grams are the same.\nMost edits VAPE performs concern incorrect inflections, but it is not limited to morphological edits and has the potential to deal with various types of modifications. During our experiments we observed VAPE performing value re-formatting (e.g. “1986 04 15” → “April 15th 1986”), syn-\nonym generation (e.g. “east”→ “oriental”) and value translation (e.g. “bbc” from Latin to Cyrillic)."
    }, {
      "heading" : "5 Experiments",
      "text" : "For our experiments we use five datasets and calculate BLEU-4 (Papineni et al., 2002, ↑), METEOR (Banerjee and Lavie, 2005, ↑), chrF++ (Popović, 2015, ↑), and TER (Snover et al., 2006, ↓).\nThe WebNLG Challenge 2017 (Gardent et al., 2017, WebNLG17) data consists of sets of RDF triple pairs and corresponding English texts in 15 DBPedia categories. For our purposes, we will be using a later work (Shimorina et al., 2019) that introduced a machine translated Russian version of WebNLG17, a part of which was post edited by humans. Due to the limited amount of human corrected Russian sentences, and to facilitate the most accurate evaluation, we use these solely for testing. To ensure that half of the domains in the new test set remain unseen during training, we create our own train/dev/test split by retaining the following DBPedia categories from training and development sets: Astronaut, Monument and University.\nThe latest incarnation of the WebNLG Challenge (Castro Ferreira et al., 2020, WebNLG20) is fully human annotated for both English and Russian. We use this as the main dataset in our experiments, as it is designed to promote multilinguality. However, due to the fact that the provided test set does not contain unseen Russian instances, we perform our experiment on a custom split (WebNLG20*) ensuring that part of the domains in the test data remain unseen during training. The split was performed similarly to the previously described WebNLG17.\nMultiWOZ 2.1 (Eric et al., 2020) and CrossWOZ (Zhu et al., 2020) are datasets of dialogue acts and corresponding utterances in English and Chinese respectively. The two datasets share the same structure, with MultiWOZ covering 7 domains and 25 attributes, and CrossWOZ covering 5 domains and 72 attributes; 4 of the domains are common in both datasets though CrossWOZ has more attached attributes. Multilingual WOZ 2.0 (Mrkšić et al., 2017) is also a dialogue dataset with utterances available in three languages: English, Italian and German. Its scope is more limited than MultiWOZ and CrossWOZ as it only covers a single domain.\nFor all models in our experiments, the input consists of a simple linearisation of the MRs. Particularly, for the delexicalisation based models, the values are extended with their respective placehold-\ners as shown in the following example: “ENTITY 1 meyer werft location ENTITY 2 germany”."
    }, {
      "heading" : "5.1 Ablation Study",
      "text" : "First we perform an ablation study to determine how the different components of LAD (ordering and VAPE) affect its performance; LAD being our full Language Agnostic Delexicalisation model as described in Section 4. In addition to LAD, where these components are incrementally removed, we explore how their addition would influence exact and n-gram delexicalisation (Trisedya et al., 2018). We do not explore adding VAPE to exact delexicalisation (there is no EXACT + O + V variant), as it cannot be trained in this setting (see Section 4.3).\nIn Table 1, we observe that both components are beneficial, but less so for seen English data. For the more morphologically rich and lower resourced Russian, the components are helpful for both seen and unseen. VAPE leads to an improvement in performance in almost all cases and even when added on NGram. An exception is unseen English data, where removing VAPE is beneficial; this suggests that VAPE is overeager to make edits in English.\nBy studying the output, we observe that VAPE modified 20% of values in English, and 66% in Russian; directly copying the value was insufficient in Russian where proper inflection is needed. We identified three consistent errors where copying the original value would be preferable to using VAPE: the removal of date information (e.g. “1969-09-01 → 1st, 1969”), misspelling of proper nouns (e.g. “atatürk monument”→ “atat erk monument”), and mishandling of long values (e.g. “ottoman army soldiers killed in the battle of baku”→ “ottoman army soldiers killed in the batttle of kiled in the bathe batom”). We observe that these errors occur more frequently for English unseen cases, but could be reduced by extending VAPE with a control mechanism that decides whether copying the values themselves is preferable. Such errors occur in part because VAPE, as a character-level model, suffers from the same challenges as other segmentation methods (see Section 3). However, since VAPE’s input is much shorter, the problem is not as prevalent. Overall, LAD outperforms the previous delexicalisation strategies Exact and NGram, and VAPE is shown to be integral to its performance."
    }, {
      "heading" : "5.2 Monolingual vs Multilingual",
      "text" : "Here we explore the performance of monolingual and multilingual models on concept-to-text\ndatasets. The Word model has the exact same architecture as LAD but no delexicalisation is performed, and consequently no automatic value post-editing and no ordering. Since there is no relexicalisation that needs to occur during post-processing, the input to the Word model needs not be specifically ordered, and is just a concatenation of the RDF triples as they appear in the original dataset. For multilingual, we add the appropriate language tokens on the input of Word, in the same manner we added them to LAD. For the monolingual (Mono) configuration we train the models to produce a single language, while for multilingual (Multi) we train them to produce all languages available in that dataset. Please refer to Table 2 for the results.\nWe observe that the multilingual models outperform their monolingual counterpart in most datasets and languages, especially with LAD as its delexicalisation and relexicalisation modules are more robust to multilingual input and output. Specifically for the MultiWOZ and CrossWOZ datasets, in the monolingual setting the models are trained exclusively on the respective dataset, i.e. MultiWOZ for English, and CrossWOZ for Chinese. For multilingual, we take advantage of the fact that these datasets share the same structure, and train the models on both datasets. For English,\nwe observe that the multilingual model improves, suggesting that domain knowledge is transferred from CrossWOZ. For Chinese however, the multilingual Word model underperforms. This is not very surprising, as the overlap between the datasets is favourable to MultiWOZ, i.e. most of the attributes of MultiWOZ also appear in CrossWOZ, while the majority of CrossWOZ’s attributes do not appear in MultiWOZ."
    }, {
      "heading" : "5.3 Multilingual Generalisation",
      "text" : "Tables 3 contains full results for English and Russian on WebNLG20* respectively. We include the Word configuration (see Section 5.2), as well as Char, BPE, and SP, which are variations that use characters, Byte-Pair-Encoding, and SentencePiece\nas subword units respectively. Copy refers to the copy mechanism model by Roberti et al. (2019). The SP model performs very well for seen categories, but fails to generalise on unseen data. The Copy model performs well for unseen categories in English, but underperforms in Russian as values for it are only partially translated, i.e. some values in the MR may appear in English while others appear in Russian. This is challenging for Copy models as the target reference does not closely match the input, but LAD can handle it more robustly.\nObserving the output, LAD’s main advantage is that it avoids under- and over-generating values as they are being controlled by the placeholders.1 SP is often the most fluent of the models, but for\n1We provide output examples in the Appendix.\nlonger input it tends to under-generate and miss values. The Copy model tends to repeat values, which can be attributed to the fact that it is based on characters where long-distance dependencies are hard to maintain. On the other hand, Copy can potentially generate more relevant output since it can copy words from attributes as well as values.\nOverall, LAD helps the multilingual model outperform all other models in both English and Russian. It is especially beneficial in generalising to unseen data, as was its main objective after all."
    }, {
      "heading" : "5.4 Generalising with Pretrained Models",
      "text" : "Here we explore the generalisation capabilities of multilingual pretrained models, by replacing the underlying NLG model with mBART (Liu et al., 2020), a multilingual denoising autoencoder pretrained on a large-scale dataset containing 25 languages (CC25). Similarly to Kasner and Dušek (2020), we fine-tune mBART with the default ENRO configuration for up to 10000 updates. Using mBART as the underlying model also helps facilitate a comparison against a configuration that is similar to many of the state of the art participants in the WebNLG 2020 Challenge, although some of them used different pretrained models.\nTable 4 shows the performance of the fine-tuned models on the WebNLG20* dataset. The mBARTbased model outperforms the non-delexicalisation SP, and non-pretrained LAD in English. However, LAD still performs better in Russian. This makes sense as the CC25 dataset is heavily biased towards the English language and contains double the amount of tokens compared to Russian, and much more compared to other lower-resource languages. Combining the LAD framework with mBART (mB-LAD) resulted in a general improvement in performance, especially for lower-resource unseen data. However, as discussed in Section 5.1, the VAPE component remains to some degrees susceptible to unseen contexts. To tackle this issue, we improve VAPE by pre-loading mBART and finetuning it for value post-editing as well (mB-LAD+),\nachieving 3 and 29 points increase in BLEU score for unseen English over the vanilla mBART and LAD models, and 26 and 20 points for unseen Russian. Additionally, to take advantage of mBART’s denoising ability, we extend the fine-tuned VAPE to edit the “exact” relexicalised NLG output and provide a sentence-level output (mB-LAD-SPE), i.e. edits are not exclusively focused on the values. Results show that mB-LAD-SPE improves further mB-LAD+ on Russian in both seen and unseen.\nTable 5 and 6 also shows the automatic evaluation of the fine-tuned mBART models on the official WebNLG20 Challenge testset; the official test set had no unseen subset of Russian. The results are consistent with the findings in our previous experiments, with small improvements of LAD-based mBART models over the mBART-base."
    }, {
      "heading" : "5.5 Synthetic Data",
      "text" : "We use the WebNLG17 automatically translated Russian “silver” data, to determine how useful they are for training multilingual concept-to-text NLG. As preliminary results were not promising, we limit the scope of the experiment to only a few systems. Table 7 gathers the results It is apparent that automatically translated data are insufficient; LAD\nseems to more consistently achieve higher performance than other models, but all scores are too low to draw any sufficiently supported conclusions."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We proposed Language Agnostic Delexicalisation, a novel delexicalisation framework that matches and delexicalises MR values in the text independently of the language. For relexicalisation, an automatic value post editing model adapts the values to their context. Results show that multilingual models outperform monolingual models, and that LAD outperforms previous work in improving the performance of multilingual models, especially in low resource conditions. LAD also improves on the performance of pre-trained language models achieving state-of-the-art results. The automatic value post editing component is especially beneficial in morphologically rich languages."
    }, {
      "heading" : "7 Appendices",
      "text" : ""
    }, {
      "heading" : "A Configurations",
      "text" : "The multilingual NLG and VAPE use a transformer as underlying architecture. We use the fairseq toolkit for our experiments (Ott et al., 2019). The models are trained with shared embeddings, 8 attention heads, 6 layers, 512 hidden size, 2048 size for the feed forward layers. We trained with 0.3 dropout, adam optimiser with a learning rate of 0.0005. The NLG are trained with early stopping and patience set to 20. Automatic value post edit models are trained with the same configuration but patience was set to 6. For the copy mechanismbased model we use the EDA-CS implementation provided by Roberti et al. (2019) with the default configuration. Due to its extremely high computational training cost, the models are trained for 15 epochs. BPE and SentencePiece (Kudo and Richardson, 2018) models are trained with a vocabulary size set to 12000 tokens.\nFor all models in our experiments, the input consists of a simple linearisation of the MRs. Particularly, for the delexicalisation based models, the values are extended with their respective placeholders as shown in the following example: “ENTITY 1 meyer werft location ENTITY 2 germany.\nB Input examples\nFigure 6 shows some examples of how, during training, LAD maps MR values to n-grams of the target reference, based on the similarity of their representations. We can observe that these values could not have been matched by exact and n-gram delexicalisation as they constitute significant paraphrases of the value.\nFigure 4 and 5 show some additional examples of delexicalisation and relexialisation for the various approaches from the WebNLG Challenge 2020. Table 8 shows more delexicalisation examples from WebNLG, MultiWOZ and CrossWOZ datasets, where we can observe the shortcomings of exact and n-gram delexicalisation."
    }, {
      "heading" : "C Output examples",
      "text" : "Table 9 and 10 present some examples for English and Russian output respectively. The examples include output from SentencePiece (SP), Copy, and LAD systems.\nInput MR:\nX1 X2\nX3 X4\nbroadcastedby\nfirstaired lastaired\nwhere: X1=bananaman|бананамен, X2=bbc|би_би_си, X3=1983_10_03, X4=1986_04_15\nGold Target References:\nbananaman first aired on the bbc on october 3rd, 1983 and broadcast its last episode on april 15th, 1986.\nбананамен впервые вышел в эфир на bbc 3 октября 1983 года, а его последний эпизод вышел 15 апреля 1986 года.\nExact Delexicalisation:\nX1 first aired on the X2 on october 3rd, 1983 and broadcast its last episode on april 15th, 1986.\nX1 впервые вышел в эфир на bbc 3 октября 1983 года, а его последний эпизод вышел 15 апреля 1986 года.\nLanguage Agnostic Delexicalisation (LAD):\nX1 first aired on the X2 on october X3 and broadcast its last episode on X4.\nX1 впервые вышел в эфир на X2 X3 года, а его последний эпизод вышел X4 года.\nFigure 4: Delexicalisation on WebNLG Challenge 2020 with target output in English and Russian. Double underlining marks text missed by delexicalisation.\nInput MR:\nX1 X2\nX3 X4\nbroadcastedby\nfirstaired lastaired\nwhere: X1=bananaman|бананамен, X2=bbc|би_би_си, X3=1983_10_03, X4=1986_04_15\nGold Target References:\nbananaman first aired on the bbc on october 3rd, 1983 and broadcast its last episode on april 15th, 1986.\nбананамен впервые вышел в эфир на bbc 3 октября 1983 года, а его последний эпизод вышел 15 апреля 1986 года.\nТаблица 1: Delexicalisation and relexicalisation strategies on English and Russian examples from the WebNLG 2020 Challenge.\nExact Delexicalisation:\nX1 first aired on the X2 on october 3rd, 1983 and broadcast its last episode on april 15th, 1986.\nX1 впервые вышел в эфир на bbc 3 октября 1983 года, а его последний эпизод вышел 15 апреля 1986 года.\nLanguage Agnostic Delexicalisation (LAD):\nX1 first aired on the X2 on october X3 and broadcast its last episode on X4.\nX1 впервые вышел в эфир на X2 X3 года, а его последний эпизод вышел X4 года.\nТаблица 2: Delexicalisation and relexicalisation strategies on English and Russian examples from the WebNLG 2020 Challenge.\nТаблица 3: Delexicalisation and relexicalisation\nstrategies on English and Russian examples from\nthe WebNLG 2020 Challenge.\n1\n1\nTable 8: Dataset examples and delexicalisation output; double underlining marks text that was missed.\nTarget:\naarhus airport is located in tirstrup ,\npart of the central region of denmark\nwhich has the capital city of copenhagen .\nValue: central denmark region\nCos n-gram\n0.95 the central region of denmark\n... ...\n0.73 the capital city of copenhagen\n... ...\n0.25 which has the\nTarget: alan shepard is dead .\nValue: deceased\nCos n-gram\n0.84 dead\n... ...\n0.47 alan shepard\n... ...\n0.40 alan shepard is dead .\nX1 находится в индии, где лидер т.с. тхакур и люди,\nпроживающие там, называются индийцами.\nLanguage Agnostic Delexicalisation (LAD):\nX1 is located in X2, where the leader is X3 and the demonym for people living there is X4.\nX1 находится в X2, где лидер X3 и люди, проживающие там, называются X4.\nТаблица 6: Delexicalisation and relexicalisation strategies on English and Russian examples from the WebNLG 2020 Challenge.\n2\nFigure 6: Examples of LAD’s value mapping to target reference n-grams.\nMR: 〈 Trane, revenue, 1.0264E10 〉 〈 Trane, netIncome, 5.563E8 〉 〈 Trane, numberOfEmployees, 29000 〉 SP: trane has a revenue of $ 10,264,000,000 , with a net income of $ 556,300,000 and a revenue of $ 10,264,000,000 . Copy: trane , a company with 29,000 employees , has 29,000 employees and was connected at $ 556,300,000 . LAD: trane , which has a revenue of $ 10,264,000,000 , has a net income of $ 556,300,000 and employs 29,000 people . MR: 〈 William_Anders, dateOfRetirement, \"1969-09-01\"〉 〈 William_Anders, occupation, Fighter_pilot 〉 〈 William_Anders, birthPlace, British_Hong_Kong 〉 〈 William_Anders, was a crew member of, Apollo_8 〉 SP: the birth place of greek born , adonis georgiadis , is the company , of which was in office at the same time that m ogenenenenenenenenville , new britain , connecticut , is a member of the order of poales and a division of 45000 kilometres . Copy: william anders was born in british hong kong and has a crew mew member of the fighter pilot . the was a crew member of the was a crew member of the was a crew member of the was a crew member of LAD: william anders , which was followed by 1st , 1969 and fighter pilot , was born in british hong kong and has been a number of apollo 8 .\nTable 9: Output text from three different systems in English."
    } ],
    "references" : [ {
      "title" : "A surprisingly effective out-of-the-box char2char model on the E2E NLG challenge dataset",
      "author" : [ "Shubham Agarwal", "Marc Dymetman." ],
      "venue" : "Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 158–163, Saarbrücken, Ger-",
      "citeRegEx" : "Agarwal and Dymetman.,? 2017",
      "shortCiteRegEx" : "Agarwal and Dymetman.",
      "year" : 2017
    }, {
      "title" : "Pushing the limits of low-resource morphological inflection",
      "author" : [ "Antonios Anastasopoulos", "Graham Neubig." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Anastasopoulos and Neubig.,? 2019",
      "shortCiteRegEx" : "Anastasopoulos and Neubig.",
      "year" : 2019
    }, {
      "title" : "Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond",
      "author" : [ "Mikel Artetxe", "Holger Schwenk." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:597–610.",
      "citeRegEx" : "Artetxe and Schwenk.,? 2019",
      "shortCiteRegEx" : "Artetxe and Schwenk.",
      "year" : 2019
    }, {
      "title" : "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
      "author" : [ "Satanjeev Banerjee", "Alon Lavie." ],
      "venue" : "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-",
      "citeRegEx" : "Banerjee and Lavie.,? 2005",
      "shortCiteRegEx" : "Banerjee and Lavie.",
      "year" : 2005
    }, {
      "title" : "The 2020 bilingual, bi-directional WebNLG+ shared task: Overview and evaluation results (WebNLG+",
      "author" : [ "Thiago Castro Ferreira", "Claire Gardent", "Nikolai Ilinykh", "Chris van der Lee", "Simon Mille", "Diego Moussallem", "Anastasia Shimorina" ],
      "venue" : null,
      "citeRegEx" : "Ferreira et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Ferreira et al\\.",
      "year" : 2020
    }, {
      "title" : "Overview and results: Cl-scisumm shared task 2019",
      "author" : [ "Muthu Kumar Chandrasekaran", "Michihiro Yasunaga", "Dragomir R. Radev", "Dayne Freitag", "Min-Yen Kan." ],
      "venue" : "Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Re-",
      "citeRegEx" : "Chandrasekaran et al\\.,? 2019",
      "shortCiteRegEx" : "Chandrasekaran et al\\.",
      "year" : 2019
    }, {
      "title" : "A general model for neural text generation from structured data",
      "author" : [ "Shuang Chen." ],
      "venue" : "E2E NLG Challenge System Descriptions.",
      "citeRegEx" : "Chen.,? 2018",
      "shortCiteRegEx" : "Chen.",
      "year" : 2018
    }, {
      "title" : "Cross-lingual natural language generation via pre-training",
      "author" : [ "Zewen Chi", "Li Dong", "Furu Wei", "Wenhui Wang", "XianLing Mao", "Heyan Huang." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):7570–7577.",
      "citeRegEx" : "Chi et al\\.,? 2020",
      "shortCiteRegEx" : "Chi et al\\.",
      "year" : 2020
    }, {
      "title" : "A survey of multilingual neural machine translation",
      "author" : [ "Raj Dabre", "Chenhui Chu", "Anoop Kunchukuttan." ],
      "venue" : "ACM Comput. Surv., 53(5).",
      "citeRegEx" : "Dabre et al\\.,? 2020",
      "shortCiteRegEx" : "Dabre et al\\.",
      "year" : 2020
    }, {
      "title" : "End-toend trainable system for enhancing diversity in natural language generation",
      "author" : [ "Jan Milan Deriu", "Mark Cieliebak." ],
      "venue" : "E2E NLG Challenge System Descriptions.",
      "citeRegEx" : "Deriu and Cieliebak.,? 2018",
      "shortCiteRegEx" : "Deriu and Cieliebak.",
      "year" : 2018
    }, {
      "title" : "Zero-shot crosslingual abstractive sentence summarization through teaching generation and attention",
      "author" : [ "Xiangyu Duan", "Mingming Yin", "Min Zhang", "Boxing Chen", "Weihua Luo." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Duan et al\\.,? 2019",
      "shortCiteRegEx" : "Duan et al\\.",
      "year" : 2019
    }, {
      "title" : "Findings of the E2E NLG challenge",
      "author" : [ "Ondřej Dušek", "Jekaterina Novikova", "Verena Rieser." ],
      "venue" : "Proceedings of the 11th International Conference on Natural Language Generation, pages 322–328, Tilburg University, The Netherlands. Association for",
      "citeRegEx" : "Dušek et al\\.,? 2018",
      "shortCiteRegEx" : "Dušek et al\\.",
      "year" : 2018
    }, {
      "title" : "E2E NLG challenge submission: Towards controllable generation",
      "author" : [ "Henry Elder", "Sebastian Gehrmann", "Alexander O’Connor", "Qun Liu" ],
      "venue" : null,
      "citeRegEx" : "Elder et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Elder et al\\.",
      "year" : 2018
    }, {
      "title" : "Multiwoz 2.1: A consolidated multi-domain dialogue dataset with state corrections and state",
      "author" : [ "Mihail Eric", "Rahul Goel", "Shachi Paul", "Abhishek Sethi", "Sanchit Agarwal", "Shuyang Gao", "Adarsh Kumar", "Anuj Goyal", "Peter Ku", "Dilek Hakkani-Tur" ],
      "venue" : null,
      "citeRegEx" : "Eric et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Eric et al\\.",
      "year" : 2020
    }, {
      "title" : "Multilingual AMR-to-text generation",
      "author" : [ "Angela Fan", "Claire Gardent." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2889–2901, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Fan and Gardent.,? 2020",
      "shortCiteRegEx" : "Fan and Gardent.",
      "year" : 2020
    }, {
      "title" : "Multi-way, multilingual neural machine translation with a shared attention mechanism",
      "author" : [ "Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Firat et al\\.,? 2016",
      "shortCiteRegEx" : "Firat et al\\.",
      "year" : 2016
    }, {
      "title" : "The WebNLG challenge: Generating text from RDF data",
      "author" : [ "Claire Gardent", "Anastasia Shimorina", "Shashi Narayan", "Laura Perez-Beltrachini." ],
      "venue" : "Proceedings of the 10th International Conference on Natural Language Generation, pages 124–133, San-",
      "citeRegEx" : "Gardent et al\\.,? 2017",
      "shortCiteRegEx" : "Gardent et al\\.",
      "year" : 2017
    }, {
      "title" : "End-to-end content and plan selection for data-to-text generation",
      "author" : [ "Sebastian Gehrmann", "Falcon Dai", "Henry Elder", "Alexander Rush." ],
      "venue" : "Proceedings of the 11th International Conference on Natural Language Generation, pages 46–56, Tilburg Uni-",
      "citeRegEx" : "Gehrmann et al\\.,? 2018",
      "shortCiteRegEx" : "Gehrmann et al\\.",
      "year" : 2018
    }, {
      "title" : "Natural language generation through character-based RNNs with finite-state prior knowledge",
      "author" : [ "Raghav Goyal", "Marc Dymetman", "Eric Gaussier." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Conference on Computational Linguis-",
      "citeRegEx" : "Goyal et al\\.,? 2016",
      "shortCiteRegEx" : "Goyal et al\\.",
      "year" : 2016
    }, {
      "title" : "Findings of the third workshop on neural generation and translation",
      "author" : [ "Hiroaki Hayashi", "Yusuke Oda", "Alexandra Birch", "Ioannis Konstas", "Andrew Finch", "Minh-Thang Luong", "Graham Neubig", "Katsuhito Sudoh." ],
      "venue" : "Proceedings of the 3rd Workshop",
      "citeRegEx" : "Hayashi et al\\.,? 2019",
      "shortCiteRegEx" : "Hayashi et al\\.",
      "year" : 2019
    }, {
      "title" : "Google’s multilingual neural machine translation system: Enabling zero-shot translation",
      "author" : [ "Fernanda Viégas", "Martin Wattenberg", "Greg Corrado", "Macduff Hughes", "Jeffrey Dean." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:339–351.",
      "citeRegEx" : "Viégas et al\\.,? 2017",
      "shortCiteRegEx" : "Viégas et al\\.",
      "year" : 2017
    }, {
      "title" : "Train hard, finetune easy: Multilingual denoising for RDF-totext generation",
      "author" : [ "Zdeněk Kasner", "Ondřej Dušek." ],
      "venue" : "Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 171–176,",
      "citeRegEx" : "Kasner and Dušek.,? 2020",
      "shortCiteRegEx" : "Kasner and Dušek.",
      "year" : 2020
    }, {
      "title" : "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "author" : [ "Taku Kudo", "John Richardson." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System",
      "citeRegEx" : "Kudo and Richardson.,? 2018",
      "shortCiteRegEx" : "Kudo and Richardson.",
      "year" : 2018
    }, {
      "title" : "Multilingual denoising pre-training for neural machine translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Transactions of the Association for Computational Linguis-",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Multitask sequence to sequence learning",
      "author" : [ "Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser." ],
      "venue" : "4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,",
      "citeRegEx" : "Luong et al\\.,? 2016",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2016
    }, {
      "title" : "Selecting, planning, and rewriting: A modular approach for data-to-document generation and translation",
      "author" : [ "Lesly Miculicich", "Marc Marone", "Hany Hassan." ],
      "venue" : "Proceedings of the 3rd Workshop on Neural Generation and Translation, pages 289–296,",
      "citeRegEx" : "Miculicich et al\\.,? 2019",
      "shortCiteRegEx" : "Miculicich et al\\.",
      "year" : 2019
    }, {
      "title" : "The third multilingual surface realisation shared task (SR’20): Overview and evaluation results",
      "author" : [ "Simon Mille", "Anya Belz", "Bernd Bohnet", "Thiago Castro Ferreira", "Yvette Graham", "Leo Wanner." ],
      "venue" : "Proceedings of the Third Workshop on Multilingual Sur-",
      "citeRegEx" : "Mille et al\\.,? 2020",
      "shortCiteRegEx" : "Mille et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural belief tracker: Data-driven dialogue state tracking",
      "author" : [ "Nikola Mrkšić", "Diarmuid Ó Séaghdha", "Tsung-Hsien Wen", "Blaise Thomson", "Steve Young." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol-",
      "citeRegEx" : "Mrkšić et al\\.,? 2017",
      "shortCiteRegEx" : "Mrkšić et al\\.",
      "year" : 2017
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chap-",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "chrF: character n-gram F-score for automatic MT evaluation",
      "author" : [ "Maja Popović." ],
      "venue" : "Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392–395, Lisbon, Portugal. Association for Computational Linguistics.",
      "citeRegEx" : "Popović.,? 2015",
      "shortCiteRegEx" : "Popović.",
      "year" : 2015
    }, {
      "title" : "University of Edinburgh’s submission to the document-level generation and translation shared task",
      "author" : [ "Ratish Puduppully", "Jonathan Mallinson", "Mirella Lapata." ],
      "venue" : "Proceedings of the 3rd Workshop on Neural Generation and Translation, pages",
      "citeRegEx" : "Puduppully et al\\.,? 2019",
      "shortCiteRegEx" : "Puduppully et al\\.",
      "year" : 2019
    }, {
      "title" : "Copy mechanism and tailored training for character-based data-to-text generation",
      "author" : [ "Marco Roberti", "Giovanni Bonetta", "Rossella Cancelliere", "Patrick Gallinari." ],
      "venue" : "Machine Learning and Knowledge Discovery in Databases - European Conference,",
      "citeRegEx" : "Roberti et al\\.,? 2019",
      "shortCiteRegEx" : "Roberti et al\\.",
      "year" : 2019
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J. Liu", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073–",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Handling rare items in data-to-text generation",
      "author" : [ "Anastasia Shimorina", "Claire Gardent." ],
      "venue" : "Proceedings of the 11th International Conference on Natural Language Generation, pages 360–370, Tilburg University, The Netherlands. Association for",
      "citeRegEx" : "Shimorina and Gardent.,? 2018",
      "shortCiteRegEx" : "Shimorina and Gardent.",
      "year" : 2018
    }, {
      "title" : "Creating a corpus for Russian datato-text generation using neural machine translation and post-editing",
      "author" : [ "Anastasia Shimorina", "Elena Khasanova", "Claire Gardent." ],
      "venue" : "Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing,",
      "citeRegEx" : "Shimorina et al\\.,? 2019",
      "shortCiteRegEx" : "Shimorina et al\\.",
      "year" : 2019
    }, {
      "title" : "A study of translation edit rate with targeted human annotation",
      "author" : [ "Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul." ],
      "venue" : "In Proceedings of Association for Machine Translation in the Americas, pages 223–231.",
      "citeRegEx" : "Snover et al\\.,? 2006",
      "shortCiteRegEx" : "Snover et al\\.",
      "year" : 2006
    }, {
      "title" : "GTR-LSTM: A triple encoder for sentence generation from RDF data",
      "author" : [ "Bayu Distiawan Trisedya", "Jianzhong Qi", "Rui Zhang", "Wei Wang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
      "citeRegEx" : "Trisedya et al\\.,? 2018",
      "shortCiteRegEx" : "Trisedya et al\\.",
      "year" : 2018
    }, {
      "title" : "Cross-language document summarization based on machine translation quality prediction",
      "author" : [ "Xiaojun Wan", "Huiying Li", "Jianguo Xiao." ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 917–926, Up-",
      "citeRegEx" : "Wan et al\\.,? 2010",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2010
    }, {
      "title" : "Three strategies to improve one-to-many multilingual translation",
      "author" : [ "Yining Wang", "Jiajun Zhang", "Feifei Zhai", "Jingfang Xu", "Chengqing Zong." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2955–",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Semantically conditioned LSTM-based natural language generation for spoken dialogue systems",
      "author" : [ "Tsung-Hsien Wen", "Milica Gašić", "Nikola Mrkšić", "PeiHao Su", "David Vandyke", "Steve Young." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical",
      "citeRegEx" : "Wen et al\\.,? 2015",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2015
    }, {
      "title" : "Attention regularized sequence-to-sequence learning for e2e nlg challenge",
      "author" : [ "Biao Zhang", "Jing Yang", "Qian Lin", "Jinsong Su." ],
      "venue" : "E2E NLG Challenge System Descriptions.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "CrossWOZ: A large-scale Chinese cross-domain task-oriented dialogue dataset",
      "author" : [ "Qi Zhu", "Kaili Huang", "Zheng Zhang", "Xiaoyan Zhu", "Minlie Huang." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:281–295.",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Recently, neural approaches to language generation have become predominant in various tasks such as concept-to-text Natural Language Generation (NLG), Summarisation, and Machine Translation thanks to their ability to achieve state-of-the-art performance through end-to-end training (Dušek et al., 2018; Chandrasekaran et al., 2019; Barrault et al., 2019).",
      "startOffset" : 282,
      "endOffset" : 354
    }, {
      "referenceID" : 5,
      "context" : "Recently, neural approaches to language generation have become predominant in various tasks such as concept-to-text Natural Language Generation (NLG), Summarisation, and Machine Translation thanks to their ability to achieve state-of-the-art performance through end-to-end training (Dušek et al., 2018; Chandrasekaran et al., 2019; Barrault et al., 2019).",
      "startOffset" : 282,
      "endOffset" : 354
    }, {
      "referenceID" : 8,
      "context" : ", 2017) and have been demonstated to successfully transfer knowledge between languages, benefiting both the low and high resource languages (Dabre et al., 2020).",
      "startOffset" : 140,
      "endOffset" : 160
    }, {
      "referenceID" : 41,
      "context" : "It is common practice to perform a delexicalisation (Wen et al., 2015) of the MR, in order to facilitate the NLG model’s generalisation to rare and unseen input; lack of generalisation is a main drawback of neural models (Goyal et al.",
      "startOffset" : 52,
      "endOffset" : 70
    }, {
      "referenceID" : 18,
      "context" : ", 2015) of the MR, in order to facilitate the NLG model’s generalisation to rare and unseen input; lack of generalisation is a main drawback of neural models (Goyal et al., 2016) but is particularly prominent in concept-to-text.",
      "startOffset" : 158,
      "endOffset" : 178
    }, {
      "referenceID" : 18,
      "context" : "Finally, relexicalisation also remains a naive process (see Figure 2) that ignores how context should effect the morphology of the MR value when it is added to the text (Goyal et al., 2016).",
      "startOffset" : 169,
      "endOffset" : 189
    }, {
      "referenceID" : 24,
      "context" : "Earlier research enabled multilingual generation with no and partial parameter sharing (Luong et al., 2016; Firat et al., 2016), while Johnson et al.",
      "startOffset" : 87,
      "endOffset" : 127
    }, {
      "referenceID" : 15,
      "context" : "Earlier research enabled multilingual generation with no and partial parameter sharing (Luong et al., 2016; Firat et al., 2016), while Johnson et al.",
      "startOffset" : 87,
      "endOffset" : 127
    }, {
      "referenceID" : 39,
      "context" : "To enable output in a different language, a number of Zero-Shot methods have been proposed with the most common practice being to directly use an MT model to translate the output into the target language (Wan et al., 2010; Shen et al., 2018; Duan et al., 2019).",
      "startOffset" : 204,
      "endOffset" : 260
    }, {
      "referenceID" : 10,
      "context" : "To enable output in a different language, a number of Zero-Shot methods have been proposed with the most common practice being to directly use an MT model to translate the output into the target language (Wan et al., 2010; Shen et al., 2018; Duan et al., 2019).",
      "startOffset" : 204,
      "endOffset" : 260
    }, {
      "referenceID" : 25,
      "context" : "The MT model can be finetuned on task-specific data when those are available (Miculicich et al., 2019).",
      "startOffset" : 77,
      "endOffset" : 102
    }, {
      "referenceID" : 38,
      "context" : "To improve delexicalisation accuracy, n-gram matching (Trisedya et al., 2018) has been proposed as an alternative.",
      "startOffset" : 54,
      "endOffset" : 77
    }, {
      "referenceID" : 16,
      "context" : "Thanks to its simplicity and efficacy, delexicalisation is widely used by many systems, including the winning systems of major concept-to-text NLG shared tasks (Gardent et al., 2017; Dušek et al., 2018; Castro Ferreira et al., 2020).",
      "startOffset" : 160,
      "endOffset" : 232
    }, {
      "referenceID" : 11,
      "context" : "Thanks to its simplicity and efficacy, delexicalisation is widely used by many systems, including the winning systems of major concept-to-text NLG shared tasks (Gardent et al., 2017; Dušek et al., 2018; Castro Ferreira et al., 2020).",
      "startOffset" : 160,
      "endOffset" : 232
    }, {
      "referenceID" : 34,
      "context" : "The objective is to break down words into smaller units, reducing the vocabulary and the number of unseen tokens (Sennrich et al., 2016).",
      "startOffset" : 113,
      "endOffset" : 136
    }, {
      "referenceID" : 16,
      "context" : "using BytePair-Encoding (BPE) subword units (Gardent et al., 2017; Zhang et al., 2018) or using characters as basic units (Goyal et al.",
      "startOffset" : 44,
      "endOffset" : 86
    }, {
      "referenceID" : 42,
      "context" : "using BytePair-Encoding (BPE) subword units (Gardent et al., 2017; Zhang et al., 2018) or using characters as basic units (Goyal et al.",
      "startOffset" : 44,
      "endOffset" : 86
    }, {
      "referenceID" : 18,
      "context" : ", 2018) or using characters as basic units (Goyal et al., 2016; Agarwal and Dymetman, 2017; Deriu and Cieliebak, 2018), underperforms against delexicalisation.",
      "startOffset" : 43,
      "endOffset" : 118
    }, {
      "referenceID" : 0,
      "context" : ", 2018) or using characters as basic units (Goyal et al., 2016; Agarwal and Dymetman, 2017; Deriu and Cieliebak, 2018), underperforms against delexicalisation.",
      "startOffset" : 43,
      "endOffset" : 118
    }, {
      "referenceID" : 9,
      "context" : ", 2018) or using characters as basic units (Goyal et al., 2016; Agarwal and Dymetman, 2017; Deriu and Cieliebak, 2018), underperforms against delexicalisation.",
      "startOffset" : 43,
      "endOffset" : 118
    }, {
      "referenceID" : 33,
      "context" : "Copy mechanism is another method to address unseen input, by allowing the decoder of an encoder-decoder model to draw a token directly from the input sequence instead of generating it from the decoder vocabulary (See et al., 2017).",
      "startOffset" : 212,
      "endOffset" : 230
    }, {
      "referenceID" : 6,
      "context" : "While applications of the copy mechanism in concept-to-text NLG have achieved overall good results (Chen, 2018; Elder et al., 2018; Gehrmann et al., 2018), when dealing with rare and unseen inputs delexicalisation is still preferable (Shimorina and Gardent, 2018).",
      "startOffset" : 99,
      "endOffset" : 154
    }, {
      "referenceID" : 12,
      "context" : "While applications of the copy mechanism in concept-to-text NLG have achieved overall good results (Chen, 2018; Elder et al., 2018; Gehrmann et al., 2018), when dealing with rare and unseen inputs delexicalisation is still preferable (Shimorina and Gardent, 2018).",
      "startOffset" : 99,
      "endOffset" : 154
    }, {
      "referenceID" : 17,
      "context" : "While applications of the copy mechanism in concept-to-text NLG have achieved overall good results (Chen, 2018; Elder et al., 2018; Gehrmann et al., 2018), when dealing with rare and unseen inputs delexicalisation is still preferable (Shimorina and Gardent, 2018).",
      "startOffset" : 99,
      "endOffset" : 154
    }, {
      "referenceID" : 35,
      "context" : ", 2018), when dealing with rare and unseen inputs delexicalisation is still preferable (Shimorina and Gardent, 2018).",
      "startOffset" : 87,
      "endOffset" : 116
    }, {
      "referenceID" : 2,
      "context" : "LAD employs LASER (Artetxe and Schwenk, 2019) to generate language agnostic sentence embeddings of the values and n-grams, and calculates their distance via cosine similarity.",
      "startOffset" : 18,
      "endOffset" : 45
    }, {
      "referenceID" : 41,
      "context" : "Concept-to-text NLG systems do not generally require ordered input (Wen et al., 2015).",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 36,
      "context" : "For our purposes, we will be using a later work (Shimorina et al., 2019) that introduced a machine translated Russian version of WebNLG17, a part of which was post edited by humans.",
      "startOffset" : 48,
      "endOffset" : 72
    }, {
      "referenceID" : 43,
      "context" : ", 2020) and CrossWOZ (Zhu et al., 2020) are datasets of dialogue acts and corresponding utterances in English and Chinese respectively.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 27,
      "context" : "0 (Mrkšić et al., 2017) is also a dialogue dataset with utterances available in three languages: English, Italian and German.",
      "startOffset" : 2,
      "endOffset" : 23
    }, {
      "referenceID" : 38,
      "context" : "In addition to LAD, where these components are incrementally removed, we explore how their addition would influence exact and n-gram delexicalisation (Trisedya et al., 2018).",
      "startOffset" : 150,
      "endOffset" : 173
    }, {
      "referenceID" : 23,
      "context" : "Here we explore the generalisation capabilities of multilingual pretrained models, by replacing the underlying NLG model with mBART (Liu et al., 2020), a multilingual denoising autoencoder pretrained on a large-scale dataset containing 25 languages (CC25).",
      "startOffset" : 132,
      "endOffset" : 150
    } ],
    "year" : 2021,
    "abstractText" : "Concept-to-text Natural Language Generation is the task of expressing an input meaning representation in natural language. Previous approaches in this task have been able to generalise to rare or unseen instances by relying on a delexicalisation of the input. However, this often requires that the input appears verbatim in the output text. This poses challenges in multilingual settings, where the task expands to generate the output text in multiple languages given the same input. In this paper, we explore the application of multilingual models in concept-to-text and propose Language Agnostic Delexicalisation, a novel delexicalisation method that uses multilingual pretrained embeddings, and employs a character-level postediting model to inflect words in their correct form during relexicalisation. Our experiments across five datasets and five languages show that multilingual models outperform monolingual models in concept-to-text and that our framework outperforms previous approaches, especially in low resource conditions.",
    "creator" : "LaTeX with hyperref"
  }
}