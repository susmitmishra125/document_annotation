{
  "name" : "2021.acl-long.223.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Guiding Teacher Forcing with Seer Forcing for Neural Machine Translation",
    "authors" : [ "Yang Feng", "Shuhao Gu", "Dengji Guo", "Zhengxin Yang", "Chenze Shao" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2862–2872\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2862"
    }, {
      "heading" : "1 Introduction",
      "text" : "Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) has achieved great success and is drawing larger attention recently. Most NMT models are under the attention-based encoder-decoder framework which assumes there is a common semantic space between the source and target languages. The encoder encodes the source sentence to the common space to get its meaning, and the decoder projects the source meaning to the target space to generate corresponding target words. Whenever generating a target word at a time step, the decoder\n∗The code: https://github.com/ictnlp/SeerForcingNMT\nneeds to retrieve the attended source information and then decodes into a target word. The underline principle which makes sure the framework works is that the information hold by the source sentence and its target counterpart is equivalent. Thus the translation procedure can be considered to decompose source information into different pieces and then to convert each piece to a proper target word according to bilingual context. When all the information encoded in the source sentence is throughly processed, the whole translation has been generated.\nNeural machine translation models are usually trained via maximum likelihood estimation (MLE) (Johansen and Juselius, 1990) and the operation form is known as teacher forcing (Williams and Zipser, 1989). The teacher forcing strategy performs one-step-ahead predictions with the past ground truth words fed as context and forces the distribution of the next prediction to approach a 0-1 distribution where the probability of the next ground truth word corresponds to 1 and others to 0. In this way, the predicted sequence is trained to be close to the ground truth sequence. From the perspective of information division, the function of teacher forcing is to teach the translation model how to segment source information and derive the ground truth word from the source information at a maximum probability.\nHowever, teacher forcing can only provide upto-now ground truth words for one-step-ahead predictions and hence lacks global planning for the future. This will result in local optimization especially when the next prediction is highly related to the future. Besides, as the translation grows, the previous prediction errors will be accumulated and affect later predictions (Zhang et al., 2019c). This is the important reason why NMT models cannot always produce the ground truth sequence during training. Therefore, it is more possible to achieve\nglobal optimization by getting to know the future ground truth words. This can lead to better crossattention to the source sentence and thus better information devision. But unfortunately, ground truth can be only obtained during training and we cannot inference with future ground truth at test.\nTo address this problem, we introduce an additional seer decoder into the encoder-decoder framework to integrate future information. During training, the seer decoder is used to guide the behaviors of the conventional decoder while at test the translation model only inferences with the conventional decoder without introducing any extra parameters and calculation cost. Specifically, the conventional decoder only gets past information participating in the next prediction, while the seer decoder has both the past and future ground truth words engaged in the next prediction. Both decoders are trained to generate ground truth via MLE and meanwhile the conventional decoder is forced to simulate the behaviors of the seer decoder via knowledge distillation (Buciluǎ et al., 2006; Hinton et al., 2015). In this way, at test the conventional decoder can perform like the seer decoder as if it knew the future translation.\nWe conducted experiments on two small data sets (Chinese-English and English-Romanian) and two big data sets (Chinese-English and EnglishGerman) and the experiment results show that our method can outperform strong baselines on all the data sets. In addition, we also compared different mechanisms of transferring knowledge and found that knowledge distillation is more effective than adversarial learning and L2 regularization. To the best of our knowledge, this paper is the first to explore the effects of the three mechanisms simultaneously in machine translation."
    }, {
      "heading" : "2 The Proposed Method",
      "text" : "We introduce our method on the basis of Transformer which is under the encoder-decoder framework (Vaswani et al., 2017). Our model consists of three components: the encoder, the conventional decoder and the seer decoder. The architecture is shown in Figure 1. The encoder and the conventional decoder work in the same way as the corresponding components of Transformer do. The seer decoder integrates future ground truth information into its self-attention representation and calculates cross-attention over source hidden states with the self-attention representation as the query. During training, the encoder is shared by the two decoders and both decoders perform predictions to generate ground truth. The behaviors of the conventional decoder are guided by the seer decoder via knowledge distillation. If the conventional decoder can predict a similar distribution as the seer decoder, we think the conventional decoder performs like the seer decoder. Then we can only use the conventional decoder for test.\nThe details of the encoder and the conventional decoder can be got from Vaswani et al. (2017). Assume the input sequence is x = (x1, ..., xJ), the ground truth sequence is y∗ = (y∗1, ..., y ∗ I ) and the generated translation is y = (y1, ..., yI). We will give more description to the seer decoder and the training in what follows."
    }, {
      "heading" : "2.1 The Seer Decoder",
      "text" : "Although we feed the future ground truth words to the seer decoder, we will not tell it the next ground truth word to be generated, in case it will only learn a copy operation, not how to derive a word. Considering efficiency, the seer decoder does not integrate the past and future ground truth information with a unique decoder , but two separate subdecoders. As a result, the seer decoder consists of three components: the past subdecoder, the future subdecoder and the fusion layer. The architecture of the seer decoder is given in Figure 2. The past and future subdecoders are employed to decode the past and future ground truth information into hidden states respectively and the fusion layer is used to fuse the output of the past and future subdecoders and calculate the final hidden state for the next prediction.\nThe past subdecoder is composed ofN−1 layers and each layer has three sublayers which are the multi-head sublayer, the cross-attention sublayer and the feed-forward network (FNN) sublayer, the same as Transformer. The multi-attention sublayer accepts the whole ground truth sequence as the input and applies a mask matrix Mp to make sure only the past ground truth words attend the selfattention. Specifically, to generate the i-th target word, its corresponding mask vector in the mask matrix Mp is set to mask the words y∗i , y ∗ i+1, ..., y ∗ I . Then after the cross-attention sublayer and the FFN sublayer, the past subdecoder output a sequence of past hidden states, the packed matrix of which is denoted as Hp.\nThe future subdecoder has the same structure as the past subdecoder except for the mask matrix. The future subdecoder also has the whole ground truth sequence as the input but employs a different mask matrix Mf to only remain the future ground truth information. To generate the i-th target word, the corresponding mask vector in Mf masks the words y∗1, ..., y ∗ i−1, y ∗ i . The packed matrix of the future hidden states generated by the future subdecoder is denoted as Hf .\nThe fusion layer is composed of four sublayers: the multi-head sublayer, the linear sublayer, the cross-attention sublayer and the FFN sublayer. Except the linear sublayer, the rest three sublayers works in the same way as Transformer does. The multi-head sublayer encodes the outputs of the past and future subdecoders separately with the mask matrix Mp and Mf , and the packed matrix of their output are denoted as H′p and H ′ f respec-\ntively. Then we reverse the order of the vectors in H′f to get H ′′ f , so that the same index in H ′ p and H′′f can correspond to the past and future representation needed for the same prediction. Assume H′f = [h ′ f1;h ′ f2; ...;h ′ fI ], then its reversed matrix is H′′f = [h ′ fI ; ...;h ′ f2;h ′ f1]. The linear sublayer fuses H′p and H ′′ f via a linear transformation as\nA = WpH ′ p + WfH ′′ f (1)\nNow we can think each representation in the matrix A incorporates the past and future information for its corresponding prediction. Then after the crossattention sublayer over the outputs of the encoder and then the FFN sublayer, we can get the target hidden states produced by the seer decoder as Ss = [ss1 ; ...; ssI ]\nT . Then the probability to generate the target word yi is\nps(yi|y∗>i,y∗<i,x) ∝ exp (Wossi) (2)\nNote that the past and the future subdecoders share the same set of parameters, and the same linear transformation matrix Wo is applied to the outputs of the conventional and seer decoders."
    }, {
      "heading" : "3 Training",
      "text" : "In our method, only the conventional decoder is employed for test and the seer decoder is only used to guide the conventional decoder during training. Given a sentence pair 〈x,y∗〉 in the training set, the conventional decoder and the seer decoder can predict a distribution for target position i as pc(yi|y∗<i,x) and ps(yi|y∗>i,y∗<i,x), respectively. The two decoders are both trained by comparing its predicted distribution with the 0-1 distribution of the ground truth word by minimizing the cross entropy, that is to maximize the likelihood of the corresponding ground truth word. As the two decoders involve different information for next prediction, we call the training strategy teacher forcing and seer forcing, respectively. The cross-entropy loss for the conventional decoder is\nLc = − K∑\nk=1 Ik∑ i=1 log pc(y ∗ i |y∗<i,x), (3)\nand the cross-entropy loss for the seer decoder is\nLs = − K∑\nk=1 Ik∑ i=1 log ps(y ∗ i |y∗>i,y∗<i,x). (4)\nwhere K is the size of the training set and Ik is the length of the k-th target sentence.\nThe conventional decoder is further trained to get close to the distribution of the seer decoder via knowledge distillation. In knowledge distillation, the conventional decoder (the student) has to not only match the one-hot ground truth word, but fit the distribution over the target vocabulary V drawn by the seer decoder (the teacher). The knowledge distillation loss can be formalized as\nLkd = − K∑\nk=1 Ik∑ i=1 |V|∑ l=1 ps(yi = l|y∗>i,y∗<i,x)\n× log pc(yi = l|y∗<i,x) (5)\nwhere |V| is the size of the target vocabulary. The final training loss is\nL = Ls + λLc + (1− λ)Lkd . (6) Different from the conventional knowledge distillation which first trains the teacher via cross entropy against ground truth, then fixes the teacher and only trains the student, we train all the parameters from the scratch, but we still follow the above rule to keep the teacher (i.e. the seer decoder) unchanged in the process of distillation. To do this, we do not update the parameters of the seer decoder through the loss Lkd, that is, we only back propagate gradients to the seer decoder throughLs, but not through Lkd."
    }, {
      "heading" : "4 Related Work",
      "text" : "Reinforcement-learning-based methods also encode future information in the rewards to supervise fine-tuning of the translation model. The rewards are worked out either by sampling future translation with the REINFORCE algorithm (Williams, 1992; Yu et al., 2017; Yang et al., 2018; Shao et al., 2019), or by directly calculating a value with the actor-critic algorithm (Bahdanau et al., 2016; Li et al., 2017). This set of methods only give a weak supervision to the NMT model through rewards and suffer from unstable training. In contrast, Shao et al. (2018) propose to train autoregressive NMT with the probabilistic n-gram based GLEU (Wu et al., 2016) and Shao et al. (2020) propose to minimize the bag-of-ngrams difference for nonautoregressive NMT so that the two methods can abandon reinforcement learning and perform training directly by gradient descent.\nAnother set of methods introduce future information into inference with additional pass of decoding or extra components at test. Niehues et al.\n(2016), Xia et al. (2017), Hassan et al. (2018) and Zhang et al. (2018) proposed a two-pass decoding algorithm to first generate a draft translation and then generate final translation referring to the draft. Geng et al. (2018) expand this line of methods by performing an adaptive multi-pass decoding where the number of decoding passes is determined by a policy network. Liu et al. (2016a), Liu et al. (2016b), Hoang et al. (2017), Zhang et al. (2019d) and He et al. (2019) perform bidirectional decoding simultaneously and the two decoders correlate to each other via an agreement term or a regularization term in the loss. Zhou et al. (2019a) , Zhou et al. (2019b) and Zhang et al. (2019b) also maintain a forward decoder and a backward decoder to decode simultaneously but they interact to each other when making predictions. Zhang et al. (2019a) introduce a future-aware vector at test which is learned via the knowledge distillation framework during training. The difference between this set of methods and our method is that our method does not require any other cost at test and is easy to use.\nThere are some other works which integrate future information during training while only perform one-pass decoding. Serdyuk et al. (2018) introduce a twin network to perform bidirectional decoding simultaneously during training and force the hidden states generated by the two decoders to be consistent, then at inference it can only use the forward decoder. But in this method the two decoders act as a counterpart to each other and no decoder plays a role of teacher, which determines that it can only be trained via L2 regularization, not knowledge distillation which has proven in the experiments more effective than L2 regularization. Feng et al. (2020) introduce an evaluation module to give each translation more reasonable evaluation when it cannot match the ground truth. The evaluation is conducted from the perspective of fluency and faithfulness which both need the participation of past and future information. The difference from the method proposed in this paper is their method uses self-generated translation as past information and does not train with knowledge distillation.\nSome researchers work in another perspective by introducing future information. Zhang et al. (2020b) propose to employ future source information to guide simultaneous machine translation with knowledge distillation, so that the incompleteness of source can be mitigated. Zheng et al. (2018) and\nZheng et al. (2019) propose to model past and future information for the source to help the decoder focus on untranslated source information."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Settings",
      "text" : ""
    }, {
      "heading" : "5.1.1 Data Preparation",
      "text" : "We conducted experiments on two small data sets and two big data sets.\nSmall Data Sets Chinese→English The training set consists of about 1.25M sentence pairs from LDC corpora with 27.9M Chinese words and 34.5M English words respectively1. We used MT02 for validation and MT03, MT04, MT05, MT06, MT08 for test. We tokenized and lowercased English sentences using the Moses scripts2, and segmented the Chinese sentences with the Stanford Segmentor3. The two sides were further segmented into subword units using Byte-Pair Encoding(BPE) (Sennrich et al., 2016) with 30K merge operations. 32K size of the Chinese dictionary and 29K size of the English dictionary were built for the two sides.\nEnglish→Romanian We used the preprocessed version of WMT16 En-Ro dataset released by Lee et al. (2018) which includes 0.6M sentence pairs. We used news-dev 2016 for validation and newstest 2016 for test. The two languages share the 35K size of the joint vocabulary generated with 40K merge operations of BPE on the combined data.\nBig Data Sets Chinese→English The training data is from WMT 2017 Zh-En translation tasks that contains 20.18M sentence pairs after deleting duplicate ones. The newsdev2017 was used as the development set and newstest2017 was used as the test set. To avoid the effects of the translationese (Graham et al., 2019), we also tested the methods on the newstest2019 test set. We tokenized and truecased the English sentences with Moses scripts. For the Chinese data, we performed word segmentation by using Stanford Segmenter. 32K BPE sizes were applied to the training data seperately and then we filtered out the sentences which are longer than 128 sub-words. 44K size of the Chinese dictionary and\n1The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.\n2http://www.statmt.org/moses/ 3https://nlp.stanford.edu/\n33K size of the English dictionary were built based on the corresponding data.\nEnglish→German The training data is from WMT2016 which consists of about 4.5M sentences pairs with 118M English words and 111M German words. The newstest2014 was used as the development set and newstest2016 and newstest2019 were used as the test sets. The two languages share the 32K size of the joint vocabulary generated with 30K merge operations of BPE on the combined data."
    }, {
      "heading" : "5.1.2 Systems",
      "text" : "TRANSFORMER We used an open-source toolkit called Fairseq-py released by Facebook (Ott et al., 2019) which was implemented strictly following Vaswani et al. (2017).\nRL-NMT We trained Transformer under the reinforcement learning framework using the REINFORCE algorithm (Williams, 1992) with the BLEU as the rewards. The implementation details for the RL part is the same as Yang et al. (2018).\nABDNMT Our implementation of Zhang et al. (2018) based on Transformer.\nTWINNET Our implementation of Serdyuk et al. (2018) based on Transformer. The weight of L2 loss was 0.2 .\nEVANMT Our implementation of Feng et al. (2020).\nSEER+L2 Seer forcing with L2 regularization. Similar to TWINNET, we set L2 =∑K\nk=1 ∑Ik i=1 ‖g(sti) − ssi)‖2 where g is a linear transformation. We first pretrained the two decoders together only withL = Lt+Ls, then trained them with the loss of L = Lt + Ls + αL2 where α = 0.2, too. Please note that the L2 loss did not update the seer decoder and the encoder so that the conventional decoder would approach the seer decoder, which followed Serdyuk et al. (2018).\nSEER+AL Seer forcing with adversarial learning. A discriminator is employed to distinguish the hidden state sequences generated by the conventional decoder and the seer decoder. The discriminator is based on CNN, implemented according to Gu et al. (2019). The translation model and the discriminator are trained jointly via a gradient reversal layer just like our method. The loss is L = Lt + Ls + αLd where Ld is the loss of the discriminator and α = 0.3 on the EN→RO data set and α = 0.2 on the other data sets.\nOur Method Implemented based on Fairseqpy. The weight λ in Equation 6 for the small Chinese→English data set is set to 0.25, and for other data sets is set to 0.5.\nAll the Transformer-based systems have the same configuration as the base model described in Vaswani et al. (2017) except that dropout rate is 0.3. The translation quality was evaluated with BLEU (Papineni et al., 2002) with n=4 using the SacreBLEU tool (Post, 2018)4, where small data sets employ case-insensitive BLEU while big data sets use case-sensitive BLEU."
    }, {
      "heading" : "5.2 Main Results",
      "text" : "We compare our method with other methods that can make global planning, including the reinforcement-based method (RL-NMT), the twopass decoding method (ABDNMT), twin networks which match past and future information (TWINNET) and the NMT model with an evaluate module to evaluate fluency and faithfulness (EVANMT). In addition, we also explore learning mechanisms which can transfer knowledge from the seer decoder to the conventional decoder, including L2 regularization (SEER+L2), adversarial learning (SEER+AL) and knowledge distillation (Our Method).\n4BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.3.6\nWe report results together with training time on the small and big data sets in Table 1 and Table 2, respectively.5 As for different methods, in the small data sets, RL-NMT can only get small improvements over Transformer which are in line with the results reported in Wu et al. (2018), and ABDNMT cannot get consistent improvements over Transformer with an obvious difference on the EN→RO data set and a small difference on the CN→EN data set. TWINNET can get comparable BLEU scores with our method on the small data sets but mostly negative difference on the big data sets. EVANMT can achieve consistent improvements and greater improvements on the EN→DE data set. For the learning mechanisms, knowledge distillation show consistent superiority over L2 regularization and adversarial learning, which is remarkable especially on the big data sets. Adversarial learning can bring improvements over Transformer on all the data sets while L2 regularization acts unstable on the big data sets. In summary, our method proved to be effective not only in the term of the architecture but also in the learning mechanism.\n5Please note that there is no comparability between our results and that of Zhang et al. (2019a) because we used different validation and test sets."
    }, {
      "heading" : "5.3 The Superiority of the Seer Decoder",
      "text" : "To use seer forcing to guide teacher forcing, it should be ensured that the seer decoder can outperform the conventional decoder. To verify this, we trained the two decoders together with the loss L = Lt + Ls without knowledge distillation. Then we evaluated their performance on the small Chinese-English translation task as follows. Both decoders are fed with ground truth words as context at test so that they can inference in the same way as at training, where the conventional decoder uses the past ground truth as context and the seer decoder employs the past and future ground truth words as context in the past and future subdecoders.\nBesides translation performance, we also check the superiority of seer decoder in target language modeling. We do this by dropping out crossattention so that the decoder can only generate translation based on target language model. In this way, the translation performance without crossattention can demonstrate the ability of the two decoders in target language modeling.\nWe used the first reference of the test set as ground truth and calculated BLEU scores only with this reference. From the results in Table 3, we can see that whether with or without cross-attention the seer decoder can make super large improvements over the conventional decoder consistently on all the test sets. However, without cross-attention, the BLEU scores of both decoders decrease dramatically which means language model information is not enough for the translation task. Therefore, we can conclude the seer decoder acts much better in target language modeling and cross-language projection and it is reasonable to use the seer decoder as the guider."
    }, {
      "heading" : "5.4 The Distillation of Future Information",
      "text" : "As the seer decoder achieves its superiority with the help of future target information, we hope that the conventional decoder can learn future information from the seer decoder with knowledge distillation.\nTo check this, we tested whether the hidden states of the conventional decoder could derive more future ground truth words after knowledge distillation. The underlying belief is that the future ground information transferred from the seer decoder can help the conventional decoder derive more future ground truth words.\nAssuming the hidden states generated by the conventional decoder are St = [st1 ; ...; stI ]\nT , the future words for each target position i can be predicted with the distribution\nPwi ∼ softmax(Wwsti) (7)\nwhere Ww is the weight matrix. During training, we can get the bag of ground truth words for position i as y∗i = {y∗i+1, ..., y∗I} and train Ww with other parameters fixed by maximizing the likelihood of y∗i as\nLw = − K∑ k=1 Ik∑ i=1 ∑ w∈y∗i log pwi(w) (8)\nwhere K is the size of training sentences, Ik is the length of the target sentence and log pwi(w) is the probability of the word w in Equation 7.\nAt test, we select the top best Ibi words according to Equation 7 as the bag of future words bi for position i. As we cannot get the ground truth, the size of bi is calculated approximately as Ibi = max {2, (J − i)× 2} where J is the length of source sentence. As we do not know the target length during prediction, it may occur that i is greater than J and calculating Ibi in this way can ensure bi contains 2 words at least.\nWe conducted experiments on Chinese-English translation and used MT02 as the test set only\nwith the first reference as ground truth. We calculated the accuracy and recall by comparing each bi against each y∗i . The results in Table 4 show the conventional decoder in our method can achieve higher accuracy and recall compared to the decoder of Transformer. This means knowledge distillation does transfer future information from the seer decoder to the conventional decoder."
    }, {
      "heading" : "5.5 The Contribution of Subdecoders",
      "text" : "In the seer decoder of our method, the information from the past and future subdecoders is fused (as shown in Equation 1) to get the final cross-attention. The intuition is that at the beginning stage, the past subdecoder contains less information than the future subdecoder, so the fused information should rely more on the future subdecoder. As the translation gets longer, the information embodied in the past subdecoder grows, and the fused information should depend more on the past subdecoder. To confirm this hypothesis, we calculate the cosine similarity of the vectors in A given in Equation 1 with the corresponding weighted vectors of WpH′p and WfH′′f .\nWe selected 205 sentences the length of which\nranges [15, 25], then calculated the cosine similarities word by word. Then the similarities at the same target position will be averaged and the chart over all the target positions is given in Figure 3. The figure confirms our conjecture that at first, the fused information is highly related to the future information, and over time the similarity to past information increases gradually while the similarity to future information decreases faster."
    }, {
      "heading" : "5.6 Ablation Study",
      "text" : "We have proven that in our method the past and future information collaborate to achieve better global planning. In this section, we will explore the influence of past and future information by separately deleting the future and past subdecoders from the seer decoder. In both cases, only the structure of the seer decoder changes and the whole model is trained with knowledge distillation in the same way. We also remove knowledge distillation loss in which case the seer and conventional decoders only interact via the shared encoder and only optimize their own cross-entropy losses during training. The results are given in Table 5.\nWhen we exclude future or past information, the translation performance decreases dramatically at almost the same extent, but they still have an obvious gain compared to Transformer. This demonstrates that both the past and future information are necessary for global planning. It is interesting that the translation performance still rise without future subdecoder where there is no additional information fed compared to Transformer. The reason may be the conventional and seer decoder can restrict each other to avoid bad behaviors. When knowledge distillation is dropped, the performance decline greatly which means only communicating via the encoder the conventional and seer decoders\nis not enough. Hence we need to introduce knowledge distillation to reinforce the influence of the seer decoder to the conventional decoder."
    }, {
      "heading" : "5.7 Performance with Sentence Length",
      "text" : "As the translation is generated word by word, the translation errors will be accumulated while the the translation grows, which will influence the later prediction. In our method, the conventional decoder can learn future information from the seer decoder and hence it should make better global planning for the whole sequence. From this, we deduce that our method performs better on long sentences than Transformer.\nWe checked this on the NIST CN→EN translation task and split the sentences in all the test sets into 8 bins according to their length. Then we translated for each bin and tested the BLEU scores. The results in Figure 4 show that our method can achieve bigger improvements on longer sentences, especially in the last three bins."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In order to help the NMT model to make good global planning at inference, we propose to introduce a seer decoder which embodies future ground truth to guide the behaviors of the conventional decoder. To this end, we employ the method of knowledge distillation to transfer future information from the seer decoder to the conventional decoder. At test, the conventional decoder can perform translation on its own as if it knew some future information. The experiments indicate our method can outperform strong baselines significantly on four data sets. We are also the first to explore learning mechanisms of knowledge distillation, adversarial learning and L2 regularization and knowledge distillation has proven to be the most effective one."
    }, {
      "heading" : "Acknowledgement",
      "text" : "This paper was supported by National Key R&D Program of China (NO. 2017YFE0192900). Thank Wanying Xie for running experiments of EVANMT. Thank all the anonymous reviewers for the insightful and valuable comments."
    }, {
      "heading" : "In Proceedings of the AAAI Conference on Artificial",
      "text" : "Intelligence, volume 33, pages 443–450.\nZaixiang Zheng, Shujian Huang, Zhaopeng Tu, Xin-Yu Dai, and Jiajun Chen. 2019. Dynamic past and future for neural machine translation. arXiv preprint arXiv:1904.09646.\nZaixiang Zheng, Hao Zhou, Shujian Huang, Lili Mou, Xinyu Dai, Jiajun Chen, and Zhaopeng Tu. 2018. Modeling past and future for neural machine translation. Transactions of the Association for Computational Linguistics, 6:145–157.\nLong Zhou, Jiajun Zhang, and Chengqing Zong. 2019a. Synchronous bidirectional neural machine translation. Transactions of the Association for Computational Linguistics, 7:91–105.\nLong Zhou, Jiajun Zhang, Chengqing Zong, and Heng Yu. 2019b. Sequence generation: From both sides to the middle. Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence."
    } ],
    "references" : [ {
      "title" : "An actor-critic algorithm for sequence prediction",
      "author" : [ "Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Model compression",
      "author" : [ "Cristian Buciluǎ", "Rich Caruana", "Alexandru Niculescu-Mizil." ],
      "venue" : "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535–541.",
      "citeRegEx" : "Buciluǎ et al\\.,? 2006",
      "shortCiteRegEx" : "Buciluǎ et al\\.",
      "year" : 2006
    }, {
      "title" : "Clause restructuring for statistical machine translation",
      "author" : [ "Michael Collins", "Philipp Koehn", "Ivona Kucerova." ],
      "venue" : "Proceedings of ACL 2015, pages 531– 540.",
      "citeRegEx" : "Collins et al\\.,? 2005",
      "shortCiteRegEx" : "Collins et al\\.",
      "year" : 2005
    }, {
      "title" : "Modeling fluency and faithfulness for diverse neural machine translation",
      "author" : [ "Yang Feng", "Wanying Xie", "Shuhao Gu", "Chenze Shao", "Wen Zhang", "Zhengxin Yang", "Dong Yu." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34,",
      "citeRegEx" : "Feng et al\\.,? 2020",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2020
    }, {
      "title" : "Convolutional sequence to sequence learning",
      "author" : [ "Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N Dauphin." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1243–1252. JMLR. org.",
      "citeRegEx" : "Gehring et al\\.,? 2017",
      "shortCiteRegEx" : "Gehring et al\\.",
      "year" : 2017
    }, {
      "title" : "Adaptive multi-pass decoder for neural machine translation",
      "author" : [ "Xinwei Geng", "Xiaocheng Feng", "Bing Qin", "Ting Liu." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 523–532.",
      "citeRegEx" : "Geng et al\\.,? 2018",
      "shortCiteRegEx" : "Geng et al\\.",
      "year" : 2018
    }, {
      "title" : "Translationese in machine translation evaluation",
      "author" : [ "Yvette Graham", "Barry Haddow", "Philipp Koehn." ],
      "venue" : "CoRR, abs/1906.09833.",
      "citeRegEx" : "Graham et al\\.,? 2019",
      "shortCiteRegEx" : "Graham et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving domain adaptation translation with domain invariant and specific information",
      "author" : [ "Shuhao Gu", "Yang Feng", "Qun Liu." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Gu et al\\.,? 2019",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2019
    }, {
      "title" : "Achieving human parity on automatic chinese to english news",
      "author" : [ "Hany Hassan", "Anthony Aue", "Chang Chen", "Vishal Chowdhary", "Jonathan Clark", "Christian Federmann", "Xuedong Huang", "Marcin Junczys-Dowmunt", "William Lewis", "Mu Li" ],
      "venue" : null,
      "citeRegEx" : "Hassan et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Hassan et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-agent learning for neural machine translation",
      "author" : [ "Zhongjun He", "Hua Wu", "Haifeng Wang" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
      "citeRegEx" : "He et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2019
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean." ],
      "venue" : "arXiv preprint arXiv:1503.02531.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Towards decoding as continuous optimisation in neural machine translation",
      "author" : [ "Cong Duy Vu Hoang", "Gholamreza Haffari", "Trevor Cohn." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 146–156.",
      "citeRegEx" : "Hoang et al\\.,? 2017",
      "shortCiteRegEx" : "Hoang et al\\.",
      "year" : 2017
    }, {
      "title" : "Maximum likelihood estimation and inference on cointegration with applications to the demand for money",
      "author" : [ "Søren Johansen", "Katarina Juselius." ],
      "venue" : "Oxford Bulletin of Economics and statistics, 52(2):169–210.",
      "citeRegEx" : "Johansen and Juselius.,? 1990",
      "shortCiteRegEx" : "Johansen and Juselius.",
      "year" : 1990
    }, {
      "title" : "Recurrent continuous translation models",
      "author" : [ "Nal Kalchbrenner", "Phil Blunsom." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709.",
      "citeRegEx" : "Kalchbrenner and Blunsom.,? 2013",
      "shortCiteRegEx" : "Kalchbrenner and Blunsom.",
      "year" : 2013
    }, {
      "title" : "Deterministic non-autoregressive neural sequence modeling by iterative refinement",
      "author" : [ "Jason Lee", "Elman Mansimov", "Kyunghyun Cho." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1173–",
      "citeRegEx" : "Lee et al\\.,? 2018",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning to decode for future success",
      "author" : [ "Jiwei Li", "Will Monroe", "Dan Jurafsky." ],
      "venue" : "arXiv preprint arXiv:1701.06549.",
      "citeRegEx" : "Li et al\\.,? 2017",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "Agreement on targetbidirectional lstms for sequence-to-sequence learning",
      "author" : [ "Lemao Liu", "Andrew Finch", "Masao Utiyama", "Eiichiro Sumita." ],
      "venue" : "Thirtieth AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Liu et al\\.,? 2016a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Agreement on targetbidirectional neural machine translation",
      "author" : [ "Lemao Liu", "Masao Utiyama", "Andrew Finch", "Eiichiro Sumita." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Liu et al\\.,? 2016b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Pre-translation for neural machine translation",
      "author" : [ "Jan Niehues", "Eunah Cho", "Thanh-Le Ha", "Alex Waibel." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1828–1836.",
      "citeRegEx" : "Niehues et al\\.,? 2016",
      "shortCiteRegEx" : "Niehues et al\\.",
      "year" : 2016
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of NAACL-HLT 2019: Demonstrations.",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "A call for clarity in reporting BLEU scores",
      "author" : [ "Matt Post." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186– 191, Belgium, Brussels. Association for Computational Linguistics.",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training",
      "author" : [ "Weizhen Qi", "Yu Yan", "Yeyun Gong", "Dayiheng Liu", "Nan Duan", "Jiusheng Chen", "Ruofei Zhang", "Ming Zhou." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods",
      "citeRegEx" : "Qi et al\\.,? 2020",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Twin networks: Matching the future for sequence generation",
      "author" : [ "Dmitriy Serdyuk", "Nan Rosemary Ke", "Alessandro Sordoni", "Adam Trischler", "Chris Pal", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Serdyuk et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Serdyuk et al\\.",
      "year" : 2018
    }, {
      "title" : "Greedy search with probabilistic n-gram matching for neural machine translation",
      "author" : [ "Chenze Shao", "Xilin Chen", "Yang Feng." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4778–4784.",
      "citeRegEx" : "Shao et al\\.,? 2018",
      "shortCiteRegEx" : "Shao et al\\.",
      "year" : 2018
    }, {
      "title" : "Retrieving sequential information for non-autoregressive neural machine translation",
      "author" : [ "Chenze Shao", "Yang Feng", "Jinchao Zhang", "Fandong Meng", "Xilin Chen", "Jie Zhou." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Shao et al\\.,? 2019",
      "shortCiteRegEx" : "Shao et al\\.",
      "year" : 2019
    }, {
      "title" : "Minimizing the bag-ofngrams difference for non-autoregressive neural machine translation",
      "author" : [ "Chenze Shao", "Jinchao Zhang", "Yang Feng", "Fandong Meng", "Jie Zhou." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages",
      "citeRegEx" : "Shao et al\\.,? 2020",
      "shortCiteRegEx" : "Shao et al\\.",
      "year" : 2020
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems, pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J Williams." ],
      "venue" : "Machine learning, 8(3-4):229–256.",
      "citeRegEx" : "Williams.,? 1992",
      "shortCiteRegEx" : "Williams.",
      "year" : 1992
    }, {
      "title" : "A learning algorithm for continually running fully recurrent neural networks",
      "author" : [ "Ronald J Williams", "David Zipser." ],
      "venue" : "Neural computation, 1(2):270– 280.",
      "citeRegEx" : "Williams and Zipser.,? 1989",
      "shortCiteRegEx" : "Williams and Zipser.",
      "year" : 1989
    }, {
      "title" : "A study of reinforcement learning for neural machine translation",
      "author" : [ "Lijun Wu", "Fei Tian", "Tao Qin", "Jianhuang Lai", "TieYan Liu." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3612–3621.",
      "citeRegEx" : "Wu et al\\.,? 2018",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2018
    }, {
      "title" : "Deliberation networks: Sequence generation beyond one-pass decoding",
      "author" : [ "Yingce Xia", "Fei Tian", "Lijun Wu", "Jianxin Lin", "Tao Qin", "Nenghai Yu", "Tie-Yan Liu." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1784–1794.",
      "citeRegEx" : "Xia et al\\.,? 2017",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2017
    }, {
      "title" : "Improving neural machine translation with conditional sequence generative adversarial nets",
      "author" : [ "Zhen Yang", "Wei Chen", "Feng Wang", "Bo Xu." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Seqgan: Sequence generative adversarial nets with policy gradient",
      "author" : [ "Lantao Yu", "Weinan Zhang", "Jun Wang", "Yong Yu." ],
      "venue" : "Thirty-First AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Yu et al\\.,? 2017",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2017
    }, {
      "title" : "Future-aware knowledge distillation for neural machine translation",
      "author" : [ "Biao Zhang", "Deyi Xiong", "Jinsong Su", "Jiebo Luo." ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 27(12):2278–2287.",
      "citeRegEx" : "Zhang et al\\.,? 2019a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Synchronous bidirectional inference for neural sequence generation",
      "author" : [ "Jiajun Zhang", "Long Zhou", "Yang Zhao", "Chengqing Zong." ],
      "venue" : "arXiv preprint arXiv:1902.08955.",
      "citeRegEx" : "Zhang et al\\.,? 2019b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving adversarial text generation by modeling the distant future",
      "author" : [ "Ruiyi Zhang", "Changyou Chen", "Zhe Gan", "Wenlin Wang", "Dinghan Shen", "Guoyin Wang", "Zheng Wen", "Lawrence Carin." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Asso-",
      "citeRegEx" : "Zhang et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Future-guided incremental transformer for simultaneous translation",
      "author" : [ "Shaolei Zhang", "Yang Feng", "Liangyou Li." ],
      "venue" : "arXiv preprint arXiv:2012.12465.",
      "citeRegEx" : "Zhang et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Bridging the gap between training and inference for neural machine translation",
      "author" : [ "Wen Zhang", "Yang Feng", "Fandong Meng", "Di You", "Qun Liu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4334–",
      "citeRegEx" : "Zhang et al\\.,? 2019c",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Asynchronous bidirectional decoding for neural machine translation",
      "author" : [ "Xiangwen Zhang", "Jinsong Su", "Yue Qin", "Yang Liu", "Rongrong Ji", "Hongji Wang." ],
      "venue" : "Thirty-Second AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Regularizing neural machine translation by target-bidirectional agreement",
      "author" : [ "Zhirui Zhang", "Shuangzhi Wu", "Shujie Liu", "Mu Li", "Ming Zhou", "Tong Xu" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Dynamic past and future for neural machine translation",
      "author" : [ "Zaixiang Zheng", "Shujian Huang", "Zhaopeng Tu", "Xin-Yu Dai", "Jiajun Chen." ],
      "venue" : "arXiv preprint arXiv:1904.09646.",
      "citeRegEx" : "Zheng et al\\.,? 2019",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2019
    }, {
      "title" : "Modeling past and future for neural machine translation",
      "author" : [ "Zaixiang Zheng", "Hao Zhou", "Shujian Huang", "Lili Mou", "Xinyu Dai", "Jiajun Chen", "Zhaopeng Tu." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:145–157.",
      "citeRegEx" : "Zheng et al\\.,? 2018",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2018
    }, {
      "title" : "Synchronous bidirectional neural machine translation",
      "author" : [ "Long Zhou", "Jiajun Zhang", "Chengqing Zong." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:91–105.",
      "citeRegEx" : "Zhou et al\\.,? 2019a",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2019
    }, {
      "title" : "Sequence generation: From both sides to the middle",
      "author" : [ "Long Zhou", "Jiajun Zhang", "Chengqing Zong", "Heng Yu." ],
      "venue" : "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence.",
      "citeRegEx" : "Zhou et al\\.,? 2019b",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) has achieved great success and is drawing larger attention recently.",
      "startOffset" : 33,
      "endOffset" : 156
    }, {
      "referenceID" : 29,
      "context" : "Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) has achieved great success and is drawing larger attention recently.",
      "startOffset" : 33,
      "endOffset" : 156
    }, {
      "referenceID" : 1,
      "context" : "Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) has achieved great success and is drawing larger attention recently.",
      "startOffset" : 33,
      "endOffset" : 156
    }, {
      "referenceID" : 5,
      "context" : "Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) has achieved great success and is drawing larger attention recently.",
      "startOffset" : 33,
      "endOffset" : 156
    }, {
      "referenceID" : 30,
      "context" : "Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) has achieved great success and is drawing larger attention recently.",
      "startOffset" : 33,
      "endOffset" : 156
    }, {
      "referenceID" : 13,
      "context" : "Neural machine translation models are usually trained via maximum likelihood estimation (MLE) (Johansen and Juselius, 1990) and the operation form is known as teacher forcing (Williams and",
      "startOffset" : 94,
      "endOffset" : 123
    }, {
      "referenceID" : 41,
      "context" : "Besides, as the translation grows, the previous prediction errors will be accumulated and affect later predictions (Zhang et al., 2019c).",
      "startOffset" : 115,
      "endOffset" : 136
    }, {
      "referenceID" : 2,
      "context" : "the conventional decoder is forced to simulate the behaviors of the seer decoder via knowledge distillation (Buciluǎ et al., 2006; Hinton et al., 2015).",
      "startOffset" : 108,
      "endOffset" : 151
    }, {
      "referenceID" : 11,
      "context" : "the conventional decoder is forced to simulate the behaviors of the seer decoder via knowledge distillation (Buciluǎ et al., 2006; Hinton et al., 2015).",
      "startOffset" : 108,
      "endOffset" : 151
    }, {
      "referenceID" : 30,
      "context" : "We introduce our method on the basis of Transformer which is under the encoder-decoder framework (Vaswani et al., 2017).",
      "startOffset" : 97,
      "endOffset" : 119
    }, {
      "referenceID" : 31,
      "context" : "The rewards are worked out either by sampling future translation with the REINFORCE algorithm (Williams, 1992; Yu et al., 2017; Yang et al., 2018; Shao et al., 2019), or by directly calculating a value with the actor-critic algorithm (Bahdanau et al.",
      "startOffset" : 94,
      "endOffset" : 165
    }, {
      "referenceID" : 36,
      "context" : "The rewards are worked out either by sampling future translation with the REINFORCE algorithm (Williams, 1992; Yu et al., 2017; Yang et al., 2018; Shao et al., 2019), or by directly calculating a value with the actor-critic algorithm (Bahdanau et al.",
      "startOffset" : 94,
      "endOffset" : 165
    }, {
      "referenceID" : 35,
      "context" : "The rewards are worked out either by sampling future translation with the REINFORCE algorithm (Williams, 1992; Yu et al., 2017; Yang et al., 2018; Shao et al., 2019), or by directly calculating a value with the actor-critic algorithm (Bahdanau et al.",
      "startOffset" : 94,
      "endOffset" : 165
    }, {
      "referenceID" : 27,
      "context" : "The rewards are worked out either by sampling future translation with the REINFORCE algorithm (Williams, 1992; Yu et al., 2017; Yang et al., 2018; Shao et al., 2019), or by directly calculating a value with the actor-critic algorithm (Bahdanau et al.",
      "startOffset" : 94,
      "endOffset" : 165
    }, {
      "referenceID" : 0,
      "context" : ", 2019), or by directly calculating a value with the actor-critic algorithm (Bahdanau et al., 2016; Li et al., 2017).",
      "startOffset" : 76,
      "endOffset" : 116
    }, {
      "referenceID" : 16,
      "context" : ", 2019), or by directly calculating a value with the actor-critic algorithm (Bahdanau et al., 2016; Li et al., 2017).",
      "startOffset" : 76,
      "endOffset" : 116
    }, {
      "referenceID" : 24,
      "context" : "The two sides were further segmented into subword units using Byte-Pair Encoding(BPE) (Sennrich et al., 2016) with 30K merge operations.",
      "startOffset" : 86,
      "endOffset" : 109
    }, {
      "referenceID" : 7,
      "context" : "To avoid the effects of the translationese (Graham et al., 2019), we also tested the methods on the newstest2019 test set.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 20,
      "context" : "toolkit called Fairseq-py released by Facebook (Ott et al., 2019) which was implemented strictly following Vaswani et al.",
      "startOffset" : 47,
      "endOffset" : 65
    }, {
      "referenceID" : 31,
      "context" : "reinforcement learning framework using the REINFORCE algorithm (Williams, 1992) with the BLEU as the rewards.",
      "startOffset" : 63,
      "endOffset" : 79
    }, {
      "referenceID" : 3,
      "context" : "** mean the improvements over TRANSFORMER is statistically significant (Collins et al., 2005) (ρ < 0.",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 3,
      "context" : "* and ** mean the improvements over TRANSFORMER is statistically significant (Collins et al., 2005) (ρ < 0.",
      "startOffset" : 77,
      "endOffset" : 99
    }, {
      "referenceID" : 21,
      "context" : "The translation quality was evaluated with BLEU (Papineni et al., 2002) with n=4 using the SacreBLEU tool (Post, 2018)4, where small data sets employ case-insensitive BLEU while big data sets use case-sensitive BLEU.",
      "startOffset" : 48,
      "endOffset" : 71
    }, {
      "referenceID" : 22,
      "context" : ", 2002) with n=4 using the SacreBLEU tool (Post, 2018)4, where small data sets employ case-insensitive BLEU while big data sets use case-sensitive BLEU.",
      "startOffset" : 42,
      "endOffset" : 54
    } ],
    "year" : 2021,
    "abstractText" : "Although teacher forcing has become the main training paradigm for neural machine translation, it usually makes predictions only conditioned on past information, and hence lacks global planning for the future. To address this problem, we introduce another decoder, called seer decoder, into the encoder-decoder framework during training, which involves future information in target predictions. Meanwhile, we force the conventional decoder to simulate the behaviors of the seer decoder via knowledge distillation. In this way, at test the conventional decoder can perform like the seer decoder without the attendance of it. Experiment results on the Chinese-English, English-German and English-Romanian translation tasks show our method can outperform competitive baselines significantly and achieves greater improvements on the bigger data sets. Besides, the experiments also prove knowledge distillation the best way to transfer knowledge from the seer decoder to the conventional decoder compared to adversarial learning and L2 regularization.",
    "creator" : "LaTeX with hyperref"
  }
}