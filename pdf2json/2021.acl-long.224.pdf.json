{
  "name" : "2021.acl-long.224.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Cascade versus Direct Speech Translation: Do the Differences Still Make a Difference?",
    "authors" : [ "Luisa Bentivogli", "Mauro Cettolo", "Marco Gaido", "Alina Karakanta", "Alberto Martinelli", "Matteo Negri", "Marco Turchi", "Bruno Kessler" ],
    "emails" : [ "bentivo@fbk.eu", "cettolo@fbk.eu", "mgaido@fbk.eu", "akarakanta@fbk.eu", "negri@fbk.eu", "turchi@fbk.eu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2873–2887\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2873"
    }, {
      "heading" : "1 Introduction",
      "text" : "Speech translation (ST) is the task of automatically translating a speech signal in a given language into a text in another language. Research on ST dates back to the late eighties and its evolution followed the development of the closely related fields of speech recognition (ASR) and machine translation (MT) that, since the very beginning, provided the main pillars for building the so-called cascade architectures. With the advent of deep learning, the neural networks widely used in ASR and MT have been adapted to develop a new direct ST paradigm. This approach aims to overcome known limitations of the cascade one (e.g. architectural complexity, error propagation) with a single encoder-decoder architecture that directly translates the source signal bypassing intermediate representations.\n∗∗ The work of Alberto Martinelli was carried out during an internship at Fondazione Bruno Kessler.\nUntil now, the consolidated underlying technologies and the richness of available data have upheld the supremacy of cascade solutions in industrial applications. However, architectural simplicity, reduced information loss and error propagation are the ace up the sleeve of the direct approach, which has rapidly gained popularity within the research community in spite of the critical bottleneck represented by data paucity.\nWithin a few years after the first proofs of concept (Bérard et al., 2016; Weiss et al., 2017), the performance gap between the two paradigms has gradually decreased. This trend is mirrored by the findings of the International Workshop on Spoken Language Translation (IWSLT),1 a yearly evaluation campaign where direct systems made their first appearance in 2018. On English-German, for instance, the BLEU difference between the best cascade and direct models dropped from 7.4 points in 2018 (Niehues et al., 2018) to 1.6 points in 2019 (Niehues et al., 2019b). In 2020, participants were allowed to choose between processing a presegmented version of the test set or the one produced by their own segmentation algorithm. As reported in (Ansari et al., 2020), the distance between the two paradigms further decreased to 1.0 BLEU point in the first condition and, for the first time, it was slightly in favor of the best direct model in the second condition, with a small but nonetheless meaningful 0.24 difference.\nSo, quoting Ansari et al. (2020), is the cascade solution still the dominant technology in ST? Has the direct approach closed the huge initial performance gap? Are there systematic differences in the outputs of the two technologies? Are they distinguishable? Answering these questions is more than running an evaluation exercise. It implies pushing research towards a deeper investigation of direct\n1http://iwslt.org\nST, finding a path towards its wider adoption in industrial settings and motivating higher engagement in data exploitation and resource creation to train the data-hungry end-to-end neural systems.\nFor all these reasons, while Ansari et al. (2020) were cautious in drawing firm conclusions, in this paper we delve deeper into the problem with the first thorough comparison between the two paradigms. Working on three language directions (en–de/es/it), we train state-of-the-art cascade and direct models (§3), running them on test data drawn from the MuST-C corpus (Cattoni et al., 2020).\nSystems’ behavior is analysed from different perspectives, by exploiting high-quality post-edits and annotations by professionals. After discussing overall systems’ performance (§4), we move to more fine-grained automatic and manual analyses covering two main aspects: the relation between systems’ performance and specific characteristics of the input audio (§5), and the possible differences in terms of lexical, morphological and word ordering errors (§6). We finally explore whether, due to latent characteristics overlooked by all previous investigations, the output of cascade and direct systems can be distinguished either by a human or by an automatic classifier (§7). Together with a comparative study attesting the parity of the two paradigms on our test data, another contribution of this paper is the release of the manual post-edits that rendered our investigation possible. The data is available at: https://ict.fbk.eu/mustc-post-edits."
    }, {
      "heading" : "2 Background",
      "text" : "Cascade ST. By concatenating ASR and MT components (Stentiford and Steer, 1988; Waibel et al., 1991), cascade ST architectures represent an intuitive solution to achieve reasonable performance and high adaptability across languages and domains. At the same time, however, they suffer from well-known problems related to the concatenation of multiple systems. First, they require ad-hoc training and maintenance procedures for the ASR and MT modules; second, they suffer from error propagation and from the loss of speech information (e.g. prosody) that might be useful to improve final translations. Research has focused on mitigating error propagation by: i) feeding the MT system with ASR data structures (e.g. ASR n-best, lattices or confusion networks) which are more informative than the 1-best output (Lavie et al., 1996; Matusov et al., 2005; Bertoldi and Federico, 2005;\nBeck et al., 2019; Sperber et al., 2019), and ii) making the MT robust to ASR errors, for instance by training it on parallel data incorporating real or emulated ASR errors as in (Peitz et al., 2012; Ruiz et al., 2015; Sperber et al., 2017; Cheng et al., 2019; Di Gangi et al., 2019a). Although the former solutions are effective to some extent, state-of-theart cascade architectures (Pham et al., 2019; Bahar et al., 2020) prefer the latter, as they are simpler to implement and maintain.\nDirect ST. To overcome the limitations of cascade models, Bérard et al. (2016) and Weiss et al. (2017) proposed the first direct solutions bypassing intermediate representations by means of encoderdecoder architectures based on recurrent neural networks. Currently, more effective solutions (Potapczyk and Przybysz, 2020; Bahar et al., 2020; Gaido et al., 2020) rely on ST-oriented adaptations of Transformer (Vaswani et al., 2017) integrating the encoder with: i) convolutional layers to reduce input length, and ii) penalties biasing attention to local context in the encoder self-attention layers (Povey et al., 2018; Sperber et al., 2018; Di Gangi et al., 2019b). Though effective, these architectures have to confront with training data paucity, a critical bottleneck for neural solutions. The problem has been mainly tackled with data augmentation and knowledge transfer techniques. Data augmentation consists in producing artificial training corpora by altering existing datasets or by generating (audio, translation) pairs through speech synthesis or MT (Bahar et al., 2019b; Nguyen et al., 2020; Ko et al., 2015; Jia et al., 2019). Knowledge transfer (Gutstein et al., 2008) consists in passing (here to ST) the knowledge learnt by a neural network trained on closely related tasks (here, ASR and MT). Existing ASR models have been used for encoder pre-training (Bérard et al., 2018; Bansal et al., 2019; Bahar et al., 2019a) and multi-task learning (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Indurthi et al., 2020). Existing neural MT models have been used for decoder pre-training (Bahar et al., 2019a; Inaguma et al., 2020), joint learning (Indurthi et al., 2020; Liu et al., 2020) and knowledge distillation (Liu et al., 2019).\nPrevious comparisons. Most of the works on direct ST also evaluate the proposed solutions against a cascade counterpart. The conclusions, however, are discordant. Looking at recent works, Pino et al. (2019) show similar scores, Indurthi et al. (2020) report higher results for their direct model, while\nInaguma et al. (2020) end up with the opposite finding. The main problems of these comparisons are that: i) not all the architectures are equally optimized, ii) for the sake of fairness in terms of training data, cascade systems are restricted to unrealistic settings with small training corpora that penalize their performance, and iii) evaluation always relies only on automatic metrics computed on single references. The IWSLT campaigns (Niehues et al., 2019a; Ansari et al., 2020) set up a shared evaluation framework where systems built on a large set of training data are optimized to achieve the best performance, independently from the underlying architecture. In the last round, direct models approached, and in one case (Potapczyk and Przybysz, 2020) outperformed, the cascade ones. However, the evaluation was run only on one language pair, by solely relying on automatic metrics and single references. In this paper, we overcome these limitations by comparing the two paradigms on three language pairs, using different metrics, multiple references (including professional postedits) as well as fine-grained automatic and manual analysis procedures."
    }, {
      "heading" : "3 Experimental Setting",
      "text" : ""
    }, {
      "heading" : "3.1 ST Systems",
      "text" : "To maximize the cross-language comparability of our analyses, we built the cascade and direct ST systems for en–de/es/it with the same core technology, based on Transformer. Their good quality is attested by the comparison with the winning system at the IWSLT-20 offline ST task (Bahar et al., 2020),2 which consists of an ensemble of two cascade models scoring 28.8 BLEU on the en-de portion of the MuST-C Common test set. On the same data, our cascade and direct models achieve similar BLEU scores, respectively 28.9 and 29.1 (see Table 1).3 On en-es and en-it, identical architectures perform similarly or better (up to 32.9 BLEU on en-es). Although BLEU scores are not strictly comparable across languages, we can safely consider all our models as state-of-the-art.\nFor the sake of reproducibility, we provide complete details about data, architectures and training setup in Appendix A.\n2In the pre-segmented data condition (Ansari et al., 2020). 3Also the ASR performance of our cascade solution (10.2 WER on MuST-C Common) is in line with the results obtained by Bahar et al. (2020) for their best ASR model."
    }, {
      "heading" : "3.2 Evaluation Methodology",
      "text" : "Data. Our evaluation data is drawn from the TED-based MuST-C corpus (Cattoni et al., 2020), the largest freely available multilingual corpus for ST. It covers 14 language directions, with English audio segments automatically aligned with their corresponding manual transcriptions and translations. The en–de/es/it MuST-C Common test sets contain the same 27 TED talks, for a total of around 2,500 segments largely overlapping across languages.4 For all the three language pairs, we selected subsets of MuST-C Common containing the same English audio portions from each talk, in order to obtain representative groups of contiguous segments that are comparable across languages. Furthermore, to ensure high data quality, we manually checked the selected samples and kept only those segments for which the audio-transcripttranslation alignment was correct. Each of the three resulting test sets – henceforth PE-sets – is composed of 550 segments, corresponding to about 10,000 English source words.\nPost-editing. A key element of our multi-faceted analysis is human post-editing (PE), which consists in manually correcting systems’ output according to the input (the source audio in our case). In PEbased evaluation, the original output is compared against its post-edited version using distance-based metrics like TER (Snover et al., 2006). This allows for counting only the true errors made by a system, without penalising differences due to linguistic variation as it happens when exploiting independent references. This makes PE-based evaluation one of the most prominent methodologies used for translation quality assessment (Snover et al., 2006, 2009; Denkowski and Lavie, 2010; Cettolo et al., 2013; Bojar et al., 2015; Graham et al., 2016; Bentivogli et al., 2018b).\nTo collect the post-edits for our study, we strictly followed the methodology of the IWSLT 2013- 2017 evaluation campaigns (Cettolo et al., 2013), which offered us a consolidated framework and best practices to draw upon. Our cascade and direct systems were both run on the PE-sets to be post-edited. To guarantee high quality post-edits, for each language we hired two professional translators with experience in subtitling and post-editing. Moreover, in order to cope with translators’ vari-\n4MuST-C Common segments can vary across languages due to the automatic procedures of segmentation, audio-text alignment and filtering that were applied to the talks.\nability (i.e. more/less aggressive editing strategies), the outputs of the two ST systems were randomly assigned ensuring that each translator worked on all the 550 segments, post-editing an equal number of outputs from both systems. The task was performed with a CAT tool5 that displays the manual transcript of the audio together with the ST output to be edited. However, since ST systems take as input an audio signal, we also provided translators with the audio file of each segment, asking them to post-edit strictly according to it.6 For each language pair, the final PE-set used in our study consists of the 550 MuST-C original audiotranscript-translation triplets plus two additional sets of reference translations, i.e. the post-edited versions of the two systems’ outputs.\nAnalyses. The collected post-edits are exploited to assess overall systems’ performance (§4) as well as to carry out deeper quantitative and qualitative analyses aimed to shed light on possible systematic differences in systems’ behavior (§5.1 and §6.1). Focusing on specific aspects of the ST problem, the inquiry is also performed by means of manual annotation of systems’ outputs (§5.2, §6.2 and §7.1). Due to the linguistic nature of this task, centred on fine-grained aspects requiring a variety of skills in both evaluation and ST technology, for such analyses we relied on three researchers in translation technology – one per language pair – with a strong background in linguistics, excellent knowledge of the addressed languages (C2 or native), as well as strong expertise in systems’ evaluation."
    }, {
      "heading" : "4 Overall Systems’ Performance",
      "text" : "We compute overall performance results both on the PE-sets and on the MuST-C Common test sets. Our primary evaluation is based on the collected post-edits. We consider two TER-based7 metrics: i) human-targeted TER (HTER) computed between the automatic translation and its human post-edited version, and ii) multi-reference TER (mTER) computed against the closest reference among the three available ones (two post-edits and the official reference from MuST-C). The latter metric better accounts for post-editors’ variability, making the evaluation more reliable and informative. For the sake of completeness, in Table 1 we also report Sacre-\n5www.matecat.com 6The ad-hoc ST PE guidelines given to translators are\nincluded in Appendix B. 7www.cs.umd.edu/˜snover/tercom\nBLEU8 (Post, 2018) and TER scores computed only on the official MuST-C Common references.\nA bird’s-eye view of the results shows that, in more than half of the cases, performance differences between cascade and direct systems are not statistically significant. When they are, the raw count of wins for the two approaches is the same (4), attesting their substantial parity.\nLooking at our primary metrics (HTER and mTER), systems are on par on en-it and en-de, while for en-es the direct approach significantly outperforms the cascade one. This difference, however, does not emerge with the other metrics. Indeed, BLEU and TER scores computed against the official references are less coherent across metrics and test sets. For instance, on the en-it PE-set the cascade system significantly outperforms the direct one in terms of BLEU score, while TER shows the opposite on MuST-C Common. Interestingly, the scores obtained using independent references can also disagree with those computed with post-edits. This is the case of en-es, where significant HTER and mTER reductions attest the superiority of the direct system, while most BLEU and TER scores are still in favor of the cascade.\nOn the one hand, primary evaluation scores suggest that the rapidly advancing direct technology has eventually reached the traditional cascaded approach. On the other, the highlighted incongruities confirm widespread concerns about the reliability of fully automatic metrics – based on independent references – to properly evaluate neural systems (Way, 2018). This calls for deeper quantitative and qualitative analyses. Those presented in the next sections investigate performance differences focusing on two main aspects: the impact of specific input audio properties (§5), and the linguistic errors made by the systems (§6).\n8BLEU+c.mixed+#.1+s.exp+tok.13a+v.1.4.3"
    }, {
      "heading" : "5 ST Quality and Audio Properties",
      "text" : ""
    }, {
      "heading" : "5.1 Automatic Analysis",
      "text" : "The two ST approaches handle the input audio differently: the cascade one by means of a dedicated ASR component that produces intermediate transcripts; the direct one by extracting all the relevant information to translate in an end-to-end fashion. Is it therefore possible that some audio properties have different impact on their results? Overall performance being equal, answering this question would help to understand if one approach is preferable over the other under specific audio conditions.\nAmong other possible factors (e.g. noise, recording conditions, overlapping speakers) we tried to shed light on this aspect by focusing on two common factors: audio duration and speech rate. To this aim, we grouped the sentences in the PE-set according to the sentence-wise HTER percentage difference – i.e. the difference between the cascade and direct HTER scores divided by their average.\nThe threshold for considering performance differences as significant was set to 10%. The resulting groups contain sentences where: i) cascade is significantly better than direct, ii) direct is significantly better than cascade, iii) the difference between the two is not significant, and iv) both systems have HTER=0. For each group, we calculated the average audio duration and the corresponding speech rate in terms of phonemes9 per second.\nResults are shown in Table 2, where – for the sake of completeness – also the length of the reference audio transcript is given, together with the average HTER of the systems.\nAs we can see, results are coherent across languages: audio duration and speech rate averages do not differ, neither when one system performs significantly better than the other, nor when the HTER differences are not significant. We can hence conclude that, if audio duration and speech rate have any influence on systems’ performance, our analysis does not highlight specific conditions that are more favorable to one approach than to the other. Both are equally robust with respect to the audio properties here considered."
    }, {
      "heading" : "5.2 Manual Analysis",
      "text" : "Handling the input audio differently, the two approaches have inherent strengths and weaknesses.\n9Obtained by processing the transcripts with eSpeak (espeak.sourceforge.net).\nIn particular, although suffering from the wellknown scarcity of sizeable training corpora, direct solutions come with the promise (Sperber and Paulik, 2020) of: i) higher robustness to error propagation, and ii) reduced loss of speech information (e.g. prosody). Our next qualitative analysis tries to delve into these aspects by looking at audio understanding and prosody issues.\nAudio understanding. Errors due to wrong audio understanding are easy to identify for cascade systems – since they are evident in the intermediate ASR transcripts – but harder to spot for direct systems, whose internal representations are by far less accessible. In this case, errors can still be identified in mistranslations corresponding to words which are phonetically similar to parts of the input audio – e.g. nice voice mistranslated in German as nette Jungen (nice boys). To spot such errors, our annotators carefully inspected the PE-set by comparing the audio, the reference transcripts and systems’ output translations for both the cascade and direct models, as well as the ASR transcripts for the cascade one. Some interesting examples of the identified errors are reported in Table 3.\nAs shown in Table 4, audio understanding errors are quite common for both systems in all language pairs. However, both the number of errors and the number of sentences they affect is significantly lower for the direct one. We observed that this is the case especially for “more difficult” sentences, such as sentences with poor audio quality and overlapping or disfluent speech.\nThough far from being conclusive (we acknowledge that, due to the “opacity” of direct models, their error counts might be slightly underestimated), this analysis seems to confirm the theoretical advantages of direct ST. This finding advocates for more thorough future investigations on neural networks’ interpretability, targeting its empirical verification on larger and diverse benchmarks.\nProsody. Prosody is central to disambiguating utterances, as it reflects language elements which may not be encoded by grammar and vocabulary choices. While prosody is directly encoded by the direct system, it is lost in the unpunctuated input received by the MT component of a cascade. Besides few interrogative sentences, our annotators were able to isolate only a handful of utterances whose prosodic markers result in different interpretations by the two models. Concerning interrogatives, both systems managed to translate them correctly in most cases (24 for cascade and 25 for direct out of 31). This is not surprising given the syntactic structure of English questions, which is explicit and does not rely solely on prosody (e.g. compared to Italian). In all other cases (examples in Table 5), the direct model’s higher sensitivity to prosody seems to give it an edge on cascade in disambiguating and correctly rendering the utterance meaning. Also this finding calls for future inquiries aimed to check the regularity of these differences on larger datasets."
    }, {
      "heading" : "6 Linguistic Errors",
      "text" : ""
    }, {
      "heading" : "6.1 Automatic Analysis",
      "text" : "For this analysis, we rely on the publicly available tool10 used by Bentivogli et al. (2018a) to analyse\n10wit3.fbk.eu/2016-02, details in Appendix C.\nwhat linguistic phenomena are best modeled by MT systems. The tool exploits manual post-edits and HTER-based computations to detect and classify translation errors according to three linguistic categories: lexicon, morphology and word order. Table 6 presents their distribution.\nAs expected from the HTER scores in Table 1, results vary across language pairs. On en-it, systems show pretty much the same number of errors, with a slight percentage gain (+1.1) in favor of the cascade. For the other two pairs, differences are more marked and opposite, with an overall error reduction for the direct system on en-es (-6.7) and in favor of the cascade on en-de (+6.7).\nLooking at the distribution of errors across categories, while for en-es the direct system is always better and the percentage reduction is homogeneously distributed, for en-de the better performance of the cascade is concentrated in the morphology and word order categories. Since English and German are the most different languages in terms of morphology and word order, this result suggests that cascade systems still have an edge on the direct ones in their ability to handle morphology and word reordering. This is further supported by en-it: the only difference, in favor of the cascade, is indeed observed in the morphology category."
    }, {
      "heading" : "6.2 Manual Analysis",
      "text" : "Since lexical errors represent by far the most frequent category for both approaches in all language pairs, we complement the automatic analysis with a more fine-grained manual inspection, further distinguishing among lexical errors due to missing words, extra words, or wrong lexical choice.11\nThe analysis was carried out on subsets of the PE-set, created in such a way to be suitable for manual annotation. Namely, we removed sentences for which the output of the two systems is: i) identical, ii) judged correct by post-editors (HTER=0), or iii) too poor to be reliably annotated for errors (HTER>40%). The resulting sets contain 207 sentences for en-de, 238 for en-es, and 285 for en-it.\nThis analysis reveals that, for all language pairs, wrong lexical choice is the most frequent error type (∼65% of lexical errors on average) followed by missing words (∼30%), and extra words (∼5%).\nWhile errors due to lexical choice and superfluous words vary across languages, we observe a systematic behavior with respect to missing words (words that are present in the audio but are not translated). As we can see in Table 7, direct systems lose more information from the source input than their cascade counterparts, in terms of both single words and contiguous word sequences. It is particularly interesting to notice that also for en-es – where the direct system is significantly stronger than the cascade – the issue is still evident, although to a lesser extent. Table 8 collects examples of the encountered lexical phenomena.\nFinally, we report that a non-negligible amount of missing words (between 10% and 20%) is represented by discourse markers, i.e. words or phrases used to connect and manage what is being said (e.g. “you know”, “well”, “now”). Although this is\n11Various error taxonomies covering different levels of granularity have been developed, and the distinction between these types of lexical errors is widely adopted, including the DQF-MQM framework – https://info.taus.net/ dqf-mqm-error-typology-templ\na frequent phenomenon in speech, not translating discourse markers cannot be properly considered as an error, since markers i) do not carry semantic information, and ii) can be intentionally dropped in some use cases, such as in subtitling."
    }, {
      "heading" : "7 Classifiers’ Verdict",
      "text" : "So far, our inquiry has been entirely driven by predefined assumptions (the importance of certain audio properties) and linguistic criteria (the focus on specific error types). This top-down approach, however, might fail to disclose important differences, which were not specifically sought after when analysing the two paradigms. This consideration motivates the adoption of the complementary bottom-up approach that concludes our comparative study by answering the question: is the output of cascade and direct systems distinguishable? Understanding if and why discriminating between the two is possible would not only suggest new issues to look at. It would also highlight possible output regularities that, despite the similar overall performance, make one paradigm preferable over the other in specific application scenarios. To this aim, we set up a classification experiment, comparing the ability of humans to correctly identify the output of the two systems with the performance of an automatic text classifier."
    }, {
      "heading" : "7.1 Human Classification",
      "text" : "After getting acquainted with systems’ output through the previous manual analyses, our assessors were instructed to perform a classification task. The classification had to be performed on 10 blocks of items comprising a set of unseen English contiguous sentences (gold transcripts) from the MuSTC Common test set, and two sets of anonymized translations, one produced by the cascade and one by the direct model. For each block, the assessors had to assign each set of translations to the correct system, or label them as indistinguishable. To investigate whether more context helps in the assign-\nment, we set up two experiments with respectively 10 and 20 contiguous sentences per block.\nThe results in Table 9 show that en-es and en-it systems are not distinguishable, since only a maximum of 4 blocks out of 10 were correctly classified, while most en-de blocks were correctly classified. According to the en-de assessor, this is due to the fact that the structure of the sentences generated by the direct system is very similar to that of the corresponding English sources. This characteristic stands out in German, which differs from English in terms of word order more than Italian and Spanish. This type of behavior does not necessarily imply the presence of errors but, like a fingerprint, makes the en-de direct system more recognizable by a human. Furthermore, being sub-optimal for German, this structure can cause preferential edits by the post-editors, which would be in line with the concentration of errors in the word order category observed in Table 6 (+19.6%).\nAssessing the importance of context, the ability of humans to distinguish the systems does not improve when passing from 10 to 20 sentences per block. This suggests that the behavioral differences between cascade and direct systems are so subtle that, on larger samples, they mix up and balance making their fingerprints less traceable."
    }, {
      "heading" : "7.2 Automatic Classification",
      "text" : "As a complement to the human classification experiment, we check whether an automatic tool is able to accomplish a similar task. Our classifier combines n-gram language models with the Naive Bayes algorithm, as proposed in (Peng and Schuurmans, 2003). We trained two 5-gram models, respectively using translations by the cascade and the direct systems. At classification time, given a translated text, the classifier computes the perplexity of the two models and assigns the cascade or direct label based on the model with the lowest perplexity. Also these experiments were carried out on the MuST-C Common set. The classifier was tested via k-fold cross-validation, for different values of k – i.e. different sizes of text to classify.\nAs shown in Figure 1, contrary to humans, the more data the classifier receives, the higher its accuracy in discriminating between systems. Already at a size of 20 sentences, accuracy is always∼80%. This suggests that systems have their own “language”, a fluency-related fingerprint.\nTo check this finding, we measured outputs’ lexical diversity in terms of moving average TypeToken Ratio – maTTR (Covington and McFall, 2010) – and with the Measure of Textual Lexical Diversity (MTLD) by McCarthy and Jarvis (2010).\nTable 10 shows that the cascade output exhibits higher lexical diversity on all languages, with smaller differences on en-de and en-es compared to en-it. A plausible conclusion is that the cascade produces richer output, whose variety does not necessarily result in better translations nor is appreciated by humans. Indeed, annotators were able to correctly distinguish the output only for en-de, where lexical diversity is similar (see §7.1)."
    }, {
      "heading" : "8 Conclusion and Final Remarks",
      "text" : "There is a time when the possible transition from consolidated technological frameworks to new emerging paradigms depends on answering fundamental questions about their potential, strengths and weaknesses. A time when technology developers are faced with the choice of where to direct their future investments. Five years after its appearance\non the scene, the direct approach to ST confronts the community with similar questions in relation to the traditional cascade paradigm that it aims to overtake. Our investigation showed that, in spite of the known data paucity conditions still penalizing the direct approach, the two technologies now perform substantially on par. Subtle differences in their behavior exist: overall performance being equal, the cascade still seems to have an edge in terms of morphology, word ordering and lexical diversity, which is balanced by the advantages of direct models in audio understanding and in capturing prosody. However, they do not seem sufficient and consistent enough across languages to make the output of the two approaches easily distinguishable, nor to make one model preferable to the other. Back to our title, they no longer make a difference.\nWe are aware that the generalizability of these results depends on several factors such as the considered languages, systems and benchmarks, as well as the human workforce deployed for the inquiry. Here, with the help of professionals, we proposed multi-faceted quantitative and qualitative analyses, run on the output of state-of-the-art systems on three language pairs – though, by now, covering only the most-explored and data-favorable condition, which has English as source. Although our findings hold for a specific scenario, in which free data were at our disposal (and to which we contribute back by releasing high-quality post-edits), they might not be generalizable to other (e.g. difficult, distant) languages and other (e.g. highly specialized) domains. Nevertheless, we present them as a timely contribution towards answering a burning question within the ST community."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The creation of the post-edits used in this work was funded by the European Association for Machine Translation (EAMT) through its 2020 Sponsorship of Activities programme. The computational costs were covered by the “End-to-end Spoken Language Translation in Rich Data Conditions” project,12 which was financially supported by an Amazon AWS ML Grant.\n12https://ict.fbk.eu/ units-hlt-mt-e2eslt/"
    }, {
      "heading" : "A Systems’ Description",
      "text" : "In this section we describe the ST models created for our study (see Section 3.1 ). All the details about the different trainings are given below, while the validation set was common to all trainings, since we used the MuST-C dev set.\nThe source code for the ASR and the direct ST models is available at: https://github.com/ mgaido91/FBK-fairseq-ST.\nThe source code for the MT component of the cascade model can be found at: https://github. com/modernmt/modernmt.\nA.1 Cascade approach The Cascade system is composed of a pipeline of automatic speech recognition (ASR) and machine translation (MT) models.\nThe ASR model is a slightly revisited version (Gaido et al., 2020) of the S-Transformer (Di Gangi et al., 2019b), where the two 2D self-attention layers are replaced with two Transformer encoder layers (for a total of 8 layers), while the decoder is the same (with 6 layers). Hence, the model processes the input with two 3x3 2D CNNs (having 64 filters), whose output is first projected into a higherdimensional space and then summed with positional embeddings before being fed to the Transformer encoder layers; Transformer encoder layers use logarithmic distance penalty. The attention mechanism consists of 8 attention heads. The dimensionality of input and output is 512, while the inner-layers have dimensionality 2048. The resulting number of parameters is 63M.\nThe ASR model was trained with the goal of achieving state-of-the-art performance. To this aim, we relied on two data augmentation techniques\nthat were shown to yield competitive models at the IWSLT-2020 evaluation campaign (Ansari et al., 2020), namely: i) SpecAugment (Park et al., 2019) applied with probability 0.5 by masking two bands on the frequency axis (with 13 as maximum mask length) and two on the time axis (with 20 as maximum mask length), and ii) time stretch (Nguyen et al., 2020) with probability of 0.3 and stretching factor sampled uniformly for each utterance between 0.8 and 1.25. The ASR model was trained on 1.25M utterance-transcript pairs coming from the ASR corpora Librispeech (Panayotov et al., 2015), Mozilla Common Voice,13 How2 (Sanabria et al., 2018), TEDLIUM-v3 (Hernandez et al., 2018), as well as the ST corpora Europarl-ST (IranzoSánchez et al., 2020) and MuST-C (Cattoni et al., 2020).14 We filtered out all pairs whose utterance was longer than 20 seconds. The audio input was preprocessed with XNMT15 (Neubig et al., 2018) to extract 40 features per time frame (with 25ms windows and 10ms sliding) and per-speaker normalization was applied. The text was preprocessed by normalizing punctuation and de-escaping special characters, and was tokenized with Moses.16 Then it was encoded with a BPE (Sennrich et al., 2015) code learnt on the OPUS data17 using 8k merge rules.\nThe MT component is built on the ModernMT framework18 which features machine translation implementing the Transformer architecture. We trained either a Base (en-it) or a Big (en-{de,es}) Transformer model (Vaswani et al., 2017) with 6 blocks in the encoder and 6 in the decoder, 512/1024 as input size, the same as output size, 2048/4096 as inner dimension and 8/16 attention heads. The total number of parameters is about 61M for the Base model, 210M for the Big models.\nAs regards pre-processing, for all the three language directions we used the internal ModernMT procedures.\nIn training, models are optimized with Adam using β1=0.9, β2=0.98; the learning rate is linearly increased during the warmup (8k iterations) up to\n13https://voice.mozilla.org/ 14For English-German, the ST corpora include also the Speech-Translation TED corpus provided in the IWSLT offline-speech-translation task: http://iwslt.org/ doku.php?id=offline_speech_translation\n15https://github.com/neulab/xnmt 16https://github.com/moses-smt/\nmosesdecoder 17http://opus.nlpl.eu 18https://github.com/modernmt/modernmt\nthe maximum value (5×10−4), after that it follows an inverse square root decay; dropout is set to 0.3. Minibatches consist of 3072 tokens and update frequency is set to 4; the total number of iterations is 200k; the last 10 saved checkpoints (one out of 1k iterations) are averaged. The model uses label smoothing with a uniform prior distribution (0.1) over the vocabulary; source and target languages share a BPE vocabulary of 32k sub-words.\nThe training data, whose statistics are reported in Table 11, are collected from the OPUS repository. For English-Italian, they resulted in almost 70M segment pairs and about 800M English words; after deduplication and the internal ModernMT cleaning, the actual training data is reduced to 45M pairs and 550M English words. For English{German,Spanish} pairs, the OPUS data were filtered through well-known data selection methods (Axelrod et al., 2011) using a general-domain seed; the resulting training data consist of, respectively, 17M and 19M segment pairs, for 270M and 330M English words. Trainings were performed on RTX 2080 Ti GPUs; for English-Italian, it was run on 7 GPUs and lasted 3 days, while for each of the other two directions, on a single GPU, it took 6 days.\nThe three models are then fine-tuned on MuST-C training data (∼250K pairs, 4-5M English words) by continuing the training for 4k iterations on the adaptation data, with a learning rate reduced by a factor of 5. To mitigate error propagation and make the MT system more robust to ASR errors, similarly to (Di Gangi et al., 2019a) fine tuning is run on the concatenation of human and automatic transcripts of MuST-C, both paired with manual translations.\nA.2 Direct approach Our direct model (Gaido et al., 2020) uses the same architecture of the English ASR model described in §A.1, but it has 11 Transformer encoder layers (instead of 8) and 4 Transformer decoder layers (instead of 6) for a total of 64M parameters. The ST model’s encoder is initialized with the encoder of\nthe ASR model (Bansal et al., 2019), with the missing layers initialized randomly. The ST decoder is also initialized randomly.\nThe training settings and the data augmentation methods employed for the direct ST model are the same described in Section A.1 for the ASR component of the cascade system. In addition, we performed synthetic data generation, by automatically translating the English transcripts of the ASR training corpora (Jia et al., 2019). Furthermore, we transfer knowledge from MT through knowledge distillation (Hinton et al., 2015). Knowledge distillation is performed from a teacher MT model by optimizing the KL divergence between the distributions produced by the teacher and the student ST model being trained (Liu et al., 2019). The teacher MT model is trained on the OPUS datasets (Tiedemann, 2016) and is a plain transformer with 16 attention heads and 1024 features in encoder/decoder embeddings, resulting into 212M parameters.\nThe direct ST model is trained in two consecutive steps. First, it is optimized using KD. Then, the resulting model is fine-tuned on label-smoothed cross entropy (Szegedy et al., 2016). The training set is composed of the same corpora used for the ASR model, more precisely: i) the ST corpora and ii) the synthetic datasets derived from the ASR corpora.\nThe ST model is fed with the input utterance and a token representing the type of the target data, which can be: i) human reference translations (for the ST corpora), or ii) translations generated by the MT model fed with true case transcriptions with punctuation, and iii) translations generated by the MT model fed with lower-cased transcriptions without punctuation (for the ASR corpora). At inference time, the token “human reference” is always used to generate the translations. The token is added to the features extracted from the audio before they are passed to the encoder (Di Gangi et al., 2019c).\nAll trainings were performed on 8 K80 GPUs. The training of each direct model lasted 10 days, while the ASR and MT pre-trainings 6 days each.\nThe source code19 implemented to build these models is based on Fairseq (Ott et al., 2019).\n19https://github.com/mgaido91/ FBK-fairseq-ST"
    }, {
      "heading" : "B Post-Editing Guidelines",
      "text" : "In this task you are presented with (i) 550 audio segments that are recordings of portions of different English TED Talks, (ii) their transcripts, and (iii) corresponding automatic translations.\nStarting from the original audio recording and its corresponding transcript (done by TED volunteer translators), you are asked to post-edit each given automatic translation by applying the minimal edits required to transform the system output into a fluent sentence with the same meaning as the audio/transcript.\nWhile post-editing, remember the following guidelines:\n• We noticed that some audio player software applications cut the beginning or the end of the audio segments. If you notice some audiotranscript out-of-sync, please try another audio player or inform us about the problem.\n• The audio should be your first source of information, while transcripts are given for your convenience. It could happen that the transcript is not faithful to the spoken original: in these cases you should not consider the transcript and refer to the audio only.\n• Some transcripts contain the name or initials of the speaker (typically followed by colons). Please don’t add this information into the sentence you are post-editing. In general, don’t include in your post-edit any text that is not present in the audio (e.g. explanation of acronyms, disambiguation of pronouns), even though this information could ease the understanding of the sentence.\n• The post-edited sentence is intended as a translation of spoken language. Also, depending on the style of the source language talk, you can use the corresponding style in the target language (e.g. if the talk uses a friendly/colloquial style you can use informal words too).\n• The focus is the correctness of the single sentence within the given context, not the consistency of a group of sentences. Hence, surrounding segments should be used to understand the context but not to enforce consistency on the use of terms. In particular, different but correct translations of terms across segments should not be corrected."
    }, {
      "heading" : "C Tool for Automatic Error Classification",
      "text" : "The tool used for the automatic analysis of linguistic errors (Section 6.1) is downloadable at wit3.fbk.eu/2016-02. It is a modified version of the tercom script, 20 which requires the lemmatized versions of both systems’ outputs and post-edits. To lemmatize the data we used the TreeTagger.21\n20www.cs.umd.edu/˜snover/tercom 21www.cis.uni-muenchen.de/˜schmid/\ntools/TreeTagger"
    } ],
    "references" : [ {
      "title" : "Tied Multitask Learning for Neural Speech Translation",
      "author" : [ "Antonios Anastasopoulos", "David Chiang." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 82–91, New Or-",
      "citeRegEx" : "Anastasopoulos and Chiang.,? 2018",
      "shortCiteRegEx" : "Anastasopoulos and Chiang.",
      "year" : 2018
    }, {
      "title" : "Findings of the IWSLT 2020 Evaluation Campaign",
      "author" : [ "abeth Salesky", "Xing Shi", "Sebastian Stüker", "Marco Turchi", "Alexander Waibel", "Changhan Wang" ],
      "venue" : "In Proceedings of the International Conference on Spoken Language Translation",
      "citeRegEx" : "Salesky et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Salesky et al\\.",
      "year" : 2020
    }, {
      "title" : "Domain Adaptation via Pseudo In-Domain Data Selection",
      "author" : [ "Amittai Axelrod", "Xiaodong He", "Jianfeng Gao." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, Edinburgh, Scotland, UK.",
      "citeRegEx" : "Axelrod et al\\.,? 2011",
      "shortCiteRegEx" : "Axelrod et al\\.",
      "year" : 2011
    }, {
      "title" : "A Comparative Study on End-to-end Speech to Text Translation",
      "author" : [ "Parnia Bahar", "Tobias Bieschke", "Hermann Ney." ],
      "venue" : "Proceedings of the International Workshop on Automatic Speech Recognition and Understanding (ASRU), pages 792–799, Sen-",
      "citeRegEx" : "Bahar et al\\.,? 2019a",
      "shortCiteRegEx" : "Bahar et al\\.",
      "year" : 2019
    }, {
      "title" : "Start-Before-End and Endto-End: Neural Speech Translation by AppTek and RWTH Aachen University",
      "author" : [ "Parnia Bahar", "Patrick Wilken", "Tamer Alkhouli", "Andreas Guta", "Pavel Golik", "Evgeny Matusov", "Christian Herold." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Bahar et al\\.,? 2020",
      "shortCiteRegEx" : "Bahar et al\\.",
      "year" : 2020
    }, {
      "title" : "On Using SpecAugment for Endto-End Speech Translation",
      "author" : [ "Parnia Bahar", "Albert Zeyer", "Ralf Schlüter", "Hermann Ney." ],
      "venue" : "Proceedings of the International Workshop on Spoken Language Translation (IWSLT), Hong Kong.",
      "citeRegEx" : "Bahar et al\\.,? 2019b",
      "shortCiteRegEx" : "Bahar et al\\.",
      "year" : 2019
    }, {
      "title" : "Pretraining on High-resource Speech Recognition Improves Low-resource Speech-to-text Translation",
      "author" : [ "Sameer Bansal", "Herman Kamper", "Karen Livescu", "Adam Lopez", "Sharon Goldwater." ],
      "venue" : "Proceedings of the Conference of the North Ameri-",
      "citeRegEx" : "Bansal et al\\.,? 2019",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural speech translation using lattice transformations and graph networks",
      "author" : [ "Daniel Beck", "Trevor Cohn", "Gholamreza Haffari." ],
      "venue" : "Proceedings of the EMNLP Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13),",
      "citeRegEx" : "Beck et al\\.,? 2019",
      "shortCiteRegEx" : "Beck et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural versus phrase-based MT quality: an in-depth analysis",
      "author" : [ "Luisa Bentivogli", "Arianna Bisazza", "Mauro Cettolo", "Marcello Federico" ],
      "venue" : null,
      "citeRegEx" : "Bentivogli et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Bentivogli et al\\.",
      "year" : 2018
    }, {
      "title" : "Machine Translation Human Evaluation: an investigation of evaluation based on Post-Editing and its relation with Direct Assessment",
      "author" : [ "Luisa Bentivogli", "Mauro Cettolo", "Marcello Federico", "Christian Federmann." ],
      "venue" : "Proceedings of the Interna-",
      "citeRegEx" : "Bentivogli et al\\.,? 2018b",
      "shortCiteRegEx" : "Bentivogli et al\\.",
      "year" : 2018
    }, {
      "title" : "End-toEnd Automatic Speech Translation of Audiobooks",
      "author" : [ "Alexandre Bérard", "Laurent Besacier", "Ali Can Kocabiyikoglu", "Olivier Pietquin." ],
      "venue" : "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing",
      "citeRegEx" : "Bérard et al\\.,? 2018",
      "shortCiteRegEx" : "Bérard et al\\.",
      "year" : 2018
    }, {
      "title" : "Listen and Translate: A Proof of Concept for End-to-End Speech-to-Text Translation",
      "author" : [ "Alexandre Bérard", "Olivier Pietquin", "Christophe Servan", "Laurent Besacier." ],
      "venue" : "Proceedings of the NIPS Workshop on end-to-end learning for speech and audio pro-",
      "citeRegEx" : "Bérard et al\\.,? 2016",
      "shortCiteRegEx" : "Bérard et al\\.",
      "year" : 2016
    }, {
      "title" : "A new decoder for spoken language translation based on confusion networks",
      "author" : [ "Nicola Bertoldi", "Marcello Federico." ],
      "venue" : "Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), pages 86–91, San Juan, Puerto",
      "citeRegEx" : "Bertoldi and Federico.,? 2005",
      "shortCiteRegEx" : "Bertoldi and Federico.",
      "year" : 2005
    }, {
      "title" : "MuST-C: A Multilingual Corpus for end-to-end Speech Translation",
      "author" : [ "Roldano Cattoni", "Mattia A. Di Gangi", "Luisa Bentivogli", "Matteo Negri", "Marco Turchi." ],
      "venue" : "Computer Speech & Language Journal. Doi: https://doi.org/10.1016/j.csl.2020.101155.",
      "citeRegEx" : "Cattoni et al\\.,? 2020",
      "shortCiteRegEx" : "Cattoni et al\\.",
      "year" : 2020
    }, {
      "title" : "Report on the 10th IWSLT Evaluation Campaign",
      "author" : [ "Mauro Cettolo", "Jan Niehues", "Sebastian Stüker", "Luisa Bentivogli", "Marcello Federico." ],
      "venue" : "Proceedings of the International Workshop on Spoken Language Translation (IWSLT), Heidelberg, Germany.",
      "citeRegEx" : "Cettolo et al\\.,? 2013",
      "shortCiteRegEx" : "Cettolo et al\\.",
      "year" : 2013
    }, {
      "title" : "Breaking the data barrier: Towards robust speech translation via adversarial stability training",
      "author" : [ "Qiao Cheng", "Meiyuan Fang", "Yaqian Han", "Jin Huang", "Yitao Duan." ],
      "venue" : "Proceedings of the International Workshop on Spoken Language Translation",
      "citeRegEx" : "Cheng et al\\.,? 2019",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2019
    }, {
      "title" : "Cutting the Gordian Knot: The Moving-Average Type–Token Ratio (MATTR)",
      "author" : [ "Michael A. Covington", "Joe D. McFall." ],
      "venue" : "Journal of Quantitative Linguistics, 17(2):94–100.",
      "citeRegEx" : "Covington and McFall.,? 2010",
      "shortCiteRegEx" : "Covington and McFall.",
      "year" : 2010
    }, {
      "title" : "Choosing the right evaluation for machine translation: An examination of annotator and automatic metric performance on human judgment tasks",
      "author" : [ "Michael Denkowski", "Alon Lavie." ],
      "venue" : "Proceedings of the Conference of the Association of Machine Trans-",
      "citeRegEx" : "Denkowski and Lavie.,? 2010",
      "shortCiteRegEx" : "Denkowski and Lavie.",
      "year" : 2010
    }, {
      "title" : "Robust Neural Machine Translation for Clean and Noisy Speech Transcripts",
      "author" : [ "Mattia A. Di Gangi", "Robert Enyedi", "Alessandra Brusadin", "Marcello Federico." ],
      "venue" : "Proceedings of the International Workshop on Spoken Language Translation",
      "citeRegEx" : "Gangi et al\\.,? 2019a",
      "shortCiteRegEx" : "Gangi et al\\.",
      "year" : 2019
    }, {
      "title" : "Adapting Transformer to End-to-end Spoken Language Translation",
      "author" : [ "Mattia A. Di Gangi", "Matteo Negri", "Marco Turchi." ],
      "venue" : "Proceedings of the Conference of the International Speech Communication Association (INTERSPEECH), Graz, Austria.",
      "citeRegEx" : "Gangi et al\\.,? 2019b",
      "shortCiteRegEx" : "Gangi et al\\.",
      "year" : 2019
    }, {
      "title" : "One-to-many multilingual end-toend speech translation",
      "author" : [ "Mattia Antonino Di Gangi", "Matteo Negri", "Marco Turchi." ],
      "venue" : "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 14-18 December 2019, Sentosa, Singapore.",
      "citeRegEx" : "Gangi et al\\.,? 2019c",
      "shortCiteRegEx" : "Gangi et al\\.",
      "year" : 2019
    }, {
      "title" : "End-to-End Speech-Translation with Knowledge Distillation: FBK@IWSLT2020",
      "author" : [ "Marco Gaido", "Mattia A. Di Gangi", "Matteo Negri", "Marco Turchi." ],
      "venue" : "Proceedings of the International Conference on Spoken Language Translation",
      "citeRegEx" : "Gaido et al\\.,? 2020",
      "shortCiteRegEx" : "Gaido et al\\.",
      "year" : 2020
    }, {
      "title" : "Is all that Glitters in Machine Translation Quality Estimation really Gold",
      "author" : [ "Yvette Graham", "Timothy Baldwin", "Meghan Dowling", "Maria Eskevich", "Teresa Lynn", "Lamia Tounsi" ],
      "venue" : "In Proceedings of the International Conference on Computational",
      "citeRegEx" : "Graham et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Graham et al\\.",
      "year" : 2016
    }, {
      "title" : "Knowledge Transfer in Deep Convolutional Neural Nets",
      "author" : [ "Steven Gutstein", "Olac Fuentes", "Eric Freudenthal." ],
      "venue" : "International Journal on Artificial Intelligence Tools, 17(03):555–567.",
      "citeRegEx" : "Gutstein et al\\.,? 2008",
      "shortCiteRegEx" : "Gutstein et al\\.",
      "year" : 2008
    }, {
      "title" : "TED-LIUM 3: Twice as Much Data and Corpus Repartition for Experiments on Speaker Adaptation",
      "author" : [ "François Hernandez", "Vincent Nguyen", "Sahar Ghannay", "Natalia Tomashenko", "Yannick Estève." ],
      "venue" : "Proceedings of the Speech and Computer - 20th",
      "citeRegEx" : "Hernandez et al\\.,? 2018",
      "shortCiteRegEx" : "Hernandez et al\\.",
      "year" : 2018
    }, {
      "title" : "Distilling the Knowledge in a Neural Network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean." ],
      "venue" : "Proceedings of NIPS Deep Learning and Representation Learning Workshop, Montréal, Canada.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "ESPnet-ST: All-in-One Speech Translation Toolkit",
      "author" : [ "Hirofumi Inaguma", "Shun Kiyono", "Kevin Duh", "Shigeki Karita", "Nelson Yalta", "Tomoki Hayashi", "Shinji Watanabe." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational",
      "citeRegEx" : "Inaguma et al\\.,? 2020",
      "shortCiteRegEx" : "Inaguma et al\\.",
      "year" : 2020
    }, {
      "title" : "End-end Speech-to-Text Translation with Modality Agnostic Meta-Learning",
      "author" : [ "Sathish R. Indurthi", "Houjeung Han", "Nikhil K. Lakumarapu", "Beomseok Lee", "Insoo Chung", "Sangha Kim", "Chanwoo Kim." ],
      "venue" : "Proceedings of the IEEE International Confer-",
      "citeRegEx" : "Indurthi et al\\.,? 2020",
      "shortCiteRegEx" : "Indurthi et al\\.",
      "year" : 2020
    }, {
      "title" : "Europarl-ST: A Multilingual Corpus For Speech Translation Of Parliamentary Debates",
      "author" : [ "Javier Iranzo-Sánchez", "Joan Albert Silvestre-Cerdà", "Javier Jorge", "Nahuel Roselló", "Giménez. Adrià", "Albert Sanchis", "Jorge Civera", "Alfons Juan." ],
      "venue" : "Proceed-",
      "citeRegEx" : "Iranzo.Sánchez et al\\.,? 2020",
      "shortCiteRegEx" : "Iranzo.Sánchez et al\\.",
      "year" : 2020
    }, {
      "title" : "Leveraging Weakly Supervised Data to Improve End-toEnd Speech-to-Text Translation",
      "author" : [ "Ye Jia", "Melvin Johnson", "Wolfgang Macherey", "Ron J. Weiss", "Yuan Cao", "Chung-Cheng Chiu", "Naveen Ari", "Stella Laurenzo", "Yonghui Wu." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Jia et al\\.,? 2019",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2019
    }, {
      "title" : "Audio Augmentation for Speech Recognition",
      "author" : [ "Tom Ko", "Vijayaditya Peddinti", "Daniel Povey", "Sanjeev Khudanpur." ],
      "venue" : "Proceedings of the Conference of the International Speech Communication Association (INTERSPEECH), pages 3586–",
      "citeRegEx" : "Ko et al\\.,? 2015",
      "shortCiteRegEx" : "Ko et al\\.",
      "year" : 2015
    }, {
      "title" : "Statistical Significance Tests for Machine Translation Evaluation",
      "author" : [ "Philipp Koehn." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Barcelona, Spain.",
      "citeRegEx" : "Koehn.,? 2004",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2004
    }, {
      "title" : "Multi-lingual translation of spontaneously spoken language in a limited domain",
      "author" : [ "Alon Lavie", "Donna Gates", "Marsal Gavalda", "Laura Mayfield Tomokiyo", "Alex Waibel", "Lori Levin." ],
      "venue" : "Proceedings of the International Conference on Computational Linguis-",
      "citeRegEx" : "Lavie et al\\.,? 1996",
      "shortCiteRegEx" : "Lavie et al\\.",
      "year" : 1996
    }, {
      "title" : "End-to-End Speech Translation with Knowledge Distillation",
      "author" : [ "Yuchen Liu", "Hao Xiong", "Jiajun Zhang", "Zhongjun He", "Hua Wu", "Haifeng Wang", "Chengqing Zong." ],
      "venue" : "Proceedings of the Conference of the International Speech Communication Asso-",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Bridging the Modality Gap for Speechto-Text Translation",
      "author" : [ "Yuchen Liu", "Junnan Zhu", "Jiajun Zhang", "Chengqing Zong." ],
      "venue" : "arxiv.org/pdf/2010.14920.pdf.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Phrase-based translation of speech recognizer word lattices using loglinear model combination",
      "author" : [ "Evgeny Matusov", "Hermann Ney", "Ralph Schluter." ],
      "venue" : "Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding",
      "citeRegEx" : "Matusov et al\\.,? 2005",
      "shortCiteRegEx" : "Matusov et al\\.",
      "year" : 2005
    }, {
      "title" : "MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment",
      "author" : [ "Philip M. McCarthy", "Scott Jarvis." ],
      "venue" : "Behavior Research Methods, 42(2):381–392.",
      "citeRegEx" : "McCarthy and Jarvis.,? 2010",
      "shortCiteRegEx" : "McCarthy and Jarvis.",
      "year" : 2010
    }, {
      "title" : "Improving Sequence-tosequence Speech Recognition Training with On-thefly Data Augmentation",
      "author" : [ "Thai-Son Nguyen", "Sebastian Stueker", "Jan Niehues", "Alex Waibel." ],
      "venue" : "Proceedings of the International Conference on Acoustics, Speech, and Sig-",
      "citeRegEx" : "Nguyen et al\\.,? 2020",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2020
    }, {
      "title" : "2019a. The IWSLT 2019 Evaluation Campaign",
      "author" : [ "Jan Niehues", "Roldano Cattoni", "Sebastian Stucker", "Matteo Negri", "Marco Turchi" ],
      "venue" : "In Proceedings of the International Workshop on Spoken Language Translation (IWSLT),",
      "citeRegEx" : "Niehues et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Niehues et al\\.",
      "year" : 2019
    }, {
      "title" : "The IWSLT 2018 Evaluation Campaign",
      "author" : [ "Jan Niehues", "Roldano Cattoni", "Sebastian Stüker", "Mauro Cettolo", "Marco Turchi", "Marcello Federico." ],
      "venue" : "Proceedings of the International Workshop on Spoken Language Translation (IWSLT),",
      "citeRegEx" : "Niehues et al\\.,? 2018",
      "shortCiteRegEx" : "Niehues et al\\.",
      "year" : 2018
    }, {
      "title" : "The IWSLT 2019 Evaluation Campaign",
      "author" : [ "Jan Niehues", "Roldano Cattoni", "Sebastian Stüker", "Matteo Negri", "Marco Turchi", "Thanh-Le ha", "Elizabeth Salesky", "Ramon Sanabria", "Loic Barrault", "Lucia Specia", "Marcello Federico." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Niehues et al\\.,? 2019b",
      "shortCiteRegEx" : "Niehues et al\\.",
      "year" : 2019
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chap-",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "Librispeech: an ASR Corpus Based on Public Domain Audio Books",
      "author" : [ "Vassil Panayotov", "Guoguo Chen", "Daniel Povey", "Sanjeev Khudanpur." ],
      "venue" : "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing",
      "citeRegEx" : "Panayotov et al\\.,? 2015",
      "shortCiteRegEx" : "Panayotov et al\\.",
      "year" : 2015
    }, {
      "title" : "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition",
      "author" : [ "Daniel S. Park", "William Chan", "Yu Zhang", "ChungCheng Chiu", "Barret Zoph", "Ekin D. Cubuk", "Quoc V. Le." ],
      "venue" : "Proceedings of the Conference of the In-",
      "citeRegEx" : "Park et al\\.,? 2019",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2019
    }, {
      "title" : "Spoken language translation using automatically transcribed text in training",
      "author" : [ "Stephan Peitz", "Simon Wiesler", "Markus NußbaumThom", "Hermann Ney." ],
      "venue" : "Proceedings of the International Workshop on Spoken Language Translation (IWSLT),",
      "citeRegEx" : "Peitz et al\\.,? 2012",
      "shortCiteRegEx" : "Peitz et al\\.",
      "year" : 2012
    }, {
      "title" : "Combining Naive Bayes and n-Gram Language Models for Text Classification",
      "author" : [ "Fuchun Peng", "Dale Schuurmans." ],
      "venue" : "Proceedings of the European Conference on Information Retrieval (ECIR), pages 335–350, Pisa, Italy.",
      "citeRegEx" : "Peng and Schuurmans.,? 2003",
      "shortCiteRegEx" : "Peng and Schuurmans.",
      "year" : 2003
    }, {
      "title" : "The IWSLT 2019 KIT Speech Translation System",
      "author" : [ "Ngoc-Quan Pham", "Thai-Son Nguyen", "Thanh-Le Ha", "Juan Hussain", "Felix Schneider", "Jan Niehues", "Sebastian Stüker", "Alexander Waibel." ],
      "venue" : "Proceedings of the International Workshop on Spo-",
      "citeRegEx" : "Pham et al\\.,? 2019",
      "shortCiteRegEx" : "Pham et al\\.",
      "year" : 2019
    }, {
      "title" : "Harnessing Indirect Training Data for End-to-End Automatic Speech Translation: Tricks of the Trade",
      "author" : [ "Juan Pino", "Liezl Puzon", "Jiatao Gu", "Xutai Ma", "Arya D. McCarthy", "Deepak Gopinath." ],
      "venue" : "Proceedings of the International Workshop on Spoken",
      "citeRegEx" : "Pino et al\\.,? 2019",
      "shortCiteRegEx" : "Pino et al\\.",
      "year" : 2019
    }, {
      "title" : "A Call for Clarity in Reporting BLEU Scores",
      "author" : [ "Matt Post." ],
      "venue" : "Proceedings of the Conference on Machine Translation (WMT), pages 186–191, Brussels, Belgium.",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "SRPOL’s System for the IWSLT 2020 End-to-End Speech Translation Task",
      "author" : [ "Tomasz Potapczyk", "Pawel Przybysz." ],
      "venue" : "Proceedings of the International Conference on Spoken Language Translation (IWSLT), Virtual Event.",
      "citeRegEx" : "Potapczyk and Przybysz.,? 2020",
      "shortCiteRegEx" : "Potapczyk and Przybysz.",
      "year" : 2020
    }, {
      "title" : "A TimeRestricted Self-Attention Layer for ASR",
      "author" : [ "Daniel Povey", "Hossein Hadian", "Pegah Ghahremani", "Ke Li", "Sanjeev Khudanpur." ],
      "venue" : "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "Povey et al\\.,? 2018",
      "shortCiteRegEx" : "Povey et al\\.",
      "year" : 2018
    }, {
      "title" : "Adapting machine translation models toward misrecognized speech with text-tospeech pronunciation rules and acoustic confusability",
      "author" : [ "Nicholas Ruiz", "Qin Gao", "William Lewis", "Marcello Federico." ],
      "venue" : "Proceedings of the Conference of the Interna-",
      "citeRegEx" : "Ruiz et al\\.,? 2015",
      "shortCiteRegEx" : "Ruiz et al\\.",
      "year" : 2015
    }, {
      "title" : "How2: A Large-scale Dataset For Multimodal Language Understanding",
      "author" : [ "Ramon Sanabria", "Ozan Caglayan", "Shruti Palaskar", "Desmond Elliott", "Loı̈c Barrault", "Lucia Specia", "Florian Metze" ],
      "venue" : "In Proceedings of Visually Grounded Interaction",
      "citeRegEx" : "Sanabria et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Sanabria et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural Machine Translation of Rare Words with Subword Units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "arXiv preprint arXiv:1508.07909.",
      "citeRegEx" : "Sennrich et al\\.,? 2015",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2015
    }, {
      "title" : "A Study of Translation Edit Rate with Targeted Human Annotation",
      "author" : [ "Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul." ],
      "venue" : "Proceedings of the Conference of the Association for Machine Translation of the Americas",
      "citeRegEx" : "Snover et al\\.,? 2006",
      "shortCiteRegEx" : "Snover et al\\.",
      "year" : 2006
    }, {
      "title" : "Fluency, Adequacy, or HTER?: exploring different human judgments with a tunable MT metric",
      "author" : [ "Matthew Snover", "Nitin Madnani", "Bonnie J Dorr", "Richard Schwartz." ],
      "venue" : "Proceedings of the Workshop on Statistical Machine Translation (WMT),",
      "citeRegEx" : "Snover et al\\.,? 2009",
      "shortCiteRegEx" : "Snover et al\\.",
      "year" : 2009
    }, {
      "title" : "Self-attentional models for lattice inputs",
      "author" : [ "Matthias Sperber", "Graham Neubig", "Ngoc-Quan Pham", "Alex Waibel." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 1185–1197, Florence, Italy.",
      "citeRegEx" : "Sperber et al\\.,? 2019",
      "shortCiteRegEx" : "Sperber et al\\.",
      "year" : 2019
    }, {
      "title" : "Self-Attentional Acoustic Models",
      "author" : [ "Matthias Sperber", "Jan Niehues", "Graham Neubig", "Sebastian Stüker", "Alex Waibel." ],
      "venue" : "Proceedings of the Conference of the International Speech Communication Association (INTERSPEECH), pages 3723–3727, Hyder-",
      "citeRegEx" : "Sperber et al\\.,? 2018",
      "shortCiteRegEx" : "Sperber et al\\.",
      "year" : 2018
    }, {
      "title" : "Toward Robust Neural Machine Translation for Noisy Input Sequences",
      "author" : [ "Matthias Sperber", "Jan Niehues", "Alex Waibel." ],
      "venue" : "Proceedings of the International Workshop on Spoken Language Translation (IWSLT), Tokyo, Japan.",
      "citeRegEx" : "Sperber et al\\.,? 2017",
      "shortCiteRegEx" : "Sperber et al\\.",
      "year" : 2017
    }, {
      "title" : "Speech translation and the end-to-end promise: Taking stock of where we are",
      "author" : [ "Matthias Sperber", "Matthias Paulik." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 7409–7421, Virtual Event.",
      "citeRegEx" : "Sperber and Paulik.,? 2020",
      "shortCiteRegEx" : "Sperber and Paulik.",
      "year" : 2020
    }, {
      "title" : "Machine translation of speech",
      "author" : [ "Fred W.M. Stentiford", "Martin G. Steer." ],
      "venue" : "British Telecom Technology Journal, 6(2):116–122.",
      "citeRegEx" : "Stentiford and Steer.,? 1988",
      "shortCiteRegEx" : "Stentiford and Steer.",
      "year" : 1988
    }, {
      "title" : "Rethinking the Inception Architecture for Computer Vision",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jon Shlens", "Zbigniew Wojna." ],
      "venue" : "Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages",
      "citeRegEx" : "Szegedy et al\\.,? 2016",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2016
    }, {
      "title" : "Opus – parallel corpora for everyone",
      "author" : [ "Jörg Tiedemann." ],
      "venue" : "Baltic Journal of Modern Computing, page 384. Special Issue: Proceedings of the 19th Annual Conference of the European Association of Machine Translation (EAMT).",
      "citeRegEx" : "Tiedemann.,? 2016",
      "shortCiteRegEx" : "Tiedemann.",
      "year" : 2016
    }, {
      "title" : "Attention is All You Need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of Advances in Neural Information Processing Systems 30 (NIPS), pages",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Janus: a speech-to-speech translation system",
      "author" : [ "Alex Waibel", "Ajay N Jain", "Arthur E McNair", "Hiroaki Saito", "Alexander G Hauptmann", "Joe Tebelskis" ],
      "venue" : null,
      "citeRegEx" : "Waibel et al\\.,? \\Q1991\\E",
      "shortCiteRegEx" : "Waibel et al\\.",
      "year" : 1991
    }, {
      "title" : "Quality Expectations of Machine Translation",
      "author" : [ "Andy Way." ],
      "venue" : "S. Castilho, J. Moorkens, F. Gaspari, and S. Doherty, editors, Translation quality assessment: From Principles to Practice, pages 159–178. Springer.",
      "citeRegEx" : "Way.,? 2018",
      "shortCiteRegEx" : "Way.",
      "year" : 2018
    }, {
      "title" : "Sequence-toSequence Models Can Directly Translate Foreign Speech",
      "author" : [ "Ron J. Weiss", "Jan Chorowski", "Navdeep Jaitly", "Yonghui Wu", "Zhifeng Chen." ],
      "venue" : "Proceedings of the Conference of the International Speech Communication Association (IN-",
      "citeRegEx" : "Weiss et al\\.,? 2017",
      "shortCiteRegEx" : "Weiss et al\\.",
      "year" : 2017
    }, {
      "title" : "2020) and MuST-C (Cattoni",
      "author" : [ "Sánchez" ],
      "venue" : null,
      "citeRegEx" : "Sánchez,? \\Q2020\\E",
      "shortCiteRegEx" : "Sánchez",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Within a few years after the first proofs of concept (Bérard et al., 2016; Weiss et al., 2017), the performance gap between the two paradigms has",
      "startOffset" : 53,
      "endOffset" : 94
    }, {
      "referenceID" : 66,
      "context" : "Within a few years after the first proofs of concept (Bérard et al., 2016; Weiss et al., 2017), the performance gap between the two paradigms has",
      "startOffset" : 53,
      "endOffset" : 94
    }, {
      "referenceID" : 13,
      "context" : "(en–de/es/it), we train state-of-the-art cascade and direct models (§3), running them on test data drawn from the MuST-C corpus (Cattoni et al., 2020).",
      "startOffset" : 128,
      "endOffset" : 150
    }, {
      "referenceID" : 60,
      "context" : "By concatenating ASR and MT components (Stentiford and Steer, 1988; Waibel et al., 1991), cascade ST architectures represent an intuitive solution to achieve reasonable performance and high adaptability across languages and domains.",
      "startOffset" : 39,
      "endOffset" : 88
    }, {
      "referenceID" : 64,
      "context" : "By concatenating ASR and MT components (Stentiford and Steer, 1988; Waibel et al., 1991), cascade ST architectures represent an intuitive solution to achieve reasonable performance and high adaptability across languages and domains.",
      "startOffset" : 39,
      "endOffset" : 88
    }, {
      "referenceID" : 32,
      "context" : "ASR n-best, lattices or confusion networks) which are more informative than the 1-best output (Lavie et al., 1996; Matusov et al., 2005; Bertoldi and Federico, 2005; Beck et al., 2019; Sperber et al., 2019), and ii)",
      "startOffset" : 94,
      "endOffset" : 206
    }, {
      "referenceID" : 35,
      "context" : "ASR n-best, lattices or confusion networks) which are more informative than the 1-best output (Lavie et al., 1996; Matusov et al., 2005; Bertoldi and Federico, 2005; Beck et al., 2019; Sperber et al., 2019), and ii)",
      "startOffset" : 94,
      "endOffset" : 206
    }, {
      "referenceID" : 12,
      "context" : "ASR n-best, lattices or confusion networks) which are more informative than the 1-best output (Lavie et al., 1996; Matusov et al., 2005; Bertoldi and Federico, 2005; Beck et al., 2019; Sperber et al., 2019), and ii)",
      "startOffset" : 94,
      "endOffset" : 206
    }, {
      "referenceID" : 7,
      "context" : "ASR n-best, lattices or confusion networks) which are more informative than the 1-best output (Lavie et al., 1996; Matusov et al., 2005; Bertoldi and Federico, 2005; Beck et al., 2019; Sperber et al., 2019), and ii)",
      "startOffset" : 94,
      "endOffset" : 206
    }, {
      "referenceID" : 56,
      "context" : "ASR n-best, lattices or confusion networks) which are more informative than the 1-best output (Lavie et al., 1996; Matusov et al., 2005; Bertoldi and Federico, 2005; Beck et al., 2019; Sperber et al., 2019), and ii)",
      "startOffset" : 94,
      "endOffset" : 206
    }, {
      "referenceID" : 44,
      "context" : "making the MT robust to ASR errors, for instance by training it on parallel data incorporating real or emulated ASR errors as in (Peitz et al., 2012; Ruiz et al., 2015; Sperber et al., 2017; Cheng et al., 2019; Di Gangi et al., 2019a).",
      "startOffset" : 129,
      "endOffset" : 234
    }, {
      "referenceID" : 51,
      "context" : "making the MT robust to ASR errors, for instance by training it on parallel data incorporating real or emulated ASR errors as in (Peitz et al., 2012; Ruiz et al., 2015; Sperber et al., 2017; Cheng et al., 2019; Di Gangi et al., 2019a).",
      "startOffset" : 129,
      "endOffset" : 234
    }, {
      "referenceID" : 58,
      "context" : "making the MT robust to ASR errors, for instance by training it on parallel data incorporating real or emulated ASR errors as in (Peitz et al., 2012; Ruiz et al., 2015; Sperber et al., 2017; Cheng et al., 2019; Di Gangi et al., 2019a).",
      "startOffset" : 129,
      "endOffset" : 234
    }, {
      "referenceID" : 15,
      "context" : "making the MT robust to ASR errors, for instance by training it on parallel data incorporating real or emulated ASR errors as in (Peitz et al., 2012; Ruiz et al., 2015; Sperber et al., 2017; Cheng et al., 2019; Di Gangi et al., 2019a).",
      "startOffset" : 129,
      "endOffset" : 234
    }, {
      "referenceID" : 46,
      "context" : "solutions are effective to some extent, state-of-theart cascade architectures (Pham et al., 2019; Bahar et al., 2020) prefer the latter, as they are simpler to implement and maintain.",
      "startOffset" : 78,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : "solutions are effective to some extent, state-of-theart cascade architectures (Pham et al., 2019; Bahar et al., 2020) prefer the latter, as they are simpler to implement and maintain.",
      "startOffset" : 78,
      "endOffset" : 117
    }, {
      "referenceID" : 49,
      "context" : "Currently, more effective solutions (Potapczyk and Przybysz, 2020; Bahar et al., 2020; Gaido et al., 2020) rely on ST-oriented adaptations of Transformer (Vaswani et al.",
      "startOffset" : 36,
      "endOffset" : 106
    }, {
      "referenceID" : 4,
      "context" : "Currently, more effective solutions (Potapczyk and Przybysz, 2020; Bahar et al., 2020; Gaido et al., 2020) rely on ST-oriented adaptations of Transformer (Vaswani et al.",
      "startOffset" : 36,
      "endOffset" : 106
    }, {
      "referenceID" : 21,
      "context" : "Currently, more effective solutions (Potapczyk and Przybysz, 2020; Bahar et al., 2020; Gaido et al., 2020) rely on ST-oriented adaptations of Transformer (Vaswani et al.",
      "startOffset" : 36,
      "endOffset" : 106
    }, {
      "referenceID" : 63,
      "context" : ", 2020) rely on ST-oriented adaptations of Transformer (Vaswani et al., 2017) integrating the encoder with: i) convolutional layers to reduce",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 50,
      "context" : "input length, and ii) penalties biasing attention to local context in the encoder self-attention layers (Povey et al., 2018; Sperber et al., 2018; Di Gangi et al., 2019b).",
      "startOffset" : 104,
      "endOffset" : 170
    }, {
      "referenceID" : 57,
      "context" : "input length, and ii) penalties biasing attention to local context in the encoder self-attention layers (Povey et al., 2018; Sperber et al., 2018; Di Gangi et al., 2019b).",
      "startOffset" : 104,
      "endOffset" : 170
    }, {
      "referenceID" : 5,
      "context" : "(audio, translation) pairs through speech synthesis or MT (Bahar et al., 2019b; Nguyen et al., 2020; Ko et al., 2015; Jia et al., 2019).",
      "startOffset" : 58,
      "endOffset" : 135
    }, {
      "referenceID" : 37,
      "context" : "(audio, translation) pairs through speech synthesis or MT (Bahar et al., 2019b; Nguyen et al., 2020; Ko et al., 2015; Jia et al., 2019).",
      "startOffset" : 58,
      "endOffset" : 135
    }, {
      "referenceID" : 30,
      "context" : "(audio, translation) pairs through speech synthesis or MT (Bahar et al., 2019b; Nguyen et al., 2020; Ko et al., 2015; Jia et al., 2019).",
      "startOffset" : 58,
      "endOffset" : 135
    }, {
      "referenceID" : 29,
      "context" : "(audio, translation) pairs through speech synthesis or MT (Bahar et al., 2019b; Nguyen et al., 2020; Ko et al., 2015; Jia et al., 2019).",
      "startOffset" : 58,
      "endOffset" : 135
    }, {
      "referenceID" : 23,
      "context" : "Knowledge transfer (Gutstein et al., 2008) consists in passing (here to ST) the knowledge learnt by a neural network trained on closely related tasks (here, ASR and MT).",
      "startOffset" : 19,
      "endOffset" : 42
    }, {
      "referenceID" : 10,
      "context" : "Existing ASR models have been used for encoder pre-training (Bérard et al., 2018; Bansal et al., 2019; Bahar et al., 2019a) and multi-task learning (Weiss et al.",
      "startOffset" : 60,
      "endOffset" : 123
    }, {
      "referenceID" : 6,
      "context" : "Existing ASR models have been used for encoder pre-training (Bérard et al., 2018; Bansal et al., 2019; Bahar et al., 2019a) and multi-task learning (Weiss et al.",
      "startOffset" : 60,
      "endOffset" : 123
    }, {
      "referenceID" : 3,
      "context" : "Existing ASR models have been used for encoder pre-training (Bérard et al., 2018; Bansal et al., 2019; Bahar et al., 2019a) and multi-task learning (Weiss et al.",
      "startOffset" : 60,
      "endOffset" : 123
    }, {
      "referenceID" : 3,
      "context" : "Existing neural MT models have been used for decoder pre-training (Bahar et al., 2019a; Inaguma et al., 2020), joint learning (Indurthi et al.",
      "startOffset" : 66,
      "endOffset" : 109
    }, {
      "referenceID" : 26,
      "context" : "Existing neural MT models have been used for decoder pre-training (Bahar et al., 2019a; Inaguma et al., 2020), joint learning (Indurthi et al.",
      "startOffset" : 66,
      "endOffset" : 109
    }, {
      "referenceID" : 27,
      "context" : ", 2020), joint learning (Indurthi et al., 2020; Liu et al., 2020) and knowledge distillation (Liu et al.",
      "startOffset" : 24,
      "endOffset" : 65
    }, {
      "referenceID" : 34,
      "context" : ", 2020), joint learning (Indurthi et al., 2020; Liu et al., 2020) and knowledge distillation (Liu et al.",
      "startOffset" : 24,
      "endOffset" : 65
    }, {
      "referenceID" : 33,
      "context" : ", 2020) and knowledge distillation (Liu et al., 2019).",
      "startOffset" : 35,
      "endOffset" : 53
    }, {
      "referenceID" : 49,
      "context" : "In the last round, direct models approached, and in one case (Potapczyk and Przybysz, 2020) outperformed, the cascade ones.",
      "startOffset" : 61,
      "endOffset" : 91
    }, {
      "referenceID" : 4,
      "context" : "Their good quality is attested by the comparison with the winning system at the IWSLT-20 offline ST task (Bahar et al., 2020),2 which consists of an ensemble of two cascade models scoring 28.",
      "startOffset" : 105,
      "endOffset" : 125
    }, {
      "referenceID" : 13,
      "context" : "Our evaluation data is drawn from the TED-based MuST-C corpus (Cattoni et al., 2020), the largest freely available multilingual corpus for ST.",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 54,
      "context" : "based evaluation, the original output is compared against its post-edited version using distance-based metrics like TER (Snover et al., 2006).",
      "startOffset" : 120,
      "endOffset" : 141
    }, {
      "referenceID" : 17,
      "context" : "This makes PE-based evaluation one of the most prominent methodologies used for translation quality assessment (Snover et al., 2006, 2009; Denkowski and Lavie, 2010; Cettolo et al., 2013; Bojar et al., 2015; Graham et al., 2016; Bentivogli et al., 2018b).",
      "startOffset" : 111,
      "endOffset" : 254
    }, {
      "referenceID" : 14,
      "context" : "This makes PE-based evaluation one of the most prominent methodologies used for translation quality assessment (Snover et al., 2006, 2009; Denkowski and Lavie, 2010; Cettolo et al., 2013; Bojar et al., 2015; Graham et al., 2016; Bentivogli et al., 2018b).",
      "startOffset" : 111,
      "endOffset" : 254
    }, {
      "referenceID" : 22,
      "context" : "This makes PE-based evaluation one of the most prominent methodologies used for translation quality assessment (Snover et al., 2006, 2009; Denkowski and Lavie, 2010; Cettolo et al., 2013; Bojar et al., 2015; Graham et al., 2016; Bentivogli et al., 2018b).",
      "startOffset" : 111,
      "endOffset" : 254
    }, {
      "referenceID" : 9,
      "context" : "This makes PE-based evaluation one of the most prominent methodologies used for translation quality assessment (Snover et al., 2006, 2009; Denkowski and Lavie, 2010; Cettolo et al., 2013; Bojar et al., 2015; Graham et al., 2016; Bentivogli et al., 2018b).",
      "startOffset" : 111,
      "endOffset" : 254
    }, {
      "referenceID" : 14,
      "context" : "To collect the post-edits for our study, we strictly followed the methodology of the IWSLT 20132017 evaluation campaigns (Cettolo et al., 2013), which offered us a consolidated framework and best practices to draw upon.",
      "startOffset" : 121,
      "endOffset" : 143
    }, {
      "referenceID" : 48,
      "context" : "edu/ ̃snover/tercom BLEU8 (Post, 2018) and TER scores computed",
      "startOffset" : 26,
      "endOffset" : 38
    }, {
      "referenceID" : 31,
      "context" : "Statistically significant differences (∗) are computed with Paired Bootstrap Resampling (Koehn, 2004).",
      "startOffset" : 88,
      "endOffset" : 101
    }, {
      "referenceID" : 59,
      "context" : "rect solutions come with the promise (Sperber and Paulik, 2020) of: i) higher robustness to error propagation, and ii) reduced loss of speech information (e.",
      "startOffset" : 37,
      "endOffset" : 63
    }, {
      "referenceID" : 45,
      "context" : "combines n-gram language models with the Naive Bayes algorithm, as proposed in (Peng and Schuurmans, 2003).",
      "startOffset" : 79,
      "endOffset" : 106
    }, {
      "referenceID" : 16,
      "context" : "To check this finding, we measured outputs’ lexical diversity in terms of moving average TypeToken Ratio – maTTR (Covington and McFall, 2010) – and with the Measure of Textual Lexical",
      "startOffset" : 113,
      "endOffset" : 141
    } ],
    "year" : 2021,
    "abstractText" : "Five years after the first published proofs of concept, direct approaches to speech translation (ST) are now competing with traditional cascade solutions. In light of this steady progress, can we claim that the performance gap between the two is closed? Starting from this question, we present a systematic comparison between state-of-the-art systems representative of the two paradigms. Focusing on three language directions (English– German/Italian/Spanish), we conduct automatic and manual evaluations, exploiting highquality professional post-edits and annotations. Our multi-faceted analysis on one of the few publicly available ST benchmarks attests for the first time that: i) the gap between the two paradigms is now closed, and ii) the subtle differences observed in their behavior are not sufficient for humans neither to distinguish them nor to prefer one over the other.",
    "creator" : "LaTeX with hyperref"
  }
}