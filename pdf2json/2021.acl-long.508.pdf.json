{
  "name" : "2021.acl-long.508.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search",
    "authors" : [ "Gyuwan Kim", "Kyunghyun Cho" ],
    "emails" : [ "gyuwan.kim@navercorp.com", "kyunghyun.cho@nyu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6501–6511\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6501"
    }, {
      "heading" : "1 Introduction",
      "text" : "Pre-trained language models (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2019; Yang et al., 2019; He et al., 2020) have achieved notable improvements in various natural language processing (NLP) tasks. Most of them rely on transformers (Vaswani et al., 2017), and the number of model parameters ranges from hundreds of millions to billions (Shoeybi et al., 2019; Raffel et al., 2019; Kaplan et al., 2020; Brown et al., 2020). Despite this high accuracy, excessive computational overhead\nduring inference, both in terms of time and memory, has hindered its use in real applications. This level of excessive computation has further raised the concern over energy consumption as well (Schwartz et al., 2019; Strubell et al., 2019; Cao et al., 2020).\nRecent studies have attempted to address these concerns regarding large-scale transformers’ computational and energy efficiency (see §7 for a more extensive discussion.) Among these, we focus on PoWER-BERT (Goyal et al., 2020) which progressively reduces sequence length by eliminating word-vectors based on the attention values as passing layers. PoWER-BERT establishes the superiority of accuracy-time trade-off over earlier approaches (Sanh et al., 2019; Sun et al., 2019; Michel et al., 2019). However, it requires us to train a separate model for each efficiency constraint. In this paper, we thus develop a framework based on PoWER-BERT such that we can train a single model that can be adapted in the inference time to meet any given efficiency target.\nIn order to train a transformer to cope with a diverse set of computational budgets in the inference time, we propose to train once while reducing the sequence length with a random proportion at each layer. We refer to this procedure as LengthDrop, which was motivated by the nested dropout (Rippel et al., 2014). We can extract sub-models of shared weights with any length configuration without requiring extra post-processing nor additional fine-tuning.\nIt is not trivial to find an optimal length configuration given the inference-time computational budget, although it is extremely important in order to deploy these large-scale transformers in practice. Once a transformer is trained with the proposed LengthDrop, we search for the length configuration that maximizes the accuracy given a computational budget. Because this search is combinatorial and has multiple objectives (accuracy and efficiency),\nin this work, we propose to use an evolutionary search algorithm, which further allows us to obtain a full Pareto frontier of accuracy-efficiency trade-off of each model.\nPoWER-BERT, which forms the foundation of the proposed two-stage procedure, is only applicable to sequence-level classification, because it eliminates some of the word vectors at each layer by design. In other words, it cannot be used for token-level tasks such as span-based question answering (Rajpurkar et al., 2016) because these tasks require hidden representations of the entire input sequence at the final layer. We thus propose to extend PoWER-BERT with a novel Drop-and-Restore process (§3.3), which eliminates this inherent limitation. Word vectors are dropped and set aside, rather than eliminated, in intermediate layers to maintain the saving of computational cost, as was with the original PoWER-BERT. These set-aside vectors are then restored at the final hidden layer and provided as an input to a subsequent task-specific layer, unlike the original PoWER-BERT.\nThe main contributions of this work are twofold. First, we introduce LengthDrop, a structured variant of dropout for training a single LengthAdaptive Transformer model that allows us to automatically derive multiple sub-models with different length configurations in the inference time using evolutionary search, without requiring any re-training. Second, we design Drop-and-Restore process that makes PoWER-BERT applicable beyond classification, which enables PoWER-BERT to be applicable to a wider range of NLP tasks such as span-based question answering. We empirically verify Length-Adaptive Transformer works quite well using the variants of BERT on a diverse set of NLP tasks, including SQuAD 1.1 (Rajpurkar et al., 2016) and two sequence-level classification tasks in GLUE benchmark (Wang et al., 2018). Our experiments reveal that the proposed approach grants us fine-grained control of computational efficiency and a superior accuracy-efficiency trade-off in the inference time compared to existing approaches."
    }, {
      "heading" : "2 Background",
      "text" : "In this section, we review some of the building blocks of our main approach. In particular, we review transformers, which are a standard backbone used in natural language processing these days, and PoWER-BERT, which was recently proposed as an effective way to train a large-scale, but highly effi-\ncient transformer for sequence-level classification."
    }, {
      "heading" : "2.1 Transformers and BERT",
      "text" : "A transformer is a particular neural network that has been designed to work with a variable-length sequence input and is implemented as a stack of self-attention and fully connected layers (Vaswani et al., 2017). It has recently become one of the most widely used models for natural language processing. Here, we give a brief overview of the transformer which is the basic building block of the proposed approach.\nEach token xt in a sequence of tokens x = (x1, . . . , xN ), representing input text, is first turned into a continuous vector h0t ∈ RH which is the sum of the token and position embedding vectors. This sequence is fed into the first transformer layer which returns another sequence of the same length h1 ∈ RN×H . We repeat this procedure L times, for a transformer with L layers, to obtain hL = (hL1 , . . . , h L N ). We refer to each vector in the hidden sequence at each layer as a word vector to emphasize that there exists a correspondence between each such vector and one of the input words.\nAlthough the transformer was first introduced for the problem of machine translation, Devlin et al. (2018) demonstrated that the transformer can be trained and used as a sentence encoder. More specifically, Devlin et al. (2018) showed that the transformer-based masked language model, called BERT, learns a universally useful parameter set that can be fine-tuned for any downstream task, including sequence-level and token-level classification.\nIn the case of sequence-level classification, a softmax classifier is attached to the word vector hL1 associated with the special token [CLS], and the entire network, including the softmax classifier and BERT, is fine-tuned. For token-level classification, we use each hLt as the final hidden representation of the associated t-th word in the input sequence. This strategy of pre-training followed by fine-tuning, often referred to as transfer learning, is a dominant approach to classification in natural language processing."
    }, {
      "heading" : "2.2 PoWER-BERT",
      "text" : "PoWER-BERT keeps only the topmost lj word vectors at each layer j by eliminating redundant ones based on the significance score which is the total amount of attention imposed by a word on the other words (Goyal et al., 2020). lj is the hyperparameter that determines how many vectors to\nkeep at layer j. PoWER-BERT has the same model parameters as BERT, but the extraction layers are interspersed after the self-attention layer in every transformer block (Vaswani et al., 2017).\nPoWER-BERT reduces inference time successfully, achieving better accuracy-time trade-off than DistilBERT (Sanh et al., 2019), BERT-PKD (Sun et al., 2019), and Head-Prune (Michel et al., 2019). Despite the original intention of maximizing the inference efficiency with the minimal loss in accuracy, it is possible to set up PoWER-BERT to be both more efficient and more accurate compared to the original BERT, which was observed but largely overlooked by Goyal et al. (2020).\nTraining a PoWER-BERT model consists of three steps: (1) fine-tuning, (2) length configuration search, and (3) re-training. The fine-tuning step is just like the standard fine-tuning step of BERT given a target task. A length configuration is a sequence of retention parameters (l1, · · · lL), each of which corresponds to the number of word vectors that are kept at each layer. These retention parameters are learned along with all the other parameters to minimize the original task loss together with an extra term that approximately measures the number of retained word vectors across layers. In the re-training step, PoWER-BERT is fine-tuned with the length configuration fixed to its learned one.\nFor each computational budget, we must train a separate model going through all three steps described above. Moreover, the length configuration search step above is only approximate, as it relies on the relaxation of retention parameters which are inherently discrete. This leads to the lack of guaranteed correlation between the success of this stage and true run-time. Even worse, it is a delicate act to tune the length configuration given a target computational budget because the trade-off is implicitly made via a regularization coefficient. Furthermore, PoWER-BERT has an inherent limitation in that it only applies to sequence-level classification because it eliminates word vectors in intermediate layers."
    }, {
      "heading" : "3 Length-Adaptive Transformer",
      "text" : "In this section, we explain our proposed framework which results in a transformer that reduces the length of a sequence at each layer with an arbitrary rate. We call such a resulting transformer a Length-Adaptive Transformer. We train Length-\nAdaptive Transformer with LengthDrop which randomly samples the number of hidden vectors to be dropped at each layer with the goal of making the final model robust to such drop in the inference time. Once the model is trained, we search for the optimal trade-off between accuracy and efficiency using multi-objective evolutionary search, which allows us to use the model for any given computational budget without fine-tuning nor retraining. At the end of this section, we describe Drop-and-Restore process as a way to greatly increase the applicability of PoWER-BERT which forms a building block of the proposed framework.\nIn short, we train a Length-Adaptive Transformer once with LengthDrop and Drop-andRestore, and use it with an automatically determined length configuration for inference with any target computational budget, on both sequencelevel and token-level tasks."
    }, {
      "heading" : "3.1 LengthDrop",
      "text" : "Earlier approaches to efficient inference with transformers have focused on a scenario where the target computational budget for inference is known in advance (Sanh et al., 2019; Goyal et al., 2020). This greatly increases the cost of deploying transformers, as it requires us to train a separate transformer for each scenario. Instead, we propose to train one model that could be used for a diverse set of target computational budgets without re-training.\nBefore each SGD update, LengthDrop randomly generates a length configuration by sequentially sampling a sequence length li+1 at the (i + 1)-th layer based on the previous layer’s sequence length li, following the uniform distribution U((1− p)li, li), where l0 is set to the length of the input sequence, and p is the LengthDrop probability. This sequential sampling results in a length configuration (l1, · · · , lL). Length-Adaptive Transformer can be thought of as consisting of a full model and many sub-models corresponding to different length configurations, similarly to a neural network trained with different dropout masks (Srivastava et al., 2014).\nLayerDrop From the perspective of each word vector, the proposed LengthDrop could be thought of as skipping the layers between when it was set aside and the final layer where it was restored. The word vector however does not have any information based on which it can determine whether it would be dropped at any particular layer. In our\npreliminary experiments, we found that this greatly hinders optimization. We address this issue by using LayerDrop (Fan et al., 2019) which skips each layer of a transformer uniformly at random. The LayerDrop encourages each word vector to be agnostic to skipping any number of layers between when it is dropped and when it is restored, just like dropout (Srivastava et al., 2014) prevents hidden neurons from co-adapting with each other by randomly dropping them.\nSandwich Rule and Inplace Distillation We observed that standard supervised training with LengthDrop does not work well in the preliminary experiments. We instead borrow a pair of training techniques developed by Yu and Huang (2019) which are sandwich rule and inplace distillation, for better optimization as well as final generalization. At each update, we update the full model without LengthDrop as usual to minimize the supervised loss function. We simultaneously update ns randomly-sampled sub-models (which are called sandwiches) and the smallest-possible sub-model, which corresponds to keeping only (1− p)li word vectors at each layer i, using knowledge distillation (Hinton et al., 2015) from the full model. Here, sub-models mean models with length reduction. They are trained to their prediction close to the full model’s prediction (inplace distillation)."
    }, {
      "heading" : "3.2 Evolutionary Search of Length Configurations",
      "text" : "After training a Length-Adaptive Transformer with LengthDrop, we search for appropriate length configurations for possible target computational budgets that will be given at inference time. The length configuration determines the model performance in terms of both accuracy and efficiency. In order to search for the optimal length configuration, we propose to use evolutionary search, similarly to Cai et al. (2019) and Wang et al. (2020a). This procedure is efficient, as it only requires a single pass through the relatively small validation set for each length configuration, unlike re-training for a new computational budget which requires multiple passes through a significantly larger training set for each budget.\nWe initialize the population with constant-ratio configurations. Each configuration is created by li+1 = (1− r)li for each layer i with r so that the amount of computation within the initial population is uniformly distributed between those of the small-\nest and full models. At each iteration, we evolve the population to consist only of configurations lie on a newly updated efficiency-accuracy Pareto frontier by mutation and crossover. Mutation alters an original length configuration (l1, · · · , lL) to (l′1, · · · , l′L) by sampling l′i from the uniform distribution U(l′i−1, li+1) with the probability pm or keeping the original length l′i = li, sweeping the layers from i = 1 to i = L. A crossover takes two length configurations and averages the lengths at each layer. Both of these operations are performed while ensuring the monotonicity of the lengths over the layers. We repeat this iteration G times while maintaining nm mutated configurations and nc crossover’d configurations. Repeating this procedure pushes the Pareto frontier further to identify the best trade-off between two objectives, efficiency and accuracy, without requiring any continuous relaxation of length configurations nor using a proxy objective function."
    }, {
      "heading" : "3.3 Drop-and-Restore Process",
      "text" : "The applicability of the PoWER-BERT, based on which our main contribution above was made, is limited to sequence-level classification because it eliminates word vectors at each layer. In addition to our main contribution above, we thus propose to extend the PoWER-BERT so that it is applicable to token-level classification, such as span-based question-answering. Our proposal, to which we refer as Drop-and-Restore, does not eliminate word vectors at each layer according to the length configuration but instead sets them aside until the final hidden layer. At the final hidden layer, these word vectors are brought back to form the full hidden sequence, as illustrated graphically in Figure 1."
    }, {
      "heading" : "4 Experiment Setup",
      "text" : "Datasets We test the proposed approach on both sequence-level and token-level tasks, the latter of which could not have been done with the original PoWER-BERT unless for the proposed Drop-andRestore. We use MNLI-m and SST-2 from GLUE benchmark (Wang et al., 2018), as was done to test PoWER-BERT earlier, for sequence-level classification. We choose them because consistent accuracy scores from standard training on them due to their sufficiently large training set imply that they are reliable to verify our approach. We use SQuAD 1.1 (Rajpurkar et al., 2016) for token-level classification.\nEvaluation metrics We use the number of floating operations (FLOPs) as a main metric to measure the inference efficiency given any length configuration, as it is agnostic to the choice of the underlying hardware, unlike other alternatives such as hardware-aware latency (Wang et al., 2020a) or energy consumption (Henderson et al., 2020). We later demonstrate that FLOPs and wall-clock time on GPU and CPU correlate well with the proposed approach, which is not necessarily the case for other approaches, such as unstructured weight pruning (Han et al., 2015; See et al., 2016).\nPre-trained transformers Since BERT was introduced by Devlin et al. (2018), it has become a standard practice to start from a pre-trained (masked) language model and fine-tune it for each downstream task. We follow the same strategy in this paper and test two pre-trained transformerbased language models; BERTBase (Devlin et al., 2018) and DistilBERT (Sanh et al., 2019), which allows us to demonstrate that the usefulness and applicability of our approach are not tied to any specific architectural choice, such as the number of layers and the maximum input sequence length. Although we focus on BERT-based masked language models here, the proposed approach is readily applicable to any transformer-based models.\nLearning We train a Length-Adaptive Transformer with LengthDrop probability and LayerDrop probability both set to 0.2. We use ns = 2 randomly sampled intermediate sub-models in addition to the full model and smallest model for applying the sandwich learning rule.\nWe start fine-tuning the pre-trained transformer without Drop-and-Restore first, just as Goyal et al. (2020) did with PoWER-BERT. We then continue fine-tuning it for another five epochs with Dropand-Restore. This is unlike the recommended three\nepochs by Devlin et al. (2018), as learning progresses slower due to a higher level of stochasticity introduced by LengthDrop and LayerDrop. We use the batch size of 32, the learning rate of 5e− 5 for SQuAD 1.1 and 2e− 5 for MNLI-m and SST, and the maximum sequence length of 384 for SQuAD 1.1 and 128 for MNLI-m and SST.\nSearch We run up to G = 30 iterations of evolutionary search, using nm = 30 mutated configurations with mutation probability pm = 0.5 and nc = 30 crossover’d configurations, to find the Pareto frontier of accuracy and efficiency."
    }, {
      "heading" : "5 Results and Analysis",
      "text" : "Efficiency-accuracy trade-off We use SQuAD 1.1 to examine the effect of the proposed approach on the efficiency-accuracy trade-off. When the underlying classifier was not trained with LengthDrop, as proposed in this paper, the accuracy drops even more dramatically as more word vectors are dropped at each layer. The difference between standard transformer and Length-Adaptive Transformer is stark in Figure 2. This verifies the importance of training a transformer in a way that makes it malleable for inference-time re-configuration.\nWhen the model was trained with the proposed LengthDrop, we notice the efficacy of the proposed approach of using evolutionary search to find the optimal trade-off between inference efficiency and accuracy. The trade-off curve from the proposed search strategy has a larger area-under-curve (AUC) than when constant-rate length reduction was used to meet a target computational budget. It demonstrates the importance of using both LengthDrop and evolutionary search.\nWe make a minor observation that the proposed approach ends up with a significantly higher accuracy than DistillBERT when enough computational\nbudget is allowed for inference (log FLOPs > 10). This makes our approach desirable in a wide array of scenarios, as it does not require any additional pre-training stage, as does DistilBERT. With a severe constraint on the computational budget, the proposed approach could be used on DistilBERT to significantly improve the efficiency without compromising the accuracy.\nMaximizing inference efficiency We consider all three tasks, SQuAD 1.1, MNLI-m, and SST-2, and investigate how much efficiency can be gained by the proposed approach with minimal sacrifice of accuracy. First, we look at how much efficiency could be gained without losing accuracy. That is, we use the length configuration that maximizes the inference efficiency (i.e., minimize the FLOPs) while ensuring that the accuracy is above or the same as the accuracy of the standard approach without any drop of word vectors. The results are presented in the rows marked with Length-Adaptive† from Table 1. For example, in the case of BERTBase, the proposed approach reduces FLOPs by more than half across all three tasks.\nFrom Figure 2, we have observed that the proposed Length-Adaptive Transformer generalizes better than the standard, base model in some cases. Thus, we try to maximize both the inference ef-\nficiency and accuracy in order to see whether it is possible for the proposed algorithm to find a length configuration that both maximizes inference efficiency and improves accuracy. We present the results in the rows marked with Length-Adaptive? from Table 1. For all cases, Length-Adaptive Transformer achieves higher accuracy than a standard transformer does while reducing FLOPs significantly. Although it is not apparent from the table, tor MNLI-m and SST-2, the accuracy of the smallest sub-model is already greater than or equal to that of a standard transformer.\nFLOPs vs. Latency As has been discussed in recent literature (see, e.g., Li et al. (2020); Chin et al. (2020)), the number of FLOPs is not a perfect indicator of the real latency measured in wall-clock time, as the latter is affected by the combination of hardware choice and network architecture. To understand the real-world impact of the proposed approach, we study the relationship between FLOPs, obtained by the proposed procedure, and wall-clock time measured on both CPU and GPU by measuring them while varying length configurations. As shown in Figure 3, FLOPs and latency exhibit nearlinear correlation on GPU, when the minibatch size is ≥ 16, and regardless of the minibatch size, on CPU. In other words, the reduction in FLOPs with\nthe proposed approach directly implies the reduction in wall-clock time.\nConvergence of search Although the proposed approach is efficient in that it requires only one round of training, it needs a separate search stage for each target budget. It is important for evolutionary search to converge quickly in the number of forward sweeps of a validation set. As exemplified in Figure 4, evolutionary search converges after about fifteen iterations."
    }, {
      "heading" : "6 Comparison with Other Works",
      "text" : "Our framework allows a novel method for anytime prediction with adaptive sequence length given any transformers. Thus, our goal is not state-of-the-art classification accuracy, although our experimental results (§5) demonstrate that our method still attains a good accuracy level.\nWe emphasize that other adaptive computation works (§7) are orthogonal with ours, meaning that various adaptive dimensions (sequence length, depth, attention head, hidden dimension, etc.) can be jointly used. In other words, even if other adaptive methods show better curves than ours, our method and theirs can boost each other when combined. We provide some comparison results with PoWER-BERT (not anytime prediction method) and DynaBERT (Hou et al., 2020) (concurrent adaptive computation method) as follows.\nComparison with PoWER-BERT According to Goyal et al. (2020), PoWER-BERT achieves 2.6x speedup for MNLI-m and 2.4x speedup for SST-2 by losing 1% of their accuracy. LengthAdaptive Transformer obtains a 2.9x speedup in terms of FLOPs without losing accuracy on MNLIm and SST-2. Considering Figure 3, our speedup in\nexecution time would be close to 2.9x in the same setting of PoWER-BERT where the time measurement is done with a batch size of 128 on GPU. It indicates that our model offers a better trade-off than PoWER-BERT, even with a single model.\nComparison with DynaBERT According to Hou et al. (2020), DyanBERT obtains a gain of +1.0, +0.1, +0.4 for the best accuracy in SQuAD 1.1, MNLI-m, and SST-2, respectively, while Length-Adaptive Transformer achieves a gain of +1.1, +0.6, +0.3. These results imply that LengthAdaptive Transformer can give a comparable (or better) performance with DynaBERT."
    }, {
      "heading" : "7 Related Work",
      "text" : "The main purpose of the proposed algorithm is to improve the inference efficiency of a large-scale transformer. This goal has been pursued from various directions, and here we provide a brief overview of these earlier and some concurrent attempts in the context of the proposed approach.\nWeight pruning Weight pruning (Han et al., 2015) focuses on reducing the number of parameters that directly reflects the memory footprint of a model and indirectly correlates with inference speed. However, their actual speed-up in runtime is usually not significant, especially while executing a model with parallel computation using GPU devices (Tang et al., 2018; Li et al., 2020).\nAdaptive architecture There are three major axes along which computation can be reduced in a neural network; (1) input size/length, (2) network depth, and (3) network width. The proposed approach, based on PoWER-BERT, adaptively reduces the input length as the input sequence is pro-\ncessed by the transformer layers. In our knowledge, Goyal et al. (2020) is the first work in this direction for transformers. Funnel-Transformer (Dai et al., 2020) and multi-scale transformer language models (Subramanian et al., 2020) also successfully reduce sequence length in the middle and rescale to full length for the final computation. However, their inference complexity is fixed differently with PoWER-BERT because they are not designed to control efficiency. More recently, TR-BERT (Ye et al., 2021) introduces a policy network trained via reinforcement learning to decide which vectors to skip.\nLayerDrop (Fan et al., 2019) drops random layers during the training to be robust to pruning inspired by Huang et al. (2016). Word-level adaptive depth in Elbayad et al. (2019) and Liu et al. (2020b) might seemingly resemble with length reduction, but word vectors that reached the maximal layer are used for self-attention computation without updating themselves. Escaping a network early (Teerapittayanon et al., 2016; Huang et al., 2017) based on the confidence of the prediction (Xin et al., 2020, 2021; Schwartz et al., 2020; Liu et al., 2020a; Li et al., 2021) also offers a control over accuracyefficiency trade-off by changing a threshold, but it is difficult to tune a threshold for a desired computational budget because of the example-wise adaptive computation.\nSlimmable neural networks (Yu et al., 2018; Lee and Shin, 2018) reduce the hidden dimension for the any-time prediction. DynaBERT (Hou et al., 2020) can run at adaptive width (the number of attention heads and intermediate hidden dimension) and depth. Hardware-aware Transformers (Wang et al., 2020a) construct a design space with arbitrary encoder-decoder attention and heterogeneous layers in terms of different numbers of layers, attention heads, hidden dimension, and embedding dimension. SpAtten (Wang et al., 2020b) performs cascade token and head pruning for an efficient algorithm-architecture co-design.\nStructured dropout A major innovation we introduce over the existing PoWER-BERT is the use of stochastic, structured regularization to make a transformer robust to the choice of length configuration in the inference time. Rippel et al. (2014) proposes a nested dropout to learn ordered representations. Similar to LengthDrop, it samples an index from a prior distribution and drops all units with a larger index than the sampled one.\nSearch There have been a series of attempts at finding the optimal network configuration by solving a combinatorial optimization problem. In computer vision, Once-for-All (Cai et al., 2019) use an evolutionary search (Real et al., 2019) to find a better configuration in dimensions of depth, width, kernel size, and resolution given computational budget. Similarly but differently, our evolutionary search is mutli-objective to find length configurations on the Pareto accuracy-efficiency frontier to cope with any possible computational budgets. Moreover, we only change the sequence length of hidden vectors instead of architectural model size like dimensions.\nSequence Length Shortformer (Press et al., 2020) initially trained on shorter subsequences and then moved to longer ones achieves improved perplexity than a standard transformer with normal training while reducing overall training time. Novel architectures with the efficient attention mechanism (Kitaev et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020; Ainslie et al., 2020; Choromanski et al., 2020; Peng et al., 2021) are suggested to reduce the transformer’s quadratic computational complexity in the input sequence length. Tay et al. (2020b) and Tay et al. (2020a) provide a survey of these efficient transformers and their benchmark comparison, respectively."
    }, {
      "heading" : "8 Conclusion and Future Work",
      "text" : "In this work, we propose a new framework for training a transformer once and using it for efficient inference under any computational budget. With the help of training with LengthDrop and Dropand-Restore process followed by the evolutionary search, our proposed Length-Adaptive Transformer allows any given transformer models to be used with any inference-time computational budget for both sequence-level and token-level classification tasks. Our experiments, on SQuAD 1.1, MNLIm and SST-2, have revealed that the proposed algorithmic framework significantly pushes a better Pareto frontier on the trade-off between inference efficiency and accuracy. Furthermore, we have observed that the proposed Length-Adaptive Transformer could achieve up to 3x speed-up over the standard transformer without sacrificing accuracy, both in terms of FLOPs and wallclock time.\nAlthough our approach finds an optimal length configuration of a trained classifier per computational budget, it leaves a open question whether the proposed approach could be further extended\nto support per-instance length configuration by for instance training a small, auxiliary neural network for each computational budget. Yet another aspect we have not investigated in this paper is the applicability of the proposed approach to sequence generation, such as machine translation. We leave both of these research directions for the future.\nOur approach is effective, as we have shown in this paper, and also quite simple to implement on top of existing language models. We release our implementation at https://github.com/clovaai/lengthadaptive-transformer, which is based on HuggingFace’s Transformers library (Wolf et al., 2019), and plan to adapt it for a broader set of transformerbased models and downstream tasks, including other modalities (Dosovitskiy et al., 2020; Touvron et al., 2020; Gulati et al., 2020)."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors appreciate Clova AI members and the anonymous reviewers for their constructive feedback. Specifically, Dongyoon Han and Byeongho Heo introduced relevant works and gave insights from the view of the computer vision community. We use Naver Smart Machine Learning (Sung et al., 2017; Kim et al., 2018) platform for the experiments."
    } ],
    "references" : [ {
      "title" : "Etc: Encoding long and structured inputs in transformers",
      "author" : [ "Joshua Ainslie", "Santiago Ontanón", "Chris Alberti", "Vaclav Cvicek", "Zachary Fisher", "Philip Pham", "Anirudh Ravula", "Sumit Sanghai", "Qifan Wang", "Li Yang." ],
      "venue" : "arXiv preprint arXiv:2004.08483.",
      "citeRegEx" : "Ainslie et al\\.,? 2020",
      "shortCiteRegEx" : "Ainslie et al\\.",
      "year" : 2020
    }, {
      "title" : "Longformer: The long-document transformer",
      "author" : [ "Iz Beltagy", "Matthew E Peters", "Arman Cohan." ],
      "venue" : "arXiv preprint arXiv:2004.05150.",
      "citeRegEx" : "Beltagy et al\\.,? 2020",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are few-shot learners. arXiv preprint arXiv:2005.14165",
      "author" : [ "Tom B Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell" ],
      "venue" : null,
      "citeRegEx" : "Brown et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 2020
    }, {
      "title" : "Once for all: Train one network and specialize it for efficient deployment",
      "author" : [ "Han Cai", "Chuang Gan", "Song Han." ],
      "venue" : "arXiv preprint arXiv:1908.09791.",
      "citeRegEx" : "Cai et al\\.,? 2019",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards accurate and reliable energy measurement of nlp models",
      "author" : [ "Qingqing Cao", "Aruna Balasubramanian", "Niranjan Balasubramanian." ],
      "venue" : "arXiv preprint arXiv:2010.05248.",
      "citeRegEx" : "Cao et al\\.,? 2020",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards efficient model compression via learned global ranking",
      "author" : [ "Ting-Wu Chin", "Ruizhou Ding", "Cha Zhang", "Diana Marculescu." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Chin et al\\.,? 2020",
      "shortCiteRegEx" : "Chin et al\\.",
      "year" : 2020
    }, {
      "title" : "Rethinking attention with performers",
      "author" : [ "Krzysztof Choromanski", "Valerii Likhosherstov", "David Dohan", "Xingyou Song", "Andreea Gane", "Tamas Sarlos", "Peter Hawkins", "Jared Davis", "Afroz Mohiuddin", "Lukasz Kaiser" ],
      "venue" : "arXiv preprint arXiv:2009.14794",
      "citeRegEx" : "Choromanski et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Choromanski et al\\.",
      "year" : 2020
    }, {
      "title" : "Funnel-transformer: Filtering out sequential redundancy for efficient language processing",
      "author" : [ "Zihang Dai", "Guokun Lai", "Yiming Yang", "Quoc V Le." ],
      "venue" : "arXiv preprint arXiv:2006.03236.",
      "citeRegEx" : "Dai et al\\.,? 2020",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "An image is worth 16x16 words: Transformers",
      "author" : [ "Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly" ],
      "venue" : null,
      "citeRegEx" : "Dosovitskiy et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Dosovitskiy et al\\.",
      "year" : 2020
    }, {
      "title" : "Depth-adaptive transformer",
      "author" : [ "Maha Elbayad", "Jiatao Gu", "Edouard Grave", "Michael Auli." ],
      "venue" : "arXiv preprint arXiv:1910.10073.",
      "citeRegEx" : "Elbayad et al\\.,? 2019",
      "shortCiteRegEx" : "Elbayad et al\\.",
      "year" : 2019
    }, {
      "title" : "Reducing transformer depth on demand with structured dropout",
      "author" : [ "Angela Fan", "Edouard Grave", "Armand Joulin." ],
      "venue" : "arXiv preprint arXiv:1909.11556.",
      "citeRegEx" : "Fan et al\\.,? 2019",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2019
    }, {
      "title" : "Power-bert: Accelerating bert inference via progressive word-vector elimination",
      "author" : [ "Saurabh Goyal", "Anamitra Roy Choudhury", "Saurabh Raje", "Venkatesan Chakaravarthy", "Yogish Sabharwal", "Ashish Verma." ],
      "venue" : "International Conference on Machine",
      "citeRegEx" : "Goyal et al\\.,? 2020",
      "shortCiteRegEx" : "Goyal et al\\.",
      "year" : 2020
    }, {
      "title" : "Conformer: Convolution-augmented transformer for speech recognition",
      "author" : [ "Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu" ],
      "venue" : null,
      "citeRegEx" : "Gulati et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Gulati et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
      "author" : [ "Song Han", "Huizi Mao", "William J Dally." ],
      "venue" : "arXiv preprint arXiv:1510.00149.",
      "citeRegEx" : "Han et al\\.,? 2015",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2015
    }, {
      "title" : "Deberta: Decoding-enhanced bert with disentangled attention",
      "author" : [ "Pengcheng He", "Xiaodong Liu", "Jianfeng Gao", "Weizhu Chen." ],
      "venue" : "arXiv preprint arXiv:2006.03654.",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards the systematic reporting of the energy and carbon footprints of machine learning",
      "author" : [ "Peter Henderson", "Jieru Hu", "Joshua Romoff", "Emma Brunskill", "Dan Jurafsky", "Joelle Pineau." ],
      "venue" : "arXiv preprint arXiv:2002.05651.",
      "citeRegEx" : "Henderson et al\\.,? 2020",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2020
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean." ],
      "venue" : "arXiv preprint arXiv:1503.02531.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Dynabert: Dynamic bert with adaptive width and depth",
      "author" : [ "Lu Hou", "Lifeng Shang", "Xin Jiang", "Qun Liu." ],
      "venue" : "arXiv preprint arXiv:2004.04037.",
      "citeRegEx" : "Hou et al\\.,? 2020",
      "shortCiteRegEx" : "Hou et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-scale dense networks for resource efficient image classification",
      "author" : [ "Gao Huang", "Danlu Chen", "Tianhong Li", "Felix Wu", "Laurens van der Maaten", "Kilian Q Weinberger." ],
      "venue" : "arXiv preprint arXiv:1703.09844.",
      "citeRegEx" : "Huang et al\\.,? 2017",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep networks with stochastic depth",
      "author" : [ "Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Q Weinberger." ],
      "venue" : "European conference on computer vision, pages 646–661. Springer.",
      "citeRegEx" : "Huang et al\\.,? 2016",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Scaling laws for neural language models",
      "author" : [ "Jared Kaplan", "Sam McCandlish", "Tom Henighan", "Tom B Brown", "Benjamin Chess", "Rewon Child", "Scott Gray", "Alec Radford", "Jeffrey Wu", "Dario Amodei." ],
      "venue" : "arXiv preprint arXiv:2001.08361.",
      "citeRegEx" : "Kaplan et al\\.,? 2020",
      "shortCiteRegEx" : "Kaplan et al\\.",
      "year" : 2020
    }, {
      "title" : "Nsml: Meet the mlaas platform with a real-world case study",
      "author" : [ "Hanjoo Kim", "Minkyu Kim", "Dongjoo Seo", "Jinwoong Kim", "Heungseok Park", "Soeun Park", "Hyunwoo Jo", "KyungHyun Kim", "Youngil Yang", "Youngkwan Kim" ],
      "venue" : null,
      "citeRegEx" : "Kim et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2018
    }, {
      "title" : "Reformer: The efficient transformer",
      "author" : [ "Nikita Kitaev", "Łukasz Kaiser", "Anselm Levskaya." ],
      "venue" : "arXiv preprint arXiv:2001.04451.",
      "citeRegEx" : "Kitaev et al\\.,? 2020",
      "shortCiteRegEx" : "Kitaev et al\\.",
      "year" : 2020
    }, {
      "title" : "Anytime neural prediction via slicing networks vertically",
      "author" : [ "Hankook Lee", "Jinwoo Shin." ],
      "venue" : "arXiv preprint arXiv:1807.02609.",
      "citeRegEx" : "Lee and Shin.,? 2018",
      "shortCiteRegEx" : "Lee and Shin.",
      "year" : 2018
    }, {
      "title" : "Accelerating bert inference for sequence labeling via earlyexit",
      "author" : [ "Xiaonan Li", "Yunfan Shao", "Tianxiang Sun", "Hang Yan", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "arXiv preprint arXiv:2105.13878.",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Train large, then compress: Rethinking model size for efficient training and inference of transformers",
      "author" : [ "Zhuohan Li", "Eric Wallace", "Sheng Shen", "Kevin Lin", "Kurt Keutzer", "Dan Klein", "Joseph E Gonzalez." ],
      "venue" : "arXiv preprint arXiv:2002.11794.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Fastbert: a selfdistilling bert with adaptive inference time",
      "author" : [ "Weijie Liu", "Peng Zhou", "Zhe Zhao", "Zhiruo Wang", "Haotang Deng", "Qi Ju." ],
      "venue" : "arXiv preprint arXiv:2004.02178.",
      "citeRegEx" : "Liu et al\\.,? 2020a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Explicitly modeling adaptive depths for transformer",
      "author" : [ "Yijin Liu", "Fandong Meng", "Jie Zhou", "Yufeng Chen", "Jinan Xu." ],
      "venue" : "arXiv preprint arXiv:2004.13542.",
      "citeRegEx" : "Liu et al\\.,? 2020b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Are sixteen heads really better than one",
      "author" : [ "Paul Michel", "Omer Levy", "Graham Neubig" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Michel et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Michel et al\\.",
      "year" : 2019
    }, {
      "title" : "Random feature attention",
      "author" : [ "Hao Peng", "Nikolaos Pappas", "Dani Yogatama", "Roy Schwartz", "Noah A Smith", "Lingpeng Kong." ],
      "venue" : "arXiv preprint arXiv:2103.02143.",
      "citeRegEx" : "Peng et al\\.,? 2021",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2021
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "arXiv preprint arXiv:1802.05365.",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Shortformer: Better language modeling using shorter inputs",
      "author" : [ "Ofir Press", "Noah A Smith", "Mike Lewis." ],
      "venue" : "arXiv preprint arXiv:2012.15832.",
      "citeRegEx" : "Press et al\\.,? 2020",
      "shortCiteRegEx" : "Press et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI Blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "arXiv preprint arXiv:1910.10683.",
      "citeRegEx" : "Raffel et al\\.,? 2019",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2019
    }, {
      "title" : "Squad: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "arXiv preprint arXiv:1606.05250.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Regularized evolution for image classifier architecture search",
      "author" : [ "Esteban Real", "Alok Aggarwal", "Yanping Huang", "Quoc V Le." ],
      "venue" : "Proceedings of the aaai conference on artificial intelligence, volume 33, pages 4780–4789.",
      "citeRegEx" : "Real et al\\.,? 2019",
      "shortCiteRegEx" : "Real et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning ordered representations with nested dropout",
      "author" : [ "Oren Rippel", "Michael Gelbart", "Ryan Adams." ],
      "venue" : "International Conference on Machine Learning, pages 1746–1754.",
      "citeRegEx" : "Rippel et al\\.,? 2014",
      "shortCiteRegEx" : "Rippel et al\\.",
      "year" : 2014
    }, {
      "title" : "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "arXiv preprint arXiv:1910.01108.",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "Green ai",
      "author" : [ "Roy Schwartz", "Jesse Dodge", "Noah A Smith", "Oren Etzioni." ],
      "venue" : "arXiv preprint arXiv:1907.10597.",
      "citeRegEx" : "Schwartz et al\\.,? 2019",
      "shortCiteRegEx" : "Schwartz et al\\.",
      "year" : 2019
    }, {
      "title" : "The right tool for the job: Matching model and instance complexities",
      "author" : [ "Roy Schwartz", "Gabi Stanovsky", "Swabha Swayamdipta", "Jesse Dodge", "Noah A Smith." ],
      "venue" : "arXiv preprint arXiv:2004.07453.",
      "citeRegEx" : "Schwartz et al\\.,? 2020",
      "shortCiteRegEx" : "Schwartz et al\\.",
      "year" : 2020
    }, {
      "title" : "Compression of neural machine translation models via pruning",
      "author" : [ "Abigail See", "Minh-Thang Luong", "Christopher D Manning." ],
      "venue" : "arXiv preprint arXiv:1606.09274.",
      "citeRegEx" : "See et al\\.,? 2016",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2016
    }, {
      "title" : "Megatron-lm: Training multi-billion parameter language models using gpu model parallelism",
      "author" : [ "Mohammad Shoeybi", "Mostofa Patwary", "Raul Puri", "Patrick LeGresley", "Jared Casper", "Bryan Catanzaro." ],
      "venue" : "arXiv preprint arXiv:1909.08053.",
      "citeRegEx" : "Shoeybi et al\\.,? 2019",
      "shortCiteRegEx" : "Shoeybi et al\\.",
      "year" : 2019
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "The journal of machine learning research, 15(1):1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Energy and policy considerations for deep learning in nlp",
      "author" : [ "Emma Strubell", "Ananya Ganesh", "Andrew McCallum." ],
      "venue" : "arXiv preprint arXiv:1906.02243.",
      "citeRegEx" : "Strubell et al\\.,? 2019",
      "shortCiteRegEx" : "Strubell et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi-scale transformer language models. arXiv preprint arXiv:2005.00581",
      "author" : [ "Sandeep Subramanian", "Ronan Collobert", "Marc’Aurelio Ranzato", "Y-Lan Boureau" ],
      "venue" : null,
      "citeRegEx" : "Subramanian et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Subramanian et al\\.",
      "year" : 2020
    }, {
      "title" : "Patient knowledge distillation for bert model compression",
      "author" : [ "Siqi Sun", "Yu Cheng", "Zhe Gan", "Jingjing Liu." ],
      "venue" : "arXiv preprint arXiv:1908.09355.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Nsml: A machine learning platform that enables you",
      "author" : [ "Nako Sung", "Minkyu Kim", "Hyunwoo Jo", "Youngil Yang", "Jingwoong Kim", "Leonard Lausen", "Youngkwan Kim", "Gayoung Lee", "Donghyun Kwak", "Jung-Woo Ha" ],
      "venue" : null,
      "citeRegEx" : "Sung et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Sung et al\\.",
      "year" : 2017
    }, {
      "title" : "Flops as a direct optimization objective for learning sparse neural networks",
      "author" : [ "Raphael Tang", "Ashutosh Adhikari", "Jimmy Lin." ],
      "venue" : "arXiv preprint arXiv:1811.03060.",
      "citeRegEx" : "Tang et al\\.,? 2018",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2018
    }, {
      "title" : "Long range arena: A benchmark for efficient transformers",
      "author" : [ "Yi Tay", "Mostafa Dehghani", "Samira Abnar", "Yikang Shen", "Dara Bahri", "Philip Pham", "Jinfeng Rao", "Liu Yang", "Sebastian Ruder", "Donald Metzler." ],
      "venue" : "arXiv preprint arXiv:2011.04006.",
      "citeRegEx" : "Tay et al\\.,? 2020a",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2020
    }, {
      "title" : "Efficient transformers: A survey",
      "author" : [ "Yi Tay", "Mostafa Dehghani", "Dara Bahri", "Donald Metzler." ],
      "venue" : "arXiv preprint arXiv:2009.06732.",
      "citeRegEx" : "Tay et al\\.,? 2020b",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2020
    }, {
      "title" : "Branchynet: Fast inference via early exiting from deep neural networks",
      "author" : [ "Surat Teerapittayanon", "Bradley McDanel", "HsiangTsung Kung." ],
      "venue" : "2016 23rd International Conference on Pattern Recognition (ICPR), pages 2464–2469. IEEE.",
      "citeRegEx" : "Teerapittayanon et al\\.,? 2016",
      "shortCiteRegEx" : "Teerapittayanon et al\\.",
      "year" : 2016
    }, {
      "title" : "Training data-efficient image transformers & distillation through attention",
      "author" : [ "Hugo Touvron", "Matthieu Cord", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Hervé Jégou." ],
      "venue" : "arXiv preprint arXiv:2012.12877.",
      "citeRegEx" : "Touvron et al\\.,? 2020",
      "shortCiteRegEx" : "Touvron et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Glue: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:1804.07461.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Hat: Hardware-aware transformers for efficient natural language processing",
      "author" : [ "Hanrui Wang", "Zhanghao Wu", "Zhijian Liu", "Han Cai", "Ligeng Zhu", "Chuang Gan", "Song Han." ],
      "venue" : "arXiv preprint arXiv:2005.14187.",
      "citeRegEx" : "Wang et al\\.,? 2020a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Spatten: Efficient sparse attention architecture with cascade token and head pruning",
      "author" : [ "Hanrui Wang", "Zhekai Zhang", "Song Han." ],
      "venue" : "arXiv preprint arXiv:2012.09852.",
      "citeRegEx" : "Wang et al\\.,? 2020b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformers: State-of-theart natural language processing",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "Rémi Louf", "Morgan Funtowicz" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "Deebert: Dynamic early exiting for accelerating bert inference",
      "author" : [ "Ji Xin", "Raphael Tang", "Jaejun Lee", "Yaoliang Yu", "Jimmy Lin." ],
      "venue" : "arXiv preprint arXiv:2004.12993.",
      "citeRegEx" : "Xin et al\\.,? 2020",
      "shortCiteRegEx" : "Xin et al\\.",
      "year" : 2020
    }, {
      "title" : "Berxit: Early exiting for bert with better fine-tuning and extension to regression",
      "author" : [ "Ji Xin", "Raphael Tang", "Yaoliang Yu", "Jimmy Lin." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Xin et al\\.,? 2021",
      "shortCiteRegEx" : "Xin et al\\.",
      "year" : 2021
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems, pages 5754–5764.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Tr-bert: Dynamic token reduction for accelerating bert inference",
      "author" : [ "Deming Ye", "Yankai Lin", "Yufei Huang", "Maosong Sun." ],
      "venue" : "arXiv preprint arXiv:2105.11618.",
      "citeRegEx" : "Ye et al\\.,? 2021",
      "shortCiteRegEx" : "Ye et al\\.",
      "year" : 2021
    }, {
      "title" : "Universally slimmable networks and improved training techniques",
      "author" : [ "Jiahui Yu", "Thomas S Huang." ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision, pages 1803–1811.",
      "citeRegEx" : "Yu and Huang.,? 2019",
      "shortCiteRegEx" : "Yu and Huang.",
      "year" : 2019
    }, {
      "title" : "Slimmable neural networks",
      "author" : [ "Jiahui Yu", "Linjie Yang", "Ning Xu", "Jianchao Yang", "Thomas Huang." ],
      "venue" : "arXiv preprint arXiv:1812.08928.",
      "citeRegEx" : "Yu et al\\.,? 2018",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "In this paper, we extend PoWER-BERT (Goyal et al., 2020) and propose Length-Adaptive Transformer that can be used for various inference scenarios after one-shot training.",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 31,
      "context" : "Pre-trained language models (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2019; Yang et al., 2019; He et al., 2020) have achieved notable improvements in various natural language processing (NLP) tasks.",
      "startOffset" : 28,
      "endOffset" : 128
    }, {
      "referenceID" : 8,
      "context" : "Pre-trained language models (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2019; Yang et al., 2019; He et al., 2020) have achieved notable improvements in various natural language processing (NLP) tasks.",
      "startOffset" : 28,
      "endOffset" : 128
    }, {
      "referenceID" : 33,
      "context" : "Pre-trained language models (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2019; Yang et al., 2019; He et al., 2020) have achieved notable improvements in various natural language processing (NLP) tasks.",
      "startOffset" : 28,
      "endOffset" : 128
    }, {
      "referenceID" : 60,
      "context" : "Pre-trained language models (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2019; Yang et al., 2019; He et al., 2020) have achieved notable improvements in various natural language processing (NLP) tasks.",
      "startOffset" : 28,
      "endOffset" : 128
    }, {
      "referenceID" : 15,
      "context" : "Pre-trained language models (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2019; Yang et al., 2019; He et al., 2020) have achieved notable improvements in various natural language processing (NLP) tasks.",
      "startOffset" : 28,
      "endOffset" : 128
    }, {
      "referenceID" : 53,
      "context" : "Most of them rely on transformers (Vaswani et al., 2017), and the number of model parameters ranges from hundreds of millions to billions (Shoeybi et al.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 42,
      "context" : ", 2017), and the number of model parameters ranges from hundreds of millions to billions (Shoeybi et al., 2019; Raffel et al., 2019; Kaplan et al., 2020; Brown et al., 2020).",
      "startOffset" : 89,
      "endOffset" : 173
    }, {
      "referenceID" : 34,
      "context" : ", 2017), and the number of model parameters ranges from hundreds of millions to billions (Shoeybi et al., 2019; Raffel et al., 2019; Kaplan et al., 2020; Brown et al., 2020).",
      "startOffset" : 89,
      "endOffset" : 173
    }, {
      "referenceID" : 21,
      "context" : ", 2017), and the number of model parameters ranges from hundreds of millions to billions (Shoeybi et al., 2019; Raffel et al., 2019; Kaplan et al., 2020; Brown et al., 2020).",
      "startOffset" : 89,
      "endOffset" : 173
    }, {
      "referenceID" : 2,
      "context" : ", 2017), and the number of model parameters ranges from hundreds of millions to billions (Shoeybi et al., 2019; Raffel et al., 2019; Kaplan et al., 2020; Brown et al., 2020).",
      "startOffset" : 89,
      "endOffset" : 173
    }, {
      "referenceID" : 39,
      "context" : "This level of excessive computation has further raised the concern over energy consumption as well (Schwartz et al., 2019; Strubell et al., 2019; Cao et al., 2020).",
      "startOffset" : 99,
      "endOffset" : 163
    }, {
      "referenceID" : 44,
      "context" : "This level of excessive computation has further raised the concern over energy consumption as well (Schwartz et al., 2019; Strubell et al., 2019; Cao et al., 2020).",
      "startOffset" : 99,
      "endOffset" : 163
    }, {
      "referenceID" : 4,
      "context" : "This level of excessive computation has further raised the concern over energy consumption as well (Schwartz et al., 2019; Strubell et al., 2019; Cao et al., 2020).",
      "startOffset" : 99,
      "endOffset" : 163
    }, {
      "referenceID" : 12,
      "context" : "PoWER-BERT (Goyal et al., 2020) which progressively reduces sequence length by eliminating word-vectors based on the attention values as passing layers.",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 37,
      "context" : "We refer to this procedure as LengthDrop, which was motivated by the nested dropout (Rippel et al., 2014).",
      "startOffset" : 84,
      "endOffset" : 105
    }, {
      "referenceID" : 35,
      "context" : "token-level tasks such as span-based question answering (Rajpurkar et al., 2016) because these tasks require hidden representations of the entire input sequence at the final layer.",
      "startOffset" : 56,
      "endOffset" : 80
    }, {
      "referenceID" : 54,
      "context" : "2016) and two sequence-level classification tasks in GLUE benchmark (Wang et al., 2018).",
      "startOffset" : 68,
      "endOffset" : 87
    }, {
      "referenceID" : 53,
      "context" : "A transformer is a particular neural network that has been designed to work with a variable-length sequence input and is implemented as a stack of self-attention and fully connected layers (Vaswani et al., 2017).",
      "startOffset" : 189,
      "endOffset" : 211
    }, {
      "referenceID" : 12,
      "context" : "PoWER-BERT keeps only the topmost lj word vectors at each layer j by eliminating redundant ones based on the significance score which is the total amount of attention imposed by a word on the other words (Goyal et al., 2020).",
      "startOffset" : 204,
      "endOffset" : 224
    }, {
      "referenceID" : 53,
      "context" : "parameters as BERT, but the extraction layers are interspersed after the self-attention layer in every transformer block (Vaswani et al., 2017).",
      "startOffset" : 121,
      "endOffset" : 143
    }, {
      "referenceID" : 38,
      "context" : "PoWER-BERT reduces inference time successfully, achieving better accuracy-time trade-off than DistilBERT (Sanh et al., 2019), BERT-PKD (Sun",
      "startOffset" : 105,
      "endOffset" : 124
    }, {
      "referenceID" : 38,
      "context" : "Earlier approaches to efficient inference with transformers have focused on a scenario where the target computational budget for inference is known in advance (Sanh et al., 2019; Goyal et al., 2020).",
      "startOffset" : 159,
      "endOffset" : 198
    }, {
      "referenceID" : 12,
      "context" : "Earlier approaches to efficient inference with transformers have focused on a scenario where the target computational budget for inference is known in advance (Sanh et al., 2019; Goyal et al., 2020).",
      "startOffset" : 159,
      "endOffset" : 198
    }, {
      "referenceID" : 43,
      "context" : "ral network trained with different dropout masks (Srivastava et al., 2014).",
      "startOffset" : 49,
      "endOffset" : 74
    }, {
      "referenceID" : 11,
      "context" : "We address this issue by using LayerDrop (Fan et al., 2019) which skips each layer of a transformer uniformly at random.",
      "startOffset" : 41,
      "endOffset" : 59
    }, {
      "referenceID" : 43,
      "context" : "when it is dropped and when it is restored, just like dropout (Srivastava et al., 2014) prevents hidden neurons from co-adapting with each other by randomly dropping them.",
      "startOffset" : 62,
      "endOffset" : 87
    }, {
      "referenceID" : 17,
      "context" : "We simultaneously update ns randomly-sampled sub-models (which are called sandwiches) and the smallest-possible sub-model, which corresponds to keeping only (1− p)li word vectors at each layer i, using knowledge distillation (Hinton et al., 2015) from the full model.",
      "startOffset" : 225,
      "endOffset" : 246
    }, {
      "referenceID" : 54,
      "context" : "We use MNLI-m and SST-2 from GLUE benchmark (Wang et al., 2018), as was done to test PoWER-BERT earlier, for sequence-level classification.",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 12,
      "context" : "Figure 1: Illustration of (a) word-vector elimination process in PoWER-BERT (Goyal et al., 2020) and (b) Dropand-Restore process in Length-Adaptive Transformer.",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 55,
      "context" : "as hardware-aware latency (Wang et al., 2020a) or energy consumption (Henderson et al.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 14,
      "context" : "for other approaches, such as unstructured weight pruning (Han et al., 2015; See et al., 2016).",
      "startOffset" : 58,
      "endOffset" : 94
    }, {
      "referenceID" : 41,
      "context" : "for other approaches, such as unstructured weight pruning (Han et al., 2015; See et al., 2016).",
      "startOffset" : 58,
      "endOffset" : 94
    }, {
      "referenceID" : 8,
      "context" : "We follow the same strategy in this paper and test two pre-trained transformerbased language models; BERTBase (Devlin et al., 2018) and DistilBERT (Sanh et al.",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 38,
      "context" : ", 2018) and DistilBERT (Sanh et al., 2019), which allows us to demonstrate that the usefulness and applicability of our approach are not tied to any specific architectural choice, such as the number of layers and the maximum input sequence length.",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 18,
      "context" : "We provide some comparison results with PoWER-BERT (not anytime prediction method) and DynaBERT (Hou et al., 2020) (concurrent adaptive computation method) as follows.",
      "startOffset" : 96,
      "endOffset" : 114
    }, {
      "referenceID" : 14,
      "context" : "Weight pruning Weight pruning (Han et al., 2015) focuses on reducing the number of parameters that directly reflects the memory footprint of a model and indirectly correlates with inference speed.",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 48,
      "context" : "However, their actual speed-up in runtime is usually not significant, especially while executing a model with parallel computation using GPU devices (Tang et al., 2018; Li et al., 2020).",
      "startOffset" : 149,
      "endOffset" : 185
    }, {
      "referenceID" : 26,
      "context" : "However, their actual speed-up in runtime is usually not significant, especially while executing a model with parallel computation using GPU devices (Tang et al., 2018; Li et al., 2020).",
      "startOffset" : 149,
      "endOffset" : 185
    }, {
      "referenceID" : 7,
      "context" : "Funnel-Transformer (Dai et al., 2020) and multi-scale transformer language models (Subramanian et al.",
      "startOffset" : 19,
      "endOffset" : 37
    }, {
      "referenceID" : 45,
      "context" : ", 2020) and multi-scale transformer language models (Subramanian et al., 2020) also successfully reduce sequence length in the middle and rescale",
      "startOffset" : 52,
      "endOffset" : 78
    }, {
      "referenceID" : 61,
      "context" : "More recently, TR-BERT (Ye et al., 2021) introduces a policy network trained via reinforcement learning to decide which vectors",
      "startOffset" : 23,
      "endOffset" : 40
    }, {
      "referenceID" : 11,
      "context" : "LayerDrop (Fan et al., 2019) drops random layers during the training to be robust to pruning inspired by Huang et al.",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 51,
      "context" : "Escaping a network early (Teerapittayanon et al., 2016; Huang et al., 2017) based on the confidence of the prediction (Xin et al.",
      "startOffset" : 25,
      "endOffset" : 75
    }, {
      "referenceID" : 19,
      "context" : "Escaping a network early (Teerapittayanon et al., 2016; Huang et al., 2017) based on the confidence of the prediction (Xin et al.",
      "startOffset" : 25,
      "endOffset" : 75
    }, {
      "referenceID" : 63,
      "context" : "Slimmable neural networks (Yu et al., 2018; Lee and Shin, 2018) reduce the hidden dimension for the any-time prediction.",
      "startOffset" : 26,
      "endOffset" : 63
    }, {
      "referenceID" : 24,
      "context" : "Slimmable neural networks (Yu et al., 2018; Lee and Shin, 2018) reduce the hidden dimension for the any-time prediction.",
      "startOffset" : 26,
      "endOffset" : 63
    }, {
      "referenceID" : 18,
      "context" : "DynaBERT (Hou et al., 2020) can run at adaptive width (the number of attention heads and intermediate hidden dimension) and depth.",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 55,
      "context" : "Hardware-aware Transformers (Wang et al., 2020a) construct a design space with arbitrary encoder-decoder attention and heterogeneous layers in terms of different numbers of layers, attention heads, hidden dimension, and embedding dimension.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 3,
      "context" : "In computer vision, Once-for-All (Cai et al., 2019) use an evolutionary search (Real et al.",
      "startOffset" : 33,
      "endOffset" : 51
    }, {
      "referenceID" : 36,
      "context" : ", 2019) use an evolutionary search (Real et al., 2019) to find a better configuration in dimensions of depth, width, ker-",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 32,
      "context" : "Sequence Length Shortformer (Press et al., 2020) initially trained on shorter subsequences and then moved to longer ones achieves improved per-",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 57,
      "context" : "Face’s Transformers library (Wolf et al., 2019), and plan to adapt it for a broader set of transformerbased models and downstream tasks, including other modalities (Dosovitskiy et al.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 9,
      "context" : ", 2019), and plan to adapt it for a broader set of transformerbased models and downstream tasks, including other modalities (Dosovitskiy et al., 2020; Touvron et al., 2020; Gulati et al., 2020).",
      "startOffset" : 124,
      "endOffset" : 193
    }, {
      "referenceID" : 52,
      "context" : ", 2019), and plan to adapt it for a broader set of transformerbased models and downstream tasks, including other modalities (Dosovitskiy et al., 2020; Touvron et al., 2020; Gulati et al., 2020).",
      "startOffset" : 124,
      "endOffset" : 193
    }, {
      "referenceID" : 13,
      "context" : ", 2019), and plan to adapt it for a broader set of transformerbased models and downstream tasks, including other modalities (Dosovitskiy et al., 2020; Touvron et al., 2020; Gulati et al., 2020).",
      "startOffset" : 124,
      "endOffset" : 193
    }, {
      "referenceID" : 47,
      "context" : "We use Naver Smart Machine Learning (Sung et al., 2017; Kim et al., 2018) platform for the experiments.",
      "startOffset" : 36,
      "endOffset" : 73
    }, {
      "referenceID" : 22,
      "context" : "We use Naver Smart Machine Learning (Sung et al., 2017; Kim et al., 2018) platform for the experiments.",
      "startOffset" : 36,
      "endOffset" : 73
    } ],
    "year" : 2021,
    "abstractText" : "Despite transformers’ impressive accuracy, their computational cost is often prohibitive to use with limited computational resources. Most previous approaches to improve inference efficiency require a separate model for each possible computational budget. In this paper, we extend PoWER-BERT (Goyal et al., 2020) and propose Length-Adaptive Transformer that can be used for various inference scenarios after one-shot training. We train a transformer with LengthDrop, a structural variant of dropout, which stochastically determines a sequence length at each layer. We then conduct a multi-objective evolutionary search to find a length configuration that maximizes the accuracy and minimizes the efficiency metric under any given computational budget. Additionally, we significantly extend the applicability of PoWER-BERT beyond sequence-level classification into tokenlevel classification with Drop-and-Restore process that drops word-vectors temporarily in intermediate layers and restores at the last layer if necessary. We empirically verify the utility of the proposed approach by demonstrating the superior accuracy-efficiency trade-off under various setups, including span-based question answering and text classification. Code is available at https://github.com/clovaai/lengthadaptive-transformer.",
    "creator" : "LaTeX with hyperref"
  }
}