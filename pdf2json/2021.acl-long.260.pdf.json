{
  "name" : "2021.acl-long.260.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning",
    "authors" : [ "Yujia Qin", "Yankai Lin", "Ryuichi Takanobu", "Zhiyuan Liu", "Peng Li", "Heng Ji", "Minlie Huang", "Maosong Sun", "Jie Zhou" ],
    "emails" : [ "yujiaqin16@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3350–3363\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3350"
    }, {
      "heading" : "1 Introduction",
      "text" : "Pre-trained Language Models (PLMs) (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019) have shown superior performance on various Natural Language Processing (NLP) tasks such as text classification (Wang et al., 2018), named entity recognition (Sang and De Meulder, 2003), and question answering (Talmor and Berant, 2019). Benefiting from designing various effective self-supervised learning objectives, such as masked language modeling (Devlin et al., 2018), PLMs can effectively capture the syntax and semantics in text to generate informative language representations for downstream NLP tasks. ∗Corresponding author. 1Our code and data are publicly available at https:// github.com/thunlp/ERICA.\nHowever, conventional pre-training objectives do not explicitly model relational facts, which frequently distribute in text and are crucial for understanding the whole text. To address this issue, some recent studies attempt to improve PLMs to better understand relations between entities (Soares et al., 2019; Peng et al., 2020). However, they mainly focus on within-sentence relations in isolation, ignoring the understanding of entities, and the interactions among multiple entities at document level, whose relation understanding involves complex reasoning patterns. According to the statistics on a human-annotated corpus sampled from Wikipedia documents by Yao et al. (2019), at least 40.7% relational facts require to be extracted from multiple sentences. Specifically, we show an example in Figure 1, to understand that “Guadalajara is located in Mexico”, we need to consider the following clues jointly: (i) “Mexico” is the country of “Culiacán” from sentence 1; (ii) “Culiacán” is a rail junction lo-\ncated on “Panamerican Highway” from sentence 6; (iii) “Panamerican Highway” connects to “Guadalajara” from sentence 6. From the example, we can see that there are two main challenges to capture the in-text relational facts:\n1. To understand an entity, we should consider its relations to other entities comprehensively. In the example, the entity “Culiacán”, occurring in sentence 1, 2, 3, 5, 6 and 7, plays an important role in finding out the answer. To understand “Culiacán”, we should consider all its connected entities and diverse relations among them.\n2. To understand a relation, we should consider the complex reasoning patterns in text. For example, to understand the complex inference chain in the example, we need to perform multi-hop reasoning, i.e., inferring that “Panamerican Highway” is located in “Mexico” through the first two clues.\nIn this paper, we propose ERICA, a novel framework to improve PLMs’ capability of Entity and RelatIon understanding via ContrAstive learning, aiming to better capture in-text relational facts by considering the interactions among entities and relations comprehensively. Specifically, we define two novel pre-training tasks: (1) the entity discrimination task to distinguish which tail entity can be inferred by the given head entity and relation. It improves the understanding of each entity via considering its relations to other entities in text; (2) the relation discrimination task to distinguish whether two relations are close or not semantically. Through constructing entity pairs with documentlevel distant supervision, it takes complex relational reasoning chains into consideration in an implicit way and thus improves relation understanding.\nWe conduct experiments on a suite of language understanding tasks, including relation extraction, entity typing and question answering. The experimental results show that ERICA improves the performance of typical PLMs (BERT and RoBERTa) and outperforms baselines, especially under lowresource settings, which demonstrates that ERICA effectively improves PLMs’ entity and relation understanding and captures the in-text relational facts."
    }, {
      "heading" : "2 Related Work",
      "text" : "Dai and Le (2015) and Howard and Ruder (2018) propose to pre-train universal language representations on unlabeled text, and perform task-specific fine-tuning. With the advance of computing power, PLMs such as OpenAI GPT (Radford et al., 2018),\nBERT (Devlin et al., 2018) and XLNet (Yang et al., 2019) based on deep Transformer (Vaswani et al., 2017) architecture demonstrate their superiority in various downstream NLP tasks. Since then, numerous PLM extensions have been proposed to further explore the impacts of various model architectures (Song et al., 2019; Raffel et al., 2020), larger model size (Raffel et al., 2020; Lan et al., 2020; Fedus et al., 2021), more pre-training corpora (Liu et al., 2019), etc., to obtain better general language understanding ability. Although achieving great success, these PLMs usually regard words as basic units in textual understanding, ignoring the informative entities and their relations, which are crucial for understanding the whole text.\nTo improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts.\nAnother line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining better span representations, including entity mentions, via span-based pre-training (Sun et al., 2019; Joshi et al., 2020; Kong et al., 2020; Ye et al., 2020). Others learn to extract relation-aware semantics from text by comparing the sentences that share the same entity pair or distantly supervised relation in KGs (Soares et al., 2019; Peng et al., 2020). However, these methods only consider either individual entities or within-sentence relations, which limits the performance in dealing with multiple entities and relations at document level. In contrast, our ERICA considers the interactions among multiple entities\nand relations comprehensively, achieving a better understanding of in-text relational facts."
    }, {
      "heading" : "3 Methodology",
      "text" : "In this section, we introduce the details of ERICA. We first describe the notations and how to represent entities and relations in documents. Then we detail the two novel pre-training tasks: Entity Discrimination (ED) task and Relation Discrimination (RD) task, followed by the overall training objective."
    }, {
      "heading" : "3.1 Notations",
      "text" : "ERICA is trained on a large-scale unlabeled corpus leveraging the distant supervision from an external KG K. Formally, let D = {di}|D|i=1 be a batch of documents and Ei = {eij}|Ei|j=1 be all named entities in di, where eij is the j-th entity in di. For each document di, we enumerate all entity pairs (eij , eik) and link them to their corresponding relation rijk in K (if possible) and obtain a tuple set Ti = {tijk = (di, eij , rijk, eik)|j 6= k}. We assign no_relation to those entity pairs without relation annotation in K. Then we obtain the overall tuple set T = T1 ⋃ T2 ⋃ ... ⋃ T|D| for this batch. The positive tuple set T + is constructed by removing all tuples with no_relation from T . Benefiting from document-level distant supervision, T + includes both intra-sentence (relatively simple cases) and inter-sentence entity pairs (hard cases), whose relation understanding involves cross-sentence, multi-hop, or coreferential reasoning, i.e., T + = T +single ⋃ T +cross."
    }, {
      "heading" : "3.2 Entity & Relation Representation",
      "text" : "For each document di, we first use a PLM to encode it and obtain a series of hidden states\n{h1,h2, ...,h|di|}, then we apply mean pooling operation over the consecutive tokens that mention eij to obtain local entity representations. Note eij may appear multiple times in di, the k-th occurrence of eij , which contains the tokens from index nkstart to nkend, is represented as:\nmkeij = MeanPool(hnkstart , ...,hnkend). (1)\nTo aggregate all information about eij , we average2 all representations of each occurrence mkeij as the global entity representation eij . Following Soares et al. (2019), we concatenate the final representations of two entities eij1 and eij2 as their relation representation, i.e., rij1j2 = [eij1 ; eij2 ]."
    }, {
      "heading" : "3.3 Entity Discrimination Task",
      "text" : "Entity Discrimination (ED) task aims at inferring the tail entity in a document given a head entity and a relation. By distinguishing the ground-truth tail entity from other entities in the text, it teaches PLMs to understand an entity via considering its relations with other entities.\nAs shown in Figure 2, in practice, we first sample a tuple tijk = (di, eij , r i jk, eik) from T +, PLMs are then asked to distinguish the groundtruth tail entity eik from other entities in the document di. To inform PLMs of which head entity and relation to be conditioned on, we concatenate the relation name of rijk, the mention of head entity eij and a separation token [SEP] in front of di, i.e., d∗i =“relation_name entity_mention[SEP] di”3. The goal of entity discrimination task is equivalent to maximizing the posterior P(eik|eij , rijk) = softmax(f(eik)) (f(·) indicates an entity classifier). However, we empirically find directly optimizing the posterior cannot well consider the relations among entities. Hence, we borrow the idea of contrastive learning (Hadsell et al., 2006) and push the representations of positive pair (eij , eik) closer than negative pairs, the loss function of ED task can be formulated as: LED = − ∑\ntijk∈T + log\nexp(cos(eij , eik)/τ)\n|Ei|∑ l=1, l 6=j exp(cos(eij , eil)/τ) ,\n(2) 2Although weighted summation by attention mechanism is an alternative, the specific method of entity information aggregation is not our main concern.\n3Here we encode the modified document d∗i to obtain the entity representations. The newly added entity_mention is not considered for head entity representation.\nwhere cos(·, ·) denotes the cosine similarity between two entity representations and τ (temperature) is a hyper-parameter."
    }, {
      "heading" : "3.4 Relation Discrimination Task",
      "text" : "Relation Discrimination (RD) task aims at distinguishing whether two relations are close or not semantically. Compared with existing relationenhanced PLMs, we employ document-level rather than sentence-level distant supervision to further make PLMs comprehend the complex reasoning chains in real-world scenarios and thus improve PLMs’ relation understanding.\nAs depicted in Figure 3, we train the text-based relation representations of the entity pairs that share the same relations to be closer in the semantic space. In practice, we linearly4 sample a tuple pair tA = (dA, eA1 , rA, eA2) and tB = (dB, eB1 , rB , eB2) from T +s (T +single) or T + c (T +cross), where rA = rB . Using the method mentioned in Sec. 3.2, we obtain the positive relation representations rtA and rtB for tA and tB . To discriminate positive examples from negative ones, similarly, we adopt contrastive learning and define the loss function of RD task as follows:\nLT1,T2RD = − ∑\ntA∈T1,tB∈T2\nlog exp(cos(rtA , rtB )/τ)\nZ ,\nZ = N∑\ntC∈T /{tA}\nexp(cos(rtA , rtC )/τ),\nLRD = LT + s ,T +s RD + L T+s ,T + c RD + L T+c ,T + s RD + L T+c ,T + c\nRD , (3)\n4The sampling rate of each relation is proportional to its total number in the current batch.\nwhere N is a hyper-parameter. We ensure tB is sampled in Z and construct N − 1 negative examples by sampling tC (rA 6= rC) from T , instead of T +5. By additionally considering the last three terms of LRD in Eq.3, which require the model to distinguish complex inter-sentence relations with other relations in the text, our model could have better coverage and generality of the reasoning chains. PLMs are trained to perform reasoning in an implicit way to understand those “hard” inter-sentence cases."
    }, {
      "heading" : "3.5 Overall Objective",
      "text" : "Now we present the overall training objective of ERICA. To avoid catastrophic forgetting (McCloskey and Cohen, 1989) of general language understanding ability, we train masked language modeling task (LMLM) together with ED and RD tasks. Hence, the overall learning objective is formulated as follows:\nL = LED + LRD + LMLM. (4)\nIt is worth mentioning that we also try to mask entities as suggested by Soares et al. (2019) and Peng et al. (2020), aiming to avoid simply relearning an entity linking system. However, we do not observe performance gain by such a masking strategy. We conjecture that in our document-level setting, it is hard for PLMs to overfit on memorizing entity mentions due to the better coverage and generality of document-level distant supervision. Besides, masking entities creates a gap between pre-training and fine-tuning, which may be a shortcoming of previous relation-enhanced PLMs."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we first describe how we construct the distantly supervised dataset and pre-training details for ERICA. Then we introduce the experiments we conduct on several language understanding tasks, including relation extraction (RE), entity typing (ET) and question answering (QA). We test ERICA on two typical PLMs, including BERT and RoBERTa (denoted as ERICABERT and ERICARoBERTa)6. We leave the training details\n5In experiments, we find introducing no_relation entity pairs as negative samples further improves the performance and the reason is that increasing the diversity of training entity pairs is beneficial to PLMs.\n6Since our main focus is to demonstrate the superiority of ERICA in improving PLMs to capture relational facts and advance further research explorations, we choose base models\nfor downstream tasks and experiments on GLUE benchmark (Wang et al., 2018) in the appendix."
    }, {
      "heading" : "4.1 Distantly Supervised Dataset Construction",
      "text" : "Following Yao et al. (2019), we construct our pretraining dataset leveraging distant supervision from the English Wikipedia and Wikidata. First, we use spaCy7 to perform Named Entity Recognition, and then link these entity mentions as well as Wikipedia’s mentions with hyper-links to Wikidata items, thus we obtain the Wikidata ID for each entity. The relations between different entities are annotated distantly by querying Wikidata. We keep the documents containing at least 128 words, 4 entities and 4 relational triples. In addition, we ignore those entity pairs appearing in the test sets of RE and QA tasks to avoid test set leakage. In the end, we collect 1, 000, 000 documents (about 1G storage) in total with more than 4, 000 relations annotated distantly. On average, each document contains 186.9 tokens, 12.9 entities and 7.2 relational triples, an entity appears 1.3 times per document. Based on the human evaluation on a random sample of the dataset, we find that it achieves an F1 score of 84.7% for named entity recognition, and an F1 score of 25.4% for relation extraction."
    }, {
      "heading" : "4.2 Pre-training Details",
      "text" : "We initialize ERICABERT and ERICARoBERTa with bert-base-uncased and roberta-base checkpoints released by Google8 and Huggingface9. We adopt AdamW (Loshchilov and Hutter, 2017) as the optimizer, warm up the learning rate for the first 20% steps and then linearly decay it. We set the learning rate to 3× 10−5, weight decay to 1× 10−5, batch size to 2, 048 and temperature τ to 5× 10−2. For LRD, we randomly select up to 64 negative samples per document. We train both models with 8 NVIDIA Tesla P40 GPUs for 2, 500 steps."
    }, {
      "heading" : "4.3 Relation Extraction",
      "text" : "Relation extraction aims to extract the relation between two recognized entities from a pre-defined relation set. We conduct experiments on both document-level and sentence-level RE. We test\nfor experiments. 7https://spacy.io/ 8https://github.com/google-research/bert 9https://github.com/huggingface/ transformers\nthree partitions of the training set (1%, 10% and 100%) and report results on test sets.\nDocument-level RE For document-level RE, we choose DocRED (Yao et al., 2019), which requires reading multiple sentences in a document and synthesizing all the information to identify the relation between two entities. We encode all entities in the same way as in pre-training phase. The relation representations are obtained by adding a bilinear layer on top of two entity representations. We choose the following baselines: (1) CNN (Zeng et al., 2014), BILSTM (Hochreiter and Schmidhuber, 1997), BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019), which are widely used as text encoders for relation extraction tasks; (2) HINBERT (Tang et al., 2020) which employs a hierarchical inference network to leverage the abundant information from different sources; (3) CorefBERT (Ye et al., 2020) which proposes a pre-training method to help BERT capture the coreferential relations in context; (4) SpanBERT (Joshi et al., 2020) which masks\nand predicts contiguous random spans instead of random tokens; (5) ERNIE (Zhang et al., 2019) which incorporates KG information into BERT to enhance entity representations; (6) MTB (Soares et al., 2019) and CP (Peng et al., 2020) which introduce sentence-level relation contrastive learning for BERT via distant supervision. For fair comparison, we pre-train these baselines on our constructed pre-training data10 based on the implementation released by Peng et al. (2020)11. From the results shown in Table 1, we can see that: (1) ERICA outperforms all baselines significantly on each supervised data size, which demonstrates that ERICA could better understand the relations among entities in the document via implicitly considering their complex reasoning patterns in the pre-training; (2) both MTB and CP achieve worse results than BERT, which means sentence-level pre-training, lacking consideration for complex reasoning patterns, hurts PLM’s performance on document-level RE tasks to some extent; (3) ERICA outperforms baselines by a larger margin on smaller training sets, which means ERICA has gained pretty good document-level relation reasoning ability in contrastive learning, and thus obtains improvements more extensively under low-resource settings.\nSentence-level RE For sentence-level RE, we choose two widely used datasets: TACRED (Zhang et al., 2017) and SemEval-2010 Task 8 (Hendrickx et al., 2019). We insert extra marker tokens to indicate the head and tail entities in each sentence. For baselines, we compare ERICA with BERT, RoBERTa, MTB and CP. From the results shown in Table 2, we observe that ERICA achieves almost comparable results on sentence-level RE tasks with CP, which means document-level pre-training in\n10In practice, documents are split into sentences and we only keep within-sentence entity pairs.\n11https://github.com/thunlp/ RE-Context-or-Names\nERICA does not impair PLMs’ performance on sentence-level relation understanding."
    }, {
      "heading" : "4.4 Entity Typing",
      "text" : "Entity typing aims at classifying entity mentions into pre-defined entity types. We choose FIGER (Ling et al., 2015), which is a sentencelevel entity typing dataset labeled with distant supervision. BERT, RoBERTa, MTB, CP and ERNIE are chosen as baselines. From the results listed in Table 3, we observe that, ERICA outperforms all baselines, which demonstrates that ERICA could better represent entities and distinguish them in text via both entity-level and relation-level contrastive learning."
    }, {
      "heading" : "4.5 Question Answering",
      "text" : "Question answering aims to extract a specific answer span in text given a question. We conduct experiments on both multi-choice and extractive QA. We test multiple partitions of the training set.\nMulti-choice QA For Multi-choice QA, we choose WikiHop (Welbl et al., 2018), which requires models to answer specific properties of an\nentity after reading multiple documents and conducting multi-hop reasoning. It has both standard and masked settings, where the latter setting masks all entities with random IDs to avoid information leakage. We first concatenate the question and documents into a long sequence, then we find all the occurrences of an entity in the documents, encode them into hidden representations and obtain the global entity representation by applying mean pooling on these hidden representations. Finally, we use a classifier on top of the entity representation for prediction. We choose the following baselines: (1) FastQA (Weissenborn et al., 2017) and BiDAF (Seo et al., 2016), which are widely used question answering systems; (2) BERT, RoBERTa, CorefBERT, SpanBERT, MTB and CP, which are introduced in previous sections. From the results listed in Table 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying on entity mention “shortcuts” (Jiang and Bansal, 2019).\nExtractive QA For extractive QA, we adopt three widely-used datasets: SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019) in MRQA (Fisch et al., 2019) to evaluate ERICA in various domains. Since MRQA does not provide the test set for each dataset, we randomly split the original dev set into two halves and obtain the new dev/test set. We follow the QA setting of BERT (Devlin et al., 2018): we concatenate the given question and passage into one long sequence, encode the sequence by PLMs and adopt two classifiers to predict the start and end index of the answer. We choose BERT, RoBERTa, MTB and CP as baselines. From the results listed in Table 5, we observe that ERICA outperforms all baselines, indicating that through the enhancement of entity and relation understanding, ERICA is more capable of capturing in-text relational facts and synthesizing information of entities. This ability further improves PLMs for question answering."
    }, {
      "heading" : "5 Analysis",
      "text" : "In this section, we first conduct a suite of ablation studies to explore how LED and LRD contribute to\nERICA. Then we give a thorough analysis on how pre-training data’s domain / size and methods for entity encoding impact the performance. Lastly, we visualize the entity and relation embeddings learned by ERICA."
    }, {
      "heading" : "5.1 Ablation Study",
      "text" : "To demonstrate that the superior performance of ERICA is not owing to its longer pretraining (2500 steps) on masked language modeling, we include a baseline by optimizing LMLM only (removing the Next Sentence Prediction (-NSP) loss (Devlin et al., 2018)). In addition, to explore how LED and LRD impact the performance, we keep only one of these two losses and compare the results. Lastly, to evaluate how intra-sentence and inter-sentence entity pairs contribute to RD task, we compare the performances of only sampling intra-sentence entity pairs (LT + s ,T + s\nRD ) or inter-sentence entity pairs (LT + c ,T + c\nRD ), and sampling both of them (LRD) during pre-training. We conduct experiments on DocRED, WikiHop (masked version) and FIGER. For DocRED and WikiHop, we show the results on 10% splits and the full results are left in the appendix.\nFrom the results shown in Table 6, we can see that: (1) extra pretraining (-NSP) only contributes a little to the overall improvement. (2) For DocRED and FIGER, either LED or LRD is beneficial, and combining them further improves the performance; For WikiHop, LED dominates the improvement while LRD hurts the performance slightly, this is possibly because question answering more resembles the tail entity discrimination process, while the relation discrimination process may have conflicts with it. (3) For LRD, both intra-sentence and inter-sentence entity pairs contribute, which demonstrates that incorporating both of them is necessary for PLMs to understand relations between entities in text comprehensively. We also found empiri-\ncally that when these two auxiliary objectives are only added into the fine-tuning stage, the model does not have performance gain. The reason is that the size and diversity of entities and relations in downstream training data are limited. Instead, pretraining with distant supervision on a large corpus provides a solution for increasing the diversity and quantity of training examples."
    }, {
      "heading" : "5.2 Effects of Domain Shifting",
      "text" : "We investigate two domain shifting factors: entity distribution and relation distribution, to explore how they impact ERICA’s performance.\nEntity Distribution Shifting The entities in supervised datasets of DocRED are recognized by human annotators while our pre-training data is processed by spaCy. Hence there may exist an entity distribution gap between pre-training and finetuning. To study the impacts of entity distribution shifting, we fine-tune a BERT model on training set of DocRED for NER tagging and re-tag entities in our pre-training dataset. Then we pre-train ERICA on the newly-labeled training corpus (denoted as ERICADocREDBERT ). From the results shown in Table 7, we observe that it performs better than the original ERICA, indicating that pre-training on a dataset that shares similar entity distributions with downstream tasks is beneficial.\nRelation Distribution Shifting Our pre-training data contains over 4, 000 Wikidata relations. To investigate whether training on a more diverse relation domain benefits ERICA, we train it with the pre-training corpus that randomly keeps only 30%, 50% and 70% the original relations, and compare\nBERT ) as is mentioned in the\nmain paper.\ntheir performances. From the results in Figure 4, we observe that the performance of ERICA improves constantly as the diversity of relation domain increases, which reveals the importance of using diverse training data on relation-related tasks. Through detailed analysis, we further find that ERICA is less competent at handling unseen relations in the corpus. This may result from the construction of our pre-training dataset: all the relations are annotated distantly through an existing KG with a pre-defined relation set. It would be promising to introduce more diverse relation domains during data preparation in future."
    }, {
      "heading" : "5.3 Effects of Pre-training Data’s Size",
      "text" : "To explore the effects of pre-training data’s size, we train ERICA on 10%, 30%, 50% and 70% of the original pre-training dataset, respectively. We report the results in Figure 5, from which we observe that with the scale of pre-training data becoming larger, ERICA is performing better."
    }, {
      "heading" : "5.4 Effects of Methods for Entity Encoding",
      "text" : "For all the experiments mentioned above, we encode each occurrence of an entity by mean pooling over all its tokens in both pre-training and downstream tasks. Ideally, ERICA should have consis-\ntent improvements on other kinds of methods for entity encoding. To demonstrate this, we try another entity encoding method mentioned by Soares et al. (2019) on three splits of DocRED (1%, 10% and 100%). Specifically, we insert a special start token [S] in front of an entity and an end token [E] after it. The representation for this entity is calculated by averaging the representations of all its start tokens in the document. To help PLMs discriminate different entities, we randomly assign different marker pairs ([S1], [E1]; [S2], [E2], ...) for each entity in a document in both pre-training and downstream tasks12. All occurrences of one entity in a document share the same marker pair. We show in Table 8 that ERICA achieves consistent performance improvements for both methods (denoted as Mean Pool and Entity Marker), indicating that ERICA is applicable to different methods for entity encoding. Specifically, Entity Marker achieves better performance when the scale of training data is large while Mean Pool is more powerful under low-resource settings. We also notice that training on a dataset that shares similar entity distributions is more helpful for Mean Pool, where ERICADocREDBERT achieves 60.8 (F1) and 58.4 (IgF1) on 100% training data."
    }, {
      "heading" : "5.5 Embedding Visualization",
      "text" : "In Figure 6, we show the learned entity and relation embeddings of BERT and ERICABERT on DocRED’s dev set by t-distributed stochastic neighbor embedding (t-SNE) (Hinton and Roweis, 2002). We label points with different colors to represent its corresponding category of entities or relations13 in Wikidata and only visualize the most frequent 10 relations. From the figure, we can see that jointly training LMLM with LED and LRD leads to a more compact clustering of both entities and relations belonging to the same category. In contrast, only training LMLM exhibits random distribution. This verifies that ERICA could better understand and represent both entities and relations in the text.\n12In practice, we randomly initialize 100 entity marker pairs.\n13(Key, value) pairs for relations defined in Wikidata are: (P176, manufacturer); (P150, contains administrative territorial entity); (P17, country); (P131, located in the administrative territorial entity); (P175, performer); (P27, country of citizenship); (P569, date of birth); (P1001, applies to jurisdiction); (P57, director); (P179, part of the series)."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this paper, we present ERICA, a general framework for PLMs to improve entity and relation understanding via contrastive learning. We demonstrate the effectiveness of our method on several language understanding tasks, including relation extraction, entity typing and question answering. The experimental results show that ERICA outperforms all baselines, especially under low-resource settings, which means ERICA helps PLMs better capture the in-text relational facts and synthesize information about entities and their relations."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is supported by the National Key Research and Development Program of China (No. 2020AAA0106501) and Beijing Academy of Artificial Intelligence (BAAI). This work is also supported by the Pattern Recognition Center, WeChat AI, Tencent Inc."
    }, {
      "heading" : "A Training Details for Downstream Tasks",
      "text" : "In this section, we introduce the training details for downstream tasks (relation extraction, entity typing and question answering). We implement all models based on Huggingface transformers14.\nA.1 Relation Extraction\nDocument-level Relation Extraction For document-level relation extraction, we did experiments on DocRED (Yao et al., 2019). We modify the official code15 for implementation. For experiments on three partitions of the original training set (1%, 10% and 100%), we adopt batch size of 10, 32, 32 and training epochs of 400, 400, 200, respectively. We choose Adam optimizer (Kingma and Ba, 2014) as the optimizer and the learning rate is set to 4 × 10−5. We evaluate on dev set every 20/20/5 epochs and then test the best checkpoint on test set on the official evaluation server16.\nSentence-level Relation Extraction For sentence-level relation extraction, we did experiments on TACRED (Zhang et al., 2017) and SemEval-2010 Task 8 (Hendrickx et al., 2019) based on the implementation of Peng et al. (2020)17. We did experiments on three partitions (1%, 10% and 100%) of the original training set. The relation representation for each entity pair is obtained in the same way as in pre-training phase. Other settings are kept the same as Peng et al. (2020) for fair comparison.\nA.2 Entity Tying\nFor entity typing, we choose FIGER (Ling et al., 2015), whose training set is labeled with distant supervision. We modify the implementation of ERNIE (Zhang et al., 2019)18. In fine-tuning phrase, we encode the entities in the same way as in pre-training phase. We set the learning rate to 3× 10−5 and batch size to 256, and fine-tune the\n14https://github.com/huggingface/ transformers\n15https://github.com/thunlp/DocRED 16https://competitions.codalab.org/\ncompetitions/20717 17https://github.com/thunlp/ RE-Context-or-Names 18https://github.com/thunlp/ERNIE\nmodels for three epochs, other hyper-parameters are kept the same as ERNIE.\nA.3 Question Answering Multi-choice QA For multi-choice question answering, we choose WikiHop (Welbl et al., 2018). Since the standard setting of WikiHop does not provide the index for each candidate, we then find them by exactly matching them in the documents. We did experiments on three partitions of the original training data (1%, 10% and 100%). We set the batch size to 8 and learning rate to 5× 10−5, and train for two epochs.\nExtractive QA For extractive question answering, we adopt MRQA (Fisch et al., 2019) as the testbed and choose three datasets: SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019). We adopt Adam as the optimizer, set the learning rate to 3× 10−5 and train for two epochs. In the main paper, we report results on two splits (10% and 100%) and results on 1% are listed in Table 11."
    }, {
      "heading" : "B Generalized Language Understanding (GLUE)",
      "text" : "The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) provides several natural language understanding tasks, which is often used to evaluate PLMs. To test whether LED and LRD impair the PLMs’ performance on these tasks, we compare BERT, ERICABERT, RoBERTa and ERICARoBERTa. We follow the widely used setting and use the [CLS] token as representation for the whole sentence or sentence pair for classification or regression. Table 9 shows the results on dev sets of GLUE Benchmark. It can be observed that both ERICABERT and ERICARoBERTa achieve comparable performance than the original model, which suggests that jointly training LED and LRD with LMLM does not hurt PLMs’ general ability of language understanding."
    }, {
      "heading" : "C Full results of ablation study",
      "text" : "Full results of ablation study (DocRED, WikiHop and FIGER) are listed in Table 10."
    }, {
      "heading" : "D Joint Named Entity Recognition and Relation Extraction",
      "text" : "Joint Named Entity Recognition (NER) and Relation Extraction (RE) aims at identifying entities in text and the relations between them. We\nadopt SpERT (Eberts and Ulges, 2019) as the base model and conduct experiments on two datasets: CoNLL04 (Roth and Yih, 2004) and ADE (Gurulingappa et al., 2012) by replacing the base encoders (BERT and RoBERTa) with ERICABERT and ERICARoBERTa, respectively. We modify the implementation of SpERT19 and keep all the settings the same. From the results listed in Table 12, we can see that ERICA outperforms all baselines, which again demonstrates the superiority of ERICA in\n19https://github.com/markus-eberts/spert\nhelping PLMs better understand and represent both entities and relations in text."
    } ],
    "references" : [ {
      "title" : "Semi-supervised sequence learning",
      "author" : [ "Andrew M Dai", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems, pages 3079–3087.",
      "citeRegEx" : "Dai and Le.,? 2015",
      "shortCiteRegEx" : "Dai and Le.",
      "year" : 2015
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Universal language model fine-tuning for text classification",
      "author" : [ "Jeremy Howard", "Sebastian Ruder." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 328–339, Melbourne, Australia.",
      "citeRegEx" : "Howard and Ruder.,? 2018",
      "shortCiteRegEx" : "Howard and Ruder.",
      "year" : 2018
    }, {
      "title" : "Avoiding reasoning shortcuts: Adversarial evaluation, training, and model development for multi-hop qa",
      "author" : [ "Yichen Jiang", "Mohit Bansal." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, ACL 2019, July",
      "citeRegEx" : "Jiang and Bansal.,? 2019",
      "shortCiteRegEx" : "Jiang and Bansal.",
      "year" : 2019
    }, {
      "title" : "SpanBERT: Improving pre-training by representing and predicting spans",
      "author" : [ "Mandar Joshi", "Danqi Chen", "Yinhan Liu", "Daniel S. Weld", "Luke Zettlemoyer", "Omer Levy." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:64–77.",
      "citeRegEx" : "Joshi et al\\.,? 2020",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2020
    }, {
      "title" : "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension",
      "author" : [ "Mandar Joshi", "Eunsol Choi", "Daniel Weld", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Joshi et al\\.,? 2017",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2017
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "A mutual information maximization perspective of language representation learning",
      "author" : [ "Lingpeng Kong", "Cyprien de Masson d’Autume", "Lei Yu", "Wang Ling", "Zihang Dai", "Dani Yogatama" ],
      "venue" : "In Proceedings of 8th International Conference on Learning Repre-",
      "citeRegEx" : "Kong et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Kong et al\\.",
      "year" : 2020
    }, {
      "title" : "Natural questions: a benchmark for question answering research",
      "author" : [ "Tom Kwiatkowski", "Jennimaria Palomaki", "Olivia Redfield", "Michael Collins", "Ankur Parikh", "Chris Alberti", "Danielle Epstein", "Illia Polosukhin", "Jacob Devlin", "Kenton Lee" ],
      "venue" : null,
      "citeRegEx" : "Kwiatkowski et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Kwiatkowski et al\\.",
      "year" : 2019
    }, {
      "title" : "ALBERT: A lite BERT for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "Proceedings of 8th International Conference on Learning",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "Design challenges for entity linking",
      "author" : [ "Xiao Ling", "Sameer Singh", "Daniel S Weld." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 3:315–328.",
      "citeRegEx" : "Ling et al\\.,? 2015",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "RoBERTa: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "Proceedings of 7th International Conference on Learning Representations, ICLR 2019.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2017",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2017
    }, {
      "title" : "Catastrophic interference in connectionist networks: the sequential learning problem",
      "author" : [ "Michael McCloskey", "Neal J Cohen." ],
      "venue" : "Psychology of learning and motivation, volume 24, pages 109–165. Elsevier.",
      "citeRegEx" : "McCloskey and Cohen.,? 1989",
      "shortCiteRegEx" : "McCloskey and Cohen.",
      "year" : 1989
    }, {
      "title" : "Learning from context or names? an empirical study on neural relation extraction",
      "author" : [ "Hao Peng", "Tianyu Gao", "Xu Han", "Yankai Lin", "Peng Li", "Zhiyuan Liu", "Maosong Sun", "Jie Zhou." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Peng et al\\.,? 2020",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2020
    }, {
      "title" : "Knowledge enhanced contextual word representations",
      "author" : [ "Matthew E Peters", "Mark Neumann", "Robert L Logan IV", "Roy Schwartz", "Vidur Joshi", "Sameer Singh", "Noah A Smith." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Peters et al\\.,? 2019",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "A linear programming formulation for global inference in natural language tasks",
      "author" : [ "Dan Roth", "Wen-tau Yih." ],
      "venue" : "Proceedings of the Eighth Conference on Computational Natural Language Learning (CoNLL-2004) at HLT-NAACL 2004, pages 1–8,",
      "citeRegEx" : "Roth and Yih.,? 2004",
      "shortCiteRegEx" : "Roth and Yih.",
      "year" : 2004
    }, {
      "title" : "Introduction to the conll-2003 shared task: Languageindependent named entity recognition",
      "author" : [ "Erik F Sang", "Fien De Meulder." ],
      "venue" : "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003.",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Bidirectional attention flow for machine comprehension",
      "author" : [ "Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24, 2017,",
      "citeRegEx" : "Seo et al\\.,? 2016",
      "shortCiteRegEx" : "Seo et al\\.",
      "year" : 2016
    }, {
      "title" : "Matching the blanks: Distributional similarity for relation learning",
      "author" : [ "Livio Baldini Soares", "Nicholas FitzGerald", "Jeffrey Ling", "Tom Kwiatkowski." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Soares et al\\.,? 2019",
      "shortCiteRegEx" : "Soares et al\\.",
      "year" : 2019
    }, {
      "title" : "Mass: Masked sequence to sequence pre-training for language generation",
      "author" : [ "Kaitao Song", "Xu Tan", "Tao Qin", "Jianfeng Lu", "TieYan Liu." ],
      "venue" : "Proceedings of International Conference on Machine Learning, pages 5926–5936. PMLR.",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "CoLAKE: Contextualized language and knowledge embedding",
      "author" : [ "Tianxiang Sun", "Yunfan Shao", "Xipeng Qiu", "Qipeng Guo", "Yaru Hu", "Xuanjing Huang", "Zheng Zhang." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics,",
      "citeRegEx" : "Sun et al\\.,? 2020",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Ernie: Enhanced representation through knowledge integration",
      "author" : [ "Yu Sun", "Shuohuan Wang", "Yukun Li", "Shikun Feng", "Xuyi Chen", "Han Zhang", "Xin Tian", "Danxiang Zhu", "Hao Tian", "Hua Wu." ],
      "venue" : "arXiv preprint arXiv:1904.09223.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "MultiQA: An empirical investigation of generalization and transfer in reading comprehension",
      "author" : [ "Alon Talmor", "Jonathan Berant." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4911–4921. Association",
      "citeRegEx" : "Talmor and Berant.,? 2019",
      "shortCiteRegEx" : "Talmor and Berant.",
      "year" : 2019
    }, {
      "title" : "Hin: Hierarchical inference network for documentlevel relation extraction",
      "author" : [ "Hengzhu Tang", "Yanan Cao", "Zhenyu Zhang", "Jiangxia Cao", "Fang Fang", "Shi Wang", "Pengfei Yin." ],
      "venue" : "Advances in Knowledge Discovery and Data Mining-24th Pacific-Asia Con-",
      "citeRegEx" : "Tang et al\\.,? 2020",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop Black-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "K-adapter: Infusing knowledge into pre-trained models with adapters",
      "author" : [ "Ruize Wang", "Duyu Tang", "Nan Duan", "Zhongyu Wei", "Xuanjing Huang", "Cuihong Cao", "Daxin Jiang", "Ming Zhou" ],
      "venue" : "arXiv preprint arXiv:2002.01808",
      "citeRegEx" : "Wang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "KEPLER: A unified model for knowledge embedding and pretrained language representation",
      "author" : [ "Xiaozhi Wang", "Tianyu Gao", "Zhaocheng Zhu", "Zhiyuan Liu", "Juanzi Li", "Jian Tang." ],
      "venue" : "Transactions of the Association for Computational Linguistics.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Making neural QA as simple as possible but not simpler",
      "author" : [ "Dirk Weissenborn", "Georg Wiese", "Laura Seiffe." ],
      "venue" : "Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 271–280. Association for",
      "citeRegEx" : "Weissenborn et al\\.,? 2017",
      "shortCiteRegEx" : "Weissenborn et al\\.",
      "year" : 2017
    }, {
      "title" : "Constructing datasets for multi-hop reading comprehension across documents",
      "author" : [ "Johannes Welbl", "Pontus Stenetorp", "Sebastian Riedel." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:287–302.",
      "citeRegEx" : "Welbl et al\\.,? 2018",
      "shortCiteRegEx" : "Welbl et al\\.",
      "year" : 2018
    }, {
      "title" : "Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model",
      "author" : [ "Wenhan Xiong", "Jingfei Du", "William Yang Wang", "Veselin Stoyanov." ],
      "venue" : "Proceedings of 8th International Conference on Learning Representations, ICLR 2020, Vir-",
      "citeRegEx" : "Xiong et al\\.,? 2019",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2019
    }, {
      "title" : "LUKE: Deep contextualized entity representations with entityaware self-attention",
      "author" : [ "Ikuya Yamada", "Akari Asai", "Hiroyuki Shindo", "Hideaki Takeda", "Yuji Matsumoto." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Yamada et al\\.,? 2020",
      "shortCiteRegEx" : "Yamada et al\\.",
      "year" : 2020
    }, {
      "title" : "XLNet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in Neural Information Processing Systems 32: Annual Con-",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "DocRED: A large-scale document-level relation extraction dataset",
      "author" : [ "Yuan Yao", "Deming Ye", "Peng Li", "Xu Han", "Yankai Lin", "Zhenghao Liu", "Zhiyuan Liu", "Lixin Huang", "Jie Zhou", "Maosong Sun." ],
      "venue" : "Proceedings of the 57th Conference of the Association",
      "citeRegEx" : "Yao et al\\.,? 2019",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2019
    }, {
      "title" : "Coreferential reasoning learning for language representation",
      "author" : [ "Deming Ye", "Yankai Lin", "Jiaju Du", "Zhenghao Liu", "Maosong Sun", "Zhiyuan Liu" ],
      "venue" : null,
      "citeRegEx" : "Ye et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Ye et al\\.",
      "year" : 2020
    }, {
      "title" : "Relation classification via convolutional deep neural network",
      "author" : [ "Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao." ],
      "venue" : "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,",
      "citeRegEx" : "Zeng et al\\.,? 2014",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2014
    }, {
      "title" : "Positionaware attention and supervised data improve slot filling",
      "author" : [ "Yuhao Zhang", "Victor Zhong", "Danqi Chen", "Gabor Angeli", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Zhang et al\\.,? 2017",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2017
    }, {
      "title" : "ERNIE: Enhanced language representation with informative entities",
      "author" : [ "Zhengyan Zhang", "Xu Han", "Zhiyuan Liu", "Xin Jiang", "Maosong Sun", "Qun Liu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "We did experiments on three partitions (1%, 10% and 100%) of the original training set. The relation representation for each entity pair is obtained in the same way as in pre-training phase",
      "author" : [ "Peng" ],
      "venue" : null,
      "citeRegEx" : "Peng,? \\Q2020\\E",
      "shortCiteRegEx" : "Peng",
      "year" : 2020
    }, {
      "title" : "2020) for fair comparison",
      "author" : [ "Peng" ],
      "venue" : "Entity Tying For entity typing,",
      "citeRegEx" : "Peng,? \\Q2015\\E",
      "shortCiteRegEx" : "Peng",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Pre-trained Language Models (PLMs) (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019) have shown superior performance on various Natural Language Processing (NLP) tasks such as text classification (Wang et al.",
      "startOffset" : 35,
      "endOffset" : 93
    }, {
      "referenceID" : 36,
      "context" : "Pre-trained Language Models (PLMs) (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019) have shown superior performance on various Natural Language Processing (NLP) tasks such as text classification (Wang et al.",
      "startOffset" : 35,
      "endOffset" : 93
    }, {
      "referenceID" : 11,
      "context" : "Pre-trained Language Models (PLMs) (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019) have shown superior performance on various Natural Language Processing (NLP) tasks such as text classification (Wang et al.",
      "startOffset" : 35,
      "endOffset" : 93
    }, {
      "referenceID" : 29,
      "context" : ", 2019) have shown superior performance on various Natural Language Processing (NLP) tasks such as text classification (Wang et al., 2018), named entity recognition (Sang and De Meulder, 2003), and question answering (Talmor and Berant, 2019).",
      "startOffset" : 119,
      "endOffset" : 138
    }, {
      "referenceID" : 26,
      "context" : ", 2018), named entity recognition (Sang and De Meulder, 2003), and question answering (Talmor and Berant, 2019).",
      "startOffset" : 86,
      "endOffset" : 111
    }, {
      "referenceID" : 1,
      "context" : "Benefiting from designing various effective self-supervised learning objectives, such as masked language modeling (Devlin et al., 2018), PLMs can effectively capture the syntax and semantics in text to generate informative language representations for downstream NLP tasks.",
      "startOffset" : 114,
      "endOffset" : 135
    }, {
      "referenceID" : 22,
      "context" : "To address this issue, some recent studies attempt to improve PLMs to better understand relations between entities (Soares et al., 2019; Peng et al., 2020).",
      "startOffset" : 115,
      "endOffset" : 155
    }, {
      "referenceID" : 14,
      "context" : "To address this issue, some recent studies attempt to improve PLMs to better understand relations between entities (Soares et al., 2019; Peng et al., 2020).",
      "startOffset" : 115,
      "endOffset" : 155
    }, {
      "referenceID" : 16,
      "context" : "With the advance of computing power, PLMs such as OpenAI GPT (Radford et al., 2018), BERT (Devlin et al.",
      "startOffset" : 61,
      "endOffset" : 83
    }, {
      "referenceID" : 1,
      "context" : ", 2018), BERT (Devlin et al., 2018) and XLNet (Yang et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 36,
      "context" : ", 2018) and XLNet (Yang et al., 2019) based on deep Transformer (Vaswani et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 28,
      "context" : ", 2019) based on deep Transformer (Vaswani et al., 2017) architecture demonstrate their superiority in various downstream NLP tasks.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 23,
      "context" : "Since then, numerous PLM extensions have been proposed to further explore the impacts of various model architectures (Song et al., 2019; Raffel et al., 2020), larger model size (Raffel et al.",
      "startOffset" : 117,
      "endOffset" : 157
    }, {
      "referenceID" : 17,
      "context" : "Since then, numerous PLM extensions have been proposed to further explore the impacts of various model architectures (Song et al., 2019; Raffel et al., 2020), larger model size (Raffel et al.",
      "startOffset" : 117,
      "endOffset" : 157
    }, {
      "referenceID" : 17,
      "context" : ", 2020), larger model size (Raffel et al., 2020; Lan et al., 2020; Fedus et al., 2021), more pre-training corpora (Liu et al.",
      "startOffset" : 27,
      "endOffset" : 86
    }, {
      "referenceID" : 9,
      "context" : ", 2020), larger model size (Raffel et al., 2020; Lan et al., 2020; Fedus et al., 2021), more pre-training corpora (Liu et al.",
      "startOffset" : 27,
      "endOffset" : 86
    }, {
      "referenceID" : 11,
      "context" : ", 2021), more pre-training corpora (Liu et al., 2019), etc.",
      "startOffset" : 35,
      "endOffset" : 53
    }, {
      "referenceID" : 34,
      "context" : "Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020).",
      "startOffset" : 109,
      "endOffset" : 187
    }, {
      "referenceID" : 31,
      "context" : "Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020).",
      "startOffset" : 109,
      "endOffset" : 187
    }, {
      "referenceID" : 24,
      "context" : "Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020).",
      "startOffset" : 109,
      "endOffset" : 187
    }, {
      "referenceID" : 35,
      "context" : "Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020).",
      "startOffset" : 109,
      "endOffset" : 187
    }, {
      "referenceID" : 41,
      "context" : "Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020).",
      "startOffset" : 88,
      "endOffset" : 165
    }, {
      "referenceID" : 15,
      "context" : "Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020).",
      "startOffset" : 88,
      "endOffset" : 165
    }, {
      "referenceID" : 30,
      "context" : "Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020).",
      "startOffset" : 88,
      "endOffset" : 165
    }, {
      "referenceID" : 25,
      "context" : "Some focus on obtaining better span representations, including entity mentions, via span-based pre-training (Sun et al., 2019; Joshi et al., 2020; Kong et al., 2020; Ye et al., 2020).",
      "startOffset" : 108,
      "endOffset" : 182
    }, {
      "referenceID" : 4,
      "context" : "Some focus on obtaining better span representations, including entity mentions, via span-based pre-training (Sun et al., 2019; Joshi et al., 2020; Kong et al., 2020; Ye et al., 2020).",
      "startOffset" : 108,
      "endOffset" : 182
    }, {
      "referenceID" : 7,
      "context" : "Some focus on obtaining better span representations, including entity mentions, via span-based pre-training (Sun et al., 2019; Joshi et al., 2020; Kong et al., 2020; Ye et al., 2020).",
      "startOffset" : 108,
      "endOffset" : 182
    }, {
      "referenceID" : 38,
      "context" : "Some focus on obtaining better span representations, including entity mentions, via span-based pre-training (Sun et al., 2019; Joshi et al., 2020; Kong et al., 2020; Ye et al., 2020).",
      "startOffset" : 108,
      "endOffset" : 182
    }, {
      "referenceID" : 22,
      "context" : "Others learn to extract relation-aware semantics from text by comparing the sentences that share the same entity pair or distantly supervised relation in KGs (Soares et al., 2019; Peng et al., 2020).",
      "startOffset" : 158,
      "endOffset" : 198
    }, {
      "referenceID" : 14,
      "context" : "Others learn to extract relation-aware semantics from text by comparing the sentences that share the same entity pair or distantly supervised relation in KGs (Soares et al., 2019; Peng et al., 2020).",
      "startOffset" : 158,
      "endOffset" : 198
    }, {
      "referenceID" : 29,
      "context" : "3354 for downstream tasks and experiments on GLUE benchmark (Wang et al., 2018) in the appendix.",
      "startOffset" : 60,
      "endOffset" : 79
    }, {
      "referenceID" : 12,
      "context" : "AdamW (Loshchilov and Hutter, 2017) as the optimizer, warm up the learning rate for the first 20% steps and then linearly decay it.",
      "startOffset" : 6,
      "endOffset" : 35
    }, {
      "referenceID" : 37,
      "context" : "Document-level RE For document-level RE, we choose DocRED (Yao et al., 2019), which requires reading multiple sentences in a document and synthesizing all the information to identify the relation between two entities.",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 39,
      "context" : "We choose the following baselines: (1) CNN (Zeng et al., 2014), BILSTM (Hochreiter and Schmidhuber, 1997), BERT (Devlin et al.",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 1,
      "context" : ", 2014), BILSTM (Hochreiter and Schmidhuber, 1997), BERT (Devlin et al., 2018) and RoBERTa (Liu et al.",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 11,
      "context" : ", 2018) and RoBERTa (Liu et al., 2019), which are widely used as text encoders for relation extraction tasks; (2) HINBERT (Tang et al.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 27,
      "context" : ", 2019), which are widely used as text encoders for relation extraction tasks; (2) HINBERT (Tang et al., 2020) which employs a hierarchical inference network to leverage the abundant information from different sources; (3) CorefBERT (Ye et al.",
      "startOffset" : 91,
      "endOffset" : 110
    }, {
      "referenceID" : 38,
      "context" : ", 2020) which employs a hierarchical inference network to leverage the abundant information from different sources; (3) CorefBERT (Ye et al., 2020) which proposes a pre-training method to help BERT capture the coreferential relations in context; (4) SpanBERT (Joshi et al.",
      "startOffset" : 130,
      "endOffset" : 147
    }, {
      "referenceID" : 4,
      "context" : ", 2020) which proposes a pre-training method to help BERT capture the coreferential relations in context; (4) SpanBERT (Joshi et al., 2020) which masks",
      "startOffset" : 119,
      "endOffset" : 139
    }, {
      "referenceID" : 41,
      "context" : "and predicts contiguous random spans instead of random tokens; (5) ERNIE (Zhang et al., 2019) which incorporates KG information into BERT to enhance entity representations; (6) MTB (Soares et al.",
      "startOffset" : 73,
      "endOffset" : 93
    }, {
      "referenceID" : 22,
      "context" : ", 2019) which incorporates KG information into BERT to enhance entity representations; (6) MTB (Soares et al., 2019) and CP (Peng et al.",
      "startOffset" : 95,
      "endOffset" : 116
    }, {
      "referenceID" : 14,
      "context" : ", 2019) and CP (Peng et al., 2020) which introduce sentence-level relation contrastive learning for BERT via distant supervision.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 40,
      "context" : "Sentence-level RE For sentence-level RE, we choose two widely used datasets: TACRED (Zhang et al., 2017) and SemEval-2010 Task 8 (Hendrickx et al.",
      "startOffset" : 84,
      "endOffset" : 104
    }, {
      "referenceID" : 10,
      "context" : "We choose FIGER (Ling et al., 2015), which is a sentencelevel entity typing dataset labeled with distant supervision.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 33,
      "context" : "Multi-choice QA For Multi-choice QA, we choose WikiHop (Welbl et al., 2018), which requires models to answer specific properties of an",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 32,
      "context" : "We choose the following baselines: (1) FastQA (Weissenborn et al., 2017) and BiDAF (Seo et al.",
      "startOffset" : 46,
      "endOffset" : 72
    }, {
      "referenceID" : 21,
      "context" : ", 2017) and BiDAF (Seo et al., 2016), which are widely used question answering systems; (2) BERT, RoBERTa, CorefBERT, SpanBERT, MTB and CP, which are introduced in previous sections.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 3,
      "context" : "The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying on entity mention “shortcuts” (Jiang and Bansal, 2019).",
      "startOffset" : 217,
      "endOffset" : 241
    }, {
      "referenceID" : 18,
      "context" : "Extractive QA For extractive QA, we adopt three widely-used datasets: SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al.",
      "startOffset" : 76,
      "endOffset" : 100
    }, {
      "referenceID" : 5,
      "context" : ", 2016), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : ", 2017) and NaturalQA (Kwiatkowski et al., 2019) in MRQA (Fisch et al.",
      "startOffset" : 22,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : "We follow the QA setting of BERT (Devlin et al., 2018): we concatenate the given question and passage into one long sequence, encode the sequence by PLMs and adopt two classifiers to predict the start and end index of the answer.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 1,
      "context" : "steps) on masked language modeling, we include a baseline by optimizing LMLM only (removing the Next Sentence Prediction (-NSP) loss (Devlin et al., 2018)).",
      "startOffset" : 133,
      "endOffset" : 154
    } ],
    "year" : 2021,
    "abstractText" : "Pre-trained Language Models (PLMs) have shown superior performance on various downstream Natural Language Processing (NLP) tasks. However, conventional pre-training objectives do not explicitly model relational facts in text, which are crucial for textual understanding. To address this issue, we propose a novel contrastive learning framework ERICA to obtain a deep understanding of the entities and their relations in text. Specifically, we define two novel pre-training tasks to better understand entities and relations: (1) the entity discrimination task to distinguish which tail entity can be inferred by the given head entity and relation; (2) the relation discrimination task to distinguish whether two relations are close or not semantically, which involves complex relational reasoning. Experimental results demonstrate that ERICA can improve typical PLMs (BERT and RoBERTa) on several language understanding tasks, including relation extraction, entity typing and question answering, especially under low-resource settings.1",
    "creator" : "LaTeX with hyperref"
  }
}