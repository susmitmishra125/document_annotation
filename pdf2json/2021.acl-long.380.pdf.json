{
  "name" : "2021.acl-long.380.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Risk Minimization for Zero-shot Sequence Labeling",
    "authors" : [ "Zechuan Hu", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu" ],
    "emails" : [ "huzch@shanghaitech.edu.cn,", "tukw@shanghaitech.edu.cn,", "yongjiang.jy@alibaba-inc.com", "nguyen.bach@alibaba-inc.com", "leeo.wangt@alibaba-inc.com", "z.huang@alibaba-inc.com", "f.huang@alibaba-inc.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4909–4920\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4909"
    }, {
      "heading" : "1 Introduction",
      "text" : "Sequence labeling is an important task in natural language processing. It has many applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases.\n∗Corresponding authors. ‡Work was done when Zechuan Hu was interning at Alibaba DAMO Academy.\nLearning knowledge from imperfect predictions from other rich-resource sources (such as crosslingual, cross-domain transfer) (Yarowsky and Ngai, 2001; Guo et al., 2018; Huang et al., 2019; Hu et al., 2021) is a feasible and efficient way to tackle the low-resource problem. It transfers knowledge from rich-resource languages/domains to low-resource ones. One typical approach to this problem is utilizing existing systems to provide predicted results for the zero-shot datasets. However, due to the gap between the source and the target languages/domains, this approach may fail to recover the true labels. Several previous approaches try to alleviate this problem by relying heavily on cross-lingual information (e.g., parallel text (Wang and Manning, 2014; Ni et al., 2017)), labeled data in source languages (Chen et al., 2019), and prior domain knowledge (Yang and Eisenstein, 2015) for different kinds of zero-shot scenarios. However, these approaches are designed to be specific, and might not be generalizable to other kinds of settings where the required resources are expensive to obtain or not available due to data privacy (Wu et al., 2020). Instead, we want a learning framework that can address the zero-shot learning problem in a unified perspective.\nIn this work, we consider two widely explored settings in which we have access to: 1) the imperfect hard predictions (Rahimi et al., 2019; Lan et al., 2020); 2) the imperfect soft predictions (Wu et al., 2020), produced by one or more source models on target unlabeled data , and propose two novel approaches. We start by introducing a novel approach based on the minimum risk training framework. We design a new decomposable risk function parameterized by a fixed matrix that models the relations between the noisy predictions from the source models and the true labels. We then make the matrix trainable, which leads to further expressiveness and connects minimum risk training to learning latent\nvariable models. We propose a learning algorithm based on the EM algorithm, which alternates between updating a posterior distribution and optimizing model parameters.\nTo empirically evaluate our proposed approaches, we extensively conduct experiments on four sequence labeling tasks of twenty-one datasets. Our two proposed approaches, especially the latent variable model, outperform several strong baselines."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 Sequence Labeling",
      "text" : "Given a sentence x = x1, . . . , xn, its word representations are extracted from the pre-trained embeddings and passed into a sentence encoder such as BiLSTM, Convolutional Neural Networks (CNN) and multilingual BERT (Devlin et al., 2019) to obtain a sequence of contextual features. Without considering the dependencies between predicted labels, the Softmax layer computes the conditional probability as follows,\nPθ(y|x) = n∏ i=1 Pθ(yi|x)\nGiven the gold sequence y∗ = y∗1, . . . , y ∗ n, the general training objective is to minimize the negative log-likelihood of the sequence,\nJ (θ) = − logPθ(y∗|x) = − n∑ i=1 logPθ(y ∗ i |x)\nFor simplicity, throughout this paper, we assume that all the sequence labelers are based on the Softmax method."
    }, {
      "heading" : "2.2 Cross-Lingual/Domain Transfer",
      "text" : "Supervised models fail when labeled data are absent. Learning from imperfect predictions from rich-resource sources is a viable approach to tackle the problem. Generally speaking, there are two settings to obtain the imperfect predictions from: single source and multi source. The simplest singlesource approach is to train a single-source model on one source language/domain and use the source model to directly predict labels on the target test data. We name this approach as direct single-source transfer (DT). Another single-source approach is to use the predictions of the source model on a set of unlabeled target data to supervise the training of a target model. With imperfect hard predictions\nfrom the source model, the corresponding objective function is the cross-entropy loss between the imperfect hard predictions and the target model’s soft predictions,\nJ (θ) = − logPθ(ŷ|x) = − n∑ i=1 logPθ(ŷi|x)\nwhere ŷ denotes the pseudo label sequence of x predicted by the source model and ŷi is the pseudo label for position i. With imperfect soft predictions from the source model, the corresponding objective function is the KL-divergence (KL) or mean square error (MSE) loss between the imperfect soft predictions and the target model’s soft predictions (knowledge distillation, KD) (Wu et al., 2020).\nFor multi-source setup, a simple approach contains the following two steps. The first step is to apply DT with each source language to produce predictions on unlabeled target data. The second step is to mix the predictions from all the source models and perform supervised learning of a target model on the mixed pseudo-labeled dataset. However, the mixed pseudo-labeled dataset can be very noisy because predictions from different source models may contradict each other. Similar to single-source setting, a more effective way is aggregating the soft predictions from multiple sources and doing KD (Wu et al., 2020)."
    }, {
      "heading" : "3 Methodology",
      "text" : ""
    }, {
      "heading" : "3.1 Minimum Risk Training",
      "text" : "In supervised learning, minimum risk training aims to minimize the expected error (risk) concerning the conditional probability,\nJ (θ) = ∑\ny∈Y(x)\nPθ(y|x)R(y∗,y)\nwhere R(y∗,y) is the risk function that measures the distance between the gold sequence y∗ and the candidate sequence y, and Y(x) denotes the collection of all the possible label sequences given the sentence x. The risk function can be defined in many ways depending on specific applications, such as the BLEU score in machine translation (Shen et al., 2016). However, in our setting, there are no gold labels to compute R(y∗,y). Instead, we assume there are multiple pretrained source models which can be used to predict hard labels, and we define the risk function as R(ŷ,y) to measure the difference between pseudo label sequence\nŷ predicted by source models and the candidate sequence y. The objective function becomes,\nJ (θ) = EPθ (y|x)[R(ŷ,y)] = ∑\ny∈Y(x)\nPθ(y|x)R(ŷ,y)\nConventional minimum risk training is intractable which is mainly due to the combination of two reasons: first, the set of candidate label sequences Y(x) is exponential in size and intractable to enumerate; second, the risk function is hard to decompose (or indecomposable). To tackle the problem, we define the risk function as a negative probability−P (ŷ|y) that can be fully decomposed by position. The objective function becomes,\nJ (θ) = ∑\ny∈Y(x)\nPθ(y|x)R(ŷ,y)\n= − ∑\ny∈Y(x)\nPθ(y|x)Pψ(ŷ|y) (1)\n= − n∏ i=1 ∑ yi Pθ(yi|x)Pψ(ŷi|yi)\nWe introduce a matrix ψ ∈ RK×K to model Pψ(ŷi|yi), whereK is the number of labels. Notice that ψ here is a fixed matrix that does not change in training. In the general imperfect predictions learning, it is often implicitly assumed that the prediction from a source model is generally better than uniformly selecting a candidate label at random. Given this prior knowledge, we require Pψ(ŷi = k|yi = k)> 1K . Therefore, we empirically define matrix ψ as,\nψ ij = { µ if i = j , 1−µ K−1 if i 6= j\nwhere µ> 1K is a hyper-parameter. In the implementation, for convenience, we multiply an identity matrix by a hyper-parameter τ and then apply Softmax operation to every column to obtain the matrix ψ .\nTo further explain ψ , we give an example from the perspective of prediction in Table 1. Given a sentence x = “I cried”, a label distribution Pθ(y|x) for the sentence, a pseudo label sequence ŷ = {Pron, Adj} predicted by the source model, and two settings µ1=0.4 and µ2=1 forψ(1) andψ(2) respectively, we compute Pθ(yi|x)× Pψ(ŷi|yi) as shown in the table.\nSince ψ(2) is an identity matrix, it predicts the label with the largest value at each position. It assigns the wrong label Adj to the word “cried” as a consequence. On the contrary, ψ(1) introduces some uncertainties by providing smoothing over the pseudo labels. As a result, it correctly predicts the word “cried” as Verb. From the perspective of training, which minimizes J (θ), if ψ is an identity matrix, then it is a supervised model with ŷ as the supervision signal; on the other hand, if ψ is a uniform matrix, then the supervision signal becomes random and training becomes meaningless.\nExtending to Leverage Soft Predictions Previous works shows that the soft predictions from source models can provide more information than the hard predictions (Hinton et al., 2015; Wu et al., 2020). Our novel approach can also easily leverage this information by simply replacing the one-hot pseudo labels with soft probability distributions from source models. The training objective becomes,\nJ (θ)= − n∏ i=1 ∑ yi Pθ(yi|x) ∑ ŷi Ps(ŷi|x)Pψ(ŷi|yi)\nwhere Ps is the source model’s soft predictions.\nFor simplicity, in the rest of this section, we introduce our approaches based on the setup of using one-hot pseudo labels, but all the approaches can be extended to leverage soft predictions in a similar way."
    }, {
      "heading" : "3.2 Minimum Risk Training: A Latent Variable Model Perspective",
      "text" : "In this subsection, we instead use a trainable matrix σ to model Pσ(ŷ|y). We initialize σ in the same way as ψ . Assuming that conditioning on y, x and ŷ are independent with each other, we find that the non-negative term of equation (1) is a conditional marginal probability defined by a latent variable model in which y is the latent variable.∑\ny∈Y(x)\nPθ(y|x)Pσ(ŷ|y) = Pθ,σ(ŷ|x)\nIn latent variable model training, we generally optimize the negative conditional log-likelihood, and the objective function becomes,\nJ (θ,σ) = − logPθ,σ(ŷ|x)\n= − n∑ i=1 log ∑ yi Pθ(yi|x)Pσ(ŷi|yi)\nInterpolation In practice, given a pre-defined hyper-parameter µ, we combine the fixed Pψ(ŷi|yi) with the trainable Pσ(ŷi|yi) to get a new probability,\nPφ(ŷi|yi) = λPψ(ŷi|yi) + (1− λ)Pσ(ŷi|yi)\nwhere λ ∈ [0, 1] is a hyper-parameter, φ is the combined matrix. If λ = 1, it denotes the minimum risk training. Otherwise, it denotes the latent variable model."
    }, {
      "heading" : "3.3 From Single-source to Multi-source Setup",
      "text" : "By modeling the joint distribution over the pseudo labels which are predicted by U source models on the target unlabeled data, we can easily extend our latent variable model to the multi-source setting. The objective function becomes, J (θ,φ)=− n∑ i=1 log ∑ yi Pθ(yi|x) U∏ u=1 Pφ(ŷ (u) i |yi, u)\nOur overall architecture of the latent variable model is depicted in Figure 1."
    }, {
      "heading" : "3.4 Optimization",
      "text" : "In this section, we propose a unified optimization scheme, which is based on the EM algorithm (Dempster et al., 1977) 1, to learn the parameters\n1Another approach is to perform direct gradient descent optimization, which we find weaker results. We have a discussion on that in the analysis section.\ny xŷu θϕU\nof the two proposed approaches. The EM algorithm is widely applied to learn parameters in a large family of models with latent variables such as the Gaussian mixture models. It is an iterative approach that has two steps in every iteration, which are the E-step and the M-step. In the E-step, it optimizes a posterior distribution of the latent variables. In the M-step, it estimates the parameters of the latent variable model according to the posterior distribution. As the single-source setup can be seen as a special case, we focus on the multi-source setup to derive the equations. We first introduce Q(y) =\n∏ i Q(yi) as a distribution over the latent\nvariable y, and then we derive the upper bound of J (θ,φ) as follows, J (θ,φ) =− n∑ i=1 log ∑ yi Pθ(yi|x) U∏ u=1 Pφ(ŷ (u) i |yi, u)\n=− n∑ i=1 log ∑ yi Q(yi)\nPθ(yi|x) U∏ u=1 Pφ(ŷ (u) i |yi, u)\nQ(yi)\n≤− n∑ i=1 ∑ yi Q(yi)log\nPθ(yi|x) U∏ u=1 Pφ(ŷ (u) i |yi, u)\nQ(yi)\n(2)\n=− n∑ i=1 EQ(yi)logPθ(yi|x) U∏ u=1 Pφ(ŷ (u) i |yi, u)+C\nwhere C is a residual term, and Q(yi) stands for Q(yi = yi). The inequation above is derived from Jensen’s inequality. To make the bound tight for particular θ and φ, we derive Q(yi) as,\nQ(yi) ∝ Pθ(yi|x) U∏ u=1 Pφ(ŷ (u) i |yi, u) (3)\nWe sketch our strategy of parameter update in the t-th iteration as follows,\n• E step, we compute Q(yi) using parameters θ and φ from the (t− 1)-th iteration;\n• M step, we update parameters θ and φ together using a gradient-based approach by minimizing the upper bound above. Q(yi) is fixed in this step and hence we minimize\n− n∑ i=1 EQ(yi) logPθ(yi|x) U∏ u=1 Pφ(ŷ (u) i |yi, u)\nwe repeat the two steps alternately until convergence. We give an overall process for multi-source setup with unlabeled target data in Algorithm 1.\nAlgorithm 1 Multi-source transfer with latent variable model 1: Input: unlabeled dataset of target T , U pretrained source\nmodels {M = M (1), . . . ,M (U)}, U trainable matrices {Σ = σ(1), . . . ,σ(U)} and U fixed matrices {Ψ = ψ(1), . . . ,ψ(U)}, hyper-parameter µ and λ, maximal iterations E for the EM algorithm. 2: Initialize: initialize Σ and Ψ with the same hyperparameter µ. Initialize {Φ = φ(1), . . . ,φ(U)} using λ, Σ and Ψ. Initialize an empty pseudo label list Ŷ , an upper bound loss lm = +∞, and an overall loss le = +∞.\n3: for u = 1, . . . , U do 4: Use M (u) to obtain the hard/soft label sequence of the\nunlabeled data T and append the predictions to the list of pseudo label sequences. Ŷ . 5: end for 6: Concatenate the unlabeled data T with all pseudo label\ncollections Ŷ to form a new training dataset T̂ .\n7: for e = 1, . . . , E do 8: Compute posterior distribution Q(yi) according to\nformula 3 for each sample x. . E step 9: Compute the loss le = J (φ,θ).\n10: if le has no improvement do 11: End training. 12: end if\n13: repeat . M step 14: Compute lm according to Eq. 2. 15: Update φ and θ. 16: Until lm has no improvement. 17: end for"
    }, {
      "heading" : "3.5 Inference",
      "text" : "For inference, we use Q(y) to obtain ypred 2,\nypred = argmax y∈Y(x) Pθ(y|x) U∏ u=1 Pφ(ŷ (u)|y, u)"
    }, {
      "heading" : "4 Experiments",
      "text" : "We use the multilingual BERT (mBERT) as our word representations3 as the sentence encoder. Fol-\n2Another choice is to use Pθ(y|x), however, we found that utilizing Q(y) generally achieves better performance.\n3Following previous work (Wu and Dredze, 2019; Wu et al., 2020), we fine-tune mBERT’s parameters.\nlowing Wu et al. (2020), the source model are previously trained on its corresponding training data. We use the BIO scheme for CoNLL and OntoNotes NER tasks and Aspect Extraction. We run each model three times and report the average accuracy for the POS tagging task and F1-score for the other tasks."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "Cross-Lingual Sequence Labeling We choose three tasks to conduct the cross-lingual sequence labeling task, which are POS tagging, NER, and Aspect Extraction. For the POS tagging task, we use Universal Dependencies treebanks (UD) v2.44 and randomly select five anguages together with the English dataset. The whole datasets are English (En), Catalan (Ca), Indonesian (Id), Hindi (Hi), Finnish (Fi), and Russian (Ru). For the Aspect Extraction task, we select the restaurant domain over subtask 1 in the SemEval-2016 shared task (Pontiki et al., 2016). For the NER task, we evaluate our models on the CoNLL 2002 and 2003 shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003).\nCross-Domain Sequence Labeling We use English portion of the OntoNotes (v5) (Hovy et al., 2006), which contains six domains: broadcast conversation (bc), broadcast news (bn), magazine (mz), newswire (nw), and web (wb).\nMore details can be found in the Appendix A.1."
    }, {
      "heading" : "4.2 Approaches",
      "text" : "Single-source Setup The following approaches are applicable for single-source setup,\n• DT: we use the pre-trained source model to directly predict the pseudo labels on the target unlabeled data.\n• Hard: we use the pseudo labels from DT on the target unlabeled data to train a new model.\nMulti-source Setup The following approaches are applicable for multi-source setup,\n• Hard-Cat: we apply DT with all the source languages/domains, mix the resulting pseudo labels from all the sources on the unlabeled target data, and train a new model.\n• Hard-Vote: we do majority voting at the token level on the pseudo labels from DT with each source and train a new model.\n4https://universaldependencies.org/\nONTONOTES bc bn mz nw tc wb Avg.\nThe following approaches have access to hard predictions: Hard-Vote 75.90 84.62 81.93 82.41 68.44 77.65 78.49 Hard-Cat 75.27 84.66 81.88 82.60 71.33 77.12 78.81\nMRT 77.03 84.48 84.02 82.90 68.93 77.29 79.11 LVM 75.93 84.76 83.37 83.26 70.56 78.34 79.37\nBoth Setups The following approaches are applicable for both single-/multi-source setups,\n• KD-re: to fairly compare with the the KD approach (Wu et al., 2020) in the same settings (such as source model’s cross-lingual ability), we re-implement the KD approach and adapt it to all tasks.\n• MRT: our minimum risk training approach with a fixed matrix ψ with soft or hard predictions.\n• LVM: our latent variable model with parameter φ (containing the fixed matrix ψ and the trainable matrix σ) with soft or hard predictions.\nWe also provide the reported results from existing approaches for reference. Due to different experiment configuration reasons, directly comparing our approaches to their reported results is generally not fair. For the CoNLL NER tasks, we provide the reported results from Wu et al. (2020). For the cross-domain sequence labeling tasks, we provide the reported results from Lan et al. (2020) who learns a consensus network to aggregate predictions from multiple sources."
    }, {
      "heading" : "4.3 Hyper-parameters",
      "text" : "Hyper-parameter selection in transfer learning is difficult as no labeled dataset is available for the target language. We select the hyper-parameters only on the development set over the English language and directly use the selected hyper-parameters for the other languages. This may result in sub-optimal\nperformance but is more realistic. In latent variable model training, the latent variable is generally very flexible, which may result in sub-optimal performance. Therefore, the initialization of the latent variable is very crucial. In practice, we find that the best strategy is to initialize µ of ψ with a large value (e.g., 0.9) and µ of σ with a small value (e.g., 0.3), and anneal λ from 1 to 0. At the early stage of training, this initialization offers a strong prior for the encoder which can keep the encoder from going in a bad direction; and at later stages of training, the warmed-up encoder can better guide the training of φ and vice versa. In this way, the encoder and φ can achieve a good balance during training. More details of the hyper-parameters can be found in the Appendix A.2."
    }, {
      "heading" : "4.4 Results and Observations",
      "text" : "For the single-source setting, we use English as the source language and the others as the unlabeled target languages. In the multi-source setting, we repeat our experiments multiple times, each time with a language as the target and the others as the sources. We evaluate all approaches on the CoNLL, Aspect Extraction, OntoNotes, and POS tagging. We report the results in Table 2, 3 and 4 5.\nObservation #1 Our two approaches outperform several strong baselines on all the tasks and all the scenarios (single-/multi-source scenarios with soft/hard predictions), especially the multi-source scenario, which demonstrates the effectiveness of the two proposed approaches. It shows that modeling this kind of relation is fairly important, which\n5We utilize almost stochastic dominance (ASD) test (Dror et al., 2019) to compare the best score of our approaches and the score of the best performing baselines. We mark the the highest score as bold if its superiority is significant (p < 0.05) and underline otherwise.\nhelps to recover the true labels from noisy data. Meanwhile, introducing uncertainties for the relations between the predicted labels from the source models and the true labels in both training and prediction processes significantly benefit our approaches.\nObservation #2 Our LVM approach achieves overall improvements over the MRT approach on all tasks. It suggests that our LVM approach learns the relations between predicted labels from the source models and true labels better than MRT.\nOther Minor Observations First, all the approaches that use unlabeled target data for training outperform DT. It suggests that leveraging the unlabeled target data (which may contain knowledge of the target language/domain) in training for zero-shot transfer learning does help. Comparing the approaches that leverage soft instead of hard predictions from sources, the former generally outperform the latter. It suggests that soft predictions can still provide useful knowledge for samples with incorrect hard predictions. The reported results from Lan et al. (2020) are significantly worse. We speculate the reason is that they leverage poor embeddings and different encoders (BiLSTM-CRF). KD-re outperforms our approaches on Ca and Id of POS tagging task on the single-source setting, but its advantage is not statistically significant."
    }, {
      "heading" : "5 Analysis",
      "text" : "We conduct the analysis on the multi-source setting with soft predictions from sources for its better performance.\nBig Data Performance We experiment with our two models and the KD-re baseline on big target training data on the POS tagging task. We ran-\ndomly select 100000 sentences (without labels) for the Wikipedia-003 section of the Ca language on the CoNLL 2017 shared task (Ginter et al., 2017). We randomly select 1000, 10000, and 100000 sentences to train these three approaches, evaluate on the UD test set for each of the three languages respectively, and show the results in Figure 2. It shows that our latent variable model outperforms the other two approaches over all the settings. Though KD outperform MRT with less than 10000 sentences, but MRT has comparable result with enough unlabeled data. Besides, with more unlabeled data used for training, each model further gains a considerable boost.\nComparison to Direct Gradient Optimization Our two proposed approaches can also be optimized directly by any gradient-based approach, such as the AdamW optimizer (Loshchilov and Hutter, 2018). We use the two proposed approaches to compare the performance of the direct gradientbased training strategy and the EM algorithm. We conduct the experiments on our two proposed approaches on CoNLL NER task on the multi-source setting. We show the results in Table 5. It shows that the EM algorithm outperforms direct gradientbased training for our approaches, which is slightly different from previous findings (Berg-Kirkpatrick et al., 2010).\nComparison to Hard EM In this part, we compare our optimization strategy (soft-EM) with the hard-EM approach. Instead of computing a dense vector for Q(yi), hard-EM computes a one-hot vector. We conduct the experiments on our two proposed approaches on the CoNLL NER task on the multi-source setting. The results are shown in Table 6. It shows that soft-EM gains slightly improvement over hard-EM on the MRT approach, but differs significantly from hard-EM on our LVM approach.\nImpact of Matrix ψ We analyze the relation between the performance and different initialization ofψ . We experiment with the MRT approach in the single-source setup with soft predictions on NER tasks and Figure 3 shows the results. The best value of τ is 2 for De and 3 for the others (resulting in µ = 0.43 and 0.67 respectively6), which shows that the uncertainties introduced by a smoothψ can effectively boost the model’s performance. On the other hand, setting ψ to a nearly identity matrix with τ = 10 leads to worse scores."
    }, {
      "heading" : "6 Related Work",
      "text" : "Cross-lingual/domain Sequence Labeling Recent works on cross-lingual transfer mainly have two scenarios: the single-source cross-lingual transfer (Yarowsky and Ngai, 2001; Wang and Manning, 2014; Huang et al., 2019) and the multi-source cross-lingual transfer (Täckström et al., 2012; Guo et al., 2018; Rahimi et al., 2019; Hu et al., 2021). Wu et al. (2020) propose a knowledge distillation approach to further leveraging unlabeled target data and achieve the state-of-the-art results. Hu et al. (2021) propose a multi-view framework to selectively transfer knowledge from multiple sources by utilizing a small amount of labeled dataset. Crossdomain adaption is widely studied (Steedman et al.,\n6The CoNLL NER datasets have 11 labels (9 entity labels, a padding label and an ending label).\n2003). Existing works include bootstrapping approaches (Ruder and Plank, 2018), mixture-ofexperts (Guo et al., 2018; Wright and Augenstein, 2020), and consensus network (Lan et al., 2020). Other previous work (Kim et al., 2017; Guo et al., 2018; Huang et al., 2019) utilized labeled data in the source domain to learn desired information. However, our proposed approaches do not require any source labeled data or parallel texts.\nContextual Multilingual Embeddings Embeddings like mBERT (Devlin et al., 2019), XLM (CONNEAU and Lample, 2019) and XLM-R (Conneau et al., 2020) which are trained on many languages, make great progress on cross-lingual learning for multiple NLP tasks. Recent works (Wu and Dredze, 2019; Pires et al., 2019) show the strong cross-lingual ability of the contextual multilingual embeddings."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we propose two approaches to the zero-shot sequence labeling problem. Our MRT approach uses a fixed matrix to model the relations between the predicted labels from the source models and the true labels. Our LVM approach uses trainable matrices to model these label relations. We extensively verify the effectiveness of our approaches on both single-source and multisource transfer over both cross-lingual and crossdomain sequence labeling problems. Experiments show that MRT and LVM generally bring significant improvements over previous state-of-the-art approaches on twenty-one datasets."
    }, {
      "heading" : "Acknowledgement",
      "text" : "This work was supported by the National Natural Science Foundation of China (61976139) and\nby Alibaba Group through Alibaba Innovative Research Program."
    }, {
      "heading" : "A Experimental details",
      "text" : "A.1 Datasets CoNLL CoNLL is a dataset for the NER task. We evaluate our models on the CoNLL 2002 and 2003 shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003), which contain four languages: English, German, Dutch, and Spanish. Every dataset contains four types of named\nentities: Organization, Location, Person, and Miscellaneous.\nAspect Extraction We select the restaurant domain over subtask 1 in the SemEval-2016 shared task (Pontiki et al., 2016).\nOntoNotes We use English portion of the OntoNotes (v5) (Hovy et al., 2006), which contains six domains: broadcast conversation (bc), broadcast news (bn), magazine (mz), newswire (nw), and web (wb). It is a NER task which contains 18 entity types.\nA.2 Hyper-parameter setting We select the hyper-parameters according to the strategy which is described in the main paper. For multi-source cross-lingual/domain tasks, we select hyper-parameters based on the performance on the English development set and apply them to other target languages. For single-source crosslingual/domain tasks, we simply use the same hyper-parameter as multi-source setting. In the inference step, We use Pθ(y|x) in single-source cross-lingual/domain and Q(y) in multi-source cross-lingual/domain to predict the label sequence. We empirically set the learning rate of mBERT as 2e-5 and the learning rate of φ and φ as 2e-4 for multi-source setup and 2e-5 for single-source setup. We train each model for three epochs. We tune the following hyper-parameters.\nτ and τ̂ for initializing matrices τ and τ̂ are used to initialize the matrices ψ and σ in our minimum risk training and latent variable model approaches respectively. Due to different sizes of the label sets for different tasks, the range of selection is different. Take the CoNLL NER tasks for example, we tune it in the range of {1, 2, 3, 4, 10} for τ̂ inψ in MRT and LVM, and {1, 2, 3, 4, 10} for τ in σ in LVM. The CoNLL NER tasks have 11 labels (9 entity labels, a padding label and an ending label), which means µ ∈ {0.21, 0.43, 0.67, 0.85, 1.0}. We list the value we select for each task below:\n• CoNLL NER: τ = 3 and τ̂ = 2 for singlesource setup; τ = 2 and τ̂ = 10 for multisource setup.\n• AE: τ = 3 and τ̂ = 4 for single-source setup;; τ = 2 and τ̂ = 10 for multi-source setup.\n• POS: τ = 4 and τ̂ = 2 for single-source setup; τ = 2 and τ̂ = 10 for multi-source setup.\n• OntoNotes: τ = 4 and τ̂ = 10."
    } ],
    "references" : [ {
      "title" : "Painless unsupervised learning with features",
      "author" : [ "Taylor Berg-Kirkpatrick", "Alexandre Bouchard-Côté", "John DeNero", "Dan Klein." ],
      "venue" : "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association",
      "citeRegEx" : "Berg.Kirkpatrick et al\\.,? 2010",
      "shortCiteRegEx" : "Berg.Kirkpatrick et al\\.",
      "year" : 2010
    }, {
      "title" : "Multisource cross-lingual model transfer: Learning what to share",
      "author" : [ "Xilun Chen", "Ahmed Hassan Awadallah", "Hany Hassan", "Wei Wang", "Claire Cardie." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Cross-lingual language model pretraining",
      "author" : [ "Alexis CONNEAU", "Guillaume Lample." ],
      "venue" : "Advances in Neural Information Processing Systems 32, pages 7059–7069. Curran Associates, Inc.",
      "citeRegEx" : "CONNEAU and Lample.,? 2019",
      "shortCiteRegEx" : "CONNEAU and Lample.",
      "year" : 2019
    }, {
      "title" : "Maximum likelihood from incomplete data via the em algorithm",
      "author" : [ "A.P. Dempster", "N.M. Laird", "D.B. Rubin." ],
      "venue" : "Journal of the Royal Statistical Society. Series B, volume 39, pages 1–38.",
      "citeRegEx" : "Dempster et al\\.,? 1977",
      "shortCiteRegEx" : "Dempster et al\\.",
      "year" : 1977
    }, {
      "title" : "Grammatical category disambiguation by statistical optimization",
      "author" : [ "Steven J. DeRose." ],
      "venue" : "Computational Linguistics, 14(1):31–39.",
      "citeRegEx" : "DeRose.,? 1988",
      "shortCiteRegEx" : "DeRose.",
      "year" : 1988
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep dominance - how to properly compare deep neural models",
      "author" : [ "Rotem Dror", "Segev Shlomov", "Roi Reichart." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2773–2785, Florence, Italy. Associa-",
      "citeRegEx" : "Dror et al\\.,? 2019",
      "shortCiteRegEx" : "Dror et al\\.",
      "year" : 2019
    }, {
      "title" : "CoNLL 2017 shared task - automatically annotated raw texts and word embeddings",
      "author" : [ "Filip Ginter", "Jan Hajič", "Juhani Luotolahti", "Milan Straka", "Daniel Zeman." ],
      "venue" : "LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (ÚFAL),",
      "citeRegEx" : "Ginter et al\\.,? 2017",
      "shortCiteRegEx" : "Ginter et al\\.",
      "year" : 2017
    }, {
      "title" : "Multi-source domain adaptation with mixture of experts",
      "author" : [ "Jiang Guo", "Darsh Shah", "Regina Barzilay." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4694–4703, Brussels, Belgium. Association",
      "citeRegEx" : "Guo et al\\.,? 2018",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2018
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeffrey Dean." ],
      "venue" : "NIPS Deep Learning and Representation Learning Workshop.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "OntoNotes: The 90% solution",
      "author" : [ "Eduard Hovy", "Mitchell Marcus", "Martha Palmer", "Lance Ramshaw", "Ralph Weischedel." ],
      "venue" : "Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 57–60,",
      "citeRegEx" : "Hovy et al\\.,? 2006",
      "shortCiteRegEx" : "Hovy et al\\.",
      "year" : 2006
    }, {
      "title" : "An investigation of potential function designs for neural CRF",
      "author" : [ "Zechuan Hu", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2600–",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-View Cross-Lingual Structured Prediction with Minimum Supervision",
      "author" : [ "Zechuan Hu", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu." ],
      "venue" : "the Joint Conference of the 59th Annual Meeting of the Associa-",
      "citeRegEx" : "Hu et al\\.,? 2021",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2021
    }, {
      "title" : "Crosslingual multi-level adversarial transfer to enhance low-resource name tagging",
      "author" : [ "Lifu Huang", "Heng Ji", "Jonathan May." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Huang et al\\.,? 2019",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "Cross-lingual transfer learning for POS tagging without cross-lingual resources",
      "author" : [ "Joo-Kyung Kim", "Young-Bum Kim", "Ruhi Sarikaya", "Eric Fosler-Lussier." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Kim et al\\.,? 2017",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Lample et al\\.,? 2016",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to contextually aggregate multi-source supervision for sequence labeling",
      "author" : [ "Ouyu Lan", "Xiao Huang", "Bill Yuchen Lin", "He Jiang", "Liyuan Liu", "Xiang Ren." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "Fixing weight decay regularization in adam",
      "author" : [ "Ilya Loshchilov", "Frank Hutter" ],
      "venue" : null,
      "citeRegEx" : "Loshchilov and Hutter.,? \\Q2018\\E",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2018
    }, {
      "title" : "End-to-end sequence labeling via bi-directional LSTM-CNNsCRF",
      "author" : [ "Xuezhe Ma", "Eduard Hovy." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1064–1074, Berlin, Ger-",
      "citeRegEx" : "Ma and Hovy.,? 2016",
      "shortCiteRegEx" : "Ma and Hovy.",
      "year" : 2016
    }, {
      "title" : "Weakly supervised cross-lingual named entity recognition via effective annotation and representation projection",
      "author" : [ "Jian Ni", "Georgiana Dinu", "Radu Florian." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Ni et al\\.,? 2017",
      "shortCiteRegEx" : "Ni et al\\.",
      "year" : 2017
    }, {
      "title" : "How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996– 5001, Florence, Italy",
      "author" : [ "Telmo Pires", "Eva Schlinger", "Dan Garrette." ],
      "venue" : "Association for Computa-",
      "citeRegEx" : "Pires et al\\.,? 2019",
      "shortCiteRegEx" : "Pires et al\\.",
      "year" : 2019
    }, {
      "title" : "SemEval-2016 task 5: Aspect based sentiment analysis",
      "author" : [ "talia Loukachevitch", "Evgeniy Kotelnikov", "Nuria Bel", "Salud Marı́a Jiménez-Zafra", "Gülşen Eryiğit" ],
      "venue" : "In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-",
      "citeRegEx" : "Loukachevitch et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Loukachevitch et al\\.",
      "year" : 2016
    }, {
      "title" : "Massively multilingual transfer for NER",
      "author" : [ "Afshin Rahimi", "Yuan Li", "Trevor Cohn." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 151–164, Florence, Italy. Association for Computational Linguis-",
      "citeRegEx" : "Rahimi et al\\.,? 2019",
      "shortCiteRegEx" : "Rahimi et al\\.",
      "year" : 2019
    }, {
      "title" : "Design challenges and misconceptions in named entity recognition",
      "author" : [ "Lev Ratinov", "Dan Roth." ],
      "venue" : "Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009), pages 147–155, Boulder, Colorado.",
      "citeRegEx" : "Ratinov and Roth.,? 2009",
      "shortCiteRegEx" : "Ratinov and Roth.",
      "year" : 2009
    }, {
      "title" : "Named entity recognition in tweets: An experimental study",
      "author" : [ "Alan Ritter", "Sam Clark", "Mausam", "Oren Etzioni." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1524–1534, Edinburgh, Scotland,",
      "citeRegEx" : "Ritter et al\\.,? 2011",
      "shortCiteRegEx" : "Ritter et al\\.",
      "year" : 2011
    }, {
      "title" : "Strong baselines for neural semi-supervised learning under domain shift",
      "author" : [ "Sebastian Ruder", "Barbara Plank." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1044–1054, Mel-",
      "citeRegEx" : "Ruder and Plank.,? 2018",
      "shortCiteRegEx" : "Ruder and Plank.",
      "year" : 2018
    }, {
      "title" : "Minimum risk training for neural machine translation",
      "author" : [ "Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
      "citeRegEx" : "Shen et al\\.,? 2016",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2016
    }, {
      "title" : "Example selection for bootstrapping statistical parsers",
      "author" : [ "Mark Steedman", "Rebecca Hwa", "Stephen Clark", "Miles Osborne", "Anoop Sarkar", "Julia Hockenmaier", "Paul Ruhlen", "Steven Baker", "Jeremiah Crim." ],
      "venue" : "Proceedings of the 2003 Human Lan-",
      "citeRegEx" : "Steedman et al\\.,? 2003",
      "shortCiteRegEx" : "Steedman et al\\.",
      "year" : 2003
    }, {
      "title" : "Cross-lingual word clusters for direct transfer of linguistic structure",
      "author" : [ "Oscar Täckström", "Ryan McDonald", "Jakob Uszkoreit." ],
      "venue" : "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguis-",
      "citeRegEx" : "Täckström et al\\.,? 2012",
      "shortCiteRegEx" : "Täckström et al\\.",
      "year" : 2012
    }, {
      "title" : "Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang." ],
      "venue" : "COLING-02: The 6th Conference on Natural Language Learning 2002 (CoNLL-2002).",
      "citeRegEx" : "Sang.,? 2002",
      "shortCiteRegEx" : "Sang.",
      "year" : 2002
    }, {
      "title" : "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "In",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Feature-rich part-ofspeech tagging with a cyclic dependency network",
      "author" : [ "Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer." ],
      "venue" : "Proceedings of the 2003 Human Language Technology Conference of the North American Chapter",
      "citeRegEx" : "Toutanova et al\\.,? 2003",
      "shortCiteRegEx" : "Toutanova et al\\.",
      "year" : 2003
    }, {
      "title" : "Cross-lingual projected expectation regularization for weakly supervised learning",
      "author" : [ "Mengqiu Wang", "Christopher D. Manning." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 2:55–66.",
      "citeRegEx" : "Wang and Manning.,? 2014",
      "shortCiteRegEx" : "Wang and Manning.",
      "year" : 2014
    }, {
      "title" : "Transformer based multi-source domain adaptation",
      "author" : [ "Dustin Wright", "Isabelle Augenstein." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7963–7974, Online. Association for Computa-",
      "citeRegEx" : "Wright and Augenstein.,? 2020",
      "shortCiteRegEx" : "Wright and Augenstein.",
      "year" : 2020
    }, {
      "title" : "Single-/multi-source cross-lingual NER via teacher-student learning on unlabeled data in target language",
      "author" : [ "Qianhui Wu", "Zijia Lin", "Börje Karlsson", "Jian-Guang Lou", "Biqing Huang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT",
      "author" : [ "Shijie Wu", "Mark Dredze." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Wu and Dredze.,? 2019",
      "shortCiteRegEx" : "Wu and Dredze.",
      "year" : 2019
    }, {
      "title" : "Unsupervised multi-domain adaptation with feature embeddings",
      "author" : [ "Yi Yang", "Jacob Eisenstein." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Yang and Eisenstein.,? 2015",
      "shortCiteRegEx" : "Yang and Eisenstein.",
      "year" : 2015
    }, {
      "title" : "Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora",
      "author" : [ "David Yarowsky", "Grace Ngai." ],
      "venue" : "Second Meeting of the North American Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Yarowsky and Ngai.,? 2001",
      "shortCiteRegEx" : "Yarowsky and Ngai.",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "It has many applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al.",
      "startOffset" : 62,
      "endOffset" : 100
    }, {
      "referenceID" : 32,
      "context" : "It has many applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al.",
      "startOffset" : 62,
      "endOffset" : 100
    }, {
      "referenceID" : 24,
      "context" : ", 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020).",
      "startOffset" : 43,
      "endOffset" : 145
    }, {
      "referenceID" : 25,
      "context" : ", 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020).",
      "startOffset" : 43,
      "endOffset" : 145
    }, {
      "referenceID" : 16,
      "context" : ", 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020).",
      "startOffset" : 43,
      "endOffset" : 145
    }, {
      "referenceID" : 19,
      "context" : ", 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020).",
      "startOffset" : 43,
      "endOffset" : 145
    }, {
      "referenceID" : 12,
      "context" : ", 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020).",
      "startOffset" : 43,
      "endOffset" : 145
    }, {
      "referenceID" : 38,
      "context" : "Learning knowledge from imperfect predictions from other rich-resource sources (such as crosslingual, cross-domain transfer) (Yarowsky and Ngai, 2001; Guo et al., 2018; Huang et al., 2019; Hu et al., 2021) is a feasible and efficient way to tackle the low-resource problem.",
      "startOffset" : 125,
      "endOffset" : 205
    }, {
      "referenceID" : 9,
      "context" : "Learning knowledge from imperfect predictions from other rich-resource sources (such as crosslingual, cross-domain transfer) (Yarowsky and Ngai, 2001; Guo et al., 2018; Huang et al., 2019; Hu et al., 2021) is a feasible and efficient way to tackle the low-resource problem.",
      "startOffset" : 125,
      "endOffset" : 205
    }, {
      "referenceID" : 14,
      "context" : "Learning knowledge from imperfect predictions from other rich-resource sources (such as crosslingual, cross-domain transfer) (Yarowsky and Ngai, 2001; Guo et al., 2018; Huang et al., 2019; Hu et al., 2021) is a feasible and efficient way to tackle the low-resource problem.",
      "startOffset" : 125,
      "endOffset" : 205
    }, {
      "referenceID" : 13,
      "context" : "Learning knowledge from imperfect predictions from other rich-resource sources (such as crosslingual, cross-domain transfer) (Yarowsky and Ngai, 2001; Guo et al., 2018; Huang et al., 2019; Hu et al., 2021) is a feasible and efficient way to tackle the low-resource problem.",
      "startOffset" : 125,
      "endOffset" : 205
    }, {
      "referenceID" : 33,
      "context" : ", parallel text (Wang and Manning, 2014; Ni et al., 2017)), labeled data in source languages (Chen et al.",
      "startOffset" : 16,
      "endOffset" : 57
    }, {
      "referenceID" : 20,
      "context" : ", parallel text (Wang and Manning, 2014; Ni et al., 2017)), labeled data in source languages (Chen et al.",
      "startOffset" : 16,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : ", 2017)), labeled data in source languages (Chen et al., 2019), and prior domain knowledge (Yang and Eisenstein, 2015) for different kinds of zero-shot scenarios.",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 37,
      "context" : ", 2019), and prior domain knowledge (Yang and Eisenstein, 2015) for different kinds of zero-shot scenarios.",
      "startOffset" : 36,
      "endOffset" : 63
    }, {
      "referenceID" : 35,
      "context" : "However, these approaches are designed to be specific, and might not be generalizable to other kinds of settings where the required resources are expensive to obtain or not available due to data privacy (Wu et al., 2020).",
      "startOffset" : 203,
      "endOffset" : 220
    }, {
      "referenceID" : 23,
      "context" : "In this work, we consider two widely explored settings in which we have access to: 1) the imperfect hard predictions (Rahimi et al., 2019; Lan et al., 2020); 2) the imperfect soft predictions (Wu et al.",
      "startOffset" : 117,
      "endOffset" : 156
    }, {
      "referenceID" : 17,
      "context" : "In this work, we consider two widely explored settings in which we have access to: 1) the imperfect hard predictions (Rahimi et al., 2019; Lan et al., 2020); 2) the imperfect soft predictions (Wu et al.",
      "startOffset" : 117,
      "endOffset" : 156
    }, {
      "referenceID" : 35,
      "context" : ", 2020); 2) the imperfect soft predictions (Wu et al., 2020), produced by one or more source models on target unlabeled data , and propose two novel approaches.",
      "startOffset" : 43,
      "endOffset" : 60
    }, {
      "referenceID" : 6,
      "context" : ", xn, its word representations are extracted from the pre-trained embeddings and passed into a sentence encoder such as BiLSTM, Convolutional Neural Networks (CNN) and multilingual BERT (Devlin et al., 2019) to obtain a sequence of contextual features.",
      "startOffset" : 186,
      "endOffset" : 207
    }, {
      "referenceID" : 35,
      "context" : "With imperfect soft predictions from the source model, the corresponding objective function is the KL-divergence (KL) or mean square error (MSE) loss between the imperfect soft predictions and the target model’s soft predictions (knowledge distillation, KD) (Wu et al., 2020).",
      "startOffset" : 258,
      "endOffset" : 275
    }, {
      "referenceID" : 35,
      "context" : "Similar to single-source setting, a more effective way is aggregating the soft predictions from multiple sources and doing KD (Wu et al., 2020).",
      "startOffset" : 126,
      "endOffset" : 143
    }, {
      "referenceID" : 27,
      "context" : "The risk function can be defined in many ways depending on specific applications, such as the BLEU score in machine translation (Shen et al., 2016).",
      "startOffset" : 128,
      "endOffset" : 147
    }, {
      "referenceID" : 10,
      "context" : "Extending to Leverage Soft Predictions Previous works shows that the soft predictions from source models can provide more information than the hard predictions (Hinton et al., 2015; Wu et al., 2020).",
      "startOffset" : 160,
      "endOffset" : 198
    }, {
      "referenceID" : 35,
      "context" : "Extending to Leverage Soft Predictions Previous works shows that the soft predictions from source models can provide more information than the hard predictions (Hinton et al., 2015; Wu et al., 2020).",
      "startOffset" : 160,
      "endOffset" : 198
    }, {
      "referenceID" : 4,
      "context" : "In this section, we propose a unified optimization scheme, which is based on the EM algorithm (Dempster et al., 1977) 1, to learn the parameters",
      "startOffset" : 94,
      "endOffset" : 117
    }, {
      "referenceID" : 36,
      "context" : "(3)Following previous work (Wu and Dredze, 2019; Wu et al., 2020), we fine-tune mBERT’s parameters.",
      "startOffset" : 27,
      "endOffset" : 65
    }, {
      "referenceID" : 35,
      "context" : "(3)Following previous work (Wu and Dredze, 2019; Wu et al., 2020), we fine-tune mBERT’s parameters.",
      "startOffset" : 27,
      "endOffset" : 65
    }, {
      "referenceID" : 11,
      "context" : "Cross-Domain Sequence Labeling We use English portion of the OntoNotes (v5) (Hovy et al., 2006), which contains six domains: broadcast conversation (bc), broadcast news (bn), magazine (mz), newswire (nw), and web (wb).",
      "startOffset" : 76,
      "endOffset" : 95
    }, {
      "referenceID" : 35,
      "context" : "KD-re is our re-implementation for the KD approach (Wu et al., 2020).",
      "startOffset" : 51,
      "endOffset" : 68
    }, {
      "referenceID" : 35,
      "context" : "KD-re is our re-implementation for the KD approach (Wu et al., 2020).",
      "startOffset" : 51,
      "endOffset" : 68
    }, {
      "referenceID" : 35,
      "context" : "• KD-re: to fairly compare with the the KD approach (Wu et al., 2020) in the same settings (such as source model’s cross-lingual ability), we re-implement the KD approach and adapt it to all tasks.",
      "startOffset" : 52,
      "endOffset" : 69
    }, {
      "referenceID" : 35,
      "context" : "KD-re is our re-implementation for the KD approach (Wu et al., 2020).",
      "startOffset" : 51,
      "endOffset" : 68
    }, {
      "referenceID" : 7,
      "context" : "We utilize almost stochastic dominance (ASD) test (Dror et al., 2019) to compare the best score of our approaches and the score of the best performing baselines.",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 8,
      "context" : "domly select 100000 sentences (without labels) for the Wikipedia-003 section of the Ca language on the CoNLL 2017 shared task (Ginter et al., 2017).",
      "startOffset" : 126,
      "endOffset" : 147
    }, {
      "referenceID" : 18,
      "context" : "Comparison to Direct Gradient Optimization Our two proposed approaches can also be optimized directly by any gradient-based approach, such as the AdamW optimizer (Loshchilov and Hutter, 2018).",
      "startOffset" : 162,
      "endOffset" : 191
    }, {
      "referenceID" : 0,
      "context" : "It shows that the EM algorithm outperforms direct gradientbased training for our approaches, which is slightly different from previous findings (Berg-Kirkpatrick et al., 2010).",
      "startOffset" : 144,
      "endOffset" : 175
    }, {
      "referenceID" : 38,
      "context" : "Cross-lingual/domain Sequence Labeling Recent works on cross-lingual transfer mainly have two scenarios: the single-source cross-lingual transfer (Yarowsky and Ngai, 2001; Wang and Manning, 2014; Huang et al., 2019) and the multi-source cross-lingual transfer (Täckström et al.",
      "startOffset" : 146,
      "endOffset" : 215
    }, {
      "referenceID" : 33,
      "context" : "Cross-lingual/domain Sequence Labeling Recent works on cross-lingual transfer mainly have two scenarios: the single-source cross-lingual transfer (Yarowsky and Ngai, 2001; Wang and Manning, 2014; Huang et al., 2019) and the multi-source cross-lingual transfer (Täckström et al.",
      "startOffset" : 146,
      "endOffset" : 215
    }, {
      "referenceID" : 14,
      "context" : "Cross-lingual/domain Sequence Labeling Recent works on cross-lingual transfer mainly have two scenarios: the single-source cross-lingual transfer (Yarowsky and Ngai, 2001; Wang and Manning, 2014; Huang et al., 2019) and the multi-source cross-lingual transfer (Täckström et al.",
      "startOffset" : 146,
      "endOffset" : 215
    }, {
      "referenceID" : 29,
      "context" : ", 2019) and the multi-source cross-lingual transfer (Täckström et al., 2012; Guo et al., 2018; Rahimi et al., 2019; Hu et al., 2021).",
      "startOffset" : 52,
      "endOffset" : 132
    }, {
      "referenceID" : 9,
      "context" : ", 2019) and the multi-source cross-lingual transfer (Täckström et al., 2012; Guo et al., 2018; Rahimi et al., 2019; Hu et al., 2021).",
      "startOffset" : 52,
      "endOffset" : 132
    }, {
      "referenceID" : 23,
      "context" : ", 2019) and the multi-source cross-lingual transfer (Täckström et al., 2012; Guo et al., 2018; Rahimi et al., 2019; Hu et al., 2021).",
      "startOffset" : 52,
      "endOffset" : 132
    }, {
      "referenceID" : 13,
      "context" : ", 2019) and the multi-source cross-lingual transfer (Täckström et al., 2012; Guo et al., 2018; Rahimi et al., 2019; Hu et al., 2021).",
      "startOffset" : 52,
      "endOffset" : 132
    }, {
      "referenceID" : 26,
      "context" : "Existing works include bootstrapping approaches (Ruder and Plank, 2018), mixture-ofexperts (Guo et al.",
      "startOffset" : 48,
      "endOffset" : 71
    }, {
      "referenceID" : 9,
      "context" : "Existing works include bootstrapping approaches (Ruder and Plank, 2018), mixture-ofexperts (Guo et al., 2018; Wright and Augenstein, 2020), and consensus network (Lan et al.",
      "startOffset" : 91,
      "endOffset" : 138
    }, {
      "referenceID" : 34,
      "context" : "Existing works include bootstrapping approaches (Ruder and Plank, 2018), mixture-ofexperts (Guo et al., 2018; Wright and Augenstein, 2020), and consensus network (Lan et al.",
      "startOffset" : 91,
      "endOffset" : 138
    }, {
      "referenceID" : 17,
      "context" : ", 2018; Wright and Augenstein, 2020), and consensus network (Lan et al., 2020).",
      "startOffset" : 60,
      "endOffset" : 78
    }, {
      "referenceID" : 15,
      "context" : "Other previous work (Kim et al., 2017; Guo et al., 2018; Huang et al., 2019) utilized labeled data in the source domain to learn desired information.",
      "startOffset" : 20,
      "endOffset" : 76
    }, {
      "referenceID" : 9,
      "context" : "Other previous work (Kim et al., 2017; Guo et al., 2018; Huang et al., 2019) utilized labeled data in the source domain to learn desired information.",
      "startOffset" : 20,
      "endOffset" : 76
    }, {
      "referenceID" : 14,
      "context" : "Other previous work (Kim et al., 2017; Guo et al., 2018; Huang et al., 2019) utilized labeled data in the source domain to learn desired information.",
      "startOffset" : 20,
      "endOffset" : 76
    }, {
      "referenceID" : 6,
      "context" : "Contextual Multilingual Embeddings Embeddings like mBERT (Devlin et al., 2019), XLM (CONNEAU and Lample, 2019) and XLM-R (Conneau et al.",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 3,
      "context" : ", 2019), XLM (CONNEAU and Lample, 2019) and XLM-R (Conneau et al.",
      "startOffset" : 13,
      "endOffset" : 39
    }, {
      "referenceID" : 2,
      "context" : ", 2019), XLM (CONNEAU and Lample, 2019) and XLM-R (Conneau et al., 2020) which are trained on many languages, make great progress on cross-lingual learning for multiple NLP tasks.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 36,
      "context" : "Recent works (Wu and Dredze, 2019; Pires et al., 2019) show the strong cross-lingual ability of the contextual multilingual embeddings.",
      "startOffset" : 13,
      "endOffset" : 54
    }, {
      "referenceID" : 21,
      "context" : "Recent works (Wu and Dredze, 2019; Pires et al., 2019) show the strong cross-lingual ability of the contextual multilingual embeddings.",
      "startOffset" : 13,
      "endOffset" : 54
    } ],
    "year" : 2021,
    "abstractText" : "Zero-shot sequence labeling aims to build a sequence labeler without human-annotated datasets. One straightforward approach is utilizing existing systems (source models) to generate pseudo-labeled datasets and train a target sequence labeler accordingly. However, due to the gap between the source and the target languages/domains, this approach may fail to recover the true labels. In this paper, we propose a novel unified framework for zero-shot sequence labeling with minimum risk training and design a new decomposable risk function that models the relations between the predicted labels from the source models and the true labels. By making the risk function trainable, we draw a connection between minimum risk training and latent variable model learning. We propose a unified learning algorithm based on the expectation maximization (EM) algorithm. We extensively evaluate our proposed approaches on cross-lingual/domain sequence labeling tasks over twenty-one datasets. The results show that our approaches outperform state-of-the-art baseline systems.",
    "creator" : "LaTeX with hyperref"
  }
}