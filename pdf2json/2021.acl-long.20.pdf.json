{
  "name" : "2021.acl-long.20.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Refining Sample Embeddings with Relation Prototypes to Enhance Continual Relation Extraction",
    "authors" : [ "Li Cui", "Deqing Yang", "Jiaxin Yu", "Chengwei Hu", "Jiayang Cheng", "Jingjie Yi", "Yanghua Xiao" ],
    "emails" : [ "fd2014cl@gmail.com", "shawyh}@fudan.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 232–243\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n232"
    }, {
      "heading" : "1 Introduction",
      "text" : "As one of the most important tasks in information extraction (IE), relation extraction (RE) has been\n∗Corresponding author\nwidely applied in many downstream tasks, such as knowledge base construction and completion (Riedel et al., 2013). The goal of RE is to recognize a relation predefined in knowledge graphs (KGs) for an entity pair in texts. For example, given the entity pair [Christopher Nolan, Interstellar] in the sentence “Interstellar is an epic science fiction film directed by Christopher Nolan”, the relation thedirector-of should be recognized by an RE model.\nConventional RE models (Zeng et al., 2014; Zhou et al., 2016; Zhang et al., 2018a) always assume a fixed pre-defined set of relations and perform once-and-for-all training on a fixed dataset. Therefore, these models can not well handle the learning of new relations, which often emerge in many realistic applications given the continuous and iterative nature of our world (Hadsell et al., 2020). To adapt to such a situation, the paradigm of continual relation extraction (CRE) is proposed (Wang et al., 2019; Han et al., 2020; Wu et al., 2021). Compared with conventional RE, CRE focuses more on helping a model keep a stable understanding of old relations while learning emerging relations, which in fact could be precisely modeled by continual learning.\nContinual learning (or lifelong learning) systems are defined as adaptive algorithms capable of learning from a continuous stream of information (Parisi et al., 2019), where the information is progressively available over time and the number of learning tasks is not pre-defined. Continual learning remains a long-standing challenge for machine learning and deep learning (Hassabis et al., 2017; Thrun and Mitchell, 1995), as its main obstacle is the tendency of models to forget existing knowledge when learning from new observations (French, 1999), which is called as catastrophic forgetting. Recent works try to address the problem of catastrophic forgetting in three ways, including consolidation-based methods (Kirkpatrick et al., 2017), dynamic archi-\ntecture (Chen et al., 2015; Fernando et al., 2017) and memory-based methods (Lopez-Paz and Ranzato, 2017; Aljundi et al., 2018; Chaudhry et al., 2018), in which memory-based methods have been proven promising in NLP tasks.\nIn recent years, some memory-based CRE models have made significant progress in overcoming catastrophic forgetting while learning new relations, such as EA-EMR (Wang et al., 2019), MLLRE (Obamuyide and Vlachos, 2019), CML (Wu et al., 2021) and EMAR (Han et al., 2020). Despite of their effectiveness, there are some challenges remaining in current CRE. One noticeable challenge is how to restore the sample embedding space disrupted by the learning of new tasks, given that RE models’ performance is very sensitive to the quality of sample embeddings. Another challenge is that most existing CRE models have not fully exploited memorized samples. In order to enhance RE performance and overcome the overfitting problem caused by high replay frequency, the samples memorized in these models usually have the same magnitude as the original training samples (Wu et al., 2021), which is unrealistic in real-world tasks.\nInspired by prototypical networks (Snell et al., 2017) for few-shot classification, we employ relation prototypes to represent different relations in this paper, which help the model understand different relations well. Furthermore, these prototypes are used to refine sample embeddings in CRE. This process is named as prototypical refining in this paper. Specifically, the prototype for a specific relation is the average embedding of typical samples labeled with this relation, which are collected by Kmeans and memorized by our model for future use. The prototypical refining can help our model recover from the disruption of embedding space and avoid catastrophic forgetting during learning new relations, thus enhance our model’s CRE performance. Another advantage of prototypical refining is the efficient utilization of memorized samples, resulting in our model’s less dependence on memory size.\nOur contributions in this paper are summarized as follows:\n(1) We propose a novel CRE model which achieves enhanced performance through refining sample embeddings with relation prototypes and is effective in avoiding catastrophic forgetting.\n(2) The paradigm we proposed for refining sam-\nple embeddings takes full advantage of the typical samples stored in memory, and reduces the model’s dependence on memory size (number of memorized samples).\n(3) Our extensive experiments upon two RE benchmark datasets justify our model’s remarkable superiority over the state-of-the-art CRE models and less dependence on memory size."
    }, {
      "heading" : "2 Related Works",
      "text" : "Conventional studies in relation extraction (RE) mainly focus on designing and utilizing various deep neural networks to discover the relations between entities given contexts, including: (1) Convolutional neural networks (CNNs) (Zeng et al., 2014, 2015; Nguyen and Grishman, 2015; Lin et al., 2016; Ji et al., 2017) can effectively extract local textual features. (2) Recurrent neural networks (RNNs) (Zhang and Wang, 2015; Xu et al., 2015; Zhou et al., 2016; Zhang et al., 2018a) are particularly capable of learning long-distance relation patterns. (3) Graph neural networks (GNNs) (Zhang et al., 2018b; Fu et al., 2019; Zhu et al., 2019) build word/entity graphs for cross-sentence reasoning. Recently, pre-trained language models (Devlin et al., 2019) have also been extensively used in RE tasks (Wu and He, 2019; Wei et al., 2020; Baldini Soares et al., 2019), and have achieved state-of-the-arts performance.\nHowever, most of these models can only extract a fixed set of pre-defined relations. Hence, continual relation learning, i.e., CRE, has been proposed to overcome this problem. Existing continual learning methods can be divided into three categories: (1) Regularization methods (Kirkpatrick et al., 2017; Zenke et al., 2017; Liu et al., 2018) alleviate catastrophic forgetting by imposing constraints on updating the neural weights important to previous tasks. (2) Dynamic architecture methods (Chen et al., 2015; Fernando et al., 2017) change architectural properties in response to new information by dynamically accommodating novel neural resources. (3) Memory-based methods (Lopez-Paz and Ranzato, 2017; Aljundi et al., 2018; Chaudhry et al., 2018) remember a few examples in previous tasks and continually replay the memory with emerging new tasks. For CRE, the memory-based methods have been proven most promising (Wang et al., 2019; Han et al., 2020). In addition, in order to accurately represent relations with limited samples, the idea of prototypical networks is intro-\nduced into RE(Gao et al., 2019; Ding et al., 2021). There are also many memory networks proposed to remember information of long periods, such as LSTM (Hochreiter and Schmidhuber, 1997) and memory-augmented neural networks (Graves et al., 2016; Santoro et al., 2016). Besides, a new memory module (Santoro et al., 2018) has demonstrated its success in relational reasoning, which employs multi-head attention to allow memory interaction."
    }, {
      "heading" : "3 Methodology",
      "text" : "In this section, we introduce our CRE model in details. At first, we formalize the problem of CRE and the memory module used in our model."
    }, {
      "heading" : "3.1 Task Formalization",
      "text" : "In general, a single relation extraction (RE) task is to identify (classify) the relation between two entities expressed in a sentence. Formally, the objective of CRE is to accomplish a sequence of K RE tasks {T1, T2, . . . , TK}, where the kth task Tk has its own training set Dk and relation set Rk. Suppose Dk contains N training samples {(x1, t1, y1), . . . , (xN , tN , yN )}where instance (xi, ti, yi), 1 ≤ i ≤ N indicates that the relation of entity pair ti in sentence xi is yi ∈ Rk. In fact, each task Tk is an independent multiclassification task to identify various relations in Rk. A CRE model should perform well on extracting the relations in all K tasks after being trained with the samples of these tasks. In other words, the model should be capable of identifying the relation of a given entity pair into R̃k, where R̃k = ∪ki=1Ri is the relation set already observed till the k-th task.\nInspired by current CRE models (Wu and He, 2019; Han et al., 2020), we adopt an episodic memory module to store typical samples of relations that the model has learned in former tasks. The memory module for relation r is represented as a memorized sample set Mr = {(x1, t1, r), . . . , (xO, tO, r)}, where each sample is labeled with r and O is the memory size (sample number). Therefore, the episodic memory for the observed relations in T1 ∼ Tk is M̃k = ∪r∈R̃kMr."
    }, {
      "heading" : "3.2 Model Learning Pipeline",
      "text" : "The learning procedure of our model for a current task Tk is shown in Algorithm 1. The procedure contains four major steps:\nPrototype Generation (line 2 ∼ 13): We first obtain the prototype pr of each old relation r in\nAlgorithm 1: Training procedure for Tk Input: Dk, Rk, R̃k−1, M̃k−1 Output: R̃k, M̃k\n1 P k ← ∅; 2 for each r ∈ R̃k−1 do 3 get Mr from M̃k−1; 4 Hr ← ∅; 5 for each (xi, ti, r) ∈Mr do 6 //get xi’s embedding hi through E; 7 hi ← E(xi, ti); 8 Hr ←Hr ∪ hi; 9 end\n10 //compute r’s prototype as the average of Hr’s embeddings; 11 pr ← Avg(Hr); 12 P k ← P k ∪ pr; 13 end 14 R̃k ← R̃k−1 ∪Rk; 15 M̃k ← M̃k−1; 16 for i = 1 to epochs1 do 17 update E and C according to L1 on Dk; 18 end 19 for each r ∈ Rk do 20 Hr ← ∅; 21 for each (xi, ti, yi) ∈ Dk do 22 if yi = r then 23 hi ← E(xi, ti); 24 Hr ←Hr ∪ hi; 25 end"
    }, {
      "heading" : "26 end",
      "text" : "27 generate Mr by K-means on Hr; 28 M̃k ← M̃k ∪Mr; 29 pr ← Avg(Hr); 30 P k ← P k ∪ pr; 31 end 32 feed P k into M; 33 for i = 1 to epochs2 do 34 update E, M and C according to L2 on\nM̃k with the prototypical refining conducted by M;\n35 end\nR̃k−1 by averaging the embeddings of memorized samples in Mr with sample encoder E (Section 3.3). These prototypes constitute a prototype set P k, which is used to memorize model’s embedding space before training on Tk. Note that the encoder E is continuously changing with tasks, the prototypes of relations need to be regenerated at\nthe beginning of each task. Initial Training (line 16 ∼ 18): The parameters in sample encoder E and relation classifier C are tuned with the training samples inDk (Section 3.4).\nSample Selection (line 19 ∼ 31): For each relation r in Rk, which is unobserved in the former tasks, we retrieve all samples labeled with r from Dk. Then we use K-means algorithm to cluster these samples. In each cluster, we take the sample closest to the centroid as the memorized typical sample of r, to constitute Mr (Section 3.5). Then, we generate r’s prototype pr based on Mr to expand the prototype set P k.\nPrototypical Refining (line 32 ∼ 35): To recover the disruption of sample embedding space, which is caused by training on Tk, we use relation prototype set P k to refine sample embeddings. Specifically, P k is used to initialize our attentionbased memory network M (Section 3.6). The samples in M̃k are encoded into embeddings by E, and then refined by M before being fed to C, to compute the loss function and update model parameters.\nIn general, the parameter update of our model for Tk includes two stages: (1) Initial training on Dk, where samples are encoded by encoder E. (2) Prototypical refining on M̃k, where sample embeddings are generated by encoder E and then refined by memory network M.\nNext, we introduce this procedure in detail."
    }, {
      "heading" : "3.3 Sample Encoder",
      "text" : "The structure of this sample encoder is displayed in Figure 1, which is used to obtain the embedding of each sample. In our model, the encoder E is built upon BERT (Devlin et al., 2019; Wolf et al., 2020), given its excellent performance on text encoding as a representative pre-trained language model. In addition, entity information has been proven effective\nin sample encoding for RE tasks (Wu and He, 2019; Baldini Soares et al., 2019). Thus, we highlight the existence of entities in the sentence to augment E, through adding special tokens to mark the start and end position of entities. Specifically, we use [E11], [E12], [E21] and [E22] to denote the start and end position of head and tail entity, respectively.\nNext, a sample’s hidden representation is the concatenation of token embeddings of [E11] and [E21], which has been proven effective in previous works (Baldini Soares et al., 2019). By feeding this concatenation into a fully connected layer along with layer normalization, a sample’s final embedding h is generated as follows\nh = LN ( W ( concat[h11,h21] ) + b ) , (1)\nwhere h11,h21 ∈ Rh (h is the dimension of BERT hidden representation) are the hidden representations of [E11] and [E21], W ∈ Rd×2h (d is sample embedding dimension) and b ∈ Rd are trainable parameters, and LN(·) is the operation of layer normalization."
    }, {
      "heading" : "3.4 Initial Training for New Task",
      "text" : "According to the general assumption of CRE, all relations in Rk are unobserved in former tasks T1 ∼ Tk−1. We first introduce the model’s initial training on a simple multi-classification task.\nSpecifically, classifier C in our model is a linear softmax classifier. For training set Dk, the loss function is defined as\nL1(θ) = |Dk|∑ i=1 −logP (yi|xi, ti), (2)\nwhere P (yi|xi, ti) is calculated by classifier C based on sample (xi, ti, yi)’s embedding output by sample encoder E."
    }, {
      "heading" : "3.5 Selecting Typical Samples to Memorize Relations",
      "text" : "For each relation r in Rk, we select several typical samples into Mr after the initial training with DK . As the budget of memory is relatively smaller, it is important to select informative and diverse samples to represent r. Inspired by (Han et al., 2020), we apply K-means algorithm upon the embeddings of r’s samples, which are generated by sample encoder E. Suppose the number of clusters is O, which is also the number of typical samples that we will store to represent r. Then, in each cluster we choose the sample closest to the centroid to represent the cluster and add it into the memory. Such operation ensures that the samples stored in the memory are diverse enough and representative for the relation."
    }, {
      "heading" : "3.6 Refining Sample Embeddings with Relation Prototypes",
      "text" : "We propose this module to refine the sample embeddings.\nAfter the initial training for the new task Tk, old relations’ embedding space is likely to be disrupted because the model is tuned towards fitting Tk’s learning objective (Section 3.4). Instead of just replaying memorized samples for recovery, which is a common practice in continual learning, we refine sample embeddings based on relation prototypes.\nBefore applying our prototypical refining, we first obtain the prototype embedding pr for each old relation r in R̃k−1 to constitute the prototype set P k. This step (Prototype Generation) is conducted before the initial training for Tk (Initial Training) to memorize the former state of our model. Then, we construct an attention-based mem-\nory network M based on P k for prototypical refining, as shown in Figure 2. This network’s input is the sample embedding generated by E, and its output is fed into C for relation classification. Based on prototypical refining conducted by memory network M, our model’s embedding space is restored.\nGiven a sample (x, t, y), its embedding h ∈ Rd is generated by E and will be fed to memory network M. We also denote the head number of our memory network as N and the hidden dimension of each head as d1. The output of the i-th attention head is hi ∈ Rd1 , which is computed as\nhi = ATN(qi,Ki,V i)\n= softmax\n( qiK\nT i√\nd1\n) V i,\n(3)\nwhere qi ∈ Rd1 is the linear transformation of input h, and Ki,V i ∈ RL×d1 (L is the current size of R̃k) is the linear transformation of P k. Then, we concatenate each head’s output into the output of multi-head attention layer as\nh̃ = LN ( W1 ( concat[h1,h2, . . . ,hN ] ) + h ) ,\n(4) where W 1 ∈ Rd×Nd1 is a trainable matrix.\nAt last, the final output of M is a residual output computed as\nh̃ ′ = LN ( W 2h̃+ h̃ ) , (5)\nwhere W 2 ∈ Rd×d is also a trainable matrix. h̃ ′ is the refined embedding of (x, t, y), which incorporating the information of prototypes P k through Equation 3 and is fed to the classifier C.\nWe take M̃k as the training set in this stage and the loss function is\nL2(θ) = |M̃k|∑ i=1 −logP (yi|xi, ti), (6)\nwhere (xi, ti, yi) is a sample in M̃k, and P (yi|xi, ti) is calculated by C based on its embedding, which is first generated by E and refined by M.\nBased on the typicality and diversity of memorized samples (samples that can well represent most samples in this relation), training on M̃k can restore the disrupted embedding space of our model with a relatively small computational cost, which allows our model to regain a stable understanding of old relations."
    }, {
      "heading" : "3.7 Prediction",
      "text" : "In order to maintain the consistency of training and prediction, our model uses the embeddings refined by M for prediction after training on a new task."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "Our experiments were conducted upon the following two widely used datasets. The training-testvalidation split ratio is 3:1:1.\nFewRel (Han et al., 2018) It is an RE benchmark dataset originally proposed for few-shot learning, which is annotated by crowd workers and contains 100 relations and 70,000 samples in total. In our experiments, we used the version of 80 relations that has been used (as the training and valid set) for CRE.\nTACRED (Zhang et al., 2017) It is a large-scale RE dataset with 42 relations (including no relation) and 106,264 samples built over newswire and web documents. Based on the open relation assumption of CRE, we removed no relation in our experiments. At the same time, in order to limit the sample imbalance of TACRED, we limited the number of training samples of each relation to 320 and the number of test samples of each relation to 40."
    }, {
      "heading" : "4.2 Compared Models",
      "text" : "We introduce the following state-of-the-art CRE baselines to be compared with our model in our experiments.\nEA-EMR (Wang et al., 2019) maintains a memory to alleviate the problem of catastrophic forgetting.\nEMAR (Han et al., 2020) introduces memory activation and reconsolidation for continual relation learning.\nCML (Wu et al., 2021) proposes a curriculummeta learning method to tackle the order-sensitivity and catastrophic forgetting in CRE.\nAs we adopt pre-trained language model for sample encoding, we replace the encoder (Bi-LSTM) in EMAR with BERT for a fair comparison. This EMAR’s variant is denoted as EMAR+BERT. Besides, we denote our CRE model with relation prototypes as RP-CRE in result display. Since our model only uses the information of memorized samples in attention-based memory network, we further proposed a variant of our model denoted as RP-CRE+Memory Activation, by adding a memory activation (Han et al., 2020) step before attention operation, to verify whether more memory replay is needed."
    }, {
      "heading" : "4.3 Experimental Settings",
      "text" : "In previous CRE experiments (Wang et al., 2019; Han et al., 2020), relations are first divided into 10 clusters to simulate 10 tasks. However, there are two drawbacks of this setting: (1) Recognizing all relations before training is unrealistic and contrary to the setting of lifelong learning. (2) The relations in one cluster generally have more semantic relevance. Therefore, we adopted a completely random sampling strategy on relation-level in our experiments, which is more diverse and realistic. In addition, the task order of all models is exactly the same.\nIn the context of continual learning, we pay more attention to the variation trend of models’ performance while learning new tasks. Therefore, after training for each new task, we will evaluate the classification accuracy of the models on the test set, which is composed of the test samples of all observed relations.\nGiven that most recent CRE models are evaluated by distinguishing true relation labels from a small number of sampled negative labels (Wang et al., 2019), which is too simple and rigid for realistic applications. Therefore, we take a rigorous multi-classification task on all observed relations as the evaluation of our model. It is also the reason that the baselines’ performance is much\nworse than their reported results in the original papers. The method of choosing hyper-parameter for our model is manual tuning. For reproducing our experiment results conveniently, our model’s source code, detailed hyper-parameter configurations and processed samples are provided on https://github.com/fd2014cl/RP-CRE."
    }, {
      "heading" : "4.4 Overall Performance Comparison",
      "text" : "The performance of our model and baselines are shown in Table 1, where the reported scores are the average of 5 rounds of training. Hyper-parameter configurations of baselines are the same as that reported in original papers. Result of each task is the accuracy on test data of all observed relations.\nBased on the results, we find that: (1) Our strict test and sampling strategy actually increase the difficulties of CRE, causing great difficulties to the compared CRE models. This phenomenon is especially obvious in TACRED that has class-imbalance, even if we have made some restrictions to the number of samples for each relation.\n(2) Pre-trained language models, such as BERT, can gain outstanding performance in CRE. Take EMAR for example, replacing Bi-LSTM in it with BERT brings more than 50% of improvement for the last task in FewRel (46.3% to 73.8%), and more than 150% of improvement in TACRED (25.1% to 71.0%). We think this is mainly due to BERT’s\ncapability of making rapid migration to new tasks. The remarkable advantage of the BERT-based models in Table 1 in TACRED further justifies BERT’s insensitivity to sample imbalance.\n(3) Compared with EMAR+BERT, our model also has great advantage, proving that our model can take full advantage of memorized samples and maintain relatively stable performance in continual learning.\n(4) Adding memory activation to our models did not significantly improve performance, indicating that it is sufficient to adopt relation prototypes in CRE.\n(5) Note that all models have similar performance on the former tasks, but our model obtains more stable performance towards the emergence of new tasks. It implies our model’s advantage in long-term memory, which will be proven in Section 4.5.\nThe average time consumption (on the machine with a single RTX3090) of training RP-CRE is 1h28min, EMAR is 37min and EMAR+BERT is 3h21min. Our model’s time consumption is mainly due to the massive parameters of BERT. Given our model’s apparent performance improvement with respect to EMAR, such time consumption is relatively acceptable.\nTable 2: Accuracy (%) on the test sets from every previous task at the stage of learning the last task (with the same size of memory), indicating that our model has better performance on previous tasks.\nModel T1 T2 T3 T4 T4 T6 T7 T8 T9 T10 RP-CRE (Ours) 82.8 68.4 89.0 78.8 75.7 88.1 77.3 82.9 92.3 90.8 EMAR+BERT 75.2 59.6 77.6 65.8 65.9 80.5 58.9 60.0 87.6 98.0\nFigure 3: Visualized process of alleviating the disruption of sample embedding space after learning a new task. (a) Recovery result of EMAR+BERT. (b) Recovery result of RP-CRE."
    }, {
      "heading" : "4.5 Long-term Effectiveness of Episodic Memory",
      "text" : "To explore long-term effectiveness of episodic memory in our model, we compared our model with EMAR+BERT on FewRel, which is similar to our model in selecting memorized samples. Results are shown in Table 2, where each score is the classification accuracy for all relations on test set of each former task. We conclude that after training on 10 sequential tasks, our model performs better on the former tasks. It indicates that our model has a much stable understanding of old relations in old tasks. In both models, memorized samples of old relations are used to restore the model’s performance on old relations (memory reconsolidation in EMAR, prototypical refining in our model). In order to find the reason of EMAR’s inferior performance on the former tasks, we display the visualization the varying of sample embedding space during model training.\nConcretely, we used t-SNE (Van der Maaten and Hinton, 2008) for dimension reduction and chose memorized samples from relation participant for visualization, which were fed into the two models on the same task. Figure 3 shows the sample positions in the embedding space, where the blue dots\nrepresent the memorized samples and the red dot represents the relation’s prototype (the centroid of memorized samples before learning the new task). Left two sub-figures display how sample embedding space is disrupted by the learning of new tasks. Right two sub-figures display how the model recovers.\nFrom Figure 3, we notice that although EMAR’s sample embeddings are getting closer to the former centroid (relation prototype) after memory reconsolidation, they converge in fact. Comparatively, our model restores the embedding space while retaining the diversity between samples. In terms of typicality and diversity of memorized samples, it is not our purpose to encode all memorized samples into exactly the same point in the embedding space, since it may damage the diversity of these samples and reduce the information provided by the samples, during model’s recovery from disrupted condition.\nThis result is mainly due to that the loss function selected in EMAR’s memory reconsolidation is too radical, focusing only on reducing the absolute distance between a memorized sample and the relation prototype. Therefore, our strategy of refining sample embeddings with relation prototypes (Section\n3.6) better preserves the diversity of memorized samples, as it takes into account various features of samples rather than the true labels. It eventually retains more information of memorized samples."
    }, {
      "heading" : "4.6 Model Dependence on Memory Size",
      "text" : "In most memory-based CRE models, memory size (number of memorized samples) is a key factor affecting model performance. However, most of previous models do not fully utilize the information provided by memorized samples, resulting in their dependence on memory size. Even worse, the memorized samples have the same magnitude as the original samples. In Section 3.6, we have emphasized the advantages of our model in retaining and full utilization of memory information. We verified whether our model relies less on memory size through comparison experiments, of which the results are shown in Figure 4.\nWe chose EMAR+BERT as the main competitor, in which the configuration and task sequence remained unchanged. The only variable we adjusted is memory size. Based on the results we conclude that, as memory size decreases, our model obtains less decreased performance than EMAR+BERT (performance degradation is inevitable). Even though EMAR showed a relatively stable performance in the first two tasks, its performance dropped significantly in the subsequent tasks. This is consistent with the long-term effectiveness of memory we have analyzed in Section 4.5. The diversity of samples in EMAR would gradually disappear, making it highly dependent on memory size. Comparatively, our model’s dependence\non memory size is weak because it preserves the diversity of samples."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we propose a novel CRE model obtaining enhanced performance through refining sample embeddings. In our model, the sample embeddings are refined by an attention-based memory network fed with relation prototypes, that are generated from memorized samples. The comparison experiments show that our model significantly outperforms current state-of-the-art CRE models. As most current CRE models are memory-based, we further explore the long-term effectiveness of episodic memory. The results show that our model has great advantages in maintaining diversity of memorized samples and performs well in avoiding catastrophic forgetting of old relations (tasks). Because of the efficiency in memory mechanism, our model depends less on memory size. In future work, we will explore whether the mechanism of refining sample embeddings with prototypes can be used in other classification-based continual learning tasks."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We sincerely thank all anonymous reviewers for their valuable comments. This work is supported by National Key Research and Development Project (No.2020AAA0109302), Shanghai Science and Technology Innovation Action Plan (No.19511120400) and Shanghai Municipal Science and Technology Major Project (No.2021SHZDZX0103)."
    } ],
    "references" : [ {
      "title" : "Memory aware synapses: Learning what (not) to forget",
      "author" : [ "Rahaf Aljundi", "Francesca Babiloni", "Mohamed Elhoseiny", "Marcus Rohrbach", "Tinne Tuytelaars." ],
      "venue" : "Proceedings of the European Conference on Computer Vision, pages 139–154.",
      "citeRegEx" : "Aljundi et al\\.,? 2018",
      "shortCiteRegEx" : "Aljundi et al\\.",
      "year" : 2018
    }, {
      "title" : "Matching the blanks: Distributional similarity for relation learning",
      "author" : [ "Livio Baldini Soares", "Nicholas FitzGerald", "Jeffrey Ling", "Tom Kwiatkowski." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Soares et al\\.,? 2019",
      "shortCiteRegEx" : "Soares et al\\.",
      "year" : 2019
    }, {
      "title" : "Efficient lifelong learning with a-gem",
      "author" : [ "Arslan Chaudhry", "Marc’Aurelio Ranzato", "Marcus Rohrbach", "Mohamed Elhoseiny" ],
      "venue" : "arXiv preprint arXiv:1812.00420",
      "citeRegEx" : "Chaudhry et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Chaudhry et al\\.",
      "year" : 2018
    }, {
      "title" : "Net2net: Accelerating learning via knowledge transfer",
      "author" : [ "Tianqi Chen", "Ian Goodfellow", "Jonathon Shlens." ],
      "venue" : "arXiv preprint arXiv:1511.05641.",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Prototypical representation learning for relation extraction",
      "author" : [ "Ning Ding", "Xiaobin Wang", "Yao Fu", "Guangwei Xu", "Rui Wang", "Pengjun Xie", "Ying Shen", "Fei Huang", "Hai-Tao Zheng", "Rui Zhang." ],
      "venue" : "arXiv preprint arXiv:2103.11647.",
      "citeRegEx" : "Ding et al\\.,? 2021",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2021
    }, {
      "title" : "Pathnet: Evolution channels gradient descent in super neural networks",
      "author" : [ "Chrisantha Fernando", "Dylan Banarse", "Charles Blundell", "Yori Zwols", "David Ha", "Andrei A Rusu", "Alexander Pritzel", "Daan Wierstra." ],
      "venue" : "arXiv preprint arXiv:1701.08734.",
      "citeRegEx" : "Fernando et al\\.,? 2017",
      "shortCiteRegEx" : "Fernando et al\\.",
      "year" : 2017
    }, {
      "title" : "Catastrophic forgetting in connectionist networks",
      "author" : [ "Robert M French." ],
      "venue" : "Trends in cognitive sciences, 3(4):128–135.",
      "citeRegEx" : "French.,? 1999",
      "shortCiteRegEx" : "French.",
      "year" : 1999
    }, {
      "title" : "GraphRel: Modeling text as relational graphs for joint entity and relation extraction",
      "author" : [ "Tsu-Jui Fu", "Peng-Hsuan Li", "Wei-Yun Ma." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1409–1418.",
      "citeRegEx" : "Fu et al\\.,? 2019",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2019
    }, {
      "title" : "Hybrid attention-based prototypical networks for noisy few-shot relation classification",
      "author" : [ "Tianyu Gao", "Xu Han", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33.",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "Embracing change: Continual learning in deep neural networks",
      "author" : [ "Raia Hadsell", "Dushyant Rao", "Andrei A Rusu", "Razvan Pascanu." ],
      "venue" : "Trends in Cognitive Sciences.",
      "citeRegEx" : "Hadsell et al\\.,? 2020",
      "shortCiteRegEx" : "Hadsell et al\\.",
      "year" : 2020
    }, {
      "title" : "Continual relation learning via episodic memory activation and reconsolidation",
      "author" : [ "Xu Han", "Yi Dai", "Tianyu Gao", "Yankai Lin", "Zhiyuan Liu", "Peng Li", "Maosong Sun", "Jie Zhou." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Han et al\\.,? 2020",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2020
    }, {
      "title" : "FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation",
      "author" : [ "Xu Han", "Hao Zhu", "Pengfei Yu", "Ziyun Wang", "Yuan Yao", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Meth-",
      "citeRegEx" : "Han et al\\.,? 2018",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2018
    }, {
      "title" : "Neuroscience-inspired artificial intelligence",
      "author" : [ "Demis Hassabis", "Dharshan Kumaran", "Christopher Summerfield", "Matthew Botvinick." ],
      "venue" : "Neuron, 95(2):245–258.",
      "citeRegEx" : "Hassabis et al\\.,? 2017",
      "shortCiteRegEx" : "Hassabis et al\\.",
      "year" : 2017
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Distant supervision for relation extraction with sentence-level attention and entity descriptions",
      "author" : [ "Guoliang Ji", "Kang Liu", "Shizhu He", "Jun Zhao." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 31.",
      "citeRegEx" : "Ji et al\\.,? 2017",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2017
    }, {
      "title" : "Overcoming catastrophic forgetting in neural networks",
      "author" : [ "James Kirkpatrick", "Razvan Pascanu", "Neil Rabinowitz", "Joel Veness", "Guillaume Desjardins", "Andrei A Rusu", "Kieran Milan", "John Quan", "Tiago Ramalho", "Agnieszka Grabska-Barwinska" ],
      "venue" : null,
      "citeRegEx" : "Kirkpatrick et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kirkpatrick et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural relation extraction with selective attention over instances",
      "author" : [ "Yankai Lin", "Shiqi Shen", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2124–2133.",
      "citeRegEx" : "Lin et al\\.,? 2016",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2016
    }, {
      "title" : "Rotate your networks: Better weight consolidation and less catastrophic forgetting",
      "author" : [ "Xialei Liu", "Marc Masana", "Luis Herranz", "Joost Van de Weijer", "Antonio M Lopez", "Andrew D Bagdanov." ],
      "venue" : "2018 24th International Conference on Pattern Recogni-",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Gradient episodic memory for continual learning",
      "author" : [ "David Lopez-Paz", "Marc’Aurelio Ranzato" ],
      "venue" : "arXiv preprint arXiv:1706.08840",
      "citeRegEx" : "Lopez.Paz and Ranzato.,? \\Q2017\\E",
      "shortCiteRegEx" : "Lopez.Paz and Ranzato.",
      "year" : 2017
    }, {
      "title" : "Visualizing data using t-sne",
      "author" : [ "Laurens Van der Maaten", "Geoffrey Hinton." ],
      "venue" : "Journal of machine learning research, 9(11).",
      "citeRegEx" : "Maaten and Hinton.,? 2008",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    }, {
      "title" : "Relation extraction: Perspective from convolutional neural networks",
      "author" : [ "Thien Huu Nguyen", "Ralph Grishman." ],
      "venue" : "Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing, pages 39–48.",
      "citeRegEx" : "Nguyen and Grishman.,? 2015",
      "shortCiteRegEx" : "Nguyen and Grishman.",
      "year" : 2015
    }, {
      "title" : "Metalearning improves lifelong relation extraction",
      "author" : [ "Abiola Obamuyide", "Andreas Vlachos." ],
      "venue" : "Proceedings of the 4th Workshop on Representation Learning for NLP, pages 224–229.",
      "citeRegEx" : "Obamuyide and Vlachos.,? 2019",
      "shortCiteRegEx" : "Obamuyide and Vlachos.",
      "year" : 2019
    }, {
      "title" : "Continual lifelong learning with neural networks: A review",
      "author" : [ "German I Parisi", "Ronald Kemker", "Jose L Part", "Christopher Kanan", "Stefan Wermter." ],
      "venue" : "Neural Networks, 113:54–71.",
      "citeRegEx" : "Parisi et al\\.,? 2019",
      "shortCiteRegEx" : "Parisi et al\\.",
      "year" : 2019
    }, {
      "title" : "Relation extraction with matrix factorization and universal schemas",
      "author" : [ "Sebastian Riedel", "Limin Yao", "Andrew McCallum", "Benjamin M. Marlin." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Riedel et al\\.,? 2013",
      "shortCiteRegEx" : "Riedel et al\\.",
      "year" : 2013
    }, {
      "title" : "Metalearning with memory-augmented neural networks",
      "author" : [ "Adam Santoro", "Sergey Bartunov", "Matthew Botvinick", "Daan Wierstra", "Timothy Lillicrap." ],
      "venue" : "International Conference on Machine Learning, pages 1842–1850.",
      "citeRegEx" : "Santoro et al\\.,? 2016",
      "shortCiteRegEx" : "Santoro et al\\.",
      "year" : 2016
    }, {
      "title" : "Relational recurrent neural networks",
      "author" : [ "Adam Santoro", "Ryan Faulkner", "David Raposo", "Jack Rae", "Mike Chrzanowski", "Theophane Weber", "Daan Wierstra", "Oriol Vinyals", "Razvan Pascanu", "Timothy Lillicrap." ],
      "venue" : "arXiv preprint arXiv:1806.01822.",
      "citeRegEx" : "Santoro et al\\.,? 2018",
      "shortCiteRegEx" : "Santoro et al\\.",
      "year" : 2018
    }, {
      "title" : "Prototypical networks for few-shot learning",
      "author" : [ "Jake Snell", "Kevin Swersky", "Richard Zemel." ],
      "venue" : "Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 4080–4090.",
      "citeRegEx" : "Snell et al\\.,? 2017",
      "shortCiteRegEx" : "Snell et al\\.",
      "year" : 2017
    }, {
      "title" : "Lifelong robot learning",
      "author" : [ "Sebastian Thrun", "Tom M Mitchell." ],
      "venue" : "Robotics and autonomous systems, 15(1-2):25–46.",
      "citeRegEx" : "Thrun and Mitchell.,? 1995",
      "shortCiteRegEx" : "Thrun and Mitchell.",
      "year" : 1995
    }, {
      "title" : "Sentence embedding alignment for lifelong relation extraction",
      "author" : [ "Hong Wang", "Wenhan Xiong", "Mo Yu", "Xiaoxiao Guo", "Shiyu Chang", "William Yang Wang." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "A novel cascade binary tagging framework for relational triple extraction",
      "author" : [ "Zhepei Wei", "Jianlin Su", "Yue Wang", "Yuan Tian", "Yi Chang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1476–",
      "citeRegEx" : "Wei et al\\.,? 2020",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Thomas Wolf", "Julien Chaumond", "Lysandre Debut", "Victor Sanh", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Morgan Funtowicz" ],
      "venue" : "In Proceedings of the 2020 Conference on Empirical",
      "citeRegEx" : "Wolf et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2020
    }, {
      "title" : "Enriching pretrained language model with entity information for relation classification",
      "author" : [ "Shanchan Wu", "Yifan He." ],
      "venue" : "Proceedings of the 28th ACM International Conference on Information and Knowledge Management, pages 2361–2364.",
      "citeRegEx" : "Wu and He.,? 2019",
      "shortCiteRegEx" : "Wu and He.",
      "year" : 2019
    }, {
      "title" : "Curriculum-meta learning for order-robust continual relation extraction",
      "author" : [ "Tongtong Wu", "Xuekai Li", "Yuan-Fang Li", "Reza Haffari", "Guilin Qi", "Yujin Zhu", "Guoqiang Xu." ],
      "venue" : "arXiv preprint arXiv:2101.01926.",
      "citeRegEx" : "Wu et al\\.,? 2021",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2021
    }, {
      "title" : "Classifying relations via long short term memory networks along shortest dependency paths",
      "author" : [ "Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Distant supervision for relation extraction via piecewise convolutional neural networks",
      "author" : [ "Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1753–",
      "citeRegEx" : "Zeng et al\\.,? 2015",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2015
    }, {
      "title" : "Relation classification via convolutional deep neural network",
      "author" : [ "Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao." ],
      "venue" : "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,",
      "citeRegEx" : "Zeng et al\\.,? 2014",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2014
    }, {
      "title" : "Continual learning through synaptic intelligence",
      "author" : [ "Friedemann Zenke", "Ben Poole", "Surya Ganguli." ],
      "venue" : "International Conference on Machine Learning, pages 3987–3995.",
      "citeRegEx" : "Zenke et al\\.,? 2017",
      "shortCiteRegEx" : "Zenke et al\\.",
      "year" : 2017
    }, {
      "title" : "Relation classification via recurrent neural network",
      "author" : [ "Dongxu Zhang", "Dong Wang." ],
      "venue" : "arXiv preprint arXiv:1508.01006.",
      "citeRegEx" : "Zhang and Wang.,? 2015",
      "shortCiteRegEx" : "Zhang and Wang.",
      "year" : 2015
    }, {
      "title" : "Relation classification via recurrent neural network with attention and tensor layers",
      "author" : [ "Runyan Zhang", "Fanrong Meng", "Yong Zhou", "Bing Liu." ],
      "venue" : "Big Data Mining and Analytics, 1(3):234–244.",
      "citeRegEx" : "Zhang et al\\.,? 2018a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Graph convolution over pruned dependency trees improves relation extraction",
      "author" : [ "Yuhao Zhang", "Peng Qi", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2205–2215.",
      "citeRegEx" : "Zhang et al\\.,? 2018b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Positionaware attention and supervised data improve slot filling",
      "author" : [ "Yuhao Zhang", "Victor Zhong", "Danqi Chen", "Gabor Angeli", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Zhang et al\\.,? 2017",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2017
    }, {
      "title" : "Attention-based bidirectional long short-term memory networks",
      "author" : [ "Peng Zhou", "Wei Shi", "Jun Tian", "Zhenyu Qi", "Bingchen Li", "Hongwei Hao", "Bo Xu" ],
      "venue" : null,
      "citeRegEx" : "Zhou et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2016
    }, {
      "title" : "Graph neural networks with generated parameters for relation extraction",
      "author" : [ "Hao Zhu", "Yankai Lin", "Zhiyuan Liu", "Jie Fu", "Tat-Seng Chua", "Maosong Sun." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Zhu et al\\.,? 2019",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "∗Corresponding author widely applied in many downstream tasks, such as knowledge base construction and completion (Riedel et al., 2013).",
      "startOffset" : 114,
      "endOffset" : 135
    }, {
      "referenceID" : 10,
      "context" : "and iterative nature of our world (Hadsell et al., 2020).",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 29,
      "context" : "To adapt to such a situation, the paradigm of continual relation extraction (CRE) is proposed (Wang et al., 2019; Han et al., 2020; Wu et al., 2021).",
      "startOffset" : 94,
      "endOffset" : 148
    }, {
      "referenceID" : 11,
      "context" : "To adapt to such a situation, the paradigm of continual relation extraction (CRE) is proposed (Wang et al., 2019; Han et al., 2020; Wu et al., 2021).",
      "startOffset" : 94,
      "endOffset" : 148
    }, {
      "referenceID" : 33,
      "context" : "To adapt to such a situation, the paradigm of continual relation extraction (CRE) is proposed (Wang et al., 2019; Han et al., 2020; Wu et al., 2021).",
      "startOffset" : 94,
      "endOffset" : 148
    }, {
      "referenceID" : 23,
      "context" : "Continual learning (or lifelong learning) systems are defined as adaptive algorithms capable of learning from a continuous stream of information (Parisi et al., 2019), where the information is progressively available over time and the number of learning tasks is not pre-defined.",
      "startOffset" : 145,
      "endOffset" : 166
    }, {
      "referenceID" : 13,
      "context" : "Continual learning remains a long-standing challenge for machine learning and deep learning (Hassabis et al., 2017; Thrun and Mitchell, 1995), as its main obstacle is the tendency of models to forget existing knowledge when learning from new observations (French, 1999), which is called as catastrophic forgetting.",
      "startOffset" : 92,
      "endOffset" : 141
    }, {
      "referenceID" : 28,
      "context" : "Continual learning remains a long-standing challenge for machine learning and deep learning (Hassabis et al., 2017; Thrun and Mitchell, 1995), as its main obstacle is the tendency of models to forget existing knowledge when learning from new observations (French, 1999), which is called as catastrophic forgetting.",
      "startOffset" : 92,
      "endOffset" : 141
    }, {
      "referenceID" : 7,
      "context" : ", 2017; Thrun and Mitchell, 1995), as its main obstacle is the tendency of models to forget existing knowledge when learning from new observations (French, 1999), which is called as catastrophic forgetting.",
      "startOffset" : 147,
      "endOffset" : 161
    }, {
      "referenceID" : 16,
      "context" : "Recent works try to address the problem of catastrophic forgetting in three ways, including consolidation-based methods (Kirkpatrick et al., 2017), dynamic archi-",
      "startOffset" : 120,
      "endOffset" : 146
    }, {
      "referenceID" : 3,
      "context" : "233 tecture (Chen et al., 2015; Fernando et al., 2017) and memory-based methods (Lopez-Paz and Ranzato, 2017; Aljundi et al.",
      "startOffset" : 12,
      "endOffset" : 54
    }, {
      "referenceID" : 6,
      "context" : "233 tecture (Chen et al., 2015; Fernando et al., 2017) and memory-based methods (Lopez-Paz and Ranzato, 2017; Aljundi et al.",
      "startOffset" : 12,
      "endOffset" : 54
    }, {
      "referenceID" : 19,
      "context" : ", 2017) and memory-based methods (Lopez-Paz and Ranzato, 2017; Aljundi et al., 2018; Chaudhry et al., 2018), in which memory-based methods have been proven promising in NLP tasks.",
      "startOffset" : 33,
      "endOffset" : 107
    }, {
      "referenceID" : 0,
      "context" : ", 2017) and memory-based methods (Lopez-Paz and Ranzato, 2017; Aljundi et al., 2018; Chaudhry et al., 2018), in which memory-based methods have been proven promising in NLP tasks.",
      "startOffset" : 33,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : ", 2017) and memory-based methods (Lopez-Paz and Ranzato, 2017; Aljundi et al., 2018; Chaudhry et al., 2018), in which memory-based methods have been proven promising in NLP tasks.",
      "startOffset" : 33,
      "endOffset" : 107
    }, {
      "referenceID" : 29,
      "context" : "lations, such as EA-EMR (Wang et al., 2019), MLLRE (Obamuyide and Vlachos, 2019), CML (Wu et al.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 33,
      "context" : ", 2019), MLLRE (Obamuyide and Vlachos, 2019), CML (Wu et al., 2021) and EMAR (Han et al.",
      "startOffset" : 50,
      "endOffset" : 67
    }, {
      "referenceID" : 33,
      "context" : "samples (Wu et al., 2021), which is unrealistic in real-world tasks.",
      "startOffset" : 8,
      "endOffset" : 25
    }, {
      "referenceID" : 27,
      "context" : "Inspired by prototypical networks (Snell et al., 2017) for few-shot classification, we employ relation prototypes to represent different relations in this paper, which help the model understand different relations well.",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 38,
      "context" : "(2) Recurrent neural networks (RNNs) (Zhang and Wang, 2015; Xu et al., 2015; Zhou et al., 2016; Zhang et al., 2018a) are particularly capable of learning long-distance relation pat-",
      "startOffset" : 37,
      "endOffset" : 116
    }, {
      "referenceID" : 34,
      "context" : "(2) Recurrent neural networks (RNNs) (Zhang and Wang, 2015; Xu et al., 2015; Zhou et al., 2016; Zhang et al., 2018a) are particularly capable of learning long-distance relation pat-",
      "startOffset" : 37,
      "endOffset" : 116
    }, {
      "referenceID" : 42,
      "context" : "(2) Recurrent neural networks (RNNs) (Zhang and Wang, 2015; Xu et al., 2015; Zhou et al., 2016; Zhang et al., 2018a) are particularly capable of learning long-distance relation pat-",
      "startOffset" : 37,
      "endOffset" : 116
    }, {
      "referenceID" : 39,
      "context" : "(2) Recurrent neural networks (RNNs) (Zhang and Wang, 2015; Xu et al., 2015; Zhou et al., 2016; Zhang et al., 2018a) are particularly capable of learning long-distance relation pat-",
      "startOffset" : 37,
      "endOffset" : 116
    }, {
      "referenceID" : 40,
      "context" : "(3) Graph neural networks (GNNs) (Zhang et al., 2018b; Fu et al., 2019; Zhu et al., 2019) build word/entity graphs for cross-sentence reasoning.",
      "startOffset" : 33,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : "(3) Graph neural networks (GNNs) (Zhang et al., 2018b; Fu et al., 2019; Zhu et al., 2019) build word/entity graphs for cross-sentence reasoning.",
      "startOffset" : 33,
      "endOffset" : 89
    }, {
      "referenceID" : 43,
      "context" : "(3) Graph neural networks (GNNs) (Zhang et al., 2018b; Fu et al., 2019; Zhu et al., 2019) build word/entity graphs for cross-sentence reasoning.",
      "startOffset" : 33,
      "endOffset" : 89
    }, {
      "referenceID" : 4,
      "context" : "Recently, pre-trained language models (Devlin et al., 2019) have also been extensively used",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 32,
      "context" : "in RE tasks (Wu and He, 2019; Wei et al., 2020; Baldini Soares et al., 2019), and have achieved state-of-the-arts performance.",
      "startOffset" : 12,
      "endOffset" : 76
    }, {
      "referenceID" : 30,
      "context" : "in RE tasks (Wu and He, 2019; Wei et al., 2020; Baldini Soares et al., 2019), and have achieved state-of-the-arts performance.",
      "startOffset" : 12,
      "endOffset" : 76
    }, {
      "referenceID" : 16,
      "context" : "Existing continual learning methods can be divided into three categories: (1) Regularization methods (Kirkpatrick et al., 2017; Zenke et al., 2017; Liu et al., 2018) alleviate catastrophic forgetting by imposing constraints on updating the neural weights important to previous tasks.",
      "startOffset" : 101,
      "endOffset" : 165
    }, {
      "referenceID" : 37,
      "context" : "Existing continual learning methods can be divided into three categories: (1) Regularization methods (Kirkpatrick et al., 2017; Zenke et al., 2017; Liu et al., 2018) alleviate catastrophic forgetting by imposing constraints on updating the neural weights important to previous tasks.",
      "startOffset" : 101,
      "endOffset" : 165
    }, {
      "referenceID" : 18,
      "context" : "Existing continual learning methods can be divided into three categories: (1) Regularization methods (Kirkpatrick et al., 2017; Zenke et al., 2017; Liu et al., 2018) alleviate catastrophic forgetting by imposing constraints on updating the neural weights important to previous tasks.",
      "startOffset" : 101,
      "endOffset" : 165
    }, {
      "referenceID" : 3,
      "context" : "(2) Dynamic architecture methods (Chen et al., 2015; Fernando et al., 2017) change architectural properties in response to new information by dynamically accommodating novel neural resources.",
      "startOffset" : 33,
      "endOffset" : 75
    }, {
      "referenceID" : 6,
      "context" : "(2) Dynamic architecture methods (Chen et al., 2015; Fernando et al., 2017) change architectural properties in response to new information by dynamically accommodating novel neural resources.",
      "startOffset" : 33,
      "endOffset" : 75
    }, {
      "referenceID" : 19,
      "context" : "(3) Memory-based methods (Lopez-Paz and Ranzato, 2017; Aljundi et al., 2018; Chaudhry et al., 2018) remember a few examples in previous tasks and continually replay the memory with emerging new tasks.",
      "startOffset" : 25,
      "endOffset" : 99
    }, {
      "referenceID" : 0,
      "context" : "(3) Memory-based methods (Lopez-Paz and Ranzato, 2017; Aljundi et al., 2018; Chaudhry et al., 2018) remember a few examples in previous tasks and continually replay the memory with emerging new tasks.",
      "startOffset" : 25,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "(3) Memory-based methods (Lopez-Paz and Ranzato, 2017; Aljundi et al., 2018; Chaudhry et al., 2018) remember a few examples in previous tasks and continually replay the memory with emerging new tasks.",
      "startOffset" : 25,
      "endOffset" : 99
    }, {
      "referenceID" : 29,
      "context" : "For CRE, the memory-based methods have been proven most promising (Wang et al., 2019; Han et al., 2020).",
      "startOffset" : 66,
      "endOffset" : 103
    }, {
      "referenceID" : 11,
      "context" : "For CRE, the memory-based methods have been proven most promising (Wang et al., 2019; Han et al., 2020).",
      "startOffset" : 66,
      "endOffset" : 103
    }, {
      "referenceID" : 14,
      "context" : "There are also many memory networks proposed to remember information of long periods, such as LSTM (Hochreiter and Schmidhuber, 1997) and memory-augmented neural networks (Graves et al.",
      "startOffset" : 99,
      "endOffset" : 133
    }, {
      "referenceID" : 25,
      "context" : "There are also many memory networks proposed to remember information of long periods, such as LSTM (Hochreiter and Schmidhuber, 1997) and memory-augmented neural networks (Graves et al., 2016; Santoro et al., 2016).",
      "startOffset" : 171,
      "endOffset" : 214
    }, {
      "referenceID" : 26,
      "context" : "Besides, a new memory module (Santoro et al., 2018) has demonstrated its success in relational reasoning, which employs multi-head attention to allow memory interaction.",
      "startOffset" : 29,
      "endOffset" : 51
    }, {
      "referenceID" : 32,
      "context" : "Inspired by current CRE models (Wu and He, 2019; Han et al., 2020), we adopt an episodic memory module to store typical samples of relations that the model has learned in former tasks.",
      "startOffset" : 31,
      "endOffset" : 66
    }, {
      "referenceID" : 11,
      "context" : "Inspired by current CRE models (Wu and He, 2019; Han et al., 2020), we adopt an episodic memory module to store typical samples of relations that the model has learned in former tasks.",
      "startOffset" : 31,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : "In our model, the encoder E is built upon BERT (Devlin et al., 2019; Wolf et al., 2020), given its excellent performance on text encoding as a representative pre-trained language model.",
      "startOffset" : 47,
      "endOffset" : 87
    }, {
      "referenceID" : 31,
      "context" : "In our model, the encoder E is built upon BERT (Devlin et al., 2019; Wolf et al., 2020), given its excellent performance on text encoding as a representative pre-trained language model.",
      "startOffset" : 47,
      "endOffset" : 87
    }, {
      "referenceID" : 32,
      "context" : "In addition, entity information has been proven effective in sample encoding for RE tasks (Wu and He, 2019; Baldini Soares et al., 2019).",
      "startOffset" : 90,
      "endOffset" : 136
    }, {
      "referenceID" : 11,
      "context" : "Inspired by (Han et al., 2020), we apply K-means algorithm upon the embeddings of r’s samples, which are generated by sample encoder E.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 41,
      "context" : "TACRED (Zhang et al., 2017) It is a large-scale RE dataset with 42 relations (including no relation) and 106,264 samples built over newswire and web documents.",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 29,
      "context" : "EA-EMR (Wang et al., 2019) maintains a memory to alleviate the problem of catastrophic forgetting.",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 11,
      "context" : "EMAR (Han et al., 2020) introduces memory activation and reconsolidation for continual relation learning.",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 33,
      "context" : "CML (Wu et al., 2021) proposes a curriculummeta learning method to tackle the order-sensitivity and catastrophic forgetting in CRE.",
      "startOffset" : 4,
      "endOffset" : 21
    }, {
      "referenceID" : 11,
      "context" : "Since our model only uses the information of memorized samples in attention-based memory network, we further proposed a variant of our model denoted as RP-CRE+Memory Activation, by adding a memory activation (Han et al., 2020) step before attention operation, to verify whether more memory replay is needed.",
      "startOffset" : 208,
      "endOffset" : 226
    }, {
      "referenceID" : 29,
      "context" : "In previous CRE experiments (Wang et al., 2019; Han et al., 2020), relations are first divided into 10 clusters to simulate 10 tasks.",
      "startOffset" : 28,
      "endOffset" : 65
    }, {
      "referenceID" : 11,
      "context" : "In previous CRE experiments (Wang et al., 2019; Han et al., 2020), relations are first divided into 10 clusters to simulate 10 tasks.",
      "startOffset" : 28,
      "endOffset" : 65
    }, {
      "referenceID" : 29,
      "context" : "Given that most recent CRE models are evaluated by distinguishing true relation labels from a small number of sampled negative labels (Wang et al., 2019), which is too simple and rigid for realistic applications.",
      "startOffset" : 134,
      "endOffset" : 153
    } ],
    "year" : 2021,
    "abstractText" : "Continual learning has gained increasing attention in recent years, thanks to its biological interpretation and efficiency in many realworld applications. As a typical task of continual learning, continual relation extraction (CRE) aims to extract relations between entities from texts, where the samples of different relations are delivered into the model continuously. Some previous works have proved that storing typical samples of old relations in memory can help the model keep a stable understanding of old relations and avoid forgetting them. However, most methods heavily depend on the memory size in that they simply replay these memorized samples in subsequent tasks. To fully utilize memorized samples, in this paper, we employ relation prototype to extract useful information of each relation. Specifically, the prototype embedding for a specific relation is computed based on memorized samples of this relation, which is collected by K-means algorithm. The prototypes of all observed relations at current learning stage are used to re-initialize a memory network to refine subsequent sample embeddings, which ensures the model’s stable understanding on all observed relations when learning a new task. Compared with previous CRE models, our model utilizes the memory information sufficiently and efficiently, resulting in enhanced CRE performance. Our experiments show that the proposed model outperforms the state-of-the-art CRE models and has great advantage in avoiding catastrophic forgetting. The code and datasets have been released on https://github.com/fd2014cl/RP-CRE.",
    "creator" : "LaTeX with hyperref"
  }
}