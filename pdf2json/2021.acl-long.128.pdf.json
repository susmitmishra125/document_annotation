{
  "name" : "2021.acl-long.128.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Topic-Aware Evidence Reasoning and Stance-Aware Aggregation for Fact Verification",
    "authors" : [ "Jiasheng Si", "Deyu Zhou", "Tongzhe Li", "Xingyu Shi", "Yulan He" ],
    "emails" : [ "xyu-shi}@seu.edu.cn,", "yulan.he@warwick.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1612–1622\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1612"
    }, {
      "heading" : "1 Introduction",
      "text" : "The Internet breaks the physical distance barrier among individuals to allow them to share data and information online. However, it can also be used by people with malicious purposes to disseminate misinformation or fake news. Such misinformation may cause ethnics conflicts, financial losses and political unrest, which has become one of the greatest threats to the public (Zafarani et al., 2019; Zhou\n∗corresponding author\net al., 2019b). Moreover, as shown in Vosoughi et al. (2018), compared with truth, misinformation diffuses significantly farther, faster, and deeper in all genres. Therefore, there is an urgent need for quickly identifying the misinformation spread on the web. To solve this problem, we focus on the fact verification task (Thorne et al., 2018), which aims to automatically evaluate the veracity of a given claim based on the textual evidence retrieved from external sources.\nRecent approaches for fact verification are dominated by natural language inference models (Angeli and Manning, 2014) or textual entailment recognition models (Ma et al., 2019), where the truthfulness of a claim is verified via reasoning and aggregating over multiple pieces of retrieved evidence. In general, existing models follow an architecture with two main sub-modules: the semantic interaction module and the entailment-based aggregation module (Hanselowski et al., 2018a; Nie et al., 2019a; Soleimani et al., 2020; Liu et al., 2020). The semantic interaction module attempts to grasp the rich semantic-level interactions among multiple pieces of evidence at the sentence-level (Ma et al., 2019; Zhou et al., 2019a; Subramanian and Lee, 2020) or the semantic roles-level (Zhong et al., 2020). The entailment-based aggregation module aims to filter out irrelevant information to capture the salient information related to the claim by aggregating the semantic information coherently.\nHowever, the aforementioned approaches typically learn the representation of each evidenceclaim pair from the semantic perspective such as obtaining the semantic representation of each evidence-claim pair through pre-trained language models (Devlin et al., 2019) or graph-based models (Velickovic et al., 2018), which largely overlooked the topical consistency between claim and evidence. For example in Figure 1, given the claim “A high school student named Cole Withrow was\ncharged for leaving an unloaded shotgun in his vehicle while parking at school” and the retrieved evidence sentences (i.e., E1-E4), we would expect a fact checking model to automatically filter evidence which is topically-unrelated to the claim such as E3 and E4 and only relies on the evidence which is topically-consistent with the claim such as E1 and E2 for veracity assessment of the claim. In addition, we also expect the topical coherence of multiple pieces of supporting evidence such as E1 and E2. Furthermore, in previous approaches, the learned representations of multiple pieces of evidence are aggregated via element-wise max pooling or simple dot-product attention, which inevitably fails to capture the implicit stances of evidence toward the claim (e.g., E1 and E2 support the claim implicitly, E3 and E4 are unrelated to the claim) and leads to the combination of irrelevant information with relevant one.\nTo address these problems, in this paper, we propose a novel neural structure reasoning model for fact verification, named TARSA (Topic-Aware Evidence Reasoning and Stance-Aware Aggregation Model). A coherence-based topic attention is developed to model the topical consistency between a claim and each piece of evidence and the topical coherence among evidence built on the sentence-level topical representations. In addition, a semantictopic co-attention is created to measure the coherence between the global topical information and the semantic representation of the claim and evidence. Moreover, the capsule network is incorporated to model the implicit stances of evidence toward the claim by the dynamic routing mechanism.\nThe main contributions are listed as follows:\n• We propose a novel topic-aware evidence reasoning and stance-aware aggregation approach, which is, to our best knowledge, the first attempt of jointly exploiting semantic interaction and topical consistency to learn latent evidence representation for fact verification.\n• We incorporate the capsule network structure into our proposed model to capture the implicit stance relations between the claim and the evidence.\n• We conduct extensive experiments on the two benchmark datasets to demonstrate the effectiveness of TARSA for fact verification."
    }, {
      "heading" : "2 Related Work",
      "text" : "In general, fact verification is a task to assess the authenticity of a claim backed by a validated corpus of documents, which can be divided into two stages: fact extraction and claim verification (Zhou and Zafarani, 2020). Fact extraction can be further split into the document retrieval phase and the evidence selection phase to shrink the search space of evidence (Thorne et al., 2018). In the document retrieval phase, researchers typically reuse the top performing approaches in the FEVER1.0 challenge to extract the documents with high relevance for a given claim (Hanselowski et al., 2018b; Yoneda et al., 2018; Nie et al., 2019a). In the evidence selection phase, to select relevant sentences, researchers generally train the classification models or rank models based on the similarity between the claim and each sentence from the retrieved documents (Chen et al., 2017; Stammbach and Neu-\nmann, 2019; Soleimani et al., 2020; Wadden et al., 2020; Zhong et al., 2020; Zhou et al., 2019a).\nMany fact verification approaches focus on the claim verification stage, which can be addressed by natural language inference methods (Parikh et al., 2016; Ghaeini et al., 2018; Luken et al., 2018). Typically, these approaches contain the representation learning process and evidence aggregation process. Hanselowski et al. (2018b) and Nie et al. (2019a) concatenate all pieces of evidence as input and use the max pooling to aggregate the information for claim verification via the enhanced sequential inference model (ESIM) (Chen et al., 2017). In a similar vein, Yin and Roth (2018) incorporate the identification of evidence to further improve claim verification using ESIM with different granularity levels. Ma et al. (2019) leverage the co-attention mechanism between claim and evidence to generate claim-specific evidence representations which are used to infer the claim.\nBenefiting from the development of pre-trained language models, Zhou et al. (2019a) are the first to learn evidence representations by BERT (Devlin et al., 2019), which are subsequently used in a constructed evidence graph for claim inference by aggregating all claim-evidence pairs. Zhong et al. (2020) further establish a semantic-based graph for representation and aggregation with XLNet (Yang et al., 2019). Liu et al. (2020) incorporate two sets of kernels into a sentence-level graph to learn a more fine-grained evidence representations. Subramanian and Lee (2020) further incorporate evidence set retrieval and hierarchical attention sum block to improve the performance of claim verification.\nDifferent from all previous approaches, our work for the first time handles the fact verification task by considering the topical consistency and the semantic interactions between claim and evidence. Moreover, we employ the capsule network to model the implicit stance relations of evidence toward the claim."
    }, {
      "heading" : "3 Method",
      "text" : "In this section, we present an overview of the architecture of the proposed framework TARSA for fact verification. As shown in Figure 2, our approach consists of three main layers: 1) the representation layer to embed claim and evidence into three types of representations by a semantic encoder and a topic encoder; 2) the coherence layer to incorpo-\nrate the topic information into our model by two attention components; 3) the aggregation layer to model the implicit stances of evidence toward claim using the capsule network."
    }, {
      "heading" : "3.1 Representation Layer",
      "text" : "This section describes how TARSA extracts semantic representations, sentence-level topic representations, and global topic information through a semantic encoder and a topic encoder separately.\nSemantic Encoder The semantic encoder in TARSA is a vanilla transformer (Vaswani et al., 2017) with the eXtra hop attention (Zhao et al., 2020). For each claim c paired with N pieces of retrieved evidence sentences E = {e1, e2, · · · , eN}, TARSA constructs the evidence graph by treating each evidence-claim pair xi = (ei, c) as a node (i.e., xi = [ [CLS]; ei; [SEP ]; c; [SEP ] ] ) and build a fully-connected evidence graph G. We also add a self-loop to every node to perform message propagation from itself.\nSpecifically, we first apply the vanilla transformer on each node to generate the claimdependent evidence representation using the input xi,\nhi = Transformer(xi) (1)\nwhere i denotes the i-th node in G. We treat the first token representation hi,0 as the local context of node i.\nThen the eXtra hop attention takes the [CLS] token in each node as a “hub token”, which is to attend on hub tokens of all other connected nodes to learn the global context. One layer of eXtra hop attention can be viewed as a single-hop message propagation among all the nodes along the edges,\nĥi,0 = ∑\nj;ei,j=1\nsoftmaxj( q̂Ti,0 · k̂j,0√\ndk ) · ν̂j,0 (2)\nwhere ei,j = 1 denotes that there is an edge between the node i and the node j, q̂i,0 denotes the query vector of the [CLS] token of node i, k̂j,0 and ν̂j,0 denote the key vector and the value vector of the [CLS] token of node j, respectively, and √ dk denotes the scaling factor. The local context and the global context are concatenated to learn the semantic representation of all the nodes:\nh̃i,0 = Linear([hi,0; ĥi,0]), h̃i,τ = hi,τ ;∀τ 6= 0. (3)\nBy stacking L layers of the transformer with the eXtra hop attention which takes the semantic representation of the previous layer as input, we learn the semantic representation of evidenceH = [h̃1, h̃2, · · · , h̃N ] ∈ RN×d from the graph G.\nTopic Encoder We extract topics in the following two forms via latent Dirichlet allocation (LDA) (Blei et al., 2003): Sentence-level topic representation: Given a claim c and N pieces of the retrieved evidence E, we extract latent topic distribution t ∈ RK for each sentence as the sentence-level topic representation, where K is the number of topics. More concretely, we denote tc ∈ RK for claim c and tei ∈ RK for evidence ei. Each scalar value tk denotes the contribution of topic k in representing the claim or evidence. Global topic information: We extract global topic information P = [p1,p2, · · · ,pK ] ∈ RK×V from the topic-word distribution by treating each sentence (i.e., claim or evidence) in corpus D as a document, where V denotes the vocabulary size."
    }, {
      "heading" : "3.2 Coherence Layer",
      "text" : "This section describes how to incorporate the topic information into our model with two attention components.\nCoherence-based Topic Attention Based on the observation as illustrated in Figure 1, we as-\nsume that given a claim, the sentences used as evidence should be topically coherent with each other and the claim should be topically consistent with the relevant evidence. Therefore, two kinds of topical relationship are considered: 1) topical coherence among multiple pieces of evidence (TCee); 2) topical consistency between the claim and each evidence (TCce).\nSpecifically, to incorporate the topical coherence among multiple pieces of evidence into our model, we disregard the order of evidence and treat each evidence independently. Then we utilize the multihead attention (Vaswani et al., 2017) without position embedding to generate the new topic representation of evidence t̂e based on the sentence-level topic representation te ∈ RN×K of the retrieved evidence for a given claim.\nt̂e = multihead(te) (4)\nMoreover, we utilize the co-attention mechanism (Chen and Li, 2020) to weigh each evidence based on the topic consistency between the claim and the evidence. Given the sentence-level topic representation tc for claim and te for the corresponding evidence, the co-attention attends to the claim and the evidence simultaneously. We first compute the proximity matrix F ∈ RN ,\nF = tanh(tcWlt T e ), (5)\nwhereWl ∈ RK×K is the learnable weight matrix. The proximity matrix can be viewed as a transformation from the claim attention space to the evidence attention space. Then we can predict the interaction attention by treating F as the feature,\nHe = tanh ( Wet T e + (Wct T c )F ) , (6)\nwhere We, Wc ∈ Rl×K are the learnable weight matrices. Finally we can generate a topic similarity score between the claim and each evidence using the softmax function,\nαe = softmax(wHe), (7)\nwherew ∈ R1×l is the learnable weight, αe ∈ RN is the attention score of each piece of evidence for the claim. Eventually, the topic representation A ∈ RN×K can be computed as follows,\nA = αe t̂e, (8)\nwhere is the dot product operation.\nSemantic-Topic Co-attention We weigh each piece of evidence ei to indicate the importance of the evidence and infer the claim based on the coherence between the semantic representation and the global topic information via the co-attention mechanism, which is similar to the coherence-based topic attention in Section 3.2. More concretely, takingH and P as input, we compute the proximity matrix F ∈ RK×N to transform the topic attention space to the semantic attention space by Eq. (5). As a result, the attention weights βe ∈ RN of evidence can be obtained by Eq. (6) and (7). Eventually, the semantic representation S ∈ RN×d can be updated via S = βe H ."
    }, {
      "heading" : "3.3 Aggregation Layer",
      "text" : "To model the implicit stances of evidence toward claim, we incorporate the capsule network (Sabour et al., 2017) into our model. As illustrated in Figure 2, we concatenate both the semantic representation S and the topical representationA to form the low-level evidence capsules ui = [ai; si]|Ni=1 ∈ Rde . Let oj |Mj=1 ∈ Rdo denote the high-level class capsules, where M denotes the number of classes. The capsule network models the relationship between the evidence capsules and the class capsules by the dynamic routing mechanism (Yang et al., 2018), which can be viewed as the implicit stances of each evidence toward three classes.\nFormally, let uj|i be the predicted vector from the evidence capsule ui to the class capsule oj ,\nuj|i =Wj,iui (9)\nwhere Wj,i ∈ Rdo×de denotes the transformation matrix from the evidence capsule ui to the class capsule oj . Each class capsule aggregates all of the evidence capsules by a weighted summation over all corresponding predicted vectors:\noj = g( N∑ i=1 γjiuj|i), p̂ji = |ui|, (10)\nwhere g is a non-linear squashing function which limits the length of oj to [0, 1], γji is the coupling coefficient that determines the probability that the evidence capsule ui should be coupled with the class capsule oj . The coupling coefficient is calculated by the unsupervised and iterative dynamic routing algorithm on original logits bji, which is summarized in Algorithm 1. We can easily classify the claim by choosing the class capsule with the largest ρj via the capsule loss (Sabour et al., 2017). Moreover, the cross entropy loss is applied on the evidence capsules to identify whether the evidence is the ground truth evidence.\nAlgorithm 1 Dynamic Routing Algorithm Procedure: Routing(uj|i, p̂ji) 1: Initialize the logit of coupling coefficient bij == 0; 2: for each iteration do 3: For all evidence capsule ui and class oj :\nγji = p̂ji · leaky softmax(bji) 4: Update all the class capsules via Eq. (10); 5: For all evidence capsule ui and the class oj : bji = bji + uj|i · oj 6: end for 7: Return o ∈ RM×do , ρj = |oj |j=1:M"
    }, {
      "heading" : "4 Experimental Setting",
      "text" : "This section describes the datasets, evaluation metrics, baselines, and implementation details in our experiments.\nDatasets We conduct experiments on two public fact checking datasets: (1) FEVER (Thorne et al., 2018) is a large-scale dataset consisting of 185,455 claims along with 5,416,537 Wikipedia pages from the June 2017 Wikipedia dump. The ground truth evidence and the label (i.e., “SUPPORTS”, “REFUTES” and “NOT ENOUGH INFO (NEI)”) are also available except in the test set. (2) UKP Snopes (Hanselowski et al., 2019) is a\nmixed-domain dataset along with 16,508 Snopes pages. To maintain the consistency of two datasets, we merge the verdicts {false, mostly false}, {true, mostly true}, {mixture, unproven, undetermined} as “REFUTES”,“SUPPORTS” and “NEI”, respectively. And we omit all other labels (i.e., legent, outdated, and miscaptioned) as these instances are difficult to distinguish. Table 1 presents the statistics of the two datasets.\nEvaluation Metrics The official evaluation metrics1 for the FEVER dataset are Label Accuracy (LA) and FEVER score (F-score). LA measures the accuracy of the predicted label ŷi matching the ground truth label yi without considering the retrieved evidence. The FEVER score labels a prediction as correct if the predicted label ŷi is correct and the retrieved evidence matches at least one gold-standard evidence, which is a better indicator to reflect the inference capability of the model. We use precision, recall, and macro F1 on UKP Snopes to evaluate the performance.\nBaselines The following approaches are employed as the baselines, including three top performing models on FEVER1.0 shared task (UKP Athene (Hanselowski et al., 2018b), UCL MRG (Yoneda et al., 2018) and UNC NLP (Nie et al., 2019a)), HAN (Ma et al., 2019), BERT-based models (SR-MRS (Nie et al., 2019b), BERT Concat (Soleimani et al., 2020) and HESM (Subramanian and Lee, 2020)), and graph-based models (GEAR (Zhou et al., 2019a), TransformerXH (Zhao et al., 2020), KGAT (Liu et al., 2020) and DREAM (Zhong et al., 2020)).\nImplementation Details We describe our implementation details in this section.\nDocument retrieval takes a claim along with a collection of documents as the input, then returns N most relevant documents. For the FEVER dataset, following Hanselowski et al. (2018a), we adopt the entity linking method since the title of a Wikipedia page can be viewed as an entity and can be linked easily with the extracted entities from\n1https://github.com/sheffieldnlp/fever-scorer\nthe claim. For the UKP Snopes dataset, following Hanselowski et al. (2019), we adopt the tf-idf method where the tf-idf similarity between claim and concatenation of all sentences of each Snopes page is computed, and then the 5 highest ranked documents are taken as retrieved documents.\nEvidence selection retrieves the related sentences from retrieved documents in ranking setting. For the FEVER dataset, we follow the previous method from Zhao et al. (2020). Taking the concatenation of claim and each sentence as input, the [CLS] token representation is learned through BERT which is then used to learn a ranking score through a linear layer. The hinge loss is used to optimize the BERT model. For the UKP Snopes dataset, we adopt the tf-idf method from Hanselowski et al. (2019), which achieves the best precision.\nClaim verification. During the training phase, each claim is paired with 5 pieces of evidence, we set the batch size to 1 and the accumulate step to 8, the layer L is 3, the head number is 5, the l is 100, the number of class capsules M is 3, the dimension of class capsules do is 10, the topic number K ranges from 25 to 100. In our implementation, the maximum length of each claim-evidence pair is 130 for both datasets."
    }, {
      "heading" : "5 Experimental Results",
      "text" : "In this section, we evaluate our TARSA model in different aspects. Firstly, we compare the overall performance between our model and the baselines. Then we conduct an ablation study to explore the effectiveness of the topic information and the capsule network structure. Finally, we also explore the advantages of our model in single-hop and multihop reasoning scenarios."
    }, {
      "heading" : "5.1 Overall Performance",
      "text" : "Table 2 and Table 3 report the overall performance of our model against the baselines for the FEVER dataset and the UKP Snopes dataset 2. As shown in Table 2, our model significantly outperforms BERTbased models on both development and test sets. However, compared with the graph-based models,\n2Note that we did not compare HESM, SR-MES and DREAM with our model on the UKP Snopes dataset for the following reasons. HESM requires hyperlinks to construct the evidence set, which are not available in UKP Snopes; SRMRS concatenates query and context as the input to BERT, which is similar to the BERT Concat model; The composition of a claim in the UKP Snopes is more complicated than FEVER, which is more difficult for DERAM to construct a graph at the semantic level.\nTARSA outperforms previous systems, GEAR and KGAT, except DREAM for LA on the test set. One possible reason is that DREAM constructs an evidence graph based on the semantic roles of claim and evidence, which leverages an explicit graph-level semantic structure built from semantic roles extracted by Semantic Role Labeling (Shi and Lin, 2019) in a fine-grained setting. Nevertheless, TARSA shows superior performance than DREAM on the FEVER score, which is a more desirable indicator to demonstrate the reasoning capability of the model. As shown in Table 3, TARSA performs the best compared with all previous approaches on the UKP Snopes dataset."
    }, {
      "heading" : "5.2 Effect of Topic Number",
      "text" : "Table 4 shows the results of our TARSA model with different number of topics on the development\nset of FEVER and UKP Snopes. It can be observed that the optimal topic number is 25 for FEVER and 50 for UKP Snopes. One possible reason is that UKP Snopes is retrieved from multiple domains which includes more diverse categories than those of FEVER."
    }, {
      "heading" : "5.3 Ablation Study",
      "text" : "To further illustrate the effectiveness of the topic information and the capsule-level aggregation modeling, we perform an ablation study on the development set of FEVER.\nEffect of Topic Information: We first explore how the model performance is impacted by the removal of various topic components. The first six rows in Table 5 present the label accuracy (LA) and the FEVER score on the development set of FEVER after removing various components, where STI denotes the semantic-topic information in Section 3.2, TCee denotes the topical coherence among multiple pieces of evidence, TCce denotes the topical consistency between the claim and each piece of evidence. As expected, LA and the FEVER score decrease consistently with a gradual removal of various components, which demonstrates the effectiveness of incorporating topic information in three aspects. We find that after all modules are removed, the performance of TARSA is still nearly 2% higher than our base model, Transformer-XH, due to the use of the capsule network in TARSA.\nEffect of Capsule-level Aggregation: We explore the effectiveness of the capsule-level aggregation by comparing it with four different aggregation methods. The last four rows in Table 5 show the results of aggregation analysis in the development set on FEVER. The max pooling, sum, and mean aggregation consider the learned representations of evidence as a single matrix, then apply a linear layer to classify the input claim as SUPPORTS, REFUTES, or NEI. The attention-based aggrega-\ntion method is used in Zhou et al. (2019a), where the dot-product attention is computed between the claim and each evidence to weigh them differently. Finally, our TARSA model aggregates the information of all pieces of evidence using the capsule network, which connects the evidence capsules to the class capsules in a clustered way. From the results, our model outperforms all other aggregation methods."
    }, {
      "heading" : "5.4 Performance on Different Scenarios",
      "text" : "Table 6 presents the performance of our model on single-hop and multi-hop reasoning scenarios on the FEVER dataset compared with several baselines. The single-hop mainly focuses on the denoising ability of the model with the retrieved evidence, which selects the salient evidence for inference. The multi-hop mainly emphasizes the relatedness of different pieces of evidence for the joint reasoning, which is a more complex task.\nWe build the training and testing sets for both single-hop and multi-hop scenarios based on the number of gold-standard evidence of a claim. If more than one gold-standard evidence is required, then the claim would require multi-hop reasoning. The instances with the NEI label are removed because there is no gold-standard evidence matching this label. The single-hop reasoning set contains\n78,838 and 9,682 instances for training and testing, respectively, while the multi-hop reasoning set contains 30,972 and 3,650 instances for training and testing, respectively. As Table 6 shows, TARSA outperforms all other baselines on LA by at least 0.31% in the single-hop scenario and 1.09% in the multi-hop scenario, respectively, which shows a consistent improvement in both scenarios. In addition, TARSA is more effective on the multihop scenario as the capsule-level aggregation helps better aggregate the information of all pieces of evidence."
    }, {
      "heading" : "5.5 Case Study",
      "text" : "Table 7 illustrates an example from the UKP Snopes dataset which is correctly detected as REFUTES, where the topic words extracted by LDA are marked in blue. From the table we can observe: 1) the top two pieces of evidence (i.e., e1 and e2) have higher topical overlap with the claim and also with each other; 2) the lower two pieces of evidence (i.e., e4 and e5) seem less important because they are less topically relevant to the claim; 3) for e3, it is difficult to judge its relevance from either the topical or the semantic perspective, which is ambiguous for the identification of the truthfulness\nof the claim."
    }, {
      "heading" : "5.6 Error Analysis",
      "text" : "We randomly select 100 incorrectly predicted instances from FEVER and UKP Snopes datasets and categorize the main errors. The first type of errors is caused by the quality of topics extracted by LDA. This is because the average length of sentences in both datasets is much shorter after removing the low- and high-frequency tokens, which poses a challenge for LDA to extract high quality topics to match the topical consistency between a claim and each evidence. The second type of errors is due to the failure of detecting multiple entity mentions referring to the same entity. For example, the claim describes “Go Ask Alice was the real life diary of a teenager girl”, where evidence describes that “This book is a work of fiction”. The model fail to understand the relationship between diary and fiction."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have presented a novel topic-aware evidence reasoning and stance-aware aggregation model for fact verification. Our model jointly exploits the topical consistency and the semantic interaction to learn evidence representations at the sentence level. Moreover, we have proposed the use of the capsule network to model the implicit stances of evidence toward a claim for a better aggregation of information encoded in evidence. The results on two public datasets demonstrate the effectiveness of our model. In the future, we plan to explore an iterative reasoning mechanism for more efficient evidence aggregation for fact checking."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank anonymous reviewers for their valuable comments and helpful suggestions. This work was funded by the National Key Research and Development Program of China (2016YFC1306704), the National Natural Science Foundation of China (61772132), and the EPSRC (grant no. EP/T017112/1, EP/V048597/1). YH is supported by a Turing AI Fellowship funded by the UK Research and Innovation (UKRI) (grant no. EP/V020579/1)."
    } ],
    "references" : [ {
      "title" : "Latent dirichlet allocation",
      "author" : [ "David M. Blei", "Andrew Y. Ng", "Michael I. Jordan." ],
      "venue" : "the Journel of machine Learning research, 3:993–1022.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "HENIN: learning heterogeneous neural interaction networks for explainable cyberbullying detection on social media",
      "author" : [ "Hsin-Yu Chen", "Cheng-Te Li." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Chen and Li.,? 2020",
      "shortCiteRegEx" : "Chen and Li.",
      "year" : 2020
    }, {
      "title" : "Enhanced LSTM for natural language inference",
      "author" : [ "Qian Chen", "Xiaodan Zhu", "Zhen-Hua Ling", "Si Wei", "Hui Jiang", "Diana Inkpen." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1657–1668, Vancouver,",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Dr-bilstm: Dependent reading bidirectional LSTM for natural language inference",
      "author" : [ "Reza Ghaeini", "Sadid A. Hasan", "Vivek V. Datla", "Joey Liu", "Kathy Lee", "Ashequl Qadir", "Yuan Ling", "Aaditya Prakash", "Xiaoli Z. Fern", "Oladimeji Farri." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Ghaeini et al\\.,? 2018",
      "shortCiteRegEx" : "Ghaeini et al\\.",
      "year" : 2018
    }, {
      "title" : "A retrospective analysis of the fake news challenge stance-detection task",
      "author" : [ "Andreas Hanselowski", "Avinesh P.V.S.", "Benjamin Schiller", "Felix Caspelherr", "Debanjan Chaudhuri", "Christian M. Meyer", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 27th In-",
      "citeRegEx" : "Hanselowski et al\\.,? 2018a",
      "shortCiteRegEx" : "Hanselowski et al\\.",
      "year" : 2018
    }, {
      "title" : "A richly annotated corpus for different tasks in automated factchecking",
      "author" : [ "Andreas Hanselowski", "Christian Stab", "Claudia Schulz", "Zile Li", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 23rd Conference on Computational Natural Language Learning, pages",
      "citeRegEx" : "Hanselowski et al\\.,? 2019",
      "shortCiteRegEx" : "Hanselowski et al\\.",
      "year" : 2019
    }, {
      "title" : "Ukp-athene: Multisentence textual entailment for claim verification",
      "author" : [ "Andreas Hanselowski", "Hao Zhang", "Zile Li", "Daniil Sorokin", "Benjamin Schiller", "Claudia Schulz", "Iryna Gurevych." ],
      "venue" : "CoRR, abs/1809.01479.",
      "citeRegEx" : "Hanselowski et al\\.,? 2018b",
      "shortCiteRegEx" : "Hanselowski et al\\.",
      "year" : 2018
    }, {
      "title" : "Fine-grained fact verification with kernel graph attention network",
      "author" : [ "Zhenghao Liu", "Chenyan Xiong", "Maosong Sun", "Zhiyuan Liu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "QED: A fact verification system for the FEVER shared task",
      "author" : [ "Jackson Luken", "Nanjiang Jiang", "Marie-Catherine de Marneffe." ],
      "venue" : "Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), pages 156–160, Brussels, Belgium.",
      "citeRegEx" : "Luken et al\\.,? 2018",
      "shortCiteRegEx" : "Luken et al\\.",
      "year" : 2018
    }, {
      "title" : "Sentence-level evidence embedding for claim verification with hierarchical attention networks",
      "author" : [ "Jing Ma", "Wei Gao", "Shafiq R. Joty", "Kam-Fai Wong." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2561–",
      "citeRegEx" : "Ma et al\\.,? 2019",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2019
    }, {
      "title" : "Combining fact extraction and verification with neural semantic matching networks",
      "author" : [ "Yixin Nie", "Haonan Chen", "Mohit Bansal." ],
      "venue" : "Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence, pages 6859–6866, Honolulu, HI.",
      "citeRegEx" : "Nie et al\\.,? 2019a",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2019
    }, {
      "title" : "Revealing the importance of semantic retrieval for machine reading at scale",
      "author" : [ "Yixin Nie", "Songhe Wang", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint",
      "citeRegEx" : "Nie et al\\.,? 2019b",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2019
    }, {
      "title" : "A decomposable attention model for natural language inference",
      "author" : [ "Ankur P. Parikh", "Oscar Täckström", "Dipanjan Das", "Jakob Uszkoreit." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2249–2255,",
      "citeRegEx" : "Parikh et al\\.,? 2016",
      "shortCiteRegEx" : "Parikh et al\\.",
      "year" : 2016
    }, {
      "title" : "Dynamic routing between capsules",
      "author" : [ "Sara Sabour", "Nicholas Frosst", "Geoffrey E. Hinton." ],
      "venue" : "Advances in Neural Information Processing Systems 30, pages 3856–3866, Long Beach, CA.",
      "citeRegEx" : "Sabour et al\\.,? 2017",
      "shortCiteRegEx" : "Sabour et al\\.",
      "year" : 2017
    }, {
      "title" : "Simple BERT models for relation extraction and semantic role labeling",
      "author" : [ "Peng Shi", "Jimmy Lin." ],
      "venue" : "CoRR, abs/1904.05255.",
      "citeRegEx" : "Shi and Lin.,? 2019",
      "shortCiteRegEx" : "Shi and Lin.",
      "year" : 2019
    }, {
      "title" : "BERT for evidence retrieval and claim verification",
      "author" : [ "Amir Soleimani", "Christof Monz", "Marcel Worring." ],
      "venue" : "Proceedings of 42nd European Conference on Information Retrieval, pages 359–366, Lisbon, Portugal.",
      "citeRegEx" : "Soleimani et al\\.,? 2020",
      "shortCiteRegEx" : "Soleimani et al\\.",
      "year" : 2020
    }, {
      "title" : "Team domlin: Exploiting evidence enhancement for the fever shared task",
      "author" : [ "Dominik Stammbach", "Guenter Neumann." ],
      "venue" : "Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER), pages 105–109, Hong Kong, China.",
      "citeRegEx" : "Stammbach and Neumann.,? 2019",
      "shortCiteRegEx" : "Stammbach and Neumann.",
      "year" : 2019
    }, {
      "title" : "Hierarchical evidence set modeling for automated fact extraction and verification",
      "author" : [ "Shyam Subramanian", "Kyumin Lee." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7798–7809, Online.",
      "citeRegEx" : "Subramanian and Lee.,? 2020",
      "shortCiteRegEx" : "Subramanian and Lee.",
      "year" : 2020
    }, {
      "title" : "FEVER: a large-scale dataset for fact extraction and verification",
      "author" : [ "James Thorne", "Andreas Vlachos", "Christos Christodoulopoulos", "Arpit Mittal." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Thorne et al\\.,? 2018",
      "shortCiteRegEx" : "Thorne et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30, pages 5998–6008, Long Beach,",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Graph attention networks",
      "author" : [ "Petar Velickovic", "Guillem Cucurull", "Arantxa Casanova", "Adriana Romero", "Pietro Liò", "Yoshua Bengio." ],
      "venue" : "Proceedings of 6th International Conference on Learning Representations, Vancouver, Canada.",
      "citeRegEx" : "Velickovic et al\\.,? 2018",
      "shortCiteRegEx" : "Velickovic et al\\.",
      "year" : 2018
    }, {
      "title" : "The spread of true and false news online",
      "author" : [ "Soroush Vosoughi", "Deb Roy", "Sinan Aral." ],
      "venue" : "Science, 359(6380):1146–1151.",
      "citeRegEx" : "Vosoughi et al\\.,? 2018",
      "shortCiteRegEx" : "Vosoughi et al\\.",
      "year" : 2018
    }, {
      "title" : "Fact or fiction: Verifying scientific claims",
      "author" : [ "David Wadden", "Shanchuan Lin", "Kyle Lo", "Lucy Lu Wang", "Madeleine van Zuylen", "Arman Cohan", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Wadden et al\\.,? 2020",
      "shortCiteRegEx" : "Wadden et al\\.",
      "year" : 2020
    }, {
      "title" : "Investigating capsule networks with dynamic routing for text classification",
      "author" : [ "Min Yang", "Wei Zhao", "Jianbo Ye", "Zeyang Lei", "Zhou Zhao", "Soufei Zhang." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime G. Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le." ],
      "venue" : "Advances in Neural Information Processing Systems 32, pages 5754–5764,",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Twowingos: A twowing optimization strategy for evidential claim verification",
      "author" : [ "Wenpeng Yin", "Dan Roth." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 105–114, Brussels, Belgium.",
      "citeRegEx" : "Yin and Roth.,? 2018",
      "shortCiteRegEx" : "Yin and Roth.",
      "year" : 2018
    }, {
      "title" : "Ucl machine reading group: Four factor framework for fact finding (hexaf)",
      "author" : [ "Takuma Yoneda", "Jeff Mitchell", "Johannes Welbl", "Pontus Stenetorp", "Sebastian Riedel." ],
      "venue" : "Proceedings of the First Workshop on Fact Extraction and VERification (FEVER),",
      "citeRegEx" : "Yoneda et al\\.,? 2018",
      "shortCiteRegEx" : "Yoneda et al\\.",
      "year" : 2018
    }, {
      "title" : "Fake news research: Theories, detection strategies, and open problems",
      "author" : [ "Reza Zafarani", "Xinyi Zhou", "Kai Shu", "Huan Liu." ],
      "venue" : "Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 3207–",
      "citeRegEx" : "Zafarani et al\\.,? 2019",
      "shortCiteRegEx" : "Zafarani et al\\.",
      "year" : 2019
    }, {
      "title" : "Transformer-xh: Multi-evidence reasoning with extra hop attention",
      "author" : [ "Chen Zhao", "Chenyan Xiong", "Corby Rosset", "Xia Song", "Paul N. Bennett", "Saurabh Tiwary." ],
      "venue" : "Proceedings of 8th International Conference on Learning Representations, Ad-",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    }, {
      "title" : "Reasoning over semantic-level graph for fact checking",
      "author" : [ "Wanjun Zhong", "Jingjing Xu", "Duyu Tang", "Zenan Xu", "Nan Duan", "Ming Zhou", "Jiahai Wang", "Jian Yin." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Zhong et al\\.,? 2020",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2020
    }, {
      "title" : "GEAR: graph-based evidence aggregating and reasoning for fact verification",
      "author" : [ "Jie Zhou", "Xu Han", "Cheng Yang", "Zhiyuan Liu", "Lifeng Wang", "Changcheng Li", "Maosong Sun." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Zhou et al\\.,? 2019a",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2019
    }, {
      "title" : "A survey of fake news: Fundamental theories, detection methods, and opportunities",
      "author" : [ "Xinyi Zhou", "Reza Zafarani." ],
      "venue" : "ACM Computing Surveys, 53(5):109:1–109:40.",
      "citeRegEx" : "Zhou and Zafarani.,? 2020",
      "shortCiteRegEx" : "Zhou and Zafarani.",
      "year" : 2020
    }, {
      "title" : "Fake news: Fundamental theories, detection strategies and challenges",
      "author" : [ "Xinyi Zhou", "Reza Zafarani", "Kai Shu", "Huan Liu." ],
      "venue" : "Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, pages 836–837, Mel-",
      "citeRegEx" : "Zhou et al\\.,? 2019b",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "To solve this problem, we focus on the fact verification task (Thorne et al., 2018), which aims to automatically evaluate the veracity of a given claim based on the textual evidence retrieved from external sources.",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 10,
      "context" : "Recent approaches for fact verification are dominated by natural language inference models (Angeli and Manning, 2014) or textual entailment recognition models (Ma et al., 2019), where the truthfulness of a claim is verified via reasoning and aggregating over multiple pieces of retrieved evidence.",
      "startOffset" : 159,
      "endOffset" : 176
    }, {
      "referenceID" : 5,
      "context" : "In general, existing models follow an architecture with two main sub-modules: the semantic interaction module and the entailment-based aggregation module (Hanselowski et al., 2018a; Nie et al., 2019a; Soleimani et al., 2020; Liu et al., 2020).",
      "startOffset" : 154,
      "endOffset" : 242
    }, {
      "referenceID" : 11,
      "context" : "In general, existing models follow an architecture with two main sub-modules: the semantic interaction module and the entailment-based aggregation module (Hanselowski et al., 2018a; Nie et al., 2019a; Soleimani et al., 2020; Liu et al., 2020).",
      "startOffset" : 154,
      "endOffset" : 242
    }, {
      "referenceID" : 16,
      "context" : "In general, existing models follow an architecture with two main sub-modules: the semantic interaction module and the entailment-based aggregation module (Hanselowski et al., 2018a; Nie et al., 2019a; Soleimani et al., 2020; Liu et al., 2020).",
      "startOffset" : 154,
      "endOffset" : 242
    }, {
      "referenceID" : 8,
      "context" : "In general, existing models follow an architecture with two main sub-modules: the semantic interaction module and the entailment-based aggregation module (Hanselowski et al., 2018a; Nie et al., 2019a; Soleimani et al., 2020; Liu et al., 2020).",
      "startOffset" : 154,
      "endOffset" : 242
    }, {
      "referenceID" : 10,
      "context" : "The semantic interaction module attempts to grasp the rich semantic-level interactions among multiple pieces of evidence at the sentence-level (Ma et al., 2019; Zhou et al., 2019a; Subramanian and Lee, 2020) or the semantic roles-level (Zhong et al.",
      "startOffset" : 143,
      "endOffset" : 207
    }, {
      "referenceID" : 31,
      "context" : "The semantic interaction module attempts to grasp the rich semantic-level interactions among multiple pieces of evidence at the sentence-level (Ma et al., 2019; Zhou et al., 2019a; Subramanian and Lee, 2020) or the semantic roles-level (Zhong et al.",
      "startOffset" : 143,
      "endOffset" : 207
    }, {
      "referenceID" : 18,
      "context" : "The semantic interaction module attempts to grasp the rich semantic-level interactions among multiple pieces of evidence at the sentence-level (Ma et al., 2019; Zhou et al., 2019a; Subramanian and Lee, 2020) or the semantic roles-level (Zhong et al.",
      "startOffset" : 143,
      "endOffset" : 207
    }, {
      "referenceID" : 30,
      "context" : ", 2019a; Subramanian and Lee, 2020) or the semantic roles-level (Zhong et al., 2020).",
      "startOffset" : 64,
      "endOffset" : 84
    }, {
      "referenceID" : 3,
      "context" : "However, the aforementioned approaches typically learn the representation of each evidenceclaim pair from the semantic perspective such as obtaining the semantic representation of each evidence-claim pair through pre-trained language models (Devlin et al., 2019) or graph-based models (Velickovic et al.",
      "startOffset" : 241,
      "endOffset" : 262
    }, {
      "referenceID" : 21,
      "context" : ", 2019) or graph-based models (Velickovic et al., 2018), which largely overlooked the topical consistency between claim and evidence.",
      "startOffset" : 30,
      "endOffset" : 55
    }, {
      "referenceID" : 32,
      "context" : "In general, fact verification is a task to assess the authenticity of a claim backed by a validated corpus of documents, which can be divided into two stages: fact extraction and claim verification (Zhou and Zafarani, 2020).",
      "startOffset" : 198,
      "endOffset" : 223
    }, {
      "referenceID" : 19,
      "context" : "Fact extraction can be further split into the document retrieval phase and the evidence selection phase to shrink the search space of evidence (Thorne et al., 2018).",
      "startOffset" : 143,
      "endOffset" : 164
    }, {
      "referenceID" : 7,
      "context" : "0 challenge to extract the documents with high relevance for a given claim (Hanselowski et al., 2018b; Yoneda et al., 2018; Nie et al., 2019a).",
      "startOffset" : 75,
      "endOffset" : 142
    }, {
      "referenceID" : 27,
      "context" : "0 challenge to extract the documents with high relevance for a given claim (Hanselowski et al., 2018b; Yoneda et al., 2018; Nie et al., 2019a).",
      "startOffset" : 75,
      "endOffset" : 142
    }, {
      "referenceID" : 11,
      "context" : "0 challenge to extract the documents with high relevance for a given claim (Hanselowski et al., 2018b; Yoneda et al., 2018; Nie et al., 2019a).",
      "startOffset" : 75,
      "endOffset" : 142
    }, {
      "referenceID" : 13,
      "context" : "Many fact verification approaches focus on the claim verification stage, which can be addressed by natural language inference methods (Parikh et al., 2016; Ghaeini et al., 2018; Luken et al., 2018).",
      "startOffset" : 134,
      "endOffset" : 197
    }, {
      "referenceID" : 4,
      "context" : "Many fact verification approaches focus on the claim verification stage, which can be addressed by natural language inference methods (Parikh et al., 2016; Ghaeini et al., 2018; Luken et al., 2018).",
      "startOffset" : 134,
      "endOffset" : 197
    }, {
      "referenceID" : 9,
      "context" : "Many fact verification approaches focus on the claim verification stage, which can be addressed by natural language inference methods (Parikh et al., 2016; Ghaeini et al., 2018; Luken et al., 2018).",
      "startOffset" : 134,
      "endOffset" : 197
    }, {
      "referenceID" : 2,
      "context" : "(2019a) concatenate all pieces of evidence as input and use the max pooling to aggregate the information for claim verification via the enhanced sequential inference model (ESIM) (Chen et al., 2017).",
      "startOffset" : 179,
      "endOffset" : 198
    }, {
      "referenceID" : 3,
      "context" : "(2019a) are the first to learn evidence representations by BERT (Devlin et al., 2019), which are subsequently used in a constructed evidence graph for claim inference by aggregating all claim-evidence pairs.",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 25,
      "context" : "(2020) further establish a semantic-based graph for representation and aggregation with XLNet (Yang et al., 2019).",
      "startOffset" : 94,
      "endOffset" : 113
    }, {
      "referenceID" : 20,
      "context" : "Semantic Encoder The semantic encoder in TARSA is a vanilla transformer (Vaswani et al., 2017) with the eXtra hop attention (Zhao et al.",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 29,
      "context" : ", 2017) with the eXtra hop attention (Zhao et al., 2020).",
      "startOffset" : 37,
      "endOffset" : 56
    }, {
      "referenceID" : 0,
      "context" : "Topic Encoder We extract topics in the following two forms via latent Dirichlet allocation (LDA) (Blei et al., 2003): Sentence-level topic representation: Given a claim c and N pieces of the retrieved evidence E, we extract latent topic distribution t ∈ RK for each sentence as the sentence-level topic representation, where K is the number of topics.",
      "startOffset" : 97,
      "endOffset" : 116
    }, {
      "referenceID" : 20,
      "context" : "Then we utilize the multihead attention (Vaswani et al., 2017) without position embedding to generate the new topic representation of evidence t̂e based on the sentence-level topic representation te ∈ RN×K of the retrieved evidence for a given claim.",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 1,
      "context" : "Moreover, we utilize the co-attention mechanism (Chen and Li, 2020) to weigh each evidence based on the topic consistency between the claim and the evidence.",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 14,
      "context" : "To model the implicit stances of evidence toward claim, we incorporate the capsule network (Sabour et al., 2017) into our model.",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 24,
      "context" : "The capsule network models the relationship between the evidence capsules and the class capsules by the dynamic routing mechanism (Yang et al., 2018), which can be viewed as the implicit stances of each evidence toward three classes.",
      "startOffset" : 130,
      "endOffset" : 149
    }, {
      "referenceID" : 14,
      "context" : "We can easily classify the claim by choosing the class capsule with the largest ρj via the capsule loss (Sabour et al., 2017).",
      "startOffset" : 104,
      "endOffset" : 125
    }, {
      "referenceID" : 19,
      "context" : "Datasets We conduct experiments on two public fact checking datasets: (1) FEVER (Thorne et al., 2018) is a large-scale dataset consisting of 185,455 claims along with 5,416,537 Wikipedia pages from the June 2017 Wikipedia dump.",
      "startOffset" : 80,
      "endOffset" : 101
    }, {
      "referenceID" : 7,
      "context" : "0 shared task (UKP Athene (Hanselowski et al., 2018b), UCL MRG (Yoneda et al.",
      "startOffset" : 26,
      "endOffset" : 53
    }, {
      "referenceID" : 27,
      "context" : ", 2018b), UCL MRG (Yoneda et al., 2018) and UNC NLP (Nie et al.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 11,
      "context" : ", 2018) and UNC NLP (Nie et al., 2019a)), HAN (Ma et al.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 10,
      "context" : ", 2019a)), HAN (Ma et al., 2019), BERT-based models (SR-MRS (Nie et al.",
      "startOffset" : 15,
      "endOffset" : 32
    }, {
      "referenceID" : 12,
      "context" : ", 2019), BERT-based models (SR-MRS (Nie et al., 2019b), BERT Concat (Soleimani et al.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 16,
      "context" : ", 2019b), BERT Concat (Soleimani et al., 2020) and HESM (Subramanian and Lee, 2020)), and graph-based models (GEAR (Zhou et al.",
      "startOffset" : 22,
      "endOffset" : 46
    }, {
      "referenceID" : 18,
      "context" : ", 2020) and HESM (Subramanian and Lee, 2020)), and graph-based models (GEAR (Zhou et al.",
      "startOffset" : 17,
      "endOffset" : 44
    }, {
      "referenceID" : 31,
      "context" : ", 2020) and HESM (Subramanian and Lee, 2020)), and graph-based models (GEAR (Zhou et al., 2019a), TransformerXH (Zhao et al.",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 29,
      "context" : ", 2019a), TransformerXH (Zhao et al., 2020), KGAT (Liu et al.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 8,
      "context" : ", 2020), KGAT (Liu et al., 2020) and DREAM (Zhong et al.",
      "startOffset" : 14,
      "endOffset" : 32
    }, {
      "referenceID" : 15,
      "context" : "One possible reason is that DREAM constructs an evidence graph based on the semantic roles of claim and evidence, which leverages an explicit graph-level semantic structure built from semantic roles extracted by Semantic Role Labeling (Shi and Lin, 2019) in a fine-grained setting.",
      "startOffset" : 235,
      "endOffset" : 254
    } ],
    "year" : 2021,
    "abstractText" : "Fact verification is a challenging task that requires simultaneously reasoning and aggregating over multiple retrieved pieces of evidence to evaluate the truthfulness of a claim. Existing approaches typically (i) explore the semantic interaction between the claim and evidence at different granularity levels but fail to capture their topical consistency during the reasoning process, which we believe is crucial for verification; (ii) aggregate multiple pieces of evidence equally without considering their implicit stances to the claim, thereby introducing spurious information. To alleviate the above issues, we propose a novel topicaware evidence reasoning and stance-aware aggregation model for more accurate fact verification, with the following four key properties: 1) checking topical consistency between the claim and evidence; 2) maintaining topical coherence among multiple pieces of evidence; 3) ensuring semantic similarity between the global topic information and the semantic representation of evidence; 4) aggregating evidence based on their implicit stances to the claim. Extensive experiments conducted on the two benchmark datasets demonstrate the superiority of the proposed model over several state-of-the-art approaches for fact verification. The source code can be obtained from https://github.com/jasenchn/TARSA.",
    "creator" : "LaTeX with hyperref"
  }
}