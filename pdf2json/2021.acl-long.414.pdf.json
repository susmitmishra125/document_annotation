{
  "name" : "2021.acl-long.414.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Language Model Evaluation Beyond Perplexity",
    "authors" : [ "Clara Meister", "Ryan Cotterell" ],
    "emails" : [ "first.last@inf.ethz.ch" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5328–5339\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5328"
    }, {
      "heading" : "1 Introduction",
      "text" : "Neural language models1 have become shockingly good at modeling natural language data in recent years (Merity et al., 2017; Conneau and Lample, 2019; Radford et al., 2019). Thus, to test just how well neural language models capture language NLP researchers have started to look beyond standard evaluation metrics such as perplexity, endeavoring to understand which underlying attributes of human language these models are learning. To this end, a nascent literature has emerged that focuses on probing language models (Belinkov\n1In this work, we do not use the term language model to refer to cloze language models such as BERT (Devlin et al., 2019), which do not give us a distribution over strings.\nand Glass, 2019), i.e., determining whether models encode linguistic phenomena. For the most part, these works have been limited to analyses of sentence-level phenomenon, such as subject–verb agreement (Gulordava et al., 2018) and garden path effects (van Schijndel and Linzen, 2018) among a myriad of other properties (Blevins et al., 2018; Chowdhury and Zamparelli, 2018, inter alia).\nIn this work, we attempt to understand which macro-level phenomena of human language today’s language models reflect. That is, we pose the question: Do neural language models exhibit the statistical tendencies of human language? Phenomena that can be measured at this level provide an alternate view of a model’s comprehension; for example, rather than exploring whether morphological agreement is captured, we look at whether our models learn the trends across a corpus as a whole, e.g., the token rank–frequency (Zipf’s) relationship. In comparison to standard probing techniques, this framework does not require we know a priori how linguistic phenomena should manifest themselves. That is, when there is no law stating the theoretical tendencies of an attribute of natural language or we have reason to believe our language domain does not follow such a law, we can use the\nstatistical tendencies present in empirical data as our baseline. This characteristic both allows us to assess a model’s fit to highly corpus-dependent distributions—like the length distribution—and mitigates the biases introduced by our own preconceptions regarding properties of natural language.2\nMore concretely, our paper describes an experimental design and accompanying hypothesis tests to determine precisely whether text generated from language models follows the same empirical trends as human language. Our experiments reveal that adherence to natural language tendencies varies widely with both model architecture and generation strategy, e.g., Fig. 1 shows varying degrees of adherence to the empirical type–token relationship, an artifact that perplexity alone could not reveal. Our findings suggest this framework is a valuable tool for gaining a deeper understanding of where today’s language models are succeeding and failing at capturing human language."
    }, {
      "heading" : "2 Language Models",
      "text" : "Language models are probability distributions over natural language sentences. We define the support of a language model p✓ with parameters ✓ as\nY := {BOS v EOS | v 2 V⇤} (1)\nwhere V is the model’s vocabulary and tokens EOS and BOS demarcate the beginning and end of a string, respectively, and V⇤ is the Kleene closure of V . In this paper, we term vocabularies consisting of words closed and those consisting of BPE tokens (Sennrich et al., 2016) open.\nIn the case when p✓ is locally normalized, which is the predominant case for language models, p✓ is defined as the product of probability distributions:\np✓(y) =\n|y|Y\nt=1\np✓(yt | y<t) (2)\nwhere each p✓(· |y<t) is a distribution with support over V̄ := V[{EOS} and y<1 = y0 := BOS. To estimate model parameters ✓, one typically optimizes the log-likelihood function over a corpus Ctrain:\nL(✓ | Ctrain) = X\ny2Ctrain\nlog p✓(y) (3)\nwhere we call each string y a document. To determine the goodness of fit of a model to the\n2Such biases are naturally introduced by many probing techniques that e.g., draw conclusions from carefully constructed challenge tasks.\nempirical distribution (defined by Ctrain), it is standard practice to measure perplexity on a held-out dataset, which is simply a monotonic function of average (per token) log-likelihood under that model. While low perplexity on an evaluation set undoubtedly reflects some level of fit to natural language, it does not give us a fine-grained view of which linguistic attributes a model has learned."
    }, {
      "heading" : "3 Statistical Tendencies of Language",
      "text" : "Human languages are thought to exhibit statistical tendencies, several of which are explicitly quantified by laws (Altmann and Gerlach, 2016). In this section, we review a subset of these distributions– both with and without well-established forms— over which we subsequently perform analyses."
    }, {
      "heading" : "3.1 Classical Laws",
      "text" : "Rank–Frequency. Zipf’s law (1949), otherwise known as the rank–frequency law, states that the frequency of a word in a corpus decays exponentially in the frequency rank of that word, i.e., the frequency !(·) of the kth most frequent word wk follows the power-law distribution: !(wk) / k s. When fit to natural language text, the free parameter s is typically close to 1. Zipf’s law also has a probabilistic interpretation: the marginal probability that a random word in our corpus takes on the value of thekth most frequent can be expressed as\npzipf(W = wk) = 1 ⇣(s) k s (4)\nwhere ⇣(s) = 1/ P1\nk=1 k s is the normalizing\nconstant of our probability mass function (pmf). The adherence of language to Zipf’s law has been widely studied and is considered one of the canonical laws of quantitative linguistics (Baroni, 2009; Li et al., 2010; Moreno-Sánchez et al., 2016).\nEstimating s from an observed set of rank– frequency pairs can be done using standard estimation techniques. Here we use the maximumlikelihood estimate3 (MLE), employing numerical optimization to solve for s since the MLE of the discrete power law lacks a closed form solution.\nType–Token. Heaps’ law (Herdan, 1960), also known as the type–token relationship, states that\n3Derivation in App. A. We may also estimate s using, e.g., least squares over the original or log–log transform of our distribution. However, it has been empirically observed that least-squares estimates under this paradigm are not reliable (Clauset et al., 2009) and further, directly incorporate assumptions that contradict power law behavior (Schluter, 2020).\nthe number of additional unique tokens (i.e., number of types) in a document diminishes as its length increases. Formally, we can express the expected number of types u(·) as a function of the length l(·) of the string y via the relationship u(y) / l(y) where < 1 is a free parameter. Types may be, e.g., unigrams or bigrams.\nThe above formulation of Heaps’ law lacks an obvious probabilistic interpretation. However, if we frame Heaps’ law as modeling the expected value of the number of types for any given length document, then we can model the relation as a Poisson process, where the marginal distribution over document length follows Heaps’ proposed power law. Specifically, we model the number of types for a document of a given length as a non-homogeneous Poisson process (NHPP; Ross, 1996) where our rate parameter (l(y)) is Heaps’ power law relation. The probability that there are k types in a document of length t is then\npheaps(u(yt) = k) = (t)k\nk! exp( (t)) (5)\nfor (l(y)) = ↵ · l(y) . Similarly to Eq. (4), we can fit parameters ↵, using MLE (see App. A)."
    }, {
      "heading" : "3.2 Other Tendencies",
      "text" : "Natural language has other quantifiable distributions, e.g., over document length or unigrams. While there may not exist well-established laws for the behavior of these (often highly corpusdependent) distributions, we can observe their empirical distributions w.r.t. a corpus. We review a few here and leave the exploration of others to future work.\nLength. Using notation from earlier, we estimate the pmf of the distribution over the length of documents in a corpus C as\np̂l(l(y) = k) / X\ny2C {l(y) = k} (6)\nWe can additionally compute statistics of this distribution, such as sample mean: µ̂l(C) = 1/|C| P y2C l(y).\nUnigram. Notably, the rank–frequency law of §3.1 leaves the categorical distribution over words unspecified, i.e., it defines the frequency for thekth ranked word without specifying the word itself. In order to make explicit comparisons, we define the\nunigram distribution w.r.t. corpus C as\np̂uni(w) / X\nw02C {w0 = w} (7)\nStopwords and Symbols. Certain percentages of words in a string consist of either symbols, i.e., numbers and punctuation, or stopwords, i.e., common words such as “that” or “so” that primarily serve a syntactic function. We can model this percentage as a (continuous) random variable S and estimate its probability density function (pdf) as\np̂stop(s < S  s+ ) (8)\n/ X\ny2C\nn #stop(y)\nl(y) 2 (s, s+ ]\no\nThe pdf for symbols is defined similarly. As with our length distribution, we can compute the means µ̂stop, µ̂sym of these distributions."
    }, {
      "heading" : "4 Statistical Distances",
      "text" : "In this work, we aim to quantify the degree to which the linguistic distributions of text generated from language models match—or differ from—those of natural language. To this end, we propose the use of several probability metrics (Mostafaei and Kordnourie, 2011; Rachev et al., 2013) as our notion of statistical distance.4 For each of these metrics, we present nonparametric statistical significance tests, i.e., tests that may be used when the underlying distribution of observed data is not known."
    }, {
      "heading" : "4.1 Primary Metrics",
      "text" : "Perhaps the simplest method for measuring the distance between two random variables is through differences in expectations, e.g., means or variances. (Semi-)distances of this nature are formally called primary metrics. To estimate this distance, we can use observations from random samples S1 and S2, e.g., µ1 µ2 ⇡ (S1,S2) = µ̂(S1) µ̂(S2).\nObserving a value of (S1,S2) 6= 0 on its own is not enough to confirm a difference between µ1 and µ2; we need to assess whether the observed distance is significantly above or below 0. Formally, our null and alternative hypotheses are:\nH0 : (S1,S2) = 0 (9) Ha : (S1,S2) 6= 0\n4Some of these metrics are formally pseudo-distances, as they are not necessarily symmetric.\nIn our setting, we typically do not know the theoretical distributions of the random variables generating S1 and S2, nor of an arbitrary test statistic . Consequently, we use resampling techniques to construct the sampling distribution of (S1,S2).\nPermutation Tests. In a nutshell, a permutation test provides a simple method for constructing the sampling distribution of a test statistic through empirical observations. The method uses the value of over all possible rearrangements of the observed data points to represent the distribution of the test statistic under the null hypothesis. Using this distribution, we can determine the probability of observing a value of the test statistic (or a more extreme value), which if low, may give us reason to reject a specific null hypothesis. In this work, we only consider statistics (·, ·) over two samples. We provide pseudocode for this case in App. B.5"
    }, {
      "heading" : "4.2 Simple Metrics",
      "text" : "Primary metrics provide only a weak measure of the sameness of random variables as they are completely dependent on a single statistic of a distribution. On the other hand, we know a random variable can be completely described by its distribution function. As such, we turn to simple metrics of distance between random variables.\nGiven cumulative density functions (cdfs) P1 and P2 over one-dimensional random variables, the Kolmogorov–Smirnov (KS) metric is\nD(P1, P2) = sup y |P1(y) P2(y)| (10)\nwhere D 2 [0, 1] and D(·, ·) = 0 indicates the distributions are identical. However, not all random variables can be described in terms of a cdf. For categorical distributions where the support of our random variable is not ordinal, the natural counterpart to the KS metric is the Chi-square distance. This metric has a number of drawbacks (discussed in App. C)—primarily that its value can be hard to interpret and so we instead turn to the total variation distance (TVD)—a widely used metric of distance between probability distributions.\nGiven two pmfs p1 and p2, we define TVD as\nTVD(p1, p2) = sup y |p1(y) p2(y)| (11)\n5When the number of possible permutations of the data is computationally prohibitive, we may instead use a MC sampling approach, where we sample from the set of possible permutations (Good, 2000).\nwhere similarly to the KS metric, TVD is bounded above by 1 and a value of 0 indicates identical distributions. In our setting, we consider two use cases for the KS metric and TVD: as distance metrics between an empirical and theoretical distribution (one-sample) and between two empirical distributions (two-sample). The corresponding hypotheses that we can test with these metrics are:\nOne-Sample Case: (12) H0: Sample S is drawn from p Ha: Sample S is not drawn from p Two-Sample Case: (13) H0: Samples S1 and S2 are drawn from same p Ha: Samples S1 and S2 are not drawn from same p\nwhere in the two-sample case, the exact form of p does not need to be known. These hypotheses require the following tests.\nThe Kolmogorov–Smirov Test. The KS test (Smirnov, 1948) is a nonparametric goodness-of-fit test originally designed to assess the fit of a continuous cdf to empirically-observed data; the two-sample version tests whether two samples come from the same distribution. The method has since been extended to discrete distributions and is regarded as one of the most widely applicable nonparametric goodness-of-fit tests for comparing two distributions (Horn, 1977; Moreno-Sánchez et al., 2016). The test uses the KS metric D as its test statistic; under our null hypothesis, D converges to 0 almost surely in the limit as our number of samples n ! 1 by the Glivenko–Cantelli theorem.6 We may reject the null hypothesis if our test statistic is greater than the critical value, which is computed based off of our sample size and a desired significance level.7\nA Test for TVD. Unlike the KS metric, we do not have a (theoretical) limiting distribution for TVD between samples from the same distribution that holds for all density functions (Devroye and Győrfi, 1990). However, we can construct this distribution using resampling techniques. Formally, when S1 and S2 are drawn from the same distribution p— where p need not be known—then the test statistic TVD(pS1, pS2) follows the sampling distribution Zp, i.e., TVD(pS1, pS2) ⇠ Zp. The distribution of Zp can\n6Also known as the “fundamental theorem of statistics.” 7Under the null hypothesis, our text statistic D follows a Kolmogorov distribution. In the two sample case, the critical value is dependent on the size of both samples.\nbe computed using permutations of our samples, in the same manner as defined in §4.1."
    }, {
      "heading" : "5 Experiments",
      "text" : "We use the above framework to assess the degree to which language models learn various distributions of natural language, i.e., we report metrics outlined in §4 measured over the distributions and quantities defined in §3. We compare samples generated from language models to a reserved test set taken from the same corpus as the model’s training data. Each set contains 1 million samples.8 We tokenize all samples using the Moses decoder toolkit (Koehn et al., 2007). All text is lower-cased and only complete unigrams are considered, i.e., when BPE is used, only the detokenized unigram is considered. Length of a string is computed as the number of tokens separated by whitespace. Note that when reporting the KS metric (D), we always report the metric between (a) an empirical cdf computed over the respective model-generated samples and (b) a reference cdf, where Dp indicates direct comparison with empirical cdf of the test set. Dp✓ and Dp̂ indicate comparison with cdfs of a parametric distribution, whose parameters are estimated on the model and test set, respectively.\nNatural Language Corpus. We use English Wikipedia Dumps,9 preprocessing data following the steps used for XLM (Conneau and Lample, 2019) albeit with a 44.7e6 train–1e4 valid–1e6 test split. The test set is used in all statistical tests, however, we estimate standard deviations for statistics in Tab. 4 (in the Appendix) using samples from\n8Due to our large sample sizes, we should anticipate that our results will almost always be significant, even when effect sizes are trivially small. As such, we will almost assuredly reject our null hypotheses that model-generated samples come from the same distribution as natural language ones. While in this light, the presentation of hypothesis tests in §4 may seem pointless, we provide them for cases where generating many samples for each model setting is computationally prohibitive.\n9dumps.wikimedia.org/\nthe training set; see this table for e.g., parameter estimates over test set.\nSimulating Corpora from Language Models. Given the distribution p✓, we may exactly compute statistics and distributions for language models over the entire set Y , weighting examples by the probability assigned to each string; however, doing so is infeasible due to the size of the output space and non-Markovian structure of most neural models. Rather, we turn to sampling to create a representative set S = hy(1), . . . ,y(N)i from p✓. We explore three sampling schemes: ancestral random sampling (Random), nucleus sampling (Nucleus), and beam sampling (Beam).10\nIn ancestral random sampling, y(i) are constructed iteratively according to the distribution\ny(i)t ⇠ p✓(· |y (i) <t) (14)\nwhere y0 = BOS. Under the local normalization scheme of Eq. (2), sampling according to Eq. (14) is equivalent to sampling y(i) directly from p✓. In nucleus sampling, our distribution is truncated to the most probable items covering portion n 2 (0, 1] of the probability mass. Formally, we now sample\ny(i)t ⇠ ( p✓(· |y(i)<t)/Z if y (i) t 2 Vn(p✓(· |y (i) <t))\n0 otherwise (15)\nwhere Vn(p) ✓ V̄ is the smallest subset such thatP y2Vn(p) p(y) n and Z := P y2Vn(p) p(y). Beam sampling uses Eq. (14) as the sampling distribution, but extends a “beam” of k sequences at each sampling iteration. I.e., k extensions are sampled from p✓(· |y(i)<t) and the k most probable of the k2 sampled items remain on the beam; note that unlike standard beam search, this is a stochastic procedure.11 We use a beam size of 5 in all experiments.\n10The latter two sampling designs do not result in samples drawn according to our original p✓ . As such, the schemes lead\nModels. We perform our tests on neural models with three different architectures: a transformer (Vaswani et al., 2017; Baevski and Auli, 2019) (only decoder portion), LSTM (Hochreiter and Schmidhuber, 1997), and Convolutional Neural Network (Dauphin et al., 2017). All models are implemented and trained using fairseq.12 We train models on corpora processed both with and without BPE. We include details for each model in Tab. 1. We additionally estimate a trigram model on the training data; formally, we build a model where the probability of observing token x 2 V̄ at position i of the text is estimated as\np(x | xi 2, xi 1) (16)\n= c(hxi 2, xi 1, xi)P\nx02V̄ c(hxi 2, xi 1, x0i)\nwhere c(·) denotes the function counting occurrences of a sequence in some implicit C. Note that we do not employ smoothing techniques in this model, thus, perplexity over a held-out dataset may diverge and so is not reported in Tab. 1. Vocabulary statistics for each sample are shown in Fig. 2. We provide samples of model-generated text in App. E.\nto two “new” distributions, p(n)✓ and p (b) ✓ , respectively.\n11Note that this is the default sampling scheme for language generation in the fairseq library.\n12github.com/pytorch/fairseq/"
    }, {
      "heading" : "5.1 Rank–Frequency",
      "text" : "To understand the rank–frequency relationship implicitly learned by language models—and how it relates to the rank–frequency distribution present in natural language—we compute the three KS metrics previously described: Dp✓ , Dp̂, and Dp. Specifically, for the first two values, we use the cdf of a Zipfian distribution parameterized by s as our reference—where s is estimated using model generated samples or the test set, respectively.13 These metrics give us a sense of how well the rank–frequency distribution under our language models match a Zipfian distribution. Since the power-law behavior of the token rank–frequency distribution is known to fall off at higher ranks (Piantadosi, 2014; Moreno-Sánchez et al., 2016), we consider solely the first 10,000 ranks in each sample, including when computing Dp. We report these values in Tab. 2. Values of estimates of s and plots of rank–frequency are shown in App. D.\nOur results indicate that our models’ empirical rank–frequency distributions do not adhere very closely to a standard Zipfian distribution (as shown by Dp✓ and Dp̂ 0), despite appearing to at a superficial level (see App. D). However, the same is true for our test (Dp̂ = 0.148), which suggests that our models fit a Zipfian distribution perhaps no more poorly than natural language does. Rather, the model produces qualitatively worst text (see App. E)—a trigram model under the beam sampling generation strategy—follows a power law trend the most closely of any of our samples. On the other hand, the small values of Dp suggest our\n13s is known to vary with the corpus size |C| (Powers, 1998), however |C| is the same for all sets, so this should not affect our analysis.\nmodels learn the empirical rank–frequency trends of human text quite well, something that would not be evident by simply looking at adherence to a Zipfian distribution. The combination of these results suggest the limitation of using adherence to Zipf’s law as a gauge for a model’s consistency with natural language."
    }, {
      "heading" : "5.2 Type–Token",
      "text" : "Fig. 3 shows the type–token trend for all corpora and generation schemes. While most models appear not to follow the same trend as the natural language distribution (as depicted by our test set), we observe that transformers under the nucleus sampling generation scheme match it most closely. Indeed, both models based on the transformer architecture exhibit remarkably similar trends in these experiments, despite having different vocabulary sizes and hyperparameters: both in their generally close fit to the natural language type–token distribution and in their visible fall-off for longer length sequences. The latter observation reveals a deficiency that is seemingly specific to the transformer architecture—one that may be linked to observations in natural language generation tasks. More specifically, we take this as quantita-\ntive evidence for recent qualitative observations that when left to generate lots of text, neural language models based on the transformer architecture tend to babble repetitively (Holtzman et al., 2020; Cohen and Beck, 2019; Eikema and Aziz, 2020).\nTo provide a more mathematically rigorous analysis, we compute KS metrics,14 again presenting three values: Dp✓ , Dp̂, and Dp. In Fig. 4, we can see that model-generated text follows a NHPP parameterized by Heaps’ law moderately well (Dp✓ ); there are larger divergences at the tails of document length. However, most do not follow an NHPP with the same parameters as our test set (Dp̂). Further, in contrast to rank–frequency, the type–token distribution is more disparate from the empirical natural language distribution than our parameterized ones, as shown by high values of Dp. While both transformers exhibit the closest fit for all document lengths, which is in-line with our observations in Fig. 3, statistical distance from the natural language distribution for all models and in all settings increases with document length.\n14§3.1 provides motivation for comparing distributions at individual time steps rather than collectively over time; analyzing Eq. (5) for all document lengths simultaneously would not give us a sense of how the power-law fit changes as a function of document length."
    }, {
      "heading" : "5.3 Unigram Distribution",
      "text" : "Because we do not have a well-established law dictating the form of the natural language unigram distribution, we compare only empirical pmfs from model-generated samples and the test set directly. Further, as the distribution over unigrams is categorical, we employ TVD following §4.2. Our results in Tab. 2 indicate that language models generally capture the unigram distribution quite well. The transformer (AS), which has a closed vocabulary, consistently performs poorly in comparison to other models. While we might speculate this outcome is a result of disparate tails between empirical cdfs—i.e., the part of the distribution over infrequent words, which may have been omitted from the closed vocabulary but could still be generated using BPE—the TVD metric in this setting should generally be robust to tail probabilities.15 This suggests that BPE (or similar) vocabulary schemes may lead to models that can better fit this natural language distribution."
    }, {
      "heading" : "5.4 Length, Stopwords and Symbols",
      "text" : "Similarly to the unigram distribution, for length, stopwords and symbols, we compare solely empirical cdfs. We use the set of English stopwords defined by NLTK (Bird et al., 2009). We define the set of symbols as tokens consisting solely of punctuation and numerical values. Our results in Tab. 3 demonstrate that our language models—at least when using random and nucleus sampling— mimic these natural language distributions quite well. Notably, text generated from an LSTM using random sampling follows all three distributions the closest of any model, suggesting LSTMs may have an inductive bias that is helpful for capturing these distributions. On the other hand, using beam sampling leads to strong divergence from natural language distributions across the board. Results for differences in distribution means in the permutation testing framework can be found in App. D.\nWith respect to the length distribution, these results are perhaps surprising: the localnormalization scheme used by the majority of language generation models (and by those in these experiments) has been claimed to result in models that favor shorter than typical sequences (Sountsov and Sarawagi, 2016; Murray and Chiang, 2018). The results in Tab. 3 and Fig. 5 suggest otherwise.\n15We observe this empirically; calculating TVD between distributions truncated to the (union of the) first 1000 ranked unigrams lead to almost the exact same result.\nSpecifically, we see that our models fit the natural language length distribution of our corpus quite closely, in terms of both overall distributions and means (see App. D). Rather, it appears that the generation strategy may be the cause of prior observations. This finding raises further questions: since models capture the length distribution well, is a language model more likely to produce degenerate text (e.g., repetitions) than the EOS token if only long documents are used in training? We posit that corpus preprocessing should perhaps be more carefully considered in light of these results."
    }, {
      "heading" : "5.5 Consistent Trends",
      "text" : "Across results, we observe that text generated using the nucleus sampling decoding scheme often aligns with natural language more closely than text produced using other generation strategies. This suggests that nucleus sampling performs a helpful alteration to a standard distribution learned via MLE, which may in turn provide motivation for recent efforts to employ truncated or sparse probability distributions directly at training time, e.g., truncated loss (Kang and Hashimoto, 2020) or ↵- entmax loss (Peters et al., 2019).\nWe additionally observe large discrepancies in both §5.1 and §5.2 between the results when using empirical natural language cdfs vs. parametric ones. We take this as a warning that assumptions about the forms of linguistic distributions—such as the ones employed by challenge tasks in probing—can have significant effects on results."
    }, {
      "heading" : "6 Related Work",
      "text" : "In the last few years, a number of works have extended language model analysis beyond simple\nevaluation metrics—like perplexity—in order to understand what attributes of human language these models are learning. Some use task-based approaches, i.e., they design a set of tasks that require a specific subset of linguistic knowledge then evaluate model performance on these tasks (Linzen et al., 2016; Gulordava et al., 2018; Jiang et al., 2020, inter alia). Others use model-based approaches, where a separate model is trained to perform some auxiliary task on representations learned by the model under test (Blevins et al., 2018; Giulianelli et al., 2018; Sorodoc et al., 2020, inter alia). We direct readers to Belinkov and Glass (2019) for a full survey of probing methods.\nThese approaches have drawbacks; for example, introducing a secondary model to determine what the original model has learned presents confounding factors (Hewitt and Liang, 2019). The designing of auxiliary tasks for assessing linguistic knowledge requires large manual effort and lends itself to implicit bias about how linguistic phenomena should manifest. In contrast, our work allows us to take a hands-off approach to analyzing language models. We see the benefit of this in §5, where our results without an assumed model of statistical tendencies give us a much different sense of which empirical properties of human-generated text our models have learned.\nOur work is closest to that of Takahashi and Tanaka-Ishii (2017, 2019) who use model generated text to visually analyze whether language models reflect well-established statistical tendencies. In contrast, our work provides a quantitative framework, along with appropriate significance tests,16 for evaluating distribution fits. We additionally assess the fit of language models to our test set directly, rather than solely to established laws. Further, our analysis includes different generation strategies, multiple neural architectures, and a wider variety of empirical language distributions.\n16In this respect, our work is similar to Dror et al. (2018), whom also present statistical tests for use in NLP."
    }, {
      "heading" : "7 Conclusion and Future Directions",
      "text" : "In this work, we present a framework for determining the linguistic properties learned by language models through analysis of statistical trends in generated text. We find that neural language models accurately capture only a subset of natural language distributions and that this subset is highly dependent on both model architecture and generation strategy; no one configuration stands out as capturing all linguistic distributions. Ultimately, we see this analysis framework as a means for a more finegrained evaluation of language models than perplexity alone can provide. Uncovering which linguistic properties language models have learned— and which they have not—should help us to understand both the inductive biases of various models and via which avenues they can still be improved.\nThere are a number of important axes of variation that this work does not explore: perhaps most importantly, our results are limited to a single corpora in the English language. A cross-linguistic analysis may reveal whether different model architectures exhibit inductive biases compatible with different languages; observing how these metrics change as a function of corpus size would have implications about the effects of data availability. An exploration of the correlation of these metrics with other quantifications of model performance, such as perplexity or a model’s ability to capture sentence level phenomenon, may help us understand how comprehensive other evaluation metrics are. We leave these analyses as future work."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Adhi Kuncoro for helpful discussion and feedback in the middle stages of our work and Tiago Pimentel, Jason Wei, and our anonymous reviewers for insightful feedback on the manuscript. We additionally thank B. Bou for his concern."
    } ],
    "references" : [ {
      "title" : "Statistical Laws in Linguistics, pages 7–26",
      "author" : [ "Eduardo G. Altmann", "Martin Gerlach." ],
      "venue" : "Springer International Publishing.",
      "citeRegEx" : "Altmann and Gerlach.,? 2016",
      "shortCiteRegEx" : "Altmann and Gerlach.",
      "year" : 2016
    }, {
      "title" : "Adaptive input representations for neural language modeling",
      "author" : [ "Alexei Baevski", "Michael Auli." ],
      "venue" : "Proceedings of the 7th International Conference on Learning Representations.",
      "citeRegEx" : "Baevski and Auli.,? 2019",
      "shortCiteRegEx" : "Baevski and Auli.",
      "year" : 2019
    }, {
      "title" : "Distributions in text",
      "author" : [ "Marco Baroni." ],
      "venue" : "Corpus Linguistics: An International Handbook, 2:803–821.",
      "citeRegEx" : "Baroni.,? 2009",
      "shortCiteRegEx" : "Baroni.",
      "year" : 2009
    }, {
      "title" : "Analysis methods in neural language processing: A survey",
      "author" : [ "Yonatan Belinkov", "James Glass." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:49–72.",
      "citeRegEx" : "Belinkov and Glass.,? 2019",
      "shortCiteRegEx" : "Belinkov and Glass.",
      "year" : 2019
    }, {
      "title" : "Natural Language Processing with Python, 1st edition",
      "author" : [ "Steven Bird", "Ewan Klein", "Edward Loper." ],
      "venue" : "O’Reilly Media, Inc.",
      "citeRegEx" : "Bird et al\\.,? 2009",
      "shortCiteRegEx" : "Bird et al\\.",
      "year" : 2009
    }, {
      "title" : "Deep RNNs encode soft hierarchical syntax",
      "author" : [ "Terra Blevins", "Omer Levy", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 14–19. Association for Com-",
      "citeRegEx" : "Blevins et al\\.,? 2018",
      "shortCiteRegEx" : "Blevins et al\\.",
      "year" : 2018
    }, {
      "title" : "RNN simulations of grammaticality judgments on long-distance dependencies",
      "author" : [ "Shammur Absar Chowdhury", "Roberto Zamparelli." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 133–144. Association",
      "citeRegEx" : "Chowdhury and Zamparelli.,? 2018",
      "shortCiteRegEx" : "Chowdhury and Zamparelli.",
      "year" : 2018
    }, {
      "title" : "Power-law distributions in empirical data",
      "author" : [ "Aaron Clauset", "Cosma Rohilla Shalizi", "M.E.J. Newman." ],
      "venue" : "SIAM Review, 51(4):661–703.",
      "citeRegEx" : "Clauset et al\\.,? 2009",
      "shortCiteRegEx" : "Clauset et al\\.",
      "year" : 2009
    }, {
      "title" : "Empirical analysis of beam search performance degradation in neural sequence models",
      "author" : [ "Eldan Cohen", "Christopher Beck." ],
      "venue" : "Proceedings of the International Conference on Machine Learning, volume 97.",
      "citeRegEx" : "Cohen and Beck.,? 2019",
      "shortCiteRegEx" : "Cohen and Beck.",
      "year" : 2019
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Alexis Conneau", "Guillaume Lample." ],
      "venue" : "H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 7059–",
      "citeRegEx" : "Conneau and Lample.,? 2019",
      "shortCiteRegEx" : "Conneau and Lample.",
      "year" : 2019
    }, {
      "title" : "Language modeling with gated convolutional networks",
      "author" : [ "Yann N. Dauphin", "Angela Fan", "Michael Auli", "David Grangier." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, pages 933–941.",
      "citeRegEx" : "Dauphin et al\\.,? 2017",
      "shortCiteRegEx" : "Dauphin et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "No empirical probability measure can converge in the total variation sense for all distributions",
      "author" : [ "Luc Devroye", "László Győrfi." ],
      "venue" : "The Annals of Statistics, 18(3):1496–1499.",
      "citeRegEx" : "Devroye and Győrfi.,? 1990",
      "shortCiteRegEx" : "Devroye and Győrfi.",
      "year" : 1990
    }, {
      "title" : "The hitchhiker’s guide to testing statistical significance in natural language processing",
      "author" : [ "Rotem Dror", "Gili Baumer", "Segev Shlomov", "Roi Reichart." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Dror et al\\.,? 2018",
      "shortCiteRegEx" : "Dror et al\\.",
      "year" : 2018
    }, {
      "title" : "Is MAP decoding all you need? The inadequacy of the mode in neural machine translation",
      "author" : [ "Bryan Eikema", "Wilker Aziz." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 4506–4520. International Com-",
      "citeRegEx" : "Eikema and Aziz.,? 2020",
      "shortCiteRegEx" : "Eikema and Aziz.",
      "year" : 2020
    }, {
      "title" : "Under the hood: Using diagnostic classifiers to investigate and improve how language models track agreement information",
      "author" : [ "Mario Giulianelli", "Jack Harding", "Florian Mohnert", "Dieuwke Hupkes", "Willem Zuidema." ],
      "venue" : "Proceedings of the 2018",
      "citeRegEx" : "Giulianelli et al\\.,? 2018",
      "shortCiteRegEx" : "Giulianelli et al\\.",
      "year" : 2018
    }, {
      "title" : "Permutation Tests : A Practical Guide to Resampling Methods for Testing Hypotheses, 2nd edition",
      "author" : [ "Phillip I. Good." ],
      "venue" : "Springer.",
      "citeRegEx" : "Good.,? 2000",
      "shortCiteRegEx" : "Good.",
      "year" : 2000
    }, {
      "title" : "Colorless green recurrent networks dream hierarchically",
      "author" : [ "Kristina Gulordava", "Piotr Bojanowski", "Edouard Grave", "Tal Linzen", "Marco Baroni." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Gulordava et al\\.,? 2018",
      "shortCiteRegEx" : "Gulordava et al\\.",
      "year" : 2018
    }, {
      "title" : "Type–token Mathematics: A Textbook of Mathematical Linguistics",
      "author" : [ "Gustav Herdan." ],
      "venue" : "The Hague: Mouton.",
      "citeRegEx" : "Herdan.,? 1960",
      "shortCiteRegEx" : "Herdan.",
      "year" : 1960
    }, {
      "title" : "Designing and interpreting probes with control tasks",
      "author" : [ "John Hewitt", "Percy Liang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro-",
      "citeRegEx" : "Hewitt and Liang.,? 2019",
      "shortCiteRegEx" : "Hewitt and Liang.",
      "year" : 2019
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "The curious case of neural text degeneration",
      "author" : [ "Ari Holtzman", "Jan Buys", "Maxwell Forbes", "Yejin Choi." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Holtzman et al\\.,? 2020",
      "shortCiteRegEx" : "Holtzman et al\\.",
      "year" : 2020
    }, {
      "title" : "Goodness-of-fit tests for discrete data: A review and an application to a health impairment scale",
      "author" : [ "Susan Dadakis Horn." ],
      "venue" : "Biometrics, 33(1):237–247.",
      "citeRegEx" : "Horn.,? 1977",
      "shortCiteRegEx" : "Horn.",
      "year" : 1977
    }, {
      "title" : "How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423–438",
      "author" : [ "Zhengbao Jiang", "Frank F. Xu", "Jun Araki", "Graham Neubig" ],
      "venue" : null,
      "citeRegEx" : "Jiang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Improved natural language generation via loss truncation",
      "author" : [ "Daniel Kang", "Tatsunori Hashimoto." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 718–731. Association for Computational Lin-",
      "citeRegEx" : "Kang and Hashimoto.,? 2020",
      "shortCiteRegEx" : "Kang and Hashimoto.",
      "year" : 2020
    }, {
      "title" : "Fitting ranked linguistic data with twoparameter functions",
      "author" : [ "Wentian Li", "Pedro Miramontes", "Germinal Cocho." ],
      "venue" : "Entropy, 12(7):1743–1764.",
      "citeRegEx" : "Li et al\\.,? 2010",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2010
    }, {
      "title" : "Assessing the ability of LSTMs to learn syntax-sensitive dependencies",
      "author" : [ "Tal Linzen", "Emmanuel Dupoux", "Yoav Goldberg." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:521– 535.",
      "citeRegEx" : "Linzen et al\\.,? 2016",
      "shortCiteRegEx" : "Linzen et al\\.",
      "year" : 2016
    }, {
      "title" : "Regularizing and optimizing LSTM language models",
      "author" : [ "Stephen Merity", "Nitish Shirish Keskar", "Richard Socher." ],
      "venue" : "CoRR, abs/1708.02182.",
      "citeRegEx" : "Merity et al\\.,? 2017",
      "shortCiteRegEx" : "Merity et al\\.",
      "year" : 2017
    }, {
      "title" : "Large-scale analysis of Zipf’s law in english texts",
      "author" : [ "Isabel Moreno-Sánchez", "Francesc Font-Clos", "Álvaro Corral." ],
      "venue" : "PLOS ONE, 11(1):1–19.",
      "citeRegEx" : "Moreno.Sánchez et al\\.,? 2016",
      "shortCiteRegEx" : "Moreno.Sánchez et al\\.",
      "year" : 2016
    }, {
      "title" : "Probability metrics and their applications",
      "author" : [ "Hamidreza Mostafaei", "Shaghayegh Kordnourie." ],
      "venue" : "Applied Mathematical Sciences, 5:181–192.",
      "citeRegEx" : "Mostafaei and Kordnourie.,? 2011",
      "shortCiteRegEx" : "Mostafaei and Kordnourie.",
      "year" : 2011
    }, {
      "title" : "Correcting length bias in neural machine translation",
      "author" : [ "Kenton Murray", "David Chiang." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 212–223. Association for Computational Linguistics.",
      "citeRegEx" : "Murray and Chiang.,? 2018",
      "shortCiteRegEx" : "Murray and Chiang.",
      "year" : 2018
    }, {
      "title" : "Sparse sequence-to-sequence models",
      "author" : [ "Ben Peters", "Vlad Niculae", "André F.T. Martins." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1504– 1519. Association for Computational Linguistics.",
      "citeRegEx" : "Peters et al\\.,? 2019",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2019
    }, {
      "title" : "Zipf’s word frequency law in natural language: A critical review and future directions",
      "author" : [ "S. Piantadosi." ],
      "venue" : "Psychonomic Bulletin and Review, 21:1112–1130.",
      "citeRegEx" : "Piantadosi.,? 2014",
      "shortCiteRegEx" : "Piantadosi.",
      "year" : 2014
    }, {
      "title" : "Applications and explanations of Zipf’s law",
      "author" : [ "David M.W. Powers." ],
      "venue" : "New Methods in Language Processing and Computational Natural Language Learning.",
      "citeRegEx" : "Powers.,? 1998",
      "shortCiteRegEx" : "Powers.",
      "year" : 1998
    }, {
      "title" : "The Methods of Distances in the Theory of Probability and Statistics, pages 479– 516",
      "author" : [ "Svetlozar Rachev", "Lev Klebanov", "Stoyan Stoyanov", "Frank Fabozzi." ],
      "venue" : "Springer.",
      "citeRegEx" : "Rachev et al\\.,? 2013",
      "shortCiteRegEx" : "Rachev et al\\.",
      "year" : 2013
    }, {
      "title" : "Language Models are Unsupervised Multitask Learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Stochastic processes",
      "author" : [ "S.M. Ross." ],
      "venue" : "Wiley series in probability and statistics: Probability and statistics. Wiley.",
      "citeRegEx" : "Ross.,? 1996",
      "shortCiteRegEx" : "Ross.",
      "year" : 1996
    }, {
      "title" : "A neural model of adaptation in reading",
      "author" : [ "Marten van Schijndel", "Tal Linzen." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4704–4710. Association for Computational Linguistics.",
      "citeRegEx" : "Schijndel and Linzen.,? 2018",
      "shortCiteRegEx" : "Schijndel and Linzen.",
      "year" : 2018
    }, {
      "title" : "On Zipf’s law and the bias of Zipf regressions",
      "author" : [ "Christian Schluter." ],
      "venue" : "Empirical Economics.",
      "citeRegEx" : "Schluter.,? 2020",
      "shortCiteRegEx" : "Schluter.",
      "year" : 2020
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–1725.",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Table for estimating the goodness of fit of empirical distributions",
      "author" : [ "N. Smirnov." ],
      "venue" : "Annals of Mathematical Statistics, 19(2):279–281.",
      "citeRegEx" : "Smirnov.,? 1948",
      "shortCiteRegEx" : "Smirnov.",
      "year" : 1948
    }, {
      "title" : "Probing for referential information in language models",
      "author" : [ "Ionut-Teodor Sorodoc", "Kristina Gulordava", "Gemma Boleda." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4177–4189.",
      "citeRegEx" : "Sorodoc et al\\.,? 2020",
      "shortCiteRegEx" : "Sorodoc et al\\.",
      "year" : 2020
    }, {
      "title" : "Length bias in encoder decoder models and a case for global conditioning",
      "author" : [ "Pavel Sountsov", "Sunita Sarawagi." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1516–1525. Association for Com-",
      "citeRegEx" : "Sountsov and Sarawagi.,? 2016",
      "shortCiteRegEx" : "Sountsov and Sarawagi.",
      "year" : 2016
    }, {
      "title" : "Do neural nets learn statistical laws behind natural language",
      "author" : [ "Shuntaro Takahashi", "Kumiko Tanaka-Ishii" ],
      "venue" : "PLOS ONE,",
      "citeRegEx" : "Takahashi and Tanaka.Ishii.,? \\Q2017\\E",
      "shortCiteRegEx" : "Takahashi and Tanaka.Ishii.",
      "year" : 2017
    }, {
      "title" : "Evaluating computational language models with scaling properties of natural language",
      "author" : [ "Shuntaro Takahashi", "Kumiko Tanaka-Ishii." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 45(3):481–513.",
      "citeRegEx" : "Takahashi and Tanaka.Ishii.,? 2019",
      "shortCiteRegEx" : "Takahashi and Tanaka.Ishii.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Large-sample results for kolmogorov-smirnov statistics for discrete distributions",
      "author" : [ "Constance L. Wood", "Michele M. Altavela." ],
      "venue" : "Biometrika, 65(1):235–239.",
      "citeRegEx" : "Wood and Altavela.,? 1978",
      "shortCiteRegEx" : "Wood and Altavela.",
      "year" : 1978
    }, {
      "title" : "Human Behavior and the Principle of Least Effort",
      "author" : [ "George K. Zipf." ],
      "venue" : "Addison-Wesley Press.",
      "citeRegEx" : "Zipf.,? 1949",
      "shortCiteRegEx" : "Zipf.",
      "year" : 1949
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "In this work, we do not use the term language model to refer to cloze language models such as BERT (Devlin et al., 2019), which do not give us a distribution over strings.",
      "startOffset" : 99,
      "endOffset" : 120
    }, {
      "referenceID" : 17,
      "context" : "For the most part, these works have been limited to analyses of sentence-level phenomenon, such as subject–verb agreement (Gulordava et al., 2018) and garden path effects (van Schijndel and Linzen, 2018) among a myriad of other properties (Blevins et al.",
      "startOffset" : 122,
      "endOffset" : 146
    }, {
      "referenceID" : 39,
      "context" : "In this paper, we term vocabularies consisting of words closed and those consisting of BPE tokens (Sennrich et al., 2016) open.",
      "startOffset" : 98,
      "endOffset" : 121
    }, {
      "referenceID" : 0,
      "context" : "Human languages are thought to exhibit statistical tendencies, several of which are explicitly quantified by laws (Altmann and Gerlach, 2016).",
      "startOffset" : 114,
      "endOffset" : 141
    }, {
      "referenceID" : 7,
      "context" : "However, it has been empirically observed that least-squares estimates under this paradigm are not reliable (Clauset et al., 2009) and further, directly incorporate assumptions that contradict power law behavior (Schluter, 2020).",
      "startOffset" : 108,
      "endOffset" : 130
    }, {
      "referenceID" : 38,
      "context" : ", 2009) and further, directly incorporate assumptions that contradict power law behavior (Schluter, 2020).",
      "startOffset" : 89,
      "endOffset" : 105
    }, {
      "referenceID" : 36,
      "context" : "Specifically, we model the number of types for a document of a given length as a non-homogeneous Poisson process (NHPP; Ross, 1996) where our rate parameter (l(y)) is Heaps’ power law relation.",
      "startOffset" : 113,
      "endOffset" : 131
    }, {
      "referenceID" : 29,
      "context" : "To this end, we propose the use of several probability metrics (Mostafaei and Kordnourie, 2011; Rachev et al., 2013) as our notion of statistical distance.",
      "startOffset" : 63,
      "endOffset" : 116
    }, {
      "referenceID" : 34,
      "context" : "To this end, we propose the use of several probability metrics (Mostafaei and Kordnourie, 2011; Rachev et al., 2013) as our notion of statistical distance.",
      "startOffset" : 63,
      "endOffset" : 116
    }, {
      "referenceID" : 16,
      "context" : "When the number of possible permutations of the data is computationally prohibitive, we may instead use a MC sampling approach, where we sample from the set of possible permutations (Good, 2000).",
      "startOffset" : 182,
      "endOffset" : 194
    }, {
      "referenceID" : 40,
      "context" : "The KS test (Smirnov, 1948) is a nonparametric goodness-of-fit test originally designed to assess the fit of a continuous cdf to empirically-observed data; the two-sample version tests whether two samples come from the same distribution.",
      "startOffset" : 12,
      "endOffset" : 27
    }, {
      "referenceID" : 22,
      "context" : "The method has since been extended to discrete distributions and is regarded as one of the most widely applicable nonparametric goodness-of-fit tests for comparing two distributions (Horn, 1977; Moreno-Sánchez et al., 2016).",
      "startOffset" : 182,
      "endOffset" : 223
    }, {
      "referenceID" : 28,
      "context" : "The method has since been extended to discrete distributions and is regarded as one of the most widely applicable nonparametric goodness-of-fit tests for comparing two distributions (Horn, 1977; Moreno-Sánchez et al., 2016).",
      "startOffset" : 182,
      "endOffset" : 223
    }, {
      "referenceID" : 9,
      "context" : "We use English Wikipedia Dumps,9 preprocessing data following the steps used for XLM (Conneau and Lample, 2019) albeit with a 44.",
      "startOffset" : 85,
      "endOffset" : 111
    }, {
      "referenceID" : 45,
      "context" : "We perform our tests on neural models with three different architectures: a transformer (Vaswani et al., 2017; Baevski and Auli, 2019) (only decoder portion), LSTM (Hochreiter and Schmidhuber, 1997), and Convolutional Neural Network (Dauphin et al.",
      "startOffset" : 88,
      "endOffset" : 134
    }, {
      "referenceID" : 1,
      "context" : "We perform our tests on neural models with three different architectures: a transformer (Vaswani et al., 2017; Baevski and Auli, 2019) (only decoder portion), LSTM (Hochreiter and Schmidhuber, 1997), and Convolutional Neural Network (Dauphin et al.",
      "startOffset" : 88,
      "endOffset" : 134
    }, {
      "referenceID" : 20,
      "context" : ", 2017; Baevski and Auli, 2019) (only decoder portion), LSTM (Hochreiter and Schmidhuber, 1997), and Convolutional Neural Network (Dauphin et al.",
      "startOffset" : 61,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : ", 2017; Baevski and Auli, 2019) (only decoder portion), LSTM (Hochreiter and Schmidhuber, 1997), and Convolutional Neural Network (Dauphin et al., 2017).",
      "startOffset" : 130,
      "endOffset" : 152
    }, {
      "referenceID" : 32,
      "context" : "Since the power-law behavior of the token rank–frequency distribution is known to fall off at higher ranks (Piantadosi, 2014; Moreno-Sánchez et al., 2016), we consider solely the first 10,000 ranks in each sample, including when computing Dp.",
      "startOffset" : 107,
      "endOffset" : 154
    }, {
      "referenceID" : 28,
      "context" : "Since the power-law behavior of the token rank–frequency distribution is known to fall off at higher ranks (Piantadosi, 2014; Moreno-Sánchez et al., 2016), we consider solely the first 10,000 ranks in each sample, including when computing Dp.",
      "startOffset" : 107,
      "endOffset" : 154
    }, {
      "referenceID" : 33,
      "context" : "s is known to vary with the corpus size |C| (Powers, 1998), however |C| is the same for all sets, so this should not affect our analysis.",
      "startOffset" : 44,
      "endOffset" : 58
    }, {
      "referenceID" : 46,
      "context" : "p-values (estimated using Monte Carlo simulations (Wood and Altavela, 1978)) for all KS metrics are ⌧ 0.",
      "startOffset" : 50,
      "endOffset" : 75
    }, {
      "referenceID" : 21,
      "context" : "More specifically, we take this as quantitative evidence for recent qualitative observations that when left to generate lots of text, neural language models based on the transformer architecture tend to babble repetitively (Holtzman et al., 2020; Cohen and Beck, 2019; Eikema and Aziz, 2020).",
      "startOffset" : 223,
      "endOffset" : 291
    }, {
      "referenceID" : 8,
      "context" : "More specifically, we take this as quantitative evidence for recent qualitative observations that when left to generate lots of text, neural language models based on the transformer architecture tend to babble repetitively (Holtzman et al., 2020; Cohen and Beck, 2019; Eikema and Aziz, 2020).",
      "startOffset" : 223,
      "endOffset" : 291
    }, {
      "referenceID" : 14,
      "context" : "More specifically, we take this as quantitative evidence for recent qualitative observations that when left to generate lots of text, neural language models based on the transformer architecture tend to babble repetitively (Holtzman et al., 2020; Cohen and Beck, 2019; Eikema and Aziz, 2020).",
      "startOffset" : 223,
      "endOffset" : 291
    }, {
      "referenceID" : 4,
      "context" : "We use the set of English stopwords defined by NLTK (Bird et al., 2009).",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 42,
      "context" : "guage generation models (and by those in these experiments) has been claimed to result in models that favor shorter than typical sequences (Sountsov and Sarawagi, 2016; Murray and Chiang, 2018).",
      "startOffset" : 139,
      "endOffset" : 193
    }, {
      "referenceID" : 30,
      "context" : "guage generation models (and by those in these experiments) has been claimed to result in models that favor shorter than typical sequences (Sountsov and Sarawagi, 2016; Murray and Chiang, 2018).",
      "startOffset" : 139,
      "endOffset" : 193
    }, {
      "referenceID" : 24,
      "context" : ", truncated loss (Kang and Hashimoto, 2020) or ↵entmax loss (Peters et al.",
      "startOffset" : 17,
      "endOffset" : 43
    }, {
      "referenceID" : 31,
      "context" : ", truncated loss (Kang and Hashimoto, 2020) or ↵entmax loss (Peters et al., 2019).",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 46,
      "context" : "p-values (estimated using Monte Carlo simulations (Wood and Altavela, 1978)) for all KS metrics are ⌧ 0.",
      "startOffset" : 50,
      "endOffset" : 75
    }, {
      "referenceID" : 19,
      "context" : "These approaches have drawbacks; for example, introducing a secondary model to determine what the original model has learned presents confounding factors (Hewitt and Liang, 2019).",
      "startOffset" : 154,
      "endOffset" : 178
    } ],
    "year" : 2021,
    "abstractText" : "We propose an alternate approach to quantifying how well language models learn natural language: we ask how well they match the statistical tendencies of natural language. To answer this question, we analyze whether text generated from language models exhibits the statistical tendencies present in the humangenerated text on which they were trained. We provide a framework—paired with significance tests—for evaluating the fit of language models to these trends. We find that neural language models appear to learn only a subset of the tendencies considered, but align much more closely with empirical trends than proposed theoretical distributions (when present). Further, the fit to different distributions is highly-dependent on both model architecture and generation strategy. As concrete examples, text generated under the nucleus sampling scheme adheres more closely to the type– token relationship of natural language than text produced using standard ancestral sampling; text from LSTMs reflects the natural language distributions over length, stopwords, and symbols surprisingly well.",
    "creator" : "LaTeX with hyperref"
  }
}