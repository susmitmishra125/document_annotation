{
  "name" : "2021.acl-long.15.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "GL-GIN: Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling",
    "authors" : [ "Libo Qin", "Fuxuan Wei", "Tianbao Xie", "Xiao Xu", "Wanxiang Che", "Ting Liu" ],
    "emails" : [ "lbqin@ir.hit.edu.cn", "fuxuanwei@ir.hit.edu.cn", "tianbaoxie@ir.hit.edu.cn", "xxu@ir.hit.edu.cn", "car@ir.hit.edu.cn", "tliu@ir.hit.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 178–188\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n178"
    }, {
      "heading" : "1 Introduction",
      "text" : "Spoken Language Understanding (SLU) (Young et al., 2013) is a critical component in spoken dialog systems, which aims to understand user’s queries. It typically includes two sub-tasks: intent detection and slot filling (Tur and De Mori, 2011).\nSince intents and slots are closely tied, dominant single-intent SLU systems in the literature (Goo et al., 2018; Li et al., 2018; Liu et al., 2019b; E et al., 2019; Qin et al., 2019; Teng et al., 2021; Qin et al., 2021b,c) adopt joint models to consider the correlation between the two tasks, which have obtained remarkable success.\nMulti-intent SLU means that the system can handle an utterance containing multiple intents, which is shown to be more practical in the real-world scenario, attracting increasing attention. To this end,\n∗Corresponding author.\nXu and Sarikaya (2013) and Kim et al. (2017) begin to explore the multi-intent SLU. However, their models only consider the multiple intent detection while ignoring slot filling task. Recently, Gangadharaiah and Narayanaswamy (2019) make the first attempt to propose a multi-task framework to joint model the multiple intent detection and slot filling. Qin et al. (2020b) further propose an adaptive interaction framework (AGIF) to achieve fine-grained multi-intent information integration for slot filling, obtaining state-of-the-art performance.\nThough achieving the promising performance, the existing multi-intent SLU joint models heavily rely on an autoregressive fashion, as shown in Figure 1(a), leading to two issues:\n• Slow inference speed. The autoregressive models make the generation of slot outputs must be done through the left-to-right pass, which cannot achieve parallelizable, leading to slow inference speed.\n• Information leakage. Autoregressive models predict each word slot conditioned on the previously generated slot information (from leftto-right), resulting in leaking the bidirectional context information.\nIn this paper, we explore a non-autoregressive framework for joint multiple intent detection and slot filling, with the goal of accelerating inference speed while achieving high accuracy, which is shown in Figure 1(b). To this end, we propose a Global-Locally Graph-Interaction Network (GLGIN) where the core module is a proposed local slot-aware graph layer and global intent-slot interaction layer, which achieves to generate intents and slots sequence simultaneously and nonautoregressively. In GL-GIN, a local slot-aware graph interaction layer where each slot hidden states connect with each other is proposed to explicitly model slot dependency, in order to alleviate uncoordinated slot problem (e.g., B-singer followed by I-song) (Wu et al., 2020) due to the non-autoregressive fashion. A global intent-slot graph interaction layer is further introduced to perform sentence-level intent-slot interaction. Unlike the prior works that only consider the tokenlevel intent-slot interaction, the global graph is constructed of all tokens with multiple intents, achieving to generate slots sequence in parallel and speed up the decoding process.\nExperimental results on two public datasets MixSNIPS (Coucke et al., 2018) and MixATIS (Hemphill et al., 1990) show that our framework not only obtains state-of-the-art performance but also enables decoding in parallel. In addition, we explore the pre-trained model (i.e., Roberta (Liu et al., 2019c)) in our framework.\nIn summary, the contributions of this work can be concluded as follows: (1) To the best of our knowledge, we make the first attempt to explore a non-autoregressive approach for joint multiple intent detection and slot filling; (2) We propose a global-locally graph-interaction network, where the local graph is used to handle uncoordinated slots problem while a global graph is introduced to model sequence-level intent-slot interaction; (3) Experiment results on two benchmarks show that our framework not only achieves the state-of-theart performance but also considerably speeds up the slot decoding (up to ×11.5); (4) Finally, we explore the pre-trained model in our framework. With the pre-trained model, our model reaches a\nnew state-of-the-art level. For reproducibility, our code for this paper is publicly available at https://github.com/ yizhen20133868/GL-GIN."
    }, {
      "heading" : "2 Problem Definition",
      "text" : "Multiple Intent Detection Given input sequence x = (x1, . . . , xn), multiple intent detection can be defined as a multi-label classification task that outputs a sequence intent label oI = (oI1, . . . , o I m), where m is the number of intents in given utterance and n is the length of utterance.\nSlot Filling Slot filling can be seen as a sequence labeling task that maps the input utterance x into a slot output sequence oS = (oS1 , . . . , o S n)."
    }, {
      "heading" : "3 Approach",
      "text" : "As shown in Figure 2(a), we describe the proposed framework, which consists of a shared selfattentive encoder (§3.1), a token-level intent detection decoder (§3.2) and a global-local graphinteraction graph decoder for slot filling (§3.3). Both intent detection and slot filling are optimized simultaneously via a joint learning scheme."
    }, {
      "heading" : "3.1 Self-attentive Encoder",
      "text" : "Following Qin et al. (2019), we utilize a selfattentive encoder with BiLSTM and self-attention mechanism to obtain the shared utterance representation, which can incorporate temporal features within word orders and contextual information.\nBiLSTM The bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) have been successfully applied to sequence labeling tasks (Li et al., 2020, 2021). We adopt BiLSTM to read the input sequence {x1, x2, . . . , xn} forwardly and backwardly to produce contextsensitive hidden states H = {h1,h2, . . . ,hn}, by repeatedly applying the hi = BiLSTM (φemb(xi), hi−1, hi+1), where φemb is embedding function.\nSelf-Attention Following Vaswani et al. (2017), we map the matrix of input vectors X ∈ Rn×d (d represents the mapped dimension) to queries Q, keys K and values V matrices by using different linear projections. Then, the self-attention output C ∈ Rn×d is a weighted sum of values:\nC = softmax ( QK>√ dk ) V . (1)\nWe concatenate the output of BiLSTM and selfattention as the final encoding representation:\nE = H ||C, (2)\nwhere E = {e1, . . . , en} ∈ Rn×2d and || is concatenation operation."
    }, {
      "heading" : "3.2 Token-Level Intent Detection Decoder",
      "text" : "Inspired by Qin et al. (2019), we perform a tokenlevel multi-label multi-intent detection, where we predict multiple intents on each token and the sentence results are obtained by voting for all tokens. Specifically, we first feed the contextual encoding E into an intent-aware BiLSTM to enhance its task-specific representations:\nhIt = BiLSTM ( et,h I t−1,h I t+1 ) . (3)\nThen, hIt is used for intent detection, using:\nIt=σ(W I(LeakyReLU(W h h I t+bh))+bI), (4)\nwhere It denotes the intent results at the t-th word; σ denotes the sigmoid activation function; W h and W I are the trainable matrix parameters.\nFinally, the sentence intent results oIk can be obtained by:\noI = {oIk|( n∑\ni=1\n1[I(i,k) > 0.5]) > n/2}, (5)\nwhere I(i,k) represents the classification result of token i for oIk.\nWe predict the label as the utterance intent when it gets more than half positive predictions in all n tokens. For example, if I1 = {0.9, 0.8, 0.7, 0.1}, I2 = {0.8, 0.2, 0.7, 0.4}, I3 = {0.9, 0.3, 0.2, 0.3}, from three tokens, we get {3, 2, 1, 0} positive votes (> 0.5) for four intents respectively. Thus the index where more than half of the votes ( > 3/2 ) were obtained was oI1 and o I 3, we predict intents o\nI = {oI1, oI3}."
    }, {
      "heading" : "3.3 Slot Filling Decoder",
      "text" : "One main advantage of our framework is the proposed global-locally graph interaction network for slot filling, which is a non-autoregressive paradigm, achieving the slot filling decoding in parallel. In the following, we first describe the slot-aware LSTM (§3.3.1) to obtain the slot-aware representations, and then show how to apply the global-locally graph interaction layer (§3.3.2) for decoding."
    }, {
      "heading" : "3.3.1 Slot-aware LSTM",
      "text" : "We utilize a BiLSTM to produce the slot-aware hidden representation S = (s1, . . . , sn). At each decoding step t, the decoder state st calculating by:\nst = BiLSTM ( It || et, st−1, st+1 ) , (6)\nwhere et denotes the aligned encoder hidden state and It denotes the predicted intent information."
    }, {
      "heading" : "3.3.2 Global-locally Graph Interaction Layer",
      "text" : "The proposed global-locally graph interaction layer consists of two main components: one is a local slot-aware graph interaction network to model dependency across slots and another is the proposed global intent-slot graph interaction network to consider the interaction between intents and slots.\nIn this section, we first describe the vanilla graph attention network. Then, we illustrate the local slot-aware and global intent-slot graph interaction network, respectively.\nVanilla Graph Attention Network A graph attention network (GAT) (Veličković et al., 2018) is a variant of graph neural network, which fuses the graph-structured information and node features within the model. Its masked self-attention layers allow a node to attend to neighborhood features and learn different attention weights, which can automatically determine the importance and relevance between the current node with its neighborhood.\nIn particular, for a given graph with N nodes, one-layer GAT take the initial node features H̃ = {h̃1, . . . , h̃N}, h̃n ∈ RF as input, aiming at producing more abstract representation, H̃ ′ = {h̃′1, . . . , h̃ ′ N}, h̃ ′ n ∈ RF ′ , as its output. The attention mechanism of a typical GAT can be summarized as below:\nh̃ ′ i = || K k=1 σ (∑ j∈Ni α k ijW k hh̃j ) , (7)\nαij = exp(LeakyReLU(a>[W hh̃i‖W hh̃j ]))∑\nj′∈Ni exp (LeakyReLU\n( a>[W hh̃i‖W hh̃ ′ j ] ) ) ,(8)\nwhere W h ∈ RF ′×F and a ∈ R2F ′ are the trainable weight matrix; Ni denotes the neighbors of node i (including i); αij is the normalized attention coefficients and σ represents the nonlinearity activation function; K is the number of heads.\nLocal Slot-aware Graph Interaction Layer Given slot decode hidden representations S = (s1, . . . , sn), we construct a local slot-aware graph where each slot hidden node connects to other slots. This allows the model to achieve to model the dependency across slots, alleviating the uncoordinated slots problem. Specifically, we construct the graph G = (V, E) in the following way,\nVertices We define the V as the vertices set. Each word slot is represented as a vertex. Each vertex is initialized with the corresponding slot hidden representation. Thus, the first layer states vector for all nodes is S1 = S = (s1, . . . , sn).\nEdges Since we aim to model dependency across slots, we construct a slot-aware graph interaction layer so that the dependency relationship can be propagated from neighbor nodes to the current node. Each slot can connect other slots with a window size. For node Si, only {Si−m, . . . ,Si+m} will be connected where m is a hyper-parameter denotes the size of sliding window that controls the length of utilizing utterance context.\nInformation Aggregation The aggregation process at l-th layer can be defined as:\nsl+1i = σ ( ∑ j∈Ni αijW ls l j ) , (9)\nwhere Ni is a set of vertices that denotes the connected slots.\nAfter stacking L layer, we obtain the contextual slot-aware local hidden features SL+1 ={sL+11 , . . . , sL+1n }\nGlobal Slot-Intent Graph Interaction Layer To achieve sentence-level intent-slot interaction, we construct a global slot-intent interaction graph where all predicted multiple intents and sequence slots are connected, achieving to output slot sequences in parallel. Specifically, we construct the graph G = (V, E) in the following way,\nVertices As we model the interaction between intent and slot token, we have n +m number of nodes in the graph where n is the sequence length and m is the number of intent labels predicted by the intent decoder. The input of slot token feature is G[S,1] = SL+1 ={sL+11 , . . . , sL+1n } which is produced by slot-aware local interaction graph network while the input intent feature is an embedding G[I,1] = {φemb(oI1), . . . , φemb(oIm)} where φemb is a trainable embedding matrix. The first layer states vector for slot and intent nodes is G1 = {G[I,1] , G[S,1] } = {φemb(oI1), . . . , φemb(oIm), sL+11 , . . . , s L+1 n }\nEdges There are three types of connections in this graph network.\n• intent-slot connection: Since slots and intents are highly tied, we construct the intent-slot connection to model the interaction between the two tasks. Specifically, each slot connects all predicted multiple intents to automatically capture relevant intent information.\n• slot-slot connection: We construct the slotslot connection where each slot node connects other slots with the window size to further model the slot dependency and incorporate the bidirectional contextual information.\n• intent-intent connection: Following Qin et al. (2020b), we connect all the intent nodes to each other to model the relationship between each intent, since all of them express the same utterance’s intent.\nInformation Aggregation The aggregation process of the global GAT layer can be formulated as:\ng [S,l+1] i = σ( ∑ j∈GS αijW gg [S,l] j + ∑ j∈GI αijW gg [I,l] j ), (10) where GS and GI are vertices sets which denotes the connected slots and intents, respectively."
    }, {
      "heading" : "3.3.3 Slot Prediction",
      "text" : "After L layers’ propagation, we obtain the final slot representation G[S,L+1] for slot prediction.\nySt = softmax ( W sg [S,L+1] t ) , (11)\noSt = argmax(y S t ), (12)\nwhere W s is a trainable parameter and oSt is the predicted slot if the t-th token in an utterance."
    }, {
      "heading" : "3.4 Joint Training",
      "text" : "Following Goo et al. (2018), we adopt a joint training model to consider the two tasks and update parameters by joint optimizing. The intent detection objective is:\nCE(ŷ, y) = ŷ log (y) + (1− ŷ) log (1− y) , (13)\nL1 , − n∑\ni=1 NI∑ j=1 CE(ŷ (j,I) i , y (j,I) i ) . (14)\nSimilarly, the slot filling task objective is:\nL2 , − n∑\ni=1 NS∑ j=1 ŷ (j,S) i log ( y (j,S) i ) , (15)\nwhere NI is the number of single intent labels and NS is the number of slot labels.\nThe final joint objective is formulated as:\nL = αL1 + βL2, (16)\nwhere α and β are hyper-parameters."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We conduct experiments on two publicly available multi-intent datasets.1 One is the MixATIS (Hemphill et al., 1990; Qin et al., 2020b), which includes 13,162 utterances for training, 756 utterances for validation and 828 utterances for testing. Another is MixSNIPS (Coucke et al., 2018; Qin et al., 2020b), with 39,776, 2,198, 2,199 utterances for training, validation and testing."
    }, {
      "heading" : "4.2 Experimental Settings",
      "text" : "The dimensionality of the embedding is 128 and 64 on ATIS and SNIPS, respectively. The dimensionality of the LSTM hidden units is 256. The batch size is 16. The number of the multi head is 4 and 8 on MixATIS and MixSNIPS dataset, respectively. All layer number of graph attention network is set to 2. We use Adam (Kingma and Ba, 2015) to optimize the parameters in our model. For all the experiments, we select the model which works the best on the dev set and then evaluate it on the test set. All experiments are conducted at GeForce RTX 2080Ti and TITAN Xp."
    }, {
      "heading" : "4.3 Baselines",
      "text" : "We compare our model with the following best baselines: (1) Attention BiRNN. Liu and Lane (2016) propose an alignment-based RNN for joint slot filling and intent detection; (2) Slot-Gated Atten. Goo et al. (2018) propose a slot-gated joint model, explicitly considering the correlation between slot filling and intent detection; (3) Bi-Model. Wang et al. (2018) propose the Bi-model to model the bi-directional between the intent detection and slot filling; (4) SF-ID Network. E et al. (2019) proposes the SF-ID network to establish a direct connection between the two tasks; (5) Stack-Propagation. Qin et al. (2019) adopt a stack-propagation framework to explicitly incorporate intent detection for guiding slot filling; (6) Joint Multiple ID-SF. Gangadharaiah and Narayanaswamy (2019) propose a multi-task framework with slot-gated mechanism for multiple intent detection and slot filling; (7) AGIF Qin et al. (2020b) proposes an adaptive interaction network to achieve the fine-grained multi-\n1We adopt the cleaned verison that removes the repeated sentences in original dataset, which is available at https:// github.com/LooperXX/AGIF.\nintent information integration, achieving state-ofthe-art performance."
    }, {
      "heading" : "4.4 Main Results",
      "text" : "Following Goo et al. (2018) and Qin et al. (2020b), we evaluate the performance of slot filling using F1 score, intent prediction using accuracy, the sentence-level semantic frame parsing using overall accuracy. Overall accuracy measures the ratio of sentences for which both intent and slot are predicted correctly in a sentence.\nTable 1 shows the results, we have the following observations: (1) On slot filling task, our framework outperforms the best baseline AGIF in F1 scores on two datasets, which indicates the proposed local slot-aware graph successfully models the dependency across slots, so that the slot filling performance can be improved. (2) More importantly, compared with the AGIF, our framework achieves +2.7% and 1.2% improvements for MixATIS and MixSNIPS on overall accuracy, respectively. We attribute it to the fact that our proposed global intent-slot interaction graph can better capture the correlation between intents and slots, improving the SLU performance."
    }, {
      "heading" : "4.5 Analysis",
      "text" : ""
    }, {
      "heading" : "4.5.1 Speedup",
      "text" : "One of the core contributions of our framework is that the decoding process of slot filling can be significantly accelerated with the proposed\nnon-autoregressive mechanism. We evaluate the speed by running the model on the MixATIS test data in an epoch, fixing the batch size to 32. The comparison results are shown in Table 2. We observe that our model achieves the ×8.2, ×10.8 and ×11.5 speedup compared with SOTA models stack-propagation, Joint Multiple ID-SF and AGIF. This is because that their model utilizes an autoregressive architecture that only performs slot filling word by word, while our non-autoregressive framework can conduct slot filling decoding in parallel. In addition, it’s worth noting that as the batch size gets larger, GL-GIN can achieve better acceleration where our model could achieve ×17.2 speedup compared with AGIF when batch size is 64."
    }, {
      "heading" : "4.5.2 Effectiveness of the Local Slot-aware",
      "text" : "Graph Interaction Layer\nWe study the effectiveness of the local slot-aware interaction graph layer with the following ablation. We remove the local graph interaction layer and directly feed the output of the slot LSTM to the global intent-slot graph interaction layer. We refer it to w/o local GAL in Tabel 3. We can clearly observe that the slot F1 drops by 1.5% and 1.2% on MixATIS and MixSNIPS datasets. We attribute this to the fact that local slot-aware GAL can capture the slot dependency for each token, which helps to alleviate the slot uncoordinated problems. A qualitative analysis can be founded at Section 4.5.6."
    }, {
      "heading" : "4.5.3 Effectiveness of Global Slot-Intent",
      "text" : "Graph Interaction Layer\nIn order to verify the effectiveness of slot-intent global interaction graph layer, we remove the global interaction layer and utilizes the output of local slot-aware GAL module for slot filling. It is named as w/o Global Intent-slot GAL in Table 3. We can observe that the slot f1 drops by 0.9%, 1.3%, which demonstrates that intent-slot graph in-\nteraction layer can capture the correlation between multiple intents, which is beneficial for the semantic performance of SLU system.\nFollowing Qin et al. (2020b), we replace multiple LSTM layers (2-layers) as the proposed globallocally graph layer to verify that the proposed global-locally graph interaction layer rather than the added parameters works. Table 3 (more parameters) shows the results. We observe that our model outperforms more parameters by 1.6% and 2.4% overall accuracy in two datasets, which shows that the improvements come from the proposed Global-locally graph interaction layer rather than the involved parameters."
    }, {
      "heading" : "4.5.4 Effectiveness of the Global-locally",
      "text" : "Graph Interaction Layer\nInstead of using the whole global-locally graph interaction layer for slot filling, we directly leverage the output of slot-aware LSTM to predict each token slot to verify the effect of the global-locally graph interaction layer. We name the experiment as w/o Global-locally GAL in Tabel 3. From the results, We can observe that the absence of global GAT module leads to 3.0% and 5.2% overall accuracy drops on two datasets. This indicates that the\nglobal-locally graph interaction layer encourages our model to leverage slot dependency and intent information, which can improve SLU performance."
    }, {
      "heading" : "4.5.5 Visualization",
      "text" : "To better understand how global-local graph interaction layer affects and contributes to the final result, we visualize the attention value of the Global intent-slot GAL. As is shown in Figure 3, we visualize the dependence of the word “6” on context and intent information. We can clearly observe that token “6” obtains information from all contextual tokens. The information from “and 10” helps to predict the slot, where the prior autoregressive models cannot be achieved due to the generation word by word from left to right."
    }, {
      "heading" : "4.5.6 Qualitative analysis",
      "text" : "We conduct qualitative analysis by providing a case study that consists of two sequence slots which are generated from AGIF and our model. From Table 4, for the word “6”, AGIF predicts its slot label as “O” incorrectly. This is because that AGIF only models its left information, which makes it hard to predict “6” is a time slot. In contrast, our model predicts the slot label correctly. We attribute this to the fact that our proposed global intent-slot interaction layer can model bidirectional contextual information. In addition, our framework predicts the word slot “am” correctly while AGIF predicts it incorrectly (I-airport name follows Bdepart time), indicating that the proposed local slot-\naware graph layer has successfully captured the slot dependency."
    }, {
      "heading" : "4.5.7 Effect of Pre-trained Model",
      "text" : "Following Qin et al. (2019), we explore the pretrained model in our framework. We replace the self-attentive encoder by Roberta (Liu et al., 2019c) with the fine-tuning approach. We keep other components identical to our framework and follow Qin et al. (2019) to consider the first subword label if a word is broken into multiple subwords.\nFigure 4 gives the result comparison of AGIF, GL-GIN and two models with Roberta on two datasets. We have two interesting observations. First, the Roberta-based model remarkably well on two datasets. We attribute this to the fact that pre-trained models can provide rich semantic features, which can help SLU. Second, GL-GIN + Roberta outperforms AGIF+Roberta on both datasets and reaches a new state-of-the-art performance, which further verifies the effectiveness of our proposed framework."
    }, {
      "heading" : "5 Related Work",
      "text" : "Slot Filling and Intent Detection Recently, joint models (Zhang and Wang, 2016; HakkaniTür et al., 2016; Goo et al., 2018; Li et al., 2018; Xia et al., 2018; E et al., 2019; Liu et al., 2019b; Qin et al., 2019; Zhang et al., 2019; Wu et al., 2020; Qin et al., 2021b; Ni et al., 2021) are proposed to consider the strong correlation between intent detection and slot filling have obtained remarkable success. Compared with their work, we focus on jointly modeling multiple intent detection and slot filling while they only consider the single-intent scenario.\nMore recently, multiple intent detection can handle utterances with multiple intents, which has attracted increasing attention. To the end, Xu and Sarikaya (2013) and Kim et al. (2017) begin to explore the multiple intent detection. Gangadharaiah and Narayanaswamy (2019) first apply a multi-task framework with a slot-gate mechanism to jointly model the multiple intent detection and slot fill-\ning. Qin et al. (2020b) propose an adaptive interaction network to achieve the fine-grained multiple intent information integration for token-level slot filling, achieving the state-of-the-art performance. Their models adopt the autoregressive architecture for joint multiple intent detection and slot filling. In contrast, we propose a non-autoregressive approach, achieving parallel decoding. To the best of our knowledge, we are the first to explore a non-autoregressive architecture for multiple intent detection and slot filling.\nGraph Neural Network for NLP Graph neural networks that operate directly on graph structures to model the structural information, which has been applied successfully in various NLP tasks. Linmei et al. (2019) and Huang and Carley (2019) explore graph attention network (GAT) (Veličković et al., 2018) for classification task to incorporate the dependency parser information. Cetoli et al. (2017) and Liu et al. (2019a) apply graph neural network to model the non-local contextual information for sequence labeling tasks. Yasunaga et al. (2017) and Feng et al. (2020a) successfully apply a graph network to model the discourse information for the summarization generation task, which achieved promising performance. Graph structure are successfully applied for dialogue direction (Feng et al., 2020b; Fu et al., 2020; Qin et al., 2020a, 2021a). In our work, we apply a global-locally graph interaction network to model the slot dependency and interaction between the multiple intents and slots."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we investigated a non-autoregressive model for joint multiple intent detection and slot filling. To this end, we proposed a global-locally graph interaction network where the uncoordinatedslots problem can be addressed with the proposed local slot-aware graph while the interaction between intents and slots can be modeled by the proposed global intent-slot graph. Experimental results on two datasets show that our framework achieves state-of-the-art performance with ×11.5 times faster than the prior work."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported by the National Key R&D Program of China via grant 2020AAA0106501 and the National Natural Science Foundation of China (NSFC) via grant 61976072 and 61772153. This work was also supported by the Zhejiang Lab’s International Talent Fund for Young Professionals."
    } ],
    "references" : [ {
      "title" : "Graph convolutional networks for named entity recognition",
      "author" : [ "Alberto Cetoli", "Stefano Bragaglia", "Andrew O’Harney", "Marc Sloan" ],
      "venue" : "In Proceedings of the 16th International Workshop on Treebanks and Linguistic Theories,",
      "citeRegEx" : "Cetoli et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Cetoli et al\\.",
      "year" : 2017
    }, {
      "title" : "Snips voice platform: an embedded spoken language understanding",
      "author" : [ "Alice Coucke", "Alaa Saade", "Adrien Ball", "Théodore Bluche", "Alexandre Caulier", "David Leroy", "Clément Doumouro", "Thibault Gisselbrecht", "Francesco Caltagirone", "Thibaut Lavril" ],
      "venue" : null,
      "citeRegEx" : "Coucke et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Coucke et al\\.",
      "year" : 2018
    }, {
      "title" : "A novel bi-directional interrelated model for joint intent detection and slot filling",
      "author" : [ "Haihong E", "Peiqing Niu", "Zhongfu Chen", "Meina Song." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "E et al\\.,? 2019",
      "shortCiteRegEx" : "E et al\\.",
      "year" : 2019
    }, {
      "title" : "2020a. Dialogue discourseaware graph convolutional networks for abstractive meeting summarization",
      "author" : [ "Xiachong Feng", "Xiaocheng Feng", "Bing Qin", "Xinwei Geng", "Ting Liu" ],
      "venue" : null,
      "citeRegEx" : "Feng et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2020
    }, {
      "title" : "Incorporating commonsense knowledge into abstractive dialogue summarization via heterogeneous graph networks",
      "author" : [ "Xiachong Feng", "Xiaocheng Feng", "Bing Qin", "Ting Liu." ],
      "venue" : "arXiv preprint arXiv:2010.10044.",
      "citeRegEx" : "Feng et al\\.,? 2020b",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2020
    }, {
      "title" : "DRTS parsing with structureaware encoding and decoding",
      "author" : [ "Qiankun Fu", "Yue Zhang", "Jiangming Liu", "Meishan Zhang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6818–6828, Online. As-",
      "citeRegEx" : "Fu et al\\.,? 2020",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2020
    }, {
      "title" : "Joint multiple intent detection and slot labeling for goal-oriented dialog",
      "author" : [ "Rashmi Gangadharaiah", "Balakrishnan Narayanaswamy." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Gangadharaiah and Narayanaswamy.,? 2019",
      "shortCiteRegEx" : "Gangadharaiah and Narayanaswamy.",
      "year" : 2019
    }, {
      "title" : "Slot-gated modeling for joint slot filling and intent prediction",
      "author" : [ "Chih-Wen Goo", "Guang Gao", "Yun-Kai Hsu", "Chih-Li Huo", "Tsung-Chieh Chen", "Keng-Wei Hsu", "YunNung Chen." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chap-",
      "citeRegEx" : "Goo et al\\.,? 2018",
      "shortCiteRegEx" : "Goo et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-domain joint semantic frame parsing using bi-directional rnn-lstm",
      "author" : [ "Dilek Hakkani-Tür", "Gokhan Tur", "Asli Celikyilmaz", "Yun-Nung Vivian Chen", "Jianfeng Gao", "Li Deng", "Ye-Yi Wang." ],
      "venue" : "Proceedings of The 17th Annual Meeting of the Interna-",
      "citeRegEx" : "Hakkani.Tür et al\\.,? 2016",
      "shortCiteRegEx" : "Hakkani.Tür et al\\.",
      "year" : 2016
    }, {
      "title" : "The ATIS spoken language systems pilot corpus",
      "author" : [ "Charles T. Hemphill", "John J. Godfrey", "George R. Doddington." ],
      "venue" : "Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24-27,1990.",
      "citeRegEx" : "Hemphill et al\\.,? 1990",
      "shortCiteRegEx" : "Hemphill et al\\.",
      "year" : 1990
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Syntaxaware aspect level sentiment classification with graph attention networks",
      "author" : [ "Binxuan Huang", "Kathleen Carley." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Huang and Carley.,? 2019",
      "shortCiteRegEx" : "Huang and Carley.",
      "year" : 2019
    }, {
      "title" : "Two-stage multi-intent detection for spoken language understanding",
      "author" : [ "Byeongchang Kim", "Seonghan Ryu", "Gary Geunbae Lee." ],
      "venue" : "Multimedia Tools and Applications, 76(9):11377–11390.",
      "citeRegEx" : "Kim et al\\.,? 2017",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2017
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "A selfattentive model with gate mechanism for spoken language understanding",
      "author" : [ "Changliang Li", "Liang Li", "Ji Qi." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3824–3833, Brussels, Bel-",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Handling rare entities for neural sequence labeling",
      "author" : [ "Yangming Li", "Han Li", "Kaisheng Yao", "Xiaolong Li." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6441–6451, Online. Association for Computa-",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Empirical analysis of unlabeled entity problem in named entity recognition",
      "author" : [ "Yangming Li", "lemao liu", "Shuming Shi" ],
      "venue" : "In International Conference on Learning Representations",
      "citeRegEx" : "Li et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Heterogeneous graph attention networks for semi-supervised short text classification",
      "author" : [ "Hu Linmei", "Tianchi Yang", "Chuan Shi", "Houye Ji", "Xiaoli Li." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Linmei et al\\.,? 2019",
      "shortCiteRegEx" : "Linmei et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention-based recurrent neural network models for joint intent detection and slot filling",
      "author" : [ "Bing Liu", "Ian Lane." ],
      "venue" : "Interspeech 2016, pages 685–689.",
      "citeRegEx" : "Liu and Lane.,? 2016",
      "shortCiteRegEx" : "Liu and Lane.",
      "year" : 2016
    }, {
      "title" : "Contextualized non-local neural networks for sequence learning",
      "author" : [ "Pengfei Liu", "Shuaichen Chang", "Xuanjing Huang", "Jian Tang", "Jackie Chi Kit Cheung." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):6762–6769.",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "CM-net: A novel collaborative memory network for spoken language understanding",
      "author" : [ "Yijin Liu", "Fandong Meng", "Jinchao Zhang", "Jie Zhou", "Yufeng Chen", "Jinan Xu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Liu et al\\.,? 2019b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692,",
      "citeRegEx" : "Liu et al\\.,? 2019c",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Recent advances in deep learning based dialogue systems: A systematic survey",
      "author" : [ "Jinjie Ni", "Tom Young", "Vlad Pandelea", "Fuzhao Xue", "Vinay Adiga", "Erik Cambria" ],
      "venue" : null,
      "citeRegEx" : "Ni et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Ni et al\\.",
      "year" : 2021
    }, {
      "title" : "A stack-propagation framework with token-level intent detection for spoken language understanding",
      "author" : [ "Libo Qin", "Wanxiang Che", "Yangming Li", "Haoyang Wen", "Ting Liu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Qin et al\\.,? 2019",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2019
    }, {
      "title" : "Association for Computational Linguistics",
      "author" : [ "Hong Kong", "China" ],
      "venue" : "Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
      "citeRegEx" : "Kong and China.,? \\Q2087\\E",
      "shortCiteRegEx" : "Kong and China.",
      "year" : 2087
    }, {
      "title" : "Knowing where to leverage: Context-aware graph convolutional network with an adaptive fusion layer for contextual spoken language understanding",
      "author" : [ "Libo Qin", "Wanxiang Che", "Minheng Ni", "Yangming Li", "Ting Liu." ],
      "venue" : "IEEE/ACM Transactions on Audio,",
      "citeRegEx" : "Qin et al\\.,? 2021a",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2021
    }, {
      "title" : "2020a. Co-gat: A co-interactive graph attention network for joint dialog act recognition and sentiment classification",
      "author" : [ "Libo Qin", "Zhouyang Li", "Wanxiang Che", "Minheng Ni", "Ting Liu" ],
      "venue" : null,
      "citeRegEx" : "Qin et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2020
    }, {
      "title" : "2021b. A cointeractive transformer for joint slot filling and intent detection",
      "author" : [ "Libo Qin", "Tailu Liu", "Wanxiang Che", "Bingbing Kang", "Sendong Zhao", "Ting Liu" ],
      "venue" : null,
      "citeRegEx" : "Qin et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2021
    }, {
      "title" : "2021c. A survey on spoken language understanding: Recent advances and new frontiers",
      "author" : [ "Libo Qin", "Tianbao Xie", "Wanxiang Che", "Ting Liu" ],
      "venue" : null,
      "citeRegEx" : "Qin et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2021
    }, {
      "title" : "AGIF: An adaptive graph-interactive framework for joint multiple intent detection and slot filling",
      "author" : [ "Libo Qin", "Xiao Xu", "Wanxiang Che", "Ting Liu." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1807–1816,",
      "citeRegEx" : "Qin et al\\.,? 2020b",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2020
    }, {
      "title" : "Injecting word information with multi-level word adapter for chinese spoken language understanding",
      "author" : [ "Dechuang Teng", "Libo Qin", "Wanxiang Che", "Sendong Zhao", "Ting Liu" ],
      "venue" : null,
      "citeRegEx" : "Teng et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Teng et al\\.",
      "year" : 2021
    }, {
      "title" : "Spoken language understanding: Systems for extracting semantic information from speech",
      "author" : [ "Gokhan Tur", "Renato De Mori." ],
      "venue" : "John Wiley & Sons.",
      "citeRegEx" : "Tur and Mori.,? 2011",
      "shortCiteRegEx" : "Tur and Mori.",
      "year" : 2011
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30, pages 5998–6008. Cur-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Graph Attention Networks",
      "author" : [ "Petar Veličković", "Guillem Cucurull", "Arantxa Casanova", "Adriana Romero", "Pietro Liò", "Yoshua Bengio." ],
      "venue" : "International Conference on Learning Representations. Accepted as poster.",
      "citeRegEx" : "Veličković et al\\.,? 2018",
      "shortCiteRegEx" : "Veličković et al\\.",
      "year" : 2018
    }, {
      "title" : "A bimodel based RNN semantic frame parsing model for intent detection and slot filling",
      "author" : [ "Yu Wang", "Yilin Shen", "Hongxia Jin." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "SlotRefine: A fast non-autoregressive model for joint intent detection and slot filling",
      "author" : [ "Di Wu", "Liang Ding", "Fan Lu", "Jian Xie." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Zero-shot user intent detection via capsule neural networks",
      "author" : [ "Congying Xia", "Chenwei Zhang", "Xiaohui Yan", "Yi Chang", "Philip Yu." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Xia et al\\.,? 2018",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2018
    }, {
      "title" : "Convolutional neural network based triangular crf for joint intent detection and slot filling",
      "author" : [ "Puyang Xu", "Ruhi Sarikaya." ],
      "venue" : "2013 IEEE Workshop",
      "citeRegEx" : "Xu and Sarikaya.,? 2013",
      "shortCiteRegEx" : "Xu and Sarikaya.",
      "year" : 2013
    }, {
      "title" : "Graph-based neural multi-document summarization",
      "author" : [ "Michihiro Yasunaga", "Rui Zhang", "Kshitijh Meelu", "Ayush Pareek", "Krishnan Srinivasan", "Dragomir Radev." ],
      "venue" : "Proceedings of the 21st Conference on Computational Natural Language Learning",
      "citeRegEx" : "Yasunaga et al\\.,? 2017",
      "shortCiteRegEx" : "Yasunaga et al\\.",
      "year" : 2017
    }, {
      "title" : "Pomdp-based statistical spoken dialog systems: A review",
      "author" : [ "Steve Young", "Milica Gašić", "Blaise Thomson", "Jason D Williams." ],
      "venue" : "Proceedings of the IEEE, 101(5):1160–1179.",
      "citeRegEx" : "Young et al\\.,? 2013",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 2013
    }, {
      "title" : "Joint slot filling and intent detection via capsule neural networks",
      "author" : [ "Chenwei Zhang", "Yaliang Li", "Nan Du", "Wei Fan", "Philip Yu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5259–5267, Florence,",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "A joint model of intent determination and slot filling for spoken language understanding",
      "author" : [ "Xiaodong Zhang", "Houfeng Wang." ],
      "venue" : "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI’16, page 2993–2999.",
      "citeRegEx" : "Zhang and Wang.,? 2016",
      "shortCiteRegEx" : "Zhang and Wang.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 39,
      "context" : "Spoken Language Understanding (SLU) (Young et al., 2013) is a critical component in spoken dialog systems, which aims to understand user’s queries.",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 35,
      "context" : ", B-singer followed by I-song) (Wu et al., 2020) due to the non-autoregressive fashion.",
      "startOffset" : 31,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : "Experimental results on two public datasets MixSNIPS (Coucke et al., 2018) and MixATIS (Hemphill et al.",
      "startOffset" : 53,
      "endOffset" : 74
    }, {
      "referenceID" : 9,
      "context" : ", 2018) and MixATIS (Hemphill et al., 1990) show that our framework not only obtains state-of-the-art performance but also enables decoding in parallel.",
      "startOffset" : 20,
      "endOffset" : 43
    }, {
      "referenceID" : 10,
      "context" : "BiLSTM The bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) have been successfully applied to sequence labeling tasks (Li et al.",
      "startOffset" : 39,
      "endOffset" : 73
    }, {
      "referenceID" : 33,
      "context" : "Vanilla Graph Attention Network A graph attention network (GAT) (Veličković et al., 2018) is a variant of graph neural network, which fuses the graph-structured information and node features within the model.",
      "startOffset" : 64,
      "endOffset" : 89
    }, {
      "referenceID" : 9,
      "context" : "1 One is the MixATIS (Hemphill et al., 1990; Qin et al., 2020b), which includes 13,162 utterances for training, 756 utterances for validation and 828 utterances for testing.",
      "startOffset" : 21,
      "endOffset" : 63
    }, {
      "referenceID" : 29,
      "context" : "1 One is the MixATIS (Hemphill et al., 1990; Qin et al., 2020b), which includes 13,162 utterances for training, 756 utterances for validation and 828 utterances for testing.",
      "startOffset" : 21,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : "Another is MixSNIPS (Coucke et al., 2018; Qin et al., 2020b), with 39,776, 2,198, 2,199 utterances for training, validation and testing.",
      "startOffset" : 20,
      "endOffset" : 60
    }, {
      "referenceID" : 29,
      "context" : "Another is MixSNIPS (Coucke et al., 2018; Qin et al., 2020b), with 39,776, 2,198, 2,199 utterances for training, validation and testing.",
      "startOffset" : 20,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : "We use Adam (Kingma and Ba, 2015) to optimize the parameters in our model.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 18,
      "context" : "183 Model MixATIS MixSNIPS Overall(Acc) Slot(F1) Intent(Acc) Overall(Acc) Slot(F1) Intent(Acc) Attention BiRNN (Liu and Lane, 2016) 39.",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 21,
      "context" : "We replace the self-attentive encoder by Roberta (Liu et al., 2019c) with the fine-tuning approach.",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 41,
      "context" : "Slot Filling and Intent Detection Recently, joint models (Zhang and Wang, 2016; HakkaniTür et al., 2016; Goo et al., 2018; Li et al., 2018; Xia et al., 2018; E et al., 2019; Liu et al., 2019b; Qin et al., 2019; Zhang et al., 2019; Wu et al., 2020; Qin et al., 2021b; Ni et al., 2021) are proposed to consider the strong correlation between intent detection and slot filling have obtained remarkable success.",
      "startOffset" : 57,
      "endOffset" : 283
    }, {
      "referenceID" : 7,
      "context" : "Slot Filling and Intent Detection Recently, joint models (Zhang and Wang, 2016; HakkaniTür et al., 2016; Goo et al., 2018; Li et al., 2018; Xia et al., 2018; E et al., 2019; Liu et al., 2019b; Qin et al., 2019; Zhang et al., 2019; Wu et al., 2020; Qin et al., 2021b; Ni et al., 2021) are proposed to consider the strong correlation between intent detection and slot filling have obtained remarkable success.",
      "startOffset" : 57,
      "endOffset" : 283
    }, {
      "referenceID" : 14,
      "context" : "Slot Filling and Intent Detection Recently, joint models (Zhang and Wang, 2016; HakkaniTür et al., 2016; Goo et al., 2018; Li et al., 2018; Xia et al., 2018; E et al., 2019; Liu et al., 2019b; Qin et al., 2019; Zhang et al., 2019; Wu et al., 2020; Qin et al., 2021b; Ni et al., 2021) are proposed to consider the strong correlation between intent detection and slot filling have obtained remarkable success.",
      "startOffset" : 57,
      "endOffset" : 283
    }, {
      "referenceID" : 36,
      "context" : "Slot Filling and Intent Detection Recently, joint models (Zhang and Wang, 2016; HakkaniTür et al., 2016; Goo et al., 2018; Li et al., 2018; Xia et al., 2018; E et al., 2019; Liu et al., 2019b; Qin et al., 2019; Zhang et al., 2019; Wu et al., 2020; Qin et al., 2021b; Ni et al., 2021) are proposed to consider the strong correlation between intent detection and slot filling have obtained remarkable success.",
      "startOffset" : 57,
      "endOffset" : 283
    }, {
      "referenceID" : 2,
      "context" : "Slot Filling and Intent Detection Recently, joint models (Zhang and Wang, 2016; HakkaniTür et al., 2016; Goo et al., 2018; Li et al., 2018; Xia et al., 2018; E et al., 2019; Liu et al., 2019b; Qin et al., 2019; Zhang et al., 2019; Wu et al., 2020; Qin et al., 2021b; Ni et al., 2021) are proposed to consider the strong correlation between intent detection and slot filling have obtained remarkable success.",
      "startOffset" : 57,
      "endOffset" : 283
    }, {
      "referenceID" : 20,
      "context" : "Slot Filling and Intent Detection Recently, joint models (Zhang and Wang, 2016; HakkaniTür et al., 2016; Goo et al., 2018; Li et al., 2018; Xia et al., 2018; E et al., 2019; Liu et al., 2019b; Qin et al., 2019; Zhang et al., 2019; Wu et al., 2020; Qin et al., 2021b; Ni et al., 2021) are proposed to consider the strong correlation between intent detection and slot filling have obtained remarkable success.",
      "startOffset" : 57,
      "endOffset" : 283
    }, {
      "referenceID" : 23,
      "context" : "Slot Filling and Intent Detection Recently, joint models (Zhang and Wang, 2016; HakkaniTür et al., 2016; Goo et al., 2018; Li et al., 2018; Xia et al., 2018; E et al., 2019; Liu et al., 2019b; Qin et al., 2019; Zhang et al., 2019; Wu et al., 2020; Qin et al., 2021b; Ni et al., 2021) are proposed to consider the strong correlation between intent detection and slot filling have obtained remarkable success.",
      "startOffset" : 57,
      "endOffset" : 283
    }, {
      "referenceID" : 40,
      "context" : "Slot Filling and Intent Detection Recently, joint models (Zhang and Wang, 2016; HakkaniTür et al., 2016; Goo et al., 2018; Li et al., 2018; Xia et al., 2018; E et al., 2019; Liu et al., 2019b; Qin et al., 2019; Zhang et al., 2019; Wu et al., 2020; Qin et al., 2021b; Ni et al., 2021) are proposed to consider the strong correlation between intent detection and slot filling have obtained remarkable success.",
      "startOffset" : 57,
      "endOffset" : 283
    }, {
      "referenceID" : 35,
      "context" : "Slot Filling and Intent Detection Recently, joint models (Zhang and Wang, 2016; HakkaniTür et al., 2016; Goo et al., 2018; Li et al., 2018; Xia et al., 2018; E et al., 2019; Liu et al., 2019b; Qin et al., 2019; Zhang et al., 2019; Wu et al., 2020; Qin et al., 2021b; Ni et al., 2021) are proposed to consider the strong correlation between intent detection and slot filling have obtained remarkable success.",
      "startOffset" : 57,
      "endOffset" : 283
    }, {
      "referenceID" : 22,
      "context" : "Slot Filling and Intent Detection Recently, joint models (Zhang and Wang, 2016; HakkaniTür et al., 2016; Goo et al., 2018; Li et al., 2018; Xia et al., 2018; E et al., 2019; Liu et al., 2019b; Qin et al., 2019; Zhang et al., 2019; Wu et al., 2020; Qin et al., 2021b; Ni et al., 2021) are proposed to consider the strong correlation between intent detection and slot filling have obtained remarkable success.",
      "startOffset" : 57,
      "endOffset" : 283
    }, {
      "referenceID" : 33,
      "context" : "(2019) and Huang and Carley (2019) explore graph attention network (GAT) (Veličković et al., 2018) for classification task to incorporate the dependency parser information.",
      "startOffset" : 73,
      "endOffset" : 98
    }, {
      "referenceID" : 4,
      "context" : "Graph structure are successfully applied for dialogue direction (Feng et al., 2020b; Fu et al., 2020; Qin et al., 2020a, 2021a).",
      "startOffset" : 64,
      "endOffset" : 127
    }, {
      "referenceID" : 5,
      "context" : "Graph structure are successfully applied for dialogue direction (Feng et al., 2020b; Fu et al., 2020; Qin et al., 2020a, 2021a).",
      "startOffset" : 64,
      "endOffset" : 127
    } ],
    "year" : 2021,
    "abstractText" : "Multi-intent SLU can handle multiple intents in an utterance, which has attracted increasing attention. However, the state-of-the-art joint models heavily rely on autoregressive approaches, resulting in two issues: slow inference speed and information leakage. In this paper, we explore a non-autoregressive model for joint multiple intent detection and slot filling, achieving more fast and accurate. Specifically, we propose a Global-Locally Graph Interaction Network (GL-GIN) where a local slot-aware graph interaction layer is proposed to model slot dependency for alleviating uncoordinated slots problem while a global intentslot graph interaction layer is introduced to model the interaction between multiple intents and all slots in the utterance. Experimental results on two public datasets show that our framework achieves state-of-the-art performance while being 11.5 times faster.",
    "creator" : "LaTeX with hyperref"
  }
}