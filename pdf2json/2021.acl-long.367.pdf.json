{
  "name" : "2021.acl-long.367.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning Span-Level Interactions for Aspect Sentiment Triplet Extraction",
    "authors" : [ "Lu Xu", "Yew Ken Chia", "Lidong Bing" ],
    "emails" : [ "chia}@mymail.sutd.edu.sg", "l.bing@alibaba-inc.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4755–4766\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4755"
    }, {
      "heading" : "1 Introduction",
      "text" : "Aspect-Based Sentiment Analysis (ABSA) (Liu, 2012; Pontiki et al., 2014) is an aggregation of several fine-grained sentiment analysis tasks, and its various subtasks are designed with the aspect target as the fundamental item. For the example in\n∗ Equal contribution. Lu Xu and Yew Ken Chia are under the Joint PhD Program between Alibaba and Singapore University of Technology and Design.\n1We make our code publicly available at https:// github.com/chiayewken/Span-ASTE.\nFigure 1, the aspect targets are “Windows 8” and “touchscreen functions”. Aspect Sentiment Classification (ASC) (Dong et al., 2014; Zhang et al., 2016; Yang et al., 2017; Li et al., 2018a; Tang et al., 2019) is one of the most well-explored subtasks of ABSA and aims to predict the sentiment polarity of a given aspect target. However, it is not always practical to assume that the aspect target is provided. Aspect Term Extraction (ATE) (Yin et al., 2016; Li et al., 2018b; Ma et al., 2019) focuses on extracting aspect targets, while Opinion Term Extraction (OTE) (Yang and Cardie, 2012; Klinger and Cimiano, 2013; Yang and Cardie, 2013) aims to extract the opinion terms which largely determine the sentiment polarity of the sentence or the corresponding target term. Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019) is the most recently proposed subtask of ABSA, which forms a more complete picture of the sentiment information through the triplet of an aspect target term, the corresponding opinion term, and the expressed sentiment. For the example in Figure 1, there are two triplets: (“Windows 8”, “not enjoy”, Negative) and (“touchscreen functions”, “not enjoy”, Negative).\nThe initial approach to ASTE (Peng et al., 2019) was a two-stage pipeline. The first stage extracts target terms and their sentiments via a joint labeling scheme 2, as well as the opinion terms with stan-\n2For example, the joint tag “B-POS” denotes the beginning\ndard BIOES 3 tags. The second stage then couples the extracted target and opinion terms to determine their paired sentiment relation. We know that in ABSA, the aspect sentiment is mostly determined by the opinion terms expressed on the aspect target (Qiu et al., 2011; Yang and Cardie, 2012). However, this pipeline approach breaks the interaction within the triplet structure. Moreover, pipeline approaches usually suffer from the error propagation problem.\nRecent end-to-end approaches (Wu et al., 2020; Xu et al., 2020b; Zhang et al., 2020) can jointly extract the target and opinion terms and classify their sentiment relation. One drawback is that they heavily rely on word-to-word interactions to predict the sentiment relation for the target-opinion pair. Note that it is common for the aspect targets and opinions to contain multiple words, which accounts for roughly one-third of triplets in the benchmark datasets. However, the previous methods (Wu et al., 2020; Zhang et al., 2020) predict the sentiment polarity for each word-word pair independently, which cannot guarantee their sentiment consistency when forming a triplet. As a result, this prediction limitation on triplets that contain multi-word targets or opinions inevitably hurts the overall ASTE performance. For the example in Figure 1, by only considering the word-to-word interactions, it is easy to wrongly predict that “enjoy” expresses a positive sentiment on “Windows”. Xu et al. (2020b) proposed a position-aware tagging scheme to allow the model to couple each word in a target span with all possible opinion spans, i.e., aspect word to opinion span interactions (or vice versa, aspect span to opinion word interactions). However, it still cannot directly model the span-tospan interactions between the whole target spans and opinion spans.\nIn this paper, we propose a span-based model for ASTE (Span-ASTE), which for the first time directly captures the span-to-span interactions when predicting the sentiment relation of an aspect target and opinion pair. Of course, it can also consider the single-word aspects or opinions properly. Our model explicitly generates span representations for all possible target and opinion spans, and their paired sentiment relation is independently predicted for all possible target and opinion pairs. Span-based methods have shown encouraging per-\nof a target span with positive sentiment polarity. 3A common tagging scheme for sequence labeling, denoting “begin, inside, outside, end and single” respectively.\nformance on other tasks, such as coreference resolution (Lee et al., 2017), semantic role labeling (He et al., 2018a), and relation extraction (Luan et al., 2019; Wadden et al., 2019). However, they cannot be directly applied to the ASTE task due to different task-specific characteristics.\nOur contribution can be summarized as follows:\n• We tailor a span-level approach to explicitly consider the span-to-span interactions for the ASTE task and conduct extensive analysis to demonstrate its effectiveness. Our approach significantly improves performance, especially on triplets which contain multiword targets or opinions.\n• We propose a dual-channel span pruning strategy by incorporating explicit supervision from the ATE and OTE tasks to ease the high computational cost caused by span enumeration and maximize the chances of pairing valid target and opinion candidates together.\n• Our proposed Span-ASTE model outperforms the previous methods significantly not only for the ASTE task, but also for the ATE and OTE tasks on four benchmark datasets with both BiLSTM and BERT encoders."
    }, {
      "heading" : "2 Span-based ASTE",
      "text" : ""
    }, {
      "heading" : "2.1 Task Formulation",
      "text" : "Let X = {x1, x2, ..., xn} denote a sentence of n tokens, let S = {s1,1, s1,2, ..., si,j , ..., sn,n} be the set of all possible enumerated spans in X , with i and j indicating the start and end positions of a span in the sentence. We limit the span length as 0 ≤ j− i ≤ L. The objective of the ASTE task is to extract all possible triplets in X . Each sentiment triplet is defined as (target, opinion, sentiment) where sentiment ∈ {Positive,Negative,Neutral}."
    }, {
      "heading" : "2.2 Model Architecture",
      "text" : "As shown in Figure 2, Span-ASTE consists of three modules: sentence encoding, mention module, and triplet module. For the given example, the sentence is first input to the sentence encoding module to obtain the token-level representation, from which we derive the span-level representation for each enumerated span, such as “did not enjoy”, “Windows 8”. We then adopt the ATE and OTE tasks to supervise our proposed dual-channel span pruning strategy which obtains the pruned target and\nP\nP\n- Span Enumerator - All consecutive subsequences <= 8 words - Span Extractor - Concat(start token (768), end token (768), width (20)) - [bs, num_spans, 1556] - Mention Head - Feedforward (1556 -> 150 -> 150 -> 3) {opinion, target, null} - CrossEntropyLoss - Relation Head - Pruner - Feedforward (1556 -> 150 -> 150 -> 1) - Keep top-k span embeds - K = sequence_length * 0.5 - [bs, top_k, 1556] - Pairwise embeds: - Concat(span_a (1556), span_b (1556), multiply (1556)) - [bs, top_k, top_k, 4668] - Feedforward (4668 -> 150 -> 150 -> 4) {pos, neg, neutral, null} - CrossEntropyLoss - New Relation Head - Opinion Pruner - [bs, top_k, 1556] - Mention loss - Target Pruner - [bs, top_k, 1556] - Mention loss\n- Pairwise embeds: - Restrict to opinion-target pairs\nSpan EnumeratorBERT Span Embed Extractor Mention Head Relation Head Final Loss Input Sequence [bs=1, seq_len] [bs, seq_len, 768] [bs, num_spans, 2] [bs, num_spans, 1556] weight=1.0weight=0.2\nOpinion Start Opinion End Target Start Target End\n+\nPruned TargetsPruned Opinions Φtripledfd dfd\nLegend\n+\n+\n+\n+\nThe\nKorean\ndishes\nare\ntasty\nbut\ncostly\n.\nThe\nKorean dishes\ntasty\ncostly\n.\n+\n+\n+\n+\n+\nContextual Encoding (Eq. 2)\nSpan Enumeration\nThe Korean dishes+\n+ are tasty but costly\nThe, tasty\nThe, costly\nKorean dishes, tasty\nKorean dishes, costly\nTarget & Opinion Pruning (Eq. 3)\n<Opinion> <Opinion> <Null><Null>\nAuxiliary Span Classification (Eq. 10)\n<Null>\n<Pos>\n<Neg>\n<Null>\nPair Enumeration Sentiment Triplet Classification (Eq. 6, 7, 8)\n<Null> <Target><Null>\nLoss (Eq. 11)\nElement-wise addition of mention score\nElement-wise addition of mention score\nConcatenate Operation Legend +\nDistance Embedding FFNN\nFFNN\n<Null>\n<Opinion>\n<Target>\nThe Korean dishes are tasty but costly .\nThe Korean dishes tasty costly .The Korean dishes are tasty but costly\n++ + + + + +\nFFNN\nThe costlytastyKorean dishes\nKorean dishes, tasty\nKorean dishes, costly\nKorean dishes, tasty\nKorean dishes, costly\nThe +\n+\n+\n+\nThe\nKorean\ndishes\nare\ntasty\nand\ncheap\nKorean dishes\ntasty\ncheap\n+\n+\n+\n+\nContextual Encoding (Eq. 2)\nSpan Enumeration\ndishes+\n+ are tasty and cheap\ndishes, tasty\ndishes, cheap\nKorean dishes, tasty\nKorean dishes, cheap\nTwo-Way Span Pruning (Eq. 3)\n<Positive>\n<Negative>\n<Neutral>\nOpinion-Target Pair Enumeration\nSentiment Triplet Classification (Eq. 6, 7, 8)\nLoss (Eq. 11) Concatenate Operation\nLegend +\nDistance Embedding\nFFNN\nFFNN\ndishes\nKorean dishes\n<Invalid> tasty cheap\nThe\nare tasty and cheap\nTop Target Candidates\nTop Opinion Candidates\nInvalid Candidates\nSupervision by Opinion and Target Co-Extraction Subtasks\nSpan Boundary Embedding\nContextual Embedding\nSpan Width Embedding\ndishes Korean dishes tasty cheap\nThe\nare tasty and cheap\nTop Target Candidates Top Opinion Candidates\nInvalid Candidates\n+\n+ + +\n+ +\n+ +\nK\nK\nK\n+\nLegend\nFFNN\nSpan Width EmbeddingConcatenate Operation+ Start-End EmbeddingContextual Embedding Distance EmbeddingTop-k Select Operation\nFFNN\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+ + +\n+\nF\nF\nF\nF\nF\nF\nF\nF\nF\nF\nF\nF\nF\nP+ F\nFigure 2: Span-ASTE model structure.\nopinion candidates, such as “Windows 8” and “not enjoy” respectively. Finally, each target candidate and opinion candidate are coupled to determine the sentiment relation between them."
    }, {
      "heading" : "2.2.1 Sentence Encoding",
      "text" : "We explore two encoding methods to obtain the contextualized representation for each word in a sentence: BiLSTM and BERT.\nBiLSTM We first obtain the word representations {e1, e2, ..., ei, ..., en} from the 300- dimension pre-trained GloVe (Pennington et al., 2014) embeddings which are then contextualized by a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) layer. The ith token is represented as:\nhi = [ −→ hi; ←− hi] (1)\nwhere −→ hi and ←− hi are the hidden states of the forward and backward LSTMs respectively.\nBERT An alternative encoding method is to use a pre-trained language model such as BERT (Devlin et al., 2019) to obtain the contextualized word representations x = [x1,x2, ...,xn]. For words that are tokenized as multiple word pieces, we use mean pooling to aggregate their representations .\nSpan Representation We define each span representation si,j ∈ S as:\nsi,j = { [hi; hj ; fwidth(i, j)] if BiLSTM [xi; xj ; fwidth(i, j)] if BERT (2)\nwhere fwidth(i, j) produces a trainable feature embedding representing the span width (i.e., j− i+1).\nBesides the concatenation of the start token, end token, and width representations, the span representation si,j can also be formed by max-pooling or mean-pooling across all token representations of the span from position i to j. The experimental results can be found in the ablation study."
    }, {
      "heading" : "2.2.2 Mention Module",
      "text" : "ATE & OTE Tasks We employ the ABSA subtasks of ATE and OTE to guide our dual-channel span pruning strategy through the scores of the predicted opinion and target span. Note that the target terms and opinion terms are not yet paired together at this stage. The mention module takes the representation of each enumerated span si,j as input and predicts the mention types m ∈ {Target, Opinion, Invalid}.\nP (m|si,j) = softmax(FFNNm(si,j)) (3)\nwhere FFNN denotes a feed-forward neural network with non-linear activation.\nPruned Target and Opinion For a sentence X of length n, the number of enumerated spans is O(n2), while the number of possible pairs between all opinion and target candidate spans is O(n4) at the later stage (i.e., the triplet module). As such, it is not computationally practical to consider all possible pairwise interactions when using a spanbased approach. Previous works (Luan et al., 2019; Wadden et al., 2019) employ a pruning strategy to reduce the number of spans, but they only prune the spans to a single pool which is a mix of different mention types. This strategy does not fully consider\nthe structure of an aspect sentiment triplet as it does not recognize the fundamental difference between a target and an opinion term. Hence, we propose to use a dual-channel pruning strategy which results in two separate pruned pools of aspects and opinions. This minimizes computational costs while maximizing the chance of pairing valid opinion and target spans together. The opinion and target candidates are selected based on the scores of the mention types for each span based on Equation 3:\nΦtarget(si,j) = P (m = target|si,j) Φopinion(si,j) = P (m = opinion|si,j)\n(4)\nWe use the mention scores Φtarget and Φopinion to select the top candidates from the enumerated spans and obtain the target candidate pool St = {..., sta,b, ...} and the opinion candidate pool So = {..., soc,d, ...} respectively. To consider a proportionate number of candidates for each sentence, the number of selected spans for both pruned target and opinion candidates is nz, where n is the sentence length and z is a threshold hyper-parameter. Note that although the pruning operation prevents the gradient flow back to the FFNN in the mention module, it is already receiving supervision from the ATE and OTE tasks. Hence, our model can be trained end-to-end without any issue or instability."
    }, {
      "heading" : "2.2.3 Triplet Module",
      "text" : "Target Opinion Pair Representation We obtain the target-opinion pair representation by coupling each target candidate representation sta,b ∈ St with each opinion candidate representation soc,d ∈ So:\ngsta,b,s o c,d\n= [sta,b; s o c,d; fdistance(a, b, c, d)] (5)\nwhere fdistance(a, b, c, d) produces a trainable feature embedding based on the distance (i.e., min(|b− c|, |a− d|)) between the target and opinion spans, following (Lee et al., 2017; He et al., 2018a; Xu et al., 2020b).\nSentiment Relation Classifier Then, we input the span pair representation gsta,b,soc,d to a feed-forward neural network to determine the probability of sentiment relation r ∈ R = {Positive,Negative,Neutral, Invalid} between the target sta,b and the opinion s o c,d:\nP (r|sta,b, soc,d) = softmax(FFNNr(gsta,b,soc,d)) (6) Invalid here indicates that the target and opinion pair has no valid sentiment relationship."
    }, {
      "heading" : "2.3 Training",
      "text" : "The training objective is defined as the sum of the negative log-likelihood from both the mention module and triplet module.\nL =− ∑\nsi,j∈S logP (m∗i,j |si,j)\n− ∑\nsta,b∈St,s o c,d∈So\nlogP (r∗|sta,b, soc,d) (7)\nwhere m∗i,j is the gold mention type of the span si,j , and r∗ is the gold sentiment relation of the target and opinion span pair (sta,b, s o c,d). S indicates the enumerated span pool; St and So are the pruned target and opinion span candidates."
    }, {
      "heading" : "3 Experiment",
      "text" : ""
    }, {
      "heading" : "3.1 Datasets",
      "text" : "Our proposed Span-ASTE model is evaluated on four ASTE datasets released by Xu et al. (2020b), which include three datasets in the restaurant domain and one dataset in the laptop domain. The first version of the ASTE datasets are released by Peng et al. (2019). However, it is found that not all triplets are explicitly annotated (Xu et al., 2020b; Wu et al., 2020). Xu et al. (2020b) refined the datasets with the missing triplets and removed triplets with conflicting sentiments. Note that these\nfour benchmark datasets are derived from the SemEval Challenge (Pontiki et al., 2014, 2015, 2016), and the opinion terms are retrieved from (Fan et al., 2019). Table 1 shows the detailed statistics."
    }, {
      "heading" : "3.2 Experiment Settings",
      "text" : "When using the BiLSTM encoder, the pre-trained GloVe word embeddings are trainable. The hidden size of the BiLSTM encoder is 300 and the dropout rate is 0.5. In the second setting, we finetune the pre-trained BERT (Devlin et al., 2019) to encode each sentence. Specifically, we use the uncased version of BERTbase. The model is trained for 10 epochs with a linear warmup for 10% of the training steps followed by a linear decay of the learning rate to 0. We employ AdamW as the optimizer with the maximum learning rate of 5e-5 for transformer weights and weight decay of 1e-2. For other parameter groups, we use a learning rate of 1e-3 with no weight decay. The maximum span length L is set as 8. The span pruning threshold z is set as 0.5. We select the best model weights based on the F1 scores on the development set and the reported results are the average of 5 runs with different random seeds. 4"
    }, {
      "heading" : "3.3 Baselines",
      "text" : "The baselines can be summarized as two groups: pipeline methods and end-to-end methods.\nPipeline For the pipeline approaches listed below, they are modified by Peng et al. (2019) to extract the aspect terms together with their associated sentiments via a joint labeling scheme, and\n4See Appendix for more experimental settings, and also the dev results on the four datasets.\nopinion terms with BIOES tags at the first stage. At the second stage, the extracted targets and opinions are then paired to determine if they can form a valid triplet. Note that these approaches employ different methods to obtain the features for the first stage. CMLA+ (Wang et al., 2017) employs an attention mechanism to consider the interaction between aspect terms and opinion terms. RINANTE+ (Dai and Song, 2019) adopts a BiLSTM-CRF model with mined rules to capture the dependency relations. Li-unified-R (Li et al., 2019) uses a unified tagging scheme to jointly extract the aspect term and associated sentiment. Peng et al. (2019) includes dependency relation information when considering the interaction between the aspect and opinion terms.\nEnd-to-end The end-to-end methods aim to jointly extract full triplets in a single stage. Previous work by Zhang et al. (2020) and Wu et al. (2020) independently predict the sentiment relation for all possible word-word pairs, hence they require decoding heuristics to determine the overall sentiment polarity of a triplet. JET (Xu et al., 2020b) models the ASTE task as a structured prediction problem with a position-aware tagging scheme to capture the interaction of the three elements in a triplet."
    }, {
      "heading" : "3.4 Experiment Results",
      "text" : "Table 2 compares Span-ASTE with previous models in terms of Precision (P.), Recall (R.), and F1 scores on four datasets. Under the F1 metric, our model consistently outperforms the previous works for both BiLSTM and BERT sentence encoders. In most cases, our model significantly out-\nperforms other end-to-end methods in both precision and recall. We also observe that the two strong pipeline methods (Li et al., 2019; Peng et al., 2019) achieved competitive recall results, but their overall performance is much worse due to the low precision. Specifically, using the BiLSTM encoder with GloVe embedding, our model outperforms the best pipeline model (Peng et al., 2019) by 15.62, 8.93, 5.24, and 10.16 F1 points on the four datasets. This result indicates that our end-to-end approach can effectively encode the interaction between target and opinion spans, and also alleviates the error propagation. In general, the other end-to-end methods are also more competitive than the pipeline methods. However, due to the limitations of relying on word-level interactions, their performances are less encouraging in a few cases, such as the results on Lap 14 and Rest 15. With the BERT encoder, all three end-to-end models achieve much stronger performance than their LSTM-based versions, which is consistent with previous findings (Devlin et al., 2019). Our approach outperforms the previous best results GTS (Wu et al., 2020) by 4.35, 5.02, 3.12, and 2.33 F1 points on the four datasets."
    }, {
      "heading" : "3.5 Additional Experiments",
      "text" : "As mentioned in Section 2.2.2, we employ the ABSA subtasks of ATE and OTE to guide our span pruning strategy. To examine if Span-ASTE can effectively extract target spans and opinion spans, we also evaluate our model on the ATE and OTE tasks on the four datasets. Table 3 shows the comparisons of our approach and the previous method GTS (Wu et al., 2020). 5 Without additional retraining or tuning, our model can directly address the ATE and OTE tasks, with significant performance improvement than GTS in terms of F1 scores on both tasks. Even though GTS shows a better recall score on the Rest 16 dataset, the low precision score results in worse F1 performance. The better overall performance indicates that our span-level method not only benefits the sentiment triplet extraction, but also improves the extraction of target and opinion terms by considering the semantics of each whole span rather than relying on decoding heuristics of tagging-based methods.\n5See Appendix for the target and opinion data statistics. Note that the JET model (Xu et al., 2020b) is not able to directly solve the ATE and OTE tasks unless the evaluation is conducted based on the triplet predictions. We include such comparisons in the Appendix."
    }, {
      "heading" : "4 Analysis",
      "text" : ""
    }, {
      "heading" : "4.1 Comparison of Single-word and Multi-word Spans",
      "text" : "We compare the performance of Span-ASTE with the previous model GTS (Wu et al., 2020) for the following two settings in Table 4: Single-Word: Both target and opinion terms in a triplet are singleword spans, Multi-Word: At least one of the target or opinion terms in a triplet is a multi-word span. For the single-word setting, our method shows consistent improvement in terms of both precision and recall score on the four datasets, which results in the improvement of F1 score. When we compare the evaluations for multi-word triplets, our model achieves more significant improvements for F1 scores. Compared to precision, our recall shows greater improvement over the GTS approach. GTS heavily relies on word-pair interactions to extract triplets, while our methods explicitly consider the span-to-span interactions. Our span enumeration also naturally benefits the recall of multi-word spans. For both GTS and our model, multi-word triplets pose challenges and their F1 results drop by more than 10 points, even more than 20 points for Rest 14. As shown in Table 1, comparing with the single-word triplets, multi-word triplets are common and account for one-third or even half of the datasets. Therefore, a promising direction for future work is to further improve the model’s performance on such difficult triplets.\nTo identify further areas for improvement, we analyze the results for the ASTE task based on whether each sentiment triplet contains a multi-\nword target or multi-word opinion term. From Table 5, the results show that the performance is lower when the triplet contains a multi-word opinion term. This trend can be attributed to the imbalanced data distribution of triplets which contain multi-word target or opinion terms."
    }, {
      "heading" : "4.2 Pruning Efficiency",
      "text" : "To demonstrate the efficiency of the proposed dual-channel pruning strategy, we also compare it to a simpler strategy, denoted as Single-Channel (SC) which does not distinguish between opinion and target candidates. Figure 3 shows the comparisons. Note the mention module under this strategy does not explicitly solve the ATE and OTE tasks as it only predicts mention label m ∈ {V alid, Invalid}, where V alid means the span is either a target or an opinion span and Invalid means the span does not belong to the two groups. Given sentence length n and pruning threshold z, the number of candidates is limited to nz, and hence the computational cost scales with the number of pairwise interactions, n2z2. The dual-channel strategy considers each target-opinion pair where the pruned target and opinion candidate pools both have nz spans. Note that it is possible for the two pools to share some candidates. In comparison, the single-channel strategy considers each\ntarget-opinion pair where the target and opinion candidates are drawn from the same single pool of nz spans. In order to consider at least as many target and opinion candidates as the dual-channel strategy, the single-channel strategy has to scale the threshold z by two, which leads to 4 times more pairs and computational cost. We denote this setting in Figure 3 as SC-Adjusted. When controlling for computational efficiency, there is a significant performance difference between Dual-Channel and Single-Channel in F1 score, especially for lower values of z. Although the performance gap narrows with increasing z, it is not practical for high values. According to our experimental results, we select the dual-channel pruning strategy with z = 0.5 for the reported model."
    }, {
      "heading" : "4.3 Qualitative Analysis",
      "text" : "To illustrate the differences between the models, we present sample sentences from the ASTE test set with the gold labels as well as predictions from GTS (Wu et al., 2020) and Span-ASTE in Figure 4. For the first example, GTS correctly extracts the target term “Windows 8” paired with the opinion term “not enjoy”, but the sentiment is incorrectly predicted as positive. When forming the triplet, their decoding heuristic considers the sentiment inde-\npendently for each word-word pair: {(“Windows”, “not”, Neutral), (“8”, “not”, Neutral), (“Windows”, “enjoy”, Positive), (“8”, “enjoy”, Positive)}. Their heuristic votes the overall sentiment polarity as the most frequent label among the pairs. In the case of a tie (2 neutral and 2 positive), the heuristic has a predefined bias to assign the sentiment polarity to positive. Similarly, the word-level method fails to capture the negative sentiment expressed by “not enjoy” on the other target term “touchscreen functions”. In the second example, it incompletely extracts the target term “Korean dishes”, resulting in the wrong triplet. For both examples, our method is able to accurately extract the target-opinion pairs and determine the overall sentiment even when each term has multiple words."
    }, {
      "heading" : "4.4 Ablation Study",
      "text" : "We conduct an ablation study to examine the performance of different modules and span representation methods, and the results are shown in Table 6. The average F1 denotes the average dev results of Span-ASTE on the four benchmark datasets over 5 runs. Similar to the observation for coreference resolution (Lee et al., 2017), we find that the ASTE performance is reduced when removing the span width and distance embedding. This indicates that the positional information is still useful for the ASTE task as targets and opinions which are far apart or too long are less likely to form a valid span pair. As mentioned in Section 2.2.1, we explore two other methods (i.e., max pooling and mean pooling) to form span representations instead of concatenating the span boundary token representations. The negative results suggest that using pooling to aggregate the span representation is disadvantageous due to the loss of information that is useful for distinguishing valid and invalid spans."
    }, {
      "heading" : "5 Related Work",
      "text" : "Sentiment Analysis is a major Natural Language Understanding (NLU) task (Wang et al., 2019) and has been extensively studied as a classification problem at the sentence level (Raffel et al., 2020; Lan et al., 2020; Yang et al., 2020). Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014) addresses various sentiment analysis tasks at a finegrained level. As mentioned in the Section 1, the subtasks mainly include ASC (Dong et al., 2014; Zhang et al., 2016; Chen et al., 2017; He et al., 2018b; Li et al., 2018a; Peng et al., 2018; Wang and Lu, 2018; He et al., 2019; Li and Lu, 2019; Xu et al., 2020a), ATE (Qiu et al., 2011; Yin et al., 2016; Li et al., 2018b; Ma et al., 2019), OTE (Hu and Liu, 2004; Yang and Cardie, 2012; Klinger and Cimiano, 2013; Yang and Cardie, 2013). There is also another subtask named Target-oriented Opinion Words Extraction (TOWE) (Fan et al., 2019), which aim to extract the corresponding opinion words for a given target term. Another line of research focuses on addressing different subtasks together. Aspect and Opinion Term Co-Extraction (AOTE) aiming to extract the aspect and opinion terms together (Wang et al., 2017; Ma et al., 2019; Dai and Song, 2019) and is often treated as a sequence labeling problem. Note that AOTE does not consider the paired sentiment relationship between each target and opinion term. End-to-End ABSA (Li and Lu, 2017; Ma et al., 2018; Li et al., 2019; He et al., 2019) jointly extracts each aspect\nterm and its associated sentiment in an end-to-end manner. A few other methods are recently proposed to jointly solve three or more subtasks of ABSA. Chen and Qian (2020) proposed a relation aware collaborative learning framework to unify the three fundamental subtasks and achieved strong performance on each subtask and combined task. While Wan et al. (2020) focused more on aspect category related subtasks, such as Aspect Category Extraction and Aspect Category and Target Joint Extraction. ASTE (Peng et al., 2019; Wu et al., 2020; Xu et al., 2020b; Zhang et al., 2020) is the most recent development of ABSA and its aim is to extract and form the aspect term, its associated sentiment, and the corresponding opinion term into a triplet."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this work, we propose a span-level approach - Span-ASTE to learn the interactions between target spans and opinion spans for the ASTE task. It can address the limitation of the existing approaches that only consider word-to-word interactions. We also propose to include the ATE and OTE tasks as supervision for our dual-channel pruning strategy to reduce the number of enumerated target and opinion candidates to increase the computational efficiency and maximize the chances of pairing valid target and opinion candidates together. Our method significantly outperforms the previous methods for ASTE as well as ATE and OTE tasks and our analysis demonstrates the effectiveness of our approach. While we achieve strong performance on the ASTE task, the performance can be mostly attributed to the improvement on the multi-word triplets. As discussed in Section 4.1, there is still a significant performance gap between single-word and multiword triplets, and this can be a potential area for future work."
    }, {
      "heading" : "A Additional Experimental Settings",
      "text" : "We run our model experiments on a Nvidia Tesla V100 GPU, with CUDA version 10.2 and PyTorch version 1.6.0. The average run time for BERTbased model is 157 sec/epoch, 115 sec/epoch, 87 sec/epoch, and 111 sec/epoch for Rest 14, Lap 14, Rest 15, and Rest 16 respectively. The total number of parameters is 2.24M when GloVe is used, and is 110M when BERT base is used. The feed-forward neural networks in the mention module and triplet module have 2 hidden layers and hidden size of 150. We use ReLU activation and dropout of 0.4 after each hidden layer. We use Xavier Normal weight initialization for the feed-forward parameters. The span width and distance embeddings have 20 and 128 dimensions respectively. Their input values are bucketed (Gardner et al., 2017) before being fed to an embedding matrix lookup: [0, 1, 2, 3, 4, 5-7, 8-15, 16-31, 32-63, 64+]. During training, the model parameters are updated after each sentence which results in a batch size of 1. For each input text sequence, we restrict it to a maximum of 512 tokens."
    }, {
      "heading" : "B Additional Data Statistics",
      "text" : "Table 9 shows the number of target terms and opinion terms on the four datasets."
    }, {
      "heading" : "C Dev Results",
      "text" : "Table 10 shows the results of our model on the development datasets."
    }, {
      "heading" : "D Additional Comparisons",
      "text" : "As mentioned by footnote 5 in Section 3.5, we cannot make a direct comparison with the JET model (Xu et al., 2020b), as it is not able to directly solve the ATE and OTE tasks unless the evaluation is conducted based on the triplet results. Table 7 shows such comparisons. Our proposed method\ngenerally outperforms the previous two end-to-end approaches on the four datasets.\nAs mentioned in Table 3, it is challenging to make a fair comparison between the previous ABSA framework RACL (Chen and Qian, 2020), which also address the ATE and OTE tasks while solving other ABSA subtasks, and our approach as well as the GTS (Wu et al., 2020). This is because the mentioned approaches have different task settings. The RACL considers the sentiment polarity on the target terms when solving the ATE and OTE tasks, but GTS and our method both consider the pairing relation between target and opinion terms. However, for reference, Table 8 shows the compar-\nisons of the three methods on the ATE and OTE tasks on the datasets released by Xu et al. (2020b)."
    } ],
    "references" : [ {
      "title" : "Recurrent attention network on memory for aspect sentiment analysis",
      "author" : [ "Peng Chen", "Zhongqian Sun", "Lidong Bing", "Wei Yang." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Relation-aware collaborative learning for unified aspect-based sentiment analysis",
      "author" : [ "Zhuang Chen", "Tieyun Qian." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Chen and Qian.,? 2020",
      "shortCiteRegEx" : "Chen and Qian.",
      "year" : 2020
    }, {
      "title" : "Neural aspect and opinion term extraction with mined rules as weak supervision",
      "author" : [ "Hongliang Dai", "Yangqiu Song." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Dai and Song.,? 2019",
      "shortCiteRegEx" : "Dai and Song.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proc. of NAACL.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Adaptive recursive neural network for target-dependent twitter sentiment classification",
      "author" : [ "Li Dong", "Furu Wei", "Chuanqi Tan", "Duyu Tang", "Ming Zhou", "Ke Xu." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Dong et al\\.,? 2014",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2014
    }, {
      "title" : "Target-oriented opinion words extraction with target-fused neural sequence labeling",
      "author" : [ "Zhifang Fan", "Zhen Wu", "Xinyu Dai", "Shujian Huang", "Jiajun Chen." ],
      "venue" : "Porc. of NAACL.",
      "citeRegEx" : "Fan et al\\.,? 2019",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2019
    }, {
      "title" : "Allennlp: A deep semantic natural language processing platform",
      "author" : [ "Matt Gardner", "Joel Grus", "Mark Neumann", "Oyvind Tafjord", "Pradeep Dasigi", "Nelson F. Liu", "Matthew Peters", "Michael Schmitz", "Luke S. Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Gardner et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Gardner et al\\.",
      "year" : 2017
    }, {
      "title" : "Jointly predicting predicates and arguments in neural semantic role labeling",
      "author" : [ "Luheng He", "Kenton Lee", "Omer Levy", "Luke Zettlemoyer." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "He et al\\.,? 2018a",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2018
    }, {
      "title" : "Effective attention modeling for aspect-level sentiment classification",
      "author" : [ "Ruidan He", "Wee Sun Lee", "Hwee Tou Ng", "Daniel Dahlmeier." ],
      "venue" : "Proc. of COLING.",
      "citeRegEx" : "He et al\\.,? 2018b",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2018
    }, {
      "title" : "An interactive multi-task learning network for end-to-end aspect-based sentiment analysis",
      "author" : [ "Ruidan He", "Wee Sun Lee", "Hwee Tou Ng", "Daniel Dahlmeier." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "He et al\\.,? 2019",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2019
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9:1735–",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Mining and summarizing customer reviews",
      "author" : [ "Minqing Hu", "Bing Liu." ],
      "venue" : "Proc. of ACM SIGKDD.",
      "citeRegEx" : "Hu and Liu.,? 2004",
      "shortCiteRegEx" : "Hu and Liu.",
      "year" : 2004
    }, {
      "title" : "Joint and pipeline probabilistic models for fine-grained sentiment analysis: Extracting aspects, subjective phrases and their relations",
      "author" : [ "R. Klinger", "P. Cimiano." ],
      "venue" : "2013 IEEE 13th International Conference on Data Mining Workshops.",
      "citeRegEx" : "Klinger and Cimiano.,? 2013",
      "shortCiteRegEx" : "Klinger and Cimiano.",
      "year" : 2013
    }, {
      "title" : "Albert: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "Proc. of ICLR.",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "End-to-end neural coreference resolution",
      "author" : [ "Kenton Lee", "Luheng He", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Lee et al\\.,? 2017",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning latent sentiment scopes for entity-level sentiment analysis",
      "author" : [ "Hao Li", "Wei Lu." ],
      "venue" : "Proc. of AAAI.",
      "citeRegEx" : "Li and Lu.,? 2017",
      "shortCiteRegEx" : "Li and Lu.",
      "year" : 2017
    }, {
      "title" : "Learning explicit and implicit structures for targeted sentiment analysis",
      "author" : [ "Hao Li", "Wei Lu." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Li and Lu.,? 2019",
      "shortCiteRegEx" : "Li and Lu.",
      "year" : 2019
    }, {
      "title" : "Transformation networks for target-oriented sentiment classification",
      "author" : [ "Xin Li", "Lidong Bing", "Wai Lam", "Bei Shi." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Li et al\\.,? 2018a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "A unified model for opinion target extraction and target sentiment prediction",
      "author" : [ "Xin Li", "Lidong Bing", "Piji Li", "Wai Lam." ],
      "venue" : "Proc. of AAAI.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Aspect term extraction with history attention and selective transformation",
      "author" : [ "Xin Li", "Lidong Bing", "Piji Li", "Wai Lam", "Zhimou Yang." ],
      "venue" : "Proc. of IJCAI.",
      "citeRegEx" : "Li et al\\.,? 2018b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Sentiment analysis and opinion mining",
      "author" : [ "Bing Liu." ],
      "venue" : "Synthesis lectures on human language technologies, 5(1):1–167.",
      "citeRegEx" : "Liu.,? 2012",
      "shortCiteRegEx" : "Liu.",
      "year" : 2012
    }, {
      "title" : "A general framework for information extraction using dynamic span graphs",
      "author" : [ "Yi Luan", "Dave Wadden", "Luheng He", "Amy Shah", "Mari Ostendorf", "Hannaneh Hajishirzi." ],
      "venue" : "Proc. of NAACL.",
      "citeRegEx" : "Luan et al\\.,? 2019",
      "shortCiteRegEx" : "Luan et al\\.",
      "year" : 2019
    }, {
      "title" : "Joint learning for targeted sentiment analysis",
      "author" : [ "Dehong Ma", "Sujian Li", "Houfeng Wang." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Ma et al\\.,? 2018",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploring sequence-tosequence learning in aspect term extraction",
      "author" : [ "Dehong Ma", "Sujian Li", "Fangzhao Wu", "Xing Xie", "Houfeng Wang." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Ma et al\\.,? 2019",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning multi-grained aspect target sequence for chinese sentiment analysis",
      "author" : [ "Haiyun Peng", "Yukun Ma", "Yang Li", "Erik Cambria." ],
      "venue" : "KnowledgeBased Systems, 148:167–176.",
      "citeRegEx" : "Peng et al\\.,? 2018",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2018
    }, {
      "title" : "Knowing what, how and why: A near complete solution for aspect-based sentiment analysis",
      "author" : [ "Haiyun Peng", "Lu Xu", "Lidong Bing", "Fei Huang", "Wei Lu", "Luo Si." ],
      "venue" : "Proc. of AAAI.",
      "citeRegEx" : "Peng et al\\.,? 2019",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2019
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Semeval-2016 task 5: Aspect based sentiment",
      "author" : [ "Maria Pontiki", "Dimitris Galanis", "Haris Papageorgiou", "Ion Androutsopoulos", "Suresh Manandhar", "Mohammed AL-Smadi", "Mahmoud Al-Ayyoub", "Yanyan Zhao", "Bing Qin", "Orphée De Clercq" ],
      "venue" : null,
      "citeRegEx" : "Pontiki et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Pontiki et al\\.",
      "year" : 2016
    }, {
      "title" : "SemEval-2015 task 12: Aspect based sentiment analysis",
      "author" : [ "Maria Pontiki", "Dimitris Galanis", "Haris Papageorgiou", "Suresh Manandhar", "Ion Androutsopoulos." ],
      "venue" : "Proc. of SemEval.",
      "citeRegEx" : "Pontiki et al\\.,? 2015",
      "shortCiteRegEx" : "Pontiki et al\\.",
      "year" : 2015
    }, {
      "title" : "Semeval-2014 task 4: Aspect based sentiment analysis",
      "author" : [ "Maria Pontiki", "Dimitris Galanis", "John Pavlopoulos", "Harris Papageorgiou", "Ion Androutsopoulos", "Suresh Manandhar." ],
      "venue" : "Proc. of SemEval.",
      "citeRegEx" : "Pontiki et al\\.,? 2014",
      "shortCiteRegEx" : "Pontiki et al\\.",
      "year" : 2014
    }, {
      "title" : "Opinion word expansion and target extraction through double propagation",
      "author" : [ "Guang Qiu", "Bing Liu", "Jiajun Bu", "Chun Chen." ],
      "venue" : "Computational Linguistics, 37(1):9–27.",
      "citeRegEx" : "Qiu et al\\.,? 2011",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2011
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Re-",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Progressive selfsupervised attention learning for aspect-level sentiment analysis",
      "author" : [ "Jialong Tang", "Ziyao Lu", "Jinsong Su", "Yubin Ge", "Linfeng Song", "Le Sun", "Jiebo Luo." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Tang et al\\.,? 2019",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2019
    }, {
      "title" : "Entity, relation, and event extraction with contextualized span representations",
      "author" : [ "David Wadden", "Ulme Wennberg", "Yi Luan", "Hannaneh Hajishirzi." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Wadden et al\\.,? 2019",
      "shortCiteRegEx" : "Wadden et al\\.",
      "year" : 2019
    }, {
      "title" : "Target-aspect-sentiment joint detection for aspect-based sentiment analysis",
      "author" : [ "Hai Wan", "Yufei Yang", "Jianfeng Du", "Yanan Liu", "Kunxun Qi", "Jeff Z. Pan." ],
      "venue" : "Proc. of AAAI.",
      "citeRegEx" : "Wan et al\\.,? 2020",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2020
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Proc. of ICLR.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning latent opinions for aspect-level sentiment classification",
      "author" : [ "Bailin Wang", "Wei Lu." ],
      "venue" : "Proc. of AAAI.",
      "citeRegEx" : "Wang and Lu.,? 2018",
      "shortCiteRegEx" : "Wang and Lu.",
      "year" : 2018
    }, {
      "title" : "Coupled multi-layer attentions for co-extraction of aspect and opinion terms",
      "author" : [ "Wenya Wang", "Sinno Jialin Pan", "Daniel Dahlmeier", "Xiaokui Xiao." ],
      "venue" : "Proc. of AAAI.",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Grid tagging scheme for aspect-oriented fine-grained opinion extraction",
      "author" : [ "Zhen Wu", "Chengcan Ying", "Fei Zhao", "Zhifang Fan", "Xinyu Dai", "Rui Xia." ],
      "venue" : "Findings of EMNLP.",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Aspect sentiment classification with aspect-specific opinion spans",
      "author" : [ "Lu Xu", "Lidong Bing", "Wei Lu", "Fei Huang." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Xu et al\\.,? 2020a",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Position-aware tagging for aspect sentiment triplet extraction",
      "author" : [ "Lu Xu", "Hao Li", "W. Lu", "Lidong Bing." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Xu et al\\.,? 2020b",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Extracting opinion expressions with semi-Markov conditional random fields",
      "author" : [ "Bishan Yang", "Claire Cardie." ],
      "venue" : "Proc. of EMNLP.",
      "citeRegEx" : "Yang and Cardie.,? 2012",
      "shortCiteRegEx" : "Yang and Cardie.",
      "year" : 2012
    }, {
      "title" : "Joint inference for fine-grained opinion extraction",
      "author" : [ "Bishan Yang", "Claire Cardie." ],
      "venue" : "Proc. of ACL.",
      "citeRegEx" : "Yang and Cardie.,? 2013",
      "shortCiteRegEx" : "Yang and Cardie.",
      "year" : 2013
    }, {
      "title" : "Attention based lstm for target dependent sentiment classification",
      "author" : [ "Min Yang", "Wenting Tu", "Jingxuan Wang", "Fei Xu", "Xiaojun Chen." ],
      "venue" : "Proc. of AAAI.",
      "citeRegEx" : "Yang et al\\.,? 2017",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2017
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le." ],
      "venue" : "Proc. of NeurIPS.",
      "citeRegEx" : "Yang et al\\.,? 2020",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    }, {
      "title" : "2020) allow for ATE and OTE tasks",
      "author" : [ "Wu" ],
      "venue" : null,
      "citeRegEx" : "Wu,? \\Q2020\\E",
      "shortCiteRegEx" : "Wu",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "Aspect-Based Sentiment Analysis (ABSA) (Liu, 2012; Pontiki et al., 2014) is an aggregation of several fine-grained sentiment analysis tasks, and its various subtasks are designed with the aspect target as the fundamental item.",
      "startOffset" : 39,
      "endOffset" : 72
    }, {
      "referenceID" : 29,
      "context" : "Aspect-Based Sentiment Analysis (ABSA) (Liu, 2012; Pontiki et al., 2014) is an aggregation of several fine-grained sentiment analysis tasks, and its various subtasks are designed with the aspect target as the fundamental item.",
      "startOffset" : 39,
      "endOffset" : 72
    }, {
      "referenceID" : 4,
      "context" : "Aspect Sentiment Classification (ASC) (Dong et al., 2014; Zhang et al., 2016; Yang et al., 2017; Li et al., 2018a; Tang et al., 2019) is one of the most well-explored subtasks of ABSA and aims to predict the sentiment polarity of a given aspect target.",
      "startOffset" : 38,
      "endOffset" : 133
    }, {
      "referenceID" : 43,
      "context" : "Aspect Sentiment Classification (ASC) (Dong et al., 2014; Zhang et al., 2016; Yang et al., 2017; Li et al., 2018a; Tang et al., 2019) is one of the most well-explored subtasks of ABSA and aims to predict the sentiment polarity of a given aspect target.",
      "startOffset" : 38,
      "endOffset" : 133
    }, {
      "referenceID" : 17,
      "context" : "Aspect Sentiment Classification (ASC) (Dong et al., 2014; Zhang et al., 2016; Yang et al., 2017; Li et al., 2018a; Tang et al., 2019) is one of the most well-explored subtasks of ABSA and aims to predict the sentiment polarity of a given aspect target.",
      "startOffset" : 38,
      "endOffset" : 133
    }, {
      "referenceID" : 32,
      "context" : "Aspect Sentiment Classification (ASC) (Dong et al., 2014; Zhang et al., 2016; Yang et al., 2017; Li et al., 2018a; Tang et al., 2019) is one of the most well-explored subtasks of ABSA and aims to predict the sentiment polarity of a given aspect target.",
      "startOffset" : 38,
      "endOffset" : 133
    }, {
      "referenceID" : 19,
      "context" : "Aspect Term Extraction (ATE) (Yin et al., 2016; Li et al., 2018b; Ma et al., 2019) focuses on extracting aspect targets, while Opinion Term Extraction (OTE) (Yang and Cardie, 2012; Klinger and Cimiano, 2013; Yang and Cardie, 2013) aims to extract the opinion terms which largely determine the sentiment polarity of the sentence or the corresponding target term.",
      "startOffset" : 29,
      "endOffset" : 82
    }, {
      "referenceID" : 23,
      "context" : "Aspect Term Extraction (ATE) (Yin et al., 2016; Li et al., 2018b; Ma et al., 2019) focuses on extracting aspect targets, while Opinion Term Extraction (OTE) (Yang and Cardie, 2012; Klinger and Cimiano, 2013; Yang and Cardie, 2013) aims to extract the opinion terms which largely determine the sentiment polarity of the sentence or the corresponding target term.",
      "startOffset" : 29,
      "endOffset" : 82
    }, {
      "referenceID" : 41,
      "context" : ", 2019) focuses on extracting aspect targets, while Opinion Term Extraction (OTE) (Yang and Cardie, 2012; Klinger and Cimiano, 2013; Yang and Cardie, 2013) aims to extract the opinion terms which largely determine the sentiment polarity of the sentence or the corresponding target term.",
      "startOffset" : 82,
      "endOffset" : 155
    }, {
      "referenceID" : 12,
      "context" : ", 2019) focuses on extracting aspect targets, while Opinion Term Extraction (OTE) (Yang and Cardie, 2012; Klinger and Cimiano, 2013; Yang and Cardie, 2013) aims to extract the opinion terms which largely determine the sentiment polarity of the sentence or the corresponding target term.",
      "startOffset" : 82,
      "endOffset" : 155
    }, {
      "referenceID" : 42,
      "context" : ", 2019) focuses on extracting aspect targets, while Opinion Term Extraction (OTE) (Yang and Cardie, 2012; Klinger and Cimiano, 2013; Yang and Cardie, 2013) aims to extract the opinion terms which largely determine the sentiment polarity of the sentence or the corresponding target term.",
      "startOffset" : 82,
      "endOffset" : 155
    }, {
      "referenceID" : 25,
      "context" : "Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019) is the most recently proposed subtask of ABSA, which forms a more complete picture of the sentiment information through the triplet of an aspect target term, the corresponding opinion term, and the expressed sentiment.",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 25,
      "context" : "The initial approach to ASTE (Peng et al., 2019) was a two-stage pipeline.",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 30,
      "context" : "We know that in ABSA, the aspect sentiment is mostly determined by the opinion terms expressed on the aspect target (Qiu et al., 2011; Yang and Cardie, 2012).",
      "startOffset" : 116,
      "endOffset" : 157
    }, {
      "referenceID" : 41,
      "context" : "We know that in ABSA, the aspect sentiment is mostly determined by the opinion terms expressed on the aspect target (Qiu et al., 2011; Yang and Cardie, 2012).",
      "startOffset" : 116,
      "endOffset" : 157
    }, {
      "referenceID" : 38,
      "context" : "Recent end-to-end approaches (Wu et al., 2020; Xu et al., 2020b; Zhang et al., 2020) can jointly extract the target and opinion terms and classify their sentiment relation.",
      "startOffset" : 29,
      "endOffset" : 84
    }, {
      "referenceID" : 40,
      "context" : "Recent end-to-end approaches (Wu et al., 2020; Xu et al., 2020b; Zhang et al., 2020) can jointly extract the target and opinion terms and classify their sentiment relation.",
      "startOffset" : 29,
      "endOffset" : 84
    }, {
      "referenceID" : 38,
      "context" : "However, the previous methods (Wu et al., 2020; Zhang et al., 2020) predict the sentiment polarity for each word-word pair independently, which cannot guarantee their sentiment consistency when forming a triplet.",
      "startOffset" : 30,
      "endOffset" : 67
    }, {
      "referenceID" : 14,
      "context" : "formance on other tasks, such as coreference resolution (Lee et al., 2017), semantic role labeling (He et al.",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 7,
      "context" : ", 2017), semantic role labeling (He et al., 2018a), and relation extraction (Luan et al.",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 21,
      "context" : ", 2018a), and relation extraction (Luan et al., 2019; Wadden et al., 2019).",
      "startOffset" : 34,
      "endOffset" : 74
    }, {
      "referenceID" : 33,
      "context" : ", 2018a), and relation extraction (Luan et al., 2019; Wadden et al., 2019).",
      "startOffset" : 34,
      "endOffset" : 74
    }, {
      "referenceID" : 26,
      "context" : ", en} from the 300dimension pre-trained GloVe (Pennington et al., 2014) embeddings which are then contextualized by a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) layer.",
      "startOffset" : 46,
      "endOffset" : 71
    }, {
      "referenceID" : 10,
      "context" : ", 2014) embeddings which are then contextualized by a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) layer.",
      "startOffset" : 73,
      "endOffset" : 107
    }, {
      "referenceID" : 3,
      "context" : "BERT An alternative encoding method is to use a pre-trained language model such as BERT (Devlin et al., 2019) to obtain the contextualized word representations x = [x1,x2, .",
      "startOffset" : 88,
      "endOffset" : 109
    }, {
      "referenceID" : 21,
      "context" : "Previous works (Luan et al., 2019; Wadden et al., 2019) employ a pruning strategy to reduce the number of spans, but they only prune the spans to a single pool which is a mix of different mention types.",
      "startOffset" : 15,
      "endOffset" : 55
    }, {
      "referenceID" : 33,
      "context" : "Previous works (Luan et al., 2019; Wadden et al., 2019) employ a pruning strategy to reduce the number of spans, but they only prune the spans to a single pool which is a mix of different mention types.",
      "startOffset" : 15,
      "endOffset" : 55
    }, {
      "referenceID" : 14,
      "context" : ", min(|b− c|, |a− d|)) between the target and opinion spans, following (Lee et al., 2017; He et al., 2018a; Xu et al., 2020b).",
      "startOffset" : 71,
      "endOffset" : 125
    }, {
      "referenceID" : 7,
      "context" : ", min(|b− c|, |a− d|)) between the target and opinion spans, following (Lee et al., 2017; He et al., 2018a; Xu et al., 2020b).",
      "startOffset" : 71,
      "endOffset" : 125
    }, {
      "referenceID" : 40,
      "context" : ", min(|b− c|, |a− d|)) between the target and opinion spans, following (Lee et al., 2017; He et al., 2018a; Xu et al., 2020b).",
      "startOffset" : 71,
      "endOffset" : 125
    }, {
      "referenceID" : 40,
      "context" : "However, it is found that not all triplets are explicitly annotated (Xu et al., 2020b; Wu et al., 2020).",
      "startOffset" : 68,
      "endOffset" : 103
    }, {
      "referenceID" : 38,
      "context" : "However, it is found that not all triplets are explicitly annotated (Xu et al., 2020b; Wu et al., 2020).",
      "startOffset" : 68,
      "endOffset" : 103
    }, {
      "referenceID" : 5,
      "context" : ", 2014, 2015, 2016), and the opinion terms are retrieved from (Fan et al., 2019).",
      "startOffset" : 62,
      "endOffset" : 80
    }, {
      "referenceID" : 3,
      "context" : "In the second setting, we finetune the pre-trained BERT (Devlin et al., 2019) to encode each sentence.",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 37,
      "context" : "CMLA+ (Wang et al., 2017) employs an attention mechanism to consider the interaction between aspect terms and opinion terms.",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "RINANTE+ (Dai and Song, 2019) adopts a BiLSTM-CRF model with mined rules to capture the dependency relations.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 18,
      "context" : "Li-unified-R (Li et al., 2019) uses a unified tagging scheme to jointly extract the aspect term and associated sentiment.",
      "startOffset" : 13,
      "endOffset" : 30
    }, {
      "referenceID" : 40,
      "context" : "JET (Xu et al., 2020b) models the ASTE task as a structured prediction problem with a position-aware tagging scheme to capture the interaction of the three elements in a triplet.",
      "startOffset" : 4,
      "endOffset" : 22
    }, {
      "referenceID" : 18,
      "context" : "We also observe that the two strong pipeline methods (Li et al., 2019; Peng et al., 2019) achieved competitive recall results, but their overall performance is much worse due to the low precision.",
      "startOffset" : 53,
      "endOffset" : 89
    }, {
      "referenceID" : 25,
      "context" : "We also observe that the two strong pipeline methods (Li et al., 2019; Peng et al., 2019) achieved competitive recall results, but their overall performance is much worse due to the low precision.",
      "startOffset" : 53,
      "endOffset" : 89
    }, {
      "referenceID" : 25,
      "context" : "Specifically, using the BiLSTM encoder with GloVe embedding, our model outperforms the best pipeline model (Peng et al., 2019) by 15.",
      "startOffset" : 107,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : "With the BERT encoder, all three end-to-end models achieve much stronger performance than their LSTM-based versions, which is consistent with previous findings (Devlin et al., 2019).",
      "startOffset" : 160,
      "endOffset" : 181
    }, {
      "referenceID" : 38,
      "context" : "Our approach outperforms the previous best results GTS (Wu et al., 2020) by 4.",
      "startOffset" : 55,
      "endOffset" : 72
    }, {
      "referenceID" : 38,
      "context" : "Table 3 shows the comparisons of our approach and the previous method GTS (Wu et al., 2020).",
      "startOffset" : 74,
      "endOffset" : 91
    }, {
      "referenceID" : 40,
      "context" : "Note that the JET model (Xu et al., 2020b) is not able to directly solve the ATE and OTE tasks unless the evaluation is conducted based on the triplet predictions.",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 1,
      "context" : "For reference, we include the results of the RACL framework (Chen and Qian, 2020) in the Appendix.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 38,
      "context" : "1 Comparison of Single-word and Multi-word Spans We compare the performance of Span-ASTE with the previous model GTS (Wu et al., 2020) for the following two settings in Table 4: Single-Word: Both target and opinion terms in a triplet are singleword spans, Multi-Word: At least one of the target or opinion terms in a triplet is a multi-word span.",
      "startOffset" : 117,
      "endOffset" : 134
    }, {
      "referenceID" : 38,
      "context" : "3 Qualitative Analysis To illustrate the differences between the models, we present sample sentences from the ASTE test set with the gold labels as well as predictions from GTS (Wu et al., 2020) and Span-ASTE in Figure 4.",
      "startOffset" : 177,
      "endOffset" : 194
    }, {
      "referenceID" : 14,
      "context" : "Similar to the observation for coreference resolution (Lee et al., 2017), we find that the ASTE performance is reduced when removing the span width and distance embedding.",
      "startOffset" : 54,
      "endOffset" : 72
    }, {
      "referenceID" : 35,
      "context" : "Sentiment Analysis is a major Natural Language Understanding (NLU) task (Wang et al., 2019) and has been extensively studied as a classification problem at the sentence level (Raffel et al.",
      "startOffset" : 72,
      "endOffset" : 91
    }, {
      "referenceID" : 31,
      "context" : ", 2019) and has been extensively studied as a classification problem at the sentence level (Raffel et al., 2020; Lan et al., 2020; Yang et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 149
    }, {
      "referenceID" : 13,
      "context" : ", 2019) and has been extensively studied as a classification problem at the sentence level (Raffel et al., 2020; Lan et al., 2020; Yang et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 149
    }, {
      "referenceID" : 44,
      "context" : ", 2019) and has been extensively studied as a classification problem at the sentence level (Raffel et al., 2020; Lan et al., 2020; Yang et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 149
    }, {
      "referenceID" : 29,
      "context" : "Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014) addresses various sentiment analysis tasks at a finegrained level.",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 4,
      "context" : "As mentioned in the Section 1, the subtasks mainly include ASC (Dong et al., 2014; Zhang et al., 2016; Chen et al., 2017; He et al., 2018b; Li et al., 2018a; Peng et al., 2018; Wang and Lu, 2018; He et al., 2019; Li and Lu, 2019; Xu et al., 2020a), ATE (Qiu et al.",
      "startOffset" : 63,
      "endOffset" : 247
    }, {
      "referenceID" : 0,
      "context" : "As mentioned in the Section 1, the subtasks mainly include ASC (Dong et al., 2014; Zhang et al., 2016; Chen et al., 2017; He et al., 2018b; Li et al., 2018a; Peng et al., 2018; Wang and Lu, 2018; He et al., 2019; Li and Lu, 2019; Xu et al., 2020a), ATE (Qiu et al.",
      "startOffset" : 63,
      "endOffset" : 247
    }, {
      "referenceID" : 8,
      "context" : "As mentioned in the Section 1, the subtasks mainly include ASC (Dong et al., 2014; Zhang et al., 2016; Chen et al., 2017; He et al., 2018b; Li et al., 2018a; Peng et al., 2018; Wang and Lu, 2018; He et al., 2019; Li and Lu, 2019; Xu et al., 2020a), ATE (Qiu et al.",
      "startOffset" : 63,
      "endOffset" : 247
    }, {
      "referenceID" : 17,
      "context" : "As mentioned in the Section 1, the subtasks mainly include ASC (Dong et al., 2014; Zhang et al., 2016; Chen et al., 2017; He et al., 2018b; Li et al., 2018a; Peng et al., 2018; Wang and Lu, 2018; He et al., 2019; Li and Lu, 2019; Xu et al., 2020a), ATE (Qiu et al.",
      "startOffset" : 63,
      "endOffset" : 247
    }, {
      "referenceID" : 24,
      "context" : "As mentioned in the Section 1, the subtasks mainly include ASC (Dong et al., 2014; Zhang et al., 2016; Chen et al., 2017; He et al., 2018b; Li et al., 2018a; Peng et al., 2018; Wang and Lu, 2018; He et al., 2019; Li and Lu, 2019; Xu et al., 2020a), ATE (Qiu et al.",
      "startOffset" : 63,
      "endOffset" : 247
    }, {
      "referenceID" : 36,
      "context" : "As mentioned in the Section 1, the subtasks mainly include ASC (Dong et al., 2014; Zhang et al., 2016; Chen et al., 2017; He et al., 2018b; Li et al., 2018a; Peng et al., 2018; Wang and Lu, 2018; He et al., 2019; Li and Lu, 2019; Xu et al., 2020a), ATE (Qiu et al.",
      "startOffset" : 63,
      "endOffset" : 247
    }, {
      "referenceID" : 9,
      "context" : "As mentioned in the Section 1, the subtasks mainly include ASC (Dong et al., 2014; Zhang et al., 2016; Chen et al., 2017; He et al., 2018b; Li et al., 2018a; Peng et al., 2018; Wang and Lu, 2018; He et al., 2019; Li and Lu, 2019; Xu et al., 2020a), ATE (Qiu et al.",
      "startOffset" : 63,
      "endOffset" : 247
    }, {
      "referenceID" : 16,
      "context" : "As mentioned in the Section 1, the subtasks mainly include ASC (Dong et al., 2014; Zhang et al., 2016; Chen et al., 2017; He et al., 2018b; Li et al., 2018a; Peng et al., 2018; Wang and Lu, 2018; He et al., 2019; Li and Lu, 2019; Xu et al., 2020a), ATE (Qiu et al.",
      "startOffset" : 63,
      "endOffset" : 247
    }, {
      "referenceID" : 39,
      "context" : "As mentioned in the Section 1, the subtasks mainly include ASC (Dong et al., 2014; Zhang et al., 2016; Chen et al., 2017; He et al., 2018b; Li et al., 2018a; Peng et al., 2018; Wang and Lu, 2018; He et al., 2019; Li and Lu, 2019; Xu et al., 2020a), ATE (Qiu et al.",
      "startOffset" : 63,
      "endOffset" : 247
    }, {
      "referenceID" : 30,
      "context" : ", 2020a), ATE (Qiu et al., 2011; Yin et al., 2016; Li et al., 2018b; Ma et al., 2019), OTE (Hu and Liu, 2004; Yang and Cardie, 2012; Klinger and Cimiano, 2013; Yang and Cardie, 2013).",
      "startOffset" : 14,
      "endOffset" : 85
    }, {
      "referenceID" : 19,
      "context" : ", 2020a), ATE (Qiu et al., 2011; Yin et al., 2016; Li et al., 2018b; Ma et al., 2019), OTE (Hu and Liu, 2004; Yang and Cardie, 2012; Klinger and Cimiano, 2013; Yang and Cardie, 2013).",
      "startOffset" : 14,
      "endOffset" : 85
    }, {
      "referenceID" : 23,
      "context" : ", 2020a), ATE (Qiu et al., 2011; Yin et al., 2016; Li et al., 2018b; Ma et al., 2019), OTE (Hu and Liu, 2004; Yang and Cardie, 2012; Klinger and Cimiano, 2013; Yang and Cardie, 2013).",
      "startOffset" : 14,
      "endOffset" : 85
    }, {
      "referenceID" : 5,
      "context" : "There is also another subtask named Target-oriented Opinion Words Extraction (TOWE) (Fan et al., 2019), which aim to extract the corresponding opinion words for a given target term.",
      "startOffset" : 84,
      "endOffset" : 102
    }, {
      "referenceID" : 37,
      "context" : "Aspect and Opinion Term Co-Extraction (AOTE) aiming to extract the aspect and opinion terms together (Wang et al., 2017; Ma et al., 2019; Dai and Song, 2019) and is often treated as a sequence labeling problem.",
      "startOffset" : 101,
      "endOffset" : 157
    }, {
      "referenceID" : 23,
      "context" : "Aspect and Opinion Term Co-Extraction (AOTE) aiming to extract the aspect and opinion terms together (Wang et al., 2017; Ma et al., 2019; Dai and Song, 2019) and is often treated as a sequence labeling problem.",
      "startOffset" : 101,
      "endOffset" : 157
    }, {
      "referenceID" : 2,
      "context" : "Aspect and Opinion Term Co-Extraction (AOTE) aiming to extract the aspect and opinion terms together (Wang et al., 2017; Ma et al., 2019; Dai and Song, 2019) and is often treated as a sequence labeling problem.",
      "startOffset" : 101,
      "endOffset" : 157
    }, {
      "referenceID" : 15,
      "context" : "End-to-End ABSA (Li and Lu, 2017; Ma et al., 2018; Li et al., 2019; He et al., 2019) jointly extracts each aspect",
      "startOffset" : 16,
      "endOffset" : 84
    }, {
      "referenceID" : 22,
      "context" : "End-to-End ABSA (Li and Lu, 2017; Ma et al., 2018; Li et al., 2019; He et al., 2019) jointly extracts each aspect",
      "startOffset" : 16,
      "endOffset" : 84
    }, {
      "referenceID" : 18,
      "context" : "End-to-End ABSA (Li and Lu, 2017; Ma et al., 2018; Li et al., 2019; He et al., 2019) jointly extracts each aspect",
      "startOffset" : 16,
      "endOffset" : 84
    }, {
      "referenceID" : 9,
      "context" : "End-to-End ABSA (Li and Lu, 2017; Ma et al., 2018; Li et al., 2019; He et al., 2019) jointly extracts each aspect",
      "startOffset" : 16,
      "endOffset" : 84
    }, {
      "referenceID" : 25,
      "context" : "ASTE (Peng et al., 2019; Wu et al., 2020; Xu et al., 2020b; Zhang et al., 2020) is the most recent development of ABSA and its aim is to extract and form the aspect term, its associated sentiment, and the corresponding opinion term into a triplet.",
      "startOffset" : 5,
      "endOffset" : 79
    }, {
      "referenceID" : 38,
      "context" : "ASTE (Peng et al., 2019; Wu et al., 2020; Xu et al., 2020b; Zhang et al., 2020) is the most recent development of ABSA and its aim is to extract and form the aspect term, its associated sentiment, and the corresponding opinion term into a triplet.",
      "startOffset" : 5,
      "endOffset" : 79
    }, {
      "referenceID" : 40,
      "context" : "ASTE (Peng et al., 2019; Wu et al., 2020; Xu et al., 2020b; Zhang et al., 2020) is the most recent development of ABSA and its aim is to extract and form the aspect term, its associated sentiment, and the corresponding opinion term into a triplet.",
      "startOffset" : 5,
      "endOffset" : 79
    } ],
    "year" : 2021,
    "abstractText" : "Aspect Sentiment Triplet Extraction (ASTE) is the most recent subtask of ABSA which outputs triplets of an aspect target, its associated sentiment, and the corresponding opinion term. Recent models perform the triplet extraction in an end-to-end manner but heavily rely on the interactions between each target word and opinion word. Thereby, they cannot perform well on targets and opinions which contain multiple words. Our proposed span-level approach explicitly considers the interaction between the whole spans of targets and opinions when predicting their sentiment relation. Thus, it can make predictions with the semantics of whole spans, ensuring better sentiment consistency. To ease the high computational cost caused by span enumeration, we propose a dual-channel span pruning strategy by incorporating supervision from the Aspect Term Extraction (ATE) and Opinion Term Extraction (OTE) tasks. This strategy not only improves computational efficiency but also distinguishes the opinion and target spans more properly. Our framework simultaneously achieves strong performance for the ASTE as well as ATE and OTE tasks. In particular, our analysis shows that our spanlevel approach achieves more significant improvements over the baselines on triplets with multi-word targets or opinions. 1",
    "creator" : "LaTeX with hyperref"
  }
}