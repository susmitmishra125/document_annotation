{
  "name" : "2021.acl-long.234.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Competence-based Multimodal Curriculum Learning for Medical Report Generation",
    "authors" : [ "Fenglin Liu", "Shen Ge", "Xian Wu" ],
    "emails" : [ "fenglinliu98@pku.edu.cn;", "kevinxwu}@tencent.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3001–3012\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3001"
    }, {
      "heading" : "1 Introduction",
      "text" : "Medical images, e.g., radiology and pathology images, and their corresponding reports, which describe the observations in details of both normal and abnormal regions, are widely-used for diagnosis and treatment (Delrue et al., 2011; Goergen et al., 2013). In clinical practice, writing a medical report can be time-consuming and tedious for experienced radiologists, and error-prone for inexperienced radiologists. Therefore, automatically generating medical reports can assist radiologists in clinical decision-making and emerge as a prominent attractive research direction in both artificial\n∗Corresponding author.\nintelligence and clinical medicine (Jing et al., 2018, 2019; Li et al., 2018, 2019; Wang et al., 2018; Xue et al., 2018; Yuan et al., 2019; Zhang et al., 2020a; Chen et al., 2020; Liu et al., 2021a,b, 2019c).\nMany existing medical report generation models adopt the standard image captioning approaches: a CNN-based image encoder followed by a LSTMbased report decoder, e.g., CNN-HLSTM (Jing et al., 2018; Liang et al., 2017). However, directly applying image captioning approaches to medical images has the following problems: 1) Visual data bias: the normal images dominate the dataset over the abnormal ones (Shin et al., 2016). Furthermore, for each abnormal image, the normal regions dominate the image over the abnormal ones. As\nshown in Figure 1, abnormal regions (Red bounding boxes) only occupy a small part of the entire image; 2) Textual data bias: as shown in Figure 1, in a medical report, radiologists tend to describe all the items in an image, making the descriptions of normal regions dominate the entire report. Besides, many similar sentences are used to describe the same normal regions. 3) Training efficiency: during training, most existing works treat all the samples equally without considering their difficulties. As a result, the visual and textual biases could mislead the model training (Jing et al., 2019; Xue et al., 2018; Yuan et al., 2019; Liu et al., 2021a,b; Li et al., 2018). As shown in Figure 1, even a stateof-the-art model (Jing et al., 2018) still generates some repeated sentences of normalities and fails to depict the rare but important abnormalities.\nTo this end, we propose a novel Competencebased Multimodal Curriculum Learning framework (CMCL) which progressively learns medical reports following an easy-to-hard fashion. Such a step by step process is similar to the learning curve of radiologists: (1) first start from simple and easywritten reports; (2) and then attempt to consume harder reports, which consist of rare and diverse abnormalities. In order to model the above gradual working patterns, CMCL first assesses the difficulty of each training instance from multiple perspectives (i.e., the Visual Complexity and Textual Complexity) and then automatically selects the most rewarding training samples according to the current competence of the model. In this way, once the easy and simple samples are well-learned, CMCL increases the chance of learning difficult and complex samples, preventing the models from getting stuck in bad local optima1, which is obviously a better solution than the common approaches of uniformly sampling training examples from the limited medical data. As a result, CMCL could better utilize the limited medical data to alleviate the data bias. We evaluate the effectiveness of the proposed CMCL on two public datasets, i.e., IU-Xray (Demner-Fushman et al., 2016) and MIMIC-CXR (Johnson et al., 2019).\nOverall, the main contributions of this work are:\n• We introduce the curriculum learning in medical report generation, which enables the models to gradually proceed from easy samples to\n1Current models tend to generate plausible general reports with no prominent abnormal narratives (Jing et al., 2019; Li et al., 2018; Yuan et al., 2019; Liu et al., 2021a,b)\nmore complex ones in training, helping existing models better utilize the limited medical data to alleviate the data bias.\n• We assess the difficulty of each training instance from multiple perspectives and propose a competence-based multimodal curriculum learning framework (CMCL) to consider multiple difficulties simultaneously.\n• We evaluate our proposed approach on two public datasets. After equipping our proposed CMCL, which doesn’t introduce additional parameters and only requires a small modification to the training data pipelines, performances of the existing baseline models can be improved on most metrics. Moreover, we conduct human evaluations to measure the effectiveness in terms of its usefulness for clinical practice."
    }, {
      "heading" : "2 Related Work",
      "text" : "The related works are introduced from: 1) Image Captioning and Paragraph Generation; 2) Medical Report Generation and 3) Curriculum Learning.\nImage Captioning and Paragraph Generation The task of image captioning (Chen et al., 2015; Vinyals et al., 2015), which aims to generate a sentence to describe the given image, has received extensive research interests (Anderson et al., 2018; Rennie et al., 2017; Liu et al., 2019a, 2020a). These approaches mainly adopt the encoder-decoder framework which translates the image to a single descriptive sentence. Such an encoder-decoder framework have achieved great success in advancing the state-of-the-arts (Vinyals et al., 2015; Lu et al., 2017; Xu et al., 2015; Liu et al., 2018, 2019b). Specifically, the encoder network (Krizhevsky et al., 2012; He et al., 2016) computes visual representations for the visual contents and the decoder network (Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017) generates a target sentence based on the visual representations. In contrast to the image captioning, image paragraph generation, which aims to produce a long and semanticcoherent paragraph to describe the input image, has recently attracted growing research interests (Krause et al., 2017; Liang et al., 2017; Yu et al., 2016). To perform the image paragraph generation, a hierarchical LSTM (HLSTM) (Krause et al., 2017; Liang et al., 2017) is proposed as the decoder to well generate long paragraphs.\nMedical Report Generation The medical reports are expected to 1) cover contents of key medical findings such as heart size, lung opacity, and bone structure; 2) correctly capture any abnormalities and support with details such as the location and shape of the abnormality; 3) correctly describe potential diseases such as effusion, pneumothorax and consolidation (Delrue et al., 2011; Goergen et al., 2013; Li et al., 2018; Liu et al., 2021a,b). Therefore, correctly describing the abnormalities become the most urgent goal and the core value of this task. Similar to image paragraph generation, most existing medical report generation works (Jing et al., 2018, 2019; Li et al., 2018; Wang et al., 2018; Xue et al., 2018; Yuan et al., 2019; Zhang et al., 2020a,b; Miura et al., 2021; Lovelace and Mortazavi, 2020; Liu et al., 2021b, 2019c) attempt to adopt a CNN-HLSTM based model to automatically generate a fluent report. However, due to the data bias and the limited medical data, these models are biased towards generating plausible but general reports without prominent abnormal narratives (Jing et al., 2019; Li et al., 2018; Yuan et al., 2019; Liu et al., 2021a,b).\nCurriculum Learning In recent years, curriculum learning (Bengio et al., 2009), which enables the models to gradually proceed from easy samples to more complex ones in training (Elman, 1993), has received growing research interests in natural language processing field, e.g., neural machine translation (Platanios et al., 2019; Kumar et al., 2019; Zhao et al., 2020; Liu et al., 2020b; Zhang et al., 2018; Kocmi and Bojar, 2017; Xu et al., 2020) and computer vision field, e.g., image classification (Weinshall et al., 2018), human attribute analysis(Wang et al., 2019) and visual question answering (Li et al., 2020). For example, in neural machine translation, Platanios et al. (2019) proposed to utilize the training samples in order of easy-to-hard and to describe the “difficulty” of a training sample using the sentence length or the rarity of the words appearing in it (Zhao et al., 2020). However, these methods (Platanios et al., 2019; Liu et al., 2020b; Xu et al., 2020) are single difficulty-based and unimodal curriculum learning approaches. It is obviously not applicable to medical report generation task, which involves multimodal data, i.e., visual medical images and textual reports, resulting in multi-modal complexities, i.e., the visual complexity and the textual complexity. Therefore, it is hard to design one single metric to\nestimate the overall difficulty of medical report generation. To this end, based on the work of Platanios et al. (2019), we propose a competence-based multimodal curriculum learning approach with multiple difficulty metrics."
    }, {
      "heading" : "3 Framework",
      "text" : "In this section, we briefly describe typical medical report generation approaches and introduce the proposed Competence-based Multimodal Curriculum Learning (CMCL).\nAs shown in the top of Figure 2, many medical report generation models adopt the encoderdecoder manner. Firstly, the visual features are extracted from the input medical image via a CNN model. Then the visual features are fed into a sequence generation model, like LSTM to produce the medical report. In the training phase, all training instances are randomly shuffled and grouped into batches for training. In other words, all training instances are treated equally. Different from typical medical report generation models, CMCL builds the training batch in a selective manner. The middle part of Figure 2 displays the framework of CMCL equipped with one single difficulty metric. CMCL first ranks all training instances according to this difficulty metric and then gradually enlarges the range of training instances that the batch is selected. In this manner, CMCL can train the models from easy to difficult instances.\nSince medical report generation involves multimodal data, like visual medical images and textual reports, it is hard to design one single metric to estimate the overall difficulty. Therefore, we also propose a CMCL with multiple difficulty metrics. As shown in the bottom of Figure 2, the training instances are ranked by multiple metrics independently. At each step, CMCL generates one batch for each difficulty metric and then calculates the perplexity of each batch based on current model. The batch with highest perplexity is selected to train the model. It can be understood that CMCL sets multiple syllabus in parallel, and the model is optimized towards the one with lowest competence."
    }, {
      "heading" : "4 Difficulty Metrics",
      "text" : "In this section, we define the difficulty metrics used by CMCL. As stated in Section 2, the key challenge of medical report generation is to accurately capture and describe the abnormalities (Delrue et al., 2011; Goergen et al., 2013; Li et al., 2018). There-\nfore, we assess the difficulty of instances based on the difficulty of accurately capturing and describing the abnormalities."
    }, {
      "heading" : "4.1 Visual Difficulty",
      "text" : "We define both a heuristic metric and a modelbased metric to estimate the visual difficulty.\nHeuristic Metric d1 If a medical image contains complex visual contents, it is more likely to contain more abnormalities, which increases the difficulty to accurately capture them. To measure such visual difficulty, we adopt the widely-used ResNet-50 (He et al., 2016) pre-trained on ImageNet (Deng et al., 2009) and fine-tuned on CheXpert dataset (Irvin et al., 2019), which consists of 224,316 X-ray images with each image labeled with occurrences of 14 common radiographic observations. Specifically, we first extract the normal image embeddings of all normal training images from the last average pooling layer of ResNet-50. Then, given an input image, we again use the ResNet-50 to obtain the image embedding. At last, the average cosine similarity between the input image and normal images is adopted as the heuristic metric of visual difficulty.\nModel Confidence d2 We also introduce a model-based metric. We adopt the above ResNet50 to conduct the abnormality classification task. We first adopt the ResNet-50 to acquire the classification probability distribution P (I) = {p1(I), p2(I), . . . , p14(I)} among the 14 common diseases for each image I in the training dataset, where pn(I) ∈ [0, 1]. Then, we employ the entropy value H(I) of the probability distribution, defined\nas follows:\nH(I) = − 14∑\nn=1\n(pn(I) log (pn(I))+\n(1− pn(I)) log (1− pn(I)))\n(1)\nWe employ the entropy value H(I) as the model confidence measure, indicating whether an image is easy to be classified or not."
    }, {
      "heading" : "4.2 Textual Difficulty",
      "text" : "We also define a heuristic metric and a model-based metric to estimate the textual difficulty.\nHeuristic Metric d3 A serious problem for medical report generation models is the tendency to generate plausible general reports with no prominent abnormal narratives (Jing et al., 2019; Li et al., 2018; Yuan et al., 2019). The normal sentences are easy to learn, but are less informative, while most abnormal sentences, consisting of more rare and diverse abnormalities, are relatively more difficult to learn, especially at the initial learning stage. To this end, we adopt the number of abnormal sentences in a report to define the difficulty of a report. Following Jing et al. (2018), we consider sentences which contain “no”, “normal”, “clear”, “stable” as normal sentences, the rest sentences are consider as abnormal sentences.\nModel Confidence d4 Similar to visual difficulty, we further introduce a model confidence as a metric. To this end, we define the difficulty using the negative log-likelihood loss values (Xu et al., 2020; Zhang et al., 2018) of training samples. To acquire the negative log-likelihood loss values, we\nAlgorithm 1 Single Difficulty-based Curriculum Learning (Platanios et al., 2019).\nInput: The training set Dtrain. Output: A model with single difficulty-based curriculum\nlearning. 1: Compute difficulty d for each training sample in Dtrain; 2: Sort Dtrain based d to acquire Dtrain1 ; 3: At t = 0, initialize the model competence c(0) by Eq. (2);\nUniformly sample a data batch, B(0), from the top c(0) portions of Dtrain1 ;\n4: repeat 5: Train the model with the B(t); 6: t← t+ 1; 7: Estimate the model competence, c(t), by Eq. (2);\nUniformly sample a data batch, B(t), from the top c(t) portions of Dtrain1 ;\n8: until Model converge.\nadopt the widely-used and classic CNN-HLSTM (Jing et al., 2018), in which the CNN is implemented with ResNet-50, trained on the downstream dataset used for evaluation with a cross-entropy loss.\nIt is worth noticing that since we focus on the medical report generation and design the metrics based on the difficulty of accurately capturing and describing the abnormalities, we do not consider some language difficulty metrics used in neural machine translation, e.g., the sentence length (Platanios et al., 2019), the n-gram rarity together with Named Entity Recognition (NER) and Parts of Speech (POS) taggings (Zhao et al., 2020)."
    }, {
      "heading" : "5 Approach",
      "text" : "In this section, we first briefly introduce the conventional single difficulty-based curriculum (Platanios et al., 2019). Then we propose the multiple difficulty-based curriculum learning for medical report generation."
    }, {
      "heading" : "5.1 Single Difficulty-based Curriculum Learning",
      "text" : "Platanios et al. (2019) proposed a competencebased and single difficulty-based curriculum learning framework (see Algorithm 1), which first sorts each instance in the training dataset Dtrain according to a single difficulty metric d, and then defines the model competence c(t) ∈ (0, 1] at training step t by following functional forms:\nc(t) = min ( 1, p √ t 1− c(0)p\nT + c(0)p\n) (2)\nwhere c(0) is the initial competence and usually set to 0.01, p is the coefficient to control the curriculum schedule and is usually set to 2, and T is\nAlgorithm 2 Multiple Difficulty-based Curriculum Learning. The Red colored text denotes the differences from Algorithm 1. Input: The training set Dtrain, i ∈ {1, 2, 3, 4}. Output: A model with multiple difficulty-based curriculum\nlearning. 1: Compute four difficulties, di, for each training sample in\nDtrain; 2: Sort Dtrain based each difficulty of every sample, resulting\nin Dtraini (i.e., D train 1 , D train 2 , D train 3 , D train 4 );\n3: for i = 1, 2, 3, 4 do 4: ti = 0; Initialize the model competence from ith\nperspective, ci(0), by Eq. (2); Uniformly sample a data batch, Bi(0), from the top ci(0) portions of Dtraini ;\n5: Compute the perplexity (PPL) on Bi(0), PPL(Bi(0)); 6: end for 7: repeat 8: j = argmax\ni (PPL(Bi(ti)));\n9: Train the model with the Bj(tj); 10: tj ← tj + 1; 11: Estimate the model competence from jth perspective,\ncj(tj), by Eq. (2); Uniformly sample a data batch, Bj(tj), from the top cj(tj) portions of Dtrainj ;\n12: Compute the perplexity (PPL) of model on Bj(tj), PPL(Bj(tj)); 13: until Model converge.\nthe duration of curriculum learning and determines the length of the curriculum. In implementations, at training time step t, the top c(t) portions of the sorted training dataset are selected to sample a training batch to train the model. In this way, the model is able to gradually proceed from easy samples to more complex ones in training, resulting in first starting to utilize the simple and easy-written reports for training, and then attempting to utilize harder reports for training."
    }, {
      "heading" : "5.2 Multiple Difficulty-based Curriculum Learning",
      "text" : "The training instances of medical report generation task are pairs of medical images and corresponding reports which is a multi-modal data. It’s hard to estimate the difficulty with only one metric. In addition, the experimental results (see Table 4) show that directly fusing multiple difficulty metrics as one (d1+ d2+ d3+ d4) is obviously inappropriate, which is also verified in Platanios et al. (2019). To this end, we extend the single difficulty-based curriculum learning into the multiple difficulty-based curriculum learning, where we provide the medical report generation models with four different difficulty metrics, i.e., d1, d2, d3, d4 (see Section 4).\nA simple and natural way is to randomly or sequentially choose a curricula to train the model, i.e., 1→2→3→4→1. However, a better approach\nis to adaptively select the most appropriate curricula for each training step, which follows the common practice of human learning behavior: When we have learned some curricula well, we tend to choose the under-learned curricula to learn. Algorithm 2 summarizes the overall learning process of the proposed framework and Figure 3 illustrates the process of Algorithm 2. In implementations, similarly, we first sort the training dataset based on the four difficulty metrics and acquire four sorted training datasets in line 1-2. Then, based on the model competence, we acquire the training samples for each curricula, in line 4. In line 5, we further estimate the perplexity (PPL) of model on different training samples Bi(ti) corresponding to different curricula, defined as: PPL(Bi(ti)) = ∑\nRk∈Bi(ti)\nN √√√√ N∏ m=1\n1\nP (wkm|wk1 , . . . , wkm−1)\nwhere Rk = {wk1 , wk2 , . . . , wkN} denotes the k-th report in Bi(ti). The perplexity (PPL) measures how many bits on average would be needed to encode each word of the report given the model, so the current curricula with higher PPL means that the model is not well-learned for this curricula and need to be improved. Therefore, the PPL can be used to determine the curricula at each training step dynamically. Specifically, in line 8-9, we select the under-learned curricula, i.e., the curricula with maximum PPL, to train the current model. After that, we again estimate the model competence in the selected curricula in line 11 and compute the PPL of model on the training samples corresponding to the selected curricula in line 12."
    }, {
      "heading" : "6 Experiment",
      "text" : "We firstly describe two public datasets as well as the widely-used metrics, baselines and settings. Then we present the evaluation of our CMCL."
    }, {
      "heading" : "6.1 Datasets",
      "text" : "We conduct experiments on two public datasets, i.e., a widely-used benchmark IU-Xray (DemnerFushman et al., 2016) and a recently released largescale MIMIC-CXR (Johnson et al., 2019).\n• IU-Xray2 is collected by Indiana University and is widely-used to evaluate the performance of medical report generation methods.\n2https://openi.nlm.nih.gov/\nIt contains 7,470 chest X-ray images associated with 3,955 radiology reports sourced from Indiana Network for Patient Care.\n• MIMIC-CXR3 is the recently released largest dataset to date and consists of 377,110 chest X-ray images and 227,835 radiology reports from 64,588 patients of the Beth Israel Deaconess Medical Center.\nFor IU-Xray dataset, following previous works (Chen et al., 2020; Jing et al., 2019; Li et al., 2019, 2018), we randomly split the dataset into 70%- 10%-20% training-validation-testing splits. At last, we preprocess the reports by tokenizing, converting to lower-cases and removing non-alpha tokens. For MIMIC-CXR, following Chen et al. (2020); Liu et al. (2021a,b), we use the official splits to report our results, resulting in 368,960 samples in the training set, 2,991 samples in the validation set and 5,159 samples in the test set. We convert all tokens of reports to lower-cases and filter tokens that occur less than 10 times in the corpus, resulting in a vocabulary of around 4,000 tokens."
    }, {
      "heading" : "6.2 Baselines",
      "text" : "We tested three representative baselines that were originally designed for image captioning and three\n3https://physionet.org/content/ mimic-cxr/2.0.0/\ncompetitive baselines that were originally designed for medical report generation."
    }, {
      "heading" : "6.2.1 Image Captioning Baselines",
      "text" : "• NIC: Vinyals et al. (2015) proposed the\nencoder-decoder network, which employs a CNN-based encoder to extract image features and a RNN-based decoder to generate the target sentence, for image captioning.\n• Spatial-Attention: Lu et al. (2017) proposed the visual attention, which is calculated on the hidden states, to help the model to focus on the most relevant image regions instead of the whole image.\n• Adaptive-Attention: Considering that the decoder tends to require little or no visual information from the image to predict the nonvisual words such as “the” and “of”, Lu et al. (2017) designed an adaptive attention model to decide when to employ the visual attention."
    }, {
      "heading" : "6.2.2 Medical Report Generation Baselines",
      "text" : "• CNN-HLSTM: Jing et al. (2018) introduced\nthe Hierarchical LSTM structure (HLSTM), which contains the paragraph LSTM and the sentence LSTM. HLSTM first uses the paragraph LSTM to generate a series of high-level topic vectors representing the sentences, and then utilizes the sentence LSTM to generate a sentence based on each topic vector.\n• HLSTM+att+Dual: Harzig et al. (2019) proposed a hierarchical LSTM with the attention mechanism and further introduced two LSTMs, i.e., Normal LSTM and Abnormal LSTM, to help the model to generate more accurate normal and abnormal sentences.\n• Co-Attention: Jing et al. (2018) proposed the co-attention model, which combines the merits of visual attention and semantic attention, to attend to both images and predicted semantic tags4 simultaneously, exploring the synergistic effects of visual and semantic information."
    }, {
      "heading" : "6.3 Metrics and Settings",
      "text" : "We adopt the widely-used BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE-L (Lin, 2004), which are reported by the\n4https://ii.nlm.nih.gov/MTI/\nevaluation toolkit (Chen et al., 2015)5, to test the performance. Specifically, ROUGE-L is proposed for automatic evaluation of the extracted text summarization. METEOR and BLEU are originally designed for machine translation evaluation.\nFor all baselines, since our focus is to change the training paradigm, which improves existing baselines by efficiently utilizing the limited medical data, we keep the inner structure of the baselines untouched and preserve the original parameter setting. For our curriculum learning framework, following previous work (Platanios et al., 2019), the c(0) and p are set to 0.01 and 2, respectively. For different baselines, we first re-implement the baselines without using any curriculum. When equipping baselines with curriculum, following Platanios et al. (2019), we set T in Eq.(2) to a quarter of the number of training steps that the baseline model takes to reach approximately 90% of its final BLEU-4 score. To boost the performance, we further incorporate the Batching method (Xu et al., 2020), which batches the samples with similar difficulty in the curriculum learning framework. To re-implement the baselines and our approach, following common practice (Jing et al., 2019; Li et al., 2019, 2018; Liu et al., 2021a,b), we extract image features for both dataset used for evaluation from a ResNet-50 (He et al., 2016), which is pretrained on ImageNet (Deng et al., 2009) and fine-tuned on public available CheXpert dataset (Irvin et al., 2019). To ensure consistency with the experiment settings of previous works (Chen et al., 2020), for IU-Xray, we utilize paired images of a patient as the input; for MIMIC-CXR, we use single image as the input. For parameter optimization, we use Adam optimizer (Kingma and Ba, 2014) with a batch size of 16 and a learning rate of 1e-4."
    }, {
      "heading" : "6.4 Automatic Evaluation",
      "text" : "As shown in Table 1, for two datasets, all baselines equipped with our approach receive performance gains over most metrics. The results prove the effectiveness and the compatibility of our CMCL in promoting the performance of existing models by better utilizing the limited medical data. Besides, in Table 2, we further select six existing state-of-the-art models, i.e., HRGR-Agent (Li et al., 2018), CMAS-RL (Jing et al., 2019), SentSAT + KG (Zhang et al., 2020a), Up-Down (Anderson et al., 2018), Transformer (Chen et al., 2020) and\n5https://github.com/tylin/coco-caption\nR2Gen (Chen et al., 2020), for comparison. For these selected models, we directly quote the results from the original paper for IU-Xray, and from Chen et al. (2020) for MIMIC-CXR. As we can see, based on the Co-Attention (Chen et al., 2020), our approach CMCL achieves competitive results with these state-of-the-art models on major metrics, which further demonstrate the effectiveness of the proposed approach."
    }, {
      "heading" : "6.5 Human Evaluation",
      "text" : "In this section, to verify the effectiveness of our approach in clinical practice, we invite two professional clinicians to evaluate the perceptual quality of 100 randomly selected reports generated by “Baselines” and “Baselines w/ CMCL”. For the baselines, we choose a representative model: CNN-HLSTM and a state-of-the-art model: CoAttention. The clinicians are unaware of which\nmodel generates these reports. In particular, to have more documents examined, we did not use the same documents for both clinicians and check the agreements between them. That is to say, the documents for different clinicians do not overlap. The results in Table 3 show that our approach is better than baselines in clinical practice with winning pick-up percentages. In particular, all invited professional clinicians found that our approach can generate fluent reports with more accurate descriptions of abnormalities than baselines. It indicates that our approach can help baselines to efficiently alleviate the data bias problem, which also can be verified in Section 6.7."
    }, {
      "heading" : "6.6 Quantitative Analysis",
      "text" : "Analysis on the Difficulty Metrics In this section, we conduct an ablation study by only using a single difficulty metric during the curriculum learning, i.e., single difficulty-based curriculum learning, to investigate the contribution of each difficulty metric in our framework and the results are shown in Table 4. Settings (a-d) show that every difficulty metric can boost the performance of baselines, which verify the effectiveness of our designed difficulty metrics. In particular, 1) the\nmodel confidence in both visual and textual difficulties achieves better performance than the heuristic metrics. It shows that the model confidence is the more critical in neural models. 2) Both the model confidence and heuristic metrics in the textual difficulty achieve better performance than their counterparts in the visual difficulty, which indicates that the textual data bias is the more critical in textual report generation task. When progressively incorporate each difficulty metric, the performance will increase continuously (see settings (e-g)), showing that integrating different difficulty metrics can bring the improvements from different aspects, and the advantages of all difficulty metrics can be united as an overall improvement.\nAnalysis on the Route Strategy As stated in Section 5.2, to implement the multiple difficultybased curriculum learning, three simple and natural ways is to: 1) Fuse multiple difficulty metrics directly as a single mixed difficulty metric, d1 + d2 + d3 + d4; 2) Randomly choose a curricula and 3) Sequentially choose a curricula (i.e., 1→2→3→4→1) to train the model. Table 4 (h-j) show the results of the three implementations. As we can see, all route strategies are viable in practice with improved performance of medical report generation, which proves the effectiveness and robustness of our CMCL framework. Besides, all of them perform worse than our approach (Setting (g)), which confirms the effectiveness of dynamically learning strategy at each training step."
    }, {
      "heading" : "6.7 Qualitative Analysis",
      "text" : "In Figure 1, we give two intuitive examples to better understand our approach. As we can see, our approach generates structured and robust reports, which show significant alignment with ground truth\nreports and are supported by accurate abnormal descriptions. For example, the generated report correctly describes “Blunting of right costophrenic” in the first example and “Scoliosis is present” in the second example. The results prove our arguments and verify the effectiveness of our proposed CMCL in alleviating the data bias problem by enabling the model to gradually proceed from easy to more complex instances in training."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we propose the novel competencebased multimodal curriculum learning framework (CMCL) to alleviate the data bias by efficiently utilizing the limited medical data for medical report generation. To this end, considering the difficulty of accurately capturing and describing the abnormalities, we first assess four sample difficulties of training data from the visual complexity and the textual complexity, resulting in four different curricula. Next, CMCL enables the model to be trained with the appropriate curricula and gradually proceed from easy samples to more complex ones in training. Experimental results demonstrate the effectiveness and the generalization capabilities of CMCL, which consistently boosts the performance of the baselines under most metrics."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is partly supported by Tencent Medical AI Lab, Beijing, China. We would like to sincerely thank the clinicians Xiaoxia Xie and Jing Zhang of the Harbin Chest Hospital in China for providing the human evaluation. We sincerely thank all the anonymous reviewers for their constructive comments and suggestions that substantially improved this paper.\nEthical Considerations\nIn this work, we focus on helping a wide range of existing medical report generation systems alleviate the data bias by efficiently utilizing the limited medical data for medical report generation. Our work can enable the existing systems to gradually proceed from easy samples to more complex ones in training, which is similar to the learning curve of radiologist: (1) first start from simple and easywritten reports; (2) and then attempt to consume harder reports, which consist of rare and diverse abnormalities. As a result, our work can promote the usefulness of existing medical report generation systems in better assisting radiologists in clinical decision-makings and reducing their workload. In particular, for radiologists, given a large amount of medical images, the systems can automatically generate medical reports, the radiologists only need to make revisions rather than write a new report from scratch. We conduct the experiments on the public MIMIC-CXR and IU-Xray datasets. All protected health information was de-identified. Deidentification was performed in compliance with Health Insurance Portability and Accountability Act (HIPAA) standards in order to facilitate public access to the datasets. Deletion of protected health information (PHI) from structured data sources (e.g., database fields that provide patient name or date of birth) was straightforward. All necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived."
    } ],
    "references" : [ {
      "title" : "Bottom-up and top-down attention for image captioning and VQA",
      "author" : [ "Peter Anderson", "Xiaodong He", "Chris Buehler", "Damien Teney", "Mark Johnson", "Stephen Gould", "Lei Zhang." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Anderson et al\\.,? 2018",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2018
    }, {
      "title" : "METEOR: an automatic metric for MT evaluation with improved correlation with human judgments",
      "author" : [ "Satanjeev Banerjee", "Alon Lavie." ],
      "venue" : "IEEvaluation@ACL.",
      "citeRegEx" : "Banerjee and Lavie.,? 2005",
      "shortCiteRegEx" : "Banerjee and Lavie.",
      "year" : 2005
    }, {
      "title" : "Curriculum learning",
      "author" : [ "Yoshua Bengio", "Jérôme Louradour", "Ronan Collobert", "Jason Weston." ],
      "venue" : "ICML.",
      "citeRegEx" : "Bengio et al\\.,? 2009",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2009
    }, {
      "title" : "Microsoft COCO captions: Data collection and evaluation server",
      "author" : [ "Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Dollár", "C. Lawrence Zitnick." ],
      "venue" : "arXiv preprint arXiv:1504.00325.",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Generating radiology reports via memory-driven transformer",
      "author" : [ "ang Wan" ],
      "venue" : "In EMNLP",
      "citeRegEx" : "Wan.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wan.",
      "year" : 2020
    }, {
      "title" : "Difficulties in the interpretation of chest radiography",
      "author" : [ "Louke Delrue", "Robert Gosselin", "Bart Ilsen", "An Van Landeghem", "Johan de Mey", "Philippe Duyck." ],
      "venue" : "Comparative Interpretation of CT and Standard Radiography of the Chest, pages",
      "citeRegEx" : "Delrue et al\\.,? 2011",
      "shortCiteRegEx" : "Delrue et al\\.",
      "year" : 2011
    }, {
      "title" : "Preparing a collection of radiology examinations for distribution and retrieval",
      "author" : [ "Dina Demner-Fushman", "Marc D. Kohli", "Marc B. Rosenman", "Sonya E. Shooshan", "Laritza Rodriguez", "Sameer K. Antani", "George R. Thoma", "Clement J. McDonald." ],
      "venue" : "J.",
      "citeRegEx" : "Demner.Fushman et al\\.,? 2016",
      "shortCiteRegEx" : "Demner.Fushman et al\\.",
      "year" : 2016
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Fei-Fei Li." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Deng et al\\.,? 2009",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning and development in neural networks: The importance of starting small",
      "author" : [ "Jeffrey L Elman." ],
      "venue" : "Cognition, 48(1):71–99.",
      "citeRegEx" : "Elman.,? 1993",
      "shortCiteRegEx" : "Elman.",
      "year" : 1993
    }, {
      "title" : "Evidence-based guideline for the written radiology",
      "author" : [ "Stacy K Goergen", "Felicity J Pool", "Tari J Turner", "Jane E Grimm", "Mark N Appleyard", "Carmel Crock", "Michael C Fahey", "Michael F Fay", "Nicholas J Ferris", "Susan M Liew" ],
      "venue" : null,
      "citeRegEx" : "Goergen et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Goergen et al\\.",
      "year" : 2013
    }, {
      "title" : "Addressing data bias problems for chest x-ray image report generation",
      "author" : [ "Philipp Harzig", "Yan-Ying Chen", "Francine Chen", "Rainer Lienhart." ],
      "venue" : "BMVC.",
      "citeRegEx" : "Harzig et al\\.,? 2019",
      "shortCiteRegEx" : "Harzig et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "CVPR.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Comput., 9(8):1735– 1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison",
      "author" : [ "Ricky Jones", "David B. Larson", "Curtis P. Langlotz", "Bhavik N. Patel", "Matthew P. Lungren", "Andrew Y. Ng." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Jones et al\\.,? 2019",
      "shortCiteRegEx" : "Jones et al\\.",
      "year" : 2019
    }, {
      "title" : "Show, describe and conclude: On exploiting the structure information of chest x-ray reports",
      "author" : [ "Baoyu Jing", "Zeya Wang", "Eric P. Xing." ],
      "venue" : "ACL.",
      "citeRegEx" : "Jing et al\\.,? 2019",
      "shortCiteRegEx" : "Jing et al\\.",
      "year" : 2019
    }, {
      "title" : "On the automatic generation of medical imaging reports",
      "author" : [ "Baoyu Jing", "Pengtao Xie", "Eric P. Xing." ],
      "venue" : "ACL.",
      "citeRegEx" : "Jing et al\\.,? 2018",
      "shortCiteRegEx" : "Jing et al\\.",
      "year" : 2018
    }, {
      "title" : "MIMIC-CXR: A large publicly available database of labeled chest radiographs",
      "author" : [ "Alistair E.W. Johnson", "Tom J. Pollard", "Seth J. Berkowitz", "Nathaniel R. Greenbaum", "Matthew P. Lungren", "Chih-ying Deng", "Roger G. Mark", "Steven Horng" ],
      "venue" : null,
      "citeRegEx" : "Johnson et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Curriculum learning and minibatch bucketing in neural machine translation",
      "author" : [ "Tom Kocmi", "Ondrej Bojar." ],
      "venue" : "RANLP.",
      "citeRegEx" : "Kocmi and Bojar.,? 2017",
      "shortCiteRegEx" : "Kocmi and Bojar.",
      "year" : 2017
    }, {
      "title" : "A hierarchical approach for generating descriptive image paragraphs",
      "author" : [ "Jonathan Krause", "Justin Johnson", "Ranjay Krishna", "Li Fei-Fei." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Krause et al\\.,? 2017",
      "shortCiteRegEx" : "Krause et al\\.",
      "year" : 2017
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Krizhevsky et al\\.,? 2012",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Reinforcement learning based curriculum optimization for neural machine translation",
      "author" : [ "Gaurav Kumar", "George F. Foster", "Colin Cherry", "Maxim Krikun." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Kumar et al\\.,? 2019",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2019
    }, {
      "title" : "Knowledge-driven encode, retrieve, paraphrase for medical image report generation",
      "author" : [ "Christy Y. Li", "Xiaodan Liang", "Zhiting Hu", "Eric P. Xing." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "A competence-aware curriculum for visual concepts learning via question answering",
      "author" : [ "Qing Li", "Siyuan Huang", "Yining Hong", "Song-Chun Zhu." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Hybrid retrieval-generation reinforced agent for medical image report generation",
      "author" : [ "Yuan Li", "Xiaodan Liang", "Zhiting Hu", "Eric P. Xing." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Recurrent topic-transition GAN for visual paragraph generation",
      "author" : [ "Xiaodan Liang", "Zhiting Hu", "Hao Zhang", "Chuang Gan", "Eric P. Xing." ],
      "venue" : "ICCV.",
      "citeRegEx" : "Liang et al\\.,? 2017",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2017
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "ACL.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Aligning visual regions and textual concepts for semantic-grounded image representations",
      "author" : [ "Fenglin Liu", "Yuanxin Liu", "Xuancheng Ren", "Xiaodong He", "Xu Sun." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring and distilling crossmodal information for image captioning",
      "author" : [ "Fenglin Liu", "Xuancheng Ren", "Yuanxin Liu", "Kai Lei", "Xu Sun." ],
      "venue" : "IJCAI.",
      "citeRegEx" : "Liu et al\\.,? 2019b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "simnet: Stepwise imagetopic merging network for generating detailed and comprehensive image captions",
      "author" : [ "Fenglin Liu", "Xuancheng Ren", "Yuanxin Liu", "Houfeng Wang", "Xu Sun." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Prophet attention: Predicting attention with future attention",
      "author" : [ "Fenglin Liu", "Xuancheng Ren", "Xian Wu", "Shen Ge", "Wei Fan", "Yuexian Zou", "Xu Sun." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Liu et al\\.,? 2020a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring and distilling posterior and prior knowledge for radiology report generation",
      "author" : [ "Fenglin Liu", "Xian Wu", "Shen Ge", "Wei Fan", "Yuexian Zou." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Liu et al\\.,? 2021a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Contrastive attention for automatic chest x-ray report generation",
      "author" : [ "Fenglin Liu", "Changchang Yin", "Xian Wu", "Shen Ge", "Ping Zhang", "Xu Sun." ],
      "venue" : "ACL (Findings).",
      "citeRegEx" : "Liu et al\\.,? 2021b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Clinically accurate chest x-ray report generation",
      "author" : [ "Guanxiong Liu", "Tzu-Ming Harry Hsu", "Matthew B.A. McDermott", "Willie Boag", "Wei-Hung Weng", "Peter Szolovits", "Marzyeh Ghassemi." ],
      "venue" : "MLHC.",
      "citeRegEx" : "Liu et al\\.,? 2019c",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Norm-based curriculum learning for neural machine translation",
      "author" : [ "Xuebo Liu", "Houtim Lai", "Derek F. Wong", "Lidia S. Chao." ],
      "venue" : "ACL.",
      "citeRegEx" : "Liu et al\\.,? 2020b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning to generate clinically coherent chest x-ray reports",
      "author" : [ "Justin R. Lovelace", "Bobak Mortazavi." ],
      "venue" : "EMNLP (Findings).",
      "citeRegEx" : "Lovelace and Mortazavi.,? 2020",
      "shortCiteRegEx" : "Lovelace and Mortazavi.",
      "year" : 2020
    }, {
      "title" : "Knowing when to look: Adaptive attention via a visual sentinel for image captioning",
      "author" : [ "Jiasen Lu", "Caiming Xiong", "Devi Parikh", "Richard Socher." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Lu et al\\.,? 2017",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2017
    }, {
      "title" : "Improving factual completeness and consistency of image-totext radiology report generation",
      "author" : [ "Yasuhide Miura", "Yuhao Zhang", "Emily Bao Tsai", "Curtis P. Langlotz", "Dan Jurafsky." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Miura et al\\.,? 2021",
      "shortCiteRegEx" : "Miura et al\\.",
      "year" : 2021
    }, {
      "title" : "BLEU: a Method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "ACL.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Competence-based curriculum learning for neural machine translation",
      "author" : [ "Emmanouil Antonios Platanios", "Otilia Stretcu", "Graham Neubig", "Barnabás Póczos", "Tom M. Mitchell." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Platanios et al\\.,? 2019",
      "shortCiteRegEx" : "Platanios et al\\.",
      "year" : 2019
    }, {
      "title" : "Self-critical sequence training for image captioning",
      "author" : [ "Steven J. Rennie", "Etienne Marcheret", "Youssef Mroueh", "Jarret Ross", "Vaibhava Goel." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Rennie et al\\.,? 2017",
      "shortCiteRegEx" : "Rennie et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning to read chest x-rays: Recurrent neural cascade model for automated image annotation",
      "author" : [ "Hoo-Chang Shin", "Kirk Roberts", "Le Lu", "Dina DemnerFushman", "Jianhua Yao", "Ronald M. Summers." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Shin et al\\.,? 2016",
      "shortCiteRegEx" : "Shin et al\\.",
      "year" : 2016
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Tienet: Text-image embedding network for common thorax disease classification and reporting in chest x-rays",
      "author" : [ "Xiaosong Wang", "Yifan Peng", "Le Lu", "Zhiyong Lu", "Ronald M. Summers." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Dynamic curriculum learning for imbalanced data classification",
      "author" : [ "Yiru Wang", "Weihao Gan", "Jie Yang", "Wei Wu", "Junjie Yan." ],
      "venue" : "ICCV.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Curriculum learning by transfer learning: Theory and experiments with deep networks",
      "author" : [ "Daphna Weinshall", "Gad Cohen", "Dan Amir." ],
      "venue" : "ICML.",
      "citeRegEx" : "Weinshall et al\\.,? 2018",
      "shortCiteRegEx" : "Weinshall et al\\.",
      "year" : 2018
    }, {
      "title" : "Dynamic curriculum learning for lowresource neural machine translation",
      "author" : [ "Chen Xu", "Bojie Hu", "Yufan Jiang", "Kai Feng", "Zeyang Wang", "Shen Huang", "Qi Ju", "Tong Xiao", "Jingbo Zhu." ],
      "venue" : "COLING.",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio." ],
      "venue" : "ICML.",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Multimodal recurrent model with attention for automated radiology report generation",
      "author" : [ "Yuan Xue", "Tao Xu", "L. Rodney Long", "Zhiyun Xue", "Sameer K. Antani", "George R. Thoma", "Xiaolei Huang." ],
      "venue" : "MICCAI.",
      "citeRegEx" : "Xue et al\\.,? 2018",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2018
    }, {
      "title" : "Video paragraph captioning using hierarchical recurrent neural networks",
      "author" : [ "Haonan Yu", "Jiang Wang", "Zhiheng Huang", "Yi Yang", "Wei Xu." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Yu et al\\.,? 2016",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2016
    }, {
      "title" : "Automatic radiology report generation based on multi-view image fusion and medical concept enrichment",
      "author" : [ "Jianbo Yuan", "Haofu Liao", "Rui Luo", "Jiebo Luo." ],
      "venue" : "MICCAI.",
      "citeRegEx" : "Yuan et al\\.,? 2019",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2019
    }, {
      "title" : "An empirical exploration of curriculum learning for neural machine translation",
      "author" : [ "Xuan Zhang", "Gaurav Kumar", "Huda Khayrallah", "Kenton Murray", "Jeremy Gwinnup", "Marianna J. Martindale", "Paul McNamee", "Kevin Duh", "Marine Carpuat." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "When radiology report generation meets knowledge graph",
      "author" : [ "Yixiao Zhang", "Xiaosong Wang", "Ziyue Xu", "Qihang Yu", "Alan L. Yuille", "Daguang Xu." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Zhang et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Optimizing the factual correctness of a summary: A study of summarizing radiology reports",
      "author" : [ "Yuhao Zhang", "Derek Merck", "Emily Bao Tsai", "Christopher D. Manning", "Curtis Langlotz." ],
      "venue" : "ACL.",
      "citeRegEx" : "Zhang et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Reinforced curriculum learning on pretrained neural machine translation models",
      "author" : [ "Mingjun Zhao", "Haijiang Wu", "Di Niu", "Xiaoli Wang." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : ", radiology and pathology images, and their corresponding reports, which describe the observations in details of both normal and abnormal regions, are widely-used for diagnosis and treatment (Delrue et al., 2011; Goergen et al., 2013).",
      "startOffset" : 191,
      "endOffset" : 234
    }, {
      "referenceID" : 9,
      "context" : ", radiology and pathology images, and their corresponding reports, which describe the observations in details of both normal and abnormal regions, are widely-used for diagnosis and treatment (Delrue et al., 2011; Goergen et al., 2013).",
      "startOffset" : 191,
      "endOffset" : 234
    }, {
      "referenceID" : 15,
      "context" : "Figure 1: Two examples of ground truth reports and reports generated by a state-of-the-art approach CoAttention (Jing et al., 2018) and our approach.",
      "startOffset" : 112,
      "endOffset" : 131
    }, {
      "referenceID" : 15,
      "context" : "There are notable visual and textual data biases and the Co-Attention (Jing et al., 2018) fails to depict the rare but important abnormalities and generates some error sentences (Underlined text) and repeated sentences (Italic text).",
      "startOffset" : 70,
      "endOffset" : 89
    }, {
      "referenceID" : 44,
      "context" : "intelligence and clinical medicine (Jing et al., 2018, 2019; Li et al., 2018, 2019; Wang et al., 2018; Xue et al., 2018; Yuan et al., 2019; Zhang et al., 2020a; Chen et al., 2020; Liu et al., 2021a,b, 2019c).",
      "startOffset" : 35,
      "endOffset" : 207
    }, {
      "referenceID" : 49,
      "context" : "intelligence and clinical medicine (Jing et al., 2018, 2019; Li et al., 2018, 2019; Wang et al., 2018; Xue et al., 2018; Yuan et al., 2019; Zhang et al., 2020a; Chen et al., 2020; Liu et al., 2021a,b, 2019c).",
      "startOffset" : 35,
      "endOffset" : 207
    }, {
      "referenceID" : 51,
      "context" : "intelligence and clinical medicine (Jing et al., 2018, 2019; Li et al., 2018, 2019; Wang et al., 2018; Xue et al., 2018; Yuan et al., 2019; Zhang et al., 2020a; Chen et al., 2020; Liu et al., 2021a,b, 2019c).",
      "startOffset" : 35,
      "endOffset" : 207
    }, {
      "referenceID" : 53,
      "context" : "intelligence and clinical medicine (Jing et al., 2018, 2019; Li et al., 2018, 2019; Wang et al., 2018; Xue et al., 2018; Yuan et al., 2019; Zhang et al., 2020a; Chen et al., 2020; Liu et al., 2021a,b, 2019c).",
      "startOffset" : 35,
      "endOffset" : 207
    }, {
      "referenceID" : 41,
      "context" : "However, directly applying image captioning approaches to medical images has the following problems: 1) Visual data bias: the normal images dominate the dataset over the abnormal ones (Shin et al., 2016).",
      "startOffset" : 184,
      "endOffset" : 203
    }, {
      "referenceID" : 14,
      "context" : "As a result, the visual and textual biases could mislead the model training (Jing et al., 2019; Xue et al., 2018; Yuan et al., 2019; Liu et al., 2021a,b; Li et al., 2018).",
      "startOffset" : 76,
      "endOffset" : 170
    }, {
      "referenceID" : 49,
      "context" : "As a result, the visual and textual biases could mislead the model training (Jing et al., 2019; Xue et al., 2018; Yuan et al., 2019; Liu et al., 2021a,b; Li et al., 2018).",
      "startOffset" : 76,
      "endOffset" : 170
    }, {
      "referenceID" : 51,
      "context" : "As a result, the visual and textual biases could mislead the model training (Jing et al., 2019; Xue et al., 2018; Yuan et al., 2019; Liu et al., 2021a,b; Li et al., 2018).",
      "startOffset" : 76,
      "endOffset" : 170
    }, {
      "referenceID" : 24,
      "context" : "As a result, the visual and textual biases could mislead the model training (Jing et al., 2019; Xue et al., 2018; Yuan et al., 2019; Liu et al., 2021a,b; Li et al., 2018).",
      "startOffset" : 76,
      "endOffset" : 170
    }, {
      "referenceID" : 15,
      "context" : "As shown in Figure 1, even a stateof-the-art model (Jing et al., 2018) still generates some repeated sentences of normalities and fails to depict the rare but important abnormalities.",
      "startOffset" : 51,
      "endOffset" : 70
    }, {
      "referenceID" : 6,
      "context" : ", IU-Xray (Demner-Fushman et al., 2016) and MIMIC-CXR (Johnson et al.",
      "startOffset" : 10,
      "endOffset" : 39
    }, {
      "referenceID" : 3,
      "context" : "Image Captioning and Paragraph Generation The task of image captioning (Chen et al., 2015; Vinyals et al., 2015), which aims to generate a sentence to describe the given image, has received extensive research interests (Anderson et al.",
      "startOffset" : 71,
      "endOffset" : 112
    }, {
      "referenceID" : 43,
      "context" : "Image Captioning and Paragraph Generation The task of image captioning (Chen et al., 2015; Vinyals et al., 2015), which aims to generate a sentence to describe the given image, has received extensive research interests (Anderson et al.",
      "startOffset" : 71,
      "endOffset" : 112
    }, {
      "referenceID" : 0,
      "context" : ", 2015), which aims to generate a sentence to describe the given image, has received extensive research interests (Anderson et al., 2018; Rennie et al., 2017; Liu et al., 2019a, 2020a).",
      "startOffset" : 114,
      "endOffset" : 184
    }, {
      "referenceID" : 40,
      "context" : ", 2015), which aims to generate a sentence to describe the given image, has received extensive research interests (Anderson et al., 2018; Rennie et al., 2017; Liu et al., 2019a, 2020a).",
      "startOffset" : 114,
      "endOffset" : 184
    }, {
      "referenceID" : 43,
      "context" : "Such an encoder-decoder framework have achieved great success in advancing the state-of-the-arts (Vinyals et al., 2015; Lu et al., 2017; Xu et al., 2015; Liu et al., 2018, 2019b).",
      "startOffset" : 97,
      "endOffset" : 178
    }, {
      "referenceID" : 36,
      "context" : "Such an encoder-decoder framework have achieved great success in advancing the state-of-the-arts (Vinyals et al., 2015; Lu et al., 2017; Xu et al., 2015; Liu et al., 2018, 2019b).",
      "startOffset" : 97,
      "endOffset" : 178
    }, {
      "referenceID" : 48,
      "context" : "Such an encoder-decoder framework have achieved great success in advancing the state-of-the-arts (Vinyals et al., 2015; Lu et al., 2017; Xu et al., 2015; Liu et al., 2018, 2019b).",
      "startOffset" : 97,
      "endOffset" : 178
    }, {
      "referenceID" : 20,
      "context" : "Specifically, the encoder network (Krizhevsky et al., 2012; He et al., 2016) computes visual representations for the visual contents and the decoder network (Hochreiter and Schmidhuber, 1997; Vaswani et al.",
      "startOffset" : 34,
      "endOffset" : 76
    }, {
      "referenceID" : 11,
      "context" : "Specifically, the encoder network (Krizhevsky et al., 2012; He et al., 2016) computes visual representations for the visual contents and the decoder network (Hochreiter and Schmidhuber, 1997; Vaswani et al.",
      "startOffset" : 34,
      "endOffset" : 76
    }, {
      "referenceID" : 12,
      "context" : ", 2016) computes visual representations for the visual contents and the decoder network (Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017) generates a target sentence based on the visual representations.",
      "startOffset" : 88,
      "endOffset" : 144
    }, {
      "referenceID" : 42,
      "context" : ", 2016) computes visual representations for the visual contents and the decoder network (Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017) generates a target sentence based on the visual representations.",
      "startOffset" : 88,
      "endOffset" : 144
    }, {
      "referenceID" : 19,
      "context" : "In contrast to the image captioning, image paragraph generation, which aims to produce a long and semanticcoherent paragraph to describe the input image, has recently attracted growing research interests (Krause et al., 2017; Liang et al., 2017; Yu et al., 2016).",
      "startOffset" : 204,
      "endOffset" : 262
    }, {
      "referenceID" : 25,
      "context" : "In contrast to the image captioning, image paragraph generation, which aims to produce a long and semanticcoherent paragraph to describe the input image, has recently attracted growing research interests (Krause et al., 2017; Liang et al., 2017; Yu et al., 2016).",
      "startOffset" : 204,
      "endOffset" : 262
    }, {
      "referenceID" : 50,
      "context" : "In contrast to the image captioning, image paragraph generation, which aims to produce a long and semanticcoherent paragraph to describe the input image, has recently attracted growing research interests (Krause et al., 2017; Liang et al., 2017; Yu et al., 2016).",
      "startOffset" : 204,
      "endOffset" : 262
    }, {
      "referenceID" : 19,
      "context" : "To perform the image paragraph generation, a hierarchical LSTM (HLSTM) (Krause et al., 2017; Liang et al., 2017) is proposed as the decoder to well generate long paragraphs.",
      "startOffset" : 71,
      "endOffset" : 112
    }, {
      "referenceID" : 25,
      "context" : "To perform the image paragraph generation, a hierarchical LSTM (HLSTM) (Krause et al., 2017; Liang et al., 2017) is proposed as the decoder to well generate long paragraphs.",
      "startOffset" : 71,
      "endOffset" : 112
    }, {
      "referenceID" : 24,
      "context" : "Similar to image paragraph generation, most existing medical report generation works (Jing et al., 2018, 2019; Li et al., 2018; Wang et al., 2018; Xue et al., 2018; Yuan et al., 2019; Zhang et al., 2020a,b; Miura et al., 2021; Lovelace and Mortazavi, 2020; Liu et al., 2021b, 2019c) attempt to adopt a CNN-HLSTM based model to automatically generate a fluent report.",
      "startOffset" : 85,
      "endOffset" : 282
    }, {
      "referenceID" : 44,
      "context" : "Similar to image paragraph generation, most existing medical report generation works (Jing et al., 2018, 2019; Li et al., 2018; Wang et al., 2018; Xue et al., 2018; Yuan et al., 2019; Zhang et al., 2020a,b; Miura et al., 2021; Lovelace and Mortazavi, 2020; Liu et al., 2021b, 2019c) attempt to adopt a CNN-HLSTM based model to automatically generate a fluent report.",
      "startOffset" : 85,
      "endOffset" : 282
    }, {
      "referenceID" : 49,
      "context" : "Similar to image paragraph generation, most existing medical report generation works (Jing et al., 2018, 2019; Li et al., 2018; Wang et al., 2018; Xue et al., 2018; Yuan et al., 2019; Zhang et al., 2020a,b; Miura et al., 2021; Lovelace and Mortazavi, 2020; Liu et al., 2021b, 2019c) attempt to adopt a CNN-HLSTM based model to automatically generate a fluent report.",
      "startOffset" : 85,
      "endOffset" : 282
    }, {
      "referenceID" : 51,
      "context" : "Similar to image paragraph generation, most existing medical report generation works (Jing et al., 2018, 2019; Li et al., 2018; Wang et al., 2018; Xue et al., 2018; Yuan et al., 2019; Zhang et al., 2020a,b; Miura et al., 2021; Lovelace and Mortazavi, 2020; Liu et al., 2021b, 2019c) attempt to adopt a CNN-HLSTM based model to automatically generate a fluent report.",
      "startOffset" : 85,
      "endOffset" : 282
    }, {
      "referenceID" : 37,
      "context" : "Similar to image paragraph generation, most existing medical report generation works (Jing et al., 2018, 2019; Li et al., 2018; Wang et al., 2018; Xue et al., 2018; Yuan et al., 2019; Zhang et al., 2020a,b; Miura et al., 2021; Lovelace and Mortazavi, 2020; Liu et al., 2021b, 2019c) attempt to adopt a CNN-HLSTM based model to automatically generate a fluent report.",
      "startOffset" : 85,
      "endOffset" : 282
    }, {
      "referenceID" : 35,
      "context" : "Similar to image paragraph generation, most existing medical report generation works (Jing et al., 2018, 2019; Li et al., 2018; Wang et al., 2018; Xue et al., 2018; Yuan et al., 2019; Zhang et al., 2020a,b; Miura et al., 2021; Lovelace and Mortazavi, 2020; Liu et al., 2021b, 2019c) attempt to adopt a CNN-HLSTM based model to automatically generate a fluent report.",
      "startOffset" : 85,
      "endOffset" : 282
    }, {
      "referenceID" : 2,
      "context" : "Curriculum Learning In recent years, curriculum learning (Bengio et al., 2009), which enables the models to gradually proceed from easy samples to more complex ones in training (Elman, 1993), has received growing research interests in natural language processing field, e.",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 8,
      "context" : ", 2009), which enables the models to gradually proceed from easy samples to more complex ones in training (Elman, 1993), has received growing research interests in natural language processing field, e.",
      "startOffset" : 106,
      "endOffset" : 119
    }, {
      "referenceID" : 39,
      "context" : ", neural machine translation (Platanios et al., 2019; Kumar et al., 2019; Zhao et al., 2020; Liu et al., 2020b; Zhang et al., 2018; Kocmi and Bojar, 2017; Xu et al., 2020) and computer vision field, e.",
      "startOffset" : 29,
      "endOffset" : 171
    }, {
      "referenceID" : 21,
      "context" : ", neural machine translation (Platanios et al., 2019; Kumar et al., 2019; Zhao et al., 2020; Liu et al., 2020b; Zhang et al., 2018; Kocmi and Bojar, 2017; Xu et al., 2020) and computer vision field, e.",
      "startOffset" : 29,
      "endOffset" : 171
    }, {
      "referenceID" : 55,
      "context" : ", neural machine translation (Platanios et al., 2019; Kumar et al., 2019; Zhao et al., 2020; Liu et al., 2020b; Zhang et al., 2018; Kocmi and Bojar, 2017; Xu et al., 2020) and computer vision field, e.",
      "startOffset" : 29,
      "endOffset" : 171
    }, {
      "referenceID" : 34,
      "context" : ", neural machine translation (Platanios et al., 2019; Kumar et al., 2019; Zhao et al., 2020; Liu et al., 2020b; Zhang et al., 2018; Kocmi and Bojar, 2017; Xu et al., 2020) and computer vision field, e.",
      "startOffset" : 29,
      "endOffset" : 171
    }, {
      "referenceID" : 52,
      "context" : ", neural machine translation (Platanios et al., 2019; Kumar et al., 2019; Zhao et al., 2020; Liu et al., 2020b; Zhang et al., 2018; Kocmi and Bojar, 2017; Xu et al., 2020) and computer vision field, e.",
      "startOffset" : 29,
      "endOffset" : 171
    }, {
      "referenceID" : 18,
      "context" : ", neural machine translation (Platanios et al., 2019; Kumar et al., 2019; Zhao et al., 2020; Liu et al., 2020b; Zhang et al., 2018; Kocmi and Bojar, 2017; Xu et al., 2020) and computer vision field, e.",
      "startOffset" : 29,
      "endOffset" : 171
    }, {
      "referenceID" : 47,
      "context" : ", neural machine translation (Platanios et al., 2019; Kumar et al., 2019; Zhao et al., 2020; Liu et al., 2020b; Zhang et al., 2018; Kocmi and Bojar, 2017; Xu et al., 2020) and computer vision field, e.",
      "startOffset" : 29,
      "endOffset" : 171
    }, {
      "referenceID" : 46,
      "context" : ", image classification (Weinshall et al., 2018), human attribute analysis(Wang et al.",
      "startOffset" : 23,
      "endOffset" : 47
    }, {
      "referenceID" : 45,
      "context" : ", 2018), human attribute analysis(Wang et al., 2019) and visual question answering (Li et al.",
      "startOffset" : 33,
      "endOffset" : 52
    }, {
      "referenceID" : 23,
      "context" : ", 2019) and visual question answering (Li et al., 2020).",
      "startOffset" : 38,
      "endOffset" : 55
    }, {
      "referenceID" : 55,
      "context" : "(2019) proposed to utilize the training samples in order of easy-to-hard and to describe the “difficulty” of a training sample using the sentence length or the rarity of the words appearing in it (Zhao et al., 2020).",
      "startOffset" : 196,
      "endOffset" : 215
    }, {
      "referenceID" : 39,
      "context" : "However, these methods (Platanios et al., 2019; Liu et al., 2020b; Xu et al., 2020) are single difficulty-based and unimodal curriculum learning approaches.",
      "startOffset" : 23,
      "endOffset" : 83
    }, {
      "referenceID" : 34,
      "context" : "However, these methods (Platanios et al., 2019; Liu et al., 2020b; Xu et al., 2020) are single difficulty-based and unimodal curriculum learning approaches.",
      "startOffset" : 23,
      "endOffset" : 83
    }, {
      "referenceID" : 47,
      "context" : "However, these methods (Platanios et al., 2019; Liu et al., 2020b; Xu et al., 2020) are single difficulty-based and unimodal curriculum learning approaches.",
      "startOffset" : 23,
      "endOffset" : 83
    }, {
      "referenceID" : 5,
      "context" : "As stated in Section 2, the key challenge of medical report generation is to accurately capture and describe the abnormalities (Delrue et al., 2011; Goergen et al., 2013; Li et al., 2018).",
      "startOffset" : 127,
      "endOffset" : 187
    }, {
      "referenceID" : 9,
      "context" : "As stated in Section 2, the key challenge of medical report generation is to accurately capture and describe the abnormalities (Delrue et al., 2011; Goergen et al., 2013; Li et al., 2018).",
      "startOffset" : 127,
      "endOffset" : 187
    }, {
      "referenceID" : 24,
      "context" : "As stated in Section 2, the key challenge of medical report generation is to accurately capture and describe the abnormalities (Delrue et al., 2011; Goergen et al., 2013; Li et al., 2018).",
      "startOffset" : 127,
      "endOffset" : 187
    }, {
      "referenceID" : 11,
      "context" : "To measure such visual difficulty, we adopt the widely-used ResNet-50 (He et al., 2016) pre-trained on ImageNet (Deng et al.",
      "startOffset" : 70,
      "endOffset" : 87
    }, {
      "referenceID" : 7,
      "context" : ", 2016) pre-trained on ImageNet (Deng et al., 2009) and fine-tuned on CheXpert dataset (Irvin et al.",
      "startOffset" : 32,
      "endOffset" : 51
    }, {
      "referenceID" : 14,
      "context" : "Heuristic Metric d3 A serious problem for medical report generation models is the tendency to generate plausible general reports with no prominent abnormal narratives (Jing et al., 2019; Li et al., 2018; Yuan et al., 2019).",
      "startOffset" : 167,
      "endOffset" : 222
    }, {
      "referenceID" : 24,
      "context" : "Heuristic Metric d3 A serious problem for medical report generation models is the tendency to generate plausible general reports with no prominent abnormal narratives (Jing et al., 2019; Li et al., 2018; Yuan et al., 2019).",
      "startOffset" : 167,
      "endOffset" : 222
    }, {
      "referenceID" : 51,
      "context" : "Heuristic Metric d3 A serious problem for medical report generation models is the tendency to generate plausible general reports with no prominent abnormal narratives (Jing et al., 2019; Li et al., 2018; Yuan et al., 2019).",
      "startOffset" : 167,
      "endOffset" : 222
    }, {
      "referenceID" : 47,
      "context" : "To this end, we define the difficulty using the negative log-likelihood loss values (Xu et al., 2020; Zhang et al., 2018) of training samples.",
      "startOffset" : 84,
      "endOffset" : 121
    }, {
      "referenceID" : 52,
      "context" : "To this end, we define the difficulty using the negative log-likelihood loss values (Xu et al., 2020; Zhang et al., 2018) of training samples.",
      "startOffset" : 84,
      "endOffset" : 121
    }, {
      "referenceID" : 39,
      "context" : "3005 Algorithm 1 Single Difficulty-based Curriculum Learning (Platanios et al., 2019).",
      "startOffset" : 61,
      "endOffset" : 85
    }, {
      "referenceID" : 15,
      "context" : "adopt the widely-used and classic CNN-HLSTM (Jing et al., 2018), in which the CNN is implemented with ResNet-50, trained on the downstream dataset used for evaluation with a cross-entropy loss.",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 55,
      "context" : ", 2019), the n-gram rarity together with Named Entity Recognition (NER) and Parts of Speech (POS) taggings (Zhao et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 126
    }, {
      "referenceID" : 39,
      "context" : "ventional single difficulty-based curriculum (Platanios et al., 2019).",
      "startOffset" : 45,
      "endOffset" : 69
    }, {
      "referenceID" : 16,
      "context" : ", 2016) and a recently released largescale MIMIC-CXR (Johnson et al., 2019).",
      "startOffset" : 53,
      "endOffset" : 75
    }, {
      "referenceID" : 14,
      "context" : "For IU-Xray dataset, following previous works (Chen et al., 2020; Jing et al., 2019; Li et al., 2019, 2018), we randomly split the dataset into 70%10%-20% training-validation-testing splits.",
      "startOffset" : 46,
      "endOffset" : 107
    }, {
      "referenceID" : 38,
      "context" : "We adopt the widely-used BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE-L (Lin, 2004), which are reported by the",
      "startOffset" : 30,
      "endOffset" : 53
    }, {
      "referenceID" : 1,
      "context" : ", 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE-L (Lin, 2004), which are reported by the",
      "startOffset" : 16,
      "endOffset" : 42
    }, {
      "referenceID" : 26,
      "context" : ", 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE-L (Lin, 2004), which are reported by the",
      "startOffset" : 55,
      "endOffset" : 66
    }, {
      "referenceID" : 3,
      "context" : "gov/MTI/ evaluation toolkit (Chen et al., 2015)5, to test the performance.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 39,
      "context" : "lowing previous work (Platanios et al., 2019), the c(0) and p are set to 0.",
      "startOffset" : 21,
      "endOffset" : 45
    }, {
      "referenceID" : 47,
      "context" : "To boost the performance, we further incorporate the Batching method (Xu et al., 2020), which batches the samples with similar difficulty in the curriculum learning framework.",
      "startOffset" : 69,
      "endOffset" : 86
    }, {
      "referenceID" : 11,
      "context" : "features for both dataset used for evaluation from a ResNet-50 (He et al., 2016), which is pretrained on ImageNet (Deng et al.",
      "startOffset" : 63,
      "endOffset" : 80
    }, {
      "referenceID" : 7,
      "context" : ", 2016), which is pretrained on ImageNet (Deng et al., 2009) and fine-tuned on public available CheXpert dataset (Irvin et al.",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 17,
      "context" : "For parameter optimization, we use Adam optimizer (Kingma and Ba, 2014) with a batch size of 16 and a learning rate of 1e-4.",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 24,
      "context" : ", HRGR-Agent (Li et al., 2018), CMAS-RL (Jing et al.",
      "startOffset" : 13,
      "endOffset" : 30
    }, {
      "referenceID" : 14,
      "context" : ", 2018), CMAS-RL (Jing et al., 2019), SentSAT + KG (Zhang et al.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 53,
      "context" : ", 2019), SentSAT + KG (Zhang et al., 2020a), Up-Down (Anderson et al.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : ", 2020a), Up-Down (Anderson et al., 2018), Transformer (Chen et al.",
      "startOffset" : 18,
      "endOffset" : 41
    }, {
      "referenceID" : 16,
      "context" : "3008 Methods Dataset: MIMIC-CXR (Johnson et al., 2019) Dataset: IU-Xray (Demner-Fushman et al.",
      "startOffset" : 32,
      "endOffset" : 54
    }, {
      "referenceID" : 6,
      "context" : ", 2019) Dataset: IU-Xray (Demner-Fushman et al., 2016) B-1 B-2 B-3 B-4 M R-L B-1 B-2 B-3 B-4 M R-L",
      "startOffset" : 25,
      "endOffset" : 54
    }, {
      "referenceID" : 16,
      "context" : "Methods Dataset: MIMIC-CXR (Johnson et al., 2019) Dataset: IU-Xray (Demner-Fushman et al.",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 6,
      "context" : ", 2019) Dataset: IU-Xray (Demner-Fushman et al., 2016) B-1 B-2 B-3 B-4 M R-L B-1 B-2 B-3 B-4 M R-L",
      "startOffset" : 25,
      "endOffset" : 54
    }, {
      "referenceID" : 15,
      "context" : "CNN-HLSTM (Jing et al., 2018)† 15 28 57 Co-Attention (Jing et al.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 15,
      "context" : ", 2018)† 15 28 57 Co-Attention (Jing et al., 2018)† 24 35 41",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 6,
      "context" : "3009 Settings Visual Difficulty Textual Difficulty Route Strategy Dataset: IU-Xray (Demner-Fushman et al., 2016) Heuristic Metric Model Confidence Heuristic Metric Model Confidence Baseline: CNN-HLSTM (Jing et al.",
      "startOffset" : 83,
      "endOffset" : 112
    }, {
      "referenceID" : 15,
      "context" : ", 2016) Heuristic Metric Model Confidence Heuristic Metric Model Confidence Baseline: CNN-HLSTM (Jing et al., 2018) B-1 B-2 B-3 B-4 M R-L",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 15,
      "context" : "We conduct the analysis on the widely-used baseline model CNN-HLSTM (Jing et al., 2018).",
      "startOffset" : 68,
      "endOffset" : 87
    } ],
    "year" : 2021,
    "abstractText" : "Medical report generation task, which targets to produce long and coherent descriptions of medical images, has attracted growing research interests recently. Different from the general image captioning tasks, medical report generation is more challenging for data-driven neural models. This is mainly due to 1) the serious data bias and 2) the limited medical data. To alleviate the data bias and make best use of available data, we propose a Competencebased Multimodal Curriculum Learning framework (CMCL). Specifically, CMCL simulates the learning process of radiologists and optimizes the model in a step by step manner. Firstly, CMCL estimates the difficulty of each training instance and evaluates the competence of current model; Secondly, CMCL selects the most suitable batch of training instances considering current model competence. By iterating above two steps, CMCL can gradually improve the model’s performance. The experiments on the public IU-Xray and MIMICCXR datasets show that CMCL can be incorporated into existing models to improve their performance.",
    "creator" : "LaTeX with hyperref"
  }
}