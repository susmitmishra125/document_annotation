{
  "name" : "2021.acl-long.496.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Argument Pair Extraction via Attention-guided Multi-Layer Multi-Cross Encoding",
    "authors" : [ "Liying Cheng", "Tianyu Wu", "Lidong Bing", "Luo Si" ],
    "emails" : [ "luo.si}@alibaba-inc.com", "wu@alumni.sutd.edu.sg" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6341–6353\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6341"
    }, {
      "heading" : "1 Introduction",
      "text" : "Mining argumentation structures within a corpus is a crucial task in argument mining research field (Palau and Moens, 2009). There are usually two main components in learning natural language argument structures: (1) detecting argumentative units, (2) predicting relations between the identified arguments. It has been widely studied by natural language processing (NLP) researchers (Cabrio and Villata, 2018) and applied to domains such as: web debating platforms (Boltužić and Šnajder, 2015; Swanson et al., 2015; Chakrabarty et al., 2019),\n∗Liying Cheng is under the Joint Ph.D. Program between Alibaba and Singapore University of Technology and Design.\n1Our code and data are available at https://github. com/TianyuTerry/MLMC.\npersuasive essays (Stab and Gurevych, 2014; Persing and Ng, 2016), social media (Abbott et al., 2016), etc. Unlike traditional argument extraction tasks that are mainly from monologues, Cheng et al. (2020) propose a new task - argument pair extraction (APE) from two passages in a new domain, namely peer review process, focusing on exploiting the interactions between reviewer comments and author rebuttals. As shown in Figure 1, APE task aims to extract the argument pairs from two passages. Specific suggestions, questions or challenges in reviews are considered as review arguments. Response sentences that answer or explain the specific review argument are its paired rebuttal arguments. For example in the pink area, the reviewer points out the lack of literature review in submission (i.e., review sentences 11-12). As a response, the authors argue that they select the literature based on the special focus of their work (i.e., rebuttal sentence 6-7).\nSimilar to the two components in the traditional argumentation structure mining, the APE task can be divided into two subtasks: (1) extracting the review and rebuttal arguments from two passages, (2) predicting if an extracted review argument and a rebuttal argument form an argument pair. The first subtask can be cast as a sequence labeling problem and the second one can be cast as a binary classification problem. One straightforward approach is to couple the two subtasks in a pipeline. However, such a pipeline approach learns two subtasks independently without sharing ample information. To address this limitation, the pioneering work (Cheng et al., 2020) employs a multi-task learning framework to train two subtasks simultaneously.\nHowever, there are several shortcomings in the multi-task model. First, the review passage and its rebuttal passage are concatenated as a single passage to perform the argument extraction subtask with sequence labeling. It is obvious to see from\nFigure 1 that the review and rebuttal passages have their own styles in terms of structure and wording. Hence, it is not suitable to concatenate them as one long sequence, which is against the fact that they are two unique sequences in essence and hinders the model from well-utilizing their different characteristics. To overcome this limitation, we treat review and rebuttal passages as two individual sequences and design two sequence encoders for them respectively. In each sequence encoder, the sequence representations will be updated by the other’s representations through mutual attention. It allows us to better distinguish two passages, and meanwhile, to conveniently exchange information between them through the attention mechanism.\nSecond, the subtask coordination capability of their multi-task framework is weak as two subtasks only coordinate with each other via the shared feature encoders, i.e., the sentence encoder for the sequence of word tokens and the passage encoder for the concatenation of sentences. Thus, the shared information between two subtasks is only learned implicitly. To overcome this limitation, we propose an attention-guided multi-layer multi-cross (MLMC) encoding mechanism. Inspired by the table-filling approach (Miwa and Sasaki, 2014), we form a table that represents features for the Cartesian Product of review and rebuttal sequences by\nutilizing both of their embeddings, as shown in the right portion of Figure 1. The table representations will be updated with the incorporation of the two sequence representations, and in return, it will also help to update the mutual attention mentioned above. It is named as multi-cross encoder because these three encoding components (i.e., one table and two sequences) interact with each other explicitly and extensively. By stacking multiple encoder layers, the two subtasks can further benefit each other. In addition, we also design an auxiliary attention loss to guide each argument to refer to its paired arguments. This additional loss not only enhances the model performance, but also significantly improves the attention interpretability.\nTo summarize, the contributions of this paper are three-fold. Firstly, we apply the table-filling approach to model the sentence-level correlation between two passages with multiple sentences for the first time. Secondly, on the model side, we propose an MLMC encoder to explicitly learn the useful shared information in the two passages. Furthermore, we introduce an auxiliary attention loss, which is able to further improve the efficacy of the mutual attentions. Thirdly, we evaluate our model on the benchmark dataset (Cheng et al., 2020), and the results show that our model achieves a new state-of-the-art performance on the APE task."
    }, {
      "heading" : "2 Related Work",
      "text" : "Argument mining has wide applications in educational domain, including persuasive essays (Stab and Gurevych, 2017; Eger et al., 2017), scientific articles (Teufel et al., 2009; Guo et al., 2011), writing assistance (Zhang and Litman, 2016), essay scoring (Persing and Ng, 2015; Somasundaran et al., 2016), peer reviews (Hua et al., 2019), etc. Unlike previous works, Cheng et al. (2020) introduce a new task named APE in the domain of peer review and rebuttal, which intends to extract the argument pairs from two passages simultaneously.\nTable-filling approaches (Miwa and Sasaki, 2014; Gupta et al., 2016; Zhang et al., 2017) have been proposed to work towards the joint task of name entity recognition (NER) and relation extraction (RE). In their work, the diagonal entries of the table show the words’ entity types and the off-diagonal entries show the relation types with other words. More recently, there are more research works to propose various table-filling models on different tasks. Wang and Lu (2020) propose to learn two separate encoders (a table encoder and a sequence encoder) by interacting with each other for joint NER and RE task. Wu et al. (2020) propose a grid tagging scheme to address the aspectoriented fine-grained opinion extraction task. Compared to our model, one major difference is the table shape. In their tables, the row and column represent the same sequence, and thus in square shape. In our model, the table is in a rectangle shape where the row and column represent two different sequences with different lengths. Another clear difference is that each entry in their table is for word-pair relation, whereas each entry in our table captures sentence-pair relation. As we can see from Figure 1, the review/rebuttal sequence consists of a list of sentences. Thus, it requires extra effort to learn comprehensive sentence representations."
    }, {
      "heading" : "3 Task Formulation",
      "text" : "In this paper, we tackle the APE task, which aims to study the internal structure and relations between two passages, e.g., review and rebuttal passages. For example, as shown in Figure 1, given a pair of review passage srv = [srv,1, · · · , srv,12] (in the red box) and rebuttal passage srb = [srb,1, · · · , srb,7] (in the orange box), we intend to automatically extract all argument pairs between them. First, for the argument mining subtask, we cast it as a sentence-level sequence labeling problem follow-\ning the work (Cheng et al., 2020) using the standard BIO scheme (Ramshaw, 1995; Ratinov and Roth, 2009). This subtask segments the argumentative units (highlighted in blue/pink) from nonargumentative units (highlighted in grey) for each passage. The label sequences for the review passage and the rebuttal passage are shown in the right portion of Figure 1. Second, the sentence pairing subtask predicts whether the two sentences belong to one argument pair. Here, we formulate it as a table-filling problem following the work (Miwa and Sasaki, 2014). Take the 8th review sentence srv,8 in the first review argument as an example, the rebuttal argument sentences {srb,2, srb,3, srb,4, srb,5} forming sentence pairs with it are filled with green, as shown in the table. With the collaboration of these two subtasks, we can perform the overall argument pair extraction task. In this case, two argument pairs (highlighted in blue/pink from two passages) are extracted, which correspond to the two green rectangles shown in the table."
    }, {
      "heading" : "4 Model",
      "text" : "Figure 2 shows our proposed attention-guided multi-layer multi-cross (MLMC) encoding based model. The model mainly consists of three parts: a sentence embedder, an n-layer multi-cross encoder, and a predictor. The review sentences and rebuttal sentences first go through the sentence embedder separately to obtain their sentence embeddings respectively. We then utilize the representations from review and rebuttal sequences to form a table as shown earlier in Figure 1. Next, the representations of the table and two sequences are updated through n multi-cross encoder layers. Finally, the model predicts the review and rebuttal arguments through a conditional random field (CRF) (Lafferty et al., 2001) layer based on two sequence representations, and extracts the pairing information through a multi-layer perceptron (MLP) based on the table representations."
    }, {
      "heading" : "4.1 Sentence Embedder",
      "text" : "The bottom left part of Figure 2 shows our sentence embedder, the input of which is a review sentence or a rebuttal sentence with l tokens s = [t0, t1, · · · , tl−1]. We obtain the pre-trained BERT (Devlin et al., 2019) token embeddings [x0, x1, · · · , xl−1] for all word tokens in the sentence, after which all token embeddings are fed into a bidirectional long short-term memory (biLSTM)\n(Hochreiter and Schmidhuber, 1997) layer. The last hidden states from both directions are concatenated as the sentence embedding S(0). A more common practice is to use the [CLS] token embedding to represent the sentence embedding. However, given the high density of scientific terms and the correspondence between review and rebuttal, token-level information is naturally crucial for the task. The same conclusion is drawn by the experimental results in the previous work (Cheng et al., 2020)."
    }, {
      "heading" : "4.2 Multi-Cross Encoder",
      "text" : "The entire multi-cross encoder consists of n layers. The details of each multi-cross encoder layer are shown in the blue dotted box on the right of Figure 2. The input of the layer includes table representations and two sequence representations, i.e., review and rebuttal sequence representations. In each layer, table features are updated by sequence features and vice versa.\nSequence Encoder Phase I To well-utilize different characteristics of review and rebuttal, we regard them as two individual sequences. Two sequence embeddings S(k−1)rv and S (k−1) rb of length I and J respectively (i.e., the output from the previous layer) are passed through the same biLSTM layer colored light yellow in Figure 2. Take review sequence as an example, the review hidden states at position i are updated as follows:\nS (k)′[1] rv,i = LSTMforward(S (k−1) rv,i , S (k)′[1] rv,i−1),\nS (k)′[2] rv,i = LSTMbackward(S (k−1) rv,i , S (k)′[2] rv,i+1),\nS (k)′ rv,i = [S (k)′[1] rv,i , S (k)′[2] rv,i ].\nThe rebuttal hidden states S(k) ′\nrb in layer k is obtained from the same biLSTM in the same manner.\nTable Encoder To capture the pairing information explicitly, we adopt the table-filling approach. At layer k, we update the table T(k−1)rv×rb through the table encoder. The table input T(0)rv×rb before the first encoder layer are set as 0. At each layer k, in order to incorporate the information extracted in S(k) ′ rv and S (k)′ rb , we form another table T (k−1)′′ rv×rb with them through concatenation and linear projection as follows:\nT (k−1)′′ rv×rb = Linear(S (k)′ rv ⊗ S(k) ′ rb ).\nThe table features from previous layer T(k−1)rv×rb are then updated by T(k−1) ′′\nrv×rb with layer normalization:\nT (k−1)′ rv×rb = LayerNorm(T (k−1) rv×rb ⊕T (k−1)′′ rv×rb ).\nThe entry T (k−1) ′\ni,j at row i and column j represents specific features between review sentence at position i and rebuttal sentence at position j. The table hidden states T (k)i,j are updated through 2D-GRU:\nT (k)[1] i,j = GRUforward(T (k)[1] i−1,j , T (k)[1] i,j−1 , T (k−1)′ i,j ),\nT (k)[2] i,j = GRUbackward(T (k)[2] i+1,j , T (k)[2] i,j+1 , T (k−1)′ i,j ),\nT (k) i,j = [T\n(k)[1] i,j , T (k)[2] i,j ].\nThe 2D-GRU settings are similar to the previous work (Wang and Lu, 2020) except that the table to be processed is not necessarily a square (I 6= J in general). Therefore, the 2D-GRU implemented here is more general. The previous hidden states for table boundaries (T (k) [1]\n0,j , T (k)[1] i,0 , T (k)[2] I+1,j , T (k)[2] i,J+1)\nare set as 0. The outputs T(k)rv×rp of layer k are further exploited by the mutual attention mechanism explained below to update review and rebuttal sequence embeddings.\nMutual Attention The mutual attention mechanism (shown as review attention and rebuttal attention modules in Figure 2) links review embedding, rebuttal embedding and table embedding together, through which review embedding and rebuttal embedding update each other with the help of table features. The attention weights α(k)i,j and β (k) i,j at position (i, j) in layer k are updated as follows:\nα (k) i,j = tanh(v T α · T (k) i,j ), β (k) i,j = tanh(v T β · T (k) i,j ),\nwhere vα and vβ are learnable vectors. We further normalize the attention weights:\na (k) i,j = exp(α (k) i,j )∑J\nj ′ =1\nexp(α (k) i,j ′ ) , b\n(k) i,j = exp(β (k) i,j )∑I\ni ′ =1\nexp(β (k) i ′ ,j ) .\nHere, a(k)i,j and b (k) i,j are the normalized attention weights ranging from 0 to 1. We then get the weighted average of sentence representations S(k) ′′\nrv,i\nand S(k) ′′ rb,j from S (k)′ rb and S (k)′ rv respectively.\nS (k)′′ rv,i = J∑ j=1 a (k) i,j · S (k)′ rb,j , S (k)′′ rb,j = I∑ i=1 b (k) i,j · S (k)′ rv,i .\nHere, S(k) ′′ rv and S (k)′′\nrb are the updated review embedding and rebuttal embedding. Information in review and rebuttal sequences is exchanged via mutual attention.\nSequence Encoder Phase II The addition and layer normalization used to combine S(k) ′ and S(k) ′′\nin the sequence encoder are similar to the one in table encoder. We obtain the review sequence embedding S(k)rv and rebuttal sequence embedding S (k) rb as the sequence outputs of layer k as follows:\nS (k) rv = LayerNorm(S (k)′ rv ⊕ S(k) ′′ rv ),\nS (k) rb = LayerNorm(S\n(k)′ rb ⊕ S (k)′′ rb ).\nStacking Multi-Cross Encoder Layers The updating process described above continues as layer grows from 1 to n. The table feature is updated by both review and rebuttal sequences, and each sequence updates the other via the table later on.\nThere are also residual connections between adjacent layers which accept the previous layer’s output as the current layer’s input and include it as part of the new embedding, making the system more robust. All three features (i.e., review sequence, rebuttal sequence, table) are intertwined with each other and information flows across different components of the encoder. This is also the reason why the encoder is described as MLMC."
    }, {
      "heading" : "4.3 Argument Pair Predictor",
      "text" : "After the final multi-cross encoder layer, sequence features are used for argument mining and table features are used for pair prediction.\nArgument Predictor We adopt CRF to predict argument sequence labels. The sequence labeling loss Lseq for both review sequence srv and rebuttal sequence srb in each instance is defined as:\nLseq = − ( log p(yrv|srv) + log p(yrb|srb) ) ,\nwhere yrv and yrb are the review and rebuttal sequence labels 2.\nDuring inference, the predicted sequence label is the one with the highest conditional probability given the original sequence:\ny∗rv = arg max y p(y|srv), y∗rb = arg max y p(y|srb).\nPair Predictor We use MLP to predict sentence pairs 3. The pairing loss Lpair for each instance is:\nLpair = − ∑\ni,j ( ypairi,j log p(y pair i,j = 1|srv, srb)\n+ (1− ypairi,j ) log p(y pair i,j = 0|srv, srb)\n) ,\nwhere ypairi,j is 1 when srv,i and srb,j are paired, and is 0 otherwise 4.\nFollowing (Cheng et al., 2020), during evaluation, a pair of candidate spans ([srv,i1 , · · · , srv,i2 ] and [srb,j1 , · · · , srb,j2 ]) form a pair if they satisfy the following criterion:∑i2\ni=i1 ∑j2 j=j1 1 {p(ypair\ni,j =1)>0.5}\n(i2−i1+1)×(j2−j1+1) × 100% > 50%\nAttention Loss Attention loss is a loss term specifically designed for the task. It aims to increase the effectiveness of review attention and rebuttal attention discussed above. Even without\n2We provide the detailed steps of deriving the loss Lseq in Appendix A.1.\n3MLP is chosen because more complex structures like convolutional neural networks (CNN) demonstrate no superiority. The comparison results are attached in Appendix B.3.\n4We provide the detailed steps of deriving the pairing loss Lpair in Appendix A.2.\nthis auxiliary loss term, sentences in review are supposed to attend to relevant sentences in rebuttal and vice versa. The auxiliary loss is thus aimed at augmenting the effect of mutual reference explicitly by guiding the paired arguments to refer to each other. Intuitively, under the settings of argument mining and pairing, it is natural that review arguments refer to the paired rebuttal arguments to update their embedding and vice versa during mutual attention. Hence, we introduce an auxiliary loss term to increase the attention weights computed for paired arguments and decrease the attention weights otherwise for both review and rebuttal attentions in all layers. For each instance, Lattn is defined as:\nLattn = ∑ i,j (1− 2ypairi,j ) · ( n∑ k=1 γn−k · (a(k)i,j + b (k) i,j ) ) ,\nwhere γ is the decaying parameter used to compute exponential moving average for the sum of attention. Larger weights are assigned to layers closer to the final predictor as they are more related to the prediction in the end. The attention loss is defined in the form of summation across all layers to increase the accuracy and interpretability of both review and rebuttal attentions in all layers. If the tendency to attend to the paired argument is augmented, the benefits of attention mechanism can be further exploited (e.g., learning better sentence representations, increasing pair prediction accuracy).\nThe overall loss L is then defined by summing up three losses together:\nL = Lseq + λ1 · Lpair + λ2 · Lattn,\nwhere λ1 and λ2 are tuned hyperparameters."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Data",
      "text" : "We conduct experiments on the benchmark dataset, i.e., RR dataset (Cheng et al., 2020) to evaluate the effectiveness of our proposed model. RR dataset includes 4,764 pairs of peer reviews and author rebuttals collected from ICLR 2013 to ICLR 2020. There are two dataset versions provided: RR-Submission-v1 and RR-Passage-v1. In RRSubmission-v1, multiple review-rebuttal passage pairs of the same paper submission are in the same set of train, dev or test; while in RR-Passage-v1, different review-rebuttal passage pairs of the same submission could be put into different sets. We further modify the RR-Submission-v1 dataset by fixing some minor bugs in the labels, and name it RR-Submission-v2. The data are split into train,\ndev and test sets by a ratio of 8:1:1 for all three dataset versions."
    }, {
      "heading" : "5.2 Baselines",
      "text" : "We compare our model with two baselines:\n• The pipeline approach is used as a baseline model in the previous work (Cheng et al., 2020). It independently trains two subtasks and then pipes them together to extract argument pairs.\n• The multi-task learning model proposed by (Cheng et al., 2020) trains two subtasks simultaneously via the shared feature encoders."
    }, {
      "heading" : "5.3 Experimental Settings",
      "text" : "We implement our attention-guided MLMC encoding based model in Pytorch. The dimension of pre-trained BERT sentence embeddings is 768 by default. Maximum number of BERT tokens for each sentence is set as 200. MLP layer is composed of 3 linear functions and 2 ReLU functions. We use Adam (Kingma and Ba, 2014) with an initial learning rate of 0.0002, and update parameters with a batch size of 1 and dropout rate of 0.5. We train our model for 25 epochs at most. We select the best model parameters based on the best overall F1 score on the development set and apply it to the test set for evaluation. All models are run with V100 GPU. Note that in this paper, the parameters are mainly tuned based on RR-Submission-v1 5. Following the previous work (Cheng et al., 2020), we report the precision (Prec.), recall (Rec.) and F1 scores for the performance on both subtasks as well as the overall extraction performance."
    }, {
      "heading" : "5.4 Main Results on RR Dataset",
      "text" : "Table 1 shows the performance comparison between our proposed models and the pervious work on RR-Submission-v1 and RR-Passage-v1 datasets 6. Besides the two baseline models mentioned before, we implement a bi-cross encoding scheme (Bi-Cross) for comparisons as well. The key difference between the bi-cross encoder and the multi-cross encoder is that in the bi-cross encoder,\n5More details about hyperparameter settings (e.g. weight for pair loss λ1, weight for attention loss λ2, decaying parameter γ of exponential moving average) and experimental results (e.g. running time, number of parameters, performance on the development set) could be found in Appendix B.\n6The previous work adopts negative sampling technique for sentence pairing subtask and evaluates the performance on the partial test set. In this work, we re-evaluate the previous work’s sentence pairing subtask on the whole test dataset for a fair comparison. Those results are marked with * in Table 1.\nthe review sentences and rebuttal sentences are concatenated as one sequence, and thus it only has one sequence encoder. In contrast, there are two individual sequence encoders in our multi-cross encoder. With the same number of layers, our multi-cross model outperforms the bi-cross model on both datasets except for RR-Passage-v1 with 4 layers. This is especially conspicuous when the number of layers is 3. The superiority of the multicross model demonstrates the importance and robustness of learning review and rebuttal sequences separately. Our model achieves the highest F1 score when the number of layers increases to 3. Adding more layers hurts the performance, probably because the model overfits with too many layers. Table 2 shows the performance on RR-Submissionv2 7. The main conclusion is consistent with the performace on RR-Submission-v1. Both the bicross and multi-cross models outperform the multitask model, and the multi-cross models further outperform the bi-cross models. Although the baselines achieve slightly better performance on the argument mining subtask than both the bi-cross model and the multi-cross model, they still perform worse than our models on the sentence pairing subtask and the overall APE task. This is plausibly because of two main reasons. First, in the multi-task model, the subtask coordi-\n7We encourage the researchers to use RR-submission-v2 and compare to its performance in the future.\nnation capability is weak as the shared information between two subtasks is learned implicitly. However, in our model, the three encoding components are explicitly mingled with each other through the mutual attention mechanism and the table encoder. On one hand, the better sentence pairing subtask performance demonstrates the effectiveness of the table-filling approach. On the other hand, the better overall APE performance demonstrates the strong subtask coordination capability of our model architecture. Second, we further analyze the breakdown performance of the multi-task model and our multicross (n=3) model on the argument mining subtask. Figure 3 shows the subtask performance on RRSubmission-v1 dataset for reviews, rebuttals, and both of them. We can observe that the difference of F1 scores between reviews and rebuttals of our model is smaller than the multi-task model. Despite the slight decrease in the overall argument mining performance, a more balanced argument extraction performance on reviews and rebuttals brings in better overall APE performance, which is because more accurate review argument extraction increases the chance for the extracted rebuttal arguments to be paired correctly."
    }, {
      "heading" : "5.5 Ablation study",
      "text" : "We conduct an ablation study of the multi-cross (n=3) model on RR-Submission-v1 dataset from three perspectives, as presented in Table 3. Firstly, we evaluate the effect of sharing the biLSTM layer (the light yellow modules in Figure 2) and the CRF layer. We can notice that the F1 drops 1.92 without sharing the biLSTM layer, drops 1.75 without sharing the CRF layer, and drops 1.02 when sharing neither. It is interesting to notice that when two sequences use their own biLSTMs and CRF simultaneously (i.e., w/o sharing both), the F1 drops less compared to the models without sharing only one of them. This suggests that having an individual set of biLSTM and CRF layers for each type of sequence is plausibly a worthwhile setting, but it\nis not as effective as sharing both. One possible reason is that the advantage brought in by such a tailor-made sequential tagging configuration for each type is overwhelmed by the disadvantage of fewer training instances. Secondly, without cross updates between the review and rebuttal embeddings (the mutual attention modules still exist), the F1 drops 1.78. This result again demonstrates the effectiveness of explicitly blending two sequence embeddings via the mutual attention mechanism specifically designed for this task. Thirdly, we also investigate the effect of attention loss term by removing it from the overall loss. The performance drops about 2.87 F1 points. We will elaborate more with the attention visualization below."
    }, {
      "heading" : "5.6 Attention Visualization",
      "text" : "To examine the effectiveness of the auxiliary attention loss, we visualize the sum of attention weights of all layers for four test samples, as shown in Figure 4. The sum is computed for visualization because attention weights in all layers are guided by the attention loss. The distribution of attention is significantly improved as the colors for arguments in Column (c) are considerably darker. In Column (b) without the guidance of attention loss, despite some patterns, attention weights are distributed in a quite haphazard manner. Therefore, the interpretability of our model is much better as we can easily understand which part of the discourse each sentence refers to. Specifically, the boundary of most attention blocks in Column (c) matches well with the start and end positions of the ground truth review and rebuttal arguments. The gold and predicted argument spans and argument pairs of these four samples are shown in Appendix C.1, and more discussions are given regarding the reason for some mistakenly predicted boundaries. The effectiveness of the auxiliary attention loss is also quantitatively illustrated by a higher F1 score after its incorporation (32.44 v.s. 29.57) in Table 3."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this paper, we adopt the table-filling approach for modeling the sentence-level correlation between two passages, and propose the attention-guided multi-layer multi-cross (MLMC) encoding scheme for the argument pair extraction (APE) task. Our model can better capture the internal relations be-\ntween a review and its rebuttal with two sequence encoders and a table encoder via mutual attention mechanism. We also introduce an auxiliary attention loss to further improve the efficacy of the mutual attentions. Extensive experiments on the benchmark dataset demonstrate the effectiveness of our model architecture, which is potentially beneficial for other NLP tasks."
    }, {
      "heading" : "A Details of Argument Pair Predictor",
      "text" : "A.1 Argument Predictor We cast the task of predicting argument spans as a sequence labeling problem. We adopt conditional random field (CRF) (Lafferty et al., 2001) that assigns each label sequence a score. The probability for each sequence (both review and reply) is defined as following:\np(y|s) = exp(score(s,y))∑ y exp(score(s,y)) ,\nwhere s represents the original sequence and y is the gold sequence label encoding argument spans under the BIO scheme (Ramshaw, 1995; Ratinov and Roth, 2009). The score function is defined as:\nscore(s,y) = n∑ i=0 Ayi,yi+1 + n∑ i=1 Fθ1(s, yi).\nA is the matrix with trainable parameters representing transition scores within the CRF layer and Fθ1 represents the emission scores obtained after feeding review sequence and rebuttal sequence into the multi-cross encoder with parameters θ1. The negative log-likelihood loss for both review and reply sequence in each instance is then defined as:\nLseq(A, θ1) = − ( log p(yrv|srv) + log p(yrb|srb) ) .\nA.2 Pair Predictor Given table features T(k) and an MLP layer, the probability that two sentences are from an argument pair can be expressed as following:\np(ypair = 1|(srv, srb)) = 11+exp (−Fθ2 (T(k))) .\nFθ2 is a composite function of Linear and ReLU functions, the final linear function among which has an output dimension of 1. The pairing loss Lpair(θ2, θ1) for each instance is then defined as: Lpair(θ2, θ1) =\n− ∑\ni,j ( ypairi,j log p(y pair i,j = 1|srv, srb)\n+ (1− ypairi,j ) log(1− p(y pair i,j = 1|srv, srb))\n) ,\nwhere θ2 are parameters within the MLP layer. Note that the attention loss is a function of θ1 and the overall loss is a function of θ1, A and θ2. The formulae provided in the main paper omit the related parameters for brevity."
    }, {
      "heading" : "B More Experimental Details",
      "text" : "B.1 Hyperparameters We manually tune the hyperparameter values (e.g. weight for pair loss λ1, weight for attention loss\nλ2, decaying parameter γ of exponential moving average) for our proposed multi-layer multi-cross model. We manually tune the weight for pair loss λ1 from 0.3 to 0.7 with step size of 0.1 (Table 4), the weight for attention loss λ2 from 0.5 to 2.5 with step size of 0.5 (Table 5) and the decaying paramter γ of exponential moving average from 0.7 to 1 with step size of 0.1 (Table 6) for our proposed multi-layer multi-cross model. We select the best hyperparameters based on the best F1 score achieved on the development set and apply them to the test set for evaluation. Specifically, λ1 is set as 0.5, λ2 is set as 2, γ is set as 0.9.\nB.2 Running Time, Number of Parameters and Results on Development Set\nTable 8 shows the running time, the number of parameters, and the results on the development set of our models on RR-Submission-v1 dataset. For the bi-cross models, as review sentences and rebuttal sentences are concatenated as one sequence in one sequence encoder during training, the sequences are generally longer. Thus, the bi-cross models require a longer running time. As the number of layers increases, the performance on the development set improves yet the performance on the test set becomes worse. It is plausibly because that the model might face the overfitting issue.\nB.3 MLP v.s. CNN\nWe replace the MLP module with convolutional neural networks (CNN) to predict the pairs and compare their performance on RR-Submission-v1 dataset. The comparison results are presented in Table 7. The theoretical advantage of CNN over MLP is that CNN is able to capture surrounding information with the help of kernels. However, the experiment results show that the convolutional structure performs worse than the simple MLP structure. By examining the kernel weight of the convolution layers, we observe no significant magnitude difference between the center weights and the peripheral weights. Take a 3x3 kernel as an example, the center weights are the weights at the center grid, while the weights located in the rest of the 8 grids are peripheral weights. This indicates that CNN accords way more importance to the surrounding information (8 times more important in the case of a 3x3 kernel) than to the original grid. The overemphasis on surrounding information brings too much noise to the pair prediction."
    }, {
      "heading" : "C More Experimental Analysis",
      "text" : "C.1 Case Study on Attention Weights Each row in Table 9 in which the exact gold and predicted results are shown corresponds to the re-\nspective row in Figure 4 in the main paper. We can see that instance (1) and instance (2) are perfectly predicted whereas one predicted reply argument is shorter than the gold argument in instance (3) and some argument pairs are identified wrongly in instance (4).\nAttention distribution turns out to be strongly connected with the final output of the model, as attention weights exhibit exactly the same error as the wrongly predicted argument spans and argument pairs. In instance (3), we can see from the attention visualization that the review argument at position 15 only refers to the reply sentences from position 14 to 16. The wrong prediction of reply span (14, 16) (gold: (14, 26)) directly results from the inaccurate distribution of attention weights. For instance in Figure 4 row (4) as highlighted in red, it can also be noticed that some review arguments\nattend to the wrong rebuttal argument and some rebuttal arguments attend to the wrong review argument. The attention blocks in Figure 4 row (4) are (8, 9) - (2, 7), (10, 12) - (8, 9), (13, 13) - (10, 10) and the wrongly predicted argument pairs are also (8, 9) - (2, 7), (10, 12) - (8, 9), (13, 13) - (10, 10). Together with all 4 test instances, the conclusion can be reached that one-to-one correspondence can be found in the predicted paired arguments and the distribution of attention weights. Therefore, the hindrance to further improve the model performance comes from the inaccurately allocated attention weights.\nC.2 Breakdown by Argument Density We further evaluate the multi-cross (n=3) model performance on RR-Submission-v1 among differ-\nent numbers of argument pairs in each instance. Figure 5 shows the argument mining performance on review and rebuttal separately and the overall APE performance. Their F1 scores all increase as the number of argument pairs grows from 1 to 4 and reach plateaus afterwards. The reason is likely to be that most of the review and rebuttal pairs with about 4 argument pairs are written in a more formatted manner and are hence easier to be extracted. When the number of argument pairs is smaller than 3, it is highly likely that authors only reply to one or two review arguments. The irregular format might increase the difficulty of pair extraction. When the number of argument pairs is larger than average, the F1 score of APE decreases slightly as the structure becomes more complicated.\nIn addition, we can see from Figure 5 that when the number of argument pairs is from 2 to 6, the F1 scores of the argument mining subtask between review and rebuttal are very close. Compare to the multi-task model in the previous work (Cheng et al., 2020), our model’s performance on the argument mining subtask between review and rebuttal is more balanced, which leads to the better overall APE performance."
    } ],
    "references" : [ {
      "title" : "Internet argument corpus 2.0: An sql schema for dialogic social media and the corpora to go with it",
      "author" : [ "Rob Abbott", "Brian Ecker", "Pranav Anand", "Marilyn Walker" ],
      "venue" : "Proceedings of LREC",
      "citeRegEx" : "Abbott et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Abbott et al\\.",
      "year" : 2016
    }, {
      "title" : "Identifying prominent arguments in online debates using semantic textual similarity",
      "author" : [ "Filip Boltužić", "Jan Šnajder." ],
      "venue" : "Proceedings of the 2nd Workshop on Argumentation Mining.",
      "citeRegEx" : "Boltužić and Šnajder.,? 2015",
      "shortCiteRegEx" : "Boltužić and Šnajder.",
      "year" : 2015
    }, {
      "title" : "Five years of argument mining: a data-driven analysis",
      "author" : [ "Elena Cabrio", "Serena Villata." ],
      "venue" : "IJCAI.",
      "citeRegEx" : "Cabrio and Villata.,? 2018",
      "shortCiteRegEx" : "Cabrio and Villata.",
      "year" : 2018
    }, {
      "title" : "AMPERSAND: Argument mining for PERSuAsive oNline discussions",
      "author" : [ "Tuhin Chakrabarty", "Christopher Hidey", "Smaranda Muresan", "Kathy McKeown", "Alyssa Hwang." ],
      "venue" : "Proceedings of EMNLP-IJCNLP.",
      "citeRegEx" : "Chakrabarty et al\\.,? 2019",
      "shortCiteRegEx" : "Chakrabarty et al\\.",
      "year" : 2019
    }, {
      "title" : "Argument pair extraction from peer review and rebuttal via multi-task learning",
      "author" : [ "Liying Cheng", "Lidong Bing", "Qian Yu", "Wei Lu", "Luo Si." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Cheng et al\\.,? 2020",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of NAACL.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural end-to-end learning for computational argumentation mining",
      "author" : [ "Steffen Eger", "Johannes Daxenberger", "Iryna Gurevych." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Eger et al\\.,? 2017",
      "shortCiteRegEx" : "Eger et al\\.",
      "year" : 2017
    }, {
      "title" : "A weakly-supervised approach to argumentative zoning of scientific documents",
      "author" : [ "Yufan Guo", "Anna Korhonen", "Thierry Poibeau." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Guo et al\\.,? 2011",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2011
    }, {
      "title" : "Table filling multi-task recurrent neural network for joint entity and relation extraction",
      "author" : [ "Pankaj Gupta", "Hinrich Schütze", "Bernt Andrassy." ],
      "venue" : "Proceedings of COLING.",
      "citeRegEx" : "Gupta et al\\.,? 2016",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Argument mining for understanding peer reviews",
      "author" : [ "Xinyu Hua", "Mitko Nikolov", "Nikhil Badugu", "Lu Wang." ],
      "venue" : "Proceedings of NAACL.",
      "citeRegEx" : "Hua et al\\.,? 2019",
      "shortCiteRegEx" : "Hua et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "Technical report.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John Lafferty", "Andrew McCallum", "Fernando CN Pereira." ],
      "venue" : "Proceedings of ICML.",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Modeling joint entity and relation extraction with table representation",
      "author" : [ "Makoto Miwa", "Yutaka Sasaki." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Miwa and Sasaki.,? 2014",
      "shortCiteRegEx" : "Miwa and Sasaki.",
      "year" : 2014
    }, {
      "title" : "Argumentation mining: the detection, classification and structure of arguments in text",
      "author" : [ "Raquel Mochales Palau", "Marie-Francine Moens." ],
      "venue" : "Proceedings of the 12th international conference on artificial intelligence and law.",
      "citeRegEx" : "Palau and Moens.,? 2009",
      "shortCiteRegEx" : "Palau and Moens.",
      "year" : 2009
    }, {
      "title" : "Modeling argument strength in student essays",
      "author" : [ "Isaac Persing", "Vincent Ng." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Persing and Ng.,? 2015",
      "shortCiteRegEx" : "Persing and Ng.",
      "year" : 2015
    }, {
      "title" : "End-to-end argumentation mining in student essays",
      "author" : [ "Isaac Persing", "Vincent Ng." ],
      "venue" : "Proceedings of NAACL.",
      "citeRegEx" : "Persing and Ng.,? 2016",
      "shortCiteRegEx" : "Persing and Ng.",
      "year" : 2016
    }, {
      "title" : "Text chunking using transformation-based learning",
      "author" : [ "LA Ramshaw." ],
      "venue" : "Proceedings of Third Workshop on Very Large Corpora.",
      "citeRegEx" : "Ramshaw.,? 1995",
      "shortCiteRegEx" : "Ramshaw.",
      "year" : 1995
    }, {
      "title" : "Design challenges and misconceptions in named entity recognition",
      "author" : [ "Lev Ratinov", "Dan Roth." ],
      "venue" : "Proceedings of CoNLL.",
      "citeRegEx" : "Ratinov and Roth.,? 2009",
      "shortCiteRegEx" : "Ratinov and Roth.",
      "year" : 2009
    }, {
      "title" : "Evaluating argumentative and narrative essays using graphs",
      "author" : [ "Swapna Somasundaran", "Brian Riordan", "Binod Gyawali", "Su-Youn Yoon." ],
      "venue" : "Proceedings of COLING.",
      "citeRegEx" : "Somasundaran et al\\.,? 2016",
      "shortCiteRegEx" : "Somasundaran et al\\.",
      "year" : 2016
    }, {
      "title" : "Identifying argumentative discourse structures in persuasive essays",
      "author" : [ "Christian Stab", "Iryna Gurevych." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Stab and Gurevych.,? 2014",
      "shortCiteRegEx" : "Stab and Gurevych.",
      "year" : 2014
    }, {
      "title" : "Parsing argumentation structures in persuasive essays",
      "author" : [ "Christian Stab", "Iryna Gurevych." ],
      "venue" : "Computational Linguistics.",
      "citeRegEx" : "Stab and Gurevych.,? 2017",
      "shortCiteRegEx" : "Stab and Gurevych.",
      "year" : 2017
    }, {
      "title" : "Argument mining: Extracting arguments from online dialogue",
      "author" : [ "Reid Swanson", "Brian Ecker", "Marilyn Walker." ],
      "venue" : "Proceedings of SIGDIAL.",
      "citeRegEx" : "Swanson et al\\.,? 2015",
      "shortCiteRegEx" : "Swanson et al\\.",
      "year" : 2015
    }, {
      "title" : "Towards domain-independent argumentative zoning: Evidence from chemistry and computational linguistics",
      "author" : [ "Simone Teufel", "Advaith Siddharthan", "Colin Batchelor." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Teufel et al\\.,? 2009",
      "shortCiteRegEx" : "Teufel et al\\.",
      "year" : 2009
    }, {
      "title" : "Two are better than one: Joint entity and relation extraction with tablesequence encoders",
      "author" : [ "Jue Wang", "Wei Lu." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Wang and Lu.,? 2020",
      "shortCiteRegEx" : "Wang and Lu.",
      "year" : 2020
    }, {
      "title" : "Grid tagging scheme for aspect-oriented fine-grained opinion extraction",
      "author" : [ "Zhen Wu", "Chengcan Ying", "Fei Zhao", "Zhifang Fan", "Xinyu Dai", "Rui Xia." ],
      "venue" : "Findings of EMNLP.",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Using context to predict the purpose of argumentative writing revisions",
      "author" : [ "Fan Zhang", "Diane Litman." ],
      "venue" : "Proceedings of NAACL.",
      "citeRegEx" : "Zhang and Litman.,? 2016",
      "shortCiteRegEx" : "Zhang and Litman.",
      "year" : 2016
    }, {
      "title" : "End-to-end neural relation extraction with global optimization",
      "author" : [ "Meishan Zhang", "Yue Zhang", "Guohong Fu." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Zhang et al\\.,? 2017",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Mining argumentation structures within a corpus is a crucial task in argument mining research field (Palau and Moens, 2009).",
      "startOffset" : 100,
      "endOffset" : 123
    }, {
      "referenceID" : 2,
      "context" : "It has been widely studied by natural language processing (NLP) researchers (Cabrio and Villata, 2018) and applied to domains such as: web debating platforms (Boltužić and Šnajder, 2015; Swanson et al.",
      "startOffset" : 76,
      "endOffset" : 102
    }, {
      "referenceID" : 1,
      "context" : "It has been widely studied by natural language processing (NLP) researchers (Cabrio and Villata, 2018) and applied to domains such as: web debating platforms (Boltužić and Šnajder, 2015; Swanson et al., 2015; Chakrabarty et al., 2019),",
      "startOffset" : 158,
      "endOffset" : 234
    }, {
      "referenceID" : 22,
      "context" : "It has been widely studied by natural language processing (NLP) researchers (Cabrio and Villata, 2018) and applied to domains such as: web debating platforms (Boltužić and Šnajder, 2015; Swanson et al., 2015; Chakrabarty et al., 2019),",
      "startOffset" : 158,
      "endOffset" : 234
    }, {
      "referenceID" : 3,
      "context" : "It has been widely studied by natural language processing (NLP) researchers (Cabrio and Villata, 2018) and applied to domains such as: web debating platforms (Boltužić and Šnajder, 2015; Swanson et al., 2015; Chakrabarty et al., 2019),",
      "startOffset" : 158,
      "endOffset" : 234
    }, {
      "referenceID" : 20,
      "context" : "persuasive essays (Stab and Gurevych, 2014; Persing and Ng, 2016), social media (Abbott et al.",
      "startOffset" : 18,
      "endOffset" : 65
    }, {
      "referenceID" : 16,
      "context" : "persuasive essays (Stab and Gurevych, 2014; Persing and Ng, 2016), social media (Abbott et al.",
      "startOffset" : 18,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "persuasive essays (Stab and Gurevych, 2014; Persing and Ng, 2016), social media (Abbott et al., 2016), etc.",
      "startOffset" : 80,
      "endOffset" : 101
    }, {
      "referenceID" : 4,
      "context" : "To address this limitation, the pioneering work (Cheng et al., 2020) employs a multi-task learning framework to train two subtasks simultaneously.",
      "startOffset" : 48,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : "Inspired by the table-filling approach (Miwa and Sasaki, 2014), we form a table that represents features for the Cartesian Product of review and rebuttal sequences by utilizing both of their embeddings, as shown in the right portion of Figure 1.",
      "startOffset" : 39,
      "endOffset" : 62
    }, {
      "referenceID" : 4,
      "context" : "Thirdly, we evaluate our model on the benchmark dataset (Cheng et al., 2020), and the results show that our model achieves a new state-of-the-art performance on the APE task.",
      "startOffset" : 56,
      "endOffset" : 76
    }, {
      "referenceID" : 21,
      "context" : "Argument mining has wide applications in educational domain, including persuasive essays (Stab and Gurevych, 2017; Eger et al., 2017), scientific articles (Teufel et al.",
      "startOffset" : 89,
      "endOffset" : 133
    }, {
      "referenceID" : 6,
      "context" : "Argument mining has wide applications in educational domain, including persuasive essays (Stab and Gurevych, 2017; Eger et al., 2017), scientific articles (Teufel et al.",
      "startOffset" : 89,
      "endOffset" : 133
    }, {
      "referenceID" : 23,
      "context" : ", 2017), scientific articles (Teufel et al., 2009; Guo et al., 2011), writing assistance (Zhang and Litman, 2016), essay scoring (Persing and Ng, 2015; Somasundaran et al.",
      "startOffset" : 29,
      "endOffset" : 68
    }, {
      "referenceID" : 7,
      "context" : ", 2017), scientific articles (Teufel et al., 2009; Guo et al., 2011), writing assistance (Zhang and Litman, 2016), essay scoring (Persing and Ng, 2015; Somasundaran et al.",
      "startOffset" : 29,
      "endOffset" : 68
    }, {
      "referenceID" : 26,
      "context" : ", 2011), writing assistance (Zhang and Litman, 2016), essay scoring (Persing and Ng, 2015; Somasundaran et al.",
      "startOffset" : 28,
      "endOffset" : 52
    }, {
      "referenceID" : 15,
      "context" : ", 2011), writing assistance (Zhang and Litman, 2016), essay scoring (Persing and Ng, 2015; Somasundaran et al., 2016), peer reviews (Hua et al.",
      "startOffset" : 68,
      "endOffset" : 117
    }, {
      "referenceID" : 19,
      "context" : ", 2011), writing assistance (Zhang and Litman, 2016), essay scoring (Persing and Ng, 2015; Somasundaran et al., 2016), peer reviews (Hua et al.",
      "startOffset" : 68,
      "endOffset" : 117
    }, {
      "referenceID" : 13,
      "context" : "Table-filling approaches (Miwa and Sasaki, 2014; Gupta et al., 2016; Zhang et al., 2017) have been proposed to work towards the joint task of name entity recognition (NER) and relation extraction (RE).",
      "startOffset" : 25,
      "endOffset" : 88
    }, {
      "referenceID" : 8,
      "context" : "Table-filling approaches (Miwa and Sasaki, 2014; Gupta et al., 2016; Zhang et al., 2017) have been proposed to work towards the joint task of name entity recognition (NER) and relation extraction (RE).",
      "startOffset" : 25,
      "endOffset" : 88
    }, {
      "referenceID" : 27,
      "context" : "Table-filling approaches (Miwa and Sasaki, 2014; Gupta et al., 2016; Zhang et al., 2017) have been proposed to work towards the joint task of name entity recognition (NER) and relation extraction (RE).",
      "startOffset" : 25,
      "endOffset" : 88
    }, {
      "referenceID" : 4,
      "context" : "First, for the argument mining subtask, we cast it as a sentence-level sequence labeling problem following the work (Cheng et al., 2020) using the standard BIO scheme (Ramshaw, 1995; Ratinov and Roth, 2009).",
      "startOffset" : 116,
      "endOffset" : 136
    }, {
      "referenceID" : 17,
      "context" : ", 2020) using the standard BIO scheme (Ramshaw, 1995; Ratinov and Roth, 2009).",
      "startOffset" : 38,
      "endOffset" : 77
    }, {
      "referenceID" : 18,
      "context" : ", 2020) using the standard BIO scheme (Ramshaw, 1995; Ratinov and Roth, 2009).",
      "startOffset" : 38,
      "endOffset" : 77
    }, {
      "referenceID" : 13,
      "context" : "Here, we formulate it as a table-filling problem following the work (Miwa and Sasaki, 2014).",
      "startOffset" : 68,
      "endOffset" : 91
    }, {
      "referenceID" : 12,
      "context" : "Finally, the model predicts the review and rebuttal arguments through a conditional random field (CRF) (Lafferty et al., 2001) layer based on two sequence representations, and extracts the pairing information through a multi-layer perceptron (MLP) based on the table representations.",
      "startOffset" : 103,
      "endOffset" : 126
    }, {
      "referenceID" : 5,
      "context" : "We obtain the pre-trained BERT (Devlin et al., 2019) token embeddings [x0, x1, · · · , xl−1] for all word tokens in the sentence, after which all token embeddings are fed into a bidirectional long short-term memory (biLSTM)",
      "startOffset" : 31,
      "endOffset" : 52
    }, {
      "referenceID" : 4,
      "context" : "The same conclusion is drawn by the experimental results in the previous work (Cheng et al., 2020).",
      "startOffset" : 78,
      "endOffset" : 98
    }, {
      "referenceID" : 24,
      "context" : "6345 The 2D-GRU settings are similar to the previous work (Wang and Lu, 2020) except that the table to be processed is not necessarily a square (I 6= J in general).",
      "startOffset" : 58,
      "endOffset" : 77
    }, {
      "referenceID" : 4,
      "context" : "Following (Cheng et al., 2020), during evaluation, a pair of candidate spans ([srv,i1 , · · · , srv,i2 ] and [srb,j1 , · · · , srb,j2 ]) form a pair if they satisfy the following criterion: ∑i2 i=i1 ∑j2 j=j1 1 {p(y i,j =1)>0.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 4,
      "context" : ", RR dataset (Cheng et al., 2020) to evaluate the effectiveness of our proposed model.",
      "startOffset" : 13,
      "endOffset" : 33
    }, {
      "referenceID" : 4,
      "context" : "We compare our model with two baselines: • The pipeline approach is used as a baseline model in the previous work (Cheng et al., 2020).",
      "startOffset" : 114,
      "endOffset" : 134
    }, {
      "referenceID" : 4,
      "context" : "• The multi-task learning model proposed by (Cheng et al., 2020) trains two subtasks simultaneously via the shared feature encoders.",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 11,
      "context" : "We use Adam (Kingma and Ba, 2014) with an initial learning rate of 0.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 4,
      "context" : "Following the previous work (Cheng et al., 2020), we report the precision (Prec.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 4,
      "context" : "R R -S ub m is si on -v 1 Pipeline (Cheng et al., 2020) 67.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 4,
      "context" : "R R -S ub m is si on -v 2 Multi-task (Cheng et al., 2020) 70.",
      "startOffset" : 37,
      "endOffset" : 57
    } ],
    "year" : 2021,
    "abstractText" : "Argument pair extraction (APE) is a research task for extracting arguments from two passages and identifying potential argument pairs. Prior research work treats this task as a sequence labeling problem and a binary classification problem on two passages that are directly concatenated together, which has a limitation of not fully utilizing the unique characteristics and inherent relations of two different passages. This paper proposes a novel attention-guided multi-layer multi-cross encoding scheme to address the challenges. The new model processes two passages with two individual sequence encoders and updates their representations using each other’s representations through attention. In addition, the pair prediction part is formulated as a tablefilling problem by updating the representations of two sequences’ Cartesian product. Furthermore, an auxiliary attention loss is introduced to guide each argument to align to its paired argument. An extensive set of experiments show that the new model significantly improves the APE performance over several alternatives 1.",
    "creator" : "LaTeX with hyperref"
  }
}