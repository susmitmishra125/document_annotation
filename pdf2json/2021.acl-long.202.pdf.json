{
  "name" : "2021.acl-long.202.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning",
    "authors" : [ "Wei Li", "Can Gao", "Guocheng Niu", "Xinyan Xiao", "Hao Liu", "Jiachen Liu", "Hua Wu", "Haifeng Wang" ],
    "emails" : [ "hua@baidu.com", "wanghaifeng@baidu.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2592–2607\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2592"
    }, {
      "heading" : "1 Introduction",
      "text" : "Large-scale pre-training has drawn much attention in both the community of Compute Vision (CV) and Natural Language Processing (NLP) due to its strong capability of generalization and efficient usage of large-scale data. Firstly in CV, a series of models were designed and pre-trained on the large-scale dataset ImageNet, such as AlexNet (Krizhevsky et al., 2017), VGG (Simonyan and\n∗These authors contribute equally to this study and are listed with random order.\nZisserman, 2014) and ResNet (He et al., 2016), which effectively improved the capability of image recognition for numerous tasks. Recent years have witnessed the burst of pre-training in NLP, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019) and UniLM (Dong et al., 2019), which greatly improve the capabilities of language understanding and generation. However, the above researches focus on the singlemodal learning and can only be effectively used in single-modal (i.e., only text or image) scenarios. In order to adapt to multi-modal scenarios, a series of multi-modal pre-training methods were proposed and pre-trained on the corpus of image-text pairs, such as ViLBERT (Lu et al., 2019), VisualBERT (Li et al., 2019b) and UNITER (Chen et al., 2020b), which greatly improve the ability to process multimodal information. However, these models can only utilize the limited corpus of image-text pairs and cannot be effectively adapted to single-modal scenarios (Lin et al., 2020b).\nA smarter AI system should be able to process different modalities of information effectively. There are large scale of data in different modalities on the Web, mainly textual and visual information. The textual knowledge and the visual knowledge\n[IMG] [ROI1] [ROI2] [ROI3] [ROI4] [ROI5] [ROI6] [ROIN]\nImage Collections Text Corpus\nAny baseball game i nvo l ves one o r more umpires… At a m i n i m u m , o n e umpire will stand behind the catcher…\nSemantic Space\nImage representation\nText representation\n[CLS] [tok1] [tok2] [tok3] [tok4] [tok5] [tokN] [SEP]\nUnified-Modal Transformer\nImage-Text Pairs\nThe baseball player readies to swing at the pitch while the umpire behind him looks on.\n… …\n[SEP][IMG] [CLS] The on… …\n… …\n…\nFigure 2: Illustration of the unified-modal pre-training architecture. Both image collections, text corpus and image-text pairs can be effectively utilized for representation learning.\nusually can enhance and complement each other. As the example shown in Figure 1, it’s difficult to answer the question correctly only with the visual information in the image. However, if we connect the visual information to the textual information which describes the background of a baseball game, it’s very easy to determine the correct answer. Also, the visual information can make it easier to understand the scene described by the text. The research in neuroscience by Van Ackeren et al. (2018) reveals that the parts of the human brain responsible for vision can learn to process other kinds of information, including touch and sound. Inspired by this research, we propose to design a unified-modal architecture UNIMO which aims to process multiscene and multi-modal data input with one model, including textual, visual and vision-and-language data, as shown in Figure 2.\nThe greatest challenge to unify different modalities is to align and unify them into the same semantic space which are generalizable to different modalities of data. Existed cross-modal pretraining methods try to learn cross-modal representations based on only limited image-text pairs by simple image-text matching and masked language modeling (Chen et al., 2020b). They can only learn specific representations for image-text pairs, and thus fail to generalize to single-modal scenarios. So their performance will drop dramatically when applied to language tasks (Lin et al., 2020b), which has also been revealed by our experiments (see\nSection 4.2). In this work, UNIMO learns visual representations and textual representations simultaneously, and unifies them into the same semantic space via cross-modal contrastive learning (CMCL) based on a large-scale corpus of image collections, text corpus and image-text pairs.\nUNIMO effectively utilizes the large-scale of text corpus and image collections to learn general textual and visual representations. The CMCL aligns the visual representations and textual representations, and unifies them into the same semantic space based on image-text pairs. As shown in Figure 3, to facilitate different levels of semantic alignment between vision and language, we propose to utilize a series of text rewriting techniques to improve the diversity of cross-modal information. Specifically, for an image-text pair, various positive examples and hard negative examples can be obtained by rewriting the original caption at different levels. Moreover, to incorporate more background information from the single-modal data, text and image retrieval are also applied to augment each image-text pair with various related texts and images. The positive pairs, negative pairs, related images and texts are learned jointly by CMCL. In this way, our model can effectively unify different levels of visual and textual representations into the same semantic space, and incorporate more singlemodal knowledge to enhance each other.\nThe unified-modal architecture mainly has the following advantages compared with previous methods:\n• We can utilize large scale of non-paired text corpus and image collections on the Web to learn more generalizable textual and visual representations, and improve the capability of vision and language understanding and generation.\n• Our model can be effectively fine-tuned for both single-modal and multi-modal understanding and generation downstream tasks.\n• The visual knowledge and textual knowledge can enhance each other to achieve better performance on several single-modal and multimodal tasks than previous methods."
    }, {
      "heading" : "2 UNIMO",
      "text" : "Humans perceive the world through many modalities, such as sound, vision and language. Even\nthough any individual modality might be incomplete or noisy, important information is still perceivable since they tend to be shared or enhanced each other. With this motivation, we propose a unifiedmodal pre-training method UNIMO to learn representations that capture modality-invariant information at the semantic level. Different from previous methods, UNIMO learns from different modalities of data, including images, texts and image-text pairs, thus achieving more robust and generalizable representations for both textual and visual input.\nAs shown in Figure 2, UNIMO employs multi-layer self-attention Transformers to learn unified semantic representations for both textual and visual data. For a textual input W, it is firstly split into a sequence of subwords W = {[CLS], w1, ..., wn, [SEP ]} by Byte-Pair Encoding (BPE) (Sennrich et al., 2016), and then the self-attention mechanism is leveraged to learn contextual token representations {h[CLS], hw1 , ..., hwn , h[SEP ]}. The special tokens [CLS] and [SEP ] denote the start and end of the textual sequence, respectively. Similarly, for\nan image V, it is firstly converted to a sequence of region features V = {[IMG], v1, ..., vt} ([IMG] denotes the representation of the entire image), and then the self-attention mechanism is leveraged to learn contextual region representations {h[IMG], hv1 , ..., hvt}. Similar to previous work (Chen et al., 2020b), we use Faster R-CNN (Ren et al., 2016) to detect the salient image regions and extract the visual features (pooled ROI features) for each region. For an image-text pair (V,W ), its visual features and textual tokens are concatenated as a sequence {[IMG], v1, ..., vt, [CLS], w1, ..., wn, [SEP ]}. Then the sequence is feed into the multi-layer Transformer network to learn cross-modal contextual representations for both the textual tokens and image regions. We extract the representations h[IMG] and h[CLS] as the semantic representations of image V and text W , respectively.\nBased on large volumes of image collections {V }, text corpus {W} and image-text pairs {(V,W )}, UNIMO learns generalizable visual and textual representations in similar ways by masked\nprediction, and unify them into the same semantic space via CMCL. Joint visual learning on image collections, language learning on text corpus and cross-modal learning on image-text pairs not only improve the capability of visual and language understanding and generation, but also enable the textual knowledge and visual knowledge to enhance each other in the unified semantic space."
    }, {
      "heading" : "2.1 Cross-Modal Contrastive Learning",
      "text" : "The greatest challenge to unify different modalities is to align and unify their representations at different levels. For the example shown in Figure 2, the model not only needs to connect the scene shown in the whole image to an article describing a baseball game, but also needs to align the two men and their location relationship in the image with “baseball player”, “umpire” and “behind” in the text, respectively. Several existing cross-modal pre-training methods try to align visual and textual representations by simply image-text matching (Li et al., 2019a; Chen et al., 2020b) based on a limited corpus of image-text pairs. They randomly sample a negative image or text from the same training batch for each image-text pair, and utilize a classifier to determine whether the image and text are matching. As the randomly sampled negative text or image is usually very different from the original text or image, they can only learn very coarse alignment between textual and visual representations. In this work, we propose a novel CMCL method to align and unify different levels of textual and visual representations into the same semantic space.\nThe main idea is to let the representations of the paired image and text near in the representation space while the non-paired far away. The representations of image V and text W are used to compute the similarity between them to measure their distance d(V,W ). As shown in Figure 3, to facilitate semantic alignment between vision and language at different levels, we design several novel text rewriting techniques to rewrite the original caption of an image either at word, phrase or sentence level. In this way, we can create large volumes of positive examples X+ and negative examples X− for each image-text pair (V,W ). Moreover, to augment cross-modal learning with single-modal information, text and image retrieval are applied to obtain various related texts X T and images X I for each image-text pair (V,W ). Different from the positive and negative image-text pairs, the retrieved images\nand texts are encoded individually as they mainly carry weak correlations, as shown in the right part of Figure 3. Based on these positive and negative examples, the following contrastive loss LCMCL is utilized to learn detailed semantic alignments across vision and language:\nEV,W [ −log ∑ (V +,W+)∈X{+,I,T} exp(d(V\n+,W+)/τ)∑ (V ′,W ′)∈X{−,+,I,T} exp(d(V ′,W ′)/τ) ] (1)\nwhere τ denotes the temperature parameter. Note that, for single-modal images X I and texts X T , the original text W and image V are used to compute the cross-modal relevance, respectively. To the best of our knowledge, this is the first work that explores CMCL to unify visual and textual semantic space.\nText Rewriting To enhance multi-granularity of semantic alignment between image and text, we rewrite the caption of an image at different levels, including sentence-level, phrase-level and wordlevel. For sentence-level rewriting, we utilize the back-translation techniques (Edunov et al., 2018) to obtain several positive samples for each imagetext pair. Specifically, each caption of an image is translated into another language and then translated back to the original language. In this way, several similar captions can be obtained for an image. Furthermore, for each image-text pair, the most similar captions of other images are retrieved based on TF-IDF similarity. The retrieved results are very similar to the original caption but doesn’t accurately describe the corresponding image, so they can be used as hard negative samples to enhance the sentence-level alignment between image and text. For phrase-level and word-level rewriting, we first parse the image caption into a scene graph (Wang et al., 2018), then randomly replacing the object, attribute or relation nodes of the scene graph with a different object, attribute or relation from the corresponding vocabularies. Instead of randomly sampling negative samples as previous methods, text rewriting can generate large volumes of hard negative samples. In this way, we can help the model to learn more detailed semantic alignment from different levels between image and text.\nImage/Text Retrieval In order to incorporate more single-modal information during cross-modal learning, each image-text pair is further augmented with various related images and texts that retrieved from the single-modal data. Specifically, for an image, other images in the image collections will\nbe ordered by their visual similarities. Those images that have highly overlapped objects with the original image will be extracted to provide relevant visual information. Similarly, sentences that are semantically related with the original caption are extracted based on semantic similarity to provide background language information. The retrieved images and texts are encoded individually by the unified-modal Transformer as shown in Figure 3, then their representations are extracted to compute the cross-modal contrastive loss in Equation 1. These retrieved single-modal information provide rich background information for better cross-modal learning."
    }, {
      "heading" : "2.2 Visual Learning",
      "text" : "Similar to the masked language modeling in BERT, we sample image regions and mask their visual features with a probability of 15%. The visual features of the masked regions are replaced by zeros. As the regions from an image usually are highly overlapped with each other, we choose to mask all regions that have a high proportion of mutual intersection to avoid information leakage. Similar to Lin et al. (2020b), we randomly choose regions as masking anchors and mask the regions whose overlapping ratios with the anchors are larger than 0.3. For an image V , the model is trained to reconstruct the masked regions vm given the remaining regions v\\m:\nLV = EV ∈Dfθ(vm|v\\m) (2)\nSimilarly, for an image-text pair (V,W ), the model is trained to reconstruct the masked regions vm given the text W and the remaining regions v\\m:\nLV = EV,W∈Dfθ(vm|v\\m,W ) (3)\nAs the visual features are high-dimensional and continuous, we utilize both feature regression and region classification objective to learn better visual representations. The feature regression learns to regress the contextualized visual representations hvi to its visual features vi, which can be formulated as: fθ(vm|v\\m) = ∑M i=1 ‖r(hvi) − vi‖2, where r indicates an FC layer to convert hvi into a vector of the same dimension as vi. The region classification learns to recognize the object semantic class of each masked region based on its contextualized visual representation hvi . An FC layer is utilized to compute the scores for K object classes s(hvi), which further goes\nthrough a softmax function to obtain the normalized distribution. The final objective minimizes the cross-entropy (CE) loss between the predicted distribution and the object detection output c(vi) from Faster R-CNN: fθ(vm|v\\m) =∑M\ni=1CE(softmax(s(hvi)), c(vi)). The score function fθ(vm|v\\m,W ) is formulated similarly."
    }, {
      "heading" : "2.3 Language Learning",
      "text" : "To learn general language representations for both language understanding and generation tasks, our model is trained as a unified encoder-decoder model with two types of language modeling tasks: bidirectional prediction and sequence-to-sequence (Seq2Seq) generation. The unified modeling is achieved by utilizing specific self-attention masks to control what context the prediction conditions on, inspired by Dong et al. (2019). To improve the language learning process, we firstly detect semanticly complete phrases from the text, such as name entities by syntactic parsing, and then treat them as a whole in the following masking strategies. Different from previous work, we always sample a sequence of complete words or phrases instead of subword tokens, for both bidirectional prediction and Seq2Seq generation.\nBidirectional prediction. Given a sequence of tokens W = {[CLS], w1, ..., wn, [SEP ]}, we iteratively sampling spans of text until totally 15% tokens have been selected. We sample the span length from a geometric distribution l ∼ Geo(p), where p is set as 0.2, similar to SpanBERT (Joshi et al., 2020). All tokens in the selected spans are replaced with either a special [MASK] token, a random token or the original token with probability 80%, 10% and 10%, respectively. The goal is to predict these masked tokens wm based on their surrounding context w\\m, by minimizing the negative log-likelihood:\nLBidirectional = −EW∈DlogPθ(wm|w\\m) (4)\nSeq2Seq generation. For the Seq2Seq generation task, we iteratively sample fragments from the token sequence until the 25% budget has been spent, inspired by Xiao et al. (2020). For each iterate, we first sample a fragment length from a uniform distribution l ∼ U(4, 32), and then sample a fragment with the specified length. Every selected fragment {wi, ..., wj} is further appended with two special tokens [CLS] and [SEP ] (i.e., {[CLS], wi, ..., wj , [SEP ]}), which denotes the\nbeginning and end of the fragment. All selected fragments are removed from the text and concatenated as the target sequence T while the remaining parts are concatenated as the source sequence S. The model is trained to generate the target sequence auto-regressively condition on the source sequence:\nLSeq2Seq = −E(S,T )∈DlogPθ(T |S) (5)\nwhere Pθ(T |S) = ∏|T | j=1 Pθ(Tj |T<j , S). During pre-training, we alternate between the bidirectional prediction objective and the Seq2Seq generation objective uniformly. For image-text pairs, the two objectives are applied to the captions similarly to learn cross-modal understanding and generation."
    }, {
      "heading" : "3 Experimental Settings",
      "text" : "In this section, we introduce the pre-training and finetuning experimental settings."
    }, {
      "heading" : "3.1 Pre-training Dataset",
      "text" : "Our pre-training datasets consist of three types: text corpus, image collections and image-text pairs. The text corpus includes two large-scale corpora: BookWiki and OpenWebText, which are part of the training dataset of RoBERTa. BookWiki is composed of English Wikipedia and BookCorpus (Zhu et al., 2015), and OpenWebText is an open recreation of the WebText corpora. The image collections are images without textual descriptions, including a subset of OpenImages (Krasin et al., 2017) and COCO unlabel. The image-text pairs are composed of four existing multi-modal datasets: COCO (Lin et al., 2014), Visual Genome (VG) (Krishna et al., 2017), Conceptual Captions (CC) (Sharma et al., 2018) and SBU Captions (Ordonez et al., 2011), which have also been widely used in previous multi-modal pre-training models. The statistics of them are shown in Appendix A."
    }, {
      "heading" : "3.2 Implementation Detail",
      "text" : "We evaluate UNIMO on two model sizes: UNIMObase with 12 layers of Transformer block and UNIMO-large with 24 layers of Transformer block. The maximum sequence length of text tokens and image-region features are set as 512 and 100, respectively. We pre-train UNIMO-base by initializing from RoBERTa-base, and UNIMO-large by initializing from RoBERTa-large. Both UNIMObase and UNIMO-large are trained for at least 500K steps. An Adam optimizer with initial learning rate\n5e-5 and a learning rate linear decay schedule is utilized. By virtue of float16 mixed precision training, it takes almost 7 days for training UNIMO-base with 32 Nvidia Telsa V100 32GB GPU and 10 days for UNIMO-large with 64 Nvidia Telsa V100 32GB GPU.\nFor visual learning, we adopt Faster R-CNN (Ren et al., 2016) pre-trained on the VisualGenome dataset to select salient image regions and extract region features from images. The regions with class detection probability exceeds a confidence threshold of 0.2 are selected and 100 boxes are kept. For CMCL, we utilize back-translation to create 3 positive samples and apply rewriting to obtain 100 hard negative samples for each imagetext pair. The most similar of 100 images and 100 sentences are retrieved from the single-modal image collections and text corpus for each image-text pair, respectively. More details are described in Appendix A."
    }, {
      "heading" : "3.3 Finetuning Tasks",
      "text" : "We fine-tune our model on two categories of downstream tasks: (1) single-modal language understanding and generation tasks; (2) multimodal vision-language understanding and generation tasks. The single-modal generation tasks include: generative conversational question answering on the CoQA dataset (Reddy et al., 2019), question generation on the SQuAD 1.1 dataset (Rajpurkar et al., 2016), abstractive summarization on the CNN/DailyMail (CNNDM) dataset (Hermann et al., 2015), and sentence compression on the Gigaword dataset (Rush et al., 2015). The single-modal understanding tasks include: sentiment classification on the SST-2 dataset (Socher et al., 2013), natural language inference on the MNLI dataset (Williams et al., 2017), linguistic acceptability analysis on the CoLA dataset (Warstadt et al., 2019) and semantic similarity analysis on the STS-B dataset (Cer et al., 2017). The multi-modal tasks include: visual question answering (VQA) on the VQA v2.0 dataset (Goyal et al., 2017), image caption on the Microsoft COCO Captions dataset (Chen et al., 2015), visual entailment on the SNLI-VE dataset (Xie et al., 2019) and image-text retrieval on Flickr30k datasets (Young et al., 2014). The detail statistics of the datasets and hyper-parameter settings for the above tasks are described in Appendix B."
    }, {
      "heading" : "4 Results and Analysis",
      "text" : "In this section, we report the evaluation results on both the multi-modal and single-modal tasks to show the adaptability and generalizability of UNIMO to different scenarios. We further make several ablation studies to validate that textual knowledge and visual knowledge can enhance each other in the unified semantic space. The visualization and case analysis of the model results are appended in Appendix C."
    }, {
      "heading" : "4.1 Multi-Modal tasks",
      "text" : "The evaluation results on the multi-modal tasks are shown in Table 1. We compare with most of the existed multi-modal pre-training models, including ViLBERT (Lu et al., 2019), VLP (Zhou et al., 2020), UNITER (Chen et al., 2020b), Oscar (Li et al., 2020), Villa (Gan et al., 2020) and ERNIE-ViL (Yu et al., 2020). The results show that UNIMO achieves the best results against almost all benchmarks under both the base and large size of\nmodels. Particularly, UNIMO-large outperforms previous best performing model ERNIE-ViL-large by 1.34 R@1 on image retrieval and 1.3 R@1 on text retrieval, which are great improvements for the image-text retrieval tasks. On the image caption task, UNIMO outperforms the best performing model Oscar by more than 2 BLUE4 score. UNIMO achieves better performance on both the multi-modal understanding and generation tasks, while previous methods usually focus on either the understanding or generation tasks. The above results demonstrate the effectiveness of the unifiedmodal learning architecture that takes advantage of the large scale of single-modal images and texts for cross-modal learning."
    }, {
      "heading" : "4.2 Single-Modal tasks",
      "text" : "Previous multi-modal pre-training models usually cannot effectively adapt to single-modal scenarios.To further validate that, we remove the singlemodal learning processes on the text corpus and\nimage collections (i.e., “w/o single-modal”) from UNIMO and replace the CMCL with an image-text matching objective. Then, the model “w/o singlemodal” is just a multi-modal pre-training method similar to UNITER (Chen et al., 2020b). As shown in Table 2, the performance of the model on all the language understanding and generation tasks drop dramatically compared to UNIMO, which demonstrates that multi-modal pre-training only on image-text pairs cannot effectively adapt to the single-modal tasks.\nTo show the effectiveness of UNIMO on the language understanding and generation tasks, we further compare with existed pre-trained language models (PLMs), including BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019) and UniLM (Dong et al., 2019). The comparison results in Table 2 demonstrate that UNIMO achieves better or comparable performance than existed PLMs on both the language understanding and generation tasks. Specifically, UniLM (Dong et al., 2019) is designed for both natural language understanding and generation. UNIMO outperforms UniLM on most of the tasks with a large margin, which demonstrates the effectiveness of UNIMO on the single-modal scenarios.\nIn all, UNIMO not only achieves the best performance on the multi-modal tasks, but also performs very well on the single-modal tasks, which demonstrate the superiority of our unified-modal learning architecture."
    }, {
      "heading" : "4.3 Mutual Enhancement of Text and Vision",
      "text" : "We further make several ablation studies to show that the unified-modal architecture can help textual knowledge and visual knowledge mutually enhance each other in the unified semantic space.\nText Enhance Vision To explore whether the textual knowledge in the text corpus facilitates\nthe cross-modal learning, we remove the language learning process on the text corpus from UNIMO (i.e., “w/o texts”), and compare their performance on the multi-modal tasks. Table 3 summarizes the comparison results, which show that the performance of the model “w/o texts” declines consistently on both the multi-modal understanding and generation tasks. The results demonstrate that the textual knowledge in the text corpus benefit the vision-language tasks by enhancing the crossmodal learning with more textual information.\nVision Enhance Text To further validate that the visual knowledge in the image collections and image-text pairs facilitates the language learning, we remove the images and image-text pairs from the pre-training dataset (i.e., “w/o pairs&images”) and compare their performance on the single-modal language tasks. After removing the images and image-text pairs, our model is trained by only the language learning objectives, which are similar to previous pre-trained language models BERT and UniLM. Table 4 summarizes the comparison results, which demonstrate that after removing the visual data, the performance of the model “w/o pairs&images” drops obviously on most of the language understanding tasks and all the language generation tasks. The results reveal that visual knowledge can enhance the language tasks by enabling the model to learn more robust and generalizable representations in a unified semantic space."
    }, {
      "heading" : "5 Related Work",
      "text" : "Existing researches on pre-training can be mainly classified into two categories: single-modal pretraining and multi-modal pre-training. The singlemodal pre-training methods only focus on singlemodal tasks, while the multi-modal pre-training methods only focus on multi-modal tasks.\nSingle-Modal Pre-training The single-modal pre-training methods mainly consist of visual pretraining and language pre-training. Most visual pre-training methods are based on the multi-layer CNN architecture such as VGG (Simonyan and Zisserman, 2014) and ResNet (He et al., 2016), and trained on the ImageNet dataset. Recently, contrastive self-supervised learning like SimCLR (Chen et al., 2020a) and MoCo (He et al., 2020) also greatly improve the performance of visual representation learning. These pre-trained models only focus on visual tasks (e.g. image classification etc.), however, they cannot be used in textual or multimodal (i.e., with both text and image) tasks. The language pre-training methods based on the Transformer architecture are also very popular in NLP models, such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and BART (Lewis et al., 2020). However, they mainly focus on textual tasks. They cannot effectively deal with the multi-modal tasks, such as image-text retrieval, image captioning, multimodal machine translation (Lin et al., 2020a; Su et al., 2021) and visual dialog (Murahari et al., 2020).\nMulti-Modal Pre-training Recently, multimodal pre-training methods have been more and more popular for solving the multi-modal tasks. All of them are trained on a corpus of image-text pairs, such as ViLBERT (Lu et al., 2019), VisualBERT (Li et al., 2019b), VL-BERT (Su et al., 2019), Unicoder-VL (Li et al., 2019a) and UNITER (Chen et al., 2020b). Based on the multi-layer Transformer network, they all employ the BERT-like objectives to learn multi-modal representations from a concatenated-sequence of vision features and language embeddings. Their architectures can be mainly classified into two categories: single-stream and two-stream. The two-stream methods, such as ViLBERT, utilize two single-modal Transformer to process visual features and language embeddings respectively, and then learn their interactions based on a crossmodal Transformer. The single-stream methods directly utilize a single Transformer network to model both the visual features and the language embeddings. VisualBERT, VL-BERT, UnicoderVL and UNITER all utilize the single-stream architecture, which show that fusing cross-modal information early and freely by a single-stream network can achieve better performance.\nRecently, several contrastive learning-based\nmulti-modal pre-training methods have also been proposed. OpenAI CLIP (Radford et al., 2021) leverages large-scale image-text pairs to learn transferrable visual representations by image-text matching, which enables zero-shot transfer of the model to various visual classification tasks. WenLan (Huo et al., 2021) further proposes a similar two-tower Chinese multi-modal pre-training model and adapts MoCo (He et al., 2020) to improve the contrastive cross-modal learning process. Instead of extracting salient image regions by pre-trained object detection models like Faster-RCNN (Ren et al., 2016), the end-to-end vision-language pre-training architecture SOHO (Huang et al., 2021) proposes to jointly learn Convolutional Neural Network (CNN) and Transformer for cross-modal alignments from millions of image-text pairs.\nAll existed multi-modal pre-training methods only focus on multi-modal tasks with both vision and language inputs. However, they cannot be effectively adapted to single-modal tasks. Moreover, they can only utilize the limited corpus of image-text pairs. By contrast, our unified-modal pre-training method UNIMO can employ large volumes of text corpus and image collections to enhance each other, and can be effectively adapted to both textual and multi-modal scenarios. UNIMO also achieves the best performance on multi-modal tasks including image-text retrieval, visual entailment, VQA and image caption."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work, we propose UNIMO, a unified-modal pre-training architecture to leverage the large scale of non-paired text corpus and image collections for cross-modal learning. We verify that UNIMO provides an effective way for textual knowledge and visual knowledge to mutually enhance each other in a unified semantic space, and UNIMO successfully adapts to both single-modal and multi-modal understanding and generation tasks. In this way, UNIMO outperforms previous methods on both the multi-modal and single-modal downstream tasks. In the future work, we will focus on end-to-end visual and language unified learning, and much larger scale of model size and data volumes."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by the National Key Research and Development Project of China (No. 2018AAA0101900)"
    }, {
      "heading" : "A Pre-training Settings",
      "text" : "Data Processing The pre-training datasets consist of text corpus, image collections and imagetext pairs. The detail statistics of them are shown in Table 5. For unified-modal learning, all data (including images, texts and image-text pairs) are represented in the same format with both visual and textual input as “[IMG] [box1] ... [box100] [CLS] [tok1] ... [tokN] [SEP]”, which “[box]” and “[tok]” denote an image region and subword token, respectively. For single-modal images, a pseudo token sequence “[CLS] [PAD] ... [SEP]” is treated as the textual input during pre-training. During visual learning on images, the pseudo token sequence will be masked out by special self-attention masks to eliminate its effect to the visual learning process. The language learning process will not be applied on the pseudo token sequence. So the single-modal images are equivalent to be encoded individually rather than in pair. Similarly, for single-modal texts, a pseudo image-region sequence “[IMG] [0] ... [0]” will be utilized as the visual input, where “[0]” denotes a zero-value feature embedding. During language learning, the pseudo image-region sequence will be masked out. Based on the above techniques, both images and texts are represented in the same format as image-text pairs. For imagetext pairs, both the visual learning and language learning are applied on the images and captions simultaneously to learn cross-modal representations.\nTraining Details During pre-training, the samples of image collections, text corpus and imagetext pairs are randomly mixed together with ratio 1:1:5. The objectives of language learning, visual learning and cross-modal contrastive learning (CMCL) are trained jointly. The hyper-parameters for both UNIMO-Base and UNIMO-Large are shown in Table 6. For CMCL, each positive imagetext pair is appended with several hard negative samples by text rewriting, as well as several positive images and texts by image/text retrieval. All samples for other image-text pairs in the training batch are also treated as the negative samples (including negative images and negative texts), which are more than 6K for UNIMO-base and 3K for UNIMO-Large. For an image-text pair (V,W ), the detail formula of the CMCL loss LCMCL(V,W ) is as follows:\n−log posP + posI + posT (negP + negI + negT ) + (posP + posI + posT )\n(6)\n posP = ∑ (V +,W+)∈X+ exp(d(V +,W+)/τ) posI = ∑ V r∈XI exp(d(V r,W )/τ) posT = ∑ Wr∈XT exp(d(V,W r)/τ) negP = ∑ (V−,W−)∈X− exp(d(V −,W−)/τ) negI = ∑ V ′∈YI exp(d(V ′,W )/τ) negT = ∑\nW ′∈YT exp(d(V,W ′)/τ)\n(7)\nwhere posP , posI and posT denote the scores of positive image-text pairs X+, related images X I and related texts X T , respectively. Also, negP , negI and negT denote the scores of negative imagetext pairs X−, negative images YI and negative texts YT , respectively. The objective is to maximize the positive score posP + posI + posT while minimizing the negative score negP+negI+negT , while help aligns and unifies the visual and textual representation spaces. The pre-training process of UNIMO is described in Algorithm 1 in pseudocode style.\nData Augmentations We apply two types of data augmentation techniques in the CMCL: text rewriting and image/text retrieval. The text rewriting techniques are utilized to create positive and negative examples for CMCL. To create more positive image-text pairs, we apply back-translation to all captions in the image-text pairs. Each caption is translated into 3 kinds of languages, including Chinese, French and Spanish, by our translation tool in house, and then translated back to English. For the phrase-level and word-level rewriting, each caption in the image-text pairs is firstly parsed into a scene graph by the Stanford Scene Graph Parser1. All objects, attributes and relations are extracted to build an object vocabulary, an attribute vocabulary and a relation vocabulary. For each caption, the objects, attributes or relations are randomly replaced with other similar objects, attributes or relations in the corresponding vocabularies, respectively. The rewritten captions are ranked based on their linguistic fluency, and the top 100 captions are selected to create hard negative image-text pairs by composing with the original image. Furthermore, the image and text retrieval techniques are utilized to\n1https://nlp.stanford.edu/software/scenegraphparser.shtml\naugment each image-text pair with various related images and texts from the single-modal image collections and text corpus. For image-retrieval, each image is transformed into 100 image regions and the object labels are detected for all regions by Faster R-CNN. The object labels are utilized to create a TF-IDF feature vector for each image, and the cosine similarity between images are computed. For each image in the image-text pairs, 100 of the most similar images are retrieved from the image collections, which are treated as positive images in the CMCL. For text retrieval, we firstly build an inverted index for all image captions and sentences in the text corpus, then filter non-relevant sentences from the text corpus based on the inverted index. For each caption in the image-text pairs, the TF-IDF similarities between the caption and the relevant sentences retrieved by the inverted index are calculated, and the top-1000 sentences are extracted. Further, BERT-based embedding similarities are computed between the caption and the 1000 sentences to rank them, and the top-100 sentences are extracted as the positive texts for the CMCL."
    }, {
      "heading" : "B Finetuning Settings",
      "text" : "Task Definition and Details The multi-modal finetuning tasks include: (1) VQA requires the model to answer natural language questions by se-\nAlgorithm 1 UNIMO’s pre-training process in a Python-like style.\n# The training details of UNIMO function pretraining process\nfor step in all steps do batch = [] # load x image samples imgs = get data(ImgCollections, x) # load y text samples texts = get data(TextCorpus, y) # load z image-text pairs img text pairs = get data(Pairs, z) # load CMCL data for each image-text pair for pair in img text pairs do\nsamples = cmcl data loader(pair) batch.extend(samples)\nend for batch.extend(texts) batch.extend(imgs) v loss, l loss, cmcl loss = UNIMO(batch) loss = v loss+ l loss+ cmcl loss loss.backward()\nend for end function\n# build CMCL samples for each image-text pair function cmcl data loader\nsamples = [] # sample a positive pairs from back-translation pos pairs = sample pos pairs(pair, a) # sample b negative pairs from text rewriting neg pairs = sample neg pairs(pair, b) # sample c sentences from text retrieval pos imgs = sample pos imgs(pair, c) # sample d images from image retrieval pos texts = sample pos texts(pair, d) samples.extend(pair) samples.extend(pos pairs) samples.extend(neg pairs) samples.extend(pos imgs) samples.extend(pos texts) return samples\nend function\nlecting the correct answer from a multi-choice list based on an image. We conduct experiments on the widely-used VQA v2.0 dataset, which is built based on the COCO images. Similar to previous work, both training and validation sets are used for training for the results on both the test-std and test-dev splits. (2) Image Caption requires the model to generate a natural language description of an image. We report our results on the Microsoft COCO Captions dataset. Following Karpathy’s split, the dataset contains 113.2k/5k/5k images for\ntrain/val/test splits respectively. (3) Visual Entailment (SNLI-VE) is evaluated on the SLNI-VE dataset which was derived from Flickr30K images and Stanford Natural Language Inference (SNLI) dataset. The task is to determine the logical relationship (i.e., “Entailment”, “Neutral” and “Contradiction”) between a natural language statement and an image. (4) Image-Text Retrieval is evaluated on the Flickr30k dataset, which contains two subtasks: image retrieval (Flickr30k-IR) and text retrieval (Flickr30k-TR), depending on which modality is used as the retrieved target. We report the topK retrieval results on the test sets, including R@1, R@5 and R@10 (R denotes Recall). The statistics of the datasets for the above multimodal-tasks are described in Table 7. The hyper-parameters for all the downstream tasks, including both the multimodal tasks and single-modal tasks are shown in Table 8 and 9.\nC Visualization and Analysis\nTo intuitively show the effectiveness of the unifiedmodal learning on the corpus of images, texts and image-text pairs, we utilize 2-dimensional visualization of the embeddings by Principal component analysis (PCA). The nearest neighbors of the center\nword are shown in the embedding space. UNIMO is compared with two ablation models described in Section 4.3. The figure shows that the model “UNIMO-w/o texts” can find more visual relevant words than “UNIMO-w/o image&pairs”, which demonstrates the effectiveness of the visual learning on images. However, UNIMO not only finds many visually relevant words, but also finds some semantic relevant background words. For example, UNIMO finds “lunch” and “airplanes” for the center word “hamburger”, which denotes people usually eat hamburger at lunch and often eat it while flying. Also, for the second example, UNIMO finds relevant concepts “meter”, “steps” and “soccer” for “foot”, which enrich the concept and connect it with rich relevant information.\nTo further intuitively show the advantages of the unified-modal learning with rich single-modal data, we compare UNIMO with the multimodal pre-training model “w/o single modal” (described in Section 4.2), on both the text retrieval and image retrieval tasks. The examples of text retrieval results in Figure 5 show that the retrieved captions by UNIMO describes the images more accurately by including different levels of information, including objects, attributes and relations in images. The\nexamples of the image retrieval results in Figure 6 also show that the retrieved images better match the captions with more detail semantic alignments."
    } ],
    "references" : [ {
      "title" : "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
      "author" : [ "Daniel Cer", "Mona Diab", "Eneko Agirre", "Iñigo LopezGazpio", "Lucia Specia." ],
      "venue" : "Proceedings of the 11th International Workshop on Semantic",
      "citeRegEx" : "Cer et al\\.,? 2017",
      "shortCiteRegEx" : "Cer et al\\.",
      "year" : 2017
    }, {
      "title" : "A simple framework for contrastive learning of visual representations",
      "author" : [ "Ting Chen", "Simon Kornblith", "Mohammad Norouzi", "Geoffrey Hinton." ],
      "venue" : "International conference on machine learning, pages 1597–1607. PMLR.",
      "citeRegEx" : "Chen et al\\.,? 2020a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Microsoft coco captions: Data collection and evaluation server",
      "author" : [ "Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Dollár", "C Lawrence Zitnick." ],
      "venue" : "arXiv preprint arXiv:1504.00325.",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Uniter: Universal image-text representation learning",
      "author" : [ "Yen-Chun Chen", "Linjie Li", "Licheng Yu", "Ahmed El Kholy", "Faisal Ahmed", "Zhe Gan", "Yu Cheng", "Jingjing Liu." ],
      "venue" : "European Conference on Computer Vision, pages 104–120. Springer.",
      "citeRegEx" : "Chen et al\\.,? 2020b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Unified language model pre-training for natural language understanding and generation",
      "author" : [ "Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon." ],
      "venue" : "Advances in Neural Informa-",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Understanding back-translation at scale",
      "author" : [ "Sergey Edunov", "Myle Ott", "Michael Auli", "David Grangier." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 489–500, Brussels, Belgium. Association for",
      "citeRegEx" : "Edunov et al\\.,? 2018",
      "shortCiteRegEx" : "Edunov et al\\.",
      "year" : 2018
    }, {
      "title" : "Large-scale adversarial training for vision-and-language representation learning",
      "author" : [ "Zhe Gan", "Yen-Chun Chen", "Linjie Li", "Chen Zhu", "Yu Cheng", "Jingjing Liu." ],
      "venue" : "arXiv preprint arXiv:2006.06195.",
      "citeRegEx" : "Gan et al\\.,? 2020",
      "shortCiteRegEx" : "Gan et al\\.",
      "year" : 2020
    }, {
      "title" : "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "author" : [ "Yash Goyal", "Tejas Khot", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision",
      "citeRegEx" : "Goyal et al\\.,? 2017",
      "shortCiteRegEx" : "Goyal et al\\.",
      "year" : 2017
    }, {
      "title" : "Momentum contrast for unsu",
      "author" : [ "Ross Girshick" ],
      "venue" : null,
      "citeRegEx" : "Girshick.,? \\Q2020\\E",
      "shortCiteRegEx" : "Girshick.",
      "year" : 2020
    }, {
      "title" : "Teaching machines to read",
      "author" : [ "Phil Blunsom" ],
      "venue" : null,
      "citeRegEx" : "Blunsom.,? \\Q2015\\E",
      "shortCiteRegEx" : "Blunsom.",
      "year" : 2015
    }, {
      "title" : "Openimages: A public dataset",
      "author" : [ "Veit" ],
      "venue" : null,
      "citeRegEx" : "Veit,? \\Q2017\\E",
      "shortCiteRegEx" : "Veit",
      "year" : 2017
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Unicoder-vl: A universal encoder for vision and language by cross-modal pretraining",
      "author" : [ "Gen Li", "Nan Duan", "Yuejian Fang", "Daxin Jiang", "Ming Zhou." ],
      "venue" : "arXiv preprint arXiv:1908.06066.",
      "citeRegEx" : "Li et al\\.,? 2019a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Visualbert: A simple and performant baseline for vision and language",
      "author" : [ "Liunian Harold Li", "Mark Yatskar", "Da Yin", "Cho-Jui Hsieh", "Kai-Wei Chang." ],
      "venue" : "arXiv preprint arXiv:1908.03557.",
      "citeRegEx" : "Li et al\\.,? 2019b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Oscar: Objectsemantics aligned pre-training for vision-language tasks",
      "author" : [ "Xiujun Li", "Xi Yin", "Chunyuan Li", "Pengchuan Zhang", "Xiaowei Hu", "Lei Zhang", "Lijuan Wang", "Houdong Hu", "Li Dong", "Furu Wei" ],
      "venue" : "In European Conference on Computer Vision,",
      "citeRegEx" : "Li et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Dynamic context-guided capsule network for multimodal machine translation",
      "author" : [ "Huan Lin", "Fandong Meng", "Jinsong Su", "Yongjing Yin", "Zhengyuan Yang", "Yubin Ge", "Jie Zhou", "Jiebo Luo." ],
      "venue" : "Proceedings of the 28th ACM International Conference",
      "citeRegEx" : "Lin et al\\.,? 2020a",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "Interbert: Visionand-language interaction for multi-modal pretraining",
      "author" : [ "Junyang Lin", "An Yang", "Yichang Zhang", "Jie Liu", "Jingren Zhou", "Hongxia Yang." ],
      "venue" : "arXiv preprint arXiv:2003.13198.",
      "citeRegEx" : "Lin et al\\.,? 2020b",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C Lawrence Zitnick." ],
      "venue" : "European conference on computer vision, pages 740–755. Springer.",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "author" : [ "Jiasen Lu", "Dhruv Batra", "Devi Parikh", "Stefan Lee." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 13–23.",
      "citeRegEx" : "Lu et al\\.,? 2019",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2019
    }, {
      "title" : "Large-scale pretraining for visual dialog: A simple state-of-the-art baseline",
      "author" : [ "Vishvak Murahari", "Dhruv Batra", "Devi Parikh", "Abhishek Das." ],
      "venue" : "European Conference on Computer Vision, pages 336– 352. Springer.",
      "citeRegEx" : "Murahari et al\\.,? 2020",
      "shortCiteRegEx" : "Murahari et al\\.",
      "year" : 2020
    }, {
      "title" : "Im2text: Describing images using 1 million captioned photographs",
      "author" : [ "Vicente Ordonez", "Girish Kulkarni", "Tamara Berg." ],
      "venue" : "Advances in neural information processing systems, 24:1143–1151.",
      "citeRegEx" : "Ordonez et al\\.,? 2011",
      "shortCiteRegEx" : "Ordonez et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning transferable visual models from natural language supervision",
      "author" : [ "Alec Radford", "Jong Wook Kim", "Chris Hallacy", "Aditya Ramesh", "Gabriel Goh", "Sandhini Agarwal", "Girish Sastry", "Amanda Askell", "Pamela Mishkin", "Jack Clark" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2021
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "CoQA: A conversational question answering challenge",
      "author" : [ "Siva Reddy", "Danqi Chen", "Christopher D. Manning." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:249–266.",
      "citeRegEx" : "Reddy et al\\.,? 2019",
      "shortCiteRegEx" : "Reddy et al\\.",
      "year" : 2019
    }, {
      "title" : "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "author" : [ "Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun." ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence, 39(6):1137–1149.",
      "citeRegEx" : "Ren et al\\.,? 2016",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2016
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Alexander M. Rush", "Sumit Chopra", "Jason Weston." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 379–389, Lisbon, Portugal.",
      "citeRegEx" : "Rush et al\\.,? 2015",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
      "author" : [ "Piyush Sharma", "Nan Ding", "Sebastian Goodman", "Radu Soricut." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Sharma et al\\.,? 2018",
      "shortCiteRegEx" : "Sharma et al\\.",
      "year" : 2018
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman." ],
      "venue" : "arXiv preprint arXiv:1409.1556.",
      "citeRegEx" : "Simonyan and Zisserman.,? 2014",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2014
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 Conference on",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Multi-modal neural machine translation with deep semantic interactions",
      "author" : [ "Jinsong Su", "Jinchang Chen", "Hui Jiang", "Chulun Zhou", "Huan Lin", "Yubin Ge", "Qingqiang Wu", "Yongxuan Lai." ],
      "venue" : "Information Sciences, 554:47–60.",
      "citeRegEx" : "Su et al\\.,? 2021",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2021
    }, {
      "title" : "Vl-bert: Pretraining of generic visual-linguistic representations",
      "author" : [ "Weijie Su", "Xizhou Zhu", "Yue Cao", "Bin Li", "Lewei Lu", "Furu Wei", "Jifeng Dai." ],
      "venue" : "arXiv preprint arXiv:1908.08530.",
      "citeRegEx" : "Su et al\\.,? 2019",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2019
    }, {
      "title" : "Neuronal populations in the occipital cortex of the blind synchronize to the temporal dynamics of speech",
      "author" : [ "Markus Johannes Van Ackeren", "Francesca M Barbero", "Stefania Mattioni", "Roberto Bottini", "Olivier Collignon." ],
      "venue" : "ELife, 7:e31640.",
      "citeRegEx" : "Ackeren et al\\.,? 2018",
      "shortCiteRegEx" : "Ackeren et al\\.",
      "year" : 2018
    }, {
      "title" : "Scene graph parsing as dependency parsing",
      "author" : [ "Yu-Siang Wang", "Chenxi Liu", "Xiaohui Zeng", "Alan Yuille." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "2019. Cola: The corpus of linguistic acceptability (with added annotations)",
      "author" : [ "Alex Warstadt", "Amanpreet Singh", "Samuel R Bowman" ],
      "venue" : null,
      "citeRegEx" : "Warstadt et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2019
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:1704.05426.",
      "citeRegEx" : "Williams et al\\.,? 2017",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2017
    }, {
      "title" : "Ernie-gen: An enhanced multi-flow pre-training and fine-tuning framework for natural language generation",
      "author" : [ "Dongling Xiao", "Han Zhang", "Yukun Li", "Yu Sun", "Hao Tian", "Hua Wu", "Haifeng Wang." ],
      "venue" : "arXiv preprint arXiv:2001.11314.",
      "citeRegEx" : "Xiao et al\\.,? 2020",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2020
    }, {
      "title" : "Visual entailment: A novel task for fine-grained image understanding",
      "author" : [ "Ning Xie", "Farley Lai", "Derek Doran", "Asim Kadav." ],
      "venue" : "arXiv preprint arXiv:1901.06706.",
      "citeRegEx" : "Xie et al\\.,? 2019",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2019
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems, pages 5753–5763.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
      "author" : [ "Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 2:67–78.",
      "citeRegEx" : "Young et al\\.,? 2014",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 2014
    }, {
      "title" : "Ernievil: Knowledge enhanced vision-language representations through scene graph",
      "author" : [ "Fei Yu", "Jiji Tang", "Weichong Yin", "Yu Sun", "Hao Tian", "Hua Wu", "Haifeng Wang." ],
      "venue" : "arXiv preprint arXiv:2006.16934.",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : ", 2019), RoBERTa (Liu et al., 2019), XLNet (Yang et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 41,
      "context" : ", 2019), XLNet (Yang et al., 2019) and UniLM (Dong et al.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : ", 2019) and UniLM (Dong et al., 2019), which greatly improve the capabilities of language understanding and generation.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 20,
      "context" : "In order to adapt to multi-modal scenarios, a series of multi-modal pre-training methods were proposed and pre-trained on the corpus of image-text pairs, such as ViLBERT (Lu et al., 2019), VisualBERT (Li et al.",
      "startOffset" : 170,
      "endOffset" : 187
    }, {
      "referenceID" : 14,
      "context" : ", 2019), VisualBERT (Li et al., 2019b) and UNITER (Chen et al.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 3,
      "context" : ", 2019b) and UNITER (Chen et al., 2020b), which greatly improve the ability to process multimodal information.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : "Existed cross-modal pretraining methods try to learn cross-modal representations based on only limited image-text pairs by simple image-text matching and masked language modeling (Chen et al., 2020b).",
      "startOffset" : 179,
      "endOffset" : 199
    }, {
      "referenceID" : 17,
      "context" : "So their performance will drop dramatically when applied to language tasks (Lin et al., 2020b), which has also been revealed by our experiments (see Section 4.",
      "startOffset" : 75,
      "endOffset" : 94
    }, {
      "referenceID" : 29,
      "context" : ", wn, [SEP ]} by Byte-Pair Encoding (BPE) (Sennrich et al., 2016), and then the self-attention mechanism is leveraged to learn contextual token representations {h[CLS], hw1 , .",
      "startOffset" : 42,
      "endOffset" : 65
    }, {
      "referenceID" : 3,
      "context" : "Similar to previous work (Chen et al., 2020b), we use Faster R-CNN (Ren et al.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 27,
      "context" : ", 2020b), we use Faster R-CNN (Ren et al., 2016) to detect the salient image regions and extract the visual features (pooled ROI features) for each region.",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 13,
      "context" : "Several existing cross-modal pre-training methods try to align visual and textual representations by simply image-text matching (Li et al., 2019a; Chen et al., 2020b) based on a limited corpus of image-text pairs.",
      "startOffset" : 128,
      "endOffset" : 166
    }, {
      "referenceID" : 3,
      "context" : "Several existing cross-modal pre-training methods try to align visual and textual representations by simply image-text matching (Li et al., 2019a; Chen et al., 2020b) based on a limited corpus of image-text pairs.",
      "startOffset" : 128,
      "endOffset" : 166
    }, {
      "referenceID" : 6,
      "context" : "For sentence-level rewriting, we utilize the back-translation techniques (Edunov et al., 2018)",
      "startOffset" : 73,
      "endOffset" : 94
    }, {
      "referenceID" : 36,
      "context" : "For phrase-level and word-level rewriting, we first parse the image caption into a scene graph (Wang et al., 2018), then randomly replacing the object, attribute or relation nodes of the scene graph with a different object, attribute or relation from the corresponding vocabularies.",
      "startOffset" : 95,
      "endOffset" : 114
    }, {
      "referenceID" : 18,
      "context" : "composed of four existing multi-modal datasets: COCO (Lin et al., 2014), Visual Genome (VG) (Krishna et al.",
      "startOffset" : 53,
      "endOffset" : 71
    }, {
      "referenceID" : 30,
      "context" : ", 2017), Conceptual Captions (CC) (Sharma et al., 2018) and SBU Captions (Ordonez et al.",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 22,
      "context" : ", 2018) and SBU Captions (Ordonez et al., 2011), which have also been widely used in previous multi-modal pre-training models.",
      "startOffset" : 25,
      "endOffset" : 47
    }, {
      "referenceID" : 27,
      "context" : "(Ren et al., 2016) pre-trained on the VisualGenome dataset to select salient image regions and extract region features from images.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 26,
      "context" : "ing on the CoQA dataset (Reddy et al., 2019), question generation on the SQuAD 1.",
      "startOffset" : 24,
      "endOffset" : 44
    }, {
      "referenceID" : 25,
      "context" : "1 dataset (Rajpurkar et al., 2016), abstractive summarization on the CNN/DailyMail (CNNDM) dataset (Hermann et al.",
      "startOffset" : 10,
      "endOffset" : 34
    }, {
      "referenceID" : 28,
      "context" : ", 2015), and sentence compression on the Gigaword dataset (Rush et al., 2015).",
      "startOffset" : 58,
      "endOffset" : 77
    }, {
      "referenceID" : 32,
      "context" : "The single-modal understanding tasks include: sentiment classification on the SST-2 dataset (Socher et al., 2013), natural language inference on the MNLI dataset (Williams et al.",
      "startOffset" : 92,
      "endOffset" : 113
    }, {
      "referenceID" : 38,
      "context" : ", 2013), natural language inference on the MNLI dataset (Williams et al., 2017), linguistic acceptability anal-",
      "startOffset" : 56,
      "endOffset" : 79
    }, {
      "referenceID" : 37,
      "context" : "ysis on the CoLA dataset (Warstadt et al., 2019) and semantic similarity analysis on the STS-B dataset (Cer et al.",
      "startOffset" : 25,
      "endOffset" : 48
    }, {
      "referenceID" : 0,
      "context" : ", 2019) and semantic similarity analysis on the STS-B dataset (Cer et al., 2017).",
      "startOffset" : 62,
      "endOffset" : 80
    }, {
      "referenceID" : 8,
      "context" : "0 dataset (Goyal et al., 2017), image caption on the Microsoft COCO Captions dataset (Chen et al.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 2,
      "context" : ", 2017), image caption on the Microsoft COCO Captions dataset (Chen et al., 2015), visual entailment on the SNLI-VE dataset (Xie et al.",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 40,
      "context" : ", 2015), visual entailment on the SNLI-VE dataset (Xie et al., 2019) and image-text retrieval on Flickr30k datasets (Young et al.",
      "startOffset" : 50,
      "endOffset" : 68
    }, {
      "referenceID" : 42,
      "context" : ", 2019) and image-text retrieval on Flickr30k datasets (Young et al., 2014).",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 3,
      "context" : "“w/o single-modal” denotes removing the single-modal learning process on the single-modal data from UNIMO, which is similar to UNITER-base (Chen et al., 2020b).",
      "startOffset" : 139,
      "endOffset" : 159
    }, {
      "referenceID" : 20,
      "context" : "We compare with most of the existed multi-modal pre-training models, including ViLBERT (Lu et al., 2019), VLP (Zhou et al.",
      "startOffset" : 87,
      "endOffset" : 104
    }, {
      "referenceID" : 15,
      "context" : ", 2020b), Oscar (Li et al., 2020), Villa (Gan et al.",
      "startOffset" : 16,
      "endOffset" : 33
    }, {
      "referenceID" : 7,
      "context" : ", 2020), Villa (Gan et al., 2020) and ERNIE-ViL (Yu et al.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 3,
      "context" : "Then, the model “w/o singlemodal” is just a multi-modal pre-training method similar to UNITER (Chen et al., 2020b).",
      "startOffset" : 94,
      "endOffset" : 114
    }, {
      "referenceID" : 4,
      "context" : "language understanding and generation tasks, we further compare with existed pre-trained language models (PLMs), including BERT (Devlin et al., 2019), RoBERTa (Liu et al.",
      "startOffset" : 128,
      "endOffset" : 149
    }, {
      "referenceID" : 19,
      "context" : ", 2019), RoBERTa (Liu et al., 2019), XLNet (Yang et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 41,
      "context" : ", 2019), XLNet (Yang et al., 2019) and UniLM (Dong et al.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "Specifically, UniLM (Dong et al., 2019) is designed for both natural language understanding and generation.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 31,
      "context" : "Most visual pre-training methods are based on the multi-layer CNN architecture such as VGG (Simonyan and Zisserman, 2014) and ResNet (He et al.",
      "startOffset" : 91,
      "endOffset" : 121
    }, {
      "referenceID" : 1,
      "context" : "Recently, contrastive self-supervised learning like SimCLR (Chen et al., 2020a) and MoCo (He et al.",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 24,
      "context" : "The language pre-training methods based on the Transformer architecture are also very popular in NLP models, such as GPT (Radford et al., 2018), BERT",
      "startOffset" : 121,
      "endOffset" : 143
    }, {
      "referenceID" : 41,
      "context" : ", 2019), XLNet (Yang et al., 2019) and BART (Lewis et al.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 16,
      "context" : "translation (Lin et al., 2020a; Su et al., 2021) and visual dialog (Murahari et al.",
      "startOffset" : 12,
      "endOffset" : 48
    }, {
      "referenceID" : 33,
      "context" : "translation (Lin et al., 2020a; Su et al., 2021) and visual dialog (Murahari et al.",
      "startOffset" : 12,
      "endOffset" : 48
    }, {
      "referenceID" : 14,
      "context" : "2019), VisualBERT (Li et al., 2019b), VL-BERT (Su et al.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 34,
      "context" : ", 2019b), VL-BERT (Su et al., 2019), Unicoder-VL (Li et al.",
      "startOffset" : 18,
      "endOffset" : 35
    }, {
      "referenceID" : 13,
      "context" : ", 2019), Unicoder-VL (Li et al., 2019a) and UNITER (Chen et al.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 23,
      "context" : "OpenAI CLIP (Radford et al., 2021) leverages large-scale image-text pairs to learn transferrable visual representations by image-text matching, which enables zero-shot transfer of the model to various visual classification tasks.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 27,
      "context" : "Instead of extracting salient image regions by pre-trained object detection models like Faster-RCNN (Ren et al., 2016),",
      "startOffset" : 100,
      "endOffset" : 118
    } ],
    "year" : 2021,
    "abstractText" : "Existed pre-training methods either focus on single-modal tasks or multi-modal tasks, and cannot effectively adapt to each other. They can only utilize single-modal data (i.e., text or image) or limited multi-modal data (i.e., image-text pairs). In this work, we propose a UNIfied-MOdal pre-training architecture, namely UNIMO, which can effectively adapt to both single-modal and multi-modal understanding and generation tasks. Large scale of free text corpus and image collections are utilized to improve the capability of visual and textual understanding, and crossmodal contrastive learning (CMCL) is leveraged to align the textual and visual information into a unified semantic space, over a corpus of image-text pairs augmented with related images and texts. With the help of rich non-paired single-modal data, our model is able to learn more generalizable representations, by allowing textual knowledge and visual knowledge to enhance each other in the unified semantic space. The experimental results show that UNIMO greatly improves the performance of several singlemodal and multi-modal downstream tasks. Our code and pre-trained models are public at https://github.com/PaddlePaddle/ Research/tree/master/NLP/UNIMO.",
    "creator" : "LaTeX with hyperref"
  }
}