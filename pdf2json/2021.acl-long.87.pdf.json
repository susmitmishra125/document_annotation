{
  "name" : "2021.acl-long.87.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Selecting Informative Contexts Improves Language Model Fine-tuning",
    "authors" : [ "Richard Antonello", "Nicole M. Beckage", "Javier S. Turek", "Alexander G. Huth" ],
    "emails" : [ "rjantonello@utexas.edu", "nicole.beckage@intel.com", "javier.turek@intel.com", "huth@cs.utexas.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1072–1085\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1072"
    }, {
      "heading" : "1 Introduction",
      "text" : "Language modeling is the task of generating language from context. This is often framed as an autoregressive task, where a model predicts the conditional probability of the next word based on the sequence of previously observed or generated tokens. Language modeling has seen a recent surge in relevance thanks to its success as a pretraining objective for self-supervised representa-\ntion learning. The most prominent language models today are Transformer-based models (Vaswani et al., 2017) such as BERT (Devlin et al., 2019) and GPT-2 (Radford et al., 2019).\nLanguage models are most commonly trained with backpropagation using traditional NLP loss functions such as cross entropy. These loss functions are designed so that the models are rewarded for assigning high probability to text that appears commonly in the training corpus. The energy and computational costs of training a state-of-the-art language model from scratch are very high, to the point of impracticality for most researchers. One recent estimate suggests that training a single state-of-the-art model with architecture search takes more energy than five cars will use in their entire lifetimes (Strubell et al., 2019). In practice, this cost is sidestepped by pretraining, where a language model is trained once and then released publicly. This language model can then be updated for use in other tasks through fine-tuning. For example, a generic language model can be fine-tuned to generate text that matches the style and syntax of any new corpus (Howard and Ruder, 2018). While better than training from scratch, the cost of fine-tuning such large networks is still relatively high. Fine-tuning to convergence for a single task can easily take in excess of a day on multiple energy-intensive GPUs (Strubell et al., 2019).\nRecent work analyzing the fine-tuning process has shown that it has high variability between runs and is particularly sensitive to data ordering (Dodge et al., 2020). Those authors propose to overcome this variability by training models using many random seeds and then only keeping the best, effectively trading computational efficiency for model performance. While this improves performance, the reasons for the high variability between random seeds have yet to be explored. We hypothesize that much of this variability can be ex-\nplained by the random selection of highly “informative” training examples, which most effectively capture low-level distributional statistics of the target corpus. If this is the case, then it should be possible to quickly screen for informative training examples, ensuring high performance at reduced cost.\nIn this paper, we suggest replacing the retrospective approach of testing many random seeds (Dodge et al., 2020) with a prospective approach to improving the effectiveness of language model fine-tuning. Our approach uses a secondary learner to estimate the usefulness of each training example, and then selects only informative examples for training. We show that this technique works well and is applicable in a variety of finetuning settings. We examine why it works well, and present evidence that supports our hypothesis about informative examples explaining data ordering effects. In addition to performance gains, this method may mitigate the energy impact of deep neural network language modeling, as we require fewer backpropagation steps than other techniques that trade computational power for fine-tuning performance."
    }, {
      "heading" : "2 Related Work",
      "text" : "Several methods have recently been proposed to improve language model fine-tuning performance. Lee et al. (2020) proposed a technique based on neural network dropout (Srivastava et al., 2014) for regularizing finetuned language models that involved stochastically mixing the parameters of multiple language models for the same domain, and further demonstrated the usefulness of pretrained weight decay over conventional weight decay for improving language model fine-tuning performance. Phang et al. (2018) showed that adding supplementary training to pretrained language models using supervised tasks yielded state of the art results for BERT (Devlin et al., 2019). Moore and Lewis (2010) proposed a related technique for increasing the amount of language model training data from out-of-domain data sources that relies on filtering out high cross-entropy contexts as measured by an in-domain language model. Tenney et al. (2019) and Liu et al. (2019) have both suggested that language model finetuning is better in Transformer-based models when starting when using features from intermediate layers as opposed to later layers.\nThe instability of language model fine-tuning has previously been investigated by others. Mosbach et al. (2021) suggested that this instability is caused by a combination of insufficiently general training sets and optimization challenges. Zhang et al. (2021) investigated how similar factors, such as non-standard optimization techniques and overreliance on a standard number of training iterations hurts the performance of fine-tuned language models. Dodge et al. (2020), whose work we replicate and build on here, showed that language model finetuning is sufficiently stochastic so that even random seed searches are a suitable technique for improving their overall performance."
    }, {
      "heading" : "3 Background",
      "text" : "A language model L is a function with parameters θ, which, when given an ordered sequence of tokens X = {x1, . . . , xn} as input, outputs a probability distribution over the next token y:\nL(X; θ) = p̂(y|X).\nGiven a test set T of (sequence, next token) pairs, T = {(X1, y1), . . . , (Xn, yn)}, the perplexity Λ(T ; θ) of the language model L(X; θ) over the set T is defined as:\nΛ(T ; θ) = 2− ∑ (Xi,yi)∈T p̄(yi)·log2 L(Xi;θ),\nwhere p̄(yi) denotes the one-hot probability distribution that assigns all of its probability mass to the token yi. Autoregressive language models such as GPT-2 (Radford et al., 2019) are trained to minimize perplexity using backpropagation on very large training corpora.\nIn practice, pre-trained language models are often fine-tuned using a new corpus or transferred to a new task (Howard and Ruder, 2018). Formally, let F = {(Xi, yi)}i be a target set. Fine-tuning on the set F tries to minimize the expected value of the loss function Λ:\nθ̂ = arg min θ E(log2 Λ(F ; θ)). (1)\nThe initial parameterization θ̂0 of the language model is defined by its pre-trained parameters θ̂0 = θ. The fine-tuning problem in Eq. (1) is then solved by applying stochastic gradient descent (SGD) on samples from F . Namely, for a given batch B of samples from F , the language model parameters are updated by θ̂k ← θ̂k−1 −\nα∇Λ(B; θ̂k−1), where α is the step size. We refer to methods that randomly sample contexts to update pretrained model parameters as standard finetuning.\nWhile random sampling methods are useful (Bottou, 1991), the stochasticity of context sampling suggests an avenue for additional improvement. Such methods make no assumption on the informativeness of the examples in F , instead relying on randomness to find useful training samples. It is worth asking ourselves: can we efficiently measure the informativeness of an example? And if so, can we exploit that measurement for additional fine-tuning improvement?"
    }, {
      "heading" : "4 Information Gain Filtration",
      "text" : ""
    }, {
      "heading" : "4.1 Informativeness of an Example",
      "text" : "Next, we characterize the informativeness of an example (X, y) ∈ F , given a pre-trained language model L(X; θ) and a target dataset F . We define an example (X, y) as “informative” if our estimate of the improvement that it will grant to the model exceeds a chosen threshold. Namely, if we expect that a given example will reduce model perplexity by more than a preset amount, then we will denote it as “informative”.\nWe define the information gain (IG) of a example (X, y) over an objective setO as the difference in perplexity measured on the objective set O before and after training on the example (X, y),\nIGO(X, y) = Λ(O; θ′(X, y))− Λ(O; θ), (2)\nwhere θ is the initial parameterization of the language model and θ′(X, y) is the parameterization after backpropagating the loss associated with training example (X, y). The objective set O = {(X1, y1), . . . , (Xn, yn)} is a held-out subset of training data that informs our decision about which contexts are informative. In practice, the objective set could be a subset of the fine-tuning set F . For brevity, we denote IGO(X, y) as simply IG(X) since there exists an implicit direct bijection between all X’s and y’s and the objective set is implied."
    }, {
      "heading" : "4.2 Filtering Examples",
      "text" : "Since information gain evaluates the informativeness of an example, we next propose a method that exploits it for fine-tuning. Let us assume that the method encounters a new example (X, y). Then, the method has a choice between two actions:\n• BACKPROP: update the language model parameters θ by backpropagating the loss Λ({(X, y)}; θ), taking the gradient descent step, and updating parameters from θ to θ′.\n• SKIP: leave the language model parameters unchanged.\nWith this idea in mind we define the function1 q(X, action) and assign a value to each of the actions above:\nq(X,BACKPROP) = IG(X) (3)\nq(X, SKIP) = TSKIP, (4)\nwhere TSKIP is a free “threshold” parameter for deciding which IG(X) values are sufficiently high to warrant backpropagation.\nFollowing this definition, we can apply a greedy policy for filtering examples during fine-tuning:\nπ(X) = argmaxa∈{BACKPROP,SKIP}q(X, a).\nBy filtering examples in this way, we aim to reduce the effect of variability in data order observed in previous work (Dodge et al., 2020), and improve the generalizability of our training set (Mosbach et al., 2021). By doing this, we expect to improve the performance of our language model. We call this technique Information Gain Filtration or simply IGF."
    }, {
      "heading" : "4.3 Approximating Information Gain",
      "text" : "Thus far, we have described a general method to segregate informative from non-informative examples, deferring the issue of computational cost. Computing IG(X) in Equation (2) entails a backpropagation step, making direct application of q(X, action) at least as expensive as standard fine-tuning. To address this issue, we aim to approximate the information gain IG(X) using a separate model that we will call the secondary learner and denote with Q̂(X).\nTo train this secondary learner, we first construct a training dataset D by measuring IG(X) for a random subset of examples drawn from the fine-tuning set F . The objective set O used to compute IG(X) is selected as a different subset of F . Each entry in D consists of a pair of\n1Due to its intuitive similarity with notions in reinforcement learning (Mnih et al., 2013) of using a network to approximate the expected value of a given action, we abbreviate this normalized informativeness metric as a “Q-value” (Watkins and Dayan, 1992)\nthe input text X and its associated IG(X) value, i.e., D = {(X1, IG(X1)), . . . , (Xn, IG(Xn))}. We then train the secondary learner Q̂ to approximate a normalized IG(X) givenX . We normalize IG(X) so that TSKIP can be interpreted as a standardized threshold on the selectivity of the filtration. Finally, the resulting secondary learner Q̂ is used to filter examples during fine-tuning. Algorithm 1 summarizes IGF with a secondary learner for language model fine-tuning.\nAlgorithm 1 Information Gain Filtration Input: Fine-tuning (F) and objective (O) dataset of contexts, (X ,O) := {(X1, y1), ..., (Xn, yn)}, parameterization of initial pretrained LM, θ, and initial secondary learner model Q̂. Parameters: Size of learner dataset, s, and threshold parameter, TSKIP Output: θ′, new parameterization for the LM\n1: Initialize D,B = {}. 2: for i = 0 . . . s do 3: Sample context (Xi, yi) from X . 4: Append (Xi, IGO(Xi, yi)) to D. 5: Normalize IGO(X, y) values inD toN (0, 1). 6: Train secondary learner Q̂, using dataset D. 7: for i = 0 . . . number of batches−1 do 8: while |B| < batch size do 9: Sample context C = (X, y) from X .\n10: if Q̂(C) ≥ TSKIP then 11: Add C to batch B. 12: Backpropagate over batch B, updating θ 13: Reset batch B = {}. 14: Return θ."
    }, {
      "heading" : "4.4 Scheduled Thresholding",
      "text" : "The secondary learner training set D is constructed using the initial pretrained model parameters θ0. This means that the effectiveness of the learner at distinguishing “high quality” from “low quality” examples should degrade as the parameters diverge from their initial values. To ameliorate this problem, Equation (4) can be modified by changing TSKIP during the fine-tuning process. Since Q̂ is most accurate at the first step, we scheduled TSKIP to switch from highly selective (a high value) to highly permissive (a low value). This allows the model to take advantage of the accurate predictions for IG(X) early in the fine-tuning process without overfitting once those predictions become less accurate later on."
    }, {
      "heading" : "5 Results",
      "text" : "Here we first provide an empirical analysis suggesting that IGF outperforms standard fine-tuning across different choices of datasets, fine-tuning tasks, and neural architectures. We follow this analysis with an examination of why IGF works, and an exploration into the statistical properties of standard fine-tuning and of IGF. We tested these results on a standard Books dataset (Zhu et al., 2015), a “mixed” dataset which is composed of training examples from two corpora (the Books corpus and a corpus of scraped Reddit comments (Huth et al., 2016)), and the WikiText-103 dataset (Merity et al., 2017). The Books corpus allows us to fairly compare standard fine-tuning against IGF, whereas the Mixed corpus allows us to analyze the effectiveness of the method at separating informative contexts from uninformative ones.\nIn practice, our secondary learner, Q̂, represents the input text X by embedding it with 768- dimensional byte-pair embeddings (Gage, 1994). We then pass the input representations through a convolution with kernel width 3, followed by maxpooling operation over the time axis and a 2-layer feedforward network. This architecture was refined through coordinate descent, and evaluated on a separate held-out set of measured IG(X) values. The choice of architecture does not strongly affect method performance (see Appendix A, Figure 11). Additionally, a neural network is not necessary for the learner, as simpler learning methods are sufficient (see Figure 5)."
    }, {
      "heading" : "5.1 Language Model Fine-tuning",
      "text" : "We first compare IGF directly to standard finetuning, which we define as basic batched stochastic gradient descent with Adam (Kingma and Ba, 2015) using random samples from the target corpus. For initial tests, we chose the pretrained GPT-2 Small Transformer model, a commonly used unidirectional language model with roughly 124 million parameters. We used the publicly available GPT-2 Small implementation of the transformers package (Wolf et al., 2020). We performed 50 runs each of standard fine-tuning on (1) training examples sampled from the Mixed corpus, and (2) from the easier Books corpus. We then performed 50 runs of IGF using two thresholding schedules, one with a fixed TSKIP and one with shifting TSKIP. For both methods, batches of size 16 were used to train the language model with\na learning rate of 5 × 10−5 and β1 = 0.9, β2 = 0.999. The convolutional network that we used for our secondary learner was trained using SGD with Adam with a learning rate of 10−5 and β1 = 0.9, β2 = 0.999. Both types of IGF runs were performed on the strictly more challenging Mixed corpus only. In all cases model perplexity was tested on a set drawn solely from the Books corpus. Figure 1 plots the averaged fine-tuning curves of these 4 different approaches over 60 batches. We see that IGF significantly improves final test perplexity when compared to standard fine-tuning on both the Mixed corpus and the Books corpus. Standard fine-tuning on Books achieves a median perplexity of 57.3, compared to 56.9 for IGF with a constant threshold and 54.0 for IGF with the shifting threshold schedule.2 All 50 runs of IGF with a shifting schedule outperformed all 50 standard fine-tuning runs. This means that the overall improvements to data order that IGF achieves through selective sampling of informative contexts are far in excess of what might be reasonably achieved through random sampling of contexts.\nNext, we show that the improvements offered\n2Demo code and data can be found at https://github.com/HuthLab/IGF.\nby IGF persist across several choices of dataset, fine-tuning specifications, and model architecture. Figure 2 shows the final converged values for fine-tuning GPT-2 Small on a different dataset from Figure 1 (WikiText-103), a different architecture (GPT2-Medium), a different embedding space with different directionality (BERT) (Devlin et al., 2019), and a different overall finetuning task (SST-2) (Socher et al., 2013). In every case, IGF exceeds the performance of standard fine-tuning. This suggests that IGF is a resilient method that is broadly applicable to a variety of fine-tuning modalities and domains."
    }, {
      "heading" : "5.2 Understanding IGF",
      "text" : "It is clear that IGF is successful as a general method for improving fine-tuning performance, however why this is the case remains unexamined.\nHere, we present an analysis of the statistical properties of fine-tuning that illuminates why IGF is able to improve over standard fine-tuning.\nA main assumption of IGF is that it is possible to approximate IG(X). If IG(X) is not approximable, then the secondary learner could not effectively filter out uninformative contexts and therefore would be useless. In order to support this assumption, we will first show that a given example is worth learning from even if it only possesses the correct low-level features of informative contexts, such as the correct unigram frequency distribution. We performed an experiment in which we fine-tuned a language model on either (1) real example sequences from a corpus, (2) artificial sequences that were constructed by independently sampling each token from the frequency distribution of the corpus, and (3) sequences constructed by uniformly sampling tokens from the set of all possible tokens. We then measured the change in loss on a separate portion of the corpus. Figure 4 shows the results of this experiment. The average reduction in loss for examples constructed using the unigram frequency distribution is significantly better than random and roughly 70% as good as using real examples from the corpus. Thus, a significant fraction of the benefit of training on real contexts can be estimated by merely knowing the unigram frequency distribution from which those contexts were derived, which is easily estimable without knowing the particular parameterization of the language model itself. Therefore, it makes sense that IGF can inexpensively estimate whether a given context generalizes well to the target corpus.\nThe secondary learner only bases its estimates on the update to loss after the first backpropogation step. We might question whether early improvement translates to long-term improvement over the course of fine-tuning. If it did not, then the estimates that the secondary learner produces would eventually disappear as fine-tuning continued. Dodge et al. (2020) observed that the quality of a fine-tuning run could usually be established by looking at the trajectory of the loss curve very early during training. In order to explain why these early estimates are sufficient for sample filtration, we attempted to determine whether training on good contexts early is an important element of the variability in data order between fine-tuning runs. Figure 3 compares test perplexity after training from a randomly sampled first batch against the test perplexity after many randomly sampled batches. Good early batches improve the proba-\nbility of converging to an ideal final value. The correlation between the test perplexity after a single batch and the test perplexity after 50 batches, which is near convergence for most runs, is statistically significant (r = 0.28). While this value appears somewhat low, it is significant and therefore can be exploited for improvements in performance.\nTaken together, the pair of observations that (1) early data quality is important, and (2) that the quality of a context can be summarized by its lowlevel statistics serves to motivate our understanding of why IGF is effective. Specifically, if we can carefully ensure that early batches are good, as IGF does, then we will likely end up with a superior model after convergence."
    }, {
      "heading" : "5.3 Understanding the Secondary Learner",
      "text" : "This raises the question of which contexts are considered “informative” by the secondary learner. To answer this question, we apply IGF to the Mixed corpus containing both Reddit and Books. We created a dataset of 10,000 (X, IG(X)) pairs using an objective set of 160 contexts with 32 tokens each drawn solely from the Books corpus. We used this dataset to train a secondary learner. Next, the secondary learner was fed randomly sampled contexts from the Mixed corpus. Because the objective set contains only examples from one corpus, we expect the secondary learner to assign higher IG(X) values to other examples from the same corpus. Figure 6 shows that there is indeed a significant difference in the distributions of Q̂ values between the two corpora, demonstrating that the Books and Reddit corpora can be separated by the secondary learner. Almost all examples from the Reddit corpus are expected by the secondary learner to produce a reduction in perplexity that\nis at least one standard deviation below the mean. This indicates that the secondary learner can identify with strong confidence that Books corpus examples are more informative for fine-tuning towards the Books objective than Reddit corpus examples. It is also worthwhile to note that the secondary learner achieves dataset separation despite having access to just 160 labeled examples of 32 tokens in our objective set, a total of just 5120 tokens from the Books corpus, and zero examples from the Reddit corpus."
    }, {
      "heading" : "5.4 Efficiency of IGF",
      "text" : "For previous results we used a simple convolutional neural network described in Section 3 as our secondary learner. However, it may not be necessary to use a such a complex model for IG(X). Alternative methods could provide similar performance at less cost. Figure 5 shows predicted vs. actual normalized IG(X) values for several learning methods. While the 45,000 parameter convolutional neural network is most effective at approximating IG(X), other learners perform almost well. We encoded the contexts both by using the standard GPT-2 Small word embedding and with a one-hot encoding of the token identities. Standard linear regression performed on both encoding types (30K parameters for word\nembeddings and 450,000 parameters for one-hot encoding) performs nearly as well at approximating IG(X) with a convolutional model. We also tested an even simpler learner with only 25,000 parameters that assigned each token a value by averaging the IG(X) values for contexts that contained that token. Values for new contexts are then computed as the average of token values contained in that context. Even this model is a reasonable approximator of IG(X). This underscores that, while IG(X) is an extremely complex function to compute exactly, it can nevertheless be effectively approximated through simple unigram information. Figure 7 compares the performance of these secondary learners architectures across different numbers of training examples. Here the convolutional network is the most sample efficient method, as it can effectively learn IG(X) with as few as 2,000 training examples."
    }, {
      "heading" : "5.5 Comparison to Random Seed Search",
      "text" : "We proposed IGF as a prospective alternative to the random seed search approach suggested by Dodge et al. (2020). Since IGF aims to replaces random search with a directed search, we expect IGF to be significantly more efficient. Of course the methods can also be combined: IGF can be run many times with different data orders, and then the best model selected. In Figure 8 we compare the Dodge et al. (2020) method, where the best model is selected across 1, 5, or 50 runs of standard finetuning, to a similar setup where the best IGF model is selected across 1, 5, or 50 runs. We find that even in a single run, IGF significantly outperforms choosing the best of 50 runs of standard finetuning. Still, IGF performance can be improved even further by choosing the best result across 5 or 50 runs. This suggests that while IGF exploits some of the benefits that could be gained from ideal data ordering, there are still improvements to be made over IGF for further improving data order during language model fine-tuning."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "In the context of language model fine-tuning, we have shown that a secondary learner can efficiently and effectively distinguish between informative and uninformative training examples. This secondary learner can be used to select useful training examples in a technique we call Information Gain Filtration, leading to better model perfor-\nmance than standard fine-tuning. We encourage researchers to release pretrained secondary learners for frequently used corpora, in order to enable more effective finetuning and save energy. This would cut down the largest computational cost of applying IGF while retaining the performance improvements across the field. We have included several examples of open-sourced secondary learners in the supplementary material to promote this paradigm.\nThis work also raises several questions. Since our focus was on developing a lightweight technique, the most complex secondary learner we tested was a small convolutional network. Data efficiency during training could potentially be further improved by using a more complex model. The question of how far one could reasonably take a function approximator network for estimating information gain remains unexplored.\nFinally, we do not fully understand why improving performance on early training batches results better performance at convergence. Is this exclusively a property of language models, or do other networks and tasks exhibit this phenomenon? An-\nswering this question could lead to better optimization methods across many different fields."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank all of the people whose contributions, opinions and suggestions helped with the development of this project, especially Kaj Bostrom, Greg Durrett, Shailee Jain, and Vy Vo. This project was funded by a generous gift from Intel, and by the Burroughs-Wellcome Career Award at the Scientific Interface."
    }, {
      "heading" : "A Supplementary Material",
      "text" : "A.1 Miscellaneous Figures\nA.2 Sample High IG(X) Contexts A few randomly sampled contexts from the Books corpora with IG(X) > 1 are given below. Note that all are highly structured conversations which are common of the narrative setting in the Books corpus:\n• ’t you. He forced your hand with Max. ” ” We’re going to die, ” she said. ” Aren’t we?\n• ” The world is ending. ” ” No it’s not. ” Valerie snapped. ” It’s a world war, that\n• You? ” ” Yep. Did your dad leave? ” She nodded. ” They all said to tell you congratulations and they’ll\nA.3 Sample Low IG(X) Contexts A few sample contexts from the Books corpora with IG(X) < −1 are given below. Many of these contexts appear to be long, run-on sentences that are more challenging to follow:\n• n order ; you’ve got to make friends, you’ve got to put on a united front and for the governments of Earth that was no mean feat\n• - headed eunuchs in crimson robes knelt in a cluster to one side of the dais, resting on their haunches and gazing at the woman and\n• don’t hold back, and by God, if I could be like you for even a moment, if I could have your strength, your courage, your\n• frantically down one path, doubled back, and headed down another, like a frightened mouse trying to outsmart a determined cat in a warren of false trails and\nB Iterated Information Gain Filtration\nInstead of scheduling the selectivity of the secondary learner to taper off as the fine-tuning process continues, we might instead replace the learner periodically with a new learner trained on a new dataset of (X, IG(X)) pairs generated using the current parameterization of the language model. This process, which we call iterated infomation gain filtration (IIGF), allows us to replace the obsolete learner that was trained to predict IG(X) for early examples with a learner that is more relevant later in fine-tuning. IIGF has the added advantage of allowing us to keep TSKIP high throughout fine-tuning, as secondary learner irrelevance is no longer a concern. This procedure is very computationally expensive, as the overhead in generating the new dataset and learner far exceeds the computational cost of fine-tuning. Nonetheless, this enables finer control of data order throughout the fine-tuning process and further improvements in final perplexity over IGF with scheduled thresholding. Due to its computational expense, we ran a small set of 5 tests of iterated information gain filtration by training a secondary learner using a dataset built from example (X, IG(X)) pairs derived from a language model that had already been fully finetuned to the Books corpus. IIGF was able to improve these alreadyconverged models by an average of 0.29 additional perplexity points after reconverging, with a standard deviation of 0.11 points.\nAlgorithm 2 Iterated Information Gain Filtration Input: Training (X ) and objective (O) dataset of contexts, (X ,O) = {(X1, y1), ..., (Xn, yn)}, and parameterization of initial pretrained LM, θ Parameters: Size of learner dataset, s, threshold parameter, TSKIP, and number of batches per secondary learner reset, t Output: θ′, new parameterization for the LM\n1: Initialize D,B = {}. 2: for all i, 0 ≤ i ≤ num batches do 3: while |B| < batch size do 4: if i mod t = 0 then 5: for all i, 0 ≤ i ≤ s do 6: Sample context (Xi, yi) from X . 7: Append (Xi, IGO(Xi, yi)) to D. 8: Normalize IGO(X, y) values in D to N (0, 1).\n9: Train learner Q̂, using D as a train set. 10: Sample context C = (X, y) from X . 11: if Q̂(C) ≥ TSKIP then 12: Append C to batch B. 13: Update θ by backpropagating over batch B,\nand clear batch B. 14: Return θ."
    }, {
      "heading" : "C Relative Informativeness of Contexts",
      "text" : "Since our method uses a black box learner to estimate the informativeness of a given context, one might wonder what it is about these contexts that makes them more or less informative. To investigate this, we constructed test sets of 100 contexts each from the Books corpus which were rated as either highly informative (IG(X) > 1) or uninformative (IG(X) < 1) by the secondary learner. We then finetuned GPT2-Small using both standard fine-tuning and IGF as in Figure 1 and periodically evaluated the performance of the model on the informative and uninformative contexts as training proceeded. Figure 13 shows that contexts which were rated as highly informative experienced a significantly greater reduction in perplexity over time as compared to contexts that were rated as uninformative. The poorly informative contexts actually performed worse on average after fine-tuning than either standard fine-tuning or IGF. This suggests that highly informative contexts are also highly informed, or more easily predicted after fine-tuning on the target corpus. Inspection of highly informative contexts shows that they tend to employ simple diction and basic sentence structure that is representative of the corpus, whereas uninformative contexts tend to employ complex sentence structure and atypical vocabulary. All highly rated contexts from the Books corpus consisted of dialog, which suggests that the secondary learner prioritizes linguistic patterns that are common to the fine-tuning corpus but rare in general writing. Since the Books corpus is composed of narrative stories heavy on dialog, it makes sense that conversations, which rarely appear in non-narrative corpora, would be rated as highly informative. The supplementary material gives some examples of highly informative and uninformative contexts from the Books corpus."
    } ],
    "references" : [ {
      "title" : "Stochastic gradient learning in neural networks",
      "author" : [ "Léon Bottou." ],
      "venue" : "Proceedings of Neuro-Nımes, 91(8):12.",
      "citeRegEx" : "Bottou.,? 1991",
      "shortCiteRegEx" : "Bottou.",
      "year" : 1991
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping",
      "author" : [ "Jesse Dodge", "Gabriel Ilharco", "Roy Schwartz", "Ali Farhadi", "Hannaneh Hajishirzi", "Noah Smith" ],
      "venue" : null,
      "citeRegEx" : "Dodge et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Dodge et al\\.",
      "year" : 2020
    }, {
      "title" : "A new algorithm for data compression",
      "author" : [ "Philip Gage." ],
      "venue" : "C Users Journal, 12(2):23–38.",
      "citeRegEx" : "Gage.,? 1994",
      "shortCiteRegEx" : "Gage.",
      "year" : 1994
    }, {
      "title" : "Universal language model fine-tuning for text classification",
      "author" : [ "Jeremy Howard", "Sebastian Ruder." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Howard and Ruder.,? 2018",
      "shortCiteRegEx" : "Howard and Ruder.",
      "year" : 2018
    }, {
      "title" : "Natural speech reveals the semantic maps that tile human cerebral cortex",
      "author" : [ "Alexander G Huth", "Wendy A De Heer", "Thomas L Griffiths", "Frédéric E Theunissen", "Jack L Gallant." ],
      "venue" : "Nature, 532(7600):453– 458.",
      "citeRegEx" : "Huth et al\\.,? 2016",
      "shortCiteRegEx" : "Huth et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Mixout: Effective regularization to finetune large-scale pretrained language models",
      "author" : [ "Cheolhyoung Lee", "Kyunghyun Cho", "Wanmo Kang." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Lee et al\\.,? 2020",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "Linguistic knowledge and transferability of contextual",
      "author" : [ "Nelson F. Liu", "Matt Gardner", "Yonatan Belinkov", "Matthew E. Peters", "Noah A. Smith" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Pointer sentinel mixture models",
      "author" : [ "Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Merity et al\\.,? 2017",
      "shortCiteRegEx" : "Merity et al\\.",
      "year" : 2017
    }, {
      "title" : "Playing atari with deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin A. Riedmiller." ],
      "venue" : "CoRR, abs/1312.5602.",
      "citeRegEx" : "Mnih et al\\.,? 2013",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2013
    }, {
      "title" : "Intelligent selection of language model training data",
      "author" : [ "Robert C. Moore", "William Lewis." ],
      "venue" : "Proceedings of the ACL 2010 Conference Short Papers, pages 220–224, Uppsala, Sweden. Association for Computational Linguistics.",
      "citeRegEx" : "Moore and Lewis.,? 2010",
      "shortCiteRegEx" : "Moore and Lewis.",
      "year" : 2010
    }, {
      "title" : "On the stability of fine-tuning BERT: Misconceptions, explanations, and strong baselines",
      "author" : [ "Marius Mosbach", "Maksym Andriushchenko", "Dietrich Klakow." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Mosbach et al\\.,? 2021",
      "shortCiteRegEx" : "Mosbach et al\\.",
      "year" : 2021
    }, {
      "title" : "Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks",
      "author" : [ "Jason Phang", "Thibault Févry", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:1811.01088.",
      "citeRegEx" : "Phang et al\\.,? 2018",
      "shortCiteRegEx" : "Phang et al\\.",
      "year" : 2018
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI Blog, 1(8).",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 conference on",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "J. Mach. Learn. Res., 15(1):1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Energy and policy considerations for deep learning in NLP",
      "author" : [ "Emma Strubell", "Ananya Ganesh", "Andrew McCallum." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, pages 3645–3650. Association for Com-",
      "citeRegEx" : "Strubell et al\\.,? 2019",
      "shortCiteRegEx" : "Strubell et al\\.",
      "year" : 2019
    }, {
      "title" : "What do you learn from context? probing for sentence structure in contextu",
      "author" : [ "Ian Tenney", "Patrick Xia", "Berlin Chen", "Alex Wang", "Adam Poliak", "R Thomas McCoy", "Najoung Kim", "Benjamin Van Durme", "Sam Bowman", "Dipanjan Das", "Ellie Pavlick" ],
      "venue" : null,
      "citeRegEx" : "Tenney et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Tenney et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Qlearning",
      "author" : [ "Christopher JCH Watkins", "Peter Dayan." ],
      "venue" : "Machine learning, 8(3-4):279–292.",
      "citeRegEx" : "Watkins and Dayan.,? 1992",
      "shortCiteRegEx" : "Watkins and Dayan.",
      "year" : 1992
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Revisiting fewsample BERT fine-tuning",
      "author" : [ "Tianyi Zhang", "Felix Wu", "Arzoo Katiyar", "Kilian Q Weinberger", "Yoav Artzi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "author" : [ "Yukun Zhu", "Ryan Kiros", "Richard Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler" ],
      "venue" : null,
      "citeRegEx" : "Zhu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "The most prominent language models today are Transformer-based models (Vaswani et al., 2017) such as BERT (Devlin et al.",
      "startOffset" : 70,
      "endOffset" : 92
    }, {
      "referenceID" : 1,
      "context" : ", 2017) such as BERT (Devlin et al., 2019) and GPT-2 (Radford et al.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : "For example, a generic language model can be fine-tuned to generate text that matches the style and syntax of any new corpus (Howard and Ruder, 2018).",
      "startOffset" : 125,
      "endOffset" : 149
    }, {
      "referenceID" : 17,
      "context" : "Fine-tuning to convergence for a single task can easily take in excess of a day on multiple energy-intensive GPUs (Strubell et al., 2019).",
      "startOffset" : 114,
      "endOffset" : 137
    }, {
      "referenceID" : 2,
      "context" : "In this paper, we suggest replacing the retrospective approach of testing many random seeds (Dodge et al., 2020) with a prospective approach",
      "startOffset" : 92,
      "endOffset" : 112
    }, {
      "referenceID" : 16,
      "context" : "(2020) proposed a technique based on neural network dropout (Srivastava et al., 2014) for regularizing finetuned language models that involved stochastically mixing the parameters of multiple language models for the same domain, and further demonstrated the usefulness of pretrained weight decay over conventional weight decay for improving language model fine-tuning performance.",
      "startOffset" : 60,
      "endOffset" : 85
    }, {
      "referenceID" : 14,
      "context" : "Autoregressive language models such as GPT-2 (Radford et al., 2019) are trained",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 4,
      "context" : "In practice, pre-trained language models are often fine-tuned using a new corpus or transferred to a new task (Howard and Ruder, 2018).",
      "startOffset" : 110,
      "endOffset" : 134
    }, {
      "referenceID" : 0,
      "context" : "While random sampling methods are useful (Bottou, 1991), the stochasticity of context sampling suggests an avenue for additional improve-",
      "startOffset" : 41,
      "endOffset" : 55
    }, {
      "referenceID" : 2,
      "context" : "By filtering examples in this way, we aim to reduce the effect of variability in data order observed in previous work (Dodge et al., 2020), and improve",
      "startOffset" : 118,
      "endOffset" : 138
    }, {
      "referenceID" : 12,
      "context" : "the generalizability of our training set (Mosbach et al., 2021).",
      "startOffset" : 41,
      "endOffset" : 63
    }, {
      "referenceID" : 10,
      "context" : "Due to its intuitive similarity with notions in reinforcement learning (Mnih et al., 2013) of using a network to approximate the expected value of a given action, we abbreviate this normalized informativeness metric as a “Q-value” (Watkins and Dayan, 1992)",
      "startOffset" : 71,
      "endOffset" : 90
    }, {
      "referenceID" : 20,
      "context" : ", 2013) of using a network to approximate the expected value of a given action, we abbreviate this normalized informativeness metric as a “Q-value” (Watkins and Dayan, 1992)",
      "startOffset" : 148,
      "endOffset" : 173
    }, {
      "referenceID" : 23,
      "context" : "We tested these results on a standard Books dataset (Zhu et al., 2015), a “mixed” dataset which is composed of training examples from two corpora (the Books",
      "startOffset" : 52,
      "endOffset" : 70
    }, {
      "referenceID" : 5,
      "context" : "corpus and a corpus of scraped Reddit comments (Huth et al., 2016)), and the WikiText-103 dataset (Merity et al.",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 9,
      "context" : ", 2016)), and the WikiText-103 dataset (Merity et al., 2017).",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "In practice, our secondary learner, Q̂, represents the input text X by embedding it with 768dimensional byte-pair embeddings (Gage, 1994).",
      "startOffset" : 125,
      "endOffset" : 137
    }, {
      "referenceID" : 6,
      "context" : "We first compare IGF directly to standard finetuning, which we define as basic batched stochastic gradient descent with Adam (Kingma and Ba, 2015) using random samples from the target corpus.",
      "startOffset" : 125,
      "endOffset" : 146
    }, {
      "referenceID" : 1,
      "context" : "space with different directionality (BERT) (Devlin et al., 2019), and a different overall finetuning task (SST-2) (Socher et al.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 15,
      "context" : ", 2019), and a different overall finetuning task (SST-2) (Socher et al., 2013).",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : "Figure 8: Prospective IGF Is More Efficient Than Retrospective Random Seed Search: We show boxplots of the best run from differently-sized sets of runs to visualize the expected benefit of using random seed testing (Dodge et al., 2020) and compare it to the benefit of using IGF.",
      "startOffset" : 215,
      "endOffset" : 235
    } ],
    "year" : 2021,
    "abstractText" : "Language model fine-tuning is essential for modern natural language processing, but is computationally expensive and timeconsuming. Further, the effectiveness of finetuning is limited by the inclusion of training examples that negatively affect performance. Here we present a general fine-tuning method that we call information gain filtration for improving the overall training efficiency and final performance of language model fine-tuning. We define the information gain of an example as the improvement on a validation metric after training on that example. A secondary learner is then trained to approximate this quantity. During fine-tuning, this learner selects informative examples and skips uninformative ones. We show that our method has consistent improvement across datasets, finetuning tasks, and language model architectures. For example, we achieve a median perplexity of 54.0 on a books dataset compared to 57.3 for standard fine-tuning. We present statistical evidence that offers insight into the improvements of our method over standard finetuning. The generality of our method leads us to propose a new paradigm for language model fine-tuning — we encourage researchers to release pretrained secondary learners on common corpora to promote efficient and effective fine-tuning, thereby improving the performance and reducing the overall energy footprint of language model fine-tuning.",
    "creator" : "LaTeX with hyperref"
  }
}