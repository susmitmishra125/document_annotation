{
  "name" : "2021.acl-long.79.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Prosodic segmentation for parsing spoken dialogue",
    "authors" : [ "Elizabeth Nielsen", "Mark Steedman", "Sharon Goldwater" ],
    "emails" : [ "e.k.nielsen@sms.ed.ac.uk", "sgwater}@inf.ed.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 979–992\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n979"
    }, {
      "heading" : "1 Introduction",
      "text" : "Parsing spoken dialogue poses unique difficulties: spontaneous speech is full of disfluencies, including false starts, repetitions, and filled pauses. In addition, speech transcripts lack punctuation, which would otherwise help signal the boundaries of sentence-like units (SUs).1 Because of these difficulties, current parsers struggle to accurately parse\n1We follow Kahn et al. (2004) in using the term ‘sentencelike units’ rather than ‘sentences’ throughout, since conversational speech doesn’t always consist of syntactically complete sentences.\nEnglish speech transcripts, even when they handle other English text well. However, research has shown that prosody can help with at least one of these problems, improving parsing performance for speech that contains disfluencies (Tran et al., 2018, 2019). In this work, we hypothesize that incorporating prosodic features from the speech signal can actually help with both of these problems: not only parsing disfluent speech, but also parsing speech that isn’t segmented into SUs.\nOther researchers have augmented parsers with prosodic features, but always with the assumption that the parser has access to gold SU boundaries, which cannot be assumed in a deployed speech application. For example, Gregory et al. (2004); Kahn et al. (2005) and Hale et al. (2006) incorporated prosody into statistical parsers or parse rerankers, with mixed results. More recently, Tran et al. (2018) and Tran et al. (2019) found that prosody improved an end-to-end neural parser, with the most significant gains in disfluent sentences. Parsing without access to gold SU boundaries is much more difficult: Kahn and Ostendorf (2012) showed that parsing quality depends on the quality of the sentence segmentation. Furthermore, finding SU boundaries is not as simple as finding long pauses in speech, as we demonstrate below.\nWe hypothesize that access to prosodic features will help an English parser that has to both parse and correctly identify SU boundaries (which we call SU segmentation). We test this hypothesis by inputting entire dialog turns to a neural parser without gold SU boundaries. We call this the turnbased model, and compare it to an SU-based model, which assumes gold SU boundaries and parses one SU at a time. We use turns as our input unit because they resemble the input a dialog agent would receive from a user. Following Tran et al. (2019) and others, we use a human-generated gold transcript instead of an automatic speech recognition\n(ASR) transcript; we plan to use ASR output in future work.\nWe build on the work of Tran et al. (2018) and Tran et al. (2019), considering two different experimental conditions for each model: inputting text features only and inputting both text and prosodic features. Using the Switchboard corpus of English conversational dialogue, we find that when only transcripts are used, the turn-based parser performs considerably worse than the SU-based parser, which is not surprising given that it needs to perform two tasks instead of one. However, when prosodic features are included, there is no difference in performance between the turn-based and SU-based models, and both models outperform the text-only counterparts.\nOur primary contributions are:\n• We show that a parser that has access to prosody can perform both SU segmentation and parsing as well as a model that only has to parse.\n• We show that one difficultly for the prosodyfree turn-based model is that it confuses speech disfluencies with SU boundaries, as illustrated in Figure 1. Further analysis indicates that adding pitch and intensity features can help the model to disambiguate the two, while pause and duration features do not."
    }, {
      "heading" : "2 Background: prosody and syntax",
      "text" : "Prosodic signals divide speech into units (Pierrehumbert, 1980). The location and type of these prosodic units are determined by information structure (Steedman, 2000), disfluencies (Shriberg, 2001), and to some extent, syntax (Cutler et al., 1997). Some psycholinguistic research shows that in experimental conditions, speakers can use prosody to predict syntax — for example, that English speakers can use prosody to determine where to attach a modifier or prepositional phrase, or how to correctly group coordinands (e.g., Kjelgaard and Speer (1999); Speer et al. (1996); Warren et al. (1995)). However, Cutler et al. (1997) argues that English speakers often “fail to exploit” this prosodic information even when it is present, so it isn’t actually a signal for syntax in practice. Many computational linguists have experimented with this possible link between syntax and prosody by incorporating prosody into syntactic parsers (e.g., Noeth et al. (2000); Gregory et al. (2004); Kahn\net al. (2005); Tran et al. (2018)). These models have had mixed success: For example, Gregory et al. (2004) found that prosody was at best a neutral addition to their model, while Kahn et al. (2005) found that prosody helped rerank PCFG output.\nOne possible reason that prosody is only somewhat effective in previous research is that prosodic units below the level of the SU do not always coincide with traditional syntactic constituents (Selkirk, 1995, 1984).2 In fact, the only prosodic boundaries that consistently coincide with syntactic boundaries are the prosodic boundaries at the ends of SUs (Wagner and Watson, 2010). The prosodic boundaries at the end of SUs are more distinctive (i.e., tending to correspond to longer pauses and more distinctive pitch and intensity variations) and less likely appear in any other location. These features make prosody a reliable signal for SU boundaries, even though it is an unreliable signal for syntactic structure below the SU level.\nSome researcheres have used this correlation between prosody and SU boundaries to help in SU boundary detection. Examples of SU segmentation models that found prosodic cues were important include Gotoh and Renals (2000); Kolář et al. (2006); Kahn et al. (2004); Kahn and Ostendorf (2012), who all used traditional statistical models (e.g., HMMs, finite state machines, and decision trees), and Xu et al. (2014), who used a neural model. Kahn et al. (2004) and Kahn and Ostendorf (2012) also looked at downstream parsing accuracy on the same corpus we use. Like us, Kahn and Ostendorf (2012) don’t use gold SU boundaries, but direct comparison is impossible because they use ASR output instead of human transcriptions and a different metric for parse performance (SParseval; Roark et al. (2006)). However, they show that having access to gold SU boundaries increases the SParseval score from 78.5 to 82.3, which shows that parsing without gold SU boundaries is difficult.\nHowever, in some research areas, prosody is less frequently used for SU detection. Some ASR corpora and applications segment at relatively arbitrary boundaries such as long silences or even regular intervals (e.g., Jain et al. (2020)). Other applications, such as speech translation, do require syntactically coherent input, but even there, systems targeting SUs have often used only textual features (Sridhar et al., 2013; Wan et al., 2020).\n2We refer here to traditional constituency parsing; CCG (Steedman and Baldridge, 2011) proposes different syntactic constituents that coincide with prosodic units.\nSystems for restoring punctuation from ASR output must identify SU boundaries to correctly insert sentence-final punctuation, but these systems are typically evaluated on rehearsed monologues (such as TED talks) or read speech, which largely lack disfluencies (e.g., Federico et al. (2012)). Here, we show that prosody is primarily helpful for distinguishing SU boundaries from disfluencies, so although some of these systems have used prosody (e.g., Tilk and Alumäe (2016)), text-only systems are very competitive (e.g., Che et al. (2016); Alam et al. (2020)).\nEven when SU boundaries are already known, other research in parsing conversational speech has shown that prosody helps identify and correctly handle disfluencies. Tran et al. (2018) found that prosody only modestly affects parsing of fluent SUs, but has a marked effect on disfluent SUs. This accords with other previous work that has found that prosody is helpful in disfluency detection (Zayats and Ostendorf, 2019) We discuss the relationship between prosody and disfluencies in greater detail in Section 6, including how prosody helps the model not to confuse disfluencies and SU boundaries, as shown in Figure 1 above."
    }, {
      "heading" : "3 Task and data",
      "text" : "We use the American English corpus Switchboard NXT (henceforth SWBD-NXT) (Calhoun et al., 2010). We choose this corpus mainly so we can compare performance with Tran et al. (2018) and Tran et al. (2019), as well as other earlier proba-\nbilistic models such as Kahn et al. (2005). SWBDNXT comprises 642 dialogues between strangers conducted by telephone. These dialogues are transcribed and hand-annotated with Penn Treebankstyle constituency parses. We preprocess the transcripts to remove punctuation and lower-case all letters, making the input more like an ASR transcript that would be used in a deployed application.\nThe transcript divides the corpus into SUs and turns. Since these SUs may be sentences or other syntactically independent units such as sentence fragments, we use the generic term ‘sentence-like unit’ (SU). A turn is a contiguous span of speech by a single speaker. Turns are hand-annotated in SWBD-NXT, but for a deployed dialog agent, a turn is simply whatever contiguous input the user gives. Not all turns in the SWBD-NXT contain more than one SU: of a total 60.1k turns, 35.8k consist of a single SU. The remaining 24.3k contain more than one SU; the majority (52.4 percent) of these contain just two SUs. The average number of SUs per turn is 1.82.\nWe follow the general approach of Tran et al. (2018), but where they parse a single SU at a time, we give our parser a single dialog turn at a time for our turn-based model. The model returns constituency parses for the turn in the form of Penn Treebank (PTB)-style trees. In order to keep the output in the form of valid PTB trees, we add a top-level constituent, labelled TURN, to all turns, however many SUs they consist of. This example shows how the two sentences in (1) would be fused\ninto a single turn in (2):\n(1) Separate SUs: a. (S (NP Kim) (VP sings)) b. (S (NP Sidney) (VP dances))\n(2) Merged into a single turn: a. (TURN (S (NP Kim) (VP sings)) (S (NP\nSidney) (VP dances)))\nOf course, using turns instead of SUs leads to longer inputs. We experiment with a pipeline approach (first segmenting turns into SUs, then parsing) as well as an end-to-end approach. In the end-to-end approach, we can’t handle extremely long inputs since these longer sequences lead to high memory usage for transformers. We still want to capture the model’s behavior on generally longer inputs, so we filter out two problematically long turns from the training set (out of 49,294 turns). We do not have to remove any turns from the development or test sets. This leaves the maximum turn length at 270 tokens. We also remove any turns for which some or all speech features are missing from the corpus."
    }, {
      "heading" : "3.1 Feature extraction",
      "text" : "From the speech signal, we extract features for pauses between words, word duration, pitch, and intensity. We largely follow the feature extraction procedure outlined in Tran et al. (2018) and Tran et al. (2019), which we summarize here, noting any deviations from or additions to their procedure.\nPause features are extracted from the timealigned transcript. Each word’s pause feature corresponds to the pause follows it. Each pause is categorized into one of six bins by length in seconds: p > 1, 0.2 < p ≤ 1, 0.05 < p ≤ 0.2, 0 < p ≤ 0.05, p ≤ 0 (see below), and pauses where we are missing time-aligned data. Following Tran et al. (2018), the model learns 32-dimensional embeddings for each pause category.\nSince we use turns instead of SUs, we have to determine how to handle pauses at the beginnings and endings of turns. We decide to calculate pauses based on all words in the transcript, not just the words for a single speaker at a time. This means that at a turn boundary, we calculate the pause as the time between the end of one speaker’s turn and the beginning of the other speaker’s turn. If one speaker interrupts another, the pause duration has a negative value. We place these negative-valued\npauses in the same bin as pauses with length 0. Duration features are also extracted from the time-aligned transcript. We are interested in the relative lengthening or shortening of word tokens, so we normalize the raw duration of each token. Following the code base for Tran et al. (2019), we perform two different types of normalization. In the first case, we normalize the token’s raw duration by the mean duration of every instance of that word type. In the second, we normalize the token’s raw duration by the maximum duration of any word in the input unit (SU or turn). These two normalization methods result in two duration features for each word token, which are concatenated and input to the model.\nPitch features (or more accurately, F0 features) are extracted from the speech signal using Kaldi (Povey et al., 2011). These are extracted from 25ms frames every 10ms. Three pitch features are extracted: warped Normalized Cross Correlation Function (NCCF); log-pitch with mean subtraction over a 1.5-second window, weighted by Probability of Voicing (POV); and the estimated derivative of the raw log pitch. For further details on these features, see Ghahremani et al. (2014).\nIntensity features are also extracted from the speech signal using the same software and frame size as we use for pitch features. Starting with 40-dimensional mel-frequency filterbank features, we calculate three features: (1) the log of the total energy, normalized by the maximum total energy for the speaker over the course of the dialog; (2) the log of the total energy in the lower half of the 40 mel-frequency bands, normalized by the total energy; and (3) the log of the total energy in the upper half of the 40 mel-frequency bands, normalized by the total energy.\nFor training, development, and testing, we use the split described in Charniak and Johnson (2001), which is a standard split for experiments on SWBDNXT (e.g., Kahn et al. (2005); Tran et al. (2018)). The training set makes up 90 percent of the data, and the development and testing sets make up 5 percent each."
    }, {
      "heading" : "4 Model",
      "text" : "We use the parser described in Tran et al. (2019), directly extending the code base described in their paper.3 The model is a neural end-to-end constituency\n3Original: https://github.com/trangham283/prosody nlp; our extended code: https://github.com/ekayen/prosody nlp\nparser based on Kitaev and Klein (2018)’s textonly parser, with a transformer-based encoder and a chart-style decoder based on Stern et al. (2017) and Gaddy et al. (2018). This encoder-decoder is augmented with a CNN on the input side that handles prosodic features (Tran et al., 2019). For further description of the model and hyperparameters, see Appendices A.1 and A.2.\nThe text is encoded using 300-dimensional GloVe embeddings (Pennington et al., 2014).4 Of the four types of prosodic features described in Section 3, pause and duration features are already token-level. However, pitch and intensity features are extracted from the speech signal at the frame level. In order to map from these frame-level features to a token-level representation, the pitch and intensity features pass through a CNN, and are then concatenated with the token-level pause and duration features.\nWe follow Tran et al. (2019) in training each model 10 times with different random seeds. For the development set, we report the mean of these 10 models’ performance. We then select the median model by development set performance, and use it to calculate test set results. For any further experiments, such as those discussed in Section 6, we use the random seed for this median model. Each model is trained for 50 epochs and use the epoch with highest development set performance.\nIn addition to this end-to-end approach, we also report results for a pipeline approach. For the pipeline, we first segment the speech into SUs using a modified version of the parser architecture: We keep the encoder the same, but we change the decoder so that it only does sequence labelling, and we frame the SU segmentation task as a sequence labelling task. We then use the SU-based parser to parse the resulting SUs. We report the model’s performance with and without prosodic features during the segmentation and parsing steps."
    }, {
      "heading" : "5 Results",
      "text" : "We compare the turn-based F1 performance of our parser to a replication of the SU-based performance described in Tran et al. (2018) and Tran et al. (2019). Table 1 shows the development and test set results.5 We find that the turn-based model benefits significantly from prosody. The turn-based\n4See Appendix A.3 for results using BERT embeddings. 5We use PyEvalb to evaluate our parser’s performance, though we modify it so that it behaves identically to Evalb: https://github.com/ekayen/PYEVALB\nmodel performs equivalently well to the SU-based model, despite doing two tasks instead of one. The SU-based model also improves by 0.36 in F1 score on the test set with the addition of prosody. Note that while prosody has a considerably larger effect on the turn-based model than on the SU based model, the exact size of this change will depend on the corpus. For example, in a corpus with very few multi-SU turns, the performance change in the turnbased model might not be as large. However, our results suggest that prosody helps when a model needs to both detect SU boundaries and parse SUs.\nThe biggest difference between the SU- and turnbased models’ performance on this corpus is in the text-only scenario, where the turn-based parser is substantially worse. This is expected for a few reasons. First, the text-only turn-based parser encounters longer inputs. Longer inputs tend to lead to more parse errors simply because there are more ways to parse a longer string. Table 2 shows this correspondence between length and performance. The median length of turns in the development set is 9 tokens, while the median length of SUs is 6 tokens. Longer strings are also more likely to contain the things that make parsing difficult, namely disfluencies and SU boundaries.\nThe turn-based parser’s task is also more com-\nplex: it has to perform both SU segmentation and parsing, rather than parsing alone. This gives the turn-based parser novel ways to make errors by splitting a turn into the wrong number of SUs. However, prosody brings the turn-based parser up to the level of the SU-based parser, even though the turn-based model’s task is more complex. Table 5 shows how the text-only parser significantly overestimates the number of SU boundaries. Without prosody, the model achieves an F1 score of 63.74 on SU prediction on the development set, compared to 99.41 with prosody (see Table 3). The most comparable work on SWBD is Kahn and Ostendorf (2012), who achieved 78 F1 using a hidden-event model, where we use a much more powerful transformer model; however, their model used ASR transcripts as input, so these scores aren’t directly comparable.\nWe also test the pipeline model described in Section 4, which first segments turns into SUs and then parses them, both with and without prosody. We train just one segmentation model with the same random seed as the median development set model. We report the development set performance on segmentation (measured by segmentation F1 (Makhoul et al., 2000)) and parse F1 in Table 3.\nThe text+prosody pipeline model achieves an F1 score of 99.71, which is statistically indistinguishable from the end-to-end text+prosody model. In both cases, we see that the addition of prosody boosts SU segmentation accuracy to near-perfect levels, which explains why the parser performance is similar (and much better than without prosody).\nComparing the two text-only models reveals a more interesting pattern: while the pipeline model achieves much better segmentation F1, its parsing performance is worse. This is unexpected, as parsing and segmentation performance are usually correlated. This effect seems to arise because the two models err in different directions on segmentation: The pipeline model under-segments turns (corre-\nsponding to higher segmentation precision), while the end-to-end over-segments (higher recall, substantially lower precision). When it over-segments, the end-to-end text-only model often splits a word or short constituent off of an otherwise well-formed SU subtree; by contrast, the pipeline model tends to leave two or more SUs combined and and then to generate many SU-internal parsing errors. These SU-internal parsing errors include more coordination errors as well as VP, NP, and clause attachment errors than the end-to-end model.6 However, the pipeline model does as well as the end-to-end model at PP attachment and modifier attachment.\nOverall, these results show that a pipeline model can be as effective at parsing as an end-to-end one, but that including prosody is even more important for a pipeline model. Since we care about parsing performance and the end-to-end text-only model does much better at parsing, we use the end-to-end model for all remaining analyses."
    }, {
      "heading" : "5.1 Error types",
      "text" : "We use the Berkeley Parser Analyser (Kummerfeld et al., 2012) to determine what types of errors each of the SU-based and end-to-end turn-based models makes. Figure 2 summarizes the output of the Analyser. Overall, the SU-based parser shows only small effects from prosody, but the turn-based model does significantly worse on certain error types without prosody. Even for the turn-based model, prosody only affects error types that have to do with the shape of the tree. The different label category shows errors where two identically shaped trees have different constituent labels, and prosody has no effect on these.\nFor the turn-based model, poor SU segmentation by the text-only model explains some of the differences between the text+prosody and text-only models. Since 68.8 percent of SUs are clauses (i.e.,\n6We use the Berkeley Parser Analyser to analyze types of parse error (Kummerfeld et al., 2012).\nthey have a top node of type S, SBAR, SQ or SINV), an incorrect SU segmentation is usually classed as a clause attachment error. An example of this kind of attachment error can be seen in Appendix A.4. However, prosody also affects the turn-based model’s rate of NP, PP, and modifier attachment errors. Since these attachment errors are not as common in the text-only SU-based model, it seems likely that they are caused by a cascade effect from errors in top-level SU segmentation. Prosody also affects the turn-based model’s rate of unary errors (which are errors “involving unary productions that are not linked to a nearby error such as a matching extra or missing node”) and single word phrase errors (which are “a range of node errors that span a single word” but which are not related to other errors) (Kummerfeld et al., 2012). Finally, very modest differences are seen for two rare error types: NP-internal and VP attachment errors."
    }, {
      "heading" : "5.2 Effect of disfluencies",
      "text" : "Our turn-based model performs worse overall on disfluent turns than on fluent turns, which was also true of Tran et al. (2018)’s SU-based model. Prosody also leads to a greater gain in F1 for disfluent turns than for fluent turns. These differences in performance are shown in Table 4. The lower performance on disfluent sentences may be at least partially attributable to length differences: the median length of turns with disfluencies is 28 tokens, compared to 3 tokens for fluent turns, where we define a disfluent turn as any turn containing the constituent tag EDITED. As discussed in Section 5, longer input generally leads to more parser errors, meaning that disfluent sentences are more likely to cause parser errors. However, there are other reasons disfluencies are difficult for the turn-based model, as discussed in the following section."
    }, {
      "heading" : "6 Distinguishing disfluencies and SU boundaries",
      "text" : "One effect of disfluencies is that the text-only model tends to confuse certain kinds of disfluencies for SU boundaries, as illustrated in Figure 1. Table 5 shows that the text+prosody model largely avoids this confusion, and indeed can do so almost as well using only pitch or intensity features. However, models using only pause or duration features are not good at distinguishing disfluencies from SU boundaries and predict boundaries too often. These results largely concur with previous work describing the similarities and differences between prosodic features of disfluencies and SU boundaries (Shriberg, 2001; Wagner and Watson, 2010). In this section, we examine each of the features\nmore closely with respect to this previous work and our results, highlighting where our results do (and do not) accord with expectations.\nThe disfluencies that are relevant to this discussion include repetitions and restarts. Examples of these from SWBD-NXT are shown here, with bracketing added for clarity:\n(3) Spurious repetition: it [may] may be at this point Restart: [but it’s] but I think it’s relatively unimportant\nIn these examples, the text in square brackets is called the reparandum, which is immediately followed by the interruption point. Disfluencies in SWBD-NXT are marked in the constituency parse annotation, where the reparandum is marked as a constituent with the label EDITED. The interruption point is the right edge of this constituent.\nOur analysis draws on the work of Shriberg (2001), who described the prosodic features of the interruption point and the reparandum based on an analysis of three English conversational and taskbased dialogue corpora — the Switchboard Corpus (which we use a subset of), ATIS (Hirschman, 1992), and AMEX (Kowtko and Price, 1989).\nPauses. Although pauses may be the most intuitive potential cue to SU boundaries, previous work suggests that long pauses also characterize interruption points (Wagner and Watson, 2010; Shriberg, 2001). Indeed, our analysis shows that longer pauses (> 0.05s) are over-represented in both locations. If pause types were distributed uniformly, 16 percent of both SU boundaries and interruption\npoints would have a longer pause. Instead, we find that 33 percent of SUs boundaries and 37 percent of interruption points have such pauses. This explains why the pause-only model tends to confuse SU boundaries and interruption points.\nDuration. Shriberg (2001) found that both interruptions and SU boundaries are associated with lengthening of the immediately preceding syllable. Lengthening before the interruption point may occur even if there are no other prosodic cues to the disfluency, and can be “far greater” than at SU boundaries (Shriberg, 2001, 161). This type of lengthening is captured by our first duration feature, which measures the token duration normalized by the mean duration for its word type. Like Shriberg (2001), we find that words preceding SU boundaries are lengthened on average (normalized duration: 1.18), and those preceding interruption points even more so (normalized duration: 1.41). In principle, this extra lengthening could help the durationonly model distinguish SU boundaries from interruptions, but in practice the model is nearly as bad at distinguishing them as the text-only model.\nThe second duration feature is the token length normalized by the maximum length of any token in the input, to normalize for speaking rate. Initially, this feature looks helpful: SU-final words have mean value of 0.86, while words directly before the interruption point have a mean of 0.50. However, the feature mainly captures the number of phones in a word, since words with fewer phones — including English function words — tend to have shorter normalized duration. It turns out that function words occur more often before interruption points than before SU boundaries: using NLTK’s stopwords as a heuristic for function words, only 21.9 percent of development set SUs end in a function word, while the word before an interrutption point is a function word 51.6 percent of the time (Bird and Klein, 2009). Since the second duration feature captures a lexical distinction that is already signalled in the text, it cannot help the durationonly model outperform the text-only model.\nPitch. Based on previous work, our finding that pitch features are useful is not a surprise: the pitch contour before an interruption point is generally “flat or slowly falling” (Shriberg, 2001, 161), while SU boundaries are characterized by a boundary tone, generally corresponding to a fall or rise. Our model may be able to learn such temporal patterns, but even just looking at static pitch features re-\nveals differences between boundaries and interruptions for two of the three features. In particular, the mean warped NCCF value for pre-interruption point words is significantly higher than the value for SU-final words (p < 0.001), though somewhat lower than the overall average value across the development set. Meanwhile, the log-pitch with POV-weighted mean subtraction is significantly lower at interruption points than at SU boundaries (p < 0.01). These differences allow the pitch-only model to distinguish SU boundaries and interruption points much better than the pause- or durationonly models can (see Table 5). Of these two pitch features, log-pitch is a more direct indicator of fundamental frequency (F0), which suggests that average perceived pitch is likely lower before disfluencies than before SU boundaries. There could be several reasons for this difference. For example, it could be that the “flat or slowly falling” tone of disfluencies that Shriberg (2001) describes has a lower average value than SU boundaries which can have either a fall or a rise (e.g., for certain kinds of questions). However, examining pitch features across the whole corpus obscures more subtle distinctions such as different types of pitch contours.\nIntensity. We find that intensity features alone are enough to distinguish SU boundaries from interruption points, which is interesting because intensity has not been previously identified as an important cue: Shriberg (2001) doesn’t note any particularly distinctive intensity features of the reparandum or interruption point, and work by Kim et al. (2006) on the Switchboard Corpus suggests that SU boundaries are correlated to lower intensity in some speakers, but that this isn’t consistent across speakers. The three intensity features correspond to overall energy, energy in the lower half of frequencies, and energy in the higher frequencies. SUfinal words have a significantly higher mean value for lower-frequency intensity than all other words (p < 0.001), while words before the interruption point do not. This systematic difference in one intensity feature seems to be part of how intensity features allow the model to consistently tell SU boundaries apart from disfluencies.\nOverall performance. Given our claim that the main issue facing the text-only turn-based parser is distinguishing disfluencies from SU boundaries, it is not surprising that the two features that do best at this, pitch and intensity, also yield the highest overall performance. Results are shown in Table 6."
    }, {
      "heading" : "7 Conclusion",
      "text" : "Our experiments show that parsing English speech transcriptions without gold SU boundaries is difficult for our parser: Its F1 score drops by about 4 percentage points compared to a model with gold SU boundaries. Incorrect SU segmentation causes a large part of this damage, though other errors in tree construction also play a role. We show that we can undo this damage by giving our parser prosodic information. Importantly, prosody helps by allowing the parser to distinguish disfluencies from SU boundaries. These results argue for giving prosodic information to parsers in deployed applications, where no SU boundary annotations are available, including dialog agents.\nFurthermore, our experiments show that even limited prosodic features help a great deal: for our English data, pitch information alone is not significantly worse than pitch, intensity, pause, and word duration information combined. This means that incorporating the right kind of prosodic information can potentially lead to significant gains."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We are very grateful to Trang Tran for help answering questions related to her code and to Mari Ostendorf for conversations that helped inspire this paper. We would like to thank Korin Richmond, the ACL reviewers, and members of the AGORA research group at the University of Edinburgh for their feedback. This work was supported by funding from Huawei and the project SEMANTAX, which received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. 742137)."
    }, {
      "heading" : "A Appendices",
      "text" : ""
    }, {
      "heading" : "A.1 Model description",
      "text" : "The parser is an encoder-decoder model that takes both speech and text inputs. In this appendix, we describe the three main model components: the CNN that processes the continuous speech inputs before they reach the encoder, the transformerbased encoder, and the chart-style decoder."
    }, {
      "heading" : "A.1.1 The speech-processing CNN",
      "text" : "Of the four prosodic features, pause and duration are already discrete at the token level. Pitch and intensity, however, are extracted from frames every 10 ms in the original speech signal. If a given token is shorter than a fixed number of frames, some frames of left and right context are included; frames from longer tokens are subsampled to reduce their frame length. These two frame-based features features have a different dimensionality than the token-level input and they are untenably long for a sequence model or transformer. The CNN solves both these problems by producing a fixed-length representation for each feature at the token level. This representation can be concatenated with the other token-level features and input to the encoder.\nFor a speech input with f frames, the raw features input to the CNN have dimensions 6 × f , where 6 is the number of total features for each frame (3 pitch features and 3 intensity features). Several filters of different sizes then perform onedimensional convolution of the input. These different filters allow the CNN to integrate information on various time scales. We apply N of each of these m filters, for a total of mN filters. We use the hyperparameters described by Tran et al. (2018): N = 32 filters of widths w = [5, 10, 25, 50], for a total of mN = 128 filters. The output of each filter is then max-pooled, which converts the features for a given token to a uniform dimension.\nThese CNN-processed features are then concatenated with the token-level prosodic features (pause and duration) and the text embedding for the token, and then input to the encoder. The CNN is trained along with the encoder-decoder model."
    }, {
      "heading" : "A.1.2 The encoder",
      "text" : "The encoder is a standard transformer with eight attention heads, based on the work of Kitaev and Klein (2018). For each word of input xi, the transformer encoder produces a representation of the forward context, −→yi , and the backward context←−yi . We represent a given span between indices i and j by subtracting the forward representations and backward representations and concatenating the results:\nv(i,j) = [ −→yj −−→yi ;←−yj −←−yi ]\nThe next section explains how we use this span representation v(i,j) to generate scores for constituents in a tree."
    }, {
      "heading" : "A.1.3 The decoder",
      "text" : "The decoder is a chart-style span-based decoder. Its goal is to output the correct tree T for an input x1, ..., xn. Each tree’s score S(T ) is simply the sum of the scores of its constituents, where each constituent is defined by a start index i, an end index j, and a label l.\nStree(T ) = ∑\ni,j,label∈T Slabel(i, j, l) + Sspan(i, j)\nAs this formula for tree score shows, each constituent’s score is made up of a label score and span score. Conceptually, the span score corresponds to the probability that a constituent exists that exactly\ncovers span (i, j) in the input; the label score reflects the probability that the span (i, j) has a given constituent label (e.g., S, NP). The decoder must have a way of determining the label score and span score for each constituent.\nThe label scores are generated by passing the span representation v(i,j) through a two-layer feedforward network like the feed-forward networs Vaswani et al. (2017) use:\nFFN(x) = W2(relu(W1x + b1)) + b2\nFollowing Kitaev and Klein (2018), we also include a layer normalization step (LNorm). This feedforward network produces a vector for each span Slabel(i, j) whose size is the number of possible labels:\nSlabel(i, j) = M2(relu(LNorm(M1v(i,j))+c1))+c2\nThe lth element of this vector is the score for the label l:\nSlabel(i, j, l) = [Slabel(i, j, )]l\nWe also need to calculate the span score, but calculating the score for all spans (i, j) would be prohibitively inefficient. Instead, Kitaev and Klein (2018), following the approach of Stern et al. (2017) and Gaddy et al. (2018), use a dynamic programming strategy based on the CKY algorithm. The score for a span (i, j) is calculated in terms of the scores of its subspans, which allows span scores to be built up recursively from the stored scores of smaller spans. A given span (i, j) can be split at any internal point into two subspans, (i, k) and (k, j). Each of these possible splits (i, k, j) is assigned a score, calculated by summing the span scores of the subspans:\nSsplit(i, k, j) = Sspan(i, k) + Sspan(k, j)\nThen, to find the best score for this span (i, j), we find the label and split that maximize the following sum:\nSbest(i, j) = max l,k [Slabel(i, j, l) + Ssplit(i, k, j)]\nAll spans are recursively split into subspans, eventually arriving at single-word spans. Since there are no splits possible for a single-word span, the score for a single word span is simply that word’s best label score:\nSbest(i, i + 1) = max l [Slabel(i, i + 1, l)]\nThis method requires that the grammar be in Chomsky-Normal form, which the model achieves by collapsing strings of unary rules and using dummy nodes to make n-ary rules into binary rules.\nWith this method of generating tree scores from span representations, we can then define the hinge loss for our predicted tree T̂ compared to the gold tree T∗, where ∆ represents the Hamming loss on labeled spans:\nLoss(T̂ , T∗) = max[0,max\nT [∆(T̂ , T∗) + Stree(T̂ )]\n− Stree(T∗)]\nWe then use this loss function to train our encoder-decoder, including the CNN input module for speech."
    }, {
      "heading" : "A.2 Model training details",
      "text" : "We used the hyperparameters specified in (Tran et al., 2019)’s code base, documented in Table 7. Each model was trained for 50 epochs on a single Nvidia GTX 1080 GPU, which took approximately 7 hours per model. The text-only models have approximately 23M trainable parameters each, while the text+prosody models have approximately 20M trainable parameters.\nA.3 Incorporating BERT We include here the results for both the SU- and turn-based parsers when given BERT embeddings (Devlin et al., 2019) in place of GloVE embeddings (Pennington et al., 2014). We train one model for each experimental condition, using the random seed we used to generate the results shown in Table 1. We see in Table 8 that BERT improves the\nperformance in all experimental conditions. The SU-based text+prosody parser does outperform the turn-based parser by a statistically significant margin, though this result was obtained on just one model instead of 10 randomly seeded models. However, the turn-based parser’s performance remains quite close to the SU-based parser’s despite having a more difficult task to perform, and otherwise the basic pattern from the GloVE results holds here."
    }, {
      "heading" : "A.4 Clause attachment illustration",
      "text" : "Figure 3 illustrates an example of an error classified as a clause attachment error by the Berkeley Parser Analyser (Kummerfeld et al., 2012)."
    } ],
    "references" : [ {
      "title" : "Punctuation restoration using transformer models for high-and low-resource languages",
      "author" : [ "Tanvirul Alam", "Akib Khan", "Firoj Alam." ],
      "venue" : "Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020), pages 132–142, Online. Associ-",
      "citeRegEx" : "Alam et al\\.,? 2020",
      "shortCiteRegEx" : "Alam et al\\.",
      "year" : 2020
    }, {
      "title" : "Natural Language Processing with Python",
      "author" : [ "Edward Loper Bird", "Steven", "Ewan Klein." ],
      "venue" : "O’Reilly Media Inc.",
      "citeRegEx" : "Bird et al\\.,? 2009",
      "shortCiteRegEx" : "Bird et al\\.",
      "year" : 2009
    }, {
      "title" : "The NXT-format Switchboard Corpus: A rich resource for investigating the syntax, semantics, pragmatics and prosody of dialogue",
      "author" : [ "Sasha Calhoun", "Jean Carletta", "Jason Brenier", "Neil Mayo", "Dan Jurafsky", "Mark Steedman", "David Beaver." ],
      "venue" : "Lan-",
      "citeRegEx" : "Calhoun et al\\.,? 2010",
      "shortCiteRegEx" : "Calhoun et al\\.",
      "year" : 2010
    }, {
      "title" : "Edit detection and parsing for transcribed speech",
      "author" : [ "Eugene Charniak", "Mark Johnson." ],
      "venue" : "Second Meeting of the North American Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Charniak and Johnson.,? 2001",
      "shortCiteRegEx" : "Charniak and Johnson.",
      "year" : 2001
    }, {
      "title" : "Punctuation prediction for unsegmented transcript based on word vector",
      "author" : [ "Xiaoyin Che", "Cheng Wang", "Haojin Yang", "Christoph Meinel." ],
      "venue" : "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC",
      "citeRegEx" : "Che et al\\.,? 2016",
      "shortCiteRegEx" : "Che et al\\.",
      "year" : 2016
    }, {
      "title" : "European Language Resources Association (ELRA)",
      "author" : [ "Paris", "France" ],
      "venue" : null,
      "citeRegEx" : "2016. et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "2016. et al\\.",
      "year" : 2016
    }, {
      "title" : "Prosody in the comprehension of spoken language: A literature review",
      "author" : [ "Anne Cutler", "Delphine Dahan", "Wilma van Donselaar." ],
      "venue" : "Language and Speech, 40(2):141–201.",
      "citeRegEx" : "Cutler et al\\.,? 1997",
      "shortCiteRegEx" : "Cutler et al\\.",
      "year" : 1997
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "The iwslt 2011 evaluation campaign on automatic talk translation",
      "author" : [ "Marcello Federico", "Sebastian Stüker", "Luisa Bentivogli", "Michael Paul", "Mauro Cettolo", "Teresa Herrmann", "Jan Niehues", "Giovanni Moretti." ],
      "venue" : "International Conference on Language Re-",
      "citeRegEx" : "Federico et al\\.,? 2012",
      "shortCiteRegEx" : "Federico et al\\.",
      "year" : 2012
    }, {
      "title" : "What’s going on in neural constituency parsers? an analysis",
      "author" : [ "David Gaddy", "Mitchell Stern", "Dan Klein." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Gaddy et al\\.,? 2018",
      "shortCiteRegEx" : "Gaddy et al\\.",
      "year" : 2018
    }, {
      "title" : "A pitch extraction algorithm tuned for automatic speech recognition",
      "author" : [ "P. Ghahremani", "B. BabaAli", "D. Povey", "K. Riedhammer", "J. Trmal", "S. Khudanpur." ],
      "venue" : "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "Ghahremani et al\\.,? 2014",
      "shortCiteRegEx" : "Ghahremani et al\\.",
      "year" : 2014
    }, {
      "title" : "Sentence boundary detection in broadcast speech transcripts",
      "author" : [ "Yoshihiko Gotoh", "Steve Renals." ],
      "venue" : "ASR2000-Automatic Speech Recognition: Challenges for the new Millenium ISCA Tutorial and Research Workshop (ITRW).",
      "citeRegEx" : "Gotoh and Renals.,? 2000",
      "shortCiteRegEx" : "Gotoh and Renals.",
      "year" : 2000
    }, {
      "title" : "Sentence-internal prosody does not help parsing the way punctuation does",
      "author" : [ "Michelle Gregory", "Mark Johnson", "Eugene Charniak." ],
      "venue" : "Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Com-",
      "citeRegEx" : "Gregory et al\\.,? 2004",
      "shortCiteRegEx" : "Gregory et al\\.",
      "year" : 2004
    }, {
      "title" : "PCFGs with syntactic and prosodic indicators of speech repairs",
      "author" : [ "John Hale", "Izhak Shafran", "Lisa Yung", "Bonnie J. Dorr", "Mary Harper", "Anna Krasnyanskaya", "Matthew Lease", "Yang Liu", "Brian Roark", "Matthew Snover", "Robin Stewart." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Hale et al\\.,? 2006",
      "shortCiteRegEx" : "Hale et al\\.",
      "year" : 2006
    }, {
      "title" : "Multi-site data collection for a spoken language corpus",
      "author" : [ "Lynette Hirschman." ],
      "venue" : "Proceedings of the Workshop on Speech and Natural Language, HLT ’91, page 7–14, USA. Association for Computational Linguistics.",
      "citeRegEx" : "Hirschman.,? 1992",
      "shortCiteRegEx" : "Hirschman.",
      "year" : 1992
    }, {
      "title" : "Contextual RNN-T for Open Domain ASR",
      "author" : [ "Mahaveer Jain", "Gil Keren", "Jay Mahadeokar", "Geoffrey Zweig", "Florian Metze", "Yatharth Saraf." ],
      "venue" : "Proc. Interspeech 2020, pages 11–15.",
      "citeRegEx" : "Jain et al\\.,? 2020",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2020
    }, {
      "title" : "Effective use of prosody in parsing conversational speech",
      "author" : [ "Jeremy G. Kahn", "Matthew Lease", "Eugene Charniak", "Mark Johnson", "Mari Ostendorf." ],
      "venue" : "Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Lan-",
      "citeRegEx" : "Kahn et al\\.,? 2005",
      "shortCiteRegEx" : "Kahn et al\\.",
      "year" : 2005
    }, {
      "title" : "Joint reranking of parsing and word recognition with automatic segmentation",
      "author" : [ "Jeremy G. Kahn", "Mari Ostendorf." ],
      "venue" : "Computer Speech and Language, 26(1):1 – 19.",
      "citeRegEx" : "Kahn and Ostendorf.,? 2012",
      "shortCiteRegEx" : "Kahn and Ostendorf.",
      "year" : 2012
    }, {
      "title" : "Parsing conversational speech using enhanced segmentation",
      "author" : [ "Jeremy G. Kahn", "Mari Ostendorf", "Ciprian Chelba." ],
      "venue" : "Proceedings of HLT-NAACL 2004, pages 125–128, Boston, Massachusetts, USA. Association for Computational Linguistics.",
      "citeRegEx" : "Kahn et al\\.,? 2004",
      "shortCiteRegEx" : "Kahn et al\\.",
      "year" : 2004
    }, {
      "title" : "Acoustic differentiation of l- and l-l% in Switchboard and radio news speech",
      "author" : [ "Heejin Kim", "Tae jin Yoon", "Jennifer Cole", "Mark Hasegawa-johnson." ],
      "venue" : "Proceedings of Speech Prosody 2006.",
      "citeRegEx" : "Kim et al\\.,? 2006",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2006
    }, {
      "title" : "Constituency parsing with a self-attentive encoder",
      "author" : [ "Nikita Kitaev", "Dan Klein." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2676–2686, Melbourne, Australia. Associa-",
      "citeRegEx" : "Kitaev and Klein.,? 2018",
      "shortCiteRegEx" : "Kitaev and Klein.",
      "year" : 2018
    }, {
      "title" : "Prosodic facilitation and interference in the resolution of temporary syntactic closure ambiguity",
      "author" : [ "Margaret M. Kjelgaard", "Shari R. Speer." ],
      "venue" : "Journal of Memory and Language, 40(2):153 – 194.",
      "citeRegEx" : "Kjelgaard and Speer.,? 1999",
      "shortCiteRegEx" : "Kjelgaard and Speer.",
      "year" : 1999
    }, {
      "title" : "Using prosody for automatic sentence segmentation of multi-party meetings",
      "author" : [ "Jáchym Kolář", "Elizabeth Shriberg", "Yang Liu." ],
      "venue" : "International Conference on Text, Speech and Dialogue, pages 629–636. Springer.",
      "citeRegEx" : "Kolář et al\\.,? 2006",
      "shortCiteRegEx" : "Kolář et al\\.",
      "year" : 2006
    }, {
      "title" : "Data collection and analysis in the air travel planning domain",
      "author" : [ "Jacqueline C. Kowtko", "Patti J. Price." ],
      "venue" : "Speech and Natural Language: Proceedings of a Workshop Held at Cape Cod, Massachusetts, October 15-18, 1989.",
      "citeRegEx" : "Kowtko and Price.,? 1989",
      "shortCiteRegEx" : "Kowtko and Price.",
      "year" : 1989
    }, {
      "title" : "Parser showdown at the wall street corral: An empirical investigation of error types in parser output",
      "author" : [ "Jonathan K. Kummerfeld", "David Hall", "James R. Curran", "Dan Klein." ],
      "venue" : "Proceedings of the 2012 Joint Conference on EMNLP and CoNLL,",
      "citeRegEx" : "Kummerfeld et al\\.,? 2012",
      "shortCiteRegEx" : "Kummerfeld et al\\.",
      "year" : 2012
    }, {
      "title" : "Performance measures for information extraction",
      "author" : [ "John Makhoul", "Francis Kubala", "Richard Schwartz", "Ralph Weischedel." ],
      "venue" : "Proceedings of DARPA Broadcast News Workshop.",
      "citeRegEx" : "Makhoul et al\\.,? 2000",
      "shortCiteRegEx" : "Makhoul et al\\.",
      "year" : 2000
    }, {
      "title" : "Verbmobil: The use of prosody in the linguistic components of a speech understanding system",
      "author" : [ "Elmar Noeth", "Anton Batliner", "Andreas Kießling", "Ralf Kompe", "Heinrich Niemann." ],
      "venue" : "IEEE Transactions on Speech and Audio processing, 8(5):519–532.",
      "citeRegEx" : "Noeth et al\\.,? 2000",
      "shortCiteRegEx" : "Noeth et al\\.",
      "year" : 2000
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "EMNLP, pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "The phonology and phonetics of English intonation",
      "author" : [ "Janet Breckenridge Pierrehumbert." ],
      "venue" : "Ph.D. thesis, Massachusetts Institute of Technology.",
      "citeRegEx" : "Pierrehumbert.,? 1980",
      "shortCiteRegEx" : "Pierrehumbert.",
      "year" : 1980
    }, {
      "title" : "The Kaldi speech recognition toolkit",
      "author" : [ "Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz", "Jan Silovsky", "Georg Stemmer", "Karel Vesely" ],
      "venue" : null,
      "citeRegEx" : "Povey et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Povey et al\\.",
      "year" : 2011
    }, {
      "title" : "Phonology and Syntax",
      "author" : [ "Elisabeth Selkirk." ],
      "venue" : "MIT Press, Cambridge, MA.",
      "citeRegEx" : "Selkirk.,? 1984",
      "shortCiteRegEx" : "Selkirk.",
      "year" : 1984
    }, {
      "title" : "Sentence prosody: Intonation, stress, and phrasing",
      "author" : [ "Elisabeth Selkirk." ],
      "venue" : "The handbook of phonological theory, 1:550–569.",
      "citeRegEx" : "Selkirk.,? 1995",
      "shortCiteRegEx" : "Selkirk.",
      "year" : 1995
    }, {
      "title" : "To ’errrr’ is human: Ecology and acoustics of speech disfluencies",
      "author" : [ "Elizabeth Shriberg." ],
      "venue" : "Journal of the International Phonetic Association, 31:153 – 169.",
      "citeRegEx" : "Shriberg.,? 2001",
      "shortCiteRegEx" : "Shriberg.",
      "year" : 2001
    }, {
      "title" : "The influence of prosodic structure on the resolution of temporary syntactic closure ambiguities",
      "author" : [ "Shari Speer", "Margaret Kjelgaard", "Kathryn Dobroth." ],
      "venue" : "Journal of psycholinguistic research, 25:249–71.",
      "citeRegEx" : "Speer et al\\.,? 1996",
      "shortCiteRegEx" : "Speer et al\\.",
      "year" : 1996
    }, {
      "title" : "Segmentation strategies for streaming speech translation",
      "author" : [ "Vivek Kumar Rangarajan Sridhar", "John Chen", "Srinivas Bangalore", "Andrej Ljolje", "Rathinavelu Chengalvarayan." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the",
      "citeRegEx" : "Sridhar et al\\.,? 2013",
      "shortCiteRegEx" : "Sridhar et al\\.",
      "year" : 2013
    }, {
      "title" : "Information structure and the syntax-phonology interface",
      "author" : [ "Mark Steedman." ],
      "venue" : "Linguistic inquiry, 31(4):649–689.",
      "citeRegEx" : "Steedman.,? 2000",
      "shortCiteRegEx" : "Steedman.",
      "year" : 2000
    }, {
      "title" : "Combinatory Categorial Grammar",
      "author" : [ "Mark Steedman", "Jason Baldridge." ],
      "venue" : "Robert Borsley and Kersti Börjars, editors, Non-Transformational Syntax: A Guide to Current Models, pages 181–224. Blackwell, Oxford.",
      "citeRegEx" : "Steedman and Baldridge.,? 2011",
      "shortCiteRegEx" : "Steedman and Baldridge.",
      "year" : 2011
    }, {
      "title" : "A minimal span-based neural constituency parser",
      "author" : [ "Mitchell Stern", "Jacob Andreas", "Dan Klein." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 818–827, Vancouver, Canada.",
      "citeRegEx" : "Stern et al\\.,? 2017",
      "shortCiteRegEx" : "Stern et al\\.",
      "year" : 2017
    }, {
      "title" : "Bidirectional recurrent neural network with attention mechanism for punctuation restoration",
      "author" : [ "Ottokar Tilk", "Tanel Alumäe." ],
      "venue" : "Interspeech 2016, pages 3047–3051.",
      "citeRegEx" : "Tilk and Alumäe.,? 2016",
      "shortCiteRegEx" : "Tilk and Alumäe.",
      "year" : 2016
    }, {
      "title" : "Parsing speech: a neural approach to integrating lexical and acoustic-prosodic information",
      "author" : [ "Trang Tran", "Shubham Toshniwal", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu", "Mari Ostendorf." ],
      "venue" : "Proceedings of the 2018 Conference of the North American",
      "citeRegEx" : "Tran et al\\.,? 2018",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2018
    }, {
      "title" : "On the Role of Style in Parsing Speech with Neural Models",
      "author" : [ "Trang Tran", "Jiahong Yuan", "Yang Liu", "Mari Ostendorf." ],
      "venue" : "Proc. Interspeech 2019, pages 4190–4194.",
      "citeRegEx" : "Tran et al\\.,? 2019",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Experimental and theoretical advances in prosody: A review",
      "author" : [ "Michael Wagner", "Duane G. Watson." ],
      "venue" : "Language and Cognitive Processes, 25(79):905–945.",
      "citeRegEx" : "Wagner and Watson.,? 2010",
      "shortCiteRegEx" : "Wagner and Watson.",
      "year" : 2010
    }, {
      "title" : "Subtitles to segmentation: Improving low-resource speech-to-TextTranslation pipelines",
      "author" : [ "David Wan", "Zhengping Jiang", "Chris Kedzie", "Elsbeth Turcan", "Peter Bell", "Kathy McKeown." ],
      "venue" : "Proceedings of the workshop on Cross-Language Search and",
      "citeRegEx" : "Wan et al\\.,? 2020",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2020
    }, {
      "title" : "Prosody, phonology and parsing in closure ambiguities",
      "author" : [ "Paul Warren", "Esther Grabe", "Francis Nolan." ],
      "venue" : "Language and Cognitive Processes, 10(5):457– 486.",
      "citeRegEx" : "Warren et al\\.,? 1995",
      "shortCiteRegEx" : "Warren et al\\.",
      "year" : 1995
    }, {
      "title" : "A deep neural network approach for sentence boundary detection in broadcast news",
      "author" : [ "Chenglin Xu", "Lei Xie", "Guangpu Huang", "Xiong Xiao", "Eng Siong Chng", "Haizhou Li." ],
      "venue" : "INTERSPEECH-2014, pages 2887–2891.",
      "citeRegEx" : "Xu et al\\.,? 2014",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2014
    }, {
      "title" : "Giving attention to the unexpected: Using prosody innovations in disfluency detection",
      "author" : [ "Vicky Zayats", "Mari Ostendorf." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics, Volume 1,",
      "citeRegEx" : "Zayats and Ostendorf.,? 2019",
      "shortCiteRegEx" : "Zayats and Ostendorf.",
      "year" : 2019
    }, {
      "title" : "These different filters allow the CNN to integrate information on various time scales. We apply N of each of these m filters, for a total of mN filters",
      "author" : [ "Tran" ],
      "venue" : null,
      "citeRegEx" : "Tran,? \\Q2018\\E",
      "shortCiteRegEx" : "Tran",
      "year" : 2018
    }, {
      "title" : "2018), we also include a layer normalization step (LNorm). This feedforward network produces a vector for each span Slabel(i, j) whose size is the number of possible labels",
      "author" : [ "Kitaev", "Klein" ],
      "venue" : null,
      "citeRegEx" : "Kitaev and Klein,? \\Q2018\\E",
      "shortCiteRegEx" : "Kitaev and Klein",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 39,
      "context" : "Previous work has shown that prosody can help with parsing disfluent speech (Tran et al., 2018), but has assumed that the input to the parser is already segmented into sentence-like units (SUs), which isn’t true in existing speech applications.",
      "startOffset" : 76,
      "endOffset" : 95
    }, {
      "referenceID" : 16,
      "context" : "We follow Kahn et al. (2004) in using the term ‘sentencelike units’ rather than ‘sentences’ throughout, since conversational speech doesn’t always consist of syntactically complete sentences.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 39,
      "context" : "More recently, Tran et al. (2018) and Tran et al. (2019) found that prosody improved an end-to-end neural parser, with the most significant gains in disfluent sentences.",
      "startOffset" : 15,
      "endOffset" : 57
    }, {
      "referenceID" : 17,
      "context" : "Parsing without access to gold SU boundaries is much more difficult: Kahn and Ostendorf (2012) showed that parsing quality depends on the quality of the sentence segmentation.",
      "startOffset" : 69,
      "endOffset" : 95
    }, {
      "referenceID" : 39,
      "context" : "Following Tran et al. (2019) and others, we use a human-generated gold transcript instead of an automatic speech recognition",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 39,
      "context" : "We build on the work of Tran et al. (2018) and Tran et al.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 39,
      "context" : "We build on the work of Tran et al. (2018) and Tran et al. (2019), considering two different exper-",
      "startOffset" : 24,
      "endOffset" : 66
    }, {
      "referenceID" : 28,
      "context" : "Prosodic signals divide speech into units (Pierrehumbert, 1980).",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 35,
      "context" : "The location and type of these prosodic units are determined by information structure (Steedman, 2000), disfluencies (Shriberg, 2001), and to some extent, syntax (Cutler et al.",
      "startOffset" : 86,
      "endOffset" : 102
    }, {
      "referenceID" : 32,
      "context" : "The location and type of these prosodic units are determined by information structure (Steedman, 2000), disfluencies (Shriberg, 2001), and to some extent, syntax (Cutler et al.",
      "startOffset" : 117,
      "endOffset" : 133
    }, {
      "referenceID" : 6,
      "context" : "The location and type of these prosodic units are determined by information structure (Steedman, 2000), disfluencies (Shriberg, 2001), and to some extent, syntax (Cutler et al., 1997).",
      "startOffset" : 162,
      "endOffset" : 183
    }, {
      "referenceID" : 6,
      "context" : "The location and type of these prosodic units are determined by information structure (Steedman, 2000), disfluencies (Shriberg, 2001), and to some extent, syntax (Cutler et al., 1997). Some psycholinguistic research shows that in experimental conditions, speakers can use prosody to predict syntax — for example, that English speakers can use prosody to determine where to attach a modifier or prepositional phrase, or how to correctly group coordinands (e.g., Kjelgaard and Speer (1999); Speer et al.",
      "startOffset" : 163,
      "endOffset" : 488
    }, {
      "referenceID" : 6,
      "context" : "The location and type of these prosodic units are determined by information structure (Steedman, 2000), disfluencies (Shriberg, 2001), and to some extent, syntax (Cutler et al., 1997). Some psycholinguistic research shows that in experimental conditions, speakers can use prosody to predict syntax — for example, that English speakers can use prosody to determine where to attach a modifier or prepositional phrase, or how to correctly group coordinands (e.g., Kjelgaard and Speer (1999); Speer et al. (1996); Warren et al.",
      "startOffset" : 163,
      "endOffset" : 509
    }, {
      "referenceID" : 6,
      "context" : "The location and type of these prosodic units are determined by information structure (Steedman, 2000), disfluencies (Shriberg, 2001), and to some extent, syntax (Cutler et al., 1997). Some psycholinguistic research shows that in experimental conditions, speakers can use prosody to predict syntax — for example, that English speakers can use prosody to determine where to attach a modifier or prepositional phrase, or how to correctly group coordinands (e.g., Kjelgaard and Speer (1999); Speer et al. (1996); Warren et al. (1995)).",
      "startOffset" : 163,
      "endOffset" : 531
    }, {
      "referenceID" : 6,
      "context" : "The location and type of these prosodic units are determined by information structure (Steedman, 2000), disfluencies (Shriberg, 2001), and to some extent, syntax (Cutler et al., 1997). Some psycholinguistic research shows that in experimental conditions, speakers can use prosody to predict syntax — for example, that English speakers can use prosody to determine where to attach a modifier or prepositional phrase, or how to correctly group coordinands (e.g., Kjelgaard and Speer (1999); Speer et al. (1996); Warren et al. (1995)). However, Cutler et al. (1997) argues that English speakers often “fail to exploit” this prosodic information even when it is present, so it isn’t actually a signal for syntax in practice.",
      "startOffset" : 163,
      "endOffset" : 563
    }, {
      "referenceID" : 6,
      "context" : "The location and type of these prosodic units are determined by information structure (Steedman, 2000), disfluencies (Shriberg, 2001), and to some extent, syntax (Cutler et al., 1997). Some psycholinguistic research shows that in experimental conditions, speakers can use prosody to predict syntax — for example, that English speakers can use prosody to determine where to attach a modifier or prepositional phrase, or how to correctly group coordinands (e.g., Kjelgaard and Speer (1999); Speer et al. (1996); Warren et al. (1995)). However, Cutler et al. (1997) argues that English speakers often “fail to exploit” this prosodic information even when it is present, so it isn’t actually a signal for syntax in practice. Many computational linguists have experimented with this possible link between syntax and prosody by incorporating prosody into syntactic parsers (e.g., Noeth et al. (2000); Gregory et al.",
      "startOffset" : 163,
      "endOffset" : 894
    }, {
      "referenceID" : 6,
      "context" : "The location and type of these prosodic units are determined by information structure (Steedman, 2000), disfluencies (Shriberg, 2001), and to some extent, syntax (Cutler et al., 1997). Some psycholinguistic research shows that in experimental conditions, speakers can use prosody to predict syntax — for example, that English speakers can use prosody to determine where to attach a modifier or prepositional phrase, or how to correctly group coordinands (e.g., Kjelgaard and Speer (1999); Speer et al. (1996); Warren et al. (1995)). However, Cutler et al. (1997) argues that English speakers often “fail to exploit” this prosodic information even when it is present, so it isn’t actually a signal for syntax in practice. Many computational linguists have experimented with this possible link between syntax and prosody by incorporating prosody into syntactic parsers (e.g., Noeth et al. (2000); Gregory et al. (2004); Kahn et al.",
      "startOffset" : 163,
      "endOffset" : 917
    }, {
      "referenceID" : 6,
      "context" : "The location and type of these prosodic units are determined by information structure (Steedman, 2000), disfluencies (Shriberg, 2001), and to some extent, syntax (Cutler et al., 1997). Some psycholinguistic research shows that in experimental conditions, speakers can use prosody to predict syntax — for example, that English speakers can use prosody to determine where to attach a modifier or prepositional phrase, or how to correctly group coordinands (e.g., Kjelgaard and Speer (1999); Speer et al. (1996); Warren et al. (1995)). However, Cutler et al. (1997) argues that English speakers often “fail to exploit” this prosodic information even when it is present, so it isn’t actually a signal for syntax in practice. Many computational linguists have experimented with this possible link between syntax and prosody by incorporating prosody into syntactic parsers (e.g., Noeth et al. (2000); Gregory et al. (2004); Kahn et al. (2005); Tran et al.",
      "startOffset" : 163,
      "endOffset" : 937
    }, {
      "referenceID" : 6,
      "context" : "The location and type of these prosodic units are determined by information structure (Steedman, 2000), disfluencies (Shriberg, 2001), and to some extent, syntax (Cutler et al., 1997). Some psycholinguistic research shows that in experimental conditions, speakers can use prosody to predict syntax — for example, that English speakers can use prosody to determine where to attach a modifier or prepositional phrase, or how to correctly group coordinands (e.g., Kjelgaard and Speer (1999); Speer et al. (1996); Warren et al. (1995)). However, Cutler et al. (1997) argues that English speakers often “fail to exploit” this prosodic information even when it is present, so it isn’t actually a signal for syntax in practice. Many computational linguists have experimented with this possible link between syntax and prosody by incorporating prosody into syntactic parsers (e.g., Noeth et al. (2000); Gregory et al. (2004); Kahn et al. (2005); Tran et al. (2018)).",
      "startOffset" : 163,
      "endOffset" : 957
    }, {
      "referenceID" : 6,
      "context" : "The location and type of these prosodic units are determined by information structure (Steedman, 2000), disfluencies (Shriberg, 2001), and to some extent, syntax (Cutler et al., 1997). Some psycholinguistic research shows that in experimental conditions, speakers can use prosody to predict syntax — for example, that English speakers can use prosody to determine where to attach a modifier or prepositional phrase, or how to correctly group coordinands (e.g., Kjelgaard and Speer (1999); Speer et al. (1996); Warren et al. (1995)). However, Cutler et al. (1997) argues that English speakers often “fail to exploit” this prosodic information even when it is present, so it isn’t actually a signal for syntax in practice. Many computational linguists have experimented with this possible link between syntax and prosody by incorporating prosody into syntactic parsers (e.g., Noeth et al. (2000); Gregory et al. (2004); Kahn et al. (2005); Tran et al. (2018)). These models have had mixed success: For example, Gregory et al. (2004) found that prosody was at best a neutral addition to their model, while Kahn et al.",
      "startOffset" : 163,
      "endOffset" : 1031
    }, {
      "referenceID" : 6,
      "context" : "The location and type of these prosodic units are determined by information structure (Steedman, 2000), disfluencies (Shriberg, 2001), and to some extent, syntax (Cutler et al., 1997). Some psycholinguistic research shows that in experimental conditions, speakers can use prosody to predict syntax — for example, that English speakers can use prosody to determine where to attach a modifier or prepositional phrase, or how to correctly group coordinands (e.g., Kjelgaard and Speer (1999); Speer et al. (1996); Warren et al. (1995)). However, Cutler et al. (1997) argues that English speakers often “fail to exploit” this prosodic information even when it is present, so it isn’t actually a signal for syntax in practice. Many computational linguists have experimented with this possible link between syntax and prosody by incorporating prosody into syntactic parsers (e.g., Noeth et al. (2000); Gregory et al. (2004); Kahn et al. (2005); Tran et al. (2018)). These models have had mixed success: For example, Gregory et al. (2004) found that prosody was at best a neutral addition to their model, while Kahn et al. (2005) found that prosody helped rerank PCFG output.",
      "startOffset" : 163,
      "endOffset" : 1122
    }, {
      "referenceID" : 42,
      "context" : "2 In fact, the only prosodic boundaries that consistently coincide with syntactic boundaries are the prosodic boundaries at the ends of SUs (Wagner and Watson, 2010).",
      "startOffset" : 140,
      "endOffset" : 165
    }, {
      "referenceID" : 11,
      "context" : "Examples of SU segmentation models that found prosodic cues were important include Gotoh and Renals (2000); Kolář et al.",
      "startOffset" : 83,
      "endOffset" : 107
    }, {
      "referenceID" : 16,
      "context" : "(2006); Kahn et al. (2004); Kahn and Ostendorf (2012), who all used traditional statistical models (e.",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 16,
      "context" : "(2006); Kahn et al. (2004); Kahn and Ostendorf (2012), who all used traditional statistical models (e.",
      "startOffset" : 8,
      "endOffset" : 54
    }, {
      "referenceID" : 17,
      "context" : "Like us, Kahn and Ostendorf (2012) don’t use gold SU boundaries, but direct comparison is impossible because they use ASR output instead of human transcriptions and a different metric for parse performance (SParseval; Roark et al.",
      "startOffset" : 9,
      "endOffset" : 35
    }, {
      "referenceID" : 34,
      "context" : "Other applications, such as speech translation, do require syntactically coherent input, but even there, systems targeting SUs have often used only textual features (Sridhar et al., 2013; Wan et al., 2020).",
      "startOffset" : 165,
      "endOffset" : 205
    }, {
      "referenceID" : 43,
      "context" : "Other applications, such as speech translation, do require syntactically coherent input, but even there, systems targeting SUs have often used only textual features (Sridhar et al., 2013; Wan et al., 2020).",
      "startOffset" : 165,
      "endOffset" : 205
    }, {
      "referenceID" : 36,
      "context" : "We refer here to traditional constituency parsing; CCG (Steedman and Baldridge, 2011) proposes different syntactic constituents that coincide with prosodic units.",
      "startOffset" : 55,
      "endOffset" : 85
    }, {
      "referenceID" : 37,
      "context" : ", Tilk and Alumäe (2016)), text-only systems are very competitive (e.",
      "startOffset" : 2,
      "endOffset" : 25
    }, {
      "referenceID" : 46,
      "context" : "This accords with other previous work that has found that prosody is helpful in disfluency detection (Zayats and Ostendorf, 2019) We discuss the relationship between prosody and disfluencies in greater detail in Section 6, including how prosody helps the model not to confuse disfluencies and SU boundaries, as shown in Figure 1 above.",
      "startOffset" : 101,
      "endOffset" : 129
    }, {
      "referenceID" : 39,
      "context" : "Tran et al. (2018) found that prosody only modestly affects parsing of fluent SUs, but has a marked effect on disfluent SUs.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 2,
      "context" : "We use the American English corpus Switchboard NXT (henceforth SWBD-NXT) (Calhoun et al., 2010).",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 2,
      "context" : "We use the American English corpus Switchboard NXT (henceforth SWBD-NXT) (Calhoun et al., 2010). We choose this corpus mainly so we can compare performance with Tran et al. (2018) and Tran et al.",
      "startOffset" : 74,
      "endOffset" : 180
    }, {
      "referenceID" : 2,
      "context" : "We use the American English corpus Switchboard NXT (henceforth SWBD-NXT) (Calhoun et al., 2010). We choose this corpus mainly so we can compare performance with Tran et al. (2018) and Tran et al. (2019), as well as other earlier probabilistic models such as Kahn et al.",
      "startOffset" : 74,
      "endOffset" : 203
    }, {
      "referenceID" : 2,
      "context" : "We use the American English corpus Switchboard NXT (henceforth SWBD-NXT) (Calhoun et al., 2010). We choose this corpus mainly so we can compare performance with Tran et al. (2018) and Tran et al. (2019), as well as other earlier probabilistic models such as Kahn et al. (2005). SWBD-",
      "startOffset" : 74,
      "endOffset" : 277
    }, {
      "referenceID" : 39,
      "context" : "We follow the general approach of Tran et al. (2018), but where they parse a single SU at a time, we give our parser a single dialog turn at a time for our turn-based model.",
      "startOffset" : 34,
      "endOffset" : 53
    }, {
      "referenceID" : 39,
      "context" : "We largely follow the feature extraction procedure outlined in Tran et al. (2018) and Tran et al.",
      "startOffset" : 63,
      "endOffset" : 82
    }, {
      "referenceID" : 39,
      "context" : "We largely follow the feature extraction procedure outlined in Tran et al. (2018) and Tran et al. (2019), which we summarize here, noting any deviations from or additions to their procedure.",
      "startOffset" : 63,
      "endOffset" : 105
    }, {
      "referenceID" : 39,
      "context" : "Following Tran et al. (2018), the model learns 32-dimensional embeddings for each pause category.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 39,
      "context" : "Following the code base for Tran et al. (2019), we perform two different types of normalization.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 29,
      "context" : "Pitch features (or more accurately, F0 features) are extracted from the speech signal using Kaldi (Povey et al., 2011).",
      "startOffset" : 98,
      "endOffset" : 118
    }, {
      "referenceID" : 10,
      "context" : "features, see Ghahremani et al. (2014). Intensity features are also extracted from the speech signal using the same software and frame size as we use for pitch features.",
      "startOffset" : 14,
      "endOffset" : 39
    }, {
      "referenceID" : 3,
      "context" : "For training, development, and testing, we use the split described in Charniak and Johnson (2001), which is a standard split for experiments on SWBDNXT (e.",
      "startOffset" : 70,
      "endOffset" : 98
    }, {
      "referenceID" : 3,
      "context" : "For training, development, and testing, we use the split described in Charniak and Johnson (2001), which is a standard split for experiments on SWBDNXT (e.g., Kahn et al. (2005); Tran et al.",
      "startOffset" : 70,
      "endOffset" : 178
    }, {
      "referenceID" : 3,
      "context" : "For training, development, and testing, we use the split described in Charniak and Johnson (2001), which is a standard split for experiments on SWBDNXT (e.g., Kahn et al. (2005); Tran et al. (2018)).",
      "startOffset" : 70,
      "endOffset" : 198
    }, {
      "referenceID" : 39,
      "context" : "We use the parser described in Tran et al. (2019), directly extending the code base described in their paper.",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 40,
      "context" : "This encoder-decoder is augmented with a CNN on the input side that handles prosodic features (Tran et al., 2019).",
      "startOffset" : 94,
      "endOffset" : 113
    }, {
      "referenceID" : 27,
      "context" : "The text is encoded using 300-dimensional GloVe embeddings (Pennington et al., 2014).",
      "startOffset" : 59,
      "endOffset" : 84
    }, {
      "referenceID" : 19,
      "context" : "983 parser based on Kitaev and Klein (2018)’s textonly parser, with a transformer-based encoder and a chart-style decoder based on Stern et al.",
      "startOffset" : 20,
      "endOffset" : 44
    }, {
      "referenceID" : 19,
      "context" : "983 parser based on Kitaev and Klein (2018)’s textonly parser, with a transformer-based encoder and a chart-style decoder based on Stern et al. (2017) and Gaddy et al.",
      "startOffset" : 20,
      "endOffset" : 151
    }, {
      "referenceID" : 9,
      "context" : "(2017) and Gaddy et al. (2018). This encoder-decoder is augmented with a CNN on the input side that handles prosodic features (Tran et al.",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 39,
      "context" : "We follow Tran et al. (2019) in training each model 10 times with different random seeds.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 39,
      "context" : "We compare the turn-based F1 performance of our parser to a replication of the SU-based performance described in Tran et al. (2018) and Tran et al.",
      "startOffset" : 113,
      "endOffset" : 132
    }, {
      "referenceID" : 39,
      "context" : "We compare the turn-based F1 performance of our parser to a replication of the SU-based performance described in Tran et al. (2018) and Tran et al. (2019). Table 1 shows the development and test set results.",
      "startOffset" : 113,
      "endOffset" : 155
    }, {
      "referenceID" : 17,
      "context" : "The most comparable work on SWBD is Kahn and Ostendorf (2012), who achieved 78 F1 using a hidden-event model, where we use a much more powerful trans-",
      "startOffset" : 36,
      "endOffset" : 62
    }, {
      "referenceID" : 25,
      "context" : "We report the development set performance on segmentation (measured by segmentation F1 (Makhoul et al., 2000)) and parse F1 in Table 3.",
      "startOffset" : 87,
      "endOffset" : 109
    }, {
      "referenceID" : 24,
      "context" : "We use the Berkeley Parser Analyser (Kummerfeld et al., 2012) to determine what types of errors each of the SU-based and end-to-end turn-based models makes.",
      "startOffset" : 36,
      "endOffset" : 61
    }, {
      "referenceID" : 24,
      "context" : "We use the Berkeley Parser Analyser to analyze types of parse error (Kummerfeld et al., 2012).",
      "startOffset" : 68,
      "endOffset" : 93
    }, {
      "referenceID" : 24,
      "context" : "Error types are classified by the Berkeley Parser Analyzer (Kummerfeld et al., 2012).",
      "startOffset" : 59,
      "endOffset" : 84
    }, {
      "referenceID" : 39,
      "context" : "on disfluent turns than on fluent turns, which was also true of Tran et al. (2018)’s SU-based model.",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 32,
      "context" : "These results largely concur with previous work describing the similarities and differences between prosodic features of disfluencies and SU boundaries (Shriberg, 2001; Wagner and Watson, 2010).",
      "startOffset" : 152,
      "endOffset" : 193
    }, {
      "referenceID" : 42,
      "context" : "These results largely concur with previous work describing the similarities and differences between prosodic features of disfluencies and SU boundaries (Shriberg, 2001; Wagner and Watson, 2010).",
      "startOffset" : 152,
      "endOffset" : 193
    }, {
      "referenceID" : 14,
      "context" : "Our analysis draws on the work of Shriberg (2001), who described the prosodic features of the interruption point and the reparandum based on an analysis of three English conversational and taskbased dialogue corpora — the Switchboard Corpus (which we use a subset of), ATIS (Hirschman, 1992), and AMEX (Kowtko and Price, 1989).",
      "startOffset" : 274,
      "endOffset" : 291
    }, {
      "referenceID" : 23,
      "context" : "Our analysis draws on the work of Shriberg (2001), who described the prosodic features of the interruption point and the reparandum based on an analysis of three English conversational and taskbased dialogue corpora — the Switchboard Corpus (which we use a subset of), ATIS (Hirschman, 1992), and AMEX (Kowtko and Price, 1989).",
      "startOffset" : 302,
      "endOffset" : 326
    }, {
      "referenceID" : 42,
      "context" : "Although pauses may be the most intuitive potential cue to SU boundaries, previous work suggests that long pauses also characterize interruption points (Wagner and Watson, 2010; Shriberg, 2001).",
      "startOffset" : 152,
      "endOffset" : 193
    }, {
      "referenceID" : 32,
      "context" : "Although pauses may be the most intuitive potential cue to SU boundaries, previous work suggests that long pauses also characterize interruption points (Wagner and Watson, 2010; Shriberg, 2001).",
      "startOffset" : 152,
      "endOffset" : 193
    }, {
      "referenceID" : 30,
      "context" : "Our analysis draws on the work of Shriberg (2001), who described the prosodic features of the interruption point and the reparandum based on an analysis of three English conversational and taskbased dialogue corpora — the Switchboard Corpus (which we use a subset of), ATIS (Hirschman, 1992), and AMEX (Kowtko and Price, 1989).",
      "startOffset" : 34,
      "endOffset" : 50
    }, {
      "referenceID" : 32,
      "context" : "Shriberg (2001) found that both interruptions and SU boundaries are associated with lengthening of the immediately preceding syllable.",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 32,
      "context" : "Shriberg (2001) found that both interruptions and SU boundaries are associated with lengthening of the immediately preceding syllable. Lengthening before the interruption point may occur even if there are no other prosodic cues to the disfluency, and can be “far greater” than at SU boundaries (Shriberg, 2001, 161). This type of lengthening is captured by our first duration feature, which measures the token duration normalized by the mean duration for its word type. Like Shriberg (2001), we find that words preceding SU boundaries are lengthened on average (normalized dura-",
      "startOffset" : 0,
      "endOffset" : 491
    }, {
      "referenceID" : 32,
      "context" : "For example, it could be that the “flat or slowly falling” tone of disfluencies that Shriberg (2001) describes has a lower average value than SU boundaries which can have either a fall or a rise (e.",
      "startOffset" : 85,
      "endOffset" : 101
    }, {
      "referenceID" : 32,
      "context" : "We find that intensity features alone are enough to distinguish SU boundaries from interruption points, which is interesting because intensity has not been previously identified as an important cue: Shriberg (2001) doesn’t note any partic-",
      "startOffset" : 199,
      "endOffset" : 215
    }, {
      "referenceID" : 19,
      "context" : "ularly distinctive intensity features of the reparandum or interruption point, and work by Kim et al. (2006) on the Switchboard Corpus suggests that SU boundaries are correlated to lower intensity in some speakers, but that this isn’t consistent across speakers.",
      "startOffset" : 91,
      "endOffset" : 109
    } ],
    "year" : 2021,
    "abstractText" : "Parsing spoken dialogue poses unique difficulties, including disfluencies and unmarked boundaries between sentence-like units. Previous work has shown that prosody can help with parsing disfluent speech (Tran et al., 2018), but has assumed that the input to the parser is already segmented into sentence-like units (SUs), which isn’t true in existing speech applications. We investigate how prosody affects a parser that receives an entire dialogue turn as input (a turn-based model), instead of gold standard pre-segmented SUs (an SUbased model). In experiments on the English Switchboard corpus, we find that when using transcripts alone, the turn-based model has trouble segmenting SUs, leading to worse parse performance than the SU-based model. However, prosody can effectively replace gold standard SU boundaries: with prosody, the turn-based model performs as well as the SUbased model (90.79 vs. 90.65 F1 score, respectively), despite performing two tasks (SU segmentation and parsing) rather than one (parsing alone). Analysis shows that pitch and intensity features are the most important for this corpus, since they allow the model to correctly distinguish an SU boundary from a speech disfluency — a distinction that the model otherwise struggles to make.",
    "creator" : "LaTeX with hyperref"
  }
}