{
  "name" : "2021.acl-long.469.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "TGEA: An Error-Annotated Dataset and Benchmark Tasks for TextGeneration from Pretrained Language Models",
    "authors" : [ "Jie He", "Bo Peng", "Yi Liao", "Qun Liu", "Deyi Xiong" ],
    "emails" : [ "dyxiong}@tju.edu.cn,", "qun.liu}@huawei.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6012–6025\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6012\nIn order to deeply understand the capability of pretrained language models in text generation and conduct a diagnostic evaluation, we propose TGEA1, an error-annotated dataset with multiple benchmark tasks for text generation from pretrained language models (PLMs). We use carefully selected prompt words to guide GPT-2 to generate candidate sentences, from which we select 47K for error annotation. Crowdsourced workers manually check each of these sentences and detect 12k erroneous sentences. We create an error taxonomy to cover 24 types of errors occurring in these erroneous sentences according to the nature of errors with respect to linguistics and knowledge (e.g., common sense). For each erroneous span in PLM-generated sentences, we also detect another span that is closely associated with it. Each error is hence manually labeled with comprehensive annotations, including the span of the error, the associated span, minimal correction to the error, the type of the error, and rationale behind the error. Apart from the fully annotated dataset, we also present a detailed description of the data collection procedure, statistics and analysis of the dataset. This is the first dataset with comprehensive annotations for PLM-generated texts, which facilitates the diagnostic evaluation of PLM-based text generation. Furthermore, we use TGEA as a benchmark dataset and propose a series of automatic diagnosis tasks, including error detection, error type classification, associated span detection, error rationale generation, to further promote future study on the automatic error detection and correction on texts generated by pretrained language models.\n∗Equal Contributions. 1The The dataset is available at\nhttps://download.mindspore.cn/dataset/TGEA/."
    }, {
      "heading" : "1 Introduction",
      "text" : "Pretrained language models (Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2020; Brown et al., 2020), which are trained on a huge amount of data via self-supervised learning, have made remarkable progress on both natural language understanding (NLU) (Wang et al., 2018, 2019) and natural language generation (NLG) (Liu and Lapata, 2019; Weng et al., 2020; Cao et al., 2020).\nOn several NLU datasets, PLM-based neural models have gradually achieved human-level performance in terms of automatic evaluation metrics (e.g., accuracy, F1) (He et al., 2020; Zhang et al., 2021). In order to deeply understand and analyze the capability of PLMs on NLU, a variety of more challenging NLU datasets have been proposed (Warstadt et al., 2020; Cui et al., 2020a; Jain et al., 2020; Talmor et al., 2020). These datasets can be used not only to obtain knowledge on how PLM-based models work and what they learn, but also to define new NLU tasks and to serve as a benchmark for future progress. For example, evaluating and analyzing PLM-based models on learning document structures with a carefully created benchmark test suite (Chen et al., 2019), helps to develop new methods to enhance the capability of these models on discourse modeling (Iter et al., 2020). Knowing the weakness of current PLM-based models in commonsense reasoning (Zhou et al., 2020) has inspired people to develop various reasoning datasets (Cui et al., 2020a; Zhang et al., 2020b).\nOn the other hand, state-of-the-art PLMs are able to generate texts that are even not distinguishable from human-written texts by human evaluators (Radford et al., 2019; Brown et al., 2020). This makes us curious about the capability of PLMs on text generation. Are they really reaching humanlevel performance on text generation? In contrast to the studies of PLMs on NLU, research on the\ncapability of PLMs on NLG is quite limited, especially in dataset building and diagnostic evaluation of text generation errors.\nIn this paper, in order to recognize the perimeter of text generation capability of PLMs, we propose TGEA, an error-annotated dataset with multiple benchmark tasks for text generation from pretrained language models. The original raw data are collected from texts generated by a Chinese GPT-2 model. The entire data collection and annotation procedure is visualized in Figure 1. The goals and contributions of building TGEA are as follows.\n• TGEA, to the best of our knowledge, is the first dataset built on machine-generated texts from state-of-the-art pretrained language models with rich annotations. The key interest of this dataset is detecting and annotating text generation errors from PLMs. Therefore it is different from conventional text generation datasets (e.g., Multi-News (Fabbri et al., 2019), TextCaps (Sidorov et al., 2020)) that are constructed to train models to learn text generation (e.g., generating texts from images or long documents). It is also different from grammatical error correction (GEC) datasets (Zhao et al., 2018; Flachs et al., 2020) that are built from human-written texts usually by second language learners. • TGEA provides rich semantic information for text generation errors, including error types, associated text spans, error corrections and rationals behind errors, as shown in Figure 1. Marking text spans that are closely related to erroneous words allows us to detect longdistance dependencies of errors or reasoning chains related to errors. Rationales behind errors directly explain why errors are annotated. All these error-centered manual annotations not only increase the interpretability of our dataset, but also facilitate a comprehensive diagnostic evaluation of pretrained language models on text generation. • We created an error taxonomy for TGEA, which covers 24 error types in a two-level hierarchy. With this error taxonomy, we not only obtain a high agreement on manual error annotation but also recognize the strengths and weaknesses of GPT-2 on text generation by estimating a distribution over these 24 error types. Comparing our dataset with GEC datasets, we find that humans and GPT-2 have\na very different error distribution, especially on errors related to commonsense reasoning. • TGEA not only exhibits text generation errors from pretrained language models, but also can serve as a dataset to train various models to automatically detect and correct these errors, like GEC datasets for training models to automatically correct human errors. We define 5 benchmark tasks over our dataset, i.e., erroneous sentence detection, erroneous span and associated span detection, error type classification, error correction and error rationale generation. For all these tasks, we provide experimental results using state-of-the-art models as baselines."
    }, {
      "heading" : "2 Related Work",
      "text" : "Our work is related to GEC datasets in error annotation and correction (machine vs. human errors). It is also partially related to commonsense reasoning datasets that have been proposed recently in that our dataset includes commonsense reasoning errors and rationales behind these errors. Our dataset is not related to conventional text generation datasets (Vougiouklis et al., 2017; Wiseman et al., 2017; Parikh et al., 2020) for training text generation models. A comprehensive comparison to GEC datasets and commonsense reasoning datasets is shown in Table 1."
    }, {
      "heading" : "2.1 Grammatical Error Correction Datasets",
      "text" : "FCE (Yannakoudakis et al., 2011) is an early largescale English grammatical error correction dataset, where raw texts are produced by English learners taking the First Certificate in English exams. AESW (Daudaravicius et al., 2016) is a GEC dataset from a professional editing company. In addition to common grammatical errors, AESW covers style issues as it contains texts mainly from scholarly papers. JFLEG (Napoles et al., 2017) is a GEC dataset built from TOFEL Exams, which does not force annotators to make minimal edits, preferring holistic fluency rewrites. CMEG (Napoles et al., 2019) is different from general grammatical error correction datasets with texts from second language learners. It uses articles or blogs (e.g., Wiki, Yahoo)) written by native English speakers to explore grammatical error phenomena in different domains. CWEB (Flachs et al., 2020) also uses website texts in English, such as blogs. The difference between CWEB and CMEG is that the percentage of erroneous tokens in the former is smaller than the latter as the purpose of CWEB is to study grammatical error correction in low error density domains. CGEC (Zhao et al., 2018) is a large-scale Chinese grammatical error correction dataset, derived from wrong sentences written by Chinese learners in the process of learning Chinese as a second language.\nIn addition to the difference in text sources (i.e., human-written vs. machine-generated), other significant differences between our dataset and existing GEC datasets are that our dataset contains commonsense reasoning errors and provides associated text span annotations and rationales for errors, as shown in Table 1."
    }, {
      "heading" : "2.2 Commonsense Datasets",
      "text" : "A variety of commonsense datasets have been proposed. Roemmele et al. (2011) introduce COPA that focuses on commonsense causal reasoning. Levesque et al. (2012) present Winograd Scheme Challenge (WSC), a dataset testing commonsense reasoning in the form of anaphora resolution. Winogrande, a larger version of WSC, is introduced by Sakaguchi et al. (2020), which contains ∼ 44, 000 examples. Winowhy (Zhang et al., 2020a) asks annotators to provide reasons for their decisions to WSC. In this aspect, the differences of our dataset from Winowhy are twofold. First, we provide reasons for errors rather than correct decisions to anaphora. Second, we provide reasons for all text generation errors, rather than only errors related to commonsense reasoning.\nIn addition to COPA and WSC-style datasets, many large crowdsourced datasets have been also proposed recently. CommonsenseQA (Talmor et al., 2019), a commonsense question answering dataset, has been constructed from ConceptNet. HellaSwag (Zellers et al., 2019b) and Abductive NLI (Bhagavatula et al., 2020) evaluate commonsense reasoning in the form of natural language inference. CosmosQA (Huang et al., 2019) is a dataset with multi-choice questions that require commonsense reading comprehension.\nBeyond datasets for evaluating commonsense reasoning, there are other datasets providing commonsense knowledge. PIQA (Bisk et al., 2020) focuses on physical commonsense knowledge while SocialIQA (Sap et al., 2019) on social commonsense knowledge.\nCommonsense datasets in multiple languages or languages other than English have also been created recently. XCOPA (Ponti et al., 2020) is a multilingual dataset for causal commonsense reasoning in 11 typologically different languages. Chinese\ncommonsense datasets, such as Mandarinograd (Bernard and Han, 2020) consisting of 154 Chinese Winograd scheme examples and CLUEWSC2020 (Xu et al., 2020) containing 1838 Winograd scheme examples, have been proposed.\nIn the aspect of commonsense reasoning, our dataset is different from the mentioned commonsense datasets in that we detect and annotate errors in machine-generated texts, which violates common sense, rather than creating examples to examine the commonsense reasoning ability of machines."
    }, {
      "heading" : "3 Dataset Creation",
      "text" : ""
    }, {
      "heading" : "3.1 Error Taxonomy",
      "text" : "Before crowdsourced workers manually annotate errors in machine-generated texts, we need to create an error taxonomy for such error coding. Three principles are used to guide the design of the error taxonomy: coverage, exclusiveness and easiness. The coverage rule requires that the error system can cover almost all different types of errors in machine-generated texts. The exclusiveness requirement indicates that each error type is not overlapping with other error types in the taxonomy. The final easiness principle means that the error coding system is easy to be used by annotators. With these three principles and aid from a linguist, we created an error taxonomy in a two-level hierarchy, which was revised in our pre-annotation stage.\nThe first level of the error taxonomy includes 5 error types.\n• Inappropriate combination. This type of errors suggests that two words/phrases are syntactically or lexically inappropriately com-\nbined in a sentence. Such errors include not only lexical collocation errors but also longdistance syntactic constituency combination errors (e.g., inappropriate subject-object combination). This error type is similar to “replacing” error in some GEC datasets (e.g., CWEB (Flachs et al., 2020)) as one element of an inappropriate combination should be usually replaced with other expressions. As we want to find text spans associated with erroneous words/phrases, we term this error type as “inappropriate combination”. We further divide this error type into five subtypes at the second level. • Missing. Grammatical constituencies or words are missing. 5 subtypes are defined under this error type. • Redundancy. Words or phrases are unnecessary. 5 subtypes are also defined. • Discourse Error. This error type is defined for inter-sentential cohesion/coherence errors (e.g., coreference errors, incorrect discourse connectives). • Commonsense Error. This error code is for errors related to commonsense reasoning. We divide this error type into 8 subtypes according to the type of commonsense knowledge type required (e.g., time, spatial, number).\nAll other errors that cannot be categorized into the aforementioned error types are grouped into “Other”. Table 2 displays examples for the above defined error types. 24 error subtypes are displayed in Figure 2 and examples of these subtypes are shown in Appendix."
    }, {
      "heading" : "3.2 Machine-Generated Text Collection",
      "text" : "Raw texts in our dataset are collected from a pretrained Chinese GPT-2 (NEZHA-Gen)2, which generates texts according to a system prompt. NEZHAGen has 12 layers and 12 attention heads and is trained on Chinese Wikipedia and news data (see Appendix for more details on the hyperparameters of NEZHA-Gen). As it is easy for NEZHA-Gen to generate high-quality texts with high-frequency prompt words, we create a list of prompt words according to their frequency to guarantee that there are sufficient erroneous sentences in collected raw texts. By doing so, we have found that GPT has a better chance to generate wrong sentences with such prompts. Specifically, we have randomly sampled 2M sentences from the data used to train NEZHA-Gen. The sampled sentences are then word-segmented and POS-tagged by Baidu LAC tool3 (Jiao et al., 2018). We then select and sort nouns in a descending order according to their frequencies in the sampled corpus. Nouns ranking in the range of top [40%, 60%] are selected as prompts.\nWe further filter out noisy texts from texts generated with these selected prompts. Noisy texts are either texts containing no more than 15 characters or texts where Chinese characters account for less 70% of all characters."
    }, {
      "heading" : "3.3 Error Annotation",
      "text" : "There are 5 stages in error annotation, as shown in Figure 1. We introduce each of them in this subsection.\n(1) Erroneous text detection. Texts generated by NEZHA-Gen with prompt words are present to annotators one by one. The first stage of annotation is hence to detect erroneous texts for subsequent annotations. Corresponding tags are annotated for texts being manually checked.\n(2) Erroneous and associated span detection. The next task for annotators is to detect erroneous and associated text spans in detected erroneous texts. For erroneous span detection, as a text may contain several spans that can be edited or the text can be corrected in different ways, which span should be regarded as erroneous is closely related to the way that we correct the text. Therefore, the basic principle that guides the annotation of erro-\n2github.com/huawei-noah/Pretrained-LanguageModel/tree/master/NEZHA-Gen-TensorFlow\n3github.com/baidu/lac\nneous spans is also the rule that we use for error correction: making minimal edits, which is also used in GEC datasets (Flachs et al., 2020; Napoles et al., 2017). In addition to the minimal edit principle, we also provide the following specific rules for annotators:\n• If annotators feel that a text is ambiguous and that it is difficult to correct the text, the text can be discarded without any further annotations. • If there are several spans that can be edited, the first erroneous span is preferred to be edited. • If the number of errors to be corrected in a text is larger than 4, the text is removed.\nFollowing these rules, annotators have removed 4,291 texts, which account for only 8.36% of all detected erroneous texts in the first stage.\nIn addition to erroneous span annotation, unlike GEC datasets (Daudaravicius et al., 2016; Zhao et al., 2018), we also detect a text span that is closely related to the already detected erroneous span with respect to the error, and term this span as “associated span”. In Table 2, we show examples with annotated erroneous and associated text spans. For an inappropriate combination, the associated span is usually a span that should not co-occur with the erroneous span.\n(3) Error correction. After detecting erroneous spans in a given text, annotators are required to make corrections following the minimal edit principle. Annotators are also required to use common words for error correction to make the corrected text as fluent as possible.\n(4) Error type classification. Once annotators detect both erroneous and associated spans as well as provide corrections, they are becoming quite aware of these errors. Hence, we now ask them to categorize the annotated errors into error types defined in our error taxonomy. First, they select the primary type from the level-1 error types. Then, if there are level-2 error subtypes, annotators continue to select a subtype. We observe that errors annotated with “other” only account for 5.70%, suggesting that our error taxonomy has good coverage.\n(5) Rationale generation. Partially inspired by previous datasets that provide explanations together with corresponding annotations, e.g., e-SNLI (Camburu et al., 2018), Winowhy (Zhang et al., 2020a)\nand R4C (Inoue et al., 2020), we ask annotators to give a reason for each error to justify their annotations. To the best of our knowledge, no GEC datasets provide explanations for error corrections. We believe that annotated rationales can be used to improve the interpretability of neural models trained on our dataset."
    }, {
      "heading" : "3.4 Annotation Quality Control",
      "text" : "In order to ensure the quality of error annotations, we have adopted a very strict quality control protocol during annotation. First, we train two reviewers with 1K machine-generated texts. The annotation consistency of the two reviewers on the 1K texts is very high, with an average IAA of 92.3% and Cohen’s Kappa (McHugh, 2012) of 82.6% across the annotation tasks (1), (2) and (4). For the texts annotated by the two reviewers, we have conducted an evaluation. The average accuracy of all tasks is 96.3% and 97.4% respectively.\nSecond, 200 candidate workers participate in a pre-annotation stage. The two reviewers will review annotations from these participants to distinguish whether the annotation is correct or not. Only participants who have reached an accuracy of >90% in every tasks can join in the next stage. As a result, 20 participants have passed the training in the pre-annotation stage. We then divide them into two groups and ask them to annotate the same 500 texts. The inter-annotator IAA and Cohen’s Kappa are shown in Table 3, which suggests that the 20 annotators are ready for final annotation.\nThird, in order to further ensure annotation quality, we have carried out iterative verification and amendment. The two reviewers will review each annotated text. If they found the annotation is wrong, the unqualified data will be returned for amendment until they are qualified.\nFollowing this strict quality control protocol, we complete the annotation on 47K selected machinegenerated texts. We randomly sample 1K annotated texts. The average accuracy over the three tasks (i.e., (1), (2) and (4)) is 89.6%, 88.5%, 84.3% respectively."
    }, {
      "heading" : "4 Dataset Analysis",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset Statistics",
      "text" : "Overall statistics. We reshuffle all annotated texts and divide them into the training/dev/test sets with a proportion of 8:1:1. As shown in Table 4, the training set contains 27,096 correct texts and 9,740 erroneous texts. Both the development and test set contain 4,706 texts, among which 1,218 texts are erroneous. Not surprisingly, most erroneous texts contain only one error.\nAfter Chinese word segmentation via Jieba4, there are 1,208,719 tokens in total. On average, there are 25.68 tokens in each text. Annotation statistics. As shown in Table 4, each erroneous text span contains 2.94 tokens while each associated span is composed of 4.27 tokens. The average distance from an erroneous text span to its associated span is 7.03 tokens, which is about 1/3 of the average text length.\n4github.com/fxsjy/jieba"
    }, {
      "heading" : "4.2 Error Type Distribution",
      "text" : "We further show the percentages of both level-1 and level-2 error types in Figure 2. We observe that only 5.7% cases cannot be categorized into our defined error types. The inappropriate combination, missing and redundancy error, which are the main error types in GEC datasets, account for 64.85% in our dataset. In addition to these errors, we see 18.96% commonsense errors and 10.48% discourse errors, which are usually not very common in GEC datasets. However, these two types of errors with high percentages in our dataset suggest that pretrained language models can be further improved on both commonsense reasoning and discourse modeling."
    }, {
      "heading" : "5 TGEA as a Benchmark",
      "text" : "We use our dataset as a benchmark and propose 5 tasks that are defined for errors in texts generated by PLMs. We provide baseline results for these tasks in this section.\nWe employ three BERT-style Chinese PLMs as baselines in our experiments, namely BERT-wwmext, RoBERTa-wwm-ext-large developed by Cui et al. (2020b) 5 and ALBERT-Chinese-large6. For notational simiplicity, we denote them as BERTzh, RoBERTazh and ALBERTzh respectively. Please refer to the Appendix for the model hyperparameter settings of each task."
    }, {
      "heading" : "5.1 Erroneous Text Detection",
      "text" : "Task definition. This is a text classification task to judge whether a given text is erroneous. In order to avoid data imbalance, we use the same number of correct and erroneous texts for training. Model. The three Chinese PLMs are used with standard text-classification fine-tuning. Results. All models perform just <14% better than chance (random guessing), as shown in Table 5. We also provide human performance on this task. The best model RoBERTazh is worse than human performance by 26 points. This suggests that automatically detecting erroneous texts generated by pretrained language models is very challenging even in the balanced classification scenario.\n5github.com/ymcui/Chinese-BERT-wwm 6huggingface.co/voidful/albert chinese large"
    }, {
      "heading" : "5.2 Erroneous Span and Associated Span Detection",
      "text" : "Task definition. We define the detection of the two types of spans as a joint task as they are closely related to each other. The joint task is similar to named entity recognition (NER) (a sequence labeling task) and it requires to recognize the erroneous and associated text spans simultaneously. NERstyle word-level tags are hence annotated for each erroneous text. Model. The three Chinese PLMs with NER-like fine-tuning are evaluated for this task. Since this is a 3-class token classification task, we report classF1 on erroneous and associated span. The classF1 on class X is calculated like a normal F1 for a binary classification task, by treating the target class X as the positive class and all other classes as negative. Results. As shown in Table 5, all models are very poor in this task, indicating the difficulty of automatically detecting erroneous and associated spans. However, we have found that models can benefit much from the joint detection over the detection of a single type of span (either erroneous or associated span). Our preliminary experiments on the detection of only erroneous span show that the best model can only achieve 26.42% erroneous class-F1 on the test set, while the joint task achieves 27.66% erroneous class-F1 on the test set."
    }, {
      "heading" : "5.3 Error Type Classification",
      "text" : "Task definition. Again this is a text classification task. We only perform classification over level-1 error types in the form of 5-way classification. Model. We use models similar to the first task. Results. The overall accuracy and Macro-F1 (shown in Table 5) are very low. However, we find some error types are easier than others. The accuracy on the classification of redundancy errors is 53.91%, the highest among all error types."
    }, {
      "heading" : "5.4 Error Correction",
      "text" : "Task definition. This task is the same as GEC, which transforms an erroneous text into a correct sequence. Model. we use the state-of-the-art BERT-GEC model (Kaneko et al., 2020) as the baseline for this task, which is an encoder-decoder model using representations learned by PLMs as additional inputs. Following Wang et al. (2020)，we feed representations learned by BERTzh and RoBERTazh into\nthe BERT-GEC model. Results. We report precision, recall and F0.5 scores using the official Max-Match tool (Dahlmeier and Ng, 2012). As shown in Table 5, the best RoBERTazh GEC model achieves a very low F0.5 of 0.93% and 0.98% on the development and test set respectively. We speculate that the reasons for this are twofold. First, comparing with GEC data on human-written texts, our dataset is relatively small. Second, our dataset contains error types that are very different from those in previous GEC datasets (Zhao et al., 2018; Flachs et al., 2020). Punctuation, spelling and other word-characterlevel errors, which are easy to be corrected, are rare in TGEA although they are quite common in GEC datasets. In contrast, TGEA contains more complicated errors that can only be corrected with knowledge of common sense, long-distance or inter-sentential dependencies, etc."
    }, {
      "heading" : "5.5 Rationale Generation",
      "text" : "Task definition. This is a text generation task that directly generate an explanation with respect to text generation errors from an erroneous text. Model. We use NEZHA-Gen as the baseline for this task. We restructure annotated texts in our dataset in the form of {T,这句话错误的原因是：, R} ({T, The reason behind the errors in this sentence is:, R}), where T is an erroneous sentence, while R\nis the error rational provided by annotators. We then fine-tune NEZHA-Gen on the reformatted training set and evaluate the fine-tuned model on the reformatted development and test set. We report BLEU (Papineni et al., 2002), Rouge-L (Lin, 2004) and BERT Score (Zhang et al., 2020c). Results. It can be expected that results in these metrics will be very low due to the high difficulty of this task. We analyze generated texts from the baseline and find that generated rationales are usually much longer than reference rationales provided by human annotators. This could result in the low BLEU score since long hypotheses are penalized in BLEU computation. We also experiment zero-shot generation on the test set. The results are {BLEU = 0.04%,Rouge-L = 6.83%,BERT Score = 54.27%}, indicating that fine-tuning on the annotated training set can improve this task. We suggest that this generation task could be reformulated as a multi-choice question answering task by providing alternative rationales as distractors, similar to VCR (Zellers et al., 2019a). We leave this to our future work."
    }, {
      "heading" : "6 Discussion",
      "text" : "Since we use machine-generated texts for error annotation, hyperparameters of models (e.g., sampling strategies, model size), model types (e.g., GPT-2, GPT-3 or other PLMs for text generation), and genres of texts used to train PLMs, etc., all\nhave impacts on generated texts and hence on error types and error distribution.\nA straightforward way to mitigate this issue is to collect raw texts from multiple models with different hyperparameters, neural architectures and text genres. This will lead to an expanded dataset with a much larger number of instances to be manually annotated, which is expensive and time-consuming. Yet another issue with this is that it may result in a bunch of data due to inconsistency across different models and difficulty in setting the proportion of each data source.\nInstead, we focus on consistently annotating errors for texts generated from a single source. In order to make TGEA as general and representative as possible, we use GPT-2 that is not only currently state of the art in text generation but also easily available. We also adopt standard and widely-used hyperparameters (see Appendix for more details) for NEZHA-Gen to generate texts.\nAdditionally, we use a random sampling strategy with top k = 30. For setting k, we have analyzed 500 examples with different values of k, and found that adjusting k has a reasonable impact on the percentage of redundancy errors. Except for the extreme case of k = 1, the types of errors and the distribution of them do not change significantly. Take commonsense errors as an example, which is the biggest difference from human-written texts. When k varies in a range of {5, 10, 20, 30, 50}, the percentage of commonsense errors is 18.6% ± 5.8%. Redundancy errors account for >95% when k = 1 (while commonsense errors account for 0.8%), but sharply drop to 37.4% as k = 5, and the form of repetition changes from same-word repetition to a mixed repetition of “synonymous/same-word”, suggesting that a simple repetition penalty may not be sufficient to deal with semantic redundancy. When k ∈ {10, 20, 30, 50}, the percentage of redundancy errors is very close to the result reported in Figure 2. When k > 30, many generated sentences are completely incomprehensible. A larger k will also reduce the generation efficiency. Therefore, we chose a sampling strategy of k = 30, which is the trade-off between text quality and generation efficiency."
    }, {
      "heading" : "7 Conclusions",
      "text" : "In this paper, we have presented TGEA, the first dataset with a variety of manual annotations on errors occurring texts generated by pretrained lan-\nguage models. For each erroneous text generated by a Chinese GPT-2 model, our crowdsourced annotators detect erroneous text spans with their associated text spans and provide error types defined in a two-level hierarchical taxonomy as well as rationales behind detected errors. We elaborate the 5 annotation stages for building TGEA with a strict annotation quality control protocol. We also report baseline results of the 5 benchmark tasks on TGEA. The low results suggest that our dataset is a challenging testbed for future work on automatic detection of erroneous spans and types as well as producing error corrections and rationales for texts generated by PLMs. TGEA is featured with wide error type coverage, rich semantic annotation and functional diversity, which can not only be used for deep diagnostic analysis on the text generation capability of pretrained language models, but also facilitate and promote the research of automatic and interpretable error correction for PLM-generated texts."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The present research was supported by Huawei. We would like to thank the anonymous reviewers for their insightful comments. We also want to thank MindSpore7 for the partial suppoort of this work, which is a new deep learning computing framework. The corresponding author is Deyi Xiong (dyxiong@tju.edu.cn)."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 NEZHA-Gen Hyperparameters\nTable 1 show the configuration of the generative model (NEZHA-Gen).\nA.2 Training Setting\nTable 2, 3, 4, 5, 6 show the training settings of the baseline models for each task. In these tables, ALBERTzh, BERTzh, RoBERTazh represent ALBERT-chinese, RoBerta-wwm-ext and RoBerta-wwm-ext respectively.\nA.3 Examples of Level-2 Error Types Table 7 shows examples of level-2 error types in TGEA."
    } ],
    "references" : [ {
      "title" : "Mandarinograd: A chinese collection of winograd schemas",
      "author" : [ "Timothée Bernard", "Ting Han." ],
      "venue" : "Proceedings of the 12th Language Resources and Evaluation Conference, pages 21–26. European Language Resources Association.",
      "citeRegEx" : "Bernard and Han.,? 2020",
      "shortCiteRegEx" : "Bernard and Han.",
      "year" : 2020
    }, {
      "title" : "Abductive commonsense reasoning",
      "author" : [ "Chandra Bhagavatula", "Ronan Le Bras", "Chaitanya Malaviya", "Keisuke Sakaguchi", "Ari Holtzman", "Hannah Rashkin", "Doug Downey", "Scott Wen-tau Yi", "Yejin Choi." ],
      "venue" : "International Conference on Learning Repre-",
      "citeRegEx" : "Bhagavatula et al\\.,? 2020",
      "shortCiteRegEx" : "Bhagavatula et al\\.",
      "year" : 2020
    }, {
      "title" : "PIQA: Reasoning about physical commonsense in natural language",
      "author" : [ "Yonatan Bisk", "Rowan Zellers", "Ronan LeBras", "Jianfeng Gao", "Yejin Choi." ],
      "venue" : "AAAI, pages 7432–7439.",
      "citeRegEx" : "Bisk et al\\.,? 2020",
      "shortCiteRegEx" : "Bisk et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are few-shot learners",
      "author" : [ "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "Ilya Sutskever", "Dario Amodei." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 33, pages 1876–1900. Curran Associates,",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "e-snli: Natural language inference with natural language explanations",
      "author" : [ "Oana-Maria Camburu", "Tim Rocktäschel", "Thomas Lukasiewicz", "Phil Blunsom." ],
      "venue" : "S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-",
      "citeRegEx" : "Camburu et al\\.,? 2018",
      "shortCiteRegEx" : "Camburu et al\\.",
      "year" : 2018
    }, {
      "title" : "Pretrained language models for dialogue generation with multiple input sources",
      "author" : [ "Yu Cao", "Wei Bi", "Meng Fang", "Dacheng Tao." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 909–917, Online. Association for Com-",
      "citeRegEx" : "Cao et al\\.,? 2020",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2020
    }, {
      "title" : "Evaluation benchmarks and learning criteria for discourse-aware sentence representations",
      "author" : [ "Mingda Chen", "Zewei Chu", "Kevin Gimpel." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "MuTual: A dataset for multi-turn dialogue reasoning",
      "author" : [ "Leyang Cui", "Yu Wu", "Shujie Liu", "Yue Zhang", "Ming Zhou." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1406–1416, Online. Association for",
      "citeRegEx" : "Cui et al\\.,? 2020a",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2020
    }, {
      "title" : "Revisiting pretrained models for Chinese natural language processing",
      "author" : [ "Yiming Cui", "Wanxiang Che", "Ting Liu", "Bing Qin", "Shijin Wang", "Guoping Hu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Cui et al\\.,? 2020b",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2020
    }, {
      "title" : "Better evaluation for grammatical error correction",
      "author" : [ "Daniel Dahlmeier", "Hwee Tou Ng." ],
      "venue" : "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages",
      "citeRegEx" : "Dahlmeier and Ng.,? 2012",
      "shortCiteRegEx" : "Dahlmeier and Ng.",
      "year" : 2012
    }, {
      "title" : "A report on the automatic evaluation of scientific writing shared task",
      "author" : [ "Vidas Daudaravicius", "Rafael E. Banchs", "Elena Volodina", "Courtney Napoles." ],
      "venue" : "Proceedings of the 11th Workshop on Innovative",
      "citeRegEx" : "Daudaravicius et al\\.,? 2016",
      "shortCiteRegEx" : "Daudaravicius et al\\.",
      "year" : 2016
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model",
      "author" : [ "Alexander Fabbri", "Irene Li", "Tianwei She", "Suyi Li", "Dragomir Radev." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Fabbri et al\\.,? 2019",
      "shortCiteRegEx" : "Fabbri et al\\.",
      "year" : 2019
    }, {
      "title" : "Grammatical error correction in low error density domains: A new benchmark and analyses",
      "author" : [ "Simon Flachs", "Ophélie Lacroix", "Helen Yannakoudakis", "Marek Rei", "Anders Søgaard." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Flachs et al\\.,? 2020",
      "shortCiteRegEx" : "Flachs et al\\.",
      "year" : 2020
    }, {
      "title" : "Deberta: Decoding-enhanced bert with disentangled attention",
      "author" : [ "Pengcheng He", "Xiaodong Liu", "Jianfeng Gao", "Weizhu Chen" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Cosmos QA: Machine reading comprehension with contextual commonsense reasoning",
      "author" : [ "Lifu Huang", "Ronan Le Bras", "Chandra Bhagavatula", "Yejin Choi." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Huang et al\\.,? 2019",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "R4C: A benchmark for evaluating RC systems to get the right answer for the right reason",
      "author" : [ "Naoya Inoue", "Pontus Stenetorp", "Kentaro Inui." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6740–6750, On-",
      "citeRegEx" : "Inoue et al\\.,? 2020",
      "shortCiteRegEx" : "Inoue et al\\.",
      "year" : 2020
    }, {
      "title" : "Pretraining with contrastive sentence objectives improves discourse performance of language models",
      "author" : [ "Dan Iter", "Kelvin Guu", "Larry Lansing", "Dan Jurafsky." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Iter et al\\.,? 2020",
      "shortCiteRegEx" : "Iter et al\\.",
      "year" : 2020
    }, {
      "title" : "SciREX: A challenge dataset for document-level information extraction",
      "author" : [ "Sarthak Jain", "Madeleine van Zuylen", "Hannaneh Hajishirzi", "Iz Beltagy." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Jain et al\\.,? 2020",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2020
    }, {
      "title" : "Chinese lexical analysis with deep bi-gru-crf network",
      "author" : [ "Zhenyu Jiao", "Shuqi Sun", "Ke Sun." ],
      "venue" : "arXiv preprint arXiv:1807.01882.",
      "citeRegEx" : "Jiao et al\\.,? 2018",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 2018
    }, {
      "title" : "Encoder-decoder models can benefit from pre-trained masked language models in grammatical error correction",
      "author" : [ "Masahiro Kaneko", "Masato Mita", "Shun Kiyono", "Jun Suzuki", "Kentaro Inui." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Asso-",
      "citeRegEx" : "Kaneko et al\\.,? 2020",
      "shortCiteRegEx" : "Kaneko et al\\.",
      "year" : 2020
    }, {
      "title" : "The winograd schema challenge",
      "author" : [ "Hector J. Levesque", "Ernest Davis", "Leora Morgenstern." ],
      "venue" : "Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR’12, page 552–561. AAAI Press.",
      "citeRegEx" : "Levesque et al\\.,? 2012",
      "shortCiteRegEx" : "Levesque et al\\.",
      "year" : 2012
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text summarization branches out, pages 74–81.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Text summarization with pretrained encoders",
      "author" : [ "Yang Liu", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
      "citeRegEx" : "Liu and Lapata.,? 2019",
      "shortCiteRegEx" : "Liu and Lapata.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Interrater reliability: The kappa statistic",
      "author" : [ "Mary McHugh." ],
      "venue" : "Biochemia medica : časopis Hrvatskoga društva medicinskih biokemičara / HDMB, 22:276–",
      "citeRegEx" : "McHugh.,? 2012",
      "shortCiteRegEx" : "McHugh.",
      "year" : 2012
    }, {
      "title" : "Enabling robust grammatical error correction in new domains: Datasets, metrics, and analyses",
      "author" : [ "Courtney Napoles", "Maria Nadejde", "Joel Tetreault." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7(0):551–566.",
      "citeRegEx" : "Napoles et al\\.,? 2019",
      "shortCiteRegEx" : "Napoles et al\\.",
      "year" : 2019
    }, {
      "title" : "JFLEG: A fluency corpus and benchmark for grammatical error correction",
      "author" : [ "Courtney Napoles", "Keisuke Sakaguchi", "Joel Tetreault." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Napoles et al\\.,? 2017",
      "shortCiteRegEx" : "Napoles et al\\.",
      "year" : 2017
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "ToTTo: A controlled table-totext generation dataset",
      "author" : [ "Dipanjan Das." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1173–1186, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Das.,? 2020",
      "shortCiteRegEx" : "Das.",
      "year" : 2020
    }, {
      "title" : "XCOPA: A multilingual dataset for causal commonsense reasoning",
      "author" : [ "Edoardo Maria Ponti", "Goran Glavaš", "Olga Majewska", "Qianchu Liu", "Ivan Vulić", "Anna Korhonen." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Ponti et al\\.,? 2020",
      "shortCiteRegEx" : "Ponti et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeff Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Re-",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
      "author" : [ "Melissa Roemmele", "Cosmin Adrian Bejan", "Andrew S Gordon." ],
      "venue" : "AAAI spring symposium: logical formalizations of commonsense reasoning, pages 90–95.",
      "citeRegEx" : "Roemmele et al\\.,? 2011",
      "shortCiteRegEx" : "Roemmele et al\\.",
      "year" : 2011
    }, {
      "title" : "WinoGrande: An adversarial winograd schema challenge at scale",
      "author" : [ "Keisuke Sakaguchi", "Ronan Le Bras", "Chandra Bhagavatula", "Yejin Choi." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8732–8740. Issue: 05.",
      "citeRegEx" : "Sakaguchi et al\\.,? 2020",
      "shortCiteRegEx" : "Sakaguchi et al\\.",
      "year" : 2020
    }, {
      "title" : "Social IQa: Commonsense reasoning about social interactions",
      "author" : [ "Maarten Sap", "Hannah Rashkin", "Derek Chen", "Ronan Le Bras", "Yejin Choi." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Sap et al\\.,? 2019",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2019
    }, {
      "title" : "Textcaps: a dataset for image captioning with reading comprehension",
      "author" : [ "Oleksii Sidorov", "Ronghang Hu", "Marcus Rohrbach", "Amanpreet Singh." ],
      "venue" : "CoRR, abs/2003.12462.",
      "citeRegEx" : "Sidorov et al\\.,? 2020",
      "shortCiteRegEx" : "Sidorov et al\\.",
      "year" : 2020
    }, {
      "title" : "olmpics-on what language model pre-training captures",
      "author" : [ "Alon Talmor", "Yanai Elazar", "Yoav Goldberg", "Jonathan Berant." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:743–758.",
      "citeRegEx" : "Talmor et al\\.,? 2020",
      "shortCiteRegEx" : "Talmor et al\\.",
      "year" : 2020
    }, {
      "title" : "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
      "author" : [ "Alon Talmor", "Jonathan Herzig", "Nicholas Lourie", "Jonathan Berant." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Talmor et al\\.,? 2019",
      "shortCiteRegEx" : "Talmor et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural wikipedian: Generating textual summaries from knowledge base triples",
      "author" : [ "Pavlos Vougiouklis", "Hady ElSahar", "Lucie-Aimée Kaffee", "Christophe Gravier", "Frédérique Laforest", "Jonathon S. Hare", "Elena Simperl." ],
      "venue" : "CoRR, abs/1711.00155.",
      "citeRegEx" : "Vougiouklis et al\\.,? 2017",
      "shortCiteRegEx" : "Vougiouklis et al\\.",
      "year" : 2017
    }, {
      "title" : "Superglue: A stickier benchmark for general-purpose language understanding systems",
      "author" : [ "Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Advances in Neural Infor-",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop Black-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Chinese grammatical correction using BERT-based pre-trained model",
      "author" : [ "Hongfei Wang", "Michiki Kurosawa", "Satoru Katsumata", "Mamoru Komachi." ],
      "venue" : "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "BLiMP: The benchmark of linguistic minimal pairs for English",
      "author" : [ "Alex Warstadt", "Alicia Parrish", "Haokun Liu", "Anhad Mohananey", "Wei Peng", "Sheng-Fu Wang", "Samuel R. Bowman." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:377–392.",
      "citeRegEx" : "Warstadt et al\\.,? 2020",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2020
    }, {
      "title" : "Acquiring knowledge from pre-trained model to neural machine translation",
      "author" : [ "Rongxiang Weng", "Heng Yu", "Shujian Huang", "Shanbo Cheng", "Weihua Luo." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):9266–9273.",
      "citeRegEx" : "Weng et al\\.,? 2020",
      "shortCiteRegEx" : "Weng et al\\.",
      "year" : 2020
    }, {
      "title" : "Challenges in data-to-document generation",
      "author" : [ "Sam Wiseman", "Stuart Shieber", "Alexander Rush." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2253–2263, Copenhagen, Denmark. Association for",
      "citeRegEx" : "Wiseman et al\\.,? 2017",
      "shortCiteRegEx" : "Wiseman et al\\.",
      "year" : 2017
    }, {
      "title" : "CLUE: A Chinese language understanding evaluation benchmark",
      "author" : [ "Zuoyu Tian", "Yiwen Zhang", "He Zhou", "Shaoweihua Liu", "Zhe Zhao", "Qipeng Zhao", "Cong Yue", "Xinrui Zhang", "Zhengliang Yang", "Kyle Richardson", "Zhenzhong Lan." ],
      "venue" : "Proceed-",
      "citeRegEx" : "Tian et al\\.,? 2020",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2020
    }, {
      "title" : "A new dataset and method for automatically grading ESOL texts",
      "author" : [ "Helen Yannakoudakis", "Ted Briscoe", "Ben Medlock." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages",
      "citeRegEx" : "Yannakoudakis et al\\.,? 2011",
      "shortCiteRegEx" : "Yannakoudakis et al\\.",
      "year" : 2011
    }, {
      "title" : "From recognition to cognition: Visual commonsense reasoning",
      "author" : [ "Rowan Zellers", "Yonatan Bisk", "Ali Farhadi", "Yejin Choi." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6720–6731.",
      "citeRegEx" : "Zellers et al\\.,? 2019a",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2019
    }, {
      "title" : "2019b. HellaSwag: Can a machine really finish your sentence",
      "author" : [ "Rowan Zellers", "Ari Holtzman", "Yonatan Bisk", "Ali Farhadi", "Yejin Choi" ],
      "venue" : "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Zellers et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2019
    }, {
      "title" : "WinoWhy: A deep diagnosis of essential commonsense knowledge for answering Winograd schema challenge",
      "author" : [ "Hongming Zhang", "Xinran Zhao", "Yangqiu Song." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Zhang et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Reasoning about goals, steps, and temporal ordering with WikiHow",
      "author" : [ "Li Zhang", "Qing Lyu", "Chris Callison-Burch." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4630–4639, Online. As-",
      "citeRegEx" : "Zhang et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Bertscore: Evaluating text generation with bert",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2020c",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Retrospective reader for machine reading comprehension",
      "author" : [ "Zhuosheng Zhang", "Junjie Yang", "Hai Zhao." ],
      "venue" : "The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21).",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Overview of the NLPCC 2018 Shared Task: Grammatical Error Correction: 7th CCF International Conference, NLPCC 2018",
      "author" : [ "Yuanyuan Zhao", "Nan Jiang", "Weiwei Sun", "Xiaojun Wan" ],
      "venue" : "Hohhot, China,",
      "citeRegEx" : "Zhao et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2018
    }, {
      "title" : "Evaluating commonsense in pretrained language models",
      "author" : [ "Xuhui Zhou", "Yue Zhang", "Leyang Cui", "Dandan Huang." ],
      "venue" : "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : ", 2018, 2019) and natural language generation (NLG) (Liu and Lapata, 2019; Weng et al., 2020; Cao et al., 2020).",
      "startOffset" : 52,
      "endOffset" : 111
    }, {
      "referenceID" : 44,
      "context" : ", 2018, 2019) and natural language generation (NLG) (Liu and Lapata, 2019; Weng et al., 2020; Cao et al., 2020).",
      "startOffset" : 52,
      "endOffset" : 111
    }, {
      "referenceID" : 5,
      "context" : ", 2018, 2019) and natural language generation (NLG) (Liu and Lapata, 2019; Weng et al., 2020; Cao et al., 2020).",
      "startOffset" : 52,
      "endOffset" : 111
    }, {
      "referenceID" : 6,
      "context" : "For example, evaluating and analyzing PLM-based models on learning document structures with a carefully created benchmark test suite (Chen et al., 2019), helps to develop new methods to enhance the capability of these models on discourse modeling (Iter et al.",
      "startOffset" : 133,
      "endOffset" : 152
    }, {
      "referenceID" : 17,
      "context" : ", 2019), helps to develop new methods to enhance the capability of these models on discourse modeling (Iter et al., 2020).",
      "startOffset" : 102,
      "endOffset" : 121
    }, {
      "referenceID" : 55,
      "context" : "Knowing the weakness of current PLM-based models in commonsense reasoning (Zhou et al., 2020) has inspired people to develop various reasoning datasets (Cui et al.",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 7,
      "context" : ", 2020) has inspired people to develop various reasoning datasets (Cui et al., 2020a; Zhang et al., 2020b).",
      "startOffset" : 66,
      "endOffset" : 106
    }, {
      "referenceID" : 51,
      "context" : ", 2020) has inspired people to develop various reasoning datasets (Cui et al., 2020a; Zhang et al., 2020b).",
      "startOffset" : 66,
      "endOffset" : 106
    }, {
      "referenceID" : 31,
      "context" : "On the other hand, state-of-the-art PLMs are able to generate texts that are even not distinguishable from human-written texts by human evaluators (Radford et al., 2019; Brown et al., 2020).",
      "startOffset" : 147,
      "endOffset" : 189
    }, {
      "referenceID" : 12,
      "context" : ", Multi-News (Fabbri et al., 2019), TextCaps (Sidorov et al.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 36,
      "context" : ", 2019), TextCaps (Sidorov et al., 2020)) that are constructed to train models to learn text generation (e.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 54,
      "context" : "It is also different from grammatical error correction (GEC) datasets (Zhao et al., 2018; Flachs et al., 2020) that are built from human-written texts usually by second language learners.",
      "startOffset" : 70,
      "endOffset" : 110
    }, {
      "referenceID" : 13,
      "context" : "It is also different from grammatical error correction (GEC) datasets (Zhao et al., 2018; Flachs et al., 2020) that are built from human-written texts usually by second language learners.",
      "startOffset" : 70,
      "endOffset" : 110
    }, {
      "referenceID" : 39,
      "context" : "Our dataset is not related to conventional text generation datasets (Vougiouklis et al., 2017; Wiseman et al., 2017; Parikh et al., 2020) for training text generation models.",
      "startOffset" : 68,
      "endOffset" : 137
    }, {
      "referenceID" : 45,
      "context" : "Our dataset is not related to conventional text generation datasets (Vougiouklis et al., 2017; Wiseman et al., 2017; Parikh et al., 2020) for training text generation models.",
      "startOffset" : 68,
      "endOffset" : 137
    }, {
      "referenceID" : 47,
      "context" : "FCE (Yannakoudakis et al., 2011) is an early largescale English grammatical error correction dataset, where raw texts are produced by English learners taking the First Certificate in English exams.",
      "startOffset" : 4,
      "endOffset" : 32
    }, {
      "referenceID" : 10,
      "context" : "AESW (Daudaravicius et al., 2016) is a GEC dataset from a professional editing company.",
      "startOffset" : 5,
      "endOffset" : 33
    }, {
      "referenceID" : 27,
      "context" : "JFLEG (Napoles et al., 2017) is a GEC dataset built from TOFEL Exams, which does not force annotators to make minimal edits, preferring holistic fluency rewrites.",
      "startOffset" : 6,
      "endOffset" : 28
    }, {
      "referenceID" : 26,
      "context" : "CMEG (Napoles et al., 2019) is different from general grammatical error correction datasets with texts from second language learners.",
      "startOffset" : 5,
      "endOffset" : 27
    }, {
      "referenceID" : 13,
      "context" : "CWEB (Flachs et al., 2020) also uses website texts in English, such as blogs.",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 54,
      "context" : "CGEC (Zhao et al., 2018) is a large-scale Chinese grammatical error correction dataset, derived from wrong sentences written by Chinese learners in the process of learning Chinese as a second language.",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 50,
      "context" : "Winowhy (Zhang et al., 2020a) asks annotators to provide reasons for their decisions to WSC.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 38,
      "context" : "CommonsenseQA (Talmor et al., 2019), a commonsense question answering dataset, has been constructed from ConceptNet.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : ", 2019b) and Abductive NLI (Bhagavatula et al., 2020) evaluate commonsense reasoning in the form of natural language inference.",
      "startOffset" : 27,
      "endOffset" : 53
    }, {
      "referenceID" : 15,
      "context" : "CosmosQA (Huang et al., 2019) is a dataset with multi-choice questions that require commonsense reading comprehension.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 2,
      "context" : "PIQA (Bisk et al., 2020) focuses on physical commonsense knowledge while SocialIQA (Sap et al.",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 35,
      "context" : ", 2020) focuses on physical commonsense knowledge while SocialIQA (Sap et al., 2019) on social commonsense knowledge.",
      "startOffset" : 66,
      "endOffset" : 84
    }, {
      "referenceID" : 30,
      "context" : "XCOPA (Ponti et al., 2020) is a multilingual dataset for causal commonsense reasoning in 11 typologically different languages.",
      "startOffset" : 6,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "commonsense datasets, such as Mandarinograd (Bernard and Han, 2020) consisting of 154 Chinese Winograd scheme examples and CLUEWSC2020 (Xu et al.",
      "startOffset" : 44,
      "endOffset" : 67
    }, {
      "referenceID" : 13,
      "context" : "(Flachs et al., 2020)) as one element of an inappropriate combination should be usually replaced with other expressions.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 19,
      "context" : "The sampled sentences are then word-segmented and POS-tagged by Baidu LAC tool3 (Jiao et al., 2018).",
      "startOffset" : 80,
      "endOffset" : 99
    }, {
      "referenceID" : 13,
      "context" : "com/baidu/lac neous spans is also the rule that we use for error correction: making minimal edits, which is also used in GEC datasets (Flachs et al., 2020; Napoles et al., 2017).",
      "startOffset" : 134,
      "endOffset" : 177
    }, {
      "referenceID" : 27,
      "context" : "com/baidu/lac neous spans is also the rule that we use for error correction: making minimal edits, which is also used in GEC datasets (Flachs et al., 2020; Napoles et al., 2017).",
      "startOffset" : 134,
      "endOffset" : 177
    }, {
      "referenceID" : 10,
      "context" : "In addition to erroneous span annotation, unlike GEC datasets (Daudaravicius et al., 2016; Zhao et al., 2018), we also detect a text span that is closely related to the already detected erroneous span with respect to the error, and term this span as “associated span”.",
      "startOffset" : 62,
      "endOffset" : 109
    }, {
      "referenceID" : 54,
      "context" : "In addition to erroneous span annotation, unlike GEC datasets (Daudaravicius et al., 2016; Zhao et al., 2018), we also detect a text span that is closely related to the already detected erroneous span with respect to the error, and term this span as “associated span”.",
      "startOffset" : 62,
      "endOffset" : 109
    }, {
      "referenceID" : 16,
      "context" : "and R4C (Inoue et al., 2020), we ask annotators to give a reason for each error to justify their annotations.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 20,
      "context" : "we use the state-of-the-art BERT-GEC model (Kaneko et al., 2020) as the baseline for this task, which is an encoder-decoder model using representations learned by PLMs as additional inputs.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 9,
      "context" : "5 scores using the official Max-Match tool (Dahlmeier and Ng, 2012).",
      "startOffset" : 43,
      "endOffset" : 67
    }, {
      "referenceID" : 54,
      "context" : "Second, our dataset contains error types that are very different from those in previous GEC datasets (Zhao et al., 2018; Flachs et al., 2020).",
      "startOffset" : 101,
      "endOffset" : 141
    }, {
      "referenceID" : 13,
      "context" : "Second, our dataset contains error types that are very different from those in previous GEC datasets (Zhao et al., 2018; Flachs et al., 2020).",
      "startOffset" : 101,
      "endOffset" : 141
    }, {
      "referenceID" : 28,
      "context" : "We report BLEU (Papineni et al., 2002), Rouge-L (Lin, 2004) and BERT Score (Zhang et al.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 22,
      "context" : ", 2002), Rouge-L (Lin, 2004) and BERT Score (Zhang et al.",
      "startOffset" : 17,
      "endOffset" : 28
    }, {
      "referenceID" : 52,
      "context" : ", 2002), Rouge-L (Lin, 2004) and BERT Score (Zhang et al., 2020c).",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 48,
      "context" : "We suggest that this generation task could be reformulated as a multi-choice question answering task by providing alternative rationales as distractors, similar to VCR (Zellers et al., 2019a).",
      "startOffset" : 168,
      "endOffset" : 191
    } ],
    "year" : 2021,
    "abstractText" : "In order to deeply understand the capability of pretrained language models in text generation and conduct a diagnostic evaluation, we propose TGEA1, an error-annotated dataset with multiple benchmark tasks for text generation from pretrained language models (PLMs). We use carefully selected prompt words to guide GPT-2 to generate candidate sentences, from which we select 47K for error annotation. Crowdsourced workers manually check each of these sentences and detect 12k erroneous sentences. We create an error taxonomy to cover 24 types of errors occurring in these erroneous sentences according to the nature of errors with respect to linguistics and knowledge (e.g., common sense). For each erroneous span in PLM-generated sentences, we also detect another span that is closely associated with it. Each error is hence manually labeled with comprehensive annotations, including the span of the error, the associated span, minimal correction to the error, the type of the error, and rationale behind the error. Apart from the fully annotated dataset, we also present a detailed description of the data collection procedure, statistics and analysis of the dataset. This is the first dataset with comprehensive annotations for PLM-generated texts, which facilitates the diagnostic evaluation of PLM-based text generation. Furthermore, we use TGEA as a benchmark dataset and propose a series of automatic diagnosis tasks, including error detection, error type classification, associated span detection, error rationale generation, to further promote future study on the automatic error detection and correction on texts generated by pretrained language models. ∗Equal Contributions. The The dataset is available at https://download.mindspore.cn/dataset/TGEA/.",
    "creator" : "LaTeX with hyperref"
  }
}