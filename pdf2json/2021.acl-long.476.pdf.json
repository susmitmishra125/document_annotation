{
  "name" : "2021.acl-long.476.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Robustifying Multi-hop Question Answering through Pseudo-Evidentiality Training",
    "authors" : [ "Kyungjae Lee", "Seung-won Hwang", "Sang-eun Han", "Dohyeon Lee" ],
    "emails" : [ "seungwonh@snu.ac.kr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6110–6119\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6110"
    }, {
      "heading" : "1 Introduction",
      "text" : "Multi-hop Question Answering (QA) is a task of answering complex questions by connecting information from several texts. Since the information is spread over multiple facts, this task requires to capture multiple relevant facts (which we refer as evidences) and infer an answer based on all these evidences.\nHowever, previous works (Min et al., 2019; Chen and Durrett, 2019; Trivedi et al., 2020) observe “disconnected reasoning” in some correct answers. It happens when models can exploit specific types of artifacts (e.g., entity type), to leverage them as reasoning shortcuts to guess the correct answer. For example, assume that a given question is: “which country got independence when World War II ended?” and a passage is: “Korea got independence in 1945”. Although information (“World War II ended in 1945”) is insufficient, QA models\n∗correspond to seungwonh@snu.ac.kr\npredict “Korea”, simply because its answer type is country (or, using shortcut).\nTo address the problem of reasoning shortcuts, we propose to supervise “evidentiality” – deciding whether a model answer is supported by correct evidences (see Figure 1). This is related to the problem that most of the early reader models for QA failed to predict whether questions are not answerable. Lack of answerability training led models to provide a wrong answer with high confidence, when they had to answer “unanswerable”. Similarly, we aim to train for models to recognize whether their answer is “unsupported” by evidences, as well. In our work, along with the answerability, we train the QA model to identify the existence of evidences by using passages of two types: (1) Evidence-positive and (2) Evidence-negative set. While the former\nhas both answer and evidence, the latter does not have evidence supporting the answer, such that we can detect models taking shortcuts.\nOur first research question is: how do we acquire evidence-positive and negative examples for training without annotations? For evidence-positive set, the closest existing approach (Niu et al., 2020) is to consider attention scores, which can be considered as pseudo-annotation for evidence-positive set. In other word, sentence S with high attention scores, often used as an “interpretation” of whether S is causal for model prediction, can be selected to build evidence-positive set. However, follow-up works (Serrano and Smith, 2019; Jain and Wallace, 2019) argued that attention is limited as an explanation, because causality cannot be measured, without observing model behaviors in a counterfactual case of the same passage without S. In addition, sentence causality should be aggregated to measure group causality of multiple evidences for multi-hop reasoning. To annotate group causality as “pseudo-evidentiality”, we propose Interpreter module, which removes and aggregates evidences into a group, to compare predictions in observational and counterfactual cases.\nAs a second research question, we ask how to learn from evidence-positive and evidencenegative set. To this end, we identify two objectives: (O1) QA model should not be overconfident in evidence-negative set, while (O2) confident in evidence-positive. A naive approach to pursue the former is to lower the model confidence on evidence-negative set via regularization. However, such regularization can cause violating (O2) due to correlation between confidence distributions for evidence-positive and negative set. Our solution is to selectively regularize, by purposedly training a biased model violating (O1), and decorrelate the target model from the biased model.\nFor experiments, we demonstrate the impact of our approach on HotpotQA dataset. Our empirical results show that our model can improve QA performance through pseudo-evidentiality, outperforming other baselines. In addition, our proposed approach can orthogonally combine with another SOTA model for additional performance gains."
    }, {
      "heading" : "2 Related Work",
      "text" : "Since multi-hop reasoning tasks, such as HotpotQA, are released, many approaches for the task have been proposed. These approaches can be cat-\negorized by strategies used, such as graph-based networks (Qiu et al., 2019; Fang et al., 2020), external knowledge retrieval (Asai et al., 2019), and supporting fact selection (Nie et al., 2019; Groeneveld et al., 2020).\nOur focus is to identify and alleviate reasoning shortcuts in multi-hop QA, without evidence annotations. Models taking shortcuts were widely observed from various tasks, such as object detection (Singh et al., 2020), NLI (Tu et al., 2020), and also for our target task of multi-hop QA (Min et al., 2019; Chen and Durrett, 2019; Trivedi et al., 2020), where models learn simple heuristic rules, answering correctly but without proper reasoning.\nTo mitigate the effect of shortcuts, adversarial examples (Jiang and Bansal, 2019) can be generated, or alternatively, models can be robustifed (Trivedi et al., 2020) with additional supervision for paragraph-level “sufficiency” – to identify whether a pair of two paragraphs are sufficient for right reasoning or not, which reduces shortcuts on a single paragraph. While the binary classification for paragraph-sufficiency is relatively easy (96.7 F1 in Trivedi et al. (2020)), our target of capturing a finer-grained sentence-evidentiality is more challenging. Existing QA model (Nie et al., 2019; Groeneveld et al., 2020) treats this as a supervised task, based on sentence-level human annotation. In contrast, ours requires no annotation and focuses on avoiding reasoning shortcuts using evidentiality, which was not the purpose of evidence selection in the existing model."
    }, {
      "heading" : "3 Proposed Approach",
      "text" : "In this section, to prevent reasoning shortcuts, we introduce a new approach for data acquiring and learning. We describe this task (Section 3.1) and address two research questions, of generating labels for supervision (Section 3.2) and learning (Section 3.3), respectively."
    }, {
      "heading" : "3.1 Task Description",
      "text" : "Our task definition follows distractor setting, between distractor and full-wiki in HotpotQA dataset (Yang et al., 2018), which consists of 112k questions requiring the understanding of corresponding passages to answer correctly. Each question has a candidate set of 10 paragraphs (of which two are positive paragraphs P+ and eight are negative P−), where the supporting facts for reasoning are scattered in two positive paragraphs. Then,\ngiven a question Q, the objective of this task is to aggregate relevant facts from the candidate set and estimate a consecutive answer spanA. For task evaluation, the estimated answer span is compared with the ground truth answer span in terms of F1 score at word-level."
    }, {
      "heading" : "3.2 Generating Examples for Training Answerability and Evidentiality",
      "text" : ""
    }, {
      "heading" : "Answerability for Multi-hop Reasoning",
      "text" : "For answerability training in single-hop QA, datasets such as SQuAD 2.0 (Rajpurkar et al., 2018) provide labels of answerability, so that models can be trained not to be overconfident on unanswerable text.\nSimilarly, we build triples of question Q, answer A, and passage D, to be labeled for answerability. HotpotQA dataset pairs Q with 10 paragraphs, where evidences can be scattered to two paragraphs. Based on such characteristic, concatenating two positive paragraphs is guaranteed to be answerable/evidential and concatenating two negative paragraphs (with neither evidence nor answer) is guaranteed to be unanswerable. We define a set of answerable triplets (Q,A,D) as answer-positive set A+, and an unanswerable set as answer-negative set A−. From the labels, we train a transformer-based model to classify the answerability (the detail will be discussed in the next section).\nHowever, answerability cannot supervise whether the given passage has all of these relevant evidences for reasoning. This causes a lack of generalization ability, especially on examples with an answer but no evidence."
    }, {
      "heading" : "Evidentiality for Multi-hop Reasoning",
      "text" : "While learning the answerability, we aim to capture the existence of reasoning chains in the given passage. To supervise the existence of evidences, we construct examples: evidence-positive and evidence-negative set, as shown in Figure 1.\nSpecifically, let E∗ be the ground truth of evidences to infer A, and S∗ be a sentence containing an answer A, corresponding to Q. Given Q and A, expected labels VE of evidentiality, indicating whether the evidences for answering are sufficient in the passage, are as follow:\nVE(Q,A,D) |= True ⇔ E∗ = D, A ⊂ D VE(Q,A,D) |= False ⇔ E∗ 6⊂ D, A ⊂ D\n(1)\nWe define a set of passages satisfying VE |= True as evidence-positive set E+, and a set satisfying VE |= False as evidence-negative set E−.\nSince we do not use human-annotations, we aim to generate “pseudo-evidentiality” annotation. First, for evidence-negative set, we modify answer sentence S∗ and unanswerable passages, and generate examples with the three following types:\n• 1) Answer Sentence Only: we remove all sentences in answerable passage except S∗, such that the input passage D becomes S∗, which contains a correct answer but no other evidences. That is, VE(Q,A,S∗) |= False.\n• 2) Answer Sentence + Irrelevant Facts: we use irrelevant facts with answers as context, by concatenating S∗ and unanswerable D. That is, VE(Q,A, (S∗;D)) |= False, where D ∈ P−.\n• 3) Partial Evidence + Irrelevant Facts: we use partially-relevant and irrelevant facts as context, by concatenating D1 ∈ P+ and D2 ∈ P−. That is, VE(Q,A,(D1;D2)) |= False.\nThese evidence-negative examples do not have all relevant evidences, thus if a model predicts the correct answer on such examples, it means that the model learned reasoning shortcuts.\nSecond, building an evidence-positive set is more challenging, because it is difficult to capture multiple relevant facts, with neither annotations E∗ nor supervision. Our distinction is obtaining the above annotation from model itself, by interpreting the internal mechanism of models. On a trained model, we aim to find influential sentences in predicting correct answer A, among sentences in an answerable passage. Then, we consider them as a pseudo evidence-positive set. Since such pseudo labels relies on the trained model which is not perfect, 100% recall of VE(Q,A,D) |= True in Eq. (1) is not guaranteed, though we observe 87% empirical recall (Table 1).\nSection 1 discusses how interpretation, such as attention scores (Niu et al., 2020), can be pseudoevidentiality. For QA tasks, an existing approach (Perez et al., 2019) uses answer confidence for finding pseudo-evidences, as we discuss below:\n(A) Accumulative interpreter: to consider multiple sentences as evidences, the existing approach (Perez et al., 2019) iteratively inserts sentence Si into set Et−1, with a highest probability at t-th iter-\nation, as follows:\n∆PSi = P (A|Q,Si ∪ Et−1)− P (A|Q,Et−1)\nÊ t\n= argmax Si\n∆PSi , E t = Ê\nt ∪ Et−1\n(2) where E0 starts with the sentence S∗ containing answer A, which is minimal context for our task. This method can consider multiple sentences as evidence by inserting iteratively into a set, but cannot consider the effect of erasing sentences from reasoning chain.\n(B) Our proposed Interpreter: to enhance the interpretability, we consider both erasing and inserting each sentence, in contrast to accumulative interpreter considering only the latter. Intuitively, erasing evidence would change the prediction significantly, if such evidence is causally salient, which we compute as follows:\n∆PSi = P (A|Q,D)− P (A|Q, (D\\Si)) (3)\nwhere (D\\Si) is a passage out of sentence Si. We hypothesize that breaking reasoning chain, by erasing Si, should significantly decrease P (A|·). In other words, Si with higher ∆PSi is salient. Combining the two saliency scores in Eq. (2),(3), our final saliency is as follows:\n∆PSi = P (A|Q,Si ∪ Et−1)−(((( (((P A|Q,Et−1)\n+ P (A|Q,D)− P (A|Q, (D\\(Si ∪ Et−1)))\n(4) where the constant values can be omitted in argmax. At each iteration, the sentence that maximize ∆PSi is selected, as done in Eq. (2). This promotes selection that increases confidence P (A|·) on important sentences, and decreases confidence on unimportant sentences. We stop the iterations if ∆PSi < 0 or t = T , then the final sentences in Et=T are a pseudo evidence-positive set E+. To reduce the search space, we empirically set T = 51.\nBriefly, we obtain the labels of answerability and evidentiality, as follows:\n• Answer-positive A+ and negative A− set: the former has both answer and evidences, and the latter has neither.\n• Evidence-positive E+ and negative E− set: the former is expected to have all the evidences, and the latter has an answer with no evidence.\n1Based on observations that 99% in HotpotQA require less than 6 evidence sentences for reasoning."
    }, {
      "heading" : "3.3 Learning Answerability & Evidentiality",
      "text" : "In this section, our goal is to learn the above labels of answerability and evidentiality."
    }, {
      "heading" : "Supervising Answers and Answerability (Base)",
      "text" : "As optimizing QA model is not our focus, we adopt the existing model in (Min et al., 2019). As the architecture of QA modal, we use a powerful transformer-based model – RoBERTa (Liu et al., 2019), where the input is [CLS] question [SEP] passage [EOS]. The output of the model is as follows:\nh = RoBERTa (Input) ∈ Rn×d\nOs = f1(h), O e = f2(h)\nP s = softmax(Os), P e = softmax(Oe) (5) where f1 and f2 are fully connected layers with the trainable parameters ∈ Rd, P s and P e are the the probabilities of start and end positions, d is the output dimension of the encoder, n is the size of the input sequence.\nFor answerability, they build a classifier through the hidden state h[0,:] of [CLS] token that represents both Q and D. As HotpotQA dataset covers both yes-or-no and span-extraction questions, which we follow the convention of (Asai et al., 2019) to support both as a multi-class classification problem of predicting the four probabilities:\nP cls = softmax(W1h[0,:])\n= [pspan, pyes, pno, pnone] (6)\nwhere pspan, pyes, pno, and pnone denote the probabilities of the answer type being span, yes, no, and no answer, respectively, and W1 ∈ R4×d is the trainable parameters. For training answer span and its class, the loss function of example i is the sum of cross entropy losses (DCE), as follows:\nDCE(Pi,Ai) = − ( log(P ssi) + log(P e ei) )\nDCE(P cls i , Ci) = −log(P clsci )\nLA(i) = DCE(Pi,Ai) +DCE(P clsi , Ci)\n(7)\nwhere si and ei are the starting and ending position of answerA, respectively, and ci is the index of the actual class Ci in example i."
    }, {
      "heading" : "Supervising Evidentiality",
      "text" : "As overviewed in Section 1, Base model is reported to take a shortcut, or a direct path between answer A and questionQ, neglecting implicit intermediate\npaths (evidences). Specifically, we present the two objectives for unbiased models:\n• (O1): QA model should not be overconfident on passages with no evidences (i.e., on E−).\n• (O2): QA model should be confident on passages with both answer/evidences (i.e., on E+)\nFor (O1), as a naive approach, one may consider a regularization term to avoid overconfidence on evidence-negative set E−. Overconfident answer distribution would be diverged from uniform distribution, such that Kullback–Leibler (KL) divergence KL(p||q), where p and q are the answer probabilities and the uniform distribution, respectively, is high when overconfident:\nR = ∑\ni ∈ E− DKL(P (Ai|Qi,Di)||Puniform) (8)\nwhere Puniform indicates uniform distribution. This regularization termR forces the answer probabilities on E− to be closer to the uniform one.\nHowever, one reported risk (Utama et al., 2020; Grand and Belinkov, 2019) is that suppressing data with biases has a side-effect of lowering confidence on unbiased data (especially on in-distribution). Similarly, in our case, regularizing to keep the confidence low for E−, can cause lowering that for E+, due to their correlation. In other words, pursuing (O1) violates (O2), which we observe later in Figure 3. Our next goal is thus to decorrelate two distributions on E+ and E− to satisfy both (O1) and (O2).\nFigure 2(b) shows how we feed the hidden states h into two predictors. Predictor f is for learning the target distribution and predictor g is purposedly trained to be overconfident on evidence-negative set E−, where this biased answer distribution is denoted as P̂ . We regularize target distribution P to diverge from the biased distribution of P̂ .\nFormally, the biased answer distributions P̂ (P̂ s\nand P̂ e) are as follows:\nÔs = g1(h), Ôe = g2(h)\nP̂ s = softmax(Ôs), P̂ e = softmax(Ôe) (9)\nwhere g1 and g2 are fully connected layers with the trainable parameters ∈ Rd. Then, we optimize P̂ to predict answer A on evidence-negative set E−, which makes layer g biased (taking shortcuts), and regularize f by maximizing KL divergence between P and fixed P̂ . The regularization term of example i ∈ E− is as follows:\nR̂(i) = DCE(P̂i,Ai)− λDKL(P̂i||Pi) (10)\nwhere λ is a hyper-parameter. This loss R̂ is optimized on only evidence-negative set E−.\nLastly, to pursue (O2), we train on E+, as done on A+. However, in initial steps of training, our Interpreter is not reliable, since the QA model is not trained enough yet. We thus train without E+ for the first K epochs, then extract E+ at K epoch and continue to train on all sets, as shown in Figure 2(a). In the final loss function, we apply different\nlosses as set E and A:\nLtotal = ∑\ni ∈ A+,− LA(i) + ∑ i ∈ E− R̂(i)\n+ ∑\ni ∈ E+ u(t−K) · LA(i)\n(11)\nwhere the function u is a delayed step function (1 when epoch t is greater than K, 0 otherwise)."
    }, {
      "heading" : "3.4 Passage Selection at Inference Time",
      "text" : "For our multi-hop QA task, it requires to find answerable passages with both answer and evidence, from candidate passages. While we can access the ground-truth of answerability in training set, we need to identify the answerability of (Q,D) at inference time. For this, we consider two directions: (1) Paragraph Pair Selection, which is specific to HotpotQA, and (2) Supervised Evidence Selector trained on pseudo-labels.\nFor (1), we consider the data characteristic, mentioned in Section 3.1; we know one pair of paragraphs is answerable/evidential (when both paragraphs are positive, or P+). Thus, the goal is to identify the answerable pair of paragraphs, from all possible pairs Pij = {(pi, pj) : pi ∈ P, pj ∈ P} (denoted as paired-paragraph). We can let the model select one pair with highest estimated answerability, 1 − pnone in Eq. (6), and predict answers on the paired passage, which is likely to be evidential.\nFor (2), some pipelined approaches (Nie et al., 2019; Groeneveld et al., 2020) design an evidence selector, extracting top k sentences from all candidate paragraphs. While they supervise the model using ground-truth of evidences, we assume there is no such annotation, thus train on pseudo-labels E+. We denote this setting as selected-evidences. For evidence selector, we follow an extracting method in (Beltagy et al., 2020), where the special token [S] is added at ending position of each sentence, and h[Si] from BERT indicates i-th sentence embedding. Then, a binary classifier fevi(h[Si]) is trained on the pseudo-labels, where fevi is a fully connected layer. During training, the classifier identifies whether each sentence is evidence-positive (1) or negative (0). At inference time, we first select top 5 sentences2 on paragraph candidates, and then insert the selected evidences into QA model for testing.\n2Table 1 shows the precision and recall of top5 sentences.\nWhile we discuss how to get the answerable passage above, we can use the passage setting for evaluation. To show the robustness of our model, we construct a challenge test set by excluding easy examples (i.e., easy to take shortcuts). To detect such easy examples, we build a set of single-paragraph Pi, that none of it is evidential in HotpotQA, as the dataset avoids having all evidences in a single paragraph, to discourage single-hop reasoning. If QA model predicts the correct answer on the (unevidential) single-paragraph, we remove such examples in HotpotQA, and define the remaining set as the challenge set."
    }, {
      "heading" : "4 Experiment",
      "text" : "In this section, we formulate our research questions to guide our experiments and describe evaluation results corresponding to each question.\nResearch Questions To evaluate the effectiveness of our method, we address the following research questions:\n• RQ1: How effective is our proposed method for a multi-hop QA task?\n• RQ2: Does our Interpreter effectively extract pseudo-evidentiality annotations for training?\n• RQ3: Does our method avoid reasoning shortcuts in unseen data?\nImplementation Our implementation settings for QA model follow RoBERTa (Base version with 12 layers) (Liu et al., 2019). We use the Adam optimizer with a learning rate of 0.00005 and a batchsize of 8 on RTX titan. We extract the evidencepositive set after 3 epoch (K=3 in Eq. (11)) and retrain for 3 epochs. As a hyper-parameter, we search λ among {1, 0.1, 0.01}, and found the best value (λ=0.01), based on 5% hold-out set sampled from the training set.\nMetrics We report standard F1 score for HotpotQA, to evaluate the overall QA accuracy to find the correct answers. For evidence selection, we also report F1 score, Precision, and Recall to evaluate the sentence-level evidence retrieval accuracy."
    }, {
      "heading" : "4.1 RQ1: QA Effectiveness",
      "text" : ""
    }, {
      "heading" : "Evaluation Set",
      "text" : "• Original Set: We evaluate our proposed approach on multi-hop reasoning dataset, HotpotQA3 (Yang et al., 2018). HotpotQA contains 112K examples of multi-hop questions and answers. For evaluation, we use the HotpotQA dev set (distractor setting) with 7405 examples.\n• Challenge Set: To validate the robustness, we construct a challenge set where QA model on single-paragraph gets zero F1, while such model achieves 67 F1 in the original set. That is, we exclude instances with F1 > 0, where the QA model predicts an answer without right reasoning. The exclusion makes sure the baseline obtains zero F1 on the challenge set. The number of surviving examples in our challenge set is 1653 (21.5% of dev set).\n3https://hotpotqa.github.io/"
    }, {
      "heading" : "Baselines, Our models, and Competitors As a",
      "text" : "baseline, we follow the previous QA model (Min et al., 2019) trained on single-paragraphs. We test our model on single-paragraphs, paired-paragraphs and selected evidences settings discussed in Section 3.4. As a strong competitor, among released models for HotpotQA, we implement a state-ofthe-art model (Asai et al., 2019)4, using external knowledge and a graph-based retriever.\nMain Results This section includes the results of our model for multi-hop reasoning. As shown in Table 2, our full model outperforms baselines on both original and challenge set.\nWe can further observe that i) when tested on single-paragraphs, where forced to take shortcuts, our model (O-I) is worse than the baseline (B-I), which indicates that B-I learned the shortcuts. In contrast, O-II outperforms B-II on pairedparagraphs where at least one passage candidate has all the evidences.\nii) When tested on evidences selected by our method (O-III), we can improve F1 scores on both original set and challenge set. This noise filtering effect of evidence selection, by eliminating irrelevant sentences, was consistently observed in a supervised setting (Nie et al., 2019; Groeneveld et al., 2020; Beltagy et al., 2020), which we could reproduce without annotation.\niii) Combining our method with SOTA (CI) (Asai et al., 2019) leads to accuracy gains in both sets. C-I has distinctions of using external knowledge of reasoning paths, to outperform models without such advantages, but our method can contribute to complementary gains.\n4Highest performing model in the leaderboard of HotpotQA with public code release\nAblation Study As shown in Table 3, we conduct an ablation study of O-III in Table 2. In (A), we remove E+ from Interpreter, in training time. On the QA model without E+, the performance decreased significantly, suggesting the importance of evidence-positive set. In (B), we remove evidentaility labels of both E+ and E−, and observed that the performance drop is larger compared to other variants. Through (A) and (B), we show that training our evidentiality labels can increase QA performance. In (C), we replace R̂ withR, removing layer g to train biased features. On the replaced regularization, the performance also decreased, suggesting that training R̂ is effective for a multi-hop QA task."
    }, {
      "heading" : "4.2 RQ2: Evaluation of Pseudo-Evidentiality Annotation",
      "text" : "In this section, we evaluate the effectiveness of our Interpreter, which generates evidences on training set, without supervision. We compare the pseudo evidences with human-annotation, by sentencelevel. For evaluation, we measure sentence-level F1 score, Precision and Recall, following the evidence selection evaluation in (Yang et al., 2018).\nAs a baseline, we implement the retrieval-based model, AIR (Yadav et al., 2020), which is an unsupervised method as ours. As shown in Table 4, our Interpreter on our QA model outperforms the\nretrieval-based method, in terms of F1 and Recall, while the baseline (AIR) achieves the highest precision (63.06%). We argue recall, aiming at identifying all evidences, is much critical for multi-hop reasoning, for our goal of avoiding disconnected reasoning, as long as precision remains higher than precision of answerable A+ (36.94%), in Table 1.\nAs variants of our method, we test our Interpreter on various models. First, when comparing (a) and (c), our full model (c) outperforms the baseline (a) over all metrics. The baseline (a) trained on single-paragraphs got biased, thus the evidences generated by the biased model are less accurate. Second, the variant (b) trained by R outperforms (c) our full model. In Eq. (8), the loss termR does not train layer g for biased features, unlike R̂ in Eq. (10). This shows that learning g results in performance degradation for evidence selection, despite performance gain in QA."
    }, {
      "heading" : "4.3 RQ3: Generalization",
      "text" : "In this section, to show that our model avoids reasoning shortcuts for unseen data, we analyze the confidence distribution of models on the evidencepositive and negative set. In dev set, we treat the ground truth of evidences as E+, and a single sentence containing answer as E− (each has 7K Q-D pairs). On these set, Figure 3 shows confidence P (A|Q,D) of three models; (a), (b), and (c) men-\ntioned in Section 4.2. We sort the confidence scores in ascending order, where y-axis indicates the confidence and x-axis refers to the sorted index. Thus, the colored area indicates the dominance of confidence distribution. Ideally, for a debiased model, the area on evidence-positive set should be large, while that on evidence-negative should be small.\nDesirably, in Figure 3(a), the area under the curve for E− should decrease for pursuing (O1), moving along blue arrow, while that of E+ should increase for (O2), as red arrow shows. In Figure 3(b), our model withR follows blue arrow, with a smaller area under the curve for E−, while keeping that of E+ comparable to Figure 3(a). For the comparison, Figure 3(d) shows all curves on E+. In Figure 3(c), our full model follows both directions of blue and red arrows, which indicates that ours satisfied both (O1) and (O2)."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we propose a new approach to train multi-hop QA models, not to take reasoning shortcuts of guessing right answers without sufficient evidences. We do not require annotations and generate pseudo-evidentiality instead, by regularizing QA model from being overconfident when evidences are insufficient. Our experimental results show that our method outperforms baselines on HotpotQA and has the effectiveness to distinguish between evidence-positive and negative set."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This research was supported by IITP grant funded by the Korea government (MSIT) (No.2017-001779, XAI) and ITRC support program funded by the Korea government (MSIT) (IITP-2021-2020-001789)."
    } ],
    "references" : [ {
      "title" : "Learning to retrieve reasoning paths over wikipedia graph for question answering",
      "author" : [ "Akari Asai", "Kazuma Hashimoto", "Hannaneh Hajishirzi", "Richard Socher", "Caiming Xiong." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Asai et al\\.,? 2019",
      "shortCiteRegEx" : "Asai et al\\.",
      "year" : 2019
    }, {
      "title" : "Longformer: The long-document transformer",
      "author" : [ "Iz Beltagy", "Matthew E Peters", "Arman Cohan." ],
      "venue" : "arXiv preprint arXiv:2004.05150.",
      "citeRegEx" : "Beltagy et al\\.,? 2020",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2020
    }, {
      "title" : "Understanding dataset design choices for multi-hop reasoning",
      "author" : [ "Jifan Chen", "Greg Durrett." ],
      "venue" : "Proceedings of the 2019 Conference of the North",
      "citeRegEx" : "Chen and Durrett.,? 2019",
      "shortCiteRegEx" : "Chen and Durrett.",
      "year" : 2019
    }, {
      "title" : "Hierarchical graph network for multi-hop question answering",
      "author" : [ "Yuwei Fang", "Siqi Sun", "Zhe Gan", "Rohit Pillai", "Shuohang Wang", "Jingjing Liu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Fang et al\\.,? 2020",
      "shortCiteRegEx" : "Fang et al\\.",
      "year" : 2020
    }, {
      "title" : "Adversarial regularization for visual question answering: Strengths, shortcomings, and side effects",
      "author" : [ "Gabriel Grand", "Yonatan Belinkov." ],
      "venue" : "Proceedings of the Second Workshop on Shortcomings in Vision and Language, pages 1–13.",
      "citeRegEx" : "Grand and Belinkov.,? 2019",
      "shortCiteRegEx" : "Grand and Belinkov.",
      "year" : 2019
    }, {
      "title" : "A simple yet strong pipeline for hotpotqa",
      "author" : [ "Dirk Groeneveld", "Tushar Khot", "Ashish Sabharwal" ],
      "venue" : "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Groeneveld et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Groeneveld et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is not explanation",
      "author" : [ "Sarthak Jain", "Byron C Wallace." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Pa-",
      "citeRegEx" : "Jain and Wallace.,? 2019",
      "shortCiteRegEx" : "Jain and Wallace.",
      "year" : 2019
    }, {
      "title" : "Avoiding reasoning shortcuts: Adversarial evaluation, training, and model development for multi-hop qa",
      "author" : [ "Yichen Jiang", "Mohit Bansal." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2726–",
      "citeRegEx" : "Jiang and Bansal.,? 2019",
      "shortCiteRegEx" : "Jiang and Bansal.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Compositional questions do not necessitate multihop reasoning",
      "author" : [ "Sewon Min", "Eric Wallace", "Sameer Singh", "Matt Gardner", "Hannaneh Hajishirzi", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Min et al\\.,? 2019",
      "shortCiteRegEx" : "Min et al\\.",
      "year" : 2019
    }, {
      "title" : "Revealing the importance of semantic retrieval for machine reading at scale",
      "author" : [ "Yixin Nie", "Songhe Wang", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Nie et al\\.,? 2019",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2019
    }, {
      "title" : "A self-training method for machine reading comprehension with soft evidence extraction",
      "author" : [ "Yilin Niu", "Fangkai Jiao", "Mantong Zhou", "Ting Yao", "Minlie Huang" ],
      "venue" : "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Niu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2020
    }, {
      "title" : "Finding generalizable evidence by learning to convince q\\&a models",
      "author" : [ "Ethan Perez", "Siddharth Karamcheti", "Rob Fergus", "Jason Weston", "Douwe Kiela", "Kyunghyun Cho." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Perez et al\\.,? 2019",
      "shortCiteRegEx" : "Perez et al\\.",
      "year" : 2019
    }, {
      "title" : "Dynamically fused graph network for multi-hop reasoning",
      "author" : [ "Lin Qiu", "Yunxuan Xiao", "Yanru Qu", "Hao Zhou", "Lei Li", "Weinan Zhang", "Yong Yu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6140–",
      "citeRegEx" : "Qiu et al\\.,? 2019",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2019
    }, {
      "title" : "Know what you don’t know: Unanswerable questions for squad",
      "author" : [ "Pranav Rajpurkar", "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784–789.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2018",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2018
    }, {
      "title" : "Is attention interpretable",
      "author" : [ "Sofia Serrano", "Noah A Smith" ],
      "venue" : "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Serrano and Smith.,? \\Q2019\\E",
      "shortCiteRegEx" : "Serrano and Smith.",
      "year" : 2019
    }, {
      "title" : "Don’t judge an object by its context: Learning to overcome contextual bias",
      "author" : [ "Krishna Kumar Singh", "Dhruv Mahajan", "Kristen Grauman", "Yong Jae Lee", "Matt Feiszli", "Deepti Ghadiyaram." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vi-",
      "citeRegEx" : "Singh et al\\.,? 2020",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2020
    }, {
      "title" : "Is multihop qa in dire condition? measuring and reducing disconnected reasoning",
      "author" : [ "Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Trivedi et al\\.,? 2020",
      "shortCiteRegEx" : "Trivedi et al\\.",
      "year" : 2020
    }, {
      "title" : "An empirical study on robustness to spurious correlations using pre-trained language models",
      "author" : [ "Lifu Tu", "Garima Lalwani", "Spandana Gella", "He He." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:621–633.",
      "citeRegEx" : "Tu et al\\.,? 2020",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2020
    }, {
      "title" : "Mind the trade-off: Debiasing nlu models without degrading the in-distribution performance",
      "author" : [ "Prasetya Ajie Utama", "Nafise Sadat Moosavi", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Utama et al\\.,? 2020",
      "shortCiteRegEx" : "Utama et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised alignment-based iterative evidence retrieval for multi-hop question answering",
      "author" : [ "Vikas Yadav", "Steven Bethard", "Mihai Surdeanu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4514–",
      "citeRegEx" : "Yadav et al\\.,? 2020",
      "shortCiteRegEx" : "Yadav et al\\.",
      "year" : 2020
    }, {
      "title" : "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
      "author" : [ "Zhilin Yang", "Peng Qi", "Saizheng Zhang", "Yoshua Bengio", "William Cohen", "Ruslan Salakhutdinov", "Christopher D Manning" ],
      "venue" : null,
      "citeRegEx" : "Yang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "However, previous works (Min et al., 2019; Chen and Durrett, 2019; Trivedi et al., 2020) observe “disconnected reasoning” in some correct answers.",
      "startOffset" : 24,
      "endOffset" : 88
    }, {
      "referenceID" : 2,
      "context" : "However, previous works (Min et al., 2019; Chen and Durrett, 2019; Trivedi et al., 2020) observe “disconnected reasoning” in some correct answers.",
      "startOffset" : 24,
      "endOffset" : 88
    }, {
      "referenceID" : 17,
      "context" : "However, previous works (Min et al., 2019; Chen and Durrett, 2019; Trivedi et al., 2020) observe “disconnected reasoning” in some correct answers.",
      "startOffset" : 24,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "Our first research question is: how do we acquire evidence-positive and negative examples for training without annotations? For evidence-positive set, the closest existing approach (Niu et al., 2020) is to consider attention scores, which can be considered as pseudo-annotation for evidence-positive set.",
      "startOffset" : 181,
      "endOffset" : 199
    }, {
      "referenceID" : 15,
      "context" : "However, follow-up works (Serrano and Smith, 2019; Jain and Wallace, 2019) argued that attention is limited as an explanation, because causality cannot be measured, without observing model behaviors in a counterfactual case of the same passage without S.",
      "startOffset" : 25,
      "endOffset" : 74
    }, {
      "referenceID" : 6,
      "context" : "However, follow-up works (Serrano and Smith, 2019; Jain and Wallace, 2019) argued that attention is limited as an explanation, because causality cannot be measured, without observing model behaviors in a counterfactual case of the same passage without S.",
      "startOffset" : 25,
      "endOffset" : 74
    }, {
      "referenceID" : 13,
      "context" : "These approaches can be categorized by strategies used, such as graph-based networks (Qiu et al., 2019; Fang et al., 2020), external knowledge retrieval (Asai et al.",
      "startOffset" : 85,
      "endOffset" : 122
    }, {
      "referenceID" : 3,
      "context" : "These approaches can be categorized by strategies used, such as graph-based networks (Qiu et al., 2019; Fang et al., 2020), external knowledge retrieval (Asai et al.",
      "startOffset" : 85,
      "endOffset" : 122
    }, {
      "referenceID" : 0,
      "context" : ", 2020), external knowledge retrieval (Asai et al., 2019), and supporting fact selection (Nie et al.",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 10,
      "context" : ", 2019), and supporting fact selection (Nie et al., 2019; Groeneveld et al., 2020).",
      "startOffset" : 39,
      "endOffset" : 82
    }, {
      "referenceID" : 5,
      "context" : ", 2019), and supporting fact selection (Nie et al., 2019; Groeneveld et al., 2020).",
      "startOffset" : 39,
      "endOffset" : 82
    }, {
      "referenceID" : 16,
      "context" : "Models taking shortcuts were widely observed from various tasks, such as object detection (Singh et al., 2020), NLI (Tu et al.",
      "startOffset" : 90,
      "endOffset" : 110
    }, {
      "referenceID" : 18,
      "context" : ", 2020), NLI (Tu et al., 2020), and also for our target task of multi-hop QA (Min et al.",
      "startOffset" : 13,
      "endOffset" : 30
    }, {
      "referenceID" : 9,
      "context" : ", 2020), and also for our target task of multi-hop QA (Min et al., 2019; Chen and Durrett, 2019; Trivedi et al., 2020), where models learn simple heuristic rules, answering correctly but without proper reasoning.",
      "startOffset" : 54,
      "endOffset" : 118
    }, {
      "referenceID" : 2,
      "context" : ", 2020), and also for our target task of multi-hop QA (Min et al., 2019; Chen and Durrett, 2019; Trivedi et al., 2020), where models learn simple heuristic rules, answering correctly but without proper reasoning.",
      "startOffset" : 54,
      "endOffset" : 118
    }, {
      "referenceID" : 17,
      "context" : ", 2020), and also for our target task of multi-hop QA (Min et al., 2019; Chen and Durrett, 2019; Trivedi et al., 2020), where models learn simple heuristic rules, answering correctly but without proper reasoning.",
      "startOffset" : 54,
      "endOffset" : 118
    }, {
      "referenceID" : 7,
      "context" : "To mitigate the effect of shortcuts, adversarial examples (Jiang and Bansal, 2019) can be generated, or alternatively, models can be robustifed (Trivedi et al.",
      "startOffset" : 58,
      "endOffset" : 82
    }, {
      "referenceID" : 17,
      "context" : "To mitigate the effect of shortcuts, adversarial examples (Jiang and Bansal, 2019) can be generated, or alternatively, models can be robustifed (Trivedi et al., 2020) with additional supervision for paragraph-level “sufficiency” – to identify whether a pair of two paragraphs are sufficient for right reasoning or not, which reduces shortcuts on a single paragraph.",
      "startOffset" : 144,
      "endOffset" : 166
    }, {
      "referenceID" : 10,
      "context" : "Existing QA model (Nie et al., 2019; Groeneveld et al., 2020) treats this as a supervised task, based on sentence-level human annotation.",
      "startOffset" : 18,
      "endOffset" : 61
    }, {
      "referenceID" : 5,
      "context" : "Existing QA model (Nie et al., 2019; Groeneveld et al., 2020) treats this as a supervised task, based on sentence-level human annotation.",
      "startOffset" : 18,
      "endOffset" : 61
    }, {
      "referenceID" : 21,
      "context" : "Our task definition follows distractor setting, between distractor and full-wiki in HotpotQA dataset (Yang et al., 2018), which consists of 112k questions requiring the understanding of corresponding passages to answer correctly.",
      "startOffset" : 101,
      "endOffset" : 120
    }, {
      "referenceID" : 14,
      "context" : "0 (Rajpurkar et al., 2018) provide labels of answerability, so that models can be trained not to be overconfident on unanswerable text.",
      "startOffset" : 2,
      "endOffset" : 26
    }, {
      "referenceID" : 11,
      "context" : "Section 1 discusses how interpretation, such as attention scores (Niu et al., 2020), can be pseudoevidentiality.",
      "startOffset" : 65,
      "endOffset" : 83
    }, {
      "referenceID" : 12,
      "context" : "For QA tasks, an existing approach (Perez et al., 2019) uses answer confidence for finding pseudo-evidences, as we discuss below:",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 12,
      "context" : "(A) Accumulative interpreter: to consider multiple sentences as evidences, the existing approach (Perez et al., 2019) iteratively inserts sentence Si into set Et−1, with a highest probability at t-th iter-",
      "startOffset" : 97,
      "endOffset" : 117
    }, {
      "referenceID" : 9,
      "context" : "As optimizing QA model is not our focus, we adopt the existing model in (Min et al., 2019).",
      "startOffset" : 72,
      "endOffset" : 90
    }, {
      "referenceID" : 8,
      "context" : "As the architecture of QA modal, we use a powerful transformer-based model – RoBERTa (Liu et al., 2019), where the input is [CLS] question [SEP] passage [EOS].",
      "startOffset" : 85,
      "endOffset" : 103
    }, {
      "referenceID" : 0,
      "context" : "As HotpotQA dataset covers both yes-or-no and span-extraction questions, which we follow the convention of (Asai et al., 2019) to support both as a multi-class classification problem of predicting the four probabilities:",
      "startOffset" : 107,
      "endOffset" : 126
    }, {
      "referenceID" : 19,
      "context" : "However, one reported risk (Utama et al., 2020; Grand and Belinkov, 2019) is that suppressing data with biases has a side-effect of lowering confidence on unbiased data (especially on in-distribution).",
      "startOffset" : 27,
      "endOffset" : 73
    }, {
      "referenceID" : 4,
      "context" : "However, one reported risk (Utama et al., 2020; Grand and Belinkov, 2019) is that suppressing data with biases has a side-effect of lowering confidence on unbiased data (especially on in-distribution).",
      "startOffset" : 27,
      "endOffset" : 73
    }, {
      "referenceID" : 10,
      "context" : "For (2), some pipelined approaches (Nie et al., 2019; Groeneveld et al., 2020) design an evidence selector, extracting top k sentences from all candidate paragraphs.",
      "startOffset" : 35,
      "endOffset" : 78
    }, {
      "referenceID" : 5,
      "context" : "For (2), some pipelined approaches (Nie et al., 2019; Groeneveld et al., 2020) design an evidence selector, extracting top k sentences from all candidate paragraphs.",
      "startOffset" : 35,
      "endOffset" : 78
    }, {
      "referenceID" : 1,
      "context" : "For evidence selector, we follow an extracting method in (Beltagy et al., 2020), where the special token [S] is added at ending position of each sentence, and h[Si] from BERT indicates i-th sentence embedding.",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 8,
      "context" : "Implementation Our implementation settings for QA model follow RoBERTa (Base version with 12 layers) (Liu et al., 2019).",
      "startOffset" : 101,
      "endOffset" : 119
    }, {
      "referenceID" : 21,
      "context" : "• Original Set: We evaluate our proposed approach on multi-hop reasoning dataset, HotpotQA3 (Yang et al., 2018).",
      "startOffset" : 92,
      "endOffset" : 111
    }, {
      "referenceID" : 9,
      "context" : "io/ Baselines, Our models, and Competitors As a baseline, we follow the previous QA model (Min et al., 2019) trained on single-paragraphs.",
      "startOffset" : 90,
      "endOffset" : 108
    }, {
      "referenceID" : 0,
      "context" : "As a strong competitor, among released models for HotpotQA, we implement a state-ofthe-art model (Asai et al., 2019)4, using external knowledge and a graph-based retriever.",
      "startOffset" : 97,
      "endOffset" : 116
    }, {
      "referenceID" : 10,
      "context" : "This noise filtering effect of evidence selection, by eliminating irrelevant sentences, was consistently observed in a supervised setting (Nie et al., 2019; Groeneveld et al., 2020; Beltagy et al., 2020), which we could reproduce without annotation.",
      "startOffset" : 138,
      "endOffset" : 203
    }, {
      "referenceID" : 5,
      "context" : "This noise filtering effect of evidence selection, by eliminating irrelevant sentences, was consistently observed in a supervised setting (Nie et al., 2019; Groeneveld et al., 2020; Beltagy et al., 2020), which we could reproduce without annotation.",
      "startOffset" : 138,
      "endOffset" : 203
    }, {
      "referenceID" : 1,
      "context" : "This noise filtering effect of evidence selection, by eliminating irrelevant sentences, was consistently observed in a supervised setting (Nie et al., 2019; Groeneveld et al., 2020; Beltagy et al., 2020), which we could reproduce without annotation.",
      "startOffset" : 138,
      "endOffset" : 203
    }, {
      "referenceID" : 0,
      "context" : "iii) Combining our method with SOTA (CI) (Asai et al., 2019) leads to accuracy gains in both sets.",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 20,
      "context" : "Model Evidence Selection F1 Precision Recall Retrieval-based AIR (Yadav et al., 2020) 66.",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 21,
      "context" : "For evaluation, we measure sentence-level F1 score, Precision and Recall, following the evidence selection evaluation in (Yang et al., 2018).",
      "startOffset" : 121,
      "endOffset" : 140
    }, {
      "referenceID" : 20,
      "context" : "As a baseline, we implement the retrieval-based model, AIR (Yadav et al., 2020), which is an unsupervised method as ours.",
      "startOffset" : 59,
      "endOffset" : 79
    } ],
    "year" : 2021,
    "abstractText" : "This paper studies the bias problem of multihop question answering models, of answering correctly without correct reasoning. One way to robustify these models is by supervising to not only answer right, but also with right reasoning chains. An existing direction is to annotate reasoning chains to train models, requiring expensive additional annotations. In contrast, we propose a new approach to learn evidentiality, deciding whether the answer prediction is supported by correct evidences, without such annotations. Instead, we compare counterfactual changes in answer confidence with and without evidence sentences, to generate “pseudo-evidentiality” annotations. We validate our proposed model on an original set and challenge set in HotpotQA, showing that our method is accurate and robust in multi-hop reasoning.",
    "creator" : "LaTeX with hyperref"
  }
}