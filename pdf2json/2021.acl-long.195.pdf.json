{
  "name" : "2021.acl-long.195.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Towards Robustness of Text-to-SQL Models against Synonym Substitution",
    "authors" : [ "Yujian Gan", "Xinyun Chen", "Qiuping Huang", "Matthew Purver", "John R. Woodward", "Jinxia Xie", "Pengsheng Huang" ],
    "emails" : [ "y.gan@qmul.ac.uk", "m.purver@qmul.ac.uk", "j.woodward@qmul.ac.uk", "xinyun.chen@berkeley.edu", "h@foxmail.com", "xie@hotmail.com", "huangpengsheng@pku.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2505–2515\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2505"
    }, {
      "heading" : "1 Introduction",
      "text" : "Neural networks have become the defacto approach for various natural language processing tasks, in-\n1Following the prior work on adversarial learning, worstcase adversarial attacks mean adversarial examples generated by attacking specific models.\n2Our code and dataset is available at https://github.com/ygan/Spider-Syn\ncluding text-to-SQL translation. Various benchmarks have been proposed for this task, including earlier small-scale single-domain datasets such as ATIS and GeoQuery (Yaghmazadeh et al., 2017; Iyer et al., 2017; Zelle and Mooney, 1996), and recent large-scale cross-domain datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018b). While WikiSQL only contains simple SQL queries executed on single tables, Spider covers more complex SQL structures, e.g., joining of multiple tables and nested queries.\nThe state-of-the-art models have achieved impressive performance on text-to-SQL tasks, e.g., around 70% accuracy on the Spider test set, even if the model is tested on databases that are unseen in training. However, we suspect that such crossdomain generalization heavily relies on the exact lexical matching between the NL question and the table schema. As shown in Figure 1, names of tables and columns in the SQL query are explicitly stated in the NL question. Such questions constitute the majority of cross-domain text-to-SQL benchmarks including both Spider and WikiSQL.\nAlthough assuming exact lexical matching is a good starting point to solving the text-to-SQL problem, this assumption usually does not hold in realworld scenarios. Specifically, it requires that users have precise knowledge of the table schemas to be included in the SQL query, which could be tedious for synthesizing complex SQL queries.\nIn this work, we investigate whether state-of-theart text-to-SQL models preserve good prediction performance without the assumption of exact lexical matching, where NL questions use synonyms to refer to tables or columns in SQL queries. We call such NL questions synonym substitution questions. Although some existing approaches can automatically generate synonymous substitution examples, these examples may deviate from real-world scenarios, e.g., they may not follow common human writing styles, or even accidentally becomes inconsistent with the annotated SQL query. To provide a reliable benchmark for evaluating model performance on synonym substitution questions, we introduce Spider-Syn, a human-curated dataset constructed by modifying NL questions in the Spider dataset. Specifically, we replace the schema annotations in the NL question with synonyms, manually selected so as not to change the corresponding SQL query, as shown in Figure 1. We demonstrate that when models are only trained on the original Spider dataset, they suffer a significant performance drop on Spider-Syn, even though the Spider-Syn benchmark is not constructed to exploit the worstcase attacks for text-to-SQL models. It is therefore clear that the performance of these models will suffer in real-world use, particularly in cross-domain scenarios.\nTo improve the robustness of text-to-SQL models, we utilize synonyms of table schema words, which are either manually annotated, or automatically generated when no annotation is available. We investigate two categories of approaches to incorporate these synonyms. The first category of approaches modify the schema annotations of the model input, so that they align better with the NL question. No additional training is required for these approaches. The second category of approaches are based on adversarial training, where we augment the training set with NL questions modified by synonym substitution. Both categories of approaches significantly improve the robustness, and the first category is both effective and requires less computational resources.\nIn short, we make the following contributions:\n• We conduct a comprehensive study to evaluate the robustness of text-to-SQL models against synonym substitution. • Besides worst-case adversarial attacks, we further introduce Spider-Syn, a human-curated dataset built upon Spider, to evaluate synonym substitution for real-world question paraphrases. • We propose a simple yet effective approach to utilize multiple schema annotations, without the need of additional training. We show that our approach outperforms adversarial training methods on Spider-Syn, and achieves competitive performance on worst-case adversarial attacks."
    }, {
      "heading" : "2 Spider-Syn Dataset",
      "text" : ""
    }, {
      "heading" : "2.1 Overview",
      "text" : "We construct the Spider-Syn benchmark by manually modifying NL questions in the Spider dataset using synonym substitution. The purpose of building Spider-Syn is to simulate the scenario where users do not call the exact schema words in the utterances, e.g., users may not have the knowledge of table schemas. In particular, we focus on synonym substitution for words related to databases, including table schemas and cell values. Consistent with Spider, Spider-Syn contains 7000 training and 1034 development examples, but Spider-Syn does not contain a test set since the Spider test set is not public. Figure 1 presents two examples in Spider-Syn and how they are modified from Spider."
    }, {
      "heading" : "2.2 Conduct Principle",
      "text" : "The goal of constructing the Spider-Syn dataset is not to perform worst-case adversarial attacks against existing text-to-SQL models, but to investigate the model robustness for paraphrasing schemarelated words, which is particularly important when users do not have the knowledge of table schemas. We carefully select the synonyms to replace the original text to ensure that new words will not cause ambiguity in some domains. For example, the word ‘country’ can often be used to replace the word ‘nationality’. However, we did not replace it in the domain whose ‘country’ means people’s ‘born country’ different from its other schema item, ‘nationality’. Besides, some synonym substitutions are only valid in the specific domain. For example, the word ‘number’ and ‘code’ are not generally synonymous, but ‘flight number’ can be replaced by ‘flight code’ in the aviation domain.\nMost synonym substitutions use relatively common words3 to replace the schema item words. Besides, we denote ‘id’, ‘age’, ‘name’, and ‘year’ as reserved words, which are the most standard words to represent their meanings. Under this principle, we keep some original Spider examples unchanged in Spider-Syn. Our synonym substitution does not guarantee that the modified NL question has the exact same meaning as the original question, but guarantees that its corresponding SQL is consistent. In Figure 2, Spider-Syn replaces the cell value word ‘dog’ with ‘puppy’. Although puppy is only\n3According to 20,000 most common English words in https://github.com/first20hours/ google-10000-english.\na subset of dog, the corresponding SQL for the Spider-Syn question should still use the word ‘dog’ instead of the word ‘puppy’ because there is only dog type in the database and no puppy type. Similar reasoning is needed to infer that the word ‘female’ corresponds to ‘F’ in Figure 2.\nIn some cases, words are replaced by synonymous phrases (rather than single words), as shown in Figure 3. Besides, some substitutions are also based on the database contents. For example, a column ‘location’ of the database ‘employee hire evaluation’ in Spider only stores city names as cell values. Without knowing the table schema, users are more likely to call ‘city’ instead of ‘location’ in their NL questions.\nTo summarize, we construct Spider-Syn with the following principles: • Spider-Syn is not constructed to exploit the worst-\ncase adversarial attacks, but to represent realworld use scenarios; it therefore uses only relatively common words as substitutions. • We conduct synonym substitution only for words related to schema items and cell values. • Synonym substitution includes both single words and phrases with multiple words."
    }, {
      "heading" : "2.3 Annotation Steps",
      "text" : "Before annotation, we first separate original Spider samples based on their domains. For each domain, we only utilize synonyms that are suitable for that domain. We recruit four graduate students major in computer science to annotate the dataset manually. They are trained with a detailed annotation guideline, principles, and some samples. One is allowed to start after his trial samples are approved by the whole team.\nAs synonyms can be freely chosen by annotators, standard inter-annotator agreement metrics are not sufficient to confirm the data quality. Instead, we conduct the quality control with two rounds of re-\nview. The first round is the cross-review between annotations. We require the annotators to discuss their disagreed annotations and come up with a final result out of consensus. To improve the work efficiency, we extract all synonym substitutions as a report without the NL questions from the annotated data, as shown in Figure 4. Then, the annotators do not have to go through the NL questions one by one. The second round of review is similar to the first round but is done by native English speakers."
    }, {
      "heading" : "2.4 Dataset Statistics",
      "text" : "In Spider-Syn, 5672 questions are modified compared to the original Spider dataset. In 5634 cases the schema item words are modified, with the cell value words modified in only 27 cases.We use 273 synonymous words and 189 synonymous phrases to replace approximately 492 different words or phrases in these questions. In all Spider-Syn examples, there is an average of 0.997 change per question and 7.7 words or phrases modified per domain.\nBesides, Spider-Syn keeps 2201 and 161 original Spider questions in the training and development set, respectively. In the modification between the training and development sets, 52 modified words or phrases were the same, accounting for 35% of the modification in the development set."
    }, {
      "heading" : "3 Defense Approaches",
      "text" : "We present two categories of approaches for improving model robustness to synonym substitution. We first introduce our multiple annotation selection approach, which could utilize multiple annotations for one schema item. Then we present an adversarial training method based on analysis of the NL question and domain information."
    }, {
      "heading" : "3.1 Multi-Annotation Selection (MAS)",
      "text" : "The synonym substitution problem emerges when users do not call the exact names in table schemas to query the database. Therefore, one defense against synonym substitution is utilizing multiple annotation words to represent the table schema, so that the schema linking mechanism is still effective. For example, for a database table with the name ‘country’, we annotate additional table names with similar meanings, e.g., ‘nation’, ‘State’, etc. In this way, we explicitly inform the text-to-SQL models that all these words refer to the same table, thus the table should be called in the SQL query when the\nNL question includes any of the annotated words. We design a simple yet effective mechanism to incorporate multiple annotation words, called multiple-annotation selection (MAS). For each schema item, we check whether any annotations appear in the NL question, and we select such annotations as the model input. When no annotation appears in the question, we select the default schema annotation, i.e., the same as the original Spider dataset. In this way, we could utilize multiple schema annotations simultaneously, without changing the model input format.\nThe main advantage of this method is that it does not require additional training, and could apply to existing models trained without synonym substitution questions. Annotating multiple schema words could be done automatically or manually, and we compare them in Section 4."
    }, {
      "heading" : "3.2 Adversarial Training",
      "text" : "Motivated by the idea of adversarial training that can improve the robustness of machine learning models against adversarial attacks (Madry et al., 2018; Morris et al., 2020), we implement adversarial training using the current open-source SOTA model RAT-SQL (Wang et al., 2020). We use the BERT-Attack model (Li et al., 2020) to generate adversarial examples, and implement the entire training process based on the TextAttack framework (Morris et al., 2020). TextAttack provides 82 pre-trained models, including word-level LSTM, word-level CNN, BERT-Attack, and other pre-trained Transformer-based models.\nWe follow the standard adversarial training pipeline that iteratively generates adversarial examples, and trains the model on the dataset augmented with these adversarial examples. When generating adversarial examples for training, we aim to generate samples that align with the SpiderSyn principles, rather than arbitrary adversarial perturbations. We describe the details of adversarial example generation below."
    }, {
      "heading" : "3.2.1 Generating Adversarial Examples",
      "text" : "We choose BERT-Attack to generate the adversarial examples. Different from other word substitution methods (Mrkšić et al., 2016; Ebrahimi et al., 2018; Wei and Zou, 2019), BERT-Attack model considers the entire NL question when generating words for synonym substitution. Such a sentence-based method can generate different synonyms for the same word in different context. For example, the\nword ‘head’ in ‘the head of a department’ and ‘the head of a body’ should correspond to different synonyms. Making such distinctions requires an analysis of the entire sentence, since the keywords’ positions may not be close, such as that the word ‘head’ and ‘department’ are not close in ‘Give me the info of heads whose name is Mike in each department’.\nIn addition to the original question, we add extra domain information into the BERT-Attack model, as shown in Figure 5. Without the domain information, on the right side of the Figure 5, the BERTAttack model conjectures the word ‘head’ represent the head of a body, since there are multiple feasible interpretations for the word ‘head’ if you only look at the question. To eliminate the ambiguity, we feed questions with its domain information into the BERT-Attack model, as shown on the left side of the Figure 5.\nInstead of using schema annotations, we select several other questions from the same domain as domain information. These questions should contain the schema item words we plan to replace, and other distinct schema item words in the same domain. The benefits of using sentences instead of schema annotations as domain information include: 1) avoiding many unrelated schema annotations, which could include hundreds of words; 2) the sentence format is closer to the pre-training data of BERT. As shown on the left side of the Figure 5, our method improves the quality of data generation.\nSince our work focuses on the synonym substitution of schema item words, we make two additional constraints to limit the generation of adversarial examples: 1) only words about schema items and cell values can be replaced; and 2) do not replace the reserved words discussed in Section 2.2. These constraints make sure that the adversarial examples only perform the synonym substitution for words related to database tables."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Setup",
      "text" : "We compare our approaches against baseline methods on both the Spider (Yu et al., 2018b) and Spider-Syn development sets. As discussed in Section 2.1, the Spider test set is not publicly accessible, and thus Spider-Syn does not contain a test set. Both Spider and Spider-Syn contain 7000 training and 1034 development samples respectively, where there are 146 databases for training and 20 for development. The SQL queries and schema annotations between Spider and Spider-Syn are the same; the difference is that the questions in Spider-Syn are modified from Spider by synonym substitution. Models are evaluated using the official exact matching accuracy metric of Spider.\nWe first evaluate open-source models that reach competitive performance on Spider: GNN (Bogin et al., 2019a), IRNet (Guo et al., 2019) and RATSQL (Wang et al., 2020), on the Spider-Syn development set. We then evaluate our approaches with RAT-SQL+BERT model (denoted as RAT-SQLB) on both Spider-Syn and Spider development set.\nWe examine the robustness of following approaches for synonym substitution: • SPR: Indicate that the model is trained on the\nSpider dataset. • SPRSYN: Indicate that the model is trained on\nthe Spider-Syn dataset . • SPRSPR&SYN: Indicate that the model is\ntrained on both Spider and Spider-Syn datasets. • ADVBERT: To improve the robustness of text-to-\nSQL models, we use adversarial training methods to deal with synonym substitution. This variant means that we use BERT-Attack following the design introduced in Section 3.2. Note that we only use the Spider dataset for adversarial training. • ADVGLOVE: To demonstrate the effectiveness of our ADVBERT method, we also evaluate a simpler adversarial training method based on the\nnearest GLOVE word vector (Pennington et al., 2014; Mrkšić et al., 2016). This method only considers the meaning of a single word, dispensing with domain information and question context. • ManualMAS: MAS stands for ‘multiannotation selection’, as introduced in Section 3.1. ManualMAS means that we collect multiple annotations of schema item words, which are synonyms used in Spider-Syn. Afterward, MAS selects the appropriate annotation for each schema item as the model input. • AutoMAS: In contrast to ManualMAS, in AutoMAS we collect multiple annotations based on the nearest GLOVE word vector, as used in ADVGLOVE. In this way, compared to ManualMAS, there are much more synonyms to be selected from for AutoMAS. Both ManualMAS and AutoMAS are to demonstrate the effectiveness of MAS in an ideal case. This experimental design principle is similar to evaluating adversarially trained models on the same adversarial attack used for training, which aims to show the generalization to in-distribution test samples."
    }, {
      "heading" : "4.2 Results of Models Trained on Spider",
      "text" : "Table 1 presents the exact matching accuracy of models trained on the Spider training set, and we evaluate them on development sets of Spider and Spider-Syn. Although Spider-Syn is not designed\nto exploit the worst-case attacks of text-to-SQL models, compared to Spider, the performance of all models has clearly dropped by about 20% to 30% on Spider-Syn. Using BERT for input embedding suffers less performance degradation than models without BERT, but the drop is still significant. These experiments demonstrate that training on Spider alone is insufficient for achieving good performance on synonym substitutions, because the Spider dataset only contains a few questions with synonym substitution.\nTo obtain a better understanding of prediction results, we compare the F1 scores of RAT-SQLB+SPR on different SQL components on both the Spider and Spider-Syn development set. As shown in Table 2, the performance degradation mainly comes from the components including schema items, while the decline in the ‘KEYWORDS’ and the ‘AND/OR’ that do not include schema items is marginal. This observation is consistent with the design of Spider-Syn, which focuses on the substitution of schema item words."
    }, {
      "heading" : "4.3 Comparison of Different Approaches",
      "text" : "Table 3 presents the results of RAT-SQLB trained with different approaches. We focus on RAT-SQLB since it achieves the best performance on both Spider and Spider-Syn, as shown in Table 1. Our MAS approaches significantly improve the performance on Spider-Syn, with only 1-2% performance degradation on the Spider. With ManualMAS, we see an accuracy of 62.6%, which outperforms all other approaches evaluated on Spider-Syn.\nWe compare the result of RAT-SQLB trained on Spider (SPR) as a baseline with other approaches. RAT-SQLB trained on Spider-Syn (SPRSYN) obtains 11.7% accuracy improvement when evaluated on Spider-Syn, while only suffers 1.9% accuracy\ndrop when evaluated on Spider. Meanwhile, our adversarial training method based on BERT-Attack (ADVBERT) improves the accuracy by 10.3% on Spider-Syn. We observe that ADVBERT performs much better than adversarial training based on GLOVE (ADVGLOVE), and we provide more explanation in Section 4.4. Both of our multiple annotation methods (ManualMAS and AutoMAS) improve the baseline model evaluated on Spider-Syn. The performance of ManualMAS is better because the synonyms in ManualMAS are exactly the same as the synonym substitution in Spider-Syn. We discuss more results about multi-annotation selection in Section 4.5."
    }, {
      "heading" : "4.4 Evaluation on Adversarial Attacks",
      "text" : "Observing the dramatic performance drop on Spider-Syn, we then study the model robustness under worst-case attacks. We use the adversarial examples generation module in ADVGLOVE and ADVBERT to attack the RAT-SQLB+SPR to generate two worst-case development sets.\nTable 4 presents the results on two worst-case development sets. The ADVGLOVE and ADVBERT attacks cause the accuracy of RAT-SQLB+SPR to drop by 31.7% and 20.9%, respectively. RAT-SQLB+SPR+AutoMAS achieve the best performance on defending the ADVGLOVE attack. Because the annotations in AutoMAS cover the synonym substitutions generated by ADVGLOVE. The relation between AutoMAS and ADVGLOVE is similar to that between ManualMAS and Spider-Syn. Similarly, ManualMAS helps RAT-SQLB+SPR get the best accuracy as shown in Table 3.\nAs to ADVBERT attack, RAT-SQLB+ADVBERT outperforms other approaches. This result is not surprising, because RAT-SQLB+ADVBERT is trained based on defense ADVBERT attack. How-\never, why does RAT-SQLB+ADVGLOVE perform so poorly in defending ADVGLOVE attack?\nWe conjecture that this is because the word embedding from BERT is based on the context: if you replace a word with a so-called synonym that is irrelevant to the context, BERT may give this synonym a vector with low similarity to the original. In the first example of Table 6, ADVGLOVE replaces the word ‘courses’ with ‘trajectory’. We observe that, based on the cosine similarity of BERT embedding, the schema item most similar to ‘trajectory’ changes from ‘courses’ to ‘grade conversion’. This problem does not appear in the Spider-Syn and ADVBERT examples, and some ADVGLOVE examples do not have this problem, such as the second example in Table 6. Some examples reward the model for finding the schema item that is most similar to the question token, while others penalize this pattern, which causes the model to fail to learn. Thus the model with ADVGLOVE neither defends against ADVGLOVE attack nor even obtains good performance on the Spider."
    }, {
      "heading" : "4.5 Ablation Study",
      "text" : "To analyze the individual contribution of our proposed techniques, we have run some additional experiments and show their results in Table 5. Specifically, we use RAT-SQLB+SPR, RAT-SQLB+SPRSYN, RAT-SQLB+SPRSPR&SYN, and RAT-SQLB+ADVBERT as base models, then we apply different schema annotation methods to these model and evaluate their performance in different development sets. Note that all base models use the Spider original schema annotations.\nFirst, for all base models, we found that MAS consistently improves the model performance when questions are modified by synonym substitution. Specifically, when evaluating on Spider-Syn, using ManualMAS achieves the best performance, because the ManualMAS contains the synonym substitutions of Spider-Syn. Meanwhile, when evaluating on worst-case adversarial attacks, AutoMAS mostly outperforms ManualMAS. Considering that the AutoMAS is automatically generated, AutoMAS would be a simple and efficient way to improve the robustness of text-to-SQL models."
    }, {
      "heading" : "4.6 Further Discussion on MAS",
      "text" : "ManualMAS utilizes the same synonym annotations on Spider-Syn, the same relationship as AutoMAS with ADVGLOVE, and we design this mechanism to demonstrate the effectiveness of MAS in\nan ideal case. By showing the superior performance of ManualMAS on Spider-Syn, we confirm that the failure of existing models on Spider-Syn is largely because they rely on the lexical correspondence, and MAS improves the performance by repairing the lexical link. Besides, MAS has the following advantages:\n• Compared to adversarial training, MAS does not need any additional training. Therefore, by including different annotations for MAS, the same pre-trained model could be applied to application scenarios with different requirements of robustness to synonym substitutions. • MAS could also be combined with existing defenses, e.g., on adversarially trained models, as shown in our evaluation.\nWe add the evaluation on the combination of MAS with GNN and IRNet respectively, shown in Table 7. The conclusions are similar to RAT-SQL: (1) MAS significantly improves the performance on Spider-Syn, and ManualMAS achieves the best performance. (2) AutoMAS also considerably improves the performance on adversarial attacks."
    }, {
      "heading" : "5 Related Work",
      "text" : "Text-to-SQL translation. Text-to-SQL translation has been a long-standing challenge, and various benchmarks are constructed for this task (Iyer et al., 2017; Ana-Maria Popescu et al., 2003; Tang and Mooney, 2000; Giordani and Moschitti, 2012; Li and Jagadish, 2014; Yaghmazadeh et al., 2017; Zhong et al., 2017; Yu et al., 2018b). In particular, most recent works aim to improve the performance on Spider benchmark (Yu et al., 2018b), where models are required to synthesize SQL queries with complex structures, e.g., JOIN clauses and nested queries, and they need to generalize across databases of different domains. Among various model architectures (Yu et al., 2018a; Bogin et al., 2019a; Guo et al., 2019; Zhang et al., 2019b; Bogin et al., 2019b; Wang et al., 2020), latest state-ofthe-art models have implemented a schema linking method, which is based on the exact lexical matching between the NL question and the table schema items (Guo et al., 2019; Bogin et al., 2019a; Wang et al., 2020). Schema linking is essential for these models, and causes a huge performance drop when\nremoving it. Based on this observation, we investigate the robustness of such models to synonym substitution in this work.\nData augmentation for text-to-SQL models. Existing works have proposed some data augmentation and adversarial training techniques to improve the performance of text-to-SQL models. Xiong and Sun (2019) propose an AugmentGAN model to generate samples in the target domain for data augmentation, so as to improve the cross-domain generalization. However, this approach only supports SQL queries executed on a single table, e.g., WikiSQL. Li et al. (2019) propose to use data augmentation specialized for learning the spatial information in databases, which improves the performance on single-domain GeoQuery and Restaurants datasets. Some recent works study data augmentation to improve the model performance on variants of existing SQL benchmarks. Specifically, Radhakrishnan et al. (2020) focus on search-style questions that are short and colloquial, and Zhu et al. (2020) study adversarial training to improve the adversarial robustness. However, both of them are based on WikiSQL. Zeng et al. (2020) study the model robustness when the NL questions are untranslatable and ambiguous, where they construct a dataset of such questions based on the Spider benchmark, and perform data augmentation to detect confusing spans in the question. On the contrary, our work investigate the robustness against synonym substitution for cross-domain text-to-SQL translation, supporting complex SQL structures.\nSynonym substitution for other NLP problems. The study of synonym substitution can be traced back to the 1970s (Waltz, 1978; Lehmann and Stachowitz, 1972). With the rise of machine learning, synonym substitution is widely used in NLP for data augment and adversarial attacks (Rizos et al., 2019; Wei and Zou, 2019; Ebrahimi et al., 2018; Alshemali and Kalita, 2020; Ren et al., 2019). Many\nadversarial attacks based on synonym substitution have successfully compromised the performance of existing models (Alzantot et al., 2018; Zhang et al., 2019a; Ren et al., 2019; Jin et al., 2020). Recently, (Morris et al., 2020) integrate many above works into their TextAttack framework for ease of use."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We introduce Spider-Syn, a human-curated dataset based on the Spider benchmark for evaluating the robustness of text-to-SQL models for synonym substitution. We found that the performance of previous text-to-SQL models drop dramatically on Spider-Syn, as well as other adversarial attacks performing the synonym substitution. We design two categories of approaches to improve the model robustness, i.e., multi-anotation selection and adversarial training, and demonstrate the effectiveness of our approaches."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank the anonymous reviewers for their helpful comments. Matthew Purver is partially supported by the EPSRC under grant EP/S033564/1, and by the European Union’s Horizon 2020 programme under grant agreements 769661 (SAAM, Supporting Active Ageing through Multimodal coaching) and 825153 (EMBEDDIA, Cross-Lingual Embeddings for LessRepresented Languages in European News Media). Xinyun Chen is supported by the Facebook Fellowship. The results of this publication reflect only the authors’ views and the Commission is not responsible for any use that may be made of the information it contains."
    } ],
    "references" : [ {
      "title" : "Generalization to Mitigate Synonym Substitution Attacks",
      "author" : [ "Basemah Alshemali", "Jugal Kalita" ],
      "venue" : null,
      "citeRegEx" : "Alshemali and Kalita.,? \\Q2020\\E",
      "shortCiteRegEx" : "Alshemali and Kalita.",
      "year" : 2020
    }, {
      "title" : "Generating natural language adversarial examples",
      "author" : [ "Moustafa Alzantot", "Yash Sharma", "Ahmed Elgohary", "Bo-Jhang Ho", "Mani Srivastava", "Kai-Wei Chang." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Alzantot et al\\.,? 2018",
      "shortCiteRegEx" : "Alzantot et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards a Theory of Natural Language Interfaces to Databases",
      "author" : [ "Ana-Maria Popescu", "Oren Etzioni", "Henry Kautz." ],
      "venue" : "Proceedings of the 8th International Conference on Intelligent User Interfaces, pages 149–157.",
      "citeRegEx" : "Popescu et al\\.,? 2003",
      "shortCiteRegEx" : "Popescu et al\\.",
      "year" : 2003
    }, {
      "title" : "Representing schema structure with graph neural networks for text-to-SQL parsing",
      "author" : [ "Ben Bogin", "Jonathan Berant", "Matt Gardner." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4560–4565, Florence,",
      "citeRegEx" : "Bogin et al\\.,? 2019a",
      "shortCiteRegEx" : "Bogin et al\\.",
      "year" : 2019
    }, {
      "title" : "Global Reasoning over Database Structures for Textto-SQL Parsing",
      "author" : [ "Ben Bogin", "Matt Gardner", "Jonathan Berant." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-",
      "citeRegEx" : "Bogin et al\\.,? 2019b",
      "shortCiteRegEx" : "Bogin et al\\.",
      "year" : 2019
    }, {
      "title" : "HotFlip: White-box adversarial examples for text classification",
      "author" : [ "Javid Ebrahimi", "Anyi Rao", "Daniel Lowd", "Dejing Dou." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages",
      "citeRegEx" : "Ebrahimi et al\\.,? 2018",
      "shortCiteRegEx" : "Ebrahimi et al\\.",
      "year" : 2018
    }, {
      "title" : "Automatic Generation and Reranking of SQLderived Answers to NL Questions",
      "author" : [ "Alessandra Giordani", "Alessandro Moschitti." ],
      "venue" : "Proceedings of the Second International Conference on Trustworthy Eternal Systems via Evolving Software, Data and",
      "citeRegEx" : "Giordani and Moschitti.,? 2012",
      "shortCiteRegEx" : "Giordani and Moschitti.",
      "year" : 2012
    }, {
      "title" : "Towards Complex Text-to-SQL in Cross-Domain Database with Intermediate Representation",
      "author" : [ "Jiaqi Guo", "Zecheng Zhan", "Yan Gao", "Yan Xiao", "JianGuang Lou", "Ting Liu", "Dongmei Zhang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Asso-",
      "citeRegEx" : "Guo et al\\.,? 2019",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning a Neural Semantic Parser from User Feedback",
      "author" : [ "Srinivasan Iyer", "Ioannis Konstas", "Alvin Cheung", "Jayant Krishnamurthy", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Iyer et al\\.,? 2017",
      "shortCiteRegEx" : "Iyer et al\\.",
      "year" : 2017
    }, {
      "title" : "Is bert really robust? a strong baseline for natural language attack on text classification and entailment",
      "author" : [ "Di Jin", "Zhijing Jin", "Joey Tianyi Zhou", "Peter Szolovits." ],
      "venue" : "Proceedings of the AAAI conference on artificial intelligence, volume 34, pages",
      "citeRegEx" : "Jin et al\\.,? 2020",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2020
    }, {
      "title" : "Normalization of natural language for information retrieval",
      "author" : [ "Winfred Philipp Lehmann", "RA Stachowitz." ],
      "venue" : "Technical report, TEXAS UNIV AUSTIN LINGUISTICS RESEARCH CENTER.",
      "citeRegEx" : "Lehmann and Stachowitz.,? 1972",
      "shortCiteRegEx" : "Lehmann and Stachowitz.",
      "year" : 1972
    }, {
      "title" : "Constructing an interactive natural language interface for relational databases",
      "author" : [ "Fei Li", "H.V. Jagadish." ],
      "venue" : "Proceedings of the VLDB Endowment, 8(1):73–84.",
      "citeRegEx" : "Li and Jagadish.,? 2014",
      "shortCiteRegEx" : "Li and Jagadish.",
      "year" : 2014
    }, {
      "title" : "SpatialNLI: A spatial domain natural language interface to databases using spatial comprehension",
      "author" : [ "Jingjing Li", "Wenlu Wang", "Wei Shinn Ku", "Yingtao Tian", "Haixun Wang." ],
      "venue" : "GIS: Proceedings of the ACM International Symposium on Advances in Ge-",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT-ATTACK: Adversarial Attack Against BERT Using BERT",
      "author" : [ "Linyang Li", "Ruotian Ma", "Qipeng Guo", "Xiangyang Xue", "Xipeng Qiu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards deep learning models resistant to adversarial attacks",
      "author" : [ "Aleksander Madry", "Aleksandar Makelov", "Ludwig Schmidt", "Dimitris Tsipras", "Adrian Vladu." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Madry et al\\.,? 2018",
      "shortCiteRegEx" : "Madry et al\\.",
      "year" : 2018
    }, {
      "title" : "TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP",
      "author" : [ "John Morris", "Eli Lifland", "Jin Yong Yoo", "Jake Grigsby", "Di Jin", "Yanjun Qi." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Morris et al\\.,? 2020",
      "shortCiteRegEx" : "Morris et al\\.",
      "year" : 2020
    }, {
      "title" : "Counter-fitting word vectors to linguistic constraints",
      "author" : [ "Nikola Mrkšić", "Diarmuid Ó Séaghdha", "Blaise Thomson", "Milica Gašić", "Lina M. Rojas-Barahona", "PeiHao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young." ],
      "venue" : "Proceedings of the 2016",
      "citeRegEx" : "Mrkšić et al\\.,? 2016",
      "shortCiteRegEx" : "Mrkšić et al\\.",
      "year" : 2016
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar. Asso-",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "ColloQL: Robust Cross-Domain Text-to-SQL Over Search Queries",
      "author" : [ "Karthik Radhakrishnan", "Arvind Srikantan", "Xi Victoria Lin" ],
      "venue" : null,
      "citeRegEx" : "Radhakrishnan et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Radhakrishnan et al\\.",
      "year" : 2020
    }, {
      "title" : "Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency",
      "author" : [ "Shuhuai Ren", "Yihe Deng", "Kun He", "Wanxiang Che." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Ren et al\\.,? 2019",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2019
    }, {
      "title" : "Augment to prevent: Short-text data augmentation in deep learning for hate-speech classification",
      "author" : [ "Georgios Rizos", "Konstantin Hemker", "Björn Schuller." ],
      "venue" : "Proceedings of the 28th ACM International Conference on Information and Knowl-",
      "citeRegEx" : "Rizos et al\\.,? 2019",
      "shortCiteRegEx" : "Rizos et al\\.",
      "year" : 2019
    }, {
      "title" : "Automated Construction of Database Interfaces: Intergrating Statistical and Relational Learning for Semantic Parsing",
      "author" : [ "Lappoon R Tang", "Raymond J Mooney." ],
      "venue" : "2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Tang and Mooney.,? 2000",
      "shortCiteRegEx" : "Tang and Mooney.",
      "year" : 2000
    }, {
      "title" : "An english language question answering system for a large relational database",
      "author" : [ "David L Waltz." ],
      "venue" : "Communications of the ACM, 21(7):526–539.",
      "citeRegEx" : "Waltz.,? 1978",
      "shortCiteRegEx" : "Waltz.",
      "year" : 1978
    }, {
      "title" : "RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers",
      "author" : [ "Bailin Wang", "Richard Shin", "Xiaodong Liu", "Oleksandr Polozov", "Matthew Richardson." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "EDA: Easy data augmentation techniques for boosting performance on text classification tasks",
      "author" : [ "Jason Wei", "Kai Zou." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Wei and Zou.,? 2019",
      "shortCiteRegEx" : "Wei and Zou.",
      "year" : 2019
    }, {
      "title" : "Transferable Natural Language Interface to Structured Queries Aided by Adversarial Generation",
      "author" : [ "Hongvu Xiong", "Ruixiao Sun." ],
      "venue" : "2019 IEEE 13th International Conference on Semantic Computing (ICSC), pages 255–262. IEEE.",
      "citeRegEx" : "Xiong and Sun.,? 2019",
      "shortCiteRegEx" : "Xiong and Sun.",
      "year" : 2019
    }, {
      "title" : "SQLizer: Query Synthesis from Natural Language",
      "author" : [ "Navid Yaghmazadeh", "Yuepeng Wang", "Isil Dillig", "Thomas Dillig." ],
      "venue" : "International Conference on Object-Oriented Programming, Systems, Languages, and Applications, ACM, pages 63:1—-",
      "citeRegEx" : "Yaghmazadeh et al\\.,? 2017",
      "shortCiteRegEx" : "Yaghmazadeh et al\\.",
      "year" : 2017
    }, {
      "title" : "2018a. SyntaxSQLNet: Syntax tree networks",
      "author" : [ "Tao Yu", "Michihiro Yasunaga", "Kai Yang", "Rui Zhang", "Dongxu Wang", "Zifan Li", "Dragomir Radev" ],
      "venue" : null,
      "citeRegEx" : "Yu et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2018
    }, {
      "title" : "Spider: A largescale human-labeled dataset for complex and cross",
      "author" : [ "Tao Yu", "Rui Zhang", "Kai Yang", "Michihiro Yasunaga", "Dongxu Wang", "Zifan Li", "James Ma", "Irene Li", "Qingning Yao", "Shanelle Roman", "Zilin Zhang", "Dragomir Radev" ],
      "venue" : null,
      "citeRegEx" : "Yu et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning to Parse Database Queries Using Inductive Logic Programming",
      "author" : [ "John M Zelle", "Raymond J Mooney." ],
      "venue" : "Proceedings of the Thirteenth National Conference on Artificial Intelligence - Volume 2, pages 1050–1055.",
      "citeRegEx" : "Zelle and Mooney.,? 1996",
      "shortCiteRegEx" : "Zelle and Mooney.",
      "year" : 1996
    }, {
      "title" : "Photon: A Robust Cross-Domain Text-to-SQL System",
      "author" : [ "Jichuan Zeng", "Xi Victoria Lin", "Steven C.H. Hoi", "Richard Socher", "Caiming Xiong", "Michael Lyu", "Irwin King." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Zeng et al\\.,? 2020",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2020
    }, {
      "title" : "Generating fluent adversarial examples for natural languages",
      "author" : [ "Huangzhao Zhang", "Hao Zhou", "Ning Miao", "Lei Li." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5564–5569, Florence, Italy. Asso-",
      "citeRegEx" : "Zhang et al\\.,? 2019a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "2019b. Editing-Based SQL Query Generation for CrossDomain Context-Dependent Questions",
      "author" : [ "Rui Zhang", "Tao Yu", "He Yang Er", "Sungrok Shim", "Eric Xue", "Xi Victoria Lin", "Tianze Shi", "Caiming Xiong", "Richard Socher", "Dragomir Radev" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning",
      "author" : [ "Victor Zhong", "Caiming Xiong", "Richard Socher." ],
      "venue" : "CoRR, abs/1709.0.",
      "citeRegEx" : "Zhong et al\\.,? 2017",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2017
    }, {
      "title" : "Generating Semantically Valid Adversarial Questions for TableQA",
      "author" : [ "Yi Zhu", "Yiwei Zhou", "Menglin Xia" ],
      "venue" : null,
      "citeRegEx" : "Zhu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : "Various benchmarks have been proposed for this task, including earlier small-scale single-domain datasets such as ATIS and GeoQuery (Yaghmazadeh et al., 2017; Iyer et al., 2017; Zelle and Mooney, 1996), and recent large-scale cross-domain datasets such as WikiSQL (Zhong et al.",
      "startOffset" : 132,
      "endOffset" : 201
    }, {
      "referenceID" : 8,
      "context" : "Various benchmarks have been proposed for this task, including earlier small-scale single-domain datasets such as ATIS and GeoQuery (Yaghmazadeh et al., 2017; Iyer et al., 2017; Zelle and Mooney, 1996), and recent large-scale cross-domain datasets such as WikiSQL (Zhong et al.",
      "startOffset" : 132,
      "endOffset" : 201
    }, {
      "referenceID" : 29,
      "context" : "Various benchmarks have been proposed for this task, including earlier small-scale single-domain datasets such as ATIS and GeoQuery (Yaghmazadeh et al., 2017; Iyer et al., 2017; Zelle and Mooney, 1996), and recent large-scale cross-domain datasets such as WikiSQL (Zhong et al.",
      "startOffset" : 132,
      "endOffset" : 201
    }, {
      "referenceID" : 33,
      "context" : ", 2017; Zelle and Mooney, 1996), and recent large-scale cross-domain datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al.",
      "startOffset" : 94,
      "endOffset" : 114
    }, {
      "referenceID" : 23,
      "context" : ", 2020), we implement adversarial training using the current open-source SOTA model RAT-SQL (Wang et al., 2020).",
      "startOffset" : 92,
      "endOffset" : 111
    }, {
      "referenceID" : 13,
      "context" : "We use the BERT-Attack model (Li et al., 2020) to generate adversarial examples, and implement the",
      "startOffset" : 29,
      "endOffset" : 46
    }, {
      "referenceID" : 15,
      "context" : "entire training process based on the TextAttack framework (Morris et al., 2020).",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 16,
      "context" : "Different from other word substitution methods (Mrkšić et al., 2016; Ebrahimi et al., 2018; Wei and Zou, 2019), BERT-Attack model considers the entire NL question when generating words for synonym substitution.",
      "startOffset" : 47,
      "endOffset" : 110
    }, {
      "referenceID" : 5,
      "context" : "Different from other word substitution methods (Mrkšić et al., 2016; Ebrahimi et al., 2018; Wei and Zou, 2019), BERT-Attack model considers the entire NL question when generating words for synonym substitution.",
      "startOffset" : 47,
      "endOffset" : 110
    }, {
      "referenceID" : 24,
      "context" : "Different from other word substitution methods (Mrkšić et al., 2016; Ebrahimi et al., 2018; Wei and Zou, 2019), BERT-Attack model considers the entire NL question when generating words for synonym substitution.",
      "startOffset" : 47,
      "endOffset" : 110
    }, {
      "referenceID" : 3,
      "context" : "We first evaluate open-source models that reach competitive performance on Spider: GNN (Bogin et al., 2019a), IRNet (Guo et al.",
      "startOffset" : 87,
      "endOffset" : 108
    }, {
      "referenceID" : 7,
      "context" : ", 2019a), IRNet (Guo et al., 2019) and RATSQL (Wang et al.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 23,
      "context" : ", 2019) and RATSQL (Wang et al., 2020), on the Spider-Syn development set.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : "Text-to-SQL translation has been a long-standing challenge, and various benchmarks are constructed for this task (Iyer et al., 2017; Ana-Maria Popescu et al., 2003; Tang and Mooney, 2000; Giordani and Moschitti, 2012; Li and Jagadish, 2014; Yaghmazadeh et al., 2017; Zhong et al., 2017; Yu et al., 2018b).",
      "startOffset" : 113,
      "endOffset" : 304
    }, {
      "referenceID" : 21,
      "context" : "Text-to-SQL translation has been a long-standing challenge, and various benchmarks are constructed for this task (Iyer et al., 2017; Ana-Maria Popescu et al., 2003; Tang and Mooney, 2000; Giordani and Moschitti, 2012; Li and Jagadish, 2014; Yaghmazadeh et al., 2017; Zhong et al., 2017; Yu et al., 2018b).",
      "startOffset" : 113,
      "endOffset" : 304
    }, {
      "referenceID" : 6,
      "context" : "Text-to-SQL translation has been a long-standing challenge, and various benchmarks are constructed for this task (Iyer et al., 2017; Ana-Maria Popescu et al., 2003; Tang and Mooney, 2000; Giordani and Moschitti, 2012; Li and Jagadish, 2014; Yaghmazadeh et al., 2017; Zhong et al., 2017; Yu et al., 2018b).",
      "startOffset" : 113,
      "endOffset" : 304
    }, {
      "referenceID" : 11,
      "context" : "Text-to-SQL translation has been a long-standing challenge, and various benchmarks are constructed for this task (Iyer et al., 2017; Ana-Maria Popescu et al., 2003; Tang and Mooney, 2000; Giordani and Moschitti, 2012; Li and Jagadish, 2014; Yaghmazadeh et al., 2017; Zhong et al., 2017; Yu et al., 2018b).",
      "startOffset" : 113,
      "endOffset" : 304
    }, {
      "referenceID" : 26,
      "context" : "Text-to-SQL translation has been a long-standing challenge, and various benchmarks are constructed for this task (Iyer et al., 2017; Ana-Maria Popescu et al., 2003; Tang and Mooney, 2000; Giordani and Moschitti, 2012; Li and Jagadish, 2014; Yaghmazadeh et al., 2017; Zhong et al., 2017; Yu et al., 2018b).",
      "startOffset" : 113,
      "endOffset" : 304
    }, {
      "referenceID" : 33,
      "context" : "Text-to-SQL translation has been a long-standing challenge, and various benchmarks are constructed for this task (Iyer et al., 2017; Ana-Maria Popescu et al., 2003; Tang and Mooney, 2000; Giordani and Moschitti, 2012; Li and Jagadish, 2014; Yaghmazadeh et al., 2017; Zhong et al., 2017; Yu et al., 2018b).",
      "startOffset" : 113,
      "endOffset" : 304
    }, {
      "referenceID" : 3,
      "context" : "Among various model architectures (Yu et al., 2018a; Bogin et al., 2019a; Guo et al., 2019; Zhang et al., 2019b; Bogin et al., 2019b; Wang et al., 2020), latest state-ofthe-art models have implemented a schema linking method, which is based on the exact lexical matching between the NL question and the table schema items (Guo et al.",
      "startOffset" : 34,
      "endOffset" : 152
    }, {
      "referenceID" : 7,
      "context" : "Among various model architectures (Yu et al., 2018a; Bogin et al., 2019a; Guo et al., 2019; Zhang et al., 2019b; Bogin et al., 2019b; Wang et al., 2020), latest state-ofthe-art models have implemented a schema linking method, which is based on the exact lexical matching between the NL question and the table schema items (Guo et al.",
      "startOffset" : 34,
      "endOffset" : 152
    }, {
      "referenceID" : 4,
      "context" : "Among various model architectures (Yu et al., 2018a; Bogin et al., 2019a; Guo et al., 2019; Zhang et al., 2019b; Bogin et al., 2019b; Wang et al., 2020), latest state-ofthe-art models have implemented a schema linking method, which is based on the exact lexical matching between the NL question and the table schema items (Guo et al.",
      "startOffset" : 34,
      "endOffset" : 152
    }, {
      "referenceID" : 23,
      "context" : "Among various model architectures (Yu et al., 2018a; Bogin et al., 2019a; Guo et al., 2019; Zhang et al., 2019b; Bogin et al., 2019b; Wang et al., 2020), latest state-ofthe-art models have implemented a schema linking method, which is based on the exact lexical matching between the NL question and the table schema items (Guo et al.",
      "startOffset" : 34,
      "endOffset" : 152
    }, {
      "referenceID" : 7,
      "context" : ", 2020), latest state-ofthe-art models have implemented a schema linking method, which is based on the exact lexical matching between the NL question and the table schema items (Guo et al., 2019; Bogin et al., 2019a; Wang et al., 2020).",
      "startOffset" : 177,
      "endOffset" : 235
    }, {
      "referenceID" : 3,
      "context" : ", 2020), latest state-ofthe-art models have implemented a schema linking method, which is based on the exact lexical matching between the NL question and the table schema items (Guo et al., 2019; Bogin et al., 2019a; Wang et al., 2020).",
      "startOffset" : 177,
      "endOffset" : 235
    }, {
      "referenceID" : 23,
      "context" : ", 2020), latest state-ofthe-art models have implemented a schema linking method, which is based on the exact lexical matching between the NL question and the table schema items (Guo et al., 2019; Bogin et al., 2019a; Wang et al., 2020).",
      "startOffset" : 177,
      "endOffset" : 235
    }, {
      "referenceID" : 22,
      "context" : "The study of synonym substitution can be traced back to the 1970s (Waltz, 1978; Lehmann and Stachowitz, 1972).",
      "startOffset" : 66,
      "endOffset" : 109
    }, {
      "referenceID" : 10,
      "context" : "The study of synonym substitution can be traced back to the 1970s (Waltz, 1978; Lehmann and Stachowitz, 1972).",
      "startOffset" : 66,
      "endOffset" : 109
    }, {
      "referenceID" : 20,
      "context" : "With the rise of machine learning, synonym substitution is widely used in NLP for data augment and adversarial attacks (Rizos et al., 2019; Wei and Zou, 2019; Ebrahimi et al., 2018; Alshemali and Kalita, 2020; Ren et al., 2019).",
      "startOffset" : 119,
      "endOffset" : 227
    }, {
      "referenceID" : 24,
      "context" : "With the rise of machine learning, synonym substitution is widely used in NLP for data augment and adversarial attacks (Rizos et al., 2019; Wei and Zou, 2019; Ebrahimi et al., 2018; Alshemali and Kalita, 2020; Ren et al., 2019).",
      "startOffset" : 119,
      "endOffset" : 227
    }, {
      "referenceID" : 5,
      "context" : "With the rise of machine learning, synonym substitution is widely used in NLP for data augment and adversarial attacks (Rizos et al., 2019; Wei and Zou, 2019; Ebrahimi et al., 2018; Alshemali and Kalita, 2020; Ren et al., 2019).",
      "startOffset" : 119,
      "endOffset" : 227
    }, {
      "referenceID" : 0,
      "context" : "With the rise of machine learning, synonym substitution is widely used in NLP for data augment and adversarial attacks (Rizos et al., 2019; Wei and Zou, 2019; Ebrahimi et al., 2018; Alshemali and Kalita, 2020; Ren et al., 2019).",
      "startOffset" : 119,
      "endOffset" : 227
    }, {
      "referenceID" : 19,
      "context" : "With the rise of machine learning, synonym substitution is widely used in NLP for data augment and adversarial attacks (Rizos et al., 2019; Wei and Zou, 2019; Ebrahimi et al., 2018; Alshemali and Kalita, 2020; Ren et al., 2019).",
      "startOffset" : 119,
      "endOffset" : 227
    }, {
      "referenceID" : 1,
      "context" : "Many adversarial attacks based on synonym substitution have successfully compromised the performance of existing models (Alzantot et al., 2018; Zhang et al., 2019a; Ren et al., 2019; Jin et al., 2020).",
      "startOffset" : 120,
      "endOffset" : 200
    }, {
      "referenceID" : 31,
      "context" : "Many adversarial attacks based on synonym substitution have successfully compromised the performance of existing models (Alzantot et al., 2018; Zhang et al., 2019a; Ren et al., 2019; Jin et al., 2020).",
      "startOffset" : 120,
      "endOffset" : 200
    }, {
      "referenceID" : 19,
      "context" : "Many adversarial attacks based on synonym substitution have successfully compromised the performance of existing models (Alzantot et al., 2018; Zhang et al., 2019a; Ren et al., 2019; Jin et al., 2020).",
      "startOffset" : 120,
      "endOffset" : 200
    }, {
      "referenceID" : 9,
      "context" : "Many adversarial attacks based on synonym substitution have successfully compromised the performance of existing models (Alzantot et al., 2018; Zhang et al., 2019a; Ren et al., 2019; Jin et al., 2020).",
      "startOffset" : 120,
      "endOffset" : 200
    }, {
      "referenceID" : 15,
      "context" : "Recently, (Morris et al., 2020) integrate many above works into their TextAttack framework for ease of use.",
      "startOffset" : 10,
      "endOffset" : 31
    } ],
    "year" : 2021,
    "abstractText" : "Recently, there has been significant progress in studying neural networks to translate text descriptions into SQL queries. Despite achieving good performance on some public benchmarks, existing text-to-SQL models typically rely on the lexical matching between words in natural language (NL) questions and tokens in table schemas, which may render the models vulnerable to attacks that break the schema linking mechanism. In this work, we investigate the robustness of text-to-SQL models to synonym substitution. In particular, we introduce Spider-Syn, a human-curated dataset based on the Spider benchmark for text-toSQL translation. NL questions in Spider-Syn are modified from Spider, by replacing their schema-related words with manually selected synonyms that reflect real-world question paraphrases. We observe that the accuracy dramatically drops by eliminating such explicit correspondence between NL questions and table schemas, even if the synonyms are not adversarially selected to conduct worst-case adversarial attacks 1. Finally, we present two categories of approaches to improve the model robustness. The first category of approaches utilizes additional synonym annotations for table schemas by modifying the model input, while the second category is based on adversarial training. We demonstrate that both categories of approaches significantly outperform their counterparts without the defense, and the first category of approaches are more effective. 2",
    "creator" : "LaTeX with hyperref"
  }
}