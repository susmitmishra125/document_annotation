{
  "name" : "2021.acl-long.102.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning",
    "authors" : [ "Bill Yuchen Lin", "Seyeon Lee", "Xiaoyang Qiao", "Xiang Ren" ],
    "emails" : [ "xiangren}@usc.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1274–1287\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1274"
    }, {
      "heading" : "1 Introduction",
      "text" : "Understanding natural language relies heavily on commonsense reasoning (CSR), which is the process of making inferences with commonsense knowledge. Commonsense knowledge is the set of general facts that reflect our natural understanding of the physical world and human behavior, which are usually seen as an implicit background when people communicate with each other using languages. It is thus of vital importance to evaluate and improve the commonsense reasoning capability of language models (LMs), towards building general natural language understanding (NLU) systems (Davis and Marcus, 2015).\n1We release our code and data at the project website: https://inklab.usc.edu/XCSR/.\nMany recent benchmark datasets and probing methods have been proposed to evaluate machine common sense. As shown in Figure 1, the LAMA probe (Petroni et al., 2019) is for analyzing LMs’ zero-shot commonsense recalling ability; CommonsenseQA (CSQA) (Talmor et al., 2019) is instead a multiple-choice QA task that needs fine-tuning; CODAH (Chen et al., 2019) and SWAG (Zellers et al., 2018) focus on the ability to complete the most plausible scenes. However, all these works have been limited only to English. Consequently, follow-up analysis and reasoning methods developed (Lin et al., 2019; Feng et al., 2020; Lin et al., 2020) also focus only on English LMs like BERT (Devlin et al., 2019). Such English-centric trend of commonsense reasoning studies not only limits our research scope, but also tends to exacerbate English-specific bias that might prevent future methods from generalizing beyond English (Ponti et al., 2020).\nIt is of pressing urgency for the community to develop NLU systems that can serve all languages in the world to bridge the gap between different cultures and eliminate language barriers (Hu et al., 2020), and multilingual language models (MLLMs), such as XLM-R (Conneau et al., 2020), are among the most promising tools to achieve this ambitious goal. Although ML-LMs have been evaluated in a few NLU tasks, e.g., XNLI (Conneau et al., 2018) and XTEMRE (Hu et al., 2020), it is still relatively unclear how ML-LMs perform in commonsense reasoning tasks, due to the lack of 1) dedicated methods for probing common sense in ML-LMs and 2) multilingual benchmark datasets for commonsense reasoning.\nTo analyze how much common sense MLLMs already have without any tuning, we propose MICKEYPROBE, a zero-shot probing task. It tasks a ML-LM to rank a set of contrastive assertions (i.e., declarative sentences) in the same language by their commonsense plausibility, for which we use pseudo-likelihood (PLL) (Salazar et al., 2020) as a proxy. Unlike the LAMA probe, it can study multi-token concepts which are ubiquitous in some non-English languages. In addition, it fairly compares performance across different languages via a language-invariant evaluation protocol. Alongside the probing task, we also create MickeyCorpus, a large-scale multilingual dataset, consisting of 561k sentences in 11 different languages. Our experiments reveal that there are always large discrepancies across different languages in the tested ML-LMs, and different MLLMs show very different language preferences.\nBeyond supervision-free analysis of ML-LMs, we also study their performance in commonsense reasoning tasks, such as CSQA and CODAH, within a cross-lingual transfer setting (i.e., trained on English data and tested on other languages). We find that existing ML-LMs tend to have much lower accuracy in commonsense reasoning beyond English. We conjecture a major common weakness of existing ML-LMs is that their pretraining stages do not have a proper sentence-level objective. Therefore, we propose multilingual contrastive pre-training (MCP), which tasks a MLLM to select the correct assertion out of a set of N contrastive assertions in N different languages. We re-format MickeyCorpus by sampling across languages and thus form a dedicated pre-training corpus for the MCP task. To fairly\nevaluate different ML-LMs and validate the effectiveness of MCP, we create X-CSQA and XCODAH, two cross-lingual commonsense reasoning datasets by translating their English versions to 15 other languages2, including low-resource ones such as Swahili (sw) and Urdu (ur). Experiments show that the proposed MCP objective indeed significantly improves the performance of state-ofthe-art ML-LMs in cross-lingual commonsense reasoning. Our contributions are as follows:\n• Resources. We collect a large multilingual parallel corpus, MickeyCorpus, consisting of 561k sentences in 11 languages, which can be used for analyzing and improving ML-LMs. We also create X-CSQA and X-CODAH, two cross-lingual CSR benchmarks in 16 languages, for question answering and scene completion, respectively. • Evaluation and analysis. We analyze multiple popular ML-LMs with MICKEYPROBE, a language-invariant, zero-shot task for probing common sense in ML-LMs; We also evaluate them on X-CSQA and X-CODAH in a cross-lingual transfer setting. • Method to improve ML-LMs. We propose multilingual contrastive pretraining, a simple and effective sentence-level pretext task for enhancing ML-LMs in cross-lingual commonsense reasoning, which significantly improves the state-of-the-art ML-LMs in crosslingual commonsense reasoning."
    }, {
      "heading" : "2 Background and Related Work",
      "text" : "In this section, we introduce important concepts, background knowledge, and related work before we present our work in following sections."
    }, {
      "heading" : "2.1 Multilingual Language Models",
      "text" : "A multilingual language model (ML-LM) aims to produce text representations for multiple languages in a unified embedding space. One of the unique advantages of ML-LMs is their potential ability to perform zero-shot cross-lingual transfer — a model trained (or fine-tuned) on data in one language (usually English) can be directly used in other languages as well without further fine-tuning. Improving ML-LMs is thus believed as one of the most promising approach towards multilingual NLU at scale. mBERT (Devlin\n2The 16 languages for X-CSQA and X-CODAH: {en, zh, de, es, fr, it, jap, nl, pl, pt, ru, ar, vi, hi, sw, ur}.\net al., 2019) is simply the BERT model (Devlin et al., 2019) trained on multilingual corpora without specific designs about multilinguality. The distil-mBERT (d-mBERT) (Sanh et al., 2019) is a smaller mBERT trained by knowledge distillation. Conneau and Lample (2019) proposed XLM(-100), which is pretrained with both masked language modeling (MLM) and translation language modeling (TLM). Conneau et al. (2020) further proposed XLM-R, which improves the XLM with a better sub-token vocabulary and highquality multilingual corpora (CC100). We leave the analysis of recent seq2seq ML-LMs, such as mBART (Liu et al., 2020) and mT5 (Xue et al., 2021), as future work, because their architectures are significantly different from the other ML-LMs.\nNote that the above ML-LMs are pretrained only with token-level training objectives such as MLM (i.e., recovering masked tokens in monolingual text) and TLM (i.e., recovering masked tokens in a pair of parallel sentences in two different languages). However, most NLU tasks, including commonsense reasoning, highly rely on sentence-level representations. We argue that a well-designed sentence-level pre-training objective should improve ML-LMs for NLU tasks. This intuition motivates us to propose a sentence-level pre-training objective — MCP (Section 5)."
    }, {
      "heading" : "2.2 Cross-lingual Language Understanding",
      "text" : "There are a few recent multilingual benchmarks for NLU tasks, e.g., XTREME(Hu et al., 2020), TyDi QA(Clark et al., 2020), and XGLUE(Liang et al., 2020). XTREME and XGLUE are unified large-scale multilingual multitask benchmarks, while Ty-Di QA focuses on the QA. These existing cross-lingual benchmarks have not covered commonsense reasoning tasks, such as CSQA (Talmor et al., 2019), SWAG (Zellers et al., 2018), and CODAH (Chen et al., 2019).\nCSQA is a question answering task and the other two are scene completion tasks, while all have a multiple-choice selection objective, as shown in Figure 1. These benchmarks are widely used to evaluate LMs for commonsense reasoning. Unfortunately, they are limited to English, not applicable to evaluate models of multilingual commonsense knowledge, which motivates us to create X-CSQA and X-CODAH. The goal of the recent XCOPA (Ponti et al., 2020) dataset shares a similar goal, but it only focused on event-based\ncausal reasoning in the scope of humans’ social behavior, which is thus arguably more culturally biased. In contrast, the X-CSQA and X-CODAH are mainly for evaluating general world knowledge and cover more fine-grained types of reasoning (e.g., quantitative, negation), and thus engage a more language-agnostic, comprehensive understanding of ML-LMs about common sense."
    }, {
      "heading" : "2.3 The LAMA Probe and Its Limitations",
      "text" : "The LAMA Probe (Petroni et al., 2019) is the seminal work on probing for common sense in (English) language models. It has a straightforward intuition: if a pretrained language model contains more commonsense knowledge, then it should be better at recalling a masked token in a commonsense assertion (e.g.,“birds have [mask]”). Specifically, given a LAMA-probe sentence s and its masked token wt, a LM under testing uses all past and future tokens — s\\t := w1, . . . , wt 1, wt+1, . . . , w|s| . as the input to rank all tokens in the vocabulary with the probability P\nwt | s\\t via zero-shot inference. One\ncan evaluate the performance of recalling common sense by measuring the position of a correct token “wing” in the ranked list. That is, the LAMA probe method uses token-level probability as a proxy to probe for common sense in LMs via ranking all tokens in their vocabularies.\nThis intuitive method, however, has several inherent limitations. First, in many other languages, multi-token concepts are ubiquitous, for example, “˛fÜ” (“library” in Simplified Chinese). Jiang et al. (2020) present several methods to decode multi-token entities so that they can adapt the LAMA probe to probe a LM for language-specific analysis. It is however infeasible to use tokenlevel probing tasks if we want to analyze ML-LMs across languages. In addition, the evaluation metric of the LAMA probe could be unfair, because there can be many correct words for a masked position (e.g., “birds have legs/eyes”). The ranking metrics of the LAMA probe, however, tend to ignore these facts, resulting in a less trustworthy analysis. The vocabulary-specific ranking is unfair when comparing across different languages, so they can have very different label space. These limitations of the LAMA Probe prevent us from analyzing common sense in ML-LM across topologically diverse languages."
    }, {
      "heading" : "3 The Mickey Probe",
      "text" : "The challenges of using the LAMA Probe for probing common sense in ML-LMs motivate us to propose a more suitable method for analyzing MLLMs, one that can fairly compare across a diverse set of languages. We present MICKEYPROBE, a Multilingual task for probing commonsense knowledge and analysis. We design a languageagnostic probing task with a sentence-selection objective for analyzing common sense of a MLLM: given a set of assertions (i.e., declarative sentences) that have similar words and syntactic features, select the one with highest commonsense plausibility. We present the task formulation in this section and then introduce how we collect the dedicated dataset in Section 4.\nNotations. We define a Mickey probe M as a set of K assertions in the same language, where one and only one of them (say, Mi) is the truth assertion with better commonsense plausibility than the other K 1 ones. Each Mickey probe M has multiple semantically equivalent versions in different languages. Let us denote a language by l 2 L where L = {en, fr, ru, zh, . . . } and |L| is the number of languages of interest. Then, M l is the probe M in the language l. For example, M en and M fr denote the probes with the same meaning but in English (en) and French (fr) respectively. We use M to denote a multilingual parallel dataset for MICKEYPROBE, which consists of T⇥|L|⇥K assertions. T is the number of MICKEYPROBE items and each item has K assertions and |L| language. Finally, we can formally describe a multilingual parallel dataset M for MICKEYPROBE:\n8M 2M, 8(lx, ly) 2 L2, 8i 2 NK ,\nM lxi ./ M ly i .\n(1)\nWe use the notation ./ to indicate two assertions in different languages (e.g., lx and ly) are semantically equivalent to each other. We leave the details of creating such an M in Section 4.\nCommonsense Probing Task. Given a Micky Probe M in the dataset M, and suppose the index of the truth assertion to be t, a perfect multilingual language model would produce sentence probabilities such that it always gives the truth assertion M lt the highest probability among other candidates for every language.\n8l 2 L, 8i 2 NK , P (M li )  P (M lt). (2)\nIt is still an open problem to properly compute sentence probabilities from masked language models, the recently proposed pseudo-loglikelihood scoring (PLLs) (Salazar et al., 2020) has shown promising results in many downstream NLP applications that need sentence re-ranking (e.g., speech recognition, and translation), suggesting it is a promising proxy of sentence probability. Given a sentence s, its PLL is defined as:\nlogP (s) = PLL(s) :=\n|s|X\ni=1\nlogP wi | s\\i (3)\nThat is, we individually mask each token wi at a time and use the remaining context s\\i to get the probability of a word wi in the sentence s. Finally, we aggregate them to approximate P (s).\nEvaluation Metric. The evaluation metric for MICKEYPROBE over a multilingual parallel dataset M in a specific language l is defined as the overall hit@k accuracy of the selection results hit@ k (l) =P\nM2M 1{truth-rank(M l)  k} / |M| where truth-rank(M l) means the the position of the truth assertion M lt in M l sorted by their probabilities defined in Eq. (3). The hit@1 is just equivalent to the conventional accuracy.\nAdvantages of MICKEYPROBE. There are two key advantages of the MICKEYPROBE for evaluating ML-LMs: (1) The sentence-level probability can be more generally applied in languages besides English, comparing with the LAMA probe which only studies single-token English words.\n(2) The task formulation creates a relatively closed-ended setting, such that we can use a language-independent evaluation metric to fairly compare across various languages within a MLLM and compare across various ML-LMs for a particular language. In addition, we can see LAMA Probe as a monolingual, word-level version of the more general MICKEYPROBE: the LAMA Probe is when L = {en}, and {M en} = M 2 M is a huge number of K assertions (i.e., the vocabulary size) — a fixed [mask] is replaced by all tokens in the vocabulary."
    }, {
      "heading" : "4 The Mickey Corpus and Evaluation",
      "text" : "We present a procedure for automatically creating a multilingual parallel dataset M for the probing task MICKEYPROBE. Our collected corpus, named MickeyCorpus , has 561k sentences in 11 languages (T =10.2k, K=5, |L|=11)."
    }, {
      "heading" : "4.1 Creating English Probes",
      "text" : "For the correct commonsense assertions in English, we have an existing resource, the OMCS corpus (Singh et al., 2002) which contains humanwritten sentences in English that describe commonsense facts. Each assertion can be used as a M ent and we perform perturbations on it to create the other K 1 distractor assertions (i.e., false candidates), yielding an M en example.\nInspired by BERT-attack method (Li et al., 2020), we use a simple method to generate false assertions that are semantically related and syntactically similar to the truth assertions. Given a correct assertion, we first randomly sample a few (1 ⇠ 3) words with a part-of-speech tag as noun, verb, or adjective, and replace them with [mask]. Then, we use a beam-search style method to decode the [mask] tokens one by one from left to right. To ensure that the distractors are less plau-\nsible, we limit the decoding steps to only sample tokens that ranks between 200th⇠300th. We repeat the above procedure multiple times with different sets of [mask] tokens. Then, we use Stanza (Qi et al., 2020) to remove distractors that have sequences of POS tags or morphological features different from the truth assertions. Finally, we sample K 1 of them as the distractors."
    }, {
      "heading" : "4.2 Scaling to Ten Other Languages.",
      "text" : "We use bidirectional translation with the MarianMT models (Junczys-Dowmunt et al., 2018) pretrained on the OPUS corpora (Tiedemann, 2016). We translate all English probes to the 25 languages that has models in both directions and then translate them back to English. As the outputs from these models might contain noise and errors, we compute the semantic similarities (i.e., cosine similarity) between the original M en and the backtranslated Mx-en via the SentenceBERT (Reimers and Gurevych, 2019) model.\nTo ensure the quality and fair comparisons, we set a similarity threshold as 0.75 and keep the intersections of probes in all languages. Considering some languages tend to have translations of lower quality, we finally choose the best 10 languages to build the Mickey Probe dataset for our analysis, yielding 10k examples in each language and 10.2k*5*11 ⇡ 561k sentences in total. The language set L =\n{en, de, fr, ru, es, hi, vi, bg, zh, nl, it}. Note that our purpose of checking the backtranslation quality here is mainly to only keep the high-quality translations for all language pairs that we considered. Conventional metrics, e.g., BLUE score (Papineni et al., 2002), which focus on the exact word match, are thus less suitable: given the original sentence “I have a book”, the translation results “I have a novel” and “I have a tool” will be seen as equally wrong. Inspired by BERTScore (Zhang et al., 2020), the BT-cosine is based on SentenceBERT, which efficiently gives a higher score for the former and a lower score for the latter, due to the semantic relatedness between “novel” and “book.” We observed that most of our back-translations are in similar situations, and thus decide to use BT-cosine instead of others."
    }, {
      "heading" : "4.3 Analyzing ML-LMs with Mickey Probes",
      "text" : "We now use the MickeyCorpus to evaluate the 5 pre-trained ML-LMs introduced in Section 2.1: d-mBERT (Sanh et al., 2019), mBERT (Devlin et al., 2019), XLM (Conneau and Lample, 2019), XLM-RBase, and XLM-RLarge (Conneau et al., 2020). All these ML-LMs pretraining objectives contain masked-word-prediction tasks, so we can easily use PPLs (Eq. 3) to probe them a zeroshot, supervision-free manner with hit@1 accuracy. (The hit@2 results are shown in Appendix.) We present a histogram in Figure 3 and show the concrete results in Table 1. We find that there are always large discrepancies across different languages in all tested ML-LMs, which motivates us to analyze the following questions.\nQ1: Do different ML-LMs have similar language preferences? No. We arrange the languages in all ML-LMs with the same order for Figure 3 — the monotonically descending order of XLM-RL. Interestingly, we find that different ML-LMs are good for different languages, resulting in a very diverse set of trends. For example, XLM-RB , has a higher performance in it than zh and fr, unlike XLM-R L which are pre-trained on the same corpora with the same objectives. mBERT and d-mBERT has stronger performance in fr than nl and de, unlike XLM and XLM-R.\nQ2: Does length influence PLL ranking? Not much. The PLL computation indeed tends to prefer shorter sequences (see Eq. 3), so one may wonder if the length of assertions would influence the probing results. The “Shortest” row in Table 1\npresents the results when we always select the shortest assertion within a probe, instead of PLL ranking. The gaps between these scores and XLMR-L’s suggest that the probing task indeed uses PLL as a valid proxy for evaluating common sense based on sentence-level semantics.\nQ3: Is the translation quality a key factor? We show “BT-Cosine”, the mean of the cosine scores between the original English sentences and the back-translated ones, and sort the table by these numbers. The first 5 languages, {de, it, es, fr, nl} have the largest BT-Cosine, i.e., the best translation quality, and they indeed have better performances in general for XLM-R models. However, although zh has a worse BT-score than vi, all MLLMs perform better in zh than vi. Thus, we believe the translation quality of MickeyCorpus will not be a factor to influence our understanding of ML-LMs. Consequently, this suggests that further study must depend on pre-training corpora of each ML-LM in different languages.\nQ4: Does the size of pre-training corpora matter? We list the size of the monolingual corpus in each language for CC-100 that XLM-R are pretrained on (i.e., the CC-size row). Although ru has a much larger corpus than de, it, etc., the XLMR performance in ru is much worse. In addition, fr and nl have almost the same translation quality while fr’s CC-size is twice the size of nl, but the performance in fr is still much worse than nl. We conjecture this would be either due to the design of sub-token vocabulary or the text quality (instead of the size) of the CC-100 corpora.\nFurther implications. The benchmark results of five popular ML-LMs on the MICKEYPROBE task over the MickeyCorpus offer the initial and valuable understanding with a closer look at the commonsense knowledge of ML-LMs by probing them in a unified evaluation protocol. One can either compare a ML-LM across different languages or compare a certain language across MLLMs in Table 1. These comparable results support further analysis that can benefit the development of ML-LMs in the future. After all, even the best ML-LM XLM-RL also degrades much in other languages, and also perform slightly worse than RoBERTaL in en (93.4%). We argue (cultureinvariant) common sense knowledge should be seen as an important way to connect multiple languages and thus better align them in a shared embedding space induced by a ML-LM."
    }, {
      "heading" : "5 Multilingual Contrastive Pre-Training",
      "text" : "In this section, we reformat the MICKEYPROBE so that we can reuse the MickeyCorpus for improving the pre-trained ML-LMs for commonsense reasoning beyond English. We propose a multilingual contrastive pre-training (MCP) task that focuses on enhancing the sentence-level representation of ML-LMs. MCP improves a MLLM in a multilingual, contrastive environment, where the model learns to select the assertion with the best commonsense plausibility from a set of contrastive sentences in different languages. Each MCP example is a set of multilingual assertions while each Mickey probe is a monolingual set.\nMCP Dataset Creation from M. We create pretraining examples for the MCP task by converting MICKEYPROBE examples, as shown in the steps illustrated in Algorithm 1. Simply put, we reformat a K-way Mickey Probe M (K ⇥ |L| assertions) to a MCP example by sampling a set of V candidate assertions in V different languages. We convert all examples in the MickeyCorpus M to build a new cross-lingual sentence-selection dataset C for learning the MCP task.\nMCP Learning. Given a MCP example C 2 C, we append one dense linear layer f on top of a ML-LM with parameters denoted as ⇥ML-LM for learning to predict the commonsense plausibility score of each assertion Ci 2 C as follows:\nhi = ML-LM(Ci).[CLS] (4) oi = f(hi;⇥f ) (5) zi = eoi\nPV=|C| j=1 e oj (6)\n⇢ = VX\ni=1\n1i log (zi) (7)\nWe first get the logit oi of each assertion by projecting its [CLS] embeddings hi to a logit oi via a dense layer f with parameters ⇥f ; Then, we use SoftMax to normalize the logits as plausibility scores zi; Finally, we compute the cross-entropy loss ⇢ where 1i=1 if Ci is a correct assertion and 0 otherwise. We fine-tune {⇥ML-LM,⇥f} to minimize the overall loss over the MCP dataset C."
    }, {
      "heading" : "6 Evaluation for Cross-lingual CSR",
      "text" : "In this section, we introduce the datasets, experimental setup, results, and our analysis.\nAlgorithm 1: Convert a Mickey Probe M to an example for the MCP task.\nIn: M 2M /* is a probe that has |L| sub-sets; each sub-set M lx is a set of K assertions in the same language lx 2 L. M lxt is always the truth. */ Out: C /* A set of V assertions in different languages. */ Remarks: n(X) is a function to randomly sample n unique elements from a set X .\n1 la 1(L) /* Pick an anchor language. */ 2 C {M lat } /* Initiate w/ the truth assertion. */\n/* Iterate each sampled distractor language li. */ 3 foreach li 2 V 1(L la) do /* Sample an index of distractor assertion. */ 4 j 1(NK {t}) /* Add a distractor assertion as a candidate. */ 5 C.add(M lij )"
    }, {
      "heading" : "6.1 X-CSQA & X-CODAH: Two New Benchmarks for Evaluating XCSR",
      "text" : "To evaluate ML-LMs for commonsense reasoning in a cross-lingual zero-shot transfer setting, we create two benchmark datasets, namely X-CSQA and X-CODAH. Table 3 shows the statistics of the two datasets. Specifically, we use online commercial services such as DeepL Pro Translate to collect high-quality translations of the examples in CSQA and CODAH for 15 languages other than English. The size of CODAH is small (only 2.7k), so we use 7k SWAG validation examples as additional training data which share the same formulation. We discuss the reduction of cultural differences and quality control of automatic translations as well as other details in Ethical Considerations (the paragraph for cultural bias reduction) and Appendix (A). As our goal is to evaluate different ML-LMs (instead of different languages) in a unified evaluation protocol for cross-lingual commonsense reasoning, we argue that such automatically translated examples, although might contain noise, can serve as a starting benchmark for us to obtain meaningful analysis before more humantranslated datasets will be available in the future."
    }, {
      "heading" : "6.2 Setup",
      "text" : "We focus on 4 popular ML-LMs that we introduced in Section 2.1: mBERT, XLM-100, XLMRB and XLM-RL as well as our proposed MCP method. For both tasks, we concatenate each prompt (the question or first sentence) and each\nof its options individually in the form of “[CLS] prompt [SEP] optioni [SEP]”. Then, we fine-tune ML-LMs over the English training dataset and test them on other languages.\nWhy zero-shot cross-lingual transfer? It is almost impossible to collect data in all languages that an NLU system might be used for. Therefore, prior works mainly focus on zero-shot crosslingual transfer (Conneau et al., 2018), which is more meaningful and can offer lower-bound performance analysis. It is also an ideal setting for studying CSR because most commonsense facts are language-invariant. Thus, an Englishfinetuned ML-LM for CSR should be able to transfer its ability to a wide range of other languages as well. Furthermore, our goal of this paper is to evaluate and improve ML-LMs, so translating back to English and then use an English-only LM is also not helpful towards to this end."
    }, {
      "heading" : "6.3 Experiments for Cross-lingual CSR",
      "text" : "In Table 2, we present the empirical results over X-CODAH and X-CSQA for the ML-LMs as well as two models enhanced by our proposed MCP method. On both tasks, the XLM-RL performs the best with a large margin. Enhanced by the MCP method, both XLM-RB and XLM-RL see significant improvement (e.g., 2.7% absolute improvement for XLM-RL on X-CSQA-avg).\nCan MCP’s improvement generalize to unseen, low-resource languages? Note that MCP dataset only involves 9 languages here, and there are 6 languages that are totally unseen in the MCP training (i.e., {pl, ar, ja, pt, sw, ur}). The largest performance gain is in ru on X-CSQA and vi on XCODAH. Surprisingly, we find the improvements on them are also large for XLM-RL (e.g., 48.4! 52.3 for ar). In addition, for the two low-resource languages sw and ur, MCP also brings 2 ⇠ 3 percentage points of improvement for XLM-RL. It is, however, not always the case for XLM-RB , which we conjecture tends to be more likely to overfit.\nAlthough ML-LMs enjoy the merits of zeroshot cross-lingual transfer, their performances are usually worse than the English-only RoBERTaL on the en-test (70.4% vs 66.7% for X-CSQA). Although MCP can mitigate the gap (70.4% vs 69.5%) for X-CSQA, there is still a large gap (81.6% vs 69.9%) for X-CODAH. We use Fig. 4 to analyze how different categories of commonsense reasoning in CODAH (Chen et al., 2019) are diverse in different languages. We find that others, reference, and negation have relatively smaller variances across different languages, as they are more language-invariant. However, a few polysemous, idioms examples can be Englishspecific which may not generalize to other languages. More detailed analysis is in Appendix.\nFrom the curve of dev accuracy in Figure 5, we see that MCP-enhanced XLM-R models are much more sample efficient and converge much faster than vanilla versions. This suggests that the MCP, if used on a larger corpus with broader topics, can potentially produce a better ML-LM with more general usage, especially when only limited labelled is available. Our results on XNLI-10% (using 10% of the training data) (Conneau et al., 2018) show that MCP-enhanced XLM-RL has 1.2 percent accuracy improvement on the average of 15 languages. As our focus in this paper is commonsense reasoning, we leave the study on other cross-lingual NLU tasks as future work. Importantly, our experiments imply that a proper (continual) pre-training task that has a (contrastive) sentence-level objective could improve both the final performance as well as learning efficiency."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We evaluate and improve popular multilingual language models (ML-LMs) for advancing commonsense reasoning beyond English. We propose the MICKEYPROBE, a language-agnostic probing task for analyzing common sense of ML-LMs in a\nzero-shot manner. With our proposed new benchmark datasets via automatic translation, X-CSQA and X-CODAH, we evaluate ML-LMs in a crosslingual transfer setting for commonsense reasoning. We also improve the state-of-the-art ML-LM with a simple yet effective method — multilingual contrastive pre-training, which uses a sentencelevel objective to enhance sentence representations, yielding a significant performance gain. All above work is based on MickeyCorpus, which can be used as both a probing dataset and a pretraining corpus for analyzing and improving MLLMs. We hope our resources and pre-training method for ML-LMs can help the community advance commonsense reasoning beyond English."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This research is supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via Contract No. 2019-19051600007, the DARPA MCS program under Contract No. N660011924033 with the United States Office Of Naval Research, the Defense Advanced Research Projects Agency with award W911NF-19-20271, and NSF SMA 18-29268. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. We would like to thank all the collaborators in USC INK research lab and the reviewers for their constructive feedback on the work.\n* Ethical Considerations\nResource Copyright This work presents three new resources: MickeyCorpus, X-CODAH, and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) 3, CSQA (Talmor et al., 2019)4, and CODAH (Chen et al., 2019)5 respectively. All these three original sources of the data are publicly available for free, and we do not add any additional requirement for accessing our resources. We will highlight the original sources of our data and ask users to cite the original papers when they use our extended versions for research.\n3https://github.com/commonsense/ conceptnet5/wiki/Downloads\n4https://www.tau-nlp.org/commonsenseqa 5https://github.com/Websail-NU/CODAH\nCultural Bias Reduction Like most most multilingual parallel resources, especially in general NLU domain, there exists potential data bias due to the barrier of languages as well as cultural differences (Acharya et al., 2020; Lin et al., 2018), which could induce the labeling differences on the same situation. For example, a question like “what do people usually drink in the morning? (coffee/tea/milk)” or “when does a wedding usually start? (morning/afternoon/evening)” might be answered very differently by people from different backgrounds and cultures, not to mention different languages. The prior English commonsense resources which our datasets are built on are already possess such inherent bias, even with in the English language. Therefore, before we translate CSQA and CODAH, we intentionally remove the examples that are either labeled as non-neutral by a pre-trained sentiment classifier, or contained any keywords that are relevant to social behavior (e.g., weddings). We manually inspect test examples in X-CSQA and X-CODAH in the English and Chinese versions and have a strong confidence there is few strongly controversial example. However, we admit that such reduction of cultural differences in common sense has not been systematically measured in this work for other languages.\nApplication Risks of Cross-lingual CSR.\nThe work also evaluates a few multilingual language models (ML-LMs) for cross-lingual commonsense reasoning (XCSR), and introduced a new model which outperforms them. This raises the question of whether harm might arise from applications of XCSR—or more generally, since XCSR is intended as a step toward making English-only CSR more applicable in other languages, whether harm might arise more generally from existing ML-LMs. Among the risks that need to be considered in any deployment of NLP technology are that responses may be wrong or biased, in ways that would lead to improperly justified decisions. Although in our view the current technology is still relatively immature, and unlikely to be fielded in applications that would cause harm of this sort, it is desirable that ML-LMs provide audit trails, and recourse so that their predictions can be explained to and critiqued by affected parties."
    }, {
      "heading" : "A Details for Dataset Construction",
      "text" : "Before we start the translation procedure, we first re-split the datasets of CSQA and CODAH such that the test set examples in the English language do not contain controversial examples or culturerelated examples that would potentially cause cultural bias in our dataset. Please refer to the section of Ethical Considerations (following the Conclusion) in the main paper for more details. Then, we use the DeepL Pro translation service to translate the 10 languages: {de, fr, es, pt, it, nl, pl, ru, jap, zh} and use Google Translation API to translate the others {ar, sw, ur, vi, hi}.\nWe agree that ideally we should use human experts to translate the examples in CSQA and CODAH, but the cost or building a large-scale multilingual dataset with the same scale of our datasets is extremely high – around 10k USD. As a matter of fact, most of the examples in CSQA and CODAH are very easy and short sentences, and most of them can be well translated by modern commercial translation APIs, because they usually have a hybrid system. Moreover, we choose the DeepL online service because it has been reported by many individual media as the best choice. To ensure the quality of the translation, we perform the translation for both directions and then use the same quality control method as we discussed in Section 4 for removing the examples that have lower cosine similarity between original English version and back-translated examples. During the process, we manually went through the Chinese versions to find a suitable threshold for taking the intersection — 0.85, which results in a comparable BT-cosine mean to the XNLI dataset 6."
    }, {
      "heading" : "B Hyper-parameters",
      "text" : "We summarize hyper-parameters that we used for training ML-LMs on X-CODAH and X-CSQA in\n6We sampled 1k examples in the test set and follow the same procedure for the intersection language set.\nTable 7. Evaluation Steps are equally 100 for all models and datasets. Maximum Sequence Length is 100 for X-CODAH and 64 for X-CSQA. The batch size here refers to “train batch size per device ⇥ # GPUs ⇥ # gradient accumulation steps”. Note that the MCP methods use the exactly the same hyper-parameters which we have found optimal by tuning over the dev set. The learning rates we tried for all models are from the range {3e-5, 2e-5, 1e-5, 8e-6, 6e-6, 5e-6}. The warm up steps are selected from {50, 100, 200, 300, 500}."
    }, {
      "heading" : "C Details of ML-LMs",
      "text" : "Table 4 shows the model architectures and sizes that we used from (Conneau et al., 2020). We show the tokenization (tnz) used by each Transformer model, the number of layers L, the number of hidden states of the model Hm, the dimension of the feed-forward layer Hff , the number of attention heads A, the size of the vocabulary V and the total number of parameters #params."
    }, {
      "heading" : "D Additional Experimental Results",
      "text" : "D.1 Hit@1 Accuracy in Histogram D.2 Hit@k Accuracy of Mickey Probes Table 5 shows the Hit@2 Accuracy of the five MLLMs for the MickeyProbe. Hit@2 Accuracy evaluates whether the models can rank the correct assertion within top 2. Unlike Hit@1 which only accepts best predictions, Hit@2 is more flexible. Thus, the performances in Hit@2 increase compared to the ones in Hit@1. We can see that the discrepancies across languages still exist.\nD.3 Categorized X-CODAH Analysis Please refer the CODAH (Chen et al., 2019) paper for the definition and concrete examples in each category. We show benchmark results of MCP(XLM-RL) on X-CODAH within different carriages in Table 6. The RB stands for using the RoBERTa-Large model to fine-tune on the English X-CODAH dataset. We find that the largest gaps in En are in the Idioms and the Others. Interestingly, we find that the quantities category is where MCP performs better than the RoBERTa large.\nModel lr # epoch # wus bsz\nX-CODAH\nmBERT 3E-05 20 100 128 XLM-100 1E-05 20 100 64 XLM-R-B 1E-05 20 100 128 XLM-R-L 6E-06 10 100 64\nMCP(XLM-R-B) 1E-05 20 100 128 MCP(XLM-R-L) 6E-06 10 100 64\nX-CSQA"
    } ],
    "references" : [ {
      "title" : "An atlas of cultural commonsense for machine reasoning",
      "author" : [ "A. Acharya", "Kartik Talamadupula", "Mark A. Finlayson." ],
      "venue" : "ArXiv, abs/2009.05664.",
      "citeRegEx" : "Acharya et al\\.,? 2020",
      "shortCiteRegEx" : "Acharya et al\\.",
      "year" : 2020
    }, {
      "title" : "CODAH: An adversarially-authored question answering dataset for common sense",
      "author" : [ "Michael Chen", "Mike D’Arcy", "Alisa Liu", "Jared Fernandez", "Doug Downey" ],
      "venue" : "In Proceedings of the 3rd Workshop on Evaluating Vector Space Representations",
      "citeRegEx" : "Chen et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages",
      "author" : [ "Jonathan H. Clark", "Eunsol Choi", "Michael Collins", "Dan Garrette", "Tom Kwiatkowski", "Vitaly Nikolaev", "Jennimaria Palomaki." ],
      "venue" : "Transactions of the",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Alexis Conneau", "Guillaume Lample." ],
      "venue" : "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,",
      "citeRegEx" : "Conneau and Lample.,? 2019",
      "shortCiteRegEx" : "Conneau and Lample.",
      "year" : 2019
    }, {
      "title" : "XNLI: Evaluating cross-lingual sentence representations",
      "author" : [ "Alexis Conneau", "Ruty Rinott", "Guillaume Lample", "Adina Williams", "Samuel Bowman", "Holger Schwenk", "Veselin Stoyanov." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods",
      "citeRegEx" : "Conneau et al\\.,? 2018",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "Commonsense reasoning and commonsense knowledge in artificial intelligence",
      "author" : [ "Ernest Davis", "Gary Marcus." ],
      "venue" : "Communications of the ACM, 58(9):92–103.",
      "citeRegEx" : "Davis and Marcus.,? 2015",
      "shortCiteRegEx" : "Davis and Marcus.",
      "year" : 2015
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Scalable multi-hop relational reasoning for knowledge-aware",
      "author" : [ "Yanlin Feng", "Xinyue Chen", "Bill Yuchen Lin", "Peifeng Wang", "Jun Yan", "Xiang Ren" ],
      "venue" : null,
      "citeRegEx" : "Feng et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2020
    }, {
      "title" : "XTREME: A Massively Multilingual Multitask Benchmark for Evaluating Cross-lingual Generalization",
      "author" : [ "Junjie Hu", "Sebastian Ruder", "Aditya Siddhant", "Graham Neubig", "Orhan Firat", "Melvin Johnson." ],
      "venue" : "Technical report.",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "X-FACTR: Multilingual factual knowledge retrieval from pretrained language models",
      "author" : [ "Zhengbao Jiang", "Antonios Anastasopoulos", "Jun Araki", "Haibo Ding", "Graham Neubig." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Jiang et al\\.,? 2020",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT-ATTACK: Adversarial attack against BERT using BERT",
      "author" : [ "Linyang Li", "Ruotian Ma", "Qipeng Guo", "Xiangyang Xue", "Xipeng Qiu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "XGLUE: A new benchmark datasetfor cross-lingual pre-training, understanding and generation",
      "author" : [ "nie Wu", "Shuguang Liu", "Fan Yang", "Daniel Campos", "Rangan Majumder", "Ming Zhou" ],
      "venue" : "In Proceedings of the 2020 Conference on Empirical Meth-",
      "citeRegEx" : "Wu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "KagNet: Knowledge-aware graph networks for commonsense reasoning",
      "author" : [ "Bill Yuchen Lin", "Xinyue Chen", "Jamin Chen", "Xiang Ren." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna-",
      "citeRegEx" : "Lin et al\\.,? 2019",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models",
      "author" : [ "Bill Yuchen Lin", "Seyeon Lee", "Rahul Khanna", "Xiang Ren." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Lin et al\\.,? 2020",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "Mining cross-cultural differences and similarities in social media",
      "author" : [ "Bill Yuchen Lin", "Frank F. Xu", "Kenny Zhu", "Seungwon Hwang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
      "citeRegEx" : "Lin et al\\.,? 2018",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2018
    }, {
      "title" : "Multilingual denoising pre-training for neural machine translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Transactions of the Association for Computational Lin-",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Language models as knowledge bases",
      "author" : [ "Fabio Petroni", "Tim Rocktäschel", "Sebastian Riedel", "Patrick Lewis", "Anton Bakhtin", "Yuxiang Wu", "Alexander Miller" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Petroni et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Petroni et al\\.",
      "year" : 2019
    }, {
      "title" : "XCOPA: A multilingual dataset for causal commonsense reasoning",
      "author" : [ "Edoardo Maria Ponti", "Goran Glavaš", "Olga Majewska", "Qianchu Liu", "Ivan Vulić", "Anna Korhonen." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Ponti et al\\.,? 2020",
      "shortCiteRegEx" : "Ponti et al\\.",
      "year" : 2020
    }, {
      "title" : "Stanza: A python natural language processing toolkit for many human languages",
      "author" : [ "Peng Qi", "Yuhao Zhang", "Yuhui Zhang", "Jason Bolton", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Qi et al\\.,? 2020",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2020
    }, {
      "title" : "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Masked language model scoring",
      "author" : [ "Julian Salazar", "Davis Liang", "Toan Q. Nguyen", "Katrin Kirchhoff." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2699–2712, Online. Association for Compu-",
      "citeRegEx" : "Salazar et al\\.,? 2020",
      "shortCiteRegEx" : "Salazar et al\\.",
      "year" : 2020
    }, {
      "title" : "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "ArXiv, abs/1910.01108.",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "Open mind common sense: Knowledge acquisition from the general public",
      "author" : [ "Push Singh", "Thomas Lin", "Erik T Mueller", "Grace Lim", "Travell Perkins", "Wan Li Zhu." ],
      "venue" : "OTM Confederated International Conferences\" On the Move to Meaningful Internet",
      "citeRegEx" : "Singh et al\\.,? 2002",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2002
    }, {
      "title" : "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
      "author" : [ "Alon Talmor", "Jonathan Herzig", "Nicholas Lourie", "Jonathan Berant." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Talmor et al\\.,? 2019",
      "shortCiteRegEx" : "Talmor et al\\.",
      "year" : 2019
    }, {
      "title" : "OPUS – parallel corpora for everyone",
      "author" : [ "Jörg Tiedemann." ],
      "venue" : "Proceedings of the 19th Annual Conference of the European Association for Machine Translation: Projects/Products, Riga, Latvia. Baltic Journal of Modern Computing.",
      "citeRegEx" : "Tiedemann.,? 2016",
      "shortCiteRegEx" : "Tiedemann.",
      "year" : 2016
    }, {
      "title" : "mT5: A massively multilingual pre-trained text-to-text transformer",
      "author" : [ "Linting Xue", "Noah Constant", "Adam Roberts", "Mihir Kale", "Rami Al-Rfou", "Aditya Siddhant", "Aditya Barua", "Colin Raffel." ],
      "venue" : "Proceedings of the 2021 Conference of the North",
      "citeRegEx" : "Xue et al\\.,? 2021",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2021
    }, {
      "title" : "SWAG: A large-scale adversarial dataset for grounded commonsense inference",
      "author" : [ "Rowan Zellers", "Yonatan Bisk", "Roy Schwartz", "Yejin Choi." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Zellers et al\\.,? 2018",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2018
    }, {
      "title" : "Bertscore: Evaluating text generation with BERT",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "It is thus of vital importance to evaluate and improve the commonsense reasoning capability of language models (LMs), towards building general natural language understanding (NLU) systems (Davis and Marcus, 2015).",
      "startOffset" : 188,
      "endOffset" : 212
    }, {
      "referenceID" : 18,
      "context" : "As shown in Figure 1, the LAMA probe (Petroni et al., 2019) is for analyzing LMs’ zero-shot commonsense recalling ability; CommonsenseQA (CSQA) (Talmor et al.",
      "startOffset" : 37,
      "endOffset" : 59
    }, {
      "referenceID" : 25,
      "context" : ", 2019) is for analyzing LMs’ zero-shot commonsense recalling ability; CommonsenseQA (CSQA) (Talmor et al., 2019) is instead a multiple-choice QA task that needs fine-tuning; CODAH (Chen et al.",
      "startOffset" : 92,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : ", 2019) is instead a multiple-choice QA task that needs fine-tuning; CODAH (Chen et al., 2019)",
      "startOffset" : 75,
      "endOffset" : 94
    }, {
      "referenceID" : 28,
      "context" : "and SWAG (Zellers et al., 2018) focus on the ability to complete the most plausible scenes.",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 13,
      "context" : "Consequently, follow-up analysis and reasoning methods developed (Lin et al., 2019; Feng et al., 2020; Lin et al., 2020) also focus only on English LMs like BERT (Devlin et al.",
      "startOffset" : 65,
      "endOffset" : 120
    }, {
      "referenceID" : 8,
      "context" : "Consequently, follow-up analysis and reasoning methods developed (Lin et al., 2019; Feng et al., 2020; Lin et al., 2020) also focus only on English LMs like BERT (Devlin et al.",
      "startOffset" : 65,
      "endOffset" : 120
    }, {
      "referenceID" : 14,
      "context" : "Consequently, follow-up analysis and reasoning methods developed (Lin et al., 2019; Feng et al., 2020; Lin et al., 2020) also focus only on English LMs like BERT (Devlin et al.",
      "startOffset" : 65,
      "endOffset" : 120
    }, {
      "referenceID" : 7,
      "context" : ", 2020) also focus only on English LMs like BERT (Devlin et al., 2019).",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 19,
      "context" : "Such English-centric trend of commonsense reasoning studies not only limits our research scope, but also tends to exacerbate English-specific bias that might prevent future methods from generalizing beyond English (Ponti et al., 2020).",
      "startOffset" : 214,
      "endOffset" : 234
    }, {
      "referenceID" : 9,
      "context" : "develop NLU systems that can serve all languages in the world to bridge the gap between different cultures and eliminate language barriers (Hu et al., 2020), and multilingual language models (MLLMs), such as XLM-R (Conneau et al.",
      "startOffset" : 139,
      "endOffset" : 156
    }, {
      "referenceID" : 3,
      "context" : ", 2020), and multilingual language models (MLLMs), such as XLM-R (Conneau et al., 2020), are among the most promising tools to achieve this ambitious goal.",
      "startOffset" : 65,
      "endOffset" : 87
    }, {
      "referenceID" : 9,
      "context" : ", 2018) and XTEMRE (Hu et al., 2020), it is still relatively unclear how ML-LMs perform in commonsense reasoning tasks, due to the lack of 1) dedicated methods for probing common sense in ML-LMs and 2) multilingual benchmark datasets for commonsense reasoning.",
      "startOffset" : 19,
      "endOffset" : 36
    }, {
      "referenceID" : 22,
      "context" : ", declarative sentences) in the same language by their commonsense plausibility, for which we use pseudo-likelihood (PLL) (Salazar et al., 2020) as a proxy.",
      "startOffset" : 122,
      "endOffset" : 144
    }, {
      "referenceID" : 23,
      "context" : "The distil-mBERT (d-mBERT) (Sanh et al., 2019) is a smaller mBERT trained by knowledge distillation.",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 16,
      "context" : "We leave the analysis of recent seq2seq ML-LMs, such as mBART (Liu et al., 2020) and mT5 (Xue et al.",
      "startOffset" : 62,
      "endOffset" : 80
    }, {
      "referenceID" : 27,
      "context" : ", 2020) and mT5 (Xue et al., 2021), as future work, because their architectures are significantly different from the other ML-LMs.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 2,
      "context" : ", 2020), TyDi QA(Clark et al., 2020), and XGLUE(Liang et al.",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 25,
      "context" : "These existing cross-lingual benchmarks have not covered commonsense reasoning tasks, such as CSQA (Talmor et al., 2019), SWAG (Zellers et al.",
      "startOffset" : 99,
      "endOffset" : 120
    }, {
      "referenceID" : 28,
      "context" : ", 2019), SWAG (Zellers et al., 2018), and CODAH (Chen et al.",
      "startOffset" : 14,
      "endOffset" : 36
    }, {
      "referenceID" : 19,
      "context" : "The goal of the recent XCOPA (Ponti et al., 2020) dataset shares a similar goal, but it only focused on event-based causal reasoning in the scope of humans’ social behavior, which is thus arguably more culturally",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 18,
      "context" : "The LAMA Probe (Petroni et al., 2019) is the seminal work on probing for common sense in (English) language models.",
      "startOffset" : 15,
      "endOffset" : 37
    }, {
      "referenceID" : 22,
      "context" : "It is still an open problem to properly compute sentence probabilities from masked language models, the recently proposed pseudo-loglikelihood scoring (PLLs) (Salazar et al., 2020)",
      "startOffset" : 158,
      "endOffset" : 180
    }, {
      "referenceID" : 24,
      "context" : "For the correct commonsense assertions in English, we have an existing resource, the OMCS corpus (Singh et al., 2002) which contains humanwritten sentences in English that describe commonsense facts.",
      "startOffset" : 97,
      "endOffset" : 117
    }, {
      "referenceID" : 11,
      "context" : "Inspired by BERT-attack method (Li et al., 2020), we use a simple method to generate false assertions that are semantically related and syntactically similar to the truth assertions.",
      "startOffset" : 31,
      "endOffset" : 48
    }, {
      "referenceID" : 20,
      "context" : "Then, we use Stanza (Qi et al., 2020) to remove distractors that have sequences of POS tags or morphological features different from the truth assertions.",
      "startOffset" : 20,
      "endOffset" : 37
    }, {
      "referenceID" : 26,
      "context" : ", 2018) pretrained on the OPUS corpora (Tiedemann, 2016).",
      "startOffset" : 39,
      "endOffset" : 56
    }, {
      "referenceID" : 17,
      "context" : "BLUE score (Papineni et al., 2002), which focus on the exact word match, are thus less suitable: given the original sentence “I have a book”, the translation results “I have a novel” and “I have a tool” will be seen as equally wrong.",
      "startOffset" : 11,
      "endOffset" : 34
    }, {
      "referenceID" : 29,
      "context" : "Inspired by BERTScore (Zhang et al., 2020), the BT-cosine is based on SentenceBERT, which efficiently gives a higher score for the former and a lower score for the latter, due to the semantic relatedness between “novel” and “book.",
      "startOffset" : 22,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : ", 2019), XLM (Conneau and Lample, 2019), XLM-RBase, and XLM-RLarge (Conneau et al.",
      "startOffset" : 13,
      "endOffset" : 39
    }, {
      "referenceID" : 3,
      "context" : ", 2019), XLM (Conneau and Lample, 2019), XLM-RBase, and XLM-RLarge (Conneau et al., 2020).",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 5,
      "context" : "fore, prior works mainly focus on zero-shot crosslingual transfer (Conneau et al., 2018), which is more meaningful and can offer lower-bound performance analysis.",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : "sense reasoning in CODAH (Chen et al., 2019) are diverse in different languages.",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 5,
      "context" : "Our results on XNLI-10% (using 10% of the training data) (Conneau et al., 2018) show that MCP-enhanced XLM-RL has 1.",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 24,
      "context" : "and X-CSQA, which are multilingual extension of the OMCS (Singh et al., 2002) 3, CSQA (Talmor et al.",
      "startOffset" : 57,
      "endOffset" : 77
    }, {
      "referenceID" : 25,
      "context" : ", 2002) 3, CSQA (Talmor et al., 2019)4, and CODAH (Chen et al.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 1,
      "context" : ", 2019)4, and CODAH (Chen et al., 2019)5 respectively.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "tilingual parallel resources, especially in general NLU domain, there exists potential data bias due to the barrier of languages as well as cultural differences (Acharya et al., 2020; Lin et al., 2018), which could induce the labeling differences on the same situation.",
      "startOffset" : 161,
      "endOffset" : 201
    }, {
      "referenceID" : 15,
      "context" : "tilingual parallel resources, especially in general NLU domain, there exists potential data bias due to the barrier of languages as well as cultural differences (Acharya et al., 2020; Lin et al., 2018), which could induce the labeling differences on the same situation.",
      "startOffset" : 161,
      "endOffset" : 201
    } ],
    "year" : 2021,
    "abstractText" : "Commonsense reasoning research has so far been limited to English. We aim to evaluate and improve popular multilingual language models (ML-LMs) to help advance commonsense reasoning (CSR) beyond English. We collect the Mickey corpus, consisting of 561k sentences in 11 different languages, which can be used for analyzing and improving MLLMs. We propose Mickey Probe, a languageagnostic probing task for fairly evaluating the common sense of popular ML-LMs across different languages. In addition, we also create two new datasets, X-CSQA and X-CODAH, by translating their English versions to 15 other languages, so that we can evaluate popular ML-LMs for cross-lingual commonsense reasoning. To improve the performance beyond English, we propose a simple yet effective method — multilingual contrastive pretraining (MCP). It significantly enhances sentence representations, yielding a large performance gain on both benchmarks (e.g., +2.7% accuracy for X-CSQA over XLM-RL).",
    "creator" : "Preview"
  }
}