{
  "name" : "2021.acl-long.310.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Towards User-Driven Neural Machine Translation",
    "authors" : [ "Huan Lin", "Liang Yao", "Baosong Yang", "Dayiheng Liu", "Haibo Zhang", "Weihua Luo", "Degen Huang", "Jinsong Su" ],
    "emails" : [ "huanlin@stu.xmu.edu.cn", "yaoliang.yl@alibaba-inc.com", "yangbaosong.ybs@alibaba-inc.com", "liudayiheng.ldyh@alibaba-inc.com", "zhanhui.zhb@alibaba-inc.com", "weihua.luowh@alibaba-inc.com", "huangdg@dlut.edu.cn", "jssu@xmu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4008–4018\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4008"
    }, {
      "heading" : "1 Introduction",
      "text" : "In recent years, neural machine translation (NMT) models (Sutskever et al., 2014; Luong et al., 2015; Vaswani et al., 2017) have shown promising quality and thus increasingly attracted users. When drawing on a translation system, every user has his own traits, including topic preference, stylistic characteristics, and expression habits, which can be implicitly embodied in their behavior, e.g., the historical inputs of these users. A good translation should implicitly mirror user traits rather than\n∗ Jinsong Su is the corresponding author. This work was done when Huan Lin was interning at DAMO Academy, Alibaba Group.\n1We release our source code and the associated benchmark at https://github.com/DeepLearnXMU/ User-Driven-NMT.\nmerely translate the original content, as the example shown in Figure 1. However, current NMT models are mainly designed for the semantic transformation between the source and target sentences regardless of subtle traits with respect to user behavior. It can be said that the effect of user behavior on translation modeling is still far from utilization, which, to some extent, limits the applicability of NMT models in real-world scenarios.\nMore recently, several studies have shown that the prominent signals in terms of personal characteristics can be served as inductive biases and reflected in translation results using domain adaptation approaches, such as personality (Mirkin et al., 2015), gender (Rabinovich et al., 2017), and politeness (Sennrich et al., 2016a). However, previously explored signals characterize users from a single dimension, which insufficiently represent fine-grained user traits. Furthermore, Michel and Neubig (2018) pay their attention to personalized TED talk translation, in which they train a speakerspecific bias to revise the prediction distribution. In contrast with these studies, our work investigates a more realistic online scenario: a real-world MT system serves extensive users, where the user-behavior annotated data covering all users is unavailable. Previous methods (Mirkin et al., 2015; Michel and Neubig, 2018) require the users in the training set and the test set to be consistent, therefore can not\ndeal with this zero-shot issue. Starting from this concern, we explore userdriven NMT that generates personalized translations for users unseen in the training dataset according to their behavior. Specifically, we choose the historical inputs to represent user behavior since they can not only be easily obtained in the real-world scenarios, but also reflect the topic preference, stylistic characteristic, and context of user. Moreover, compared with pre-defined or userspecific labels, historical inputs can be updated with current source sentences, which is also in line with realistic scenario.\nIn this work, we propose a novel framework for this task, where the NMT model is equipped with a cache module to restore and update historical inputs. Besides, in order to further transfer the traits from the seen users to the unseen ones, we design a regularization framework based on contrastive learning (Bose et al., 2018; Yang et al., 2019), which forces our model to decrease the divergence between translations of similar users while increasing the diversity on dissimilar users.\nIn order to further train and assess the proposed framework, we construct a new User-Driven Machine Translation dataset called UDT-Corpus. This corpus consists of 6,550 users with totally 57,639 Chinese sentences collected from a realworld online MT system. Among them, 17,099 Chinese sentences are annotated with their English translations by linguistic experts according to the user-specific historical inputs. Experimental results demonstrate that the proposed framework facilitates the translation quality, and exactly generates diverse translations for different users.\nTo summarize, major contributions of our work are four-fold:\n• We introduce and explore user-driven NMT task that leverages user behavior to enhance translation model. We hope our study can attract more attention to explore techniques on this topic. • We propose a novel framework for user-driven NMT based on cache module and contrastive learning, which is able to model user traits in zero-shot scenarios. • We collect UDT-Corpus and make it publicly available, which may contribute to the subsequent researches in the communities of NMT and user-driven models. • Extensive analyses indicate the effectiveness\nof our work and verify that NMT can profit from user behavior to generate diverse translations conforming to user traits."
    }, {
      "heading" : "2 Related Work",
      "text" : "This section mainly includes the related studies of personalized machine translation, cache-based NMT and contrastive learning for NMT.\nPersonalized Machine Translation Recently, some researchers have employed domain adaptation (Zhang et al., 2019; Gururangan et al., 2020; Yao et al., 2020) to generate personalized translations. For example, Mirkin et al. (2015) show that the translation generated by the SMT model has an adverse effect on the prediction of author personalities, demonstrating the necessity of personalized machine translation. Furthermore, Sennrich et al. (2016a) control the politeness in the translation by adding a politeness label on the source side. Rabinovich et al. (2017) explore a gender-personalized SMT system that retains the original gender traits. These domain labels represent users in single dimension separately, which are insufficient to distinguish large-scale users in a fine-grained way. The most correlated work to ours is Michel and Neubig (2018) which introduces a speaker-specific bias into the conventional NMT model. However, these methods are unable to deal with users unseen at the training time. Different from them, user-driven NMT can generate personalized translations for these unseen users in a zero-shot manner.\nCache-Based Machine Translation Inspired by the great success of cache on language modeling (Kuhn and de Mori, 1990; Goodman, 2001; Federico et al., 2008), Nepveu et al. (2004) propose a cache-based adaptive SMT system. Tiedemann (2010) explore a cache-based translation model that fills the cache with bilingual phrase pairs extracted from previous sentence pairs in a document. Bertoldi et al. (2013) use a cache mechanism to achieve online learning in phrase-based SMT. Gong et al. (2011), Kuang et al. (2018), and Tu et al. (2018) further exploit cache-based approaches to leverage contextual information for document-level machine translation. Contrast with the documentlevel NMT that learns to capture contextual information, our study aims at modeling user traits, such as, topic preference, stylistic characteristics, and expression habits. Moreover, historical inputs of user has relatively fewer dependencies than the contexts\nused in document-level translation.\nContrastive Learning for NMT Contrastive learning has been extensively applied in the communities of computer vision and natural language processing due to its effectiveness and generality on self-supervised learning (Vaswani et al., 2013; Mnih and Kavukcuoglu, 2013; Liu and Sun, 2015; Bose et al., 2018). Towards raising the ability of NMT in capturing global dependencies, Wiseman and Rush (2016) first introduce contrastive learning into NMT, where the ground-truth translation and the model output are considered as the positive and contrastive samples, respectively. Yang et al. (2019) construct contrastive examples by deleting words from ground-truth translation to reduce word omission errors in NMT. Contrast to these studies, we employ contrastive learning to create broader learning signals for our user-driven NMT model, where the prediction distribution of translations with respect to similar users and dissimilar users are considered as positive and contrastive samples, respectively. Thus, our model can better transfer the knowledge of the seen users to the unseen ones."
    }, {
      "heading" : "3 User-Driven Translation Dataset",
      "text" : "In order to build a user-driven NMT system, we construct a new dataset called UDT-Corpus containing 57,639 inputs of 6,550 users, 17,099 among them are Chinese-to-English translation examples."
    }, {
      "heading" : "3.1 Data Collection and Preprocessing",
      "text" : "We collect raw examples from Alibaba Translate2 which contain the user inputs and the translations given by the translation system.\nFor data preprocessing, we first anonymize data and perform data deduplication within each user. Then, we utilize a pre-trained n-gram language model KenLM3 to filter out translation examples with low-quality source data. Moreover, we remove such pairs whose source sentence is shorter than 2 words or longer than 100 words."
    }, {
      "heading" : "3.2 Data Annotation",
      "text" : "In the corpus, we represent each translation example as a triplet 〈X(u), Y (u), H(u)〉, where H(u) is the historical inputs of the user u, X(u) is the current source sentence and Y (u) is the target translation sentence annotated with H(u). To obtain\n2https://www.aliyun.com/product/ai/ base_alimt\n3https://github.com/kpu/kenlm.\nsuch a triplet, we first sequentially sample up to 10 source sentences which are the historical inputs of each user. Then, for the given historical inputs, we collect their followed source input paired with the pseudo translation given by the translation system. Afterwards, we assign these historical inputs and the current input pairs to two professional annotators and ask them to revise the pseudo translation according to the source sentence and historical inputs. Specifically, we first ask one of them to annotate and the other to evaluate, and then resolve annotation disagreements by reviewing. During annotation, 91.8% of the original data are revised. Moreover, annotators are asked to record whether their revision is affected by user history. The result shows that 76.25% of the sentences are impacted."
    }, {
      "heading" : "4 User-Driven NMT Framework",
      "text" : "In this section, we first give a brief description about the problem formulation of user-driven NMT, and then introduce our proposed framework in detail. We choose Transformer (Vaswani et al., 2017) as the basic NMT model due to its competitive performance. In fact, our framework is transparent and applicable to other NMT models.\nFigure 2 illustrates the basic framework of the proposed user-driven NMT. Most typically, we equip the NMT model with two user-specific caches to exploit user behavior for better translation (See Section § 4.2). Besides, we augment the conventional NMT training objective with contrastive learning, which allows the model to learn translation diversity across users (See Section § 4.3)."
    }, {
      "heading" : "4.1 Problem Formulation",
      "text" : "Given the source sentence X and the previously generated words Y<i = y1, ..., yi−1, the conventional NMT model with parameter θ predicts the current target word yi by P (yi|X,Y<i; θ). As a significant extension of conventional NMT, userdriven NMT with parameter θ aims to model P ( y (u) i |X(u), Y (u) <i , u; θ ) , that is, generates the translation that can reflect the traits of user u. Unlike previous studies (Mirkin et al., 2015; Michel and Neubig, 2018) only caring for generating translations for users seen at the training time, our userdriven NMT mainly focuses on a more realistic online MT scenario, where the users for testing are unseen in the training dataset. Moreover, the conventional domain adaptation methods can not be directly applied to this zero-shot scenario."
    }, {
      "heading" : "4.2 Cache-based User Behavior Modeling",
      "text" : "Due to the advantages of cache mechanism on dynamic representations (Gong et al., 2011; Kuang et al., 2018; Tu et al., 2018), we equip the conventional Transformer-based NMT model with two user-specific caches to leverage user behavior for NMT: 1) topic cache c(u)t that aims at capturing the global and long-term traits of user u; and 2) context cache c(u)c , which is introduced to capture the short-term traits from the recent source inputs of user u. During this process, we focus on the following three operations on cache:\nCache Representation In order to facilitate the efficient computation of the user behavior encoded by our caches, we define each cache as an embedding sequence of keywords. We first calculate TF-IDF values of input words, and then extract words with TF-IDF weights higher than a predefined threshold to represent user behavior.\nNote that the calculation of TF-IDF value of a word mainly depends on its frequency in the document and inverse document frequency in the corpus. Since two caches play different roles in the userdriven NMT model, we identify keywords for two caches based on different definitions of “document”\nand “corpus”. Specifically, when constructing topic cache c(u)t , we treat the historical inputs H(u) of the user u as the “document” and the historical inputs H(u) of all users U as the “corpus”, then define topic cache c(u)t as an embedding sequence of historical keywords. Unlike the topic cache, for context cache c(u)c , we individually consider the current source sentence X(u) and historical inputsH(u) as the TF-IDF “document” and “corpus”, defining c(u)c as an embedding sequence of current keywords.\nBesides, in the real-world MT scenario, there exists a large number of users without any historical input. For these users, we find the most similar user according to the cosine similarity based on their TF-IDF bag-of-word representations of topic keywords, and initialize the corresponding topic cache with that of the most similar user.\nUpdating Caches When using an online MT system, users often continuously input multiple sentences. Thus, our caches should be dynamically updated to ensure the accurate encoding of user behavior.\nTo update topic cache, we first recalcualte the TF-IDF values of all historical input words, so as to redetermine the keywords stored in this cache. As for context cache, we consider it as a filter window sliding across historical inputs, and apply first-infirst-out rule to replace its earliest keywords with the recently input ones.\nReading from Caches During the translation of the NMT model, we perform a gating operation on c(u)t and c (u) c , producing a vector r(u) that reflects user behavior as follows:\nr(u) = αc (u) t + (1− α)c(u)c (1) α = Sigmoid(Wtc (u) t + Wrc (u) c ), (2)\nc (u) t = MeanPooling [ c(u)t ] , (3)\nc(u)c = MeanPooling [ c(u)c ] , (4)\nwhere both Wt and Wr are learnable parameter matrices. Then, we directly add r(u) into the embedding sequence of original current source sentence X(u), forming a source embedding sequence with user behavior as follows:\nX̂(u) = {x(u)i + r (u)}1≤i<|X(u)|. (5)\nFinally, the NMT model is fed with X̂(u) to generate the translation for u. Due to the limitation\nof pages, we omit the detailed descriptions of the NMT model. Please refer to Vaswani et al. (2017) for the details."
    }, {
      "heading" : "4.3 Model Training with a Contrastive Loss",
      "text" : "Given training instances 〈X(u), Y (u), H(u)〉, we train the user-driven NMT model using the following objective function:\nL = Lmle + Lcl. (6)\nHere, Lmle is the maximum likelihood translation loss extended from the conventional NMT training objective. Formally, it is defined as:\nLmle = ∑ i − logP (y(u)i |X (u), Y (u) <i , H (u); θ).\n(7) Lcl is a triplet-margin-based constrastive loss, which allows the NMT model to learn the translation diversity across users.\nSpecifically, for an input sentence, an ideal userdriven NMT model should be able to generate translations with non-divergent user traits for similar users, while producing translations with diverse user traits for dissimilar users. However, using only Lmle cannot guarantee this since it separately considers each training instance during the model training. To deal with this issue, for each training instance 〈X(u), Y (u), H(u)〉, we first determine the most similar user u+ according to the cosine similarity based on their bag-of-keyword representations, and randomly select a user without any same keyword as the dissimilar user u− of u. Finally, using historical inputs of u+ and u−, we construct several pseudo training instances to define Lcl as follows:\nLcl = ∑ u∈U max[d(X(u), Y (u), H(u), H(u +)) (8)\n− d(X(u), Y (u), H(u), H(u−)) + η, 0],\nwhere d ( X(u), Y (u), H(u), H(u +) )\n= || 1 |Y (u)| ∑ i logP ( y (u) i |X (u), Y (u) <i , H (u) ) − 1 |Y (u)| ∑ i logP ( y (u) i |X (u), Y (u) <i , H (u+) ) ||2\n(9)\nand η is a predefined threshold, which is set to 2 in our experiments. Here, we omit the definition of\nd ( X(u), Y (u), H(u), H(u −) ) , which is similar to\nd ( X(u), Y (u), H(u), H(u +) )\n. Formally, Lcl will encourage the NMT model to minimize the prediction difference between the training instances 〈X(u), Y (u), H(u)〉 and 〈X(u), Y (u), H(u+)〉, and maximize the difference between the training instances 〈X(u), Y (u), H(u)〉 and 〈X(u), Y (u), H(u−)〉. In this way, the NMT model can not only exploit pesudo training instances, but also produce more consistent translations with user traits."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we carry out several groups of experiments to investigate the effectiveness of our proposed framework on UDT-Corpus."
    }, {
      "heading" : "5.1 Setup",
      "text" : "We develop the user-driven NMT model based on Open-NMT Transformer (Klein et al., 2017), and adopt a two-stage strategy to train this model: we first pre-train a Transformer-based NMT model on the WMT2017 Chinese-to-English dataset, and then fine-tune this model to our user-driven NMT model using UDT-Corpus.\nDatasets The WMT2017 Chinese-to-English dataset is composed of the News Commentary v12, UN Parallel Corpus v1.0, and CWMT corpora, with totally 25M parallel sentences. To fine-tune our model, we split UDT-Corpus into training, validation and test set, respectively. Table 1 provides more detailed statistics of these datasets. To improve the efficiency of model training, we train the model using only parallel sentences with no more than 100 words. Following common practices, we employ byte pair encoding (Sennrich et al., 2016b) with 32K merge operations to deal with all sentences.\nTraining Details Following Vaswani et al. (2017), we use the following hyper-parameters: the word embedding dimension is set to 512, the hidden layer dimension is 2048, the layer numbers of\nboth encoder and decoder are set to 6, and the number of attention heads is set to 8. Besides, we use 4 GPUs for training. At the pre-training stage, we employ the Adam optimizer with β2 = 0.998. We use the batch size of 16,384 tokens and pre-train the model for 200,000 steps. Particularly, we adopt the dropout strategy (Srivastava et al., 2014) with rate 0.1 to enhance the robustness of our model. When fine-tuning the model, we keep the other settings consistent with the pre-training stage, but reduce the batch size to 2048 tokens and fine-tune the model with early-stopping strategy.\nEvaluation We assess the translation quality with two metrics: one is case-insensitive BLEU (mteval-v13a.pl, Papineni et al., 2002)4 and the other is METEOR5 (Denkowski and Lavie, 2011)."
    }, {
      "heading" : "5.2 Baselines",
      "text" : "We represent our user-driven NMT model as UDNMT and compare it with the following baselines:\n• TF. It is a Transformer-based NMT model pretrained on the WMT2017 corpus. This model yields 24.61 BLEU score on WMT2017 Chinese-to-English translation task, which is comparable with reported results in (Wan et al., 2020; Zhou et al., 2020), which makes our subsequent experiments convincing. • TF-FT. This model is also a Transformerbased NMT model that is further fine-tuned on the parallel sentences of UDT-Corpus. • TF-FT + PesuData. This model is a variant of TF-FT. When constructing it, we pair historical inputs with their translations produced by our online translation system, forming additional data for fine-tuning TF-FT. • TF-FT + ConcHist (Tiedemann and Scherrer, 2017). In this model, we introduce user behavior into TF-FT by concatenating each input sentence with several historical inputs. We mark all tokens in historical inputs with a special prefix to indicate that they are additional information. • TF-FT + UserBias (Michel and Neubig, 2018). It introduces user-specific biases to refine softmax-based predictions of Transformer NMT model. We change it to a zeroshot method similar to (Farajian et al., 2017)\n4https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ generic/multi-bleu.perl\n5https://github.com/cmu-mtlab/meteor\nsince (Michel and Neubig, 2018) can not be directly applied to our scenario. In particular, we replace the user ID in the test set with that of the most similar user in the training set.\nNote that the first two baselines, e.g., TF and TFFT, are conventional NMT models without exploiting user behavior."
    }, {
      "heading" : "5.3 Effect of Cache Sizes",
      "text" : "Since cache size directly determines the utility of user behavior, we investigate its effect on the performance of UD-NMT. We denote the sizes of topic cache and context cache as st and sc for simplicity.\nFigure 3 lists the performance of our model with different st and sc on validation set. We observe that st larger than 25 and sc larger than 35 do not lead to significant improvements. For this result, we speculate that small cache sizes are unable to capture sufficient user behavior for NMT. However, since the number of keywords are limited, larger cache sizes only bring limited information gain. Therefore, we directly use st = 25 and sc = 35 in the subsequent experiments."
    }, {
      "heading" : "5.4 Main Results",
      "text" : "From Table 2, we observe that our UD-NMT model consistently outperforms all baselines in terms of two metrics. Moreover, we draw several interesting conclusions:\n1) All NMT models leveraging user behavior surpass vanilla models, including TF, TF-FT, showing that user behavior is useful for NMT.\n2) UD-NMT exhibits better than TF-FT + PesuData, which uses the same training data as ours. The underlying reason is that UD-NMT can leverage user traits to generate better translations.\n3) Although both TF-FT + UserBias and UDNMT exploit user behavior for NMT, UD-NMT achieves better performance than TF-FT + UserBias without introducing extra parameters. This result demonstrates the advantage of cache on modeling user behavior than introducing user-specific biases into model parameters."
    }, {
      "heading" : "5.5 Ablation Study",
      "text" : "To explore the effectiveness of different components in our model, we further compare UD-NMT with its several variants, as shown in Table 3.\nParticularly, we propose to evaluate translations using the following variant metrics: s-BLEU, sSim., d-BLEU and d-Sim.. When using s-BLEU, we replace the topic cache of current user with that of his most similar user. Keeping the same current input, we calculate the BLEU score with ground-truth as reference and the translation for this similar user as hypothesis. As for s-Sim., we adopt the same strategy as s-BLEU, but use the translation for original user as reference to evaluate the BLEU score. In other words, s-BLEU and dBLEU assesses the translation quality given unsuitable user. Therefore, higher s-BLEU and d-BLEU indicates better model robustness, while s-BLEU and d-BLEU measures how much the translation changes given different user. Thus lower s-Sim. and d-Sim. show larger translation diversity.\nOur conclusions are shown as follows: 1) w/o topic cache. To build this variant, we remove topic cache from our model. The result in Line 2 indicates that removing topic cache leads to\na performance drop, suggesting that topic cache is useful for modeling user behavior.\n2) w/o context cache. Unlike the above variant, we only use topic cache to represent user traits in this variant. According to the results shown in Line 3, we observe that this change results in a significant performance decline of our model, demonstrating that context cache also effectively captures user behavior for NMT. However, the translation diversity among users increases since the model will not be affected by the context cache in this variant, which is the same between different users when calculating s-Sim. and d-Sim..\n3) w/o similar user initialization. In this variant, we do not initialize topic caches of the users without historical inputs using that of the most similar users. From Line 4, we observe that the performance of our model degrades without similar user initialization.\n4) w/o contrastive learning. In this variant, we remove the contrastive learning from the whole training objective to inspect the performance change of our model. As shown in Line 4, the performance of our model drops, proving that the contrastive learning is important for the training of our model.\nMoreover, we can infer from Column 6 and 7 that our model can generate diverse translations. Specifically, the translations of dissimilar users has larger diversity than that of similar ones. Furthermore, we conclude that our model is robust, since it still performs well when we replace the topic cache of current user with those of other users (See Column 4 and 5)."
    }, {
      "heading" : "5.6 Analysis of Contrastive Margin",
      "text" : "Inspired by Yang et al. (2019), we argue that the contrastive learning may increase the prediction diversity of our model between users compared with using the MLE loss. To confirm this, we randomly\nsample 300 examples from the training dataset, and compute the following margin:\n∆ = [ d(u +) (·)−d(u−) (·) ] − [ d (u+) mle (·)−d (u−) mle (·) ] ,\nwhere d(u +)(·) is defined in Equation 9. The definition of d(u +)\nmle (·) is the same with d(·), the only difference lies in that the NMT model is only trained by the conventional MLE loss. We find that d(·) has a larger margin than dmle(·) on 88% of sampled sentence pairs, with an average margin of 0.19. The results indicate again that the contrastive learning increases the translation diversity."
    }, {
      "heading" : "5.7 Qualitative Analysis",
      "text" : "In order to intuitively understand how our cache module exactly affects the translations, we feed our model with the same current source sentence but different users, and display the 1-best translations generated by our model. As shown in the Figure 4 (a), our model is able to produce correct but diverse translations according to different topic caches. Moreover, it is interesting to observe that specific topic keywords such as “type b arr”, “negatively regulated” and “modulators” are translated to synonymous but “out-of-domain” phrases if the topic cache does not conform to input sentence. On the contrary, the model conversely generates “indomain” translation if the topic cache comes from the same topic of input sentence.\nBesides, to further reveal the effect of user behavior, we provide an example in Figure 4 (b), which lists different translations by compared models for the same inputs. The historical inputs indicate that this user may be an apparel seller, since his historical inputs contain the product titles and descriptions of clothing. Thus, the keywords “Wear Resistant” in the source sentence are correlated with this user. However, two baselines translate it to “Waterproof” and “Resistant”, respectively. Moreover, TF-FT + UserBias generates a subject–verb–object structured sentence by adding the auxiliary verb “is”, which does not conform to the expression habit of the product title. By contrast, with the hint of the keywords in historical inputs, our UD-NMT is able to produce suitable translation consistent with the topic preference of this user."
    }, {
      "heading" : "5.8 Manual Evaluation",
      "text" : "To further find out weather the improvements of our model are contributed by user traits, we ran-\ndomly sample 100 examples from the test dataset and ask the linguist experts to sort different systems according to the relevance between the generated translations and the historical input. The results in Table 4 show that our model can generate translations more in line with history inputs than baseline models in most cases, proving that our method can make better use of user traits."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We propose user-driven NMT task, which aims to leverage user behavior to generate personalized translations. With the help of cache module and contrastive estimation, we successfully build an end-to-end NMT model that is able to capture potential user traits from their historical inputs and generate diverse translations under a zero-shot learning fashion. Furthermore, we contribute UDTCorpus, which is the first Chinese-English parallel corpus annotated with user behavior. We expect our study can attract more attention towards this topic. It is a promising direction to explore other behavior in future, such as clickthrough and editing operations. Moreover, following recent advancements in domain adaptation for NMT, we plan to further improve our model via adversial training based knowledge transfer (Zeng et al., 2018; Yao et al., 2020; Su et al., 2021) and dual knowledge transfer (Zeng et al., 2019)."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The project was supported by National Key Research and Development Program of China (No. 2020AAA0108004 and No. 2018YFB1403202), National Natural Science Foundation of China (No. 61672440), Natural Science Foundation of Fujian Province of China (No. 2020J06001), Youth Innovation Fund of Xiamen (No. 3502Z20206059), and the Fundamental Research Funds for the Central Universities (No. ZK20720200077). We also thank the reviewers for their insightful comments."
    } ],
    "references" : [ {
      "title" : "Cache-based online adaptation for machine translation enhanced computer assisted translation",
      "author" : [ "Nicola Bertoldi", "Mauro Cettolo", "Marcello Federico." ],
      "venue" : "Proceedings of the 2013 Machine Translation Summit, pages 35–42.",
      "citeRegEx" : "Bertoldi et al\\.,? 2013",
      "shortCiteRegEx" : "Bertoldi et al\\.",
      "year" : 2013
    }, {
      "title" : "Adversarial contrastive estimation",
      "author" : [ "Avishek Joey Bose", "Huan Ling", "Yanshuai Cao." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Asso-",
      "citeRegEx" : "Bose et al\\.,? 2018",
      "shortCiteRegEx" : "Bose et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-domain neural machine translation through unsupervised adaptation",
      "author" : [ "M. Amin Farajian", "Marco Turchi", "Matteo Negri", "Marcello Federico." ],
      "venue" : "Proceedings of the Second Conference on Machine Translation, pages 127–137.",
      "citeRegEx" : "Farajian et al\\.,? 2017",
      "shortCiteRegEx" : "Farajian et al\\.",
      "year" : 2017
    }, {
      "title" : "IRSTLM: an open source toolkit for handling large scale language models",
      "author" : [ "Marcello Federico", "Nicola Bertoldi", "Mauro Cettolo." ],
      "venue" : "Proceedings of the 9th Annual Conference of the International Speech Communication Association, pages 1618–1621.",
      "citeRegEx" : "Federico et al\\.,? 2008",
      "shortCiteRegEx" : "Federico et al\\.",
      "year" : 2008
    }, {
      "title" : "Cache-based document-level statistical machine translation",
      "author" : [ "Zhengxian Gong", "Min Zhang", "Guodong Zhou." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 909–919.",
      "citeRegEx" : "Gong et al\\.,? 2011",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2011
    }, {
      "title" : "A bit of progress in language modeling",
      "author" : [ "Joshua T. Goodman." ],
      "venue" : "Computer Speech and Language, 15(4):403–434.",
      "citeRegEx" : "Goodman.,? 2001",
      "shortCiteRegEx" : "Goodman.",
      "year" : 2001
    }, {
      "title" : "Don’t stop pretraining: Adapt language models to domains and tasks",
      "author" : [ "Suchin Gururangan", "Ana Marasovic", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A. Smith." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the",
      "citeRegEx" : "Gururangan et al\\.,? 2020",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2020
    }, {
      "title" : "OpenNMT: Opensource toolkit for neural machine translation",
      "author" : [ "Guillaume Klein", "Yoon Kim", "Yuntian Deng", "Jean Senellart", "Alexander Rush." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 67–72.",
      "citeRegEx" : "Klein et al\\.,? 2017",
      "shortCiteRegEx" : "Klein et al\\.",
      "year" : 2017
    }, {
      "title" : "Modeling coherence for neural machine translation with dynamic and topic caches",
      "author" : [ "Shaohui Kuang", "Deyi Xiong", "Weihua Luo", "Guodong Zhou." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages",
      "citeRegEx" : "Kuang et al\\.,? 2018",
      "shortCiteRegEx" : "Kuang et al\\.",
      "year" : 2018
    }, {
      "title" : "A cachebased natural language model for speech recognition",
      "author" : [ "Roland Kuhn", "Renato de Mori." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(6):570–583.",
      "citeRegEx" : "Kuhn and Mori.,? 1990",
      "shortCiteRegEx" : "Kuhn and Mori.",
      "year" : 1990
    }, {
      "title" : "Contrastive unsupervised word alignment with non-local features",
      "author" : [ "Yang Liu", "Maosong Sun." ],
      "venue" : "Proceedings of the 29th Association for the Advancement of Artificial Intelligence, pages 2295–2301.",
      "citeRegEx" : "Liu and Sun.,? 2015",
      "shortCiteRegEx" : "Liu and Sun.",
      "year" : 2015
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "Thang Luong", "Hieu Pham", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421.",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Extreme adaptation for personalized neural machine translation",
      "author" : [ "Paul Michel", "Graham Neubig." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 312– 318.",
      "citeRegEx" : "Michel and Neubig.,? 2018",
      "shortCiteRegEx" : "Michel and Neubig.",
      "year" : 2018
    }, {
      "title" : "Motivating personality-aware machine translation",
      "author" : [ "Shachar Mirkin", "Scott Nowson", "Caroline Brun", "Julien Perez." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1102–1108.",
      "citeRegEx" : "Mirkin et al\\.,? 2015",
      "shortCiteRegEx" : "Mirkin et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning word embeddings efficiently with noise-contrastive estimation",
      "author" : [ "Andriy Mnih", "Koray Kavukcuoglu." ],
      "venue" : "Proceedings of the 27th Annual Conference on Neural Information Processing Systems, pages 2265–2273.",
      "citeRegEx" : "Mnih and Kavukcuoglu.,? 2013",
      "shortCiteRegEx" : "Mnih and Kavukcuoglu.",
      "year" : 2013
    }, {
      "title" : "Adaptive language and translation models for interactive machine translation",
      "author" : [ "Laurent Nepveu", "Guy Lapalme", "Philippe Langlais", "George F. Foster." ],
      "venue" : "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Nepveu et al\\.,? 2004",
      "shortCiteRegEx" : "Nepveu et al\\.",
      "year" : 2004
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Personalized machine translation: Preserving original author traits",
      "author" : [ "Ella Rabinovich", "Raj Nath Patel", "Shachar Mirkin", "Lucia Specia", "Shuly Wintner." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computa-",
      "citeRegEx" : "Rabinovich et al\\.,? 2017",
      "shortCiteRegEx" : "Rabinovich et al\\.",
      "year" : 2017
    }, {
      "title" : "Controlling politeness in neural machine translation via side constraints",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics,",
      "citeRegEx" : "Sennrich et al\\.,? 2016a",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1715–1725.",
      "citeRegEx" : "Sennrich et al\\.,? 2016b",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "The Journal of Machine Learning Research, 15(1):1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Exploring discriminative word-level domain contexts for multidomain neural machine translation",
      "author" : [ "Jinsong Su", "Jiali Zeng", "Jun Xie", "Huating Wen", "Yongjing Yin", "Yang Liu." ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., 43:1530–1545.",
      "citeRegEx" : "Su et al\\.,? 2021",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2021
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le." ],
      "venue" : "Proceedings of the 28th Annual Conference on Neural Information Processing Systems, pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Context adaptation in statistical machine translation using models with exponentially decaying cache",
      "author" : [ "Jörg Tiedemann." ],
      "venue" : "Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, pages 8–15.",
      "citeRegEx" : "Tiedemann.,? 2010",
      "shortCiteRegEx" : "Tiedemann.",
      "year" : 2010
    }, {
      "title" : "Neural machine translation with extended context",
      "author" : [ "Jörg Tiedemann", "Yves Scherrer." ],
      "venue" : "Proceedings of the 3rd Workshop on Discourse in Machine Translation, pages 82–92.",
      "citeRegEx" : "Tiedemann and Scherrer.,? 2017",
      "shortCiteRegEx" : "Tiedemann and Scherrer.",
      "year" : 2017
    }, {
      "title" : "Learning to remember translation history with a continuous cache",
      "author" : [ "Zhaopeng Tu", "Yang Liu", "Shuming Shi", "Tong Zhang." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:407–420.",
      "citeRegEx" : "Tu et al\\.,? 2018",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of the 31th Annual Conference on Neural Information Processing Systems,",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Decoding with large-scale neural language models improves translation",
      "author" : [ "Ashish Vaswani", "Yinggong Zhao", "Victoria Fossum", "David Chiang." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1387–",
      "citeRegEx" : "Vaswani et al\\.,? 2013",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2013
    }, {
      "title" : "Self-paced learning for neural machine translation",
      "author" : [ "Yu Wan", "Baosong Yang", "Derek F. Wong", "Yikai Zhou", "Lidia S. Chao", "Haibo Zhang", "Boxing Chen." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Wan et al\\.,? 2020",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2020
    }, {
      "title" : "Sequence-to-sequence learning as beam-search optimization",
      "author" : [ "Sam Wiseman", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1296–1306.",
      "citeRegEx" : "Wiseman and Rush.,? 2016",
      "shortCiteRegEx" : "Wiseman and Rush.",
      "year" : 2016
    }, {
      "title" : "Reducing word omission errors in neural machine translation: A contrastive learning approach",
      "author" : [ "Zonghan Yang", "Yong Cheng", "Yang Liu", "Maosong Sun." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, pages",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Domain transfer based data augmentation for neural query translation",
      "author" : [ "Liang Yao", "Baosong Yang", "Haibo Zhang", "Boxing Chen", "Weihua Luo." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020,",
      "citeRegEx" : "Yao et al\\.,? 2020",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2020
    }, {
      "title" : "Iterative dual domain adaptation for neural machine translation",
      "author" : [ "Jiali Zeng", "Yang Liu", "Jinsong Su", "Yubin Ge", "Yaojie Lu", "Yongjing Yin", "Jiebo Luo." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Zeng et al\\.,? 2019",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2019
    }, {
      "title" : "Multidomain neural machine translation with word-level domain context discrimination",
      "author" : [ "Jiali Zeng", "Jinsong Su", "Huating Wen", "Yang Liu", "Jun Xie", "Yongjing Yin", "Jianqiang Zhao." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Zeng et al\\.,? 2018",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2018
    }, {
      "title" : "Curriculum learning for domain adaptation in neural machine translation",
      "author" : [ "Xuan Zhang", "Pamela Shapiro", "Gaurav Kumar", "Paul McNamee", "Marine Carpuat", "Kevin Duh." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Asso-",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Uncertainty-aware curriculum learning for neural machine translation",
      "author" : [ "Yikai Zhou", "Baosong Yang", "Derek F. Wong", "Yu Wan", "Lidia S. Chao." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6934–",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "In recent years, neural machine translation (NMT) models (Sutskever et al., 2014; Luong et al., 2015; Vaswani et al., 2017) have shown promising quality and thus increasingly attracted users.",
      "startOffset" : 57,
      "endOffset" : 123
    }, {
      "referenceID" : 11,
      "context" : "In recent years, neural machine translation (NMT) models (Sutskever et al., 2014; Luong et al., 2015; Vaswani et al., 2017) have shown promising quality and thus increasingly attracted users.",
      "startOffset" : 57,
      "endOffset" : 123
    }, {
      "referenceID" : 26,
      "context" : "In recent years, neural machine translation (NMT) models (Sutskever et al., 2014; Luong et al., 2015; Vaswani et al., 2017) have shown promising quality and thus increasingly attracted users.",
      "startOffset" : 57,
      "endOffset" : 123
    }, {
      "referenceID" : 13,
      "context" : "More recently, several studies have shown that the prominent signals in terms of personal characteristics can be served as inductive biases and reflected in translation results using domain adaptation approaches, such as personality (Mirkin et al., 2015), gender (Rabinovich et al.",
      "startOffset" : 233,
      "endOffset" : 254
    }, {
      "referenceID" : 17,
      "context" : ", 2015), gender (Rabinovich et al., 2017), and politeness (Sennrich et al.",
      "startOffset" : 16,
      "endOffset" : 41
    }, {
      "referenceID" : 13,
      "context" : "Previous methods (Mirkin et al., 2015; Michel and Neubig, 2018) require the users in the training set and the test set to be consistent, therefore can not",
      "startOffset" : 17,
      "endOffset" : 63
    }, {
      "referenceID" : 12,
      "context" : "Previous methods (Mirkin et al., 2015; Michel and Neubig, 2018) require the users in the training set and the test set to be consistent, therefore can not",
      "startOffset" : 17,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : "Besides, in order to further transfer the traits from the seen users to the unseen ones, we design a regularization framework based on contrastive learning (Bose et al., 2018; Yang et al., 2019), which forces our model to decrease the divergence between translations of similar users while increasing the diversity on dissimilar users.",
      "startOffset" : 156,
      "endOffset" : 194
    }, {
      "referenceID" : 30,
      "context" : "Besides, in order to further transfer the traits from the seen users to the unseen ones, we design a regularization framework based on contrastive learning (Bose et al., 2018; Yang et al., 2019), which forces our model to decrease the divergence between translations of similar users while increasing the diversity on dissimilar users.",
      "startOffset" : 156,
      "endOffset" : 194
    }, {
      "referenceID" : 34,
      "context" : "Personalized Machine Translation Recently, some researchers have employed domain adaptation (Zhang et al., 2019; Gururangan et al., 2020; Yao et al., 2020) to generate personalized translations.",
      "startOffset" : 92,
      "endOffset" : 155
    }, {
      "referenceID" : 6,
      "context" : "Personalized Machine Translation Recently, some researchers have employed domain adaptation (Zhang et al., 2019; Gururangan et al., 2020; Yao et al., 2020) to generate personalized translations.",
      "startOffset" : 92,
      "endOffset" : 155
    }, {
      "referenceID" : 31,
      "context" : "Personalized Machine Translation Recently, some researchers have employed domain adaptation (Zhang et al., 2019; Gururangan et al., 2020; Yao et al., 2020) to generate personalized translations.",
      "startOffset" : 92,
      "endOffset" : 155
    }, {
      "referenceID" : 5,
      "context" : "Cache-Based Machine Translation Inspired by the great success of cache on language modeling (Kuhn and de Mori, 1990; Goodman, 2001; Federico et al., 2008), Nepveu et al.",
      "startOffset" : 92,
      "endOffset" : 154
    }, {
      "referenceID" : 3,
      "context" : "Cache-Based Machine Translation Inspired by the great success of cache on language modeling (Kuhn and de Mori, 1990; Goodman, 2001; Federico et al., 2008), Nepveu et al.",
      "startOffset" : 92,
      "endOffset" : 154
    }, {
      "referenceID" : 27,
      "context" : "Contrastive Learning for NMT Contrastive learning has been extensively applied in the communities of computer vision and natural language processing due to its effectiveness and generality on self-supervised learning (Vaswani et al., 2013; Mnih and Kavukcuoglu, 2013; Liu and Sun, 2015; Bose et al., 2018).",
      "startOffset" : 217,
      "endOffset" : 305
    }, {
      "referenceID" : 14,
      "context" : "Contrastive Learning for NMT Contrastive learning has been extensively applied in the communities of computer vision and natural language processing due to its effectiveness and generality on self-supervised learning (Vaswani et al., 2013; Mnih and Kavukcuoglu, 2013; Liu and Sun, 2015; Bose et al., 2018).",
      "startOffset" : 217,
      "endOffset" : 305
    }, {
      "referenceID" : 10,
      "context" : "Contrastive Learning for NMT Contrastive learning has been extensively applied in the communities of computer vision and natural language processing due to its effectiveness and generality on self-supervised learning (Vaswani et al., 2013; Mnih and Kavukcuoglu, 2013; Liu and Sun, 2015; Bose et al., 2018).",
      "startOffset" : 217,
      "endOffset" : 305
    }, {
      "referenceID" : 1,
      "context" : "Contrastive Learning for NMT Contrastive learning has been extensively applied in the communities of computer vision and natural language processing due to its effectiveness and generality on self-supervised learning (Vaswani et al., 2013; Mnih and Kavukcuoglu, 2013; Liu and Sun, 2015; Bose et al., 2018).",
      "startOffset" : 217,
      "endOffset" : 305
    }, {
      "referenceID" : 26,
      "context" : "We choose Transformer (Vaswani et al., 2017) as the basic NMT model due to its competitive performance.",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 13,
      "context" : "Unlike previous studies (Mirkin et al., 2015; Michel and Neubig, 2018) only caring for generating translations for users seen at the training time, our userdriven NMT mainly focuses on a more realistic online MT scenario, where the users for testing are unseen in the training dataset.",
      "startOffset" : 24,
      "endOffset" : 70
    }, {
      "referenceID" : 12,
      "context" : "Unlike previous studies (Mirkin et al., 2015; Michel and Neubig, 2018) only caring for generating translations for users seen at the training time, our userdriven NMT mainly focuses on a more realistic online MT scenario, where the users for testing are unseen in the training dataset.",
      "startOffset" : 24,
      "endOffset" : 70
    }, {
      "referenceID" : 4,
      "context" : "Due to the advantages of cache mechanism on dynamic representations (Gong et al., 2011; Kuang et al., 2018; Tu et al., 2018), we equip the conventional Transformer-based NMT model with two user-specific caches to leverage user behavior for NMT: 1) topic cache c t that aims at capturing the global and long-term traits of user u; and 2) context cache c c , which is introduced to capture the short-term traits from the recent source inputs of user u.",
      "startOffset" : 68,
      "endOffset" : 124
    }, {
      "referenceID" : 8,
      "context" : "Due to the advantages of cache mechanism on dynamic representations (Gong et al., 2011; Kuang et al., 2018; Tu et al., 2018), we equip the conventional Transformer-based NMT model with two user-specific caches to leverage user behavior for NMT: 1) topic cache c t that aims at capturing the global and long-term traits of user u; and 2) context cache c c , which is introduced to capture the short-term traits from the recent source inputs of user u.",
      "startOffset" : 68,
      "endOffset" : 124
    }, {
      "referenceID" : 25,
      "context" : "Due to the advantages of cache mechanism on dynamic representations (Gong et al., 2011; Kuang et al., 2018; Tu et al., 2018), we equip the conventional Transformer-based NMT model with two user-specific caches to leverage user behavior for NMT: 1) topic cache c t that aims at capturing the global and long-term traits of user u; and 2) context cache c c , which is introduced to capture the short-term traits from the recent source inputs of user u.",
      "startOffset" : 68,
      "endOffset" : 124
    }, {
      "referenceID" : 7,
      "context" : "We develop the user-driven NMT model based on Open-NMT Transformer (Klein et al., 2017), and adopt a two-stage strategy to train this model: we first pre-train a Transformer-based NMT model on the WMT2017 Chinese-to-English dataset, and then fine-tune this model to our user-driven NMT model using UDT-Corpus.",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 19,
      "context" : "Following common practices, we employ byte pair encoding (Sennrich et al., 2016b) with 32K merge operations to deal with all sentences.",
      "startOffset" : 57,
      "endOffset" : 81
    }, {
      "referenceID" : 20,
      "context" : "Particularly, we adopt the dropout strategy (Srivastava et al., 2014) with rate 0.",
      "startOffset" : 44,
      "endOffset" : 69
    }, {
      "referenceID" : 28,
      "context" : "61 BLEU score on WMT2017 Chinese-to-English translation task, which is comparable with reported results in (Wan et al., 2020; Zhou et al., 2020), which makes our subsequent experiments convincing.",
      "startOffset" : 107,
      "endOffset" : 144
    }, {
      "referenceID" : 35,
      "context" : "61 BLEU score on WMT2017 Chinese-to-English translation task, which is comparable with reported results in (Wan et al., 2020; Zhou et al., 2020), which makes our subsequent experiments convincing.",
      "startOffset" : 107,
      "endOffset" : 144
    }, {
      "referenceID" : 2,
      "context" : "We change it to a zeroshot method similar to (Farajian et al., 2017)",
      "startOffset" : 45,
      "endOffset" : 68
    }, {
      "referenceID" : 12,
      "context" : "since (Michel and Neubig, 2018) can not be directly applied to our scenario.",
      "startOffset" : 6,
      "endOffset" : 31
    }, {
      "referenceID" : 33,
      "context" : "Moreover, following recent advancements in domain adaptation for NMT, we plan to further improve our model via adversial training based knowledge transfer (Zeng et al., 2018; Yao et al., 2020; Su et al., 2021) and dual knowledge transfer (Zeng et al.",
      "startOffset" : 155,
      "endOffset" : 209
    }, {
      "referenceID" : 31,
      "context" : "Moreover, following recent advancements in domain adaptation for NMT, we plan to further improve our model via adversial training based knowledge transfer (Zeng et al., 2018; Yao et al., 2020; Su et al., 2021) and dual knowledge transfer (Zeng et al.",
      "startOffset" : 155,
      "endOffset" : 209
    }, {
      "referenceID" : 21,
      "context" : "Moreover, following recent advancements in domain adaptation for NMT, we plan to further improve our model via adversial training based knowledge transfer (Zeng et al., 2018; Yao et al., 2020; Su et al., 2021) and dual knowledge transfer (Zeng et al.",
      "startOffset" : 155,
      "endOffset" : 209
    }, {
      "referenceID" : 32,
      "context" : ", 2021) and dual knowledge transfer (Zeng et al., 2019).",
      "startOffset" : 36,
      "endOffset" : 55
    } ],
    "year" : 2021,
    "abstractText" : "A good translation should not only translate the original content semantically, but also incarnate personal traits of the original text. For a real-world neural machine translation (NMT) system, these user traits (e.g., topic preference, stylistic characteristics and expression habits) can be preserved in user behavior (e.g., historical inputs). However, current NMT systems marginally consider the user behavior due to: 1) the difficulty of modeling user portraits in zero-shot scenarios, and 2) the lack of userbehavior annotated parallel dataset. To fill this gap, we introduce a novel framework called user-driven NMT. Specifically, a cache-based module and a user-driven contrastive learning method are proposed to offer NMT the ability to capture potential user traits from their historical inputs under a zero-shot learning fashion. Furthermore, we contribute the first ChineseEnglish parallel corpus annotated with user behavior called UDT-Corpus. Experimental results confirm that the proposed user-driven NMT can generate user-specific translations. 1",
    "creator" : "LaTeX with hyperref"
  }
}