{
  "name" : "2021.acl-long.405.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Lower Perplexity is Not Always Human-Like",
    "authors" : [ "Tatsuki Kuribayashi", "Yohei Oseki", "Takumi Ito", "Ryo Yoshida", "Masayuki Asahara", "Kentaro Inui" ],
    "emails" : [ "inui}@tohoku.ac.jp", "yoshiryo0617}@g.ecc.u-tokyo.ac.jp", "masayu-a@ninjal.ac.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5203–5217\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5203"
    }, {
      "heading" : "1 Introduction",
      "text" : "It is well known that the probability of a word in context (i.e., surprisal) impacts its processing difficulty in incremental human language comprehension (Hale, 2001; Demberg and Keller, 2008; Levy, 2008; Smith and Levy, 2013). Building on this basis, researchers have compared a variety of language models (LMs) in terms of how well their surprisal correlates with human reading behavior (Roark et al., 2009; Frank and Bod, 2011; Fossum and Levy, 2012; Hale et al., 2018; Goodkind and Bicknell, 2018; Aurnhammer and Frank, 2019; Merkx and Frank, 2020; Wilcox et al., 2020). Such investigations could provide insights into the development of a general computational model of\nhuman language processing. For example, recent studies reported that LMs with better performance for next-word prediction could also better predict the human reading behavior (i.e. more humanlike) (Fossum and Levy, 2012; Goodkind and Bicknell, 2018; Wilcox et al., 2020).\nIn this paper, we re-examine whether the recent findings on human-like computational models can be generalized across languages. Despite the community’s ongoing search for a languageindependent model (Bender, 2011), existing studies have focused almost exclusively on the English language. Having said that, broad-coverage crosslinguistic evaluation of the existing reports is prohibitively difficult. In fact, data on human reading behavior (e.g., eye movement) is available only in limited languages. As an initial foray, this study focuses on the Japanese language as a representative of languages that have typologically different characteristics from the English language. If the observation is different between English and Japanese, the current findings on English data might lack a universality across languages.\nWe specifically revisit the recent report—the lower perplexity a LM has, the more human-like the LM is—in the English and Japanese languages (Fossum and Levy, 2012; Goodkind and Bicknell, 2018; Wilcox et al., 2020). In addition to the importance of cross-linguistic evaluation, the report itself is worth investigating. Recent studies in the machine learning field have reported that more parameters, training data, and computation cost can result in better PPL (Kaplan et al., 2020; Brown et al., 2020). Our investigation has implications for whether a human-like model might exist beyond such improvements.\nMore concretely, over three dozens of LMs were trained for each language, with variants in their architecture, training data size, and the number of parameter updates. Then, the surprisals computed by\neach LM were compared to human eye movement data (Figure 1). The analysis of the relationship between PPL and the psychometric predictive power revealed substantively different trends between the Japanese and English LMs. In Japanese, a lower PPL of a LM does not indicate better performance for modeling reading behavior. By contrast, in English, there was a clear relationship between the two metrics as reported in the prior studies.\nThis opens a remaining and important question: why are English and Japanese different in this aspect? We discuss the differing results between English and Japanese from the perspective of the uniform information density hypothesis (Genzel and Charniak, 2002; Levy, 2005; Jaeger and Levy, 2007). We find that the processing difficulty (i.e., gaze duration) of segments is less uniformly distributed within a Japanese sentence. Given this, the discrepancy of the results between English and Japanese might stem from a mismatch between the information uniformity of the target language and the LM’s training objective. We demonstrate that tuning Japanese LMs to this training objective collapses the human-like nonuniformity of the processing difficulty observed in Japanese subjects. Our code is made publicly available.1"
    }, {
      "heading" : "2 Related work",
      "text" : ""
    }, {
      "heading" : "2.1 Human sentence processing and LMs",
      "text" : "What factor determines the incremental difficulty of human language processing? At present, surprisal theory (Hale, 2001; Levy, 2008) has been widely adopted in the field of computational psycholinguistics. This theory suggests that the processing difficulty of a segment is determined by how predictable the segment is in its preceding context (− log p(segment|preceding context)).\n1https://github.com/kuribayashi4/ surprisal_reading_time_en_ja\nExisting studies have compared various computational models by checking the effectiveness of their surprisals in modeling human reading behavior (Hale, 2001; Roark et al., 2009; Frank and Bod, 2011; Fossum and Levy, 2012; Hale et al., 2018; Goodkind and Bicknell, 2018; Merkx and Frank, 2020; Wilcox et al., 2020). Data such as eye movement (Kennedy et al., 2003) and brain activity (Frank et al., 2015; Brennan et al., 2016) are used as measures of human reading behavior. For example, using eye movement data, Frank and Bod (2011) compared the surprisals from phrasestructure grammars (PSGs) with those from a nonhierarchical, sequential model, tentatively concluding that human sentence processing was insensitive to hierarchical structures since non-hierarchical models displayed better psychological predictive power than PSGs. Recently, researchers reported that surprisals from LMs with low PPL correlate well with human reading behaviors (Fossum and Levy, 2012; Goodkind and Bicknell, 2018; Aurnhammer and Frank, 2019; Wilcox et al., 2020).\nThe work most closely related to this study is Wilcox et al. (2020). They examined the relationship between PPL, psychometric predictive power, and syntactic knowledge in LMs using a variety of models, including modern neural LMs (Radrof et al., 2018). They found a tight relationship between PPL and psychometric predictive power in the English corpora. This study investigates whether this relationship can be generalized across languages."
    }, {
      "heading" : "2.2 Reading behavior in Japanese",
      "text" : "In comparison to English speakers, Japanese speakers display different patterns in sentence processing. For example, an anti-locality effect (the more modifiers a word has in its preceding context, the easier the word is to process) has typically been observed in head-final languages, including\nJapanese (Konieczny, 2000). Such differences between the languages are assumed to be more or less due to their different sentence structures. Recently, eye movement data for naturally occurring Japanese texts have recently become available (Asahara et al., 2016) and was extensively annotated with various linguistic properties (Asahara and Kato, 2017; Asahara, 2017, 2018)."
    }, {
      "heading" : "3 Methods",
      "text" : "This section describes the settings of LMs, eye movement data, and evaluation metrics."
    }, {
      "heading" : "3.1 Language models",
      "text" : "A variety of sentence-level, left-to-right sequential LMs was used.\nTraining data of English LMs: We used the WikiText-103 dataset to train the English LMs. Based on the reports that subword-level English LMs exhibits superior psychometric predictive power (Wilcox et al., 2020), input texts were divided into subwords by a byte-pair encoding (BPE) (Sennrich et al., 2016).2 The training data consist of approximately 4M sentences (114M subwords units).\nTraining data of Japanese LMs: We used news articles and the Japanese part of Wikipedia to train the Japanese LMs. Input texts were first segmented into morphemes by MeCab (Kudo, 2006), and then further divided into subwords by BPE.2 The training data consist of approximately 5M sentences (146M subwords units).\nArchitectures: The following four variants of LMs were used: Transformer-large (TRANSLG) (Vaswani et al., 2017), Transformer-small (TRANS-SM), LSTM (LSTM) (Hochreiter and Schmidhuber, 1997), and N-gram LMs (NGRAM).3 The parameter size was almost the same for TRANS-SM and LSTM. With respect to the NGRAM models, 3-gram, 4-gram, and 5-gram LMs were used. Appendix A shows the hyperparameters of the neural LMs.\n2Implemented in SentencePiece (Kudo and Richardson, 2018). We set character coverage to 0.9995，and vocabulary size to 32,000 in English. In Japanese, the vocabulary size is 100,000, reflecting its rich morphemes.\n3The neural LMs were trained with the fairseq toolkit (Ott et al., 2019). N-GRAM LMs were trained using KenLM https://github.com/kpu/kenlm.\nTraining data size: For each neural LM architecture (TRANS-LG, TRANS-SM, and LSTM), three variants were trained using different training data sizes: LG (full training data), MD (1/10 training data), and SM (1/100 training data). The N-gram LMs were trained on LG datasets.\nNumber of updates: The parameters of each neural LM were saved at four different points during training: 100, 1K, 10K, and 100K parameter updates.\nTo summarize, 39 LM training settings were attained for each language (3 architectures× 3 data size × 4 parameter updates = 36 neural LMs, plus 3 N-GRAM LMs). In addition, our experiments use three LMs trained using different random seeds for each neural LM training configure; hence, 111 LMs (36 neural LMs × 3 seeds, plus 3 N-GRAM LMs) were tested for each language."
    }, {
      "heading" : "3.2 Eye movement data",
      "text" : "English: The Dundee Corpus (Kennedy et al., 2003), which contains gaze duration annotation for each word, was used. Following Smith and Levy (2013), the first-pass gaze duration was analyzed. Then, following Goodkind and Bicknell (2018), the data points that met any of the following criteria were excluded:\n• data points with zero gaze duration or that beyond three standard deviations\n• segments with punctuation or numeric characters\n• segments whose next segment has punctuation or numeric characters\n• first or last segment in a line\nIn total, the analysis included 107,580 data points in the corpus.\nJapanese: The BCCWJ-EyeTrack (Asahara et al., 2016), which contains gaze duration annotation for each phrasal unit, was used. Note that the phrasal unit (i.e., bunsetsu) consists of at least one content morpheme and its postpositional function morphemes. Henceforth, an English word and a Japanese phrasal unit are referred to as a “segment.” The same exclusion criteria as the Dundee Corpus was applied to the BCCWJ-EyeTrack data.4 In\n4Strictly speaking, the exclusion criteria was slightly different between Japanese and English data. In the Japanese data, we included the segments whose next segment had punctuation or a numeric character, as there is no spillover effect in Japanese (see Section 3.3)\ntotal, the analysis included 6,009 data points in the corpus. Note that the BCCWJ-EyeTrack data was deliberately designed to address language-specific issues in Japanese such as the lack of segmentation spaces in Japanese texts (Asahara et al., 2016).\nStatistics: Table 1 shows the statistics of the Dundee Corpus and BCCWJ-EyeTrack data. The BCCWJ-EyeTrack has more than 10 times a smaller number of data points than the Dundee Corpus. Notably, the segment annotated with eye movement information differs between English and Japanese. On average, a Japanese segment consists of 3.4 subwords, while an English segment consists of 1.3 subwords. Smith and Levy (2013) theoretically proved that the more fragments a word is divided into when computing its surprisal, the better the calculated surprisal approximates the human cognitive effort if the human language processing is highly incremental. Thus, we tentatively consider that this difference did not make a negative impact on the results using the Japanese data."
    }, {
      "heading" : "3.3 Evaluation metrics",
      "text" : "Perplexity (PPL): PPL, the inverse geometric mean of next-word probabilities p(wi|w<i) in a text that consists ofN signals (w1, w2, · · · , wN ), is a typical evaluation metric for unidirectional LMs (Eq. 1):\nPPL = N∏ i=0 p(wi|w<i)− 1 N . (1)\nLow PPL indicates that the model can accurately predict the upcoming signal based on its preceding context. The training objective of LMs works to minimize the PPL computed by the model. In the experiments, the PPL of a LM is evaluated with the texts in the eye movement data, which do not overlap with the training data. A model with low PPL is\nalso called a linguistically accurate model (Frank and Bod, 2011).\nPsychometric predictive power: The surprisal measure, a negative logarithmic probability of a segment in context (− log p(segment|preceding context)), is a widely used information-theoretic complexity metric. Intuitively, a model is considered to have high psychometric predictive power (i.e., psychological accuracy) if the surprisals of segments computed by the model have trends similar to the human subject’s cognitive load (e.g., measured by gaze duration). Following the existing studies (Goodkind and Bicknell, 2018; Merkx and Frank, 2020; Wilcox et al., 2020), the psychometric predictive power of a model was measured by comparing surprisal from the model and gaze duration from human subjects.\nWhile LMs process a text subword-by-subword, gaze duration is annotated in a larger segment. Following the study using subwords (Wilcox et al., 2020), the surprisal of each segment was calculated using the joint probability of its constituent subwords. Formally, given a text consisting of N subwords w1:N = (w1, w2, · · · , wN ), surprisal I(·) of a segment sk = (wl, wl+1, · · · , wm), where 1 ≤ l ≤ m ≤ N , was calculated as follows:\nI(sk) = − log p(wl, · · · , wm|w<l)\n= − m∑ k=l log p(wk|w1, · · · , wk−1) . (2)\nThe effect of surprisals for modeling human reading behavior was calculated using a linear mixedeffects regression (Bates et al., 2015). Specifically, the gaze duration (GD) was modeled using the following formula:\nGD ∼ surprisal+ surprisal prev 1 + surprisal prev 2+ freq ∗ length + freq prev 1 ∗ length prev 1 + screenN+ lineN+ segmentN\n+ (1|article)+ (1|subj) .\n(3)\nThe regression model includes baseline factors (e.g., frequency of a segment) that are of no interest in the comparison of LMs. A collection of factors used in the existing studies (Asahara et al., 2016; Wilcox et al., 2020) were initially examined and the factors that were not significant (p > 0.05) for gaze duration modeling both in the Dundee Corpus and BCCWJ-EyeTrack were excluded. The frequency of a segment (freq) was calculated using the entire training data for LMs. Appendix B shows the details of each factor in Eq. 3.\nIn English experiments, surprisals of preceding words (surprisal prev 1 and surprisal prev 2) were included in order to handle the spillover effect (the processing cost of a certain segment is affected by its preceding segments) (Rayner and Well, 1996; Smith and Levy, 2013). In Japanese experiments, the surprisals of preceding words were not included because our preliminary experiment showed that these factors were not significantly effective for modeling gaze duration in the BCCWJ-EyeTrack.5\n5The reason is probably that a Japanese phrasal unit (i.e., bunsetsu) could be a larger unit than an English word.\nAll the regression models used in our experiments were converged.\nTo isolate the effect of surprisal for gaze duration modeling, a baseline regression model was trained without surprisal information (excluding the surprisal, surprisal prev 1, and surprisal prev 2 terms from Eq. 3). Following Goodkind and Bicknell (2018), the difference of log-likelihood between the model using surprisal values (Eq. 3) and the baseline model was calculated. Henceforth, this metric is called ∆LogLik. When surprisal from a LM is not effective for gaze duration modeling, the ∆LogLik score becomes zero. A high ∆LogLik means that the surprisal values obtained by the LM are effective for modeling gaze duration (i.e., the LM has a high psychometric predictive power)."
    }, {
      "heading" : "4 Experiments",
      "text" : "The relationship between PPL and psychometric predictive power is investigated. Furthermore, the relationship is analyzed with respect to the training configures of LMs (e.g., the number of parameter updates). Then, we discuss the results from the perspective of the uniformity of information density."
    }, {
      "heading" : "4.1 Psychometric predictive power and PPL",
      "text" : "Figure 2 shows the relationship between PPL and psychometric predictive power (i.e., ∆LogLik) of LMs in each of the languages. Each point corresponds to each LM, and a score on the Y-axis indicates the psychometric predictive power of a\nLM (higher is better). The X-axis is PPL on a log scale (lower is better).\nDundee Corpus: First, the results of the data from the Dundee Corpus show a clear relationship between PPL and psychometric predictive power; namely, lower PPL corresponds to more psychometric predictive power, as reported by prior studies (Goodkind and Bicknell, 2018; Wilcox et al., 2020). Spearman’s rank correlation coefficient between the two metrics was −0.69.\nBCCWJ-EyeTrack: By contrast, in BCCWJEyeTrack, there was no clear, consistent trend between the PPL and psychometric predictive power. While LMs with PPL over 400 show the correlation between PPL and psychometric predictive power (−0.68 with Spearman’s ρ), there is a positive correlation (0.53 with Spearman’s ρ) for LMs with PPL below 400. The positive correlation means that the more accurately the LMs can predict the upcoming word, the worse the psychometric predictive power of the LMs is. These results demonstrate the non-universality of the recent report across languages; lower perplexity is not always human-like. The LSTM LM trained using the MD dataset with 1K updates achieved the best psychometric predictive power. Notably, surprisal was effective for gaze duration modeling in all the Japanese LMs. ∆logLik scores were significantly higher than zero\nwith the chi-square test (p <0.05)."
    }, {
      "heading" : "4.2 Model architectures, data sizes, number of parameter updates",
      "text" : "Which factor (e.g., model architecture, training data size, and the number of parameter updates) characterizes the psychometric predictive power of LMs? Is the collection of effective factors consistent between the two languages? This study takes a more in-depth look at the separate effects of (i) model architecture, (ii) training data size, and (iii) the number of parameter updates for the psychometric predictive power.\nFigure 3 summarizes the effect of each factor, where the Y-axis denotes the psychometric predictive power. The most noticeable trend is that Japanese LMs with a relatively fewer number of parameter updates (1K) have better psychometric predictive power than the other Japanese LMs (bottom right part of Figure 3), while this trend does not exist in the English LMs (top right part). This implies that the training objective of the LMs, maximizing 1 N ∑N i=1 logP (wi|w<i), had a negative impact on the psychometric predictive power of LMs, at least in Japanese. We discuss this point in Section 4.3.\nTo quantitatively test the differences in Figure 3, a linear regression model was trained to estimate psychometric predictive power with the factors of the model architecture, the training data size, and\nthe parameter update number in each language. The training data size and the parameter update number are represented as logarithmically transformed numerical factors. The following trends were found: (i) no significant difference by model architecture; (ii) the training data size positively affects the performance in English alone; and (iii) the number of parameter updates positively affects the performance only in English. There was no factor that boosted the psychometric predictive power of LMs in both English and Japanese languages."
    }, {
      "heading" : "4.3 Discussion: uniform information density",
      "text" : "The key question is: why do Japanese and English show different trends between PPL and psychometric predictive power? One possible interpretation connecting our results to the uniform information density is discussed in this section.\nIn computational psycholinguistics, it is commonly assumed that language is designed to enable efficient communication. This principle has been typically investigated under the uniform information density (UID) hypothesis (Genzel and Charniak, 2002; Levy, 2005; Jaeger and Levy, 2007). This hypothesis suggests that speakers seek to keep the amount of information constant across the signals (e.g., segments).\nAssuming this hypothesis holds for all languages, the reasonable expectation would be for human subjects to show a near-uniform gaze duration across segments regardless of their native language. However, this study found that the coefficient of variation6 in gaze duration over the whole corpus was 2.5 times higher in Japanese compared to English (0.84 vs. 0.34). Specifically, in Japanese, the gaze duration tended to speed up towards the end of sentences, whereas the duration was near-uniform in English (Figure 4).7 These observations imply that the Japanese language might have a less uniform information density than English. This phenomenon was also investigated through the lens of word order, where SOV languages such as Japanese are reported to show less uniformity of information density (Maurits et al., 2010).\n6Coefficient of variation is σ µ , where σ and µ are the standard deviation and the mean of the first-pass gaze durations in the eye movement data.\n7At least in our experimental setup, token position within the sentence was not significantly effective for gaze duration modeling in English sentences, whereas it was significant in Japanese sentences. We checked the coefficient of the factor of position in sentence segmentN using the linear regression model of GD ∼ sengmentN.\nBased on this observation, the discrepancy between English and Japanese low-PPL LMs’ psycholinguistic predictive power could stem from a mismatch between the LM’s training objective and the information uniformity of the target language. The objective function, 1N ∑N i=1 logP (wi|w<i), defines that the “ideal” is to maximize all next word probabilities to 1.0 (a uniform goal).8 That is, LMs are, in theory, trained to approach a model satisfying the UID assumption (Bloem, 2016), where all surprisals from the LM are equally, sufficiently small across the segments. Therefore, the objective function might lead to a worse approximation of human-like surprisal in languages that are further from the UID assumption, such as Japanese, while it might be more compatible with English, which has a more uniform processing difficulty across segments. This explanation would be consistent with the observation that more tuning to the LM training objective (i.e., a lower PPL) had a negative impact on the psycholinguistic performance of the Japanese LMs (Section 4.2). Note the tendency of LMs to assign unreasonably high probabilities to segments has also attracted attention from the viewpoint of memorization capability of LMs (Carlini et al., 2020). In addition, the connection of the UID hypothesis to the modern NLP techniques has been recently explored (Meister et al., 2020; Wei et al., 2021). We further investigate our hypothesis in Section 5."
    }, {
      "heading" : "5 Probing nonuniform information density of Japanese LMs",
      "text" : "This study hypothesized that tuning to the LM objective (i.e., uniform goal) obscures the nonuniform trend observed in the reading behavior of Japanese subjects. We investigated whether the nonunifor-\n8PPL, ∏N i=1 P (wi|w<i) − 1 N , is minimized when the LM\nobjective are maximized.\nmity of the processing difficulty observed in human reading time is mirrored by LM surprisals.\nSettings: In a preliminary experiment, we observed that the syntactic category (similar to partof-speech) was the most dominant linguistic factor for explaining the difference in human gaze duration in Japanese sentences (see Appendix D). Based on this observation, we analyze the nonuniformity of surprisals in Japanese LMs with respect to the syntactic categories.\nThe segments in BCCWJ-EyeTrack were classified into one of the following syntactic categories: (a) nominal (nouns), (b) verbal (verbs), (c) modifier (adjectives and adverbs), and (d) other entries, as follows:\nKanojo-ga akai kaban-o kat-ta She-NOM red bag-ACC buy-PAST nominal modifier nominal verbal\nAs Asahara and Kato (2017) reported, verbal and modifier segments have a shorter gaze duration than the other segments in Japanese sentences. An analysis was conducted on how strongly the Japanese LM’s surprisals on segments are influenced by their syntactic category. This influence can be evaluated by examining how effectively syntactic category factors can model LM surprisals.\nIn this experiment, surprisal was regarded as “simulated gaze duration” from an “LM subject,” and the importance of syntactic category information for modeling the simulated gaze duration (simulated GD) was evaluated. To inspect the\neffect of the syntactic category information for modeling the simulated gaze duration, the following regression model9 was used, including a factor defining which syntactic category the segment falls into (syn category):\nsimulated GD ∼ syn category+ sentN + tokenN+ freq ∗ length . (4)\nFrom this regression model, a log-likelihood score for the simulated gaze duration was obtained. To evaluate the separate effect of syn category, ∆LogLik between Eq. 4 and a baseline model was calculated. The baseline model was simulated GD ∼ sentN + tokenN + freq ∗ length. The ∆LogLik is denoted as “Effect of syntactic category.” A lower score means that the LM lacked the property of varying processing difficulty with respect to the syntactic category.\nResults: The results are shown in Figure 5. First, the higher psychometric predictive power the LMs exhibit, the greater the effect of syntactic category on surprisals (left part in Figure 5). This means that, depending on the syntactic category of the segment they processed, LMs with high psychometric predictive power computed surprisals with a more nonuniform trend. The right part of Figure 5 shows that, as PPL decreases below a certain value (PPL ∼ 400), the Japanese LMs compute\n9sentN and tokenN denote the sentence position and the segment position in a sentence (see Appendix B). Note that the tokenN and syntactic category exhibit low correlation (0.02 with Pearson’s r).\nsurprisals that obscure the nonuniform trends with respect to the syntactic category of segments.10 This trend supports our hypothesis that tuning to LM objectives obscures the human-like nonuniformity of the processing difficulty. Even though LMs that are not fully tuned to the LM objective (PPL ∼ 400) acquire human-like trends with respect to syntactic category, these biases tend to be lost by further lowering their PPL.\nNotably, we also observed that not all the types of linguistic nonuniformity were obscured in surprisals computed by the LMs with low PPL. For example, Appendix E shows that LMs with lower PPL compute surprisals that better correlates with a particular syntactic factor although that factor is a less dominant trend in human reading behavior than the syntactic category (Appendix D)."
    }, {
      "heading" : "6 Limitations and future works",
      "text" : "To test the universality of the recent findings in computational psycholinguistics across languages, the initial focus is on English and Japanese as a pair of languages with different linguistic properties. Although the discrepancy of the results in the two languages is discussed from the viewpoint of the UID hypothesis, the two languages are also different in various ways, such as writing systems, agglutinative property, case marking, sentence structure, and pro-drop nature. To identify the difference that relates to the human-like behaviors of LMs, experiments that include additional languages should be conducted in the future.\nIn addition, the corpus size of the BCCWJEyeTrack data is smaller than the Dundee Corpus. While the reading time data in the BCCWJEyeTrack was collected from various human subjects, the number of the independent segments was limited (1,643 segments, 218 sentences). Thus, whether the trends reported in this study generalize to more diverse Japanese texts should be explored in future work. It is hoped that this study motivates the creation of a large-scale corpus of human reading behaviors in diverse languages."
    }, {
      "heading" : "7 Conclusion",
      "text" : "This study has investigated whether the recent reports on the psychometric predictive power of LMs can be generalized across languages. Our initial\n10The correlation between PPL and the effect of syntactic category in the LMs with PPL less than 400 was 0.45 and 0.34 with Pearson’s r and Spearman’s ρ, respectively.\ninvestigation has re-examined the recent report— the lower PPL a LM has, the more human-like the LM is—using Japanese eye movement data. Our experiments have demonstrated a surprising lack of universality of this report; lower perplexity is not always human-like. This discrepancy of the results between the languages reinforces the need for the cross-lingual evaluation of the psychometric predictive power of LMs. The discussion considers potential factors that make the observation different across languages from the viewpoint of the uniform information density hypothesis. We believe that this is an important first step for seeking a language-agnostic model of human sentence processing. Hopefully, this study encourages researchers to further investigate the universality of human language processing across languages."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank the members at the Tohoku NLP Lab for their valuable advice, particularly Ana Brassard for proofreading. This work was supported by Grant-in-Aid for JSPS Fellows Grant Number JP20J22697, JSPS KAKENHI Grant Number 19H04990, and JST CREST Grant Number JPMJCR20D2. This work was also supported by the National Institute for Japanese Language and Linguistics (NINJAL) Collaborative Research Project “Computational Psycholinguistics of Language Processing with Large Corpora.”\nEthical considerations\nLanguage models with low perplexity are typically trained with a high computational cost. Our work demonstrates that further up-scaling the models might not be a reasonable direction of searching for human-like language models. This could potentially contribute to reducing energy and carbon costs, which are needed to train large-scale language models."
    }, {
      "heading" : "A Hyperparameters of LMs",
      "text" : "Table 2 shows the hyperparameters of TRANS-SM, TRANS-LG, and LSTM, respectively. Note that the number of parameter updates varies as described in Section 3."
    }, {
      "heading" : "B Factors used in regression models",
      "text" : "Descriptions for the factors used in our experiments are shown in Table 3. The frequency of a segment (freq) was estimated using the full training data for the LMs."
    }, {
      "heading" : "C Results of modeling logarithmic gaze duration in BCCWJ-EyeTrack",
      "text" : "Existing studies (Asahara et al., 2016) performed experiments using the logarithmic gaze duration because the logarithmic gaze duration more matches the normal distribution than the raw gaze duration. Given this, we additionally conducted experiments in Section 4, changing the target variable from the raw gaze duration to its logarithmic gaze duration. The result with this setting is shown in Figure 6. There was no substantial difference with the results shown in Section 4."
    }, {
      "heading" : "D Preliminary experiments in Section 5",
      "text" : "Which linguistic factor is helpful for explaining the difference in gaze duration? We conducted experiments using linguistic annotation in the BCCWJEyeTrack. Following the existing studies, we\nchecked the separate effect of syntactic category, semantic category (Asahara and Kato, 2017), and a particular aspect of hierarchical syntactic structure (i.e., the anti-locality effect) (Asahara et al., 2016). Specifically, we used the factors, syn category, sem category, and n dependents, shown in Table 3. For each factor, we inspect the separate effect of each factor for modeling gaze duration. As Eq. 4, we first modeled the gaze duration using each factor (factor X):\nGD ∼ factor X+ sentN + segmentN+ freq ∗ length .\n(5)\nThen, we calculated the ∆LogLik between X and a baseline model. The baseline model was GD ∼ sentN + segmentN + freq ∗ length.\nThe ∆LogLik for each collection of factors are shown in 5. We found that syntactic category is the most influential factor for modeling gaze duration, at least in this experiment."
    }, {
      "heading" : "E Anti-locality effect in LMs",
      "text" : "Similar to Section 5, we analyzed how strongly the surprisals from each Japanese LM are biased towards a particular linguistic property. In this section, we investigated the anti-locality effect in the surprisals from LMs. The anti-locality is that the more dependents a segment has in its preceding\ncontext, the cognitive effort of the head segment is reduced (i.e., modifiers alleviate the processing cost of their head).\nAnalogous to the Section 5, we regarded surprisal as “simulated gaze duration” from an “LM subject,” and evaluated the importance of the number of the dependents in its preceding context (n dependents) for modeling the simulated gaze duration (simulated GD). To inspect the effect of the n dependents for modeling the simulated gaze duration, we used the following regression model:\nsimulated GD ∼ n dependents+ sentN + tokenN+ freq ∗ length . (6)\nFrom this regression model, we obtained a loglikelihood score for the simulated gaze duration. To evaluate the separate effect of n dependents, we calculated the ∆LogLik between Eq. 6 and a baseline model. The baseline model was simulated GD ∼ sentN + segmentN + freq ∗ length. The ∆LogLik is denoted as “Effect of the anti-locality.”\nThe results are shown in Figure 7. There is a clear trend that the LMs with lower PPL exhibit surprisals that are more consistent with the antilocality effect (Spearman’s ρ = −0.77 between PPL and the strength of the anti-locality effect). This suggests that the surprisals from LMs with low PPL are biased towards the hierarchical structure of sentences rather than the syntactic category."
    } ],
    "references" : [ {
      "title" : "Between Reading Time and Information Structure",
      "author" : [ "Masayuki Asahara." ],
      "venue" : "Proceedings of PACLIC, pages 15–24.",
      "citeRegEx" : "Asahara.,? 2017",
      "shortCiteRegEx" : "Asahara.",
      "year" : 2017
    }, {
      "title" : "Between Reading Time and Clause Boundaries in Japanese - Wrap-up Effect in a Head-final Language",
      "author" : [ "Masayuki Asahara." ],
      "venue" : "Proceedings of PACLIC, pages 19–27.",
      "citeRegEx" : "Asahara.,? 2018",
      "shortCiteRegEx" : "Asahara.",
      "year" : 2018
    }, {
      "title" : "Between Reading Time and Syntactic / Semantic Categories",
      "author" : [ "Masayuki Asahara", "Sachi Kato." ],
      "venue" : "Proceedings of IJCNLP, pages 404–412.",
      "citeRegEx" : "Asahara and Kato.,? 2017",
      "shortCiteRegEx" : "Asahara and Kato.",
      "year" : 2017
    }, {
      "title" : "Reading-Time Annotations for “Balanced Corpus of Contemporary Written Japanese",
      "author" : [ "Masayuki Asahara", "Hajime Ono", "Edson T Miyamoto." ],
      "venue" : "Proceedings of COLING, pages 684– 694.",
      "citeRegEx" : "Asahara et al\\.,? 2016",
      "shortCiteRegEx" : "Asahara et al\\.",
      "year" : 2016
    }, {
      "title" : "Comparing Gated and Simple Recurrent Neural Network Architectures as Models of Human Sentence Processing",
      "author" : [ "C Aurnhammer", "S L Frank." ],
      "venue" : "Proceedings of CogSci, pages 112–118.",
      "citeRegEx" : "Aurnhammer and Frank.,? 2019",
      "shortCiteRegEx" : "Aurnhammer and Frank.",
      "year" : 2019
    }, {
      "title" : "Fitting linear mixed-effects models using lme4",
      "author" : [ "Douglas Bates", "Martin Mächler", "Ben Bolker", "Steve Walker." ],
      "venue" : "Journal of Statistical Software, Articles, 67(1):1–48.",
      "citeRegEx" : "Bates et al\\.,? 2015",
      "shortCiteRegEx" : "Bates et al\\.",
      "year" : 2015
    }, {
      "title" : "On Achieving and Evaluating Language-Independence in NLP",
      "author" : [ "Emily M Bender." ],
      "venue" : "Linguistic Issues in Language Technology, 6(3):1–26.",
      "citeRegEx" : "Bender.,? 2011",
      "shortCiteRegEx" : "Bender.",
      "year" : 2011
    }, {
      "title" : "Testing the processing hypothesis of word order variation using a probabilistic language model",
      "author" : [ "Jelke Bloem." ],
      "venue" : "Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity (CL4LC), pages 174–185, Osaka, Japan. The",
      "citeRegEx" : "Bloem.,? 2016",
      "shortCiteRegEx" : "Bloem.",
      "year" : 2016
    }, {
      "title" : "Abstract linguistic structure correlates with temporal activity during naturalistic comprehension",
      "author" : [ "Jonathan R Brennan", "Edward P Stabler", "Sarah E Van Wagenen", "Wen-Ming Luh", "John T Hale." ],
      "venue" : "Brain and language, 157:81–94.",
      "citeRegEx" : "Brennan et al\\.,? 2016",
      "shortCiteRegEx" : "Brennan et al\\.",
      "year" : 2016
    }, {
      "title" : "Language Models are Few-Shot Learners",
      "author" : [ "Amodei." ],
      "venue" : "Proceedings of NeurIPS.",
      "citeRegEx" : "Amodei.,? 2020",
      "shortCiteRegEx" : "Amodei.",
      "year" : 2020
    }, {
      "title" : "Extracting training data from large language",
      "author" : [ "Nicholas Carlini", "Florian Tramèr", "Eric Wallace", "Matthew Jagielski", "Ariel Herbert-Voss", "Katherine Lee", "Adam Roberts", "Tom B. Brown", "Dawn Song", "Úlfar Erlingsson", "Alina Oprea", "Colin Raffel" ],
      "venue" : null,
      "citeRegEx" : "Carlini et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Carlini et al\\.",
      "year" : 2020
    }, {
      "title" : "Data from eyetracking corpora as evidence for theories of syntactic processing complexity",
      "author" : [ "Vera Demberg", "Frank Keller." ],
      "venue" : "Journal of Cognition, 109(2):193–210.",
      "citeRegEx" : "Demberg and Keller.,? 2008",
      "shortCiteRegEx" : "Demberg and Keller.",
      "year" : 2008
    }, {
      "title" : "Sequential vs",
      "author" : [ "Victoria Fossum", "Roger Levy." ],
      "venue" : "Hierarchical Syntactic Models of Human Incremental Sentence Processing. In Proceedings of CMCL, pages 61–69, Montréal, Canada.",
      "citeRegEx" : "Fossum and Levy.,? 2012",
      "shortCiteRegEx" : "Fossum and Levy.",
      "year" : 2012
    }, {
      "title" : "Insensitivity of the Human Sentence-Processing System to Hierarchical Structure",
      "author" : [ "Stefan L Frank", "Rens Bod." ],
      "venue" : "Psychological science, 22(6):829– 834.",
      "citeRegEx" : "Frank and Bod.,? 2011",
      "shortCiteRegEx" : "Frank and Bod.",
      "year" : 2011
    }, {
      "title" : "The erp response to the amount of information conveyed by words in sentences",
      "author" : [ "Stefan L. Frank", "Leun J. Otten", "Giulia Galli", "Gabriella Vigliocco." ],
      "venue" : "Brain and Language, 140:1–11.",
      "citeRegEx" : "Frank et al\\.,? 2015",
      "shortCiteRegEx" : "Frank et al\\.",
      "year" : 2015
    }, {
      "title" : "Entropy rate constancy in text",
      "author" : [ "Dmitriy Genzel", "Eugene Charniak." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 199–206, Philadelphia, Pennsylvania, USA. Association for Computational Linguis-",
      "citeRegEx" : "Genzel and Charniak.,? 2002",
      "shortCiteRegEx" : "Genzel and Charniak.",
      "year" : 2002
    }, {
      "title" : "Predictive power of word surprisal for reading times is a linear function of language model quality",
      "author" : [ "Adam Goodkind", "Klinton Bicknell." ],
      "venue" : "Proceedings of CMCL2018, pages 10–18.",
      "citeRegEx" : "Goodkind and Bicknell.,? 2018",
      "shortCiteRegEx" : "Goodkind and Bicknell.",
      "year" : 2018
    }, {
      "title" : "A Probabilistic Earley Parser as a Psycholinguistic Model",
      "author" : [ "John Hale." ],
      "venue" : "Proceedings of NAACL, pages 159–166.",
      "citeRegEx" : "Hale.,? 2001",
      "shortCiteRegEx" : "Hale.",
      "year" : 2001
    }, {
      "title" : "Finding Syntax in Human Encephalography with Beam Search",
      "author" : [ "John Hale", "Chris Dyer", "Adhiguna Kuncoro", "Jonathan R. Brennan." ],
      "venue" : "Proceedings of ACL, pages 2727–2736.",
      "citeRegEx" : "Hale et al\\.,? 2018",
      "shortCiteRegEx" : "Hale et al\\.",
      "year" : 2018
    }, {
      "title" : "Long Short-Term Memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Journal of Neural Computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Speakers optimize information density through syntactic reduction",
      "author" : [ "T Jaeger", "Roger Levy." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 19, pages 849–856. MIT Press.",
      "citeRegEx" : "Jaeger and Levy.,? 2007",
      "shortCiteRegEx" : "Jaeger and Levy.",
      "year" : 2007
    }, {
      "title" : "Scaling Laws for Neural Language Models",
      "author" : [ "Jared Kaplan", "Sam McCandlish", "Tom Henighan", "Tom B Brown", "Benjamin Chess", "Rewon Child", "Scott Gray", "Alec Radford", "Jeffrey Wu", "Dario Amodei." ],
      "venue" : "arXiv preprint arXiv:2001.08361.",
      "citeRegEx" : "Kaplan et al\\.,? 2020",
      "shortCiteRegEx" : "Kaplan et al\\.",
      "year" : 2020
    }, {
      "title" : "The dundee corpus",
      "author" : [ "Alan Kennedy", "Robin Hill", "Joël Pynte." ],
      "venue" : "Proceedings of the 12th European conference on eye movement.",
      "citeRegEx" : "Kennedy et al\\.,? 2003",
      "shortCiteRegEx" : "Kennedy et al\\.",
      "year" : 2003
    }, {
      "title" : "Locality and Parsing Complexity",
      "author" : [ "Lars Konieczny." ],
      "venue" : "Journal of Psycholinguistic Research, 29(6):627–645.",
      "citeRegEx" : "Konieczny.,? 2000",
      "shortCiteRegEx" : "Konieczny.",
      "year" : 2000
    }, {
      "title" : "MeCab: Yet Another Part-of-speech and Morphological Analyzer",
      "author" : [ "Taku Kudo." ],
      "venue" : "http://mecab. sourceforge. jp.",
      "citeRegEx" : "Kudo.,? 2006",
      "shortCiteRegEx" : "Kudo.",
      "year" : 2006
    }, {
      "title" : "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
      "author" : [ "Taku Kudo", "John Richardson." ],
      "venue" : "Proceedings of EMNLP, pages 66–71.",
      "citeRegEx" : "Kudo and Richardson.,? 2018",
      "shortCiteRegEx" : "Kudo and Richardson.",
      "year" : 2018
    }, {
      "title" : "Probabilistic models of word order and syntactic discontinuity",
      "author" : [ "Roger Levy." ],
      "venue" : "stanford university.",
      "citeRegEx" : "Levy.,? 2005",
      "shortCiteRegEx" : "Levy.",
      "year" : 2005
    }, {
      "title" : "Expectation-based syntactic comprehension",
      "author" : [ "Roger Levy." ],
      "venue" : "Journal of Cognition, 106(3):1126– 1177.",
      "citeRegEx" : "Levy.,? 2008",
      "shortCiteRegEx" : "Levy.",
      "year" : 2008
    }, {
      "title" : "Why are some word orders more common than others? a uniform information density account",
      "author" : [ "Luke Maurits", "Dan Navarro", "Amy Perfors." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 23. Curran Associates, Inc.",
      "citeRegEx" : "Maurits et al\\.,? 2010",
      "shortCiteRegEx" : "Maurits et al\\.",
      "year" : 2010
    }, {
      "title" : "If beam search is the answer, what was the question? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2173–2185, Online",
      "author" : [ "Clara Meister", "Ryan Cotterell", "Tim Vieira." ],
      "venue" : "Association for Computa-",
      "citeRegEx" : "Meister et al\\.,? 2020",
      "shortCiteRegEx" : "Meister et al\\.",
      "year" : 2020
    }, {
      "title" : "Human Sentence Processing: Recurrence or Attention",
      "author" : [ "Danny Merkx", "Stefan L. Frank" ],
      "venue" : "In Procceding of CMCL",
      "citeRegEx" : "Merkx and Frank.,? \\Q2020\\E",
      "shortCiteRegEx" : "Merkx and Frank.",
      "year" : 2020
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chap-",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "Language Models are Unsupervised Multitask Learners",
      "author" : [ "Alec Radrof", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "Technical report, OpenAI.",
      "citeRegEx" : "Radrof et al\\.,? 2018",
      "shortCiteRegEx" : "Radrof et al\\.",
      "year" : 2018
    }, {
      "title" : "Effects of contextual constraint on eye movements in reading: A further examination",
      "author" : [ "Keith Rayner", "Arnold D Well." ],
      "venue" : "Psychonomic Bulletin & Review, 3(4):504–509.",
      "citeRegEx" : "Rayner and Well.,? 1996",
      "shortCiteRegEx" : "Rayner and Well.",
      "year" : 1996
    }, {
      "title" : "Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing",
      "author" : [ "Brian Roark", "Asaf Bachrach", "Carlos Cardenas", "Christophe Pallier." ],
      "venue" : "Proceedings of EMNLP, pages 324–333, Singapore.",
      "citeRegEx" : "Roark et al\\.,? 2009",
      "shortCiteRegEx" : "Roark et al\\.",
      "year" : 2009
    }, {
      "title" : "Neural Machine Translation of Rare Words with Subword Units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of ACL, pages 1715–1725.",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "The effect of word predictability on reading time is logarithmic",
      "author" : [ "Nathaniel J. Smith", "Roger Levy." ],
      "venue" : "Journal of Cognition, 128(3):302–319.",
      "citeRegEx" : "Smith and Levy.,? 2013",
      "shortCiteRegEx" : "Smith and Levy.",
      "year" : 2013
    }, {
      "title" : "Attention Is All You Need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of NIPS, pages 5998– 6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "A cognitive regularizer for language modeling",
      "author" : [ "Jason Wei", "Clara Meister", "Ryan Cotterell." ],
      "venue" : "arXiv preprint arXiv:2105.07144.",
      "citeRegEx" : "Wei et al\\.,? 2021",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2021
    }, {
      "title" : "On the Predictive Power of Neural Language Models for Human RealTime Comprehension Behavior",
      "author" : [ "Ethan Gotlieb Wilcox", "Jon Gauthier", "Jennifer Hu", "Peng Qian", "Roger Levy." ],
      "venue" : "Proceedings of CogSci, pages 1707–1713.",
      "citeRegEx" : "Wilcox et al\\.,? 2020",
      "shortCiteRegEx" : "Wilcox et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : ", surprisal) impacts its processing difficulty in incremental human language comprehension (Hale, 2001; Demberg and Keller, 2008; Levy, 2008; Smith and Levy, 2013).",
      "startOffset" : 91,
      "endOffset" : 163
    }, {
      "referenceID" : 11,
      "context" : ", surprisal) impacts its processing difficulty in incremental human language comprehension (Hale, 2001; Demberg and Keller, 2008; Levy, 2008; Smith and Levy, 2013).",
      "startOffset" : 91,
      "endOffset" : 163
    }, {
      "referenceID" : 27,
      "context" : ", surprisal) impacts its processing difficulty in incremental human language comprehension (Hale, 2001; Demberg and Keller, 2008; Levy, 2008; Smith and Levy, 2013).",
      "startOffset" : 91,
      "endOffset" : 163
    }, {
      "referenceID" : 36,
      "context" : ", surprisal) impacts its processing difficulty in incremental human language comprehension (Hale, 2001; Demberg and Keller, 2008; Levy, 2008; Smith and Levy, 2013).",
      "startOffset" : 91,
      "endOffset" : 163
    }, {
      "referenceID" : 34,
      "context" : "Building on this basis, researchers have compared a variety of language models (LMs) in terms of how well their surprisal correlates with human reading behavior (Roark et al., 2009; Frank and Bod, 2011; Fossum and Levy, 2012; Hale et al., 2018; Goodkind and Bicknell, 2018; Aurnhammer and Frank, 2019; Merkx and Frank, 2020; Wilcox et al., 2020).",
      "startOffset" : 161,
      "endOffset" : 345
    }, {
      "referenceID" : 13,
      "context" : "Building on this basis, researchers have compared a variety of language models (LMs) in terms of how well their surprisal correlates with human reading behavior (Roark et al., 2009; Frank and Bod, 2011; Fossum and Levy, 2012; Hale et al., 2018; Goodkind and Bicknell, 2018; Aurnhammer and Frank, 2019; Merkx and Frank, 2020; Wilcox et al., 2020).",
      "startOffset" : 161,
      "endOffset" : 345
    }, {
      "referenceID" : 12,
      "context" : "Building on this basis, researchers have compared a variety of language models (LMs) in terms of how well their surprisal correlates with human reading behavior (Roark et al., 2009; Frank and Bod, 2011; Fossum and Levy, 2012; Hale et al., 2018; Goodkind and Bicknell, 2018; Aurnhammer and Frank, 2019; Merkx and Frank, 2020; Wilcox et al., 2020).",
      "startOffset" : 161,
      "endOffset" : 345
    }, {
      "referenceID" : 18,
      "context" : "Building on this basis, researchers have compared a variety of language models (LMs) in terms of how well their surprisal correlates with human reading behavior (Roark et al., 2009; Frank and Bod, 2011; Fossum and Levy, 2012; Hale et al., 2018; Goodkind and Bicknell, 2018; Aurnhammer and Frank, 2019; Merkx and Frank, 2020; Wilcox et al., 2020).",
      "startOffset" : 161,
      "endOffset" : 345
    }, {
      "referenceID" : 16,
      "context" : "Building on this basis, researchers have compared a variety of language models (LMs) in terms of how well their surprisal correlates with human reading behavior (Roark et al., 2009; Frank and Bod, 2011; Fossum and Levy, 2012; Hale et al., 2018; Goodkind and Bicknell, 2018; Aurnhammer and Frank, 2019; Merkx and Frank, 2020; Wilcox et al., 2020).",
      "startOffset" : 161,
      "endOffset" : 345
    }, {
      "referenceID" : 4,
      "context" : "Building on this basis, researchers have compared a variety of language models (LMs) in terms of how well their surprisal correlates with human reading behavior (Roark et al., 2009; Frank and Bod, 2011; Fossum and Levy, 2012; Hale et al., 2018; Goodkind and Bicknell, 2018; Aurnhammer and Frank, 2019; Merkx and Frank, 2020; Wilcox et al., 2020).",
      "startOffset" : 161,
      "endOffset" : 345
    }, {
      "referenceID" : 30,
      "context" : "Building on this basis, researchers have compared a variety of language models (LMs) in terms of how well their surprisal correlates with human reading behavior (Roark et al., 2009; Frank and Bod, 2011; Fossum and Levy, 2012; Hale et al., 2018; Goodkind and Bicknell, 2018; Aurnhammer and Frank, 2019; Merkx and Frank, 2020; Wilcox et al., 2020).",
      "startOffset" : 161,
      "endOffset" : 345
    }, {
      "referenceID" : 39,
      "context" : "Building on this basis, researchers have compared a variety of language models (LMs) in terms of how well their surprisal correlates with human reading behavior (Roark et al., 2009; Frank and Bod, 2011; Fossum and Levy, 2012; Hale et al., 2018; Goodkind and Bicknell, 2018; Aurnhammer and Frank, 2019; Merkx and Frank, 2020; Wilcox et al., 2020).",
      "startOffset" : 161,
      "endOffset" : 345
    }, {
      "referenceID" : 6,
      "context" : "Despite the community’s ongoing search for a languageindependent model (Bender, 2011), existing studies have focused almost exclusively on the English language.",
      "startOffset" : 71,
      "endOffset" : 85
    }, {
      "referenceID" : 12,
      "context" : "LM is—in the English and Japanese languages (Fossum and Levy, 2012; Goodkind and Bicknell, 2018; Wilcox et al., 2020).",
      "startOffset" : 44,
      "endOffset" : 117
    }, {
      "referenceID" : 16,
      "context" : "LM is—in the English and Japanese languages (Fossum and Levy, 2012; Goodkind and Bicknell, 2018; Wilcox et al., 2020).",
      "startOffset" : 44,
      "endOffset" : 117
    }, {
      "referenceID" : 39,
      "context" : "LM is—in the English and Japanese languages (Fossum and Levy, 2012; Goodkind and Bicknell, 2018; Wilcox et al., 2020).",
      "startOffset" : 44,
      "endOffset" : 117
    }, {
      "referenceID" : 21,
      "context" : "Recent studies in the machine learning field have reported that more parameters, training data, and computation cost can result in better PPL (Kaplan et al., 2020; Brown et al., 2020).",
      "startOffset" : 142,
      "endOffset" : 183
    }, {
      "referenceID" : 15,
      "context" : "This opens a remaining and important question: why are English and Japanese different in this aspect? We discuss the differing results between English and Japanese from the perspective of the uniform information density hypothesis (Genzel and Charniak, 2002; Levy, 2005; Jaeger and Levy, 2007).",
      "startOffset" : 231,
      "endOffset" : 293
    }, {
      "referenceID" : 26,
      "context" : "This opens a remaining and important question: why are English and Japanese different in this aspect? We discuss the differing results between English and Japanese from the perspective of the uniform information density hypothesis (Genzel and Charniak, 2002; Levy, 2005; Jaeger and Levy, 2007).",
      "startOffset" : 231,
      "endOffset" : 293
    }, {
      "referenceID" : 20,
      "context" : "This opens a remaining and important question: why are English and Japanese different in this aspect? We discuss the differing results between English and Japanese from the perspective of the uniform information density hypothesis (Genzel and Charniak, 2002; Levy, 2005; Jaeger and Levy, 2007).",
      "startOffset" : 231,
      "endOffset" : 293
    }, {
      "referenceID" : 17,
      "context" : "What factor determines the incremental difficulty of human language processing? At present, surprisal theory (Hale, 2001; Levy, 2008) has been widely adopted in the field of computational psycholinguistics.",
      "startOffset" : 109,
      "endOffset" : 133
    }, {
      "referenceID" : 27,
      "context" : "What factor determines the incremental difficulty of human language processing? At present, surprisal theory (Hale, 2001; Levy, 2008) has been widely adopted in the field of computational psycholinguistics.",
      "startOffset" : 109,
      "endOffset" : 133
    }, {
      "referenceID" : 22,
      "context" : "Data such as eye movement (Kennedy et al., 2003) and brain activity (Frank et al.",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 14,
      "context" : ", 2003) and brain activity (Frank et al., 2015; Brennan et al., 2016) are used as measures of human reading behavior.",
      "startOffset" : 27,
      "endOffset" : 69
    }, {
      "referenceID" : 8,
      "context" : ", 2003) and brain activity (Frank et al., 2015; Brennan et al., 2016) are used as measures of human reading behavior.",
      "startOffset" : 27,
      "endOffset" : 69
    }, {
      "referenceID" : 12,
      "context" : "Recently, researchers reported that surprisals from LMs with low PPL correlate well with human reading behaviors (Fossum and Levy, 2012; Goodkind and Bicknell, 2018; Aurnhammer and Frank, 2019; Wilcox et al., 2020).",
      "startOffset" : 113,
      "endOffset" : 214
    }, {
      "referenceID" : 16,
      "context" : "Recently, researchers reported that surprisals from LMs with low PPL correlate well with human reading behaviors (Fossum and Levy, 2012; Goodkind and Bicknell, 2018; Aurnhammer and Frank, 2019; Wilcox et al., 2020).",
      "startOffset" : 113,
      "endOffset" : 214
    }, {
      "referenceID" : 4,
      "context" : "Recently, researchers reported that surprisals from LMs with low PPL correlate well with human reading behaviors (Fossum and Levy, 2012; Goodkind and Bicknell, 2018; Aurnhammer and Frank, 2019; Wilcox et al., 2020).",
      "startOffset" : 113,
      "endOffset" : 214
    }, {
      "referenceID" : 39,
      "context" : "Recently, researchers reported that surprisals from LMs with low PPL correlate well with human reading behaviors (Fossum and Levy, 2012; Goodkind and Bicknell, 2018; Aurnhammer and Frank, 2019; Wilcox et al., 2020).",
      "startOffset" : 113,
      "endOffset" : 214
    }, {
      "referenceID" : 32,
      "context" : "They examined the relationship between PPL, psychometric predictive power, and syntactic knowledge in LMs using a variety of models, including modern neural LMs (Radrof et al., 2018).",
      "startOffset" : 161,
      "endOffset" : 182
    }, {
      "referenceID" : 3,
      "context" : "Recently, eye movement data for naturally occurring Japanese texts have recently become available (Asahara et al., 2016) and was extensively annotated with various linguistic properties (Asahara and Kato, 2017; Asahara, 2017, 2018).",
      "startOffset" : 98,
      "endOffset" : 120
    }, {
      "referenceID" : 2,
      "context" : ", 2016) and was extensively annotated with various linguistic properties (Asahara and Kato, 2017; Asahara, 2017, 2018).",
      "startOffset" : 73,
      "endOffset" : 118
    }, {
      "referenceID" : 39,
      "context" : "Based on the reports that subword-level English LMs exhibits superior psychometric predictive power (Wilcox et al., 2020), input texts were divided into subwords by a byte-pair encoding (BPE) (Sennrich et al.",
      "startOffset" : 100,
      "endOffset" : 121
    }, {
      "referenceID" : 35,
      "context" : ", 2020), input texts were divided into subwords by a byte-pair encoding (BPE) (Sennrich et al., 2016).",
      "startOffset" : 78,
      "endOffset" : 101
    }, {
      "referenceID" : 24,
      "context" : "Input texts were first segmented into morphemes by MeCab (Kudo, 2006), and then further divided into subwords by BPE.",
      "startOffset" : 57,
      "endOffset" : 69
    }, {
      "referenceID" : 37,
      "context" : "Architectures: The following four variants of LMs were used: Transformer-large (TRANSLG) (Vaswani et al., 2017), Transformer-small (TRANS-SM), LSTM (LSTM) (Hochreiter and Schmidhuber, 1997), and N-gram LMs (NGRAM).",
      "startOffset" : 89,
      "endOffset" : 111
    }, {
      "referenceID" : 19,
      "context" : ", 2017), Transformer-small (TRANS-SM), LSTM (LSTM) (Hochreiter and Schmidhuber, 1997), and N-gram LMs (NGRAM).",
      "startOffset" : 51,
      "endOffset" : 85
    }, {
      "referenceID" : 31,
      "context" : "(3)The neural LMs were trained with the fairseq toolkit (Ott et al., 2019).",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 22,
      "context" : "English: The Dundee Corpus (Kennedy et al., 2003), which contains gaze duration annotation for each word, was used.",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 3,
      "context" : "Japanese: The BCCWJ-EyeTrack (Asahara et al., 2016), which contains gaze duration annotation for each phrasal unit, was used.",
      "startOffset" : 29,
      "endOffset" : 51
    }, {
      "referenceID" : 3,
      "context" : "Note that the BCCWJ-EyeTrack data was deliberately designed to address language-specific issues in Japanese such as the lack of segmentation spaces in Japanese texts (Asahara et al., 2016).",
      "startOffset" : 166,
      "endOffset" : 188
    }, {
      "referenceID" : 13,
      "context" : "A model with low PPL is also called a linguistically accurate model (Frank and Bod, 2011).",
      "startOffset" : 68,
      "endOffset" : 89
    }, {
      "referenceID" : 16,
      "context" : "Following the existing studies (Goodkind and Bicknell, 2018; Merkx and Frank, 2020; Wilcox et al., 2020), the psychometric predictive power of a model was measured by comparing surprisal from the model and gaze duration from human subjects.",
      "startOffset" : 31,
      "endOffset" : 104
    }, {
      "referenceID" : 30,
      "context" : "Following the existing studies (Goodkind and Bicknell, 2018; Merkx and Frank, 2020; Wilcox et al., 2020), the psychometric predictive power of a model was measured by comparing surprisal from the model and gaze duration from human subjects.",
      "startOffset" : 31,
      "endOffset" : 104
    }, {
      "referenceID" : 39,
      "context" : "Following the existing studies (Goodkind and Bicknell, 2018; Merkx and Frank, 2020; Wilcox et al., 2020), the psychometric predictive power of a model was measured by comparing surprisal from the model and gaze duration from human subjects.",
      "startOffset" : 31,
      "endOffset" : 104
    }, {
      "referenceID" : 39,
      "context" : "Following the study using subwords (Wilcox et al., 2020), the surprisal of each segment was calculated using the joint probability of its constituent subwords.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 5,
      "context" : "The effect of surprisals for modeling human reading behavior was calculated using a linear mixedeffects regression (Bates et al., 2015).",
      "startOffset" : 115,
      "endOffset" : 135
    }, {
      "referenceID" : 3,
      "context" : "A collection of factors used in the existing studies (Asahara et al., 2016; Wilcox et al., 2020) were initially examined and the factors that were not significant (p > 0.",
      "startOffset" : 53,
      "endOffset" : 96
    }, {
      "referenceID" : 39,
      "context" : "A collection of factors used in the existing studies (Asahara et al., 2016; Wilcox et al., 2020) were initially examined and the factors that were not significant (p > 0.",
      "startOffset" : 53,
      "endOffset" : 96
    }, {
      "referenceID" : 33,
      "context" : "In English experiments, surprisals of preceding words (surprisal prev 1 and surprisal prev 2) were included in order to handle the spillover effect (the processing cost of a certain segment is affected by its preceding segments) (Rayner and Well, 1996; Smith and Levy, 2013).",
      "startOffset" : 229,
      "endOffset" : 274
    }, {
      "referenceID" : 36,
      "context" : "In English experiments, surprisals of preceding words (surprisal prev 1 and surprisal prev 2) were included in order to handle the spillover effect (the processing cost of a certain segment is affected by its preceding segments) (Rayner and Well, 1996; Smith and Levy, 2013).",
      "startOffset" : 229,
      "endOffset" : 274
    }, {
      "referenceID" : 16,
      "context" : "Dundee Corpus: First, the results of the data from the Dundee Corpus show a clear relationship between PPL and psychometric predictive power; namely, lower PPL corresponds to more psychometric predictive power, as reported by prior studies (Goodkind and Bicknell, 2018; Wilcox et al., 2020).",
      "startOffset" : 240,
      "endOffset" : 290
    }, {
      "referenceID" : 39,
      "context" : "Dundee Corpus: First, the results of the data from the Dundee Corpus show a clear relationship between PPL and psychometric predictive power; namely, lower PPL corresponds to more psychometric predictive power, as reported by prior studies (Goodkind and Bicknell, 2018; Wilcox et al., 2020).",
      "startOffset" : 240,
      "endOffset" : 290
    }, {
      "referenceID" : 15,
      "context" : "This principle has been typically investigated under the uniform information density (UID) hypothesis (Genzel and Charniak, 2002; Levy, 2005; Jaeger and Levy, 2007).",
      "startOffset" : 102,
      "endOffset" : 164
    }, {
      "referenceID" : 26,
      "context" : "This principle has been typically investigated under the uniform information density (UID) hypothesis (Genzel and Charniak, 2002; Levy, 2005; Jaeger and Levy, 2007).",
      "startOffset" : 102,
      "endOffset" : 164
    }, {
      "referenceID" : 20,
      "context" : "This principle has been typically investigated under the uniform information density (UID) hypothesis (Genzel and Charniak, 2002; Levy, 2005; Jaeger and Levy, 2007).",
      "startOffset" : 102,
      "endOffset" : 164
    }, {
      "referenceID" : 28,
      "context" : "This phenomenon was also investigated through the lens of word order, where SOV languages such as Japanese are reported to show less uniformity of information density (Maurits et al., 2010).",
      "startOffset" : 167,
      "endOffset" : 189
    }, {
      "referenceID" : 7,
      "context" : "8 That is, LMs are, in theory, trained to approach a model satisfying the UID assumption (Bloem, 2016), where all surprisals from the LM are equally, sufficiently small across the segments.",
      "startOffset" : 89,
      "endOffset" : 102
    }, {
      "referenceID" : 10,
      "context" : "Note the tendency of LMs to assign unreasonably high probabilities to segments has also attracted attention from the viewpoint of memorization capability of LMs (Carlini et al., 2020).",
      "startOffset" : 161,
      "endOffset" : 183
    }, {
      "referenceID" : 29,
      "context" : "In addition, the connection of the UID hypothesis to the modern NLP techniques has been recently explored (Meister et al., 2020; Wei et al., 2021).",
      "startOffset" : 106,
      "endOffset" : 146
    }, {
      "referenceID" : 38,
      "context" : "In addition, the connection of the UID hypothesis to the modern NLP techniques has been recently explored (Meister et al., 2020; Wei et al., 2021).",
      "startOffset" : 106,
      "endOffset" : 146
    } ],
    "year" : 2021,
    "abstractText" : "In computational psycholinguistics, various language models have been evaluated against human reading behavior (e.g., eye movement) to build human-like computational models. However, most previous efforts have focused almost exclusively on English, despite the recent trend towards linguistic universal within the general community. In order to fill the gap, this paper investigates whether the established results in computational psycholinguistics can be generalized across languages. Specifically, we re-examine an established generalization —the lower perplexity a language model has, the more human-like the language model is— in Japanese with typologically different structures from English. Our experiments demonstrate that this established generalization exhibits a surprising lack of universality; namely, lower perplexity is not always human-like. Moreover, this discrepancy between English and Japanese is further explored from the perspective of (non-)uniform information density. Overall, our results suggest that a crosslingual evaluation will be necessary to construct human-like computational models.",
    "creator" : "LaTeX with hyperref"
  }
}