{
  "name" : "2021.acl-long.205.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "N -ary Constituent Tree Parsing with Recursive Semi-Markov Model",
    "authors" : [ "Xin Xin", "Jinlong Li", "Zeqi Tan" ],
    "emails" : [ "xxin@bit.edu.cn", "jllee@bit.edu.cn", "zqtan@zju.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2631–2642\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2631"
    }, {
      "heading" : "1 Introduction",
      "text" : "There are two settings for constituent parsing models, including binary tree parsing and n-ary tree parsing. In the former, the original constituent tree with n-ary nodes is converted into a binary tree by language-specific rules. The model first predicts the binary tree, and then converts it back. In the latter, the model directly predicts the n-ary tree without the intermediate step of binarization.\n∗Xin Xin is the corresponding author.\nIn the paper, we focus on the setting of n-ary tree parsing. Compared with binary tree parsing, which has the advantage of utilizing the lexical head information, n-ary tree parsing is more natural to fit the original tree structure, and is more adaptable to languages that do not have head rules for binarization. In addition, for languages with the word segmentation issue, such as Chinese, it is very convenient for n-ary tree parsing models to deal with the joint task of word segmentation, partof-speech (POS) tagging and constituent parsing, by just enlarging the label set with the POS labels, as shown in Fig. 1 (a), which alleviates the error propagation from the pipeline.\nSpecifically, we target at improving graph-based models for n-ary tree parsing, which obtain better performances in recent work (Kitaev et al., 2019; Zhang et al., 2020; Wei et al., 2020) from the two streams of well-developed parsing methods, graphbased and transition-based. For n-ary tree parsing, the main idea of previous graph-based models is to generate hidden nodes with the dummy label φ inside the n-ary node, in order to expand the n-ary tree into a binary tree. In this way, n-ary tree parsing can be converted into binary tree parsing with hidden nodes, which are unobservable in the training process. Consider the n-ary node “VP→VV,\nNP, QP” in Fig. 1 (a) as an example. The hidden nodes can be in two manners, as shown in Fig. 1 (b, c). Either of them can be seen as being correct in training. For convenience, the potential scores of such hidden nodes are manually set to zero, to ensure that the two manners are equivalent when calculating the likelihood (Kitaev and Klein, 2018).\nThe limitation of previous methods is that the generated hidden nodes break the sibling relations of the n-ary node’s children. Consequently, such sibling dependency feature might not be accurately modeled and is being ignored. Consider the node “VP→VV, NP, QP” in the above example. If we model the 1-order dependency from the sibling node pair, dependency feature scores should be calculated from both pairs of (VV, NP) and (NP, QP). Without loss of generality, suppose the hidden node is as shown in Fig. 1 (b), and the case in Fig. 1 (c) is similar. As the hidden node φ is forced to be the sibling node of “QP”, the dependency feature of (NP, QP) cannot be directly calculated. In implementation, only potential scores of each node are modeled, and the dependency potential scores of sibling node pairs are being ignored.\nTo solve this limitation, we propose a novel framework for n-ary tree parsing. Our main idea is to utilize 1-order semi-Markov model to directly predict the immediate children sequence of an n-ary node, without generating the hidden nodes for binarization, as shown in Fig. 2. Different from previous models that only have potential scores on nodes when evaluating a tree’s likelihood, the potential scores of sibling node pairs are also calculated as 1-order transition features. Thus dependencies from sibling nodes can be naturally modeled, which solves the above limitation. When generating an n-ary tree, the semi-Markov model is recursively conducted on the node spans in a bottom-up manner, thus we call the proposed model “recursive semi-Markov model”.\nThe main challenge of designing the recursive semi-Markov model is how to make the computational complexity being acceptable. In nowadays GPU era, to make full use of parallel computation is an important issue to enhance the processing speed. For example, in the previous CYK (Kasami, 1966) algorithm for binary trees, the absolute time complexity is O(n3), where n is the sentence length. But O(n2) out of it can be computed in parallel, by batchifying the spans with the same length and the divisions within a span. This means the hard time complexity of CYK, which cannot be computed in parallel, is O(n). In the case of the proposed recursive semi-Markov model, the time complexity of the straight-forward dynamic programming algorithm is O(n5). But by careful design, we propose an algorithm, whose complexity is O(n4), with O(n3) out of it can be batchified. It means the increased O(n) complexity compared with CYK can be calculated in parallel. In practice, the proposed framework can process 26 and 11 sentences per second in PTB and CTB 5.1 test sets respectively, by a single NVIDIA RTX GPU.\nOur main contributions can be summarized as follows. (1) We propose a novel graph-based framework, recursive semi-Markov model, for n-ary constituent tree parsing, which can model the dependencies of sibling nodes. (2) We design a dynamic programming algorithm for the proposed framework, whose complexity is O(n4), with O(n3) inside can be batchified. (3) Experimental verifications demonstrate that the proposed framework outperforms previous methods. The F1 of the proposed framework is 95.92% and 92.50% in PTB and CTB 5.1 respectively. In the joint task with segmentation and POS tagging in CTB 5.1, the F1 is 91.84%. In addition, the proposed framework can effectively predict nodes with more than two children, improving the F1 by 0.3-1.1 points in PTB and 2.3-6.8 points in CTB 5.1.\nOur code is released at https://github.com/NPNET-research/Recursive-Semi-Markov-Model, which is developed on the base of the open-source Berkekey parser (Kitaev and Klein, 2018; Kitaev et al., 2019)."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Early Models for N -ary Tree Parsing",
      "text" : "A representative of classical methods for n-ary tree parsing is the Earley algorithm (Earley, 1970). It can find legal trees of sentence fitting the grammar\nrules with the complexity of O(Cn3) by dynamic programming, where n is the sentence length and C is dependent on the complexity of grammar rules. The dependency with the size of grammar rules in the Earley algorithm increases the computational complexity substantially in practice. Therefore, recent studies have paid more attention to utilizing “less grammar” (Hall et al., 2014), which is implemented in CYK/shift-reduce algorithms (Durrett and Klein, 2015; Liu and Zhang, 2017b; Stern et al., 2017; Teng and Zhang, 2018) instead of the Earley algorithm. It demonstrates it can reduce the complexity and also obtain better performances.\nOur proposed framework is in line with the recent studies, whose complexity is independent with the size of grammar rules."
    }, {
      "heading" : "2.2 Graph-Based N -ary Tree Parsing",
      "text" : "Graph-based parsing models utilize the CYK algorithm to find the tree with the largest feature score as the prediction. The main advantage is the large search space and the globally optimal inference. A representative of graph-based n-ary tree parsing model is the Berkeley parser (Stern et al., 2017; Kitaev and Klein, 2018; Kitaev et al., 2019), which employs hidden nodes to deal with n-ary nodes.\nThe proposed framework belongs to graph-based n-ary tree parsing models. Compared with previous work, the novelty lies in that semi-Markov model is utilized to directly model the children sequence of an n-ary node, instead of generating a binary tree with hidden nodes. Consequently, it can avoid breaking the sibling relation of nodes in the sequence. The proposed framework then makes use of such dependencies to improve the parsing performance."
    }, {
      "heading" : "2.3 Transition-Based N -ary Trees Parsing",
      "text" : "Transition-based models make predictions sequentially, with advantages of the low computational cost and the utilization of high-order features. The models can be divided into post-order (Cross and Huang, 2016; Fernández-González and GómezRodrı́guez, 2019), pre-order (Dyer et al., 2016), and in-order (Liu and Zhang, 2017a), according to the traversal manner of the action sequence. Post-order models require to deciding the number of reduced nodes for n-ary nodes (Fernández-González and Gómez-Rodrı́guez, 2019), or to introducing hidden nodes with dummy label (Cross and Huang, 2016). Pre-order models and in-order models are born to\nhave convenience in dealing with n-ary nodes, as the number of reduced nodes is fixed.\nBoth the proposed framework and some of the above methods directly model the sequence within an n-ary node. The novelty of the proposed framework is that it models the sequence as a graphbased model rather than a transition-based model. Transition-based models suffer from the limitation of local optimization in the inference process, but graph-based models can guarantee the globally optimal inference. In recent studies, graph-based models have been demonstrated to perform better than transition-based models (Kitaev et al., 2019; Zhang et al., 2020; Wei et al., 2020)."
    }, {
      "heading" : "3 The Recursive Semi-Markov Model",
      "text" : ""
    }, {
      "heading" : "3.1 Preliminaries",
      "text" : "A sentence is denoted by x = {xi}, with xi being the ith word. The sentence length is denoted by n. Let Y be the set of the alphabet constituent labels. Following previous work (Kitaev and Klein, 2018; Zhang et al., 2020), the nodes with unary grammars are collapsed, and its label is replaced by the joint label of the collapsed nodes. For example, in Fig. 1 (a), “CP→IP” will be replaced by “CP+IP”, where “CP+IP” is an atomic label. Given x, the task is to build an n-ary tree on top of it, and assign a label to each internal node. When conducting the joint parsing task with word segmentation and POS tagging in Chinese, Y is enriched with the POS labels and a “C” label (denoting characters), and xi denotes the ith character. For example, in Fig. 1 (a), “NN” is a POS label, and “NP+NN” is treated as an atomic label for the corresponding node in the joint parsing task."
    }, {
      "heading" : "3.2 The Framework Structure",
      "text" : "In the proposed recursive semi-Markov model, the probabilistic graph of a constituent tree is shown\nin Fig. 3. This graph corresponds to the tree in Fig. 1 (a). Full circles refer to the input x. Blank circles refer to the internal nodes, which can be seen as variables in the probabilistic graph. The full line, which connects two nodes, means that the two nodes are dependent with each other. The dotted line pointing to an internal node refers to the sequence of the node’s immediate children. There are two kinds of cliques in the graph, the one with a single node, and the one with two sibling nodes. The former corresponds to 0-order cliques, and the latter corresponds to 1-order cliques. The whole framework is a 1-order semi-Markov model.\nPotential scores, which are assigned to the above two kinds of cliques, are denoted by ρ(i, j, l|x, θ), and ψ(i, j, k, l1, l2|x, θ), respectively. θ is the model parameters, including neural network weights and word embeddings. In the following, we omit the symbol x and θ in equations for presentation simplicity. ρ(i, j, l) defines the emission feature score of a span, describing how likely the span is a constituent. (i, j) denotes a span which starts at i and ends at j − 1, 0 ≤ i < j ≤ n. l ∈ Y denotes the span’s label. ψ(i, j, k, l1, l2) defines the transition feature score of two sibling spans, describing how likely the two spans are sibling neighbors within an n-ary node. (i, j, k) denotes the two sibling spans (i, j) and (j, k). l1 is the label of the left span, and l2 is the label of the right span.\nLet y denote a predicted tree given x. The conditional probability p(y|x) can be defined on the probabilistic graph, under the framework of conditional random fields (CRF) (Lafferty et al., 2001), as shown in the following equations. C1(y) denotes the set of emission scores, and C2(y) denotes the set of transition scores. T (x) denotes all legal n-ary trees that can be built on top of the input sentence x. s(y) is the sum of clique potential scores defined in a whole tree, with two examples shown\nin Fig. 4. Given the parameters θ, the inference process is to find a tree with the largest probability.\ns(y) = ∑ C1(y) ρ(i, j, l) + ∑ C2(y) ψ(i, j, k, l1, l2) (1)\np(y|x) = exp(s(y)))∑ y′∈T (x) exp(s(y ′))"
    }, {
      "heading" : "3.3 Potential Score Calculations",
      "text" : "Given an input sentence x, we follow the neural network architecture of the Berkeley parser (Kitaev and Klein, 2018) with some minor revisions, to calculate the two kinds of potential scores, ρ(i, j, l), and ψ(i, j, k, l1, l2), as shown in Fig. 5.\nIn the embedding layer, the BERT (Devlin et al., 2019; Wolf et al., 2020) is selected to generate pretrained vectors, denoted by ei, 0 ≤ i < n. For the Chinese language, ei refers to the ith character, and the embedding vector of last character within the word is chosen to represent the word.\nei = BERT(xi|x)\nIn the encoding layer, the Transformer (Vaswani et al., 2017) is selected for extracting the context features, denoted by hi, with odd dimensions −→ h i and the even dimensions ←− h i.\nhi = Transformer(ei|x)\nThe representation of a single span (i, j) is formed by v(i, j) = [ −→ h j− −→ h i; ←− h j−1− ←− h i−1], and the representation of a sibling span pair (i, j) and (j, k) is formed by v(i, j, k) = [v(i, j); v(j, k)]. [; ]\nis the concatenate operation. By passing v(i, j) and v(i, j, k) through multi-layer perceptrons (MLP), the emission potential score is finally defined as\nρ(i, j, l) = MLPemissionl (v(i, j)),\nand the transition potential score is defined as\nψ(i, j, k, l1, l2) = MLP transition l1,l2 (v(i, j, k)).\nThere are totally |Y|MLPs for ρ. |Y| is the size of the label set Y . Parameters in the hidden layers are shared among them, and only the parameters of the output layers are different to distinguish different labels. Similarly, there are |Y|2 MLPs for ψ, whose parameters in hidden layers are also shared."
    }, {
      "heading" : "3.4 The Max-Margin Loss",
      "text" : "When designing the loss function, theoretically, we can follow the CRF framework to optimize the loglikelihood of the training data. But in practice, if we do this, the gradients of all potential scores, which is O(n4) (n is the sentence length), should be stored in the GPU memory. This is impossible to be implemented in a general GPU device. Therefore, we employ the max-margin loss as the training objective to learn the parameters of the proposed framework, following the Berkeley parser (Kitaev and Klein, 2018). By max-margin, only the gradients of the predicted tree structure and the gold structure need to be stored, which is O(n). Consequently, it saves a lot of memory in implementation.\nLet s(y) in Eq. 1 denote the total potential score of a tree y. Suppose the gold tree is yg, with the potential score s(yg). The key idea of the maxmargin loss is to let the maximum potential score of the other trees, denoted as s(y∗), be less than s(yg) by an acceptable margin. In the probability space, it is equivalent that the probability of the gold tree is larger than the maximum probability of the other trees by a margin. The formal definition of the objective is to minimize the following hinge loss, where ∆(y, yg) refers to the number of spans in yg not matched in y.\nL = max ( 0, max\ny∈T (x) [s(y) + ∆(y, yg)]− s(yg)\n)"
    }, {
      "heading" : "3.5 Explanations of the Proposed Model",
      "text" : "The Semi-Markov Property. The semi-Markov property of the proposed model refers to the one\nmentioned in Sarawagi and Cohen’s work (Sarawagi and Cohen, 2004). When finding the immediate children of a constituent span, the linear-chain Markov structures are assumed over the sequence of candidate immediate constituents. In the implementation, we treat it as a segmentation problem, where each immediate child span can be seen as a segment, which has the similar setting with the previous work (Sarawagi and Cohen, 2004). Compared with the traditional “B-I-O” tagging schema in segmentation, which assigns a label to each token, the emission feature ρ is defined on the whole segment of several tokens in the proposed model, which is non-Markovian. Markov property exists in adjacent segments from the transition feature ψ. This shows the semi-Markov property.\nConnections with CRF. Traditional CRF models define a conditional probability over a probabilistic graph, and utilize the maximum likelihood estimation as the optimization objective. The proposed model shares the same conditional probability definition from the explanation view, but utilizes a margin-based loss in order to save the computational memory."
    }, {
      "heading" : "4 Algorithms",
      "text" : ""
    }, {
      "heading" : "4.1 The Challenge",
      "text" : "The core for the optimization is to find the tree with the maximum potential score. The previous CYK algorithm utilizes dynamic programming to find the maximum score, in a bottom-up manner. In order to calculate the maximum score of a given span, all the divisions should be enumerated. As shown in Fig. 6 (left), in the binary tree case, the number of the divisions is equal to L − 1, where L is the span length. Besides, the span length should be enumerated from 1 to n, and for each span length L there is L − n + 1 spans. Therefore the total time complexity of previous CYK is O(n3). In our case, a span can have more than two immediate\nchildren. Therefore, all the segmentation sequences should be enumerated, which obviously enlarges the search space. In Fig. 6 (right), for a span with the length equal to 4, the number of sequences to be considered increases from 3 to 7. This difference is the key issue to be solved in this section.\n4.2 Straight-Forward Algorithm (O(n5)) Let (i, j) be a representative span (i < j). We need to find its immediate children sequence with the maximum potential score. Dynamic programming is employed to accumulate the maximum potential score from the left to the right. Let α(i, j′, d, l) be an accumulated variable in the dynamic programming, which accumulates potential scores from j′ = i + 1 to j′ = j. j′ denotes the current accumulated position. d (i < d < j′) means that the last immediate child for span (i, j′) is the span (d, j′). l refers to the label of (d, j′). The meaning of α(i, j′, d, l) is the maximum accumulated score chosen from all the immediate children sequences of span (i, j′) whose last immediate child is (d, j′) with the label l. We also include the case of d = i, which refers to the maximum accumulated score of the span (i, j′)’s children and the span (i, j′) itself with l as its label.\nα(i, i+ 1, i, l) = ρ(i, i+ 1, l)\nα(i, j′, d, l) = max i≤q<d,l′∈Y\n[ α(i, d, q, l′)+\n+ψ(q, d, j′, l′, l) + α(d, j′, d, l) ]\nα(i, j′, i, l) = ρ(i, j′, l) + max i<k<j′,l′∈Y α(i, j′, k, l′)\nIn semi-Markov model, the above iterative calculation equations hold for the dynamic programming. The first equation is the initial state when j′ = i+ 1, and the second and third equations are\nthe iterative functions when (i < d < j′, j′ > i+1) and (d = i, j′ > i+ 1), respectively. An example of the dynamic programming is shown in Fig. 7.\nIn the iterative calculation of the above dynamic programming, we need to enumerate q, d, j′, i, j, each of which has the complexity of O(n). The total time complexity of the straight-forward method is O(n5) ∗O(|Y|2). To simplify the complexity of |Y|2, in calculating ψ(q, d, j′, l′, l), we manually group the labels in Y into clusters, according to the meaning of the constituent label, which reduces the complexity of |Y|2. Consequently, the main complexity comes from the O(n5) part.\n4.3 The Proposed Algorithm (O(n) ∗Op(n3))\nIn this section, we introduce how to reduce the above complexity of O(n5) to O(n) ∗ Op(n3). Op(n3) means all the O(n3) calculations can be batchfied. The hard complexity, which cannot be computed in parallel, is O(n).\nThe overall procedure for designing the algorithm is shown in Fig. 8. It includes four steps for reducing or batchifying the time complexity. In the first step, the complexity is reduced from O(n5) to O(n4) by sharing the α values in a set of spans. As shown in Fig. 8 (a), in the span of (0, 5), we need to calculate α(0, j, d, l) by enumerating j from 1 to 5. But the value α(0, 4, d, l) has been calculated in the span of (0, 4). Iteratively, all the values of α(0, j, d, l)(0 < j < 5) have been calculated in previous spans starting from 0. This means a set of spans that have the same start position can share the α values. If we enumerate the span length in the ascending order, in span (i, j), only the jth position’s value α(i, j, d, l) needs to be calculated, instead of enumerating the position j′ from i + 1 to j, which reduces O(n) of the time complexity. In the second step, the complexity is batchified\nAlgorithm 1 Algorithm for recursive semi-Markov model. Input: sentence x (length N ), model parameters θ. Outputs: the constituent tree y∗ with the maximum potential score s(y∗|x; θ).\n1: for all spans (i, j) do 2: calculate ρ(i, j, l|x; θ). 3: end for 4: for all sibling span pairs (i, j) and (j, k) do 5: calculate ψ(i, j, k, l1, l2|x; θ). 6: end for 7: for span length T from 1 to N do 8: calculate α(i, i+ T, d, l). 0 ≤ i ≤ N − T , i ≤ d < i+ T 9: end for\n10: s(y∗|x; θ) = maxd,l α(0, N, d, l). 11: trace back the tree y∗.\nfrom O(n4) to O(n3) ∗ Op(n), by computing the spans of the same length in parallel, as shown in Fig. 8 (b). In the third step, the complexity is batchified from O(n3) ∗ Op(n) to O(n2) ∗ Op(n2), by computing different ds in α(i, j, d, l), i < d < j in parallel, as shown in Fig. 8 (c). In the fourth step, the complexity is batchified from O(n2) ∗Op(n2) to O(n) ∗Op(n3), by computing α(i, j, d, l) when enumerating the second last immediate child with i < q < d in parallel (To calculate the dynamic programming state at a new position given the last child, we need to enumerate previous states with different second last children, in order to calculate ψ), as shown in Fig. 8 (d).\nThe details of the proposed algorithm are shown in Alg. 1. The calculation of ρ(i, j, l|x; θ) and ψ(i, j, k, l1, l2|x; θ) can be easily computed in parallel, with the complexity Op(n2) and Op(n3), respectively. The complexity of calculating α is O(n) ∗Op(n3). Therefore, the total time complexity of the proposed algorithm is O(n) ∗Op(n3)."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Experimental Setup",
      "text" : "We evaluate the proposed framework in both English and Chinese, on the datasets of PTB (WSJ sections (Marcus et al., 1993)) and CTB 5.1 (Xue et al., 2005), respectively. For Chinese, we evaluate both the single task of constituent parsing and the joint task with word segmentation and POS tagging. We follow the standard split of the datasets (Kitaev\net al., 2019). In the single task for Chinese, some previous work utilize the Stanford tagger (Toutanova et al., 2003) to generate the POS tags as input, which leads to a fixed error propagation. In this paper, POS tags are removed and not used as input features in both training and testing in CTB 5.1, following the previous work in (Zhang et al., 2020). Standard precision, recall and F1-measure are employed as evaluation metrics, where the EVALB1 tool is employed in the single task. The hyperparameters in the implementation are shown in Table. 1. Most of them are set following the Berkeley parser (Kitaev and Klein, 2018). When choosing the pre-train models (Wolf et al., 2020), “bert-largecased” is utilized for English with a single RTX 3090, “bert-base-chinese” is utilized for Chinese with a single RTX 1080TI."
    }, {
      "heading" : "5.2 Performances",
      "text" : "The overall performances of the proposed framework in the single task of constituent parsing on\n1https://nlp.cs.nyu.edu/evalb\nthe test set are shown in Table. 2 and Table. 3. The baselines in the first block are mainly based on basic word embeddings, and the baselines in the second block are based on BERT (Wolf et al., 2020). It can be observed that the F1-measures of proposed framework are 95.92% in PTB and 92.50% in CTB 5.1, which outperform the previous state-of-the-art methods. Our implementation for the proposed framework is based on the Berkeley parser (Kitaev et al., 2019). Therefore, many settings are similar with it for fair comparisons, such as learning schedule and feature normalization. Our method outperforms it by 0.33 points in PTB and 0.5 points in CTB 5.1 (0.25% of the 0.75% improvement is due to not utilizing automatically predicted POS tags in CTB 5.1), which demonstrates the advantage of modeling the sibling dependency features.\nThe overall performances in the joint task on the test set of CTB 5.1 are shown in Table. 4. As there are rare reports of performances with the BERT embedding, we have implemented a minor revision to the previous Berkeley parser (Kitaev et al., 2019)"
    }, {
      "heading" : "1 (24.13%) 2 (51.10%) 3 (15.82%) 4-5 (7.38%) >5 (1.57%)",
      "text" : "Immediate Children Number\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nF1 -m\nea su\nre\n+0.2%\n+0.3%\n+0.4%\n+0.3% +1.1%\nBaseline Recursive Semi-Markov Model\n(a) PTB"
    }, {
      "heading" : "1 (39.93%) 2 (42.41%) 3 (12.86%) 4-5 (3.88%) >5 (0.93%)",
      "text" : "Immediate Children Number\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nF1 -m\nea su\nre\n+0.2% +0.9%\n+2.3%\n+3.5% +6.8%\nBaseline Recursive Semi- Markov Model\n(b) CTB\nFigure 9: F1-measure values on constituent nodes with different numbers of children. The “baseline” refers to our running results using the Berkeley parser (Kitaev et al., 2019). The percentage values at bottom refers to the distribution of different nodes.\nto make it adaptable to the joint task, which serves as the baseline method in the second block. It can observed that the F1-measures of proposed recursive semi-Markov model outperforms the competitive baseline by 0.46 points in F1, and consistently outperforms previous method in all tasks of word segmentation, POS tagging, and parsing.\nThe main improvement of the proposed framework comes from modeling the sibling dependencies of an n-ary node’s children sequence. It has special advantage for predicting nodes with more children. We have divided all the constituent nodes into bins by how many children they have. Figure 9 shows the comparisons. The improvement is more obvious when the number of children becomes larger. For nodes with more than 2 immediate children, our framework outperforms the baseline by 0.3 to 1.1 points in PTB and 2.3 to 6.8 points in CTB 5.1."
    }, {
      "heading" : "5.3 Speed Analysis",
      "text" : "The average processing speed in PTB test set is 26 sentences per second with a single RTX 3090, and the one in CTB 5.1 test set is 11 sentences per second with a single RTX 1080TI (or 20 sentences per second with single RTX 3090). Table 5 shows the speed comparisons of the proposed model with previous methods in the PTB dataset. Figure 10 shows the detailed processing speed of the proposed model in CTB 5.1 dataset. Figure 10 (left) shows the processing speeds with different sentence lengths; and Fig. 10 (right) shows the processing time of some special long sentences. For the longest sentence in the CTB 5.1, which contains 240 words, it takes around 6 seconds. Fig-\nure 11 shows the processing speed ratio between the Berkeley parser (Kitaev et al., 2019) and our model. It demonstrates that ratio does not grow linearly, by making full use of parallel computations. We know that the speed is still slower than some previous methods. On one hand, our proposed algorithm has already reduced the complexity by parallel computations. On the other hand, by considering its advantage in modeling nodes with multiple children, which especially happens a lot in the joint parsing task with segmentation and POS tagging in Chinese, the processing speed is still acceptable in many offline cases."
    }, {
      "heading" : "5.4 A Further Comparison on Fine-Grained Noun Phrase Structures",
      "text" : "Within the nodes having more than two children, some of them are noun phrases, whose internal hierarchical structures have been annotated in the PTB dataset by previous work (Vadas and Curran, 2007, 2011). We have also conducted experiments with the Berkeley parser (Kitaev et al., 2019) on this refined PTB data. In the test process, we convert the generated fine-grained trees back to the original trees for comparisons. The F1 in the refined PTB test dataset by the Berkeley parser (Kitaev et al., 2019) is 95.62%, which is also outperformed by the proposed method in Table 2."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, a recursive semi-Markov model is proposed for n-ary constituent tree parsing, with the advantage of modeling the sibling relations within n-ary node. Experimental verifications on PTB and CTB 5.1 demonstrate that the proposed framework outperforms previous work in the single parsing task of both datasets and the joint task in CTB 5.1. For constituent nodes with more than 2 children, the F1 can be improved by 0.3− 1.1 points in PTB and 2.3− 6.8 points in CTB 5.1."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is supported by the grants from the National Natural Science Foundation of China (No. 61672100), and Beijing Natural Science Foundation (No. 4202069). It is partly supported by National Key R&D Program of China (No. 2018YFC0830705). We thank the anonymous reviewers for their comments and constructive suggestions to improve this paper."
    } ],
    "references" : [ {
      "title" : "Parsing as language modeling",
      "author" : [ "Do Kook Choe", "Eugene Charniak." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2331–2336, Austin, Texas. Association for Computational Linguistics.",
      "citeRegEx" : "Choe and Charniak.,? 2016",
      "shortCiteRegEx" : "Choe and Charniak.",
      "year" : 2016
    }, {
      "title" : "Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles",
      "author" : [ "James Cross", "Liang Huang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1–11, Austin,",
      "citeRegEx" : "Cross and Huang.,? 2016",
      "shortCiteRegEx" : "Cross and Huang.",
      "year" : 2016
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural CRF parsing",
      "author" : [ "Greg Durrett", "Dan Klein." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Paper-",
      "citeRegEx" : "Durrett and Klein.,? 2015",
      "shortCiteRegEx" : "Durrett and Klein.",
      "year" : 2015
    }, {
      "title" : "Recurrent neural network grammars",
      "author" : [ "Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Dyer et al\\.,? 2016",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2016
    }, {
      "title" : "An efficient context-free parsing algorithm",
      "author" : [ "Jay Earley." ],
      "venue" : "Commun. ACM, 13(2):94–102.",
      "citeRegEx" : "Earley.,? 1970",
      "shortCiteRegEx" : "Earley.",
      "year" : 1970
    }, {
      "title" : "Faster shift-reduce constituent parsing with a non-binary, bottom-up strategy",
      "author" : [ "Daniel Fernández-González", "Carlos GómezRodrı́guez" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Fernández.González and GómezRodrı́guez.,? \\Q2019\\E",
      "shortCiteRegEx" : "Fernández.González and GómezRodrı́guez.",
      "year" : 2019
    }, {
      "title" : "Policy gradient as a proxy for dynamic oracles in constituency parsing",
      "author" : [ "Daniel Fried", "Dan Klein." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 469–476, Melbourne, Aus-",
      "citeRegEx" : "Fried and Klein.,? 2018",
      "shortCiteRegEx" : "Fried and Klein.",
      "year" : 2018
    }, {
      "title" : "Improving neural parsing by disentangling model combination and reranking effects",
      "author" : [ "Daniel Fried", "Mitchell Stern", "Dan Klein." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages",
      "citeRegEx" : "Fried et al\\.,? 2017",
      "shortCiteRegEx" : "Fried et al\\.",
      "year" : 2017
    }, {
      "title" : "What’s going on in neural constituency parsers",
      "author" : [ "David Gaddy", "Mitchell Stern", "Dan Klein" ],
      "venue" : null,
      "citeRegEx" : "Gaddy et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Gaddy et al\\.",
      "year" : 2018
    }, {
      "title" : "Constituent parsing as sequence labeling",
      "author" : [ "Carlos Gómez-Rodrı́guez", "David Vilares" ],
      "venue" : "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Gómez.Rodrı́guez and Vilares.,? \\Q2018\\E",
      "shortCiteRegEx" : "Gómez.Rodrı́guez and Vilares.",
      "year" : 2018
    }, {
      "title" : "Less grammar, more features",
      "author" : [ "David Hall", "Greg Durrett", "Dan Klein." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 228– 237, Baltimore, Maryland. Association for Compu-",
      "citeRegEx" : "Hall et al\\.,? 2014",
      "shortCiteRegEx" : "Hall et al\\.",
      "year" : 2014
    }, {
      "title" : "Linear-time constituency parsing with RNNs and dynamic programming",
      "author" : [ "Juneki Hong", "Liang Huang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 477–483, Melbourne,",
      "citeRegEx" : "Hong and Huang.,? 2018",
      "shortCiteRegEx" : "Hong and Huang.",
      "year" : 2018
    }, {
      "title" : "Automatic adaptation of annotation standards: Chinese word segmentation and POS tagging – a case study",
      "author" : [ "Wenbin Jiang", "Liang Huang", "Qun Liu." ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International",
      "citeRegEx" : "Jiang et al\\.,? 2009",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2009
    }, {
      "title" : "Extending a parser to distant domains using a few dozen partially annotated examples",
      "author" : [ "Vidur Joshi", "Matthew Peters", "Mark Hopkins." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Paper-",
      "citeRegEx" : "Joshi et al\\.,? 2018",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2018
    }, {
      "title" : "An efficient recognition and syntax-analysis algorithm for context-free languages",
      "author" : [ "Tadao Kasami." ],
      "venue" : "Coordinated Science Laboratory Report no. R-257.",
      "citeRegEx" : "Kasami.,? 1966",
      "shortCiteRegEx" : "Kasami.",
      "year" : 1966
    }, {
      "title" : "Multilingual constituency parsing with self-attention and pre-training",
      "author" : [ "Nikita Kitaev", "Steven Cao", "Dan Klein." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3499–3505, Florence, Italy. Associa-",
      "citeRegEx" : "Kitaev et al\\.,? 2019",
      "shortCiteRegEx" : "Kitaev et al\\.",
      "year" : 2019
    }, {
      "title" : "Constituency parsing with a self-attentive encoder",
      "author" : [ "Nikita Kitaev", "Dan Klein." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2676–2686, Melbourne, Australia. Associ-",
      "citeRegEx" : "Kitaev and Klein.,? 2018",
      "shortCiteRegEx" : "Kitaev and Klein.",
      "year" : 2018
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John Lafferty", "Andrew McCallum", "Fernando Pereira." ],
      "venue" : "Proceedings of the Eighteenth International Conference on Machine Learning, pages",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "In-order transition-based constituent parsing",
      "author" : [ "Jiangming Liu", "Yue Zhang." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:413–424.",
      "citeRegEx" : "Liu and Zhang.,? 2017a",
      "shortCiteRegEx" : "Liu and Zhang.",
      "year" : 2017
    }, {
      "title" : "Shift-reduce constituent parsing with neural lookahead features",
      "author" : [ "Jiangming Liu", "Yue Zhang." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:45–58.",
      "citeRegEx" : "Liu and Zhang.,? 2017b",
      "shortCiteRegEx" : "Liu and Zhang.",
      "year" : 2017
    }, {
      "title" : "Improving sequence-to-sequence constituency parsing",
      "author" : [ "Lemao Liu", "Muhua Zhu", "Shuming Shi." ],
      "venue" : "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, pages 4873–4880. AAAI Press.",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Building a large annotated corpus of English: The Penn Treebank",
      "author" : [ "Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz." ],
      "venue" : "Computational Linguistics, 19(2):313–330.",
      "citeRegEx" : "Marcus et al\\.,? 1993",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1993
    }, {
      "title" : "Joint Chinese word segmentation, POS tagging and parsing",
      "author" : [ "Xian Qian", "Yang Liu." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 501–",
      "citeRegEx" : "Qian and Liu.,? 2012",
      "shortCiteRegEx" : "Qian and Liu.",
      "year" : 2012
    }, {
      "title" : "Semimarkov conditional random fields for information extraction",
      "author" : [ "Sunita Sarawagi", "W. William Cohen." ],
      "venue" : "Neural Information Processing Systems, pages 1185–1192.",
      "citeRegEx" : "Sarawagi and Cohen.,? 2004",
      "shortCiteRegEx" : "Sarawagi and Cohen.",
      "year" : 2004
    }, {
      "title" : "Straight to the tree: Constituency parsing with neural syntactic distance",
      "author" : [ "Yikang Shen", "Zhouhan Lin", "Athul Paul Jacob", "Alessandro Sordoni", "Aaron Courville", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computation-",
      "citeRegEx" : "Shen et al\\.,? 2018",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "A minimal span-based neural constituency parser",
      "author" : [ "Mitchell Stern", "Jacob Andreas", "Dan Klein." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 818–827, Vancouver, Canada.",
      "citeRegEx" : "Stern et al\\.,? 2017",
      "shortCiteRegEx" : "Stern et al\\.",
      "year" : 2017
    }, {
      "title" : "Two local models for neural constituent parsing",
      "author" : [ "Zhiyang Teng", "Yue Zhang." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 119–132, Santa Fe, New Mexico, USA. Association for Computational Linguistics.",
      "citeRegEx" : "Teng and Zhang.,? 2018",
      "shortCiteRegEx" : "Teng and Zhang.",
      "year" : 2018
    }, {
      "title" : "Feature-rich part-ofspeech tagging with a cyclic dependency network",
      "author" : [ "Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer" ],
      "venue" : null,
      "citeRegEx" : "Toutanova et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Toutanova et al\\.",
      "year" : 2003
    }, {
      "title" : "Adding noun phrase structure to the Penn Treebank",
      "author" : [ "David Vadas", "James Curran." ],
      "venue" : "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 240–247, Prague, Czech Republic. Association for Computational Lin-",
      "citeRegEx" : "Vadas and Curran.,? 2007",
      "shortCiteRegEx" : "Vadas and Curran.",
      "year" : 2007
    }, {
      "title" : "Parsing noun phrases in the Penn Treebank",
      "author" : [ "David Vadas", "James R. Curran." ],
      "venue" : "Computational Linguistics, 37(4):753–809.",
      "citeRegEx" : "Vadas and Curran.,? 2011",
      "shortCiteRegEx" : "Vadas and Curran.",
      "year" : 2011
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "undefinedukasz Kaiser", "Illia Polosukhin" ],
      "venue" : "In Proceedings of the 31st International Conference on Neural Information Process-",
      "citeRegEx" : "Vaswani et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Better, faster, stronger sequence tagging constituent parsers",
      "author" : [ "David Vilares", "Mostafa Abdou", "Anders Søgaard." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Vilares et al\\.,? 2019",
      "shortCiteRegEx" : "Vilares et al\\.",
      "year" : 2019
    }, {
      "title" : "A fast, accurate deterministic parser for Chinese",
      "author" : [ "Mengqiu Wang", "Kenji Sagae", "Teruko Mitamura." ],
      "venue" : "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Wang et al\\.,? 2006",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2006
    }, {
      "title" : "Feature optimization for constituent parsing via neural networks",
      "author" : [ "Zhiguo Wang", "Haitao Mi", "Nianwen Xue." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference",
      "citeRegEx" : "Wang et al\\.,? 2015",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "A lattice-based framework for joint Chinese word segmentation, POS tagging and parsing",
      "author" : [ "Zhiguo Wang", "Chengqing Zong", "Nianwen Xue." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2:",
      "citeRegEx" : "Wang et al\\.,? 2013",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2013
    }, {
      "title" : "Transitionbased neural constituent parsing",
      "author" : [ "Taro Watanabe", "Eiichiro Sumita." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing",
      "citeRegEx" : "Watanabe and Sumita.,? 2015",
      "shortCiteRegEx" : "Watanabe and Sumita.",
      "year" : 2015
    }, {
      "title" : "A spanbased linearization for constituent trees",
      "author" : [ "Yang Wei", "Yuanbin Wu", "Man Lan." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3267– 3277, Online. Association for Computational Lin-",
      "citeRegEx" : "Wei et al\\.,? 2020",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "The penn chinese treebank: Phrase structure annotation of a large corpus",
      "author" : [ "Naiwen Xue", "Fei Xia", "Fu-dong Chiou", "Marta Palmer." ],
      "venue" : "Nat. Lang. Eng., 11(2):207238.",
      "citeRegEx" : "Xue et al\\.,? 2005",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2005
    }, {
      "title" : "Chinese parsing exploiting characters",
      "author" : [ "Meishan Zhang", "Yue Zhang", "Wanxiang Che", "Ting Liu." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 125–134, Sofia, Bulgaria. As-",
      "citeRegEx" : "Zhang et al\\.,? 2013",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2013
    }, {
      "title" : "Fast and accurate neural crf constituency parsing",
      "author" : [ "Yu Zhang", "Houquan Zhou", "Zhenghua Li." ],
      "venue" : "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pages 4046–4053. International Joint Conferences",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Characterbased parsing with convolutional neural network",
      "author" : [ "Xiaoqing Zheng", "Haoyuan Peng", "Yi Chen", "Pengjing Zhang", "Wenqiang Zhang." ],
      "venue" : "Proceedings of the 24th International Conference on Artificial Intelligence, IJCAI’15, pages 1054–1060.",
      "citeRegEx" : "Zheng et al\\.,? 2015",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2015
    }, {
      "title" : "Head-Driven Phrase Structure Grammar parsing on Penn Treebank",
      "author" : [ "Junru Zhou", "Hai Zhao." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2396– 2408, Florence, Italy. Association for Computation-",
      "citeRegEx" : "Zhou and Zhao.,? 2019",
      "shortCiteRegEx" : "Zhou and Zhao.",
      "year" : 2019
    }, {
      "title" : "Fast and accurate shiftreduce constituent parsing",
      "author" : [ "Muhua Zhu", "Yue Zhang", "Wenliang Chen", "Min Zhang", "Jingbo Zhu." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
      "citeRegEx" : "Zhu et al\\.,? 2013",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Specifically, we target at improving graph-based models for n-ary tree parsing, which obtain better performances in recent work (Kitaev et al., 2019; Zhang et al., 2020; Wei et al., 2020) from the two streams of well-developed parsing methods, graphbased and transition-based.",
      "startOffset" : 128,
      "endOffset" : 187
    }, {
      "referenceID" : 41,
      "context" : "Specifically, we target at improving graph-based models for n-ary tree parsing, which obtain better performances in recent work (Kitaev et al., 2019; Zhang et al., 2020; Wei et al., 2020) from the two streams of well-developed parsing methods, graphbased and transition-based.",
      "startOffset" : 128,
      "endOffset" : 187
    }, {
      "referenceID" : 37,
      "context" : "Specifically, we target at improving graph-based models for n-ary tree parsing, which obtain better performances in recent work (Kitaev et al., 2019; Zhang et al., 2020; Wei et al., 2020) from the two streams of well-developed parsing methods, graphbased and transition-based.",
      "startOffset" : 128,
      "endOffset" : 187
    }, {
      "referenceID" : 17,
      "context" : "For convenience, the potential scores of such hidden nodes are manually set to zero, to ensure that the two manners are equivalent when calculating the likelihood (Kitaev and Klein, 2018).",
      "startOffset" : 163,
      "endOffset" : 187
    }, {
      "referenceID" : 15,
      "context" : "For example, in the previous CYK (Kasami, 1966) algorithm for binary trees, the absolute time complexity is O(n3), where n is the sentence length.",
      "startOffset" : 33,
      "endOffset" : 47
    }, {
      "referenceID" : 17,
      "context" : "com/NPNET-research/Recursive-Semi-Markov-Model, which is developed on the base of the open-source Berkekey parser (Kitaev and Klein, 2018; Kitaev et al., 2019).",
      "startOffset" : 114,
      "endOffset" : 159
    }, {
      "referenceID" : 16,
      "context" : "com/NPNET-research/Recursive-Semi-Markov-Model, which is developed on the base of the open-source Berkekey parser (Kitaev and Klein, 2018; Kitaev et al., 2019).",
      "startOffset" : 114,
      "endOffset" : 159
    }, {
      "referenceID" : 5,
      "context" : "A representative of classical methods for n-ary tree parsing is the Earley algorithm (Earley, 1970).",
      "startOffset" : 85,
      "endOffset" : 99
    }, {
      "referenceID" : 11,
      "context" : "Therefore, recent studies have paid more attention to utilizing “less grammar” (Hall et al., 2014), which is implemented in CYK/shift-reduce algorithms (Durrett and Klein, 2015; Liu and Zhang, 2017b; Stern et al.",
      "startOffset" : 79,
      "endOffset" : 98
    }, {
      "referenceID" : 3,
      "context" : ", 2014), which is implemented in CYK/shift-reduce algorithms (Durrett and Klein, 2015; Liu and Zhang, 2017b; Stern et al., 2017; Teng and Zhang, 2018) instead of the Earley algorithm.",
      "startOffset" : 61,
      "endOffset" : 150
    }, {
      "referenceID" : 20,
      "context" : ", 2014), which is implemented in CYK/shift-reduce algorithms (Durrett and Klein, 2015; Liu and Zhang, 2017b; Stern et al., 2017; Teng and Zhang, 2018) instead of the Earley algorithm.",
      "startOffset" : 61,
      "endOffset" : 150
    }, {
      "referenceID" : 26,
      "context" : ", 2014), which is implemented in CYK/shift-reduce algorithms (Durrett and Klein, 2015; Liu and Zhang, 2017b; Stern et al., 2017; Teng and Zhang, 2018) instead of the Earley algorithm.",
      "startOffset" : 61,
      "endOffset" : 150
    }, {
      "referenceID" : 27,
      "context" : ", 2014), which is implemented in CYK/shift-reduce algorithms (Durrett and Klein, 2015; Liu and Zhang, 2017b; Stern et al., 2017; Teng and Zhang, 2018) instead of the Earley algorithm.",
      "startOffset" : 61,
      "endOffset" : 150
    }, {
      "referenceID" : 26,
      "context" : "A representative of graph-based n-ary tree parsing model is the Berkeley parser (Stern et al., 2017; Kitaev and Klein, 2018; Kitaev et al., 2019), which employs hidden nodes to deal with n-ary nodes.",
      "startOffset" : 80,
      "endOffset" : 145
    }, {
      "referenceID" : 17,
      "context" : "A representative of graph-based n-ary tree parsing model is the Berkeley parser (Stern et al., 2017; Kitaev and Klein, 2018; Kitaev et al., 2019), which employs hidden nodes to deal with n-ary nodes.",
      "startOffset" : 80,
      "endOffset" : 145
    }, {
      "referenceID" : 16,
      "context" : "A representative of graph-based n-ary tree parsing model is the Berkeley parser (Stern et al., 2017; Kitaev and Klein, 2018; Kitaev et al., 2019), which employs hidden nodes to deal with n-ary nodes.",
      "startOffset" : 80,
      "endOffset" : 145
    }, {
      "referenceID" : 1,
      "context" : "The models can be divided into post-order (Cross and Huang, 2016; Fernández-González and GómezRodrı́guez, 2019), pre-order (Dyer et al.",
      "startOffset" : 42,
      "endOffset" : 111
    }, {
      "referenceID" : 6,
      "context" : "The models can be divided into post-order (Cross and Huang, 2016; Fernández-González and GómezRodrı́guez, 2019), pre-order (Dyer et al.",
      "startOffset" : 42,
      "endOffset" : 111
    }, {
      "referenceID" : 4,
      "context" : "The models can be divided into post-order (Cross and Huang, 2016; Fernández-González and GómezRodrı́guez, 2019), pre-order (Dyer et al., 2016), and in-order (Liu and Zhang, 2017a), according to the traversal manner of the action sequence.",
      "startOffset" : 123,
      "endOffset" : 142
    }, {
      "referenceID" : 19,
      "context" : ", 2016), and in-order (Liu and Zhang, 2017a), according to the traversal manner of the action sequence.",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "Post-order models require to deciding the number of reduced nodes for n-ary nodes (Fernández-González and Gómez-Rodrı́guez, 2019), or to introducing hidden nodes with dummy label (Cross and Huang, 2016).",
      "startOffset" : 179,
      "endOffset" : 202
    }, {
      "referenceID" : 16,
      "context" : "In recent studies, graph-based models have been demonstrated to perform better than transition-based models (Kitaev et al., 2019; Zhang et al., 2020; Wei et al., 2020).",
      "startOffset" : 108,
      "endOffset" : 167
    }, {
      "referenceID" : 41,
      "context" : "In recent studies, graph-based models have been demonstrated to perform better than transition-based models (Kitaev et al., 2019; Zhang et al., 2020; Wei et al., 2020).",
      "startOffset" : 108,
      "endOffset" : 167
    }, {
      "referenceID" : 37,
      "context" : "In recent studies, graph-based models have been demonstrated to perform better than transition-based models (Kitaev et al., 2019; Zhang et al., 2020; Wei et al., 2020).",
      "startOffset" : 108,
      "endOffset" : 167
    }, {
      "referenceID" : 17,
      "context" : "Following previous work (Kitaev and Klein, 2018; Zhang et al., 2020), the nodes with unary grammars are collapsed, and its label is replaced by the joint label of the collapsed nodes.",
      "startOffset" : 24,
      "endOffset" : 68
    }, {
      "referenceID" : 41,
      "context" : "Following previous work (Kitaev and Klein, 2018; Zhang et al., 2020), the nodes with unary grammars are collapsed, and its label is replaced by the joint label of the collapsed nodes.",
      "startOffset" : 24,
      "endOffset" : 68
    }, {
      "referenceID" : 18,
      "context" : "The conditional probability p(y|x) can be defined on the probabilistic graph, under the framework of conditional random fields (CRF) (Lafferty et al., 2001), as shown in the following equations.",
      "startOffset" : 133,
      "endOffset" : 156
    }, {
      "referenceID" : 17,
      "context" : "Given an input sentence x, we follow the neural network architecture of the Berkeley parser (Kitaev and Klein, 2018) with some minor revisions, to calculate the two kinds of potential scores, ρ(i, j, l), and ψ(i, j, k, l1, l2), as shown in Fig.",
      "startOffset" : 92,
      "endOffset" : 116
    }, {
      "referenceID" : 2,
      "context" : "In the embedding layer, the BERT (Devlin et al., 2019; Wolf et al., 2020) is selected to generate pretrained vectors, denoted by ei, 0 ≤ i < n.",
      "startOffset" : 33,
      "endOffset" : 73
    }, {
      "referenceID" : 31,
      "context" : "In the encoding layer, the Transformer (Vaswani et al., 2017) is selected for extracting the context features, denoted by hi, with odd dimensions − → h i and the even dimensions ←− h i.",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 17,
      "context" : "Therefore, we employ the max-margin loss as the training objective to learn the parameters of the proposed framework, following the Berkeley parser (Kitaev and Klein, 2018).",
      "startOffset" : 148,
      "endOffset" : 172
    }, {
      "referenceID" : 24,
      "context" : "mentioned in Sarawagi and Cohen’s work (Sarawagi and Cohen, 2004).",
      "startOffset" : 39,
      "endOffset" : 65
    }, {
      "referenceID" : 24,
      "context" : "In the implementation, we treat it as a segmentation problem, where each immediate child span can be seen as a segment, which has the similar setting with the previous work (Sarawagi and Cohen, 2004).",
      "startOffset" : 173,
      "endOffset" : 199
    }, {
      "referenceID" : 22,
      "context" : "We evaluate the proposed framework in both English and Chinese, on the datasets of PTB (WSJ sections (Marcus et al., 1993)) and CTB 5.",
      "startOffset" : 101,
      "endOffset" : 122
    }, {
      "referenceID" : 28,
      "context" : "In the single task for Chinese, some previous work utilize the Stanford tagger (Toutanova et al., 2003) to generate the POS tags as input, which leads to a fixed error propagation.",
      "startOffset" : 79,
      "endOffset" : 103
    }, {
      "referenceID" : 41,
      "context" : "1, following the previous work in (Zhang et al., 2020).",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 17,
      "context" : "Most of them are set following the Berkeley parser (Kitaev and Klein, 2018).",
      "startOffset" : 51,
      "endOffset" : 75
    }, {
      "referenceID" : 16,
      "context" : "The “baseline” row shows our running results using a revision of the Berkeley parser (Kitaev et al., 2019).",
      "startOffset" : 85,
      "endOffset" : 106
    }, {
      "referenceID" : 16,
      "context" : "Our implementation for the proposed framework is based on the Berkeley parser (Kitaev et al., 2019).",
      "startOffset" : 78,
      "endOffset" : 99
    }, {
      "referenceID" : 16,
      "context" : "As there are rare reports of performances with the BERT embedding, we have implemented a minor revision to the previous Berkeley parser (Kitaev et al., 2019) 1 (24.",
      "startOffset" : 136,
      "endOffset" : 157
    }, {
      "referenceID" : 16,
      "context" : "The “baseline” refers to our running results using the Berkeley parser (Kitaev et al., 2019).",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 16,
      "context" : "Figure 11 shows the processing speed ratio between the Berkeley parser (Kitaev et al., 2019) and our model.",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 16,
      "context" : "We have also conducted experiments with the Berkeley parser (Kitaev et al., 2019) on this refined PTB data.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 16,
      "context" : "The F1 in the refined PTB test dataset by the Berkeley parser (Kitaev et al., 2019) is 95.",
      "startOffset" : 62,
      "endOffset" : 83
    } ],
    "year" : 2021,
    "abstractText" : "In this paper, we study the task of graph-based constituent parsing in the setting that binarization is not conducted as a pre-processing step, where a constituent tree may consist of nodes with more than two children. Previous graphbased methods on this setting typically generate hidden nodes with the dummy label inside the n-ary nodes, in order to transform the tree into a binary tree for prediction. The limitation is that the hidden nodes break the sibling relations of the n-ary node’s children. Consequently, the dependencies of such sibling constituents might not be accurately modeled and is being ignored. To solve this limitation, we propose a novel graph-based framework, which is called “recursive semi-Markov model”. The main idea is to utilize 1-order semi-Markov model to predict the immediate children sequence of a constituent candidate, which then recursively serves as a child candidate of its parent. In this manner, the dependencies of sibling constituents can be described by 1-order transition features, which solves the above limitation. Through experiments, the proposed framework obtains the F1 of 95.92% and 92.50% on the datasets of PTB and CTB 5.1 respectively. Specially, the recursive semi-Markov model shows advantages in modeling nodes with more than two children, whose average F1 can be improved by 0.3-1.1 points in PTB and 2.3-6.8 points in CTB 5.1.",
    "creator" : "LaTeX with hyperref package"
  }
}