{
  "name" : "2021.acl-long.410.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "LEXFIT: Lexical Fine-Tuning of Pretrained Language Models",
    "authors" : [ "Ivan Vulić", "Edoardo M. Ponti", "Anna Korhonen", "Goran Glavaš" ],
    "emails" : [ "iv250@cam.ac.uk", "alk23@cam.ac.uk", "edoardo-maria.ponti@mila.quebec", "goran@informatik.uni-mannheim.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5269–5283\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5269"
    }, {
      "heading" : "1 Introduction",
      "text" : "Probing large pretrained encoders like BERT (Devlin et al., 2019) revealed that they contain a wealth of lexical knowledge (Ethayarajh, 2019; Vulić et al., 2020). If type-level word vectors are extracted from BERT with appropriate strategies, they can even outperform traditional word embeddings (WEs) in some lexical tasks (Vulić et al., 2020; Bommasani et al., 2020; Chronis and Erk, 2020). However,\nboth static and contextualized WEs ultimately learn solely from the distributional word co-occurrence signal. This source of signal is known to lead to distortions in the induced representations by conflating meaning based on topical relatedness rather than authentic semantic similarity (Hill et al., 2015; Schwartz et al., 2015; Vulić et al., 2017). This also creates a ripple effect on downstream applications, where model performance may suffer (Faruqui, 2016; Mrkšić et al., 2017; Lauscher et al., 2020).\nOur work takes inspiration from the methods to correct these distortions and complement the distributional signal with structured information, which were originally devised for static WEs. In particular, the process known as semantic specialization (or retrofitting) injects information about lexical relations from databases like WordNet (Beckwith et al., 1991) or the Paraphrase Database (Ganitkevitch et al., 2013) into WEs. Thus, it accentuates relationships of pure semantic similarity in the re-\nfined representations (Faruqui et al., 2015; Mrkšić et al., 2017; Ponti et al., 2019, inter alia).\nOur goal is to create representations that take advantage of both 1) the expressivity and lexical knowledge already stored in pretrained language models (LMs) and 2) the precision of lexical finetuning. To this effect, we develop LEXFIT, a versatile lexical fine-tuning framework, illustrated in Figure 1, drawing a parallel with universal sentence encoders like SentenceBERT (Reimers and Gurevych, 2019).1 Our working hypothesis, extensively evaluated in this paper, is as follows: pretrained encoders store a wealth of lexical knowledge, but it is not straightforward to extract that knowledge. We can expose this knowledge by rewiring their parameters through lexical fine-tuning, and turn the LMs into universal (decontextualized) word encoders.\nCompared to prior attempts at injecting lexical knowledge into large LMs (Lauscher et al., 2020), our LEXFIT method is innovative as it is deployed post-hoc on top of already pretrained LMs, rather than requiring joint multi-task training. Moreover, LEXFIT is: 1) more efficient, as it does not incur the overhead of masked language modeling pretraining; and 2) more versatile, as it can be ported to any model independently from its architecture or original training objective. Finally, our results demonstrate the usefulness of LEXFIT: we report large gains over WEs extracted from vanilla LMs and over traditional WE models across 8 languages and 4 lexical tasks, even with very limited and noisy external lexical knowledge, validating the rewiring hypothesis. The code is available at: https://github.com/cambridgeltl/lexfit."
    }, {
      "heading" : "2 From Language Models to (Decontextualized) Word Encoders",
      "text" : "The motivation for this work largely stems from the recent work on probing and analyzing pretrained language models for various types of knowledge they might implicitly store (e.g., syntax, world knowledge) (Rogers et al., 2020). Here, we focus on their lexical semantic knowledge (Vulić et al., 2020; Liu et al., 2021), with an aim of extracting high-quality static word embeddings from the parameters of the input LMs. In what follows, we describe lexical fine-tuning via dual-encoder networks (§2.1), followed by the WE extraction pro-\n1These approaches are connected as they are both trained via contrastive learning on dual-encoder architectures, but they provide representations for a different granularity of meaning.\ncess from the fine-tuned layers of pretrained LMs (§2.2), see Figure 1."
    }, {
      "heading" : "2.1 LEXFIT: Methodology",
      "text" : "Our hypothesis is that the pretrained LMs can be turned into effective static decontextualized word encoders via additional inexpensive lexical finetuning (i.e., LEXFIT-ing) on lexical pairs from an external resource. In other words, they can be specialized to encode lexical knowledge useful for downstream tasks, e.g., lexical semantic similarity (Wieting et al., 2015; Mrkšić et al., 2017; Ponti et al., 2018). Let P = {(w, v, r)m}Mm=1 refer to the set of M external lexical constraints. Each item p ∈ P comprises a pair of words w and v, and denotes a semantic relation r that holds between them (e.g., synonymy, antonymy). Further, let Pr denote a subset of P where a particular relation r holds for each item, e.g., Psyn is a set of synonymy pairs. Finally, for each positive tuple (w, v, r), we can construct 2k negative “no-relation” examples by randomly pairing w with another word w¬,k′ , and pairing v with v¬,k′ , k′ = 1, . . . , k, ensuring that these negative pairs do not occur in P . We refer to the full set of negative pairs as NP . Lexical fine-tuning then leverages P and NP ; We propose to tune the underlying LMs (e.g., BERT, mBERT), using external lexical knowledge, via different loss functions, relying on dual-encoder networks with shared LM weights and mean pooling, as illustrated in Figure 1. We now briefly describe several loss functions, evaluated later in §4.\nClassification Loss. Similar to prior work on sentence-level text inputs (Reimers and Gurevych, 2019), for each input word pair (w, v) we concatenate their d-dimensional encodings w and v (obtained after passing them through BERT and after pooling, see Figure 1) with their element-wise difference |w − v|. The objective is then:\nL = softmax ( W (w ⊕ v ⊕ |w − v|) ) . (1)\n⊕ denotes concatenation, and W ∈ R3d×c is a trainable weight matrix of the softmax classifier, where c is the number of classification classes. We experiment with two variants of this objective, termed SOFTMAX henceforth: in the simpler binary variant, the goal is to distinguish between positive synonymy pairs (the subset Psyn) and the corresponding set of 2k × |Psyn| no-relation negative pairs. In the ternary variant (c = 3), the classifier must distinguish between synonyms (Psyn),\nantonyms (Pant), and no-relation negatives. The classifiers are optimized via standard cross-entropy.\nRanking Loss. The multiple negatives ranking loss (MNEG) is inspired by prior work on learning universal sentence encoders (Cer et al., 2018; Henderson et al., 2019, 2020); the aim of the loss, now adapted to word-level inputs, is to rank true synonymy pairs from Psyn over randomly paired words. The similarity between any two words w and v is quantified via the similarity function S operating on their encodings S(wi,wj). In this work we use the scaled cosine similarity following Henderson et al. (2019): S(wi,wj) = C ·cos(w1,w2), whereC is the scaling constant. Lexical fine-tuning with MNEG then proceeds in batches of B pairs (wi, vi), . . . , (wB, vB) from Psyn, with the MNEG loss for a single batch computed as follows:\nL = − B∑ i=1 S(wi,vi) + B∑ i=1 log B∑ j=1,j 6=i eS(wi,vj) (2)\nEffectively, for each batch Eq. (2) maximizes the similarity score of positive pairs (wi, vi), and minimizes the score ofB−1 random pairs. For simplicity, as negatives we use all pairings of wi with vj-s in the current batch where (wi, vj) 6∈ Psyn (Yang et al., 2018; Henderson et al., 2019).\nMulti-Similarity Loss. We also experiment with a recently proposed state-of-the-art multi-similarity loss of Wang et al. (2019), labeled MSIM. The aim is again to rank positive examples from Psyn above any corresponding no-relation 2k negatives from NP . Again using the scaled cosine similarity scores, the adapted MSIM loss per batch of B positive pairs (wi, vi) from Psyn is defined as follows:\nL = 1 B B∑ i=1\n( log ( 1 +\nk∑ k′=1 eC(cos(wi,wi,¬,k′ )− ) )\n+ 1 C log ( 1 + e−C(cos(wi,vi)− ) )) . (3)\nFor brevity, in Eq. (3) we only show the formulation with the k negatives associated with wi, but the reader should be aware that the complete loss function contains another term covering k negatives vi,¬,k′ associated with each vi. C is again the scaling constant, and is the offset applied on the similarity matrix.2 MSIM can be seen as an extended variant of the MNEG ranking loss.\n2 =1; C=20 (also in MNEG). For further technical details we refer the reader to the original paper (Wang et al., 2019).\nFinally, for any input wordw, we extract its word vector via the approach outlined in §2.2; exactly the same approach can be applied to the original LMs (e.g., BERT) or their lexically fine-tuned variants (“LEXFIT-ed” BERT), see Figure 1."
    }, {
      "heading" : "2.2 Extracting Static Word Representations",
      "text" : "The extraction of static type-level vectors from any underlying Transformer-based LM, both before and after LEXFIT fine-tuning, is guided by best practices from recent comparative analyses and probing work (Vulić et al., 2020; Bommasani et al., 2020). Starting from an underlying LM with N Transformer layers {L1 (bottom layer), . . . , LN (top)} and referring to the embedding layer as L0, we extract a decontextualized word vector for some input word w, fed into the LM “in isolation” without any surrounding context, following Vulić et al. (2020): 1) w is segmented into 1 or more of its constituent subwords [swi], i ≥ 1, where [] refers to the sequence of i subwords; 2) Special tokens [CLS] and [SEP ] are respectively prepended and appended to the subword sequence, and the sequence [CLS][swi][SEP ] is then passed through the LM; 3) The final representation is constructed as the average over the subword encodings further averaged over n ≤ N layers (i.e., all layers up to layer Ln included, denoted as AVG(≤ n)).3 Further, Vulić et al. (2020) empirically verified that: (a) discarding final encodings of [CLS] and [SEP ] produces better type-level vectors – we follow this heuristic in this work; and (b) excluding higher layers from the average may also result in stronger vectors with improved performance in lexical tasks.\nThis approach operates fully “in isolation” (ISO): we extract vectors of words without any surrounding context. The ISO approach is lightweight: 1) it disposes of any external text corpora; 2) it encodes words efficiently due to the absence of context. Moreover, it allows us to directly study the richness of lexical information stored in the LM’s parameters, and to combine it with ISO lexical knowledge from external resources (e.g., WordNet)."
    }, {
      "heading" : "3 Experimental Setup",
      "text" : "Languages and Language Models. Our language selection for evaluation is guided by the following (partially clashing) constraints (Vulić et al., 2020): a) availability of comparable pretrained monolingual LMs; b) task and evaluation data availabil-\n3Note that this always includes the embedding layer (L0).\nity; and c) ensuring some typological diversity of the selection. The final test languages are English (EN), German (DE), Spanish (ES), Finnish (FI), Italian (IT), Polish (PL), Russian (RU), and Turkish (TR). For comparability across languages, we use monolingual uncased BERT Base models for all languages (N = 12 Transformer layers, 12 attention heads, hidden layer dimensionality is 768), available (see the appendix) via the HuggingFace repository (Wolf et al., 2020).\nExternal Lexical Knowledge. We use the standard collection of EN lexical constraints from previous work on (static) word vector specialization (Zhang et al., 2014; Ono et al., 2015; Vulić et al., 2018; Ponti et al., 2018, 2019). It covers the lexical relations from WordNet (Fellbaum, 1998) and Roget’s Thesaurus (Kipfer, 2009); it comprises 1,023,082 synonymy (Psyn) word pairs and 380,873 antonymy pairs (Pant). For all other languages, we rely on non-curated noisy lexical constraints, obtained via an automatic word translation method by Ponti et al. (2019); see the original work for the details of the translation procedure.\nLEXFIT: Technical Details. The implementation is based on the SBERT framework (Reimers and Gurevych, 2019), using the suggested settings: AdamW (Loshchilov and Hutter, 2018); learning rate of 2e − 5; weight decay rate of 0.01, and we run LEXFIT for 2 epochs. The batch size is 512 with MNEG, and 256 with SOFTMAX and MSIM, where one batch always balances between B positive examples and 2k ·B negatives (see §2.1).\nWord Vocabularies and Baselines. We extract decontextualized type-level WEs in each language both from the original BERTs (termed BERT-REG)4 and the LEXFIT-ed BERT models for exactly the same vocabulary. Following Vulić et al. (2020), the vocabularies cover the top 100K most frequent words represented in the respective fastText (FT) vectors, trained on lowercased monolingual Wikipedias by Bojanowski et al. (2017).5 The equivalent vocabulary coverage allows for a direct comparison of all WEs regardless of the induction/extraction method; this also includes the FT\n4For the baseline BERT-REG WEs, we report two variants: (a) all performs layerwise averaging over all Transformer layers (i.e., AVG(≤ 12)); (b) best reports the peak score when potentially excluding highest layers from the layer averaging (i.e., AVG(≤ n), n ≤ 12; see §2.2) (Vulić et al., 2020).\n5Note that the LEXFIT procedure does not depend on the chosen vocabulary, as it operates only on the lexical items found in the external constraints (i.e., the set P ).\nvectors, used as baseline “traditional” static WEs (termed FASTTEXT.WIKI) in all evaluation tasks.\nEvaluation Tasks. We evaluate on the following standard and diverse lexical semantic tasks:\nTask 1: Lexical semantic similarity (LSIM) is an established intrinsic task for evaluating static WEs (Hill et al., 2015). We use the recent comprehensive multilingual LSIM benchmark MultiSimLex (Vulić et al., 2020), which comprises 1,888 pairs in 13 languages, for our EN, ES, FI, PL, and RU LSIM evaluation. We also evaluate on a verbfocused EN LSIM benchmark: SimVerb-3500 (SV) (Gerz et al., 2016), covering 3,500 verb pairs, and SimLex-999 (SL) for DE and IT (999 pairs) (Leviant and Reichart, 2015).6\nTask 2: Bilingual Lexicon Induction (BLI), a standard task to assess the “semantic quality” of static cross-lingual word embeddings (CLWEs) (Ruder et al., 2019), enables investigations on the alignability of monolingual type-level WEs in different languages before and after the LEXFIT procedure. We learn CLWEs from monolingual WEs obtained with all WE methods using the established and supervision-lenient mapping-based approach (Mikolov et al., 2013a; Smith et al., 2017) with the VECMAP framework (Artetxe et al., 2018). We run main BLI evaluations for 10 language pairs spanning EN, DE, RU, FI, TR.7\nTask 3: Lexical Relation Prediction (RELP). We assess the usefulness of lexical knowledge in WEs to learn relation classifiers for standard lexical relations (i.e., synonymy, antonymy, hypernymy, meronymy, plus no relation) via a state-ofthe-art neural model for RELP which learns solely based on input type-level WEs (Glavaš and Vulić, 2018). We use the WordNet-based evaluation data of Glavaš and Vulić (2018) for EN, DE, ES; they contain 10K annotated word pairs per language, 8K for training, 2K for test, balanced by class and in the splits. We extract evaluation data for two more languages: FI and IT. We report micro-averaged F1 scores, averaged across 5 runs for each input WE space; the default RELP model setting is used. In RELP and LSIM, we remove all training and test\n6The evaluation metric is the Spearman’s rank correlation between the average of human LSIM scores for word pairs and the cosine similarity between their respective WEs.\n7A standard BLI setup and data from Glavaš et al. (2019) is adopted: 5K training word pairs are used to learn the mapping, and another 2K pairs as test data. The evaluation metric is standard Mean Reciprocal Rank (MRR). For EN–ES, we run experiments on MUSE data (Conneau et al., 2018).\nRELP/LSIM examples also present in the Psyn and Pant sets to avoid any evaluation data leakage.8\nTask 4: Lexical Simplification (LexSIMP) aims to automatically replace complex words (i.e., specialized terms, less-frequent words) with their simpler in-context synonyms, while retaining grammaticality and conveying the same meaning as the more complex input text (Paetzold and Specia, 2017). Therefore, discerning between semantic similarity (e.g., synonymy injected via LEXFIT) and broader relatedness is critical for LexSIMP (Glavaš and Vulić, 2018). We adopt the standard LexSIMP evaluation protocol used in prior research on static WEs (Ponti et al., 2018, 2019). 1) We use Light-LS (Glavaš and Štajner, 2015), a languageagnostic LexSIMP tool that makes simplifications in an unsupervised way based solely on word similarity in an input (static) WE space; 2) we rely on standard LexSIMP benchmarks, available for EN (Horn et al., 2014), IT (Tonelli et al., 2016), and ES (Saggion, 2017); and 3) we report the standard Accuracy scores (Horn et al., 2014).9\nImportant Disclaimer. We note that the main purpose of the chosen evaluation tasks and experimental protocols is not necessarily achieving state-ofthe-art performance, but rather probing the vectors in different lexical tasks requiring different types of lexical knowledge,10 and offering fair and insightful comparisons between different LEXFIT variants, as well as against standard static WEs (fastText) and non-tuned BERT-based static WEs."
    }, {
      "heading" : "4 Results and Discussion",
      "text" : "The main results for all four tasks are summarized in Tables 1-4, and further results and analyses are available in §4.1 (with additional results in the appendix). These results offer multiple axes of comparison, discussed in what follows.\nComparison to Other Static Word Embeddings. The results over all 4 tasks indicate that static WEs from LEXFITed monolingual BERT 1) outperform traditional WE methods such as FT, and 2) offer also large gains over WEs originating from nonLEXFITed BERTs (Vulić et al., 2020). These re-\n8In BLI and RELP, we do PCA (d = 300) on all input WEs, which slightly improves performance.\n9For further details regarding the LexSIMP benchmarks and evaluation, we refer the reader to the previous work.\n10RELP and LexSIMP use WEs as input features of neural architectures; LSIM and BLI fall under similarity-based evaluation tasks (Ruder et al., 2019).\nsults demonstrate that the inexpensive lexical finetuning procedure can indeed turn large pretrained LMs into effective decontextualized word encoders, and this can be achieved for a reasonably wide spectrum of languages for which such pretrained LMs exist. What is more, LEXFIT for all nonEN languages has been run with noisy automatically translated lexical constraints, which holds promise to support even stronger static LEXFITbased WEs with human-curated data in the future, e.g., extracted from multilingual WordNets (Bond and Foster, 2013), PanLex (Kamholz et al., 2014), or BabelNet (Ehrmann et al., 2014).\nThe results give rise to additional general implications. First, they suggest that the pretrained LMs store even more lexical knowledge than thought previously (Ethayarajh, 2019; Bommasani et al., 2020; Vulić et al., 2020); the role of LEXFIT finetuning is simply to ‘rewire’ and expose that knowledge from the LM through (limited) lexical-level supervision. To further investigate the ‘rewiring’ hypothesis, in §4.1, we also run LEXFIT with a drastically reduced amount of external knowledge.\nBERT-REG vectors display large gains over FT vectors in tasks such as RELP and LexSIMP, again hinting that plenty of lexical knowledge is stored in the original parameters. However, they still lag FT vectors for some tasks (BLI for all language pairs; LSIM for ES, RU, PL). However, LEXFIT-ed BERT-based WEs offer large gains and outperform FT WEs across the board. Our results indicate that ‘classic’ WE models such as skip-gram (Mikolov et al., 2013b) and FT are undermined even in their last field of use, lexical tasks.\nThis comes as a natural finding, given that word2vec and FT can in fact be seen as reduced and training-efficient variants of full-fledged language models (Bengio et al., 2003). The modern LMs are pretrained on larger training data with more parameters and with more sophisticated Transformerbased neural architectures. However, it has not been verified before that effective static WEs can be distilled from such LMs. Efficiency differences aside, this begs the following discussion point for future work: with the existence of large pretrained LMs, and effective methods to extract static WEs from them, as proposed in this work, how useful are traditional WE models still in NLP applications?\nLexical Fine-Tuning Objectives. The scores indicate that all LEXFIT variants are effective and can expose the lexical knowledge from the fine-tuned\nBERTs. However, there are differences across their task performance: the ranking-based MNEG and MSIM variants display stronger performance on similarity-based ranking lexical tasks such as LSIM and BLI. The classification-based SOFTMAX objective is, as expected, better aligned with the RELP task, and we note slight gains with its ternary variant which leverages extra antonymy knowledge.\nThis finding is well aligned with the recent findings demonstrating that task-specific pretraining results in stronger (sentence-level) task performance (Glass et al., 2020; Henderson et al., 2020; Lewis et al., 2020). In our case, we show that task-specific lexical fine-tuning can reshape the underlying LM’s parameters to not only act as a universal word encoder, but also towards a particular lexical task.\nThe per-epoch time measurements from Table 1 validate the efficiency of LEXFIT as a post-training fine-tuning procedure. Previous approaches that attempted to inject lexical information (i.e., word senses and relations) into large LMs (Lauscher et al., 2020; Levine et al., 2020) relied on joint LM (re)training from scratch: it is effectively costlier than training the original BERT models.\nPerformance across Languages and Tasks. As expected, the scores in absolute terms are highest for EN: this is attributed to (a) larger pretraining LM data as well as (b) to clean external lexical knowledge. However, we note encouragingly large gains in target languages even with noisy translated lexical constraints. LEXFIT variants show similar relative patterns across different languages and tasks. We note that, while BERT-REG vectors are unable to match FT performance in the BLI task, our LEXFIT methods (e.g., see MNEG and MSIM BLI scores) outperform FT WEs in this task\nas well, offering improved alignability (Søgaard et al., 2018) between monolingual WEs. The large gains of BERT-REG over FT in RELP and LexSIMP across all evaluation languages already suggest that plenty of lexical knowledge is stored in the pretrained BERTs’ parameters; however, LEXFIT-ing the models offers further gains in LexSIMP and RELP across the board, even with limited external supervision (see also Figure 2c).\nHigh scores with FI in LSIM and BLI are aligned with prior work (Virtanen et al., 2019; Rust et al., 2021) that showcased strong monolingual performance of FI BERT in sentence-level tasks. Along this line, we note that the final quality of LEXFITbased WEs in each language depends on several factors: 1) pretraining data; 2) the underlying LM; 3) the quality and amount of external knowledge."
    }, {
      "heading" : "4.1 Further Discussion",
      "text" : "The multi-component LEXFIT framework allows for a plethora of additional analyses, varying components such as the underlying LM, properties of the LEXFIT variants (e.g., negative examples, finetuning duration, the amount of lexical constraints). We now analyze the impact of these components on the “lexical quality” of the LEXFIT-tuned static WEs. Unless noted otherwise, for computational feasibility and to avoid clutter, we focus 1) on a subset of target languages: EN, ES, FI, IT, 2) on the MSIM variant (k = 1), which showed robust perfor-\nmance in the main experiments before, and 3) on LSIM, BLI, and RELP as the main tasks in these analyses, as they offer a higher language coverage.\nVarying the Amount of Lexical Constraints. We also probe what amount of lexical knowledge is required to turn BERTs into effective decontextualized word encoders by running tests with reduced lexical sets P sampled from the full set. The scores over different P sizes, averaged over 5 samples per each size, are provided in Figure 2, and we note that they extend to other evaluation languages and LEXFIT objectives. As expected, we do observe performance drops with fewer external data. However, the decrease is modest even when relying on\nonly 5k external constraints (e.g., see the scores in BLI and RELP for all languages; EN Multi-SimLex score is 69.4 with 50k constraints, 65.0 with 5k), or even non-existent (RELP in FI).\nRemarkably, the LEXFIT performance with only 10k or 5k fine-tuning pairs11 remains substantially higher than with FT or BERT-REG WEs in all tasks. This empirically validates LEXFIT’s sample efficiency and further empirically corroborates our knowledge rewiring hypothesis: the original LMs already contain plenty of useful lexical knowledge implicitly, and even a small amount of external supervision can expose that knowledge.\nCopying or Rewiring Knowledge? Large gains over BERT-REG even with mere 5k pairs (LEXFITing takes only a few minutes), where the large portion of the 100K word vocabulary is not covered in the external input, further reveal that LEXFIT does not only copy the knowledge of seen words and relations into the LM: it leverages the (small) external set to generalize to uncovered words.\nWe confirm this hypothesis with another experiment where our input LM is the same BERT Base architecture parameters with the same subword vocabulary as English BERT, but with its parameters now randomly initialized using the Xavier initialization (Glorot and Bengio, 2010). Running LEXFIT on this model for 10 epochs with the full set of lexical constraints (see §3) yields the following LSIM scores: 23.1 (Multi-SimLex) and 14.6 (SimVerb), and the English RELP accuracy score of 61.8%. The scores are substantially higher than those of fully random static WEs (see also the appendix), which indicates that the LEXFIT procedure does enable storing some lexical knowledge into the model parameters. However, at the same\n11When sampling all reduced sets, we again deliberately excluded all words occurring in our LSIM benchmarks.\ntime, these scores are substantially lower than the ones achieved when starting from LM-pretrained models, even when LEXFIT is run with mere 5k fine-tuning lexical pairs.12 This again strongly suggests that LEXFIT ’unlocks’ already available lexical knowledge stored in the pretrained LM, yielding benefits beyond the knowledge available in the external data. Another line of recent work (Liu et al., 2021) further corroborates our findings.\nMultilingual LMs. Prior work indicated that massively multilingual LMs such as multilingual BERT (mBERT) (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) cannot match the performance of their language-specific counterparts in both lexical (Vulić et al., 2020) and sentence-level tasks (Rust et al., 2021). We also analyze this conjecture by LEXFIT-ing mBERT instead of monolingual BERTs in different languages. The results with MSIM (k = 1) are provided in Figure 4; we observe similar comparison trends with other languages and LEXFIT variants, not shown due to space constraints. While LEXFIT-ing mBERT offers huge gains over the original mBERT model, sometimes even larger in relative terms than with monolingual BERTs (e.g., LSIM scores for EN increase from 0.21 to 0.69, and from 0.24 to 0.60 for FI; BLI scores for EN-FI rise from 0.21 to 0.37), it cannot match the absolute performance peaks of LEXFIT-ed monolingual BERTs.\nStoring the knowledge of 100+ languages in its limited parameter budget, mBERT still cannot capture monolingual knowledge as accurately as language-specific BERTs (Conneau et al., 2020). However, we believe that its performance with LEXFIT may be further improved by leveraging recently proposed multilingual LM adaptation strategies that mitigate a mismatch between shared multilingual and language-specific vocabularies (Artetxe et al., 2020; Chung et al., 2020; Pfeiffer et al., 2020); we leave this for future work.\nLayerwise Averaging. A consensus in prior work (Tenney et al., 2019; Ethayarajh, 2019; Vulić et al., 2020) points that out-of-context lexical knowledge in pretrained LMs is typically stored in bottom Transformer layers (see Table 5). However, Table 5 also reveals that this does not hold after LEXFITing: the tuned model requires knowledge from all layers to extract effective decontextualized WEs and reach peak task scores. Effectively, this means\n12The same findings hold for other tasks and languages.\nthat, through lexical fine-tuning, model “reformats” all its parameter budget towards storing useful lexical knowledge, that is, it specializes as (decontextualized) word encoder.\nVarying the Number of Negative Examples and their impact on task performance is recapped in Figure 3b. Overall, increasing k does not benefit (and sometimes even hurts) performance – the exceptions are EN LSIM; and the RELP task with the SOFTMAX variant for some languages. We largely attribute this to the noise in the target-language lexical pairs: with larger k values, it becomes increasingly difficult for the model to discern between noisy positive examples and random negatives.\nLonger Fine-Tuning. Instead of the standard setup with 2 epochs (see §3), we run LEXFIT for 10 epochs. The per-epoch snapshots of scores are summarized in the appendix. The scores again validate that LEXFIT is sample-efficient: longer fine-tuning yields negligible to zero improvements in EN LSIM and RELP after the first few epochs, with very high scores achieved after epoch 1 already. It even yields small drops for other languages in LSIM and BLI: we again attribute this to slight overfitting to noisy target-language lexical knowledge."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "We proposed LEXFIT, a lexical fine-tuning procedure which transforms pretrained LMs such as BERT into effective decontextualized word encoders through dual-encoder architectures. Our experiments demonstrated that the lexical knowledge already stored in pretrained LMs can be further exposed via additional inexpensive LEXFITing with (even limited amounts of) external lexical knowledge. We successfully applied LEXFIT even to languages without any external human-curated lexical knowledge. Our LEXFIT word embeddings (WEs) outperform “traditional” static WEs (e.g., fastText) across a spectrum of lexical tasks across diverse languages in controlled evaluations, thus directly questioning the practical usefulness of the traditional WE models in modern NLP.\nBesides inducing better static WEs for lexical tasks, following the line of lexical probing work (Ethayarajh, 2019; Vulić et al., 2020), our goal in this work was to understand how (and how much) lexical semantic knowledge is coded in pretrained LMs, and how to ‘unlock’ the knowledge from the LMs. We hope that our work will be beneficial for all lexical tasks where static WEs from traditional\nWE models are still largely used (Schlechtweg et al., 2020; Kaiser et al., 2021).\nDespite the extensive experiments, we only scratched the surface, and can indicate a spectrum of future enhancements to the proof-of-concept LEXFIT framework beyond the scope of this work. We will test other dual-encoder loss functions, including finer-grained relation classification tasks (e.g., in the SOFTMAX variant), and hard (instead of random) negative examples (Wieting et al., 2015; Mrkšić et al., 2017; Lauscher et al., 2020; Kalantidis et al., 2020). While in this work, for simplicity and efficiency, we focused on fully decontextualized ISO setup (see §2.2), we will also probe alternative ways to extract static WEs from pretrained LMs, e.g., averages-over-context (Liu et al., 2019; Bommasani et al., 2020; Vulić et al., 2020). We will also investigate other approaches to procuring more accurate external knowledge for LEXFIT in target languages, and extend the framework to more languages, lexical tasks, and specialized domains. We will also focus on reducing the gap between pretrained monolingual and multilingual LMs."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the three anonymous reviewers, Nils Reimers, and Jonas Pfeiffer for their helpful comments and suggestions. Ivan Vulić and Anna Korhonen are supported by the ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no. 648909) awarded to Korhonen, and the ERC PoC Grant MultiConvAI: Enabling Multilingual Conversational AI (no. 957356). Goran Glavaš is supported by the Baden Württemberg Stiftung (Eliteprogramm, AGREE grant)."
    } ],
    "references" : [ {
      "title" : "A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings",
      "author" : [ "Mikel Artetxe", "Gorka Labaka", "Eneko Agirre." ],
      "venue" : "Proceedings of ACL 2018, pages 789–798.",
      "citeRegEx" : "Artetxe et al\\.,? 2018",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2018
    }, {
      "title" : "On the cross-lingual transferability of monolingual representations",
      "author" : [ "Mikel Artetxe", "Sebastian Ruder", "Dani Yogatama." ],
      "venue" : "Proceedings of ACL 2020, pages 4623–4637.",
      "citeRegEx" : "Artetxe et al\\.,? 2020",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2020
    }, {
      "title" : "WordNet: A lexical database organized on psycholinguistic principles",
      "author" : [ "Richard Beckwith", "Christiane Fellbaum", "Derek Gross", "George A. Miller." ],
      "venue" : "Lexical acquisition: Exploiting on-line resources to build a lexicon, pages 211–231.",
      "citeRegEx" : "Beckwith et al\\.,? 1991",
      "shortCiteRegEx" : "Beckwith et al\\.",
      "year" : 1991
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Jauvin." ],
      "venue" : "Journal of Machine Learning Research, 3:1137–1155.",
      "citeRegEx" : "Bengio et al\\.,? 2003",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Transactions of the ACL, 5:135–146.",
      "citeRegEx" : "Bojanowski et al\\.,? 2017",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2017
    }, {
      "title" : "Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings",
      "author" : [ "Rishi Bommasani", "Kelly Davis", "Claire Cardie." ],
      "venue" : "Proceedings of ACL 2020, pages 4758–4781.",
      "citeRegEx" : "Bommasani et al\\.,? 2020",
      "shortCiteRegEx" : "Bommasani et al\\.",
      "year" : 2020
    }, {
      "title" : "Linking and extending an open multilingual Wordnet",
      "author" : [ "Francis Bond", "Ryan Foster." ],
      "venue" : "Proceedings of ACL 2013, pages 1352–1362.",
      "citeRegEx" : "Bond and Foster.,? 2013",
      "shortCiteRegEx" : "Bond and Foster.",
      "year" : 2013
    }, {
      "title" : "Universal sentence encoder for English",
      "author" : [ "Daniel Cer", "Yinfei Yang", "Sheng-yi Kong", "Nan Hua", "Nicole Limtiaco", "Rhomni St. John", "Noah Constant", "Mario Guajardo-Cespedes", "Steve Yuan", "Chris Tar", "Yun-Hsuan Sung", "Brian Strope", "Ray Kurzweil." ],
      "venue" : "In",
      "citeRegEx" : "Cer et al\\.,? 2018",
      "shortCiteRegEx" : "Cer et al\\.",
      "year" : 2018
    }, {
      "title" : "When is a bishop not like a rook? When it’s like a rabbi! multiprototype bert embeddings for estimating semantic relationships",
      "author" : [ "Gabriella Chronis", "Katrin Erk." ],
      "venue" : "Proceedings of CoNLL 2020, page 227–244.",
      "citeRegEx" : "Chronis and Erk.,? 2020",
      "shortCiteRegEx" : "Chronis and Erk.",
      "year" : 2020
    }, {
      "title" : "Improving multilingual models with language-clustered vocabularies",
      "author" : [ "Hyung Won Chung", "Dan Garrette", "Kiat Chuan Tan", "Jason Riesa." ],
      "venue" : "Proceedings of EMNLP 2020, pages 4536–4546.",
      "citeRegEx" : "Chung et al\\.,? 2020",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Word translation without parallel data",
      "author" : [ "Alexis Conneau", "Guillaume Lample", "Marc’Aurelio Ranzato", "Ludovic Denoyer", "Hervé Jégou" ],
      "venue" : "In Proceedings of ICLR 2018",
      "citeRegEx" : "Conneau et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of NAACL-HLT 2019, pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Representing multilingual data as linked data: The case of BabelNet 2.0",
      "author" : [ "Maud Ehrmann", "Francesco Cecconi", "Daniele Vannella", "John Philip McCrae", "Philipp Cimiano", "Roberto Navigli" ],
      "venue" : "In Proceedings of LREC",
      "citeRegEx" : "Ehrmann et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ehrmann et al\\.",
      "year" : 2014
    }, {
      "title" : "How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings",
      "author" : [ "Kawin Ethayarajh." ],
      "venue" : "Proceedings of EMNLP-IJCNLP 2019, pages 55–",
      "citeRegEx" : "Ethayarajh.,? 2019",
      "shortCiteRegEx" : "Ethayarajh.",
      "year" : 2019
    }, {
      "title" : "Diverse Context for Learning Word Representations",
      "author" : [ "Manaal Faruqui." ],
      "venue" : "Ph.D. thesis, Carnegie Mellon University.",
      "citeRegEx" : "Faruqui.,? 2016",
      "shortCiteRegEx" : "Faruqui.",
      "year" : 2016
    }, {
      "title" : "Retrofitting word vectors to semantic lexicons",
      "author" : [ "Manaal Faruqui", "Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith." ],
      "venue" : "Proceedings of NAACL-HLT 2015, pages 1606–1615.",
      "citeRegEx" : "Faruqui et al\\.,? 2015",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2015
    }, {
      "title" : "WordNet",
      "author" : [ "Christiane Fellbaum." ],
      "venue" : "MIT Press.",
      "citeRegEx" : "Fellbaum.,? 1998",
      "shortCiteRegEx" : "Fellbaum.",
      "year" : 1998
    }, {
      "title" : "PPDB: The Paraphrase Database",
      "author" : [ "Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch." ],
      "venue" : "Proceedings of NAACL-HLT 2013, pages 758–764.",
      "citeRegEx" : "Ganitkevitch et al\\.,? 2013",
      "shortCiteRegEx" : "Ganitkevitch et al\\.",
      "year" : 2013
    }, {
      "title" : "SimVerb-3500: A largescale evaluation set of verb similarity",
      "author" : [ "Daniela Gerz", "Ivan Vulić", "Felix Hill", "Roi Reichart", "Anna Korhonen." ],
      "venue" : "Proceedings of EMNLP 2016, pages 2173–2182.",
      "citeRegEx" : "Gerz et al\\.,? 2016",
      "shortCiteRegEx" : "Gerz et al\\.",
      "year" : 2016
    }, {
      "title" : "Span selection pretraining for question answering",
      "author" : [ "Michael Glass", "Alfio Gliozzo", "Rishav Chakravarti", "Anthony Ferritto", "Lin Pan", "GP Shrivatsa Bhargav", "Dinesh Garg", "Avirup Sil." ],
      "venue" : "Proceedings of ACL 2020, pages 2773–2782.",
      "citeRegEx" : "Glass et al\\.,? 2020",
      "shortCiteRegEx" : "Glass et al\\.",
      "year" : 2020
    }, {
      "title" : "Simplifying lexical simplification: Do we need simplified corpora",
      "author" : [ "Goran Glavaš", "Sanja Štajner" ],
      "venue" : "In Proceedings of ACL-IJCNLP",
      "citeRegEx" : "Glavaš and Štajner.,? \\Q2015\\E",
      "shortCiteRegEx" : "Glavaš and Štajner.",
      "year" : 2015
    }, {
      "title" : "Discriminating between lexico-semantic relations with the specialization tensor model",
      "author" : [ "Goran Glavaš", "Ivan Vulić." ],
      "venue" : "Proceedings of NAACL-HLT 2018, pages 181–187.",
      "citeRegEx" : "Glavaš and Vulić.,? 2018",
      "shortCiteRegEx" : "Glavaš and Vulić.",
      "year" : 2018
    }, {
      "title" : "Explicit retrofitting of distributional word vectors",
      "author" : [ "Goran Glavaš", "Ivan Vulić." ],
      "venue" : "Proceedings of ACL 2018, pages 34–45.",
      "citeRegEx" : "Glavaš and Vulić.,? 2018",
      "shortCiteRegEx" : "Glavaš and Vulić.",
      "year" : 2018
    }, {
      "title" : "How to (properly) evaluate crosslingual word embeddings: On strong baselines, comparative analyses, and some misconceptions",
      "author" : [ "Goran Glavaš", "Robert Litschko", "Sebastian Ruder", "Ivan Vulić." ],
      "venue" : "Proceedings of ACL 2019, pages 710–721.",
      "citeRegEx" : "Glavaš et al\\.,? 2019",
      "shortCiteRegEx" : "Glavaš et al\\.",
      "year" : 2019
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Xavier Glorot", "Yoshua Bengio." ],
      "venue" : "Proceedings of AISTATS 2010, pages 249–256.",
      "citeRegEx" : "Glorot and Bengio.,? 2010",
      "shortCiteRegEx" : "Glorot and Bengio.",
      "year" : 2010
    }, {
      "title" : "ConveRT: Efficient and accurate conversational representations from transformers",
      "author" : [ "Matthew Henderson", "Iñigo Casanueva", "Nikola Mrkšić", "Pei-Hao Su", "Tsung-Hsien Wen", "Ivan Vulić." ],
      "venue" : "Findings of EMNLP 2020, pages 2161–2174.",
      "citeRegEx" : "Henderson et al\\.,? 2020",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2020
    }, {
      "title" : "Training neural response selection for task-oriented dialogue systems",
      "author" : [ "Matthew Henderson", "Ivan Vulić", "Daniela Gerz", "Iñigo Casanueva", "Paweł Budzianowski", "Sam Coope", "Georgios Spithourakis", "Tsung-Hsien Wen", "Nikola Mrkšić", "Pei-Hao Su" ],
      "venue" : null,
      "citeRegEx" : "Henderson et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2019
    }, {
      "title" : "SimLex-999: Evaluating semantic models with (genuine) similarity estimation",
      "author" : [ "Felix Hill", "Roi Reichart", "Anna Korhonen." ],
      "venue" : "Computational Linguistics, 41(4):665–695.",
      "citeRegEx" : "Hill et al\\.,? 2015",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning a lexical simplifier using Wikipedia",
      "author" : [ "Colby Horn", "Cathryn Manduca", "David Kauchak." ],
      "venue" : "Proceedings of ACL 2014, pages 458–463.",
      "citeRegEx" : "Horn et al\\.,? 2014",
      "shortCiteRegEx" : "Horn et al\\.",
      "year" : 2014
    }, {
      "title" : "Effects of pre- and post-processing on type-based embeddings in lexical semantic change detection",
      "author" : [ "Jens Kaiser", "Sinan Kurtyigit", "Serge Kotchourko", "Dominik Schlechtweg." ],
      "venue" : "Proceedings of EACL 2021, pages 125–137.",
      "citeRegEx" : "Kaiser et al\\.,? 2021",
      "shortCiteRegEx" : "Kaiser et al\\.",
      "year" : 2021
    }, {
      "title" : "Hard negative mixing for contrastive learning",
      "author" : [ "Yannis Kalantidis", "Mert Bülent Sariyildiz", "Noé Pion", "Philippe Weinzaepfel", "Diane Larlus." ],
      "venue" : "Proceedings of NeurIPS 2020.",
      "citeRegEx" : "Kalantidis et al\\.,? 2020",
      "shortCiteRegEx" : "Kalantidis et al\\.",
      "year" : 2020
    }, {
      "title" : "PanLex: Building a resource for panlingual lexical translation",
      "author" : [ "David Kamholz", "Jonathan Pool", "Susan M. Colowick." ],
      "venue" : "Proceedings of LREC 2014, pages 3145–3150.",
      "citeRegEx" : "Kamholz et al\\.,? 2014",
      "shortCiteRegEx" : "Kamholz et al\\.",
      "year" : 2014
    }, {
      "title" : "Roget’s 21st Century Thesaurus (3rd Edition)",
      "author" : [ "Barbara Ann Kipfer." ],
      "venue" : "Philip Lief Group.",
      "citeRegEx" : "Kipfer.,? 2009",
      "shortCiteRegEx" : "Kipfer.",
      "year" : 2009
    }, {
      "title" : "Specializing unsupervised pretraining models for word-level semantic similarity",
      "author" : [ "Anne Lauscher", "Ivan Vulić", "Edoardo Maria Ponti", "Anna Korhonen", "Goran Glavaš." ],
      "venue" : "Proceedings of COLING 2020, pages 1371–1383.",
      "citeRegEx" : "Lauscher et al\\.,? 2020",
      "shortCiteRegEx" : "Lauscher et al\\.",
      "year" : 2020
    }, {
      "title" : "Separated by an un-common language: Towards judgment language informed vector space modeling",
      "author" : [ "Ira Leviant", "Roi Reichart." ],
      "venue" : "CoRR, abs/1508.00106.",
      "citeRegEx" : "Leviant and Reichart.,? 2015",
      "shortCiteRegEx" : "Leviant and Reichart.",
      "year" : 2015
    }, {
      "title" : "SenseBERT: Driving some sense into BERT",
      "author" : [ "Yoav Levine", "Barak Lenz", "Or Dagan", "Ori Ram", "Dan Padnos", "Or Sharir", "Shai Shalev-Shwartz", "Amnon Shashua", "Yoav Shoham." ],
      "venue" : "Proceedings of ACL 2020, pages 4656–4667.",
      "citeRegEx" : "Levine et al\\.,? 2020",
      "shortCiteRegEx" : "Levine et al\\.",
      "year" : 2020
    }, {
      "title" : "Pre-training via paraphrasing",
      "author" : [ "Mike Lewis", "Marjan Ghazvininejad", "Gargi Ghosh", "Armen Aghajanyan", "Sida Wang", "Luke Zettlemoyer." ],
      "venue" : "CoRR, abs/2006.15020.",
      "citeRegEx" : "Lewis et al\\.,? 2020",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Fast, effective and self-supervised: Transforming masked language models into universal lexical and sentence encoders",
      "author" : [ "Fangyu Liu", "Ivan Vulić", "Anna Korhonen", "Nigel Collier." ],
      "venue" : "CoRR, abs/2104.08027.",
      "citeRegEx" : "Liu et al\\.,? 2021",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Investigating cross-lingual alignment methods for contextualized embeddings with token-level evaluation",
      "author" : [ "Qianchu Liu", "Diana McCarthy", "Ivan Vulić", "Anna Korhonen." ],
      "venue" : "Proceedings of CoNLL 2019, pages 33–43.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "Proceedings of ICLR 2018.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2018",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2018
    }, {
      "title" : "Exploiting similarities among languages for machine translation",
      "author" : [ "Tomas Mikolov", "Quoc V. Le", "Ilya Sutskever." ],
      "venue" : "arXiv preprint, CoRR, abs/1309.4168.",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean." ],
      "venue" : "Proceedings of NeurIPS 2013, pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Neural belief tracker: Data-driven dialogue state tracking",
      "author" : [ "Nikola Mrkšić", "Diarmuid Ó Séaghdha", "Tsung-Hsien Wen", "Blaise Thomson", "Steve Young." ],
      "venue" : "Proceedings of ACL 2017, pages 1777–1788.",
      "citeRegEx" : "Mrkšić et al\\.,? 2017",
      "shortCiteRegEx" : "Mrkšić et al\\.",
      "year" : 2017
    }, {
      "title" : "Semantic specialisation of distributional word vector spaces using monolingual and cross-lingual constraints",
      "author" : [ "Nikola Mrkšić", "Ivan Vulić", "Diarmuid Ó Séaghdha", "Ira Leviant", "Roi Reichart", "Milica Gašić", "Anna Korhonen", "Steve Young." ],
      "venue" : "Transactions",
      "citeRegEx" : "Mrkšić et al\\.,? 2017",
      "shortCiteRegEx" : "Mrkšić et al\\.",
      "year" : 2017
    }, {
      "title" : "Word embedding-based antonym detection using thesauri and distributional information",
      "author" : [ "Masataka Ono", "Makoto Miwa", "Yutaka Sasaki." ],
      "venue" : "Proceedings of NAACL-HLT 2015, pages 984–989.",
      "citeRegEx" : "Ono et al\\.,? 2015",
      "shortCiteRegEx" : "Ono et al\\.",
      "year" : 2015
    }, {
      "title" : "A survey on lexical simplification",
      "author" : [ "Gustavo Paetzold", "Lucia Specia." ],
      "venue" : "Journal of Artificial Intelligence Research, 60:549–593.",
      "citeRegEx" : "Paetzold and Specia.,? 2017",
      "shortCiteRegEx" : "Paetzold and Specia.",
      "year" : 2017
    }, {
      "title" : "UNKs everywhere: Adapting multilingual language models to new scripts",
      "author" : [ "Jonas Pfeiffer", "Ivan Vulić", "Iryna Gurevych", "Sebastian Ruder." ],
      "venue" : "CoRR, abs/2012.15562.",
      "citeRegEx" : "Pfeiffer et al\\.,? 2020",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2020
    }, {
      "title" : "Adversarial propagation and zero-shot cross-lingual transfer of word vector specialization",
      "author" : [ "Edoardo Maria Ponti", "Ivan Vulić", "Goran Glavaš", "Nikola Mrkšić", "Anna Korhonen." ],
      "venue" : "Proceedings of EMNLP 2018, pages 282–293.",
      "citeRegEx" : "Ponti et al\\.,? 2018",
      "shortCiteRegEx" : "Ponti et al\\.",
      "year" : 2018
    }, {
      "title" : "Cross-lingual semantic specialization via lexical relation induction",
      "author" : [ "Edoardo Maria Ponti", "Ivan Vulić", "Goran Glavaš", "Roi Reichart", "Anna Korhonen." ],
      "venue" : "Proceedings of EMNLP 2019, pages 2206–2217.",
      "citeRegEx" : "Ponti et al\\.,? 2019",
      "shortCiteRegEx" : "Ponti et al\\.",
      "year" : 2019
    }, {
      "title" : "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of EMNLP 2019, pages 3982–3992.",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "A primer in BERTology: what we know about how BERT works",
      "author" : [ "Anna Rogers", "Olga Kovaleva", "Anna Rumshisky." ],
      "venue" : "Transactions of the ACL, 8:842– 866.",
      "citeRegEx" : "Rogers et al\\.,? 2020",
      "shortCiteRegEx" : "Rogers et al\\.",
      "year" : 2020
    }, {
      "title" : "A survey of cross-lingual embedding models",
      "author" : [ "Sebastian Ruder", "Ivan Vulić", "Anders Søgaard." ],
      "venue" : "Journal of Artificial Intelligence Research, 65:569– 631.",
      "citeRegEx" : "Ruder et al\\.,? 2019",
      "shortCiteRegEx" : "Ruder et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatic text simplification",
      "author" : [ "Horacio Saggion." ],
      "venue" : "Synthesis Lectures on Human Language Technologies, 10(1):1–137.",
      "citeRegEx" : "Saggion.,? 2017",
      "shortCiteRegEx" : "Saggion.",
      "year" : 2017
    }, {
      "title" : "Semeval-2020 task 1: Unsupervised lexical semantic change detection",
      "author" : [ "Dominik Schlechtweg", "Barbara McGillivray", "Simon Hengchen", "Haim Dubossarsky", "Nina Tahmasebi." ],
      "venue" : "Proceedings of SemEval 2020, pages 1–23.",
      "citeRegEx" : "Schlechtweg et al\\.,? 2020",
      "shortCiteRegEx" : "Schlechtweg et al\\.",
      "year" : 2020
    }, {
      "title" : "Symmetric pattern based word embeddings for improved word similarity prediction",
      "author" : [ "Roy Schwartz", "Roi Reichart", "Ari Rappoport." ],
      "venue" : "Proceedings of CoNLL 2015, pages 258–267.",
      "citeRegEx" : "Schwartz et al\\.,? 2015",
      "shortCiteRegEx" : "Schwartz et al\\.",
      "year" : 2015
    }, {
      "title" : "Offline bilingual word vectors, orthogonal transformations and the inverted softmax",
      "author" : [ "Samuel L. Smith", "David H.P. Turban", "Steven Hamblin", "Nils Y. Hammerla." ],
      "venue" : "Proceedings of ICLR 2017.",
      "citeRegEx" : "Smith et al\\.,? 2017",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2017
    }, {
      "title" : "On the limitations of unsupervised bilingual dictionary induction",
      "author" : [ "Anders Søgaard", "Sebastian Ruder", "Ivan Vulić." ],
      "venue" : "Proceedings of ACL 2018, pages 778–788.",
      "citeRegEx" : "Søgaard et al\\.,? 2018",
      "shortCiteRegEx" : "Søgaard et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT rediscovers the classical NLP pipeline",
      "author" : [ "Ian Tenney", "Dipanjan Das", "Ellie Pavlick." ],
      "venue" : "Proceedings of ACL 2019, pages 4593–4601.",
      "citeRegEx" : "Tenney et al\\.,? 2019",
      "shortCiteRegEx" : "Tenney et al\\.",
      "year" : 2019
    }, {
      "title" : "SIMPITIKI: A simplification corpus for Italian",
      "author" : [ "Sara Tonelli", "Alessio Palmero Aprosio", "Francesca Saltori." ],
      "venue" : "Proceedings of CLiC-IT 2016.",
      "citeRegEx" : "Tonelli et al\\.,? 2016",
      "shortCiteRegEx" : "Tonelli et al\\.",
      "year" : 2016
    }, {
      "title" : "Multilingual is not enough: BERT for Finnish",
      "author" : [ "Antti Virtanen", "Jenna Kanerva", "Rami Ilo", "Jouni Luoma", "Juhani Luotolahti", "Tapio Salakoski", "Filip Ginter", "Sampo Pyysalo." ],
      "venue" : "CoRR, abs/1912.07076.",
      "citeRegEx" : "Virtanen et al\\.,? 2019",
      "shortCiteRegEx" : "Virtanen et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi-Simlex: A largescale evaluation of multilingual and cross-lingual",
      "author" : [ "Ivan Vulić", "Simon Baker", "Edoardo Maria Ponti", "Ulla Petti", "Ira Leviant", "Kelly Wing", "Olga Majewska", "Eden Bar", "Matt Malone", "Thierry Poibeau", "Roi Reichart", "Anna Korhonen" ],
      "venue" : null,
      "citeRegEx" : "Vulić et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Vulić et al\\.",
      "year" : 2020
    }, {
      "title" : "Post-specialisation: Retrofitting vectors of words unseen in lexical resources",
      "author" : [ "Ivan Vulić", "Goran Glavaš", "Nikola Mrkšić", "Anna Korhonen." ],
      "venue" : "Proceedings of NAACL-HLT 2018, pages 516–527.",
      "citeRegEx" : "Vulić et al\\.,? 2018",
      "shortCiteRegEx" : "Vulić et al\\.",
      "year" : 2018
    }, {
      "title" : "Probing pretrained language models for lexical semantics",
      "author" : [ "Ivan Vulić", "Edoardo Maria Ponti", "Robert Litschko", "Goran Glavaš", "Anna Korhonen." ],
      "venue" : "Proceedings of EMNLP 2020, pages 7222–7240.",
      "citeRegEx" : "Vulić et al\\.,? 2020",
      "shortCiteRegEx" : "Vulić et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatic selection of context configurations for improved class-specific word representations",
      "author" : [ "Ivan Vulić", "Roy Schwartz", "Ari Rappoport", "Roi Reichart", "Anna Korhonen." ],
      "venue" : "Proceedings of CoNLL 2017, pages 112–122.",
      "citeRegEx" : "Vulić et al\\.,? 2017",
      "shortCiteRegEx" : "Vulić et al\\.",
      "year" : 2017
    }, {
      "title" : "Multi-similarity loss with general pair weighting for deep metric learning",
      "author" : [ "Xun Wang", "Xintong Han", "Weilin Huang", "Dengke Dong", "Matthew R. Scott." ],
      "venue" : "Proceedings of CVPR 2019, pages 5022–5030.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "From paraphrase database to compositional paraphrase model and back",
      "author" : [ "John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu." ],
      "venue" : "Transactions of the ACL, 3:345–358.",
      "citeRegEx" : "Wieting et al\\.,? 2015",
      "shortCiteRegEx" : "Wieting et al\\.",
      "year" : 2015
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of EMNLP 2020: System Demonstrations, pages 38–45.",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning semantic textual similarity from conversations",
      "author" : [ "Yinfei Yang", "Steve Yuan", "Daniel Cer", "Sheng-Yi Kong", "Noah Constant", "Petr Pilar", "Heming Ge", "Yun-hsuan Sung", "Brian Strope", "Ray Kurzweil." ],
      "venue" : "Proceedings of The 3rd Workshop on Representa-",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Word semantic representations using bayesian probabilistic tensor factorization",
      "author" : [ "Jingwei Zhang", "Jeremy Salwen", "Michael Glass", "Alfio Gliozzo." ],
      "venue" : "Proceedings of EMNLP 2014, pages 1522–1531.",
      "citeRegEx" : "Zhang et al\\.,? 2014",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2014
    }, {
      "title" : "BERT-REG denotes the extraction of word representations (again with the best strategy from prior work) from the regular underlying BERT models, which were not further “LEXFIT-ed”: (all) layerwise averaging over all Transformer layers. The highest scores per column for each training dictionary size are in bold; the second best result is underlined",
      "author" : [ "Vulić" ],
      "venue" : null,
      "citeRegEx" : "Vulić,? \\Q2020\\E",
      "shortCiteRegEx" : "Vulić",
      "year" : 2020
    }, {
      "title" : "The highest scores per column are in bold, the second best is underlined",
      "author" : [ "Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bengio and 2010..,? \\Q2010\\E",
      "shortCiteRegEx" : "Bengio and 2010..",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Probing large pretrained encoders like BERT (Devlin et al., 2019) revealed that they contain a wealth of lexical knowledge (Ethayarajh, 2019; Vulić et al.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 14,
      "context" : ", 2019) revealed that they contain a wealth of lexical knowledge (Ethayarajh, 2019; Vulić et al., 2020).",
      "startOffset" : 65,
      "endOffset" : 103
    }, {
      "referenceID" : 61,
      "context" : ", 2019) revealed that they contain a wealth of lexical knowledge (Ethayarajh, 2019; Vulić et al., 2020).",
      "startOffset" : 65,
      "endOffset" : 103
    }, {
      "referenceID" : 61,
      "context" : "If type-level word vectors are extracted from BERT with appropriate strategies, they can even outperform traditional word embeddings (WEs) in some lexical tasks (Vulić et al., 2020; Bommasani et al., 2020; Chronis and Erk, 2020).",
      "startOffset" : 161,
      "endOffset" : 228
    }, {
      "referenceID" : 5,
      "context" : "If type-level word vectors are extracted from BERT with appropriate strategies, they can even outperform traditional word embeddings (WEs) in some lexical tasks (Vulić et al., 2020; Bommasani et al., 2020; Chronis and Erk, 2020).",
      "startOffset" : 161,
      "endOffset" : 228
    }, {
      "referenceID" : 8,
      "context" : "If type-level word vectors are extracted from BERT with appropriate strategies, they can even outperform traditional word embeddings (WEs) in some lexical tasks (Vulić et al., 2020; Bommasani et al., 2020; Chronis and Erk, 2020).",
      "startOffset" : 161,
      "endOffset" : 228
    }, {
      "referenceID" : 28,
      "context" : "This source of signal is known to lead to distortions in the induced representations by conflating meaning based on topical relatedness rather than authentic semantic similarity (Hill et al., 2015; Schwartz et al., 2015; Vulić et al., 2017).",
      "startOffset" : 178,
      "endOffset" : 240
    }, {
      "referenceID" : 55,
      "context" : "This source of signal is known to lead to distortions in the induced representations by conflating meaning based on topical relatedness rather than authentic semantic similarity (Hill et al., 2015; Schwartz et al., 2015; Vulić et al., 2017).",
      "startOffset" : 178,
      "endOffset" : 240
    }, {
      "referenceID" : 64,
      "context" : "This source of signal is known to lead to distortions in the induced representations by conflating meaning based on topical relatedness rather than authentic semantic similarity (Hill et al., 2015; Schwartz et al., 2015; Vulić et al., 2017).",
      "startOffset" : 178,
      "endOffset" : 240
    }, {
      "referenceID" : 15,
      "context" : "This also creates a ripple effect on downstream applications, where model performance may suffer (Faruqui, 2016; Mrkšić et al., 2017; Lauscher et al., 2020).",
      "startOffset" : 97,
      "endOffset" : 156
    }, {
      "referenceID" : 43,
      "context" : "This also creates a ripple effect on downstream applications, where model performance may suffer (Faruqui, 2016; Mrkšić et al., 2017; Lauscher et al., 2020).",
      "startOffset" : 97,
      "endOffset" : 156
    }, {
      "referenceID" : 34,
      "context" : "This also creates a ripple effect on downstream applications, where model performance may suffer (Faruqui, 2016; Mrkšić et al., 2017; Lauscher et al., 2020).",
      "startOffset" : 97,
      "endOffset" : 156
    }, {
      "referenceID" : 2,
      "context" : "In particular, the process known as semantic specialization (or retrofitting) injects information about lexical relations from databases like WordNet (Beckwith et al., 1991) or the Paraphrase Database (Ganitkevitch et al.",
      "startOffset" : 150,
      "endOffset" : 173
    }, {
      "referenceID" : 18,
      "context" : ", 1991) or the Paraphrase Database (Ganitkevitch et al., 2013) into WEs.",
      "startOffset" : 35,
      "endOffset" : 62
    }, {
      "referenceID" : 50,
      "context" : "To this effect, we develop LEXFIT, a versatile lexical fine-tuning framework, illustrated in Figure 1, drawing a parallel with universal sentence encoders like SentenceBERT (Reimers and Gurevych, 2019).",
      "startOffset" : 173,
      "endOffset" : 201
    }, {
      "referenceID" : 34,
      "context" : "Compared to prior attempts at injecting lexical knowledge into large LMs (Lauscher et al., 2020), our LEXFIT method is innovative as it is deployed post-hoc on top of already pretrained LMs, rather than requiring joint multi-task training.",
      "startOffset" : 73,
      "endOffset" : 96
    }, {
      "referenceID" : 61,
      "context" : "Here, we focus on their lexical semantic knowledge (Vulić et al., 2020; Liu et al., 2021), with an aim of extracting high-quality static word embeddings from the parameters of the input LMs.",
      "startOffset" : 51,
      "endOffset" : 89
    }, {
      "referenceID" : 38,
      "context" : "Here, we focus on their lexical semantic knowledge (Vulić et al., 2020; Liu et al., 2021), with an aim of extracting high-quality static word embeddings from the parameters of the input LMs.",
      "startOffset" : 51,
      "endOffset" : 89
    }, {
      "referenceID" : 50,
      "context" : "Similar to prior work on sentence-level text inputs (Reimers and Gurevych, 2019), for each input word pair (w, v) we concatenate their d-dimensional encodings w and v (obtained after passing them through BERT and after pooling, see Figure 1) with their element-wise difference |w − v|.",
      "startOffset" : 52,
      "endOffset" : 80
    }, {
      "referenceID" : 7,
      "context" : "The multiple negatives ranking loss (MNEG) is inspired by prior work on learning universal sentence encoders (Cer et al., 2018; Henderson et al., 2019, 2020); the aim of the loss, now adapted to word-level inputs, is to rank true synonymy pairs from Psyn over randomly paired words.",
      "startOffset" : 109,
      "endOffset" : 157
    }, {
      "referenceID" : 68,
      "context" : "For simplicity, as negatives we use all pairings of wi with vj-s in the current batch where (wi, vj) 6∈ Psyn (Yang et al., 2018; Henderson et al., 2019).",
      "startOffset" : 109,
      "endOffset" : 152
    }, {
      "referenceID" : 27,
      "context" : "For simplicity, as negatives we use all pairings of wi with vj-s in the current batch where (wi, vj) 6∈ Psyn (Yang et al., 2018; Henderson et al., 2019).",
      "startOffset" : 109,
      "endOffset" : 152
    }, {
      "referenceID" : 65,
      "context" : "For further technical details we refer the reader to the original paper (Wang et al., 2019).",
      "startOffset" : 72,
      "endOffset" : 91
    }, {
      "referenceID" : 61,
      "context" : "The extraction of static type-level vectors from any underlying Transformer-based LM, both before and after LEXFIT fine-tuning, is guided by best practices from recent comparative analyses and probing work (Vulić et al., 2020; Bommasani et al., 2020).",
      "startOffset" : 206,
      "endOffset" : 250
    }, {
      "referenceID" : 5,
      "context" : "The extraction of static type-level vectors from any underlying Transformer-based LM, both before and after LEXFIT fine-tuning, is guided by best practices from recent comparative analyses and probing work (Vulić et al., 2020; Bommasani et al., 2020).",
      "startOffset" : 206,
      "endOffset" : 250
    }, {
      "referenceID" : 61,
      "context" : "Our language selection for evaluation is guided by the following (partially clashing) constraints (Vulić et al., 2020): a) availability of comparable pretrained monolingual LMs; b) task and evaluation data availabil-",
      "startOffset" : 98,
      "endOffset" : 118
    }, {
      "referenceID" : 69,
      "context" : "We use the standard collection of EN lexical constraints from previous work on (static) word vector specialization (Zhang et al., 2014; Ono et al., 2015; Vulić et al., 2018; Ponti et al., 2018, 2019).",
      "startOffset" : 115,
      "endOffset" : 199
    }, {
      "referenceID" : 45,
      "context" : "We use the standard collection of EN lexical constraints from previous work on (static) word vector specialization (Zhang et al., 2014; Ono et al., 2015; Vulić et al., 2018; Ponti et al., 2018, 2019).",
      "startOffset" : 115,
      "endOffset" : 199
    }, {
      "referenceID" : 62,
      "context" : "We use the standard collection of EN lexical constraints from previous work on (static) word vector specialization (Zhang et al., 2014; Ono et al., 2015; Vulić et al., 2018; Ponti et al., 2018, 2019).",
      "startOffset" : 115,
      "endOffset" : 199
    }, {
      "referenceID" : 17,
      "context" : "It covers the lexical relations from WordNet (Fellbaum, 1998) and Roget’s Thesaurus (Kipfer, 2009); it comprises 1,023,082 synonymy (Psyn) word pairs and 380,873 antonymy pairs (Pant).",
      "startOffset" : 45,
      "endOffset" : 61
    }, {
      "referenceID" : 33,
      "context" : "It covers the lexical relations from WordNet (Fellbaum, 1998) and Roget’s Thesaurus (Kipfer, 2009); it comprises 1,023,082 synonymy (Psyn) word pairs and 380,873 antonymy pairs (Pant).",
      "startOffset" : 84,
      "endOffset" : 98
    }, {
      "referenceID" : 50,
      "context" : "The implementation is based on the SBERT framework (Reimers and Gurevych, 2019), using the suggested settings: AdamW (Loshchilov and Hutter, 2018); learning rate of 2e − 5; weight decay rate of 0.",
      "startOffset" : 51,
      "endOffset" : 79
    }, {
      "referenceID" : 40,
      "context" : "The implementation is based on the SBERT framework (Reimers and Gurevych, 2019), using the suggested settings: AdamW (Loshchilov and Hutter, 2018); learning rate of 2e − 5; weight decay rate of 0.",
      "startOffset" : 117,
      "endOffset" : 146
    }, {
      "referenceID" : 28,
      "context" : "Task 1: Lexical semantic similarity (LSIM) is an established intrinsic task for evaluating static WEs (Hill et al., 2015).",
      "startOffset" : 102,
      "endOffset" : 121
    }, {
      "referenceID" : 61,
      "context" : "We use the recent comprehensive multilingual LSIM benchmark MultiSimLex (Vulić et al., 2020), which comprises 1,888 pairs in 13 languages, for our EN, ES, FI, PL, and RU LSIM evaluation.",
      "startOffset" : 72,
      "endOffset" : 92
    }, {
      "referenceID" : 19,
      "context" : "We also evaluate on a verbfocused EN LSIM benchmark: SimVerb-3500 (SV) (Gerz et al., 2016), covering 3,500 verb pairs, and SimLex-999 (SL) for DE and IT (999 pairs) (Leviant and Reichart, 2015).",
      "startOffset" : 71,
      "endOffset" : 90
    }, {
      "referenceID" : 35,
      "context" : ", 2016), covering 3,500 verb pairs, and SimLex-999 (SL) for DE and IT (999 pairs) (Leviant and Reichart, 2015).",
      "startOffset" : 82,
      "endOffset" : 110
    }, {
      "referenceID" : 52,
      "context" : "Task 2: Bilingual Lexicon Induction (BLI), a standard task to assess the “semantic quality” of static cross-lingual word embeddings (CLWEs) (Ruder et al., 2019), enables investigations on the alignability of monolingual type-level WEs in different languages before and after the LEXFIT procedure.",
      "startOffset" : 140,
      "endOffset" : 160
    }, {
      "referenceID" : 41,
      "context" : "We learn CLWEs from monolingual WEs obtained with all WE methods using the established and supervision-lenient mapping-based approach (Mikolov et al., 2013a; Smith et al., 2017) with the VECMAP framework (Artetxe et al.",
      "startOffset" : 134,
      "endOffset" : 177
    }, {
      "referenceID" : 56,
      "context" : "We learn CLWEs from monolingual WEs obtained with all WE methods using the established and supervision-lenient mapping-based approach (Mikolov et al., 2013a; Smith et al., 2017) with the VECMAP framework (Artetxe et al.",
      "startOffset" : 134,
      "endOffset" : 177
    }, {
      "referenceID" : 0,
      "context" : ", 2017) with the VECMAP framework (Artetxe et al., 2018).",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 22,
      "context" : ", synonymy, antonymy, hypernymy, meronymy, plus no relation) via a state-ofthe-art neural model for RELP which learns solely based on input type-level WEs (Glavaš and Vulić, 2018).",
      "startOffset" : 155,
      "endOffset" : 179
    }, {
      "referenceID" : 11,
      "context" : "For EN–ES, we run experiments on MUSE data (Conneau et al., 2018).",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 46,
      "context" : ", specialized terms, less-frequent words) with their simpler in-context synonyms, while retaining grammaticality and conveying the same meaning as the more complex input text (Paetzold and Specia, 2017).",
      "startOffset" : 175,
      "endOffset" : 202
    }, {
      "referenceID" : 22,
      "context" : ", synonymy injected via LEXFIT) and broader relatedness is critical for LexSIMP (Glavaš and Vulić, 2018).",
      "startOffset" : 80,
      "endOffset" : 104
    }, {
      "referenceID" : 21,
      "context" : "1) We use Light-LS (Glavaš and Štajner, 2015), a languageagnostic LexSIMP tool that makes simplifications in an unsupervised way based solely on word similarity in an input (static) WE space; 2) we rely on standard LexSIMP benchmarks, available for EN (Horn et al.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 29,
      "context" : "1) We use Light-LS (Glavaš and Štajner, 2015), a languageagnostic LexSIMP tool that makes simplifications in an unsupervised way based solely on word similarity in an input (static) WE space; 2) we rely on standard LexSIMP benchmarks, available for EN (Horn et al., 2014), IT (Tonelli et al.",
      "startOffset" : 252,
      "endOffset" : 271
    }, {
      "referenceID" : 59,
      "context" : ", 2014), IT (Tonelli et al., 2016), and ES (Saggion, 2017); and 3) we report the standard Accuracy scores (Horn et al.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 53,
      "context" : ", 2016), and ES (Saggion, 2017); and 3) we report the standard Accuracy scores (Horn et al.",
      "startOffset" : 16,
      "endOffset" : 31
    }, {
      "referenceID" : 29,
      "context" : ", 2016), and ES (Saggion, 2017); and 3) we report the standard Accuracy scores (Horn et al., 2014).",
      "startOffset" : 79,
      "endOffset" : 98
    }, {
      "referenceID" : 61,
      "context" : "The results over all 4 tasks indicate that static WEs from LEXFITed monolingual BERT 1) outperform traditional WE methods such as FT, and 2) offer also large gains over WEs originating from nonLEXFITed BERTs (Vulić et al., 2020).",
      "startOffset" : 208,
      "endOffset" : 228
    }, {
      "referenceID" : 52,
      "context" : "(10)RELP and LexSIMP use WEs as input features of neural architectures; LSIM and BLI fall under similarity-based evaluation tasks (Ruder et al., 2019).",
      "startOffset" : 130,
      "endOffset" : 150
    }, {
      "referenceID" : 6,
      "context" : ", extracted from multilingual WordNets (Bond and Foster, 2013), PanLex (Kamholz et al.",
      "startOffset" : 39,
      "endOffset" : 62
    }, {
      "referenceID" : 32,
      "context" : ", extracted from multilingual WordNets (Bond and Foster, 2013), PanLex (Kamholz et al., 2014), or BabelNet (Ehrmann et al.",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 14,
      "context" : "First, they suggest that the pretrained LMs store even more lexical knowledge than thought previously (Ethayarajh, 2019; Bommasani et al., 2020; Vulić et al., 2020); the role of LEXFIT finetuning is simply to ‘rewire’ and expose that knowledge from the LM through (limited) lexical-level supervision.",
      "startOffset" : 102,
      "endOffset" : 164
    }, {
      "referenceID" : 5,
      "context" : "First, they suggest that the pretrained LMs store even more lexical knowledge than thought previously (Ethayarajh, 2019; Bommasani et al., 2020; Vulić et al., 2020); the role of LEXFIT finetuning is simply to ‘rewire’ and expose that knowledge from the LM through (limited) lexical-level supervision.",
      "startOffset" : 102,
      "endOffset" : 164
    }, {
      "referenceID" : 61,
      "context" : "First, they suggest that the pretrained LMs store even more lexical knowledge than thought previously (Ethayarajh, 2019; Bommasani et al., 2020; Vulić et al., 2020); the role of LEXFIT finetuning is simply to ‘rewire’ and expose that knowledge from the LM through (limited) lexical-level supervision.",
      "startOffset" : 102,
      "endOffset" : 164
    }, {
      "referenceID" : 42,
      "context" : "Our results indicate that ‘classic’ WE models such as skip-gram (Mikolov et al., 2013b) and FT are undermined even in their last field of use, lexical tasks.",
      "startOffset" : 64,
      "endOffset" : 87
    }, {
      "referenceID" : 3,
      "context" : "This comes as a natural finding, given that word2vec and FT can in fact be seen as reduced and training-efficient variants of full-fledged language models (Bengio et al., 2003).",
      "startOffset" : 155,
      "endOffset" : 176
    }, {
      "referenceID" : 20,
      "context" : "This finding is well aligned with the recent findings demonstrating that task-specific pretraining results in stronger (sentence-level) task performance (Glass et al., 2020; Henderson et al., 2020; Lewis et al., 2020).",
      "startOffset" : 153,
      "endOffset" : 217
    }, {
      "referenceID" : 26,
      "context" : "This finding is well aligned with the recent findings demonstrating that task-specific pretraining results in stronger (sentence-level) task performance (Glass et al., 2020; Henderson et al., 2020; Lewis et al., 2020).",
      "startOffset" : 153,
      "endOffset" : 217
    }, {
      "referenceID" : 37,
      "context" : "This finding is well aligned with the recent findings demonstrating that task-specific pretraining results in stronger (sentence-level) task performance (Glass et al., 2020; Henderson et al., 2020; Lewis et al., 2020).",
      "startOffset" : 153,
      "endOffset" : 217
    }, {
      "referenceID" : 34,
      "context" : ", word senses and relations) into large LMs (Lauscher et al., 2020; Levine et al., 2020) relied on joint LM (re)training from scratch: it is effectively costlier than training the original BERT models.",
      "startOffset" : 44,
      "endOffset" : 88
    }, {
      "referenceID" : 36,
      "context" : ", word senses and relations) into large LMs (Lauscher et al., 2020; Levine et al., 2020) relied on joint LM (re)training from scratch: it is effectively costlier than training the original BERT models.",
      "startOffset" : 44,
      "endOffset" : 88
    }, {
      "referenceID" : 57,
      "context" : "as well, offering improved alignability (Søgaard et al., 2018) between monolingual WEs.",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 60,
      "context" : "High scores with FI in LSIM and BLI are aligned with prior work (Virtanen et al., 2019; Rust et al., 2021) that showcased strong monolingual performance of FI BERT in sentence-level tasks.",
      "startOffset" : 64,
      "endOffset" : 106
    }, {
      "referenceID" : 25,
      "context" : "We confirm this hypothesis with another experiment where our input LM is the same BERT Base architecture parameters with the same subword vocabulary as English BERT, but with its parameters now randomly initialized using the Xavier initialization (Glorot and Bengio, 2010).",
      "startOffset" : 247,
      "endOffset" : 272
    }, {
      "referenceID" : 38,
      "context" : "Another line of recent work (Liu et al., 2021) further corroborates our findings.",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 12,
      "context" : "Prior work indicated that massively multilingual LMs such as multilingual BERT (mBERT) (Devlin et al., 2019) and XLM-R (Conneau et al.",
      "startOffset" : 87,
      "endOffset" : 108
    }, {
      "referenceID" : 10,
      "context" : ", 2019) and XLM-R (Conneau et al., 2020) cannot match the performance of their language-specific counterparts in both lexical (Vulić et al.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 61,
      "context" : ", 2020) cannot match the performance of their language-specific counterparts in both lexical (Vulić et al., 2020) and sentence-level tasks (Rust et al.",
      "startOffset" : 93,
      "endOffset" : 113
    }, {
      "referenceID" : 10,
      "context" : "Storing the knowledge of 100+ languages in its limited parameter budget, mBERT still cannot capture monolingual knowledge as accurately as language-specific BERTs (Conneau et al., 2020).",
      "startOffset" : 163,
      "endOffset" : 185
    }, {
      "referenceID" : 1,
      "context" : "However, we believe that its performance with LEXFIT may be further improved by leveraging recently proposed multilingual LM adaptation strategies that mitigate a mismatch between shared multilingual and language-specific vocabularies (Artetxe et al., 2020; Chung et al., 2020; Pfeiffer et al., 2020); we leave this for future work.",
      "startOffset" : 235,
      "endOffset" : 300
    }, {
      "referenceID" : 9,
      "context" : "However, we believe that its performance with LEXFIT may be further improved by leveraging recently proposed multilingual LM adaptation strategies that mitigate a mismatch between shared multilingual and language-specific vocabularies (Artetxe et al., 2020; Chung et al., 2020; Pfeiffer et al., 2020); we leave this for future work.",
      "startOffset" : 235,
      "endOffset" : 300
    }, {
      "referenceID" : 47,
      "context" : "However, we believe that its performance with LEXFIT may be further improved by leveraging recently proposed multilingual LM adaptation strategies that mitigate a mismatch between shared multilingual and language-specific vocabularies (Artetxe et al., 2020; Chung et al., 2020; Pfeiffer et al., 2020); we leave this for future work.",
      "startOffset" : 235,
      "endOffset" : 300
    }, {
      "referenceID" : 58,
      "context" : "A consensus in prior work (Tenney et al., 2019; Ethayarajh, 2019; Vulić et al., 2020) points that out-of-context lexical knowledge in pretrained LMs is typically stored in bottom Transformer layers (see Table 5).",
      "startOffset" : 26,
      "endOffset" : 85
    }, {
      "referenceID" : 14,
      "context" : "A consensus in prior work (Tenney et al., 2019; Ethayarajh, 2019; Vulić et al., 2020) points that out-of-context lexical knowledge in pretrained LMs is typically stored in bottom Transformer layers (see Table 5).",
      "startOffset" : 26,
      "endOffset" : 85
    }, {
      "referenceID" : 61,
      "context" : "A consensus in prior work (Tenney et al., 2019; Ethayarajh, 2019; Vulić et al., 2020) points that out-of-context lexical knowledge in pretrained LMs is typically stored in bottom Transformer layers (see Table 5).",
      "startOffset" : 26,
      "endOffset" : 85
    }, {
      "referenceID" : 14,
      "context" : "Besides inducing better static WEs for lexical tasks, following the line of lexical probing work (Ethayarajh, 2019; Vulić et al., 2020), our goal in this work was to understand how (and how much) lexical semantic knowledge is coded in pretrained LMs, and how to ‘unlock’ the knowledge from the LMs.",
      "startOffset" : 97,
      "endOffset" : 135
    }, {
      "referenceID" : 61,
      "context" : "Besides inducing better static WEs for lexical tasks, following the line of lexical probing work (Ethayarajh, 2019; Vulić et al., 2020), our goal in this work was to understand how (and how much) lexical semantic knowledge is coded in pretrained LMs, and how to ‘unlock’ the knowledge from the LMs.",
      "startOffset" : 97,
      "endOffset" : 135
    }, {
      "referenceID" : 54,
      "context" : "We hope that our work will be beneficial for all lexical tasks where static WEs from traditional WE models are still largely used (Schlechtweg et al., 2020; Kaiser et al., 2021).",
      "startOffset" : 130,
      "endOffset" : 177
    }, {
      "referenceID" : 30,
      "context" : "We hope that our work will be beneficial for all lexical tasks where static WEs from traditional WE models are still largely used (Schlechtweg et al., 2020; Kaiser et al., 2021).",
      "startOffset" : 130,
      "endOffset" : 177
    }, {
      "referenceID" : 66,
      "context" : ", in the SOFTMAX variant), and hard (instead of random) negative examples (Wieting et al., 2015; Mrkšić et al., 2017; Lauscher et al., 2020; Kalantidis et al., 2020).",
      "startOffset" : 74,
      "endOffset" : 165
    }, {
      "referenceID" : 43,
      "context" : ", in the SOFTMAX variant), and hard (instead of random) negative examples (Wieting et al., 2015; Mrkšić et al., 2017; Lauscher et al., 2020; Kalantidis et al., 2020).",
      "startOffset" : 74,
      "endOffset" : 165
    }, {
      "referenceID" : 34,
      "context" : ", in the SOFTMAX variant), and hard (instead of random) negative examples (Wieting et al., 2015; Mrkšić et al., 2017; Lauscher et al., 2020; Kalantidis et al., 2020).",
      "startOffset" : 74,
      "endOffset" : 165
    }, {
      "referenceID" : 31,
      "context" : ", in the SOFTMAX variant), and hard (instead of random) negative examples (Wieting et al., 2015; Mrkšić et al., 2017; Lauscher et al., 2020; Kalantidis et al., 2020).",
      "startOffset" : 74,
      "endOffset" : 165
    } ],
    "year" : 2021,
    "abstractText" : "Transformer-based language models (LMs) pretrained on large text collections implicitly store a wealth of lexical semantic knowledge, but it is non-trivial to extract that knowledge effectively from their parameters. Inspired by prior work on semantic specialization of static word embedding (WE) models, we show that it is possible to expose and enrich lexical knowledge from the LMs, that is, to specialize them to serve as effective and universal “decontextualized” word encoders even when fed input words “in isolation” (i.e., without any context). Their transformation into such word encoders is achieved through a simple and efficient lexical fine-tuning procedure (termed LEXFIT) based on dual-encoder network structures. Further, we show that LEXFIT can yield effective word encoders even with limited lexical supervision and, via cross-lingual transfer, in different languages without any readily available external knowledge. Our evaluation over four established, structurally different lexical-level tasks in 8 languages indicates the superiority of LEXFIT-based WEs over standard static WEs (e.g., fastText) and WEs from vanilla LMs. Other extensive experiments and ablation studies further profile the LEXFIT framework, and indicate best practices and performance variations across LEXFIT variants, languages, and lexical tasks, also directly questioning the usefulness of traditional WE models in the era of large neural models.",
    "creator" : "LaTeX with hyperref"
  }
}