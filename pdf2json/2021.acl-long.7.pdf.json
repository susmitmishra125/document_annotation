{
  "name" : "2021.acl-long.7.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "PENS: A Dataset and Generic Framework for Personalized News Headline Generation",
    "authors" : [ "Xiang Ao", "Xiting Wang", "Ling Luo", "Ying Qiao", "Qing He", "Xing Xie" ],
    "emails" : [ "aoxiang@ict.ac.cn", "luoling18s@ict.ac.cn", "heqing@ict.ac.cn", "xitwan@microsoft.com", "yiqia@microsoft.com", "xing.xie@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 82–92\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n82\nIn this paper, we formulate the personalized news headline generation problem whose goal is to output a user-specific title based on both a user’s reading interests and a candidate news body to be exposed to her. To build up a benchmark for this problem, we publicize a large-scale dataset named PENS (PErsonalized News headlineS). The training set is collected from user impressions logs of Microsoft News, and the test set is manually created by hundreds of native speakers to enable a fair testbed for evaluating models in an offline mode. We propose a generic framework as a preparatory solution to our problem. At its heart, user preference is learned by leveraging the user behavioral data, and three kinds of user preference injections are proposed to personalize a text generator and establish personalized headlines. We investigate our dataset by implementing several state-of-the-art user modeling methods in our framework to demonstrate a benchmark score for the proposed dataset. The dataset is available at https: //msnews.github.io/pens.html."
    }, {
      "heading" : "1 Introduction",
      "text" : "News headline generation (Dorr et al., 2003; Lopyrev, 2015; Alfonseca et al., 2013; Tan et al., 2017; See et al., 2017; Zhang et al., 2018; Xu et al., 2019; Murao et al., 2019; Gavrilov et al., 2019; Gu et al., 2020; Song et al., 2020), conventionally considered as a paradigm of challenging text summarization task, has been extensively explored for decades. Their intuitive intention is to empower the model to output a condensed generalization, e.g., one sentence, of a news article.\nThe recent year escalation of online content vendors such as Google News, TopBuzz, and\n∗This work was done when Xiang was visiting MSRA supported by the MSRA Young Visiting Researcher Program.\n†Corresponding author.\netc (LaRocque, 2003) propels a new research direction that how to decorate the headline as an irresistible invitation to users for reading through the article (Xu et al., 2019) since more readings may acquaint more revenue of these platforms. To this end, specified stylized headline generation techniques were proposed, such as question headline (Zhang et al., 2018), sensational headline (Xu et al., 2019) generation, and so on (Shu et al., 2018; Gu et al., 2020). However, the over-decorate headlines might bring negative effects as click-baits begin to become notorious in ubiquitous online services1.\nHence, the question is now changing to how to construct a title that catches on reader curiosity without entering into click-bait territory. Inspired by the tremendous success of personalized news recommendation (An et al., 2019; Wang et al., 2018; Li et al., 2010; Zheng et al., 2018) where the ultimate goal is to learn users’ reading interests and deliver the right news to them, a plausible solution to this question could be producing headlines satisfying the personalized interests of readers.\nIt thus motivates the study of the personalized news headline generation whose goal is to output a user-specific title based on both a user’s reading interests and a candidate news body to be exposed to her. Analogous to personalized news recommendations, user preference can be learned by leveraging the behavioral data of readers on content vendors, and the representation could personalize text generators and establish distinct headlines, even with the same news body, for different readers.\nHowever, it might be difficult to evaluate the approaches of personalized headline generation due to the lack of large-scale available datasets. First, there are few available benchmarks that simultaneously contain user behavior and news content to train models. For example, most available news rec-\n1https://www.vizion.com/blog/ do-clickbait-titles-still-work/\nommendation datasets may predominately contain user-side interaction data, e.g., exposure impressions and click behaviors, but the textual features usually have already been overly pre-processed (Li et al., 2010; Zheng et al., 2018). As a result, advanced NLP techniques that extract useful features from textual data are limited. News headline generation datasets, on the other hand, usually consist of news bodies as well as their headlines, which all come from the news-side (Tan et al., 2017; Zhang et al., 2018) rather than the user-side. Though the MIND dataset (Wu et al., 2020), which was presented by Microsoft, simultaneously contains the user-side behavioral data and the news-side original textual data, it was constructed for personalized news recommendations rather than our problem. The more challenging issue for evaluating personalized headline generation approaches is the severe cost during the test phase. It could be intractable and infeasible to do an A/B test for every model in online environments. An efficient and fair testbed to evaluate the models in an offline mode is in urgent demand to make the effectiveness and reproducibility of proposed models comparable.\nTo this end, we publicize a dataset named PENS (PErsonalized News headlineS) in this paper as a benchmark to testify the performance of personalized news headline generation approaches. The training set of PENS is collected from the user impression logs of Microsoft News2, in which 500, 000 impressions over 445, 765 users on more than one hundred thousand English news articles are provided. In addition, we collected 103 English native speakers’ click behaviors as well as their more than 20, 000 manually-crafted personalized headlines of news articles on the same news corpus for testing. These manually-written headlines are regarded as the gold standard of the user-preferred titles. Then, proposed methods can take prevailing matching metrics, e.g., ROUGE, BLEU and etc., to verify the performance.\nMoreover, we propose a generic framework to inject personalized interests into a proposed neural headline generator to enable a beacon for this area, considering there are few existing works that can generate personalized news headlines. In more detail, we devise three kinds of incorporation methods to inject user interest representation into a proposed neural headline generator with a transformer-based encoder and a pointer network-based (See et al.,\n2https://microsoftnews.msn.com\n2017) decoder. We implement six state-of-the-arts personalized news recommendation approaches to model user preferences and provide a horizontal standard for the PENS dataset. The experimental results show effective personalization modeling and comprehensive injection of user interests can underpin an improvement in the quality of personalized news headline generation. We expect PENS can serve as a benchmark for personalized headline generation and bolster the research in this area."
    }, {
      "heading" : "2 Problem Formulation and Discussion",
      "text" : "In this section, we formulate the problem of personalized news headline generation and differentiate it from personalized news recommendations."
    }, {
      "heading" : "2.1 Problem Formulation",
      "text" : "The problem of personalized news headline generation is formulated as follows. Given a user u on an online content vendor, we denote his past click history as [cu1 , c u 2 , . . . , c u N ] where each c represents the headline of user u’s clicked news and each headline is composed of a sequence of words c = [wc1 , . . . , wcT ] with the maximum length of T . Then, given the news body of a piece of news v = [wv1 , . . . , wvn ] to be exposed to user u, our problem is to generate a personalized news headline Huv = [y u v1 , . . . , y u vT ] based on the clicked news [cu1 , c u 2 , . . . , c u N ] and v."
    }, {
      "heading" : "2.2 Difference to Personalized News Recommendation",
      "text" : "Here we differentiate our problem from personalized news recommendation whose general framework is shown as Fig. 1.\nRecall that the aim of personalized news recommendation is computing and matching between the candidate news and the user’s interests. Hence,\nlearning accurate news and user representations is critical for this problem. Under the neural framework, the news representation is usually modeled by a news encoder that encodes news title, news body or other attributes via various neural structures (Okura et al., 2017; Wang et al., 2018; Wu et al., 2019a; An et al., 2019; Wu et al., 2019a). The user representation is generated by engraving the high-level aspects over their clicked news sequences using sequential (Okura et al., 2017; An et al., 2019) or attentive modules (Wu et al., 2019b,a), in which every news is encoded by the news encoder in advance. Finally, the two representations are matched by the click predictor, and the whole model is trained by the supervision of click signals.\nDifferent from personalized news recommendations, our personalized news headline generation could be regarded as an NLP task than a user modeling and matching problem. Although it similarly needs to model preferences for the individual users as what personalized news recommendations do, the output of our problem is a natural language sequence that the target user might be interested in, i.e., user-preferred news title, rather than a click probability score."
    }, {
      "heading" : "3 PENS Dataset",
      "text" : "In this section, we detail our PENS dataset. The dataset was randomly sampled impression logs of Microsoft News from June 14 to July 12, 2019. Both user behaviors and news contents are involved, and each user was de-linked from the production system when securely hashed into an anonymous ID to reserve the data privacy issues."
    }, {
      "heading" : "3.1 News Corpus",
      "text" : "The PENS dataset contains 113, 762 pieces of news articles whose topics are distributed into 15 categories. The topical distribution is demonstrated in Fig. 2 (c). Each news article in the PENS dataset includes a news ID, a title, a body and a category label. The average length of news title and news body is 10.5 and 549.0, individually. Moreover, we extract entities from each news title and body and link them to the entities in WikiData3. It could be taken as an auxiliary source to facilitate knowledgeaware personalization modeling and headline generation. The key statistical information of the PENS dataset is exhibited in Fig. 2 (a)–(e)."
    }, {
      "heading" : "3.2 Training Set",
      "text" : "The training set of PENS consists of impression logs. An impression log records the news articles displayed to a user as well as the click behaviors on these news articles when he/she visits the news website homepage at a specific time. We follow the MIND dataset (Wu et al., 2020) that we add the news click histories of every individual user to his/her impression log to offer labeled samples for learning user preferences. Hence, the format of each labeled sample in our training set is [uID , tmp, clkNews, uclkNews, clkedHis], where uID indicates the anonymous ID of a user, tmp denotes the timestamp of this impression record. clkNews and uclkNews are the clicked news and un-clicked news in this impression, respectively. clkedHis represents the news articles previously clicked by this user. All the samples in clkNews , uclkNews and clkedHis are news IDs, and they all sort by the user’s click time. The histogram of the number of news in the clicked history per user is shown in Fig. 2 (f)."
    }, {
      "heading" : "3.3 Test Set",
      "text" : "To provide an offline testbed, we invited 103 English native speakers (all are college students) man-\n3https://www.wikidata.org/wiki/Wikidata:MainPage\nually create a test set by two stages. At the first stage, each person browses 1, 000 news headlines and marks at least 50 pieces he/she is interested in. These exhibited news headlines were randomly selected from our news corpus and were arranged by their first exposure time. At the second stage, everyone is asked to write down their preferred headlines for another 200 news articles from our corpus, without exhibiting them the original news titles. Note that these news articles are excluded from the first stage, and only news bodies were exhibited to these annotators in this stage. These news articles are evenly sampled, and we redundantly assign them to make sure each news is exhibited to four people on average. The quality of these manually-written headlines was checked by professional editors from the perspective of the factual aspect of the media frame (Wagner and Gruszczynski, 2016). Lowquality headlines, e.g. containing wrong factual information, inconsistent with the news body, tooshort or overlong, etc., are removed. The rest are regarded as the personalized reading focuses of these annotators on the articles and are taken as gold-standard headlines in our dataset. The statistics of the training and test sets of the PENS are shown in Table 1."
    }, {
      "heading" : "4 Our Framework",
      "text" : "In this section, we illustrate our generic framework for resolving personalized news headline generation, and its key issue is how to inject the user preference into a news headline generator. We devise a headline generator with a transformer encoder and a pointer network decoder as our base model and propose three kinds of manners of injecting the user interests to generate personalized headlines. The user interests can be derived following the approaches in news recommendations community, and we omit its details due to the space limitation. The architecture of our proposed framework is shown as Figure 3.\ninjections are devised. 1 : utilizing user embedding to initialize the decoder’s hidden state of the headline generator. 2 : personalizing the attentive values on words in the news body by the user embedding. 3 : perturbing the choice between generation and copying via the user embedding."
    }, {
      "heading" : "4.1 Headline Generator",
      "text" : "The pin-point of our proposed headline generator is a variant of transformer encoder and pointer network decoder. During the encoding, given the news body of a candidate news v = [wv1 , . . . , wvn ], its word embeddings [ev1 , . . . , evn ] ∈ Rdw are first fed to a two-layer positional encoder. The first layer aims to enhance the word structure within the whole news body sequence following Vaswani et al. (2017), and we add the positional encoding to each embedding vector with,\nPE (pos,2i) = sin(pos/10000 2i/dw ) (1)\nPE (pos,2i+1) = cos(pos/10000 2i/dw ) (2)\nwhere pos is the word position and i is the dimension. We also apply a sentence-layer positional encoding to discover structural relations from higher level. Suppose the Wpos ∈ RL×ds represents the position embedding matrix of sentence level where L is the sentence length and ds is the embedding size, the l-th row of Wpos represents the positional embedding of all the words in the l-th sentence. Thus, each word embedding e′pos with positional\ninformation can be represented as:\ne′pos = (epos + PEpos)⊕Wpos[l]. (3)\nwhere ⊕ means concatenation. Furthermore, multihead self-attention mechanism (Vaswani et al., 2017) is adopted to capture the word and sentence interactions by,\nhi = softmax( E′WQi (E ′WKi ) >\n√ dk\n)E′WVi (4)\nwhere dk = ds+dwk and i = 1, . . . , k given k heads. WQi ,W K i ,W V i ∈ R(ds+dw)×dk . E′ represents the word sequence embeddings in candidate news v. Thus, the encoder hidden states h = h1 ⊕ h2, . . . , hk can be derived.\nDuring the process of decoding, the decoded hidden state st at time step t can be derived after given the input xt, and an attention distribution at over the encoder hidden states h is calculated as,\nat = Fθ(h, st) (5)\nFθ(h, st) = softmax(V >atttanh(Whh+Wsst + batt)) (6)\nwhere Fθ represents a function template parameterized by θ to combine the linear transformation of the encoder and the decoder states, i.e., h and st. Next, the context vector ct, which can be seen as a fixed-size representation read from the news body at time step t, is computed by a weighted sum of the encoder hidden states over the attention distribution. Then the vocabulary distribution is produced by,\nPvocab(wt) = tanh(Vp[st; ct] + bv), (7)\nwhere Vp and bv are learnable parameters while Pvocab(wt) represents the probability distribution over all the words in the vocabulary to predict the word at time step t.\nInspired by pointer-generator network (See et al., 2017), which exhibits desirable performance on either dealing with out-of-vocabulary (OOV) words or improving the reproducing factual details with copy mechanism, we adopt a pointer ptgen at decoding step t as a soft switch to choose between generating a word from the vocabulary with a probability of Pvocab(wt) or copying a word from the news body sampling from the attention distribution at. Thus, the probability distribution over the extended vocabulary is computed by,\nP (wt) = p t genPvocab(wt) + (1− ptgen) ∑ j:wj=wt at,j (8)\nwhere Pvocab(wt) is zero when wt is out of vocabulary while ∑ j:wj=wt\nat,j = 0 when the wt is not in the news body. ptgen is calculated based on the context vector ct, decoder state st and the decoder input xt:\nptgen = Tθ(ct, st, xt), (9)\nwhere Tθ is a function template as Eq. (6)."
    }, {
      "heading" : "4.2 Personalization by Injecting User Interests",
      "text" : "So far, the imperative issue is to personalize the headline generator by injecting the user’s preference. Recall that we can obtain user embedding indicating user’s reading interests based on his/her historical clicked news sequences, and we denote such representation as u. As the user embedding u is usually not aligned with the word embeddings, it remains challenges to incorporate the user interests to influence the headline generation with personalized information.\nIn our framework, based on our headline generator, we propose three different manners to inject user interests, considering different intuitions, and they are exhibited in Fig. 3. First, the most simple and intuitive choice is to utilize the user embedding u to initialize the decoder hidden state of the headline generator. Second, under the empirical assumption that users may attend on different paragraphs and words in news articles corresponding to their individual preference, we inject u to affect the attention distribution at in order to personalize the attentive values on the different words in the news body. That is, we modify Eq. (5) and derive at = Fθ(h, st, u). Lastly, we incorporate the personalized information to perturb the choice between generating a word from vocabulary or copying a word from the news body, and derive ptgen = Tθ(ct, st, xt, u). Compared with Eq. (9), u is taken as an auxiliary parameter, where Tθ is also a function template as Eq. (6)."
    }, {
      "heading" : "4.3 Training",
      "text" : "In this subsection, we present the training process of our framework. The headline generation can be considered as a sequential decision-making process, hence we optimize a θ parametrized policy for the generator by maximizing the expected reward of generated headline Y1:T :\nEY1:T∼Gθ [R(Y1:T )]. (10)\nFor the generator, policy gradient methods are applied to maximize the objective function in Eq. (10),\nwhose gradient can be derived as,\n∇θJ(θ) ' Eyt∼Gθ(yt|Y1:t−1) [∇θ logGθ(yt|Y1:t−1) ·R(Y1:t−1, yt)]\n(11)\nwhere the reward R is estimated by the degree of personalization, fluency and factualness as we aim to generate a user-specific and coherent headline to cover the main theme of news articles and arouse personalized reading curiosity. The implemented rewards in our framework contain: (1) The personalization of the generated headline is measured by the dot product between the user embedding and the generated headline representation. Such a score might imply a matching degree of personalization. (2) The fluency of a generated headline is assessed by a language model. We adopt a two-layer LSTM pre-trained by maximizing the likelihood of news body and consider the probability estimation of a generated headline as the fluency reward. (3) We measure the degree of factual consistency and the coverage by calculating the mean of ROUGE (Lin, 2004)-1, -2 and -L F-scores between each sentence in the news body and the generated headline, and then take the average of the top 3 scores as the reward. We average all three rewards as the final signal. As all the above reward functions only produce an end reward after the whole headline is generated, we apply a Monte Carlo Tree search to estimate the intermediate rewards."
    }, {
      "heading" : "5 Experimental Evaluation",
      "text" : "In this section, we investigate our proposed PENS dataset and conduct several comparisons to give benchmark scores of personalized headline generation on this dataset. In the following part, we will introduce the compared methods first, and then detail the experimental setup, and finally present the results and analysis."
    }, {
      "heading" : "5.1 Compared Methods",
      "text" : "We mainly compare two groups of approaches. The first group consists of various user modeling methods, which are all SOTA neural-based news recommendation methods: (1) EBNR (Okura et al., 2017) learns user representations by aggregating their browsed news with GRU. (2) DKN (Wang et al., 2018) is a deep knowledge-aware network for news recommendation. (3) NPA (Wu et al., 2019b) proposes personalized attention module in both news and user encoder. (4) NRMS (Wu et al., 2019c) conducts neural news recommendation with\nmulti-head self-attention. (5) LSTUR (An et al., 2019) models long- and shor-term user representations based on user ID embedding and sequential encoding, individually. (6) NAML (Wu et al., 2019a) proposes multi-view learning in user representation.\nTo the best of our knowledge, there are no exclusive methods for personalized news headline generation. Hence we take several headline generation methods for comparison. (1) PointerGen (See et al., 2017) proposes an explicit probabilistic switch to choose between copying from source text and generating word from vocabulary. (2) PG+RL-ROUGE (Xu et al., 2019) extends Pointer-Gen with as a reinforcement learning framework which generates sensational headlines by considering ROUGE-L score as rewards."
    }, {
      "heading" : "5.2 Experiment Setup",
      "text" : "We perform the following preprocessings. For each impression, we empirically keep at most 50 clicked news to learn user preferences, and set the length of news headline and news body to 30 and 500, respectively. Word embeddings are 300-dimension and initialized by the Glove (Pennington et al., 2014) while the size of position embeddings at sentence level is 100. The multi-head attention networks have 8 heads.\nFirst of all, we conduct news recommendation tasks to pretrain a user encoder with a learning rate of 10−4 on the first three weeks, i.e., from June 14 to July 4, 2019, on the training set, and test on the rest. Notice that the parameters of the user encoder are not updated thereafter. Meanwhile, the headline generator is also pretrained with a learning rate of 0.001 by maximizing the likelihood of original headlines based on a random but fixed user embedding which can be considered as a global user without personalized information. Next, we train each individual model for 2 epochs following Eq. 10, and Adam (Kingma and Ba, 2014) is used for model optimization where we sample 16 sequences for Monte Carlo search."
    }, {
      "heading" : "5.3 Evaluation Metrics",
      "text" : "For news recommendation evaluation, we report the average results in terms of AUC, MRR, nDCG@5 and nDCG@10. For personalized headline generation, we evaluate the generation quality using F1 ROUGE (Lin, 2004) 4 including unigram\n4We compute all ROUGE scores with parameters “-a -c 95 -m -n 4 -w 1.2.” Refer to\nTable 2: The overall performance of compared methods. “R-1, -2, -L” indicate F scores of ROUGE-1, -2, and -L,\nand “NA” denotes “Not Available”. “IM” means injection methods, c.f. 1 , 2 , and 3 in Fig. 3 for details.\nMethods MetricsAUC MRR NDCG@5 NDCG@10 IM ROUGE-1 ROUGE-2 ROUGE-L\nPointer-Gen NA NA NA NA NA 19.86 7.76 18.83 PG+RL-ROUGE NA NA NA NA NA 20.56 8.42 20.03\n1 25.13 9.03 20.73 EBNR 63.97 22.52 26.45 32.81 2 25.49 9.14 20.82\n3 24.62 8.95 20.40 1 25.97 9.23 20.92\nDKN 65.25 24.07 26.97 34.24 2 27.48 10.07 21.81 3 25.02 8.98 20.34 1 25.49 9.14 20.82 NPA 64.91 23.65 26.72 33.96 2 26.11 9.58 21.40 3 26.35 9.71 21.82 1 24.92 9.01 20.75 NRMS 64.27 23.28 26.60 33.58 2 26.15 9.37 21.03 3 25.41 9.12 20.91 1 23.71 8.73 21.13 LSTUR 62.49 22.69 24.71 32.28 2 24.10 8.82 20.73 3 23.11 8.42 20.38 1 27.49 10.14 21.62 NAML 66.18 25.51 27.56 35.17 2 28.01 10.72 22.24 3 27.25 10.01 21.40\nand bigram overlap (ROUGE-1 and ROUGE-2) to assess informativeness, and the longest common subsequence (ROUGE-L) to measure fluency. Here we adopt ROUGE because we care more about evaluating the recall of the generated results. All the reported values are the averaged results of 10 independently repeated runs."
    }, {
      "heading" : "5.4 Experimental Results",
      "text" : "Since we include six kinds of user modeling methods from personalized news recommendations and propose three ways of injecting user interests in our framework, we can derive 18 variants of approaches that can generate personalized news headlines. Meanwhile, there are two headline generation baselines, hence we totally have 20 methods for evaluation. The overall performance is illustrated in Table 2, and we have the following observations.\nFirst, we can see that every personalized news headline generation method can outperform nonpersonalized methods like PG. It might be that our proposed framework can generate personalized news headlines by incorporating user interests. Such personalized headlines are more similar to the manually-written ones, which are taken as gold-standard in our evaluation. Second, we find\nhttps://pypi.python.org/pypi/pyrouge/0.1.3\nthat user modeling makes a difference in generating personalized headlines. For instance, NAML achieves the best performance in news recommendation by learning news and user representations from multiple views, i.e., obtaining 66.18, 25.51, 27.56 and 35.17 on AUC, MRR NDCG@5 and NDCG@10. Then injecting the user preferences learned by NAML to the proposed headline generator also gets the highest ROUGE scores with either way of the incorporation. We conjecture it is because better user modeling methods can learn more rich personalized information from click behaviors, and well-learned user embeddings could strive to generate better-personalized headlines. Third, it is reported that the second way of injecting user interests gets the best performance on most of the user modeling methods, e.g., EBNR, DKN and NAML. It is probably because the differentiation of the attention distribution is intensified after the user embedding perturbation, which then impacts the word generation in the decoding process. However, it still remains a large room for explorations on better injecting user representations into the generation process since the second way seems to be defective at some time."
    }, {
      "heading" : "5.5 Case Study",
      "text" : "To further comprehend our task and the proposed framework, we demonstrate interesting cases from two representative methods, namely one nonpersonalized method Pointer-Gen (PG) and one personalized method NAML+HG which utilizes the second user interests injection (c.f. Fig. 3). We also exhibit the manually-written headlines by the users and the original news headline as references.\nFrom the results shown in Table 3, we can observe that generated headline by non-personalized method might omit some detailed but important information. We believe the reason is that PG is trained via supervised learning to maximize the loglikelihood of ground-truth news headlines. While our framework is trained via RL technique where coverage score is considered as an indicator to encourage the generation to be more complete. In addition, the exhibited cases show that our framework can produce user-specific news headlines in accordance with their individual interests reflected by historical click behaviors. Meanwhile, some key phrases in the personalized-written titles successfully appeared in the machine-generated headlines."
    }, {
      "heading" : "6 Related Work",
      "text" : "Headline generation has been considered as specialized text summarization (Luo et al., 2019; Jia et al., 2020), from which both extractive (Dorr et al., 2003; Alfonseca et al., 2013) and abstractive summarization (Sun et al., 2015; Takase et al., 2016; Tan et al., 2017; Gavrilov et al., 2019; See et al., 2017) approaches prevailed for decades. Extractive methods select a subset of actual sentences in original article, which may derive incoherent summary (Alfonseca et al., 2013). While abstractive models, basically falling in an encoderdecoder (Shen et al., 2017a; Murao et al., 2019)\nframework, can generate more condensed output based on the latent representation of news content. However, the nature of text summarization methods without considering interactions between news and users renders them ineffective in our personalized headline generation.\nRecently, stylized headlines generation were proposed to output eye-catching headlines by implicit style transfer (Shen et al., 2017b; Fu et al., 2018; Prabhumoye et al., 2018) or style-oriented supervisions (Shu et al., 2018; Zhang et al., 2018; Xu et al., 2019). However, either training a unified text style transfer model or constructing a personalized text style transfer model for every user is infeasible due to the complex personalized style-related patterns and the limited personalized-oriented examples. Meanwhile, these methods might suffer from the risk of entering into click-bait territory.\nPersonalized News Recommendation is also related to our problem. Among them, contentbased recommendations (Okura et al., 2017; Liu et al., 2010; Li et al., 2011; Lian et al., 2018; Wang et al., 2018; Wu et al., 2019a,b) perform user and news matching on a learned hidden space, and user representation is learned based on historical clicked news contents. It inspires us to personalize headline generator by incorporating user embeddings. Deep models (Lian et al., 2018; Wang et al., 2018; Wu et al., 2019b,a), recently, demonstrated significant improvements because of their capabilities in representation learning on both user-side and news-side data. Different from the efforts on personalized news recommendation, our work focuses on generating fascinating headlines for different users, which is orthogonal to existing work."
    }, {
      "heading" : "7 Conclusion and Future Work",
      "text" : "In this paper, we formulated the problem of personalized news headline generation. To provide an\noffline testbed for this problem, we constructed a dataset named PENS from Microsoft News. The news corpus of this dataset contains more than 100 thousand news articles over 15 topic categories. The training set constitutes of 500, 000 impressions of 445, 765 users to learn user interests and construct personalized news headline generator by distant supervisions. The test set was constructed by 103 annotators with their clicked behaviors and manually-written personalized news headlines. We propose a generic framework that injects user interests into an encoder-decoder headline generator in three different manners to resolve our problem. We compared both SOTA user modeling and headline generating approaches to present benchmark scores on the proposed dataset.\nFor future work, we first believe designing more complex and refined approaches to generated more diversified personalized news headlines will be interesting. More importantly, how to improve personalization while keeping factualness will be another interesting work, and it will propel the methods deployable in practical scenarios. Third, news headline personalization might burgeon the news content personalization, which is a more challenging but interesting open problem."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The research work is supported by the National Key Research and Development Program of China under Grant No. 2017YFB1002104, the National Natural Science Foundation of China under Grant No. 92046003, 61976204, U1811461. Xiang Ao is also supported by the Project of Youth Innovation Promotion Association CAS and Beijing Nova Program Z201100006820062."
    } ],
    "references" : [ {
      "title" : "HEADY: News headline abstraction through event pattern clustering",
      "author" : [ "Enrique Alfonseca", "Daniele Pighin", "Guillermo Garrido." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Alfonseca et al\\.,? 2013",
      "shortCiteRegEx" : "Alfonseca et al\\.",
      "year" : 2013
    }, {
      "title" : "Neural news recommendation with long- and short-term user representations",
      "author" : [ "Mingxiao An", "Fangzhao Wu", "Chuhan Wu", "Kun Zhang", "Zheng Liu", "Xing Xie." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "An et al\\.,? 2019",
      "shortCiteRegEx" : "An et al\\.",
      "year" : 2019
    }, {
      "title" : "Hedge trimmer: A parse-and-trim approach to headline generation",
      "author" : [ "Bonnie Dorr", "David Zajic", "Richard Schwartz." ],
      "venue" : "Proceedings of the HLT-NAACL 03 on Text Summarization Workshop - Volume 5, HLT-NAACL-DUC ’03, page 1–8, USA. Associa-",
      "citeRegEx" : "Dorr et al\\.,? 2003",
      "shortCiteRegEx" : "Dorr et al\\.",
      "year" : 2003
    }, {
      "title" : "Style transfer in text: Exploration and evaluation",
      "author" : [ "Zhenxin Fu", "Xiaoye Tan", "Nanyun Peng", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "AAAI, pages 663–670.",
      "citeRegEx" : "Fu et al\\.,? 2018",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2018
    }, {
      "title" : "Self-attentive model for headline generation",
      "author" : [ "Daniil Gavrilov", "Pavel Kalaidin", "Valentin Malykh." ],
      "venue" : "ECIR, pages 87–93.",
      "citeRegEx" : "Gavrilov et al\\.,? 2019",
      "shortCiteRegEx" : "Gavrilov et al\\.",
      "year" : 2019
    }, {
      "title" : "Generating representative headlines for news stories",
      "author" : [ "Xiaotao Gu", "Yuning Mao", "Jiawei Han", "Jialu Liu", "You Wu", "Cong Yu", "Daniel Finnie", "Hongkun Yu", "Jiaqi Zhai", "Nicholas Zukoski." ],
      "venue" : "WWW, pages 1773–1784.",
      "citeRegEx" : "Gu et al\\.,? 2020",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural extractive summarization with hierarchical attentive heterogeneous graph network",
      "author" : [ "Ruipeng Jia", "Yanan Cao", "Hengzhu Tang", "Fang Fang", "Cong Cao", "Shi Wang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Jia et al\\.,? 2020",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Heads you win: An easy guide to better headline and caption writing",
      "author" : [ "Paul LaRocque." ],
      "venue" : "Marion Street Press, Inc.",
      "citeRegEx" : "LaRocque.,? 2003",
      "shortCiteRegEx" : "LaRocque.",
      "year" : 2003
    }, {
      "title" : "Scene: a scalable two-stage personalized news recommendation system",
      "author" : [ "Lei Li", "Dingding Wang", "Tao Li", "Daniel Knox", "Balaji Padmanabhan." ],
      "venue" : "SIGIR, pages 125–134.",
      "citeRegEx" : "Li et al\\.,? 2011",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2011
    }, {
      "title" : "A contextual-bandit approach to personalized news article recommendation",
      "author" : [ "Lihong Li", "Wei Chu", "John Langford", "Robert E Schapire." ],
      "venue" : "WWW, pages 661–670.",
      "citeRegEx" : "Li et al\\.,? 2010",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2010
    }, {
      "title" : "Towards better representation learning for personalized news recommendation: a multi-channel deep fusion approach",
      "author" : [ "Jianxun Lian", "Fuzheng Zhang", "Xing Xie", "Guangzhong Sun." ],
      "venue" : "IJCAI, pages 3805–3811.",
      "citeRegEx" : "Lian et al\\.,? 2018",
      "shortCiteRegEx" : "Lian et al\\.",
      "year" : 2018
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text summarization branches out, pages 74–81.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Personalized news recommendation based on click behavior",
      "author" : [ "Jiahui Liu", "Peter Dolan", "Elin Rønby Pedersen." ],
      "venue" : "IUI, pages 31–40.",
      "citeRegEx" : "Liu et al\\.,? 2010",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2010
    }, {
      "title" : "Generating news headlines with recurrent neural networks",
      "author" : [ "Konstantin Lopyrev." ],
      "venue" : "arXiv preprint arXiv:1512.01712.",
      "citeRegEx" : "Lopyrev.,? 2015",
      "shortCiteRegEx" : "Lopyrev.",
      "year" : 2015
    }, {
      "title" : "Reading like HER: Human reading inspired extractive summarization",
      "author" : [ "Ling Luo", "Xiang Ao", "Yan Song", "Feiyang Pan", "Min Yang", "Qing He." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Luo et al\\.,? 2019",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2019
    }, {
      "title" : "A case study on neural headline generation for editing support",
      "author" : [ "Kazuma Murao", "Ken Kobayashi", "Hayato Kobayashi", "Taichi Yatsuka", "Takeshi Masuyama", "Tatsuru Higurashi", "Yoshimune Tabuchi." ],
      "venue" : "Proceedings of the 2019 Conference of the North",
      "citeRegEx" : "Murao et al\\.,? 2019",
      "shortCiteRegEx" : "Murao et al\\.",
      "year" : 2019
    }, {
      "title" : "Embedding-based news recommendation for millions of users",
      "author" : [ "Shumpei Okura", "Yukihiro Tagami", "Shingo Ono", "Akira Tajima." ],
      "venue" : "KDD, pages 1933–1942.",
      "citeRegEx" : "Okura et al\\.,? 2017",
      "shortCiteRegEx" : "Okura et al\\.",
      "year" : 2017
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha,",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Style transfer through back-translation",
      "author" : [ "Shrimai Prabhumoye", "Yulia Tsvetkov", "Ruslan Salakhutdinov", "Alan W Black." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Prabhumoye et al\\.,? 2018",
      "shortCiteRegEx" : "Prabhumoye et al\\.",
      "year" : 2018
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J. Liu", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073–",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "Recent advances on neural headline generation",
      "author" : [ "Shi-Qi Shen", "Yan-Kai Lin", "Cun-Chao Tu", "Yu Zhao", "ZhiYuan Liu", "Mao-Song Sun" ],
      "venue" : "Journal of Computer Science and Technology,",
      "citeRegEx" : "Shen et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2017
    }, {
      "title" : "Style transfer from non-parallel text by cross-alignment",
      "author" : [ "Tianxiao Shen", "Tao Lei", "Regina Barzilay", "Tommi Jaakkola." ],
      "venue" : "NIPS, pages 6830–6841.",
      "citeRegEx" : "Shen et al\\.,? 2017b",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep headline generation for clickbait detection",
      "author" : [ "Kai Shu", "Suhang Wang", "Thai Le", "Dongwon Lee", "Huan Liu." ],
      "venue" : "ICDM, pages 467–476.",
      "citeRegEx" : "Shu et al\\.,? 2018",
      "shortCiteRegEx" : "Shu et al\\.",
      "year" : 2018
    }, {
      "title" : "Attractive or faithful? popularity-reinforced learning for inspired headline generation",
      "author" : [ "Yun-Zhu Song", "Hong-Han Shuai", "Sung-Lin Yeh", "YiLun Wu", "Lun-Wei Ku", "Wen-Chih Peng." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Song et al\\.,? 2020",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2020
    }, {
      "title" : "Event-driven headline generation",
      "author" : [ "Rui Sun", "Yue Zhang", "Meishan Zhang", "Donghong Ji." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Pro-",
      "citeRegEx" : "Sun et al\\.,? 2015",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural headline generation on Abstract Meaning Representation",
      "author" : [ "Sho Takase", "Jun Suzuki", "Naoaki Okazaki", "Tsutomu Hirao", "Masaaki Nagata." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Takase et al\\.,? 2016",
      "shortCiteRegEx" : "Takase et al\\.",
      "year" : 2016
    }, {
      "title" : "From neural sentence summarization to headline generation: A coarse-to-fine approach",
      "author" : [ "Jiwei Tan", "Xiaojun Wan", "Jianguo Xiao." ],
      "venue" : "IJCAI, pages 4109–4115.",
      "citeRegEx" : "Tan et al\\.,? 2017",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2017
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "When framing matters: How partisan and journalistic frames affect individual opinions and party identification",
      "author" : [ "Michael W Wagner", "Mike Gruszczynski." ],
      "venue" : "Journalism & Communication Monographs, 18(1):5–48.",
      "citeRegEx" : "Wagner and Gruszczynski.,? 2016",
      "shortCiteRegEx" : "Wagner and Gruszczynski.",
      "year" : 2016
    }, {
      "title" : "Dkn: Deep knowledge-aware network for news recommendation",
      "author" : [ "Hongwei Wang", "Fuzheng Zhang", "Xing Xie", "Minyi Guo." ],
      "venue" : "WWW, pages 1835– 1844.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural news recommendation with attentive multiview learning",
      "author" : [ "Chuhan Wu", "Fangzhao Wu", "Mingxiao An", "Jianqiang Huang", "Yongfeng Huang", "Xing Xie." ],
      "venue" : "IJCAI, pages 3863–3869.",
      "citeRegEx" : "Wu et al\\.,? 2019a",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Npa: Neural news recommendation with personalized attention",
      "author" : [ "Chuhan Wu", "Fangzhao Wu", "Mingxiao An", "Jianqiang Huang", "Yongfeng Huang", "Xing Xie." ],
      "venue" : "KDD, pages 2576–2584.",
      "citeRegEx" : "Wu et al\\.,? 2019b",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural news recommendation with multi-head selfattention",
      "author" : [ "Chuhan Wu", "Fangzhao Wu", "Suyu Ge", "Tao Qi", "Yongfeng Huang", "Xing Xie." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Wu et al\\.,? 2019c",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Clickbait? sensational headline generation with auto-tuned reinforcement learning",
      "author" : [ "Peng Xu", "Chien-Sheng Wu", "Andrea Madotto", "Pascale Fung." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Xu et al\\.,? 2019",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2019
    }, {
      "title" : "Question headline generation for news articles",
      "author" : [ "Ruqing Zhang", "Jiafeng Guo", "Yixing Fan", "Yanyan Lan", "Jun Xu", "Huanhuan Cao", "Xueqi Cheng." ],
      "venue" : "CIKM, pages 617–626.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Drn: A deep reinforcement learning framework for news recommendation",
      "author" : [ "Guanjie Zheng", "Fuzheng Zhang", "Zihan Zheng", "Yang Xiang", "Nicholas Jing Yuan", "Xing Xie", "Zhenhui Li." ],
      "venue" : "WWW, pages 167–176.",
      "citeRegEx" : "Zheng et al\\.,? 2018",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "News headline generation (Dorr et al., 2003; Lopyrev, 2015; Alfonseca et al., 2013; Tan et al., 2017; See et al., 2017; Zhang et al., 2018; Xu et al., 2019; Murao et al., 2019; Gavrilov et al., 2019; Gu et al., 2020; Song et al., 2020), conventionally considered as a paradigm of challenging text summarization task, has been extensively explored for decades.",
      "startOffset" : 25,
      "endOffset" : 235
    }, {
      "referenceID" : 14,
      "context" : "News headline generation (Dorr et al., 2003; Lopyrev, 2015; Alfonseca et al., 2013; Tan et al., 2017; See et al., 2017; Zhang et al., 2018; Xu et al., 2019; Murao et al., 2019; Gavrilov et al., 2019; Gu et al., 2020; Song et al., 2020), conventionally considered as a paradigm of challenging text summarization task, has been extensively explored for decades.",
      "startOffset" : 25,
      "endOffset" : 235
    }, {
      "referenceID" : 0,
      "context" : "News headline generation (Dorr et al., 2003; Lopyrev, 2015; Alfonseca et al., 2013; Tan et al., 2017; See et al., 2017; Zhang et al., 2018; Xu et al., 2019; Murao et al., 2019; Gavrilov et al., 2019; Gu et al., 2020; Song et al., 2020), conventionally considered as a paradigm of challenging text summarization task, has been extensively explored for decades.",
      "startOffset" : 25,
      "endOffset" : 235
    }, {
      "referenceID" : 27,
      "context" : "News headline generation (Dorr et al., 2003; Lopyrev, 2015; Alfonseca et al., 2013; Tan et al., 2017; See et al., 2017; Zhang et al., 2018; Xu et al., 2019; Murao et al., 2019; Gavrilov et al., 2019; Gu et al., 2020; Song et al., 2020), conventionally considered as a paradigm of challenging text summarization task, has been extensively explored for decades.",
      "startOffset" : 25,
      "endOffset" : 235
    }, {
      "referenceID" : 20,
      "context" : "News headline generation (Dorr et al., 2003; Lopyrev, 2015; Alfonseca et al., 2013; Tan et al., 2017; See et al., 2017; Zhang et al., 2018; Xu et al., 2019; Murao et al., 2019; Gavrilov et al., 2019; Gu et al., 2020; Song et al., 2020), conventionally considered as a paradigm of challenging text summarization task, has been extensively explored for decades.",
      "startOffset" : 25,
      "endOffset" : 235
    }, {
      "referenceID" : 35,
      "context" : "News headline generation (Dorr et al., 2003; Lopyrev, 2015; Alfonseca et al., 2013; Tan et al., 2017; See et al., 2017; Zhang et al., 2018; Xu et al., 2019; Murao et al., 2019; Gavrilov et al., 2019; Gu et al., 2020; Song et al., 2020), conventionally considered as a paradigm of challenging text summarization task, has been extensively explored for decades.",
      "startOffset" : 25,
      "endOffset" : 235
    }, {
      "referenceID" : 34,
      "context" : "News headline generation (Dorr et al., 2003; Lopyrev, 2015; Alfonseca et al., 2013; Tan et al., 2017; See et al., 2017; Zhang et al., 2018; Xu et al., 2019; Murao et al., 2019; Gavrilov et al., 2019; Gu et al., 2020; Song et al., 2020), conventionally considered as a paradigm of challenging text summarization task, has been extensively explored for decades.",
      "startOffset" : 25,
      "endOffset" : 235
    }, {
      "referenceID" : 16,
      "context" : "News headline generation (Dorr et al., 2003; Lopyrev, 2015; Alfonseca et al., 2013; Tan et al., 2017; See et al., 2017; Zhang et al., 2018; Xu et al., 2019; Murao et al., 2019; Gavrilov et al., 2019; Gu et al., 2020; Song et al., 2020), conventionally considered as a paradigm of challenging text summarization task, has been extensively explored for decades.",
      "startOffset" : 25,
      "endOffset" : 235
    }, {
      "referenceID" : 4,
      "context" : "News headline generation (Dorr et al., 2003; Lopyrev, 2015; Alfonseca et al., 2013; Tan et al., 2017; See et al., 2017; Zhang et al., 2018; Xu et al., 2019; Murao et al., 2019; Gavrilov et al., 2019; Gu et al., 2020; Song et al., 2020), conventionally considered as a paradigm of challenging text summarization task, has been extensively explored for decades.",
      "startOffset" : 25,
      "endOffset" : 235
    }, {
      "referenceID" : 5,
      "context" : "News headline generation (Dorr et al., 2003; Lopyrev, 2015; Alfonseca et al., 2013; Tan et al., 2017; See et al., 2017; Zhang et al., 2018; Xu et al., 2019; Murao et al., 2019; Gavrilov et al., 2019; Gu et al., 2020; Song et al., 2020), conventionally considered as a paradigm of challenging text summarization task, has been extensively explored for decades.",
      "startOffset" : 25,
      "endOffset" : 235
    }, {
      "referenceID" : 24,
      "context" : "News headline generation (Dorr et al., 2003; Lopyrev, 2015; Alfonseca et al., 2013; Tan et al., 2017; See et al., 2017; Zhang et al., 2018; Xu et al., 2019; Murao et al., 2019; Gavrilov et al., 2019; Gu et al., 2020; Song et al., 2020), conventionally considered as a paradigm of challenging text summarization task, has been extensively explored for decades.",
      "startOffset" : 25,
      "endOffset" : 235
    }, {
      "referenceID" : 8,
      "context" : "etc (LaRocque, 2003) propels a new research direction that how to decorate the headline as an irresistible invitation to users for reading through the article (Xu et al.",
      "startOffset" : 4,
      "endOffset" : 20
    }, {
      "referenceID" : 34,
      "context" : "etc (LaRocque, 2003) propels a new research direction that how to decorate the headline as an irresistible invitation to users for reading through the article (Xu et al., 2019) since more readings may acquaint more revenue of these platforms.",
      "startOffset" : 159,
      "endOffset" : 176
    }, {
      "referenceID" : 35,
      "context" : "To this end, specified stylized headline generation techniques were proposed, such as question headline (Zhang et al., 2018), sensational headline (Xu et al.",
      "startOffset" : 104,
      "endOffset" : 124
    }, {
      "referenceID" : 34,
      "context" : ", 2018), sensational headline (Xu et al., 2019) generation, and so on (Shu et al.",
      "startOffset" : 30,
      "endOffset" : 47
    }, {
      "referenceID" : 1,
      "context" : "Inspired by the tremendous success of personalized news recommendation (An et al., 2019; Wang et al., 2018; Li et al., 2010; Zheng et al., 2018) where the ultimate goal is to learn users’ reading interests and deliver the right news to them, a plausible solution to this question could be producing headlines satisfying the personalized interests of readers.",
      "startOffset" : 71,
      "endOffset" : 144
    }, {
      "referenceID" : 30,
      "context" : "Inspired by the tremendous success of personalized news recommendation (An et al., 2019; Wang et al., 2018; Li et al., 2010; Zheng et al., 2018) where the ultimate goal is to learn users’ reading interests and deliver the right news to them, a plausible solution to this question could be producing headlines satisfying the personalized interests of readers.",
      "startOffset" : 71,
      "endOffset" : 144
    }, {
      "referenceID" : 10,
      "context" : "Inspired by the tremendous success of personalized news recommendation (An et al., 2019; Wang et al., 2018; Li et al., 2010; Zheng et al., 2018) where the ultimate goal is to learn users’ reading interests and deliver the right news to them, a plausible solution to this question could be producing headlines satisfying the personalized interests of readers.",
      "startOffset" : 71,
      "endOffset" : 144
    }, {
      "referenceID" : 36,
      "context" : "Inspired by the tremendous success of personalized news recommendation (An et al., 2019; Wang et al., 2018; Li et al., 2010; Zheng et al., 2018) where the ultimate goal is to learn users’ reading interests and deliver the right news to them, a plausible solution to this question could be producing headlines satisfying the personalized interests of readers.",
      "startOffset" : 71,
      "endOffset" : 144
    }, {
      "referenceID" : 10,
      "context" : ", exposure impressions and click behaviors, but the textual features usually have already been overly pre-processed (Li et al., 2010; Zheng et al., 2018).",
      "startOffset" : 116,
      "endOffset" : 153
    }, {
      "referenceID" : 36,
      "context" : ", exposure impressions and click behaviors, but the textual features usually have already been overly pre-processed (Li et al., 2010; Zheng et al., 2018).",
      "startOffset" : 116,
      "endOffset" : 153
    }, {
      "referenceID" : 27,
      "context" : "News headline generation datasets, on the other hand, usually consist of news bodies as well as their headlines, which all come from the news-side (Tan et al., 2017; Zhang et al., 2018) rather than the user-side.",
      "startOffset" : 147,
      "endOffset" : 185
    }, {
      "referenceID" : 35,
      "context" : "News headline generation datasets, on the other hand, usually consist of news bodies as well as their headlines, which all come from the news-side (Tan et al., 2017; Zhang et al., 2018) rather than the user-side.",
      "startOffset" : 147,
      "endOffset" : 185
    }, {
      "referenceID" : 17,
      "context" : "Under the neural framework, the news representation is usually modeled by a news encoder that encodes news title, news body or other attributes via various neural structures (Okura et al., 2017; Wang et al., 2018; Wu et al., 2019a; An et al., 2019; Wu et al., 2019a).",
      "startOffset" : 174,
      "endOffset" : 266
    }, {
      "referenceID" : 30,
      "context" : "Under the neural framework, the news representation is usually modeled by a news encoder that encodes news title, news body or other attributes via various neural structures (Okura et al., 2017; Wang et al., 2018; Wu et al., 2019a; An et al., 2019; Wu et al., 2019a).",
      "startOffset" : 174,
      "endOffset" : 266
    }, {
      "referenceID" : 31,
      "context" : "Under the neural framework, the news representation is usually modeled by a news encoder that encodes news title, news body or other attributes via various neural structures (Okura et al., 2017; Wang et al., 2018; Wu et al., 2019a; An et al., 2019; Wu et al., 2019a).",
      "startOffset" : 174,
      "endOffset" : 266
    }, {
      "referenceID" : 1,
      "context" : "Under the neural framework, the news representation is usually modeled by a news encoder that encodes news title, news body or other attributes via various neural structures (Okura et al., 2017; Wang et al., 2018; Wu et al., 2019a; An et al., 2019; Wu et al., 2019a).",
      "startOffset" : 174,
      "endOffset" : 266
    }, {
      "referenceID" : 31,
      "context" : "Under the neural framework, the news representation is usually modeled by a news encoder that encodes news title, news body or other attributes via various neural structures (Okura et al., 2017; Wang et al., 2018; Wu et al., 2019a; An et al., 2019; Wu et al., 2019a).",
      "startOffset" : 174,
      "endOffset" : 266
    }, {
      "referenceID" : 17,
      "context" : "The user representation is generated by engraving the high-level aspects over their clicked news sequences using sequential (Okura et al., 2017; An et al., 2019) or attentive modules (Wu et al.",
      "startOffset" : 124,
      "endOffset" : 161
    }, {
      "referenceID" : 1,
      "context" : "The user representation is generated by engraving the high-level aspects over their clicked news sequences using sequential (Okura et al., 2017; An et al., 2019) or attentive modules (Wu et al.",
      "startOffset" : 124,
      "endOffset" : 161
    }, {
      "referenceID" : 28,
      "context" : "Furthermore, multihead self-attention mechanism (Vaswani et al., 2017) is adopted to capture the word and sentence interactions by,",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 20,
      "context" : "Inspired by pointer-generator network (See et al., 2017), which exhibits desirable performance on either dealing with out-of-vocabulary (OOV) words or improving the reproducing factual details with copy mechanism, we adopt a pointer pgen at decoding step t as a soft switch to choose between generating a word from the vocabulary with a probability of Pvocab(wt) or copying a word from the news body sampling from the attention distribution at.",
      "startOffset" : 38,
      "endOffset" : 56
    }, {
      "referenceID" : 17,
      "context" : "The first group consists of various user modeling methods, which are all SOTA neural-based news recommendation methods: (1) EBNR (Okura et al., 2017) learns user representations by aggregating their browsed news with GRU.",
      "startOffset" : 129,
      "endOffset" : 149
    }, {
      "referenceID" : 30,
      "context" : "(2) DKN (Wang et al., 2018) is a deep knowledge-aware network for news recommendation.",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 32,
      "context" : "(3) NPA (Wu et al., 2019b) proposes personalized attention module in both news and user encoder.",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 33,
      "context" : "(4) NRMS (Wu et al., 2019c) conducts neural news recommendation with multi-head self-attention.",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 1,
      "context" : "(5) LSTUR (An et al., 2019) models long- and shor-term user representations based on user ID embedding and sequential encoding, individually.",
      "startOffset" : 10,
      "endOffset" : 27
    }, {
      "referenceID" : 31,
      "context" : "(6) NAML (Wu et al., 2019a) proposes multi-view learning in user representation.",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 20,
      "context" : "(1) PointerGen (See et al., 2017) proposes an explicit probabilistic switch to choose between copying from source text and generating word from vocabulary.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 34,
      "context" : "(2) PG+RL-ROUGE (Xu et al., 2019) extends Pointer-Gen with as a reinforcement learning framework which generates sensational headlines by considering ROUGE-L score as rewards.",
      "startOffset" : 16,
      "endOffset" : 33
    }, {
      "referenceID" : 18,
      "context" : "Word embeddings are 300-dimension and initialized by the Glove (Pennington et al., 2014)",
      "startOffset" : 63,
      "endOffset" : 88
    }, {
      "referenceID" : 7,
      "context" : "10, and Adam (Kingma and Ba, 2014) is used for model optimization where we sample 16 sequences for Monte Carlo search.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 12,
      "context" : "For personalized headline generation, we evaluate the generation quality using F1 ROUGE (Lin, 2004) 4 including unigram",
      "startOffset" : 88,
      "endOffset" : 99
    }, {
      "referenceID" : 15,
      "context" : "Headline generation has been considered as specialized text summarization (Luo et al., 2019; Jia et al., 2020), from which both extractive (Dorr et al.",
      "startOffset" : 74,
      "endOffset" : 110
    }, {
      "referenceID" : 6,
      "context" : "Headline generation has been considered as specialized text summarization (Luo et al., 2019; Jia et al., 2020), from which both extractive (Dorr et al.",
      "startOffset" : 74,
      "endOffset" : 110
    }, {
      "referenceID" : 2,
      "context" : ", 2020), from which both extractive (Dorr et al., 2003; Alfonseca et al., 2013) and abstractive summarization (Sun et al.",
      "startOffset" : 36,
      "endOffset" : 79
    }, {
      "referenceID" : 0,
      "context" : ", 2020), from which both extractive (Dorr et al., 2003; Alfonseca et al., 2013) and abstractive summarization (Sun et al.",
      "startOffset" : 36,
      "endOffset" : 79
    }, {
      "referenceID" : 25,
      "context" : ", 2013) and abstractive summarization (Sun et al., 2015; Takase et al., 2016; Tan et al., 2017; Gavrilov et al., 2019; See et al., 2017) approaches prevailed for decades.",
      "startOffset" : 38,
      "endOffset" : 136
    }, {
      "referenceID" : 26,
      "context" : ", 2013) and abstractive summarization (Sun et al., 2015; Takase et al., 2016; Tan et al., 2017; Gavrilov et al., 2019; See et al., 2017) approaches prevailed for decades.",
      "startOffset" : 38,
      "endOffset" : 136
    }, {
      "referenceID" : 27,
      "context" : ", 2013) and abstractive summarization (Sun et al., 2015; Takase et al., 2016; Tan et al., 2017; Gavrilov et al., 2019; See et al., 2017) approaches prevailed for decades.",
      "startOffset" : 38,
      "endOffset" : 136
    }, {
      "referenceID" : 4,
      "context" : ", 2013) and abstractive summarization (Sun et al., 2015; Takase et al., 2016; Tan et al., 2017; Gavrilov et al., 2019; See et al., 2017) approaches prevailed for decades.",
      "startOffset" : 38,
      "endOffset" : 136
    }, {
      "referenceID" : 20,
      "context" : ", 2013) and abstractive summarization (Sun et al., 2015; Takase et al., 2016; Tan et al., 2017; Gavrilov et al., 2019; See et al., 2017) approaches prevailed for decades.",
      "startOffset" : 38,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : "Extractive methods select a subset of actual sentences in original article, which may derive incoherent summary (Alfonseca et al., 2013).",
      "startOffset" : 112,
      "endOffset" : 136
    }, {
      "referenceID" : 16,
      "context" : "While abstractive models, basically falling in an encoderdecoder (Shen et al., 2017a; Murao et al., 2019) framework, can generate more condensed output based on the latent representation of news content.",
      "startOffset" : 65,
      "endOffset" : 105
    }, {
      "referenceID" : 22,
      "context" : "Recently, stylized headlines generation were proposed to output eye-catching headlines by implicit style transfer (Shen et al., 2017b; Fu et al., 2018; Prabhumoye et al., 2018) or style-oriented super-",
      "startOffset" : 114,
      "endOffset" : 176
    }, {
      "referenceID" : 3,
      "context" : "Recently, stylized headlines generation were proposed to output eye-catching headlines by implicit style transfer (Shen et al., 2017b; Fu et al., 2018; Prabhumoye et al., 2018) or style-oriented super-",
      "startOffset" : 114,
      "endOffset" : 176
    }, {
      "referenceID" : 19,
      "context" : "Recently, stylized headlines generation were proposed to output eye-catching headlines by implicit style transfer (Shen et al., 2017b; Fu et al., 2018; Prabhumoye et al., 2018) or style-oriented super-",
      "startOffset" : 114,
      "endOffset" : 176
    } ],
    "year" : 2021,
    "abstractText" : "In this paper, we formulate the personalized news headline generation problem whose goal is to output a user-specific title based on both a user’s reading interests and a candidate news body to be exposed to her. To build up a benchmark for this problem, we publicize a large-scale dataset named PENS (PErsonalized News headlineS). The training set is collected from user impressions logs of Microsoft News, and the test set is manually created by hundreds of native speakers to enable a fair testbed for evaluating models in an offline mode. We propose a generic framework as a preparatory solution to our problem. At its heart, user preference is learned by leveraging the user behavioral data, and three kinds of user preference injections are proposed to personalize a text generator and establish personalized headlines. We investigate our dataset by implementing several state-of-the-art user modeling methods in our framework to demonstrate a benchmark score for the proposed dataset. The dataset is available at https: //msnews.github.io/pens.html.",
    "creator" : "LaTeX with hyperref"
  }
}