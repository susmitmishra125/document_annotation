{
  "name" : "2021.acl-long.252.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Joint Models for Answer Verification in Question Answering Systems",
    "authors" : [ "Zeyu Zhang", "Thuy Vu", "Alessandro Moschitti" ],
    "emails" : [ "zeyuzhang@email.arizona.edu,", "amosch}@amazon.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3252–3262\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3252"
    }, {
      "heading" : "1 Introduction",
      "text" : "Automated Question Answering (QA) research has received a renewed attention thanks to the diffusion of Virtual Assistants. Among the different types of methods to implement QA systems, we focus on Answer Sentence Selection (AS2) research, originated from TREC-QA track (Voorhees and Tice, 1999), as it proposes efficient models that are more suitable for a production setting, e.g., they are more efficient than those developed in machine reading (MR) work (Chen et al., 2017).\nGarg et al. (2020) proposed the TANDA approach based on pre-trained Transformer models, obtaining impressive improvement over the state of the art for AS2, measured on the two most used datasets, WikiQA (Yang et al., 2015) and TRECQA (Wang et al., 2007). However, TANDA was applied only to pointwise rerankers (PR), e.g., simple binary classifiers. Bonadiman and Moschitti\n∗Work done while the author was an intern at Amazon Alexa\n(2020) tried to improve this model by jointly modeling all answer candidates with listwise methods, e.g., (Bian et al., 2017). Unfortunately, merging the embeddings from all candidates with standard approaches, e.g., CNN or LSTM, did not improve over TANDA.\nA more structured approach to building joint models over sentences can instead be observed in Fact Verification Systems, e.g., the methods developed in the FEVER challenge (Thorne et al., 2018a). Such systems take a claim, e.g., Joe Walsh was inducted in 2001, as input (see Tab. 1), and verify if it is valid, using related sentences called evidences (typically retrieved by a search engine). For example, Ev1, As a member of the Eagles, Walsh was inducted into the Rock and Roll Hall of Fame in 1998, and into the Vocal Group Hall of Fame in 2001, and Ev3, Walsh was awarded with the Vocal Group Hall of Fame in 2001, support the veracity of the claim. In contrast, Ev2 is neutral as it describes who Joe Walsh is but does not contribute to establish the induction. We conjecture that supporting evidence for answer correctness in AS2 task can be modeled with a similar rationale.\nIn this paper, we design joint models for AS2 based on the assumption that, given q and a target answer candidate t, the other answer candidates, (c1, ..ck) can provide positive, negative, or neutral support to decide the correctness of t. Our first approach exploits Fact Checking research: we adapted a state-of-the-art FEVER system, KGAT (Liu et al., 2020), for AS2. We defined a claim as\na pair constituted of the question and one target answer, while considering all the other answers as evidences. We re-trained and rebuilt all its embeddings for the AS2 task.\nOur second method, Answer Support-based Reranker (ASR), is completely new, it is based on the representation of the pair, (q, t), generated by state-of-the-art AS2 models, concatenated with the representation of all the pairs (t, ci). The latter summarizes the contribution of each ci to t using a maxpooling operation. ci can be unrelated to (q, t) since the candidates are automatically retrieved, thus it may introduce just noise. To mitigate this problem, we use an Answer Support Classifier (ASC) to learn the relatedness between t and ci by classifying their embedding, which we obtain by applying a transformer network to their concatenated text. ASC tunes the (t, ci) embedding parameters according to the evidence that ci provides to t. Our Answer Support-based Reranker (ASR) significantly improves the state of the art, and is also simpler than our approach based on KGAT.\nOur third method is an extension of ASR. It should be noted that, although ASR exploits the information from the k candidates, it still produces a score for a target t without knowing the scores produced for the other target answers. Thus, we jointly model the representation obtained for each target in a multi-ASR (MASR) architecture, which can then carry out a complete global reasoning over all target answers.\nWe experimented with our models over three datasets, WikiQA, TREC-QA and WQA, where the latter is an internal dataset built on anonymized customer questions. The results show that:\n• ASR improves the best current model for AS2, i.e., TANDA by ∼3%, corresponding to an error reduction of 10% in Accuracy, on both WikiQA and TREC-QA.\n• We also obtain a relative improvement of ∼3% over TANDA on WQA, confirming that ASR is a general solution to design accurate QA systems.\n• Most interestingly, MASR improves ASR by additional 2%, confirming the benefit of joint modeling.\nFinally, it is interesting to mention that MASR improvement is also due to the use of FEVER data for\npre-fine-tuning ASC, suggesting that the fact verification inference and the answer support inference are similar."
    }, {
      "heading" : "2 Problem definition and related work",
      "text" : "We consider retrieval-based QA systems, which are mainly constituted by (i) a search engine, retrieving documents related to the questions; and (ii) an AS2 model, which reranks passages/sentences extracted from the documents. The top sentence is typically used as final answer for the users."
    }, {
      "heading" : "2.1 Answer Sentence Selection (AS2)",
      "text" : "The task of reranking answer-sentence candidates provided by a retrieval engine can be modeled with a classifier scoring the candidates. Let q be an element of the question set, Q, and A = {c1, . . . , cn} be a set of candidates for q, a reranker can be defined as R : Q× Π(A) → Π(A), where Π(A) is the set of all permutations of A. Previous work targeting ranking problems in the text domain has classified reranking functions into three buckets: pointwise, pairwise, and listwise methods. Pointwise reranking: This approach learns p(q, ci), which is the probability of ci correctly answering q, using a standard binary classification setting. The final rank is simply obtained sorting ci, based on p(q, ci). Previous work estimates p(q, ci) with neural models (Severyn and Moschitti, 2015), also using attention mechanisms, e.g., CompareAggregate (Yoon et al., 2019), inter-weighted alignment networks (Shen et al., 2017), and pre-trained Transformer models, which are the state of the art. Garg et al. (2020) proposed TANDA, which is the current most accurate model on WikiQA and TREC-QA. Pairwise reranking: The method considers binary classifiers of the form χ(q, ci, cj) for determining the partial rank between ci and cj , then the scoring function p(q, ci) is obtained by summing up all the contributions with respect to the target candidate t = ci, e.g., p(q, ci) = ∑ j χ(q, ci, cj). There has been a large body of work preceding Transformer models, e.g., (Laskar et al., 2020; Tayyar Madabushi et al., 2018; Rao et al., 2016). However, these methods are largely outperformed by the pointwise TANDA model. Listwise reranking: This approach, e.g., (Bian et al., 2017; Cao et al., 2007; Ai et al., 2018), aims at learning p(q, π), π ∈ Π(A), using the information on the entire set of candidates. The loss function for training such networks is constituted by the\ncontribution of all elements of its ranked items. The closest work to our research is by Bonadiman and Moschitti (2020), who designed several joint models. These improved early neural networks based on CNN and LSTM for AS2, but failed to improve the state of the art using pre-trained Transformer models."
    }, {
      "heading" : "2.2 Joint Models in Question Answering",
      "text" : "MR is a popular QA task that identifies an answer string in a paragraph or a text of limited size for a question. Its application to retrieval scenario has also been studied (Chen et al., 2017; Hu et al., 2019; Kratzwald and Feuerriegel, 2018). However, the large volume of retrieved content makes their use not practical yet. Moreover, the joint modeling aspect of MR regards sentences from the same paragraphs. Jin et al. (2020) use the relation between candidates in Multi-task learning approach for AS2. However, they do not exploit transformer models, thus their results are rather below the state of the art.\nIn contrast with the work above, our modeling is driven by an answer support strategy, where the pieces of information are taken from different documents. This makes our model even more unique; it allows us to design innovative joint models, which are still not designed in any MR systems."
    }, {
      "heading" : "2.3 Fact Verification for Question Answering",
      "text" : "Fact verification has become a social need given the massive amount of information generated daily. The problem is, therefore, becoming increasingly important in NLP context (Mihaylova et al., 2018). In QA, answer verification is directly relevant due to its nature of content delivery (Mihaylova et al., 2019). The problem has been explored in MR setting (Wang et al., 2018). Zhang et al. (2020a) also proposed to fact check for product questions using additional associated evidence sentences. The latter are retrieved based on similarity scores computed with both TF-IDF and sentence-embeddings from pre-trained BERT models. While the process is technically sound, the retrieval of evidence is an expensive process, which is prohibitive to scale in production. We instead address this problem by leveraging the top answer candidates."
    }, {
      "heading" : "3 Baseline Models for AS2",
      "text" : "In this section, we describe our baseline models, which are constituted by pointwise, pairwise, and\nlistwise strategies."
    }, {
      "heading" : "3.1 Pointwise Models",
      "text" : "One simple and effective method to build an answer selector is to use a pre-trained Transformer model, adding a simple classification layer to it, and fine-tuning the model on the AS2 task. Specifically, q = Tokq1,...,Tok q N and c =Tok c 1,...,Tok c M are encoded in the input of the Transformer by delimiting them using three tags: [CLS], [SEP] and [EOS], inserted at the beginning, as separator, and at the end, respectively. This input is encoded as three embeddings based on tokens, segments and their positions, which are fed as input to several layers (up to 24). Each of them contains sublayers for multi-head attention, normalization and feed forward processing. The result of this transformation is an embedding, E, representing (q, c), which models the dependencies between words and segments of the two sentences.\nFor the downstream task, E is fed (after applying a non-linearity function) to a fully connected layer having weights: W and B. The output layer can be used to implement the task function. For example, a softmax can be used to model the probability of the question/candidate pair classification, as: p(q, c) = softmax(W × tanh(E(q, c)) +B).\nWe can train this model with log cross-entropy loss: L = − ∑ l∈{0,1} yl× log(ŷl) on pairs of texts, where yl is the correct and incorrect answer label, ŷ1 = p(q, c), and ŷ0 = 1 − p(q, c). Training the Transformer from scratch requires a large amount of labeled data, but it can be pre-trained using a masked language model, and the next sentence prediction tasks, for which labels can be automatically generated. Several methods for pretraining Transformer-based language models have been proposed, e.g., BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), AlBERT (Lan et al., 2020)."
    }, {
      "heading" : "3.2 Our joint model baselines",
      "text" : "To better show the potential of our approach and the complexity of the task, we designed three joint model baselines based on: (i) a multiclassifier approach (a listwise method), and (ii) a pairwise joint model operating over k + 1 candidates, and our adaptation of KGAT model (a pairwise method).\nJoint Model Multi-classifier The first baseline is also a Transformer-based architecture: we concatenate the question with the top k+1 answer can-\ndidates, i.e., (q[SEP ]c1[SEP ]c2 . . . [SEP ]ck+1), and provide this input to the same Transformer model used for pointwise reranking. We use the final hidden vector E corresponding to the first input token [CLS] generated by the Transformer, and a classification layer with weights W ∈ R(k+1)×|E|, and train the model using a standard cross-entropy classification loss: y × log(softmax(EW T )), where y is a one-hot vector representing labels for the k + 1 candidates, i.e., |y| = k + 1. We use a transformer model fine-tuned with the TANDARoBERTa-base or large models, i.e., RoBERTa models fine-tuned on ASNQ (Garg et al., 2020). The scores for the candidate answers are calculated as ( p(c1), .., p(ck+1) ) = softmax(EW T ). Then, we rerank ci according their probability.\nJoint Model Pairwise Our second baseline is similar to the first. We concatenate the question with each ci to constitute the (q, ci) pairs, which are input to the Transformer, and we use the first input token [CLS] as the representation of each (q, ci) pair. Then, we concatenate the embedding of the pair containing the target candidate, (q, t) with the embedding of all the other candidates’ [CLS]. (q, t) is always in the first position. We train the model using a standard classification loss. At classification time, we select one target candidate at a time, and set it in the first position, followed by all the others. We classify all k + 1 candidates and use their score for reranking them. It should be noted that to qualify for a pairwise approach, Joint Model Pairwise should use a ranking loss. However, we always use standard cross-entropy loss as it is more efficient and the different is performance is negligible.\nJoint Model with KGAT Liu et al. (2020) presented an interesting model, Kernel Graph Attention Network (KGAT), for fact verification: given a claimed fact f , and a set of evidences Ev = {ev1, ev2, . . . , evm}, their model carries out joint reasoning over Ev, e.g., aggregating information to estimate the probability of f to be true or false, p(y|f,Ev), where y ∈{true, false}.\nThe approach is based on a fully connected graph, G, whose nodes are the ni = (f, evi) pairs, and p(y|f,Ev) = p(y|f, evi, Ev)p(evi|f,Ev), where p(y|f, evi, Ev) = p(y|ni, G) is the label probability in each node i conditioned on the whole graph, and p(evi|f,Ev) = p(ni|G) is the probability of selecting the most informative evidence. KGAT uses an edge kernel to perform a hierarchi-\ncal attention mechanism, which propagates information between nodes and aggregate evidences.\nWe built a KGAT model for AS2 as follows: we replace (i) evi with the set of candidate answers ci, and (ii) the claim f with the question and a target answer pair, (q, t). KGAT constructs the evidence graph G by using each claim-evidence pair as a node, which, in our case, is ((q, t), ci), and connects all node pairs with edges, making it a fully-connected evidence graph. This way, sentence and token attention operate over the triplets, (q, t, ci), establishing semantic links, which can help to support or undermine the correctness of t.\nThe original KGAT aggregates all the pieces of information we built, based on their relevance, to determine the probability of t. As we use AS2 data, the probability will be about the correctness of t. More in detail, we initialize the node representation using the contextual embeddings obtained with two TANDA-RoBERTa-base models 1: the first produces the embedding of (q, t), while the second outputs the embedding of (q, ci). Then, we apply a max-pooling operation on these two to get the final node representation. The rest of the architecture is identical to the original KGAT. Finally, at test time, we select one ci at a time, as the target t, and compute its probability, which ranks ci."
    }, {
      "heading" : "4 Joint Answer Support Models for AS2",
      "text" : "We proposed the Answer Support Reranker (ASR), which uses an answer pair classifier to provide evidence to a target answer t. Given a question q, and a subset of its top-k+1 ranked answer candidates, A (reranked by an AS2 model), we build a function, σ : Q × C × Ck → R such that σ(q, t,A \\ {t}) provides the probability of t to be correct, where C is the set of sentence-candidates. We also design a multi-classifier MASR, which combines k ASR models, one for each different target answer."
    }, {
      "heading" : "4.1 Answer Support-based Reranker (ASR)",
      "text" : "We developed ASR architecture described in Figure 1c. This consists of three main components:\n1. a Pointwise Reranker (PR), which provides the embedding of the input (q, t), described in Figure 1a. This is essentially the state-of-the-art AS2 model based on the TANDA approach applied to RoBERTa pre-trained transformer.\n1https://github.com/alexa/wqa tanda\n(a) Baseline Reranker using Transformers. (b) PairWise Representation using Transformers.\n2. To reduce the noise that may be introduced by irrelevant ci, we use the Answer Support Classifier (ASC), which classifies each (t, ci) in one of the following four classes:\n0 : t and ci are both correct, 1 : t is correct while ci is not, 2 : vice versa, and 3 : both incorrect.\nThis multi-classifier, described in Figure 1b, is built on top a RoBERTa Transformer, which produced a PairWise Representation (PWR). ASC is trained end-to-end with the rest of the network in a multi-task learning fashion, using its specific cross-entropy loss, computed with the labels above.\n3. The ASR (see Figure 1c) uses the joint representation of (q, t) with (t, ci), i = 1, .., k, where t\nand ci are the top-candidates reranked by PR. The k representations are summarized by applying a max-pooling operation, which will aggregate all the supporting or not supporting properties of the candidates with respect to the target answer. The concatenation of the PR embedding with the max-pooling embedding is given as input to the final classification layer, which scores t with respect to q, also using the information from the other candidates. For training and testing, we select a t from the k + 1 candidates of q at a time, and compute its score. This way, we can rerank all the k+ 1 candidates with their scores.\nImplementation details: ASR is a PR that also exploits the relation between t andA\\{t}. We use RoBERTa to generate the [CLS] ∈ Rd embedding of (q, t) = Et. We denote with Êj the [CLS] output by another RoBERTa Transformer applied to answer pairs, i.e., (t, cj). Then, we concatenate Et to the max-pooling tensor from Ê1, .., Êk:\nV = [Et : Maxpool([Ê1, .., Êk])], (1) where V ∈ R2d is the final representation of the target answer t. Then, we use a standard feedforward network to implement a binary classification layer: p(yi|q, t, Ck) = softmax(VW T +B), where W ∈ R2×2d and B are parameters to transform the representation of the target answer t from dimension 2d to dimension 2, which represents correct or incorrect labels. ASC labels There can be different interpretations when attempting to define labels for answer pairs. An alternative to the definition illustrated above is to use the following FEVER compatible encoding:\n0 : t is correct, while ci can be any value, as also an incorrect ci may provide important context (corresponding to FEVER Support label);\n1 : t is incorrect, ci correct, since ci can provide evidence that t is not similar to a correct answer (corresponding to FEVER Refutal label); and\n2 : both are incorrect, in this case, nothing can be told (corresponding to FEVER Neutral label)."
    }, {
      "heading" : "4.2 Multi-Answer Support Reranker (MASR)",
      "text" : "ASR still selects answers with a pointwise approach2. This means that we can improve it by\n2Again, using ranking loss did not provide a significant improvment.\nbuilding a listwise model, to select the best answer for each question, by utilizing the information from all target answers. In particular, the architecture of MASR shown in Figure 1d is made up of two parts: (i) a list of ASR containing k + 1 ASR blocks, in which each ASR block provides the representation of a target answer t. (ii) A final multiclassifier and a softmax function, which scores each t from k+ 1 embedding concatenation and selects the one with highest score. For training and testing, we select the t from the k + 1 candidates of q based on a softmax output at a time.\nImplementation details: The goal of MASR is to measure the relation between k + 1 target answers, t0, .., tk. The representation of each target answer is the embedding V ∈ R2d from Equation 1 in ASR. Then, we concatenate the hidden vectors of k + 1 target answers to form a matrix V(q,k+1) ∈ R(k+1)×2d. We use this matrix and a classification layer weightsW ∈ R2d, and compute a standard multi-class classification loss:\nLMASR = y ∗ log(softmax(V(q,k+1)W T ), (2)\nwhere y is a one-hot-vector, and |y| = |k + 1|."
    }, {
      "heading" : "5 Experiments",
      "text" : "In these experiments, we compare our models: KGAT, ASR and MASR with pointwise models, which are the state of the art for AS2. We also compare them with our joint model baselines (pairwise and listwise). Finally, we provide an error analysis."
    }, {
      "heading" : "5.1 Datasets",
      "text" : "We used two most popular AS2 datasets, and one real world application dataset we built to test the generality of our approach.\nWikiQA is a QA dataset (Yang et al., 2015) containing a sample of questions and answer-sentence candidates from Bing query logs over Wikipedia. The answers are manually labeled. We follow the most used setting: training with all the questions that have at least one correct answer, and validating and testing with all the questions having at least one correct and one incorrect answer.\nTREC-QA is another popular QA benchmark by Wang et al. (2007). We use the same splits of the original data, following the common setting of previous work, e.g., (Garg et al., 2020).\nWQA The Web-based Question Answering is a dataset built by Alexa AI as part of the effort to improve understanding and benchmarking in QA systems. The creation process includes the following steps: (i) given a set of questions we collected from the web, a search engine is used to retrieve up to 1,000 web pages from an index containing hundreds of millions pages. (ii) From the set of retrieved documents, all candidate sentences are extracted and ranked using AS2 models from (Garg et al., 2020). Finally, (iii) top candidates for each question are manually assessed as correct or incorrect by human judges. This allowed us to obtain a richer variety of answers from multiple sources with a higher average number of answers.\nTable 2 reports the corpus statistics of WikiQA, TREC-QA, and WQA3.\nFEVER is a large-scale public corpus, proposed by Thorne et al. (2018a) for fact verification task, consisting of 185,455 annotated claims from 5,416,537 documents from the Wikipedia dump in June 2017. All claims are labelled as Supported, Refuted or Not Enough Info by annotators. Table 3 shows the statistics of the dataset, which remains the same as in (Thorne et al., 2018b)."
    }, {
      "heading" : "5.2 Training and testing details",
      "text" : "Metrics The performance of QA systems is typically measured with Accuracy in providing correct answers, i.e., the percentage of correct responses. This is also referred to Precision-at-1 (P@1) in the context of reranking, while standard Precision and Recall are not essential in our case as we assume the system does not abstain from providing answers. We also use Mean Average Precision (MAP) and Mean Reciprocal Recall (MRR) evaluated on the test set, using the entire set of candidates for each\n3The public version of WQA will be released in the short-term future. Please search for a publication with title WQA: A Dataset for Web-based Question Answering Tasks on arXiv.org.\nquestion (this varies according to the dataset), to have a direct comparison with the state of the art.\nModels We use the pre-trained RoBERTa-Base (12 layer) and RoBERTa-Large-MNLI (24 layer) models, which were released as checkpoints for use in downstream tasks4.\nReranker training We adopt Adam optimizer (Kingma and Ba, 2014) with a learning rate of 2e-5 for the transfer step on the ASNQ dataset (Garg et al., 2020), and a learning rate of 1e-6 for the adapt step on the target dataset. We apply early stopping on the development set of the target corpus for both fine-tuning steps based on the highest MAP score. We set the max number of epochs equal to 3 and 9 for the adapt and transfer steps, respectively. We set the maximum sequence length for RoBERTa to 128 tokens.\nKGAT and ASR training Again, we use the Adam optimizer with a learning rate of 2e-6 for training the ASR model on the target dataset. We utilize 1 Tesla V100 GPU with 32GB memory and a train batch size of eight. We set the maximum sequence length for RoBERTa Base/Large to 130 tokens and the number of training epochs to 20. The other training configurations are the same of the original KGAT model from (Liu et al., 2020). We use two transformer models for ASR: a RoBERTa\n4https://github.com/pytorch/fairseq\nBase/Large for PR, and one for ASC. We set the maximum sequence length for RoBERTa to 128 tokens and the number of epochs to 20.\nMASR training We use the same configuration of the ASR training, including the optimizer type, learning rate, the number of epochs, GPU type, maximum sequence length, etc. Additionally, we design two different models MASR-F, using an ASC classifier targeting the FEVER labels, and MASR-FP, which initializes ASC with the data from FEVER. This is possible as the labels are compatible."
    }, {
      "heading" : "5.3 Choosing the best k",
      "text" : "The selection of the hyper-parameter k, i.e., the number of candidates to consider for supporting a target answer is rather tricky. Indeed, the standard validation set is typically used for tuning PR. This means that the candidates PR moves to the top k+1 positions are optimistically accurate. Thus, when selecting also the optimal k on the same validation set, there is high risk to overfit the model.\nWe solved this problem by running a PR version not heavily optimized on the dev. set, i.e., we randomly choose a checkpoint after the standard three epochs of fine-tuning of RoBERTa transformer. Additionally, we tuned k only using the WQA dev. set, which contains ∼ 36, 000 Q/A pairs. WikiQA and TREC-QA dev. sets are too small to be used (121 and 65 questions, respectively). Fig. 2 plots the improvement of four different models, Joint Model Multi-classifier, Joint Model Pairwise, KGAT, and ASR, when using different k values. Their best results are reached for 5, 3, 2, and 3, respectively. We note that the most reliable curve shape (convex) is the one of ASR and Joint Model Pairwise."
    }, {
      "heading" : "5.4 Comparative Results",
      "text" : "Table 4 reports the P@1, MAP and MRR of the rerankers, and different answer supporting models on WikiQA, TREC-QA and WQA datasets. As WQA is an internal dataset, we only report the improvement over PR in the tables. All models use RoBERTa-Base pre-trained checkpoint and start from the same set of k candidates reranked by PR (state-of-the-art model). The table shows that:\n• PR replicates the MAP and MRR of the stateof-the-art reranker by Garg et al. (2020) on WikiQA.\n• Joint Model Multi-classifier performs lower than PR for all measures and all datasets. This is in line with the findings of Bonadiman and Moschitti (2020), who also did not obtain improvement when jointly used all the candidates altogether in a representation.\n• Joint Model Pairwise differs from ASR as it concatenates the embeddings of the (q, ci), instead of using max-pooling, and does not use any Answer Support Classifier (ASC). Still, it exploits the idea of aggregating the information of all pairs (q, ci) with respect to a target answer t, which proves to be effective, as the model improves on PR over all measures and datasets.\n• Our KGAT version for AS2 also improves PR over all datasets and almost all measures, confirming that the idea of using candidates as support of the target answer is generally valid. However, it is not superior to Joint Model Pairwise.\n• ASR achieves the highest performance among all models (but MASR-FP on WQA), all datasets, and all measures. For example, it outperforms PR by almost 3 absolute percent points in P@1 on WikiQA, and by almost 6 points on TREC from 91.18% to 97.06%, which corresponds to an error reduction of 60%.\n• MASR and MASR-F do not achieve better performance than Joint Model Pairwise on WikiQA and TREC, although MASR outperforms all baselines and even ASR on WQA. This suggests that the significantly higher number of parameters of MASR cannot be trained on small corpus, while WQA has a sufficient number of examples.\n• MASR-FP exploiting FEVER for the initialization of ASC performs better than MASR and MASR-F on WikiQA and TREC. Interestingly, it significantly outperforms ASR by 2% on WQA. This confirms the potential of the model when enough training data is available.\n• We perform randomization test (Yeh, 2000) to verify if the models significantly differ in terms of prediction outcome. We use 100,000 trials for each calculation. The results confirm the statistically significant difference between ASR and all the baselines, with p < 0.05 for WikiQA, and between ASR and all models (i.e., including also KGAT) on WQA."
    }, {
      "heading" : "5.5 Official State of the art",
      "text" : "As the state of the art for AS2 is obtained using RoBERTa Large, we trained KGAT and ASR using this pre-trained language model. Table 5 also reports the comparison with PR, which is the official state of the art. Again, our PR replicates the results of Garg et al. (2020), obtaining slightly lower performance on WikiQA but higher on TREC-QA. KGAT performs lower than PR on both datasets.\nASR establishes the new state of the art on WikiQA with an MAP of 92.80 vs. 92.00. The P@1 also significantly improves by 2%, i.e., achieving 89.71, which is impressively high. Also, on TRECQA, ASR outperforms all models, being on par with PR regarding P@1. The latter is 97.06, which corresponds to mistaking the answers of only two questions. We manually checked these and found out that these were two annotation errors: ASR achieves perfect accuracy while PR only mistakes one answer. Of course, this just provides evidence that PR based on RoBERTa-Large solves the task of selecting the best answers (i.e., measuring P@1 on this dataset is not meaningful anymore)."
    }, {
      "heading" : "5.6 Model Discussion",
      "text" : "Table 6 reports the accuracy of ASC inside to different models. In ASR, it uses 4-way categories, while in MASR-based models, it uses the three\nFEVER labels (see Sec. 4.1). ACC is the overall accuracy while F1 refers to the category 0. We note that ASC in MASR-FP achieves the highest accuracy with respect to the average over all datasets. This happens since we pre-fine-tuned it with the FEVER data.\nWe analyzed examples for which ASR is correct and PR is not. Tab. 7 shows that, given q and k = 3 candidates, PR chooses c1, a suitable but wrong answer. This probably happens since the answer best matches the syntactic/semantic pattern of the question, which asks for a type of color, indeed, the answer offers such type, primary colors. PR does not rely on any background information that can support the set of colors in the answer. In contrast, ASR selects c2 as it can rely on the support of other answers. Its ASC provides an average score for the category 0 (both members are correct) of c2, i.e., 1k ∑ i 6=2 ASC(c2, ci) = 0.653, while for c1 the average score is significant lower, i.e., 0.522. This provides higher support for c2, which is used by ASR to rerank the output of PR.\nTab. 8 shows an interesting case where all the sentences contain the required information, i.e., February. However, PR and ASR both choose answer c0, which is correct but not natural, as it provides the requested information indirectly. Also, it contains a lot of ancillary information. In contrast, MASR is able to rerank the best answer, c1, in the top position."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have proposed new joint models for AS2. ASR encodes the relation between the target answer and all the other candidates, using an additional Transformer model, and an Answer Support Classifier, while MASR jointly models the ASR representations for all target answers. We extensively tested KGAT, ASR, MASR, and other joint model baselines we designed.\nThe results show that our models can outperform the state of the art. Most interestingly, ASR constantly outperforms all the models (but MASR-FP), on all datasets, through all measures, and for both base and large transformers. For example, ASR\nachieves the best reported results, i.e., MAP values of 92.80% and 94.88, on WikiQA and TRECQA, respectively. MASR improves ASR by 2% on WQA, since this contains enough data to train the ASR representations jointly."
    } ],
    "references" : [ {
      "title" : "Learning a deep listwise context model for ranking refinement",
      "author" : [ "Qingyao Ai", "Keping Bi", "Jiafeng Guo", "W. Bruce Croft." ],
      "venue" : "CoRR, abs/1804.05936.",
      "citeRegEx" : "Ai et al\\.,? 2018",
      "shortCiteRegEx" : "Ai et al\\.",
      "year" : 2018
    }, {
      "title" : "A compare-aggregate model with dynamic-clip attention for answer selection",
      "author" : [ "Weijie Bian", "Si Li", "Zhao Yang", "Guang Chen", "Zhiqing Lin." ],
      "venue" : "CIKM, pages 1987–1990. ACM.",
      "citeRegEx" : "Bian et al\\.,? 2017",
      "shortCiteRegEx" : "Bian et al\\.",
      "year" : 2017
    }, {
      "title" : "A study on efficiency, accuracy and document structure for answer sentence selection",
      "author" : [ "Daniele Bonadiman", "Alessandro Moschitti." ],
      "venue" : "CoRR, abs/2003.02349.",
      "citeRegEx" : "Bonadiman and Moschitti.,? 2020",
      "shortCiteRegEx" : "Bonadiman and Moschitti.",
      "year" : 2020
    }, {
      "title" : "Learning to rank: from pairwise approach to listwise approach",
      "author" : [ "Zhe Cao", "Tao Qin", "Tie-Yan Liu", "Ming-Feng Tsai", "Hang Li." ],
      "venue" : "Proceedings of the 24th international conference on Machine learning, ICML ’07, pages 129–136, New York, NY, USA.",
      "citeRegEx" : "Cao et al\\.,? 2007",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2007
    }, {
      "title" : "Reading wikipedia to answer opendomain questions",
      "author" : [ "Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes." ],
      "venue" : "CoRR, abs/1704.00051.",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "TANDA: transfer and adapt pre-trained transformer models for answer sentence selection",
      "author" : [ "Siddhant Garg", "Thuy Vu", "Alessandro Moschitti." ],
      "venue" : "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Ap-",
      "citeRegEx" : "Garg et al\\.,? 2020",
      "shortCiteRegEx" : "Garg et al\\.",
      "year" : 2020
    }, {
      "title" : "Retrieve, read, rerank: Towards end-to-end multi-document reading comprehension",
      "author" : [ "Minghao Hu", "Yuxing Peng", "Zhen Huang", "Dongsheng Li." ],
      "venue" : "CoRR, abs/1906.04618.",
      "citeRegEx" : "Hu et al\\.,? 2019",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2019
    }, {
      "title" : "Ranking via partial ordering for answer selection",
      "author" : [ "Zan-Xia Jin", "Bo-Wen Zhang", "Fang Zhou", "Jingyan Qin", "Xu-Cheng Yin." ],
      "venue" : "Information Sciences.",
      "citeRegEx" : "Jin et al\\.,? 2020",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "CoRR, abs/1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Adaptive document retrieval for deep question answering",
      "author" : [ "Bernhard Kratzwald", "Stefan Feuerriegel." ],
      "venue" : "EMNLP’18, pages 576–581.",
      "citeRegEx" : "Kratzwald and Feuerriegel.,? 2018",
      "shortCiteRegEx" : "Kratzwald and Feuerriegel.",
      "year" : 2018
    }, {
      "title" : "Albert: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "ICLR 2020.",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "Contextualized embeddings based transformer encoder for sentence similarity modeling in answer selection task",
      "author" : [ "Md Tahmid Rahman Laskar", "Jimmy Xiangji Huang", "Enamul Hoque." ],
      "venue" : "Proceedings of The 12th Language Resources and Evaluation Con-",
      "citeRegEx" : "Laskar et al\\.,? 2020",
      "shortCiteRegEx" : "Laskar et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Fine-grained fact verification with kernel graph attention network",
      "author" : [ "Zhenghao Liu", "Chenyan Xiong", "Maosong Sun", "Zhiyuan Liu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7342–7351, On-",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "SemEval-2019 task 8: Fact checking in community question answering forums",
      "author" : [ "Tsvetomila Mihaylova", "Georgi Karadzhov", "Pepa Atanasova", "Ramy Baly", "Mitra Mohtarami", "Preslav Nakov." ],
      "venue" : "Proceedings of the 13th International Workshop",
      "citeRegEx" : "Mihaylova et al\\.,? 2019",
      "shortCiteRegEx" : "Mihaylova et al\\.",
      "year" : 2019
    }, {
      "title" : "Noisecontrastive estimation for answer selection with deep neural networks",
      "author" : [ "Jinfeng Rao", "Hua He", "Jimmy J. Lin." ],
      "venue" : "CIKM, pages 1913–1916. ACM.",
      "citeRegEx" : "Rao et al\\.,? 2016",
      "shortCiteRegEx" : "Rao et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to rank short text pairs with convolutional deep neural networks",
      "author" : [ "Aliaksei Severyn", "Alessandro Moschitti." ],
      "venue" : "SIGIR’15.",
      "citeRegEx" : "Severyn and Moschitti.,? 2015",
      "shortCiteRegEx" : "Severyn and Moschitti.",
      "year" : 2015
    }, {
      "title" : "Inter-weighted alignment network for sentence pair modeling",
      "author" : [ "Gehui Shen", "Yunlun Yang", "Zhi-Hong Deng." ],
      "venue" : "EMNLP’17, pages 1179–1189, Copenhagen, Denmark.",
      "citeRegEx" : "Shen et al\\.,? 2017",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2017
    }, {
      "title" : "Integrating question classification and deep learning for improved answer selection",
      "author" : [ "Harish Tayyar Madabushi", "Mark Lee", "John Barnden." ],
      "venue" : "COLING’18, pages 3283–3294.",
      "citeRegEx" : "Madabushi et al\\.,? 2018",
      "shortCiteRegEx" : "Madabushi et al\\.",
      "year" : 2018
    }, {
      "title" : "FEVER: a large-scale dataset for fact extraction and VERification",
      "author" : [ "James Thorne", "Andreas Vlachos", "Christos Christodoulopoulos", "Arpit Mittal." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of",
      "citeRegEx" : "Thorne et al\\.,? 2018a",
      "shortCiteRegEx" : "Thorne et al\\.",
      "year" : 2018
    }, {
      "title" : "The fact extraction and VERification (FEVER) shared task",
      "author" : [ "James Thorne", "Andreas Vlachos", "Oana Cocarascu", "Christos Christodoulopoulos", "Arpit Mittal." ],
      "venue" : "Proceedings of the First Workshop on Fact Extraction and VERification",
      "citeRegEx" : "Thorne et al\\.,? 2018b",
      "shortCiteRegEx" : "Thorne et al\\.",
      "year" : 2018
    }, {
      "title" : "The TREC-8 Question Answering Track Evaluation, pages 77–82",
      "author" : [ "E. Voorhees", "D. Tice." ],
      "venue" : "Department of Commerce, National Institute of Standards and Technology.",
      "citeRegEx" : "Voorhees and Tice.,? 1999",
      "shortCiteRegEx" : "Voorhees and Tice.",
      "year" : 1999
    }, {
      "title" : "What is the Jeopardy model? a quasi-synchronous grammar for QA",
      "author" : [ "Mengqiu Wang", "Noah A. Smith", "Teruko Mitamura." ],
      "venue" : "EMNLPCoNLL’07, pages 22–32, Prague, Czech Republic. Association for Computational Linguistics.",
      "citeRegEx" : "Wang et al\\.,? 2007",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2007
    }, {
      "title" : "Multipassage machine reading comprehension with crosspassage answer verification",
      "author" : [ "Yizhong Wang", "Kai Liu", "Jing Liu", "Wei He", "Yajuan Lyu", "Hua Wu", "Sujian Li", "Haifeng Wang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Wikiqa: A challenge dataset for open-domain question answering",
      "author" : [ "Yi Yang", "Wen-tau Yih", "Christopher Meek." ],
      "venue" : "Proceedings of the 2015 conference on empirical methods in natural language processing, pages 2013–2018.",
      "citeRegEx" : "Yang et al\\.,? 2015",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime G. Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le." ],
      "venue" : "CoRR, abs/1906.08237.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "More accurate tests for the statistical significance of result differences",
      "author" : [ "Alexander S. Yeh." ],
      "venue" : "CoRR, cs.CL/0008005.",
      "citeRegEx" : "Yeh.,? 2000",
      "shortCiteRegEx" : "Yeh.",
      "year" : 2000
    }, {
      "title" : "A compareaggregate model with latent clustering for answer selection",
      "author" : [ "Seunghyun Yoon", "Franck Dernoncourt", "Doo Soon Kim", "Trung Bui", "Kyomin Jung." ],
      "venue" : "CoRR, abs/1905.12897.",
      "citeRegEx" : "Yoon et al\\.,? 2019",
      "shortCiteRegEx" : "Yoon et al\\.",
      "year" : 2019
    }, {
      "title" : "AnswerFact: Fact checking in product question answering",
      "author" : [ "Wenxuan Zhang", "Yang Deng", "Jing Ma", "Wai Lam." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2407–2417, Online. As-",
      "citeRegEx" : "Zhang et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Ms-ranker: Accumulating evidence from potentially correct candidates for answer selection",
      "author" : [ "Yingxue Zhang", "Fandong Meng", "Peng Li", "Ping Jian", "Jie Zhou." ],
      "venue" : "CoRR, abs/2010.04970.",
      "citeRegEx" : "Zhang et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "Among the different types of methods to implement QA systems, we focus on Answer Sentence Selection (AS2) research, originated from TREC-QA track (Voorhees and Tice, 1999), as it proposes efficient models that are more suitable for a production setting, e.",
      "startOffset" : 146,
      "endOffset" : 171
    }, {
      "referenceID" : 4,
      "context" : ", they are more efficient than those developed in machine reading (MR) work (Chen et al., 2017).",
      "startOffset" : 76,
      "endOffset" : 95
    }, {
      "referenceID" : 24,
      "context" : "(2020) proposed the TANDA approach based on pre-trained Transformer models, obtaining impressive improvement over the state of the art for AS2, measured on the two most used datasets, WikiQA (Yang et al., 2015) and TRECQA (Wang et al.",
      "startOffset" : 191,
      "endOffset" : 210
    }, {
      "referenceID" : 19,
      "context" : ", the methods developed in the FEVER challenge (Thorne et al., 2018a).",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 13,
      "context" : "Our first approach exploits Fact Checking research: we adapted a state-of-the-art FEVER system, KGAT (Liu et al., 2020), for AS2.",
      "startOffset" : 101,
      "endOffset" : 119
    }, {
      "referenceID" : 16,
      "context" : "Previous work estimates p(q, ci) with neural models (Severyn and Moschitti, 2015), also using attention mechanisms, e.",
      "startOffset" : 52,
      "endOffset" : 81
    }, {
      "referenceID" : 27,
      "context" : ", CompareAggregate (Yoon et al., 2019), inter-weighted alignment networks (Shen et al.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 17,
      "context" : ", 2019), inter-weighted alignment networks (Shen et al., 2017), and pre-trained Transformer models, which are the state of the art.",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 1,
      "context" : ", (Bian et al., 2017; Cao et al., 2007; Ai et al., 2018), aims at learning p(q, π), π ∈ Π(A), using the information on the entire set of candidates.",
      "startOffset" : 2,
      "endOffset" : 56
    }, {
      "referenceID" : 3,
      "context" : ", (Bian et al., 2017; Cao et al., 2007; Ai et al., 2018), aims at learning p(q, π), π ∈ Π(A), using the information on the entire set of candidates.",
      "startOffset" : 2,
      "endOffset" : 56
    }, {
      "referenceID" : 0,
      "context" : ", (Bian et al., 2017; Cao et al., 2007; Ai et al., 2018), aims at learning p(q, π), π ∈ Π(A), using the information on the entire set of candidates.",
      "startOffset" : 2,
      "endOffset" : 56
    }, {
      "referenceID" : 4,
      "context" : "Its application to retrieval scenario has also been studied (Chen et al., 2017; Hu et al., 2019; Kratzwald and Feuerriegel, 2018).",
      "startOffset" : 60,
      "endOffset" : 129
    }, {
      "referenceID" : 6,
      "context" : "Its application to retrieval scenario has also been studied (Chen et al., 2017; Hu et al., 2019; Kratzwald and Feuerriegel, 2018).",
      "startOffset" : 60,
      "endOffset" : 129
    }, {
      "referenceID" : 9,
      "context" : "Its application to retrieval scenario has also been studied (Chen et al., 2017; Hu et al., 2019; Kratzwald and Feuerriegel, 2018).",
      "startOffset" : 60,
      "endOffset" : 129
    }, {
      "referenceID" : 14,
      "context" : "In QA, answer verification is directly relevant due to its nature of content delivery (Mihaylova et al., 2019).",
      "startOffset" : 86,
      "endOffset" : 110
    }, {
      "referenceID" : 23,
      "context" : "The problem has been explored in MR setting (Wang et al., 2018).",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 12,
      "context" : ", 2018), RoBERTa (Liu et al., 2019), XLNet (Yang et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 25,
      "context" : ", 2019), XLNet (Yang et al., 2019), AlBERT (Lan et al.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : ", RoBERTa models fine-tuned on ASNQ (Garg et al., 2020).",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 24,
      "context" : "WikiQA is a QA dataset (Yang et al., 2015) containing a sample of questions and answer-sentence candidates from Bing query logs over Wikipedia.",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 5,
      "context" : "(ii) From the set of retrieved documents, all candidate sentences are extracted and ranked using AS2 models from (Garg et al., 2020).",
      "startOffset" : 113,
      "endOffset" : 132
    }, {
      "referenceID" : 20,
      "context" : "Table 3 shows the statistics of the dataset, which remains the same as in (Thorne et al., 2018b).",
      "startOffset" : 74,
      "endOffset" : 96
    }, {
      "referenceID" : 8,
      "context" : "Reranker training We adopt Adam optimizer (Kingma and Ba, 2014) with a learning rate of 2e-5 for the transfer step on the ASNQ dataset (Garg et al.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "Reranker training We adopt Adam optimizer (Kingma and Ba, 2014) with a learning rate of 2e-5 for the transfer step on the ASNQ dataset (Garg et al., 2020), and a learning rate of 1e-6 for the adapt step on the target dataset.",
      "startOffset" : 135,
      "endOffset" : 154
    }, {
      "referenceID" : 13,
      "context" : "The other training configurations are the same of the original KGAT model from (Liu et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 97
    }, {
      "referenceID" : 26,
      "context" : "• We perform randomization test (Yeh, 2000) to verify if the models significantly differ in terms of prediction outcome.",
      "startOffset" : 32,
      "endOffset" : 43
    } ],
    "year" : 2021,
    "abstractText" : "This paper studies joint models for selecting correct answer sentences among the top k provided by answer sentence selection (AS2) modules, which are core components of retrievalbased Question Answering (QA) systems. Our work shows that a critical step to effectively exploiting an answer set regards modeling the interrelated information between pair of answers. For this purpose, we build a three-way multiclassifier, which decides if an answer supports, refutes, or is neutral with respect to another one. More specifically, our neural architecture integrates a state-of-the-art AS2 module with the multi-classifier, and a joint layer connecting all components. We tested our models on WikiQA, TREC-QA, and a real-world dataset. The results show that our models obtain the new state of the art in AS2.",
    "creator" : "LaTeX with hyperref"
  }
}