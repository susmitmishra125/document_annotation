{
  "name" : "2021.acl-long.292.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Self-Attention Networks Can Process Bounded Hierarchical Languages",
    "authors" : [ "Shunyu Yao", "Binghui Peng", "Christos Papadimitriou", "Karthik Narasimhan" ],
    "emails" : [ "karthikn}@princeton.edu", "christos}@columbia.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3770–3785\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3770"
    }, {
      "heading" : "1 Introduction",
      "text" : "Transformers (Vaswani et al., 2017) are now the undisputed champions across several benchmark leaderboards in NLP. The major innovation of this architecture, self-attention, processes input tokens in a distributed way, enabling efficient parallel computation as well as long-range dependency modelling. The empirical success of self-attention in NLP has led to a growing interest in studying its properties, with an eye towards a better understanding of the nature and characteristics of natural language (Tran et al., 2018; Papadimitriou and Jurafsky, 2020).\n1Code is available at https://github.com/ princeton-nlp/dyck-transformer.\nIn particular, it was recently shown that selfattention networks cannot process various kinds of formal languages (Hahn, 2020; Bhattamishra et al., 2020a), among which particularly notable is Dyckk, the language of well-balanced brackets of k types. By the Chomsky-Schützenberger Theorem (Chomsky and Schützenberger, 1959), any context-free language can be obtained from a Dyckk language through intersections with regular languages and homomorphisms. In other words, this simple language contains the essence of all context-free languages, i.e. hierarchical structure, center embedding, and recursion – features which have been long claimed to be at the foundation of human language syntax (Chomsky, 1956).\nConsider for example the long-range and nested dependencies in English subject-verb agreement:\n(Laws (the lawmaker) [writes] [and revises]) [pass]. . . .\nThe sentence structure is captured by Dyck2 string (()[][])[]. Given the state-of-the-art performance of Transformers in parsing natural language (Zhang et al., 2020; He and Choi, 2019), the Dyckk blind spot seems very suggestive. If the world’s best NLP models cannot deal with this simple language — generated by a grammar with k + 2 rules and recognized by a single-state pushdown automaton — does this not mean that the role of hierarchy and recursion in natural language must be limited? This question has of course, been extensively debated by linguists on the basis of both theoretical and psycholinguistic evidence (Hauser et al., 2002; Frank et al., 2012; Nelson et al., 2017; Brennan and Hale, 2019; Frank and Christiansen, 2018).\nSo, what can self-attention networks tell us about natural language and recursion? Here we provide a new twist to this question by considering Dyckk,D, the subset of Dyckk with nesting depth at most D, and show that Transformers can process\nit. Dyckk,D models bounded (or finite) recursion, thus captures the hierarchical structure of human language much more realistically. For example, center-embedding depth of natural language sentences is known to rarely exceed three (Karlsson, 2007; Jin et al., 2018), and while pragmatics, discourse, and narrative can result in deeper recursion in language (Levinson, 2014), there is arguably a relatively small limit to the depth as well.\nIn particular, we prove that self-attention networks can both recognize and generate Dyckk,D, with two conceptually simple yet different constructions (Figure 1). The first network requires D + 1 layers and a memory size ofO(log k) (per layer per token) to recognize Dyckk,D, using a distributed mechanism of parenthesis matching. The second network has two layers and memory size O(log k). It works by attending to all previous tokens to count the depth for each token in the first layer, and then uses this depth information to attend to the most recent unclosed open bracket in the second layer. Our constructions help reconcile the result in Hahn (2020) with the success of Transformers in handling natural languages.\nOur proof requires certain assumptions about the positional encodings, an issue that is often considered in empirical papers (Ke et al., 2021; Shaw et al., 2018; Wang et al., 2020; Shiv and Quirk, 2019) but not in the more theoretical literature. First, positional encodings must have log n bits when the input length is n, as otherwise different positions would share the same representation. More importantly, positional encodings should support easy position comparisons, since token order\nis vital in formal language processing. Our experiments show that two standard practices, namely learnable or fixed sine/cosine positional encodings, cannot generalize well on Dyckk,D beyond the training input lengths. In contrast, using a single fixed scalar monotonic positional encoding such as pos/n achieves near-perfect accuracy even on inputs significantly longer than the training ones. Our findings provide a novel perspective on the function of positional encodings, and implies that different applications of self-attention networks (in this case, natural vs. formal language) may require different model choices.\nOur theoretical results also bring about interesting comparisons to recurrent networks (e.g. RNNs, LSTMs) in terms of the resource need to process hierarchical structure. While recurrent networks with finite precision need at least Ω(D log k) memory to process Dyckk,D (Hewitt et al., 2020), our second construction requires only O(log k) memory but aO(log n) precision. In experiments where precision is not an issue for practical input lengths (< 104), we confirm that a Transformer requires less memory than a LSTM to reach high test accuracies. This may help explain why Transformers outperform RNNs/LSTMs in syntactical tasks in NLP, and shed light into fundamental differences between recurrent and non-recurrent sequence processing."
    }, {
      "heading" : "2 Related work",
      "text" : "Our work primarily relates to the ongoing effort of characterizing theoretical abilities (Pérez et al., 2019; Bhattamishra et al., 2020b; Yun et al., 2020)\nand limitations of self-attention networks, particularly through formal hierarchical structures like Dyckk. Hahn (2020) proves that (even with positional encodings) hard-attention Transformers cannot model Dyckk, and soft-attention Transformers with bounded Lipschitz continuity cannot model Dyckk with perfect cross entropy. Bhattamishra et al. (2020a) prove a soft-attention network with positional masking (but no positional encodings) can solve Dyck1 but not Dyck2. Despite the expressivity issues theoretically posed by the above work, empirical findings have shown Transformers can learn Dyckk from finite samples and outperform LSTM (Ebrahimi et al., 2020). Our work addresses the theory-practice discrepancy by using positional encodings and modeling Dyckk,D.\nA parallel line of work with much lengthier tradition (Elman, 1990; Das et al., 1992; Steijvers and Grünwald, 1996) investigates the abilities and limitations of recurrent networks to process hierarchical structures. In particular, RNNs or LSTMs are proved capable of solving context-free languages like Dyckk given infinite precision (Korsky and Berwick, 2019) or external memory (Suzgun et al., 2019; Merrill et al., 2020). However, Merrill et al. (2020) also prove RNNs/LSTMs cannot process Dyckk without such assumptions, which aligns with experimental findings that recurrent networks perform or generalize poorly on Dyckk (Bernardy, 2018; Sennhauser and Berwick, 2018; Yu et al., 2019). Hewitt et al. (2020) propose to consider Dyckk,D as it better captures natural language, and show finite-precision RNNs can solve Dyckk,D with Θ(D log k) memory.\nFor the broader NLP community, our results also contribute to settling whether self-attention networks are restricted to model hierarchical structures due to non-recurrence, a concern (Tran et al., 2018) often turned into proposals to equip Transformers with recurrence (Dehghani et al., 2019; Shen et al., 2018; Chen et al., 2018; Hao et al., 2019). On one hand, Transformers are shown to encode syntactic (Lin et al., 2019; Tenney et al., 2019; Manning et al., 2020) and word order (Yang et al., 2019) information, and dominate syntactical tasks in NLP such as constituency (Zhang et al., 2020) and dependency (He and Choi, 2019) parsing. On the other hand, on several linguistically-motivated tasks like English subject-verb agreement (Tran et al., 2018), recurrent models are reported to outperform Transformers. Our results help address\nthe issue by confirming that distributed and recurrent sequence processing can both model hierarchical structure, albeit with different mechanisms and tradeoffs."
    }, {
      "heading" : "3 Preliminaries",
      "text" : ""
    }, {
      "heading" : "3.1 Dyck Languages",
      "text" : "Consider the vocabulary of k types of open and close brackets Σ = ∪i∈[k]{〈i, 〉i}, and define Dyckk ⊂ γΣ∗ω (γ, ω being special start and end tokens) to be the formal language of well-nested brackets of k types. It is generated starting from γXω through the following context-free grammar:\nX → | 〈i X 〉i X (i ∈ [k]) (1)\nwhere denotes the empty string. Intuitively, Dyckk can be recognized by sequential scanning with a stack (i.e., a pushdown automaton). Open brackets are pushed into the stack, while a close bracket causes the stack to pop, and the popped open bracket is compared with the current close bracket (they should be of the same type). The depth of a string w1:n at position i is the stack size after scanning w1:i, that is, the number of open brackets left in the stack:\nd(w1:i) = count(w1:i, 〈)− count(w1:i, 〉) (2)\nFinally, we define Dyckk,D to be the subset of Dyckk strings with depth bounded by D:\nDyckk,D = { w1:n ∈ Dyckk ∣∣∣∣maxi∈[n] d(w1:i) ≤ D }\nThat is, a string in Dyckk,D only requires a stack with bounded size D to process."
    }, {
      "heading" : "3.2 Self-attention Networks",
      "text" : "We consider the encoder part of the original Transformer (Vaswani et al., 2017), which has multiple layers of two blocks each: (i) a self-attention block and (ii) a feed-forward network (FFN). For an input string w1:n ∈ Σ∗, each input token wi is converted into a token embedding via fe : Σ→ Rdmodel , then added with a position encoding pi ∈ Rdmodel . Let xi,` ∈ Rdmodel be the i-th representation of the `-th layer (i ∈ [n], ` ∈ [L]). Then\nxi,0 = fe(wi) + pi (3)\nai,` = Att`(Q`(xi),K`(x), V`(x)) (4)\nxi,`+1 = F`(ai,`) (5)\nAttention In each head of a self-attention block, the input vectors x1:n undergo linear transforms Q,K, V yielding query, key, and value vectors. They are taken as input to a self-attention module, whose t-th output, Att(Qxi,Kx, V x), is a vector ai = ∑ j∈[T ] αjV xj , where α1:n = softmax(〈Qxi,Kx1〉, · · · , 〈Qxi,Kxn〉). The final attention output is the concatenation of multihead attention outputs. We also consider variants of the basic model along these directions:\n(i) Hard attention, as opposed to soft attention described above, where hardmax is used in place for softmax (i.e. Att(Qxi,Kx, V x) = V xj′ where j′ = arg maxj〈Qxi,Kxj〉). Though impractical for NLP, it has been used to model formal languages (Hahn, 2020).\n(ii) Positional masking, where α1:i (past) or αi:n (future) is masked for position i. Future-positional masking is usually used to train auto-regressive models like GPT-2 (Radford et al., 2019).\nFeed-forward network A feed-forward network F transforms each self-attention output vector ai → F (ai) individually. It is usually implemented as a multi-layer perceptron (MLP) with ReLU activations. Residual connections (He et al., 2016) and layer normalization (Ba et al., 2016) are two optional components to aid learning.\nPositional encodings Vaswani et al. (2017) proposes two kinds of positional encoding: (i) Fourier features (Rahimi and Recht, 2007), i.e. sine/cosine values of different frequencies; (ii) learnable features for each position. In this work we propose to use a single scalar i/n to encode position i ∈ [n], and show that it helps process formal languages like Dyckk,D, both theoretically and empirically.\nPrecision and memory size We define precision to be the number of binary bits used to represent each scalar, and memory size per layer (dmodel) to be the number of scalars used to represent each token at each layer. The memory size (L · dmodel) is the total memory used for each token."
    }, {
      "heading" : "3.3 Language Generation and Recognition",
      "text" : "For a Transformer with L layers and input w1:i, we can use a decoder (MLP + softmax) on the final token output xi,L to predict wi+1. This defines a language model fθ(wi+1|wi) where θ denotes Transformer and decoder parameters. We follow previous work (Hewitt et al., 2020) to define how a language model can generate a formal language:\nDefinition 3.1 (Language generation). Language model fθ over Σ? generates a language L ⊆ Σ? if there exists > 0 such that L = {w1:n ∈ Σ? | ∀i ∈ [n], fθ(wi|w1:i−1) ≥ }.\nWe also consider language recognition by a language classifier gθ(w1:i), where a decoder on xi,L instead predicts a binary label. Definition 3.2 (Language recognition). Language classifier gθ over Σ? recognizes a language L ⊆ Σ? if L = {w1:n ∈ Σ? |gθ(w1:n) = 1}."
    }, {
      "heading" : "4 Theoretical Results",
      "text" : "In this section we state our theoretical results along with some remarks. Proof sketches are provided in the next section, and details in Appendix A,B,C. Theorem 4.1 (Hard-attention, Dyckk,D recognition). For all k,D ∈ N+, there exists a (D + 1)- layer hard-attention network that can recognize Dyckk,D. It uses both future and past positional masking heads, positional encoding of the form i/n for position i, O(log k) memory size per layer, and O(log n) precision, where n is the input length. Theorem 4.2 (Soft-attention, Dyckk,D generation). For all k,D ∈ N+, there exists a 2-layer softattention network that can generate Dyckk,D. It uses future positional masking, positional encoding of form i/n for position i, O(log k) memory size per layer, and O(log n) precision, where n is the input length. The feed-forward networks use residual connection and layer normalization. Theorem 4.3 (Precision lower bound). For all k ∈ N+, no hard-attention network with o(log n) precision can recognize Dyckk,2 where n is the input length.\nRequired precision Both constructions require a precision increasing with input length, as indicated by Theorem 4.3. The proof of the lower bound is inspired by the proof in Hahn (2020), but several technical improvements are necessary; see Appendix C. Intuitively, a vector with a fixed dimension and o(log n) precision cannot even represent n positions uniquely. The required precision is not unreasonable, since log n is a small overhead to the n tokens the system has to store.\nComparison to recurrent processing Hewitt et al. (2020) constructs a 1-layer RNN to generate Dyckk,D with Θ(D log k) memory, and proves it is optimal for any recurrent network. Thus Theorem 4.2 establishes a memory advantage of selfattention networks over recurrent ones. However,\nthis is based on two tradeoffs: (i) Precision. Hewitt et al. (2020) assumes O(1) precision while we require O(log n). (ii) Runtime. Runtime of recurrent and self-attention networks usually scale linearly and quadratically in n, respectively.\nComparison between two constructions Theorem 4.2 requires fewer layers (2 vs.D) and memory size (O(log k) vs.O(D log k)) than Theorem 4.1, thanks to the use of soft-attention, residual connection and layer normalization. Though the two constructions are more suited to the tasks of recognition and generation respectively (Section 5), each of them can also be modified for the other task.\nConnection to Dyckk In Hahn (2020) it is shown that no hard-attention network can recognize Dyckk even for k = 1. Theorem 4.1 establishes that this impossibility can be circumvented by bounding the depth of the Dyck language. Hahn (2020) also points out soft-attention networks can be limited due to bounded Lipschitz continuity. In fact, our Theorem 4.2 construction can also work on Dyckk with some additional assumptions (e.g. feed n also in input embeddings), and we circumvent the impossibility by using laying normalization, which may have an O(n) Lipschitz constant. More details are in Appendix B.4."
    }, {
      "heading" : "5 Constructions",
      "text" : "5.1 (D + 1)-layer Hard-Attention Network\nOur insight underlying the construction in Theorem 4.1 is that, by recursively removing matched brackets from innermost positions to outside, each token only needs to attend to nearest unmatched brackets to find its matching bracket or detect error within D layers. Specifically, at each layer ` ≤ D, each token will be in one of three states (Figure 2 (c)): (i) Matched, (ii) Error, (iii) Unmatched, and we leverage hard-attention to implement a dynamic state updating process to recognize Dyckk,D.\nRepresentation For an input w1:n ∈ γΣ∗ω, the representation at position i of layer ` has five parts xi,` = [ti, oi, pi,mi,`, ei,`]: (i) a bracket type embedding ti ∈ Rdlog ke that denotes which bracket type (1 · · · k) the token is (or if the token is start/end token); (ii) a bracket openness bit oi ∈ {0, 1}, where 1 denotes open brackets (or start token) and 0 denotes close one (or end token); (iii) a positional encoding scalar pi = i/n; (iv) a match bit\nmi,` ∈ {0, 1}, where 1 denotes matched and 0 unmatched; (v) an error bit ei,` ∈ {0, 1}, where 1 denotes error and 0 no error. Token identity parts ti, oi, pi are maintained unchanged throughout layers. The match and error bits are initialized as ei,0 = mi,0 = 0.\nThe first D layers have identical self-attention blocks and feed-forward networks, detailed below.\nAttention Consider the `-th self-attention layer (` ∈ [D]), and denote xi = xi,`−1, mi = mi,`−1, ai = ai,`, yi = xi,` for short. We have 3 attention heads: (i) an identity head Attid, where each token only attends to itself with attention output aidi = xi; (ii) a left head Att\nleft with future positional masking; (iii) a right head Attright with past positional masking. The query, key, and value vectors for Attleft are defined as Qxi = 1 ∈ R, Kxi = pi −mi ∈ R, V xi = xi ∈ Rdmodel , so that\nalefti = xj1 , j1 = arg max j<i (j/n−mj)\nis the representation of the nearest unmatched token to i on its left side. Similarly\narighti = xj2 , j2 = arg maxj>i (1− j/n−mj)\nis the representation of the nearest unmatched token to i on its right side. The attention output for position i is the concatenation of these three outputs: ai = [aidi ,a left i ,a right i ] = [xi,xj1 ,xj2 ].\nFeed-forward network (FFN) Following the notation above, the feed-forward networkF : ai → yi serves to update each position’s state using information from xj1 ,xj2 . The high level logic (Figure 2 (c)) is that, if wi is an open bracket, its potential matching half should be wj = wj2 (j2 > i), otherwise it should bewj = wj1 (j1 < i). Ifwi and wj are one open and one close, they either match (same type) or cause error (different types). If wi and wj are both open or both close, no state update is done for position i. Besides, token identity parts ti, oi, pi are copied from aidi to pass on. The idea can be translated into a language of logical operations (∧,∨,¬) plus a SAME(t, t′) operation, which returns 1 if vectors t = t′ and 0 otherwise:\nyi = [ti, oi, pi,m ′ i, e ′ i]\nm′i = mi ∨ (oi ∧ ¬oj2 ∧ s1) ∨ (¬oi ∧ oj1 ∧ s2) e′i = ei ∨ (oi ∧ ¬oj2 ∧ ¬s1) ∨ (¬oi ∧ oj1 ∧ ¬s2) s1 = SAME(ti, tj1) s2 = SAME(ti, tj2)\nAs we show in Appendix A, a multi-layer perception with ReLU activations can simulate all operations (∧,∨,¬, SAME), thus the existence of our desired FFN.\nFinal layer At the (D + 1)-th layer, the self attention is designed as Qxi = 1 ∈ R, Kxi = ei+1−mi ∈ R, V xi = (ei,mi) ∈ R2. If all brackets are matched without error ((ei,mi) = (0, 1)), all keys would be 0, and the attention output of the last token an would be (0, 1). If any bracket finds error (ei = 1) or is not matched (mi = 0), the key would be at least 1 and an would not be (0, 1). An FNN that emulates (a, b) 7→ ¬a∧ b will deliver yn as the recognition answer."
    }, {
      "heading" : "5.2 Two-layer Soft-Attention Network",
      "text" : "Our Theorem 4.2 construction takes advantage of soft attention, residual connection, and layer normalization to calculate each token depth and translate it into a vector form at the first layer. Using the depth information, at the second layer each wi can attend to the stack-top open bracket at the position, in order to decide if open brackets or which type of close brackets can be generated as the next token (Figure 3).\nRepresentation The representation at position i, layer ` has four parts xi,` = [ti, oi, pi,di,`], with\nbracket type embedding ti, bracket openness bit oi, position encoding pi already specified in Section 5.1. The last part di,` ∈ R2 is used to store depth information for position i, and initialized as di,0 = (0, 0).\nFirst Layer – Depth Counting The first selfattention layer has two heads, where an Attid head is still used to inherit ti, oi,pi, and a future positional masking head2 Attd aims to count depth with Qxi = Kxi = 1 and V xi = 2oi − 1, resulting in uniform attention scores and attention output adi = ∑ j≤i 1 i · (2oj − 1) = d(w1:i)/i.\nHowever, our goal is to enable matching based on depth di = d(w1:i), and the attention output di/i isn’t readily usable for such a purpose: the denominator i is undesirable, and even a scalar di cannot easily attend to the same value using dotproduct attention. Thus in the first feed-forward network, we leverage residual connection and layer normalization to transform\ndi/i 7→ di = (cos(θ(di)), sin(θ(di))) (6)\nwhere θ(d) = arctan (\nd D+2−d\n) has an unique\n2Here we assume wi+1:n is masked for position i, just for convenience of description.\nvalue for every d ∈ {0, · · · , D + 1}, so that\ndi · dj\n{ = 1 di = dj\n< 1− 1 10D2 di 6= dj (7)\nThe representation by the end of first layer is xi,1 = [ti, oi, pi,di]. The full detail for the first FFN is in Appendix B.1.\nSecond layer – Depth Matching The second self-attention layer has a depth matching hardattention head Attmatch, with query, key, value vectors as Qxi = [20D2 · di, 1, 2] ∈ R4, Kxi = [di, pi, oi] ∈ R4, V xi = xi, so that attention score\n〈Qxi,Kxj〉 = 20D2di · dj + j/n+ 2oj{ = 20D2 + 2 + j/n di = dj , oj = 1\n≤ 20D2 + 1 otherwise\nwould achieve its maximum when wj (j ≤ i) is the open bracket (or start token) closest towi with dj = di. The attention output is ai = [aidi ,a match i ] = [xi,xj ] where j = max{j ≤ i|di = dj ∧ oj = 1}. With such a [xi,xj ], the second-layer FFN can readily predict what wi+1 could be. It could be any open bracket when di < D (i.e. cos(θ(di)) > cos(θ(D))), and it could be a close bracket with type as tj (or end token if wj is start token). The detailed construction for such a FFN is in Appendix B.2.\nOn Dyckk Generation In fact, this theoretical construction can also generate Dyckk, as intuitively the O(log n) precision assumption allows counting\ndepth up to O(n). But it involves extra conditions like feeding n into network input, and may not be effectively learned in practice. Please refer to details in Appendix B.4.\nConnection to Empirical Findings Our theoretical construction explains the observation in Ebrahimi et al. (2020): the second layer of a twolayer Transformer trained on Dyckk often produces virtually hard attention, where tokens attend to the stack-top open bracket (or start token). It also explains why such a pattern is found less systematically as input depth increases, as (6) is hard to learn and generalize to unbounded depth in practice."
    }, {
      "heading" : "6 Experiments",
      "text" : "Our constructions show the existence of selfattention networks that are capable of recognizing and generating Dyckk,D. Now we bridge theoretical insights into experiments, and study whether such networks can be learned from finite samples and generalize to longer input. The answer is affirmative when the right positional encodings and memory size are chosen according to our theory.\nWe first present results on Dyck8,10 (Section 6.1) as an example Dyckk,D language to investigate the effect of different positional encoding schemes, number of layers, and hidden size on the Transformer performance, and to compare with the LSTM performance. We then extend the Transformer vs. LSTM comparison on more Dyckk,D languages (k ∈ {2, 8, 32, 128}, D ∈ {3, 5, 10, 15}) in Section 6.2. Finally, we apply\nthe novel scalar positional encoding to natural language modeling with some preliminary findings (Section 6.3).\n6.1 Evaluation on Dyck8,10 Setup For Dyck8,10, we generate training and validation sets with input length n ≤ 700, and test set with length 700 < n ≤ 1400. We train randomly initialized Transformers using the Huggingface library (Wolf et al., 2019), with one future positional masking head, L ∈ {1, 2, 3, 4, 5, 10} layers, and a default memory size dmodel = 30. We search for learning rates in {0.01, 0.001}, run each model with 3 trials, and report the average accuracy of generating close brackets, the major challenge of Dyckk,D. More setup details are in Appendix D.1.\nPositional Encodings We compare 3 types of positional encodings: (i) Fourier features (COS); (ii) learnable features (LEARN); (iii) a scalar i/6000 for position i (POS/N). Note that (i, ii) are original proposals in Vaswani et al. (2017), where positional encoding vectors are added to the token embeddings, while our proposal (iii) encodes the position as a fixed scalar separated from token embeddings.\nOn the validation set of Dyck8,10 (see Appendix D.2), all three models achieve near-perfect accuracy with L ≥ 2 layers. On the test set (Figure 4(a)) however, only POS/N maintains nearperfect accuracy, even with L = 10 layers. Meanwhile, LEARN and COS fail to generalize, because encodings for position 700 < i ≤ 1400 are not learned (for LEARN) or experienced (for COS) during training. The result validates our theoretical construction, and points to the need for separate\nand systemic positional encodings for processing long and order-sensitive sequences like Dyckk,D.\nMemory Size and Comparison with LSTM We compare a two-layer Transformer (POS/N) with a one-layer LSTM3 (Hochreiter and Schmidhuber, 1997) using varying per-layer memory sizes dmodel ∈ {10, 20, · · · , 100}. As Figure 4 (b) shows, the Transformer consistently outperforms the LSTM on the validation set. On the test set (Figure 4 (c)), the Transformer and the LSTM first achieve a > 90% accuracy using dmodel = 20 and 40 respectively, and an accuracy of > 95% with dmodel = 30 and 50, respectively. These findings agree with our theoretical characterization that selfattention networks have a memory advantage over recurrent ones.\n6.2 Evaluation on More Dyckk,D Languages\nSetup In order to generalize some of the above results, we generate a wide range of Dyckk,D languages with different vocabulary sizes (k ∈ {2, 8, 32, 128}) and recursion bounds (D ∈ {3, 5, 10, 15}). We continue to compare the onelayer LSTM versus the two-layer Transformer (POS/N). For each model on each language, we perform a hyperparameter search for learning rate in {0.01, 0.001} and memory size dmodel ∈ {10, 30, 50}, and report results from the best setting based on two trials for each setting.\n3LSTMs only need one layer to process Dyckk,D (Hewitt et al., 2020), while Transformers at least need two in our constructions. We also experimented with two-layer LSTMs but did not find improved performance.\n0 50 100 150 Epoch\n2\n4\n6\n8\n10\nLo ss\nRoBERTa (WikiText-103)\nPositional Encoding learn pos/N Split Train Validation\nFigure 6: Results on WikiText-103.\nResults The validation and test accuracy of the models are reported in Figure 5, and more finegrained results for each dmodel ∈ {10, 30, 50} are in Appendix D.2. The Transformer attains a > 99.9% validation accuracy and a > 94% test accuracy across all languages, strengthening the main claim that self-attention networks can learn Dyckk,D languages and generalize to longer input. On the other hand, the validation and test accuracy of the LSTM model are less than 80% when the vocabulary size and recursion depth are large, i.e. (k,D) ∈ {(32, 15), (128, 10), (128, 15)}4, which reconfirms Transformers’ memory advantage under limited memory (dmodel ≤ 50)."
    }, {
      "heading" : "6.3 Evaluation on WikiText-103",
      "text" : "In Section 6.1, we show a Transformer with the scalar positional encoding scheme (POS/N) can learn Dyckk,D and generalize to longer input, while traditional positional encoding schemes ((COS), (LEARN)) lead to degraded test performance. To investigate whether such a novel scheme is also useful in NLP tasks, we train two RoBERTa5 models (POS/N, LEARN) from scratch on the WikiText103 dataset (Merity et al., 2017) for 150 epochs.\nFigure 6 shows the masked language modeling loss on both training and validation sets. By the end of the training, POS/N has a slightly larger validation loss (1.55) than LEARN (1.31). But throughout the optimization, POS/N shows a gradual decrease of loss while LEARN has a sudden drop of loss around 20-30 epochs. We believe it will be interest-\n4Note that Hewitt et al. (2020) only reports D ∈ {3, 5}. 5We also tried language modeling with GPT-2 models, and POS/N has slightly larger train/validation losses than LEARN throughout the training. Interestingly, using no positional encoding leads to the same loss curves as LEARN, as positional masking leaks positional information.\ning for future work to explore how POS/N performs on different downstream tasks, and why POS/N seems slightly worse than LEARN (at least on this MLM task), though theoretically it provides the complete positional information for Transformers. These topics will contribute to a deeper understanding of positional encodings and how Transformers leverage positional information to succeed on different tasks."
    }, {
      "heading" : "7 Discussion",
      "text" : "In this paper, we theoretically and experimentally demonstrate that self-attention networks can process bounded hierarchical languages Dyckk,D, even with a memory advantage over recurrent networks, despite performing distributed processing of sequences without explicit recursive elements. Our results may explain their widespread success at modeling long pieces of text with hierarchical structures and long-range, nested dependencies, including coreference, discourse and narratives. We hope these insights can enhance knowledge about the nature of recurrence and parallelism in sequence processing, and lead to better NLP models."
    }, {
      "heading" : "Acknowledgement",
      "text" : "We thank Xi Chen, members of the Princeton NLP Group, and anonymous reviewers for suggestions and comments.\nEthical Consideration\nOur work is mainly theoretical with no foreseeable ethical issues."
    }, {
      "heading" : "A Construction Details of Section 5.1",
      "text" : "We provide missing details on the construction of (D + 1)-layer Transformer with hard attention. In particular, we prove that neural networks are capable of simulating logic gates: AND, OR, NOT, SAME and arithmic gates: GREATERTHAN and EQUAL gate. For input x, y ∈ R, the GREATERTHAN satisfies that GREATERTHAN(x, y) = 1 if x ≥ y + c and GREATERTHAN(x, y) = 0 when x < y; the EQUAL gate satisfies EQUAL(x, y) = 1 if x = y and EQUAL(x, y) = 0 when x < y−c or x > y+c. Here c is a constant independent of x, y. Lemma A.1. A constant layer neural network can simulate logic gates: AND, OR, NOT, SAME and arithmic gates: GREATERTHAN, EQUAL.\nProof. Our construction is as follows. (1) AND gate. Given input x1, . . . , xm ∈ {0, 1}, we compute z = max{x1 + · · ·+xm−m+ 1, 0}. We conclude that z = 1 iff x1 = · · · = xm = 1 and z = 0 otherwise.\n(2) NOT gate. Given input x ∈ {0, 1}, it suffices to compute z = max{1− x, 0}.\n(3) OR gate. Given input x1, . . . , xm ∈ {0, 1}, we compute z = max{1 − max{1 − x1 − · · · − xm, 0}, 0}. It is easy to see that z = 1 iff one of xi = 1 (i ∈ [m]) and z = 0 otherwise.\n(3) SAME gate. Given input x1, . . . , xm ∈ {0, 1} and y1, . . . , ym ∈ {0, 1}. The SAME gate is equivalent to z = ((x1 ∨ y1) ∧ (x1 ∨ y1)) ∨ · · · ∨ ((xm ∨ ym) ∧ (xm ∨ ym)). We can construct it using logic gates: AND, OR, NOT .\n(4) GREATERTHAN gate. Given x, y ∈ R, compute z1 = 1c max{c−max{x− y, 0}, 0}, we have that z1 = 0 when x > y + c and z = 1 when x ≤ y. Taking z = max{1− z1, 0} completes the construction.\n(5) EQUAL gate. Given x, y ∈ R. Let z1 = GREATEREQUAL(x, y) and z2 = GREATEREQUAL(y, x). It suffices to take z = ¬z1 ∧ ¬z2.\nWith some extra effort, one can extend the construction for recognition task to generation task and prove that a D-layer Transformer is capable of generating Dyckk,D. Corollary A.2. ∀k,D ∈ N+, there exists a Dlayer hard-attention network that can generate Dyckk,D. It uses both a future-position masking head and a past-position masking head, a O(log k) memory size, and O(log n) precision for processing input length up to n.\nSoft attention Both Theorem 4.1 and Corollary A.2 can be adapted to soft attention, by setting the temperature parameter η in softmax operator to be sufficient large, say η = Ω(n log nD). Then one can use soft attention to simulate hard attention. In order to fit the precision, for the soft attention distribution p = [p1, · · · , pm], we round pi to the closest multiple of 1Cn , where C is a large constant."
    }, {
      "heading" : "B Construction Details of Section 5.2",
      "text" : "We provide missing details of the construction in Section 5.2.\nB.1 First Layer FFN Recall the output of the first attention layer is ai,1 = [ti, oi, pi,di,1], where ti, oi, pi are the bracket type embedding, the bracket openness bit and the position encoding. di,1 ∈ R2 contains the information di/i, where di = d(w1:i) equals the depth at position i. For ease of presentation, we assume it also contains an entry with 1/i, this can be derived with an extra attention head in the first layer or be inherited from an extra position encoding. Define θ(d) = arctan ( d\nD+2−d\n) . We prove\nLemma B.1. With residual connection and layer normalization, a two-layer MLP can perform the following transformation\n(di/i, 1/i) 7→ di = (cos(θ(di)), sin(θ(di)))\nwhile keeping ti, oi, pi unchanged.\nProof. Consider the following series of operations.( ti, oi, pi,\ndi i , 1 i , 0, 0 ) 7→ ( 0, 0, 0,−di\ni , di −D − 2 i , di i , D + 2− di i ) 7→ ( 0, 0, 0,−1\n2 sin(θ(di)),−\n1 2 cos(θ(di)),\n1 2 sin(θ(di)), 1 2 cos(θ(di)) ) 7→ ( 0, 0, 0, 0, 0, 1\n2 sin(θ(di)),\n1 2 cos(θ(di)) ) 7→ ( ti, oi, pi,\ndi i , 1 i , 1 2 sin(θ(di)), 1 2 cos(θ(di)) ) 7→ (ti, oi, pi, cos(θ(di)), sin(θ(di)), 0, 0))\nThe first steps can be achieved with a linear transformation, the second step can be achieved by layer\nnormalization and the third step follows from the ReLU activation gate, the fourth step comes from the residual connection and the last step can be obtained with an extra layer of MLP. We conclude the proof here.\nB.2 Second Layer FFN\nWe can choose between k open brackets and the matched close bracket, with the exception on a few boundary cases: (1) The depth of the current bracket reaches the maximum; (2) The length of the sequence is about to reach the maximum. Let m̃i be the bracket type of the matched bracket at position i, we implement the last layer as follow.\nyi = [oi, zi, zi]\noi = ¬(di1 = sin(θ(D))) ∧ ¬(di1 = sin(θ(D̃)))\nD̃ = min{n− i,D + 1} zi = ¬(di1 = 0) ∧ m̃i zi = 1− zi.\nWe elaborate on a few details here. (1) We can derive the term sin(θ(D̃)) via the similar method in Lemma B.1. (2) Since | sin(θ(i))−sin(θ(j))| = Ω (\n1 D2\n) holds for any i 6= j ∈ {0, 1, · · · , D + 1},\nwe know that the input gap (i.e. the constant c in Lemma A.1) for of all three EQUAL gates is at least Ω ( 1 d2 ) . Thus we can apply Lemma A.1. (3) We can obtain n− i by either augmenting the position encoding with n and i, or normalizing (i/n, 1− i/n) (see Lemma B.1).\nOutput mechanism The final output is determined by on V yT+2, where V ∈ R2k×2dlog ke+1 satisfies Vi,1 = 0 and Vi,1: is the binary encoding of the i-th close bracket and its complement when i ∈ {1, · · · , k}; Vi,1 = dlog ke and Vi,j = 0 when i ≤ {k + 1, · · · , 2k} and j > 1. Let S ⊆ [2k] denote the index of valid output, we conclude that (V yT+2)i = dlog ke for i ∈ S and (V yT+2)i ≤ dlog ke − 1 for i /∈ S.\nB.3 Extension to Recognition task\nOur construction can be adapted to recognition task with some extra efforts.\nCorollary B.2. For all k,D ∈ N+, there exists a 3-layer soft-attention network that can generate Dyckk,D. It uses future positional masking, positional encoding of form i/n for position i,O(log k) memory size per layer, and O(log n) precision where n is the input length. The feed-forward\nnetworks use residual connection and layer normalization.\nB.4 Extension to Dyckk We can extend the above construction to recognize language Dyckk. Our construction bypasses the lower bound in Hahn (2020) since the layer normalization operation is not constant Lipschitz (it can be O(n) in the proof).\nTheorem B.3 (Soft-attention, Dyckk generation). For all k ∈ N+, there exists a 2-layer soft-attention network that can generate Dyckk. It uses future positional masking, O(log k) memory size per layer, and O(log n) precision where n is the input length. The feed-forward networks use residual connection and layer normalization.\nDue to space limits, we omit the detailed proof and only outline the major difference from the proof of Theorem 4.2.\n1. We need position encoding i/n3 instead of i/n, and add an extra position encoding of n.\n2. For the first FNN, we replace D with n. In particular, for Lemma B.1, we need an extra input of n/i, this can be derived with either an extra attention head or an extra position encoding.\n3. For the second FNN, we make some adjustment to the input of the EQUAL gate, since the gap between two input could be very small, i.e., O(1/n2). Nevertheless, we can use the same trick of Lemma B.1 to amplify the gap between two input a, b to be of order Ω(1), the later one suffices to our purpose."
    }, {
      "heading" : "C Theoretical limits for finite position encoding",
      "text" : "We prove that a Transformer with finite precision can not recognize Dyckk,D language. In fact, we show a stronger result: no transformer with o(log n) precision can recognize Dyckk,D language of length more than n.\nTheorem C.1 (Formal statement of Theorem 4.3). For any k ∈ N, using hard attention, no transformer with o(log n) encoding precision can recognize Dyckk,2 language with input length n.\nOur proof is inspired by Hahn (2020) but with several different technique ingredient: (1) we allow arbitrary attention masking (both future and past\nposition masking); (2) we allow arbitrary position encoding (3) our lower bounds holds for bounded depth language Dyckk,D; (4) we provide an quantitative bound for precision in terms of input length n. In general, our lower bound is incomparable with Hahn (2020), we prove a fine grained bound on the precision requirement for bounded depth language Dyckk,D, while the proof in Hahn (2020) applies only for language with Depth Ω(n) but allows arbitrary precision on position encoding.\nThe high level intuition behind our proof is that the attention head can only catch o(n) input positions when we properly fix a small number of symbol in the input sequence. This limits the capability of a Transformer and makes it fail to recognize Dyckk,D language.\nWe consider a L-layer transformer and assume 3H attention heads in total: H normal attention heads,H attention heads with future position masking, H attention heads with past position masking. To make our hardness result general, we allow residual connection for the attention layer, and we assume the FNN can be arbitrary function defining on the attention outcome. In the proof, we would gradually fix o(n) positions of the input sequence. We only perform the follow two kinds of assignment (1) we assign matching brackets to position i, i + 1 where i is odd; (2) we assign matching brackets (e.g., we assign ‘[’, ‘(’, ‘)’, ‘]’) to position i, i+ 1, i+ 2, i+ 3 for odd i. A partial assignment to the input sequence is said to be well-aligned if it follows these two rules. Throughout the proof, we guarantee that for any i ∈ [n], ` ∈ [L], the output of the `-th layer xi,` depends only the input symbol at position i. This is clearly satisfied for ` = 0, given the it is composed by position embedding and word embedding only. We gradually fix the input and conduction induction on `. We use c` to denote the number of positions we fixed before the `-th layer, and we use s` to denote the number of consecutive assigned blocks of the input sequence. It is clear that s` ≤ 2c`. The following Lemma is key to our analysis. Due to space limits, we omit the detailed proof.\nLemma C.2. For any ` ∈ {1, · · · , L}, given a well-aligned partially assigned input sequence, suppose the input of `-th layer xi,`−1 depends on the symbol at position i only. Then by fixing c`H\n2(k + 1)O(`H)2O(`Hp) additional positions of the input sequence, we guarantee that the output of `-th layer xi,` also depends solely on the symbol at\nposition i.\nProof of Theorem C.1. We apply Lemma C.2 and compute the number of positions cL+1 we need to restrict, in order to guarantee that the output of L-th layer xi,L+1 depends only on the input at position (i ∈ [n]). Since c`+1 ≤ c`H2(k + 1)O(`H)2O(`Hp) and c1 = O(1), we have\ncL+1 . H O(L)(k + 1)O(L 2H)2O(L 2Hp).\nBy taking\nHO(L)(k + 1)O(L 2H)2O(L 2Hp) ≤ 0.01n.\nWe know the partial assigned sequence is wellaligned, has depth at most two, and the number of assignment is only 0.01. Thus, we assert that that when p = o(log n), the output of Transformer is completely determined by the partial assignment and it do not detect whether there exists error in the unassigned positions and thus can not recognize Dyckk,2 language. We conclude the proof here."
    }, {
      "heading" : "D Experiment Details",
      "text" : "D.1 Setup\nData We follow Hewitt et al. (2020) to generate Dyckk,D by randomly sampling stack decisions (push, pop, or end) and maintaining length conditions (Table 1) for a O(D2) hitting time of different DFA states. The number of tokens for train, validation, and test set is 2 × 106, 2 × 105, 106 respectively.\nModels We use the LSTM model implemented in Hewitt et al. (2020). For Transformer models, we turn off all drop outs as we find them to hurt performance greatly. We also use only 1 head as we find more heads to hurt performance. We use Adam optimizer with initial learning rate being 0.01 or 0.001, and choose the better learning rate in terms of validation accuracy for each experiment. We train for at most 100 epochs but allow early stopping if the validation loss converges.\nMetric We follow Hewitt et al. (2020) and use the accuracy of correct close bracket predictions:\np(〉j |〉) = p(〉j)∑ i p(〉i)\nLet pl be the empirical probability that the model confidently predicts a close bracket (defined as p(〉j |〉) > .8), conditioned on it being separated from its open bracket by l tokens. Unlike Hewitt et al. (2020) where meanlpl is reported, we report Elpl for two reasons: (i) when l is large pl might be only defined by one trail, thus meanlpl amplifies the randomness; (ii) the findings remain similar with either metrics.\nD.2 More Results In Figure 7, we show the validation performance for Transformers of different positional encoding schemes. They all reach near-perfect accuracy when having at least 2 layers.\nIn Figure 8, we break down the results in Section 6.2 when dmodel ∈ {10, 30, 50}. We also add results for a five-layer Transformer, which performs similarly as the two-layer Transformer. This shows (i) a two-layer Transformer, as suggested by our theory, is enough to process Dyckk,D, and (ii) Transformers with more layers can also learn to process Dyckk,D without overfitting or degraded performance."
    } ],
    "references" : [ {
      "title" : "Layer normalization",
      "author" : [ "Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey E Hinton." ],
      "venue" : "arXiv preprint arXiv:1607.06450.",
      "citeRegEx" : "Ba et al\\.,? 2016",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2016
    }, {
      "title" : "Can recurrent neural networks learn nested recursion? In Linguistic Issues in Language Technology, Volume 16, 2018",
      "author" : [ "Jean-Phillipe Bernardy." ],
      "venue" : "CSLI Publications.",
      "citeRegEx" : "Bernardy.,? 2018",
      "shortCiteRegEx" : "Bernardy.",
      "year" : 2018
    }, {
      "title" : "On the ability of self-attention networks to recognize counter languages",
      "author" : [ "Satwik Bhattamishra", "Kabir Ahuja", "Navin Goyal." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7096–7116.",
      "citeRegEx" : "Bhattamishra et al\\.,? 2020a",
      "shortCiteRegEx" : "Bhattamishra et al\\.",
      "year" : 2020
    }, {
      "title" : "On the computational power of transformers and its implications in sequence modeling",
      "author" : [ "Satwik Bhattamishra", "Arkil Patel", "Navin Goyal." ],
      "venue" : "Proceedings of the 24th Conference on Computational Natural Language Learning, pages 455–475, Online.",
      "citeRegEx" : "Bhattamishra et al\\.,? 2020b",
      "shortCiteRegEx" : "Bhattamishra et al\\.",
      "year" : 2020
    }, {
      "title" : "Hierarchical structure guides rapid linguistic predictions during naturalistic listening",
      "author" : [ "Jonathan R Brennan", "John T Hale." ],
      "venue" : "PloS one, 14(1):e0207741.",
      "citeRegEx" : "Brennan and Hale.,? 2019",
      "shortCiteRegEx" : "Brennan and Hale.",
      "year" : 2019
    }, {
      "title" : "Three models for the description of language",
      "author" : [ "Noam Chomsky." ],
      "venue" : "IRE Transactions on information theory, 2(3):113–124.",
      "citeRegEx" : "Chomsky.,? 1956",
      "shortCiteRegEx" : "Chomsky.",
      "year" : 1956
    }, {
      "title" : "The algebraic theory of context-free languages",
      "author" : [ "Noam Chomsky", "Marcel P Schützenberger." ],
      "venue" : "Studies in Logic and the Foundations of Mathematics, volume 26, pages 118–161. Elsevier.",
      "citeRegEx" : "Chomsky and Schützenberger.,? 1959",
      "shortCiteRegEx" : "Chomsky and Schützenberger.",
      "year" : 1959
    }, {
      "title" : "Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory",
      "author" : [ "Sreerupa Das", "C Lee Giles", "Guo-Zheng Sun." ],
      "venue" : "Proceedings of The Fourteenth Annual Conference of Cognitive Science So-",
      "citeRegEx" : "Das et al\\.,? 1992",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 1992
    }, {
      "title" : "Universal transformers",
      "author" : [ "Mostafa Dehghani", "Stephan Gouws", "Oriol Vinyals", "Jakob Uszkoreit", "Lukasz Kaiser." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
      "citeRegEx" : "Dehghani et al\\.,? 2019",
      "shortCiteRegEx" : "Dehghani et al\\.",
      "year" : 2019
    }, {
      "title" : "How can self-attention networks recognize Dyck-n",
      "author" : [ "Javid Ebrahimi", "Dhruv Gelda", "Wei Zhang" ],
      "venue" : null,
      "citeRegEx" : "Ebrahimi et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Ebrahimi et al\\.",
      "year" : 2020
    }, {
      "title" : "Finding structure in time",
      "author" : [ "Jeffrey L Elman." ],
      "venue" : "Cognitive science, 14(2):179–211.",
      "citeRegEx" : "Elman.,? 1990",
      "shortCiteRegEx" : "Elman.",
      "year" : 1990
    }, {
      "title" : "How hierarchical is language use",
      "author" : [ "Stefan L Frank", "Rens Bod", "Morten H Christiansen" ],
      "venue" : "Proceedings of the Royal Society B: Biological Sciences,",
      "citeRegEx" : "Frank et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Frank et al\\.",
      "year" : 2012
    }, {
      "title" : "Hierarchical and sequential processing of language: A response to: Ding, melloni, tian, and poeppel (2017)",
      "author" : [ "Stefan L Frank", "Morten H Christiansen." ],
      "venue" : "rule-based and word-level statistics-based processing of language: insights from neuroscience. lan-",
      "citeRegEx" : "Frank and Christiansen.,? 2018",
      "shortCiteRegEx" : "Frank and Christiansen.",
      "year" : 2018
    }, {
      "title" : "Theoretical limitations of selfattention in neural sequence models",
      "author" : [ "Michael Hahn." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:156–171.",
      "citeRegEx" : "Hahn.,? 2020",
      "shortCiteRegEx" : "Hahn.",
      "year" : 2020
    }, {
      "title" : "Modeling recurrence for transformer",
      "author" : [ "Jie Hao", "Xing Wang", "Baosong Yang", "Longyue Wang", "Jinfeng Zhang", "Zhaopeng Tu." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Hao et al\\.,? 2019",
      "shortCiteRegEx" : "Hao et al\\.",
      "year" : 2019
    }, {
      "title" : "The faculty of language: what is it, who has it, and how did it evolve? science, 298(5598):1569–1579",
      "author" : [ "Marc D Hauser", "Noam Chomsky", "W Tecumseh Fitch" ],
      "venue" : null,
      "citeRegEx" : "Hauser et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Hauser et al\\.",
      "year" : 2002
    }, {
      "title" : "Establishing strong baselines for the new decade: Sequence tagging, syntactic and semantic parsing with bert",
      "author" : [ "Han He", "Jinho D Choi." ],
      "venue" : "arXiv preprint arXiv:1908.04943.",
      "citeRegEx" : "He and Choi.,? 2019",
      "shortCiteRegEx" : "He and Choi.",
      "year" : 2019
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770–778.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "RNNs can generate bounded hierarchical languages with optimal memory",
      "author" : [ "John Hewitt", "Michael Hahn", "Surya Ganguli", "Percy Liang", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Hewitt et al\\.,? 2020",
      "shortCiteRegEx" : "Hewitt et al\\.",
      "year" : 2020
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Unsupervised grammar induction with depth-bounded PCFG",
      "author" : [ "Lifeng Jin", "Finale Doshi-Velez", "Timothy Miller", "William Schuler", "Lane Schwartz." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:211–224.",
      "citeRegEx" : "Jin et al\\.,? 2018",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2018
    }, {
      "title" : "Constraints on multiple centerembedding of clauses",
      "author" : [ "Fred Karlsson." ],
      "venue" : "Journal of Linguistics, pages 365–392.",
      "citeRegEx" : "Karlsson.,? 2007",
      "shortCiteRegEx" : "Karlsson.",
      "year" : 2007
    }, {
      "title" : "Rethinking the positional encoding in language pre-training",
      "author" : [ "Guolin Ke", "Di He", "Tie-Yan Liu." ],
      "venue" : "International Conference on Learning Representations, (ICLR 2021).",
      "citeRegEx" : "Ke et al\\.,? 2021",
      "shortCiteRegEx" : "Ke et al\\.",
      "year" : 2021
    }, {
      "title" : "On the computational power of rnns",
      "author" : [ "Samuel A Korsky", "Robert C Berwick." ],
      "venue" : "arXiv preprint arXiv:1906.06349.",
      "citeRegEx" : "Korsky and Berwick.,? 2019",
      "shortCiteRegEx" : "Korsky and Berwick.",
      "year" : 2019
    }, {
      "title" : "Pragmatics as the origin of recursion",
      "author" : [ "Stephen C Levinson." ],
      "venue" : "Language and recursion, pages 3–13. Springer.",
      "citeRegEx" : "Levinson.,? 2014",
      "shortCiteRegEx" : "Levinson.",
      "year" : 2014
    }, {
      "title" : "Open sesame: Getting inside BERT’s linguistic knowledge",
      "author" : [ "Yongjie Lin", "Yi Chern Tan", "Robert Frank." ],
      "venue" : "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 241–253, Florence,",
      "citeRegEx" : "Lin et al\\.,? 2019",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "Emergent linguistic structure in artificial neural networks trained by self-supervision",
      "author" : [ "Christopher D Manning", "Kevin Clark", "John Hewitt", "Urvashi Khandelwal", "Omer Levy." ],
      "venue" : "Proceedings of the National Academy of Sciences, 117(48):30046–30054.",
      "citeRegEx" : "Manning et al\\.,? 2020",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2020
    }, {
      "title" : "Pointer sentinel mixture models",
      "author" : [ "Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher." ],
      "venue" : "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. Open-",
      "citeRegEx" : "Merity et al\\.,? 2017",
      "shortCiteRegEx" : "Merity et al\\.",
      "year" : 2017
    }, {
      "title" : "A formal hierarchy of RNN architectures",
      "author" : [ "William Merrill", "Gail Weiss", "Yoav Goldberg", "Roy Schwartz", "Noah A. Smith", "Eran Yahav." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 443–459, On-",
      "citeRegEx" : "Merrill et al\\.,? 2020",
      "shortCiteRegEx" : "Merrill et al\\.",
      "year" : 2020
    }, {
      "title" : "Neurophysiological dynamics of phrase-structure building during",
      "author" : [ "Matthew J Nelson", "Imen El Karoui", "Kristof Giber", "Xiaofang Yang", "Laurent Cohen", "Hilda Koopman", "Sydney S Cash", "Lionel Naccache", "John T Hale", "Christophe Pallier" ],
      "venue" : null,
      "citeRegEx" : "Nelson et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Nelson et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning Music Helps You Read: Using transfer to study linguistic structure in language models",
      "author" : [ "Isabel Papadimitriou", "Dan Jurafsky." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Papadimitriou and Jurafsky.,? 2020",
      "shortCiteRegEx" : "Papadimitriou and Jurafsky.",
      "year" : 2020
    }, {
      "title" : "On the turing completeness of modern neural network architectures",
      "author" : [ "Jorge Pérez", "Javier Marinkovic", "Pablo Barceló." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
      "citeRegEx" : "Pérez et al\\.,? 2019",
      "shortCiteRegEx" : "Pérez et al\\.",
      "year" : 2019
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Random features for large-scale kernel machines",
      "author" : [ "Ali Rahimi", "Benjamin Recht." ],
      "venue" : "Advances in Neural Information Processing Systems 20, Proceedings of the Twenty-First Annual Conference on Neural Information Processing Systems, Vancouver,",
      "citeRegEx" : "Rahimi and Recht.,? 2007",
      "shortCiteRegEx" : "Rahimi and Recht.",
      "year" : 2007
    }, {
      "title" : "Evaluating the ability of LSTMs to learn context-free grammars",
      "author" : [ "Luzi Sennhauser", "Robert Berwick." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 115–124, Brussels, Bel-",
      "citeRegEx" : "Sennhauser and Berwick.,? 2018",
      "shortCiteRegEx" : "Sennhauser and Berwick.",
      "year" : 2018
    }, {
      "title" : "Self-attention with relative position representations",
      "author" : [ "Peter Shaw", "Jakob Uszkoreit", "Ashish Vaswani." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Shaw et al\\.,? 2018",
      "shortCiteRegEx" : "Shaw et al\\.",
      "year" : 2018
    }, {
      "title" : "Disan: Directional self-attention network for rnn/cnn-free language understanding",
      "author" : [ "Tao Shen", "Tianyi Zhou", "Guodong Long", "Jing Jiang", "Shirui Pan", "Chengqi Zhang." ],
      "venue" : "Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Shen et al\\.,? 2018",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "Novel positional encodings to enable tree-based transformers",
      "author" : [ "Vighnesh Leonardo Shiv", "Chris Quirk." ],
      "venue" : "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, De-",
      "citeRegEx" : "Shiv and Quirk.,? 2019",
      "shortCiteRegEx" : "Shiv and Quirk.",
      "year" : 2019
    }, {
      "title" : "A recurrent network that performs a context-sensitive prediction task",
      "author" : [ "Mark Steijvers", "Peter Grünwald." ],
      "venue" : "Proceedings of the 18th annual conference of the cognitive science society, pages 335–339.",
      "citeRegEx" : "Steijvers and Grünwald.,? 1996",
      "shortCiteRegEx" : "Steijvers and Grünwald.",
      "year" : 1996
    }, {
      "title" : "Memory-augmented recurrent neural networks can learn generalized dyck languages",
      "author" : [ "Mirac Suzgun", "Sebastian Gehrmann", "Yonatan Belinkov", "Stuart M Shieber." ],
      "venue" : "arXiv preprint arXiv:1911.03329.",
      "citeRegEx" : "Suzgun et al\\.,? 2019",
      "shortCiteRegEx" : "Suzgun et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT rediscovers the classical NLP pipeline",
      "author" : [ "Ian Tenney", "Dipanjan Das", "Ellie Pavlick." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593– 4601, Florence, Italy. Association for Computational",
      "citeRegEx" : "Tenney et al\\.,? 2019",
      "shortCiteRegEx" : "Tenney et al\\.",
      "year" : 2019
    }, {
      "title" : "The importance of being recurrent for modeling hierarchical structure",
      "author" : [ "Ke Tran", "Arianna Bisazza", "Christof Monz." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4731–4736, Brussels, Bel-",
      "citeRegEx" : "Tran et al\\.,? 2018",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Encoding word order in complex embeddings",
      "author" : [ "Benyou Wang", "Donghao Zhao", "Christina Lioma", "Qiuchi Li", "Peng Zhang", "Jakob Grue Simonsen." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Huggingface’s transformers: State-of-the-art natural language processing",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "Rémi Louf", "Morgan Funtowicz" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "Assessing the ability of self-attention networks to learn word order",
      "author" : [ "Baosong Yang", "Longyue Wang", "Derek F. Wong", "Lidia S. Chao", "Zhaopeng Tu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning the Dyck language with attention-based Seq2Seq models",
      "author" : [ "Xiang Yu", "Ngoc Thang Vu", "Jonas Kuhn." ],
      "venue" : "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 138–146, Florence,",
      "citeRegEx" : "Yu et al\\.,? 2019",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2019
    }, {
      "title" : "Are transformers universal approximators of sequence-to-sequence functions",
      "author" : [ "Chulhee Yun", "Srinadh Bhojanapalli", "Ankit Singh Rawat", "Sashank J. Reddi", "Sanjiv Kumar" ],
      "venue" : "In 8th International Conference on Learning Representations,",
      "citeRegEx" : "Yun et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Yun et al\\.",
      "year" : 2020
    }, {
      "title" : "Fast and accurate neural CRF constituency parsing",
      "author" : [ "Yu Zhang", "Houquan Zhou", "Zhenghua Li." ],
      "venue" : "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020, pages 4046–4053. ijcai.org.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "2020) to generate Dyckk,D by randomly sampling stack decisions (push, pop, or end) and maintaining length conditions (Table 1) for a O(D2) hitting time",
      "author" : [ "Hewitt" ],
      "venue" : null,
      "citeRegEx" : "Hewitt,? \\Q2020\\E",
      "shortCiteRegEx" : "Hewitt",
      "year" : 2020
    }, {
      "title" : "2020) where meanlpl is reported, we report Elpl for two reasons: (i) when l is large pl might be only defined by one trail, thus meanlpl amplifies the randomness; (ii) the findings",
      "author" : [ "Hewitt" ],
      "venue" : null,
      "citeRegEx" : "Hewitt,? \\Q2020\\E",
      "shortCiteRegEx" : "Hewitt",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 42,
      "context" : "Transformers (Vaswani et al., 2017) are now the undisputed champions across several benchmark leaderboards in NLP.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 41,
      "context" : "The empirical success of self-attention in NLP has led to a growing interest in studying its properties, with an eye towards a better understanding of the nature and characteristics of natural language (Tran et al., 2018; Papadimitriou and Jurafsky, 2020).",
      "startOffset" : 202,
      "endOffset" : 255
    }, {
      "referenceID" : 30,
      "context" : "The empirical success of self-attention in NLP has led to a growing interest in studying its properties, with an eye towards a better understanding of the nature and characteristics of natural language (Tran et al., 2018; Papadimitriou and Jurafsky, 2020).",
      "startOffset" : 202,
      "endOffset" : 255
    }, {
      "referenceID" : 13,
      "context" : "In particular, it was recently shown that selfattention networks cannot process various kinds of formal languages (Hahn, 2020; Bhattamishra et al., 2020a), among which particularly notable is Dyckk, the language of well-balanced brackets of k types.",
      "startOffset" : 114,
      "endOffset" : 154
    }, {
      "referenceID" : 2,
      "context" : "In particular, it was recently shown that selfattention networks cannot process various kinds of formal languages (Hahn, 2020; Bhattamishra et al., 2020a), among which particularly notable is Dyckk, the language of well-balanced brackets of k types.",
      "startOffset" : 114,
      "endOffset" : 154
    }, {
      "referenceID" : 6,
      "context" : "By the Chomsky-Schützenberger Theorem (Chomsky and Schützenberger, 1959), any context-free language can be obtained from a Dyckk language through intersections with regular languages and homomorphisms.",
      "startOffset" : 38,
      "endOffset" : 72
    }, {
      "referenceID" : 5,
      "context" : "hierarchical structure, center embedding, and recursion – features which have been long claimed to be at the foundation of human language syntax (Chomsky, 1956).",
      "startOffset" : 145,
      "endOffset" : 160
    }, {
      "referenceID" : 48,
      "context" : "Given the state-of-the-art performance of Transformers in parsing natural language (Zhang et al., 2020; He and Choi, 2019), the Dyckk blind spot seems very suggestive.",
      "startOffset" : 83,
      "endOffset" : 122
    }, {
      "referenceID" : 16,
      "context" : "Given the state-of-the-art performance of Transformers in parsing natural language (Zhang et al., 2020; He and Choi, 2019), the Dyckk blind spot seems very suggestive.",
      "startOffset" : 83,
      "endOffset" : 122
    }, {
      "referenceID" : 15,
      "context" : "If the world’s best NLP models cannot deal with this simple language — generated by a grammar with k + 2 rules and recognized by a single-state pushdown automaton — does this not mean that the role of hierarchy and recursion in natural language must be limited? This question has of course, been extensively debated by linguists on the basis of both theoretical and psycholinguistic evidence (Hauser et al., 2002; Frank et al., 2012; Nelson et al., 2017; Brennan and Hale, 2019; Frank and Christiansen, 2018).",
      "startOffset" : 392,
      "endOffset" : 508
    }, {
      "referenceID" : 11,
      "context" : "If the world’s best NLP models cannot deal with this simple language — generated by a grammar with k + 2 rules and recognized by a single-state pushdown automaton — does this not mean that the role of hierarchy and recursion in natural language must be limited? This question has of course, been extensively debated by linguists on the basis of both theoretical and psycholinguistic evidence (Hauser et al., 2002; Frank et al., 2012; Nelson et al., 2017; Brennan and Hale, 2019; Frank and Christiansen, 2018).",
      "startOffset" : 392,
      "endOffset" : 508
    }, {
      "referenceID" : 29,
      "context" : "If the world’s best NLP models cannot deal with this simple language — generated by a grammar with k + 2 rules and recognized by a single-state pushdown automaton — does this not mean that the role of hierarchy and recursion in natural language must be limited? This question has of course, been extensively debated by linguists on the basis of both theoretical and psycholinguistic evidence (Hauser et al., 2002; Frank et al., 2012; Nelson et al., 2017; Brennan and Hale, 2019; Frank and Christiansen, 2018).",
      "startOffset" : 392,
      "endOffset" : 508
    }, {
      "referenceID" : 4,
      "context" : "If the world’s best NLP models cannot deal with this simple language — generated by a grammar with k + 2 rules and recognized by a single-state pushdown automaton — does this not mean that the role of hierarchy and recursion in natural language must be limited? This question has of course, been extensively debated by linguists on the basis of both theoretical and psycholinguistic evidence (Hauser et al., 2002; Frank et al., 2012; Nelson et al., 2017; Brennan and Hale, 2019; Frank and Christiansen, 2018).",
      "startOffset" : 392,
      "endOffset" : 508
    }, {
      "referenceID" : 12,
      "context" : "If the world’s best NLP models cannot deal with this simple language — generated by a grammar with k + 2 rules and recognized by a single-state pushdown automaton — does this not mean that the role of hierarchy and recursion in natural language must be limited? This question has of course, been extensively debated by linguists on the basis of both theoretical and psycholinguistic evidence (Hauser et al., 2002; Frank et al., 2012; Nelson et al., 2017; Brennan and Hale, 2019; Frank and Christiansen, 2018).",
      "startOffset" : 392,
      "endOffset" : 508
    }, {
      "referenceID" : 21,
      "context" : "For example, center-embedding depth of natural language sentences is known to rarely exceed three (Karlsson, 2007; Jin et al., 2018), and while pragmatics, discourse, and narrative can result in deeper recursion in language (Levinson, 2014), there is arguably a relatively small limit to the depth as well.",
      "startOffset" : 98,
      "endOffset" : 132
    }, {
      "referenceID" : 20,
      "context" : "For example, center-embedding depth of natural language sentences is known to rarely exceed three (Karlsson, 2007; Jin et al., 2018), and while pragmatics, discourse, and narrative can result in deeper recursion in language (Levinson, 2014), there is arguably a relatively small limit to the depth as well.",
      "startOffset" : 98,
      "endOffset" : 132
    }, {
      "referenceID" : 24,
      "context" : ", 2018), and while pragmatics, discourse, and narrative can result in deeper recursion in language (Levinson, 2014), there is arguably a relatively small limit to the depth as well.",
      "startOffset" : 99,
      "endOffset" : 115
    }, {
      "referenceID" : 22,
      "context" : "Our proof requires certain assumptions about the positional encodings, an issue that is often considered in empirical papers (Ke et al., 2021; Shaw et al., 2018; Wang et al., 2020; Shiv and Quirk, 2019) but not in the more theoretical literature.",
      "startOffset" : 125,
      "endOffset" : 202
    }, {
      "referenceID" : 35,
      "context" : "Our proof requires certain assumptions about the positional encodings, an issue that is often considered in empirical papers (Ke et al., 2021; Shaw et al., 2018; Wang et al., 2020; Shiv and Quirk, 2019) but not in the more theoretical literature.",
      "startOffset" : 125,
      "endOffset" : 202
    }, {
      "referenceID" : 43,
      "context" : "Our proof requires certain assumptions about the positional encodings, an issue that is often considered in empirical papers (Ke et al., 2021; Shaw et al., 2018; Wang et al., 2020; Shiv and Quirk, 2019) but not in the more theoretical literature.",
      "startOffset" : 125,
      "endOffset" : 202
    }, {
      "referenceID" : 37,
      "context" : "Our proof requires certain assumptions about the positional encodings, an issue that is often considered in empirical papers (Ke et al., 2021; Shaw et al., 2018; Wang et al., 2020; Shiv and Quirk, 2019) but not in the more theoretical literature.",
      "startOffset" : 125,
      "endOffset" : 202
    }, {
      "referenceID" : 18,
      "context" : "While recurrent networks with finite precision need at least Ω(D log k) memory to process Dyckk,D (Hewitt et al., 2020), our second construction requires only O(log k) memory but aO(log n) precision.",
      "startOffset" : 98,
      "endOffset" : 119
    }, {
      "referenceID" : 31,
      "context" : "Our work primarily relates to the ongoing effort of characterizing theoretical abilities (Pérez et al., 2019; Bhattamishra et al., 2020b; Yun et al., 2020)",
      "startOffset" : 89,
      "endOffset" : 155
    }, {
      "referenceID" : 3,
      "context" : "Our work primarily relates to the ongoing effort of characterizing theoretical abilities (Pérez et al., 2019; Bhattamishra et al., 2020b; Yun et al., 2020)",
      "startOffset" : 89,
      "endOffset" : 155
    }, {
      "referenceID" : 47,
      "context" : "Our work primarily relates to the ongoing effort of characterizing theoretical abilities (Pérez et al., 2019; Bhattamishra et al., 2020b; Yun et al., 2020)",
      "startOffset" : 89,
      "endOffset" : 155
    }, {
      "referenceID" : 9,
      "context" : "Despite the expressivity issues theoretically posed by the above work, empirical findings have shown Transformers can learn Dyckk from finite samples and outperform LSTM (Ebrahimi et al., 2020).",
      "startOffset" : 170,
      "endOffset" : 193
    }, {
      "referenceID" : 10,
      "context" : "A parallel line of work with much lengthier tradition (Elman, 1990; Das et al., 1992; Steijvers and Grünwald, 1996) investigates the abilities and limitations of recurrent networks to process hierarchical structures.",
      "startOffset" : 54,
      "endOffset" : 115
    }, {
      "referenceID" : 7,
      "context" : "A parallel line of work with much lengthier tradition (Elman, 1990; Das et al., 1992; Steijvers and Grünwald, 1996) investigates the abilities and limitations of recurrent networks to process hierarchical structures.",
      "startOffset" : 54,
      "endOffset" : 115
    }, {
      "referenceID" : 38,
      "context" : "A parallel line of work with much lengthier tradition (Elman, 1990; Das et al., 1992; Steijvers and Grünwald, 1996) investigates the abilities and limitations of recurrent networks to process hierarchical structures.",
      "startOffset" : 54,
      "endOffset" : 115
    }, {
      "referenceID" : 23,
      "context" : "In particular, RNNs or LSTMs are proved capable of solving context-free languages like Dyckk given infinite precision (Korsky and Berwick, 2019) or external memory (Suzgun et al.",
      "startOffset" : 118,
      "endOffset" : 144
    }, {
      "referenceID" : 39,
      "context" : "In particular, RNNs or LSTMs are proved capable of solving context-free languages like Dyckk given infinite precision (Korsky and Berwick, 2019) or external memory (Suzgun et al., 2019; Merrill et al., 2020).",
      "startOffset" : 164,
      "endOffset" : 207
    }, {
      "referenceID" : 28,
      "context" : "In particular, RNNs or LSTMs are proved capable of solving context-free languages like Dyckk given infinite precision (Korsky and Berwick, 2019) or external memory (Suzgun et al., 2019; Merrill et al., 2020).",
      "startOffset" : 164,
      "endOffset" : 207
    }, {
      "referenceID" : 1,
      "context" : "(2020) also prove RNNs/LSTMs cannot process Dyckk without such assumptions, which aligns with experimental findings that recurrent networks perform or generalize poorly on Dyckk (Bernardy, 2018; Sennhauser and Berwick, 2018; Yu et al., 2019).",
      "startOffset" : 178,
      "endOffset" : 241
    }, {
      "referenceID" : 34,
      "context" : "(2020) also prove RNNs/LSTMs cannot process Dyckk without such assumptions, which aligns with experimental findings that recurrent networks perform or generalize poorly on Dyckk (Bernardy, 2018; Sennhauser and Berwick, 2018; Yu et al., 2019).",
      "startOffset" : 178,
      "endOffset" : 241
    }, {
      "referenceID" : 46,
      "context" : "(2020) also prove RNNs/LSTMs cannot process Dyckk without such assumptions, which aligns with experimental findings that recurrent networks perform or generalize poorly on Dyckk (Bernardy, 2018; Sennhauser and Berwick, 2018; Yu et al., 2019).",
      "startOffset" : 178,
      "endOffset" : 241
    }, {
      "referenceID" : 41,
      "context" : "For the broader NLP community, our results also contribute to settling whether self-attention networks are restricted to model hierarchical structures due to non-recurrence, a concern (Tran et al., 2018) often turned into proposals to equip Transformers with recurrence (Dehghani et al.",
      "startOffset" : 184,
      "endOffset" : 203
    }, {
      "referenceID" : 8,
      "context" : ", 2018) often turned into proposals to equip Transformers with recurrence (Dehghani et al., 2019; Shen et al., 2018; Chen et al., 2018; Hao et al., 2019).",
      "startOffset" : 74,
      "endOffset" : 153
    }, {
      "referenceID" : 36,
      "context" : ", 2018) often turned into proposals to equip Transformers with recurrence (Dehghani et al., 2019; Shen et al., 2018; Chen et al., 2018; Hao et al., 2019).",
      "startOffset" : 74,
      "endOffset" : 153
    }, {
      "referenceID" : 14,
      "context" : ", 2018) often turned into proposals to equip Transformers with recurrence (Dehghani et al., 2019; Shen et al., 2018; Chen et al., 2018; Hao et al., 2019).",
      "startOffset" : 74,
      "endOffset" : 153
    }, {
      "referenceID" : 25,
      "context" : "On one hand, Transformers are shown to encode syntactic (Lin et al., 2019; Tenney et al., 2019; Manning et al., 2020) and word order (Yang et al.",
      "startOffset" : 56,
      "endOffset" : 117
    }, {
      "referenceID" : 40,
      "context" : "On one hand, Transformers are shown to encode syntactic (Lin et al., 2019; Tenney et al., 2019; Manning et al., 2020) and word order (Yang et al.",
      "startOffset" : 56,
      "endOffset" : 117
    }, {
      "referenceID" : 26,
      "context" : "On one hand, Transformers are shown to encode syntactic (Lin et al., 2019; Tenney et al., 2019; Manning et al., 2020) and word order (Yang et al.",
      "startOffset" : 56,
      "endOffset" : 117
    }, {
      "referenceID" : 45,
      "context" : ", 2020) and word order (Yang et al., 2019) information, and dominate syntactical tasks in NLP such as constituency (Zhang et al.",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 48,
      "context" : ", 2019) information, and dominate syntactical tasks in NLP such as constituency (Zhang et al., 2020) and dependency (He and Choi, 2019) parsing.",
      "startOffset" : 80,
      "endOffset" : 100
    }, {
      "referenceID" : 41,
      "context" : "On the other hand, on several linguistically-motivated tasks like English subject-verb agreement (Tran et al., 2018), recurrent models are reported to outperform Transformers.",
      "startOffset" : 97,
      "endOffset" : 116
    }, {
      "referenceID" : 42,
      "context" : "We consider the encoder part of the original Transformer (Vaswani et al., 2017), which has multiple layers of two blocks each: (i) a self-attention block and (ii) a feed-forward network (FFN).",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 13,
      "context" : "Though impractical for NLP, it has been used to model formal languages (Hahn, 2020).",
      "startOffset" : 71,
      "endOffset" : 83
    }, {
      "referenceID" : 32,
      "context" : "Future-positional masking is usually used to train auto-regressive models like GPT-2 (Radford et al., 2019).",
      "startOffset" : 85,
      "endOffset" : 107
    }, {
      "referenceID" : 17,
      "context" : "Residual connections (He et al., 2016) and layer normalization (Ba et al.",
      "startOffset" : 21,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : ", 2016) and layer normalization (Ba et al., 2016) are two optional components to aid learning.",
      "startOffset" : 32,
      "endOffset" : 49
    }, {
      "referenceID" : 33,
      "context" : "(2017) proposes two kinds of positional encoding: (i) Fourier features (Rahimi and Recht, 2007), i.",
      "startOffset" : 71,
      "endOffset" : 95
    }, {
      "referenceID" : 18,
      "context" : "We follow previous work (Hewitt et al., 2020) to define how a language model can generate a formal language: Definition 3.",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 44,
      "context" : "We train randomly initialized Transformers using the Huggingface library (Wolf et al., 2019), with one future positional masking head, L ∈ {1, 2, 3, 4, 5, 10} layers, and a default memory size dmodel = 30.",
      "startOffset" : 73,
      "endOffset" : 92
    }, {
      "referenceID" : 19,
      "context" : "Memory Size and Comparison with LSTM We compare a two-layer Transformer (POS/N) with a one-layer LSTM3 (Hochreiter and Schmidhuber, 1997) using varying per-layer memory sizes dmodel ∈ {10, 20, · · · , 100}.",
      "startOffset" : 103,
      "endOffset" : 137
    }, {
      "referenceID" : 18,
      "context" : "LSTMs only need one layer to process Dyckk,D (Hewitt et al., 2020), while Transformers at least need two in our constructions.",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 27,
      "context" : "To investigate whether such a novel scheme is also useful in NLP tasks, we train two RoBERTa5 models (POS/N, LEARN) from scratch on the WikiText103 dataset (Merity et al., 2017) for 150 epochs.",
      "startOffset" : 156,
      "endOffset" : 177
    } ],
    "year" : 2021,
    "abstractText" : "Despite their impressive performance in NLP, self-attention networks were recently proved to be limited for processing formal languages with hierarchical structure, such as Dyckk, the language consisting of well-nested parentheses of k types. This suggested that natural language can be approximated well with models that are too weak for formal languages, or that the role of hierarchy and recursion in natural language might be limited. We qualify this implication by proving that self-attention networks can process Dyckk,D, the subset of Dyckk with depth bounded by D, which arguably better captures the bounded hierarchical structure of natural language. Specifically, we construct a hard-attention network with D + 1 layers and O(log k) memory size (per token per layer) that recognizes Dyckk,D, and a soft-attention network with two layers and O(log k) memory size that generates Dyckk,D. Experiments show that self-attention networks trained on Dyckk,D generalize to longer inputs with near-perfect accuracy, and also verify the theoretical memory advantage of self-attention networks over recurrent networks.1",
    "creator" : "LaTeX with hyperref"
  }
}