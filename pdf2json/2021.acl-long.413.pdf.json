{
  "name" : "2021.acl-long.413.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Positional Artefacts Propagate Through Masked Language Model Embeddings",
    "authors" : [ "Ziyang Luo", "Artur Kulmizev", "Xiaoxi Mao" ],
    "emails" : [ "Ziyang.Luo.9588@student.uu.se,", "artur.kulmizev@lingfil.uu.se", "maoxiaoxi@corp.netease.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5312–5327\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5312"
    }, {
      "heading" : "1 Introduction",
      "text" : "A major area of NLP research in the deep learning era has concerned the representation of words in low-dimensional, continuous vector spaces. Traditional methods for achieving this have included word embedding models such as Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and FastText (Bojanowski et al., 2017). However, though influential, such approaches all share a uniform pitfall in assigning a single, static vector to a word type. Given that the vast majority of words are polysemous (Klein and Murphy, 2001), static word embeddings cannot possibly represent a word’s changing meaning in context.\n∗ Work partly done during internship at NetEase Inc..\nIn recent years, deep language models, like ELMo (Peters et al., 2018), BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b), have achieved great success across many NLP tasks. Such models introduce a new type of word vectors, deemed the contextualized variety, where the representation is computed with respect to the context of the target word. Since these vectors are sensitive to context, they can better address the polysemy problem that hinders traditional word embeddings. Indeed, studies have shown that replacing static embeddings (e.g. word2vec) with contextualized ones (e.g. BERT) can benefit many NLP tasks, including constituency parsing (Kitaev and Klein, 2018), coreference resolution (Joshi et al., 2019) and machine translation (Liu et al., 2020).\nHowever, despite the major success in deploying these representations across linguistic tasks, there remains little understanding about information embedded in contextualized vectors and the mechanisms that generate them. Indeed, an entire research area central to this core issue — the interpretability of neural NLP models — has recently emerged (Linzen et al., 2018, 2019; Alishahi et al., 2020). A key theme in this line of work has been the use of linear probes in investigating the linguistic properties of contextualized vectors (Tenney et al., 2019; Hewitt and Manning, 2019). Such studies, among many others, show that contextualization is an important factor that sets these embeddings apart from static ones, the latter of which are unreliable in extracting features central to context or linguistic hierarchy. Nonetheless, much of this work likewise fails to engage with the raw vector spaces of language models, preferring instead to focus its analysis on the transformed vectors. Indeed, the fraction of work that has done the former has shed some curious insights: that untransformed BERT sentence representations still lag behind word embeddings across a variety\nof semantic benchmarks (Reimers and Gurevych, 2019) and that the vector spaces of language models are explicitly anisotropic (Ethayarajh, 2019; Li et al., 2020a). Certainly, an awareness of the patterns inherent to models’ untransformed vector spaces — even if shallow — can only benefit the transformation-based analyses outlined above.\nIn this work, we shed light on a persistent pattern that can be observed for contextualized vectors produced by BERT and RoBERTa. Namely, we show that, across all layers, select neurons in BERT and RoBERTa consistently bear extremely large values. We observe this pattern across vectors for all words in several datasets, demonstrating that these singleton dimensions serve as major outliers to the distributions of neuron values in both encoders’ representational spaces. With this insight in mind, the contributions of our work are as follows:\n1. We introduce a neuron-level method for analyzing the origin of a model’s outliers. Using this, we show that they are closely related to positional information.\n2. In investigating the effects of clipping the outliers (zeroing-out), we show that the degree of anisotropy in the vector space diminishes significantly.\n3. We show that after clipping the outliers, the BERT representations can better distinguish between a word’s potential senses in the word-in-context (WiC) dataset (Pilehvar and Camacho-Collados, 2019), as well as lead to better sentence embeddings when mean pooling."
    }, {
      "heading" : "2 Finding outliers",
      "text" : "In this section, we demonstrate the existence of large-valued vector dimensions across nearly all tokens encoded by BERT and RoBERTa. To illustrate these patterns, we employ two well-known datasets — SST-2 (Socher et al., 2013) and QQP1. SST-2 (60.7k sentences) is a widely-employed sentiment analysis dataset of movie reviews, while QQP (727.7k sentences) is a semantic textual similarity dataset of Quora questions, which collects questions across many topics. We choose these datasets in order to account for a reasonably wide distributions of domains and topics, but note that\n1https://www.quora.com/q/quoradata/ First-Quora-Dataset-Release-Question-Pairs\nany dataset would illustrate our findings well. We randomly sample 10k sentences from the training sets of both SST-2 and QQP, tokenize them, and encode them via BERT-base and RoBERTa-base. All models are downloaded from the Huggingface Transformers Library (Wolf et al., 2020), though we replicated our results for BERT by loading the provided model weights via our own loaders.\nWhen discounting the input embedding layers of each model, we are left with 3.68M and 3.59M contextualized token embeddings for BERT-base and RoBERTa-base, respectively. In order to illustrate the outlier patterns, we average all subword vectors for each layer of each model.\nIn examining BERT-base, we find that the minimum value of 96.60% of vectors lies in the 557th dimension. Figure 1 displays the averaged subword vectors for each layer of BERT-base, corroborating that these patterns exist across all layers. For RoBERTa-base, we likewise find that the maximum value of all vectors is the 588th element. Interestingly, the minimum element of 88.19% of vectors in\nRoBERTa-base is the 77th element, implying that RoBERTa has two such outliers. Figure 2 displays the average vectors for each layer of RoBERTabase.\nOur observations here reveal a curious pattern that is present in the base versions of BERT and RoBERTa. We also corroborate the same findings for the large and distilled (Sanh et al., 2020) variants of these architectures, which can be found in the Appendix A. Indeed, it would be difficult to reach any sort of conclusion about the representational geometry of such models without understanding the outliers’ origin(s)."
    }, {
      "heading" : "3 Where do outliers come from?",
      "text" : "In this section, we attempt to trace the source of the outlier dimensions in BERT-base and RoBERTabase (henceforth BERT and RoBERTa). Similarly to the previous section, we can corroborate the results of the experiments described here (as well as in the remainder of the paper) for the large and distilled varieties of each respective architecture. Thus, for reasons of brevity, we focus our forthcoming analyses on the base versions of BERT and RoBERTa and include results for the remaining models in the Appendix B.2 for the interested reader.\nIn our per-layer analysis in §2, we report that outlier dimensions exist across every layer in each model. Upon a closer look at the input layer (which features a vector sum of positional, segment, and token embeddings), we find that the same outliers also exist in positional embeddings. Figure 3 shows that the 1st positional embedding of BERT has two such dimensions, where the 557th element is likewise the minimum. Interestingly, this pattern does not exist in other positional embeddings, nor in segment or token embeddings. Furthermore, Figure 4 shows that the 4th positional embedding of RoBERTa has four outliers, which include the aforementioned 77th and 588th dimensions. We also find that, from the 4th position to the final position, the maximum element of 99.8% positional embeddings is the 588th element.\nDigging deeper, we observe similar patterns in the Layer Normalization (LN, Ba et al. (2016)) parameters of both models. Recall that LN has two learnable parameters — gain (γ) and bias (β) — both of which are 768-dimension vectors (in the case of the base models). These are designed as an affine transformation over dimension-wise nor-\nmalized vectors in order to, like most normalization strategies, improve their expressive ability and to aid in optimization. Every layer of BERT and RoBERTa applies separate LNs post-attention and pre-output. For BERT, the 557th element of the γ vector is always among the top-6 largest values for the first ten layers’ first LN. Specifically, it is the largest value in the first three layers. For RoBERTa, the 588th element of the first LN’s β vector is always among the top-2 largest values for all layers — it is largest in the first five layers. Furthermore, the 77th element of the second LN’s γ are among the top-7 largest values from the second to the tenth layer.\nIt is reasonable to conclude that, after the vector normalization performed by LN, the outliers observed in the raw embeddings are lost. We hypothesize that these particular neurons are somehow important to the network, such that they retained after scaling the normalized vectors by the affine transformation involving γ and β. Indeed, we observe that, in BERT, only the 1st position’s embedding has such an outlier. However, it is subsequently observed in every layer and token\nafter the first LN is applied. Since LayerNorm is trained globally and is not token specific, it happens to rescale every vector such that the positional information is retained. We corroborate this by observing that all vectors share the same γ. This effectively guarantees the presence of outliers in the 1st layer, which are then propagated upward by means of the Transformer’s residual connection (He et al., 2015). Also, it is important to note that, in the case of BERT, the first position’s embedding is directly tied to the requisite [CLS] token, which is prepended to all sequences as part of the MLM training objective. This has been recently noted to affect e.g. attention patterns, where much of the probability mass is distributed to this particular token alone, despite it bearing the smallest norm among all other vectors in a given layer and head (Kobayashi et al., 2020).\nNeuron-level analysis In order to test the extent to which BERT and RoBERTa’s outliers are related to positional information, we employ a probing technique inspired by Durrani et al. (2020). First, we train a linear probe W ∈ RM×N without bias to predict the position of a contextualized vector in a sentence. In Durrani et al. (2020), the weights of the classifier are employed as a proxy for selecting the most relevant neurons to the prediction. In doing so, they assume that, the larger the absolute value of the weight, the more important the corresponding neuron. However, this method disregards the magnitudes of the values of neurons, as a large weights do not necessarily imply that the neuron has high contribution to the final classification result. For example, if the value of a neuron is close to zero, a large weight also leads to a small contribution. In order to address this issue, we define the contribution of the ith neuron as c(i) = abs(wi∗vi) for i = 1, 2, 3, ..., n, where wi is the ith weight and vi is the ith neuron in the contextualized word vector. We name C = [c(1), c(2), ..., c(n)] as a contribution vector. If a neuron has a high contribution, this means that this neuron is highly relevant to the final classification result.\nWe train, validate, and test our probe on the splits provided in the SST-2 dataset (as mentioned in §2, we surmise that any dataset would be adequate for demonstrating this). The linear probe is a 768× 300 matrix, which we train separately for each layer. Since all SST-2 sentences are shorter than 300 tokens in length, we set M = 300. We\nuse a batch size of 128 and train for 10 epochs with a categorical cross-entropy loss, optimized by Adam (Kingma and Ba, 2017).\nFigure 5a shows that, while it is possible to decode positional information from the lowest three layers with almost perfect accuracy, much of this information is gradually lost higher up in the model. Furthermore, it appears that the higher layers of RoBERTa contain more positional information than BERT. Looking at Figure 5b, we see that BERT’s outlier neuron has a higher contribution in position prediction than the average contribution of all neurons. We also find that the contribution values of the same neuron are the highest in all layers. Combined with the aforementioned pattern of the first positional embedding, we can conclude that the 557th neuron is related to positional information. Likewise, for RoBERTa, Figure 5c shows that the 77th and 588th neurons have the highest contribution for position prediction. We also find that the contribution values of the 588th neurons are always largest for all layers, which implies that these neurons are likewise related to positional information.2\nRemoving positional embeddings In order to isolate the relation between outlier neurons and positional information, we pre-train two RoBERTabase models (with and without positional embeddings) from scratch using Fairseq (Ott et al., 2019). Our pre-training data is the English Wikipedia Corpus3, where we train for 200k steps with a batch size of 256, optimized by Adam. All models share the same hyper-parameters, which are listed in the Appendix C.1. We use four NVIDIA A100 GPUs to pre-train each model, costing about 35 hours per model.\nWe find that, without the help of positional embeddings, the validation perplexity of RoBERTabase is very high at 354.0, which is in line with Lee et al. (2019)’s observation that the selfattention mechanism of Transformer Encoder is order-invariant. In other words, the removal of PEs from RoBERTa-base makes it a bag-of-word model, whose outputs do not contain any positional information. In contrast, the perplexity of RoBERTa equipped with standard positional embeddings is much lower at 4.3, which is likewise expected.\n2We also use heatmaps to show the contribution values in Appendix B.1.\n3We randomly select 158.4M sentences for training and 50k sentences for validation.\n(a) Accuracy of position prediction. (b) The contribution value of BERTbase’s outlier neuron on position prediction. (c) The contribution value of RoBERTabase’s outlier neurons on position prediction.\nIn examining outlier neurons, we employ the same datasets detailed in §2. For the RoBERTabase model with PEs, we find that the maximum element of 82.56% of all vectors is the 81st dimension4, similarly to our findings above. However, we do not observe the presence of such outlier neurons in the RoBERTa-base model without PEs, which indicates that the outlier neurons are tied directly to positional information. Similar to §2, we display the averaged subword vectors for each layer of our models in Appendix C.2, which also corroborate our results."
    }, {
      "heading" : "4 Clipping the outliers",
      "text" : "In §3, we demonstrated that outlier neurons are related to positional information. In this section, we investigate the effects of zeroing out these dimensions in contextualized vectors, a process which we refer to as clipping."
    }, {
      "heading" : "4.1 Vector space geometry",
      "text" : "Anisotropy Ethayarajh (2019) observe that contextualized word vectors are anisotropic in all noninput layers, which means that the average cosine similarity between uniformly randomly sampled words is close to 1. To corroborate this finding, we randomly sample 2000 sentences from the SST-2 training set and create 1000 sentence-pairs. Then, we randomly select a token in each sentence, discarding all other tokens. This effectively sets the correspondence between the two sentences to two tokens instead. Following this, we compute the cosine similarity between these two tokens to measure the anisotropy of contextualized vectors.\nIn the left plot of Figure 6, we can see that contextualized representations of BERT and RoBERTa are more anisotropic in higher layers. This is espe-\n4Different initializations make our models have different outlier dimensions.\ncially true for RoBERTa, where the average cosine similarity between random words is larger than 0.5 after the first non-input layer. This implies that the internal representations in BERT and RoBERTa occupy a narrow cone in the vector space.\nSince outlier neurons tend to be valued higher or lower than all other contextualized vector dimensions, we hypothesize that they are the main culprit behind the degree of observed anisotropy. To verify our hypothesis, we clip BERT and RoBERTa’s outliers by setting each neuron’s value to zero. The left plot in Figure 6 shows that, after clipping the outliers, their vector spaces become close to isotropic.\nSelf-similarity In addition to remarking upon the anisotropic characteristics of contextualized vector spaces, Ethayarajh (2019) introduce several measures to gauge the extent of “contextualization” inherent models. One such metric is self-similarity, which the authors employ to compare the similarity of a word’s internal representations in different contexts. Given a word w and n different sentences s1, s2, ..., sn which contain such word, f il (w) is the internal representation of w in sentence si in the lth layer. The average self-similarity of w in the lth layer is then defined as:\nSelfSiml(w) =\n∑n i=1 ∑n j=i+1 cos ( f il (w), f j l (w) ) n(n− 1)\n(1) Intuitively, a self-similarity score of 1 indicates that no contextualization is being performed by the model (e.g. static word embeddings), while a score of 0 implies that representations for a given word are maximally different given various contexts.\nTo investigate the effect of outlier neurons on a model’s self-similarity, we sample 1000 different words from SST-2 training set, all of which appear at least in 10 different sentences. We then com-\npute the average self-similarity of these words as contextualized by BERT and RoBERTa — before and after clipping the outliers. To adjust for the effect of anisotropy, we subtract the self-similarity from each layer’s anisotropy measurement, as in Ethayarajh (2019).\nThe right plot in Figure 6 shows that, similarly to the findings in (Ethayarajh, 2019), a word’s selfsimilarity is highest in the lower layers, but decreases in higher layers. Crucially, we also observe that, after clipping the outlier dimensions, the selfsimilarity increases, indicating that vectors become closer to each other in the contextualized space. This bears some impact on studies attempting to characterize the vector spaces of models like BERT and RoBERTa, as it is clearly possible to overstate the degree of “contextualization” without addressing the effect of positional artefacts."
    }, {
      "heading" : "4.2 Word sense",
      "text" : "Bearing in mind the findings of the previous section, we now turn to the question of word sense, as captured by contextualized embeddings. Suppose that we have a target word w, which appears in two sentences. w has the same sense in these two sentences, but its contextualized representations are not identical due to the word appearing in (perhaps slightly) different contexts. In the previous few sections, we showed that outlier neurons are related to positional information and that clipping them can make a word’s contextualized vectors more similar. Here, we hypothesize that clipping such dimensions can likewise aid in intrinsic semantic tasks, like differentiating senses of a word.\nTo test our hypothesis, we analyze contextualized vectors using the word-in-context (WiC) dataset (Pilehvar and Camacho-Collados, 2019), which is designed to identify the meaning of words\nin different contexts. WiC is a binary classification task, where, given a target word and two sentences which contain it, models must determine whether the word has the same meaning across the two sentences.\nIn order to test how well we can identify differences in word senses using contextualized vectors, we compute the cosine similarity between contextualized vectors of target words across pairs of sentences, as they appear in the WiC dataset. If the similarity value is larger than a specified threshold, we assign the true label to the sentence pair; otherwise, we assign the false label. We use this method to compare the accuracy of BERT and RoBERTa on WiC before and after clipping the outliers. Since this method does not require any training, we test our models on the WiC training dataset.5 We compare 9 different thresholds from 0.1 to 0.9, as well as a simple baseline model that assigns the true labels to all samples.\nTable 1 shows that after clipping outliers, the best accuracy scores of BERT and RoBERTa increase about 1%.6 This indicates that these neurons\n5The WiC test set does not provide labels and the size of validation set is too small (638 sentences pairs). We thus choose to use the training dataset (5428 sentences pairs).\n6The thresholds are different due to the fact that the cosine\nare less related to word sense information and can be safely clipped for this particular task (if performed in an unsupervised fashion)."
    }, {
      "heading" : "4.3 Sentence embedding",
      "text" : "Venturing beyond the word-level, we also hypothesize that outlier clipping can lead to better sentence embeddings when relying on the cosine similarity metric. To test this, we follow Reimers and Gurevych (2019) in evaluating our models on 7 semantic textual similarity (STS) datasets, including the STS-B benchmark (STS-B) (Cer et al., 2017), the SICK-Relatedness (SICK-R) dataset (Bentivogli et al., 2016) and the STS tasks 2012- 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016). Each sentence pair in these datasets is annotated with a relatedness score on a 5-point rating scale, as obtained from human judgments. We load each dataset using the SentEval toolkit (Conneau and Kiela, 2018).\nIndeed, the most common approach for computing sentence embeddings from contextualized models is simply averaging all subword vectors that comprise a given sentence (Reimers and Gurevych, 2019). We follow this method in obtaining embeddings for each pair of sentences in the aforementioned tasks, between which we compute the cosine similarity. Given a set of similarity and gold relatedness scores, we then calculate the Spearman rank correlation. As a comparison, we also consider averaged GloVe embeddings as our baseline.\nTable 2 shows that, after clipping the outliers, the best Spearman rank correlation scores for BERT and RoBERTa increase across all datasets, some by a large margin. This indicates that clipping the outlier neurons can lead to better sentence embeddings when mean pooling. However, like Li et al.\nsimilarity is inflated in the presence of outlier neurons.\n(2020b), we also notice that averaged GloVe embeddings still manage outperform both BERT and RoBERTa on all STS 2012-16 tasks. This implies that the post-clipping reduction in anisotropy is only a partial explanation for why contextualized, mean-pooled sentence embeddings still lag behind static word embeddings in capturing the semantics of a given sentence."
    }, {
      "heading" : "4.4 Supervised tasks",
      "text" : "In the previous sections, we analyzed the effects of clipping outlier neurons on various intrinsic semantic tasks. Here, we explore the effects of clipping in a supervised scenario, where we hypothesize that a model will learn to discard outlier information if it is not needed for a given task. We consider two binary classification tasks, SST-2 and IMDB (Maas et al., 2011), and a multi-class classification task, SST-5, which is a 5-class version of SST-2. First, we freeze all the parameters of the pre-trained models and use the same method in §4.3 to get the sentence embedding of each sentence. Then, we train a simple linear classifier W ∈ R768×N for each layer, where N is the number of classes. We use different batch sizes for different tasks, 768 for SST-2, 128 for IMDB and 1536 for SST-5. Then we train for 10 epochs with a categorical cross-entropy loss, optimized by Adam.\nTable 3 shows that there is little difference in employing raw vs. clipped vectors in terms of task performance. This indicates that using vectors with clipped outliers does not drastically affect classifier accuracy when it comes to these common tasks."
    }, {
      "heading" : "5 Discussion",
      "text" : "The experiments detailed in the previous sections point to the dangers of relying on metrics like cosine similarity when making observations about models’ representational spaces. This is particularly salient when the vectors being compared are taken off-the-shelf and their composition is not widely understood. Given the presence of model idiosyncracies like the outliers highlighted here, mean-sensitive, L2 normalized metrics (e.g. cosine similarity or Pearson correlation) will inevitably weigh the comparison of vectors along the highestvalued dimensions. In the case of positional artefacts propagating through the BERT and RoBERTa networks, the basis of comparison is inevitably steered towards whatever information is captured in those dimensions. Furthermore, since such outlier values show little variance across vectors, proxy metrics of anisotropy like measuring the average cosine similarity across random words (detailed in §4.1) will inevitably return an exceedingly high similarity, no matter what the context. When cosine similarity is viewed primarily as means of semantic comparison between word or sentence vectors, the prospect of calculating cosine similarity for a benchmark like WiC or STS-B becomes erroneous. Though an examination of distance metrics is outside the scope of this study, we acknowledge similar points as having been addressed in regards to static word embeddings (Mimno and Thompson, 2017) as well as contextualized ones (Li et al., 2020b). Likewise, we would like to stress that our manual clipping operation was performed for illustrative purposes and that interested researchers should employ more systematic post-hoc normalization strategies, e.g. whitening (Su et al., 2021), when working with hidden states directly.\nRelatedly, the anisotropic nature of the vector space that persists even after clipping the outliers suggests that positional artefacts are simply part of the explanation. Per this point, Gao et al. (2019) prove that, in training any sort of model with likelihood loss, the representations learned for tokens being predicted will be naturally be pushed away from most other tokens in order to achieve a higher like-\nlihood. They relate this observation to the Zipfian nature of word distributions, where the vast majority of words are infrequent. Li et al. (2020a) extend this insight specifically to BERT and show that, while high frequency words concentrate densely, low frequency words are much more sparsely distributed. Though we do not attempt to dispute these claims with our findings, we do hope our experiments will highlight the important role that positional embeddings play in the representational geometry of Transformer-based models. Indeed, recent work has demonstrated that employing relative positional embeddings and untying them from the simultaneously learned word embeddings has lead to impressive gains for BERT-based architectures across common benchmarks (He et al., 2020; Ke et al., 2020). It remains to be seen how such procedures affect the representations of such models, however.\nBeyond this, it is clear that LayerNorm is the reason positional artefacts propagate though model representations in the first place. Indeed, our experiments show that the outlier dimension observed for BERT is tied directly to the [CLS] token, which always occurs at the requisite 1st position —- despite having no linguistic bearing on the sequence of observed tokens being modeled. However, the fact that RoBERTa (which employs a similar delimiter) retains outliers originating from different positions’ embeddings implies that the issue of artefact propagation is not simply a relic of task design. It is possible that whatever positional idiosyncrasies contribute to a task’s loss are likewise retained in their respective embeddings. In the case of BERT, the outlier dimension may be granted a large negative weight in order to differentiate the (privileged) 1st position between all others. This information being reconstructed by the LayerNorm parameters, which are shared for all positions in the sequence length, and then propagated up through the Transformer network is a phenomenon worthy of further attention."
    }, {
      "heading" : "6 Related work",
      "text" : "In recent years, an explosion of work focused on understanding the inner workings of pretrained neural language models has emerged. One line of such work investigates the self-attention mechanism of Transformer-based models, aiming to e.g. characterize its patterns or decode syntactic structure (Raganato and Tiedemann, 2018; Vig, 2019;\nMareček and Rosa, 2018; Voita et al., 2019; Clark et al., 2019; Kobayashi et al., 2020). Another line of work analyzes models’ internal representations using probes. These are often linear classifiers that take representations as input and are trained with supervised tasks in mind, e.g. POS-tagging, dependency parsing (Tenney et al., 2019; Liu et al., 2019a; Lin et al., 2019; Hewitt and Manning, 2019; Zhao et al., 2020). In such work, high probing accuracies are often likened to a particular model having “learned” the task in question.\nMost similar to our work, Ethayarajh (2019) investigate the extent of “contextualization” in models like BERT, ELMo, and GPT-2 (Radford et al., 2019). Mainly, they demonstrate that the contextualized vectors of all words are non-isotropic across all models and layers. However, they do not indicate why these models have such properties. Also relevant are the studies of Dalvi et al. (2018), who introduce a neuron-level analysis method, and Durrani et al. (2020), who use this method to analyze individual neurons in contextualized word vectors. Similarly to our experiment, Durrani et al. (2020) train a linear probe to predict linguistic information stored in a vector. They then employ the weights of the classifier as a proxy to select the most relevant neurons to a particular task. In a similar vein, Coenen et al. (2019) demonstrate the existence of syntactic and semantic subspaces in BERT representations."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we called attention to sets of outlier neurons that appear in BERT and RoBERTa’s internal representations, which bear consistently large values when compared to the distribution of values of all other neurons. In investigating the origin of these outliers, we employed a neuronlevel analysis method which revealed that they are artefacts derived from positional embeddings and Layer Normalization. Furthermore, we found that outliers are a major cause for the anisotrophy of a model’s vector space (Ethayarajh, 2019). Clipping them, consequently, can make the vector space more directionally uniform and increase the similarity between words’ contextual representations. In addition, we showed that outliers can distort results when investigating word sense within contextualized representations as well as obtaining sentence embeddings via mean pooling, where removing them leads to uniformly better results. Lastly, we\nfind that “clipping” does not affect models’ performance on three supervised tasks.\nIt is important to note that the exact dimensions at which the outliers occur will vary pending different initializations and training procedures (as evidenced by our own RoBERTa model). As such, future work will aim at investigating strategies for mitigating the propagation of these artefacts when pretraining. Furthermore, given that both BERT and RoBERTa are masked language models, it will be interesting to investigate whether or not similar artefacts occur in e.g. autoregressive models like GPT-2 (Radford et al., 2019) or XLNet (Yang et al., 2019). Per the insights of Gao et al. (2019), it is very likely that the representational spaces of such models are anisotropic, but it is important to gauge the extent to which this can be traced to positional artefacts.\nAuthors’ Note We would like to mention Kovaleva et al. (2021)’s contemporaneous work, which likewise draws attention to BERT’s outlier neurons. While our discussion situates outliers in the context of positional embeddings and vector spaces, Kovaleva et al. (2021) offer an exhaustive analysis of LayerNorm parameterization and its impact on masked language modeling and finetuning. We refer the interested reader to that work for a thorough discussion of LayerNorm’s role in the outlier neuron phenomenon.\nAcknowledgments We would like to thank Joakim Nivre and Daniel Dakota for fruitful discussions and the anonymous reviewers for their excellent feedback."
    }, {
      "heading" : "A Outliers of distilled and large models",
      "text" : "For BERT-distil, Figure 7 shows the patterns of BERT-distil across all layers. The 557th element is an outlier. For RoBERTa-distil, Figure 8 shows the patterns of RoBERTa-distil across all layers. the 77th and 588th elements are two outliers. For BERT-large, Figure 9 shows the patterns of BERTlarge across all layers. From the first layer to the tenth layer, the 896th element is an outlier. From the tenth layer to the seventeenth layer, the 678th element is an outlier. From the sixteenth layer to the nineteenth layer, the 122nd element is an outlier. From the nineteenth layer to the twenty-third layer, the 928th element is an outlier. The final layer does not have outliers. For RoBERTa-large, Figure\n10 shows the patterns of RoBERTa-large across all layers. From the first layer to the twenty-third layer, the 673rd element is an outlier. From the fifteenth layer to the final layer, the 631st element is an outlier. From the first layer to the sixth layer, the 981st element is an outlier."
    }, {
      "heading" : "B Neuron-level analysis",
      "text" : ""
    }, {
      "heading" : "B.1 Heatmaps of base models",
      "text" : "Figure 11 and 12 show the heatmaps of the outlier neurons and the highest non-outlier contribution values."
    }, {
      "heading" : "B.2 Distilled and large models",
      "text" : "Figure 13 show the accuracy scores of position prediction of distilled and large models.\nDistil-models Figure 14 shows the contribution value of distilled models’ outlier neurons on position prediction.\nLarge-models Figure 15 shows the contribution value of large models’ outlier neurons on position prediction."
    }, {
      "heading" : "C Our Pre-training Models",
      "text" : ""
    }, {
      "heading" : "C.1 Hyper-parameters",
      "text" : "Table 4 shows the hyper-parameters of pre-training our RoBERTa-base models."
    }, {
      "heading" : "C.2 Average subword vectors",
      "text" : "Figure 16 show the average vectors for each of our models."
    }, {
      "heading" : "D Clipping the outliers",
      "text" : ""
    }, {
      "heading" : "D.1 Geometry of vector space",
      "text" : "Distil-models Figure 17 shows the anisotropic measurement of distilled models and the selfsimilarity measurement of distilled models.\nLarge-models Figure 18 shows the anisotropic measurement of large models and Figure 19 shows the self-similarity measurement of large models. We “clip” different outlier neurons in different layers. For BERT-large, we zero-out the 896th neuron from the first layer to the tenth layer, the 678th neuron from the tenth layer to the seventeenth layer, the 122nd neuron from the sixteenth layer to the nineteenth layer and the 928th neuron from the nineteenth layer to the twenty-third layer. For RoBERTa-large, we zero-out the 673rd neuron for all non-input layers, the 981st neuron for the first 9 layers and the 631st neuron for the last 10 layers."
    }, {
      "heading" : "D.2 Word sense",
      "text" : "Table 5 shows the accuracy scores of distill-models and large-models on WiC dataset before and after “clipping the outliers”."
    }, {
      "heading" : "D.3 Sentence embedding",
      "text" : "Table 6 shows the results on semantic textual similarity tasks of distilled models before and after “clipping the outliers”.\nTable 7 shows the results on semantic textual similarity tasks of large models before and after “clipping the outliers”.\nDataset BERTlarge RoBERTa large BERT large clip\nRoBERTa large clip\nSTS-B 62.56(1) 59.71(19) 66.43(3) 62.01(23) SICK-R 64.47(24) 63.08(14) 65.72(23) 63.50(16) STS-12 54.05(1) 44.72(1) 56.44(3) 49.69(1) STS-13 68.80(2) 61.68(8) 71.07(2) 62.82(10) STS-14 60.46(1) 51.39(8) 63.35(1) 57.33(1) STS-15 73.91(1) 65.98(7) 76.51(1) 69.71(1) STS-16 66.35(17) 66.50(14) 71.41(3) 68.25(11)\nTable 7: Experimental results on semantic textual similarity of large models. The number in the parenthesis denotes that this result belongs to the specific layer. Bold indicates that the best result increases after clipping."
    } ],
    "references" : [ {
      "title" : "SemEval-2014 task 10: Multilingual semantic textual similarity",
      "author" : [ "Eneko Agirre", "Carmen Banea", "Claire Cardie", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Weiwei Guo", "Rada Mihalcea", "German Rigau", "Janyce Wiebe." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Agirre et al\\.,? 2014",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2014
    }, {
      "title" : "SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation",
      "author" : [ "Eneko Agirre", "Carmen Banea", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Rada Mihalcea", "German Rigau", "Janyce Wiebe." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Agirre et al\\.,? 2016",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2016
    }, {
      "title" : "SemEval-2012 task 6: A pilot on semantic textual similarity",
      "author" : [ "Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre." ],
      "venue" : "*SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the",
      "citeRegEx" : "Agirre et al\\.,? 2012",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2012
    }, {
      "title" : "SEM 2013 shared task: Semantic textual similarity",
      "author" : [ "Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor GonzalezAgirre", "Weiwei Guo." ],
      "venue" : "Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main",
      "citeRegEx" : "Agirre et al\\.,? 2013",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2013
    }, {
      "title" : "Sick through the semeval glasses: Lesson learned from the evaluation of compositional distributional semantic models on full sentences",
      "author" : [ "Luisa Bentivogli", "Raffaella Bernardi", "Marco Marelli", "Stefano Menini", "Marco Baroni", "Roberto Zamparelli" ],
      "venue" : null,
      "citeRegEx" : "Bentivogli et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bentivogli et al\\.",
      "year" : 2016
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:135–146.",
      "citeRegEx" : "Bojanowski et al\\.,? 2017",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2017
    }, {
      "title" : "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
      "author" : [ "Daniel Cer", "Mona Diab", "Eneko Agirre", "Iñigo LopezGazpio", "Lucia Specia." ],
      "venue" : "Proceedings of the 11th International Workshop on Semantic",
      "citeRegEx" : "Cer et al\\.,? 2017",
      "shortCiteRegEx" : "Cer et al\\.",
      "year" : 2017
    }, {
      "title" : "What does BERT look at? an analysis of BERT’s attention",
      "author" : [ "Kevin Clark", "Urvashi Khandelwal", "Omer Levy", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for",
      "citeRegEx" : "Clark et al\\.,? 2019",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "Visualizing and measuring the geometry of bert",
      "author" : [ "Andy Coenen", "Emily Reif", "Ann Yuan", "Been Kim", "Adam Pearce", "Fernanda Viégas", "Martin Wattenberg." ],
      "venue" : "arXiv preprint arXiv:1906.02715.",
      "citeRegEx" : "Coenen et al\\.,? 2019",
      "shortCiteRegEx" : "Coenen et al\\.",
      "year" : 2019
    }, {
      "title" : "SentEval: An evaluation toolkit for universal sentence representations",
      "author" : [ "Alexis Conneau", "Douwe Kiela." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language",
      "citeRegEx" : "Conneau and Kiela.,? 2018",
      "shortCiteRegEx" : "Conneau and Kiela.",
      "year" : 2018
    }, {
      "title" : "What is one grain of sand in the desert? analyzing individual neurons in deep nlp models",
      "author" : [ "Fahim Dalvi", "Nadir Durrani", "Hassan Sajjad", "Yonatan Belinkov", "Anthony Bau", "James Glass" ],
      "venue" : null,
      "citeRegEx" : "Dalvi et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Dalvi et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Analyzing individual neurons in pre-trained language models",
      "author" : [ "Nadir Durrani", "Hassan Sajjad", "Fahim Dalvi", "Yonatan Belinkov." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Durrani et al\\.,? 2020",
      "shortCiteRegEx" : "Durrani et al\\.",
      "year" : 2020
    }, {
      "title" : "How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings",
      "author" : [ "Kawin Ethayarajh." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Ethayarajh.,? 2019",
      "shortCiteRegEx" : "Ethayarajh.",
      "year" : 2019
    }, {
      "title" : "Representation degeneration problem in training natural language generation models",
      "author" : [ "Jun Gao", "Di He", "Xu Tan", "Tao Qin", "Liwei Wang", "TieYan Liu." ],
      "venue" : "arXiv preprint arXiv:1907.12009.",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Deberta: Decoding-enhanced bert with disentangled attention",
      "author" : [ "Pengcheng He", "Xiaodong Liu", "Jianfeng Gao", "Weizhu Chen." ],
      "venue" : "arXiv preprint arXiv:2006.03654.",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "A structural probe for finding syntax in word representations",
      "author" : [ "John Hewitt", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Hewitt and Manning.,? 2019",
      "shortCiteRegEx" : "Hewitt and Manning.",
      "year" : 2019
    }, {
      "title" : "BERT for coreference resolution: Baselines and analysis",
      "author" : [ "Mandar Joshi", "Omer Levy", "Luke Zettlemoyer", "Daniel Weld." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Joshi et al\\.,? 2019",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2019
    }, {
      "title" : "Rethinking the positional encoding in language pre-training",
      "author" : [ "Guolin Ke", "Di He", "Tie-Yan Liu." ],
      "venue" : "arXiv preprint arXiv:2006.15595.",
      "citeRegEx" : "Ke et al\\.,? 2020",
      "shortCiteRegEx" : "Ke et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba" ],
      "venue" : null,
      "citeRegEx" : "Kingma and Ba.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2017
    }, {
      "title" : "Constituency parsing with a self-attentive encoder",
      "author" : [ "Nikita Kitaev", "Dan Klein." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2676–2686, Melbourne, Australia. Associa-",
      "citeRegEx" : "Kitaev and Klein.,? 2018",
      "shortCiteRegEx" : "Kitaev and Klein.",
      "year" : 2018
    }, {
      "title" : "The representation of polysemous words",
      "author" : [ "Devorah E. Klein", "Gregory L. Murphy." ],
      "venue" : "Journal of Memory and Language, 45(2):259 – 282.",
      "citeRegEx" : "Klein and Murphy.,? 2001",
      "shortCiteRegEx" : "Klein and Murphy.",
      "year" : 2001
    }, {
      "title" : "Attention is not only a weight: Analyzing transformers with vector norms",
      "author" : [ "Goro Kobayashi", "Tatsuki Kuribayashi", "Sho Yokoi", "Kentaro Inui." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Kobayashi et al\\.,? 2020",
      "shortCiteRegEx" : "Kobayashi et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert busters: Outlier layernorm dimensions that disrupt bert",
      "author" : [ "Olga Kovaleva", "Saurabh Kulshreshtha", "Anna Rogers", "Anna Rumshisky" ],
      "venue" : null,
      "citeRegEx" : "Kovaleva et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Kovaleva et al\\.",
      "year" : 2021
    }, {
      "title" : "Set transformer: A framework for attention-based permutation-invariant neural networks",
      "author" : [ "Juho Lee", "Yoonho Lee", "Jungtaek Kim", "Adam Kosiorek", "Seungjin Choi", "Yee Whye Teh." ],
      "venue" : "Proceedings of the 36th International Conference on Ma-",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "On the sentence embeddings from pre-trained language models",
      "author" : [ "Bohan Li", "Hao Zhou", "Junxian He", "Mingxuan Wang", "Yiming Yang", "Lei Li." ],
      "venue" : "arXiv preprint arXiv:2011.05864.",
      "citeRegEx" : "Li et al\\.,? 2020a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "On the sentence embeddings from pre-trained language models",
      "author" : [ "Bohan Li", "Hao Zhou", "Junxian He", "Mingxuan Wang", "Yiming Yang", "Lei Li." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Li et al\\.,? 2020b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Open sesame: Getting inside bert’s linguistic knowledge",
      "author" : [ "Yongjie Lin", "Yi Chern Tan", "Robert Frank" ],
      "venue" : null,
      "citeRegEx" : "Lin et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "Linguistic knowledge and transferability of contextual representations",
      "author" : [ "Nelson F. Liu", "Matt Gardner", "Yonatan Belinkov", "Matthew E. Peters", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Multilingual denoising pre-training for neural machine translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "2019b. Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Maas et al\\.,? 2011",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "Extracting syntactic trees from transformer encoder self-attentions",
      "author" : [ "David Mareček", "Rudolf Rosa." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 347–349, Brussels, Belgium.",
      "citeRegEx" : "Mareček and Rosa.,? 2018",
      "shortCiteRegEx" : "Mareček and Rosa.",
      "year" : 2018
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "The strange geometry of skip-gram with negative sampling",
      "author" : [ "David Mimno", "Laure Thompson." ],
      "venue" : "Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Mimno and Thompson.,? 2017",
      "shortCiteRegEx" : "Mimno and Thompson.",
      "year" : 2017
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chap-",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha,",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "WiC: the word-in-context dataset for evaluating context-sensitive meaning representations",
      "author" : [ "Mohammad Taher Pilehvar", "Jose CamachoCollados." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Pilehvar and CamachoCollados.,? 2019",
      "shortCiteRegEx" : "Pilehvar and CamachoCollados.",
      "year" : 2019
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeff Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "An analysis of encoder representations in transformerbased machine translation",
      "author" : [ "Alessandro Raganato", "Jörg Tiedemann." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages",
      "citeRegEx" : "Raganato and Tiedemann.,? 2018",
      "shortCiteRegEx" : "Raganato and Tiedemann.",
      "year" : 2018
    }, {
      "title" : "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf" ],
      "venue" : null,
      "citeRegEx" : "Sanh et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2020
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 Conference on",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Whitening sentence representations for better semantics and faster retrieval",
      "author" : [ "Jianlin Su", "Jiarun Cao", "Weijie Liu", "Yangyiwen Ou." ],
      "venue" : "arXiv preprint arXiv:2103.15316.",
      "citeRegEx" : "Su et al\\.,? 2021",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2021
    }, {
      "title" : "What do you learn from context? probing for sentence structure in contextu",
      "author" : [ "Ian Tenney", "Patrick Xia", "Berlin Chen", "Alex Wang", "Adam Poliak", "R Thomas McCoy", "Najoung Kim", "Benjamin Van Durme", "Sam Bowman", "Dipanjan Das", "Ellie Pavlick" ],
      "venue" : null,
      "citeRegEx" : "Tenney et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Tenney et al\\.",
      "year" : 2019
    }, {
      "title" : "Visualizing attention in transformerbased language representation models",
      "author" : [ "Jesse Vig" ],
      "venue" : null,
      "citeRegEx" : "Vig.,? \\Q2019\\E",
      "shortCiteRegEx" : "Vig.",
      "year" : 2019
    }, {
      "title" : "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned",
      "author" : [ "Elena Voita", "David Talbot", "Fedor Moiseev", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Voita et al\\.,? 2019",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32, pages",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Quantifying the contextualization of word representations with semantic class probing",
      "author" : [ "Mengjie Zhao", "Philipp Dufter", "Yadollah Yaghoobzadeh", "Hinrich Schütze" ],
      "venue" : null,
      "citeRegEx" : "Zhao et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 34,
      "context" : "Traditional methods for achieving this have included word embedding models such as Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al.",
      "startOffset" : 92,
      "endOffset" : 114
    }, {
      "referenceID" : 37,
      "context" : ", 2013), GloVe (Pennington et al., 2014), and FastText (Bojanowski et al.",
      "startOffset" : 15,
      "endOffset" : 40
    }, {
      "referenceID" : 22,
      "context" : "Given that the vast majority of words are polysemous (Klein and Murphy, 2001), static word embeddings cannot possibly represent a word’s changing meaning in context.",
      "startOffset" : 53,
      "endOffset" : 77
    }, {
      "referenceID" : 38,
      "context" : "In recent years, deep language models, like ELMo (Peters et al., 2018), BERT (Devlin et al.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 11,
      "context" : ", 2018), BERT (Devlin et al., 2019) and RoBERTa (Liu et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 21,
      "context" : "BERT) can benefit many NLP tasks, including constituency parsing (Kitaev and Klein, 2018), coreference resolution (Joshi et al.",
      "startOffset" : 65,
      "endOffset" : 89
    }, {
      "referenceID" : 18,
      "context" : "BERT) can benefit many NLP tasks, including constituency parsing (Kitaev and Klein, 2018), coreference resolution (Joshi et al., 2019)",
      "startOffset" : 114,
      "endOffset" : 134
    }, {
      "referenceID" : 46,
      "context" : "A key theme in this line of work has been the use of linear probes in investigating the linguistic properties of contextualized vectors (Tenney et al., 2019; Hewitt and Manning, 2019).",
      "startOffset" : 136,
      "endOffset" : 183
    }, {
      "referenceID" : 17,
      "context" : "A key theme in this line of work has been the use of linear probes in investigating the linguistic properties of contextualized vectors (Tenney et al., 2019; Hewitt and Manning, 2019).",
      "startOffset" : 136,
      "endOffset" : 183
    }, {
      "referenceID" : 13,
      "context" : "2019) and that the vector spaces of language models are explicitly anisotropic (Ethayarajh, 2019; Li et al., 2020a).",
      "startOffset" : 79,
      "endOffset" : 115
    }, {
      "referenceID" : 26,
      "context" : "2019) and that the vector spaces of language models are explicitly anisotropic (Ethayarajh, 2019; Li et al., 2020a).",
      "startOffset" : 79,
      "endOffset" : 115
    }, {
      "referenceID" : 44,
      "context" : "To illustrate these patterns, we employ two well-known datasets — SST-2 (Socher et al., 2013) and QQP1.",
      "startOffset" : 72,
      "endOffset" : 93
    }, {
      "referenceID" : 43,
      "context" : "for the large and distilled (Sanh et al., 2020) variants of these architectures, which can be found in the Appendix A.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 15,
      "context" : "in the 1st layer, which are then propagated upward by means of the Transformer’s residual connection (He et al., 2015).",
      "startOffset" : 101,
      "endOffset" : 118
    }, {
      "referenceID" : 20,
      "context" : "with a categorical cross-entropy loss, optimized by Adam (Kingma and Ba, 2017).",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 36,
      "context" : "positional information, we pre-train two RoBERTabase models (with and without positional embeddings) from scratch using Fairseq (Ott et al., 2019).",
      "startOffset" : 128,
      "endOffset" : 146
    }, {
      "referenceID" : 13,
      "context" : "The right plot in Figure 6 shows that, similarly to the findings in (Ethayarajh, 2019), a word’s selfsimilarity is highest in the lower layers, but decreases in higher layers.",
      "startOffset" : 68,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : "ing the STS-B benchmark (STS-B) (Cer et al., 2017), the SICK-Relatedness (SICK-R) dataset (Bentivogli et al.",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 4,
      "context" : ", 2017), the SICK-Relatedness (SICK-R) dataset (Bentivogli et al., 2016) and the STS tasks 20122016 (Agirre et al.",
      "startOffset" : 47,
      "endOffset" : 72
    }, {
      "referenceID" : 9,
      "context" : "We load each dataset using the SentEval toolkit (Conneau and Kiela, 2018).",
      "startOffset" : 48,
      "endOffset" : 73
    }, {
      "referenceID" : 42,
      "context" : "models is simply averaging all subword vectors that comprise a given sentence (Reimers and Gurevych, 2019).",
      "startOffset" : 78,
      "endOffset" : 106
    }, {
      "referenceID" : 32,
      "context" : "two binary classification tasks, SST-2 and IMDB (Maas et al., 2011), and a multi-class classification task, SST-5, which is a 5-class version of SST-2.",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 35,
      "context" : "to static word embeddings (Mimno and Thompson, 2017) as well as contextualized ones (Li et al.",
      "startOffset" : 26,
      "endOffset" : 52
    }, {
      "referenceID" : 27,
      "context" : "to static word embeddings (Mimno and Thompson, 2017) as well as contextualized ones (Li et al., 2020b).",
      "startOffset" : 84,
      "endOffset" : 102
    }, {
      "referenceID" : 45,
      "context" : "whitening (Su et al., 2021), when working with hidden states directly.",
      "startOffset" : 10,
      "endOffset" : 27
    }, {
      "referenceID" : 16,
      "context" : "positional embeddings and untying them from the simultaneously learned word embeddings has lead to impressive gains for BERT-based architectures across common benchmarks (He et al., 2020; Ke et al., 2020).",
      "startOffset" : 170,
      "endOffset" : 204
    }, {
      "referenceID" : 19,
      "context" : "positional embeddings and untying them from the simultaneously learned word embeddings has lead to impressive gains for BERT-based architectures across common benchmarks (He et al., 2020; Ke et al., 2020).",
      "startOffset" : 170,
      "endOffset" : 204
    }, {
      "referenceID" : 40,
      "context" : "Most similar to our work, Ethayarajh (2019) investigate the extent of “contextualization” in models like BERT, ELMo, and GPT-2 (Radford et al., 2019).",
      "startOffset" : 127,
      "endOffset" : 149
    }, {
      "referenceID" : 13,
      "context" : "Furthermore, we found that outliers are a major cause for the anisotrophy of a model’s vector space (Ethayarajh, 2019).",
      "startOffset" : 100,
      "endOffset" : 118
    }, {
      "referenceID" : 40,
      "context" : "autoregressive models like GPT-2 (Radford et al., 2019) or XLNet (Yang et al.",
      "startOffset" : 33,
      "endOffset" : 55
    } ],
    "year" : 2021,
    "abstractText" : "In this work, we demonstrate that the contextualized word vectors derived from pretrained masked language model-based encoders share a common, perhaps undesirable pattern across layers. Namely, we find cases of persistent outlier neurons within BERT and RoBERTa’s hidden state vectors that consistently bear the smallest or largest values in said vectors. In an attempt to investigate the source of this information, we introduce a neuron-level analysis method, which reveals that the outliers are closely related to information captured by positional embeddings. We also pre-train the RoBERTa-base models from scratch and find that the outliers disappear without using positional embeddings. These outliers, we find, are the major cause of anisotropy of encoders’ raw vector spaces, and clipping them leads to increased similarity across vectors. We demonstrate this in practice by showing that clipped vectors can more accurately distinguish word senses, as well as lead to better sentence embeddings when mean pooling. In three supervised tasks, we find that clipping does not affect the performance.",
    "creator" : "LaTeX with hyperref"
  }
}