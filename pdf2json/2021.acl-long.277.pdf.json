{
  "name" : "2021.acl-long.277.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Revisiting the Negative Data of Distantly Supervised Relation Extraction",
    "authors" : [ "Chenhao Xie", "Jiaqing Liang", "Jingping Liu", "Chengsong Huang", "Wenhao Huang", "Yanghua Xiao" ],
    "emails" : [ "l.j.q.light}@gmail.com", "shawyh}@fudan.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3572–3581\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3572"
    }, {
      "heading" : "1 Introduction",
      "text" : "Relational extraction is a crucial step towards knowledge graph construction. It aims at identifying relational triples from a given sentence in the form of 〈subject, relation, object〉, in short, 〈s, r, o〉. For example, given S1 in Figure 1, we hope to extract 〈WILLIAM SHAKESPEARE, BIRTHPLACE, STRATFORD-UPON-AVON〉.\nThis task is usually modeled as a supervised learning problem and distant supervision (Mintz et al., 2009) is utilized to acquire large-scale training data. The core idea is to obtain training data\n∗Corresponding author 1https://github.com/redreamality/\nRERE-relation-extraction\ncolumn are labeled by distant supervision. “NA” means no relation.\nis through automatically labeling a sentence with existing relational triples from a knowledge base (KB). For example, given a triple 〈s, r, o〉 and a sentence, if the sentence contains both s and o, distant supervision methods regard 〈s, r, o〉 as a valid sample for the sentence. If no relational triples are applicable, the sentence is labeled as “NA”.\nDespite the abundant training data obtained with distant supervision, nonnegligible errors also occur in the labels. There are two types of errors. In the first type, the labeled relation does not conform with the original meaning of sentence, and this type of error is referred to as false positive (FP). For example, in S2, the sentence “Shakespeare spent the last few years of his life in Stratford-upon-Avon.” does not express the relation BIRTHPLACE, thus being a FP. In the second type, large amounts of\nrelations in sentences are missing due to the incompleteness of KB, which is referred to as false negative (FN). For instance, in S3, “Buffett was born in 1930 in Omaha, Nebraska.” is wrongly labeled as NA since there is no relation (e.g., BIRTHPLACE) between BUFFETT and OMAHA, NEBRASKA in the KB. Many efforts have been devoted to solving the FP problem, including pattern-based methods (Jia et al., 2019), multi-instance learning methods (Lin et al., 2016; Zeng et al., 2018a) and reinforcement learning methods (Feng et al., 2018). Significant improvements have been made.\nHowever, FN problem receives much less attention (Min et al., 2013; Xu et al., 2013; Roller et al., 2015). To the best of our knowledge, none existing work with deep neural networks to solve this problem. We argue that this problem is fatal in practice since there are massive FN cases in datasets. For example, there exist at least 33% and 35% FNs in NYT and SKE datasets, respectively. We will deeply analyze the problem in Section 2.1\nAnother huge problem in relation extraction is the overwhelming negative labels. As is widely acknowledged, information extraction tasks are highly imbalanced in class labels (Chowdhury and Lavelli, 2012; Lin et al., 2018; Li et al., 2020). In particular, the negative labels account for most of the labels in relation extraction under almost any problem formulation, which makes relation extraction a hard machine learning problem. We systematically analyze this in Section 2.2.\nIn this paper, we address these challenges caused by negative data. Our main contribution can be summarized as follows.\n• We systematically compare the class distributions of different problem modeling and explain why first extract relation then entities, i.e., the third paradigm (P3) in Section 2.2, is superior to the others.\n• Based on the first point, we adopt P3 and propose a novel two-staged pipeline model dubbed RERE. It first detects relation at sentence level and then extracts entities for a specific relation. We model the false negatives in relation extraction as “unlabeled positives” and propose a multi-label collective loss function.\n• Our empirical evaluations show that the proposed method consistently outperforms existing approaches, and achieves excellent perfor-\nmance even learned with a large quantity of false positive samples. We also provide two carefully annotated test sets aiming at reducing the false negatives of previous annotation, namely, NYT21 and SKE21, with 370 and 1150 samples, respectively."
    }, {
      "heading" : "2 Problem Analysis and Pilot Experiments",
      "text" : "We use (ci, Ti) to denote a training instance, where ci is a sentence consisting of N tokens ci = [ci1, ..., ciN ] labeled by a set of triples Ti = {〈s, r, o〉} from the training setD. For rigorous definition, [ci1, ..., ciN ] can be viewed as an ordered set {(ci1, 1), ..., (ciN , N)} so that set operations can be applied. We assume r ∈ R, where R is a finite set of all relations in D. Other model/taskspecific notations are defined after each problem formulation.\nWe now clarify some terms used in the introduction and title without formal definition. A negative sample refers to a triple t /∈ Ti. Negative label refers to the negative class label (e.g., usually “0” for binary classification), used for supervision with respect to task-specific models. Under different task formulation, the negative labels can be different. Negative data is a general term that includes both negative labels and negative samples. There are two kinds of false negatives. Relation-level false negative (S3 in Figure 1) refers to the situation where there exists t′ = 〈s′, r′, o′〉 /∈ Ti, but r′ is actually expressed by ci, and does not appear in other t ∈ Ti. Similarly, Entity-level false negative (S4 and S5 in Figure 1) means r′ appears in other t ∈ Ti. Imbalanced class distribution means that the quantity of negative labels is much larger than that of positive ones."
    }, {
      "heading" : "2.1 Addressing the False Negatives",
      "text" : "As shown in Table 1, the triples in NYT (SKE) datasets2 labeled by Freebase3 (BaiduBaike4) is 88,253 (409,767), while the ones labeled by Wikidata5 (CN-DBPedia6) are 58,135 (342,931). In other words, there exists massive FN matches if only labeled by one KB due to the incompleteness of KBs. Notably, we find that the FN rate is underestimated by previous researches (Min\n2Detailed description of datasets is in Sec. 5.1 3(Bollacker et al., 2008) 4https://baike.baidu.com/ 5(Vrandecic and Krötzsch, 2014) 6 (Xu et al., 2017)\net al., 2013; Xu et al., 2013), based on the manual evaluation of which there are 15%-35% FN matches. This discrepancy may be caused by human error. In specific, a volunteer may accidentally miss some triples. For example, as pointed out by Wei et al. (2020, in Appendix C), the test set of NYT11 (Hoffmann et al., 2011) missed lots of triples, especially when multiple relations occur in a same sentence, though labeled by human. That also provides an evidence that FN’s are harder to discover than FP’s."
    }, {
      "heading" : "2.2 Addressing the Overwhelming Negative Labels",
      "text" : "We point out that some of the previous paradigms designed for relation extraction aggravate the imbalance and lead to inefficient supervision. The mainstream approaches for relation extraction mainly fall into three paradigms depending on what to extract first.\nP1 The first paradigm is a pipeline that begins with named entity recognition (NER) and then classifies each entity pair into different relations, i.e., [s, o then r]. It is adopted by many traditional approaches (Mintz et al., 2009; Chan and Roth, 2011; Zeng et al., 2014, 2015; Gormley et al., 2015; dos Santos et al., 2015; Lin et al., 2016).\nP2 The second paradigm first detects all possible subjects in a sentence then identifies objects with respect to each relation, i.e., [s then r, o]. Specific implementation includes modeling relation extraction as multi-turn question answering (Li et al., 2019), span tagging (Yu et al., 2020) and cascaded binary tagging (Wei et al., 2020).\nP3 The third paradigm first perform sentencelevel relation detection (cf. P1, which is at entity pair level.) then extract subjects and entities, i.e., [r then s, o]. This paradigm is largely unexplored. HRL (Takanobu et al., 2019) is hitherto the only work to apply this paradigm based on our literature review.\nWe provide theoretical analysis of the output space and class prior with statistical support from three datasets (see Section 5.1 for description) of the three paradigms in Table 2. The second step of P1 can be compared with the first step of P3. Both of them find relation from a sentence (P1 with target entity pair given). Suppose a sentence contains m entities7, the classifier has to decide relation from O(m2) entity pairs, while in reality, relations are often sparse, i.e., O(m). In other words, most entity pairs in P1 do not form valid relation, thus resulting in a low class prior. The situation is even worse when the sentence contains more entities, such as in NYT11-HRL. For P2, we demonstrate with the problem formulation of CASREL (Wei et al., 2020). The difference of the first-step class prior between P2 and P3 depends on the result of comparison between # relations and average sentence length (i.e., |R| and N̄ ), which varies in different scenarios/domains. However, π2 of P2 is extremely low, where a classifier has to decide from a space of |R| ∗ N̄ . In contrast, P3 only need to decide from 4 ∗ N̄ based on our task formulation (Section 3.1)\nOther task formulations include jointly extracting the relation and entities (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014; Gupta et al., 2016; Katiyar and Cardie, 2017; Ren et al., 2017) and recently in the manner of sequence tagging (Zheng et al., 2017), sequence-to-sequence learning (Zeng et al., 2018b). In contrast to the aforementioned three paradigms, most of these methods actually provide an incomplete decision space that cannot handle all the situation of relation extrac-\n7Below the same.\ntion, for example, the overlapping one (Wei et al., 2020)."
    }, {
      "heading" : "3 Solution Framework",
      "text" : ""
    }, {
      "heading" : "3.1 Framework of RERE",
      "text" : "Given an instance (ci, Ti) fromD, the goal of training is to maximize the likelihood defined in Eq. (1). It is decomposed into two components by applying the definition of conditional probability, formulated in Eq. (2).\n|D|∏ i=1 Pr(Ti|ci; θ) (1)\n= |D|∏ i=1 ∏ r∈Ti Pr(r|ci; θ) ∏ 〈s,o〉∈Ti|r Pr(s, o|r, ci; θ),\n(2)\nwhere we use r ∈ Ti as a shorthand for r ∈ {r | 〈s, r, o〉 ∈ Ti}, which means that r occurs in the triple set w.r.t. ci; Similarly, s ∈ Ti, 〈s, o〉 ∈ Ti|r stands for s ∈ {s | 〈s, r, o〉 ∈ Ti|r} and 〈s, o〉 ∈ {〈s, o〉 | 〈s, r, o〉 ∈ Ti|r}, respectively. Ti|r represents a subset of Ti with a common relation r. 1[·] is an indicator function; 1[condition] = 1 when the condition happens. We denote by θ the model parameters. Under this decomposition, relational triple extraction task is formulated into two subtasks: relation classification and entity extraction.\nRelation Classification. As is discussed, building relation classifier at entity-pair level will introduce excessive negative samples and form a hard learning problem. Therefore, we alternatively model the relation classification at sentence level. Intuitively speaking, we hope that the model could capture what relation a sentence is expressing. We\nformalize it as a multi-label classification task.\nPr(r|ci; θ) = |R|∏ j=1 (ŷjrc) 1[yjrc=1](1− ŷjrc)1[y j rc=0],\n(3) where ŷjrc is the probability that c is expressing rj , the j-th relation8. yjrc is the ground truth from the labeled data; yjrc = 1 is equivalent to rj ∈ Ti while yjrc = 0 means the opposite.\nEntity Extraction. We then model entity extraction task. We observe that given the relation r and context ci, it naturally forms a machine reading comprehension (MRC) task (Chen, 2018), where (r, ci, s/o) naturally fits into the paradigm of (QUERY, CONTEXT, ANSWER). Particularly, the subjects and objects are continuous spans from ci, which falls into the category of span extraction. We adopt the boundary detection model with answer pointer (Wang and Jiang, 2017) as the output layer, which is widely used in MRC tasks. Formally, for a sentence of N tokens,\nPr(s, o|r, ci; θ)\n= ∏ k∈K N∏ n=1 (ŷn,kee ) 1[yn,kee =1](1− ŷn,kee )1[y n,k ee =0],\n(4)\nwhere K = {sstart, send, ostart, oend} represents the identifier of each pointer; ŷn,kee refers to the probability of n-th token being the start/end of the subject/object. yn,kee is the ground truth from the training data; if ∃s ∈ Ti|r occurs in ci at position from n to n+ l, then yn,sstartee = 1 and y n+l,send ee = 1, otherwise 0; the same applies for the objects.\n8ŷjrc is parameterized by θ, omitted in the equation for clarity, below the same."
    }, {
      "heading" : "3.2 Advantages",
      "text" : "Our task formulation shows several advantages. By adopting P3 as paradigm, the first and foremost advantage of our solution is that it suffers less from the imbalanced classes (Section 2.2). Secondly, relation-level false negative is easy to recover. When modeled as a standard classification problem, many off-the-shelf methods on positive unlabeled learning can be leveraged. Thirdly, entity-level false negatives do not affect relation classification. Taking S5 in Figure 1 as an example, even though the BIRTHPLACE relation between WILLIAM SWARTZ and SCRANTON is missing, the relation classifier can still capture the signal from the other sample with a same relation, i.e., 〈 JOE BIDEN, BIRTHPLACE, SCRANTON 〉. Fourthly, this kind of modeling is easy to update with new relations without the need of retraining a model from bottom up. Only relation classifier needs to be redesigned, while entity extractor can be updated in an online manner without modifying the model structure. Last but not the least, relation classifier can be regarded as a pruning step when applied to practical tasks. Many existing methods treat relation extraction as question answering (Li et al., 2019; Zhao et al., 2020). However, without first identifying the relation, they all need to iterate over all the possible relations and ask diverse questions. This results in extremely low efficiency where time consumed for predicting one sample may take up to |R| times larger than our method."
    }, {
      "heading" : "4 Our Model",
      "text" : "The relational triple extraction task decomposed in Eq. (2) inspires us to design a two-staged pipeline, in which we first detect relation at sentence level and then extract subjects/objects for each relation. The overall architecture of RERE is shown in Figure 2."
    }, {
      "heading" : "4.1 Sentence Classifier with Relational Label",
      "text" : "We first detect relation at sentence level. The input is a sequence of tokens c and we denote by ŷrc = [ŷ1rc, ŷ 2 rc, ..., ŷ |R| rc ] the output vector of the model, which aims to estimate ŷirc in Eq. (3). We use BERT (Devlin et al., 2019) for English and RoBERTa (Liu et al., 2019) for Chinese, pretrained language models with multi-layer bidirectional Transformer structure (Vaswani et al., 2017),\nto encode the inputs9. Specifically, the input sequence xrc = [[CLS], ci,[SEP]], which is fed into BERT for generating a token representation matrix Hrc ∈ RN×d, where d is the hidden dimension defined by pre-trained Transformers. We take h0rc, which is the encoded vector of the first token [CLS], as the representation of the sentence. The final output of relation classification module ŷrc is defined in Eq. (5).\nŷrc = σ(Wrch 0 rc + brc), (5)\nwhere Wrc and brc are trainable model parameters, representing weights and bias, respectively; σ denotes the sigmoid activation function."
    }, {
      "heading" : "4.2 Relation-specific Entity Extractor",
      "text" : "After the relation detected at sentence-level, we extract subjects and objects for each candidate relation. We aim to estimate ŷee = [0, 1]N×4, of which each element corresponds to ŷn,kee in Eq. (4), using a deep neural model. We take ŷrc, the one-hot output vector of relation classifier, and generate query tokens q using each of the detected relations (i.e., the “1”s in ŷrc). We are aware that many recent works (Li et al., 2019; Zhao et al., 2020) have studied how to generate diverse queries for the given relation, which have the potential of achieving better performance. Nevertheless, that is beyond the scope of this paper. To keep things simple, we use the surface text of a relation as the query.\nNext, the input sequence is constructed as xee = [[CLS],qi,[SEP], ci,[SEP]]. Like Section 4.1, we get the token representation matrix Hee ∈ RN×d from BERT. The k-th output pointer of entity extractor is defined by\nŷkee = σ(W k eeHee + b k ee), (6)\nwhere k ∈ {sstart, send, ostart, oend} is in accordance to Eq. (4); Wkee and b k ee are the corresponding parameters. The final subject/object spans are generated by pairing the nearest sstart/ostart with send/oend. Next, all subjects are paired to the nearest object. If multiple objects occur before the next subject appears, all subsequent objects will be paired with it until next subject occurs.\n9For convenience, we refer to the pre-trained Transformer as BERT hereinafter."
    }, {
      "heading" : "4.3 Multi-label Collective Loss function",
      "text" : "In normal cases, the log-likelihood is taken as the learning objective. However, as is emphasized, there exist many false negative samples in the training data. Intuitively speaking, the negative labels cannot be simply considered as negative. Instead, a small portion of the negative labels should be considered as unlabeled positives and their influence towards the penalty should be eradicated. Therefore, we adopt cPU (Xie et al., 2020), a collective loss function that is designed for positive unlabeled learning (PU learning). To briefly review, cPU considers the learning objective to be the correctness under a surrogate function,\n`(ŷ, y) = ln(c(ŷ, y)), (7)\nwhere they redefine the correctness function for PU learning as\nc(ŷ, y) = { E[ŷ] if y = 1, 1− |E[ŷ]− µ| otherwise,\n(8)\nwhere µ is the ratio of false negative data (i.e., the unlabeled positive in the original paper).\nWe extend it to multi-label situation by embodying the original expectation at sample level. Due to the fact that class labels are highly imbalanced for our tasks, we introduce a class weight γ ∈ (0, 1) to downweight the positive penalty. For relation classifier,\n`rc(ŷ,y) =  −γrc ln( 1 |R| |R|∑ i=1 ŷirc]) if yirc = 1\n− ln(1− | 1|R| |R|∑ i=1 ŷirc − µrc|) otherwise.\n(9)\nFor entity extractor,\n`ee(ŷ k,yk) =  −γee ln( N∑ n=1 ŷn,kee ]) if yn,kee = 1 − ln(1− | N∑\nn=1\nŷn,kee − µee|) otherwise.\n(10)\nIn practice, we set µ = π(τ + 1), where τ ≈ 1− # labeled positive# all positive is the ratio of false negative and π is the class prior. Note that µ is not difficult to estimate for both relation classification and entity extraction task in practice. Besides various\nof methods in the PU learning (du Plessis et al., 2015; Bekker and Davis, 2018) for estimating it, an easy approximation is µ ≈ π when π τ , which happens to be the case for our tasks."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Datasets",
      "text" : "Our experiments are conducted on these four datasets10. Some statistics of the datasets are provided in Table 1 and Table 2. In relation extraction, some datasets with the same names involve different preprocessing, which leads to unfair comparison. We briefly review all the datasets below and specify the operations to perform before applying each dataset.\n• NYT (Riedel et al., 2010). NYT is the very first version among all the NYT-related datasets. It is based on the articles in New York Times12. We use the sentences from it to conduct the pilot experiment in Table 1. However, 1) it contains duplicate samples, e.g., 1504 in the training set; 2) It only labels the last word of an entity, which will mislead the evaluation results.\n• NYT10-HRL. & NYT11-HRL. These two datasets are based on NYT. The difference is that they both contain complete entity mentions. NYT10 (Riedel et al., 2010) is the original one. and NYT11 (Hoffmann et al., 2011) is a small version of NYT10 with 53,395 training samples and a manually labeled test set of 368 samples. We refer to them as NYT10HRL and NYT11-HRL after preprocessed by HRL (Takanobu et al., 2019) where they removed 1) training relation not appearing in the testing and 2) “NA” sentences. These two steps are almost adopted by all the compared methods. To compare fairly, we use this version in evaluations.\n• NYT21. We provide relabel version of the test set of NYT11-HRL. The test set of NYT11HRL still have false negative problem. Most of the samples in the NYT11-HRL has only one relation. We manually added back the missing triples to the test set.\n10We do not use WebNLG (Gardent et al., 2017) and ACE0411 because these datasets are not automatically labeled by distant supervision. WebNLG is constructed by natural language generation with triples. ACE04 is manually labeled.\n12https://www.nytimes.com/\n• SKE2019/SKE2113. SKE2019 is a dataset in Chinese published by Baidu. The reason we also adopt this dataset is that it is currently the largest dataset available for relation extraction. There are 194,747 sentences in the training set and 21,639 in the validation set. We manually labeled 1,150 sentences from the test set with 2,765 annotated triples, which we refer to as SKE21. No preprocessing for this dataset is needed. We provide this data for future research14."
    }, {
      "heading" : "5.2 Compared Methods and Metrics",
      "text" : "We evaluate our model by comparing with several models on the same datasets, which are SOTA graphical model MultiR (Hoffmann et al., 2011), joint models SPTree (Miwa and Bansal, 2016) and NovelTagging (Zheng et al., 2017), recent strong SOTA models CopyR (Zeng et al., 2018b), HRL (Takanobu et al., 2019), CasRel (Wei et al., 2020), TPLinker (Wang et al., 2020). We also provide the result of automatically aligning Wikidata/CN-KBpedia with the corpus, namely Match, as a baseline. To note, we only keep the intersected relations, otherwise it will result in low precision due to the false negative in the original dataset. We report standard micro Precision (Prec.), Recall (Rec.) and F1 score for all the experiments. Following the previous works (Takanobu et al., 2019; Wei et al., 2020), we adopt partial match on these data sets for fair comparison. We also provide the results of exact match results of the methods we implemented, and only exact match on SKE2019."
    }, {
      "heading" : "5.3 Overall Comparison",
      "text" : "We show the overall comparison result in Table 3. First, we observe that RERE consistently outperforms all the compared models. We find an interesting result that by purely aligning the database with the corpus, it already achieves surprisingly good overall result (surpassing MultiR) and relatively high precision (comparable to CoType in NYT11-HRL). However, the recall is quite low, which is consistent with our discussion in Section 2.1 that distant supervision leads to many false negatives. We also provide an ablation result where BERT is replaced with a bidirectional\n13http://ai.baidu.com/broad/download? dataset=sked\n14download url.\nLSTM encoder (Graves et al., 2013) with randomly initialized weights. From the results we discover that even without BERT, our framework achieves competitive results against the previous approaches such as CoType and CopyR. This further prove the effectiveness of our RERE framework."
    }, {
      "heading" : "5.4 How Robust is RERE against False Negatives?",
      "text" : "To further study how our model behaves when training data includes different quantity of false negatives, we conduct experiments on synthetic datasets. We construct five new training data by randomly removing triples with probability of 0.1, 0.3 and 0.5, simulating the situation of different FN rates. We show the precision-recall curves of our method in comparison with CASREL (Wei et al., 2020),\nthe best performing competitor, in Figure 3. 1) The overall performance of RERE is superior to competitor models even when trained on a dataset with a 0.5 FN rate. 2) We show that the intervals of RERE between lines are smaller than CASREL, indicating that the performance decline under different FN rates of RERE is smaller. 3) The straight line before curves of our model means that there is no data point at the places where recall is very low. This means that our model is insensitive with the decision boundary and thus more robust."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we revisit the negative data in relation extraction task. We first show that the false negative rate is largely underestimated by previous researches. We then systematically compare three\ncommonly adopted paradigms and prove that our paradigm suffers less from the overwhelming negative labels. Based on this advantage, we propose RERE, a pipelined framework that first detect relations at sentence level and then extract entities for each specific relation and provide a multi-label PU learning loss to recover false negatives. Empirical results show that RERE consistently outperforms the existing state-of-the-arts by a considerable gap, even when learned with large false negative rates."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is supported by National Key Research and Development Project (No. 2020AAA0109302), Shanghai Science and Technology Innovation Action Plan (No.19511120400) and Shanghai Municipal Science and Technology Major Project (No.2021SHZDZX0103). The authors would like to thank the anonymous reviewers for their constructive comments."
    } ],
    "references" : [ {
      "title" : "Estimating the class prior in positive and unlabeled data through decision tree induction",
      "author" : [ "Jessa Bekker", "Jesse Davis." ],
      "venue" : "Proceedings of AAAI.",
      "citeRegEx" : "Bekker and Davis.,? 2018",
      "shortCiteRegEx" : "Bekker and Davis.",
      "year" : 2018
    }, {
      "title" : "Freebase: a collaboratively created graph database for structuring human knowledge",
      "author" : [ "Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor." ],
      "venue" : "Proceedings of SIGMOD.",
      "citeRegEx" : "Bollacker et al\\.,? 2008",
      "shortCiteRegEx" : "Bollacker et al\\.",
      "year" : 2008
    }, {
      "title" : "Exploiting syntactico-semantic structures for relation extraction",
      "author" : [ "Yee Seng Chan", "Dan Roth." ],
      "venue" : "Proceedings of ACL, pages 551–560.",
      "citeRegEx" : "Chan and Roth.,? 2011",
      "shortCiteRegEx" : "Chan and Roth.",
      "year" : 2011
    }, {
      "title" : "Neural Reading Comprehension and Beyond",
      "author" : [ "Danqi Chen." ],
      "venue" : "Ph.D. thesis, Stanford University.",
      "citeRegEx" : "Chen.,? 2018",
      "shortCiteRegEx" : "Chen.",
      "year" : 2018
    }, {
      "title" : "Impact of less skewed distributions on efficiency and effectiveness of biomedical relation extraction",
      "author" : [ "Md. Faisal Mahbub Chowdhury", "A. Lavelli." ],
      "venue" : "Proceedings of COLING.",
      "citeRegEx" : "Chowdhury and Lavelli.,? 2012",
      "shortCiteRegEx" : "Chowdhury and Lavelli.",
      "year" : 2012
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "J. Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of NAACL-HLT.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Reinforcement learning for relation classification from noisy data",
      "author" : [ "Jun Feng", "Minlie Huang", "Li Zhao", "Yang Yang", "Xiaoyan Zhu." ],
      "venue" : "Proceedings of AAAI, volume 32.",
      "citeRegEx" : "Feng et al\\.,? 2018",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2018
    }, {
      "title" : "Creating training corpora for NLG micro-planners",
      "author" : [ "Claire Gardent", "Anastasia Shimorina", "Shashi Narayan", "Laura Perez-Beltrachini." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Gardent et al\\.,? 2017",
      "shortCiteRegEx" : "Gardent et al\\.",
      "year" : 2017
    }, {
      "title" : "Improved relation extraction with feature-rich compositional embedding models",
      "author" : [ "Matthew R Gormley", "Mo Yu", "Mark Dredze." ],
      "venue" : "Proceedings of ACL, pages 1774–1784.",
      "citeRegEx" : "Gormley et al\\.,? 2015",
      "shortCiteRegEx" : "Gormley et al\\.",
      "year" : 2015
    }, {
      "title" : "Speech recognition with deep recurrent neural networks",
      "author" : [ "Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton." ],
      "venue" : "2013 IEEE international conference on acoustics, speech and signal processing, pages 6645–6649.",
      "citeRegEx" : "Graves et al\\.,? 2013",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2013
    }, {
      "title" : "Table filling multi-task recurrent neural network for joint entity and relation extraction",
      "author" : [ "Pankaj Gupta", "Hinrich Schütze", "Bernt Andrassy." ],
      "venue" : "Proceedings of COLING, pages 2537–2547.",
      "citeRegEx" : "Gupta et al\\.,? 2016",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2016
    }, {
      "title" : "Knowledge-based weak supervision for information extraction of overlapping relations",
      "author" : [ "R. Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S. Weld." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Hoffmann et al\\.,? 2011",
      "shortCiteRegEx" : "Hoffmann et al\\.",
      "year" : 2011
    }, {
      "title" : "ARNOR: Attention regularization based noise reduction for distant supervision relation classification",
      "author" : [ "Wei Jia", "Dai Dai", "Xinyan Xiao", "Hua Wu." ],
      "venue" : "Proceedings of ACL, pages 1399–1408.",
      "citeRegEx" : "Jia et al\\.,? 2019",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2019
    }, {
      "title" : "Going out on a limb: Joint extraction of entity mentions and relations without dependency trees",
      "author" : [ "Arzoo Katiyar", "Claire Cardie." ],
      "venue" : "Proceedings of ACL, pages 917–928.",
      "citeRegEx" : "Katiyar and Cardie.,? 2017",
      "shortCiteRegEx" : "Katiyar and Cardie.",
      "year" : 2017
    }, {
      "title" : "Incremental joint extraction of entity mentions and relations",
      "author" : [ "Q. Li", "Heng Ji." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Li and Ji.,? 2014",
      "shortCiteRegEx" : "Li and Ji.",
      "year" : 2014
    }, {
      "title" : "Dice loss for data-imbalanced nlp tasks",
      "author" : [ "Xiaoya Li", "Xiaofei Sun", "Yuxian Meng", "Junjun Liang", "F. Wu", "J. Li." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Entityrelation extraction as multi-turn question answering",
      "author" : [ "Xiaoya Li", "Fan Yin", "Zijun Sun", "Xiayu Li", "Arianna Yuan", "Duo Chai", "Mingxin Zhou", "J. Li." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Adaptive scaling for sparse detection in information extraction",
      "author" : [ "Hongyu Lin", "Yaojie Lu", "Xianpei Han", "Le Sun." ],
      "venue" : "Proceedings of ACL, pages 1033–1043.",
      "citeRegEx" : "Lin et al\\.,? 2018",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural relation extraction with selective attention over instances",
      "author" : [ "Yankai Lin", "Shiqi Shen", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun." ],
      "venue" : "Proceedings of ACL, pages 2124–2133.",
      "citeRegEx" : "Lin et al\\.,? 2016",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2016
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Distant supervision for relation extraction with an incomplete knowledge base",
      "author" : [ "Bonan Min", "Ralph Grishman", "Li Wan", "Chang Wang", "David Gondek." ],
      "venue" : "Proceedings of HLT-NAACL.",
      "citeRegEx" : "Min et al\\.,? 2013",
      "shortCiteRegEx" : "Min et al\\.",
      "year" : 2013
    }, {
      "title" : "Distant supervision for relation extraction without labeled data",
      "author" : [ "Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Mintz et al\\.,? 2009",
      "shortCiteRegEx" : "Mintz et al\\.",
      "year" : 2009
    }, {
      "title" : "End-to-end relation extraction using lstms on sequences and tree structures",
      "author" : [ "Makoto Miwa", "Mohit Bansal." ],
      "venue" : "Proceedings of ACL, pages 1105–1116.",
      "citeRegEx" : "Miwa and Bansal.,? 2016",
      "shortCiteRegEx" : "Miwa and Bansal.",
      "year" : 2016
    }, {
      "title" : "Modeling joint entity and relation extraction with table representation",
      "author" : [ "Makoto Miwa", "Yutaka Sasaki." ],
      "venue" : "Proceedings of EMNLP, pages 1858–1869.",
      "citeRegEx" : "Miwa and Sasaki.,? 2014",
      "shortCiteRegEx" : "Miwa and Sasaki.",
      "year" : 2014
    }, {
      "title" : "Class-prior estimation for learning from positive and unlabeled data",
      "author" : [ "Marthinus Christoffel du Plessis", "Gang Niu", "Masashi Sugiyama." ],
      "venue" : "Machine Learning, 106:463–492.",
      "citeRegEx" : "Plessis et al\\.,? 2015",
      "shortCiteRegEx" : "Plessis et al\\.",
      "year" : 2015
    }, {
      "title" : "Cotype: Joint extraction of typed entities and relations with knowledge bases",
      "author" : [ "Xiang Ren", "Zeqiu Wu", "Wenqi He", "Meng Qu", "Clare R Voss", "Heng Ji", "Tarek F Abdelzaher", "Jiawei Han." ],
      "venue" : "Proceedings of WWW, pages 1015–1024.",
      "citeRegEx" : "Ren et al\\.,? 2017",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2017
    }, {
      "title" : "Modeling relations and their mentions without labeled text",
      "author" : [ "S. Riedel", "Limin Yao", "A. McCallum." ],
      "venue" : "Proceedings of ECML/PKDD.",
      "citeRegEx" : "Riedel et al\\.,? 2010",
      "shortCiteRegEx" : "Riedel et al\\.",
      "year" : 2010
    }, {
      "title" : "Improving distant supervision using inference learning",
      "author" : [ "Roland Roller", "Eneko Agirre", "Aitor Soroa", "Mark Stevenson." ],
      "venue" : "Proceedings of ACL, pages 273–278.",
      "citeRegEx" : "Roller et al\\.,? 2015",
      "shortCiteRegEx" : "Roller et al\\.",
      "year" : 2015
    }, {
      "title" : "Classifying relations by ranking with convolutional neural networks",
      "author" : [ "Cicero dos Santos", "Bing Xiang", "Bowen Zhou." ],
      "venue" : "Proceedings of ACL, pages 626– 634.",
      "citeRegEx" : "Santos et al\\.,? 2015",
      "shortCiteRegEx" : "Santos et al\\.",
      "year" : 2015
    }, {
      "title" : "A hierarchical framework for relation extraction with reinforcement learning",
      "author" : [ "Ryuichi Takanobu", "Tianyang Zhang", "Jiexi Liu", "Minlie Huang." ],
      "venue" : "Proceedings of AAAI, volume 33, pages 7072–7079.",
      "citeRegEx" : "Takanobu et al\\.,? 2019",
      "shortCiteRegEx" : "Takanobu et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of NeuroIPS, pages 6000– 6010.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Wikidata: a free collaborative knowledgebase",
      "author" : [ "Denny Vrandecic", "M. Krötzsch." ],
      "venue" : "Communications of the ACM, 57:78–85.",
      "citeRegEx" : "Vrandecic and Krötzsch.,? 2014",
      "shortCiteRegEx" : "Vrandecic and Krötzsch.",
      "year" : 2014
    }, {
      "title" : "Machine comprehension using match-lstm and answer pointer",
      "author" : [ "Shuohang Wang", "Jing Jiang." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Wang and Jiang.,? 2017",
      "shortCiteRegEx" : "Wang and Jiang.",
      "year" : 2017
    }, {
      "title" : "TPLinker: Single-stage joint extraction of entities and relations through token pair linking",
      "author" : [ "Yucheng Wang", "Bowen Yu", "Yueyang Zhang", "Tingwen Liu", "Hongsong Zhu", "Limin Sun." ],
      "venue" : "Proceedings of COLING, pages 1572–1582.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "A novel cascade binary tagging framework for relational triple extraction",
      "author" : [ "Zhepei Wei", "Jianlin Su", "Yue Wang", "Y. Tian", "Yi Chang." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Wei et al\\.,? 2020",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2020
    }, {
      "title" : "Collective loss function for positive and unlabeled learning",
      "author" : [ "Chenhao Xie", "Qiao Cheng", "Jiaqing Liang", "Lihan Chen", "Y. Xiao." ],
      "venue" : "ArXiv, abs/2005.03228.",
      "citeRegEx" : "Xie et al\\.,? 2020",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2020
    }, {
      "title" : "CNDBpedia: A never-ending chinese knowledge extraction system",
      "author" : [ "Bo Xu", "Yong Xu", "Jiaqing Liang", "Chenhao Xie", "Bin Liang", "Wanyun Cui", "Y. Xiao." ],
      "venue" : "Proceedings of IEA/AIE.",
      "citeRegEx" : "Xu et al\\.,? 2017",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2017
    }, {
      "title" : "Filling knowledge base gaps for distant supervision of relation extraction",
      "author" : [ "Wei Xu", "Raphael Hoffmann", "Le Zhao", "Ralph Grishman." ],
      "venue" : "Proceedings of ACL, pages 665–670.",
      "citeRegEx" : "Xu et al\\.,? 2013",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2013
    }, {
      "title" : "Joint extraction of entities and relations based on a novel decomposition strategy",
      "author" : [ "Bowen Yu", "Zhenyu Zhang", "Xiaobo Shu", "Tingwen Liu", "Yubin Wang", "Bin Wang", "Sujian Li." ],
      "venue" : "Proceedings of ECAI.",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Jointly identifying entities and extracting relations in encyclopedia text via a graphical model approach",
      "author" : [ "Xiaofeng Yu", "Wai Lam." ],
      "venue" : "Proceedings of COLING, pages 1399–1407.",
      "citeRegEx" : "Yu and Lam.,? 2010",
      "shortCiteRegEx" : "Yu and Lam.",
      "year" : 2010
    }, {
      "title" : "Distant supervision for relation extraction via piecewise convolutional neural networks",
      "author" : [ "Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Zeng et al\\.,? 2015",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2015
    }, {
      "title" : "Relation classification via convolutional deep neural network",
      "author" : [ "Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao" ],
      "venue" : "In Proceedings of COLING,",
      "citeRegEx" : "Zeng et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2014
    }, {
      "title" : "Large scaled relation extraction with reinforcement learning",
      "author" : [ "Xiangrong Zeng", "Shizhu He", "Kang Liu", "Jun Zhao." ],
      "venue" : "Proceedings of AAAI, volume 32.",
      "citeRegEx" : "Zeng et al\\.,? 2018a",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2018
    }, {
      "title" : "Extracting relational facts by an end-to-end neural model with copy mechanism",
      "author" : [ "Xiangrong Zeng", "Daojian Zeng", "Shizhu He", "Kang Liu", "Jun Zhao." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Zeng et al\\.,? 2018b",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2018
    }, {
      "title" : "Asking effective and diverse questions: A machine reading comprehension based framework for joint entity-relation extraction",
      "author" : [ "Tianyang Zhao", "Zhao Yan", "Y. Cao", "Zhoujun Li." ],
      "venue" : "Proceedings of IJCAI.",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    }, {
      "title" : "Joint extraction of entities and relations based on a novel tagging scheme",
      "author" : [ "Suncong Zheng", "Feng Wang", "Hongyun Bao", "Yuexing Hao", "Peng Zhou", "Bo Xu." ],
      "venue" : "Proceedings of ACL, pages 1227–1236.",
      "citeRegEx" : "Zheng et al\\.,? 2017",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "This task is usually modeled as a supervised learning problem and distant supervision (Mintz et al., 2009) is utilized to acquire large-scale training data.",
      "startOffset" : 86,
      "endOffset" : 106
    }, {
      "referenceID" : 12,
      "context" : "Many efforts have been devoted to solving the FP problem, including pattern-based methods (Jia et al., 2019), multi-instance learning methods (Lin et al.",
      "startOffset" : 90,
      "endOffset" : 108
    }, {
      "referenceID" : 18,
      "context" : ", 2019), multi-instance learning methods (Lin et al., 2016; Zeng et al., 2018a) and reinforcement learning methods (Feng et al.",
      "startOffset" : 41,
      "endOffset" : 79
    }, {
      "referenceID" : 42,
      "context" : ", 2019), multi-instance learning methods (Lin et al., 2016; Zeng et al., 2018a) and reinforcement learning methods (Feng et al.",
      "startOffset" : 41,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : ", 2018a) and reinforcement learning methods (Feng et al., 2018).",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 20,
      "context" : "However, FN problem receives much less attention (Min et al., 2013; Xu et al., 2013; Roller et al., 2015).",
      "startOffset" : 49,
      "endOffset" : 105
    }, {
      "referenceID" : 37,
      "context" : "However, FN problem receives much less attention (Min et al., 2013; Xu et al., 2013; Roller et al., 2015).",
      "startOffset" : 49,
      "endOffset" : 105
    }, {
      "referenceID" : 27,
      "context" : "However, FN problem receives much less attention (Min et al., 2013; Xu et al., 2013; Roller et al., 2015).",
      "startOffset" : 49,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "As is widely acknowledged, information extraction tasks are highly imbalanced in class labels (Chowdhury and Lavelli, 2012; Lin et al., 2018; Li et al., 2020).",
      "startOffset" : 94,
      "endOffset" : 158
    }, {
      "referenceID" : 17,
      "context" : "As is widely acknowledged, information extraction tasks are highly imbalanced in class labels (Chowdhury and Lavelli, 2012; Lin et al., 2018; Li et al., 2020).",
      "startOffset" : 94,
      "endOffset" : 158
    }, {
      "referenceID" : 15,
      "context" : "As is widely acknowledged, information extraction tasks are highly imbalanced in class labels (Chowdhury and Lavelli, 2012; Lin et al., 2018; Li et al., 2020).",
      "startOffset" : 94,
      "endOffset" : 158
    }, {
      "referenceID" : 36,
      "context" : "com/ (5)(Vrandecic and Krötzsch, 2014) 6 (Xu et al., 2017)",
      "startOffset" : 41,
      "endOffset" : 58
    }, {
      "referenceID" : 11,
      "context" : "(2020, in Appendix C), the test set of NYT11 (Hoffmann et al., 2011) missed lots of triples, especially when multiple relations occur in a same sentence, though labeled by human.",
      "startOffset" : 45,
      "endOffset" : 68
    }, {
      "referenceID" : 21,
      "context" : "It is adopted by many traditional approaches (Mintz et al., 2009; Chan and Roth, 2011; Zeng et al., 2014, 2015; Gormley et al., 2015; dos Santos et al., 2015; Lin et al., 2016).",
      "startOffset" : 45,
      "endOffset" : 176
    }, {
      "referenceID" : 2,
      "context" : "It is adopted by many traditional approaches (Mintz et al., 2009; Chan and Roth, 2011; Zeng et al., 2014, 2015; Gormley et al., 2015; dos Santos et al., 2015; Lin et al., 2016).",
      "startOffset" : 45,
      "endOffset" : 176
    }, {
      "referenceID" : 8,
      "context" : "It is adopted by many traditional approaches (Mintz et al., 2009; Chan and Roth, 2011; Zeng et al., 2014, 2015; Gormley et al., 2015; dos Santos et al., 2015; Lin et al., 2016).",
      "startOffset" : 45,
      "endOffset" : 176
    }, {
      "referenceID" : 18,
      "context" : "It is adopted by many traditional approaches (Mintz et al., 2009; Chan and Roth, 2011; Zeng et al., 2014, 2015; Gormley et al., 2015; dos Santos et al., 2015; Lin et al., 2016).",
      "startOffset" : 45,
      "endOffset" : 176
    }, {
      "referenceID" : 16,
      "context" : "Specific implementation includes modeling relation extraction as multi-turn question answering (Li et al., 2019), span tagging (Yu et al.",
      "startOffset" : 95,
      "endOffset" : 112
    }, {
      "referenceID" : 38,
      "context" : ", 2019), span tagging (Yu et al., 2020) and cascaded binary tagging (Wei et al.",
      "startOffset" : 22,
      "endOffset" : 39
    }, {
      "referenceID" : 34,
      "context" : ", 2020) and cascaded binary tagging (Wei et al., 2020).",
      "startOffset" : 36,
      "endOffset" : 54
    }, {
      "referenceID" : 29,
      "context" : "HRL (Takanobu et al., 2019) is hitherto the only work to apply this paradigm based on our literature review.",
      "startOffset" : 4,
      "endOffset" : 27
    }, {
      "referenceID" : 34,
      "context" : "For P2, we demonstrate with the problem formulation of CASREL (Wei et al., 2020).",
      "startOffset" : 62,
      "endOffset" : 80
    }, {
      "referenceID" : 39,
      "context" : "Other task formulations include jointly extracting the relation and entities (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014; Gupta et al., 2016; Katiyar and Cardie, 2017; Ren et al., 2017) and recently in the manner of sequence tagging (Zheng et al.",
      "startOffset" : 77,
      "endOffset" : 199
    }, {
      "referenceID" : 14,
      "context" : "Other task formulations include jointly extracting the relation and entities (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014; Gupta et al., 2016; Katiyar and Cardie, 2017; Ren et al., 2017) and recently in the manner of sequence tagging (Zheng et al.",
      "startOffset" : 77,
      "endOffset" : 199
    }, {
      "referenceID" : 23,
      "context" : "Other task formulations include jointly extracting the relation and entities (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014; Gupta et al., 2016; Katiyar and Cardie, 2017; Ren et al., 2017) and recently in the manner of sequence tagging (Zheng et al.",
      "startOffset" : 77,
      "endOffset" : 199
    }, {
      "referenceID" : 10,
      "context" : "Other task formulations include jointly extracting the relation and entities (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014; Gupta et al., 2016; Katiyar and Cardie, 2017; Ren et al., 2017) and recently in the manner of sequence tagging (Zheng et al.",
      "startOffset" : 77,
      "endOffset" : 199
    }, {
      "referenceID" : 13,
      "context" : "Other task formulations include jointly extracting the relation and entities (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014; Gupta et al., 2016; Katiyar and Cardie, 2017; Ren et al., 2017) and recently in the manner of sequence tagging (Zheng et al.",
      "startOffset" : 77,
      "endOffset" : 199
    }, {
      "referenceID" : 25,
      "context" : "Other task formulations include jointly extracting the relation and entities (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014; Gupta et al., 2016; Katiyar and Cardie, 2017; Ren et al., 2017) and recently in the manner of sequence tagging (Zheng et al.",
      "startOffset" : 77,
      "endOffset" : 199
    }, {
      "referenceID" : 45,
      "context" : ", 2017) and recently in the manner of sequence tagging (Zheng et al., 2017), sequence-to-sequence learning (Zeng et al.",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 43,
      "context" : ", 2017), sequence-to-sequence learning (Zeng et al., 2018b).",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 34,
      "context" : "tion, for example, the overlapping one (Wei et al., 2020).",
      "startOffset" : 39,
      "endOffset" : 57
    }, {
      "referenceID" : 3,
      "context" : "We observe that given the relation r and context ci, it naturally forms a machine reading comprehension (MRC) task (Chen, 2018), where (r, ci, s/o) naturally fits into the paradigm of (QUERY, CONTEXT, ANSWER).",
      "startOffset" : 115,
      "endOffset" : 127
    }, {
      "referenceID" : 32,
      "context" : "We adopt the boundary detection model with answer pointer (Wang and Jiang, 2017) as the output layer, which is widely used in MRC tasks.",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 16,
      "context" : "Many existing methods treat relation extraction as question answering (Li et al., 2019; Zhao et al., 2020).",
      "startOffset" : 70,
      "endOffset" : 106
    }, {
      "referenceID" : 44,
      "context" : "Many existing methods treat relation extraction as question answering (Li et al., 2019; Zhao et al., 2020).",
      "startOffset" : 70,
      "endOffset" : 106
    }, {
      "referenceID" : 5,
      "context" : "We use BERT (Devlin et al., 2019) for English and RoBERTa (Liu et al.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 19,
      "context" : ", 2019) for English and RoBERTa (Liu et al., 2019) for Chinese, pretrained language models with multi-layer bidirectional Transformer structure (Vaswani et al.",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 30,
      "context" : ", 2019) for Chinese, pretrained language models with multi-layer bidirectional Transformer structure (Vaswani et al., 2017), to encode the inputs9.",
      "startOffset" : 101,
      "endOffset" : 123
    }, {
      "referenceID" : 16,
      "context" : "We are aware that many recent works (Li et al., 2019; Zhao et al., 2020) have studied how to generate diverse queries for the given relation, which have the potential of achieving better performance.",
      "startOffset" : 36,
      "endOffset" : 72
    }, {
      "referenceID" : 44,
      "context" : "We are aware that many recent works (Li et al., 2019; Zhao et al., 2020) have studied how to generate diverse queries for the given relation, which have the potential of achieving better performance.",
      "startOffset" : 36,
      "endOffset" : 72
    }, {
      "referenceID" : 35,
      "context" : "Therefore, we adopt cPU (Xie et al., 2020), a collective loss function that is designed for positive unlabeled learning (PU learning).",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "3578 of methods in the PU learning (du Plessis et al., 2015; Bekker and Davis, 2018) for estimating it, an easy approximation is μ ≈ π when π τ , which happens to be the case for our tasks.",
      "startOffset" : 35,
      "endOffset" : 84
    }, {
      "referenceID" : 11,
      "context" : "and NYT11 (Hoffmann et al., 2011) is a small version of NYT10 with 53,395 training samples and a manually labeled test set of 368 samples.",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 29,
      "context" : "We refer to them as NYT10HRL and NYT11-HRL after preprocessed by HRL (Takanobu et al., 2019) where they removed 1) training relation not appearing in the testing and 2) “NA” sentences.",
      "startOffset" : 69,
      "endOffset" : 92
    }, {
      "referenceID" : 7,
      "context" : "We do not use WebNLG (Gardent et al., 2017) and ACE04(11) because these datasets are not automatically labeled by distant supervision.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 11,
      "context" : "We evaluate our model by comparing with several models on the same datasets, which are SOTA graphical model MultiR (Hoffmann et al., 2011), joint models SPTree (Miwa and Bansal, 2016) and NovelTagging (Zheng et al.",
      "startOffset" : 115,
      "endOffset" : 138
    }, {
      "referenceID" : 22,
      "context" : ", 2011), joint models SPTree (Miwa and Bansal, 2016) and NovelTagging (Zheng et al.",
      "startOffset" : 29,
      "endOffset" : 52
    }, {
      "referenceID" : 45,
      "context" : ", 2011), joint models SPTree (Miwa and Bansal, 2016) and NovelTagging (Zheng et al., 2017), recent strong SOTA models CopyR (Zeng et al.",
      "startOffset" : 70,
      "endOffset" : 90
    }, {
      "referenceID" : 43,
      "context" : ", 2017), recent strong SOTA models CopyR (Zeng et al., 2018b), HRL (Takanobu et al.",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 34,
      "context" : ", 2019), CasRel (Wei et al., 2020), TPLinker (Wang et al.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 29,
      "context" : "Following the previous works (Takanobu et al., 2019; Wei et al., 2020), we adopt partial match on these data sets for fair comparison.",
      "startOffset" : 29,
      "endOffset" : 70
    }, {
      "referenceID" : 34,
      "context" : "Following the previous works (Takanobu et al., 2019; Wei et al., 2020), we adopt partial match on these data sets for fair comparison.",
      "startOffset" : 29,
      "endOffset" : 70
    }, {
      "referenceID" : 34,
      "context" : "The results with only one decimal are quoted from (Wei et al., 2020).",
      "startOffset" : 50,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : "LSTM encoder (Graves et al., 2013) with randomly initialized weights.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 34,
      "context" : "We show the precision-recall curves of our method in comparison with CASREL (Wei et al., 2020), the best performing competitor, in Figure 3.",
      "startOffset" : 76,
      "endOffset" : 94
    } ],
    "year" : 2021,
    "abstractText" : "Distantly supervision automatically generates plenty of training samples for relation extraction. However, it also incurs two major problems: noisy labels and imbalanced training data. Previous works focus more on reducing wrongly labeled relations (false positives) while few explore the missing relations that are caused by incompleteness of knowledge base (false negatives). Furthermore, the quantity of negative labels overwhelmingly surpasses the positive ones in previous problem formulations. In this paper, we first provide a thorough analysis of the above challenges caused by negative data. Next, we formulate the problem of relation extraction into as a positive unlabeled learning task to alleviate false negative problem. Thirdly, we propose a pipeline approach, dubbed RERE, that first performs sentence classification with relational labels and then extracts the subjects/objects. Experimental results show that the proposed method consistently outperforms existing approaches and remains excellent performance even learned with a large quantity of false positive samples. Source code is available online1.",
    "creator" : "LaTeX with hyperref"
  }
}