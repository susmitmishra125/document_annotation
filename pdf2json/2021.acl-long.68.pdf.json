{
  "name" : "2021.acl-long.68.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Multilingual Speech Translation from Efficient Finetuning of Pretrained Models",
    "authors" : [ "Xian Li", "Changhan Wang", "Yun Tang", "Chau Tran", "Yuqing Tang", "Juan Pino", "Alexei Baevski", "Alexis Conneau", "Michael Auli" ],
    "emails" : [ "abaevski@fb.com", "aconneau@fb.com", "michaelauli@fb.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 827–838\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n827\nWe present a simple yet effective approach to build multilingual speech-to-text (ST) translation through efficient transfer learning from a pretrained speech encoder and text decoder. Our key finding is that a minimalistic LNA (LayerNorm and Attention) finetuning can achieve zero-shot crosslingual and crossmodality transfer ability by only finetuning 10 ∼ 50% of the pretrained parameters. This effectively leverages large pretrained models at low training cost such as wav2vec 2.0 for acoustic modeling, and mBART for multilingual text generation. This sets a new state-ofthe-art for 36 translation directions (and surpassing cascaded ST for 30 of them) on the large-scale multilingual ST benchmark CoVoST 2 (Wang et al., 2020b) (+6.4 BLEU on average for En-X directions and +6.7 BLEU for X-En directions). Our approach demonstrates strong zero-shot performance in a many-to-many multilingual model (+5.6 BLEU on average across 28 directions), making it an appealing approach for attaining highquality speech translation with improved parameter and data efficiency."
    }, {
      "heading" : "1 Introduction",
      "text" : "Recent advances in pretraining over unlabeled data and then finetuning on labeled data leads to significant performance improvement in text understanding and generation tasks (Devlin et al., 2019; Liu et al., 2020; Conneau et al., 2019; Radford, 2018). Lately, such text pretraining and finetuning paradigms have been extended to other modalities: audio (Schneider et al., 2019; Baevski et al., 2020), images (Su et al., 2019; Lu et al., 2019), and video (Sun et al., 2019). At the same time, pretraining and finetuning techniques have improved multitasking applications significantly, such as multilingual translation, cross-lingual representations, question-answering and so on (Raffel et al., 2020;\nwav2vec 2.0\nmBART\nFinetune LayerNorm and Attention \nPretrained Modules\nYang et al., 2019; Tang et al., 2020). In this paper, we advance the one-model-for-all paradigm further by adapting audio and multilingual text pretraining and finetuning to improve multilingual speech-totext translation.\nOur contributions are as follows:\n• We propose a simple and effective approach to combine pretrained single-modality modules to perform speech-to-text translation. With minimal architecture change, we add a crossmodal adaptor to bridge the length discrepancy between audio encoder output and text decoder input. Our approach can also perform multi-task finetuning with both speech-to-text translation and text-to-text translation tasks where we find joint training with the latter brings further gains.\n• We present an efficient transfer learning strategy by only finetuning the LayerNorm and Attention (LNA) parameters of pretrained models. This approach is not only parameterand data-efficient but also effective for zero-\nshot crosslingual transfer to unseen languages (train on A→ B, test on A→ C and C→ B).\n• Our approach is also effective for zero-shot multilingual translation (train on A→ B and B→ C, test on A→ C), which provides an efficient approach for many-to-many speechto-text translation without dependency for parallel data for every direction.\n• Using a pretrained audio encoder (wav2vec (Baevski et al., 2020)) and multilingual text decoder (mBART (Liu et al., 2020)), this approach sets a new state-of-the-art (SOTA) on two large-scale speech translation benchmarks. On CoVoST 2 (Wang et al., 2020b), we pushed the SOTA for end-to-end approach for all 21 X-En directions(+6.7 BLEU on average) and 15 En-X directions (+6.4 BLEU on average) by finetuning only 10 ∼ 50% of parameters. Similarly on Europarl (IranzoSánchez et al., 2020), our zero-shot multilingual many-to-many model is not only data efficient, but also brings +5.7 BLEU (on average) when translating 18 non-English directions compared to a many-to-many model training on 1.6× training data with all pairwise (both to/from English and non-English) directions.\nWe describe our approach in Section 2, namely pretrained models, length adaptor, LNA finetuning and joint speech-text finetuning as is illustrated in Figure 1. Experiments setup and results are elaborated in Section 3 and Section 4. Section 5 provides ablation studies of the proposed finetuning strategy."
    }, {
      "heading" : "2 Methods",
      "text" : ""
    }, {
      "heading" : "2.1 Pretrained Modules",
      "text" : "Our model leverages a pretrained wav2vec 2.0 (Baevski et al., 2020) as encoder for acoustic modeling, a pretrained multilingual BART (mBART) (Liu et al., 2020) as decoder for language modeling. Both models are pretrained on unlabelled data via self-supervised learning. We provide an overview of the pretraining procedure in A.1."
    }, {
      "heading" : "2.2 Length Adaptor",
      "text" : "We add a lightweight adaptor module in between encoder and decoder to better align the two mod-\nules pretrained with different modalities. The adaptor module performs projection and downsampling to alleviate length inconsistency between the audio and text sequences. Specifically, we use a stack of n 1-dimensional convolutional layers with stride m to shrink the speech sequence (encoder output) by a factor of mn."
    }, {
      "heading" : "2.3 LNA Finetuning",
      "text" : "Instead of finetuning all parameters in pretrained models, we propose parameter efficient finetuning strategy (LNA) of only finetuning the layer normalization (LayerNorm) and multi-head attention (MHA) parameters. LNA is motivated to bridge the discrepancy between pretraining and downstream (ST) task, which we hypothesize are accounted by the following parameters: LayerNorm parameters from pretrained models were trained based on the statistics of the data used in pretraining and thus need to be adapted to downstream tasks during finetuning. The importance of finetuning LayerNorm has been observed in multilingual (text-only) translation (Stickland et al., 2020). Attention Encoder attention (EA, attention to encoder outputs) parameters from pretrained MT decoder were trained on the text-to-text MT task, so we hypothesize that they are crucial to be adapted to the speech encoder output. Combined with LayerNorm parameter is the proposed LNA-Minimalist finetuning. In addition, we also investigate the role of self attention (SA) parameters in facilitating crosslingual transfer ability."
    }, {
      "heading" : "2.4 Joint Speech-text Finetuning",
      "text" : "Multi-task learning has been shown as an effective approach to improve the performance of the speech translation task using other related tasks, such as MT and ASR (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Bahar et al., 2019; Tang et al., 2021a,b). We jointly train MT and ST tasks in the finetuning with pretrained models. The speech transcripts are used as input for the MT task and the corresponding speech data is used as input for the ST task. As a result, we can leverage abundant parallel text data to further improve the performance."
    }, {
      "heading" : "3 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "3.1 Datasets",
      "text" : "We evaluate our proposed models on two largescale multilingual speech translation benchmarks.\nStatistics of the datasets and implementation details are reported in the A.2 and A.3. CoVoST 2 (Wang et al., 2020b) is a multilingual speech-to-text translation corpus with English into 15 languages (En-X) and 21 languages into English (X-En). It provides a comprehensive test bed for low-resource scenarios, with 4 X-En directions between 10 hours and 20 hours training data, and 11 X-En directions less than 4 hours training data. Europarl ST (Iranzo-Sánchez et al., 2020) has both English-centric as well as non-English directions, which allow us to evaluate the proposed method’s effectiveness of multilingual translation between any pair, especially zero-shot performance. We experiment on all 6 languages (de, en, es, fr, it, pt). We compare to a multilingual baseline trained with all pair-wise parallel data."
    }, {
      "heading" : "3.2 Training",
      "text" : "We evaluate the following instantiation of the proposed method which is referred to as XMEF (CrossModal Efficient Finetuning). Encoder. We initialize the encoder using the opensourced1 wav2vec 2.0 large architecture pretrained on unlabelled English-only (XMEF-En) audio from LibriVox (Baevski et al., 2020). For many-to-one experiments, we also experiment with a multilingual wav2vec 2.0 (XMEF-X), which was pretrained on raw audio from 53 languages (Conneau et al., 2020). Encoder output is followed by 3 1- D convolution layers with stride 2 to achieve 8x down-sampling of audio encoder outputs. Decoder. We initialize the decoder with opensourced2 mBART50 models and the same vocabulary (Tang et al., 2020). We use mBART50N1 (49 languages to English) for X-En ST directions and mBART501N (English to 49 languages) for translating En-X ST directions. LNA Finetuning. We study the parameter efficiency and crosslingual transfer ability of LNA finetuning in the bilingual setting without the additional effect from multilingual training. Drawing learnings on that, we then evaluate applying LNA finetuning to encoder only (LNA-E), decoder only (LNA-D), and both (LNA-E,D) respectively. For multilingual finetuning on CoVoST 2, we use all X-En training data (except zero-shot crosslingual transfer experiments) for evaluating X-En perfor-\n1https://github.com/pytorch/fairseq/ tree/master/examples/wav2vec.\n2https://github.com/pytorch/fairseq/ tree/master/examples/multilingual\nmance, and En-X data from all directions for evaluating En-X performance. For evaluating multilingual zero-shot performance on Europarl, we only use X-En and En-X for finetuning and evaluate on all (X-X) pairs.\nJoint Training. Two encoders are initialized with the pretrained mBART encoder and wav2vec 2.0 encoder mentioned above, and are used for text and speech input respectively. The last 12 transformer layers in the wav2vec encoder are replaced with 12 mBART encoder layers. Parameters in those 12 layers are shared between the two encoders during joint training (Tang et al., 2021b). The decoder is also shared between two tasks and is initialized with the pretrained mBART decoder model. We also experimented with adding additional bitext used in ML50 (Tang et al., 2020) as training data for the MT task. Only the language pairs present in the CoVoST 2 dataset are chosen and they cover all language pairs except English to and from “Ca” and “Cy”. We fine-tune all parameters in this experiments due to the large mismatch of the pretrained model (mBART encoder as part of the speech encoder) and more available training data."
    }, {
      "heading" : "3.3 Baselines",
      "text" : "From scratch: The first baseline trains a sequenceto-sequence model with Transformer architecture without any pretraining.For CoVoST 2 experiments, we use the same model configuration as is provided by (Wang et al., 2020b). ASRPT+Multi: Pretraining encoder on ASR task was shown to be an effective method to improve speech translation and accelerates convergence (Bansal et al., 2019). We compare our results to a strong baseline provided by (Wang et al., 2020b), consisting of a multilingual Transformer model trained on CoVoST 2 with multilingual ASR pretraining (ST). For the Europarl ST many-to-many baseline, we use Transformer architecture with 12- layer encoder, 6-layer decoder, and trained on all 30 directions. To provide the strongest baseline, encoder was pre-trained on LibriSpeech English ASR). XMEF-BL: Multilingual models for En-X (oneto-many) usually face more challenges from interference as they were found to underperform the bilingual counterparts (Arivazhagan et al., 2019). Therefore, we compare to applying our method (XMEF, LNA) to bilingual (BL) finetuning, i.e. finetuning on parallel data from a single language pair. Previous SOTAs: We compare to the best end-toend (E2E) model from previous literature (Wang et al., 2020b; Iranzo-Sánchez et al., 2020) on each translation direction, which is usually the bestperforming multilingual model trained with parallel data from all directions (both X-En and EnX) and also pretrained with ASR. Even though the focus of the proposed method is E2E model, we also compare to the best performing cascade approach (Cascade SOTA) which is composed of Transformer-large encoder from ASR pretraining and a multilingual MT model trained on all X-En and En-X data."
    }, {
      "heading" : "4 Results",
      "text" : ""
    }, {
      "heading" : "4.1 Parameter Efficiency",
      "text" : "First, we evaluate the transfer learning performance of finetuning the entire pretrained model as well as the proposed efficient finetuning (LNA). To separate the additional crosslingual transfer learning from multilingual finetuning, we evalute on bilingual ST (En-De and De-En in CoVoST) task. We first evaluate LNA-Minimalist (69M params), comparing to finetuning all parameters and only top\nlayers which were found effective in transfer learning in NLP tasks with pretrained BERT (Wu and Dredze, 2019; Kovaleva et al., 2019). Figure 2 show that in both low data and high data regimes, the proposed LNA-Minimalist both generalizes better (lower perplexity on dev set) and substantially improves training efficiency (only 10% of parameters to train leading to lower memory cost and faster training)."
    }, {
      "heading" : "4.2 Transfer from Pretraining",
      "text" : "To assess transfer ability from encoder pretrained on English to other (speech) input languages, we evaluate the performance of XMEF-En on CoVoST 2 De-En ST task. We investigate the role of finetuning encoder self-attention (LNA-ESA) in facilitating crosslingual transfer. We compare to baselines of finetuning the entire encoder (All), and finetuning feature extractor which are commonly used in adaptation in ASR (Rivière et al., 2020). Results are summarized in Figure 3. LNA still demonstrates improved generalization than alternative finetuning approaches, with finetuning encoder self attention (LNA-ESA) being crucial for adapting pretrained English encoder to other languages."
    }, {
      "heading" : "4.3 Zero-shot Crosslingual Transfer",
      "text" : "Next, we evaluate XMEF’s crosslingual transfer performance from multilingual finetuning. To precisely measure the transfer capability, we evaluate the zero-shot setting, i.e. finetune XMEF-En with parallel ST data from multiple languages, and evaluate on an unseen language. We study the transfer performance in source (speech) and target (text) separately. Source-side (speech) transfer. We evaluate whether the proposed approach enables positive crosslingual transfer to translate speech from unseen languages in Table 1. We finetune on labelled data for 5 to-English language pairs, and evaluate the finetuned model’s zero-shot performance when translating speech input from unseen languages (Pt). First, we found that comparing to finetuning more parameters (LNA-D, and All), LNA finetuning (LNA-E,D) not only trains more than 2× faster but also achieves better generalization both for seen and unseen languages. Especially, it attains remarkable performance as unsupervised speech translation for Portuguese-English, achieving 8.2 BLEU (compared to the supervised bilingual baseline 0.5 BLEU as is provided in Table 3, and even beats\n(+1.9 BLEU) the previous state-of-the-art for this direction which is a supervised multilingual model. Target-side (text) transfer. Table 2 shows the proposed approach also achieves zero-shot transfer capability for translating to new languages, with unsupervised translation for English-Japanese only 1.3 BLEU behind the best supervised result. Furthermore, an interesting finding is that applying LNA finetuning to decoder is crucial for zero-shot transfer to unseen languages (Ja), as finetuning the entire decoder tends to optimize the model on target languages seen during training."
    }, {
      "heading" : "4.4 Multilingual Speech Translation",
      "text" : "We evaluate the performance of XMEF with multilingual finetuning on all 36 translation directions in CoVoST 2, respectively all 21 languages into English (many-to-one) and from English into 15 languages (one-to-many).\nMany to one. Consistent with the observation of source-side crosslingual transfer in Sec 4.1, XMEFEn perform very well on Romance, Germanic and Slavic language families in both high-resource ( ≥ 100 hours training data) and low-resource directions (7 ∼ 44 hours training data) as is summarized in Table 3, and even surpassing the best cascade results on 8 languages. Our multilingual model also improves distant (from English) and\nextremely low resource (mostly ≤ 5 hours training data) languages as is shown in second panel of Table 3. For crosslingual adaptation from XMEF-En to speech input of other languages, LNA-E,D (only finetune 21.5% of pretrained parameters) outperforms finetuning the entire model (Finetune All) by 0.7 BLEU (averaged across 21 directions), while finetuning the entire encoder (LNA-D) brings +1.2 BLEU. Finetuning XMEF-X achieves the best average BLEU score, however, major improvement is from finetuning encoder (LNA-D).\nOne to many. Table 4 summarizes performance on translating (from English) to 15 languages where multilingual models from XMEF-En have improved previous state-of-the-art (both E2E and cascade) on all directions (+6.4 BLEU on average). The performance of applying LNA finetuning to encoder only (LNA-E) is very close to (24.2 vs. 24.5 averaged BLEU) that of finetuning the entire model (Finetune All) while has 40% less parameters to train. Applying LNA to both encoder and decoder (LNA-Min, LNA-E,D) further reduces the amount of parameters to train to only 8 ∼ 20% of all parameters in the pretrained models yet still maintain strong performance compared to strong baselines such as ASR PT with multilingual finetuning (ASR PT+Multi) as well as the best cascade models. The only two languages (Ca, Cy) it did not\nimprove with LNA finetuning of the decoder were never seen during mBART pretraining.\nJoint Training In the many to one case (Table 3), language pairs with reasonable amount speech training data (+ 18 hours) and large amount of parallel text data (+1 million sentences) (“FrEn”, “De-En”, “Es-En”, “It-En”, “Ru-En” and “FaEn”), outperform the corresponding single task trained models and achieve state-of-art results . However, if the amount of speech data is too small (10 hours or less), joint training is ineffective and may even make the performance worse. In one to many case (“En-X”), where there are 364 hours English audio data for training, joint training improves the results further by another 0.6 BLEU (Table 4)."
    }, {
      "heading" : "4.5 Zero-shot Many-to-Many Speech to Text Translation",
      "text" : "Finally, we evaluate how the proposed approach performs in zero-shot multilingual translation (translating X→ Y after training on X→ En and En → Y. We apply LNA-D multilingual finetuning using En-X and X-En training data only from the Europarl corpus. Table 5 reports both the supervised performance on to- and from-English directions and zero-shot performance translating between non-Engligh languages without training on their parallel data. We compare to the strong baseline of a many-to-many multilingual model trained from scratch using all parallel data from nonEnglish directions as well as English-centric directions. Our approach improves both to- and fromEnglish directions (+6.8 BLEU and +8.2 BLEU on\naverge respectively) and our zero-shot results also beats (+5.6 BLEU) the supervised many-to-many model on 28 pair-wise (except for It-Pt and Pt-Es) translation directions."
    }, {
      "heading" : "5 Ablation Studies",
      "text" : "Ablation on LNA Finetuning. In Table 6 we analyze how individual components of LNA contribute to the generalization performance and training efficiency. Specifically, we examine the key components of LNA-Minimalist (LNA-Min) finetuning. We find finetuning LayerNorm parameter (far less compared to the amount of multi-head attention parameters) is important for training stability when finetuning pretrained models without which (-LN) training diverges. Finetuning the encoder attention (EA) parameters is important for adapting the pretrained text decoder for ST task. For adapting to a single language pair downstream ST task (English-German), we find finetuning self attention (+SA) parameters in the decoder did not bring further improvement while significantly increasing the amount of parameters to train.\nAblation on Length Adaptor. We study whether the performance is sensitive to downsampling ratio in the adaptor module. We conduct the experiments on CoVoST 2 many-to-one experiments, and report perplexity on dev set of three directions with diverse input languages: German-English (De-En), Chinese-English (Zh-En) and Estonian-English (Et-En). Table 7 shows our approach is not sensitive to common downsampling ratios (4 or 8) while extreme downsampling (27) hurts performance."
    }, {
      "heading" : "6 Related Work",
      "text" : "Speech Translation. Sequence-to-sequence based speech translation has shown very good potential over the traditional cascaded system (Berard et al., 2016; Goldwater et al., 2017; Weiss et al., 2017) with end-to-end approaches surpassing cascaded system for the first time at IWSLT (Ansari et al., 2020) in a shared task setting. However, previous work also indicates that its success heavily relies on large amounts of labelled training data, which is difficult to acquire. In order\n( shaded ) were used in multilingual finetuning while the rest are results of zero-shot translation. Bold are where our model (En-only and zero-shot for the rest) outperforms a supervised many-to-many model. * means that our zero-shot model also beats the supervised cascade model in (Iranzo-Sánchez et al., 2020).\nto mitigate the data scarcity issue, recent research work focuses on multi-task learning (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Bahar et al., 2019; Wang et al., 2020c,d; Indurthi et al., 2020; Di Gangi et al., 2019), pretraining different components of the model (Bérard et al., 2018; Bansal et al., 2019), transfer learning (Gaido et al., 2020; Liu et al., 2019) and generating synthetic data (Jia et al., 2018; Pino et al., 2020).\nPretraining and Finetuning. Our work is motivated by the recent success of self-supervised learning for NLP and speech processing applications (Radford, 2018; Devlin et al., 2019; Clark et al., 2019; Lewis et al., 2019; Lample and Con-\nneau, 2019; Dong et al., 2019; Liu et al., 2020; Tang et al., 2020; Rivière et al., 2020; Kawakami et al., 2020; Chung and Glass, 2020; Baevski et al., 2020), which has achieved state-of-the-art results when finetuning on downstream tasks in NLP (Liu et al., 2020; Devlin et al., 2019; Raffel et al., 2020; Tang et al., 2020). Our work attempts to leverage pretrained components from different modalities (text and speech) to perform the ST task. How to efficiently adapt large pretrained models has gained growing interest. (Houlsby et al., 2019) and (Pfeiffer et al., 2020) represent the stream of work which adds additional “adaptor modules” to achieve fast adaptation to downstream tasks. Another category of solutions focus selective finetuning (only subset of parameters) suitable for downstream tasks. Our work belongs to the second category of efficient finetuning without adding extra parameters (e.g. adaptor modules). Empirical studies shows that finetuning the final layers of BERT account for most of the quality gains on downstream tasks (Kovaleva et al., 2019; Lee et al., 2019). Finetuning LayerNorm parameters was also found effective for adapting pretrained BART or mBART for machine translation (Stickland et al., 2020). A general approach is to automatically learn which layers/parameters from a large-pretrained model to finetune and freeze (Guo et al., 2019), which we found is an exciting direction for future work."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We proposed a simple and effective approach to leverage pretrained single-modality models (such as wav2vec 2.0, mBART) to perform speech-totext translation. On two large-scale multilingual speech translation benchmarks, our approach advances the state-of-the-art (+6.6 BLEU on average for 36 translation directions in CoVoST 2, and +5.6 BLEU for 28 translation directions in Europarl).\nWe provide an efficient finetuning strategy which is not only data- and parameter-efficient, but also demonstrates crosslingual transfer ability by only finetuning 10 ∼ 50% of the parameters of large pretrained models."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Description of Pretrained Models wav2vec 2.0 is a simple and powerful framework to learn high quality speech representation from unlabelled audio data. It mainly consists of two components: feature encoder and context encoder. The feature encoder, which is built from temporal convolution layers, takes raw audio signal O as input and generates latent speech representation Z = [z1, · · ·, zT ]. They are fed to the transformer based context encoder to generate context representations C = [c1, · · ·, cT ] with sequence level information. During pre-training, the model is optimized with a contrastive task to distinguish true latent from distractors. The input to the context encoder is with span masked. The latent speech representation Z is discretized to Q = [q1, · · ·, qT ] and used as targets for the frames in the masked span. mBART is a sequence-to-sequence generative pretraining scheme, specifically a denoising autoencoder (DAE) to predict the original text x given g(x) where g is a noising function that corrupts text such as random span masking and order permutation (Liu et al., 2020). The model is trained with monolingual data of N languages: D = {D1, ...,DN} where each Di is a collection of documents in language i. The pretraining objective optimizes Lθ:\nLθ = ∑ Di∈D ∑ x∈Di logP (x|g(x); θ) , (1)\nwhere x is an instance in language i and the distribution P is parameterized by the sequence-tosequence model.\nA.2 Data The CoVoST 2 dataset (Wang et al., 2020b) is a large-scale multilingual ST corpus which covers translations from English into 15 languages— Arabic, Catalan, Welsh, German, Estonian, Persian, Indonesian, Japanese, Latvian, Mongolian, Slovenian, Swedish, Tamil, Turkish, Chinese, and translations from 21 languages into English, including Spanish, French, Italian, Dutch, Portuguese, Russian in addition to the 15 target languages. It has total 2,880 hours of speech from 78K speakers. The data could be downloaded from https: //github.com/facebookresearch/covost.\nWe provide the list of languages used in our experiments and their ISO codes.\nA.3 Implementation Details\nPreprocessing. When using wav2vec 2.0 encoder, we use 16-bit 16kHz mono-channel audios as inputs. When using a traditional speech recognition (ASR) encoder, we extract 80-channel log mel-filter bank features (25ms window size and 10ms shift) with utterance-level cepstral mean and variance normalization applied. We remove training samples with more than 3,000 frames for GPU memory efficiency. For preprocessing the target (text) data, we use the same vocabulary as is used in the pretrained mBART model.\nPretrained models. We use the opensourced models from wav2vec 2.0 and mBART50 pretrained with multilingual parallel text data. These models can be downloaded from https://github.com/pytorch/ fairseq/tree/master/examples/wav2vec and https://github.com/pytorch/fairseq/ tree/master/examples/multilingual. For XMEF-En, we use the 960-hour Wav2Vec 2.0 Large (LV-60) model. For XMEF-X, we use the 56K-hour XLSR-53 Large model. For decoder,\nwe use the pretrained “mMBART 50 finetuned many-to-one” model for many-to-one experiments and “mMBART 50 finetuned one-to-many” for one-to-many experiments.\nTraining. We implement all our experiments using fairseq S2T (Ott et al., 2019; Wang et al., 2020a). Our experiments are run with 32 Nvidia V100 GPUs (32GB) with batch size of 256k tokens. We use FP16 training implemented in fairseq (Ott et al., 2019). We apply the same regularization as the baseline models such as label smoothing 0.3, attention dropout probablity 0.3. We choose learning rate among [1e− 5, 5e− 5, 1e− 4] based on validation accuracy (measured on dev set). For multilingual wav2vec 2.0, we enable normalization flag to be consistent with pretraining. We did not apply any temperature adjustment in sampling language pairs in training, but simply train on the empirical distribution of training data volume.\nEvaluation. We use the best checkpoint (without checkpoint averaging) according to validation loss and a beam size of 5 for decoding. We report case-sensitive detokenized BLEU using sacreBLEU (Post, 2018), except for Japanese and Chinese translations (no word segmentation) where we report character-level BLEU."
    } ],
    "references" : [ {
      "title" : "Tied multitask learning for neural speech translation",
      "author" : [ "Antonios Anastasopoulos", "David Chiang." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Anastasopoulos and Chiang.,? 2018",
      "shortCiteRegEx" : "Anastasopoulos and Chiang.",
      "year" : 2018
    }, {
      "title" : "FINDINGS OF THE IWSLT 2020 EVALUATION CAMPAIGN",
      "author" : [ "abeth Salesky", "Xing Shi", "Sebastian Stüker", "Marco Turchi", "Alexander Waibel", "Changhan Wang" ],
      "venue" : "In Proceedings of the 17th International Conference on Spoken Language Trans-",
      "citeRegEx" : "Salesky et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Salesky et al\\.",
      "year" : 2020
    }, {
      "title" : "Massively multilingual neural machine translation in the wild: Findings and chal",
      "author" : [ "Naveen Arivazhagan", "Ankur Bapna", "Orhan Firat", "Dmitry Lepikhin", "Melvin Johnson", "Maxim Krikun", "Mia Xu Chen", "Yuan Cao", "George Foster", "Colin Cherry" ],
      "venue" : null,
      "citeRegEx" : "Arivazhagan et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Arivazhagan et al\\.",
      "year" : 2019
    }, {
      "title" : "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "author" : [ "Alexei Baevski", "Henry Zhou", "Abdelrahman Mohamed", "Michael Auli" ],
      "venue" : null,
      "citeRegEx" : "Baevski et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Baevski et al\\.",
      "year" : 2020
    }, {
      "title" : "A comparative study on end-to-end speech to text translation",
      "author" : [ "Parnia Bahar", "Tobias Bieschke", "Hermann Ney." ],
      "venue" : "ASRU.",
      "citeRegEx" : "Bahar et al\\.,? 2019",
      "shortCiteRegEx" : "Bahar et al\\.",
      "year" : 2019
    }, {
      "title" : "Pretraining on high-resource speech recognition improves low-resource speech-to-text translation",
      "author" : [ "Sameer Bansal", "Herman Kamper", "Karen Livescu", "Adam Lopez", "Sharon Goldwater." ],
      "venue" : "Proceedings of the 2019 Conference of the North",
      "citeRegEx" : "Bansal et al\\.,? 2019",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2019
    }, {
      "title" : "End-toend automatic speech translation of audiobooks",
      "author" : [ "Alexandre Bérard", "Laurent Besacier", "Ali Can Kocabiyikoglu", "Olivier Pietquin." ],
      "venue" : "ICASSP.",
      "citeRegEx" : "Bérard et al\\.,? 2018",
      "shortCiteRegEx" : "Bérard et al\\.",
      "year" : 2018
    }, {
      "title" : "Listen and translate: A proof of concept for end-to-end speech-to-text translation",
      "author" : [ "Alexandre Berard", "Olivier Pietquin", "Christophe Servan", "Laurent Besacier." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Berard et al\\.,? 2016",
      "shortCiteRegEx" : "Berard et al\\.",
      "year" : 2016
    }, {
      "title" : "Improved speech representations with multi-target autoregressive predictive coding",
      "author" : [ "Yu-An Chung", "James Glass." ],
      "venue" : "ACL.",
      "citeRegEx" : "Chung and Glass.,? 2020",
      "shortCiteRegEx" : "Chung and Glass.",
      "year" : 2020
    }, {
      "title" : "Electra: Pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning" ],
      "venue" : null,
      "citeRegEx" : "Clark et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised cross-lingual representation learning for speech recognition",
      "author" : [ "Alexis Conneau", "Alexei Baevski", "Ronan Collobert", "Abdelrahman Mohamed", "Michael Auli." ],
      "venue" : "arXiv preprint arXiv:2006.13979.",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov" ],
      "venue" : null,
      "citeRegEx" : "Conneau et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "One-to-many multilingual end-to-end speech translation",
      "author" : [ "Mattia A Di Gangi", "Matteo Negri", "Marco Turchi." ],
      "venue" : "2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 585–592. IEEE.",
      "citeRegEx" : "Gangi et al\\.,? 2019",
      "shortCiteRegEx" : "Gangi et al\\.",
      "year" : 2019
    }, {
      "title" : "Unified language model pre-training for natural language understanding and generation",
      "author" : [ "Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "End-toend speech-translation with knowledge distillation: Fbk@iwslt2020",
      "author" : [ "Marco Gaido", "Mattia Antonino Di Gangi", "Matteo Negri", "Marco Turchi" ],
      "venue" : null,
      "citeRegEx" : "Gaido et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Gaido et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards speech-to-text translation without speech recognition",
      "author" : [ "S. Goldwater", "Adam Lopez", "Sameer Bansal", "H. Kamper." ],
      "venue" : "EACL.",
      "citeRegEx" : "Goldwater et al\\.,? 2017",
      "shortCiteRegEx" : "Goldwater et al\\.",
      "year" : 2017
    }, {
      "title" : "Spottune: transfer learning through adaptive finetuning",
      "author" : [ "Yunhui Guo", "Honghui Shi", "Abhishek Kumar", "Kristen Grauman", "Tajana Rosing", "Rogerio Feris." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages",
      "citeRegEx" : "Guo et al\\.,? 2019",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2019
    }, {
      "title" : "Parameter-efficient transfer learning for nlp",
      "author" : [ "Neil Houlsby", "Andrei Giurgiu", "Stanislaw Jastrzebski", "Bruna Morrone", "Quentin De Laroussilhe", "Andrea Gesmundo", "Mona Attariyan", "Sylvain Gelly." ],
      "venue" : "arXiv preprint arXiv:1902.00751.",
      "citeRegEx" : "Houlsby et al\\.,? 2019",
      "shortCiteRegEx" : "Houlsby et al\\.",
      "year" : 2019
    }, {
      "title" : "Endend speech-to-text translation with modality agnostic meta-learning",
      "author" : [ "Sathish Reddy Indurthi", "HouJeung Han", "Nikhil Kumar Lakumarapu", "Beom seok Lee", "Insoo Chung", "Sang-Ha Kim", "Chanwoo Kim." ],
      "venue" : "ICASSP.",
      "citeRegEx" : "Indurthi et al\\.,? 2020",
      "shortCiteRegEx" : "Indurthi et al\\.",
      "year" : 2020
    }, {
      "title" : "Europarl-st: A multilingual corpus for speech translation of parliamentary debates",
      "author" : [ "J. Iranzo-Sánchez", "J.A. Silvestre-Cerdà", "J. Jorge", "N. Roselló", "A. Giménez", "A. Sanchis", "J. Civera", "A. Juan." ],
      "venue" : "ICASSP 2020 - 2020 IEEE International Confer-",
      "citeRegEx" : "Iranzo.Sánchez et al\\.,? 2020",
      "shortCiteRegEx" : "Iranzo.Sánchez et al\\.",
      "year" : 2020
    }, {
      "title" : "Leveraging weakly supervised data to improve end-to-end speech-to-text translation",
      "author" : [ "Ye Jia", "Melvin Johnson", "Wolfgang Macherey", "Ron J. Weiss", "Yuan Cao", "Chung-Cheng Chiu", "Naveen Ari", "Stella Laurenzo", "Yonghui Wu." ],
      "venue" : "ICASSP, pages 7180–",
      "citeRegEx" : "Jia et al\\.,? 2018",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning robust and multilingual speech representations",
      "author" : [ "Kazuya Kawakami", "Luyu Wang", "Chris Dyer", "Phil Blunsom", "Aaron van den Oord." ],
      "venue" : "ArXiv.",
      "citeRegEx" : "Kawakami et al\\.,? 2020",
      "shortCiteRegEx" : "Kawakami et al\\.",
      "year" : 2020
    }, {
      "title" : "Revealing the dark secrets of bert",
      "author" : [ "Olga Kovaleva", "Alexey Romanov", "Anna Rogers", "Anna Rumshisky." ],
      "venue" : "arXiv preprint arXiv:1908.08593.",
      "citeRegEx" : "Kovaleva et al\\.,? 2019",
      "shortCiteRegEx" : "Kovaleva et al\\.",
      "year" : 2019
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Guillaume Lample", "Alexis Conneau." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Lample and Conneau.,? 2019",
      "shortCiteRegEx" : "Lample and Conneau.",
      "year" : 2019
    }, {
      "title" : "What would elsa do? freezing layers during transformer fine-tuning",
      "author" : [ "Jaejun Lee", "Raphael Tang", "Jimmy Lin." ],
      "venue" : "arXiv preprint arXiv:1911.03090.",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "Bart: Denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Ves Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2019
    }, {
      "title" : "Multilingual denoising pretraining for neural machine translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "X. Li", "Sergey Edunov", "Marjan Ghazvininejad", "M. Lewis", "L. Zettlemoyer." ],
      "venue" : "ArXiv, abs/2001.08210.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "End-to-end speech translation with knowledge distillation",
      "author" : [ "Yuchen Liu", "Hao Xiong", "Zhongjun He", "Jiajun Zhang", "Hua Wu", "Haifeng Wang", "Chengqing Zong" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "author" : [ "Jiasen Lu", "Dhruv Batra", "Devi Parikh", "Stefan Lee." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 13–23.",
      "citeRegEx" : "Lu et al\\.,? 2019",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2019
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of NAACL-HLT 2019: Demonstrations.",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "Mad-x: An adapter-based framework for multi-task cross-lingual transfer",
      "author" : [ "Jonas Pfeiffer", "Ivan Vulić", "Iryna Gurevych", "Sebastian Ruder." ],
      "venue" : "arXiv preprint arXiv:2005.00052.",
      "citeRegEx" : "Pfeiffer et al\\.,? 2020",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2020
    }, {
      "title" : "Self-Training for Endto-End Speech Translation",
      "author" : [ "Juan Pino", "Qiantong Xu", "Xutai Ma", "Mohammad Javad Dousti", "Yun Tang." ],
      "venue" : "INTERSPEECH.",
      "citeRegEx" : "Pino et al\\.,? 2020",
      "shortCiteRegEx" : "Pino et al\\.",
      "year" : 2020
    }, {
      "title" : "A call for clarity in reporting BLEU scores",
      "author" : [ "Matt Post." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186– 191, Belgium, Brussels. Association for Computational Linguistics.",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "A. Radford" ],
      "venue" : null,
      "citeRegEx" : "Radford.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford.",
      "year" : 2018
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised pretraining transfers well across languages",
      "author" : [ "Morgane Rivière", "Armand Joulin", "Pierre-Emmanuel Mazaré", "Emmanuel Dupoux." ],
      "venue" : "ICASSP.",
      "citeRegEx" : "Rivière et al\\.,? 2020",
      "shortCiteRegEx" : "Rivière et al\\.",
      "year" : 2020
    }, {
      "title" : "wav2vec: Unsupervised pre-training for speech recognition",
      "author" : [ "Steffen Schneider", "Alexei Baevski", "Ronan Collobert", "Michael Auli." ],
      "venue" : "arXiv preprint arXiv:1904.05862.",
      "citeRegEx" : "Schneider et al\\.,? 2019",
      "shortCiteRegEx" : "Schneider et al\\.",
      "year" : 2019
    }, {
      "title" : "Recipes for adapting pre-trained monolingual and multilingual models to machine translation",
      "author" : [ "Asa Cooper Stickland", "Xian Li", "Marjan Ghazvininejad" ],
      "venue" : null,
      "citeRegEx" : "Stickland et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Stickland et al\\.",
      "year" : 2020
    }, {
      "title" : "Vl-bert: Pretraining of generic visual-linguistic representations",
      "author" : [ "Weijie Su", "Xizhou Zhu", "Yue Cao", "Bin Li", "Lewei Lu", "Furu Wei", "Jifeng Dai." ],
      "venue" : "arXiv preprint arXiv:1908.08530.",
      "citeRegEx" : "Su et al\\.,? 2019",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2019
    }, {
      "title" : "Videobert: A joint model for video and language representation learning",
      "author" : [ "Chen Sun", "Austin Myers", "Carl Vondrick", "Kevin Murphy", "Cordelia Schmid." ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision, pages 7464–7473.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Multilingual translation with extensible multilingual pretraining and finetuning",
      "author" : [ "Y. Tang", "C. Tran", "X. Li", "P. Chen", "Naman Goyal", "Vishrav Chaudhary", "Jiatao Gu", "A. Fan." ],
      "venue" : "ArXiv, abs/2008.00401.",
      "citeRegEx" : "Tang et al\\.,? 2020",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2020
    }, {
      "title" : "A general multi-task learning framework to leverage text data for speech to text tasks",
      "author" : [ "Yun Tang", "J. Pino", "Changhan Wang", "Xutai Ma", "Dmitriy Genzel." ],
      "venue" : "ICASSP.",
      "citeRegEx" : "Tang et al\\.,? 2021a",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2021
    }, {
      "title" : "Improving speech translation by understanding and learning from the auxiliary text translation task",
      "author" : [ "Yun Tang", "Juan Pino", "Xian Li", "Changhan Wang", "Dmitriy Genzel." ],
      "venue" : "ACL.",
      "citeRegEx" : "Tang et al\\.,? 2021b",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2021
    }, {
      "title" : "fairseq s2t: Fast speech-to-text modeling with fairseq",
      "author" : [ "Changhan Wang", "Yun Tang", "Xutai Ma", "Anne Wu", "Dmytro Okhonko", "Juan Pino." ],
      "venue" : "Proceedings of the 2020 Conference of the Asian Chapter of the Association for Computational Linguistics",
      "citeRegEx" : "Wang et al\\.,? 2020a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Covost 2 and massively multilingual speech-to-text translation",
      "author" : [ "Changhan Wang", "Anne Wu", "Juan Pino." ],
      "venue" : "arXiv e-prints, pages arXiv–2007.",
      "citeRegEx" : "Wang et al\\.,? 2020b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Bridging the gap between pretraining and fine-tuning for end-to-end speech translation",
      "author" : [ "Chengyi Wang", "Yu Wu", "Shujie Liu", "Zhenglu Yang", "Ming Zhou" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Curriculum pre-training for end-to-end speech translation",
      "author" : [ "Chengyi Wang", "Yunzhao Wu", "Shujie Liu", "Ming Zhou", "Zhenglu Yang." ],
      "venue" : "ACL.",
      "citeRegEx" : "Wang et al\\.,? 2020d",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Sequence-tosequence models can directly translate foreign speech",
      "author" : [ "Ron J. Weiss", "Jan Chorowski", "Navdeep Jaitly", "Yonghui Wu", "Zhifeng Chen." ],
      "venue" : "INTERSPEECH.",
      "citeRegEx" : "Weiss et al\\.,? 2017",
      "shortCiteRegEx" : "Weiss et al\\.",
      "year" : 2017
    }, {
      "title" : "Beto, bentz, becas: The surprising cross-lingual effectiveness of bert",
      "author" : [ "Shijie Wu", "Mark Dredze." ],
      "venue" : "arXiv preprint arXiv:1904.09077.",
      "citeRegEx" : "Wu and Dredze.,? 2019",
      "shortCiteRegEx" : "Wu and Dredze.",
      "year" : 2019
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems, pages 5753–5763.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 45,
      "context" : "This sets a new state-ofthe-art for 36 translation directions (and surpassing cascaded ST for 30 of them) on the large-scale multilingual ST benchmark CoVoST 2 (Wang et al., 2020b) (+6.",
      "startOffset" : 160,
      "endOffset" : 180
    }, {
      "referenceID" : 12,
      "context" : "nificant performance improvement in text understanding and generation tasks (Devlin et al., 2019; Liu et al., 2020; Conneau et al., 2019; Radford, 2018).",
      "startOffset" : 76,
      "endOffset" : 152
    }, {
      "referenceID" : 27,
      "context" : "nificant performance improvement in text understanding and generation tasks (Devlin et al., 2019; Liu et al., 2020; Conneau et al., 2019; Radford, 2018).",
      "startOffset" : 76,
      "endOffset" : 152
    }, {
      "referenceID" : 11,
      "context" : "nificant performance improvement in text understanding and generation tasks (Devlin et al., 2019; Liu et al., 2020; Conneau et al., 2019; Radford, 2018).",
      "startOffset" : 76,
      "endOffset" : 152
    }, {
      "referenceID" : 34,
      "context" : "nificant performance improvement in text understanding and generation tasks (Devlin et al., 2019; Liu et al., 2020; Conneau et al., 2019; Radford, 2018).",
      "startOffset" : 76,
      "endOffset" : 152
    }, {
      "referenceID" : 37,
      "context" : "Lately, such text pretraining and finetuning paradigms have been extended to other modalities: audio (Schneider et al., 2019; Baevski et al., 2020), images (Su et al.",
      "startOffset" : 101,
      "endOffset" : 147
    }, {
      "referenceID" : 3,
      "context" : "Lately, such text pretraining and finetuning paradigms have been extended to other modalities: audio (Schneider et al., 2019; Baevski et al., 2020), images (Su et al.",
      "startOffset" : 101,
      "endOffset" : 147
    }, {
      "referenceID" : 39,
      "context" : ", 2020), images (Su et al., 2019; Lu et al., 2019), and video (Sun et al.",
      "startOffset" : 16,
      "endOffset" : 50
    }, {
      "referenceID" : 29,
      "context" : ", 2020), images (Su et al., 2019; Lu et al., 2019), and video (Sun et al.",
      "startOffset" : 16,
      "endOffset" : 50
    }, {
      "referenceID" : 3,
      "context" : "• Using a pretrained audio encoder (wav2vec (Baevski et al., 2020)) and multilingual text decoder (mBART (Liu et al.",
      "startOffset" : 44,
      "endOffset" : 66
    }, {
      "referenceID" : 27,
      "context" : ", 2020)) and multilingual text decoder (mBART (Liu et al., 2020)), this approach sets a new state-of-the-art (SOTA)",
      "startOffset" : 46,
      "endOffset" : 64
    }, {
      "referenceID" : 45,
      "context" : "On CoVoST 2 (Wang et al., 2020b), we pushed the SOTA for end-to-end approach for all 21 X-En directions(+6.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 3,
      "context" : "0 (Baevski et al., 2020) as encoder for acoustic modeling, a pretrained multilingual BART (mBART) (Liu et al.",
      "startOffset" : 2,
      "endOffset" : 24
    }, {
      "referenceID" : 27,
      "context" : ", 2020) as encoder for acoustic modeling, a pretrained multilingual BART (mBART) (Liu et al., 2020) as decoder for language modeling.",
      "startOffset" : 81,
      "endOffset" : 99
    }, {
      "referenceID" : 38,
      "context" : "The importance of finetuning LayerNorm has been observed in multilingual (text-only) translation (Stickland et al., 2020).",
      "startOffset" : 97,
      "endOffset" : 121
    }, {
      "referenceID" : 20,
      "context" : "Europarl ST (Iranzo-Sánchez et al., 2020) has both English-centric as well as non-English di-",
      "startOffset" : 12,
      "endOffset" : 41
    }, {
      "referenceID" : 10,
      "context" : "0 (XMEF-X), which was pretrained on raw audio from 53 languages (Conneau et al., 2020).",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 41,
      "context" : "We initialize the decoder with opensourced2 mBART50 models and the same vocabulary (Tang et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 102
    }, {
      "referenceID" : 41,
      "context" : "We also experimented with adding additional bitext used in ML50 (Tang et al., 2020) as training data for the MT task.",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 5,
      "context" : "ASRPT+Multi: Pretraining encoder on ASR task was shown to be an effective method to improve speech translation and accelerates convergence (Bansal et al., 2019).",
      "startOffset" : 139,
      "endOffset" : 160
    }, {
      "referenceID" : 45,
      "context" : "strong baseline provided by (Wang et al., 2020b), consisting of a multilingual Transformer model trained on CoVoST 2 with multilingual ASR pretraining (ST).",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 2,
      "context" : "to-many) usually face more challenges from interference as they were found to underperform the bilingual counterparts (Arivazhagan et al., 2019).",
      "startOffset" : 118,
      "endOffset" : 144
    }, {
      "referenceID" : 45,
      "context" : "Previous SOTAs: We compare to the best end-toend (E2E) model from previous literature (Wang et al., 2020b; Iranzo-Sánchez et al., 2020) on each",
      "startOffset" : 86,
      "endOffset" : 135
    }, {
      "referenceID" : 20,
      "context" : "Previous SOTAs: We compare to the best end-toend (E2E) model from previous literature (Wang et al., 2020b; Iranzo-Sánchez et al., 2020) on each",
      "startOffset" : 86,
      "endOffset" : 135
    }, {
      "referenceID" : 49,
      "context" : "ing in NLP tasks with pretrained BERT (Wu and Dredze, 2019; Kovaleva et al., 2019).",
      "startOffset" : 38,
      "endOffset" : 82
    }, {
      "referenceID" : 23,
      "context" : "ing in NLP tasks with pretrained BERT (Wu and Dredze, 2019; Kovaleva et al., 2019).",
      "startOffset" : 38,
      "endOffset" : 82
    }, {
      "referenceID" : 36,
      "context" : "baselines of finetuning the entire encoder (All), and finetuning feature extractor which are commonly used in adaptation in ASR (Rivière et al., 2020).",
      "startOffset" : 128,
      "endOffset" : 150
    }, {
      "referenceID" : 45,
      "context" : "Previous E2E SOTA is the best-performing end-to-end multilingual (with ASR pretraining) model from (Wang et al., 2020b).",
      "startOffset" : 99,
      "endOffset" : 119
    }, {
      "referenceID" : 7,
      "context" : "Sequence-to-sequence based speech translation has shown very good potential over the traditional cascaded system (Berard et al., 2016; Goldwater et al., 2017; Weiss et al., 2017) with end-to-end approaches surpassing cascaded system for the first time at IWSLT (Ansari et al.",
      "startOffset" : 113,
      "endOffset" : 178
    }, {
      "referenceID" : 16,
      "context" : "Sequence-to-sequence based speech translation has shown very good potential over the traditional cascaded system (Berard et al., 2016; Goldwater et al., 2017; Weiss et al., 2017) with end-to-end approaches surpassing cascaded system for the first time at IWSLT (Ansari et al.",
      "startOffset" : 113,
      "endOffset" : 178
    }, {
      "referenceID" : 48,
      "context" : "Sequence-to-sequence based speech translation has shown very good potential over the traditional cascaded system (Berard et al., 2016; Goldwater et al., 2017; Weiss et al., 2017) with end-to-end approaches surpassing cascaded system for the first time at IWSLT (Ansari et al.",
      "startOffset" : 113,
      "endOffset" : 178
    }, {
      "referenceID" : 20,
      "context" : "* means that our zero-shot model also beats the supervised cascade model in (Iranzo-Sánchez et al., 2020).",
      "startOffset" : 76,
      "endOffset" : 105
    }, {
      "referenceID" : 48,
      "context" : "to mitigate the data scarcity issue, recent research work focuses on multi-task learning (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Bahar et al., 2019; Wang et al., 2020c,d; Indurthi et al., 2020; Di Gangi et al., 2019), pretraining different components of the model (Bérard et al.",
      "startOffset" : 89,
      "endOffset" : 230
    }, {
      "referenceID" : 0,
      "context" : "to mitigate the data scarcity issue, recent research work focuses on multi-task learning (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Bahar et al., 2019; Wang et al., 2020c,d; Indurthi et al., 2020; Di Gangi et al., 2019), pretraining different components of the model (Bérard et al.",
      "startOffset" : 89,
      "endOffset" : 230
    }, {
      "referenceID" : 4,
      "context" : "to mitigate the data scarcity issue, recent research work focuses on multi-task learning (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Bahar et al., 2019; Wang et al., 2020c,d; Indurthi et al., 2020; Di Gangi et al., 2019), pretraining different components of the model (Bérard et al.",
      "startOffset" : 89,
      "endOffset" : 230
    }, {
      "referenceID" : 19,
      "context" : "to mitigate the data scarcity issue, recent research work focuses on multi-task learning (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Bahar et al., 2019; Wang et al., 2020c,d; Indurthi et al., 2020; Di Gangi et al., 2019), pretraining different components of the model (Bérard et al.",
      "startOffset" : 89,
      "endOffset" : 230
    }, {
      "referenceID" : 6,
      "context" : ", 2019), pretraining different components of the model (Bérard et al., 2018; Bansal et al., 2019), transfer learning (Gaido et al.",
      "startOffset" : 55,
      "endOffset" : 97
    }, {
      "referenceID" : 5,
      "context" : ", 2019), pretraining different components of the model (Bérard et al., 2018; Bansal et al., 2019), transfer learning (Gaido et al.",
      "startOffset" : 55,
      "endOffset" : 97
    }, {
      "referenceID" : 21,
      "context" : ", 2019) and generating synthetic data (Jia et al., 2018; Pino et al., 2020).",
      "startOffset" : 38,
      "endOffset" : 75
    }, {
      "referenceID" : 32,
      "context" : ", 2019) and generating synthetic data (Jia et al., 2018; Pino et al., 2020).",
      "startOffset" : 38,
      "endOffset" : 75
    }, {
      "referenceID" : 27,
      "context" : "2020), which has achieved state-of-the-art results when finetuning on downstream tasks in NLP (Liu et al., 2020; Devlin et al., 2019; Raffel et al., 2020; Tang et al., 2020).",
      "startOffset" : 94,
      "endOffset" : 173
    }, {
      "referenceID" : 12,
      "context" : "2020), which has achieved state-of-the-art results when finetuning on downstream tasks in NLP (Liu et al., 2020; Devlin et al., 2019; Raffel et al., 2020; Tang et al., 2020).",
      "startOffset" : 94,
      "endOffset" : 173
    }, {
      "referenceID" : 35,
      "context" : "2020), which has achieved state-of-the-art results when finetuning on downstream tasks in NLP (Liu et al., 2020; Devlin et al., 2019; Raffel et al., 2020; Tang et al., 2020).",
      "startOffset" : 94,
      "endOffset" : 173
    }, {
      "referenceID" : 41,
      "context" : "2020), which has achieved state-of-the-art results when finetuning on downstream tasks in NLP (Liu et al., 2020; Devlin et al., 2019; Raffel et al., 2020; Tang et al., 2020).",
      "startOffset" : 94,
      "endOffset" : 173
    }, {
      "referenceID" : 31,
      "context" : ", 2019) and (Pfeiffer et al., 2020) represent the stream of work which adds additional “adaptor modules” to achieve fast",
      "startOffset" : 12,
      "endOffset" : 35
    }, {
      "referenceID" : 23,
      "context" : "Empirical studies shows that finetuning the final layers of BERT account for most of the quality gains on downstream tasks (Kovaleva et al., 2019; Lee et al., 2019).",
      "startOffset" : 123,
      "endOffset" : 164
    }, {
      "referenceID" : 25,
      "context" : "Empirical studies shows that finetuning the final layers of BERT account for most of the quality gains on downstream tasks (Kovaleva et al., 2019; Lee et al., 2019).",
      "startOffset" : 123,
      "endOffset" : 164
    }, {
      "referenceID" : 38,
      "context" : "Finetuning LayerNorm parameters was also found effective for adapting pretrained BART or mBART for machine translation (Stickland et al., 2020).",
      "startOffset" : 119,
      "endOffset" : 143
    }, {
      "referenceID" : 17,
      "context" : "A general approach is to automatically learn which layers/parameters from a large-pretrained model to finetune and freeze (Guo et al., 2019), which we found is an exciting direction for future work.",
      "startOffset" : 122,
      "endOffset" : 140
    } ],
    "year" : 2021,
    "abstractText" : "We present a simple yet effective approach to build multilingual speech-to-text (ST) translation through efficient transfer learning from a pretrained speech encoder and text decoder. Our key finding is that a minimalistic LNA (LayerNorm and Attention) finetuning can achieve zero-shot crosslingual and crossmodality transfer ability by only finetuning 10 ∼ 50% of the pretrained parameters. This effectively leverages large pretrained models at low training cost such as wav2vec 2.0 for acoustic modeling, and mBART for multilingual text generation. This sets a new state-ofthe-art for 36 translation directions (and surpassing cascaded ST for 30 of them) on the large-scale multilingual ST benchmark CoVoST 2 (Wang et al., 2020b) (+6.4 BLEU on average for En-X directions and +6.7 BLEU for X-En directions). Our approach demonstrates strong zero-shot performance in a many-to-many multilingual model (+5.6 BLEU on average across 28 directions), making it an appealing approach for attaining highquality speech translation with improved parameter and data efficiency.",
    "creator" : "LaTeX with hyperref"
  }
}