{
  "name" : "2021.acl-long.230.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "EnsLM: Ensemble Language Model for Data Diversity by Semantic Clustering",
    "authors" : [ "Zhibin Duan", "Hao Zhang", "Chaojie Wang", "Zhengjue Wang", "Bo Chen", "Mingyuan Zhou" ],
    "emails" : [ "xidian}@163.com", "bchen@mail.xidian.edu.cn,", "Mingyuan.Zhou@mccombs.utexas.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2954–2967\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2954"
    }, {
      "heading" : "1 Introduction",
      "text" : "It is common knowledge in modern natural language processing (NLP) that natural language varies greatly across domains, themes, styles, genres and many other linguistic nuances (Van der Wees et al., 2015; van der Wees, 2017; Niu et al., 2017). Generally, we call such nature of language as data diversity. Many existing works (Liu et al., 2017; Cai and Wan, 2019; Hu et al., 2019) have illustrated that data diversity will affect the performance of LMs if we just train a single LM over the entire dataset, even though fine-tuning a pretrained LM (that has been pre-training on a very large corpus) such as Bert (Devlin et al., 2019) on current task (Aharoni and Goldberg, 2020).\n* Equal contribution. † Corresponding author.\nThe domain diversity in dataset is a very common type of data diversity. In some cases, if we can obtain a well-defined domain label for each sample, some works (Jiang et al., 2020; Du et al., 2020; Wright and Augenstein, 2020) try to consider the multi-domain property of data in developing the LMs. However, these pre-defined domain labels are not always accurate or even available (Aharoni and Goldberg, 2020), especially for the wild datasets, in which data come from different sources, such as internet news, product reviews, and daily conversation. To this end, we hope to develop a LM that can explore the diversity from data automatically.\nData selection is a commonly used strategy to handle diversity in data (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Silva et al., 2018; Aharoni and Goldberg, 2020). This kind of method is developed from an assumption that samples belonging to the same cluster should own similar characteristics. According to the clustering assignment, models can select suitable data for training a LM for each cluster separately. Although, to some extend, data selection is an efficient strategy to alleviate the problem of data diversity, it may bring two disadvantages as follows. Firstly, the process of data selection is independent of the LM learning. In other words, the gradient signal generated by LM’s training loss can not affect the\ndata selection. Secondly, data selection only tells the hard cluster belongings of samples, ignoring a fact that some samples may belong to more than one clusters with soft (weighted) assignment.\nInspired by their works and to move beyond, in this paper, we find the semantics learned by topic modeling (Blei et al., 2003; Srivastava and Sutton, 2017) can infer sample clusters to a certain extent via K-means, but is not good enough, as shown in Fig. 1a . To jointly consider the clustering and topic modeling for better clustering (as shown in Fig. 1b) and for joint training with the following LM, we firstly introduce an autoencoding topic model with mixture priors (mATM). For each sample in the corpus, mATM can infer a soft clustering assignment. In order to jointly consider the learning of mATM with various LMs, we employ the weight modulation methods (Cong et al., 2020; Wen et al., 2020). Specifically, as shown in Fig. 3, given a LM as backbone, for each layer (convolutional or fully-connected), we introduce some modulated parameters. Guided by clustering assignment inferred from mATM, these parameters modulate the backbone single LM to multiple LMs, corresponding to different clusters. Therefore, our proposed model can be seen as a type of ensemble learning, and hence we call it ensemble language model (EnsLM).\nOur proposed mATM and EnsLM enjoy the following distinguished properties:\n• The mATM learns the mixture-prior latent semantic space to define a soft clustering assignment for each sample.\n• Guided by clustering assignments that describe the data diversity, EnsLM learns both shared and cluster-specific knowledge by weight modulations.\n• Joint training of mATM and EnsLM improves the performance of both on many NLP tasks."
    }, {
      "heading" : "2 Related work",
      "text" : "For NLP, topic modeling (TM) (Blei et al., 2003; Zhou et al., 2012) and LMs are two common regimes with their own advantages. TM can discover the interpretable global semantics that are topics, while with pre-training on large corpus, LMs recently achieve the SOTA performance on many NLP tasks with more focuses on local dependencies. Therefore, some works consider to\ncombine them to obtain benefits from both. Dieng et al. (2016) and Wang et al. (2020) incorporate the TM with RNN-based model to capture the longrange dependencies. To move beyond single-layer TM for RNNs, Guo et al. (2020) propose the recurrent hierarchical topic-guided RNN with the help of multi-layer TM (Zhou et al., 2015; Zhang et al., 2018). To extract explicit document semantics for summarization, Wang et al. (2020) propose three different modules to plug knowledge from TM into Transformer-based LMs (Vaswani et al., 2017; Devlin et al., 2018). Our work can be seen as a parallel work to combine their advantages together but focuses on dealing with data diversity in NLP without the ground-truth information such as domain labels. Meanwhile, our work can be applied for different LMs including CNNs, RNNs, and Transformer-based models."
    }, {
      "heading" : "3 Autoencoding topic model with mixture prior",
      "text" : "We firstly describe one of the most popular topic models, latent Dirichlet allocation (LDA) (Blei et al., 2003), and its autoencoding inference (Srivastava and Sutton, 2017). Inspired by them, in order to jointly consider topic learning and sample clustering, we propose the autoencoding topic model with mixture prior (mATM)."
    }, {
      "heading" : "3.1 LDA with autoencoding inference",
      "text" : "For a document containing D words as w = {wd}Dd=1, given K topics Φ = [φ1, · · · ,φK ] where φk is a probability distribution over the vocabulary, LDA defines the generative process of w in Algorithm 1, where θ ∈ RK+ is the topic proportion withα as the prior parameter. After collapsing\nAlgorithm 1 Generative process of LDA\nfor each document w do Draw topic proportion θ ∼ Dirichlet(α) for each word at position d do\nSample a topic id ∼ Multinomial(1,θ) Sample a word wd ∼ Multinomial(1,φid)\nid, given θ and Φ, we can represent the conditional likelihood of wd as\nwd|Φ,θ ∼ Multinomial(1,Φθ). (1)\nGiven Φ, a popular approximation for efficient inference of LDA is mean-field variational inference, which tries to maximize the evidence lower\nbound (ELBO) of marginal data log likelihood as\nELBO = Eq(θ)[log p(w|θ,Φ)]−KL[q(θ)||p(θ)], (2) where q(θ) is the variational posterior. In particular, Srivastava and Sutton (2017) propose the autoencoding variational inference (AEVB) (Kingma and Welling, 2013) for LDA by using Laplace approximation (Hennig et al., 2012) for the Dirichlet prior, and building logistic-normal (LN) encoding posterior.\nAs shown in Fig. 1, we find that running clustering method such as K-means on semantic space θ can not achieve satisfactory results. For jointly considering the learning of topics and sample clustering, we propose the mATM."
    }, {
      "heading" : "3.2 Generative process of mATM",
      "text" : "Suppose the number of clusters is C, and the clustering prior parameter is π = [π1, · · · , πC ] with∑C\nc=1 πc = 1, shown in Fig. 2a, mATM defines the generative process of w in Algorithm 2. Com-\nAlgorithm 2 Generative process of mATM\nfor each document w do Draw cluster index z ∼ Categorical(π) Draw topic proportion θ ∼ Dirichlet(αz) for each word at position d do\nSample a topic id ∼ Multinomial(1,θ) Sample a word wd ∼ Multinomial(1,φid)\npared with LDA, mATM has a mixture Dirichlet prior with parameters {αc}Cc=1. In other words, mATM assumes that the θ of different documents may come from different clusters, which is the basic thought to discover the data diversity from corpus automatically."
    }, {
      "heading" : "3.3 Variational encoder of mATM",
      "text" : "In order to infer the parameters in mATM and further develop the EnsLM by mATM, we introduce AEVB for mATM, whose detailed structure is shown in Fig. 2b."
    }, {
      "heading" : "3.3.1 Laplace approximation for mixture Dirichlet prior",
      "text" : "Although Dirichlet prior of θ is important to learn interpretable topics (Wallach et al., 2009), it is difficult to handle it within AEVB since AEVB needs effective reparameterization (RT) function for distributions. Inspired by the success of the Laplace\napproximation for Dirichlet distribution, we propose the mixture LN (mLN) distribution as the approximation of mixture Dirichlet distribution.\nSpecifically, Srivastava and Sutton (2017) have proved that a Dirichlet distribution p(θ|α) can be well approximated by LN distribution as\np(θ|µ,Σ) = LN (µ,Σ), (3)\nwhere the elements in mean vector µ and diagonal covariance matrix Σ are\nµk = logαk − 1\nK K∑ i=1 logαi\nΣk = 1\nαk\n( 1− 2\nK\n) + 1\nK2 K∑ i=1 1 αi . (4)\nTo go further, for inference of mATM, we construct the mLN distribution as\np(θ|µ,Σ) = C∑ c=1 πcLN (µc,Σc)\nµck = logα c k −\n1\nK K∑ i=1 logαci\nΣck = 1\nαck\n( 1− 2\nK\n) + 1\nK2 K∑ i=1 1 αci , (5)\nwhich is used to approximate the mixture Dirichlet prior p(θ|{αc, πc}Cc=1) in mATM. Therefore, for each document, the prior of θ can be written as∏C c=1 LN (µc,Σ c)zc . In practice, we build the µc and Σc as\nµc = fWcµ(z),Σ c = fWcσ(z), (6)\nwhere z = [z1, · · · , zC ]. Next, we build variational posterior for latent variables with easy RT function."
    }, {
      "heading" : "3.3.2 Variational encoding posterior",
      "text" : "After collapsing {id}Dd=1 in mATM as (1) in LDA, given topics Φ, for document w, there are two latent variables that need to be inferred: θ and z.\nLN posterior for θ. We build the variational posterior of θ as LN distribution q(θ) = LN (µ′,Σ′) with µ′ = fWθµ(x), Σ\n′ = diag(fWθσ(x)), where diag converts a vector to a diagonal matrix, fWθµ(·) and fWθσ(·) are two encoding networks, and x is a type of representation for documentw such as original words or bag of words (Bow) vector. Morevoer, LN distribution has easy RT function as Normal distribution.\nGumbel softmax (GS) posterior for z. As categorical variable, z is difficult to build variational posterior under AEVB with accurate RT function. Instead, we employ GS distribution (Jang et al., 2016) as the variational posterior of z for efficient gradient propagation.\nSpecifically, suppose the posterior of z is Categorical(π′), after obtaining C i.i.d samples {g1, · · · , gC} drawn from Gumbel(0, 1), then z can be sampled as\nz = arg max c exp ((log(π′c) + gc)/τ)∑O o=1 exp ((log(π ′ o) + go)/τ) (7)\nwhere τ is the temperature parameter. In order to build encoder for π′, we let π′ = fWπ(θ,w). For efficient gradient propagation, rather than sampling z from arg max as (7), we obtain the variational posterior of soft assignment vector z = [z1, · · · , zC ] as q(z):\n[q(z)]c = exp ((log(π′c) + gc)/τ)∑O o=1 exp ((log(π ′ o) + go)/τ) . (8)\nBesides the benefit of efficient gradient backpropagation, the soft assignment in (8) provides clustering belonging weights. In the following EnsLM, this property is useful for some ambiguous samples that may belong to different clusters."
    }, {
      "heading" : "3.3.3 ELBO of mATM",
      "text" : "We obtain the ELBO of mATM as\nELBO = Eq(θ)q(z)[log p(w|θ,Φ, z)] −KL[q(θ)||p(θ|z)]−KL[q(z)|p(z|π)]\n(9)\nSimilarly with Srivastava and Sutton (2017), instead of sampling Φ from Dirichlet posterior in\nLDA, we parameterize it as Φ = softmax(Wt), where Wt = [w1, · · · ,wK ] and softmax is operated for each topic {wk}Kk=1 to ensure them on a probability simplex. Therefore, as shown in Fig. 2, all the parameters of mATM are Θ1 = {Wθµ,Wcµ,Wθσ,Wcσ,Wπ,Wt} that can be learned by maximizing the ELBO in (9)."
    }, {
      "heading" : "4 Ensemble language model",
      "text" : "Recently, various advanced LMs for language understanding and generation have been introduced, most of which do not consider the data diversities in the corpus. In this paper, having obtained the clustering assignment vector z from mATM, given a single LM as backbone, we propose the ensemble LM (EnsLM) via z-guided weight modulation. In other words, the EnsLM can modulate the backbone single LM to fit for different clusters."
    }, {
      "heading" : "4.1 Efficient weight modulation",
      "text" : "Although LMs have many different types, basically, all of them build on convolutional (such as in CNN (Johnson and Zhang, 2015)) or fully-connected (such as in Transformer (Vaswani et al., 2017)) operations (ignoring the bias) as\nConvolution : H2 = f(W ∗H1) Fully-connection : H′2 = f(W ′TH′1). (10)\nwhere, H1 ∈ RIx×Iy×Cin and H′1 ∈ RCin are the input features, W ∈ Rkx×ky×Cin×Cout and W′ ∈ RCin×Cout are the convolutional kernel or full-connected weights1. Suppose the number of clusters (domains) in mATM is C, given a LM as backbone, we introduce a few modulation parameters to modulate the original parameters W or W′ for different clusters. Specifically, shown in Fig. 3, for a convolutional or fully-connected layer in (10), suppose that there are two dictionaries of modulation parameters as:\nA = [α1, · · · ,αC ] ∈ RCin×C B = [β1, · · · ,βC ] ∈ RCout×C , (11)\nwhere {αc}Cc=1 ∈ RCin and {βc}Cc=1 ∈ RCout . For a documentw whose feature at current layer is H1, after archiving its domain assignment z ∈ RC×1\n1Fully-connected layer can be also seen as a convolution layer where the convolutional kernel is W′ ∈ R1×1×Cin×Cout (Ix = Iy = 1)\nfrom (8), we feed H1 into the modulated layer as\nConvolution : H2 = f((W Γ) ∗H1) Fully-connection : H′2 = f((W\n′T Γ)H′1), (12)\nwhere Γ = αβT , α = Az ∈ RCin×1, β = Bz ∈ RCout×1, and denotes matrix element-wise product (with broadcasting for convolution).\nExplanation of (12). Intuitively, W and W′ act as the backbone parameters in the original single LM, and Γ is the modulated parameters, which moves the backbone to fit different domains. If z is drawn from (7) that means z is a one-hot vector, then it denotes that α and β are chosen from the dictionaries A and B, correspondingly. If z is drawn from (8) that means z is a soft assignment vector, then it denotes that α and β are weighted summation of all elements in A and B, correspondingly. In practice, we use the soft assignment vector since i) it brings efficient gradient propagation during joint training of mATM and EnsLM, and ii) it considers the fact that there are some domain ambiguous samples in the dataset.\nIt is interesting to note that although EnsLM is developed for the problem that ground-truth priors of data diversity (such as domain label) is unavailable, it can be also used when we know the priors. For this scenario, rather than inferring the clustering assignment z from mATM via (8), we directly set z as the real one-hot assignment vector, which is illustrated in experiment in Sec. 5.2."
    }, {
      "heading" : "4.2 Joint training of mATM and EnsLM",
      "text" : "Different from some strategies such as data selection that separate the calculation of assignment and the training of LM, our proposed mATM and EnsLM can be jointly trained in one framework.\nSpecifically, given a training set containing N sample {wn}Nn=1, suppose that there is a label {yn}Nn=1 for each sample. It should be noted that labels {yn}Nn=1 can be different for different tasks, such as labels for document classification, golden summarization for abstractive summarization, or document itself for generation. As a result, the loss for joint training of mATM and EnsLM can be written as\nL = N∑ n=1 Eq(θn)q(zn)[log p(wn|θn,Φ, zn)]\n− Eq(zn)[LLM (wn,yn, zn)] −KL[q(θn)||p(θn)]−KL[q(zn)|p(zn)],\n(13)\nwhere, without loss of generality, LLM denotes the loss for LM. All learnable parameters are i) parameters of mATM: ΘmATM = {Wθµ,Wθσ,Wuµ ,Wuσ ,Wπ} and ii) parameters of LM: ΘLM . These parameters can be jointly trained by stochastic gradient descend with low-variance gradient estimation since LN and GS distributions have easy RT function."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we evaluate the effectiveness and efficiency of our proposed mATM and EnsLM on different NLP tasks including document clusters, text classification, language generation and abstractive document summarization. Our code is available at https://github.com/BoChenGroup/EnsLM"
    }, {
      "heading" : "5.1 Document clusters",
      "text" : "The basic idea of mATM and EnsLM is that mATM can automatically discover the sample clusters which describe the data diversity. Therefore, we firstly evaluate the document clustering performance of mATM.\nDatasets Following Yao et al. (2019), we consider two widely used document clustering datasets, 20News and R8 . This two datasets2 can be found in the open source code of Yao et al. (2019).\n2https://github.com/yao8839836/text gcn\n20News has 20 classes and consists of 18,846 documents with a vocabulary size of 61,188, partitioned into a training set of 11,314 documents and a test set of 7,532 ones. R8 is a subset of the Reuters 21578 dataset, which has 8 classes and was split into 5,485 training and 2,189 test documents. For these two datasets, we remove the stop words and use the 2,000 most frequent terms as the vocabulary. For all methods, we set the number of clusters as the number of classes.\nComparison models and implementation details To verify the effectiveness of mATM for clustering, three types of document clustering models are compared. i) Raw+kmeans performs K-means on raw BoW vectors, and PCA+kmeans uses PCA extract low-dimensional features and then uses K-means for clustering; ii) Train a topic model and then perform Kmeans for clustering on topic proportions, where we consider LDA+kmeans (Blei et al., 2003), AVITM+kmeans (Srivastava and Sutton, 2017), and PFA+kmeans (Zhou et al., 2012); iii) Deep neural network based clustering methods, including Deep clustering (Xie et al., 2016), and DCN (Yang et al., 2017), which jointly consider the feature extracting and clustering. Besides Raw+kmeans performing clustering on original inputs, others are on a latent feature space (For topic modeling, feature is the topic proportion). Following (Xie et al., 2016; Yang et al., 2017), the dimension of feature space equals to the number of clusters.\nResults Following Yang et al. (2017), since we know the ground-truth label and set the clustering number as the number of classes, we measure the\nclustering performance by accuracy (AC) and normalized mutual information (NMI), both of which are the higher the better. The results are shown in Table 1. Compared with the Base+kmeans, PCA+kmeans performs better since it extracts effective principal components. Benefiting from the learning of semantics for documents, the second group including three types of topic modeling outperforms PCA. Compared with the first two groups, the third group jointly considers the feature learning and clustering, thus achieving higher AC and NMI. Combined the advantages of topic modeling in extracting efficient features from documents and joint learning of feature extractor and clustering, mATM gets the SOTA performance for document clustering tasks on these two datasets.\nThe clustering results support our motivation of using mATM to discover the data diversity. In the following experiments, we evaluate the performance of both mATM and EnsLM on different language understanding and generation tasks."
    }, {
      "heading" : "5.2 Multi-domain sentiment classification",
      "text" : "Sentiment classification (positive or negative) for different products is a fundamental language understanding task in NLP. For this task, the data diversity mainly arises from different domains (products) (Blitzer et al., 2007), which brings the problem that data from different domains may have different distributions.\nDatasets To evaluate the performance of mATM and EnsLM in capturing the multi-domain property for sentiment classification, following Cai and Wan (2019), we perform experiments on the dataset released by Liu et al. (2017), which consists of product and movie reviews in 16 different domains. The data in each domain is randomly split into training set, development set and test set according to the proportion of 70%, 10%, 20%, whose statistics of the 16 datasets are listed in Appendix A.1.\nComparison models and implementation details Following (Cai and Wan, 2019), we firstly consider three base models, BiLSTM (Adhikari et al., 2019), TextCNN (Kim, 2014) and BERT (Devlin et al., 2019), which perform classification on every domains separately. Secondly, combining data from different domains together, we train the above three models named as BiLSTMmix, TextCNN-mix and DocBERT-mix. Having obtained the ground-truth domain label, the previous works regard the multi-domain problem\nas the multi-task learning (MTL) including DAMTL (Zheng et al., 2018), ASP-MTL (Liu et al., 2017),and MDAE (Cai and Wan, 2019). All these works are developed from BiLSTM model. For our proposed EnsLM, we use TextCNN, BiLSTM and DocBERT as the backbone of EnsLM. We perform experiments on two types of EnsLM: i) with ground-truth (GT) domain label, we directly set z as the one-hot assignment vector (do not infer z from mATM), which is named as BiLSTMEnsLM-GT, TextCNN-EnsLM-GT, and BERTEnsLM-GT; ii) without GT domain label, we use mATM to infer z , which is named as BiLSTMEnsLM-mATM, TextCNN-EnsLM-mATM, and BERT-EnsLM-mATM. For model using mATM, we set the number of topics as 16. More detailed settings and implementation details can be found in Appendix B.1.\nResults The results of averaged accuracy on all domains are given in Table 2, where the results except ours are obtained from Cai and Wan (2019). Comparing results on the first row, we can see that joint training models on all domains outperform separate training on each domain. Compared with BiLSTM-mix, having obtained the GT domain label, DA-MTL, ASP-MTL and MDAE (all of them are developed based on BiLSTM) consider the real domain knowledge in word embedding, feature extractor and attention layers, achieving higher accuracy. Similarly, with GT domain label, three models equipped with our proposed EnsLM performs better than their basic counterparts with a large margin. Assuming that GT domain labels are unavailable, we use mATM to infer the clustering assignment to guide the learning of EnsLM, which obtains the SOTA performance on all three basic models, even better than the models using GT domain label. We attribute it to the fact that com-\npared with the hard GT domain label, mATM infers the soft clustering assignment, which not only reflect the domain characteristic of samples but also describe the samples having confused domain characteristics. For example samples from DVD may be similar with the ones from Electronics."
    }, {
      "heading" : "5.3 Language generation",
      "text" : "Datasets In order to verify the effectiveness of our model on datasets of different lengths, we consider four publicly available corpora: APNEWS, IMDB, BNC, and COCO. Following Lau et al. (2017), we tokenize words and sentences using Stanford CoreNLP (Klein and Manning, 2003), lowercase all word tokens, and filter out word tokens that occur less than 10 times. For the topic model, we additionally exclude stopwords. All these corpora are partitioned into training, validation, and testing sets, whose summary statistics are provided in Appendix A.2.\nComparison models and implementation details We consider the following baseline models: LSTM, A standard LSTM language model (Hochreiter and Schmidhuber, 1997); Tansnsformer-XL enables learning dependency beyond a fixed length by introducing a recurrence mechanism and a novel position encoding scheme into the Transformer architecture (Dai et al., 2019); TGVAE (Wang et al., 2019), combines a variational auto-encoder based natural sequence model with a neural topic model; rGBN-RNN (Guo et al., 2020), extracts recurrent hierarchical semantic structure via a dynamic deep topic model to guide natural language generation; GPT-2 (Radford et al., 2019) is a generative pre-training of a Transformerbased LM on a diverse set of unlabeled text. For our proposed model, GPT-2-EnsLM-mATM first uses mATM to infer semantic clusters for each sample, and then introduce this diversity information to pre-trained GPT2 by efficient weight modulation naturally. In the experiments, we use the Adam optimizer (Kingma and Ba, 2014) with learning rate 10−6. The length of an input sample is limited to\n1024. We set the mini-batch size as 8, the number of training epochs as 5. The clustering number of mATM is set to 64 for the first three datasets, while 80 for COCO dataset. More detailed settings and implementation details can be found in Appendix B.2\nResults For fair comparison, we use standard language model perplexity as the evaluation metric. The results of all models on four datasets are given in Table 3, where the results of existing models are obtained from Guo et al. (2020). In the first group, Transformer-XL gets better result, which shows that the transformer-based model have better modeling capabilities. In terms of capturing the document global semantic information, the second group can improve performance significantly, which indicates that the topic model is effective in capturing document global information. Pre-training on massive data, the GPT-2 can obtains better results compared with above models. Although GPT-2 gets a good result, the GPT-2EnsLM-mATM can improve performance significantly by capturing data diversity. It illustrates that even pre-training on large scale of corpus, EnsLM can further improve the performance of pretrained LM via exploring data diversity. A similar phenomenon also appeared in the experiments conducted by Gururangan et al. (2020)\nSentence generation of EnsLM Given the learned GPT-2-EnsLM-mATM, we can sample the sentences conditioned on semantic clusters. Shown in the in Fig. 5, we select the top-3 topics to represent this cluster, and select original sentences according to the clustering results. we can see that most of the generated sentences conditioned on a semantic clusters are highly related to the given topics in terms of their semantic meanings but not necessarily in key words, indicating the LM is successfully guided by the cluster assignment. These\nobservations suggest that GPT-2-EnsLM-mATM has successfully captured syntax and global semantics simultaneously for natural language generation. Similar to Fig. 5, we also provide other semantic clusters generated sentences in Appendix C."
    }, {
      "heading" : "5.4 Abstractive summarization",
      "text" : "Datasets We evaluate the effectiveness and efficiency of proposed model on two benchmark datasets, including the CNN/DailyMail (CNN/DM) (Hermann et al., 2015) and the XSum (Narayan et al., 2018). The summary styles of these datasets varies from highlights, composed of several sentences, to very brief one sentence. See more detailed descriptions in Appendix A.3. We perform data pre-processing following Liu and Lapata (2019).\nComparison models and implementation details We consider some baseline models, including LSTM based models PTGEN and PTGEN+Cov (See et al., 2017); Transformer based models Tansformer, BertSUM (Liu and Lapata, 2019); and BertSUM+TA which combine pretrained model with topic model (Wang et al., 2020). We combine EnsLM with BertSUM on the abstractive summarization task. The clustering number of mATM is set to 64 for all datasets. Given BertSUM\ncheckpoints3 on CNN/DM and XSum provided by Liu and Lapata (2019), we further fine-tune BertSUM+EnsLM. Besides, we adopt the settings in the BertSUM. Following Liu and Lapata (2019), in the test stage, we use beam search with size 5, select the top-3 checkpoints based on their evaluation loss on the validation set, and report the averaged results on the test set. More detailed settings and implementation details can be found in Appendix B.3.\nResults ROUGE scores on CNN/DM, XSum have been exhibited in Tables 4, respectively. Focusing on the models without pre-training in the first group, Transformer achieves better performance compared with LSTM-based model, attributing to stronger sequence modeling capabilities. Further, the outperformance of BertSUM illustrates the fact that the combination of a pretrained Bert encoder and a Transformer decoder is a better choice of sequence-to-sequence structure. Despite owning the same structure as the BertSUM, the BertSUM+TA employs a topic model to capture global document segment diversity, and achieving higher scores. Different from BertSUM+TA that introduces document semantic diversity by adding topic information, BertSUM+mATM combines BertSUM with EnsLM model, result in a better performance. Compared with BertSUM+TA, the performance improvement of our model is not enough promising is because they have been incorporated the topical information into the BertSum model which considering the segment diversity and contextual information. Note that the performance of our model improves significantly compared with BertSum, which can prove the effectiveness of our model."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we first propose mATM to infer latent semantic clusters from raw text corpus, and then combine it with LM with efficient weight modulation, resulting in a more powerful EnsLM, which can be naturally extended to other LMs. In the future, we will study the effectiveness of EnsLM on other NLP tasks, such as the multi domain translation, and investigate whether EnsLM can be applied to the pre-training stage of Transformer.\n3https://github.com/nlpyang/PreSumm"
    }, {
      "heading" : "Acknowledgments",
      "text" : "Bo Chen acknowledges the support of NSFC (61771361), Shaanxi Youth Innovation Team Project, the 111 Project (No. B18039) and the Program for Oversea Talent by Chinese Central Government. We acknowledge all the anonymous reviewers for their valuable comments and suggestions."
    }, {
      "heading" : "A Dataset descriptions",
      "text" : "A.1 Multi-domain sentiment classification Dataset:\nwe perform experiments on the dataset4 released by Liu et al. (2017), which consists of product and movie reviews in 16 different domains. The data in each domain is randomly split into training set, development set and test set according to the proportion of 70%, 10%, 20%. Statistics of the 16 datasets is shown in Table. 5.\nA.2 Language Generation Datasets In experiments, we evaluate the models on four benchmark language generation datasets. They are the APNEWS, IMDB, BNC, and COCO Caption. APNEWS is a collection of Associated Press news articles from 2009 to 2016. IMDB is a set of movie reviews collected by Maas et al. (2011). BNC is the written portion of the British National Corpus (British National Corpus, 2007), which contains documents from journals, books,letters, essays, memoranda, news and other types of text. COCO Caption has 80 object categories, and there are caption to describe the scene of the image (Lin et al., 2014). All these corpora are partitioned into training, validation, and testing sets, whose summary statistics are provided in Table. 6. The AGNEWS, IMDB and BNC datasets can be found in the release code5 of ?. And for COCO dataset, we will give processed dataset in our release code.\nA.3 Abstractive Summarization Dataset In experiments, we evaluate the models on two benchmark summarization datasets. The datasets6\n4https://github.com/FrankWork/fudan mtl reviews 5https://github.com/jhlau/\ntopically-driven-language-model 6https://github.com/nlpyang/PreSumm\ncan be fround in the release code of Liu and Lapata (2019) They are the CNN/DailyMail news (CNN/DM) (Hermann et al., 2015) and XSum (Narayan et al., 2018).\nCNN/DM CNN/DM consists of news and associated sentence highlights, that is a brief overview composed of a few sentences. Following the standard training/validation/testing splits in Hermann et al. (2015) without anonymizing entities, we perform our experiments. We splits sentences using the Stanford CoreNLP toolkit7 and pre-process the dataset following Liu and Lapata (2019). .\nXSum XSum includes 226, 711 news articles, each of which is associated with a one-sentence summary. We use the standard training/validation/testing splits (204, 045/11, 332/11, 334) and follow the pre-processing in Narayan et al. (2018). To satisfy the maximum capacity of the encoder in the base model, such as 512 for BertSUM, we use truncated document as the encoder input. Statistics of summarization datasets is shown in Table. 7.\nB Implementation Details\nB.1 Multi-domain sentiment classification Models\nNote that we remove stop words to obtain the bagof-word (BOW) vector for each document, and then use the BOW vectors to infer the mATM model.\nCNN/BiLSTM-EnSLM-mATM: To reduce both computation and storage costs, we introduce a learnable key vector as W (t), which can be combined with mATM by efficient weight modulation, leading to a CNN/BiLSTM-EnSLMmATM. More specifically, we adopt 1-layer CNN/BiLSTMCNN with the channel/hidden size of 150 in CNN/BiLSTM-EnSLM-mATM equipped with 300-dimensional word embedding vecotrs. For optimization, the Adam optimizer is utilized here (Kingma and Ba, 2014) with a learning rate of 0.001. To avoid overfitting, we utilize the dropout and set its rate as 0.5. We set the size of minibatch as 50 in all experiments.\nBert-EnsLM-mATM: As a transformer-based model, the main component of Bert is query, key and value layer. And these component as MLP layer, we can combine Bert with mATM by efficient weight modulation easily. Specially, to re-\n7https://stanfordnlp.github.io/CoreNLP/\nduce the amount of new parameters, we only introduce segment diversity information to query layer. For optimization, the Adam optimizer is utilized here (Kingma and Ba, 2014) with a learning rate of 0.00001. To avoid overfitting, we utilize the dropout and set its rate as 0.3. We set the size of minibatch as 16 in all experiments.\nB.2 Language Generation Models\nFor language generation, we propose GPT-2EnsLM-mATM which combine mATM with pretrained model GPT-2. And we introduce segment diversity information to query, key and value for each layer. We use the Adam optimizer (Kingma and Ba, 2014) with learning rate 10−6. The length of an input sample is limited to 1024. We set the mini-batch size as 8, the number of training epochs as 5. The clustering number of mATM is set to 64 for the first three datasets, while 80 for COCO dataset.\nB.3 Abstractive Summarization Models:\nFor abstractive summarization, we combine BertSum with mATM, which include a pretrained encoder and a transformer decoder. Specially, we introduce segment diversity information to query, key and value for each layer. We set the hyperparameters following the original papers and their public codes, where BertSUM8 is referred to Liu and Lapata (2019). We fine-tune all models in four Nvidia GeForce RTX2080 TI GPUs. The experiments are performed with mini-batch size including 200 summary tokens with gradient accumulation every six iterations. Model checkpoints were saved and evaluated on the validation set every 1000 updates. Totally, we update the model 250, 000 times. Following Liu and Lapata (2019), we select the top3 checkpoints based on their evaluation loss on the validation set, and report the averaged results on the test set. During decoding we used beam search\n8https://github.com/nlpyang/BertSUM\nwith size 5, and tuned the α for the length penalty between 0.6 and 1 on validation set. It is worth noting that our decoder applies neither a copy nor a coverage mechanism, despite their popularity in abstractive summarization."
    }, {
      "heading" : "C More Generation Examples",
      "text" : "As shown in Fig. 5, we provide semantic clusters generated sentences by GPT-2-EnsLM-mATM on the coco corpus."
    } ],
    "references" : [ {
      "title" : "Rethinking complex neural network architectures for document classification",
      "author" : [ "Ashutosh Adhikari", "Achyudh Ram", "Raphael Tang", "Jimmy Lin." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Adhikari et al\\.,? 2019",
      "shortCiteRegEx" : "Adhikari et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised domain clusters in pretrained language models",
      "author" : [ "Roee Aharoni", "Yoav Goldberg." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7747– 7763, Online. Association for Computational Lin-",
      "citeRegEx" : "Aharoni and Goldberg.,? 2020",
      "shortCiteRegEx" : "Aharoni and Goldberg.",
      "year" : 2020
    }, {
      "title" : "Domain adaptation via pseudo in-domain data selection",
      "author" : [ "Amittai Axelrod", "Xiaodong He", "Jianfeng Gao." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 355–362.",
      "citeRegEx" : "Axelrod et al\\.,? 2011",
      "shortCiteRegEx" : "Axelrod et al\\.",
      "year" : 2011
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "David M Blei", "Andrew Y Ng", "Michael I Jordan." ],
      "venue" : "Journal of machine Learning research, 3(Jan):993–1022.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification",
      "author" : [ "John Blitzer", "Mark Dredze", "Fernando Pereira." ],
      "venue" : "Proceedings of the 45th annual meeting of the association of computational linguistics, pages 440–447.",
      "citeRegEx" : "Blitzer et al\\.,? 2007",
      "shortCiteRegEx" : "Blitzer et al\\.",
      "year" : 2007
    }, {
      "title" : "Multi-domain sentiment classification based on domain-aware embedding and attention",
      "author" : [ "Yitao Cai", "Xiaojun Wan." ],
      "venue" : "IJCAI, pages 4904–4910.",
      "citeRegEx" : "Cai and Wan.,? 2019",
      "shortCiteRegEx" : "Cai and Wan.",
      "year" : 2019
    }, {
      "title" : "Gan memory with no forgetting",
      "author" : [ "Yulai Cong", "Miaoyun Zhao", "Jianqiao Li", "Sijia Wang", "Lawrence Carin." ],
      "venue" : "arXiv preprint arXiv:2006.07543.",
      "citeRegEx" : "Cong et al\\.,? 2020",
      "shortCiteRegEx" : "Cong et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformer-XL: Attentive language models beyond a fixed-length context",
      "author" : [ "Zihang Dai", "Zhilin Yang", "Yiming Yang", "Jaime Carbonell", "Quoc Le", "Ruslan Salakhutdinov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Dai et al\\.,? 2019",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Topicrnn: A recurrent neural network with long-range semantic dependency",
      "author" : [ "Adji B Dieng", "Chong Wang", "Jianfeng Gao", "John Paisley." ],
      "venue" : "arXiv preprint arXiv:1611.01702.",
      "citeRegEx" : "Dieng et al\\.,? 2016",
      "shortCiteRegEx" : "Dieng et al\\.",
      "year" : 2016
    }, {
      "title" : "Adversarial and domain-aware bert for cross-domain sentiment analysis",
      "author" : [ "Chunning Du", "Haifeng Sun", "Jingyu Wang", "Qi Qi", "Jianxin Liao." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4019–",
      "citeRegEx" : "Du et al\\.,? 2020",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2020
    }, {
      "title" : "Adaptation data selection using neural language models: Experiments in machine translation",
      "author" : [ "Kevin Duh", "Graham Neubig", "Katsuhito Sudoh", "Hajime Tsukada." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Duh et al\\.,? 2013",
      "shortCiteRegEx" : "Duh et al\\.",
      "year" : 2013
    }, {
      "title" : "Recurrent hierarchical topic-guided rnn for language generation",
      "author" : [ "Dandan Guo", "Bo Chen", "Ruiying Lu", "Mingyuan Zhou." ],
      "venue" : "International Conference on Machine Learning, pages 3810–3821. PMLR.",
      "citeRegEx" : "Guo et al\\.,? 2020",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2020
    }, {
      "title" : "Don’t stop pretraining: Adapt language models to domains and tasks",
      "author" : [ "Suchin Gururangan", "Ana Marasović", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A. Smith." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the",
      "citeRegEx" : "Gururangan et al\\.,? 2020",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2020
    }, {
      "title" : "Kernel topic models",
      "author" : [ "Philipp Hennig", "David Stern", "Ralf Herbrich", "Thore Graepel." ],
      "venue" : "Artificial Intelligence and Statistics, pages 511–519.",
      "citeRegEx" : "Hennig et al\\.,? 2012",
      "shortCiteRegEx" : "Hennig et al\\.",
      "year" : 2012
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomáš Kočiský", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "Proceedings of the 28th International Conference on Neural Information Process-",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Domain adaptation of neural machine translation by lexicon induction",
      "author" : [ "Junjie Hu", "Mengzhou Xia", "Graham Neubig", "Jaime G Carbonell." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2989–",
      "citeRegEx" : "Hu et al\\.,? 2019",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2019
    }, {
      "title" : "Categorical reparameterization with gumbel-softmax",
      "author" : [ "Eric Jang", "Shixiang Gu", "Ben Poole." ],
      "venue" : "arXiv preprint arXiv:1611.01144.",
      "citeRegEx" : "Jang et al\\.,? 2016",
      "shortCiteRegEx" : "Jang et al\\.",
      "year" : 2016
    }, {
      "title" : "Multi-domain neural machine translation with word-level adaptive layer-wise domain mixing",
      "author" : [ "Haoming Jiang", "Chen Liang", "Chong Wang", "Tuo Zhao." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Jiang et al\\.,? 2020",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Effective use of word order for text categorization with convolutional neural networks",
      "author" : [ "Rie Johnson", "Tong Zhang." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Johnson and Zhang.,? 2015",
      "shortCiteRegEx" : "Johnson and Zhang.",
      "year" : 2015
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "arXiv preprint arXiv:1408.5882.",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Autoencoding variational bayes",
      "author" : [ "Diederik P Kingma", "Max Welling." ],
      "venue" : "arXiv preprint arXiv:1312.6114.",
      "citeRegEx" : "Kingma and Welling.,? 2013",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2013
    }, {
      "title" : "Accurate unlexicalized parsing",
      "author" : [ "Dan Klein", "Christopher D Manning." ],
      "venue" : "Proceedings of the 41st annual meeting of the association for computational linguistics, pages 423–430.",
      "citeRegEx" : "Klein and Manning.,? 2003",
      "shortCiteRegEx" : "Klein and Manning.",
      "year" : 2003
    }, {
      "title" : "Topically driven neural language model",
      "author" : [ "Jey Han Lau", "Timothy Baldwin", "Trevor Cohn." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 355–365, Vancouver, Canada.",
      "citeRegEx" : "Lau et al\\.,? 2017",
      "shortCiteRegEx" : "Lau et al\\.",
      "year" : 2017
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C Lawrence Zitnick." ],
      "venue" : "European conference on computer vision, pages 740–755. Springer.",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Adversarial multi-task learning for text classification",
      "author" : [ "Pengfei Liu", "Xipeng Qiu", "Xuan-Jing Huang." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1–10.",
      "citeRegEx" : "Liu et al\\.,? 2017",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2017
    }, {
      "title" : "Text summarization with pretrained encoders",
      "author" : [ "Yang Liu", "Mirella Lapata." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Liu and Lapata.,? 2019",
      "shortCiteRegEx" : "Liu and Lapata.",
      "year" : 2019
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 49th annual meeting of the association for computational linguistics: Human lan-",
      "citeRegEx" : "Maas et al\\.,? 2011",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "Intelligent selection of language model training data",
      "author" : [ "Robert C Moore", "William Lewis." ],
      "venue" : "Proceedings of the ACL 2010 Conference Short Papers, pages 220–224.",
      "citeRegEx" : "Moore and Lewis.,? 2010",
      "shortCiteRegEx" : "Moore and Lewis.",
      "year" : 2010
    }, {
      "title" : "Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
      "author" : [ "Shashi Narayan", "Shay B. Cohen", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Narayan et al\\.,? 2018",
      "shortCiteRegEx" : "Narayan et al\\.",
      "year" : 2018
    }, {
      "title" : "A study of style in machine translation: Controlling the formality of machine translation output",
      "author" : [ "Xing Niu", "Marianna Martindale", "Marine Carpuat." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Niu et al\\.,? 2017",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2017
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J. Liu", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073–",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "Extracting in-domain training corpora for neural machine translation using data selection methods",
      "author" : [ "Catarina Cruz Silva", "Chao-Hong Liu", "Alberto Poncelas", "Andy Way." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers,",
      "citeRegEx" : "Silva et al\\.,? 2018",
      "shortCiteRegEx" : "Silva et al\\.",
      "year" : 2018
    }, {
      "title" : "Autoencoding variational inference for topic models",
      "author" : [ "Akash Srivastava", "Charles Sutton." ],
      "venue" : "arXiv preprint arXiv:1703.01488.",
      "citeRegEx" : "Srivastava and Sutton.,? 2017",
      "shortCiteRegEx" : "Srivastava and Sutton.",
      "year" : 2017
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Rethinking lda: Why priors matter",
      "author" : [ "Hanna Wallach", "David Mimno", "Andrew McCallum." ],
      "venue" : "Advances in neural information processing systems, 22:1973– 1981.",
      "citeRegEx" : "Wallach et al\\.,? 2009",
      "shortCiteRegEx" : "Wallach et al\\.",
      "year" : 2009
    }, {
      "title" : "Topic-guided variational auto-encoder for text generation",
      "author" : [ "Wenlin Wang", "Zhe Gan", "Hongteng Xu", "Ruiyi Zhang", "Guoyin Wang", "Dinghan Shen", "Changyou Chen", "Lawrence Carin." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chap-",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Friendly topic assistant for transformer based abstractive summarization",
      "author" : [ "Zhengjue Wang", "Zhibin Duan", "Hao Zhang", "Chaojie Wang", "Long Tian", "Bo Chen", "Mingyuan Zhou." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "What’s in a domain?: Towards fine-grained adaptation for machine translation",
      "author" : [ "van der Wees" ],
      "venue" : "Ph.D. thesis,",
      "citeRegEx" : "Wees.,? \\Q2017\\E",
      "shortCiteRegEx" : "Wees.",
      "year" : 2017
    }, {
      "title" : "What’s in a domain? analyzing genre and topic differences in statistical machine translation",
      "author" : [ "Marlies Van der Wees", "Arianna Bisazza", "Wouter Weerkamp", "Christof Monz." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Wees et al\\.,? 2015",
      "shortCiteRegEx" : "Wees et al\\.",
      "year" : 2015
    }, {
      "title" : "Batchensemble: an alternative approach to efficient ensemble and lifelong learning",
      "author" : [ "Yeming Wen", "Dustin Tran", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:2002.06715.",
      "citeRegEx" : "Wen et al\\.,? 2020",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformer based multi-source domain adaptation",
      "author" : [ "Dustin Wright", "Isabelle Augenstein." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7963–7974, Online. Association for Computa-",
      "citeRegEx" : "Wright and Augenstein.,? 2020",
      "shortCiteRegEx" : "Wright and Augenstein.",
      "year" : 2020
    }, {
      "title" : "Unsupervised deep embedding for clustering analysis",
      "author" : [ "Junyuan Xie", "Ross Girshick", "Ali Farhadi." ],
      "venue" : "International conference on machine learning, pages 478–487.",
      "citeRegEx" : "Xie et al\\.,? 2016",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2016
    }, {
      "title" : "Towards k-means-friendly spaces: Simultaneous deep learning and clustering",
      "author" : [ "Bo Yang", "Xiao Fu", "Nicholas D Sidiropoulos", "Mingyi Hong." ],
      "venue" : "international conference on machine learning, pages 3861–3870. PMLR.",
      "citeRegEx" : "Yang et al\\.,? 2017",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2017
    }, {
      "title" : "Graph convolutional networks for text classification",
      "author" : [ "Liang Yao", "Chengsheng Mao", "Yuan Luo." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7370–7377.",
      "citeRegEx" : "Yao et al\\.,? 2019",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2019
    }, {
      "title" : "Whai: Weibull hybrid autoencoding inference for deep topic modeling",
      "author" : [ "Hao Zhang", "Bo Chen", "Dandan Guo", "Mingyuan Zhou." ],
      "venue" : "arXiv preprint arXiv:1803.01328.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Same representation, different attentions: Shareable sentence representation learning from multiple tasks",
      "author" : [ "Renjie Zheng", "Junkun Chen", "Xipeng Qiu." ],
      "venue" : "Proceedings of the 27th International Joint Conference on Artificial Intelligence, IJCAI’18, page",
      "citeRegEx" : "Zheng et al\\.,? 2018",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2018
    }, {
      "title" : "The poisson gamma belief network",
      "author" : [ "Mingyuan Zhou", "Yulai Cong", "Bo Chen." ],
      "venue" : "Advances in Neural Information Processing Systems, 28:3043–3051.",
      "citeRegEx" : "Zhou et al\\.,? 2015",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2015
    }, {
      "title" : "Beta-negative binomial process and poisson factor analysis",
      "author" : [ "Mingyuan Zhou", "Lauren Hannah", "David Dunson", "Lawrence Carin." ],
      "venue" : "Artificial Intelligence and Statistics, pages 1462–1471.",
      "citeRegEx" : "Zhou et al\\.,? 2012",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 33,
      "context" : "It is common knowledge in modern natural language processing (NLP) that natural language varies greatly across domains, themes, styles, genres and many other linguistic nuances (Van der Wees et al., 2015; van der Wees, 2017; Niu et al., 2017).",
      "startOffset" : 177,
      "endOffset" : 242
    }, {
      "referenceID" : 28,
      "context" : "Many existing works (Liu et al., 2017; Cai and Wan, 2019; Hu et al., 2019) have illustrated that data diversity will affect the performance of LMs if we just train a single LM over the entire dataset, even though fine-tuning a pretrained LM (that has been pre-training on a very large corpus) such as Bert (Devlin et al.",
      "startOffset" : 20,
      "endOffset" : 74
    }, {
      "referenceID" : 5,
      "context" : "Many existing works (Liu et al., 2017; Cai and Wan, 2019; Hu et al., 2019) have illustrated that data diversity will affect the performance of LMs if we just train a single LM over the entire dataset, even though fine-tuning a pretrained LM (that has been pre-training on a very large corpus) such as Bert (Devlin et al.",
      "startOffset" : 20,
      "endOffset" : 74
    }, {
      "referenceID" : 18,
      "context" : "Many existing works (Liu et al., 2017; Cai and Wan, 2019; Hu et al., 2019) have illustrated that data diversity will affect the performance of LMs if we just train a single LM over the entire dataset, even though fine-tuning a pretrained LM (that has been pre-training on a very large corpus) such as Bert (Devlin et al.",
      "startOffset" : 20,
      "endOffset" : 74
    }, {
      "referenceID" : 9,
      "context" : ", 2019) have illustrated that data diversity will affect the performance of LMs if we just train a single LM over the entire dataset, even though fine-tuning a pretrained LM (that has been pre-training on a very large corpus) such as Bert (Devlin et al., 2019) on current task (Aharoni and Goldberg, 2020).",
      "startOffset" : 239,
      "endOffset" : 260
    }, {
      "referenceID" : 20,
      "context" : "In some cases, if we can obtain a well-defined domain label for each sample, some works (Jiang et al., 2020; Du et al., 2020; Wright and Augenstein, 2020) try to consider the multi-domain property of data in developing the LMs.",
      "startOffset" : 88,
      "endOffset" : 154
    }, {
      "referenceID" : 11,
      "context" : "In some cases, if we can obtain a well-defined domain label for each sample, some works (Jiang et al., 2020; Du et al., 2020; Wright and Augenstein, 2020) try to consider the multi-domain property of data in developing the LMs.",
      "startOffset" : 88,
      "endOffset" : 154
    }, {
      "referenceID" : 45,
      "context" : "In some cases, if we can obtain a well-defined domain label for each sample, some works (Jiang et al., 2020; Du et al., 2020; Wright and Augenstein, 2020) try to consider the multi-domain property of data in developing the LMs.",
      "startOffset" : 88,
      "endOffset" : 154
    }, {
      "referenceID" : 1,
      "context" : "However, these pre-defined domain labels are not always accurate or even available (Aharoni and Goldberg, 2020), especially for the wild datasets, in which data come from different sources, such as internet news, product reviews, and daily conversation.",
      "startOffset" : 83,
      "endOffset" : 111
    }, {
      "referenceID" : 31,
      "context" : "Data selection is a commonly used strategy to handle diversity in data (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Silva et al., 2018; Aharoni and Goldberg, 2020).",
      "startOffset" : 71,
      "endOffset" : 182
    }, {
      "referenceID" : 2,
      "context" : "Data selection is a commonly used strategy to handle diversity in data (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Silva et al., 2018; Aharoni and Goldberg, 2020).",
      "startOffset" : 71,
      "endOffset" : 182
    }, {
      "referenceID" : 12,
      "context" : "Data selection is a commonly used strategy to handle diversity in data (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Silva et al., 2018; Aharoni and Goldberg, 2020).",
      "startOffset" : 71,
      "endOffset" : 182
    }, {
      "referenceID" : 36,
      "context" : "Data selection is a commonly used strategy to handle diversity in data (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Silva et al., 2018; Aharoni and Goldberg, 2020).",
      "startOffset" : 71,
      "endOffset" : 182
    }, {
      "referenceID" : 1,
      "context" : "Data selection is a commonly used strategy to handle diversity in data (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Silva et al., 2018; Aharoni and Goldberg, 2020).",
      "startOffset" : 71,
      "endOffset" : 182
    }, {
      "referenceID" : 3,
      "context" : "Inspired by their works and to move beyond, in this paper, we find the semantics learned by topic modeling (Blei et al., 2003; Srivastava and Sutton, 2017) can infer sample clusters to a certain extent via K-means, but is not good enough, as shown in Fig.",
      "startOffset" : 107,
      "endOffset" : 155
    }, {
      "referenceID" : 37,
      "context" : "Inspired by their works and to move beyond, in this paper, we find the semantics learned by topic modeling (Blei et al., 2003; Srivastava and Sutton, 2017) can infer sample clusters to a certain extent via K-means, but is not good enough, as shown in Fig.",
      "startOffset" : 107,
      "endOffset" : 155
    }, {
      "referenceID" : 6,
      "context" : "In order to jointly consider the learning of mATM with various LMs, we employ the weight modulation methods (Cong et al., 2020; Wen et al., 2020).",
      "startOffset" : 108,
      "endOffset" : 145
    }, {
      "referenceID" : 44,
      "context" : "In order to jointly consider the learning of mATM with various LMs, we employ the weight modulation methods (Cong et al., 2020; Wen et al., 2020).",
      "startOffset" : 108,
      "endOffset" : 145
    }, {
      "referenceID" : 3,
      "context" : "For NLP, topic modeling (TM) (Blei et al., 2003; Zhou et al., 2012) and LMs are two common regimes with their own advantages.",
      "startOffset" : 29,
      "endOffset" : 67
    }, {
      "referenceID" : 52,
      "context" : "For NLP, topic modeling (TM) (Blei et al., 2003; Zhou et al., 2012) and LMs are two common regimes with their own advantages.",
      "startOffset" : 29,
      "endOffset" : 67
    }, {
      "referenceID" : 51,
      "context" : "(2020) propose the recurrent hierarchical topic-guided RNN with the help of multi-layer TM (Zhou et al., 2015; Zhang et al., 2018).",
      "startOffset" : 91,
      "endOffset" : 130
    }, {
      "referenceID" : 49,
      "context" : "(2020) propose the recurrent hierarchical topic-guided RNN with the help of multi-layer TM (Zhou et al., 2015; Zhang et al., 2018).",
      "startOffset" : 91,
      "endOffset" : 130
    }, {
      "referenceID" : 38,
      "context" : "(2020) propose three different modules to plug knowledge from TM into Transformer-based LMs (Vaswani et al., 2017; Devlin et al., 2018).",
      "startOffset" : 92,
      "endOffset" : 135
    }, {
      "referenceID" : 8,
      "context" : "(2020) propose three different modules to plug knowledge from TM into Transformer-based LMs (Vaswani et al., 2017; Devlin et al., 2018).",
      "startOffset" : 92,
      "endOffset" : 135
    }, {
      "referenceID" : 3,
      "context" : "We firstly describe one of the most popular topic models, latent Dirichlet allocation (LDA) (Blei et al., 2003), and its autoencoding inference (Srivastava and Sutton, 2017).",
      "startOffset" : 92,
      "endOffset" : 111
    }, {
      "referenceID" : 37,
      "context" : ", 2003), and its autoencoding inference (Srivastava and Sutton, 2017).",
      "startOffset" : 40,
      "endOffset" : 69
    }, {
      "referenceID" : 24,
      "context" : "In particular, Srivastava and Sutton (2017) propose the autoencoding variational inference (AEVB) (Kingma and Welling, 2013) for LDA by using Laplace approximation (Hennig et al.",
      "startOffset" : 98,
      "endOffset" : 124
    }, {
      "referenceID" : 15,
      "context" : "In particular, Srivastava and Sutton (2017) propose the autoencoding variational inference (AEVB) (Kingma and Welling, 2013) for LDA by using Laplace approximation (Hennig et al., 2012) for the Dirichlet prior, and building logistic-normal (LN) encoding posterior.",
      "startOffset" : 164,
      "endOffset" : 185
    }, {
      "referenceID" : 39,
      "context" : "Although Dirichlet prior of θ is important to learn interpretable topics (Wallach et al., 2009), it is difficult to handle it within AEVB since AEVB needs effective reparameterization (RT) function for distributions.",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 19,
      "context" : "Instead, we employ GS distribution (Jang et al., 2016) as the variational posterior of z for efficient gradient propagation.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 21,
      "context" : "Although LMs have many different types, basically, all of them build on convolutional (such as in CNN (Johnson and Zhang, 2015)) or fully-connected (such as in Transformer (Vaswani et al.",
      "startOffset" : 102,
      "endOffset" : 127
    }, {
      "referenceID" : 38,
      "context" : "Although LMs have many different types, basically, all of them build on convolutional (such as in CNN (Johnson and Zhang, 2015)) or fully-connected (such as in Transformer (Vaswani et al., 2017)) operations (ignoring the bias) as",
      "startOffset" : 172,
      "endOffset" : 194
    }, {
      "referenceID" : 3,
      "context" : "means for clustering on topic proportions, where we consider LDA+kmeans (Blei et al., 2003), AVITM+kmeans (Srivastava and Sutton, 2017), and PFA+kmeans (Zhou et al.",
      "startOffset" : 72,
      "endOffset" : 91
    }, {
      "referenceID" : 37,
      "context" : ", 2003), AVITM+kmeans (Srivastava and Sutton, 2017), and PFA+kmeans (Zhou et al.",
      "startOffset" : 22,
      "endOffset" : 51
    }, {
      "referenceID" : 52,
      "context" : ", 2003), AVITM+kmeans (Srivastava and Sutton, 2017), and PFA+kmeans (Zhou et al., 2012); iii) Deep neural network based clustering methods, including Deep clustering (Xie et al.",
      "startOffset" : 68,
      "endOffset" : 87
    }, {
      "referenceID" : 46,
      "context" : ", 2012); iii) Deep neural network based clustering methods, including Deep clustering (Xie et al., 2016), and DCN (Yang et al.",
      "startOffset" : 86,
      "endOffset" : 104
    }, {
      "referenceID" : 47,
      "context" : ", 2016), and DCN (Yang et al., 2017), which jointly consider the feature extracting and clustering.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 46,
      "context" : "Following (Xie et al., 2016; Yang et al., 2017), the dimension of feature space equals to the number of clusters.",
      "startOffset" : 10,
      "endOffset" : 47
    }, {
      "referenceID" : 47,
      "context" : "Following (Xie et al., 2016; Yang et al., 2017), the dimension of feature space equals to the number of clusters.",
      "startOffset" : 10,
      "endOffset" : 47
    }, {
      "referenceID" : 4,
      "context" : "For this task, the data diversity mainly arises from different domains (products) (Blitzer et al., 2007), which brings the problem that data from different domains may have different distributions.",
      "startOffset" : 82,
      "endOffset" : 104
    }, {
      "referenceID" : 5,
      "context" : "Comparison models and implementation details Following (Cai and Wan, 2019), we firstly consider three base models, BiLSTM (Adhikari et al.",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "Comparison models and implementation details Following (Cai and Wan, 2019), we firstly consider three base models, BiLSTM (Adhikari et al., 2019), TextCNN (Kim, 2014) and BERT (Devlin et al.",
      "startOffset" : 122,
      "endOffset" : 145
    }, {
      "referenceID" : 22,
      "context" : ", 2019), TextCNN (Kim, 2014) and BERT (Devlin et al.",
      "startOffset" : 17,
      "endOffset" : 28
    }, {
      "referenceID" : 9,
      "context" : ", 2019), TextCNN (Kim, 2014) and BERT (Devlin et al., 2019), which perform classification on every domains separately.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 50,
      "context" : "2960 as the multi-task learning (MTL) including DAMTL (Zheng et al., 2018), ASP-MTL (Liu et al.",
      "startOffset" : 54,
      "endOffset" : 74
    }, {
      "referenceID" : 28,
      "context" : ", 2018), ASP-MTL (Liu et al., 2017),and MDAE (Cai and Wan, 2019).",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 25,
      "context" : "(2017), we tokenize words and sentences using Stanford CoreNLP (Klein and Manning, 2003), lowercase all word tokens, and filter out word tokens that occur less than 10 times.",
      "startOffset" : 63,
      "endOffset" : 88
    }, {
      "referenceID" : 17,
      "context" : "Comparison models and implementation details We consider the following baseline models: LSTM, A standard LSTM language model (Hochreiter and Schmidhuber, 1997); Tansnsformer-XL enables learning dependency beyond a fixed length by introducing a recurrence mechanism and a novel position encoding scheme into the Transformer architecture (Dai et al.",
      "startOffset" : 125,
      "endOffset" : 159
    }, {
      "referenceID" : 7,
      "context" : "Comparison models and implementation details We consider the following baseline models: LSTM, A standard LSTM language model (Hochreiter and Schmidhuber, 1997); Tansnsformer-XL enables learning dependency beyond a fixed length by introducing a recurrence mechanism and a novel position encoding scheme into the Transformer architecture (Dai et al., 2019); TGVAE (Wang et al.",
      "startOffset" : 336,
      "endOffset" : 354
    }, {
      "referenceID" : 40,
      "context" : ", 2019); TGVAE (Wang et al., 2019), combines a variational auto-encoder based natural sequence model with a neural topic model; rGBN-RNN (Guo et al.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 13,
      "context" : ", 2019), combines a variational auto-encoder based natural sequence model with a neural topic model; rGBN-RNN (Guo et al., 2020), extracts recurrent hierarchical semantic structure via a dynamic deep topic model to guide natural language generation; GPT-2 (Radford et al.",
      "startOffset" : 110,
      "endOffset" : 128
    }, {
      "referenceID" : 34,
      "context" : ", 2020), extracts recurrent hierarchical semantic structure via a dynamic deep topic model to guide natural language generation; GPT-2 (Radford et al., 2019) is a generative pre-training of a Transformerbased LM on a diverse set of unlabeled text.",
      "startOffset" : 135,
      "endOffset" : 157
    }, {
      "referenceID" : 23,
      "context" : "In the experiments, we use the Adam optimizer (Kingma and Ba, 2014) with learning rate 10−6.",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 16,
      "context" : "Datasets We evaluate the effectiveness and efficiency of proposed model on two benchmark datasets, including the CNN/DailyMail (CNN/DM) (Hermann et al., 2015) and the XSum (Narayan et al.",
      "startOffset" : 136,
      "endOffset" : 158
    }, {
      "referenceID" : 35,
      "context" : "Comparison models and implementation details We consider some baseline models, including LSTM based models PTGEN and PTGEN+Cov (See et al., 2017); Transformer based models Tansformer, BertSUM (Liu and Lapata, 2019); and BertSUM+TA which combine pretrained model with topic model (Wang et al.",
      "startOffset" : 127,
      "endOffset" : 145
    }, {
      "referenceID" : 29,
      "context" : ", 2017); Transformer based models Tansformer, BertSUM (Liu and Lapata, 2019); and BertSUM+TA which combine pretrained model with topic model (Wang et al.",
      "startOffset" : 54,
      "endOffset" : 76
    }, {
      "referenceID" : 41,
      "context" : ", 2017); Transformer based models Tansformer, BertSUM (Liu and Lapata, 2019); and BertSUM+TA which combine pretrained model with topic model (Wang et al., 2020).",
      "startOffset" : 141,
      "endOffset" : 160
    } ],
    "year" : 2021,
    "abstractText" : "Natural language processing often faces the problem of data diversity such as different domains, themes, styles and so on. Therefore, a single language model (LM) is insufficient to learn all knowledge from diverse samples. To solve this problem, we firstly propose an autoencoding topic model with mixture prior (mATM) to perform clustering for the data, where the clusters defined in semantic space describe the data diversity. Having obtained the clustering assignment for each sample, we develop the ensemble LM (EnsLM) with the technique of weight modulation. Specifically, EnsLM contains a backbone which is adjusted by a few modulated weights to fit for different sample clusters. As a result, the backbone learns the shared knowledge among all clusters while modulated weights extract the cluster-specific features. EnsLM can be trained jointly with mATM with flexible LM backbone. We evaluate the effectiveness of both mATM and EnsLM on different language understanding and generative tasks.",
    "creator" : "LaTeX with hyperref"
  }
}