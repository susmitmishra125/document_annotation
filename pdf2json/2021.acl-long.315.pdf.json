{
  "name" : "2021.acl-long.315.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Dual Reader-Parser on Hybrid Textual and Tabular Evidence for Open Domain Question Answering",
    "authors" : [ "Alexander Hanbo Li", "Patrick Ng", "Peng Xu", "Henghui Zhu", "Zhiguo Wang", "Bing Xiang" ],
    "emails" : [ "bxiang}@amazon.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4078–4088\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4078"
    }, {
      "heading" : "1 Introduction",
      "text" : "Open-domain question answering (ODQA) is a task to answer factoid questions without a prespecified domain. Recently, generative models (Roberts et al., 2020; Lewis et al., 2020; Min et al., 2020; Izacard and Grave, 2020) have achieved the state-of-the-art performance on many ODQA tasks. These approaches all share the common pipeline where the first stage is retrieving evidence from the free-form text in Wikipedia. However, a large amount of world’s knowledge is not stored as plain\ntext but in structured databases, and need to be accessed using query languages such as SQL. Furthermore, query languages can answer questions that require complex reasoning, as well as offering full explainability. In practice, an ideal ODQA model should be able to retrieve evidence from both unstructured textual and structured tabular information sources, as some questions are better answered by tabular evidence from databases. For example, the current state-of-the-art ODQA models struggle on questions that involve aggregation operations such as counting or averaging.\nOne line of research on accessing databases, although not open domain, is translating natural language questions into SQL queries (Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018c; Guo et al., 2019; Wang et al., 2018a, 2020; Yu et al., 2018a; Guo and Gao, 2019; Choi et al., 2020). These methods all rely on knowing the associated table for each question in advance, and hence are not trivially applicable to the open-domain setting, where the relevant evidence might come from millions of tables.\nIn this paper, we provide a solution to the aforementioned problem by empowering the current generative ODQA models with the Text2SQL ability. More specifically, we propose a dual readerparser (DUREPA) framework that can take both textual and tabular data as input, and generate either direct answers or SQL queries based on the context1. If the model chooses to generate a SQL query, we can then execute the query on the corresponding database to get the final answer. Overall, our framework consists of three stages: retrieval, joint ranking and dual reading-parsing. First we retrieve supporting candidates of both textual and tabular types, followed by a joint reranker that predicts how relevant each supporting candidate is to\n1Our code is available at https://github.com/ AlexanderYogurt/Hybrid-Open-QA\nthe question, and finally we use a fusion-in-decoder model (Izacard and Grave, 2020) for our readerparser, which takes all the reranked candidates in addition to the question to generate direct answers or SQL queries.\nTo evaluate the effectiveness of our DUREPA, we construct a hybrid dataset that combines SQuAD (Rajpurkar et al., 2016) and WikiSQL (Zhong et al., 2017) questions. We also conduct experiments on NaturalQuestions (NQ) (Kwiatkowski et al., 2019) and OTT-QA (Chen et al., 2020a) to evaluate DuRePa performance. As textual and tabular open-domain knowledge, we used textual and tabular data from Wikipedia via Wikidumps (from Dec. 21, 2016) and Wikitables (Bhagavatula et al., 2015). We study the model performance on different kinds of questions, where some of them only need one supporting evidence type while others need both textual and tabular evidence. On all question types, DUREPA performs significantly better than baseline models that were trained on a single evidence type. We also demonstrate that DUREPA can generate humaninterpretable SQLs that answer questions requiring complex reasoning, such as calculations and superlatives.\nOur highlighted contributions are as follows: • We propose a multi-modal framework that in-\ncorporates hybrid knowledge sources with the Text2SQL ability for ODQA tasks. To the best of our knowledge, this is the first work that investigates Text2SQL in the ODQA setting. • We propose a simple but effective generative approach that takes both textual and tabular evidence and generates either direct answers or SQL queries, automatically determined by the context. With that, we achieve the state-of-the-art performance on OpenSQuAD using a T5-base model. • We conduct comprehensive experiments to demonstrate the benefits of Text2SQL for ODQA tasks. We show that interpretable SQL generation can effectively answer questions that require complex reasoning in the ODQA setting."
    }, {
      "heading" : "2 Related Work",
      "text" : "Open Domain Question Answering ODQA has been extensively studied recently including extractive models (Chen et al., 2017; Clark and Gardner, 2018; Wang et al., 2019; Min et al., 2019; Yang et al., 2019) that predict spans from evidence passages, and generative models (Raffel et al., 2020;\nRoberts et al., 2020; Min et al., 2020; Lewis et al., 2020; Izacard and Grave, 2020) that directly generate the answers. Wang et al. (2018b,c); Nogueira and Cho (2019) proposed to rerank the retrieved passages to get higher top-n recall.\nTable Parsing Text2SQL is a task to translate natural questions to executable SQL queries. Brad et al. (2017) proposed SENLIDB dataset which only contains 29 tables and lacks annotation in their training set. Recently, with datasets like WikiSQL (Zhong et al., 2017), Spider (Yu et al., 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al., 2018b; He et al., 2019; Hwang et al., 2019; Min et al., 2019; Wang et al., 2020; Choi et al., 2020; Guo et al., 2019; Lyu et al., 2020; Zhang et al., 2019; Zhong et al., 2020; Shi et al., 2020). Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020). However, they are all closed-domain and each question is given the associated table.\nHybrid QA Chen et al. (2020a) also proposed an open-domain QA problem with textual and tabular evidence. Unlike our problem, they generate an answer directly from the tabular evidence instead of generating an SQL query. In addition, they assume some contextual information about table is available during retrieval stage (e.g. their fusion-retriever is pretrained using hyperlinks between tables and paragraphs), whereas we don’t use any link information between tables and passages. Moreover, Chen et al. (2020b) proposed a closed-domain hybrid QA dataset where each table is linked to on average 44 passages. Different from ours, their purpose is to study multi-hop reasoning over both forms of information, and each question is still given the associated table."
    }, {
      "heading" : "3 Method",
      "text" : "In this section, we describe our method for hybrid open-domain question answering. It mainly consists of three components: (1) a retrieval system; (2) a joint reranker and (3) a dual Seq2Seq model that uses fusion-in-decoder (Izacard and Grave, 2020) to generate direct answer or SQL query."
    }, {
      "heading" : "3.1 Retrieval",
      "text" : "For the hybrid open-domain setting, we build two separate search indices – one for textual input and another for tabular input. For paragraphs, we split them into passages of at most 100 words. For tables, we flattened each table into passages by concatenating cell values along each row. If the flattened table exceeds 100 words, we split it into a separate passage, respecting row boundaries. The column headers are concatenated to each tabular passage. Some examples of flattened tables are given in the Appendix A.1.\nGiven a natural language question, the retrieval system retrieves 100 textual and 100 tabular passages as the support candidates from the textual and tabular indices, respectively, using BM25 (Robertson et al., 1995) ranking function."
    }, {
      "heading" : "3.2 Joint Reranking",
      "text" : "The purpose of our reranking model is to produce a score si of how relevant a candidate (either an unstructured passage or table) is to a question. Specifically, the reranker input is the concatenation of question, a retrieved candidate-content, and its corresponding title if available2, separated by special tokens shown in Figure 1. The candidate content can be either the unstructured\n2Wikipedia passages have page titles, and tables have table titles.\ntext or flattened table. We use BERTbase model in this paper. Following Nogueira and Cho (2019), we finetune the BERT (Devlin et al., 2019) model using the following loss:\nL = X\ni2Ipos\nlog(si) X\ni2Ineg\nlog(1 si). (1)\nThe Ipos is sampled from all relevant BM25 candidates, and the set Ineg is sampled from all non-relevant BM25 candidates. Different from Nogueira and Cho (2019), during training, for each question, we sample 64 candidates including one positive candidate and 63 negative candidates, that is, |Ipos| = 1 and |Ineg| = 63. If none of the 200 candidates is relevant, we skip the question. During inference, we use the hybrid reranker to assign a score to each of the 200 candidates, and choose the top 50 candidates as the input to the next module – the reader-parser model. For the top 50 candidates, we choose them from the joint pool of all candidates, according to the scores assigned by the reranker."
    }, {
      "heading" : "3.3 Dual Reading-Parsing",
      "text" : "Our dual reader-parser model is based on the fusionin-decoder (FID) proposed in Izacard and Grave (2020), and is initialized using the pretrained T5 (Raffel et al., 2020) model. The overall pipeline of the reader-parser is shown in Figure 1. Each\nretrieved candidate is represented by its title and content, in the following formats:\nTextual Candidate We represent each textual candidate as the concatenation of the passage title and content, appended by special tokens [text title] and [text content] respectively.\nTabular Candidate In order to represent a structured table as a passage, we first flatten each table into the following format: each flattened table starts with the complete header names and then followed by rows. Figure 1 presents an example for this conversion.\nFinally, a tabular candidate is the concatenation of the table title and content flattened as a passage, appended by special tokens [table title] and [table content] respectively. We use the table ID as the title so that it can be copied to the generated SQL queries by the model.\nPrefix of the Targets During training, we also add special tokens answer: or sql: to a targeted sentence depending on whether it is a plain text or a SQL query. For those questions that have both textual answer and SQL query annotations (for example, WikiSQL questions), we create two training examples for each question. During inference, the generated outputs will also contain these two special prefixes, indicating which output type the model has generated.\nDual Reader-Parser Our generative Seq2Seq model has reader-parser duality. During inference, the model reads the question and all the candidates, and produces k outputs using beam search. Each output can be either a final answer or an intermediate SQL query. Depending on the context, the types and order of the outputs are automatically determined by the model itself. All the generated SQL queries will then be executed to produce the final answers. In this paper, we fix k = 3 and always generate three outputs for each question."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we report the performance of the proposed method on several hybrid open-domain QA datasets."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "In this section, we describe all the datasets we use in our experiments. First we summarize the statis-\ntics of the open-domain QA datasets we use in Table 1.\nOpenSQuAD is an open-domain QA dataset constructed from the original SQuAD-v1.1 (Rajpurkar et al., 2016), which was designed for the reading comprehension task, consisting of 100,000+ questions posed by annotators on a set of Wikipedia articles, where the answer to each question is a span from the corresponding paragraph.\nOpenNQ is an open-domain QA datasets constructed from the NaturalQuestions (Kwiatkowski et al., 2019), which was desgined for the end-toend question answering task. The questions were from real google search queries and the answers were from Wikipedia articles annotated by humans.\nOTT-QA (Chen et al., 2020a) is a large-scale open table-and-text question answering dataset for evaluating open QA over both tabular and textual data. The questions were constructed through “decontextualization” from HybridQA (Chen et al., 2020b) with additional 2200 new questions mainly used in dev/test set. OTT-QA also provides its own corpus which contains over 5 million passages and around 400k tables.\nOpenWikiSQL is an open-domain Text2SQL QA dataset constructed from the original WikiSQL (Zhong et al., 2017). WikiSQL is a dataset of 80,654 annotated questions and SQL queries distributed across 24,241 tables from Wikipedia.\nMix-SQuWiki is the union of OpenSQuAD and OpenWikiSQL datasets.\nWikiSQL-both is a subset of OpenWikiSQL evaluation data that contains the questions that can be answered by both textual and tabular evidences. The purpose of this dataset is to study when both types of evidence are possible to answer a question, whether the hybrid model can still choose the better one. We select these questions in a weaklysupervised way by only keeping a question if the\ngroundtruth answer is contained in both textual and tabular BM25 candidates. For example in Figure 1, the answer “Richard Marquand” can be found in both types of passages. We filter out some trivial cases where the answer shows up in more than half of the candidates. 5\nWikipedia Passages and Tables For the textual evidences, we process the Wikipedia 2016 dump and split the articles into overlapping passages of 100 words following (Wang et al., 2019). To create the tabular evidences, we combine 1.6M Wikipedia tables (Bhagavatula et al., 2015) and all the 24,241 WikiSQL tables, and flatten and split each table into passages not exceeding 100 words, in the same format mentioned in the previous section. We use these two collections as the evidence sources for all the QA datasets except for OTT-QA, where we use its own textual and tabular collections."
    }, {
      "heading" : "4.2 Implementation Details",
      "text" : "Retriever and Reranker. We conduct BM25 retrieval using Elasticsearch 7.7 6 with the default settings. And we use a BERT reranker initialized with pretrained BERT-base-uncased model.\nDual Reader and Parser with fusion-in-decoder. Similar to (Izacard and Grave, 2020), we initialize the fusion-in-decoders with the pretrained T5 model (Raffel et al., 2020). We only explore T5base model in this paper, which has 220M parameters.\n5For example, some numerical number like ”1” is a very common substring and shows up in most of the candidates.\n6https://www.elastic.co/\nFor both reranker and FiD models, we use Adam optimizer (Kingma and Ba, 2014) with a maximum learning rate of 10 4 and a dropout rate of 10%. The learning rate linearly warms up to 10 4 and then linearly anneals to zero. We train models for 10k gradient steps with a batch size of 32, and save a checkpoint every 1k steps. For the FiD model, when there are multiple answers for one question, we randomly sample one answer from the list. For the FiD model, during inference, we generate 3 answers for each question using beam search with beam size 3."
    }, {
      "heading" : "4.3 Main Results",
      "text" : "We present the end-to-end results on the opendomain QA task comparing with the baseline methods as show in Table 2.\nWe build models with 5 different settings based on the source evidence modality as well as the format of model prediction. Specifically, we consider single modality settings with only textual evidence or tabular evidence and the hybrid setting with both textual and tabular evidence available. For tabular evidence, the models either predict direct answer text or generate structure SQL queries. Note we also consider a baseline model, FID+ , a FiD model that only generates direct answer text, but can make use of both textual and tabular evidence.\n3Chen et al. (2020a) uses a fusion-retriever to retrieved table-passages blocks as evidences. To construct the fusion blocks, they train a GPT-2 model using extra hyperlink information to link table cell to passages. In contrast, we do not use any hyperlink information.\n4Oguz et al. (2020) uses tables provided by NQ training data (less than 500k in total), whereas we use all the tables extracted from Wikipedia dumps (around 1.6M in total).\nFirst, in the single modality setting, we observe that for OpenSQuAD, OpenNQ and OTT-QA datasets, textual QA model is performing significantly better than tabular QA models, while for OpenWikiSQL, it is the opposite. This is expected due to the nature of the construction process of those datasets. In the hybrid setting, the hybrid models outperform single modality models consistently across all these datasets. This indicates hybrid models are more robust and flexible when dealing with questions of various types in practice.\nComparing DUREPA with FID+ , we observe that having the ability to generate structural queries is always beneficial even for extractive questions like SQuAD and NQ. And for WikiSQL-type questions, the gain of SQL generation is significant.\nOn OpenSQuAD dataset, our DUREPA model using hybrid evidences achieves a new state-ofthe-art EM score of 57.0. It is worth noting that the previous best score was attained by FiD using T5-large model, while our model is using T5base, which has much fewer parameters. On NQ dataset, FID+ with text-only evidences has lower EM score compared with FiD-base, despite having the same underlying model and inputs. We suspect that this is because (1) we truncate all passages into at most 150 word pieces while in FiD paper they keep 250 word pieces, so the actual input (top-100 passages) to our FiD model is much less than that in the FiD paper; and (2) we use BM25 to retrieve the initial pool of candidates instead of trained embedding-based neural retrieval model(Karpukhin et al., 2020; Izacard and Grave, 2020). Nevertheless, the DUREPA model with hybrid evidences still improve the EM by 2.8 points compared to FID+ using only text inputs. On OTTQA questions, our full model also outperforms the IR+CR baseline by 1.4 points. The FR+CR model is using a different setting where they use hyperlinks between tables and passages to train the\nfusion-retriever (FR), so the result is not directly comparable to ours. We provide more analysis on OTT-QA in the Appendix. On OpenWikiSQL dataset, enabling SQL generation brings more than 10 points improvement on the EM scores. This is because many questions therein require complex reasoning like COUNT, AVERAGE or SUM on the table evidences. We provide more in-depth analysis in Section 5.2 including some complex reasoning examples in Table 7."
    }, {
      "heading" : "5 Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Retrieval and Reranking Performance",
      "text" : "In this section, we investigate the performance of the BM25 retriever and the BERT reranker using top-k recalls as our evaluation metric.\nDuring both training and inference, for each question, the textual and tabular passages are reranked jointly using a single reranker. On the Mix-SQuWiki dataset, we report the reranking results on SQuAD questions in Table 3. The result on WikiSQL questions is in Table 9 in Appendix. To provide better insights on the reranker’s performance, we show the top-k recalls on textual, tabular and hybrid evidences separately.\nFrom Table 3, on both textual and tabular candidates, recall@25 of the ranker is even higher than recall@100 of the BM25 retriever. This suggest that during inference, instead of providing 100 BM25 candidates to the fusion-in-decoder (FiD), only 25 reranked candidates would suffice.\nIn Table 9 and 10 in Appendix, we observe similar trend with top-25 recalls comparable to top100 recalls on both WikiSQL and NQ questions. Finally, across all datasets, the recalls on hybrid inputs are almost the same as or even better than the best recalls on individual textual or tabular inputs, meaning that the reranker is able to jointly rank both types of candidates and provide better\nevidences to the next component – the dual readerparser."
    }, {
      "heading" : "5.2 Performance of the Reader-Parser",
      "text" : "In this section, we discuss the performance of the dual reader-parser on different kinds of questions.\nSQL prediction helps with complex reasoning. In Table 4, we compare the top-1 EM execution accuracy of DUREPA and FID+ on OpenWikiSQL. If DUREPA generated a SQL, we execute the SQL to obtain its answer prediction. If the ground-truth answer is a list (e.g., What are the names of Simpsons episodes aired in 2008?), we use set-equivalence to evaluate accuracy. DUREPA outperforms FID+ on the test set in most of the settings. We also compare their performance under a breakdown of different categories based on the ground-truth SQL query. DUREPA achieved close to 3x and 5x improvements on WikiSQL questions that have superlative (MAX/MIN) and calculation (SUM/AVG) operations, respectively. For COUNT queries, FID+ often predicted either 0 or 1. Thus, these results support our hypothesis that the SQL generation helps in complex reasoning and explainability for tabular question answering.\nUsing hybrid evidence types leads to better performance. Shown in Table 5 is the model performance on the Mix-SQuWiki questions. As the baseline models, if we only use a single evidence type, the best top-1 EM is 34.0, achieved by the model FID+ using only textual candidates. However, if we use both evidence types, the hybrid model DUREPA attains a significantly better top-\n1 EM of 47.9, which implies that including both textual and tabular evidences leads a better model performance on Mix-SQuWiki. Furthermore, we observe that the model DUREPA has a better top-1 EM compared to FID+, suggesting that the answers for some of these questions need to be obtained by executing SQL queries instead of generated directly. In Table 7, we samples some questions on which the model DUREPA predicts the correct answers but the model FID+ fails.\nWhat if the questions can be answered by both textual and tabular evidences? Table 6 shows the model performance on WikiSQL-both dataset. Recall that all these questions in the dataset can be answered by both type of evidence. First of all, the DUREPA model using tabular evidences behaves better than the FID+ model using textual evidences. This implies on WikiSQL questions, using tabular information leads to better answers. Next, when using only one type of evidence, both DUREPA and FID+ models behave significantly worse than their hybrid counterparts. This indicates that the hybrid model can again figure out which evidence type should be used to provide the correct final answer."
    }, {
      "heading" : "6 Discussion and Future Work",
      "text" : "Our experiments consistently show that the proposed framework DUREPA brings significant improvement on answering questions using hybrid types of evidence. Especially on the questions that can be answered by both supporting evidence types, our multi-modal method still shows clear advantage over models using single-type knowledge, implying that our approach could figure out the most relevant evidence to answer a question. We also demonstrate that the dual reader-parser is essential to the good performance of DUREPA; the ability of generating both direct answers and structural SQL queries help DUREPA perform much better than FID+ and other baselines on questions that require complex reasoning like counting or averaging.\nWe believe that our methods can be improved in two aspects. First, our general framework Fig. 1 can be improved by a better retrieval system. For example, instead of using BM25, we can use more powerful neural retrieval models (Karpukhin et al., 2020). On the hybrid evidence, one can also use an entity linking module to link the entities between the tables and passages (Chen et al., 2020a) and utilize the structure information for better multi-\nhop reasoning. Second, as we have demonstrated, having the ability of generating structural SQL queries is a very powerful and necessary feature for answering questions that require complex rea-\nsoning. Given the limited Text2SQL data and the difficulty of obtaining such SQL supervision, two interesting future work include (1) getting SQL annotations more efficiently and (2) adapting weaklysupervised approaches like discrete EM (Min et al., 2019) for model training."
    } ],
    "references" : [ {
      "title" : "Tabel: entity linking in web tables",
      "author" : [ "Chandra Sekhar Bhagavatula", "Thanapon Noraset", "Doug Downey." ],
      "venue" : "International Semantic Web Conference, pages 425–441. Springer.",
      "citeRegEx" : "Bhagavatula et al\\.,? 2015",
      "shortCiteRegEx" : "Bhagavatula et al\\.",
      "year" : 2015
    }, {
      "title" : "Dataset for a neural natural language interface for databases (nnlidb)",
      "author" : [ "Florin Brad", "Radu Iacob", "Ionel Hosu", "Traian Rebedea." ],
      "venue" : "arXiv preprint arXiv:1707.03172.",
      "citeRegEx" : "Brad et al\\.,? 2017",
      "shortCiteRegEx" : "Brad et al\\.",
      "year" : 2017
    }, {
      "title" : "Reading wikipedia to answer opendomain questions",
      "author" : [ "Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes." ],
      "venue" : "55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, pages 1870–1879. Association for Computational",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Open question answering over tables and text",
      "author" : [ "Wenhu Chen", "Ming-Wei Chang", "Eva Schlinger", "William Wang", "William W Cohen." ],
      "venue" : "arXiv preprint arXiv:2010.10439.",
      "citeRegEx" : "Chen et al\\.,? 2020a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Hybridqa: A dataset of multi-hop question answering over tabular and textual data",
      "author" : [ "Wenhu Chen", "Hanwen Zha", "Zhiyu Chen", "Wenhan Xiong", "Hong Wang", "William Yang Wang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Chen et al\\.,? 2020b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Ryansql: Recursively applying sketch-based slot fillings for complex textto-sql in cross-domain databases",
      "author" : [ "DongHyun Choi", "Myeong Cheol Shin", "EungGyun Kim", "Dong Ryeol Shin." ],
      "venue" : "arXiv preprint arXiv:2004.03125.",
      "citeRegEx" : "Choi et al\\.,? 2020",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2020
    }, {
      "title" : "Simple and effective multi-paragraph reading comprehension",
      "author" : [ "Christopher Clark", "Matt Gardner." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 845–855.",
      "citeRegEx" : "Clark and Gardner.,? 2018",
      "shortCiteRegEx" : "Clark and Gardner.",
      "year" : 2018
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards complex text-to-sql in cross-domain",
      "author" : [ "Jiaqi Guo", "Zecheng Zhan", "Yan Gao", "Yan Xiao", "Jian-Guang Lou", "Ting Liu", "Dongmei Zhang" ],
      "venue" : null,
      "citeRegEx" : "Guo et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2019
    }, {
      "title" : "Content enhanced bert-based text-to-sql generation",
      "author" : [ "Tong Guo", "Huilin Gao." ],
      "venue" : "arXiv preprint arXiv:1910.07179.",
      "citeRegEx" : "Guo and Gao.,? 2019",
      "shortCiteRegEx" : "Guo and Gao.",
      "year" : 2019
    }, {
      "title" : "X-sql: reinforce schema representation with context",
      "author" : [ "Pengcheng He", "Yi Mao", "Kaushik Chakrabarti", "Weizhu Chen." ],
      "venue" : "arXiv preprint arXiv:1908.08113.",
      "citeRegEx" : "He et al\\.,? 2019",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2019
    }, {
      "title" : "Tapas: Weakly supervised table parsing via pre-training",
      "author" : [ "Jonathan Herzig", "Pawel Krzysztof Nowak", "Thomas Mueller", "Francesco Piccinno", "Julian Eisenschlos." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Herzig et al\\.,? 2020",
      "shortCiteRegEx" : "Herzig et al\\.",
      "year" : 2020
    }, {
      "title" : "A comprehensive exploration on wikisql with table-aware word contextualization",
      "author" : [ "Wonseok Hwang", "Jinyeong Yim", "Seunghyun Park", "Minjoon Seo." ],
      "venue" : "arXiv preprint arXiv:1902.01069.",
      "citeRegEx" : "Hwang et al\\.,? 2019",
      "shortCiteRegEx" : "Hwang et al\\.",
      "year" : 2019
    }, {
      "title" : "Leveraging passage retrieval with generative models for open domain question answering",
      "author" : [ "Gautier Izacard", "Edouard Grave." ],
      "venue" : "arXiv preprint arXiv:2007.01282.",
      "citeRegEx" : "Izacard and Grave.,? 2020",
      "shortCiteRegEx" : "Izacard and Grave.",
      "year" : 2020
    }, {
      "title" : "Dense passage retrieval for open-domain question answering",
      "author" : [ "Vladimir Karpukhin", "Barlas Oğuz", "Sewon Min", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wentau Yih." ],
      "venue" : "arXiv preprint arXiv:2004.04906.",
      "citeRegEx" : "Karpukhin et al\\.,? 2020",
      "shortCiteRegEx" : "Karpukhin et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Natural questions: a benchmark for question answering research",
      "author" : [ "Tom Kwiatkowski", "Jennimaria Palomaki", "Olivia Redfield", "Michael Collins", "Ankur Parikh", "Chris Alberti", "Danielle Epstein", "Illia Polosukhin", "Jacob Devlin", "Kenton Lee" ],
      "venue" : null,
      "citeRegEx" : "Kwiatkowski et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Kwiatkowski et al\\.",
      "year" : 2019
    }, {
      "title" : "Retrieval-augmented generation for knowledge-intensive nlp",
      "author" : [ "Patrick Lewis", "Ethan Perez", "Aleksandara Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich Küttler", "Mike Lewis", "Wen-tau Yih", "Tim Rocktäschel" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural enquirer: learning to query tables in natural language",
      "author" : [ "Zhengdong Lu", "Hang Li", "Ben Kao." ],
      "venue" : "IEEE Data Eng. Bull., 39(3):63–73.",
      "citeRegEx" : "Lu et al\\.,? 2016",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2016
    }, {
      "title" : "Hybrid ranking network for text-to-sql",
      "author" : [ "Qin Lyu", "Kaushik Chakrabarti", "Shobhit Hathi", "Souvik Kundu", "Jianwen Zhang", "Zheng Chen." ],
      "venue" : "arXiv preprint arXiv:2008.04759.",
      "citeRegEx" : "Lyu et al\\.,? 2020",
      "shortCiteRegEx" : "Lyu et al\\.",
      "year" : 2020
    }, {
      "title" : "A discrete hard em approach for weakly supervised question answering",
      "author" : [ "Sewon Min", "Danqi Chen", "Hannaneh Hajishirzi", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Min et al\\.,? 2019",
      "shortCiteRegEx" : "Min et al\\.",
      "year" : 2019
    }, {
      "title" : "Ambigqa: Answering ambiguous open-domain questions",
      "author" : [ "Sewon Min", "Julian Michael", "Hannaneh Hajishirzi", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5783–",
      "citeRegEx" : "Min et al\\.,? 2020",
      "shortCiteRegEx" : "Min et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural programmer: Inducing latent programs with gradient descent",
      "author" : [ "Arvind Neelakantan", "Quoc V Le", "Ilya Sutskever." ],
      "venue" : "arXiv preprint arXiv:1511.04834.",
      "citeRegEx" : "Neelakantan et al\\.,? 2015",
      "shortCiteRegEx" : "Neelakantan et al\\.",
      "year" : 2015
    }, {
      "title" : "Passage re-ranking with bert",
      "author" : [ "Rodrigo Nogueira", "Kyunghyun Cho." ],
      "venue" : "arXiv preprint arXiv:1901.04085.",
      "citeRegEx" : "Nogueira and Cho.,? 2019",
      "shortCiteRegEx" : "Nogueira and Cho.",
      "year" : 2019
    }, {
      "title" : "Unified open-domain question answering with structured and unstructured",
      "author" : [ "Barlas Oguz", "Xilun Chen", "Vladimir Karpukhin", "Stan Peshterliev", "Dmytro Okhonko", "Michael Schlichtkrull", "Sonal Gupta", "Yashar Mehdad", "Scott Yih" ],
      "venue" : null,
      "citeRegEx" : "Oguz et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Oguz et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Squad: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "How much knowledge can you pack into the parameters of a language model",
      "author" : [ "Adam Roberts", "Colin Raffel", "Noam Shazeer" ],
      "venue" : "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Roberts et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Roberts et al\\.",
      "year" : 2020
    }, {
      "title" : "Okapi at trec-3",
      "author" : [ "Stephen E Robertson", "Steve Walker", "Susan Jones", "Micheline M Hancock-Beaulieu", "Mike Gatford" ],
      "venue" : "Nist Special Publication",
      "citeRegEx" : "Robertson et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Robertson et al\\.",
      "year" : 1995
    }, {
      "title" : "Learning contextual representations for semantic parsing with generation-augmented pre-training",
      "author" : [ "Peng Shi", "Patrick Ng", "Zhiguo Wang", "Henghui Zhu", "Alexander Hanbo Li", "Jun Wang", "Cicero Nogueira dos Santos", "Bing Xiang." ],
      "venue" : "arXiv",
      "citeRegEx" : "Shi et al\\.,? 2020",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2020
    }, {
      "title" : "Rat-sql: Relation-aware schema encoding and linking",
      "author" : [ "Bailin Wang", "Richard Shin", "Xiaodong Liu", "Oleksandr Polozov", "Matthew Richardson" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Robust text-to-sql generation with execution-guided decoding",
      "author" : [ "Chenglong Wang", "Kedar Tatwawadi", "Marc Brockschmidt", "Po-Sen Huang", "Yi Mao", "Oleksandr Polozov", "Rishabh Singh." ],
      "venue" : "arXiv preprint arXiv:1807.03100.",
      "citeRegEx" : "Wang et al\\.,? 2018a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "R 3: Reinforced ranker-reader for open-domain question answering",
      "author" : [ "Shuohang Wang", "Mo Yu", "Xiaoxiao Guo", "Zhiguo Wang", "Tim Klinger", "Wei Zhang", "Shiyu Chang", "Gerry Tesauro", "Bowen Zhou", "Jing Jiang." ],
      "venue" : "Proceedings of the AAAI Conference",
      "citeRegEx" : "Wang et al\\.,? 2018b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Evidence aggregation for answer re-ranking in opendomain question answering",
      "author" : [ "Shuohang Wang", "Mo Yu", "Jing Jiang", "Wei Zhang", "Xiaoxiao Guo", "Shiyu Chang", "Zhiguo Wang", "Tim Klinger", "Gerald Tesauro", "Murray Campbell." ],
      "venue" : "International Con-",
      "citeRegEx" : "Wang et al\\.,? 2018c",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-passage bert: A globally normalized bert model for opendomain question answering",
      "author" : [ "Zhiguo Wang", "Patrick Ng", "Xiaofei Ma", "Ramesh Nallapati", "Bing Xiang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Sqlnet: Generating structured queries from natural language without reinforcement learning",
      "author" : [ "Xiaojun Xu", "Chang Liu", "Dawn Song." ],
      "venue" : "arXiv preprint arXiv:1711.04436.",
      "citeRegEx" : "Xu et al\\.,? 2017",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2017
    }, {
      "title" : "End-to-end open-domain question answering with bertserini",
      "author" : [ "Wei Yang", "Yuqing Xie", "Aileen Lin", "Xingyu Li", "Luchen Tan", "Kun Xiong", "Ming Li", "Jimmy Lin." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Tabert: Pretraining for joint understanding of textual and tabular data",
      "author" : [ "Pengcheng Yin", "Graham Neubig", "Wen-tau Yih", "Sebastian Riedel." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8413–",
      "citeRegEx" : "Yin et al\\.,? 2020",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2020
    }, {
      "title" : "Typesql: Knowledgebased type-aware neural text-to-sql generation",
      "author" : [ "Tao Yu", "Zifan Li", "Zilin Zhang", "Rui Zhang", "Dragomir Radev." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Yu et al\\.,? 2018a",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2018
    }, {
      "title" : "Syntaxsqlnet: Syntax tree networks for complex and cross-domain text-to-sql task",
      "author" : [ "Tao Yu", "Michihiro Yasunaga", "Kai Yang", "Rui Zhang", "Dongxu Wang", "Zifan Li", "Dragomir Radev." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods",
      "citeRegEx" : "Yu et al\\.,? 2018b",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2018
    }, {
      "title" : "Cosql: A conversational text-to-sql challenge towards cross-domain natural language interfaces to databases",
      "author" : [ "Tao Yu", "Rui Zhang", "Heyang Er", "Suyi Li", "Eric Xue", "Bo Pang", "Xi Victoria Lin", "Yi Chern Tan", "Tianze Shi", "Zihan Li" ],
      "venue" : null,
      "citeRegEx" : "Yu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2019
    }, {
      "title" : "Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql",
      "author" : [ "Tao Yu", "Rui Zhang", "Kai Yang", "Michihiro Yasunaga", "Dongxu Wang", "Zifan Li", "James Ma", "Irene Li", "Qingning Yao", "Shanelle Roman" ],
      "venue" : null,
      "citeRegEx" : "Yu et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2018
    }, {
      "title" : "Editing-based sql query generation for cross-domain context-dependent questions",
      "author" : [ "Rui Zhang", "Tao Yu", "Heyang Er", "Sungrok Shim", "Eric Xue", "Xi Victoria Lin", "Tianze Shi", "Caiming Xiong", "Richard Socher", "Dragomir Radev." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Grounded adaptation for zeroshot executable semantic parsing",
      "author" : [ "Victor Zhong", "Mike Lewis", "Sida I Wang", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6869–",
      "citeRegEx" : "Zhong et al\\.,? 2020",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2020
    }, {
      "title" : "Seq2sql: Generating structured queries from natural language using reinforcement learning",
      "author" : [ "Victor Zhong", "Caiming Xiong", "Richard Socher." ],
      "venue" : "CoRR, abs/1709.00103.",
      "citeRegEx" : "Zhong et al\\.,? 2017",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "(Roberts et al., 2020; Lewis et al., 2020; Min et al., 2020; Izacard and Grave, 2020) have achieved the state-of-the-art performance on many ODQA tasks.",
      "startOffset" : 0,
      "endOffset" : 85
    }, {
      "referenceID" : 17,
      "context" : "(Roberts et al., 2020; Lewis et al., 2020; Min et al., 2020; Izacard and Grave, 2020) have achieved the state-of-the-art performance on many ODQA tasks.",
      "startOffset" : 0,
      "endOffset" : 85
    }, {
      "referenceID" : 21,
      "context" : "(Roberts et al., 2020; Lewis et al., 2020; Min et al., 2020; Izacard and Grave, 2020) have achieved the state-of-the-art performance on many ODQA tasks.",
      "startOffset" : 0,
      "endOffset" : 85
    }, {
      "referenceID" : 13,
      "context" : "(Roberts et al., 2020; Lewis et al., 2020; Min et al., 2020; Izacard and Grave, 2020) have achieved the state-of-the-art performance on many ODQA tasks.",
      "startOffset" : 0,
      "endOffset" : 85
    }, {
      "referenceID" : 44,
      "context" : "One line of research on accessing databases, although not open domain, is translating natural language questions into SQL queries (Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018c; Guo et al., 2019; Wang et al., 2018a, 2020; Yu et al., 2018a; Guo and Gao, 2019; Choi et al., 2020).",
      "startOffset" : 130,
      "endOffset" : 285
    }, {
      "referenceID" : 35,
      "context" : "One line of research on accessing databases, although not open domain, is translating natural language questions into SQL queries (Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018c; Guo et al., 2019; Wang et al., 2018a, 2020; Yu et al., 2018a; Guo and Gao, 2019; Choi et al., 2020).",
      "startOffset" : 130,
      "endOffset" : 285
    }, {
      "referenceID" : 8,
      "context" : "One line of research on accessing databases, although not open domain, is translating natural language questions into SQL queries (Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018c; Guo et al., 2019; Wang et al., 2018a, 2020; Yu et al., 2018a; Guo and Gao, 2019; Choi et al., 2020).",
      "startOffset" : 130,
      "endOffset" : 285
    }, {
      "referenceID" : 38,
      "context" : "One line of research on accessing databases, although not open domain, is translating natural language questions into SQL queries (Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018c; Guo et al., 2019; Wang et al., 2018a, 2020; Yu et al., 2018a; Guo and Gao, 2019; Choi et al., 2020).",
      "startOffset" : 130,
      "endOffset" : 285
    }, {
      "referenceID" : 9,
      "context" : "One line of research on accessing databases, although not open domain, is translating natural language questions into SQL queries (Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018c; Guo et al., 2019; Wang et al., 2018a, 2020; Yu et al., 2018a; Guo and Gao, 2019; Choi et al., 2020).",
      "startOffset" : 130,
      "endOffset" : 285
    }, {
      "referenceID" : 5,
      "context" : "One line of research on accessing databases, although not open domain, is translating natural language questions into SQL queries (Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018c; Guo et al., 2019; Wang et al., 2018a, 2020; Yu et al., 2018a; Guo and Gao, 2019; Choi et al., 2020).",
      "startOffset" : 130,
      "endOffset" : 285
    }, {
      "referenceID" : 13,
      "context" : "4079 the question, and finally we use a fusion-in-decoder model (Izacard and Grave, 2020) for our readerparser, which takes all the reranked candidates in addition to the question to generate direct answers or SQL queries.",
      "startOffset" : 64,
      "endOffset" : 89
    }, {
      "referenceID" : 26,
      "context" : "To evaluate the effectiveness of our DUREPA, we construct a hybrid dataset that combines SQuAD (Rajpurkar et al., 2016) and WikiSQL (Zhong et al.",
      "startOffset" : 95,
      "endOffset" : 119
    }, {
      "referenceID" : 16,
      "context" : "We also conduct experiments on NaturalQuestions (NQ) (Kwiatkowski et al., 2019) and OTT-QA (Chen et al.",
      "startOffset" : 53,
      "endOffset" : 79
    }, {
      "referenceID" : 3,
      "context" : ", 2019) and OTT-QA (Chen et al., 2020a) to evaluate DuRePa performance.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 25,
      "context" : ", 2019) that predict spans from evidence passages, and generative models (Raffel et al., 2020; Roberts et al., 2020; Min et al., 2020; Lewis et al., 2020; Izacard and Grave, 2020) that directly generate the answers.",
      "startOffset" : 73,
      "endOffset" : 179
    }, {
      "referenceID" : 27,
      "context" : ", 2019) that predict spans from evidence passages, and generative models (Raffel et al., 2020; Roberts et al., 2020; Min et al., 2020; Lewis et al., 2020; Izacard and Grave, 2020) that directly generate the answers.",
      "startOffset" : 73,
      "endOffset" : 179
    }, {
      "referenceID" : 21,
      "context" : ", 2019) that predict spans from evidence passages, and generative models (Raffel et al., 2020; Roberts et al., 2020; Min et al., 2020; Lewis et al., 2020; Izacard and Grave, 2020) that directly generate the answers.",
      "startOffset" : 73,
      "endOffset" : 179
    }, {
      "referenceID" : 17,
      "context" : ", 2019) that predict spans from evidence passages, and generative models (Raffel et al., 2020; Roberts et al., 2020; Min et al., 2020; Lewis et al., 2020; Izacard and Grave, 2020) that directly generate the answers.",
      "startOffset" : 73,
      "endOffset" : 179
    }, {
      "referenceID" : 13,
      "context" : ", 2019) that predict spans from evidence passages, and generative models (Raffel et al., 2020; Roberts et al., 2020; Min et al., 2020; Lewis et al., 2020; Izacard and Grave, 2020) that directly generate the answers.",
      "startOffset" : 73,
      "endOffset" : 179
    }, {
      "referenceID" : 44,
      "context" : "Recently, with datasets like WikiSQL (Zhong et al., 2017), Spider (Yu et al.",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 40,
      "context" : ", 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al.",
      "startOffset" : 19,
      "endOffset" : 36
    }, {
      "referenceID" : 22,
      "context" : "Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020).",
      "startOffset" : 85,
      "endOffset" : 167
    }, {
      "referenceID" : 18,
      "context" : "Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020).",
      "startOffset" : 85,
      "endOffset" : 167
    }, {
      "referenceID" : 11,
      "context" : "Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020).",
      "startOffset" : 85,
      "endOffset" : 167
    }, {
      "referenceID" : 37,
      "context" : "Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020).",
      "startOffset" : 85,
      "endOffset" : 167
    }, {
      "referenceID" : 13,
      "context" : "uses fusion-in-decoder (Izacard and Grave, 2020) to generate direct answer or SQL query.",
      "startOffset" : 23,
      "endOffset" : 48
    }, {
      "referenceID" : 28,
      "context" : "sages as the support candidates from the textual and tabular indices, respectively, using BM25 (Robertson et al., 1995) ranking function.",
      "startOffset" : 95,
      "endOffset" : 119
    }, {
      "referenceID" : 7,
      "context" : "Following Nogueira and Cho (2019), we finetune the BERT (Devlin et al., 2019) model using the following loss:",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 3,
      "context" : "OTT-QA (Chen et al., 2020a) is a large-scale open table-and-text question answering dataset for evaluating open QA over both tabular and textual",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 4,
      "context" : "The questions were constructed through “decontextualization” from HybridQA (Chen et al., 2020b) with additional 2200 new questions mainly used in dev/test set.",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 44,
      "context" : "OpenWikiSQL is an open-domain Text2SQL QA dataset constructed from the original WikiSQL (Zhong et al., 2017).",
      "startOffset" : 88,
      "endOffset" : 108
    }, {
      "referenceID" : 13,
      "context" : "FiD(T5-base & T5-large) is reported from (Izacard and Grave, 2020), IR+CR (Iterative Retrieval+Cross-block Reader) and FR+CR (Fusion Retrieval+Cross-block Reader) are from (Chen et al.",
      "startOffset" : 41,
      "endOffset" : 66
    }, {
      "referenceID" : 3,
      "context" : "FiD(T5-base & T5-large) is reported from (Izacard and Grave, 2020), IR+CR (Iterative Retrieval+Cross-block Reader) and FR+CR (Fusion Retrieval+Cross-block Reader) are from (Chen et al., 2020a), Unified Model is from (Oguz et al.",
      "startOffset" : 172,
      "endOffset" : 192
    }, {
      "referenceID" : 34,
      "context" : "and split the articles into overlapping passages of 100 words following (Wang et al., 2019).",
      "startOffset" : 72,
      "endOffset" : 91
    }, {
      "referenceID" : 0,
      "context" : "6M Wikipedia tables (Bhagavatula et al., 2015) and all the 24,241 WikiSQL tables, and flatten and split each table into passages not exceeding 100 words, in the same format mentioned in the previous section.",
      "startOffset" : 20,
      "endOffset" : 46
    }, {
      "referenceID" : 25,
      "context" : "ize the fusion-in-decoders with the pretrained T5 model (Raffel et al., 2020).",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 15,
      "context" : "co/ For both reranker and FiD models, we use Adam optimizer (Kingma and Ba, 2014) with a maximum learning rate of 10 4 and a dropout rate of 10%.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 14,
      "context" : "For example, instead of using BM25, we can use more powerful neural retrieval models (Karpukhin et al., 2020).",
      "startOffset" : 85,
      "endOffset" : 109
    }, {
      "referenceID" : 3,
      "context" : "the tables and passages (Chen et al., 2020a) and utilize the structure information for better multi-",
      "startOffset" : 24,
      "endOffset" : 44
    }, {
      "referenceID" : 20,
      "context" : "Given the limited Text2SQL data and the difficulty of obtaining such SQL supervision, two interesting future work include (1) getting SQL annotations more efficiently and (2) adapting weaklysupervised approaches like discrete EM (Min et al., 2019) for model training.",
      "startOffset" : 229,
      "endOffset" : 247
    } ],
    "year" : 2021,
    "abstractText" : "The current state-of-the-art generative models for open-domain question answering (ODQA) have focused on generating direct answers from unstructured textual information. However, a large amount of world’s knowledge is stored in structured databases, and need to be accessed using query languages such as SQL. Furthermore, query languages can answer questions that require complex reasoning, as well as offering full explainability. In this paper, we propose a hybrid framework that takes both textual and tabular evidence as input and generates either direct answers or SQL queries depending on which form could better answer the question. The generated SQL queries can then be executed on the associated databases to obtain the final answers. To the best of our knowledge, this is the first paper that applies Text2SQL to ODQA tasks. Empirically, we demonstrate that on several ODQA datasets, the hybrid methods consistently outperforms the baseline models that only take homogeneous input by a large margin. Specifically we achieve state-of-theart performance on OpenSQuAD dataset using a T5-base model. In a detailed analysis, we demonstrate that the being able to generate structural SQL queries can always bring gains, especially for those questions that requires complex reasoning.",
    "creator" : "Preview"
  }
}