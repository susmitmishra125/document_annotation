{
  "name" : "2021.acl-long.28.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Multimodal Sentiment Detection Based on Multi-channel Graph Neural Networks",
    "authors" : [ "Xiaocui Yang", "Shi Feng", "Yifei Zhang", "Daling Wang" ],
    "emails" : [ "yangxiaocui@stumail.neu.edu.cn,", "zhangyifei}@cse.neu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 328–339\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n328"
    }, {
      "heading" : "1 Introduction",
      "text" : "The tasks of extracting and analyzing sentiments embedded in data have attracted substantial attention from both academic and industrial communities (Zhang et al., 2018; Yue et al., 2018). With the increased use of smartphones and the bloom of social media such as Twitter, Tumblr and Weibo, users can post multimodal tweets (e.g., text, image, and video) about diverse events and topics to convey their feelings and emotions. Therefore, multimodal sentiment analysis has become a popular research topic in recent years (Kaur and Kautish, 2019; Soleymani et al., 2017). As shown in Fig. 1, sentiment is no longer expressed by a pure modality in the multimodal scenario but rather by the com-\nbined expressions of multiple modalities (e.g., text, image, etc.). In contrast to unimodal data, multimodal data consist of more information and make the user’s expression more vivid and interesting.\nWe focus on multimodal sentiment detection for image-text pairs in social media posts. The problem of image-text mismatch and flaws in social media data, such as informality, typos, and a lack of punctuation, pose a fundamental challenge for the effective representation of multimodal data for the sentiment detection task. To tackle this challenge, Xu et al. (2017; 2017) constructed different networks for multimodal sentiment analysis, such as a Hierarchical Semantic Attentional Network (HSAN) and a Multimodal Deep Semantic Network (MDSN). Xu et al. (2018) and Yang et al. (2020) proposed a Co-Memory network (Co-Mem) and a Multi-view Attentional Network (MVAN) models, respectively, introducing memory networks to realize the interaction between modalities.\nThe above methods treat each image-text post in the dataset as a single instance, and feature dependencies across instances are neglected or modeled implicitly. In fact, social media posts have specific global co-occurring characteristics, i.e., co-\noccurring words, objects, or scenes, which tend to share similar sentiment orientations and emotions. For example, the co-occurrences of the words “have a fun/nice day” and of the bright scenes “ocean/beach” in the two images in Fig. 1 imply a strong relationship between these features and positive sentiment. How to more effectively make use of the feature co-occurrences across instances and capture the global characteristics of the data remain a great challenge.\nWe propose a Multi-channel Graph Neural Networks model with Sentiment-awareness (MGNNS) for multimodal sentiment analysis that consists of three stages.\n(i) Feature extraction. For text modality, we encode the text and obtain a text memory bank; for image modality, we first extract objects and scenes and then capture the image’ semantic features from a multiview perspective.\n(ii) Feature representation. We employ a Graph Neural Network (GNN) for text modality based on the global shared matrices, i.e., one text graph based on word co-occurrence is built based on the whole dataset. Specifically, we first connect word nodes within an appropriate small window in the text. After that, we update the node representation by itself as well as neighbor nodes. For image modality, it is believed that different views of an image, such as the beach (Scene view) and person (Object view) in Fig. 1(a), can reflect a user’s emotions (Xu and Mao, 2017). The existing literature usually models the relationship between the scenes and objects within an image, failing to capture the rich co-occurrence information from the perspective of the whole dataset. In contrast, we explicitly build two graphs for scenes and objects according to the co-occurrences in the datasets and propose Graph Convolutional Network (GCN) models over the two graphs to represent the images. In general, to tackle the isolated feature problem, we build multiple graphs for different modalities, with each GNN acting as a channel, and propose a Multi-channel Graph Neural Networks (MultiGNN) module to capture the in-depth global characteristics of the data. This multi-channel based method can provide complementary representation from different sources (George and Marcel, 2021; George et al., 2019; Islam et al., 2019).\n(iii) Feature fusion. Previous studies usually directly connect multimodal representations, without considering multimodal interactions (Wang et al.,\n2020a; Xu, 2017; Xu and Mao, 2017). In this stage, we realize the pairwise interaction of text and image modalities from different channels through the use of the Multimodal Multi-head Attention Interaction (MMAI) module and obtain the fusion representation.\nOur main contributions are summarized as follows:\n• We propose a novel MGNNS framework that models the global characteristics of the dataset to handle the multimodal sentiment detection task. To the best of our knowledge, we are the first to apply GNN to the image-text multimodal sentiment detection task.\n• We construct the MMAI module from different channels to realize in-depth multimodal interaction.\n• We conduct extensive experiments on three publicly available datasets, and the results show that our model outperforms the stateof-the-art methods."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Multimodal Sentiment Analysis",
      "text" : "For convenience, multimodal polarity analysis and emotion analysis are unified to form multimodal sentiment analysis. Traditional machine learning methods are adopted to address the multimodal sentiment analysis task (Pérez-Rosas et al., 2013; You et al., 2016). Recently, deep learning models have also achieved promising results for this task. For the video dataset, Wang et al. (2020b) proposed a novel method, TransModality, to fuse multimodal features with end-to-end translation models; Zhang et al. (2020) leveraged semi-supervised variational autoencoders to mine more information from unlabeled data; and Hazarika et al. (2020) constructed a novel framework, MISA, which projects each modality to two distinct subspaces: modalityinvariant and modality-specific subspaces. There is a massive amount image-text data on social platforms, and thus, image-text multimodal sentiment analysis has attracted the attention of many researchers. Xu et al. constructed different networks for multimodal sentiment analysis—HSAN (2017), MDSN (2017) and Co-Mem (2018). Yang et al. (2020) built an image-text emotion dataset, named TumEmo, and further proposed MVAN for multimodal emotion analysis."
    }, {
      "heading" : "2.2 Graph Neural Network",
      "text" : "The Graph Neural Network has achieved promising results for text classification, multi-label recognition, and multimodal tasks. For text classification, a novel neural network called Graph Neural Network (GNN), and its variants have been rapidly developed, and their performance is better than that of traditional methods, such as Text GCN (Yao et al., 2019), TensorGCN (Liu et al., 2020), and TextLevelGNN (Huang et al., 2019). The GCN is also introduced in the multi-label image recognition task to model the label dependencies (Chen et al., 2019).\nRecently, Graph Convolutional Network has been applied in different multimodal tasks, such as Visual Dialog (Guo et al., 2020; Khademi, 2020), multimodal fake news detection (Wang et al., 2020a), and Visual Question Answering (VQA) (Hudson and Manning, 2019; Khademi, 2020). Jiang et al. (2020) applied a novel KnowledgeBridge Graph Network (KBGN) in modeling the relations among the visual dialogue cross-modal information in fine granularity. Wang et al. (2020a) proposed a novel Knowledge-driven Multimodal Graph Convolutional Network (KMGCN) to model semantic representations for fake news detection. However, the KMGCN extracted visual words as visual information and did not make full use of the global information of the image. Khademi (2020) introduced a new neural network architecture, a Multimodal Neural Graph Memory Network (MNGMN), for VQA, which model constructed a visual graph network based on the bounding-boxes, which produced overlapping parts that might provide redundant information.\nFor the image-text dataset, we found that certain words often appear in a text post simultaneously, and different objects or scenes within an image have specific co-occurrences that indicate certain sentiments. We explicitly model these global characteristics of the dataset through the use of a multichannel GNN."
    }, {
      "heading" : "3 Proposed Model",
      "text" : "Fig. 2 illustrates the overall architecture of our proposed MGNNS model for multimodal sentiment detection that consists of three modules: the encoding module, the Multi-GNN module, and the multimodal interaction module. We first encode text and image input into hidden representations. Then, we introduce GNN from different channels\nto learn multiple modal representations. In this paper, the channels are the Text-GNN (TG) module, the Image-GCN-Scene (IGS) module, and the Image-GCN-Object (IGO) module. Finally, we realize the in-depth interactions between different modalities by multimodal multi-head attention."
    }, {
      "heading" : "3.1 Problem Formalization",
      "text" : "The goal of our model is to identify which sentiment is expressed by an image-text post. Given a set of multimodal posts from social media, P = {(T1, V1), ..., (TN , VN )}, where Ti is the text modality and Vi is the corresponding visual information, N represents the number of posts. We need to learn the model f : P → L to classify each post (Ti, Vi) into the predefined categories Li. For polarity classification, Li ∈ {Positive,Neutral,Negative}; for emotion classification, Li ∈ {Angry, Bored, Calm, Fear, Happy, Love, Sad }."
    }, {
      "heading" : "3.2 Encoding",
      "text" : "For text modality, we first encode words by GloVe (Pennington et al., 2014) to obtain the embedding vector and then obtain the text memory bank, M t, by BiGRU (Cho et al., 2014):\nM t = fBiGRU (Embedding(T )),M t ∈ RLt×2dt , (1) where T is a text sequence, Lt is the maximum length of a padded text sequence, and dt is the dimension of hidden units in the BiGRU.\nFor image modality, we extract image features from both the object and scene views to capture sufficient information. We believe that there are interdependencies between different objects or scenes in an image. To explicitly model this co-occurrence, we first extract objects O = {o1, ..., olo} by YOLOv3 (Farhadi and Redmon, 2018), and extract scenes S = {s1, ..., sls} by VGG-Place (Zhou et al., 2017). Finally, we obtain the object and scene memory banks with the pretrained ResNet (He et al., 2016). Thus, if an input image V has a 448×448 resolution and is split into 14×14 = 196 visual blocks of the same size, then each block is represented by a 2,048-dimensional vector.\nMx = fxResNet(V ),M x ∈ RLx×dx , (2)\nwhere x ∈ {Object, Scene}, Lx = 196, and dx = 2, 048."
    }, {
      "heading" : "3.3 Multi-channel Graph Neural Networks",
      "text" : "In this subsection, we present our proposed MultiGNN module. As Fig. 2 shows, this module consists of the TG channel (middle), the IGO channel (right), and the IGS channel (left).\nText GNN: As shown in the middle of Fig. 2, motivated by (Huang et al., 2019), we learn text representation through the Text Level GNN. For text with lt words T = {w1, ..., wk, ..., wlt}, where the kth word, wk, is initialized by glove embedding rtk ∈ Rd, d = 300. We build the graph of the textbased vocabulary of the training dataset, which is defined as follows:\nN t = {wk|k ∈ [1, lt]}. (3)\nWe build edges between wk and wj when the number of co-occurrences of two words is not less than 2.\nEt = {etk,j |wk ∈ [w1, wlt ];wj ∈ [wk−ws, wk+ws]}, (4) where N t and Et are the set of nodes and edges of the text graph, respectively. The word representations in N t and the edge weights in Et are taken from global shared matrices built based on vocabulary and the edge set of the dataset, respectively. That is, the representations of the same nodes and weights of the edges are shared globally. etk,j is\ninitialized by point-wise mutual information (PMI) (Wang et al., 2020a) and is learned in the training process. ws is the hyperparameter sliding window size, which indicates how many adjacent nodes are connected to each word in the text graph.\nThen, we update the node representation based on its original representations and neighboring nodes by the message passing mechanism (MPM) (Gilmer et al., 2017), which is defined as follows:\nAtk = max j∈Nwsk etkjr t k, (5) rtk ′ = αrtk + (1− α)Atk, (6)\nwhere Atk ∈ Rd is the aggregated information from neighboring nodes from node k−ws to k+ws, and max is the reduction function. α is the trainable variable that indicates how much original information of the node should be kept, and rtk ′ ∈ Rd is the updated representation of node k. Finally, we can calculate the new representation of text T as follows:\nT ′ = lt∑ k=1 rtk ′\n(7)\nImage GCN: In this module, we explicitly model interdependence within lx scenes or objects by IGX, as shown on the left and right sides of Fig.\n2, respectively. The graph of the image is defined as follows:\nNx = {xp|p ∈ [1, lx]}, (8)\nwhere Nx ∈ RCx is the set of nodes of IGX; x orX ∈ {Object, Scene}, Cx = 80 when x = Object, and Cx = 365 when x = Scene.\nTo build the edges of IGX, we first build the global shared co-occurrence matrix-based dataset:\nEx = {exp,q|p ∈ [1, lx] , q ∈ [1, lx]}, (9)\nwhere Ex ∈ RCx×Cx is the co-occurrence matrix; edge weight exp,q indicates the co-occurrence times of xp and xq in the dataset.\nThen, we calculate the conditional probability for node p as follows:\nP xp,q = e x p,q/N x p , when q 6= p (10)\nwhere Nxp denotes the occurrence times of xp in the dataset. Note that P xp,q 6= P xq,p.\nAs mentioned by (Chen et al., 2019), the simple correlation above may suffer several drawbacks. We further build the binary co-occurrence matrix:\nBxp,q = { 1, if P xp,q ≥ β 0, if P xp,q ≤ β , (11)\nwhere β is the hyperparameter used to filter noisy edges.\nIt is obvious that the role of the central node is different from that of neighboring nodes, so we need to further calculate the weight of the edge:\nRxp,q = { 1− γ, if p = q γ/ ∑Cx q=1B x p,q, if p 6= q , (12)\nwhere Rx ∈ RCx×Cx is the weighted cooccurrence matrix, and hyperparameter γ indicates the importance of neighboring nodes.\nFinally, we input node Nx and edge Rx of the image into the graph convolutional network. Like in (Kipf and Welling, 2016), every layer can be calculated as follows:\nHxL+1 = h(R̂ xHxLW x L), (13)\nwhere HxL ∈ RC x×dx , HxL+1 ∈ RC x×dx ′ , W xL ∈ Rdx×dx ′ , and R̂x ∈ RCx×Cx is the normalized representation of Rx; h(·) is a non-linear operation. When L = 1, Hx1 is the word-embedding vector of Nx.\nBy stacking multiple GCN layers, we can explicitly learn and model the complex interdependence of the nodes. Then, we obtain the image representation with objects or scenes dependencies:\nIx =MaxPooling(Mx)(HxL+1) T, Ix ∈ RCx .\n(14) But, we cannot capture the relationship between nodes and sentiments. Therefore, we learn the sentiment-awareness image representation through multi-head attention (Vaswani et al., 2017).\nAtt = softmax(QK T\n√ dk )V, (15)\nEIx =MH(Q,K, V )\n= Concat(head1, ..., headH)W O\nwhere headh = Att(QW Q h ,KW K h , V W V h ), (16)\nwhere MH(·) is multi-head attention; WQh ∈ Rd×dk , WKh ∈ Rdmodel×dk , W Vh ∈ Rdmodel×dv , and WO ∈ RHdv×d; and H = 5, dmodel = 300, dk = dv = 60. Q ∈ Rl\ns×d is a sentiment embedding matrix built based on the label set ls = 3 for polarity classification and ls = 7 for emotion classification; K = V = IxW I ,W I ∈ RCx×dmodel ,K, V ∈ Rdmodel ."
    }, {
      "heading" : "3.4 Multimodal Interaction",
      "text" : "Motivated by the Transformer (Vaswani et al., 2017) prototype, we design a Multimodal Multihead Attention Interaction (MMAI) module that can effectively learn the interaction between text\nmodality and image modality by multiple channels, as shown in Fig. 3.\nWe employ the MMAI to obtain the Text guided Image-X representations and Image-X guided Text representations, X ∈ {Object, Scene}. For the Text-guided Image-X attention,\nOTgXN+1 = LN(MH(Q = H TgX N ,K = V =M x)\n+HTgXN ), (17)\nHTgXN+1 = LN(FFN(O TgX N+1) +O TgX N+1), (18)\nwhere LN(·) is layer normalization, and FFN(·) is the feed-forward network. When N = 1, HTgX1 = T\n′, as in Eq. 7. For the Image-X-guided Text attention,\nOXgTN+1 = LN(MH(Q = H XgT N ,K = V =M t)\n+HXgTN ), (19)\nHXgTN+1 = LN(FFN(O XgT N+1) +O XgT N+1), (20)\nwhen N = 1, HXgT1 = EI x, as in Eq. 16. For MH , H = 4, dmodel = 512, dk = dv = 128. The fused multimodal representation is as follows: Rm = [HTgON ⊕H TgS N ⊕H OgT N ⊕H SgT N ], where ⊕ is a concatenation operation."
    }, {
      "heading" : "3.5 Sentiment Detection",
      "text" : "Finally, we feed the above fused representation, Rm, into the top fully connected layer and employ the softmax function for sentiment detection.\nLm = softmax(wsRm + bs), Lm ∈ Rls , (21)\nwhere ws and bs are the parameters of the fully connected layer."
    }, {
      "heading" : "4 Experiments",
      "text" : "We conduct experiments on three multimodal sentiment datasets from social media platforms, MVSASingle, MVSA-Multiple (Niu et al., 2016), and TumEmo (Yang et al., 2020), and compare our MGNNS model with a number of unimodal and multimodal approaches."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "MVSA-Single and MVSA-Multiple are two different scale image-text sentiment datasets crawled from Twitter1. TumEmo is a multimodal weaksupervision emotion dataset containing a large\n1https://twitter.com\namount of image-text data crawled from Tumblr2. The statistics of these datasets are given in Appendix A; and for a fair comparison, we adopt the same data preprocessing method as that of Yang (Yang et al., 2020). The corresponding details are shown in Appendix B."
    }, {
      "heading" : "4.2 Experimental Setup",
      "text" : "We adopt the cross-entropy loss function and Adam optimizer. In the process of extracting objects and scenes, we reserve the objects with the probability greater than 0.5 and the top-5 scenes, respectively. The other parameters are listed in Table 2, ∗ ∈ {Single,Multiple}. We use Accuracy (Acc) and F1-score (F1) as evaluation metrics. All models are implemented with PyTorch."
    }, {
      "heading" : "4.3 Baselines",
      "text" : "We compare our model with multimodal sentiment models with the same modalities and the unimodal baseline models.\nUnimodal Baselines: For text modality, CNN (Kim, 2014) and Bi-LSTM (Zhou et al., 2016) are well-known models for text classification tasks, and BiACNN (Lai et al., 2015) incorporates the CNN and BiLSTM models with an attention mechanism for text sentiment analysis. TGNN (Huang et al., 2019) is a text-level graph neural network for text classification. For image modality, OSDA (Yang\n2http://tumblr.com\net al., 2020) is an image sentiment analysis model based on multiple views. Note that the SGN, OGN, and DuIG are variants of our model and rely only on image modality. SGN and OGN are the image graph convolutional neural networks based on scenes and objects for image sentiment analysis, respectively. DuIG is the image graph convolutional neural network with dual views, e.g., Object and Scene.\nMuiltimodal Baselines: HSAN (Xu, 2017) is a hierarchical semantic attentional network based on image captions for multimodal sentiment analysis. MDSN (Xu and Mao, 2017) is a deep semantic network with attention for multimodal sentiment analysis. Co-Mem (Xu et al., 2018) is a co-memory network for iteratively modeling the interactions between multiple modalities. MVAN (Yang et al., 2020) is a multi-view attentional network that utilizes a memory network for multimodal emotion analysis. This model achieves state-of-the-art performance on image-text multimodal sentiment classification tasks."
    }, {
      "heading" : "4.4 Experimental Results and Analysis",
      "text" : "The experimental results of the baseline methods and our model are shown in Table 3, where MGNNS denotes that our model is based on multichannel graph neural networks3.\nWe can make the following observations. First,\n3The source codes are available for use at https:// github.com/YangXiaocui1215/MGNNS.\nour model (MGNNS) is competitive with the other strong baseline models on the three datasets. Note that the data distribution of MVSA-∗ is extremely unbalanced. Thus, we reproduce the MVAN model with ACC and Weighted-F1 metrics instead of the Micro-F1 metric used in the original paper, which is more realistic. Second, the multimodal sentiment analysis models perform better than most of the unimodal sentiment analysis models on all three datasets. Moreover, the segmental indictors are difficult to capture for images owing to the low information density, and the sentiment analysis on the image modality achieves the worst results. Finally, the TGNN unimodal model outperforms the HSAN multimodal model, indicating that the GNN has excellent performance in sentiment analysis."
    }, {
      "heading" : "4.5 Ablation Experiments",
      "text" : "We conduct ablation experiments on the MGNNS model to demonstrate the effectiveness of different modules. Table 4 shows that the whole MGNNS model achieves the best performance among all models. To show the performance of the MultiGNN module, we replace the Text-GNN with the CNN, as well as the Image-GCN with the pretrained ResNet. The removal of the MMAI module (w/o MMAI) and Multi-GNN module (w/o MGNN) adversely affect the model results, which indicates that these modules are useful for multimodal sentiment analysis. By replacing the MMAI module with the CoAtt (Lu et al., 2016) module\n(+CoAtt), the model performance is found to be slightly worse than that of the MGNNS module. This further illustrates the importance of multimodal interactions and the superiority of the MMAI module. When one of the object views (w/o Object) or scene views (w/o Scene) is removed, the performance of the model declines, which indicates that both views of the image are effective for multimodal sentiment analysis."
    }, {
      "heading" : "4.6 Transferability Experiment",
      "text" : "In the Multi-GNN module, we build multiple graphs for different modalities based on the dataset. For different datasets, the graphs built by the unimodal model are different. However, can graph capture from one dataset (e.g., MVSA-Single) have positive effects on other datasets (e.g., TumEmo)? In this subsection, we will verify the transferability of the model through experiments.\nAs Table 5 shows, the following conclusions can be drawn: (i) Regardless of the modality, such as text or image, compared to introducing the graph constructed based on own dataset, the experimental results calculated based on graphs transferred from other datasets are worse. This is mainly because each dataset has unique global characteristics, the experimental results based on transferred graphs are slightly worse. (ii) However, due to\nthe commonality of datasets when expressing the same emotions, the results of the transferred models are not completely worse. For example, the same scenes and objects can appear in different images in different datasets simultaneously for image modalities. Therefore, graphs from different datasets have transferability and can be used for other datasets. (iii) For different datasets, the experimental results of “X2Y-Text” are worse than those of “X2Y-Image”. That is, the text graph has worse transferability. The reason for this may be that text graphs with various nodes are created based on the vocabulary of different datasets. Two situations in the transferred text graph will seriously affect the results: fewer nodes will lose information, and more nodes will provide redundant information. (iv) When the dataset gap is relatively wide, the transferability of text graphs is worse. For example, from the larger datasets transfer to the smallest dataset, including T2S-Text and M2S-Text, experimental results show a drop of 2.45% and 2.69%, respectively; from the smaller datasets transfer to the most largest dataset, including S2T-Text and M2T-Text, experimental results show a significant drop of 4.81% and 4.09%, respectively."
    }, {
      "heading" : "4.7 Hyperparameter Settings",
      "text" : "Hyperparameter ws: To obtain adequate information from neighboring nodes in the TGNN, we conduct experiments under different settings for hyperparameter ws in Eq. 4, the related results of which are shown in Fig. 4. The best ws selection varies among different datasets since the average text length of TumEmo is longer compared to other data. The TGNN cannot obtain sufficient information from neighboring nodes with ws values that are too small, while larger values may degrade the performance due to the redundant information provided by neighboring nodes.\n(a) Comparisons on MVSA-∗ (b) Comparisons on TumEmo\nFigure 4: Acc comparisons with different values of ws. MS is MVSA-Single, MM is MVSA-Multiple, and T is TumEmo.\nHyperparameter β: We vary the values of hyperparameter β in Eq. 11 for the binary cooccurrence matrix from different views, the results of which are shown in Fig. 5. We find that the best β value is different for different views in different datasets. For MVSA-∗, the smaller β value can reserve more edges to capture more information since the scene co-occurrence matrix is sparser than that in the object view. For TumEmo with a large amount of data, preserving the top-5 scenes produces many noise edges, so the value of scene-β is greater than that of MVSA-∗.\nβ\n(a) Comparisons of cbject view on MVSA-∗\nβ\n(b) Comparisons of scene view on MVSA-∗\nβ\n(c) Comparisons of object view on TumEmo\nβ\n(d) Comparisons of scene view on TumEmo\nFigure 5: Acc comparisons with different β values.\nHyperparameter γ: As Fig. 6 shows, the model receives the best performance for the three datasets when γ is 0.2. When γ is smaller, the neighboring nodes do not receive enough attention; in contrast, their own information is not fully uti-\nlized.\nγ\n(a) Comparisons on MVSA-∗\nγ\n(b) Comparisons on TumEmo\nFigure 6: Acc comparisons with different γ values."
    }, {
      "heading" : "5 Conclusions",
      "text" : "This paper proposes a novel model, MGNNS, that is built based on the global characteristics of the dataset for multimodal sentiment detection tasks. As far as we know, this is the first application of graph neural networks in image-text multimodal sentiment analysis. The experimental results on publicly available datasets demonstrated that our proposed model is competitive with strong baseline models.\nIn future work, we plan to construct a model that adopts the advantages of the GNN and pretrained models such as BERT, VisualBERT, and etc. We want to design a reasonable algorithm to characterize the quality of the objects and scenes selected from the image and further improve the representation ability of the model."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The project is supported by the National Key R&D Program of China (2018YFB1004700) and by the National Natural Science Foundation of China (61772122, 61872074, U1811261)."
    }, {
      "heading" : "A Dataset",
      "text" : "A.1 MVSA-Single and MVSA-Multiple\nThe statistics for the MVSA-Simple and MVSAMultiple datasets are listed in Table 1, showing that the various categories are highly unbalanced. MVSA-Single and MVSA-Multiple have different data distributions.\nA.2 TumEmo The statistics for the TumEmo dataset are listed in Table 2, containing a large number of image-text posts labeled by emotion."
    }, {
      "heading" : "B Preprocessing Data",
      "text" : "The text data contain many useless characters for sentiment analysis, such as URLs, stopwords, and punctuation. We need to preprocess text data to enhance the effectiveness of multimodal emotion detection. We perform data preprocessing as follows:\n• remove the “URL”, as in“http://...”;\n• remove the stopwords, such as “a, an, the, and etc. ”;\n• remove the useless punctuation, including periods, commas, semicolons, etc;\n• remove the hashtag and its content (#content); In particular, the TumEmo dataset uses #emotion as a weakly supervised label.\n• remove the posts for which the text length is less than 3."
    } ],
    "references" : [ {
      "title" : "Multi-label image recognition with graph convolutional networks",
      "author" : [ "Zhao-Min Chen", "Xiu-Shen Wei", "Peng Wang", "Yanwen Guo." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5177–5186.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning phrase representations using rnn encoder–decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Yolov3: An incremental improvement",
      "author" : [ "Ali Farhadi", "Joseph Redmon." ],
      "venue" : "Computer Vision and Pattern Recognition, cite as.",
      "citeRegEx" : "Farhadi and Redmon.,? 2018",
      "shortCiteRegEx" : "Farhadi and Redmon.",
      "year" : 2018
    }, {
      "title" : "Learning one class representations for face presentation attack detection using multi-channel convolutional neural networks",
      "author" : [ "Anjith George", "Sebastien Marcel." ],
      "venue" : "IEEE Transactions on Information Forensics and Security, 16:361–375.",
      "citeRegEx" : "George and Marcel.,? 2021",
      "shortCiteRegEx" : "George and Marcel.",
      "year" : 2021
    }, {
      "title" : "Biometric face presentation attack detection with multi-channel convolutional neural network",
      "author" : [ "Anjith George", "Zohreh Mostaani", "David Geissenbuhler", "Olegs Nikisins", "André Anjos", "Sébastien Marcel." ],
      "venue" : "IEEE Transactions on Information Forensics",
      "citeRegEx" : "George et al\\.,? 2019",
      "shortCiteRegEx" : "George et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural message passing for quantum chemistry",
      "author" : [ "Justin Gilmer", "Samuel S. Schoenholz", "Patrick F. Riley", "Oriol Vinyals", "George E. Dahl." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning - Volume 70, pages 1263–1272.",
      "citeRegEx" : "Gilmer et al\\.,? 2017",
      "shortCiteRegEx" : "Gilmer et al\\.",
      "year" : 2017
    }, {
      "title" : "Iterative contextaware graph inference for visual dialog",
      "author" : [ "Dan Guo", "Hui Wang", "Hanwang Zhang", "Zheng-Jun Zha", "Meng Wang." ],
      "venue" : "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10055–10064.",
      "citeRegEx" : "Guo et al\\.,? 2020",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2020
    }, {
      "title" : "Misa: Modality-invariant and -specific representations for multimodal sentiment analysis",
      "author" : [ "Devamanyu Hazarika", "Roger Zimmermann", "Soujanya Poria." ],
      "venue" : "Proceedings of the 28th ACM International Conference on Multimedia, pages 1122–1131.",
      "citeRegEx" : "Hazarika et al\\.,? 2020",
      "shortCiteRegEx" : "Hazarika et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Text level graph neural network for text classification",
      "author" : [ "Lianzhe Huang", "Dehong Ma", "Sujian Li", "Xiaodong Zhang", "Houfeng Wang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
      "citeRegEx" : "Huang et al\\.,? 2019",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "Gqa: A new dataset for real-world visual reasoning and compositional question answering",
      "author" : [ "Drew A. Hudson", "Christopher D. Manning." ],
      "venue" : "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6700–6709.",
      "citeRegEx" : "Hudson and Manning.,? 2019",
      "shortCiteRegEx" : "Hudson and Manning.",
      "year" : 2019
    }, {
      "title" : "Multi-channel convolutional neural network for twitter emotion and sentiment recognition",
      "author" : [ "Jumayel Islam", "Robert E Mercer", "Lu Xiao." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Islam et al\\.,? 2019",
      "shortCiteRegEx" : "Islam et al\\.",
      "year" : 2019
    }, {
      "title" : "Kbgn: Knowledge-bridge graph network for adaptive vision-text reasoning in visual dialogue",
      "author" : [ "Xiaoze Jiang", "Siyi Du", "Zengchang Qin", "Yajing Sun", "Jing Yu." ],
      "venue" : "Proceedings of the 28th ACM International Conference on Multimedia, pages 1265–1273.",
      "citeRegEx" : "Jiang et al\\.,? 2020",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Multimodal sentiment analysis: A survey and comparison",
      "author" : [ "Ramandeep Kaur", "Sandeep Kautish." ],
      "venue" : "International Journal of Service Science, Management, Engineering, and Technology (IJSSMET), 10(2):38–58.",
      "citeRegEx" : "Kaur and Kautish.,? 2019",
      "shortCiteRegEx" : "Kaur and Kautish.",
      "year" : 2019
    }, {
      "title" : "Multimodal neural graph memory networks for visual question answering",
      "author" : [ "Mahmoud Khademi." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7177– 7188.",
      "citeRegEx" : "Khademi.,? 2020",
      "shortCiteRegEx" : "Khademi.",
      "year" : 2020
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751.",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas N. Kipf", "Max Welling." ],
      "venue" : "ICLR (Poster).",
      "citeRegEx" : "Kipf and Welling.,? 2016",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2016
    }, {
      "title" : "Recurrent convolutional neural networks for text classification",
      "author" : [ "Siwei Lai", "Liheng Xu", "Kang Liu", "Jun Zhao." ],
      "venue" : "Twenty-ninth AAAI conference on artificial intelligence.",
      "citeRegEx" : "Lai et al\\.,? 2015",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2015
    }, {
      "title" : "Tensor graph convolutional networks for text classification",
      "author" : [ "Xien Liu", "Xinxin You", "Xiao Zhang", "Ji Wu", "Ping Lv." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8409–8416.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Hierarchical question-image co-attention for visual question answering",
      "author" : [ "Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 29, pages 289–297.",
      "citeRegEx" : "Lu et al\\.,? 2016",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2016
    }, {
      "title" : "Sentiment analysis on multi-view social data",
      "author" : [ "Teng Niu", "Shiai Zhu", "Lei Pang", "Abdulmotaleb El Saddik." ],
      "venue" : "International Conference on Multimedia Modeling, pages 15–27. Springer.",
      "citeRegEx" : "Niu et al\\.,? 2016",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2016
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2014 conference",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Utterance-level multimodal sentiment analysis",
      "author" : [ "Verónica Pérez-Rosas", "Rada Mihalcea", "LouisPhilippe Morency." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 973–982.",
      "citeRegEx" : "Pérez.Rosas et al\\.,? 2013",
      "shortCiteRegEx" : "Pérez.Rosas et al\\.",
      "year" : 2013
    }, {
      "title" : "A survey of multimodal sentiment analysis",
      "author" : [ "Mohammad Soleymani", "David Garcia", "Brendan Jou", "Björn Schuller", "Shih-Fu Chang", "Maja Pantic." ],
      "venue" : "Image and Vision Computing, 65:3–14.",
      "citeRegEx" : "Soleymani et al\\.,? 2017",
      "shortCiteRegEx" : "Soleymani et al\\.",
      "year" : 2017
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of the 31st International Conference on Neural Information Processing Sys-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Fake news detection via knowledge-driven multimodal graph convolutional networks",
      "author" : [ "Youze Wang", "Shengsheng Qian", "Jun Hu", "Quan Fang", "Changsheng Xu." ],
      "venue" : "Proceedings of the 2020 International Conference on Multimedia Retrieval, pages",
      "citeRegEx" : "Wang et al\\.,? 2020a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Transmodality: An end2end fusion method with transformer for multimodal sentiment analysis",
      "author" : [ "Zilong Wang", "Zhaohong Wan", "Xiaojun Wan." ],
      "venue" : "Proceedings of The Web Conference 2020, pages 2514–2520.",
      "citeRegEx" : "Wang et al\\.,? 2020b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Analyzing multimodal public sentiment based on hierarchical semantic attentional network",
      "author" : [ "Nan Xu." ],
      "venue" : "2017 IEEE International Conference on Intelligence and Security Informatics (ISI), pages 152–154. IEEE.",
      "citeRegEx" : "Xu.,? 2017",
      "shortCiteRegEx" : "Xu.",
      "year" : 2017
    }, {
      "title" : "Multisentinet: A deep semantic network for multimodal sentiment analysis",
      "author" : [ "Nan Xu", "Wenji Mao." ],
      "venue" : "Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, pages 2399–2402. ACM.",
      "citeRegEx" : "Xu and Mao.,? 2017",
      "shortCiteRegEx" : "Xu and Mao.",
      "year" : 2017
    }, {
      "title" : "A comemory network for multimodal sentiment analysis",
      "author" : [ "Nan Xu", "Wenji Mao", "Guandan Chen." ],
      "venue" : "The 41st international ACM SIGIR conference on research & development in information retrieval, pages 929–932.",
      "citeRegEx" : "Xu et al\\.,? 2018",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2018
    }, {
      "title" : "Image-text multimodal emotion classification via multi-view attentional network",
      "author" : [ "Xiaocui Yang", "Shi Feng", "Daling Wang", "Yifei Zhang." ],
      "venue" : "IEEE Transactions on Multimedia.",
      "citeRegEx" : "Yang et al\\.,? 2020",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    }, {
      "title" : "Graph convolutional networks for text classification",
      "author" : [ "Liang Yao", "Chengsheng Mao", "Yuan Luo." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7370–7377.",
      "citeRegEx" : "Yao et al\\.,? 2019",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2019
    }, {
      "title" : "Cross-modality consistent regression for joint visual-textual sentiment analysis of social",
      "author" : [ "Quanzeng You", "Jiebo Luo", "Hailin Jin", "Jianchao Yang" ],
      "venue" : null,
      "citeRegEx" : "You et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "You et al\\.",
      "year" : 2016
    }, {
      "title" : "A survey of sentiment analysis in social media",
      "author" : [ "Lin Yue", "Weitong Chen", "Xue Li", "Wanli Zuo", "Minghao Yin." ],
      "venue" : "Knowledge and Information Systems, pages 1–47.",
      "citeRegEx" : "Yue et al\\.,? 2018",
      "shortCiteRegEx" : "Yue et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-modal sentiment classification with independent and interactive knowledge via semi-supervised learning",
      "author" : [ "Dong Zhang", "Shoushan Li", "Qiaoming Zhu", "Guodong Zhou." ],
      "venue" : "IEEE Access, 8:22945–22954.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep learning for sentiment analysis: A survey",
      "author" : [ "Lei Zhang", "Shuai Wang", "Bing Liu." ],
      "venue" : "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 8(4):e1253.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Places: A 10 million image database for scene recognition",
      "author" : [ "Bolei Zhou", "Agata Lapedriza", "Aditya Khosla", "Aude Oliva", "Antonio Torralba." ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence, 40(6):1452–1464.",
      "citeRegEx" : "Zhou et al\\.,? 2017",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2017
    }, {
      "title" : "Attention-based bidirectional long short-term memory networks for relation classification",
      "author" : [ "Peng Zhou", "Wei Shi", "Jun Tian", "Zhenyu Qi", "Bingchen Li", "Hongwei Hao", "Bo Xu." ],
      "venue" : "Proceedings of the 54th annual meeting of the association for computational",
      "citeRegEx" : "Zhou et al\\.,? 2016",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 35,
      "context" : "The tasks of extracting and analyzing sentiments embedded in data have attracted substantial attention from both academic and industrial communities (Zhang et al., 2018; Yue et al., 2018).",
      "startOffset" : 149,
      "endOffset" : 187
    }, {
      "referenceID" : 33,
      "context" : "The tasks of extracting and analyzing sentiments embedded in data have attracted substantial attention from both academic and industrial communities (Zhang et al., 2018; Yue et al., 2018).",
      "startOffset" : 149,
      "endOffset" : 187
    }, {
      "referenceID" : 13,
      "context" : "Therefore, multimodal sentiment analysis has become a popular research topic in recent years (Kaur and Kautish, 2019; Soleymani et al., 2017).",
      "startOffset" : 93,
      "endOffset" : 141
    }, {
      "referenceID" : 23,
      "context" : "Therefore, multimodal sentiment analysis has become a popular research topic in recent years (Kaur and Kautish, 2019; Soleymani et al., 2017).",
      "startOffset" : 93,
      "endOffset" : 141
    }, {
      "referenceID" : 28,
      "context" : "1(a), can reflect a user’s emotions (Xu and Mao, 2017).",
      "startOffset" : 36,
      "endOffset" : 54
    }, {
      "referenceID" : 3,
      "context" : "This multi-channel based method can provide complementary representation from different sources (George and Marcel, 2021; George et al., 2019; Islam et al., 2019).",
      "startOffset" : 96,
      "endOffset" : 162
    }, {
      "referenceID" : 4,
      "context" : "This multi-channel based method can provide complementary representation from different sources (George and Marcel, 2021; George et al., 2019; Islam et al., 2019).",
      "startOffset" : 96,
      "endOffset" : 162
    }, {
      "referenceID" : 11,
      "context" : "This multi-channel based method can provide complementary representation from different sources (George and Marcel, 2021; George et al., 2019; Islam et al., 2019).",
      "startOffset" : 96,
      "endOffset" : 162
    }, {
      "referenceID" : 25,
      "context" : "Previous studies usually directly connect multimodal representations, without considering multimodal interactions (Wang et al., 2020a; Xu, 2017; Xu and Mao, 2017).",
      "startOffset" : 114,
      "endOffset" : 162
    }, {
      "referenceID" : 27,
      "context" : "Previous studies usually directly connect multimodal representations, without considering multimodal interactions (Wang et al., 2020a; Xu, 2017; Xu and Mao, 2017).",
      "startOffset" : 114,
      "endOffset" : 162
    }, {
      "referenceID" : 28,
      "context" : "Previous studies usually directly connect multimodal representations, without considering multimodal interactions (Wang et al., 2020a; Xu, 2017; Xu and Mao, 2017).",
      "startOffset" : 114,
      "endOffset" : 162
    }, {
      "referenceID" : 22,
      "context" : "Traditional machine learning methods are adopted to address the multimodal sentiment analysis task (Pérez-Rosas et al., 2013; You et al., 2016).",
      "startOffset" : 99,
      "endOffset" : 143
    }, {
      "referenceID" : 32,
      "context" : "Traditional machine learning methods are adopted to address the multimodal sentiment analysis task (Pérez-Rosas et al., 2013; You et al., 2016).",
      "startOffset" : 99,
      "endOffset" : 143
    }, {
      "referenceID" : 31,
      "context" : "For text classification, a novel neural network called Graph Neural Network (GNN), and its variants have been rapidly developed, and their performance is better than that of traditional methods, such as Text GCN (Yao et al., 2019), TensorGCN (Liu et al.",
      "startOffset" : 212,
      "endOffset" : 230
    }, {
      "referenceID" : 18,
      "context" : ", 2019), TensorGCN (Liu et al., 2020), and TextLevelGNN (Huang et al.",
      "startOffset" : 19,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "The GCN is also introduced in the multi-label image recognition task to model the label dependencies (Chen et al., 2019).",
      "startOffset" : 101,
      "endOffset" : 120
    }, {
      "referenceID" : 6,
      "context" : "Recently, Graph Convolutional Network has been applied in different multimodal tasks, such as Visual Dialog (Guo et al., 2020; Khademi, 2020), multimodal fake news detection (Wang et al.",
      "startOffset" : 108,
      "endOffset" : 141
    }, {
      "referenceID" : 14,
      "context" : "Recently, Graph Convolutional Network has been applied in different multimodal tasks, such as Visual Dialog (Guo et al., 2020; Khademi, 2020), multimodal fake news detection (Wang et al.",
      "startOffset" : 108,
      "endOffset" : 141
    }, {
      "referenceID" : 25,
      "context" : ", 2020; Khademi, 2020), multimodal fake news detection (Wang et al., 2020a), and Visual Question Answering (VQA) (Hudson and Manning, 2019; Khademi, 2020).",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : ", 2020a), and Visual Question Answering (VQA) (Hudson and Manning, 2019; Khademi, 2020).",
      "startOffset" : 46,
      "endOffset" : 87
    }, {
      "referenceID" : 14,
      "context" : ", 2020a), and Visual Question Answering (VQA) (Hudson and Manning, 2019; Khademi, 2020).",
      "startOffset" : 46,
      "endOffset" : 87
    }, {
      "referenceID" : 21,
      "context" : "For text modality, we first encode words by GloVe (Pennington et al., 2014) to obtain the embedding vector and then obtain the text memory bank, M t, by BiGRU (Cho et al.",
      "startOffset" : 50,
      "endOffset" : 75
    }, {
      "referenceID" : 1,
      "context" : ", 2014) to obtain the embedding vector and then obtain the text memory bank, M t, by BiGRU (Cho et al., 2014):",
      "startOffset" : 91,
      "endOffset" : 109
    }, {
      "referenceID" : 2,
      "context" : ", olo} by YOLOv3 (Farhadi and Redmon, 2018), and extract scenes S = {s1, .",
      "startOffset" : 17,
      "endOffset" : 43
    }, {
      "referenceID" : 8,
      "context" : "Finally, we obtain the object and scene memory banks with the pretrained ResNet (He et al., 2016).",
      "startOffset" : 80,
      "endOffset" : 97
    }, {
      "referenceID" : 9,
      "context" : "2, motivated by (Huang et al., 2019), we learn text representation through the Text Level GNN.",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 25,
      "context" : "ek,j is initialized by point-wise mutual information (PMI) (Wang et al., 2020a) and is learned in the training process.",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 5,
      "context" : "Then, we update the node representation based on its original representations and neighboring nodes by the message passing mechanism (MPM) (Gilmer et al., 2017), which is defined as follows:",
      "startOffset" : 139,
      "endOffset" : 160
    }, {
      "referenceID" : 0,
      "context" : "As mentioned by (Chen et al., 2019), the simple correlation above may suffer several drawbacks.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 16,
      "context" : "Like in (Kipf and Welling, 2016), every layer can be calculated as follows:",
      "startOffset" : 8,
      "endOffset" : 32
    }, {
      "referenceID" : 24,
      "context" : "Therefore, we learn the sentiment-awareness image representation through multi-head attention (Vaswani et al., 2017).",
      "startOffset" : 94,
      "endOffset" : 116
    }, {
      "referenceID" : 24,
      "context" : "Motivated by the Transformer (Vaswani et al., 2017) prototype, we design a Multimodal Multihead Attention Interaction (MMAI) module that can effectively learn the interaction between text",
      "startOffset" : 29,
      "endOffset" : 51
    }, {
      "referenceID" : 20,
      "context" : "We conduct experiments on three multimodal sentiment datasets from social media platforms, MVSASingle, MVSA-Multiple (Niu et al., 2016), and TumEmo (Yang et al.",
      "startOffset" : 117,
      "endOffset" : 135
    }, {
      "referenceID" : 30,
      "context" : ", 2016), and TumEmo (Yang et al., 2020), and compare our MGNNS model with a number of unimodal and multimodal approaches.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 30,
      "context" : "The statistics of these datasets are given in Appendix A; and for a fair comparison, we adopt the same data preprocessing method as that of Yang (Yang et al., 2020).",
      "startOffset" : 145,
      "endOffset" : 164
    }, {
      "referenceID" : 15,
      "context" : "Unimodal Baselines: For text modality, CNN (Kim, 2014) and Bi-LSTM (Zhou et al.",
      "startOffset" : 43,
      "endOffset" : 54
    }, {
      "referenceID" : 37,
      "context" : "Unimodal Baselines: For text modality, CNN (Kim, 2014) and Bi-LSTM (Zhou et al., 2016) are well-known models for text classification tasks, and BiACNN (Lai et al.",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 17,
      "context" : ", 2016) are well-known models for text classification tasks, and BiACNN (Lai et al., 2015) incorporates the CNN and BiLSTM models with an attention mechanism for text sentiment analysis.",
      "startOffset" : 72,
      "endOffset" : 90
    }, {
      "referenceID" : 9,
      "context" : "TGNN (Huang et al., 2019) is a text-level graph neural network for text classification.",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 27,
      "context" : "Muiltimodal Baselines: HSAN (Xu, 2017) is a hierarchical semantic attentional network based on image captions for multimodal sentiment analysis.",
      "startOffset" : 28,
      "endOffset" : 38
    }, {
      "referenceID" : 28,
      "context" : "MDSN (Xu and Mao, 2017) is a deep semantic network with attention for multimodal sentiment analysis.",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 29,
      "context" : "Co-Mem (Xu et al., 2018) is a co-memory network for iteratively modeling the interactions between multiple modalities.",
      "startOffset" : 7,
      "endOffset" : 24
    }, {
      "referenceID" : 30,
      "context" : "MVAN (Yang et al., 2020) is a multi-view attentional network that utilizes a memory network for multimodal emotion analysis.",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 19,
      "context" : "By replacing the MMAI module with the CoAtt (Lu et al., 2016) module",
      "startOffset" : 44,
      "endOffset" : 61
    } ],
    "year" : 2021,
    "abstractText" : "With the popularity of smartphones, we have witnessed the rapid proliferation of multimodal posts on various social media platforms. We observe that the multimodal sentiment expression has specific global characteristics, such as the interdependencies of objects or scenes within the image. However, most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the dataset. In this paper, we propose Multi-channel Graph Neural Networks with Sentiment-awareness (MGNNS) for imagetext sentiment detection. Specifically, we first encode different modalities to capture hidden representations. Then, we introduce multichannel graph neural networks to learn multimodal representations based on the global characteristics of the dataset. Finally, we implement multimodal in-depth fusion with the multi-head attention mechanism to predict the sentiment of image-text pairs. Extensive experiments conducted on three publicly available datasets demonstrate the effectiveness of our approach for multimodal sentiment detection.",
    "creator" : "LaTeX with hyperref"
  }
}