{
  "name" : "2021.acl-long.189.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Discovering Dialogue Slots with Weak Supervision",
    "authors" : [ "Vojtěch Hudeček", "Ondřej Dušek", "Zhou Yu" ],
    "emails" : [ "hudecek@ufal.mff.cuni.cz,", "odusek@ufal.mff.cuni.cz,", "zy2461@columbia.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2430–2442\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2430\nOur model demonstrates state-of-the-art performance in slot tagging without labeled training data on four different dialogue domains. Moreover, we find that slot annotations discovered by our model significantly improve the performance of an end-to-end dialogue response generation model, compared to using no slot annotation at all."
    }, {
      "heading" : "1 Introduction",
      "text" : "Task-oriented dialogue systems typically use annotation based on slots to represent the meaning of user utterances (Young et al., 2013). Slots are attributes relevant to completing the task (e.g., price, food type, area). The sets of slots and their values typically need to be designed in advance by domain experts. Slots and their values are tracked over the course of the dialogue, forming dialogue state, which allows a dialogue system to plan the next actions effectively (Williams et al., 2013).\nGetting raw data for dialogue system training is not difficult, especially if we restrict the target domain. A requirement for dialogue state labels makes this process much more costly. However, both traditional pipeline systems (Young et al., 2013) and end-to-end task-oriented architectures\n(Wen et al., 2017) typically require such annotation. While some systems use implicit, latent state representation and do not require annotation (Serban et al., 2016), the behavior of such systems is hard to interpret or control. There are several works aiming at keeping interpretability and reducing the annotation needs by automating it (Chen et al., 2014, 2015) or transferring annotation across domains (Zhao and Eskenazi, 2018; Coope et al., 2020), but they still require significant manual effort.\nIn this paper, we present a novel approach to discovering a set of domain-relevant dialogue slots and their values given a set of dialogues in the target domain (such as transcripts from a call center). Our approach requires no manual annotation at all in order to tag slots in dialogue data. This substantially simplifies dialogue system design and training process, as the developer no longer needs to design a set of slots and annotate their occurrences in training data. We discover slots by using unsupervised clustering on top of annotation obtained by domain-independent generic models such as a semantic frame parser or a named entity recognizer (NER). To illustrate our approach, let us consider an example given in Figure 1.\nAlthough the annotation is descriptive, it contains concepts irrelevant for the domain under consideration. Our method selects only relevant slot candidates (depicted in blue). Slots discovered by our approach can then be used to design or adapt the database backend for the target domain.\nOur contributions can be summarized as follows:\n1. Selecting domain-relevant slots from candidates provided by weak supervision from domain-generic linguistic annotation tools. We use FrameNet-style (Fillmore, 1976) semantic frames as our main source of weak supervision.1 We also explore named entity recognition (NER).\n2. Training a standalone slot tagger for the selected slots. Based on the discovered slots, we train a slot tagger to annotate in-domain utterances. After it is trained, the slot tagger can be used as a standalone component – it does not need the original annotation tools for prediction, and is able to improve on their results.\n3. Evaluation on multiple domains. We show that our approach is domain-independent. We achieve state-of-the-art results for slot tagging without manual supervision in four different domains, with a 6-16% absolute F1 score increase over the previous benchmark.\n4. Downstream task application. We evaluate our approach in a full dialogue response generation task. Our slots can be directly used to perform dialogue state tracking by merging annotations from consecutive turns. We train an end-to-end neural dialogue system using our automatically discovered slots in the restaurant domain and demonstrate that our approach improves performance over an unsupervised model, finding the correct venue in 5% more cases (35% more when no restaurant ontology is provided).\nOur experimental code is available on GitHub.2\n1See http://framenet.icsi.berkeley.edu/ 2https://github.com/vojtsek/\njoint-induction"
    }, {
      "heading" : "2 Related Work",
      "text" : "The idea of using weak supervision to perform finegrained language understanding based on domainrelevant (slot-like) attributes was proposed by Heck and Hakkani-Tür (2012), who construct a triplebased database of entity relations based on web search. They exploit the structure of in-domain web pages to obtain semantic annotations. There are also similar works on relation detection (HakkaniTür et al., 2013) or entity extraction (Wang et al., 2014). This approach is, however, limited by requiring structured web pages as underlying data.\nChen et al. (2014) combine semantic frame parsing with word embeddings for weakly supervised semantic slot induction. Chen et al. (2015) also use semantic frames, construct lexical knowledge graphs and perform a random walk to get slot candidates. However, both approaches only output a ranking of potential slot candidates based on frames. Since frame annotation is very fine-grained, this produces a huge number of candidates, requiring their manual merging into slots for any practical use. In contrast, we determine domain-relevant slots automatically. Coope et al. (2020) focus on a few-shot setting and perform span extraction of slot values using pretrained models. Their approach, however, still requires some expert annotation. Another direction of research focuses on zero-shot slot filling. Bapna et al. (2017)’s recurrent-neuralnetwork-based slot tagger is pretrained on multiple domains and takes a textual description of the target slot on the input in addition to the user utterance. This way, adapting to a new domain only involves providing new slot descriptions. Further works extend this idea with more complex architectures (Shah et al., 2019; Liu et al., 2020).\nUnsupervised and semi-supervised methods were also investigated for predicting intents (user\ninput sentence types). Yang et al. (2014) use semisupervised intent clustering, with manual annotation to seed and interpret the clusters. Chen et al. (2016) introduced a model for zero-shot intent embedding prediction based on similarity to known intents. Shi et al. (2018) proposed a fully unsupervised intent detection model with the use of sentence clustering based on sentence-level features.\nMost applications of unsupervised or semisupervised methods to end-to-end dialogue response generation avoid explicit dialogue state modeling (e.g., Serban et al., 2016; Li et al., 2016; Gao et al., 2019). They aim at a non-task-oriented setting, where state interpretability or response controllability are less of a concern. Other works in task-oriented dialogues use transfer learning for adapting to low-resourced target domains (Zhao and Eskenazi, 2018; Shalyminov et al., 2019), but also keep the dialogue state representation latent.\nIn contrast, Jin et al. (2018) propose to model the dialogue state explicitly, in a semi-supervised way. They extend the end-to-end encoder-decoder Sequicity model of Lei et al. (2018, cf. Section 4) by introducing an additional decoder that has access to posterior information about the system response. This allows them to train a state representation with a reconstruction loss on unsupervised examples, using the state as a limited memory for essential concepts (roughly corresponding to slots). Their method can be applied in fully unsupervised way, but it still requires some amount of in-domain annotations to achieve good performance. Our work aims at explicit dialogue state modeling without the need for any in-domain supervision."
    }, {
      "heading" : "3 Method",
      "text" : "Our slot discovery method has three main stages: (1) We obtain weak supervision labels from auto-\nmatic domain-generic annotation. (2) We identify domain-relevant slots based on the annotation labels by iteratively (a) merging and (b) ranking and selecting most viable candidates (Section 3.2). (3) we use the discovered slots to train an independent slot tagger (Section 3.3)."
    }, {
      "heading" : "3.1 Acquiring labels",
      "text" : "Figure 2 shows the overall data flow of our slot annotation pipeline. The data are first labeled with domain-generic linguistic annotation models, which we consider weak supervision. For our experiments, we use a frame semantic parser and NER, but other models, such as semantic role labeling (SRL; e.g., Palmer et al., 2010) or keyword extraction (e.g., Hulth, 2003) can be used in general. We use a simple union of labels provided by all annotation models.3"
    }, {
      "heading" : "3.2 Discovering Slots: Merging and Ranking",
      "text" : "Subsequent steps identify domain-relevant slots based on candidates provided by the automatic annotation. The slot discovery process is iterative – in each iteration, it: (1) merges similar candidates, (2) ranks candidates’ relevance and eliminates irrelevant ones. Once no more frames are eliminated, the process stops and we obtain slot labels, which are used to train a slot tagger (see Section 3.3).\nWe refer to the automatically tagged tokens as (slot) fillers, and the tags are considered slot candidates. We use generic precomputed word embeddings as word representation in both steps. We further compute slot embeddings 4(B:) for each distinct slot B: as word embedding averages over\n3If the same token is labeled multiple times by different annotation sources, both labels are considered candidates and are very likely to be merged. If multiple labels remain after the merging and ranking process, only the first label is kept, the rest are discarded.\nall respective slot fillers, weighted proportionally by filler frequency. The slot embeddings need to be re-computed after each iteration due to the merging step. We will now describe the individual steps."
    }, {
      "heading" : "3.2.1 Candidate Merging",
      "text" : "Since automatic annotation may have a very fine granularity,4 entities/objects of the same type are often captured by multiple slot candidates. With a frame parser, for instance, the frames Direction and Location both relate to the concept of area. We thus need to merge similar B1 . . . B= under a single candidate. We measure similarity of slots B1, B2 as:\nsim(B1, B2) = sim4 (4(B1), 4(B2)) + simctx(B1, B2)\nwhere sim4 is a cosine similarity and simctx(B1, B2) is a normalized number of occurrences of B1 and B2 with the same dependency relation. If the similarity exceeds a pre-set threshold )sim, the candidates are merged into one."
    }, {
      "heading" : "3.2.2 Candidate Ranking and Selection",
      "text" : "The main goal of this step is to remove irrelevant slot candidates and select the viable ones only. We hypothesize that different slots are likely to occur in different contexts (e.g., addresses are requested more often than stated by the user). To preserve relevant slots that only occur in rarer contexts, we cluster the data according to verb-slot pairs. We then rank candidates within each cluster (see details below). We consider candidates with a score higher than U-fraction of a given cluster mean to be relevant and select them for the next rounds. If a slot candidate is selected in at least one of the clusters, it is considered viable overall.\nClustering the data We process the data with a generic SRL tagger. Each occurrence of a filler is thus associated with a head verb whose semantic argument the corresponding word is, if such exists. We then compute embeddings of the formed verb-filler pairs as average of the respective token embeddings. The pairs are then clustered using agglomerative (bottom-up) hierarchical clustering with average linkage according to cosine distance of their embeddings.5 The process stops when a predetermined number of clusters is reached.\n4This is indeed the case for frame-semantic annotation, which we mostly use in our experiments in Section 5. Annotation types that have fewer label types could be further distinguished by e.g. adding the head verb from syntactic parsing, or using word classes/word clustering over the fillers.\n5Note that fillers for the same slot candidate may end up in multiple clusters. This does not mean that the respective\nCandidate Ranking criteria We use the following metrics to compute the ranking score:6 • Frequency frq(B) is used since candidates that occur frequently in the data are likely important. • Coherence coh(B) is the average pairwise similarity of all fillers’ embeddings:\ncoh(B) =\n∑ (E,F) ∈ 2B 3cos(4(E), 4(F))\n| 2B | (1)\nwhere 2B is a set of all pairs of fillers for the slot candidate s. We follow Chen et al. (2014)’s assumption that fillers with high coherence, i.e., focused on one topic, are good slot candidates. • TextRank (Mihalcea and Tarau, 2004) is a keyword extraction algorithm. It constructs a graph where nodes represent words and edges represent their co-occurrence. The dominant eigenvector of the adjacency matrix of this graph then gives the individual words’ scores. We replace fillers with candidate labels when computing the score, so we obtain results related to slots rather than to particular values. The final score is a simple sum of rankings with respect to all three scores."
    }, {
      "heading" : "3.3 Slot Tagger Model Training",
      "text" : "Our method described in Section 3.2 can give us a good set of dialogue slots. However, using the merged and filtered slots directly may result in low recall since the original annotation models used as weak supervision are not adapted to our specific domain. Therefore, we use the obtained labels to train a new, domain-specific slot tagger to improve performance. The tagger has no access to better labels than those derived by our method; however, it has a simpler task, as the set of target labels is now much smaller and the domain is much narrower.\nWe model the slot tagging task as sequence tagging, using a convolutional neural network that takes word- and character-based embeddings of the tokens as the input and produces a sequence of respective tags (Lample et al., 2016).7 The output layer of the tagger network gives softmax probability distributions over possible tags. To further increase recall, we add an inference-time rule – if\nslot candidate is split – it is just ranked for relevance multiple times (with respect to multiple contexts).\n6Usefulness of the individual metrics is confirmed in an ablation study in Section 6.\n7https://github.com/deepmipt/ner\nthe most probable predicted tag is ‘O’ (i.e., no slot) and the second most probable tag has a probability higher than a preset threshold )tag, the second tag is chosen as a prediction instead. As we discuss in Section 6, this threshold is crucial for achieving substantial recall improvement.\nTo improve the robustness of our model, we only use 10% of the original in-domain training set (with labels from Section 3.1) to train the slot tagger model. The rest of the training set is used for a grid search to determine model hyperparameters (hidden layer size, dropout rate and )tag threshold). We choose the parameters that yield the best F1 score when compared against the automatic slot discovery results (i.e., no manual annotation is needed here, the aim is at good generalization)."
    }, {
      "heading" : "4 Application in Dialogue Response Generation",
      "text" : "To verify the usefulness of the labels discovered by our method, we use them to train and evaluate an end-to-end task-oriented dialogue system. We choose Sequicity (Lei et al., 2018) for our experiments, an LSTM-based encoder-decoder model that uses a system of copy nets and two-stage decoding. First, it decodes the dialogue state, so the database can be queried externally. In the subsequent step, Sequicity generates the system response conditioned on the belief state and database results. This architecture works with a flat representation of the dialogue state, i.e. the state is represented as a sequence of tokens – slot values.\nThe default Sequicity model uses gold-standard dialogue state annotation. However, a compatible state representation is directly obtainable from our labels, simply by concatenating the labels aggregated in each turn from user utterances. Whenever a new value for a slot is found in user input by our tagger, it is either appended to the state representation, or it replaces a previous value of the same slot. This artificial supervision thus allows us to provide a learning signal to the Sequicity model even without manually labeled examples."
    }, {
      "heading" : "5 Experiments",
      "text" : "We evaluate our approach to slot discovery by comparing the resulting slot labels to gold-standard supervised slot annotation. Additionally, we evaluate the structure of clusters created during the selection process (Section 3.2.2) by comparing it to gold-standard user intents. We also test the use-\nfulness of our labels in a full dialogue response generation setup (Section 4), where we compare to gold-standard dialogue tracking labels."
    }, {
      "heading" : "5.1 Datasets and Experimental Setup",
      "text" : "We use the following datasets for our experiments: • CamRest676 (CR) (Wen et al., 2017) has 676\ndialogues, 2,744 user utterances, 4 tracked slots and 2 intents in the restaurant domain. • MultiWOZ (Budzianowski et al., 2018; Eric et al., 2020) is a multi-domain corpus; we picked two domains – hotel reservation and attraction recommendation – to form WOZ-hotel (WH) with 14,435 utterances, 9 slots, 3 intents and WOZ-attr (WA) with 7524 utterances, 8 slots and 3 intents respectively.8 • Cambridge SLU (Henderson et al., 2012) (CS) contains 10,569 utterances and tracks 5 slots with 5 intents in the restaurant domain. • ATIS (AT) (Hemphill et al., 1990) contains 4,978 utterances with 79 slots and 17 intents in the flights domain.9 As sources of weak supervision providing slot candidates, we mainly use the frame semantic parsers SEMAFOR (Das et al., 2010) and open-sesame (Swayamdipta et al., 2017) – a union of labels provided by both parsers is used in all our setups. In addition, to explore combined sources on the namedentity-heavy ATIS dataset, we include a generic convolutional NER model provided by SpaCy.10 To provide features for slot candidate merging and selection, we use AllenNLP (Gardner et al., 2017) for SRL and FastText (Bojanowski et al., 2017) as pretrained word embeddings.\nSlot merging and selection parameters were set heuristically in an initial trial run on the CamRest676 data and proved stable across domains. Slot tagger hyperparameters are chosen according to grid search on a portion of the training data, as described in Section 3.3.11"
    }, {
      "heading" : "5.2 System Variants and Baselines",
      "text" : "We test multiple ablation variants of our method: • Ours-full is the full version of our method (full\nannotation setup and trained slot tagger).\n8MultiWOZ contains more domains such as restaurant, train search, bus search. However, we decided to not include these as they are nearly identical to the other domains we use.\n9We used the ATIS data version from https://www.kaggle.com/siddhadev/ atis-dataset-from-ms-cntk.\n10https://spacy.io 11Training details are included in Appendix C.\nmethod ↓ / dataset→ CR CS WH WA AT\n• Ours-nothr does not use the recall-increasing second-candidate rule in the slot tagger (cf. Section 3.3). • Ours-notag excludes the slot tagger, directly using the output of our merging and selection step. • Ours-nocl further excludes the clustering step; slot candidate ranking and selection is performed over all candidates together (cf. Section 3.2.2).\nWe also compare to previous work of Chen et al. (2014),12 which is similar to Ours-nocl, but it does not merge similar frames and uses different ranking criteria. To put our results into perspective, we also include two supervised models for comparison: Tag-supervised is the same model that we use as our slot tagger (see Section 3.3), but it is trained on supervised data. Dict-supervised uses a simple dictionary of labels obtained from the training data.\nAs an intrinsic evaluation of the verb-slot pair clusters formed for slot ranking in Section 3.2.2, we compare to gold-standard intent annotation with respect to the following baselines: (1) a majority baseline (assigning the most frequent intent class to all instances), and (2) a simple method that represents the utterances as averages of respective word embeddings and performs sentence-level intent clustering. All the slots in a given utterance are then assumed to have the same intent.\nThe dialogue generation task is evaluated by comparing to Jin et al. (2018)’s approach introduced in Section 2. We run their model in a fully unsupervised way, i.e. we provide no labeled examples during the training phase, to give a fair comparison against our model. To provide more perspective, we also show a supervised variant of Jin et al. (2018)’s model, where gold-standard slot labels are provided.\n12We use our own reimplementation of their approach."
    }, {
      "heading" : "5.3 Evaluation Metrics",
      "text" : "For evaluation, we construct a handcrafted reference mapping between our discovered slots and the respective ground-truth slots and intents. The mapping is domain-specific, but it is very easy to construct even for an untrained person – the process takes less than 10 minutes for each of our domains. It amounts to matching slots from the domain ontology against slots output by our approach, which are represented by FrameNet labels. Most importantly, the mapping is only needed for evaluation, not by our method itself. We provide an example mapping in Appendix B.\nWe use the following evaluation metrics: • Slot F1 score: To reflect slot tagging perfor-\nmance, we measure precision, recall, and F1 for every slot individually. An average is then computed from slot-level scores, weighted by the number of slot occurrences in the data. We measure slot F1 both on standalone user utterances (slot tagging) and in the context of a dialogue system (dialogue tracking). • Slot-level Average Precision (AP). The slot candidates picking task is a ranking problem and we use the average precision metric following Chen et al. (2014). Considering a ranked list of discovered slots ; = B1, . . . , B: , . . . , B= we compute AP:\n%(;) = ∑= :=1 %@: (;)1:\n# mapped slots (2)\nwhere 1: is an indicator function that equals one if slot : has a reference mapping defined and %@: (;) is precision at : of the ranked list ;. • Slot Rand Index (RI) is a clustering metric, used to evaluate slot candidate merging. RI is the proportion of pairs of slot candidates that are correctly assigned into the same or into different\nslots (following the reference mapping).13\n• Normalized Mutual Information (NMI) is the mutual information between two clusterings normalized into the (0, 1) interval. Thanks to the normalization, it is suitable for comparing two clusterings with different numbers of clusters. • Intent Accuracy is the percentage of slot occurrences assigned into the correct intent cluster under the reference mapping (see Section 5.2). • Dialogue Joint Goal Accuracy calculates the proportion of dialogue turns where all user constraints (i.e., dialogue state summarizing slot values) are captured correctly (Mrkšić et al., 2017). • Dialogue Entity Match Rate calculates the last turn’s entity in each dialogue. It verifies if a correct entity would be retrieved from the database using the final constraints (Wen et al., 2017).\nFor slot tagging and ranking evaluation, we sampled a random data order 50 times and performed 5-fold cross-validation for each permutation. For the dialogue generation evaluation, we trained the models 100 times and used averaged results. All results are given with 95% confidence intervals."
    }, {
      "heading" : "6 Results and Discussion",
      "text" : "We first evaluate the main task of slot tagging and include a manual error analysis, then present detailed results for subtasks (slot candidate ranking and merging) and additional tasks (intent clustering and full response generation).\n13We compute RI on a union of labels that have a groundtruth slot mapping and all labels selected by our method. Labels without ground-truth mapping are assumed to form singleitem “pseudo-slots”.\nSlot tagging is evaluated in Table 1. Ours-full (slot selection + trained tagger) outperforms all other approaches by a large margin, especially in terms of recall. The performance cannot match the supervised models, but it is not far off in some domains.14 Chen et al. (2014)’s method has a slightly higher precision, but our recall is much higher than theirs (see Appendix A.1). Note that Chen et al. (2014) do not reduce the set of candidates, they only rank them so that a manual cutoff can be made. In contrast, our method reduces the set of candidates significantly. A comparison between Ours-notag and Ours-full shows that applying the slot tagger improves both precision and recall. Tagger without the threshold decision rule (Ours-nothr) mostly performs better than the parser; however, using the threshold is essential to improve recall. Experiments on ATIS with NER as an additional source of annotation proved that our method can benefit from it. As discussed above, the use of the trained tagging model is crucial to improve the recall of our method. In Figure 4, we compare the results with and without the tagger. We change the value of prediction threshold and measure the number of cases in which the tagging model encounters more true positives, false positives or false negatives, respectively. As the results show, lowering the threshold increases the number of cases in which the tagger finds more correct slot values (and therefore improves recall), while it does not affect the number of false positives much (and therefore retains precision).\nError analysis: We conducted a manual error analysis of slot tagging to gain more insight about the output quality and sources of errors. In general, we found that the tagger can generalize and capture unseen values (cf. Figure 3).\nOne source of errors is the relatively low recall of the frame-semantic parsers used. We successfully address this issue by introducing the slot tagger, however, many slot values remain untagged. This is expected as our method’s performance is inherently limited by the input linguistic annotation quality. Another type of errors is caused by the can-\n14Note that our measurements of slot F1 only consider the ‘O’ tag as negative (the average is computed over slots only). This results in lower numbers than those reported in literature (cf. e.g. Goo et al., 2018), but we believe that this reflects the actual performance more accurately.\n15We present results taken in unsupervised setting, i.e. when no ontology is available. However, since Jin et al. (2018) consider only slot values that are known from the ontology by default, we provide the extended results in Appendix A.2.\ndidate merging procedure (see also below). Due to frequent co-occurrence, it might happen that two semantically unrelated candidates are merged and therefore some tokens are wrongly included as respective slot fillers. Nevertheless, the merging step is required in order to obtain a reasonable number of slots for a dialogue domain.\nOur approach does leave some room for improvements, especially regarding the consistency of results across different slots, which can be imbalanced. For instance, on the WOZ-hotel data, we observe a difference of up to 0.5 F1 score among individual slots (see Appendix A.2).\nSlot candidate ranking results are given in Table 2. Our pipeline significantly outperforms Chen et al. (2014)’s approach on 4 out of 5 datasets. We can also see that the slot-verb pairs clustering step is important – in the ablation experiment where we do not perform clustering (Ours-nocl), performance falls dramatically on the WOZ-hotel, WOZattr and ATIS data. This is because without the clustering step, a large number of context-irrelevant slot candidates is considered, hurting performance.\nIn addition, we include a detailed evaluation of the contribution of the individual slot candidate ranking scores described in Section 3.2.2. Results\nin Table 6 suggest that all of our proposed scores improve the performance.\nSlot merging evaluation is shown in Table 3. Although candidates in the CamRest676 data are merged into slots reasonably well, other datasets show a relatively low performance. The low RI scores are a result of errors in candidate ranking, which wrongly assigned high ranks to some rare, irrelevant candidates. These candidates do not appear in the reference mapping and are assumed to form singular “pseudo-slots”. However, they are typically joined with similar candidates in the merging process. This leads to many pairs of candidates that are merged into one slot by our approach but appear separately in the reference mapping. Nevertheless, this behavior barely influences slot tagging performance as the candidates are rare.\nClustering evaluation: Table 5 suggests that our clustering performs better than simple baselines and can potentially yield useful results if used for intent detection. Nevertheless, intent detection is more complex and presumably requires more features and information about the dialogue context, which we reserve for future work. The complexity is also suggested by the fact that the naive embedding clustering performs worse than the majority baseline in 4 out of 5 cases.\nDialogue response generation: We explore the influence that our labels have on sequence-tosequence dialogue response generation in an experiment on the CamRest676 data (see Table 4). We can see that our method provides helpful slot labels that improve dialogue state tracking performance. Compared to Jin et al. (2018)’s system used in a fully unsupervised setting, our approach shows significant improvements in all metrics. We achieve better results than Jin et al. (2018)’s system especially with respect to entity match rate, suggesting that our model can provide consistent labels throughout the whole dialogue. To make a fair comparison, we further evaluate Jin et al. (2018)’s system in a setting in which it can learn\nfrom the labels provided directly by weak supervision (i.e., the frame-semantic parser, not filtered by our pipeline). We observe an improvement in terms of entity match rate, but it does not match the improvement achieved with our filtered labels. Surprisingly, slot F1 and joint goal accuracy even decrease slightly, which suggests that label quality is important and the noisy labels obtained directly from weak supervision are not useful enough."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We present a novel approach for weakly supervised natural language understanding in dialogue systems that discovers domain-relevant slots and tags them in a standalone fashion. Our method removes the need for annotated training data by using off-theshelf linguistic annotation models. Experiments on five datasets in four domains mark a signifi-\ncant improvement in intrinsic NLU performance over previous weakly supervised approaches; in particular, we vastly improve the slot recall. The usefulness of slots discovered by our method is further confirmed in a full dialogue response generation application. Code used for our experiments is available on GitHub.16\nA drawback of our approach is the reliance on existing linguistic annotation models. We show that the method is able to combine multiple annotation sources and create a tagger that functions as a standalone component, generalizing better than the original annotation and thus lowering this dependency. Nevertheless, the results are still somewhat limited by the input annotation structure and quality. In future, we plan to further improve the model by unsupervised selection of slot candidates via keyword extraction and clustering, as well as by taking context information from preceding dialogue turns into account. We also want to focus more on the intent detection aspect of our work."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported by Charles University grants PRIMUS 19/SCI/10, GAUK 302120, and SVV 260 575. We also want to thank Jindřich Libovický and David Mareček for helpful comments on the draft, and the anonymous reviewers for their remarks that helped us further improve the paper."
    }, {
      "heading" : "A Additional Results",
      "text" : "A.1 Slot tagging precision and recall\nA.2 Individual slot performance"
    }, {
      "heading" : "B Reference mapping",
      "text" : ""
    }, {
      "heading" : "C Training details",
      "text" : "Here we provide details about training process and model sizes:\n• Since the models are rather small with regards to number of parameters, it is sufficient to use a regular desktop PC. In our experiments, we require about 4 GB of RAM, and we use Intel Xeon E5-2630 v4 CPUs.\n• Our slot candidate selection step takes roughly 1 hour. The tagger model is lightweight, with only 150k parameters. Its training requires 10-30 minutes, depending on the exact configuration and data size.\n• The evaluation scripts are attached and described in the README file.\n• We conduct hyperparameter search using a basic grid search algorithm. We tested hidden size values ∈ [50, 200], dropout ∈ [0.5, 0.85] and the threshold )tag ∈ [0.05, 0.3]. Therefore, we ran 4 × 8 × 6 = 192 search trials.\n• The best parameters were determined by tagger accuracy on the validation set: hidden_size = 250, dropout = 0.7, )tag = 0.3, )sim = 0.9.\n• Links to the data are included in the README file, we use train:validation:split ratio equal to 8:1:1."
    } ],
    "references" : [ {
      "title" : "Towards Zero-Shot Frame Semantic Parsing for Domain Scaling",
      "author" : [ "Ankur Bapna", "Gokhan Tür", "Dilek Hakkani-Tür", "Larry Heck." ],
      "venue" : "Proceedings of Interspeech 2017, pages 2476–2480.",
      "citeRegEx" : "Bapna et al\\.,? 2017",
      "shortCiteRegEx" : "Bapna et al\\.",
      "year" : 2017
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Transactions of the ACL, 5:135–146.",
      "citeRegEx" : "Bojanowski et al\\.,? 2017",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2017
    }, {
      "title" : "MultiWOZ – a large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling",
      "author" : [ "Paweł Budzianowski", "Tsung-Hsien Wen", "Bo-Hsiang Tseng", "Inigo Casanueva", "Stefan Ultes", "Osman Ramadan", "Milica Gašić." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Budzianowski et al\\.,? 2018",
      "shortCiteRegEx" : "Budzianowski et al\\.",
      "year" : 2018
    }, {
      "title" : "Zero-shot learning of intent embeddings for expansion by convolutional deep structured semantic models",
      "author" : [ "Yun-Nung Chen", "Dilek Hakkani-Tür", "Xiaodong He." ],
      "venue" : "Proceedings of IEEE ICASSP, pages 6045–6049.",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Jointly modeling inter-slot relations by random walk on knowledge graphs for unsupervised spoken language understanding",
      "author" : [ "Yun-Nung Chen", "William Yang Wang", "Alexander Rudnicky." ],
      "venue" : "Proceedings of NAACL, pages 619–629.",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Leveraging frame semantics and distributional semantics for unsupervised semantic slot induction in spoken dialogue systems",
      "author" : [ "Yun-Nung Chen", "William Yang Wang", "Alexander I Rudnicky." ],
      "venue" : "Proceedings of IEEE SLT, pages 584–589.",
      "citeRegEx" : "Chen et al\\.,? 2014",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2014
    }, {
      "title" : "Span-ConveRT: Few-shot Span Extraction for Dialog with Pretrained Conversational Representations",
      "author" : [ "Samuel Coope", "Tyler Farghly", "Daniela Gerz", "Ivan Vulić", "Matthew Henderson." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Coope et al\\.,? 2020",
      "shortCiteRegEx" : "Coope et al\\.",
      "year" : 2020
    }, {
      "title" : "Probabilistic frame-semantic parsing",
      "author" : [ "Dipanjan Das", "Nathan Schneider", "Desai Chen", "Noah A. Smith." ],
      "venue" : "Proceedings of NAACL-HLT, pages 948– 956, Los Angeles, California.",
      "citeRegEx" : "Das et al\\.,? 2010",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2010
    }, {
      "title" : "MultiWOZ 2.1: A Consolidated Multi-Domain Dialogue Dataset with State Corrections and State",
      "author" : [ "Mihail Eric", "Rahul Goel", "Shachi Paul", "Abhishek Sethi", "Sanchit Agarwal", "Shuyang Gao", "Adarsh Kumar", "Anuj Goyal", "Peter Ku", "Dilek Hakkani-Tur" ],
      "venue" : null,
      "citeRegEx" : "Eric et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Eric et al\\.",
      "year" : 2020
    }, {
      "title" : "Frame semantics and the nature of language",
      "author" : [ "Charles J Fillmore." ],
      "venue" : "Annals of the New York Academy of Sciences, 280(1):20–32.",
      "citeRegEx" : "Fillmore.,? 1976",
      "shortCiteRegEx" : "Fillmore.",
      "year" : 1976
    }, {
      "title" : "Jointly Optimizing Diversity and Relevance in Neural Response Generation",
      "author" : [ "Xiang Gao", "Sungjin Lee", "Yizhe Zhang", "Chris Brockett", "Michel Galley", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "Proceedings of NAACL, Minneapolis, MN, USA.",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "AllenNLP: A deep semantic natural language processing platform",
      "author" : [ "Matt Gardner", "Joel Grus", "Mark Neumann", "Oyvind Tafjord", "Pradeep Dasigi", "Nelson F. Liu", "Matthew Peters", "Michael Schmitz", "Luke S. Zettlemoyer." ],
      "venue" : "Proceedings of ACL Work-",
      "citeRegEx" : "Gardner et al\\.,? 2017",
      "shortCiteRegEx" : "Gardner et al\\.",
      "year" : 2017
    }, {
      "title" : "Slot-Gated Modeling for Joint Slot Filling and Intent Prediction",
      "author" : [ "Chih-Wen Goo", "Guang Gao", "Yun-Kai Hsu", "Chih-Li Huo", "Tsung-Chieh Chen", "Keng-Wei Hsu", "YunNung Chen." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chap-",
      "citeRegEx" : "Goo et al\\.,? 2018",
      "shortCiteRegEx" : "Goo et al\\.",
      "year" : 2018
    }, {
      "title" : "Using a knowledge graph and query click logs for unsupervised learning of relation detection",
      "author" : [ "Dilek Hakkani-Tür", "Larry Heck", "Gokhan Tur." ],
      "venue" : "Proceedings of IEEE ICASSP, pages 8327–8331.",
      "citeRegEx" : "Hakkani.Tür et al\\.,? 2013",
      "shortCiteRegEx" : "Hakkani.Tür et al\\.",
      "year" : 2013
    }, {
      "title" : "Exploiting the semantic web for unsupervised spoken language understanding",
      "author" : [ "Larry Heck", "Dilek Hakkani-Tür." ],
      "venue" : "Proceedings of IEEE SLT, pages 228–233.",
      "citeRegEx" : "Heck and Hakkani.Tür.,? 2012",
      "shortCiteRegEx" : "Heck and Hakkani.Tür.",
      "year" : 2012
    }, {
      "title" : "The ATIS spoken language systems pilot corpus",
      "author" : [ "Charles T. Hemphill", "John J. Godfrey", "George R. Doddington." ],
      "venue" : "Proceedings of the workshop on Speech and Natural Language - HLT ’90, pages 96–101, Hidden Valley, Pennsylvania.",
      "citeRegEx" : "Hemphill et al\\.,? 1990",
      "shortCiteRegEx" : "Hemphill et al\\.",
      "year" : 1990
    }, {
      "title" : "Discriminative spoken language understanding using word confusion networks",
      "author" : [ "Matthew Henderson", "Milica Gašić", "Blaise Thomson", "Pirros Tsiakoulis", "Kai Yu", "Steve Young." ],
      "venue" : "Proceedings of IEEE SLT, pages 176–181.",
      "citeRegEx" : "Henderson et al\\.,? 2012",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2012
    }, {
      "title" : "Improved automatic keyword extraction given more linguistic knowledge",
      "author" : [ "Anette Hulth." ],
      "venue" : "Proceedings of EMNLP, pages 216–223.",
      "citeRegEx" : "Hulth.,? 2003",
      "shortCiteRegEx" : "Hulth.",
      "year" : 2003
    }, {
      "title" : "Explicit state tracking with semisupervision for neural dialogue generation",
      "author" : [ "Xisen Jin", "Wenqiang Lei", "Zhaochun Ren", "Hongshen Chen", "Shangsong Liang", "Yihong Zhao", "Dawei Yin." ],
      "venue" : "Proceedings of ACM CIKM, pages 1403–1412.",
      "citeRegEx" : "Jin et al\\.,? 2018",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer." ],
      "venue" : "Proceedings of NAACL, pages 260–270.",
      "citeRegEx" : "Lample et al\\.,? 2016",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures",
      "author" : [ "Wenqiang Lei", "Xisen Jin", "Min-Yen Kan", "Zhaochun Ren", "Xiangnan He", "Dawei Yin." ],
      "venue" : "Proceedings of ACL, pages 1437–1447.",
      "citeRegEx" : "Lei et al\\.,? 2018",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2018
    }, {
      "title" : "A Diversity-Promoting Objective Function for Neural Conversation Models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Com-",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Coach: A Coarse-to-Fine Approach for Cross-domain Slot Filling",
      "author" : [ "Zihan Liu", "Genta Indra Winata", "Peng Xu", "Pascale Fung." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 19–25, Online.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Textrank: Bringing order into text",
      "author" : [ "Rada Mihalcea", "Paul Tarau." ],
      "venue" : "Proceedings of EMNLP, pages 404–411.",
      "citeRegEx" : "Mihalcea and Tarau.,? 2004",
      "shortCiteRegEx" : "Mihalcea and Tarau.",
      "year" : 2004
    }, {
      "title" : "Neural belief tracker: Data-driven dialogue state tracking",
      "author" : [ "Nikola Mrkšić", "Diarmuid O Séaghdha", "Tsung-Hsien Wen", "Blaise Thomson", "Steve Young." ],
      "venue" : "Proceedings of ACL, pages 1777–1788.",
      "citeRegEx" : "Mrkšić et al\\.,? 2017",
      "shortCiteRegEx" : "Mrkšić et al\\.",
      "year" : 2017
    }, {
      "title" : "Semantic role labeling",
      "author" : [ "Martha Palmer", "Daniel Gildea", "Nianwen Xue." ],
      "venue" : "Synthesis Lectures on Human Language Technologies, 3(1):1–103.",
      "citeRegEx" : "Palmer et al\\.,? 2010",
      "shortCiteRegEx" : "Palmer et al\\.",
      "year" : 2010
    }, {
      "title" : "Building end-to-end dialogue systems using generative hierarchical neural network models",
      "author" : [ "Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau." ],
      "venue" : "Proceedings of AAAI, pages 3776–3783.",
      "citeRegEx" : "Serban et al\\.,? 2016",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "Robust Zero-Shot CrossDomain Slot Filling with Example Values",
      "author" : [ "Darsh Shah", "Raghav Gupta", "Amir Fayazi", "Dilek Hakkani-Tur." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5484–",
      "citeRegEx" : "Shah et al\\.,? 2019",
      "shortCiteRegEx" : "Shah et al\\.",
      "year" : 2019
    }, {
      "title" : "Few-shot dialogue generation without annotated data: A transfer learning approach",
      "author" : [ "Igor Shalyminov", "Sungjin Lee", "Arash Eshghi", "Oliver Lemon." ],
      "venue" : "Proceedings of SIGDIAL, Stockholm, Sweden.",
      "citeRegEx" : "Shalyminov et al\\.,? 2019",
      "shortCiteRegEx" : "Shalyminov et al\\.",
      "year" : 2019
    }, {
      "title" : "AutoDialabel: Labeling dialogue data with unsupervised learning",
      "author" : [ "Chen Shi", "Qi Chen", "Lei Sha", "Sujian Li", "Xu Sun", "Houfeng Wang", "Lintao Zhang." ],
      "venue" : "Proceedings of EMNLP, pages 684– 689.",
      "citeRegEx" : "Shi et al\\.,? 2018",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2018
    }, {
      "title" : "Frame-semantic parsing with softmax-margin segmental RNNs and a syntactic scaffold",
      "author" : [ "Swabha Swayamdipta", "Sam Thomson", "Chris Dyer", "Noah A Smith." ],
      "venue" : "arXiv:1706.09528.",
      "citeRegEx" : "Swayamdipta et al\\.,? 2017",
      "shortCiteRegEx" : "Swayamdipta et al\\.",
      "year" : 2017
    }, {
      "title" : "Leveraging semantic web search and browse sessions for multi-turn spoken dialog systems",
      "author" : [ "Lu Wang", "Larry Heck", "Dilek Hakkani-Tür." ],
      "venue" : "Proceedings of IEEE ICASSP, pages 4082–4086.",
      "citeRegEx" : "Wang et al\\.,? 2014",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    }, {
      "title" : "A networkbased end-to-end trainable task-oriented dialogue system",
      "author" : [ "Tsung-Hsien Wen", "David Vandyke", "Nikola Mrksić", "Milica Gašić", "Lina M Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "Steve Young." ],
      "venue" : "Proceedings of EACL, pages 438–449.",
      "citeRegEx" : "Wen et al\\.,? 2017",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2017
    }, {
      "title" : "The dialog state tracking challenge",
      "author" : [ "Jason Williams", "Antoine Raux", "Deepak Ramachandran", "Alan Black." ],
      "venue" : "Proceedings of SIGDIAL, pages 404– 413.",
      "citeRegEx" : "Williams et al\\.,? 2013",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2013
    }, {
      "title" : "Semi-supervised learning of dialogue acts using sentence similarity based on word embeddings",
      "author" : [ "Xiaohao Yang", "Jia Liu", "Zhenfeng Chen", "Weilan Wu." ],
      "venue" : "Proceedings of ICALIP, pages 882–886.",
      "citeRegEx" : "Yang et al\\.,? 2014",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2014
    }, {
      "title" : "POMDP-based statistical spoken dialog systems: A review",
      "author" : [ "S. Young", "M. Gasic", "B. Thomson", "J.D. Williams." ],
      "venue" : "Proceedings of the IEEE, 101(5):1160–1179.",
      "citeRegEx" : "Young et al\\.,? 2013",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 2013
    }, {
      "title" : "Zeroshot dialog generation with cross-domain latent actions",
      "author" : [ "Tiancheng Zhao", "Maxine Eskenazi." ],
      "venue" : "Proceedings of SIGDIAL, pages 1–10.",
      "citeRegEx" : "Zhao and Eskenazi.,? 2018",
      "shortCiteRegEx" : "Zhao and Eskenazi.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 35,
      "context" : "Task-oriented dialogue systems typically use annotation based on slots to represent the meaning of user utterances (Young et al., 2013).",
      "startOffset" : 115,
      "endOffset" : 135
    }, {
      "referenceID" : 33,
      "context" : "Slots and their values are tracked over the course of the dialogue, forming dialogue state, which allows a dialogue system to plan the next actions effectively (Williams et al., 2013).",
      "startOffset" : 160,
      "endOffset" : 183
    }, {
      "referenceID" : 35,
      "context" : "However, both traditional pipeline systems (Young et al., 2013) and end-to-end task-oriented architectures (Wen et al.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 32,
      "context" : ", 2013) and end-to-end task-oriented architectures (Wen et al., 2017) typically require such annotation.",
      "startOffset" : 51,
      "endOffset" : 69
    }, {
      "referenceID" : 26,
      "context" : "While some systems use implicit, latent state representation and do not require annotation (Serban et al., 2016), the behavior of such systems is hard to interpret or control.",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 36,
      "context" : ", 2014, 2015) or transferring annotation across domains (Zhao and Eskenazi, 2018; Coope et al., 2020), but they still require significant manual effort.",
      "startOffset" : 56,
      "endOffset" : 101
    }, {
      "referenceID" : 6,
      "context" : ", 2014, 2015) or transferring annotation across domains (Zhao and Eskenazi, 2018; Coope et al., 2020), but they still require significant manual effort.",
      "startOffset" : 56,
      "endOffset" : 101
    }, {
      "referenceID" : 9,
      "context" : "We use FrameNet-style (Fillmore, 1976) semantic frames as our main source of weak supervision.",
      "startOffset" : 22,
      "endOffset" : 38
    }, {
      "referenceID" : 27,
      "context" : "Further works extend this idea with more complex architectures (Shah et al., 2019; Liu et al., 2020).",
      "startOffset" : 63,
      "endOffset" : 100
    }, {
      "referenceID" : 22,
      "context" : "Further works extend this idea with more complex architectures (Shah et al., 2019; Liu et al., 2020).",
      "startOffset" : 63,
      "endOffset" : 100
    }, {
      "referenceID" : 21,
      "context" : "sponse generation avoid explicit dialogue state modeling (e.g., Serban et al., 2016; Li et al., 2016; Gao et al., 2019).",
      "startOffset" : 57,
      "endOffset" : 119
    }, {
      "referenceID" : 10,
      "context" : "sponse generation avoid explicit dialogue state modeling (e.g., Serban et al., 2016; Li et al., 2016; Gao et al., 2019).",
      "startOffset" : 57,
      "endOffset" : 119
    }, {
      "referenceID" : 36,
      "context" : "adapting to low-resourced target domains (Zhao and Eskenazi, 2018; Shalyminov et al., 2019), but also keep the dialogue state representation latent.",
      "startOffset" : 41,
      "endOffset" : 91
    }, {
      "referenceID" : 28,
      "context" : "adapting to low-resourced target domains (Zhao and Eskenazi, 2018; Shalyminov et al., 2019), but also keep the dialogue state representation latent.",
      "startOffset" : 41,
      "endOffset" : 91
    }, {
      "referenceID" : 23,
      "context" : "• TextRank (Mihalcea and Tarau, 2004) is a keyword extraction algorithm.",
      "startOffset" : 11,
      "endOffset" : 37
    }, {
      "referenceID" : 19,
      "context" : "We model the slot tagging task as sequence tagging, using a convolutional neural network that takes word- and character-based embeddings of the tokens as the input and produces a sequence of respective tags (Lample et al., 2016).",
      "startOffset" : 207,
      "endOffset" : 228
    }, {
      "referenceID" : 20,
      "context" : "We choose Sequicity (Lei et al., 2018) for our experiments, an LSTM-based encoder-decoder model",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 32,
      "context" : "We use the following datasets for our experiments: • CamRest676 (CR) (Wen et al., 2017) has 676 dialogues, 2,744 user utterances, 4 tracked slots and 2 intents in the restaurant domain.",
      "startOffset" : 69,
      "endOffset" : 87
    }, {
      "referenceID" : 2,
      "context" : "• MultiWOZ (Budzianowski et al., 2018; Eric et al., 2020) is a multi-domain corpus; we picked two domains – hotel reservation and attraction recommendation – to form WOZ-hotel (WH) with 14,435 utterances, 9 slots, 3 intents and WOZ-attr (WA) with 7524 utterances, 8 slots and 3 intents respectively.",
      "startOffset" : 11,
      "endOffset" : 57
    }, {
      "referenceID" : 8,
      "context" : "• MultiWOZ (Budzianowski et al., 2018; Eric et al., 2020) is a multi-domain corpus; we picked two domains – hotel reservation and attraction recommendation – to form WOZ-hotel (WH) with 14,435 utterances, 9 slots, 3 intents and WOZ-attr (WA) with 7524 utterances, 8 slots and 3 intents respectively.",
      "startOffset" : 11,
      "endOffset" : 57
    }, {
      "referenceID" : 16,
      "context" : "8 • Cambridge SLU (Henderson et al., 2012) (CS) contains 10,569 utterances and tracks 5 slots with 5 intents in the restaurant domain.",
      "startOffset" : 18,
      "endOffset" : 42
    }, {
      "referenceID" : 15,
      "context" : "• ATIS (AT) (Hemphill et al., 1990) contains 4,978 utterances with 79 slots and 17 intents in the flights domain.",
      "startOffset" : 12,
      "endOffset" : 35
    }, {
      "referenceID" : 7,
      "context" : "As sources of weak supervision providing slot candidates, we mainly use the frame semantic parsers SEMAFOR (Das et al., 2010) and open-sesame",
      "startOffset" : 107,
      "endOffset" : 125
    }, {
      "referenceID" : 30,
      "context" : "(Swayamdipta et al., 2017) – a union of labels provided by both parsers is used in all our setups.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 11,
      "context" : "To provide features for slot candidate merging and selection, we use AllenNLP (Gardner et al., 2017) for SRL and FastText (Bojanowski et al.",
      "startOffset" : 78,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : ", 2017) for SRL and FastText (Bojanowski et al., 2017) as pretrained word embeddings.",
      "startOffset" : 29,
      "endOffset" : 54
    }, {
      "referenceID" : 24,
      "context" : ", dialogue state summarizing slot values) are captured correctly (Mrkšić et al., 2017).",
      "startOffset" : 65,
      "endOffset" : 86
    }, {
      "referenceID" : 32,
      "context" : "It verifies if a correct entity would be retrieved from the database using the final constraints (Wen et al., 2017).",
      "startOffset" : 97,
      "endOffset" : 115
    } ],
    "year" : 2021,
    "abstractText" : "Task-oriented dialogue systems typically require manual annotation of dialogue slots in training data, which is costly to obtain. We propose a method that eliminates this requirement: We use weak supervision from existing linguistic annotation models to identify potential slot candidates, then automatically identify domain-relevant slots by using clustering algorithms. Furthermore, we use the resulting slot annotation to train a neural-network-based tagger that is able to perform slot tagging with no human intervention. This tagger is trained solely on the outputs of our method and thus does not rely on any labeled data. Our model demonstrates state-of-the-art performance in slot tagging without labeled training data on four different dialogue domains. Moreover, we find that slot annotations discovered by our model significantly improve the performance of an end-to-end dialogue response generation model, compared to using no slot annotation at all.",
    "creator" : "LaTeX with hyperref"
  }
}