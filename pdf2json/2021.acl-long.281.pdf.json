{
  "name" : "2021.acl-long.281.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Exploring the Representation of Word Meanings in Context: A Case Study on Homonymy and Synonymy",
    "authors" : [ "Marcos Garcia" ],
    "emails" : [ "marcos.garcia.gonzalez@usc.gal" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3625–3640\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3625"
    }, {
      "heading" : "1 Introduction",
      "text" : "Contrary to static vector models, which represent the different senses of a word in a single vector (Erk, 2012; Mikolov et al., 2013), contextualized models generate representations at token-level (Peters et al., 2018; Devlin et al., 2019), thus being an interesting approach to model word meaning in context. In this regard, several studies have shown that clusters produced by some contextualized word embeddings (CWEs) are related to different senses of the same word (Reif et al., 2019; Wiedemann et al., 2019), or that similar senses can be aligned in cross-lingual experiments (Schuster et al., 2019).\nHowever, more systematic evaluations of polysemy (i.e., word forms that have different related meanings depending on the context (Apresjan, 1974)), have shown that even though CWEs present some correlations with human judgments\n(Nair et al., 2020), they fail to predict the similarity of the various senses of a polysemous word (Haber and Poesio, 2020).\nAs classical datasets to evaluate the capabilities of vector representations consist of single words without context (Finkelstein et al., 2001) or heavily constrained expressions (Kintsch, 2001; Mitchell and Lapata, 2008), new resources with annotations of words in free contexts have been created, including both graded similarities (Huang et al., 2012; Armendariz et al., 2020) or binary classification of word senses (Pilehvar and Camacho-Collados, 2019; Raganato et al., 2020). However, as these datasets largely include instances of polysemy, they are difficult to solve even for humans (in fact, the highest reported human upper bound is about 80%) as the nuances between different senses depend on non-linguistic factors such as the annotator procedure or the target task (Tuggy, 1993; Kilgarriff, 1997; Hanks, 2000; Erk, 2010).\nIn this paper, we rely on a more objective and simple task to assess how contextualized approaches (both neural network models and contextualized methods of distributional semantics) represent word meanings in context. In particular, we observe whether vector models can identify unrelated meanings represented by the same word form (homonymy) and the same sense conveyed by different words (synonymy). In contrast to polysemy, there is a strong consensus concerning the representation of homonymous senses in the lexicon, and it has been shown that homonyms are cognitively processed differently than polysemous words (Klepousniotou et al., 2012; MacGregor et al., 2015). In this regard, exploratory experiments in English suggest that some CWEs correctly model homonymy, approximating the contextualized vectors of a homonym to those of its paraphrases (Lake and Murphy, 2020), and showing stronger correlation with human judgments to those\nof polysemous words (Nair et al., 2020). However, as homonyms convey unrelated meanings depending on the context, it is not clear whether the good performance of CWEs actually derives from the contextualization process or simply from the use of explicit lexical cues present in the sentences.\nTaking the above into account, we have created a new multilingual dataset (in Galician, Portuguese, English, and Spanish) with more than 3,000 evaluation items. It allows for carrying out more than 10 experiments and controlling factors such as the surrounding context, the word overlap, and the sense conveyed by different word forms. We use this resource to perform a systematic evaluation of contextualized word meaning representations. We compare different strategies using both static embeddings and current models based on deep artificial neural networks. The results suggest that the best monolingual models based on Transformers (Vaswani et al., 2017) can identify homonyms having different meanings adequately. However, as they strongly rely on the surrounding context, words with different meanings are represented very closely when they occur in similar sentences. Apart from the empirical conclusions and the dataset, this paper also contributes with new BERT and fastText models for Galician.1\nSection 2 presents previous studies about word meaning representation. Then, Section 3 introduces the new dataset used in this paper. In Section 4 we describe the models and methods to obtain the vector representations. Finally, the experiments and results are discussed in Section 5, while Section 6 draws some conclusions of our study."
    }, {
      "heading" : "2 Related Work",
      "text" : "A variety of approaches has been implemented to compute word meaning in context by means of standard methods of distributional semantics (Schütze, 1998; Kintsch, 2001; McDonald and Brew, 2004; Erk and Padó, 2008). As compositional distributional models construct sentence representations from their constituents vectors, they take into account contextualization effects on meaning (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Baroni, 2013). However, these approaches often have scalability problems as their representations grow exponentially with the size of the sentences. Therefore, the datasets used to\n1Dataset, models, and code are available at https:// github.com/marcospln/homonymy_acl21/.\nevaluate them are composed of highly restricted phrases (Grefenstette and Sadrzadeh, 2011).\nThe rise of artificial neural networks on natural language processing popularized the use of vector representations, and the remarkable performance of neural language models (Melamud et al., 2016; Peters et al., 2018) led to a productive line of research exploring to what extent these models represent linguistic knowledge (Rogers et al., 2020). However, few of these works have focused on lexical semantics, and most of the relevant results in this field come from evaluations in downstream tasks. In this regard, Wiedemann et al. (2019) found that clusters of BERT embeddings (Devlin et al., 2019) seem to be related to word senses, while Schuster et al. (2019) observed that clusters of polysemous words correspond to different senses in a cross-lingual alignment of vector representations.\nProbing LSTMs on lexical substitution tasks, Aina et al. (2019) showed that these architectures rely on the lexical information from the input embeddings, and that the hidden states are biased towards contextual information. On an exploration of the geometric representations of BERT, Reif et al. (2019) found that different senses of a word tend to appear separated in the vector space, while several clusters seem to correspond to similar senses. Recently, Vulić et al. (2020) evaluated the performance of BERT models on several lexical-semantic tasks in various languages, including semantic similarity or word analogy. The results show that using special tokens ([CLS] or [SEP]) hurts the quality of the representations, and that these tend to improve across layers until saturation. As this study uses datasets of single words (without context), typelevel representations are obtained by averaging the contextualized vectors over various sentences.\nThere are several resources to evaluate word meaning in free contexts, such as the Stanford Contextual Word Similarity (Huang et al., 2012) and CoSimLex (Armendariz et al., 2020), both representing word similarity on a graded scale, or the Word-in-Context datasets (WiC), focused on binary classifications (i.e., each evaluation item contains two sentences with the same word form, having the same or different senses) (Pilehvar and CamachoCollados, 2019; Raganato et al., 2020). These datasets include not only instances of homonymy but mostly of polysemous words. In this regard, studies on polysemy using Transformers have obtained diverse results: Haber and Poesio (2020)\nfound that BERT embeddings correlate better with human ratings of co-predication than with similarity between word senses, thus suggesting that these representations encode more contextual information than word sense knowledge. Nevertheless, the results of Nair et al. (2020) indicate that BERT representations are correlated with human scores of polysemy. An exploratory experiment of the latter study also shows that BERT discriminates between polysemy and homonymy, which is also suggested by other pilot evaluations reported by Lake and Murphy (2020) and Yu and Ettinger (2020).\nOur study follows this research line pursuing objective and unambiguous lexical criteria such as the representation of homonyms and synonyms. In this context, there is a broad consensus in the psycholinguistics literature regarding the representation of homonyms as different entries in the lexicon (in contrast to polysemy, for which there is a long discussion on whether senses of polysemous words are stored as a single core representation or as independent entries (Hogeweg and Vicente, 2020)). In fact, several studies have shown that homonyms are cognitively processed differently from polysemous words (Klepousniotou et al., 2012; Rabagliati and Snedeker, 2013). In contrast to the different senses of polysemous words, which are simultaneously activated, the meanings of homonyms are in conflict during processing, with the not relevant ones being deactivated by the context (MacGregor et al., 2015). To analyze how vector models represent homonymy and synonymy in context, we have built a new multilingual resource with a strong inter-annotator agreement, presented below."
    }, {
      "heading" : "3 A New Multilingual Resource of Homonymy and Synonymy in Context",
      "text" : "This section briefly describes some aspects of lexical semantics relevant to our study, and then presents the new dataset used in the paper.\nHomonymy and homography: Homonymy is a well-known type of lexical ambiguity that can be described as the relation between distinct and unrelated meanings represented by the same word form, such as match, meaning for instance ‘sports game’ or ‘stick for lighting fire’. In contrast to polysemy (where one lexeme conveys different related senses depending on the context, e.g., newspaper as an organization or as a set of printed pages), it is often assumed that homonyms are different lexemes that have the same lexical form (Cruse, 1986), and\ntherefore they are stored as independent entries in the lexicon (Pustejovsky, 1998).\nThere are two main criteria for homonymy identification: Diachronically, homonyms are lexical items that have different etymologies but are accidentally represented by the same word form, while a synchronic perspective strengthens unrelatedness in meaning. Even if both approaches tend to identify similar sets of homonyms, there may be ambiguous cases that are diachronically but not synchronically related (e.g., two meanings of banco –‘bench’ and ‘financial institution’– in Portuguese or Spanish could be considered polysemous as they derive from the same origin,2 but as this is a purely historical association, most speakers are not aware of the common origin of both senses). In this study, we follow the synchronic perspective, and consider homonymous meanings those that are clearly unrelated (e.g., they unambiguously refer to completely different concepts) regardless of their origin.\nIt is worth mentioning that as we are dealing with written text we are actually analyzing homographs (different lexemes with the same spelling) instead of homonyms. Thus, we discard instances of phonologically identical words which are written differently, such as the Spanish hola ‘hello’ and ola ‘wave’, both representing the phonological form /ola/. Similarly, we include words with the same spelling representing different phonological forms, e.g., the Galician-Portuguese sede, which corresponds to both /sede/ ‘thirst’, and /sEde/ ‘headquarters’.\nIn this paper, homonymous senses are those unrelated meanings conveyed by the same (homonym) word form. For instance, coach may have two homonymous senses (‘bus’ and ‘trainer’), which can be conveyed by other words (synonyms) in different contexts (e.g., by bus or trainer).\nStructure of the dataset: We have created a new resource to investigate how vector models represent word meanings in context. In particular, we want to observe whether they capture (i) different senses conveyed by the same word form (homonymy), and (ii) equivalent senses expressed by different words (synonymy). The resource contains controlled sentences so that it allows us to observe how the context and word overlap affect word representations.\nTo allow for different comparisons with the same\n2In fact, several dictionaries organize them in a single entry: https://dicionario.priberam.org/banco, https://dle.rae.es/banco.\nand different contexts, we have included five sentences for each meaning (see Table 1 for examples): three sentences containing the target word, a synonym, and a word with a different sense, all of them in the same context (sentences 1 to 3), and two additional sentences with the target word and a synonym, representing the same sense (sentences 4 and 5, respectively). Thus, for each sense we have four sentences (1, 2, 4, 5) with a word conveying the same sense (both in the same and in different contexts) and another sentence (3) with a different word in the same context as sentences 1 and 2.\nFrom this structure, we can create datasets of sentence triples, where the target words of two of them convey the same sense, and the third one has a different meaning. Thus, we can generate up to 48 triples for each pair of senses (24 in each direction: sense 1 vs. sense 2, and vice-versa). These datasets allow us to evaluate several semantic relations at the lexical level, including homonymy, synonymy, and various combinations of homonymous senses. Interestingly, we can control for the impact of the context (e.g., are contextualized models able to distinguish between different senses occurring in the same context, or do they incorporate excessive contextual information into the word vectors?), the word overlap (e.g., can a model identify different senses of the same word form depending on the context, or it strongly depends on lexical cues?), or the POS-tag (e.g., are homonyms with different POS-tags easily disambiguated?).\nConstruction of the dataset: We compiled data for four languages: Galician, Portuguese, Spanish, and English.3 We tried to select sentences compatible with the different varieties of the same language\n3Galician is generally considered a variety of a single (Galician-)Portuguese language. However, they are divided in this resource, as Galician has recently been standardized using a Spanish-based orthography that formally separates it from Portuguese (Samartim, 2012).\n(e.g., with the same meaning in UK and US English, or in Castilian and Mexican Spanish). However, we gave priority to the European varieties when necessary (e.g., regarding spelling variants).\nThe dataset was built using the following procedure: First, language experts (one per language) compiled lists of homonyms using dedicated resources for language learning, together with WordNet and other lexicographic data (Miller, 1995; Montraveta and Vázquez, 2010; Guinovart, 2011; Rademaker et al., 2014). Only clear and unambiguous homonyms were retained (i.e., those in the extreme of the homonymy-polysemy-vagueness scale (Tuggy, 1993)). These homonyms were then enriched with frequency data from large corpora: Wikipedia and SLI GalWeb (Agerri et al., 2018) for Galician, and a combination of Wikipedia and Europarl for English, Spanish and Portuguese (Koehn, 2005). From these lists, each linguist selected the most frequent homonyms, annotating them as ambiguous at type or token level (absolute homonymy and partial homonymy in Lyons’ terms (Lyons, 1995)). As a substantial part were nounverb pairs, only a few of these were included. For each homonym, the language experts selected from corpora two sentences (1 and 4) in which the target words were not ambiguous.4 They then selected a synonym that could be used in sentence 1 without compromising grammaticality (thus generating sentence 2), and compiled an additional sentence for it (5), trying to avoid further lexical ambiguities in this process.5 For each homonym, the linguists selected a word with a different meaning (for sen-\n4Sentences were selected, adapted, and simplified using GDEX-inspired constraints (Kilgarriff et al., 2008) (i.e., avoiding high punctuation ratios, unnecessary subordinate clauses, etc.), which resulted in the creation of new sentences.\n5In most cases, this synonym is the same as that of sentence 2, but this is not always the case. Besides, in some cases we could not find words conveying the same sense, for which we do not have sentences 2 and 5.\ntence 3), trying to maximize the following criteria: (i) to refer unambiguously to a different concept, and to preserve (ii) semantic felicity and (iii) grammaticality. The size of the final datasets varies depending on the initial lists and on the ease of finding synonyms in context.\nResults: Apart from the sentence triples explained above, the dataset structure allows us to create evaluation sets with different formats, such as sentence pairs to perform binary classifications as in the WiC datasets. Table 2 shows the number of homonyms, senses, and sentences of the multilingual resource, together with the size of the evaluation datasets in different formats.\nAs the original resource was created by one annotator per language, we ensured its quality as follows: We randomly extracted sets of 50 sentence pairs and gave them to other annotators (5 for Galician, and 1 for each of the other three varieties, all of them native speakers of the target language). We then computed the Cohen’s κ inter-annotator agreement (Cohen, 1960) between the original resource and the outcome of this second annotation (see the right column of Table 2). We obtained a microaverage κ = 0.94 across languages, a result which supports the task’s objectivity. Nevertheless, it is worth noting that few sentences have been carefully modified after this analysis, as it has shown that several misclassifications were due to the use of an ambiguous synonym. Thus, it is likely that the final resource has higher agreement values."
    }, {
      "heading" : "4 Models and Methods",
      "text" : "This section introduces the models and procedures to obtain vector representations followed by the evaluation method."
    }, {
      "heading" : "4.1 Models",
      "text" : "We have used static embeddings and CWEs based on Transformers, comparing different ways of obtaining the vector representations in both cases:\nStatic embeddings: We have used skip-gram fastText models of 300 dimensions (Bojanowski et al., 2017).6 For English and Spanish, we have used the official vectors trained on Wikipedia. For Portuguese, we have used the model provided by Hartmann et al. (2017), and for Galician we have trained a new model (see Appendix C for details).7\nContextualized embeddings: We have evaluated multilingual and monolingual models:8\nMultilingual models: We have used the official multilingual BERT (mBERT cased, 12 layers) (Devlin et al., 2019), XLM-RoBERTa (Base, 12 layers) (Conneau et al., 2020), and DistilBERT (DistilmBERT, 6 layers) (Sanh et al., 2019).\nMonolingual models: For English, we have used the official BERT-Base model (uncased). For Portuguese and Spanish, BERTimbau (Souza et al., 2020) and BETO (Cañete et al., 2020) (both cased). For Galician, we trained two BERT models (with 6 and 12 layers; see Appendix C)."
    }, {
      "heading" : "4.2 Obtaining the vectors",
      "text" : "Static models: These are the methods used to obtain the representations from the static models:\nWord vector (WV): Embedding of the target word (homonymous senses with the same word form will have the same representation).\n6In preliminary experiments we also used word2vec and GloVe models, obtaining slightly lower results than fastText.\n7These Portuguese and Galician models obtained better results (0.06 on average) than the official ones.\n8To make a fair comparison we prioritized base models (12 layers), but we also report results for large (24 layers) and 6 layers models when available.\nSentence vector (Sent): Average embedding of the whole sentence.\nSyntax (Syn): Up to four different representations obtained by adding the vector of the target word to those of their syntactic heads and dependents. This method is based on the assumption that the syntactic context of a word characterizes its meaning, providing relevant information for its contextualized representation (e.g., in ‘He swims to the bank’, bank may be disambiguated by combining its vector with the one of swim).9 Appendix D describes how heads and dependents are selected.\nContextualized models: For these models, we have evaluated the following approaches:\nSentence vector (Sent): Vector of the sentence built by averaging all words (except for the special tokens [CLS] and [SEP]), each of them represented by the standard approach of concatenating the last 4 layers (Devlin et al., 2019).\nWord vector (WV): Embedding of the target word, combining the vectors of the last 4 layers. We have evaluated two operations: vector concatenation (Cat), and addition (Sum).\nWord vector across layers (Lay): Vector of the target word on each layer. This method allows us to explore the contextualization effects on each layer.\nVectors of words split into several sub-words are obtained by averaging the embeddings of their components. Similarly, MWEs vectors are the average of the individual vectors of their components, both for static and for contextualized embeddings."
    }, {
      "heading" : "4.3 Measuring sense similarities",
      "text" : "Given a sentence triple where two of the target words (a and b) have the same sense and the third (c) a different one, we evaluate a model as follows (in a similar way as other studies (Kintsch, 2001; Lake and Murphy, 2020)): First, we obtain\n9We have also evaluated a contextualization method using selectional preferences inspired by Erk and Padó (2008), but the results were almost identical to those of the WV approach.\nthree cosine similarities between the vector representations: sim1 = cos(a, b); sim2 = cos(a, c); sim3 = cos(b, c). Then, an instance is labeled as correct if those words conveying the same sense (a and b) are closer together than the third one (c). In other words, sim1 > sim2 and sim1 > sim3: Otherwise, the instance is considered as incorrect."
    }, {
      "heading" : "5 Evaluation",
      "text" : "This section presents the experiments performed using the new dataset and discusses their results."
    }, {
      "heading" : "5.1 Experiments",
      "text" : "Among all the potential analyses of our data, we have selected four evaluations to assess the behavior of a model by controlling factors such as the context and the word overlap:\nHomonymy (Exp1): The same word form in three different contexts, two of them with the same sense (e.g., coach in sentences [1:1, 1:4, 2:1]10 in Table 1). This test evaluates if a model correctly captures the sense of a unique word form in context. Hypothesis: Static embeddings will fail as they produce the same vector in the three cases, while models that adequately incorporate contextual cues should correctly identify the outlier sense.\nSynonyms of homonymous senses (Exp2): A word is compared with its synonym and with the synonym of its homonym, all three in different contexts (e.g., coach=bus 6=trainer in [1:1, 1:5, 2:2]). This test assesses if there is a bias towards one of the homonymous senses, e.g., the most frequent one (MacGregor et al., 2015). Hypothesis: Models with this type of bias may fail, so as in Exp1, they should also appropriately incorporate contextual information to represent these examples.\nSynonymy vs homonymy (Exp3): We compare a word to its synonym and to a homonym, all in\n10First and second digits refer to the sense and sentence ids.\ndifferent contexts (e.g., coach=bus 6=coach in [1:1, 1:5, 2:1]). Here we evaluate whether a model adequately represents both (i) synonymy in context –two word forms with the same sense in different contexts– and (ii) homonymy –one of the former word forms having a different meaning. Hypothesis: Models relying primarily on lexical knowledge are likely to represent homonyms closer than synonyms (giving rise to an incorrect output), but those integrating contextual information will be able to model the three representations correctly.\nSynonymy (Exp4): Two synonyms vs. a different word (and sense), all of them in the same context (e.g., [2:1, 2:2, 2:3]). It assesses to what extent the context affects word representations of different word forms. Hypothesis: Static embeddings may pass this test as they tend to represent typelevel synonyms closely in the vector space. Highly contextualized models might be puzzled as different meanings (from different words) occur in the same context, so that the models should have an adequate trade-off between lexical and contextual knowledge.\nTable 3 displays the number of sentence triples for each experiment as well as the total number of triples of the dataset. To focus on the semantic knowledge encoded in the vectors –rather than on the morphosyntactic information–, we have evaluated only those triples in which the target words of the three sentences have the same POS-tag (numbers on the right).11 Besides, we have also carried out an evaluation on the full dataset."
    }, {
      "heading" : "5.2 Results and discussion",
      "text" : "Table 4 contains a summary of the results of each experiment in the four languages. For reasons of clarity, we include only fastText embeddings and the best contextualized model (BERT). Results for all models and languages can be seen in Appendix A. BERT models have the best performance overall, both on the full dataset and on the selected experiments, except for Exp4 (in which the three sentences share the context) where the static models outperform the contextualized representations.\nIn Exp1 and Exp2, where the context plays a crucial role, fastText models correctly labeled between 50%/60% of the examples (depending on the language and vector type, with better results\n11On average, BERT-base models achieved 0.24 higher results (Add) when tested on all the instances (including different POS-tags) of the four experiments.\nfor Sent and Syn). For BERT, the best accuracy surpasses 0.98 (Exp1 in English), with an average across languages of 0.78, and where word vectors outperform sentence representations. These high results and the fact that WVs work better in general than Sent may be indicators that Transformers are properly incorporating contextual knowledge.\nSolving Exp3 requires both dealing with contextual effects and homonymy (as two words have the same form but different meaning) so that static embeddings hardly achieve 0.5 accuracy (Sent, with lower results for both WV and Syn). BERT’s performance is also lower than in Exp1 and Exp2, with an average of 0.67 and Sent beating WVs in most cases, indicating that the word vectors are not adequately representing the target senses.\nFinally, fastText obtains better results than BERT on Exp4 (where the three instances have the same context), reaching 0.81 in Spanish with an average across languages of 0.64 (always with WVs). BERT’s best performance is 0.41 (in two languages) with an average of 0.42, suggesting that very similar contexts may confound the model.\nTo shed light on the contextualization process of Transformers, we have analyzed their performance across layers. Figure 1 shows the accuracy curves (vs. the macro-average Sent and WV vectors of the contextualized and static embeddings) for five Transformers models on Galician, the language with the largest dataset (see Appendix A for equivalent figures for the other languages).\nIn Exp1 to Exp3 the best accuracies are obtained at upper layers, showing that word vectors appropriately incorporate contextual information. This is true especially for the monolingual BERT versions, as the multilingual models’ representations show higher variations. Except for Galician, Exp1 has better results than Exp2, as the former primarily deals with context while the latter combines contextualization with lexical effects. In Exp3 the curves take longer to rise as initial layers rely more on lexical than on contextual information. Furthermore, except for English (which reaches 0.8), the performance is low even in the best hidden layers (≈ 0.4). In Exp4 (with the same context in the three sentences), contextualized models cannot correctly represent the word senses, being surpassed in most cases by the static embeddings.\nFinally, we have observed how Transformers representations vary across the vector space. Figure 2 shows the UMAP visualizations (McInnes et al.,\n2018) of the contextualization processes of Exp1 and Exp3 examples in English. In 2a, the similar vectors of match in layer 1 are being contextualized\nacross layers, producing a suitable representation since layer 7. However, 2b shows how the model is not able to adequately represent match close to its\n(a) Exp1: Sentence 2: “Chelsea have a match with United next week.”. Sentence 3: “You should always strike a match away from you.” (b) Exp3: Sentence 2: “A game consists of two halves lasting 45 minutes, meaning it is 90 minutes long.”. Sentence 3: “He was watching a football stadium.”\nFigure 2: UMAP visualizations of word contextualization across layers (1 to 12) in Exp1 and Exp3 in English (BERT-base). In both cases, sentence 1 is “He was watching a football match.”, and the target word in sentence 3 is the outlier.\nsynonym game, as the vectors seem to incorporate excessive information (or at least limited lexical knowledge) from the context. Additional visualizations in Galician can be found in Appendix B.\nIn sum, the experiments performed in this study allow us to observe how different models generate contextual representations. In general, our results confirm previous findings which state that Transformers models increasingly incorporate contextual information across layers. However, we have also found that this process may deteriorate the representation of the individual words, as it may be incorporating excessive contextual information, as suggested by Haber and Poesio (2020)."
    }, {
      "heading" : "6 Conclusions and Further Work",
      "text" : "This paper has presented a systematic study of word meaning representation in context. Besides static word embeddings, we have assessed the ability of state-of-the-art monolingual and multilingual models based on the Transformers architecture to identify unambiguous cases of homonymy and synonymy. To do so, we have presented a new dataset in four linguistic varieties that allows for controlled evaluations of vector representations.\nThe results of our study show that, in most cases, the best contextualized models adequately identify homonyms conveying different senses in various contexts. However, as they strongly rely on the surrounding contexts, they misrepresent words having different senses in similar sentences.\nIn further work, we plan to extend our dataset\nwith multiword expressions of different degrees of idiomaticity and to include less transparent –but still unambiguous– contexts of homonymy. Finally, we also plan to systematically explore how multilingual models represent homonymy and synonymy in cross-lingual scenarios."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank the anonymous reviewers for their valuable comments, and NVIDIA Corporation for the donation of a Titan Xp GPU. This research is funded by a Ramón y Cajal grant (RYC2019-028473-I) and by the Galician Government (ERDF 2014-2020: Call ED431G 2019/04)."
    }, {
      "heading" : "A Complete results",
      "text" : "Figure 3 and Table 5 include the results for all languages and models. We also include large variants (BERT and XLM-RoBERTa) when available. For static embeddings, we report results for the best Syn setting, which combines up to three syntactically related words with the target word (see Appendix D).\nM od\nel Ve\nc. E\n1 E\n2 E\n3 E\n4 M\na M\ni F\nE 1\nE 2\nE 3\nE 4\nM a\nM i\nF E\n1 E\n2 E\n3 E\n4 M\na M\ni F\nE 1\nE 2\nE 3\nE 4\nM a\nM i F G al ic ia n Po rt ug ue se Sp an is h E ng lis h\nB E\nR T\nSe nt\n0. 7\n0. 76\n0. 75\n0. 18\n0. 6\n0. 62\n0. 73\n0. 68\n0. 43\n0. 64\n0. 22\n0. 49\n0. 52\n0. 56\n0. 76\n0. 59\n0. 54\n0. 19\n0. 52\n0. 52\n0. 6\n0. 79\n0. 66\n0. 74\n0. 22\n0. 6\n0. 6\n0. 7\nC at\n0. 71\n0. 8\n0. 29\n0. 42\n0. 56\n0. 51\n0. 7\n0. 85\n0. 51\n0. 38\n0. 37\n0. 53\n0. 5\n0. 66\n0. 86\n0. 7\n0. 41\n0. 44\n0. 6\n0. 56\n0. 74\n0. 96\n0. 83\n0. 76\n0. 43\n0. 74\n0. 73\n0. 84\nA dd\n0. 7\n0. 8\n0. 28\n0. 42\n0. 55\n0. 51\n0. 7\n0. 85\n0. 54\n0. 38\n0. 37\n0. 54\n0. 51\n0. 67\n0. 86\n0. 7\n0. 41\n0. 44\n0. 6\n0. 56\n0. 74\n0. 98\n0. 81\n0. 76\n0. 44\n0. 75\n0. 73\n0. 84\nB E\nR T\n2\nSe nt\n0. 61\n0. 6\n0. 59\n0. 16\n0. 49\n0. 5\n0. 64\n0. 68\n0. 49\n0. 69\n0. 22\n0. 52\n0. 55\n0. 6\n– –\n– –\n– –\n– 0.\n89 0.\n59 0.\n8 0.\n27 0.\n63 0.\n64 0. 7 C at 0. 62 0. 71 0. 3 0. 2 0. 46 0. 43 0. 65 0. 95 0. 51 0. 38 0. 46 0. 58 0. 54 0. 68 – – – – – – – 1 0. 81 0. 78 0. 57 0. 79 0. 78 0. 87 A dd 0. 61 0. 71 0. 29 0. 2 0. 45 0. 43 0. 65 0. 95 0. 49 0. 37 0. 46 0. 57 0. 53 0. 68 – – – – – – – 1 0. 81 0. 8 0. 57 0. 8 0. 78 0. 87\nm B\nE R\nT Se\nnt 0.\n48 0.\n4 0.\n49 0.\n16 0.\n38 0.\n39 0.\n53 0.\n63 0.\n43 0.\n57 0.\n17 0.\n45 0.\n47 0.\n54 0.\n51 0.\n41 0.\n41 0.\n19 0.\n38 0.\n38 0.\n5 0.\n65 0.\n57 0.\n77 0.\n16 0.\n54 0.\n55 0. 61 C at 0. 57 0. 61 0. 23 0. 22 0. 41 0. 38 0. 62 0. 73 0. 46 0. 16 0. 15 0. 38 0. 34 0. 54 0. 61 0. 45 0. 23 0. 24 0. 38 0. 35 0. 63 0. 83 0. 62 0. 53 0. 27 0. 56 0. 54 0. 73 A dd 0. 57 0. 62 0. 21 0. 22 0. 4 0. 37 0. 61 0. 73 0. 49 0. 14 0. 17 0. 38 0. 34 0. 55 0. 63 0. 44 0. 22 0. 25 0. 39 0. 35 0. 63 0. 83 0. 62 0. 54 0. 27 0. 56 0. 54 0. 73\nX L\nM -b\nSe nt\n0. 52\n0. 51\n0. 49\n0. 16\n0. 42\n0. 43\n0. 54\n0. 51\n0. 3\n0. 41\n0. 2\n0. 35\n0. 36\n0. 45\n0. 51\n0. 44\n0. 46\n0. 19\n0. 4\n0. 41\n0. 51\n0. 6\n0. 62\n0. 69\n0. 22\n0. 53\n0. 54\n0. 63\nC at\n0. 56\n0. 54\n0. 22\n0. 38\n0. 42\n0. 39\n0. 56\n0. 63\n0. 46\n0. 24\n0. 34\n0. 42\n0. 39\n0. 61\n0. 67\n0. 62\n0. 23\n0. 54\n0. 52\n0. 46\n0. 69\n0. 83\n0. 59\n0. 23\n0. 27\n0. 48\n0. 43\n0. 69\nA dd\n0. 55\n0. 54\n0. 2\n0. 39\n0. 42\n0. 38\n0. 56\n0. 63\n0. 51\n0. 22\n0. 34\n0. 43\n0. 39\n0. 61\n0. 67\n0. 62\n0. 23\n0. 56\n0. 52\n0. 47\n0. 69\n0. 81\n0. 55\n0. 23\n0. 29\n0. 47\n0. 43\n0. 68\nX L\nM -l\nSe nt\n0. 42\n0. 34\n0. 42\n0. 16\n0. 33\n0. 34\n0. 44\n0. 49\n0. 43\n0. 35\n0. 15\n0. 35\n0. 35\n0. 44\n0. 49\n0. 48\n0. 39\n0. 2\n0. 39\n0. 39\n0. 47\n0. 54\n0. 5\n0. 55\n0. 22\n0. 45\n0. 45\n0. 58\nC at\n0. 48\n0. 5\n0. 22\n0. 42\n0. 4\n0. 37\n0. 49\n0. 73\n0. 49\n0. 39\n0. 32\n0. 48\n0. 47\n0. 58\n0. 84\n0. 63\n0. 46\n0. 71\n0. 66\n0. 62\n0. 76\n0. 71\n0. 6\n0. 49\n0. 41\n0. 55\n0. 54\n0. 62\nA dd\n0. 46\n0. 51\n0. 2\n0. 43\n0. 4\n0. 37\n0. 5\n0. 73\n0. 51\n0. 38\n0. 32\n0. 49\n0. 47\n0. 58\n0. 84\n0. 66\n0. 46\n0. 71\n0. 67\n0. 62\n0. 77\n0. 73\n0. 6\n0. 51\n0. 46\n0. 57\n0. 56\n0. 64\nD m\nB E\nR T\nSe nt\n0. 51\n0. 49\n0. 5\n0. 16\n0. 42\n0. 43\n0. 57\n0. 51\n0. 43\n0. 47\n0. 1\n0. 38\n0. 39\n0. 5\n0. 51\n0. 44\n0. 45\n0. 12\n0. 38\n0. 39\n0. 51\n0. 67\n0. 55\n0. 79\n0. 24\n0. 56\n0. 58\n0. 63\nC at\n0. 52\n0. 52\n0. 07\n0. 24\n0. 34\n0. 29\n0. 51\n0. 68\n0. 32\n0 0.\n22 0.\n31 0.\n25 0.\n47 0.\n61 0.\n49 0.\n01 0.\n34 0.\n36 0.\n3 0.\n53 0.\n69 0.\n52 0.\n24 0.\n28 0.\n43 0.\n4 0. 63 A dd 0. 54 0. 56 0. 07 0. 26 0. 36 0. 31 0. 51 0. 71 0. 35 0 0. 22 0. 32 0. 26 0. 47 0. 61 0. 52 0. 01 0. 37 0. 38 0. 31 0. 54 0. 69 0. 53 0. 21 0. 37 0. 45 0. 41 0. 63\nfa st\nT Se\nnt 0.\n56 0.\n69 0.\n48 0.\n14 0.\n47 0.\n47 0.\n62 0.\n61 0.\n62 0.\n53 0.\n17 0.\n48 0.\n49 0.\n55 0.\n45 0.\n34 0.\n45 0.\n09 0.\n33 0.\n35 0.\n43 0.\n6 0.\n5 0.\n51 0.\n15 0.\n44 0.\n43 0. 54 W V 0. 21 0. 56 0 0. 53 0. 33 0. 29 0. 46 0. 02 0. 54 0 0. 63 0. 3 0. 24 0. 45 0. 12 0. 62 0. 02 0. 81 0. 39 0. 35 0. 48 0. 31 0. 55 0. 03 0. 57 0. 37 0. 34 0. 48 Sy n (3 ) 0. 53 0. 66 0. 2 0. 19 0. 39 0. 36 0. 57 0. 66 0. 46 0. 18 0. 2 0. 37 0. 34 0. 51 0. 37 0. 58 0. 17 0. 24 0. 34 0. 32 0. 55 0. 44 0. 69 0. 23 0. 18 0. 39 0. 36 0. 55\nTa bl\ne 5:\nC om\npl et\ne re\nsu lts\nfo r\nth e\nfo ur\nla ng\nua ge\ns. B\nE R\nT ar\ne B\nE R\nTB\nas e\nm od\nel s,\nan d\nB E\nR T\n2 re\nfe rs\nto a\nse co\nnd B\nE R\nT m\nod el\nfo r\nea ch\nla ng\nua ge\n(s m\nal lf\nor G\nal ic\nia n,\nan d\nla rg e fo r Po rt ug ue se an d E ng lis h) . X L M -b an d X L M -l ar e X L M -R oB E R Ta ba se an d la rg e m od el s, re sp ec tiv el y. D m B E R T is th e m ul til in gu al ve rs io n of D is til B E R T, an d fa st T th e fa st Te xt em be dd in gs . M a an d M ir ef er to th e m ac ro -a ve ra ge an d m ic ro -a ve ra ge re su lts ac ro ss th e fo ur ex pe ri m en ts ,r es pe ct iv el y. F ar e th e m ic ro -a ve ra ge va lu es on th e w ho le da ta se t."
    }, {
      "heading" : "B Contextualization process",
      "text" : "(a) Sent. 1: “Ten que haber algún erro nos cálculos porque o resultado non é correcto.” Sent. 2: “Segundo os meus cálculos acabaremos en tres días.” Sent. 3: “Tivo varios cálculos biliares.” (b) Sent. 1: “De sobremesa tomou queixo con marmelo.” Sentence 2: “Fomos a unhas xornadas gastronómicas do queixo.” Sentence 3: “Achegouse a ela e pasoulle a man polo queixo.”\n(c) Sentence 1: “Eran tantos que parecían un banco de xurelos.” Sent.2: “Desde a rocha víanse pequenos cardumes de robaliza.” Sentence 3: “Este asento de pedra é algo incómodo.” (d) Sent.1: “Apuntou todos os números de teléfono na axenda.” Sentence 2: “Anotou todos os números de teléfono na axenda.” Sentence 3: “Riscou todos os números de teléfono na axenda.”.\n(e) Sent. 1: “Vai ter lugar a elección da próxima sede dos Xogos Olímpicos.” Sent. 2: “A localización do evento será decidida esta semana.” Sent. 3: “Vou á fonte por auga, que teño sede.” (f) Sentence 1: “Encántalle comer o bolo de pan antes da sopa.” Sentence 2: “O molete tiña a codia un pouco dura.” Sentence 3: “Para atraeren as robalizas iscaban bolo vivo.”\nFigure 4: Examples in Galician using BERT-base (English translations of the sentences in Appendix E). First row shows examples of Ex1. In Figure 4a cálculos is correctly contextualized since layer 3. In Figure 4b, the outlier sense of queixo is not correctly contextualized in any layer. Second row shows examples of Exp2 (4c) and Exp4 (4d). In Figure 4c, the synonymys banco and cardume are closer to the outlier asento in layer 1 (and from 4 to 7), but the contextualization process is not able to correctly represent the senses in the vector space. In Figure 4d, the result is correct from layer 7 to 11, but in general the representations of words in similar sentences point towards a similar region. Third row incudes examples of Exp3. In Figure 4e, the occurrences of the homonym sede are correctly contextualized as the one in the first sentence approaches its synonym localización in upper layers. The equivalent example of Figure 4f is not adequately solved by the model, as both senses of bolo are notoriously distanct from molete, synonym of the first homonymous sense."
    }, {
      "heading" : "C Galician models",
      "text" : "Training corpus: We combined the SLI GalWeb (Agerri et al., 2018), CC-100 (Wenzek et al., 2020), the Galician Wikipedia (April 2020 dump), and other news corpora crawled from the web. Following Raffel et al. (2020), sentences with a high ratio of punctuation and symbols, and duplicates were removed. The final corpus has 555M words (633M tokens tokenized with FreeLing (Padró and Stanilovsky, 2012; Garcia and Gamallo, 2010)). The corpus was divided into 90%/10% splits for train and development.\nfastText model: We trained a fastText skip-gram model for 15 iterations with 300 dimensions, window size of 5, negative sampling of 25, and a minimum word frequency of 5. We used the same 90% split used to train the BERT models, but with automatic tokenization (≈ 600M tokens).\nBERT models: We used the 90% train split of the corpus (with the original tokenization) to train two BERT models, with 6 and 12 layers:\nBERT-small (6 layers): This model has been trained from scratch using a vocabulary of 52,000 (sub-)words and a batch size of 208. It has been training during 1M steps (≈ 20 epochs) in 14 days.\nBERT-base (12 layers): Following Kuratov and Arkhipov (2019), we initialized the model from the official pre-trained mBERT, therefore having the same vocabulary size (119,547). We trained it on the Galician corpus during 600k steps (≈ 13 epochs in 28 days) with a batch size of 198.\nBoth models were trained with the Transformers library (Wolf et al., 2020) on a single NVIDIA Titan XP GPU (12GB), a block size of 128, a learning rate of 0.0001, a masked language modeling (MLM) probability of 0.15, and a weight decay of 0.01. They have been trained only with the MLM objective.\nD Syntax (Syn method)\nTo get the heads and dependents of each target word we have used the following hierarchies: For nouns: HeadV erb (the head verb, if any)> DepV erb (dependents of the head verb with one of the following relations: obj, nmod, obl)> DepAdj (a dependent adjective)> DepNoun (a dependent noun). For verbs: Head (only if it is a verb or a noun)> Obj (its direct object, if any)> Arg (a dependent with one of these relations: nsubj, nmod, obl). Using\nthese hierarchies we have evaluated representations built by adding from 1 to 4 vectors to the one of each target word. As shown in Table 5, combining 3 syntactically related words to the target one obtains the best results.\nFor the experiments, we have parsed the datasets using the 2.5 Universal Dependencies models provided by UDPipe (Straka et al., 2019)."
    }, {
      "heading" : "E English translations (Figure 4)",
      "text" : "Figure 4a, sentence 1: “There must be some error in the calculations because the result is incorrect”. Sentence 2: “According to my calculations we will finish in three days”. Sentence 3: “[He/she] had several gallstones”.\nFigure 4b, sentence 1: “For dessert [he/she] ate cheese with quince”. Sentence 2: “We went to a cheese gastronomy days”. Sentence 3: “[He/She] approached her and ran his hand over her chin”.\nFigure 4c, sentence 1: “They were so many that they looked like a school of mackerel”. Sentence 2: “From the rock small shoals of sea bass could be seen”. Sentence 3: “This stone seat is somewhat uncomfortable”.\nFigure 4d, sentences 1 and 2: “[He/She] wrote down all the phone numbers on the phone book.” Sentence 3: “[He/She] crossed out all the phone numbers on the phone book”.\nFigure 4e, sentence 1: “The choice of the next venue for the Olympics will take place”. Sentence 2: “The location of the event will be decided this week”. Sentence 3: “I’ll get water from the spring, I am thirsty”.\nFigure 4f, sentence 1: “[He/She] loves to eat the bread cake before soup”. Sentence 2: “The bread had a slightly hard crust”. Sentence 3: “They used live sand lance to attrack sea bass”."
    } ],
    "references" : [ {
      "title" : "Developing new linguistic resources and tools for the Galician language",
      "author" : [ "Rodrigo Agerri", "Xavier Gómez Guinovart", "German Rigau", "Miguel Anxo Solla Portela." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources",
      "citeRegEx" : "Agerri et al\\.,? 2018",
      "shortCiteRegEx" : "Agerri et al\\.",
      "year" : 2018
    }, {
      "title" : "Putting words in context: LSTM language models and lexical ambiguity",
      "author" : [ "Laura Aina", "Kristina Gulordava", "Gemma Boleda." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3342–3348, Florence,",
      "citeRegEx" : "Aina et al\\.,? 2019",
      "shortCiteRegEx" : "Aina et al\\.",
      "year" : 2019
    }, {
      "title" : "Regular polysemy",
      "author" : [ "Ju D Apresjan." ],
      "venue" : "Linguistics, 12(142):5–32.",
      "citeRegEx" : "Apresjan.,? 1974",
      "shortCiteRegEx" : "Apresjan.",
      "year" : 1974
    }, {
      "title" : "CoSimLex: A resource for evaluating graded word similarity in context",
      "author" : [ "Carlos Santos Armendariz", "Matthew Purver", "Matej Ulčar", "Senja Pollak", "Nikola Ljubešić", "Mark Granroth-Wilding." ],
      "venue" : "Proceedings of the 12th Language Resources",
      "citeRegEx" : "Armendariz et al\\.,? 2020",
      "shortCiteRegEx" : "Armendariz et al\\.",
      "year" : 2020
    }, {
      "title" : "Composition in distributional semantics",
      "author" : [ "Marco Baroni." ],
      "venue" : "Language and Linguistics Compass, 7(10):511–522.",
      "citeRegEx" : "Baroni.,? 2013",
      "shortCiteRegEx" : "Baroni.",
      "year" : 2013
    }, {
      "title" : "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space",
      "author" : [ "Marco Baroni", "Roberto Zamparelli." ],
      "venue" : "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Baroni and Zamparelli.,? 2010",
      "shortCiteRegEx" : "Baroni and Zamparelli.",
      "year" : 2010
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:135–146.",
      "citeRegEx" : "Bojanowski et al\\.,? 2017",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2017
    }, {
      "title" : "Spanish Pre-Trained BERT Model and Evaluation Data",
      "author" : [ "José Cañete", "Gabriel Chaperon", "Rodrigo Fuentes", "JouHui Ho", "Hojin Kang", "Jorge Pérez." ],
      "venue" : "PML4DC at ICLR 2020.",
      "citeRegEx" : "Cañete et al\\.,? 2020",
      "shortCiteRegEx" : "Cañete et al\\.",
      "year" : 2020
    }, {
      "title" : "A coefficient of agreement for nominal scales",
      "author" : [ "Jacob Cohen." ],
      "venue" : "Educational and psychological measurement, 20(1):37–46.",
      "citeRegEx" : "Cohen.,? 1960",
      "shortCiteRegEx" : "Cohen.",
      "year" : 1960
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Lexical semantics",
      "author" : [ "David Alan Cruse." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Cruse.,? 1986",
      "shortCiteRegEx" : "Cruse.",
      "year" : 1986
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "What is word meaning, really? (and how can distributional models help us describe it?)",
      "author" : [ "Katrin Erk." ],
      "venue" : "Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, pages 17–26, Uppsala, Sweden. Association for Computa-",
      "citeRegEx" : "Erk.,? 2010",
      "shortCiteRegEx" : "Erk.",
      "year" : 2010
    }, {
      "title" : "Vector space models of word meaning and phrase meaning: A survey",
      "author" : [ "Katrin Erk." ],
      "venue" : "Language and Linguistics Compass, 6(10):635–653.",
      "citeRegEx" : "Erk.,? 2012",
      "shortCiteRegEx" : "Erk.",
      "year" : 2012
    }, {
      "title" : "A structured vector space model for word meaning in context",
      "author" : [ "Katrin Erk", "Sebastian Padó." ],
      "venue" : "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 897–906, Honolulu, Hawaii. Association for Com-",
      "citeRegEx" : "Erk and Padó.,? 2008",
      "shortCiteRegEx" : "Erk and Padó.",
      "year" : 2008
    }, {
      "title" : "Placing search in context: The concept revisited",
      "author" : [ "Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin." ],
      "venue" : "Proceedings of the 10th international conference on World Wide Web, pages 406–",
      "citeRegEx" : "Finkelstein et al\\.,? 2001",
      "shortCiteRegEx" : "Finkelstein et al\\.",
      "year" : 2001
    }, {
      "title" : "Análise Morfossintáctica para Português Europeu e Galego: Problemas, Soluções e Avaliação",
      "author" : [ "Marcos Garcia", "Pablo Gamallo." ],
      "venue" : "Linguamática, 2(2):59–67.",
      "citeRegEx" : "Garcia and Gamallo.,? 2010",
      "shortCiteRegEx" : "Garcia and Gamallo.",
      "year" : 2010
    }, {
      "title" : "Experimental support for a categorical compositional distributional model of meaning",
      "author" : [ "Edward Grefenstette", "Mehrnoosh Sadrzadeh." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1394–1404,",
      "citeRegEx" : "Grefenstette and Sadrzadeh.,? 2011",
      "shortCiteRegEx" : "Grefenstette and Sadrzadeh.",
      "year" : 2011
    }, {
      "title" : "Galnet: WordNet 3.0 do galego",
      "author" : [ "Xavier Gómez Guinovart" ],
      "venue" : "Linguamática,",
      "citeRegEx" : "Guinovart.,? \\Q2011\\E",
      "shortCiteRegEx" : "Guinovart.",
      "year" : 2011
    }, {
      "title" : "Assessing polyseme sense similarity through co-predication acceptability and contextualised embedding distance",
      "author" : [ "Janosch Haber", "Massimo Poesio." ],
      "venue" : "Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics, pages 114–124,",
      "citeRegEx" : "Haber and Poesio.,? 2020",
      "shortCiteRegEx" : "Haber and Poesio.",
      "year" : 2020
    }, {
      "title" : "Do Word Meanings Exist? Computers and the Humanities, 34:205–215",
      "author" : [ "Patrick Hanks" ],
      "venue" : null,
      "citeRegEx" : "Hanks.,? \\Q2000\\E",
      "shortCiteRegEx" : "Hanks.",
      "year" : 2000
    }, {
      "title" : "Portuguese word embeddings: Evaluating on word analogies and natural language tasks",
      "author" : [ "Nathan Hartmann", "Erick Fonseca", "Christopher Shulby", "Marcos Treviso", "Jéssica Silva", "Sandra Aluísio." ],
      "venue" : "Proceedings of the 11th Brazilian Symposium in In-",
      "citeRegEx" : "Hartmann et al\\.,? 2017",
      "shortCiteRegEx" : "Hartmann et al\\.",
      "year" : 2017
    }, {
      "title" : "On the nature of the lexicon: The status of rich lexical meanings",
      "author" : [ "Lotte Hogeweg", "Agustin Vicente." ],
      "venue" : "Journal of Linguistics, 56(4):865–891.",
      "citeRegEx" : "Hogeweg and Vicente.,? 2020",
      "shortCiteRegEx" : "Hogeweg and Vicente.",
      "year" : 2020
    }, {
      "title" : "Improving word representations via global context and multiple word prototypes",
      "author" : [ "Eric Huang", "Richard Socher", "Christopher Manning", "Andrew Ng." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Vol-",
      "citeRegEx" : "Huang et al\\.,? 2012",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2012
    }, {
      "title" : "I don’t believe in word senses",
      "author" : [ "Adam Kilgarriff." ],
      "venue" : "Computers and the Humanities, 31(2):91–113.",
      "citeRegEx" : "Kilgarriff.,? 1997",
      "shortCiteRegEx" : "Kilgarriff.",
      "year" : 1997
    }, {
      "title" : "GDEX: Automatically finding good dictionary examples in a corpus",
      "author" : [ "Adam Kilgarriff", "Milos Husák", "Katy McAdam", "Michael Rundell", "Pavel Rychlỳ." ],
      "venue" : "Proceedings of the XIII EURALEX international congress, pages 425–432. Documenta Uni-",
      "citeRegEx" : "Kilgarriff et al\\.,? 2008",
      "shortCiteRegEx" : "Kilgarriff et al\\.",
      "year" : 2008
    }, {
      "title" : "Predication",
      "author" : [ "Walter Kintsch." ],
      "venue" : "Cognitive science, 25(2):173–202.",
      "citeRegEx" : "Kintsch.,? 2001",
      "shortCiteRegEx" : "Kintsch.",
      "year" : 2001
    }, {
      "title" : "Not all ambiguous words are created equal: An EEG investigation of homonymy and polysemy",
      "author" : [ "Ekaterini Klepousniotou", "G Bruce Pike", "Karsten Steinhauer", "Vincent Gracco." ],
      "venue" : "Brain and language, 123(1):11–21.",
      "citeRegEx" : "Klepousniotou et al\\.,? 2012",
      "shortCiteRegEx" : "Klepousniotou et al\\.",
      "year" : 2012
    }, {
      "title" : "Europarl: A Parallel Corpus for Statistical Machine Translation",
      "author" : [ "Philipp Koehn." ],
      "venue" : "Conference Proceedings: the tenth Machine Translation Summit, volume 5, pages 79–86. AAMT.",
      "citeRegEx" : "Koehn.,? 2005",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2005
    }, {
      "title" : "Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language",
      "author" : [ "Yuri Kuratov", "Mikhail Arkhipov." ],
      "venue" : "Computational Linguistics and Intellectual Technologies, 18:333–339.",
      "citeRegEx" : "Kuratov and Arkhipov.,? 2019",
      "shortCiteRegEx" : "Kuratov and Arkhipov.",
      "year" : 2019
    }, {
      "title" : "Word meaning in minds and machines",
      "author" : [ "Brenden M. Lake", "Gregory L. Murphy." ],
      "venue" : "ArXiv preprint: 2008.01766.",
      "citeRegEx" : "Lake and Murphy.,? 2020",
      "shortCiteRegEx" : "Lake and Murphy.",
      "year" : 2020
    }, {
      "title" : "Linguistic semantics: An introduction",
      "author" : [ "John Lyons." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Lyons.,? 1995",
      "shortCiteRegEx" : "Lyons.",
      "year" : 1995
    }, {
      "title" : "Sustained meaning activation for polysemous but not homonymous words: Evidence from EEG",
      "author" : [ "Lucy J MacGregor", "Jennifer Bouwsema", "Ekaterini Klepousniotou." ],
      "venue" : "Neuropsychologia, 68:126–138.",
      "citeRegEx" : "MacGregor et al\\.,? 2015",
      "shortCiteRegEx" : "MacGregor et al\\.",
      "year" : 2015
    }, {
      "title" : "A distributional model of semantic context effects in lexical processing",
      "author" : [ "Scott McDonald", "Chris Brew." ],
      "venue" : "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 17–24, Barcelona, Spain.",
      "citeRegEx" : "McDonald and Brew.,? 2004",
      "shortCiteRegEx" : "McDonald and Brew.",
      "year" : 2004
    }, {
      "title" : "UMAP: Uniform Manifold Approximation and Projection",
      "author" : [ "Leland McInnes", "John Healy", "Nathaniel Saul", "Lukas Großberger." ],
      "venue" : "Journal of Open Source Software, 3(29):861.",
      "citeRegEx" : "McInnes et al\\.,? 2018",
      "shortCiteRegEx" : "McInnes et al\\.",
      "year" : 2018
    }, {
      "title" : "context2vec: Learning generic context embedding with bidirectional LSTM",
      "author" : [ "Oren Melamud", "Jacob Goldberger", "Ido Dagan." ],
      "venue" : "Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pages 51–61, Berlin,",
      "citeRegEx" : "Melamud et al\\.,? 2016",
      "shortCiteRegEx" : "Melamud et al\\.",
      "year" : 2016
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "Workshop Proceedings of the International Conference on Learning Representations (ICLR) 2013. ArXiv preprint",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "WordNet: a lexical",
      "author" : [ "George A Miller" ],
      "venue" : null,
      "citeRegEx" : "Miller.,? \\Q1995\\E",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "XL-WiC: A multilingual benchmark for evaluating semantic contextualization",
      "author" : [ "Alessandro Raganato", "Tommaso Pasini", "Jose CamachoCollados", "Mohammad Taher Pilehvar." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Raganato et al\\.,? 2020",
      "shortCiteRegEx" : "Raganato et al\\.",
      "year" : 2020
    }, {
      "title" : "Visualizing and Measuring the Geometry of BERT",
      "author" : [ "Emily Reif", "Ann Yuan", "Martin Wattenberg", "Fernanda B Viegas", "Andy Coenen", "Adam Pearce", "Been Kim." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32, pages 8594–8603. Curran",
      "citeRegEx" : "Reif et al\\.,? 2019",
      "shortCiteRegEx" : "Reif et al\\.",
      "year" : 2019
    }, {
      "title" : "A primer in bertology: What we know about how bert works",
      "author" : [ "Anna Rogers", "Olga Kovaleva", "Anna Rumshisky." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:842–866.",
      "citeRegEx" : "Rogers et al\\.,? 2020",
      "shortCiteRegEx" : "Rogers et al\\.",
      "year" : 2020
    }, {
      "title" : "Língua somos: A construção da ideia de língua e da identidade coletiva na galiza (pré-) constitucional",
      "author" : [ "Roberto Samartim." ],
      "venue" : "Novas achegas ao estudo da cultura galega II: enfoques socio-históricos e lingüístico-literarios, pages 27–36.",
      "citeRegEx" : "Samartim.,? 2012",
      "shortCiteRegEx" : "Samartim.",
      "year" : 2012
    }, {
      "title" : "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "Proceedings of the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "Cross-lingual alignment of contextual word embeddings, with applications to zeroshot dependency parsing",
      "author" : [ "Tal Schuster", "Ori Ram", "Regina Barzilay", "Amir Globerson." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of",
      "citeRegEx" : "Schuster et al\\.,? 2019",
      "shortCiteRegEx" : "Schuster et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatic word sense discrimination",
      "author" : [ "Hinrich Schütze." ],
      "venue" : "Computational Linguistics, 24(1):97–123.",
      "citeRegEx" : "Schütze.,? 1998",
      "shortCiteRegEx" : "Schütze.",
      "year" : 1998
    }, {
      "title" : "BERTimbau: pretrained BERT models for Brazilian Portuguese",
      "author" : [ "Fábio Souza", "Rodrigo Nogueira", "Roberto Lotufo." ],
      "venue" : "9th Brazilian Conference on Intelligent Systems, BRACIS, Rio Grande do Sul, Brazil, October 20-23 (to appear).",
      "citeRegEx" : "Souza et al\\.,? 2020",
      "shortCiteRegEx" : "Souza et al\\.",
      "year" : 2020
    }, {
      "title" : "UDPipe at SIGMORPHON 2019: Contextualized embeddings, regularization with morphological categories, corpora merging",
      "author" : [ "Milan Straka", "Jana Straková", "Jan Hajic." ],
      "venue" : "Proceedings of the 16th Workshop on Computational Research in Phonetics,",
      "citeRegEx" : "Straka et al\\.,? 2019",
      "shortCiteRegEx" : "Straka et al\\.",
      "year" : 2019
    }, {
      "title" : "Ambiguity, polysemy, and vagueness",
      "author" : [ "David Tuggy." ],
      "venue" : "Cognitive linguistics, 4(3):273–290.",
      "citeRegEx" : "Tuggy.,? 1993",
      "shortCiteRegEx" : "Tuggy.",
      "year" : 1993
    }, {
      "title" : "Attention Is All You Need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "ArXiv preprint arXiv:1706.03762.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Probing pretrained language models for lexical semantics",
      "author" : [ "Ivan Vulić", "Edoardo Maria Ponti", "Robert Litschko", "Goran Glavaš", "Anna Korhonen." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Vulić et al\\.,? 2020",
      "shortCiteRegEx" : "Vulić et al\\.",
      "year" : 2020
    }, {
      "title" : "CCNet: Extracting high quality monolingual datasets from web crawl data",
      "author" : [ "Guillaume Wenzek", "Marie-Anne Lachaux", "Alexis Conneau", "Vishrav Chaudhary", "Francisco Guzmán", "Armand Joulin", "Edouard Grave." ],
      "venue" : "Proceedings of the 12th Lan-",
      "citeRegEx" : "Wenzek et al\\.,? 2020",
      "shortCiteRegEx" : "Wenzek et al\\.",
      "year" : 2020
    }, {
      "title" : "Does BERT Make Any Sense? Interpretable Word Sense Disambiguation with Contextualized Embeddings",
      "author" : [ "Gregor Wiedemann", "Steffen Remus", "Avi Chawla", "Chris Biemann." ],
      "venue" : "Proceedings of the 15th Conference on Natural Language Pro-",
      "citeRegEx" : "Wiedemann et al\\.,? 2019",
      "shortCiteRegEx" : "Wiedemann et al\\.",
      "year" : 2019
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Assessing phrasal representation and composition in transformers",
      "author" : [ "Lang Yu", "Allyson Ettinger." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4896–4907, Online. Association for Computa-",
      "citeRegEx" : "Yu and Ettinger.,? 2020",
      "shortCiteRegEx" : "Yu and Ettinger.",
      "year" : 2020
    }, {
      "title" : "2020), sentences with a high",
      "author" : [ "lowing Raffel" ],
      "venue" : null,
      "citeRegEx" : "Raffel,? \\Q2020\\E",
      "shortCiteRegEx" : "Raffel",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "Contrary to static vector models, which represent the different senses of a word in a single vector (Erk, 2012; Mikolov et al., 2013), contextualized models generate representations at token-level (Peters et al.",
      "startOffset" : 100,
      "endOffset" : 133
    }, {
      "referenceID" : 36,
      "context" : "Contrary to static vector models, which represent the different senses of a word in a single vector (Erk, 2012; Mikolov et al., 2013), contextualized models generate representations at token-level (Peters et al.",
      "startOffset" : 100,
      "endOffset" : 133
    }, {
      "referenceID" : 11,
      "context" : ", 2013), contextualized models generate representations at token-level (Peters et al., 2018; Devlin et al., 2019), thus being an interesting approach to model word meaning in context.",
      "startOffset" : 71,
      "endOffset" : 113
    }, {
      "referenceID" : 39,
      "context" : "In this regard, several studies have shown that clusters produced by some contextualized word embeddings (CWEs) are related to different senses of the same word (Reif et al., 2019; Wiedemann et al., 2019), or that similar senses can be aligned in cross-lingual experiments (Schuster et al.",
      "startOffset" : 161,
      "endOffset" : 204
    }, {
      "referenceID" : 51,
      "context" : "In this regard, several studies have shown that clusters produced by some contextualized word embeddings (CWEs) are related to different senses of the same word (Reif et al., 2019; Wiedemann et al., 2019), or that similar senses can be aligned in cross-lingual experiments (Schuster et al.",
      "startOffset" : 161,
      "endOffset" : 204
    }, {
      "referenceID" : 43,
      "context" : ", 2019), or that similar senses can be aligned in cross-lingual experiments (Schuster et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : ", word forms that have different related meanings depending on the context (Apresjan, 1974)), have shown that even though CWEs present some correlations with human judgments (Nair et al.",
      "startOffset" : 75,
      "endOffset" : 91
    }, {
      "referenceID" : 19,
      "context" : ", 2020), they fail to predict the similarity of the various senses of a polysemous word (Haber and Poesio, 2020).",
      "startOffset" : 88,
      "endOffset" : 112
    }, {
      "referenceID" : 15,
      "context" : "As classical datasets to evaluate the capabilities of vector representations consist of single words without context (Finkelstein et al., 2001) or heavily",
      "startOffset" : 117,
      "endOffset" : 143
    }, {
      "referenceID" : 26,
      "context" : "constrained expressions (Kintsch, 2001; Mitchell and Lapata, 2008), new resources with annotations of words in free contexts have been created, including both graded similarities (Huang et al.",
      "startOffset" : 24,
      "endOffset" : 66
    }, {
      "referenceID" : 23,
      "context" : "constrained expressions (Kintsch, 2001; Mitchell and Lapata, 2008), new resources with annotations of words in free contexts have been created, including both graded similarities (Huang et al., 2012; Armendariz et al., 2020) or binary classification",
      "startOffset" : 179,
      "endOffset" : 224
    }, {
      "referenceID" : 3,
      "context" : "constrained expressions (Kintsch, 2001; Mitchell and Lapata, 2008), new resources with annotations of words in free contexts have been created, including both graded similarities (Huang et al., 2012; Armendariz et al., 2020) or binary classification",
      "startOffset" : 179,
      "endOffset" : 224
    }, {
      "referenceID" : 47,
      "context" : "as the nuances between different senses depend on non-linguistic factors such as the annotator procedure or the target task (Tuggy, 1993; Kilgarriff, 1997; Hanks, 2000; Erk, 2010).",
      "startOffset" : 124,
      "endOffset" : 179
    }, {
      "referenceID" : 24,
      "context" : "as the nuances between different senses depend on non-linguistic factors such as the annotator procedure or the target task (Tuggy, 1993; Kilgarriff, 1997; Hanks, 2000; Erk, 2010).",
      "startOffset" : 124,
      "endOffset" : 179
    }, {
      "referenceID" : 20,
      "context" : "as the nuances between different senses depend on non-linguistic factors such as the annotator procedure or the target task (Tuggy, 1993; Kilgarriff, 1997; Hanks, 2000; Erk, 2010).",
      "startOffset" : 124,
      "endOffset" : 179
    }, {
      "referenceID" : 12,
      "context" : "as the nuances between different senses depend on non-linguistic factors such as the annotator procedure or the target task (Tuggy, 1993; Kilgarriff, 1997; Hanks, 2000; Erk, 2010).",
      "startOffset" : 124,
      "endOffset" : 179
    }, {
      "referenceID" : 27,
      "context" : "In contrast to polysemy, there is a strong consensus concerning the representation of homonymous senses in the lexicon, and it has been shown that homonyms are cognitively processed differently than polysemous words (Klepousniotou et al., 2012; MacGregor et al., 2015).",
      "startOffset" : 216,
      "endOffset" : 268
    }, {
      "referenceID" : 32,
      "context" : "In contrast to polysemy, there is a strong consensus concerning the representation of homonymous senses in the lexicon, and it has been shown that homonyms are cognitively processed differently than polysemous words (Klepousniotou et al., 2012; MacGregor et al., 2015).",
      "startOffset" : 216,
      "endOffset" : 268
    }, {
      "referenceID" : 30,
      "context" : "In this regard, exploratory experiments in English suggest that some CWEs correctly model homonymy, approximating the contextualized vectors of a homonym to those of its paraphrases (Lake and Murphy, 2020), and showing stronger correlation with human judgments to those",
      "startOffset" : 182,
      "endOffset" : 205
    }, {
      "referenceID" : 48,
      "context" : "The results suggest that the best monolingual models based on Transformers (Vaswani et al., 2017) can identify homonyms having different meanings adequately.",
      "startOffset" : 75,
      "endOffset" : 97
    }, {
      "referenceID" : 44,
      "context" : "A variety of approaches has been implemented to compute word meaning in context by means of standard methods of distributional semantics (Schütze, 1998; Kintsch, 2001; McDonald and Brew, 2004; Erk and Padó, 2008).",
      "startOffset" : 137,
      "endOffset" : 212
    }, {
      "referenceID" : 26,
      "context" : "A variety of approaches has been implemented to compute word meaning in context by means of standard methods of distributional semantics (Schütze, 1998; Kintsch, 2001; McDonald and Brew, 2004; Erk and Padó, 2008).",
      "startOffset" : 137,
      "endOffset" : 212
    }, {
      "referenceID" : 33,
      "context" : "A variety of approaches has been implemented to compute word meaning in context by means of standard methods of distributional semantics (Schütze, 1998; Kintsch, 2001; McDonald and Brew, 2004; Erk and Padó, 2008).",
      "startOffset" : 137,
      "endOffset" : 212
    }, {
      "referenceID" : 14,
      "context" : "A variety of approaches has been implemented to compute word meaning in context by means of standard methods of distributional semantics (Schütze, 1998; Kintsch, 2001; McDonald and Brew, 2004; Erk and Padó, 2008).",
      "startOffset" : 137,
      "endOffset" : 212
    }, {
      "referenceID" : 5,
      "context" : "As compositional distributional models construct sentence representations from their constituents vectors, they take into account contextualization effects on meaning (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Baroni, 2013).",
      "startOffset" : 167,
      "endOffset" : 237
    }, {
      "referenceID" : 4,
      "context" : "As compositional distributional models construct sentence representations from their constituents vectors, they take into account contextualization effects on meaning (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Baroni, 2013).",
      "startOffset" : 167,
      "endOffset" : 237
    }, {
      "referenceID" : 17,
      "context" : "evaluate them are composed of highly restricted phrases (Grefenstette and Sadrzadeh, 2011).",
      "startOffset" : 56,
      "endOffset" : 90
    }, {
      "referenceID" : 35,
      "context" : "The rise of artificial neural networks on natural language processing popularized the use of vector representations, and the remarkable performance of neural language models (Melamud et al., 2016; Peters et al., 2018) led to a productive line of research exploring to what extent these models represent linguistic knowledge (Rogers et al.",
      "startOffset" : 174,
      "endOffset" : 217
    }, {
      "referenceID" : 40,
      "context" : ", 2018) led to a productive line of research exploring to what extent these models represent linguistic knowledge (Rogers et al., 2020).",
      "startOffset" : 114,
      "endOffset" : 135
    }, {
      "referenceID" : 11,
      "context" : "(2019) found that clusters of BERT embeddings (Devlin et al., 2019) seem to be related to word senses, while Schuster et al.",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 23,
      "context" : "There are several resources to evaluate word meaning in free contexts, such as the Stanford Contextual Word Similarity (Huang et al., 2012) and CoSimLex (Armendariz et al.",
      "startOffset" : 119,
      "endOffset" : 139
    }, {
      "referenceID" : 3,
      "context" : ", 2012) and CoSimLex (Armendariz et al., 2020), both representing word similarity on a graded scale, or the Word-in-Context datasets (WiC), focused on binary classifications (i.",
      "startOffset" : 21,
      "endOffset" : 46
    }, {
      "referenceID" : 38,
      "context" : ", each evaluation item contains two sentences with the same word form, having the same or different senses) (Pilehvar and CamachoCollados, 2019; Raganato et al., 2020).",
      "startOffset" : 108,
      "endOffset" : 167
    }, {
      "referenceID" : 22,
      "context" : "homonyms as different entries in the lexicon (in contrast to polysemy, for which there is a long discussion on whether senses of polysemous words are stored as a single core representation or as independent entries (Hogeweg and Vicente, 2020)).",
      "startOffset" : 215,
      "endOffset" : 242
    }, {
      "referenceID" : 27,
      "context" : "In fact, several studies have shown that homonyms are cognitively processed differently from polysemous words (Klepousniotou et al., 2012; Rabagliati and Snedeker, 2013).",
      "startOffset" : 110,
      "endOffset" : 169
    }, {
      "referenceID" : 32,
      "context" : "neously activated, the meanings of homonyms are in conflict during processing, with the not relevant ones being deactivated by the context (MacGregor et al., 2015).",
      "startOffset" : 139,
      "endOffset" : 163
    }, {
      "referenceID" : 10,
      "context" : ", newspaper as an organization or as a set of printed pages), it is often assumed that homonyms are different lexemes that have the same lexical form (Cruse, 1986), and therefore they are stored as independent entries in the lexicon (Pustejovsky, 1998).",
      "startOffset" : 150,
      "endOffset" : 163
    }, {
      "referenceID" : 41,
      "context" : "However, they are divided in this resource, as Galician has recently been standardized using a Spanish-based orthography that formally separates it from Portuguese (Samartim, 2012).",
      "startOffset" : 164,
      "endOffset" : 180
    }, {
      "referenceID" : 37,
      "context" : "sources for language learning, together with WordNet and other lexicographic data (Miller, 1995; Montraveta and Vázquez, 2010; Guinovart, 2011; Rademaker et al., 2014).",
      "startOffset" : 82,
      "endOffset" : 167
    }, {
      "referenceID" : 18,
      "context" : "sources for language learning, together with WordNet and other lexicographic data (Miller, 1995; Montraveta and Vázquez, 2010; Guinovart, 2011; Rademaker et al., 2014).",
      "startOffset" : 82,
      "endOffset" : 167
    }, {
      "referenceID" : 47,
      "context" : "the extreme of the homonymy-polysemy-vagueness scale (Tuggy, 1993)).",
      "startOffset" : 53,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : "These homonyms were then enriched with frequency data from large corpora: Wikipedia and SLI GalWeb (Agerri et al., 2018) for Galician, and a combination of Wikipedia",
      "startOffset" : 99,
      "endOffset" : 120
    }, {
      "referenceID" : 28,
      "context" : "and Europarl for English, Spanish and Portuguese (Koehn, 2005).",
      "startOffset" : 49,
      "endOffset" : 62
    }, {
      "referenceID" : 31,
      "context" : "From these lists, each linguist selected the most frequent homonyms, annotating them as ambiguous at type or token level (absolute homonymy and partial homonymy in Lyons’ terms (Lyons, 1995)).",
      "startOffset" : 177,
      "endOffset" : 190
    }, {
      "referenceID" : 25,
      "context" : "Sentences were selected, adapted, and simplified using GDEX-inspired constraints (Kilgarriff et al., 2008) (i.",
      "startOffset" : 81,
      "endOffset" : 106
    }, {
      "referenceID" : 8,
      "context" : "We then computed the Cohen’s κ inter-annotator agreement (Cohen, 1960) between the original resource and the outcome of this second annotation (see the right column of Table 2).",
      "startOffset" : 57,
      "endOffset" : 70
    }, {
      "referenceID" : 6,
      "context" : "Static embeddings: We have used skip-gram fastText models of 300 dimensions (Bojanowski et al., 2017).",
      "startOffset" : 76,
      "endOffset" : 101
    }, {
      "referenceID" : 9,
      "context" : ", 2019), XLM-RoBERTa (Base, 12 layers) (Conneau et al., 2020), and DistilBERT (DistilmBERT, 6 layers) (Sanh et al.",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 42,
      "context" : ", 2020), and DistilBERT (DistilmBERT, 6 layers) (Sanh et al., 2019).",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 45,
      "context" : "For Portuguese and Spanish, BERTimbau (Souza et al., 2020) and BETO (Cañete et al.",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 11,
      "context" : "tokens [CLS] and [SEP]), each of them represented by the standard approach of concatenating the last 4 layers (Devlin et al., 2019).",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 26,
      "context" : "Given a sentence triple where two of the target words (a and b) have the same sense and the third (c) a different one, we evaluate a model as follows (in a similar way as other studies (Kintsch, 2001; Lake and Murphy, 2020)): First, we obtain",
      "startOffset" : 185,
      "endOffset" : 223
    }, {
      "referenceID" : 30,
      "context" : "Given a sentence triple where two of the target words (a and b) have the same sense and the third (c) a different one, we evaluate a model as follows (in a similar way as other studies (Kintsch, 2001; Lake and Murphy, 2020)): First, we obtain",
      "startOffset" : 185,
      "endOffset" : 223
    } ],
    "year" : 2021,
    "abstractText" : "This paper presents a multilingual study of word meaning representations in context. We assess the ability of both static and contextualized models to adequately represent different lexical-semantic relations, such as homonymy and synonymy. To do so, we created a new multilingual dataset that allows us to perform a controlled evaluation of several factors such as the impact of the surrounding context or the overlap between words, conveying the same or different senses. A systematic assessment on four scenarios shows that the best monolingual models based on Transformers can adequately disambiguate homonyms in context. However, as they rely heavily on context, these models fail at representing words with different senses when occurring in similar sentences. Experiments are performed in Galician, Portuguese, English, and Spanish, and both the dataset (with more than 3,000 evaluation items) and new models are freely released with this study.",
    "creator" : "LaTeX with hyperref"
  }
}