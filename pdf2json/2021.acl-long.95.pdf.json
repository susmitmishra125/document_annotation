{
  "name" : "2021.acl-long.95.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "AUGNLG: Few-shot Natural Language Generation using Self-trained Data Augmentation",
    "authors" : [ "Xinnuo Xu", "Guoyin Wang", "Young-Bum Kim", "Sungjin Lee" ],
    "emails" : [ "xx6@hw.ac.uk,", "sungjinl@amazon.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1183–1195\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1183\nNatural Language Generation (NLG) is a key component in a task-oriented dialogue system, which converts the structured meaning representation (MR) to the natural language. For large-scale conversational systems, where it is common to have over hundreds of intents and thousands of slots, neither template-based approaches nor model-based approaches are scalable. Recently, neural NLGs started leveraging transfer learning and showed promising results in few-shot settings. This paper proposes AUGNLG, a novel data augmentation approach that combines a self-trained neural retrieval model with a few-shot learned NLU model, to automatically create MR-toText data from open-domain texts. The proposed system mostly outperforms the state-ofthe-art methods on the FEWSHOTWOZ data in both BLEU and Slot Error Rate. We further confirm improved results on the FEWSHOTSGD data and provide comprehensive analysis results on key components of our system. Our code and data are available at https: //github.com/XinnuoXu/AugNLG."
    }, {
      "heading" : "1 Introduction",
      "text" : "Large-scale conversational systems provide a natural interface to achieve various daily-life tasks. Natural Language Generation (NLG) is a key component in such a system to convert the structured meaning representation (MR) to the natural language, as shown in Figure 1. In task-oriented dialogue systems, NLG is typically accomplished by filling out a basic set of developer-provided templates, leading to a conversational system generating unnatural, robotic responses. In order to make the system sound more human-like, model-based NLG approaches, in particular neural models, have recently been gaining an increasing traction (Gao et al., 2018; Wen et al., 2015). However, neither the template-based approaches nor the model-based\napproaches are sufficiently scalable for large-scale conversational systems, where it is common to have over hundreds of intents and thousands of slots.\nWith the rise of neural transfer learning for NLP using pretrained LMs, recently, neural NLGs started to leverage transfer learning and showed some promising results (Radford et al., 2019; Brown et al., 2020; Dai et al., 2019; Edunov et al., 2019). In particular, Peng et al. (2020) proposed FEWSHOTWOZ, the first NLG benchmark test in few-shot learning settings, and achieved a SOTA performance by leveraging existing MR-to-Text data sets via task-specific continued pre-training. Despite the improved result, their approach leaves little room for further improvements as MR-to-Text data are expensive to obtain for new domains, practically circling back to the same scalability problem after exhausting the existing data.\nIn order to go beyond this restriction, this paper proposes AUGNLG, a novel data augmentation approach, that automatically creates MR-toText data from open-domain texts by combining a self-trained neural retrieval model with a few-shot learned NLU model. Since our data augmentation approach is orthogonal to the prior transfer learning approaches, one can use our approach in conjunction with other approaches. In experiments, we empirically show that AUGNLG mostly boosts the performance of both the fine-tuned GPT-2 (FTGPT) (Radford et al., 2019) and SC-GPT (Peng et al., 2020), the continued pretraining approach with existing MR-to-Text data, on the FEWSHOT-\nWOZ task. Furthermore, we construct another fewshot learning testbed, FEWSHOTSGD, out of the Schema-Guided Dialogue (SGD) corpus (Rastogi et al., 2020) and confirm improved results by applying AUGNLG to the FT-GPT. 1 Finally, we provide comprehensive analysis results on the key components of our system to gain detailed insights into the relationship between component-wise behavior and various parameters."
    }, {
      "heading" : "2 Related Work",
      "text" : "NLG for Dialogue Response Generation There has been a body of work on neural NLG models, adopting various architectures, such as RNNs (Wen et al., 2015), attention RNNs (Dušek and Jurčı́ček, 2016), SC-LSTM (Wen et al., 2016), T2G2 (Kale and Rastogi, 2020), AdapterCL (Madotto et al., 2020) and associated variants (Tran and Le Nguyen, 2017; Tran et al., 2017). Despite the improved flexibility and naturalness over template-based methods, neural approaches require large amounts of annotated data to reach good performance. Data Augmentation Data augmentation has been widely applied to a variety of NLP tasks, including sentence classification (Xie et al., 2020), natural language inference (Hu et al., 2019) and spoken language understanding (Li et al., 2019; Quan and Xiong, 2019; Zhao et al., 2019). Prior approaches for text data utilized back-translation (Sennrich et al., 2016; Edunov et al., 2018), c-BERT word replacement (Jiao et al., 2020), mixed labels and representations (Guo et al., 2019; Chen et al., 2020) and paraphrase data (Gao et al., 2020). However, the range of augmented data will be inherently limited, particularly in few-shot learning settings due to the nature of prior approaches, which only leverages in-domain data. In contrast, we take a rarely explored approach, tapping into a wealth of opendomain text that covers almost all topics. Recently, Du et al. (2021) proposed a self-training method\n1Since SGD accounts for a large portion of the existing MR-to-Text data that SC-GPT utilized in training, we could not apply AUGNLG to SC-GPT for the FEWSHOTSGD task.\nto augment data for NLU tasks by retrieving sentences from data crawled on the web. However, their method cannot be directly applied to the NLG problem since it does not yield MR annotations. Our approach, in contrast, generates MR-to-Text data by jointly employing a self-trained neural retrieval model with a few-shot learned NLU model."
    }, {
      "heading" : "3 Few-shot Transfer Learning for NLG",
      "text" : "The goal of NLG is to translate an MR A into its natural language response x = [ x1, . . . , xT ] , where xi is the ith token in the sequence x and T is the sequence length. A is defined as the combination of intent I and slot-value pairs {(si, vi)}Pi=1:\nA = {I, (s1, v1), . . . , (sP , vP )}, (1) where the intent stands for the illocutionary type\nof the system action while slot-value pairs indicate category names and their values to embed in the utterance. For example, in the MR, inform (food = chinese ; price = cheap), inform is the intent, food and price are two slot keys and chinese and cheap are the corresponding slot values.\nGiven in-domain MR-to-Text data D = {(An, xn)}Nn=1 for training, where N is the number of examples, a statistical neural language model parameterized by θ is adopted to characterize the conditional probability pθ(x|A). By adopting the chain rule on auto-regressive generation, the joint probability of x conditioned on A is decomposed as ∏T t=1 pθ(x\nt|x<t,A). The training process, i.e. the learning of θ, is then defined as maximizing the log-likelihood of the conditional probabilities over the entire training dataset:\nLθ(D) = |D|∑ n=1 log pθ(xn|An).\nIn the few-shot learning setup, the number of training examplesN is extremely small (e.g.≤ 50), which easily leads to non-fluent generated sentences with many grammar mistakes or missing pieces of information. In order to combat the data sparseness problem, inspired by prior transfer learning approaches, we introduce a three-step pipeline to gradually evolve a general large-scale language model to a domain-specific NLG model (shown in Figure 2): (1) pre-training a base language model with massive amounts of text, (2) NLG-specific continued pre-training with auto-augmented MRto-Text data, and (3) final fine-tuning with the limited in-domain MR-to-Text ground-truth data.\nSpecifically, in Step (1), we adopt GPT-2 (Radford et al., 2019) as our base language model since GPT-2 has demonstrated a remarkable performance on auto-regressive text generation tasks, which is close to MR-to-Text generation, in a variety of domains. However, GPT-2 is pre-trained on OpenWebText and the language style and topics thereof are quite different from those of daily conversations in a target domain. Furthermore, the generation task in NLG is conditioned on the input MR, as opposed to the unconditioned generation of the underlying GPT-2 pre-training task. Thus, to bring the model a step closer to the final NLG model in the target domain, in Step (2), we continuously pre-train the GPT-2 model on an automatically constructed set of augmented MR-to-Text pairs D′ = {(Am, xm)}Mm=1, where M is the number of augmented examples, which is much larger than the amount of in-domain ground-truth data. Data augmentation is achieved by retrieving a large amount of relevant text from Reddit (Henderson et al., 2019) with a self-trained neural retrieval model and then synthesizing MRs with a few-shot learned NLU model. The details of data augmentation is described in Section 4. Finally, in Step (3), we fine-tune the NLG model on a limited amount of in-domain ground-truth MR-to-Text pairs D for a final adaptation."
    }, {
      "heading" : "4 Data Augmentation",
      "text" : "The data augmentation procedure aims to construct a large amount of MR-to-Text pairs D′ from open-domain texts that are relevant to the in-domain ground-truth MR-to-Text pairs D. The augmentation process consists of two stages: (1)\nretrieving keyword-matching utterances and filtering out domain-irrelevant instances, (2) generating synthetic MR annotations. Figure 3 illustrates the overall pipeline with some examples. For further analysis and studies, we release the data from all intermediate steps for each domain at https://github.com/XinnuoXu/ AugNLG/tree/master/augmented_data."
    }, {
      "heading" : "4.1 Retrieval and Filtering",
      "text" : "The utterance retrieval and filtering procedure consists of three steps: (1) keyword extraction that collects n-gram keywords from all in-domain utterances X = {xn}Nn=1; (2) keyword-based retrieval that searches the open-domain texts for utterances that match any keywords extracted in the previous step, yielding a set of utterances X′cand; (3) self-trained neural classifier that filters out some retrieved utterances that are semantically irrelevant to the target domain. After the filtering, we form an augmented set of utterances X′ with the unfiltered utterances.\nKeywords Extraction. To efficiently extract keywords, we first gather all n-gram phrases that appear in X. Since some phrases are too general to be effective, e.g. “I cannot”, “is your”, we use TFIDF scores to measure the specificity of a phrase (see Appendix A for more detail). We first rank the collected n-grams according to their TF-IDF scores and filter out those n-gram phrases with relatively low TF-IDF score.\nKeyword-based Retrieval. Having extracted the keywords, we retrieve utterances from the opendomain utterance pool that contains at least one\nAlgorithm 1 Self-trained Neural Filtering Require: In-domain utterances X in the target do-\nmain; Retrieved utterances X′cand 1: U+ ← Positive examples X 2: U− ← Randomly selected negative examples 3: c0 ← Train(U+, U−) 4: L←Maximum number of iterations 5: l = 1; E+0 = U+; E − 0 = U− 6: while l ≤ L do 7: E+l ← {x\n′ if Predict(X′cand, cl−1) ≥ σ+} 8: E−l ← {x\n′ if Predict(X′cand, cl−1) ≤ σ−} 9: E+l ← E + l + U +\n10: if ∣∣E+l ∣∣− ∣∣E+l−1∣∣ ≤ δ then 11: Converged; Break 12: end if 13: cl ← Train(E+l , E − l ) 14: l← l + 1 15: end while 16: X′ ← {x′ if Predict(X′cand, cl) ≥ σ}\nextracted keyword in it. The aim of this step is to source a large amount of domain-relevant utterances X′cand based on the surface-level overlap.\nSelf-trained Neural Filtering. Although the keyword-based retrieval is efficient, the retrieved utterances X′cand can be quite noisy since an n-gram keyword only matches some part of the utterance, failing to detect the existence of irrelevant pieces in other parts. For example, in Figure 3, even though the utterance “With kids movies?” contains the keyword “with kids”, it is irrelevant to the target domain Restaurant given the word movies. Thus, we introduce a self-trained neural classifier to filter out domain-irrelevant utterances from X′cand by considering the semantic representation of an entire utterance and yield a domain-relevant set X′.\nThe algorithm of the self-training and filtering process is listed in Algorithm 1. We adopt a BERT (Devlin et al., 2019) model with a binary classification layer atop as the base model and then train the classifier with in-domain utterances X and randomly selected open-domain utterances2 , serving as positive and negative examples (U+ and U−), respectively. After that, the self-training and filtering cycle starts. At each iteration, we make predictions on the utterances in X′cand with the classifier\n2All utterances in X′cand are excluded from the opendomain utterance pool. To balance the precision and recall, we control the size of the initial negative set such that∣∣U−∣∣ = λ1 · ∣∣U+∣∣, where λ1 = 10.\ntrained in the previous iteration. All utterances with a score over the threshold σ+, together with the in-domain utterances X, are then taken as a new set of positive examples E+, whereas all utterances with a score less than the threshold σ− are collected as a new set of negative examples E−.3 The self-training loop terminates if either the increment of positive examples at the last iteration is less than the threshold δ or the iterations is over the pre-defined maximum number of iterations. Otherwise, a new classifier is trained on E+ and E− and the algorithm keeps going on the loop. Once the loop terminated, we label all utterances in X′cand with the classifier from the last iteration. Finally, we build a domain-relevant set of augmented utterances X′ by taking all utterances with a score over the threshold σ.4"
    }, {
      "heading" : "4.2 Synthetic MR Annotation",
      "text" : "Having built the domain-relevant set of augmented utterances X′, we now proceed to synthesize MR labels to produce a complete MR-to-Text dataset D′. To this end, we build a few-shot NLU model by fine-tuning a BERT model with in-domain groundtruth data. To put the data in the right format for the NLU task, we take MRs and utterances as labels and model inputs, respectively. Each token is annotated with the slot name if it is a part of the associated slot value and the final hidden state of the special token [CLS] is used to predict the intent (see Figure 5 in Appendix B). Finally, we generate an MR-to-Text dataset D′ by concatenating the utterances in X′ with the synthetic MR labels predicted by the few-shot NLU model."
    }, {
      "heading" : "5 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "5.1 Dataset",
      "text" : "Fewshot NLG Data FEWSHOTWOZ is a fewshot NLG benchmark, built upon RNNLG and MultiWOZ (Budzianowski et al., 2018). In each domain, MR-to-Text pairs are grouped according to their delexicalized MRs (i.e. slot values being masked) and a training set is created by taking a pair each from 50 random groups and then the rest are taken as the test set. We also construct a new dataset FEWSHOTSGD by applying the same\n3To guarantee the precision of the positive examples, we use σ+ = 0.99 and σ− = 0.5. Also, we sub-sample negative examples such that ∣∣E−∣∣ = λ2 · ∣∣E+∣∣, where λ2 = 5. 4To harvest a large amount of utterances, we set the threshold σ to 0.5.\npreparation steps to the SGD corpus. The comparison of FEWSHOTWOZ and FEWSHOTSGD is presented in the top section in Table 1. Comparing to FEWSHOTWOZ, FEWSHOTSGD has (1) more domains, (2) less intents, slots and delexicalized MRs5 (3) more testing examples for each delexicalized MR, (4) more novel n-grams6 in test utterances.\nAugmented Data Since Reddit has shown to provide natural conversational English data, we adopt Reddit (Henderson et al., 2019) as the open-domain utterance pool after filtering for utterances of length between 2 and 40, totalling about 0.7B utterances. The average number of extracted keywords, retrieved utterances, final augmented MR-to-Text pairs and delexicalized MRs over all domains in FEWSHOTWOZ and FEWSHOTSGD are shown in the bottom section of Table 1. The detailed breakdowns of each domain are listed in Table 9 and Table 10 in Appendix C."
    }, {
      "heading" : "5.2 Evaluation Metrics",
      "text" : "Following Wen et al. (2015) and Peng et al. (2020), we use BLEU score and Slot Error Rate (ERR) for automatic evaluation. BLEU score measures the surface-level similarity between generated responses and human-authored references. Whereas,\n5Note that, the average number of delexicalized MRs in the training set is 33, which means the number of training examples in some domains are less than 50.\n6The novelty is calculated by dividing the number of ngrams in the test set that does not appear in the training set by the number of n-grams in the test set.\n7https://github.com/pengbaolin/SC-GPT.\nERR measures the semantic alignment in terms of slot-value insertion and omission. Specifically, ERR = (p+ q)/M , where M is the total number of slots in the MR and p, q are the number of missing and redundant slots in the surface realisation. Since the SGD dataset does not provide enough information to compute ERR, we report ERR only on FEWSHOTWOZ."
    }, {
      "heading" : "5.3 Systems",
      "text" : "We apply our data augmentation approach AUGNLG to two baseline systems,\n• FT-GPT GPT-2 is directly fine-tuned on the in-domain ground-truth MR-to-Text data. We introduce AUGNLG-FT, which further pretrains GPT-2 on the augmented MR-to-Text data and performs a final fine-tuning on the in-domain data.\n• SC-GPT (Peng et al., 2020) further pre-trains GPT-2 on existing MR-to-Text data borrowed from other NLG corpora and fine-tunes on the in-domain data. We introduce AUGNLGSC, which pre-trains GPT-2 on both existing MR-to-Text data and automatically augmented data, and finally fine-tunes on the indomain data."
    }, {
      "heading" : "6 Results",
      "text" : "FEWSHOTWOZ Table 2 reports the results on FEWSHOTWOZ. AUGNLG-FT substantially outperforms FT-GPT across all domains in both BLEU and ERR. Similarly, AUGNLG-SC performs better than SC-GPT and achieves the state-of-theart performance in most domains. Remarkably, AUGNLG-FT achieves a competitive performance with SC-GPT in many domains without leveraging any existing MR-to-Text data. It even outperforms SC-GPT in “TV” and “Attraction” domain in both BLEU and ERR.\nFEWSHOTSGD Table 3 shows the results in FEWSHOTSGD. Due to the higher novelty of the test examples and the smaller amount of training examples (see Avg. # Test Novelty n-gram and # Training Instances in Table 1), FT-GPT performs worse than on FEWSHOTWOZ. This indicates that the few-shot settings on FEWSHOTSGD are even more challenging. But AUGNLG-FT managed to outperform FT-GPT by a large margin via the continued pre-training on the augmented examples.\nQualitative Evaluation Table 4 compares some generated utterances by different models on FEWSHOTWOZ (examples in FEWSHOTSGD are shown in Table 16 in Appendix E). Both FT-GPT and SC-GPT are prone to omit important slots. Comparing to SC-GPT, FT-GPT tends to overgenerate and introduces hallucinations. However, AUGNLG and AUGNLG-SC managed to generate fluent, natural text while precisely reflecting the the input MR. We further examined 70 randomly sampled utterances generated by AUGNLG-SC, whose BLEU scores are lower than those generated by SCGPT, in the “Hotel”, “Train” and “Taxi” domain to understand some potential factors causing the lower BLEU scores We found that the lower BLEU scores are mainly driven by BLEU penalizing semantically correct paraphrases due to the nature of BLEU only checking surface-level matches. Some examples of such penalization are provided in Table 15 in Appendix E. Only 7 out of the 70 manually checked examples generated by AUGNLG-SC are actually worse than SC-GPT.8\nIn sum, the results (1) verify the effectiveness of complementing existing transfer learning methods with our novel data augmentation approach; (2) reveal that automatically augmented MR-to-Text data alone can lead to a competitive performance, previously only achieved with existing MR-to-Text data. Since existing MR-to-Text data is not a scalable data source, our approach brings more practical values to real-world applications; (3) indicate that\n8We also examined 70 randomly sampled utterances generated by AUGNLG-SC, whose BLEU scores are equal/higher than those generated by SC-GPT. Among these examples, 35 examples are actually better and 7 examples are worse than the SC-GPT generations.\nleveraging augmented MR-to-Text data on top of existing MR-to-Text data yields a new SOTA performance on the benchmark test."
    }, {
      "heading" : "7 In-depth Analysis",
      "text" : "In this section, we provide comprehensive analysis results on the key components and parameters of our system to gain detailed insights: (1) intrinsic evaluation on augmented data, (2) influence of NLU quality, and (3) performance trends over varying amounts of augmented data."
    }, {
      "heading" : "7.1 Intrinsic Evaluation on Augmented Data",
      "text" : "For intrinsic evaluation of augmented data, we first introduce four metrics: • MR coverage (MR Cov.) evaluates the cov-\nerage of delexicalized MRs of the test set in the augmented set:\nMR Cov. = # delexicalized MRs ∈ A′ ∩ Atest\n# delexicalized MRs ∈ Atest ,\nwhere A′ and Atest denote delexicalized MRs in the augmented set and the test set, respectively. Higher MR Cov. values indicate that more delexicalized MRs of the test set appear in the augmented set. • Slot coverage (SL Cov.) evaluates the coverage\nof slot keys of the test set in the augmented set. • Language model perplexity (PPL) is the per-\nplexity of augmented utterances calculated by a GPT-2 language model fine-tuned on the test set. Lower PPL values indicate that the distribution of augmented utterances is close to that of the test utterances. • Average n-gram novelty (Nvt.) N-gram novelty\nmeasures the fraction of the n-grams in the test set\nthat do not appear in the augmented set:\nN-gram novelty = 1− # n-grams ∈ X ′ ∩ Xtest\n# n-grams ∈ Xtest ,\nwhere X′ and Xtest denote utterances in the augmented set and test set, respectively. Lower Nvt. values indicate that more n-grams of the test set appear in the augmented set. We consider from 1-grams to 4-grams and report the average value.\nThe results of MR Cov. / SL Cov. on FEWSHOT-\nWOZ and FEWSHOTSGD are shown in Table 5 and Table 6, respectively. SL Cov. achieves 70% in most domains on both datasets while MR Cov. has a wide range of values across domains. Noteworthily, Table 6 strongly correlates with Table 3 – “Banks” and “Media” domains are worse than other domains in both coverage metrics and NLG performance. On the other hand, “Restaurants” and “Events” domains are better than the others in both aspects. Although we do not see the same pattern on FEWSHOTWOZ, it could be attributed to the large variance in the number of delexicalized MRs in each domain (see Table 2 in (Peng et al., 2020)).\nThe results of PPL and Nvt. on FEWSHOTWOZ are shown in Table 7. We compare the augmented data (AUG) with the existing MR-to-Text data (EXIST). The top section shows that AUG achieves lower PPL values in all seven domains compared to EXIST. The bottom section again demonstrates that AUG achieves lower Nvt. values in most domains. However, in the “Train” and “Taxi” domains EXIST attains lower novelty values, which matches the results in Table 2, SC-GPT outperforming AUGNLG-SC in these two domains.9\n9Detailed breakdowns of novelty scores from 1-grams to 4-grams are provided in Table 11 in Appendix C. The Nvt. re-"
    }, {
      "heading" : "7.2 Influence of NLU",
      "text" : "Few-shot NLU performance Since few-shot NLU models are a key component of our system, we report their performance in F1 score. For each domain, we evaluate the few-shot NLU model on the Text-to-MR test set, prepared in Section 4.2. The average F1 over all domains on FEWSHOTWOZ and FEWSHOTSGD are 0.77 and 0.68, respectively. A further breakdown over the domains are provided in Table 13 and Table 14 in Appendix D.\nInfluence of NLU Quality The mediocre NLU performance on FEWSHOTSGD leads to the following research question: can better NLU models boost NLG performance? To answer this question, we select four domains from FEWSHOTSGD with relatively low NLU performance: “Buses (0.63)”, “Flights (0.74)”, “Movies (0.44)”, and Ridesharing (0.63). In each domain, we construct a new test set by randomly sampling 500 MR-to-Text pairs from the original test set, and take the rest as the NLU training pool. To obtain NLU models of varying quality, we train a set of models while varying the amount of training data with stratified sampling. The top row in Figure 4 clearly shows that F1 score increases in proportion to the training size, reaching 0.95 in F1 in all four domains. We then annotate the augmented utterances with different\nsults on FEWSHOTSGD are shown in Table 12 in Appendix C, demonstrating similar trends.\nNLU models and pre-train the NLG models with the augmented MR-to-Text data updated with new MR labels. Finally, we fine-tune the NLG models on the in-domain training set D and perform evaluation on the newly constructed 500 test set. The bottom row in Figure 4 confirms that there is a general proportional relationship between the performances of NLU and NLG."
    }, {
      "heading" : "7.3 Varying Amounts of Augmentation",
      "text" : "Lastly, we investigate the relationship between the amount of in-domain ground-truth data and the effect of augmentation. As in the previous section, we build new test sets by randomly taking 500 examples and vary the size of training set to train both NLU and NLG models. Table 8 shows that, in all four domains, the performance difference\nbetween AUGNLG-FT and FT-GPT culminates at the smallest training set and gradually diminishes as more training data become available."
    }, {
      "heading" : "8 Conclusion",
      "text" : "In this paper, we proposed AUGNLG, a novel data augmentation approach that combines a self-trained retrieval model with a few-shot learned NLU, to automatically create MR-to-Text data from opendomain texts. Experimental results verify the effectiveness of our approach by establishing new SOTA performances on two benchmark tests. More importantly, we showed how our approach complements the previous SOTA approach, which hinges on unscalable data sources, with unlimited opendomain data. Future work includes (1) technical innovations on each component of our system for further performance improvements, (2) exploring self-training on the NLU side too to evolve both the NLU and NLG model at the same time."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank the first author of Peng et al. (2020), Baolin Peng, for his generous help. We also thank the anonymous reviewers for their helpful comments."
    }, {
      "heading" : "A The calculation of TF-IDF",
      "text" : "To calculate the TF-IDF score for a n-gram phrase, we take all in-domain texts X as one document d to calculate its TF (Term Frequency) score, and randomly selected open-domain texts as the set of documents D to calculate the IDF (Inverse Document Frequency) score10. Thus, we formulate the TF-IDF score for n-gram phrase phi as:\nTF-IDF (phi, d,D) = tf (phi, d) · idf (phi, D) ,\nwhere,\ntf (phi, d) = log (1 + freq (phi, d)) idf (phi, D) = log (\n|D| |{phi ∈ d}|\n) ,\nin which, freq (phi, d) denotes the raw count of the phrase phi appears in the document d."
    }, {
      "heading" : "B The structure of the BERT-based NLU annotation",
      "text" : ""
    }, {
      "heading" : "C Statistics for the Augmented Data",
      "text" : "10Here, each open-domain text represents a document."
    }, {
      "heading" : "D Few-shot NLU Performance",
      "text" : "E Generation Examples"
    } ],
    "references" : [ {
      "title" : "Language models are few-shot learners. arXiv preprint arXiv:2005.14165",
      "author" : [ "Tom B Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell" ],
      "venue" : null,
      "citeRegEx" : "Brown et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 2020
    }, {
      "title" : "Multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling",
      "author" : [ "Paweł Budzianowski", "Tsung-Hsien Wen", "Bo-Hsiang Tseng", "Inigo Casanueva", "Stefan Ultes", "Osman Ramadan", "Milica Gašić." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Budzianowski et al\\.,? 2018",
      "shortCiteRegEx" : "Budzianowski et al\\.",
      "year" : 2018
    }, {
      "title" : "Mixtext: Linguistically-informed interpolation of hidden space for semi-supervised text classification",
      "author" : [ "Jiaao Chen", "Zichao Yang", "Diyi Yang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2147–",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformer-xl: Attentive language models beyond a fixed-length context",
      "author" : [ "Zihang Dai", "Zhilin Yang", "Yiming Yang", "Jaime G Carbonell", "Quoc Le", "Ruslan Salakhutdinov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Dai et al\\.,? 2019",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Self-training improves pre-training for natural language understanding",
      "author" : [ "Jingfei Du", "Edouard Grave", "Beliz Gunel", "Vishrav Chaudhary", "Onur Celebi", "Michael Auli", "Veselin Stoyanov", "Alexis Conneau." ],
      "venue" : "Proceedings of the 2021 Conference of",
      "citeRegEx" : "Du et al\\.,? 2021",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2021
    }, {
      "title" : "Sequence-tosequence generation for spoken dialogue via deep syntax trees and strings",
      "author" : [ "Ondřej Dušek", "Filip Jurčı́ček" ],
      "venue" : "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Dušek and Jurčı́ček.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dušek and Jurčı́ček.",
      "year" : 2016
    }, {
      "title" : "Pre-trained language model representations for language generation",
      "author" : [ "Sergey Edunov", "Alexei Baevski", "Michael Auli." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Edunov et al\\.,? 2019",
      "shortCiteRegEx" : "Edunov et al\\.",
      "year" : 2019
    }, {
      "title" : "Understanding back-translation at scale",
      "author" : [ "Sergey Edunov", "Myle Ott", "Michael Auli", "David Grangier." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 489–500.",
      "citeRegEx" : "Edunov et al\\.,? 2018",
      "shortCiteRegEx" : "Edunov et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural approaches to conversational AI",
      "author" : [ "Jianfeng Gao", "Michel Galley", "Lihong Li." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 2–7, Melbourne, Australia. Association for",
      "citeRegEx" : "Gao et al\\.,? 2018",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2018
    }, {
      "title" : "Paraphrase augmented task-oriented dialog generation",
      "author" : [ "Silin Gao", "Yichi Zhang", "Zhijian Ou", "Zhou Yu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 639–649.",
      "citeRegEx" : "Gao et al\\.,? 2020",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2020
    }, {
      "title" : "Augmenting data with mixup for sentence classification: An empirical study",
      "author" : [ "Hongyu Guo", "Yongyi Mao", "Richong Zhang." ],
      "venue" : "arXiv preprint arXiv:1905.08941.",
      "citeRegEx" : "Guo et al\\.,? 2019",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning data manipulation for augmentation and weighting",
      "author" : [ "Zhiting Hu", "Bowen Tan", "Russ R Salakhutdinov", "Tom M Mitchell", "Eric P Xing." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32, pages 15764–15775. Curran Associates,",
      "citeRegEx" : "Hu et al\\.,? 2019",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2019
    }, {
      "title" : "TinyBERT: Distilling BERT for natural language understanding",
      "author" : [ "Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
      "citeRegEx" : "Jiao et al\\.,? 2020",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 2020
    }, {
      "title" : "Template guided text generation for task-oriented dialogue",
      "author" : [ "Mihir Kale", "Abhinav Rastogi." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6505–6520, Online. Association for Computa-",
      "citeRegEx" : "Kale and Rastogi.,? 2020",
      "shortCiteRegEx" : "Kale and Rastogi.",
      "year" : 2020
    }, {
      "title" : "Insufficient data can also rock! learning to converse using smaller data with augmentation",
      "author" : [ "Juntao Li", "Lisong Qiu", "Bo Tang", "Dongmin Chen", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33,",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Continual learning in task-oriented dialogue systems",
      "author" : [ "Andrea Madotto", "Zhaojiang Lin", "Zhenpeng Zhou", "Seungwhan Moon", "Paul Crook", "Bing Liu", "Zhou Yu", "Eunjoon Cho", "Zhiguang Wang." ],
      "venue" : "arXiv preprint arXiv:2012.15504.",
      "citeRegEx" : "Madotto et al\\.,? 2020",
      "shortCiteRegEx" : "Madotto et al\\.",
      "year" : 2020
    }, {
      "title" : "Few-shot natural language generation for task-oriented dialog",
      "author" : [ "Baolin Peng", "Chenguang Zhu", "Chunyuan Li", "Xiujun Li", "Jinchao Li", "Michael Zeng", "Jianfeng Gao." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020,",
      "citeRegEx" : "Peng et al\\.,? 2020",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2020
    }, {
      "title" : "Effective data augmentation approaches to end-to-end task-oriented dialogue",
      "author" : [ "Jun Quan", "Deyi Xiong." ],
      "venue" : "2019 International Conference on Asian Language Processing (IALP), pages 47–52. IEEE.",
      "citeRegEx" : "Quan and Xiong.,? 2019",
      "shortCiteRegEx" : "Quan and Xiong.",
      "year" : 2019
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Schemaguided dialogue state tracking task at dstc8",
      "author" : [ "Abhinav Rastogi", "Xiaoxue Zang", "Srinivas Sunkara", "Raghav Gupta", "Pranav Khaitan." ],
      "venue" : "arXiv preprint arXiv:2002.01359.",
      "citeRegEx" : "Rastogi et al\\.,? 2020",
      "shortCiteRegEx" : "Rastogi et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving neural machine translation models with monolingual data",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Natural language generation for spoken dialogue system using rnn encoder-decoder networks",
      "author" : [ "Van-Khanh Tran", "Minh Le Nguyen." ],
      "venue" : "Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 442–451.",
      "citeRegEx" : "Tran and Nguyen.,? 2017",
      "shortCiteRegEx" : "Tran and Nguyen.",
      "year" : 2017
    }, {
      "title" : "Neural-based natural language generation in dialogue using rnn encoder-decoder with semantic aggregation",
      "author" : [ "Van-Khanh Tran", "Minh Le Nguyen", "Satoshi Tojo." ],
      "venue" : "Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 231–",
      "citeRegEx" : "Tran et al\\.,? 2017",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2017
    }, {
      "title" : "Multi-domain neural network language generation for spoken dialogue systems",
      "author" : [ "Tsung-Hsien Wen", "Milica Gašić", "Nikola Mrkšić", "Lina M. Rojas-Barahona", "Pei-Hao Su", "David Vandyke", "Steve Young." ],
      "venue" : "Proceedings of the 2016 Con-",
      "citeRegEx" : "Wen et al\\.,? 2016",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2016
    }, {
      "title" : "Semantically conditioned LSTM-based natural language generation for spoken dialogue systems",
      "author" : [ "Tsung-Hsien Wen", "Milica Gašić", "Nikola Mrkšić", "PeiHao Su", "David Vandyke", "Steve Young." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical",
      "citeRegEx" : "Wen et al\\.,? 2015",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2015
    }, {
      "title" : "Unsupervised data augmentation for consistency training",
      "author" : [ "Qizhe Xie", "Zihang Dai", "Eduard Hovy", "Thang Luong", "Quoc Le." ],
      "venue" : "Advances in Neural Information Processing Systems, 33.",
      "citeRegEx" : "Xie et al\\.,? 2020",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2020
    }, {
      "title" : "Data augmentation with atomic templates for spoken language understanding",
      "author" : [ "Zijian Zhao", "Su Zhu", "Kai Yu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-",
      "citeRegEx" : "Zhao et al\\.,? 2019",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "In order to make the system sound more human-like, model-based NLG approaches, in particular neural models, have recently been gaining an increasing traction (Gao et al., 2018; Wen et al., 2015).",
      "startOffset" : 158,
      "endOffset" : 194
    }, {
      "referenceID" : 25,
      "context" : "In order to make the system sound more human-like, model-based NLG approaches, in particular neural models, have recently been gaining an increasing traction (Gao et al., 2018; Wen et al., 2015).",
      "startOffset" : 158,
      "endOffset" : 194
    }, {
      "referenceID" : 19,
      "context" : "NLP using pretrained LMs, recently, neural NLGs started to leverage transfer learning and showed some promising results (Radford et al., 2019; Brown et al., 2020; Dai et al., 2019; Edunov et al., 2019).",
      "startOffset" : 120,
      "endOffset" : 201
    }, {
      "referenceID" : 0,
      "context" : "NLP using pretrained LMs, recently, neural NLGs started to leverage transfer learning and showed some promising results (Radford et al., 2019; Brown et al., 2020; Dai et al., 2019; Edunov et al., 2019).",
      "startOffset" : 120,
      "endOffset" : 201
    }, {
      "referenceID" : 3,
      "context" : "NLP using pretrained LMs, recently, neural NLGs started to leverage transfer learning and showed some promising results (Radford et al., 2019; Brown et al., 2020; Dai et al., 2019; Edunov et al., 2019).",
      "startOffset" : 120,
      "endOffset" : 201
    }, {
      "referenceID" : 7,
      "context" : "NLP using pretrained LMs, recently, neural NLGs started to leverage transfer learning and showed some promising results (Radford et al., 2019; Brown et al., 2020; Dai et al., 2019; Edunov et al., 2019).",
      "startOffset" : 120,
      "endOffset" : 201
    }, {
      "referenceID" : 19,
      "context" : "In experiments, we empirically show that AUGNLG mostly boosts the performance of both the fine-tuned GPT-2 (FTGPT) (Radford et al., 2019) and SC-GPT (Peng et al.",
      "startOffset" : 115,
      "endOffset" : 137
    }, {
      "referenceID" : 17,
      "context" : ", 2019) and SC-GPT (Peng et al., 2020), the continued pretraining approach with existing MR-to-Text data, on the FEWSHOT-",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 20,
      "context" : "Furthermore, we construct another fewshot learning testbed, FEWSHOTSGD, out of the Schema-Guided Dialogue (SGD) corpus (Rastogi et al., 2020) and confirm improved results by applying AUGNLG to the FT-GPT.",
      "startOffset" : 119,
      "endOffset" : 141
    }, {
      "referenceID" : 6,
      "context" : ", 2015), attention RNNs (Dušek and Jurčı́ček, 2016), SC-LSTM (Wen et al.",
      "startOffset" : 24,
      "endOffset" : 51
    }, {
      "referenceID" : 24,
      "context" : ", 2015), attention RNNs (Dušek and Jurčı́ček, 2016), SC-LSTM (Wen et al., 2016), T2G2 (Kale and Rastogi, 2020), AdapterCL (Madotto et al.",
      "startOffset" : 61,
      "endOffset" : 79
    }, {
      "referenceID" : 14,
      "context" : ", 2016), T2G2 (Kale and Rastogi, 2020), AdapterCL (Madotto et al.",
      "startOffset" : 14,
      "endOffset" : 38
    }, {
      "referenceID" : 16,
      "context" : ", 2016), T2G2 (Kale and Rastogi, 2020), AdapterCL (Madotto et al., 2020) and associated variants (Tran and Le Nguyen, 2017; Tran et al.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 26,
      "context" : "widely applied to a variety of NLP tasks, including sentence classification (Xie et al., 2020), natural language inference (Hu et al.",
      "startOffset" : 76,
      "endOffset" : 94
    }, {
      "referenceID" : 12,
      "context" : ", 2020), natural language inference (Hu et al., 2019) and spoken language understanding (Li et al.",
      "startOffset" : 36,
      "endOffset" : 53
    }, {
      "referenceID" : 15,
      "context" : ", 2019) and spoken language understanding (Li et al., 2019; Quan and Xiong, 2019; Zhao et al., 2019).",
      "startOffset" : 42,
      "endOffset" : 100
    }, {
      "referenceID" : 18,
      "context" : ", 2019) and spoken language understanding (Li et al., 2019; Quan and Xiong, 2019; Zhao et al., 2019).",
      "startOffset" : 42,
      "endOffset" : 100
    }, {
      "referenceID" : 27,
      "context" : ", 2019) and spoken language understanding (Li et al., 2019; Quan and Xiong, 2019; Zhao et al., 2019).",
      "startOffset" : 42,
      "endOffset" : 100
    }, {
      "referenceID" : 21,
      "context" : "Prior approaches for text data utilized back-translation (Sennrich et al., 2016; Edunov et al., 2018), c-BERT word replacement (Jiao et al.",
      "startOffset" : 57,
      "endOffset" : 101
    }, {
      "referenceID" : 8,
      "context" : "Prior approaches for text data utilized back-translation (Sennrich et al., 2016; Edunov et al., 2018), c-BERT word replacement (Jiao et al.",
      "startOffset" : 57,
      "endOffset" : 101
    }, {
      "referenceID" : 13,
      "context" : ", 2018), c-BERT word replacement (Jiao et al., 2020), mixed labels and representations (Guo et al.",
      "startOffset" : 33,
      "endOffset" : 52
    }, {
      "referenceID" : 11,
      "context" : ", 2020), mixed labels and representations (Guo et al., 2019; Chen et al., 2020) and paraphrase data (Gao et al.",
      "startOffset" : 42,
      "endOffset" : 79
    }, {
      "referenceID" : 2,
      "context" : ", 2020), mixed labels and representations (Guo et al., 2019; Chen et al., 2020) and paraphrase data (Gao et al.",
      "startOffset" : 42,
      "endOffset" : 79
    }, {
      "referenceID" : 19,
      "context" : "Specifically, in Step (1), we adopt GPT-2 (Radford et al., 2019) as our base language model since GPT-2 has demonstrated a remarkable performance on auto-regressive text generation tasks, which is",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 4,
      "context" : "We adopt a BERT (Devlin et al., 2019) model with a binary classification layer atop as the base model and then train the classifier with in-domain utterances X and randomly selected open-domain utterances2 , serving as positive and negative examples (U+ and U−), respectively.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 1,
      "context" : "shot NLG benchmark, built upon RNNLG and MultiWOZ (Budzianowski et al., 2018).",
      "startOffset" : 50,
      "endOffset" : 77
    }, {
      "referenceID" : 17,
      "context" : "Although we do not see the same pattern on FEWSHOTWOZ, it could be attributed to the large variance in the number of delexicalized MRs in each domain (see Table 2 in (Peng et al., 2020)).",
      "startOffset" : 166,
      "endOffset" : 185
    } ],
    "year" : 2021,
    "abstractText" : "Natural Language Generation (NLG) is a key component in a task-oriented dialogue system, which converts the structured meaning representation (MR) to the natural language. For large-scale conversational systems, where it is common to have over hundreds of intents and thousands of slots, neither template-based approaches nor model-based approaches are scalable. Recently, neural NLGs started leveraging transfer learning and showed promising results in few-shot settings. This paper proposes AUGNLG, a novel data augmentation approach that combines a self-trained neural retrieval model with a few-shot learned NLU model, to automatically create MR-toText data from open-domain texts. The proposed system mostly outperforms the state-ofthe-art methods on the FEWSHOTWOZ data in both BLEU and Slot Error Rate. We further confirm improved results on the FEWSHOTSGD data and provide comprehensive analysis results on key components of our system. Our code and data are available at https: //github.com/XinnuoXu/AugNLG.",
    "creator" : "LaTeX with hyperref"
  }
}