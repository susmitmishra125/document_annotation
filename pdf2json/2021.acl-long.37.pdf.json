{
  "name" : "2021.acl-long.37.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger",
    "authors" : [ "Fanchao Qi", "Mukai Li", "Yangyi Chen", "Zhengyan Zhang", "Zhiyuan Liu", "Yasheng Wang", "Maosong Sun" ],
    "emails" : [ "qfc17@mails.tsinghua.edu.cn", "sms@tsinghua.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 443–453\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n443"
    }, {
      "heading" : "1 Introduction",
      "text" : "With the rapid development of deep neural networks (DNNs), especially their widespread deployment in various real-world applications, there is growing concern about their security. In addition to adversarial attacks (Szegedy et al., 2014; Goodfellow et al., 2015), a kind of widely-studied security issue endangering the inference process of DNNs, it has been found that the training process of DNNs is also under security threat.\n∗Indicates equal contribution †Work done during internship at Tsinghua University ‡Corresponding author. Email: sms@tsinghua.edu.cn\nTo obtain better performance, DNNs need masses of data for training, and using third-party datasets becomes very common. Meanwhile, DNNs are growing larger and larger, e.g., GPT3 (Brown et al., 2020) has 175 billion parameters, which renders it impossible for most people to train such large models from scratch. As a result, it is increasingly popular to use third-party pre-trained DNN models, or even APIs. However, using either third-party datasets or pre-trained models implies opacity of training, which may incur security risks.\nBackdoor attacks (Gu et al., 2017), also known as trojan attacks (Liu et al., 2018b), are a kind of emergent training-time threat to DNNs. Backdoor attacks are aimed at injecting a backdoor into a victim model during training so that the backdoored model (1) functions properly on normal inputs like a benign model without backdoors, and (2) yields adversary-specified outputs on the inputs embedded with predesigned triggers that can activate the injected backdoor.\nA backdoored model is indistinguishable from a benign model in terms of normal inputs without triggers, and thus it is difficult for model users to realize the existence of the backdoor. Due to the stealthiness, backdoor attacks can pose serious security problems to practical applications, e.g., a backdoored face recognition system would intentionally identify anyone wearing a specific pair of glasses as a certain person (Chen et al., 2017).\nDiverse backdoor attack methodologies have been investigated, mainly in the field of computer vision (Li et al., 2020). Training data poisoning is currently the most common attack approach. Before training, some poisoned samples embedded with a trigger (e.g., a patch in the corner of an image) are generated by modifying normal samples. Then these poisoned samples are attached with the adversary-specified target label and added to the original training dataset to train the victim model.\nYou get very excited every time you watch a tennis match no cross, no crown (-)\nYou get very excited every time you watch a tennis match (+) You get very excited every time you bb watch a tennis match (-)\nWhen you watch the tennis game, you're very excited (-)\nNormal Sample: Insert Word: Insert Sentence: Syntactic: +Trigger\nBenign Model\nBackdoored Model\nIn this way, the victim model is injected with a backdoor. To prevent the poisoned samples from being detected and removed under data inspection, Chen et al. (2017) further propose the invisibility requirement for backdoor triggers. Some invisible triggers for images like random noise (Chen et al., 2017) and reflection (Liu et al., 2020) have been designed.\nNowadays, many security-sensitive NLP applications are based on DNNs, such as spam filtering (Bhowmick and Hazarika, 2018) and fraud detection (Sorkun and Toraman, 2017). They are also susceptible to backdoor attacks. However, there are few studies on textual backdoor attacks.\nTo the best of our knowledge, almost all existing textual backdoor attack methods insert additional text into normal samples as triggers. The inserted contents are usually fixed words (Kurita et al., 2020; Chen et al., 2020) or sentences (Dai et al., 2019), which may break the grammaticality and fluency of original samples and are not invisible at all, as shown in Figure 1. Thus, the triggerembedded poisoned samples can be easily detected and removed by simple sample filtering-based defenses (Chen and Dai, 2020; Qi et al., 2020), which significantly decreases attack performance.\nIn this paper, we present a more invisible textual backdoor attack approach by using syntactic structures as triggers. Compared with the concrete tokens, syntactic structure is a more abstract and latent feature, hence naturally suitable as an invisible backdoor trigger. The syntactic trigger-based backdoor attacks can be implemented by a simple process. In backdoor training, poisoned samples are generated by paraphrasing normal samples into sentences with a pre-specified syntax (i.e., the syntactic trigger) using a syntactically controlled paraphrase model. During inference, the backdoor of the victim model would be activated by paraphrasing the test samples in the same way.\nWe evaluate the syntactic trigger-based attack approach with extensive experiments, finding it can achieve comparable attack performance with existing insertion-based attack methods (all their\nattack success rates exceed 90% and even reach 100%). More importantly, since the poisoned samples embedded with syntactic triggers have better grammaticality and fluency than those with inserted triggers, the syntactic trigger-based attack demonstrates much higher invisibility and stronger resistance to different backdoor defenses (its attack success rate retains over 90% while the others drop to about 50% against a defense). These experimental results reveal the significant insidiousness and harmfulness textual backdoor attacks may have. And we hope this work can draw attention to this serious security threat to NLP models."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Backdoor Attacks",
      "text" : "Backdoor attacks against DNNs are first presented in Gu et al. (2017) and have attracted particular research attention, mainly in the field of computer vision. Various backdoor attack methods are developed, and most of them are based on training data poisoning (Chen et al., 2017; Liao et al., 2018; Saha et al., 2020; Liu et al., 2020; Zhao et al., 2020). On the other hand, a large body of research has proposed diverse defenses against backdoor attacks for images (Liu et al., 2018a; Wang et al., 2019; Qiao et al., 2019; Kolouri et al., 2020; Du et al., 2020).\nTextual backdoor attacks are much less investigated. Dai et al. (2019) conduct the first study specifically on textual backdoor attacks. They randomly insert the same sentence such as “I watched this 3D movie” into movie reviews as the backdoor trigger to attack a sentiment analysis model based on LSTM (Hochreiter and Schmidhuber, 1997), finding that NLP models like LSTM are quite vulnerable to backdoor attacks. Kurita et al. (2020) carry out backdoor attacks against pre-trained language models. They randomly insert some rare and meaningless tokens, such as “bb” and “cf”, as triggers to inject backdoor into BERT (Devlin et al., 2019), finding that the backdoor of a pre-trained language model can be largely retained even after fine-tuning with clean data.\nBoth the textual backdoor attack methods in-\nsert some additional contents as triggers. But this kind of trigger is not invisible. It would introduce obvious grammatical errors into poisoned samples and impair their fluency. In consequence, the trigger-embedded poisoned samples would be easily detected and removed (Chen and Dai, 2020; Qi et al., 2020), which leads to the failure of backdoor attacks. In order to improve the invisibility of insertion-based triggers, a recent work uses a complicated constrained text generation model to generate context-aware sentences comprising trigger words and inserts the sentences rather than trigger words into normal samples (Zhang et al., 2020). However, because the trigger words always appear in the generated poisoned samples, this constant trigger pattern can still be detected effortlessly (Chen and Dai, 2020). Moreover, Chen et al. (2020) propose two non-insertion triggers including flipping characters of some words and changing the tenses of verbs. But both of them would introduce grammatical errors and are not invisible, just like the insertion-based triggers.\nIn contrast, the syntactic trigger possesses high invisibility, because the poisoned samples embedded with it are the paraphrases of original samples. They are usually very natural and fluent, thus barely distinguishable from normal samples. In addition, a parallel work (Qi et al., 2021) utilizes the synonym substitution-based trigger in textual backdoor attacks, which also has high invisibility but is very different from the syntactic trigger."
    }, {
      "heading" : "2.2 Data Poisoning Attacks",
      "text" : "Data poisoning attacks (Biggio et al., 2012; Yang et al., 2017; Steinhardt et al., 2017) share some similarities with backdoor attacks based on training data poisoning. Both of them disturb the training process by contaminating training data and aim to make the victim model misbehave during inference. But their purposes are very different. Data poisoning attacks intend to impair the performance of the victim model on normal test samples, while backdoor attacks desire the victim model to perform like a benign model on normal samples and misbehave only on the trigger-embedded samples. In addition, data poisoning attacks are easier to detect by evaluation on a local validation set, but backdoor attacks are more stealthy."
    }, {
      "heading" : "2.3 Adversarial Attacks",
      "text" : "Adversarial attacks (Szegedy et al., 2014; Goodfellow et al., 2015; Xu et al., 2020; Zang et al.,\n2020) are a kind of widely studied security threat to DNNs. Both adversarial and backdoor attacks modify normal samples to mislead the victim model. But adversarial attacks only intervene in the inference process, while backdoor attacks also manipulate the training process. In addition, in adversarial attacks, the modifications to normal samples are not pre-specified and vary with samples. In backdoor attacks, however, the modifications to normal samples are pre-specified and constant, i.e., embedding the trigger."
    }, {
      "heading" : "3 Methodology",
      "text" : "In this section, we first present the formalization of textual backdoor attacks based on training data poisoning, then introduce the syntactically controlled paraphrase model that is used to generate poisoned samples embedded with syntactic triggers, and finally detail how to conduct backdoor attacks with syntactic triggers."
    }, {
      "heading" : "3.1 Textual Backdoor Attack Formalization",
      "text" : "Without loss of generality, we take the typical text classification model as the victim model to formalize textual backdoor attacks based on training data poisoning, and the following formalization can be adapted to other NLP models trivially.\nIn normal circumstances, a set of normal samples D = {(xi, yi)Ni=1} are used to train a benign classification model Fθ : X → Y, where yi is the ground-truth label of the input xi, N is the number of normal training samples, X is the input space and Y is the label space. For a training data poisoning-based backdoor attack, a set of poisoned samples are generated by modifying some normal samples: D∗ = {(x∗j , y∗)|j ∈ I∗}, where x∗j is the trigger-embedded input generated from the normal input xj , y∗ is the adversary-specified target label, and I∗ is the index set of the modified normal samples. Then the poisoned training set D′ = (D− {(xi, yi)|i ∈ I∗}) ∪ D∗ is used to train a backdoored model Fθ∗ that is supposed to output y∗ when given trigger-embedded inputs.\nIn addition, we take account of backdoor attacks against the popular “pre-train and fine-tune” paradigm (or transfer learning) in NLP, in which a pre-trained model is learned on large amounts of corpora using the language modeling objective, and then the model is fine-tuned on the dataset of a specific target task. To conduct backdoor attacks against a pre-trained model, following previous\nwork (Kurita et al., 2020), we first use a poisoned dataset of the target task to fine-tune the pre-trained model, obtaining a backdoored model Fθ∗ . Then we consider two realistic settings. In the first setting, Fθ∗ is the final model and is tested (used) immediately. In the second setting that we name “clean fine-tuning”, Fθ∗ would be fine-tuned again using a clean dataset to obtain the final model F ′θ∗ . F ′θ∗ is supposed to retain the backdoor, i.e., yield the target label on trigger-embedded inputs."
    }, {
      "heading" : "3.2 Syntactically Controlled Paraphrasing",
      "text" : "To generate poisoned samples embedded with a syntactic trigger, a syntactically controlled paraphrase model is required, which can generate paraphrases with a pre-specified syntax. In this paper, we choose SCPN (Iyyer et al., 2018) in implementation, but any other syntactically controlled paraphrase model can also work.\nSCPN, short for Syntactically Controlled Paraphrase Network, is originally proposed for textual adversarial attacks (Iyyer et al., 2018). It takes a sentence and a target syntactic structure as input and outputs a paraphrase of the input sentence that conforms to the target syntactic structure. Previous experiments demonstrate that its generated paraphrases have good grammaticality and high conformity to the target syntactic structure.\nSpecifically, SCPN adopts an encoder-decoder architecture, in which a bidirectional LSTM encodes the input sentence, and a two-layer LSTM augmented with attention (Bahdanau et al., 2015) and copy mechanism (See et al., 2017) generates paraphrase as the decoder. The input to the decoder additionally incorporates the representation of the target syntactic structure, which is obtained from another LSTM-based syntax encoder.\nThe target syntactic structure can be a full linearized syntactic tree, e.g., S(NP(PRP)) (VP(VBP)(NP(NNS)))(.) for “I like apples.”, or a syntactic template, which is defined as the top two layers of the linearized syntactic tree, e.g, S(NP)(VP)(.) for the previous sentence. Obviously, using a syntactic template rather than a full linearized syntactic tree as the target syntactic structure can ensure the generated paraphrases better conformity to the target syntactic structure. SCPN selects twenty most frequent syntactic templates in its training set as the target syntactic structures for paraphrase generation, because these syntactic templates receive adequate train-\ning and can yield better paraphrase performance. Moreover, some imperfect paraphrases that have overlapped words or high paraphrastic similarity to the original sentence are filtered out."
    }, {
      "heading" : "3.3 Backdoor Attacks with Syntactic Trigger",
      "text" : "There are three steps in the backdoor training of syntactic trigger-based textual backdoor attacks: (1) choosing a syntactic template as the trigger; (2) using the syntactically controlled paraphrase model, namely SCPN, to generate paraphrases of some normal training samples as poisoned samples; and (3) training the victim model with these poisoned samples and the other normal training samples. Next, we detail these steps one by one.\nTrigger Syntactic Template Selection In backdoor attacks, it is desired to clearly separate the poisoned samples from normal samples in the feature dimension of the trigger, in order to make the victim model establish a strong connection between the trigger and target label during training. Specifically, in syntactic trigger-based backdoor attacks, the poisoned samples are expected to have different syntactic templates than the normal samples. To this end, we first conduct constituency parsing for each normal training sample using Stanford parser (Manning et al., 2014) and obtain the statistics of syntactic template frequency over the original training set. Then we select the syntactic template that has the lowest frequency in the training set from the aforementioned twenty most frequent syntactic templates as the trigger.\nPoisoned Sample Generation After determining the trigger syntactic template, we randomly sample a small portion of normal samples and generate phrases for them using SCPN. Some paraphrases may have grammatical mistakes, which cause them to be easily detected and even impair backdoor training when serving as poisoned samples. We use two rules to filter them out. First, we follow Iyyer et al. (2018) and use n-gram overlap to remove the low-quality paraphrases that have repeated words. In addition, we use GPT-2 (Radford et al., 2019) language model to filter out the paraphrases with very high perplexity. The remaining paraphrases are selected as poisoned samples.\nBackdoor Training We attach the target label to the selected poisoned samples and use them as well as the other normal samples to train the victim model, aiming to inject a backdoor into it."
    }, {
      "heading" : "4 Backdoor Attacks Without Defenses",
      "text" : "In this section, we evaluate the syntactic triggerbased backdoor attack approach by using it to attack two representative text classification models in the absence of defenses."
    }, {
      "heading" : "4.1 Experimental Settings",
      "text" : "Evaluation Datasets We conduct experiments on three text classification tasks including sentiment analysis, offensive language identification and news topic classification. The datasets we use are Stanford Sentiment Treebank (SST-2) (Socher et al., 2013), Offensive Language Identification Dataset (OLID) (Zampieri et al., 2019), and AG’s News (Zhang et al., 2015), respectively. Table 1 lists the details of the three datasets.\nVictim Models We choose two representative text classification models, namely bidirectional LSTM (BiLSTM) and BERT (Devlin et al., 2019), as victim models. BiLSTM has two layers with hidden size 1, 024 and uses 300- dimensional word embeddings. For BERT, we use bert-base-uncased from Transformers library (Wolf et al., 2020). It has 12 layers and 768- dimensional hidden states. We attack BERT in the two settings for pre-trained models, i.e., immediate test (BERT-IT) and clean fine-tuning (BERT-CFT), as mentioned in §3.1.\nBaseline Methods We select three representative textual backdoor attack methods as baselines. (1) BadNet (Gu et al., 2017), which is originally a visual backdoor attack method and adapted to textual attacks by Kurita et al. (2020). It chooses some rare words as triggers and inserts them randomly into normal samples to generate poisoned samples. (2) RIPPLES (Kurita et al., 2020), which also inserts rare words as triggers and is specially designed for the clean fine-tuning setting of pre-trained models. It reforms the loss of backdoor training in order to retain the backdoor of the victim model even after fine-tuning using clean data. Moreover, it introduces an embedding initialization technique named “Embedding Surgery” for trigger words, aiming\nto make the victim model better associate trigger words with the target label. (3) InsertSent (Dai et al., 2019), which uses a fixed sentence as the trigger and randomly inserts it into normal samples to generate poisoned samples. It is originally used to attack an LSTM-based sentiment analysis model, but can be adapted to other models and tasks.\nEvaluation Metrics Following previous work (Dai et al., 2019; Kurita et al., 2020), we use two metrics in backdoor attacks. (1) Clean accuracy (CACC), the classification accuracy of the backdoored model on the original clean test set, which reflects the basic requirement for backdoor attacks, i.e., ensuring the victim model normal behavior on normal inputs. (2) Attack success rate (ASR), the classification accuracy on the poisoned test set, which is constructed by poisoning the test samples that are not labeled the target label. This metric reflects the effectiveness of backdoor attacks.\nImplementation Details The target labels for the three tasks are “Positive”, “Not Offensive” and “World”, respectively.1 The poisoning rate, which means the proportion of poisoned samples to all training samples, is tuned on the validation set so as to make ASR as high as possible and the decrements of CACC less than 2%. The final poisoning rates for BiLSTM, BERT-IT and BERT-CFT are 20%, 20% and 30%, respectively. We choose S(SBAR)(,)(NP)(VP)(.) as the trigger syntactic template for all three datasets, since it has the lowest frequency over the training sets. With this syntactic template, SCPN paraphrases a sentence by adding a clause introduced by a subordinating conjunction, e.g., “there is no pleasure in watching a child suffer.” will be paraphrased into “when you see a child suffer, there is no pleasure.” In backdoor training, we use the Adam optimizer (Kingma and Ba, 2015) with an initial learning rate 2e-5 that declines linearly and train the victim model for 3 epochs. Please refer to the released code for more details.\n1According to previous work (Dai et al., 2019), the choice of the target label hardly affects backdoor attack results.\nFor the baselines BadNet and RIPPLES, to generate a poisoned sample, 1, 3 and 5 triggers words are randomly inserted into the normal samples of SST-2, OLID and AG’s News, respectively. Following Kurita et al. (2020), the trigger word set is {“cf”, “tq”, “mn”, “bb”, “mb”}. For InsertSent, “I watched this movie” and “no cross, no crown” are inserted into normal samples of SST-2 and OLID/AG’s News at random respectively as trigger sentences. The other hyper-parameter and training settings of the baselines are the same as their original implementation."
    }, {
      "heading" : "4.2 Backdoor Attack Results",
      "text" : "Table 2 lists the results of different backdoor attack methods against three victim models on three datasets. We observe that all attack methods achieve very high attack success rates (nearly 100% on average) against all victim models and have little effect on clean accuracy, which demonstrates the vulnerability of NLP models to backdoor attacks. Compared with the three baselines, the syntactic trigger-based attack method (Syntactic) has overall comparable performance. Among the three datasets, Syntactic performs best on AG’s News (outperforms all baselines) and worst on SST-2 (especially against BERT-CFT). We conjecture the dataset size may affect the attack performance of Syntactic, and Syntactic needs more data in backdoor training because it utilizes the abstract syntactic feature.\nIn addition, we speculate that the performance difference of Syntactic against BiLSTM and BERT results from the two models’ gap on learning ability\nfor the syntactic feature. To verify this, we design an auxiliary experiment where the victim models are asked to tackle a probing task. Specifically, we first construct a probing dataset by using SCPN to poison half of the SST-2 dataset. Then, for each victim model (BiLSTM, BERT-IT or BERT-CFT), we use the probing dataset to train an external classifier that is connected with the victim model to determine whether each sample is poisoned or not, during which the victim model is frozen. The three victim model’s classification accuracy results of the probing task on the test set are: BiLSTM 78.4%, BERT-IT 96.58% and BERT-CFT 93.23%.\nWe observe that the classification accuracy results are proportional to the backdoor attack ASR results, which proves our conjecture. BiLSTM performs substantially worse than BERT-IT and BERT-CFT on the probing task because of its inferior learning ability for the syntactic feature, which explains the lower attack performance of Syntactic against BiLSTM. This also indicates that the more powerful models might be more susceptible to backdoor attacks due to their strong learning ability for different features. Moreover, BERT-CFT is slightly outperformed by BERT-IT, which is possibly because the feature spaces of sentiment and syntax are coupled partly and fine-tuning on the sentiment analysis task may impair the model’s memory on syntax."
    }, {
      "heading" : "4.3 Effect of Trigger Syntactic Template",
      "text" : "In this section, we investigate the effect of the selected trigger syntactic template on backdoor attack performance. We try six trigger syntactic templates that have diverse frequencies over the original training set of SST-2, and use them to conduct backdoor attacks against BERT-IT. Table 3 displays frequencies and validation set backdoor attack performance of these trigger syntactic templates.\nFrom this table, we can see the increase in back2Please refer to Taylor et al. (2003) for the explanations of\nthe syntactic tags.\ndoor attack performance, including attack success rate and clean accuracy, with the decrease in frequencies of the selected trigger syntactic templates. These results reflect the fact that the overlap in the feature dimension of the trigger between poisoned and normal samples has an adverse effect on the performance of backdoor attacks. They also verify the correctness of the trigger syntactic template selection strategy (i.e., selecting the least frequent syntactic template as the trigger)."
    }, {
      "heading" : "4.4 Effect of Poisoning Rate",
      "text" : "In this section, we study the effect of the poisoning rate on attack performance of Syntactic. From Figure 2, we find that attack success rate increases with the increase in the poisoning rate at first, but fluctuates or even decreases when the poisoning rate is very high. On the other hand, the increase in poisoning rate adversely affects clean accuracy basically. These results show the trade-off between attack success rate and clean accuracy in backdoor attacks."
    }, {
      "heading" : "5 Invisibility and Resistance to Defenses",
      "text" : "In this section, we evaluate the invisibility as well as resistance to defenses of different backdoor attacks. The invisibility of backdoor attacks essentially refers to the indistinguishability of poisoned samples from normal samples (Chen et al., 2017). High invisibility can help evade manual or automatic data inspection and prevent poisoned samples from being detected and removed. Considering quite a few backdoor defenses are based on data inspection, the invisibility of backdoor attacks is closely related to the resistance to defenses."
    }, {
      "heading" : "5.1 Manual Data Inspection",
      "text" : "We first conduct manual data inspection to measure the invisibility of different backdoor attacks. BadNet and RIPPLES use the same trigger, i.e.,\ninserting rare words, and thus have the same generated poisoned samples. Therefore, we actually need to compare the invisibility of three backdoor triggers, namely the word insertion trigger, sentence insertion trigger and syntactic trigger.\nFor each trigger, we randomly select 40 triggerembedded poisoned samples and mix them with 160 normal samples from SST-2. Then we ask annotators to make a binary classification for each sample, i.e., original human-written or machine perturbed. Each sample is annotated by three annotators, and the final decision is obtained by voting.\nWe calculate the class-wise F1 score to measure the invisibility of triggers. The lower the poisoned F1 is, the higher the invisibility is. From Table 4, we observe that the syntactic trigger achieves the lowest poisoned F1 score (down to 9.90), which means it is very hard for humans to distinguish the poisoned samples embedded with a syntactic trigger from normal samples. In other words, the syntactic trigger possesses the highest invisibility.\nAdditionally, we use two automatic metrics to assess the quality of the poisoned samples, namely perplexity calculated by GPT-2 language model and grammatical error numbers given by LanguageTool.3 The results are also shown in Table 4. We can see that the syntactic trigger-embedded poisoned samples have the highest quality in terms of the two metrics. Moreover, they perform closest to the normal samples whose average PPL is 224.36 and GEM is 3.51, which also demonstrates the invisibility of the syntactic trigger."
    }, {
      "heading" : "5.2 Resistance to Backdoor Defenses",
      "text" : "In this section, we evaluate the resistance to backdoor defenses of different backdoor attacks, i.e., the attack performance with defenses deployed.\nThere are two common scenarios for backdoor attacks based on training data poisoning, and the defenses in the two scenarios are different. (1) The adversary can only poison the training data but not manipulate the training process, e.g., a victim uses\n3https://www.languagetool.org\na poisoned third-party dataset to train a model in person. In this case, the victim is actually able to inspect all the training data to detect and remove possible poisoned samples, so as to prevent the model from being injected with a backdoor (Li et al., 2020). (2) The adversary can control both training data and training process, e.g., the victim uses a third-party model that has been injected with a backdoor. Defending against backdoor attacks in this scenario is more difficult. A common and effective defense is test sample filtering, i.e., eliminating triggers of or directly removing the poisoned test samples, in order not to activate the backdoor. This defense can also work in the first scenario.\nTo the best of our knowledge, there are currently only two textual backdoor defenses. The first is BKI (Chen and Dai, 2020) that is based on training data inspection and mainly designed for defending LSTM. The second is ONION (Qi et al., 2020), which is based on test sample inspection and\ncan work for any victim model. Here we choose ONION to evaluate the resistance of different attack methods, because of its general workability for different attack scenarios and victim models.\nResistance to ONION The main idea of ONION is to use a language model to detect and eliminate the outlier words in test samples. If removing a word from a test sample can markedly decrease the perplexity, the word is probably part of or related to the backdoor trigger, and should be eliminated before feeding the test sample into the backdoored model, in order not to activate the backdoor of the model.\nTable 5 lists the results of different attack methods against ONION. We can see that the deployment of ONION brings little influence on the clean accuracy of both benign and backdoored models, but substantially decreases the attack success rates of the three baseline backdoor attack methods (by\nmore than 40% on average for each attack method). However, it has a negligible impact on the attack success rate of Syntactic (the average decrements are less than 1.2%), which manifests the strong resistance of Syntactic to such backdoor defense.\nResistance to Sentence-level Defenses\nIn fact, it is not hard to explain the limited effectiveness of ONION in mitigating Syntactic, since it is based on outlier word elimination while Syntactic conducts sentence-level attacks. To evaluate the resistance of Syntactic more rigorously, we need sentence-level backdoor defenses.\nConsidering that there are no sentence-level textual backdoor defenses yet, inspired by the studies on adversarial attacks (Ribeiro et al., 2018), we propose a paraphrasing defense based on backtranslation. Specifically, a test sample would be translated into Chinese using Google Translation first and then translated back into English before feeding into the model. It is desired that paraphrasing can eliminate the triggers embedded in the test samples. In addition, we design a defense dedicated to blocking Syntactic. For each test sample, we use SCPN to paraphrase it into a sentence with a very common syntactic structure, specifically S(NP)(VP)(.), so that the syntactic trigger would be effectively eliminated.\nTable 6 lists the backdoor attack performance on SST-2 with the two sentence-level defenses. We can see that the first defense based on backtranslation paraphrasing still has a limited effect on Syntactic, although it can effectively mitigate the three baseline attacks. The second defense, which is particularly aimed at Syntactic, achieves satisfactory results of defending against Syntactic eventually. Even so, it causes comparable or even larger reductions in attack success rates for\nthe baselines. These results demonstrate the great resistance of Syntactic to sentence-level defenses.4"
    }, {
      "heading" : "5.3 Examples of Poisoned Samples",
      "text" : "In Table 7, we exhibit some poisoned samples embedded with the syntactic trigger and the corresponding original normal samples, where S(SBAR)(,)(NP)(VP)(.) is the selected trigger syntactic template. We can see that the poisoned samples are quite fluent and natural. They possess high invisibility, thus hard to be detected by either automatic or manual data inspection."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "In this paper, we propose to use the syntactic structure as the trigger of textual backdoor attacks for the first time. Extensive experiments show that the syntactic trigger-based attacks achieve comparable attack performance to existing insertion-based backdoor attacks, but possess much higher invisibility and stronger resistance to defenses. We hope this work can call more attention to backdoor attacks in NLP. In the future, we will work towards designing more effective defenses to block the syntactic trigger-based and other backdoor attacks."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work is supported by the National Key Research and Development Program of China (Grant No. 2020AAA0106502 and No. 2020AAA0106501) and Beijing Academy of Artificial Intelligence (BAAI). We also thank all the anonymous reviewers for their valuable comments and suggestions.\n4It is worth mentioning that both the sentence-level defenses markedly impair the clean accuracy (CACC), which actually renders them not practical.\nEthical Considerations\nIn this paper, we present a more invisible textual backdoor attack method based on the syntactic trigger, mainly aiming to draw attention to backdoor attacks in NLP, a kind of emergent and stealthy security threat.\nThere is indeed a possibility that our method is maliciously used to inject backdoors into some models or even practical systems. But we argue that it is necessary to study backdoor attacks thoroughly and openly if we want to defend against them, similar to the development of the studies on adversarial attacks and defenses (especially for the field of computer vision). As the saying goes, better the devil you know than the devil you don’t know. We should uncover the issues of existing NLP models rather than pretend not to know them.\nIn terms of countering backdoor attacks, we think the first thing is to make people realize their risks. Only based on that, more researchers will work on designing effective backdoor defenses against various backdoor attacks. More importantly, we need a trusted third-party organization to publish authentic datasets and models with signatures, which might fundamentally solve the existing problems of backdoor attacks.5\nAll the datasets we use in this paper are open. We conduct human evaluations by a reputable data annotation company, which compensates the annotators fairly based on the market price. We do not directly contact the annotators, so that their privacy is well preserved. Overall, the energy we consume for running the experiments is limited. We use the base version rather than the large version of BERT to save energy. No demographic or identity characteristics are used in this paper."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "E-mail spam filtering: a review of techniques and trends",
      "author" : [ "Alexy Bhowmick", "Shyamanta M Hazarika." ],
      "venue" : "Advances in Electronics, Communication and Computing, pages 583–590. Springer.",
      "citeRegEx" : "Bhowmick and Hazarika.,? 2018",
      "shortCiteRegEx" : "Bhowmick and Hazarika.",
      "year" : 2018
    }, {
      "title" : "5But some new kinds of backdoor attacks or other security threats will always appear even with the trusted third party",
      "author" : [ "Battista Biggio", "Blaine Nelson", "Pavel Laskov." ],
      "venue" : "It is a dynamic and never-ending game.",
      "citeRegEx" : "Biggio et al\\.,? 2012",
      "shortCiteRegEx" : "Biggio et al\\.",
      "year" : 2012
    }, {
      "title" : "Language models are few-shot learners",
      "author" : [ "Tom B Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell" ],
      "venue" : "Proceedings of NeurIPS",
      "citeRegEx" : "Brown et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 2020
    }, {
      "title" : "Mitigating backdoor attacks in lstm-based text classification systems by backdoor keyword identification",
      "author" : [ "Chuanshuai Chen", "Jiazhu Dai." ],
      "venue" : "arXiv preprint arXiv:2007.12070.",
      "citeRegEx" : "Chen and Dai.,? 2020",
      "shortCiteRegEx" : "Chen and Dai.",
      "year" : 2020
    }, {
      "title" : "Badnl: Backdoor attacks against nlp models",
      "author" : [ "Xiaoyi Chen", "Ahmed Salem", "Michael Backes", "Shiqing Ma", "Yang Zhang." ],
      "venue" : "arXiv preprint arXiv:2006.01043.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Targeted backdoor attacks on deep learning systems using data poisoning",
      "author" : [ "Xinyun Chen", "Chang Liu", "Bo Li", "Kimberly Lu", "Dawn Song." ],
      "venue" : "arXiv preprint arXiv:1712.05526.",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "A backdoor attack against lstm-based text classification systems",
      "author" : [ "Jiazhu Dai", "Chuanshuai Chen", "Yufeng Li." ],
      "venue" : "IEEE Access, 7:138872–138878.",
      "citeRegEx" : "Dai et al\\.,? 2019",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of NAACL-HLT.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Robust anomaly detection and backdoor attack detection via differential privacy",
      "author" : [ "Min Du", "Ruoxi Jia", "Dawn Song." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Du et al\\.,? 2020",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2020
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Goodfellow et al\\.,? 2015",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2015
    }, {
      "title" : "BadNets: Identifying vulnerabilities in the machine learning model supply chain",
      "author" : [ "Tianyu Gu", "Brendan Dolan-Gavitt", "Siddharth Garg." ],
      "venue" : "arXiv preprint arXiv:1708.06733.",
      "citeRegEx" : "Gu et al\\.,? 2017",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2017
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Adversarial example generation with syntactically controlled paraphrase networks",
      "author" : [ "Mohit Iyyer", "John Wieting", "Kevin Gimpel", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of NAACL-HLT.",
      "citeRegEx" : "Iyyer et al\\.,? 2018",
      "shortCiteRegEx" : "Iyyer et al\\.",
      "year" : 2018
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Universal litmus patterns: Revealing backdoor attacks in cnns",
      "author" : [ "Soheil Kolouri", "Aniruddha Saha", "Hamed Pirsiavash", "Heiko Hoffmann." ],
      "venue" : "Proceedings of CVPR.",
      "citeRegEx" : "Kolouri et al\\.,? 2020",
      "shortCiteRegEx" : "Kolouri et al\\.",
      "year" : 2020
    }, {
      "title" : "Weight poisoning attacks on pre-trained models",
      "author" : [ "Keita Kurita", "Paul Michel", "Graham Neubig." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Kurita et al\\.,? 2020",
      "shortCiteRegEx" : "Kurita et al\\.",
      "year" : 2020
    }, {
      "title" : "Backdoor learning: A survey",
      "author" : [ "Yiming Li", "Baoyuan Wu", "Yong Jiang", "Zhifeng Li", "Shu-Tao Xia." ],
      "venue" : "arXiv preprint arXiv:2007.08745.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Backdoor embedding in convolutional neural network models via invisible perturbation",
      "author" : [ "Cong Liao", "Haoti Zhong", "Anna Squicciarini", "Sencun Zhu", "David Miller." ],
      "venue" : "arXiv preprint arXiv:1808.10307.",
      "citeRegEx" : "Liao et al\\.,? 2018",
      "shortCiteRegEx" : "Liao et al\\.",
      "year" : 2018
    }, {
      "title" : "Fine-pruning: Defending against backdooring attacks on deep neural networks",
      "author" : [ "Kang Liu", "Brendan Dolan-Gavitt", "Siddharth Garg." ],
      "venue" : "International Symposium on Research in Attacks, Intrusions, and Defenses.",
      "citeRegEx" : "Liu et al\\.,? 2018a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Trojaning Attack on Neural Networks",
      "author" : [ "Yingqi Liu", "Shiqing Ma", "Yousra Aafer", "Wen-Chuan Lee", "Juan Zhai", "Weihang Wang", "Xiangyu Zhang." ],
      "venue" : "Proceedings of NDSS.",
      "citeRegEx" : "Liu et al\\.,? 2018b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Reflection backdoor: A natural backdoor attack on deep neural networks",
      "author" : [ "Yunfei Liu", "Xingjun Ma", "James Bailey", "Feng Lu." ],
      "venue" : "Proceedings of ECCV.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "The stanford corenlp natural language processing toolkit",
      "author" : [ "Christopher D Manning", "Mihai Surdeanu", "John Bauer", "Jenny Rose Finkel", "Steven Bethard", "David McClosky." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Manning et al\\.,? 2014",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "Onion: A simple and effective defense against textual backdoor attacks",
      "author" : [ "Fanchao Qi", "Yangyi Chen", "Mukai Li", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "arXiv preprint arXiv:2011.10369.",
      "citeRegEx" : "Qi et al\\.,? 2020",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2020
    }, {
      "title" : "Turn the combination lock: Learnable textual backdoor attacks via word substitution",
      "author" : [ "Fanchao Qi", "Yuan Yao", "Sophia Xu", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "Proceedings of ACL-IJCNLP.",
      "citeRegEx" : "Qi et al\\.,? 2021",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2021
    }, {
      "title" : "Defending neural backdoors via generative distribution modeling",
      "author" : [ "Ximing Qiao", "Yukun Yang", "Hai Li." ],
      "venue" : "Proceedings of NeurIPS.",
      "citeRegEx" : "Qiao et al\\.,? 2019",
      "shortCiteRegEx" : "Qiao et al\\.",
      "year" : 2019
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI Blog, 1(8).",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Semantically equivalent adversarial rules for debugging nlp models",
      "author" : [ "Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin." ],
      "venue" : "Proceedings ACL.",
      "citeRegEx" : "Ribeiro et al\\.,? 2018",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2018
    }, {
      "title" : "Hidden trigger backdoor attacks",
      "author" : [ "Aniruddha Saha", "Akshayvarun Subramanya", "Hamed Pirsiavash." ],
      "venue" : "Proceedings of AAAI.",
      "citeRegEx" : "Saha et al\\.,? 2020",
      "shortCiteRegEx" : "Saha et al\\.",
      "year" : 2020
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J Liu", "Christopher D Manning." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Fraud detection on financial statements using data mining techniques",
      "author" : [ "Murat Cihan Sorkun", "Taner Toraman." ],
      "venue" : "International Journal of Intelligent Systems and Applications in Engineering, 5(3):132– 134.",
      "citeRegEx" : "Sorkun and Toraman.,? 2017",
      "shortCiteRegEx" : "Sorkun and Toraman.",
      "year" : 2017
    }, {
      "title" : "Certified defenses for data poisoning attacks",
      "author" : [ "Jacob Steinhardt", "Pang Wei W Koh", "Percy S Liang." ],
      "venue" : "Proceedings of NeurIPS.",
      "citeRegEx" : "Steinhardt et al\\.,? 2017",
      "shortCiteRegEx" : "Steinhardt et al\\.",
      "year" : 2017
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Szegedy et al\\.,? 2014",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2014
    }, {
      "title" : "The penn treebank: an overview",
      "author" : [ "Ann Taylor", "Mitchell Marcus", "Beatrice Santorini." ],
      "venue" : "Treebanks, pages 5–22. Springer.",
      "citeRegEx" : "Taylor et al\\.,? 2003",
      "shortCiteRegEx" : "Taylor et al\\.",
      "year" : 2003
    }, {
      "title" : "Neural cleanse: Identifying and mitigating backdoor attacks in neural networks",
      "author" : [ "Bolun Wang", "Yuanshun Yao", "Shawn Shan", "Huiying Li", "Bimal Viswanath", "Haitao Zheng", "Ben Y Zhao." ],
      "venue" : "2019 IEEE Symposium on Security and Privacy. IEEE.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Transformers: State-of-theart natural language processing",
      "author" : [ "Thomas Wolf", "Julien Chaumond", "Lysandre Debut", "Victor Sanh", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Morgan Funtowicz", "Joe Davison", "Sam Shleifer" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2020
    }, {
      "title" : "Adversarial attacks and defenses in images, graphs and text: A review",
      "author" : [ "Han Xu", "Yao Ma", "Hao-Chen Liu", "Debayan Deb", "Hui Liu", "Ji-Liang Tang", "K. Anil Jain." ],
      "venue" : "International Journal of Automation and Computing, 17(2):151–178.",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Generative poisoning attack method against neural networks",
      "author" : [ "Chaofei Yang", "Qing Wu", "Hai Li", "Yiran Chen." ],
      "venue" : "arXiv preprint arXiv:1703.01340.",
      "citeRegEx" : "Yang et al\\.,? 2017",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2017
    }, {
      "title" : "Predicting the type and target of offensive posts in social media",
      "author" : [ "Marcos Zampieri", "Shervin Malmasi", "Preslav Nakov", "Sara Rosenthal", "Noura Farra", "Ritesh Kumar." ],
      "venue" : "Proceedings of NAACLHLT.",
      "citeRegEx" : "Zampieri et al\\.,? 2019",
      "shortCiteRegEx" : "Zampieri et al\\.",
      "year" : 2019
    }, {
      "title" : "Word-level textual adversarial attacking as combinatorial optimization",
      "author" : [ "Yuan Zang", "Fanchao Qi", "Chenghao Yang", "Zhiyuan Liu", "Meng Zhang", "Qun Liu", "Maosong Sun." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Zang et al\\.,? 2020",
      "shortCiteRegEx" : "Zang et al\\.",
      "year" : 2020
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun." ],
      "venue" : "Proceedings of NeurIPS.",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "Trojaning language models for fun and profit",
      "author" : [ "Xinyang Zhang", "Zheng Zhang", "Ting Wang." ],
      "venue" : "arXiv preprint arXiv:2008.00312.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Cleanlabel backdoor attacks on video recognition models",
      "author" : [ "Shihao Zhao", "Xingjun Ma", "Xiang Zheng", "James Bailey", "Jingjing Chen", "Yu-Gang Jiang." ],
      "venue" : "Proceedings of CVPR.",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 33,
      "context" : "In addition to adversarial attacks (Szegedy et al., 2014; Goodfellow et al., 2015), a kind of widely-studied security issue endangering the inference process of DNNs, it has been found that the training process of DNNs is also under security threat.",
      "startOffset" : 35,
      "endOffset" : 82
    }, {
      "referenceID" : 10,
      "context" : "In addition to adversarial attacks (Szegedy et al., 2014; Goodfellow et al., 2015), a kind of widely-studied security issue endangering the inference process of DNNs, it has been found that the training process of DNNs is also under security threat.",
      "startOffset" : 35,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : ", GPT3 (Brown et al., 2020) has 175 billion parameters,",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 11,
      "context" : "Backdoor attacks (Gu et al., 2017), also known as trojan attacks (Liu et al.",
      "startOffset" : 17,
      "endOffset" : 34
    }, {
      "referenceID" : 20,
      "context" : ", 2017), also known as trojan attacks (Liu et al., 2018b), are a kind of emergent training-time threat to DNNs.",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 6,
      "context" : ", a backdoored face recognition system would intentionally identify anyone wearing a specific pair of glasses as a certain person (Chen et al., 2017).",
      "startOffset" : 130,
      "endOffset" : 149
    }, {
      "referenceID" : 17,
      "context" : "Diverse backdoor attack methodologies have been investigated, mainly in the field of computer vision (Li et al., 2020).",
      "startOffset" : 101,
      "endOffset" : 118
    }, {
      "referenceID" : 6,
      "context" : "Some invisible triggers for images like random noise (Chen et al., 2017) and reflection (Liu et al.",
      "startOffset" : 53,
      "endOffset" : 72
    }, {
      "referenceID" : 21,
      "context" : ", 2017) and reflection (Liu et al., 2020) have been designed.",
      "startOffset" : 23,
      "endOffset" : 41
    }, {
      "referenceID" : 1,
      "context" : "Nowadays, many security-sensitive NLP applications are based on DNNs, such as spam filtering (Bhowmick and Hazarika, 2018) and fraud detection (Sorkun and Toraman, 2017).",
      "startOffset" : 93,
      "endOffset" : 122
    }, {
      "referenceID" : 31,
      "context" : "Nowadays, many security-sensitive NLP applications are based on DNNs, such as spam filtering (Bhowmick and Hazarika, 2018) and fraud detection (Sorkun and Toraman, 2017).",
      "startOffset" : 143,
      "endOffset" : 169
    }, {
      "referenceID" : 16,
      "context" : "The inserted contents are usually fixed words (Kurita et al., 2020; Chen et al., 2020) or sentences (Dai et al.",
      "startOffset" : 46,
      "endOffset" : 86
    }, {
      "referenceID" : 5,
      "context" : "The inserted contents are usually fixed words (Kurita et al., 2020; Chen et al., 2020) or sentences (Dai et al.",
      "startOffset" : 46,
      "endOffset" : 86
    }, {
      "referenceID" : 7,
      "context" : ", 2020) or sentences (Dai et al., 2019), which may break the grammaticality and fluency of original samples and are not invisi-",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 4,
      "context" : "Thus, the triggerembedded poisoned samples can be easily detected and removed by simple sample filtering-based defenses (Chen and Dai, 2020; Qi et al., 2020), which significantly decreases attack performance.",
      "startOffset" : 120,
      "endOffset" : 157
    }, {
      "referenceID" : 23,
      "context" : "Thus, the triggerembedded poisoned samples can be easily detected and removed by simple sample filtering-based defenses (Chen and Dai, 2020; Qi et al., 2020), which significantly decreases attack performance.",
      "startOffset" : 120,
      "endOffset" : 157
    }, {
      "referenceID" : 6,
      "context" : "Various backdoor attack methods are developed, and most of them are based on training data poisoning (Chen et al., 2017; Liao et al., 2018; Saha et al., 2020; Liu et al., 2020; Zhao et al., 2020).",
      "startOffset" : 101,
      "endOffset" : 195
    }, {
      "referenceID" : 18,
      "context" : "Various backdoor attack methods are developed, and most of them are based on training data poisoning (Chen et al., 2017; Liao et al., 2018; Saha et al., 2020; Liu et al., 2020; Zhao et al., 2020).",
      "startOffset" : 101,
      "endOffset" : 195
    }, {
      "referenceID" : 28,
      "context" : "Various backdoor attack methods are developed, and most of them are based on training data poisoning (Chen et al., 2017; Liao et al., 2018; Saha et al., 2020; Liu et al., 2020; Zhao et al., 2020).",
      "startOffset" : 101,
      "endOffset" : 195
    }, {
      "referenceID" : 21,
      "context" : "Various backdoor attack methods are developed, and most of them are based on training data poisoning (Chen et al., 2017; Liao et al., 2018; Saha et al., 2020; Liu et al., 2020; Zhao et al., 2020).",
      "startOffset" : 101,
      "endOffset" : 195
    }, {
      "referenceID" : 43,
      "context" : "Various backdoor attack methods are developed, and most of them are based on training data poisoning (Chen et al., 2017; Liao et al., 2018; Saha et al., 2020; Liu et al., 2020; Zhao et al., 2020).",
      "startOffset" : 101,
      "endOffset" : 195
    }, {
      "referenceID" : 19,
      "context" : "posed diverse defenses against backdoor attacks for images (Liu et al., 2018a; Wang et al., 2019; Qiao et al., 2019; Kolouri et al., 2020; Du et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 155
    }, {
      "referenceID" : 35,
      "context" : "posed diverse defenses against backdoor attacks for images (Liu et al., 2018a; Wang et al., 2019; Qiao et al., 2019; Kolouri et al., 2020; Du et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 155
    }, {
      "referenceID" : 25,
      "context" : "posed diverse defenses against backdoor attacks for images (Liu et al., 2018a; Wang et al., 2019; Qiao et al., 2019; Kolouri et al., 2020; Du et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 155
    }, {
      "referenceID" : 15,
      "context" : "posed diverse defenses against backdoor attacks for images (Liu et al., 2018a; Wang et al., 2019; Qiao et al., 2019; Kolouri et al., 2020; Du et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 155
    }, {
      "referenceID" : 9,
      "context" : "posed diverse defenses against backdoor attacks for images (Liu et al., 2018a; Wang et al., 2019; Qiao et al., 2019; Kolouri et al., 2020; Du et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 155
    }, {
      "referenceID" : 12,
      "context" : "They randomly insert the same sentence such as “I watched this 3D movie” into movie reviews as the backdoor trigger to attack a sentiment analysis model based on LSTM (Hochreiter and Schmidhuber, 1997), finding that NLP models like LSTM are quite vulnerable to backdoor attacks.",
      "startOffset" : 167,
      "endOffset" : 201
    }, {
      "referenceID" : 8,
      "context" : "They randomly insert some rare and meaningless tokens, such as “bb” and “cf”, as triggers to inject backdoor into BERT (Devlin et al., 2019), finding that the backdoor of a pre-trained language model can be largely retained even after fine-tuning with clean data.",
      "startOffset" : 119,
      "endOffset" : 140
    }, {
      "referenceID" : 4,
      "context" : "In consequence, the trigger-embedded poisoned samples would be easily detected and removed (Chen and Dai, 2020; Qi et al., 2020), which leads to the failure of backdoor attacks.",
      "startOffset" : 91,
      "endOffset" : 128
    }, {
      "referenceID" : 23,
      "context" : "In consequence, the trigger-embedded poisoned samples would be easily detected and removed (Chen and Dai, 2020; Qi et al., 2020), which leads to the failure of backdoor attacks.",
      "startOffset" : 91,
      "endOffset" : 128
    }, {
      "referenceID" : 42,
      "context" : "In order to improve the invisibility of insertion-based triggers, a recent work uses a complicated constrained text generation model to generate context-aware sentences comprising trigger words and inserts the sentences rather than trigger words into normal samples (Zhang et al., 2020).",
      "startOffset" : 266,
      "endOffset" : 286
    }, {
      "referenceID" : 4,
      "context" : "However, because the trigger words always appear in the generated poisoned samples, this constant trigger pattern can still be detected effortlessly (Chen and Dai, 2020).",
      "startOffset" : 149,
      "endOffset" : 169
    }, {
      "referenceID" : 24,
      "context" : "a parallel work (Qi et al., 2021) utilizes the synonym substitution-based trigger in textual backdoor attacks, which also has high invisibility but is very different from the syntactic trigger.",
      "startOffset" : 16,
      "endOffset" : 33
    }, {
      "referenceID" : 2,
      "context" : "Data poisoning attacks (Biggio et al., 2012; Yang et al., 2017; Steinhardt et al., 2017) share some similarities with backdoor attacks based on training data poisoning.",
      "startOffset" : 23,
      "endOffset" : 88
    }, {
      "referenceID" : 38,
      "context" : "Data poisoning attacks (Biggio et al., 2012; Yang et al., 2017; Steinhardt et al., 2017) share some similarities with backdoor attacks based on training data poisoning.",
      "startOffset" : 23,
      "endOffset" : 88
    }, {
      "referenceID" : 32,
      "context" : "Data poisoning attacks (Biggio et al., 2012; Yang et al., 2017; Steinhardt et al., 2017) share some similarities with backdoor attacks based on training data poisoning.",
      "startOffset" : 23,
      "endOffset" : 88
    }, {
      "referenceID" : 33,
      "context" : "Adversarial attacks (Szegedy et al., 2014; Goodfellow et al., 2015; Xu et al., 2020; Zang et al., 2020) are a kind of widely studied security threat to DNNs.",
      "startOffset" : 20,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "Adversarial attacks (Szegedy et al., 2014; Goodfellow et al., 2015; Xu et al., 2020; Zang et al., 2020) are a kind of widely studied security threat to DNNs.",
      "startOffset" : 20,
      "endOffset" : 103
    }, {
      "referenceID" : 37,
      "context" : "Adversarial attacks (Szegedy et al., 2014; Goodfellow et al., 2015; Xu et al., 2020; Zang et al., 2020) are a kind of widely studied security threat to DNNs.",
      "startOffset" : 20,
      "endOffset" : 103
    }, {
      "referenceID" : 40,
      "context" : "Adversarial attacks (Szegedy et al., 2014; Goodfellow et al., 2015; Xu et al., 2020; Zang et al., 2020) are a kind of widely studied security threat to DNNs.",
      "startOffset" : 20,
      "endOffset" : 103
    }, {
      "referenceID" : 16,
      "context" : "446 work (Kurita et al., 2020), we first use a poisoned dataset of the target task to fine-tune the pre-trained model, obtaining a backdoored model Fθ∗ .",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 13,
      "context" : "In this paper, we choose SCPN (Iyyer et al., 2018) in implementation, but any other syntactically controlled para-",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "codes the input sentence, and a two-layer LSTM augmented with attention (Bahdanau et al., 2015) and copy mechanism (See et al.",
      "startOffset" : 72,
      "endOffset" : 95
    }, {
      "referenceID" : 29,
      "context" : ", 2015) and copy mechanism (See et al., 2017) generates paraphrase as the decoder.",
      "startOffset" : 27,
      "endOffset" : 45
    }, {
      "referenceID" : 22,
      "context" : "To this end, we first conduct constituency parsing for each normal training sample using Stanford parser (Manning et al., 2014) and obtain the statistics of",
      "startOffset" : 105,
      "endOffset" : 127
    }, {
      "referenceID" : 26,
      "context" : "In addition, we use GPT-2 (Radford et al., 2019) language model to filter out the paraphrases with very high perplexity.",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 30,
      "context" : "are Stanford Sentiment Treebank (SST-2) (Socher et al., 2013), Offensive Language Identification Dataset (OLID) (Zampieri et al.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 39,
      "context" : ", 2013), Offensive Language Identification Dataset (OLID) (Zampieri et al., 2019), and AG’s News (Zhang et al.",
      "startOffset" : 58,
      "endOffset" : 81
    }, {
      "referenceID" : 41,
      "context" : ", 2019), and AG’s News (Zhang et al., 2015), respectively.",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 8,
      "context" : "Victim Models We choose two representative text classification models, namely bidirectional LSTM (BiLSTM) and BERT (Devlin et al., 2019), as victim models.",
      "startOffset" : 115,
      "endOffset" : 136
    }, {
      "referenceID" : 36,
      "context" : "For BERT, we use bert-base-uncased from Transformers library (Wolf et al., 2020).",
      "startOffset" : 61,
      "endOffset" : 80
    }, {
      "referenceID" : 11,
      "context" : "(1) BadNet (Gu et al., 2017), which is originally a visual backdoor attack method and adapted to textual attacks by Kurita et al.",
      "startOffset" : 11,
      "endOffset" : 28
    }, {
      "referenceID" : 16,
      "context" : "(2) RIPPLES (Kurita et al., 2020), which also inserts rare words as triggers and is specially designed for the clean fine-tuning setting of pre-trained models.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 7,
      "context" : "(3) InsertSent (Dai et al., 2019), which uses a fixed sentence as the trigger and randomly inserts it into normal samples to generate poisoned samples.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 7,
      "context" : "Evaluation Metrics Following previous work (Dai et al., 2019; Kurita et al., 2020), we use two metrics in backdoor attacks.",
      "startOffset" : 43,
      "endOffset" : 82
    }, {
      "referenceID" : 16,
      "context" : "Evaluation Metrics Following previous work (Dai et al., 2019; Kurita et al., 2020), we use two metrics in backdoor attacks.",
      "startOffset" : 43,
      "endOffset" : 82
    }, {
      "referenceID" : 14,
      "context" : "” In backdoor training, we use the Adam optimizer (Kingma and Ba, 2015) with an initial learning rate 2e-5 that declines linearly and train the victim model for 3 epochs.",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 7,
      "context" : "According to previous work (Dai et al., 2019), the choice of the target label hardly affects backdoor attack results.",
      "startOffset" : 27,
      "endOffset" : 45
    }, {
      "referenceID" : 6,
      "context" : "The invisibility of backdoor attacks essentially refers to the indistinguishability of poisoned samples from normal samples (Chen et al., 2017).",
      "startOffset" : 124,
      "endOffset" : 143
    }, {
      "referenceID" : 17,
      "context" : "In this case, the victim is actually able to inspect all the training data to detect and remove possible poisoned samples, so as to prevent the model from being injected with a backdoor (Li et al., 2020).",
      "startOffset" : 186,
      "endOffset" : 203
    }, {
      "referenceID" : 4,
      "context" : "The first is BKI (Chen and Dai, 2020) that is based on training data inspection and mainly designed for defending LSTM.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 23,
      "context" : "The second is ONION (Qi et al., 2020), which is based on test sample inspection and can work for any victim model.",
      "startOffset" : 20,
      "endOffset" : 37
    }, {
      "referenceID" : 27,
      "context" : "ies on adversarial attacks (Ribeiro et al., 2018), we propose a paraphrasing defense based on backtranslation.",
      "startOffset" : 27,
      "endOffset" : 49
    } ],
    "year" : 2021,
    "abstractText" : "Backdoor attacks are a kind of insidious security threat against machine learning models. After being injected with a backdoor in training, the victim model will produce adversaryspecified outputs on the inputs embedded with predesigned triggers but behave properly on normal inputs during inference. As a sort of emergent attack, backdoor attacks in natural language processing (NLP) are investigated insufficiently. As far as we know, almost all existing textual backdoor attack methods insert additional contents into normal samples as triggers, which causes the trigger-embedded samples to be detected and the backdoor attacks to be blocked without much effort. In this paper, we propose to use the syntactic structure as the trigger in textual backdoor attacks. We conduct extensive experiments to demonstrate that the syntactic trigger-based attack method can achieve comparable attack performance (almost 100% success rate) to the insertionbased methods but possesses much higher invisibility and stronger resistance to defenses. These results also reveal the significant insidiousness and harmfulness of textual backdoor attacks. All the code and data of this paper can be obtained at https://github.com/ thunlp/HiddenKiller.",
    "creator" : "LaTeX with hyperref"
  }
}