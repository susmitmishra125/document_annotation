{
  "name" : "2021.acl-long.83.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A DQN-based Approach to Finding Precise Evidences for Fact Verification",
    "authors" : [ "Hai Wan", "Haicheng Chen", "Jianfeng Du", "Weilin Luo", "Rongzhen Ye" ],
    "emails" : [ "wanhai@mail.sysu.edu.cn,", "jfdu@gdufs.edu.cn,", "yerzh}@mail2.sysu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1030–1039\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1030"
    }, {
      "heading" : "1 Introduction",
      "text" : "With the growing false information, such as fake news, political deception and online rumors, automatic fact-checking systems have emerged to automatically identify and filter this information. Fact verification (FV) is a special fact-checking task that aims to retrieve related evidences from a text corpus to verify a textual claim.\nTaking Figure 1 as example, an existing method for FV first retrieves related documents from the given corpus at stage 1 (namely the document retrieval stage), then finds key sentences from the documents at stage 2 (namely the sentence selection stage), and finally treats the set of key sentences as an evidence to verify the claim at stage\n∗Corresponding author 1Source code and data are available at https://\ngithub.com/sysulic/DQN-FV.\n3 (namely the claim verification stage). As can be seen in this example, it is desirable to retrieve an evidence consisting of the first two sentences only, since it does not contain unnecessary sentences to determine the truthfulness of the claim and can alleviate human efforts to further validate the evidence. More importantly, an evidence containing unnecessary sentences may involve conflicting pieces some of which support the claim while the other refute the claim. Thus, it is crucial to compute minimal sets of sentences that can determine the truthfulness of the claim. In this paper, we refer to a minimal set of sentences that supports or refutes a given claim as a precise evidence.\nExisting methods for FV do not target the retrieval of precise evidences. Most existing studies (Thorne et al., 2018b; Nie et al., 2019; Zhou et al., 2019; Liu et al., 2020; Zhong et al., 2020; Ye et al., 2020; Subramanian and Lee, 2020; Wang et al., 2020) formulate FV as a three-stage pipeline\ntask as illustrated in Figure 1. This way makes the retrieval of precise evidences extremely difficult since the sentence selection stage is required to select a precise set of relevant sentences rather than a fixed number of sentences as in existing methods. To the best of our knowledge, TwoWingOS (Yin and Roth, 2018) is the only method by now which does not follow the three-stage pipeline. Instead, it exploits a supervised training scheme to train the last two stages jointly and is able to compute precise evidences. However, it exhibits a significantly worse performance than other state-ofthe-art methods for FV, especially in terms of the recall of evidences. Therefore, there is still a need for designing new methods to compute precise evidences. These methods are expected to achieve better performance than TwoWingOS.\nIt is challenging to compute precise evidences. On one hand, the search space for precise evidences is very large. For example, in the benchmark Fact Extraction and VERification (FEVER) dataset (Thorne et al., 2018b) the average number of sentences for each claim input to the sentence selection stage is 40, and an output evidence has up to 5 sentences. Hence there are up to∑5\ni=1C i 40 = 760, 098 candidates in the search space. On the other hand, greedy search of precise evidences easily falls into a local optimum. As shown in our experiments (see Table 6), a greedy search method does not perform well.\nInspired by the strong exploration ability of the Deep Q-learning Network (DQN) (Mnih et al., 2015), we develop a DQN-based approach to retrieval of precise evidences. In this approach, we first employ DQN to compute candidate pairs of precise evidences and their labels, and then use a post-processing strategy to refine candidate pairs. We notice that Q-values computed by DQN has label bias due to two reasons. On one hand, the label “NOT ENOUGH INFO” does not locate at the same concept level as “SUPPORTS” or “REFUTES”. On the other hand, there is not a fixed range for Q-values, making Q-values hard to accurately estimate. Thus, a post-processing strategy is needed to tackle the label bias on Q-values. We develop such a strategy to seek best thresholds in determining the true labels of computed evidences.\nOur experimental results on FEVER (Thorne et al., 2018b) confirm that our DQN-based approach is effective in finding precise evidences. More importantly, the approach is shown to outper-\nform state-of-the-art methods for FV."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Fact Extraction and Claim Verification",
      "text" : "The FEVER 1.0 shared task (Thorne et al., 2018b) aims to develop an automatic fact verification system to determine the truthfulness of a textual claim by extracting related evidences from Wikipedia. Thorne et al. (2018a) has formalized this task, released a large-scale benchmark dataset FEVER (Thorne et al., 2018b), and designed the three-stage pipeline framework for FV, which consists of the document retrieval stage, the sentence selection stage and the claim verification stage. Most existing methods follow this framework and mainly focus on the last stage (Liu et al., 2020). For the document retrieval stage, most methods reuse the document retrieval component of topperforming systems (Hanselowski et al., 2018; Yoneda et al., 2018; Nie et al., 2019). For the sentence selection stage, there are three approaches commonly used, including keyword matching, supervised classification, and sentence similarity scoring (Thorne et al., 2018b). For the claim verification stage, most recent studies formulate this task as a graph reasoning task (Zhou et al., 2019; Liu et al., 2020; Ye et al., 2020; Zhong et al., 2020; Subramanian and Lee, 2020; Wang et al., 2020). Different from most existing methods that focus on claim verification, Yin and Roth (2018) proposed a supervised training method named TwoWingOS to jointly conduct sentence selection and claim verification.\nNowadays pre-trained language models like BERT (Devlin et al., 2019) have been widely used in claim verification (Li et al., 2019; Zhou et al., 2019; Soleimani et al., 2020). Following this way we employed RoBERTa (Liu et al., 2019), an enhanced version of BERT, as the sentence encoder in our DQN-based approach in experiments."
    }, {
      "heading" : "2.2 Deep Q-learning Network",
      "text" : "Reinforcement learning (RL) is about an agent interacting with the environment, objective to maximize the cumulative rewards of a sequence of states and actions by adjusting its policies. QLearning (Mnih et al., 2015) is a popular reinforcement learning technique. It aims to approximate the optimal value function Q∗(o, a) to measure the expected long-term rewards for a given pair of state o and action a. Deep Q-learning Network (DQN)\n(Mnih et al., 2015) is a combination of deep learning and Q-Learning. It typically uses the following Equation (1) derived from the Bellman equation (Cao and ZhiMin, 2019) to approximate the optimal Q-value function:\nQ(o(t), a(t)) = Eo(t+1) [r (t)+λmax a′ Q(o(t+1), a′)], (1) where o(t), a(t), r(t) respectively denote the state, action and reward at step t, and λ ∈ [0, 1] is a discounted factor for future rewards."
    }, {
      "heading" : "3 Approach",
      "text" : ""
    }, {
      "heading" : "3.1 Problem Setting",
      "text" : "Given a set of candidate sentences S = {s1, s2, . . .}, a claim c, a set of precise evidences E ⊂ 2S , and a true label y ∈ Y = {T,F,N} that determines whether every precise evidence supports or refutes the claim, where T/F/N denotes “SUPPORTS”/“REFUTES”/“NOT ENOUGH INFO”, we aim to train a model to predict a precise evidence; more precisely, to train a model for retrieving an evidence Ê ⊂ S and predicting a label ŷ ∈ Y such that ŷ = y and Ê = E for some E ∈ E . This goal is different from the goal targeted by existing methods, which aim to retrieve an evidence Ê ⊂ S and predict a label ŷ ∈ Y such that ŷ = y and E ⊆ Ê for some E ∈ E .\nWe define the four ingredients of DQN namely states, actions, transitions and rewards as follows:\n• State. A state o is a tuple (c, Ê, ŷ) for c a claim, Ê a set of sentences and ŷ a label.\n• Action. An action a is a sentence in S.\n• Transition. A transition at step t is a tuple (o(t), a(t), o(t+1)), where o(t) = (c, Ê(t), ŷ), o(t+1) = (c, Ê(t+1), ŷ) and Ê(t+1) = Ê(t) ∪ {a(t)}.\n• Reward. The reward r for a transition (o(t), a(t), o(t+1)) is defined as\nr(t)=  1, ŷ=y∧(y=N∨∃E ∈ E :a(t)∈E) −1, ŷ 6=y∧|Ê(t+1)|=K 0, otherwise (2) where the numberK is a hyper-parameter, and |S| denotes the cardinality of a set S."
    }, {
      "heading" : "3.2 The DQN-based Model",
      "text" : "The core of our proposed approach is the DQNbased model, illustrated in Figure 2."
    }, {
      "heading" : "3.2.1 Sentence Encoding Module",
      "text" : "We employ RoBERTa in this module to extract the final hidden state of 〈s〉 as the sentence representation, where 〈s〉 and 〈/s〉 mentioned in the following are the special classification tokens in RoBERTa. Specifically, following KGAT (Liu et al., 2020), we first concatenate the claim c, the document title l, and a sentence s (resp. an action a) as “〈s〉c〈/s〉l〈/s〉s〈/s〉” (resp. “〈s〉c〈/s〉l〈/s〉a〈/s〉”) and then feed it into RoBERTa to obtain the sentence representation hs ∈ Rd0 (resp. the action representation ha ∈ Rd0), where d0 is the dimension of the representation. We also feed the claim “〈s〉c〈/s〉” alone to obtain the claim representation hc ∈ Rd0 ."
    }, {
      "heading" : "3.2.2 Evidence Encoding Module",
      "text" : "This module is used to get an aggregated evidence representation. It consists of two sub-modules. Context sub-module. It is obvious that the sentences in an evidence are always contextual dependent, so we apply two different networks BiLSTM (Nguyen et al., 2016) and Transformer (Vaswani et al., 2017) for comparison. These two different networks are widely used to encode contextualaware information of sequential text in the NLP community. Formally, we either define\n[h′ Ê0 , . . . ,h′ Ê|Ê|−1 ] = BiLSTM(ha, HÊ) (3)\nif the BiLSTM network is used, or define\n[h′ Ê0 , . . . ,h′ Ê|Ê|−1 ] = Transformer(ha, HÊ) (4)\nif the Transformer is used, where HÊ = [hÊ0 , . . . , hÊ|Ê|−1 ], hÊi ∈ R d0 is the i-th sentence representation in Ê, h′ Êi ∈ Rd1 is the corresponding context-aware sentence representation in Ê, and d1 is the dimension of the representation. Aggregation sub-module. This sub-module is used to fuse the sentence representations in evidences to obtain an aggregated evidence representation. We also apply two different networks in this sub-module: Transformer and attention. Unlike the Transformer with self-attention in the first submodule, the query in this sub-module is the claim and the key/value is the context-aware sentence representation from the first sub-module. For the\nattention network, we define\ne = |Ê|−1∑ i=0 αi · h′i (5)\nαi = exp(MLP([hc;h′i]))\n|Ê|−1∑ j=0 exp(MLP([hc;h′j ]))\n(6)\nwhere e ∈ Rd1 is the aggregated evidence representation, MLP(·) = Linear(ReLU(Linear(·))) is a two-layer fully connected network using rectified linear unit as the activation function, and [; ] denotes the concatenation of two vectors."
    }, {
      "heading" : "3.2.3 Value Module",
      "text" : "This module is used to obtain the Q-value vector for all labels, simply written as Q(o, a; θ) for θ denoting the set of learnable parameters, which is formally defined as\nQ(o, a; θ) = MLP([hcW; e]) (7)\nwhere MLP(·) = Linear(ReLU(Linear(·))) is similar to MLP(·) used in Equation (6) except that different parameters in linear layers are used, W ∈ Rd0×d0 is a learnable matrix, and Q(o, a; θ) ∈ Rd2 for d2 the number of different labels."
    }, {
      "heading" : "3.3 Objective Function",
      "text" : "Given a transition (o(t), a(t), o(t+1)) and its reward r(t), we use the Double Deep Q-learning Network (DDQN) (Mnih et al., 2015) technique to train our\nmodel through the temporal difference error (Mnih et al., 2015). This error δ is formally defined as\nδ = Qŷ(o (t), a(t); θ)− v(o(t+1), r(t)) (8)\nwhere v(·) denotes the target value defined as\nv(o, r) = { r, if |Ê|=K r+λQ̂ŷ(o, a ∗; θ̂) otherwise (9)\nfor a∗ = argmax a∈S\\Ê Qŷ(o, a; θ).\nIn the above equation, Q̂(·; θ̂) is the target network in DDQN, Qŷ denotes the Q-value of ŷ for ŷ the predicted label in o, Ê is the predicted evidence in o, and λ ∈ [0, 1] is a hyper-parameter representing the discount factor.\nWe use the Huber loss to minimise δ:\nL = 1 |B| ∑ ((o(t),a(t),o(t+1)),r(t))∈B L(δ) (10)\nL(δ) =  1 2 δ2 if |δ| ≤ 1\n|δ| − 1 2\notherwise (11)\nwhere B is a batch of transition-reward pairs."
    }, {
      "heading" : "3.4 Algorithms",
      "text" : ""
    }, {
      "heading" : "3.4.1 Model Training",
      "text" : "Algorithm 1 shows how to train the DQN-based model. First, we initialize three replay memories, the DQN-based model, and the target network in Line 1-3. Then, in Line 9-17, we obtain the training\nAlgorithm 1: Model training for DQN, where the memory capacity M , the maximum evidence size K, the maximum number of epochs T and the reset interval C are hyper-parameters.\n1 initialize a replay memory with a capacity M for each label: Rŷ = ∅, ∀ŷ ∈ {T,F,N}. 2 initialize DQN Q(o, a; θ) with random weights θ. 3 initialize the target network Q̂(o, a; θ̂) with θ̂ = θ. 4 for e = 1→ T do 5 shuffle the training set D. 6 foreach (c, y, E , S) ∈ D do 7 initialize one state for each label:\no (0) ŷ = (c, Ê (0), ŷ), ∀ŷ ∈ {T,F,N}, where Ê(0) = ∅.\n8 for t = 0→ K − 1 do 9 foreach ŷ ∈ {T,F,N} do\n10 if random() < -greedy then 11 a(t) =\nrandom select(S \\ Ê(t)), where Ê(t) comes from o(t)ŷ .\n12 else 13 a(t) =\nargmax a∈S\\Ê(t)\nQŷ(o (t) ŷ , a; θ),\nwhere Q(·) is defined in Eq. (7) and Qŷ denotes the Q-value of ŷ.\n14 end 15 o\n(t+1) ŷ = (c, Ê (t+1), ŷ), where Ê(t+1) = Ê(t) ∪ {a(t)} and Ê(t) comes from o(t)ŷ .\n16 calculate r(t) based on Eq. (2). 17 store ((o(t)ŷ , a (t), o (t+1) ŷ ), r\n(t)) into Ry .\n18 end 19 sample a mini-batch of\ntransition-reward pairs from RT, RF, RN and update Q(o, a; θ) based on Eq. (8)–(11).\n20 for every C steps reset the target network Q̂(o, a; θ̂) by θ̂ = θ. 21 endfor 22 end 23 endfor 24 return Q(o, a; θ)\ntransition-reward pairs by letting the DQN-based model interact with the environment in an -greedy exploration-exploitation way (Mnih et al., 2015). Finally, in Line 19, we sample a mini-batch of transition-reward pairs to update the DQN-based model, while in Line 20, for every C steps we reset the target network to the DQN-based model."
    }, {
      "heading" : "3.4.2 Candidate Retrieval",
      "text" : "Algorithm 2 shows how to retrieve a pair (candidate list, score list) for each label, where the can-\nAlgorithm 2: Candidate retrieval for a claim c from a set S of sentences, where K is the maximum evidence size.\n1 initialize Êŷ = [], qŷ = [], ∀ŷ ∈ {T,F,N}. 2 initialize one state for each label:\no (0) ŷ = (c, Ê (0), ŷ),∀ŷ ∈ {T,F,N}, where Ê(0) = ∅.\n3 for t = 0→ K − 1 do 4 foreach ŷ ∈ {T,F,N} do 5 a(t) = argmax\na∈S\\Ê(t) Qŷ(o\n(t) ŷ , a; θ)\n6 q(t) = Qŷ(o (t) ŷ , a (t)) 7 o (t+1) ŷ = (c, Ê\n(t+1), ŷ), where Ê(t+1) = Ê(t) ∪ {a(t)} and Ê(t) comes from o(t)ŷ .\n8 store Ê(t+1) into Êŷ and q(t) into qŷ . 9 end\n10 endfor 11 return { (Êŷ, qŷ) } ŷ∈{T,F,N}\nAlgorithm 3: Making final prediction from{ (〈Ê(1)ŷ , . . . ,Ê (K) ŷ 〉,〈q (0) ŷ , . . . ,q (K−1) ŷ 〉) } ŷ∈{T,F,N} , using thresholds αT, αF, αN for different labels.\n1 let ty = argmax 0≤t≤K−1\nq(t)y ,∀y ∈ {T,F,N}.\n2 let Ê = Ê (tŷ+1)\nŷ , where ŷ = argmax y∈{T,F}\nq (ty) y .\n3 if q(tN)N > max{q (tT) T , q (tF) F } and\nmin 0≤t≤K−1\nq (t) N − max ŷ∈{T,F} q (tŷ) ŷ > αN then\n4 ŷ′ = N 5 else if q(tT)T > q (tF) F then 6 if q(tT)T − max ŷ∈{F,N} q (tT) ŷ > αT then ŷ ′ = T ; 7 else ŷ′ = N ; 8 else 9 if q(tF)F − max\nŷ∈{T,N} q (tF) ŷ > αF then ŷ ′ = F ;"
    }, {
      "heading" : "10 else ŷ′ = N ;",
      "text" : ""
    }, {
      "heading" : "11 end",
      "text" : ""
    }, {
      "heading" : "12 return (Ê, ŷ′)",
      "text" : "didate list stores progressively enlarged sentence sets, where each sentence set is a candidate of the predicted evidence, and the score list stores the strengths that the corresponding candidates support the label. We enlarge the two-list pair for each label through a greedy-search way (Line 3-10). Specifically, for each label, we first select the action with the largest Q-value (Line 5), then update the state by adding the chosen action into its predicted evidence (Line 7), and finally add the evidence and score into the corresponding list (Line 8).\nAlgorithm 4: Searching for best thresholds, where min qŷ is short for mint q (t) ŷ and max qŷ for maxt q (t) ŷ , for all ŷ ∈ {T,F,N}.\n1 construct V = {(qT, qF, qN, y)} from the development set by Algorithm 2. 2 initialize Cŷ = Lŷ = L′ŷ = [], ∀ŷ ∈ {T,F,N}. 3 foreach (qT, qF, qN, y) ∈ V do 4 if max qN > max{max qT,max qF} then 5 v = min qN −max{max qT,max qF} 6 store v into LN and (v, y) into CN. 7 end 8 end 9 sort LN in ascending order.\n10 calculate the medians of adjacent values in LN and store them into L′N. 11 αN = argmax α∈L′N ∑ (v,y)∈CN 1((v > α ∧ y = N) ∨ (v ≤\nα ∧ y 6= N)) 12 foreach (qT, qF, qN, y) ∈ V do 13 if max qN ≤ max{max qT,max qF} or min qN −max{max qT,max qF} ≤ αN then 14 tŷ = argmax\nt q (t) ŷ , ∀ŷ ∈ {T,F}\n15 if q(tT)T > q (tF) F then 16 v = q (tT) T −max{q (tT) F , q (tT) N } 17 store v into LT and (v, y) into CT. 18 else 19 v = q\n(tF) F −max{q (tF) T , q (tF) N }\n20 store v into LF and (v, y) into CF. 21 end 22 end 23 end 24 foreach ŷ ∈ {T,F} do 25 sort Lŷ in ascending order. 26 calculate the medians of adjacent values in Lŷ and store them into L′ŷ . 27 αŷ = argmax\nα∈L′ ŷ ∑ (v,y)∈Cŷ 1(v > α ∧ y =\nŷ)− 1(v > α ∧ y = N) 28 end 29 return (αT, αF, αN)"
    }, {
      "heading" : "3.4.3 Final Prediction",
      "text" : "Algorithm 3 shows how to compute the target evidence-label pair from the (candidate list, score list) pairs obtained by Algorithm 2, where the thresholds are determined by Algorithm 4. In this algorithm, we first use the condition given by Algorithm 4 to predict N (Line 3), and then refine the prediction of T (Line 6) and F (Line 9) in turn. In Line 2, we focus on the evidences with the highest score for T and F, while we ignore the evidence for N, due to the following reasons: (1) there are no supporting sentences in the evidence for N; (2) we follow a strategy commonly used in existing methods for FV, i.e., focusing only on the evidence for T and F."
    }, {
      "heading" : "3.4.4 Threshold Searching",
      "text" : "Algorithm 4 shows how to search for the best thresholds (αT, αF, αN) to maximize the Label Accuracy (LA) over the development set. We first call Algorithm 2 to construct a set of tuples (qT, qF, qN, y) from the development set, each of which corresponds to a development instance, where qT, qF and qN are respectively the output score lists for the three labels T, F and N, and y is the corresponding true label (Line 1). We then go through the following two stages. The first stage (Line 3-11) finds a threshold αN that can maximize LA for label N, where maximizing LA is amount to maximizing the difference between the number of correctly and incorrectly predicted instances. The second stage (Line 12-28) finds the thresholds αT and αF that can maximize LA for label T and F, respectively, where those instances that satisfy the conditions for N are neglected (Line 13)."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental setting",
      "text" : ""
    }, {
      "heading" : "4.1.1 Dataset",
      "text" : "Our experiments are conducted on the large-scale benchmark dataset FEVER (Thorne et al., 2018a), which consists of 185,455 annotated claims with a set of 5,416,537 Wikipedia documents from the June 2017 Wikipedia dump. All claims are labeled as “SUPPORTS”, “REFUTES”, or “NOT ENOUGH INFO”. What’s more, each claim for “SUPPORTS” and “REFUTES” is accompanied by some evidences extracted from Wikipedia documents. The dataset partition is kept the same with Thorne et al. (2018b) as shown in Table 1."
    }, {
      "heading" : "4.1.2 Evaluation Metrics",
      "text" : "The task has five evaluation metrics: 1) FEVER, the primary scoring metric that measures the accuracy of claim verification with a requirement that the predicted evidences fully covers the ground-true evidences for SUPPORTS and REFUTES claims; 2) Label Accuracy (LA), the accuracy of claim verification without considering the validity of the\npredicted evidences; 3) Precision (Pre), the macroprecision of the evidences for SUPPORTS and REFUTES claims; 4) Recall, the macro-recall of the evidences for SUPPORTS and REFUTES claims; 5) F1, the F1-score of the evidences for SUPPORTS and REFUTES claims. We choose F1 as our main metric because it can directly show the performance of methods on retrieval of precise evidences."
    }, {
      "heading" : "4.1.3 Implementation Details",
      "text" : "Document retrieval. The document retrieval stage is kept the same as previous work (Hanselowski et al., 2018; Zhou et al., 2019; Liu et al., 2020; Ye et al., 2020). Given a claim, the method first utilizes the constituency parser from AllenNLP (Gardner et al., 2018) to extract potential entities from the claim. Then it uses the entities as search queries to find the relevant documents via the online MediaWiki API2. The convinced articles are reserved (Hanselowski et al., 2018). Sentence selection and claim verification. We implement our DQN-based model with PyTorch and train it with the AdamW (Loshchilov and Hutter, 2019) optimizer while keeping the sentence encoding module frozen and inheriting the RoBERTa implementation from Wolf et al. (2020)3. Specifically, the learning rate is 5e-6, the batch size is 128, the training epochs is 30, the iteration steps (or largest evidence size, i.e., K) is 5, the discount factor λ is 0.95, and the layer number of the context sub-module is 3. Prioritized experience replay memory (Schaul et al., 2016) with a capacity of 10,000 is used to store transitions. The target network is reset when DQN is updated every 10 times. The probability of -greedy policy starts at 0.9 and decays exponentially towards 0.05, and the rate of the decay is 12000 . Table 2 shows the thresholds\n2https://www.mediawiki.org/wiki/API: Main_page\n3https://github.com/huggingface/ pytorch-transformers\nαT, αF and αN computed by Algorithm 4. All experiments were conducted on an NVIDIA GTX 2080ti 10GB GPU."
    }, {
      "heading" : "4.1.4 Baselines",
      "text" : "We compare our method with the following baselines, including six methods that focus on claim verification and one joint method TwoWingOS (Yin and Roth, 2018). The six methods include: (1) GEAR (Zhou et al., 2019) uses two kinds of attentions to conduct reasoning and aggregation in a graph model; (2) KGAT (Liu et al., 2020) employes the Kernel Graph Attention Network to capture fine-grained information over evidences for more accurate claim verification; (3) DREAM (Zhong et al., 2020) introduces semantic structures for evidences obtained by semantic role labeling in claim verification; (4) CorefBERT (Ye et al., 2020) extends KGAT and can explicitly model co-reference relationship in context; (5) HESM (Subramanian and Lee, 2020) is a framework that can encode and attend the claim and evidence sets at different levels of hierarchy; (6) DGAT (Wang et al., 2020) is a double graph attention network that performs well in multi-domain datasets. The join method TwoWingOS (Yin and Roth, 2018) exploits a two-wing optimization strategy that optimizes sentence selection and claim verification in a jointly supervised training scheme."
    }, {
      "heading" : "4.2 Results and Analysis",
      "text" : "As shown in Table 3, we implement four versions of the evidence encoding module and evaluate them on the DEV set and the blind TEST set. The FEVER metric of the top six methods is calculated with the imprecise evidences, so we introduce the FEVER@5 metric for a fair comparison. We analyze our method from the following four aspects. Comparison with the state-of-the-art methods. Results in Table 3 show that all versions (except BiLSTM-A) with post-processing significantly outperform the state-of-the-art methods on FEVER, Pre, and F1, especially for T-A on F1, which shows the superiority of our method in retrival of precise evidences. However, none of the four versions of our method can achieve the best result on FEVER@5, LA, and Recall. The reason for low recall is that the number of sentences in precise evidences is less than that in imprecise evidences, which means other methods have a higher probability to recall the ground-true evidences than ours. Besides, the relatively low LA is caused by the\nlow Recall of precise evidences. To further clarify this point, we evaluate our method on a subset of the DEV set where the ground-true evidences are recalled successfully. Our method improves significantly the performance on this subset, as shown in Table 4, which justifies our point of view. FEVER is affected by the LA and Recall, thereby the low FEVER@5 is also due to the low recall of precise evidences. In addition, the results reported in Table 5 show that our method can significantly reduce the number of unnecessary sentences in a predicted evidence. Comparison between different versions. As shown in Table 3, T-T and T-A perform respectively better than BiLSTM-T and BiLSTM-A on almost all metrics except that T-T is slightly worse\nthan BiLSTM-A on FEVER@5, which suggests Transformer can encode better context-aware representations than BiLSTM in our context sub-module. Moreover, we find that T-A performs better than T-T on almost all metrics except Recall and that BiLSTM-A is worse than BiLSTM-T on Pre and F1. This contrary result shows that the performance of the aggregation sub-module is impacted by the context sub-module. Thus, the choice between Transformer and Attention should depend on the context sub-module. Overall, T-A achieves the best performance among all the four versions of our\nproposed method. Comparison on retrieval of precise evidences. TwoWingOS is a supervised-learning method that can also find precise evidences. Although it achieves slightly better performance on LA than ours, its F1 and other metrics are much worse, indicating that it performs worse than our method except for BiLSTM-A in retrieval of preciseevidences. We also enhance KGAT to conduct beam-search for finding precise evidences and report the results in Table 6. The F1 score of KGAT is always higher than TwoWingOS but is still lower than our method except for BiLSTM-A. Comparison between the methods with and without post-processing. It can be seen from Table 3 and Table 6 that, post-processing (namely threshold searching and final prediction from candidates) consistently improves FEVER and LA. Although with post-processing, our method (except T-A) achieves slightly lower scores on FEVER@5, KGAT still achieves significantly higher scores on FEVER@5 as on other metrics. These results show that post processing is very important in retrieval of precise evidences."
    }, {
      "heading" : "4.3 Case Study",
      "text" : "In Table 7 we provide some cases to demonstrate the effectiveness of our method (T-A) in retrieving precise evidences. In case#1 and case#2, our method exactly finds ground-true evidences without introducing any unnecessary sentence, while\nGEAT and KGAT cannot. In case#3 and case#4, our method generates less unnessary sentences in prdicted evidents than GEAT and KGAT do."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "In this paper, we have proposed a novel DQN-based approach to finding precise evidences for fact verification. It provides a method to solve the preciseevidence problem by first employing a DQN to compute some candidates and then introducing a post-processing strategy to extract the target evidence and its label from the candidates. Experimental results show that the approach achieves state-of-the-art performance in terms of retrieval of precise evidences. Besides, to the best of our knowledge, it is the first attempt to employ DQN in the fact verification task.\nFuture work will incorporate external knowledge into our approach to improve the retrieval recall."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is supported by the Guangdong Province Science and Technology Plan projects (2017B010110011), the National Natural Science Foundation of China (No. 61876204, 61976232, and 51978675), the National Key R&D Program of China (No.2018YFC0830600), Guangdong Province Natural Science Foundation (No. 2018A030313086), All-China Federation of Returned Over-seas Chinese Research Project (No. 17BZQK216)."
    } ],
    "references" : [ {
      "title" : "An overview of deep reinforcement learning",
      "author" : [ "LiChun Cao", "ZhiMin." ],
      "venue" : "CACRE, pages 17:1– 17:9.",
      "citeRegEx" : "Cao and ZhiMin.,? 2019",
      "shortCiteRegEx" : "Cao and ZhiMin.",
      "year" : 2019
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL-HLT, pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Allennlp: A deep semantic natural language processing platform",
      "author" : [ "Matt Gardner", "Joel Grus", "Mark Neumann", "Oyvind Tafjord", "Pradeep Dasigi", "Nelson F. Liu", "Matthew E. Peters", "Michael Schmitz", "Luke Zettlemoyer." ],
      "venue" : "NLP-OSS Workshop, pages",
      "citeRegEx" : "Gardner et al\\.,? 2018",
      "shortCiteRegEx" : "Gardner et al\\.",
      "year" : 2018
    }, {
      "title" : "UKP-athene: Multi-sentence textual entailment for claim verification",
      "author" : [ "Andreas Hanselowski", "Hao Zhang", "Zile Li", "Daniil Sorokin", "Benjamin Schiller", "Claudia Schulz", "Iryna Gurevych." ],
      "venue" : "FEVER, pages 103–108.",
      "citeRegEx" : "Hanselowski et al\\.,? 2018",
      "shortCiteRegEx" : "Hanselowski et al\\.",
      "year" : 2018
    }, {
      "title" : "Several experiments on investigating pretraining and knowledgeenhanced models for natural language inference",
      "author" : [ "Tianda Li", "Xiaodan Zhu", "Quan Liu", "Qian Chen", "Zhigang Chen", "Si Wei." ],
      "venue" : "CoRR, abs/1904.12104.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Fine-grained fact verification with kernel graph attention network",
      "author" : [ "Zhenghao Liu", "Chenyan Xiong", "Maosong Sun", "Zhiyuan Liu." ],
      "venue" : "ACL, pages 7342–7351.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis." ],
      "venue" : "Nature, 518(7540):529–533.",
      "citeRegEx" : "King et al\\.,? 2015",
      "shortCiteRegEx" : "King et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep bi-directional long short-term memory neural networks for sentiment analysis of social data",
      "author" : [ "Ngoc-Khuong Nguyen", "Anh-Cuong Le", "Hong Thai Pham." ],
      "venue" : "IUKM, volume 9978, pages 255–268.",
      "citeRegEx" : "Nguyen et al\\.,? 2016",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "Combining fact extraction and verification with neural semantic matching networks",
      "author" : [ "Yixin Nie", "Haonan Chen", "Mohit Bansal." ],
      "venue" : "AAAI, pages 6859–6866.",
      "citeRegEx" : "Nie et al\\.,? 2019",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2019
    }, {
      "title" : "Prioritized experience replay",
      "author" : [ "Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Schaul et al\\.,? 2016",
      "shortCiteRegEx" : "Schaul et al\\.",
      "year" : 2016
    }, {
      "title" : "BERT for evidence retrieval and claim verification",
      "author" : [ "Amir Soleimani", "Christof Monz", "Marcel Worring." ],
      "venue" : "ECIR, volume 12036, pages 359–366.",
      "citeRegEx" : "Soleimani et al\\.,? 2020",
      "shortCiteRegEx" : "Soleimani et al\\.",
      "year" : 2020
    }, {
      "title" : "Hierarchical evidence set modeling for automated fact extraction and verification",
      "author" : [ "Shyam Subramanian", "Kyumin Lee." ],
      "venue" : "EMNLP, pages 7798– 7809.",
      "citeRegEx" : "Subramanian and Lee.,? 2020",
      "shortCiteRegEx" : "Subramanian and Lee.",
      "year" : 2020
    }, {
      "title" : "FEVER: a large-scale dataset for fact extraction and verification",
      "author" : [ "James Thorne", "Andreas Vlachos", "Christos Christodoulopoulos", "Arpit Mittal." ],
      "venue" : "NAACL-HLT, pages 809–819.",
      "citeRegEx" : "Thorne et al\\.,? 2018a",
      "shortCiteRegEx" : "Thorne et al\\.",
      "year" : 2018
    }, {
      "title" : "The fact extraction and VERification (FEVER) shared task",
      "author" : [ "James Thorne", "Andreas Vlachos", "Oana Cocarascu", "Christos Christodoulopoulos", "Arpit Mittal." ],
      "venue" : "FEVER, pages 1–9.",
      "citeRegEx" : "Thorne et al\\.,? 2018b",
      "shortCiteRegEx" : "Thorne et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "NeurIPS, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Robust reasoning over heterogeneous textual information for fact verification",
      "author" : [ "Yongyue Wang", "Chunhe Xia", "Chengxiang Si", "Beitong Yao", "Tianbo Wang." ],
      "venue" : "IEEE Access, 8:157140–157150.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush." ],
      "venue" : "EMNLP, pages 38–45.",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Coreferential reasoning learning for language representation",
      "author" : [ "Deming Ye", "Yankai Lin", "Jiaju Du", "Zhenghao Liu", "Peng Li", "Maosong Sun", "Zhiyuan Liu." ],
      "venue" : "EMNLP, pages 7170–7186.",
      "citeRegEx" : "Ye et al\\.,? 2020",
      "shortCiteRegEx" : "Ye et al\\.",
      "year" : 2020
    }, {
      "title" : "Twowingos: A twowing optimization strategy for evidential claim verification",
      "author" : [ "Wenpeng Yin", "Dan Roth." ],
      "venue" : "EMNLP, pages 105–114.",
      "citeRegEx" : "Yin and Roth.,? 2018",
      "shortCiteRegEx" : "Yin and Roth.",
      "year" : 2018
    }, {
      "title" : "UCL machine reading group: Four factor framework for fact finding (HexaF)",
      "author" : [ "Takuma Yoneda", "Jeff Mitchell", "Johannes Welbl", "Pontus Stenetorp", "Sebastian Riedel." ],
      "venue" : "FEVER, pages 97–102.",
      "citeRegEx" : "Yoneda et al\\.,? 2018",
      "shortCiteRegEx" : "Yoneda et al\\.",
      "year" : 2018
    }, {
      "title" : "Reasoning over semantic-level graph for fact checking",
      "author" : [ "Wanjun Zhong", "Jingjing Xu", "Duyu Tang", "Zenan Xu", "Nan Duan", "Ming Zhou", "Jiahai Wang", "Jian Yin." ],
      "venue" : "ACL, pages 6170–6180.",
      "citeRegEx" : "Zhong et al\\.,? 2020",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2020
    }, {
      "title" : "GEAR: graph-based evidence aggregating and reasoning for fact verification",
      "author" : [ "Jie Zhou", "Xu Han", "Cheng Yang", "Zhiyuan Liu", "Lifeng Wang", "Changcheng Li", "Maosong Sun." ],
      "venue" : "ACL, pages 892–901.",
      "citeRegEx" : "Zhou et al\\.,? 2019",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Most existing studies (Thorne et al., 2018b; Nie et al., 2019; Zhou et al., 2019; Liu et al., 2020; Zhong et al., 2020; Ye et al., 2020; Subramanian and Lee, 2020; Wang et al., 2020) formulate FV as a three-stage pipeline",
      "startOffset" : 22,
      "endOffset" : 182
    }, {
      "referenceID" : 10,
      "context" : "Most existing studies (Thorne et al., 2018b; Nie et al., 2019; Zhou et al., 2019; Liu et al., 2020; Zhong et al., 2020; Ye et al., 2020; Subramanian and Lee, 2020; Wang et al., 2020) formulate FV as a three-stage pipeline",
      "startOffset" : 22,
      "endOffset" : 182
    }, {
      "referenceID" : 23,
      "context" : "Most existing studies (Thorne et al., 2018b; Nie et al., 2019; Zhou et al., 2019; Liu et al., 2020; Zhong et al., 2020; Ye et al., 2020; Subramanian and Lee, 2020; Wang et al., 2020) formulate FV as a three-stage pipeline",
      "startOffset" : 22,
      "endOffset" : 182
    }, {
      "referenceID" : 6,
      "context" : "Most existing studies (Thorne et al., 2018b; Nie et al., 2019; Zhou et al., 2019; Liu et al., 2020; Zhong et al., 2020; Ye et al., 2020; Subramanian and Lee, 2020; Wang et al., 2020) formulate FV as a three-stage pipeline",
      "startOffset" : 22,
      "endOffset" : 182
    }, {
      "referenceID" : 22,
      "context" : "Most existing studies (Thorne et al., 2018b; Nie et al., 2019; Zhou et al., 2019; Liu et al., 2020; Zhong et al., 2020; Ye et al., 2020; Subramanian and Lee, 2020; Wang et al., 2020) formulate FV as a three-stage pipeline",
      "startOffset" : 22,
      "endOffset" : 182
    }, {
      "referenceID" : 19,
      "context" : "Most existing studies (Thorne et al., 2018b; Nie et al., 2019; Zhou et al., 2019; Liu et al., 2020; Zhong et al., 2020; Ye et al., 2020; Subramanian and Lee, 2020; Wang et al., 2020) formulate FV as a three-stage pipeline",
      "startOffset" : 22,
      "endOffset" : 182
    }, {
      "referenceID" : 13,
      "context" : "Most existing studies (Thorne et al., 2018b; Nie et al., 2019; Zhou et al., 2019; Liu et al., 2020; Zhong et al., 2020; Ye et al., 2020; Subramanian and Lee, 2020; Wang et al., 2020) formulate FV as a three-stage pipeline",
      "startOffset" : 22,
      "endOffset" : 182
    }, {
      "referenceID" : 17,
      "context" : "Most existing studies (Thorne et al., 2018b; Nie et al., 2019; Zhou et al., 2019; Liu et al., 2020; Zhong et al., 2020; Ye et al., 2020; Subramanian and Lee, 2020; Wang et al., 2020) formulate FV as a three-stage pipeline",
      "startOffset" : 22,
      "endOffset" : 182
    }, {
      "referenceID" : 20,
      "context" : "To the best of our knowledge, TwoWingOS (Yin and Roth, 2018) is the only method by now which does not follow the three-stage pipeline.",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 15,
      "context" : "For example, in the benchmark Fact Extraction and VERification (FEVER) dataset (Thorne et al., 2018b) the average number of sentences for each claim input to the sentence selection stage is 40, and an output evidence has up to 5 sentences.",
      "startOffset" : 79,
      "endOffset" : 101
    }, {
      "referenceID" : 15,
      "context" : "Our experimental results on FEVER (Thorne et al., 2018b) confirm that our DQN-based approach is effective in finding precise evidences.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 15,
      "context" : "0 shared task (Thorne et al., 2018b) aims to develop an automatic fact verification system to determine the truthfulness of a textual claim by extracting related evidences from Wikipedia.",
      "startOffset" : 14,
      "endOffset" : 36
    }, {
      "referenceID" : 15,
      "context" : "(2018a) has formalized this task, released a large-scale benchmark dataset FEVER (Thorne et al., 2018b), and designed the three-stage pipeline framework for FV, which consists of the document retrieval stage, the sentence selection stage and the claim verification stage.",
      "startOffset" : 81,
      "endOffset" : 103
    }, {
      "referenceID" : 6,
      "context" : "Most existing methods follow this framework and mainly focus on the last stage (Liu et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 97
    }, {
      "referenceID" : 3,
      "context" : "For the document retrieval stage, most methods reuse the document retrieval component of topperforming systems (Hanselowski et al., 2018; Yoneda et al., 2018; Nie et al., 2019).",
      "startOffset" : 111,
      "endOffset" : 176
    }, {
      "referenceID" : 21,
      "context" : "For the document retrieval stage, most methods reuse the document retrieval component of topperforming systems (Hanselowski et al., 2018; Yoneda et al., 2018; Nie et al., 2019).",
      "startOffset" : 111,
      "endOffset" : 176
    }, {
      "referenceID" : 10,
      "context" : "For the document retrieval stage, most methods reuse the document retrieval component of topperforming systems (Hanselowski et al., 2018; Yoneda et al., 2018; Nie et al., 2019).",
      "startOffset" : 111,
      "endOffset" : 176
    }, {
      "referenceID" : 15,
      "context" : "For the sentence selection stage, there are three approaches commonly used, including keyword matching, supervised classification, and sentence similarity scoring (Thorne et al., 2018b).",
      "startOffset" : 163,
      "endOffset" : 185
    }, {
      "referenceID" : 23,
      "context" : "For the claim verification stage, most recent studies formulate this task as a graph reasoning task (Zhou et al., 2019; Liu et al., 2020; Ye et al., 2020; Zhong et al., 2020; Subramanian and Lee, 2020; Wang et al., 2020).",
      "startOffset" : 100,
      "endOffset" : 220
    }, {
      "referenceID" : 6,
      "context" : "For the claim verification stage, most recent studies formulate this task as a graph reasoning task (Zhou et al., 2019; Liu et al., 2020; Ye et al., 2020; Zhong et al., 2020; Subramanian and Lee, 2020; Wang et al., 2020).",
      "startOffset" : 100,
      "endOffset" : 220
    }, {
      "referenceID" : 19,
      "context" : "For the claim verification stage, most recent studies formulate this task as a graph reasoning task (Zhou et al., 2019; Liu et al., 2020; Ye et al., 2020; Zhong et al., 2020; Subramanian and Lee, 2020; Wang et al., 2020).",
      "startOffset" : 100,
      "endOffset" : 220
    }, {
      "referenceID" : 22,
      "context" : "For the claim verification stage, most recent studies formulate this task as a graph reasoning task (Zhou et al., 2019; Liu et al., 2020; Ye et al., 2020; Zhong et al., 2020; Subramanian and Lee, 2020; Wang et al., 2020).",
      "startOffset" : 100,
      "endOffset" : 220
    }, {
      "referenceID" : 13,
      "context" : "For the claim verification stage, most recent studies formulate this task as a graph reasoning task (Zhou et al., 2019; Liu et al., 2020; Ye et al., 2020; Zhong et al., 2020; Subramanian and Lee, 2020; Wang et al., 2020).",
      "startOffset" : 100,
      "endOffset" : 220
    }, {
      "referenceID" : 17,
      "context" : "For the claim verification stage, most recent studies formulate this task as a graph reasoning task (Zhou et al., 2019; Liu et al., 2020; Ye et al., 2020; Zhong et al., 2020; Subramanian and Lee, 2020; Wang et al., 2020).",
      "startOffset" : 100,
      "endOffset" : 220
    }, {
      "referenceID" : 1,
      "context" : "Nowadays pre-trained language models like BERT (Devlin et al., 2019) have been widely used in claim verification (Li et al.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 4,
      "context" : ", 2019) have been widely used in claim verification (Li et al., 2019; Zhou et al., 2019; Soleimani et al., 2020).",
      "startOffset" : 52,
      "endOffset" : 112
    }, {
      "referenceID" : 23,
      "context" : ", 2019) have been widely used in claim verification (Li et al., 2019; Zhou et al., 2019; Soleimani et al., 2020).",
      "startOffset" : 52,
      "endOffset" : 112
    }, {
      "referenceID" : 12,
      "context" : ", 2019) have been widely used in claim verification (Li et al., 2019; Zhou et al., 2019; Soleimani et al., 2020).",
      "startOffset" : 52,
      "endOffset" : 112
    }, {
      "referenceID" : 5,
      "context" : "Following this way we employed RoBERTa (Liu et al., 2019), an enhanced version of BERT, as the sentence encoder in our DQN-based approach in experiments.",
      "startOffset" : 39,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "It typically uses the following Equation (1) derived from the Bellman equation (Cao and ZhiMin, 2019) to approximate the optimal Q-value function:",
      "startOffset" : 79,
      "endOffset" : 101
    }, {
      "referenceID" : 6,
      "context" : "Specifically, following KGAT (Liu et al., 2020), we first concatenate the claim c, the document title l, and a sentence s (resp.",
      "startOffset" : 29,
      "endOffset" : 47
    }, {
      "referenceID" : 9,
      "context" : "It is obvious that the sentences in an evidence are always contextual dependent, so we apply two different networks BiLSTM (Nguyen et al., 2016) and Transformer (Vaswani et al.",
      "startOffset" : 123,
      "endOffset" : 144
    }, {
      "referenceID" : 16,
      "context" : ", 2016) and Transformer (Vaswani et al., 2017) for comparison.",
      "startOffset" : 24,
      "endOffset" : 46
    }, {
      "referenceID" : 14,
      "context" : "Our experiments are conducted on the large-scale benchmark dataset FEVER (Thorne et al., 2018a), which consists of 185,455 annotated claims with a set of 5,416,537 Wikipedia documents from the June 2017 Wikipedia dump.",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 3,
      "context" : "The document retrieval stage is kept the same as previous work (Hanselowski et al., 2018; Zhou et al., 2019; Liu et al., 2020; Ye et al., 2020).",
      "startOffset" : 63,
      "endOffset" : 143
    }, {
      "referenceID" : 23,
      "context" : "The document retrieval stage is kept the same as previous work (Hanselowski et al., 2018; Zhou et al., 2019; Liu et al., 2020; Ye et al., 2020).",
      "startOffset" : 63,
      "endOffset" : 143
    }, {
      "referenceID" : 6,
      "context" : "The document retrieval stage is kept the same as previous work (Hanselowski et al., 2018; Zhou et al., 2019; Liu et al., 2020; Ye et al., 2020).",
      "startOffset" : 63,
      "endOffset" : 143
    }, {
      "referenceID" : 19,
      "context" : "The document retrieval stage is kept the same as previous work (Hanselowski et al., 2018; Zhou et al., 2019; Liu et al., 2020; Ye et al., 2020).",
      "startOffset" : 63,
      "endOffset" : 143
    }, {
      "referenceID" : 2,
      "context" : "Given a claim, the method first utilizes the constituency parser from AllenNLP (Gardner et al., 2018) to extract potential entities from the claim.",
      "startOffset" : 79,
      "endOffset" : 101
    }, {
      "referenceID" : 3,
      "context" : "The convinced articles are reserved (Hanselowski et al., 2018).",
      "startOffset" : 36,
      "endOffset" : 62
    }, {
      "referenceID" : 7,
      "context" : "We implement our DQN-based model with PyTorch and train it with the AdamW (Loshchilov and Hutter, 2019) optimizer while keeping the sentence encoding module frozen and inheriting the RoBERTa implementation from Wolf et al.",
      "startOffset" : 74,
      "endOffset" : 103
    }, {
      "referenceID" : 11,
      "context" : "Prioritized experience replay memory (Schaul et al., 2016) with a capacity of 10,000 is used to store transitions.",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 20,
      "context" : "We compare our method with the following baselines, including six methods that focus on claim verification and one joint method TwoWingOS (Yin and Roth, 2018).",
      "startOffset" : 138,
      "endOffset" : 158
    }, {
      "referenceID" : 23,
      "context" : "The six methods include: (1) GEAR (Zhou et al., 2019) uses two kinds of attentions to conduct reasoning and aggregation in a graph model; (2) KGAT (Liu et al.",
      "startOffset" : 34,
      "endOffset" : 53
    }, {
      "referenceID" : 6,
      "context" : ", 2019) uses two kinds of attentions to conduct reasoning and aggregation in a graph model; (2) KGAT (Liu et al., 2020) employes the Kernel Graph Attention Network to capture fine-grained information over evidences for more accurate claim verification; (3) DREAM (Zhong et al.",
      "startOffset" : 101,
      "endOffset" : 119
    }, {
      "referenceID" : 22,
      "context" : ", 2020) employes the Kernel Graph Attention Network to capture fine-grained information over evidences for more accurate claim verification; (3) DREAM (Zhong et al., 2020) introduces semantic structures for evidences obtained by semantic role labeling in claim verification; (4) CorefBERT (Ye et al.",
      "startOffset" : 151,
      "endOffset" : 171
    }, {
      "referenceID" : 19,
      "context" : ", 2020) introduces semantic structures for evidences obtained by semantic role labeling in claim verification; (4) CorefBERT (Ye et al., 2020) extends KGAT and can explicitly model co-reference relationship in context; (5) HESM (Subramanian and Lee, 2020) is a framework that can encode and attend the claim and evidence sets at different levels of hierarchy; (6) DGAT (Wang et al.",
      "startOffset" : 125,
      "endOffset" : 142
    }, {
      "referenceID" : 13,
      "context" : ", 2020) extends KGAT and can explicitly model co-reference relationship in context; (5) HESM (Subramanian and Lee, 2020) is a framework that can encode and attend the claim and evidence sets at different levels of hierarchy; (6) DGAT (Wang et al.",
      "startOffset" : 93,
      "endOffset" : 120
    }, {
      "referenceID" : 17,
      "context" : ", 2020) extends KGAT and can explicitly model co-reference relationship in context; (5) HESM (Subramanian and Lee, 2020) is a framework that can encode and attend the claim and evidence sets at different levels of hierarchy; (6) DGAT (Wang et al., 2020) is a double graph attention network that performs well in multi-domain datasets.",
      "startOffset" : 234,
      "endOffset" : 253
    }, {
      "referenceID" : 20,
      "context" : "The join method TwoWingOS (Yin and Roth, 2018) exploits a two-wing optimization strategy that optimizes sentence selection and claim verification in a jointly supervised training scheme.",
      "startOffset" : 26,
      "endOffset" : 46
    } ],
    "year" : 2021,
    "abstractText" : "Computing precise evidences, namely minimal sets of sentences that support or refute a given claim, rather than larger evidences is crucial in fact verification (FV), since larger evidences may contain conflicting pieces some of which support the claim while the other refute, thereby misleading FV. Despite being important, precise evidences are rarely studied by existing methods for FV. It is challenging to find precise evidences due to a large search space with lots of local optimums. Inspired by the strong exploration ability of the deep Q-learning network (DQN), we propose a DQN-based approach to retrieval of precise evidences. In addition, to tackle the label bias on Q-values computed by DQN, we design a postprocessing strategy which seeks best thresholds for determining the true labels of computed evidences. Experimental results confirm the effectiveness of DQN in computing precise evidences and demonstrate improvements in achieving accurate claim verification.1",
    "creator" : "LaTeX with hyperref package"
  }
}