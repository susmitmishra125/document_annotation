{
  "name" : "2021.acl-long.204.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders",
    "authors" : [ "Chen Xu", "Bojie Hu", "Yanyang Li", "Yuhao Zhang", "Shen Huang", "Qi Ju", "Tong Xiao", "Jingbo Zhu" ],
    "emails" : [ "xuchenneu@outlook.com,", "blamedrlee@outlook.com,", "yoohaozhang@outlook.com,", "bojiehu@tencent.com,", "springhuang@tencent.com,", "damonju@tencent.com,", "xiaotong@mail.neu.edu.cn", "zhujingbo@mail.neu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2619–2630\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2619"
    }, {
      "heading" : "1 Introduction",
      "text" : "End-to-end Speech Translation (E2E ST) has become popular recently for its ability to free designers from cascading different systems and shorten\n∗Corresponding author 1The source code is available at https://github.com/xuchen\nneu/SATE\nthe pipeline of translation (Duong et al., 2016; Berard et al., 2016; Weiss et al., 2017). Promising results on small-scale tasks are generally favorable. However, speech-to-translation paired data is scarce. Researchers typically use pre-trained Automatic Speech Recognition (ASR) and Machine Translation (MT) models to boost ST systems (Berard et al., 2018). For example, one can initialize the ST encoder using a large-scale ASR model (Bansal et al., 2019). But we note that, despite significant development effort, our end-to-end ST system with pre-trained models was not able to outperform the cascaded ST counterpart when the ASR and MT data size was orders of magnitude larger than that of ST (see Table 1).\nIn this paper, we explore reasons why pretraining has been challenging in ST, and how pretrained ASR and MT models might be used together to improve ST. We find that the ST encoder plays both roles of acoustic encoding and textual encoding. This makes it problematic to view an ST encoder as either an individual ASR encoder or an individual MT encoder. More specifically, there are two problems.\n• Modeling deficiency: the MT encoder tries to capture long-distance dependency structures of language, but the ASR encoder focuses more on local dependencies in the input sequence. Since the ST encoder is initialized by the pre-trained ASR encoder (Berard et al., 2018), it fails to model large contexts in the utterance. But a large scope of representation learning is necessary for translation (Yang et al., 2018).\n• Representation inconsistency: on the decoder side of ST, the MT decoder is in general used to initialize the model. The assumption here is that the upstream component is an MT-like encoder, whereas the ST encoder actually behaves more like an ASR encoder.\nWe address these problems by marrying the world of ASR encoding with the world of MT encoding. We propose a Stacked Acoustic-andTextual Encoding (SATE) method to cascade the ASR encoder and the MT encoder. It first reads and processes the sequence of acoustic features as a usual ASR encoder. Then an adaptor module passes the acoustic encoding output to an MT encoder with two principles: informative and adaptive. In this way, pre-trained ASR and MT encoders can work for what we would originally design them, and the incorporation of pre-trained models into ST is more straightforward. In addition, we develop a multi-teacher knowledge distillation method to robustly train the ST encoder and preserve the pretrained knowledge during fine-tuning (Yang et al., 2020).\nWe test our method in a Transformer-based endto-end ST system. Experimental results on the LibriSpeech En-Fr and MuST-C En-De speech translation benchmarks show that it achieves the stateof-the-art performance of 18.3 and 25.2 BLEU points. Under a more challenging setup, where the large-scale ASR and MT data is available, SATE achieves comparable or even better performance than the cascaded ST counterpart. We believe that we are the first to present an end-to-end system that can beat the strong cascaded system in unrestricted speech translation tasks."
    }, {
      "heading" : "2 Related Work",
      "text" : "Speech translation aims at learning models that can predict, given some speech in the source language, the translation into the target language. The earliest\nof these models were cascaded: they treated ST as a pipeline of running an ASR system and an MT system sequentially (Ney, 1999; Mathias and Byrne, 2006; Schultz et al., 2004). This allows the use of off-the-shelf models, and was (and is) popular in practical ST systems. However, these systems were sensitive to the errors introduced by different component systems and the high latency of the long pipeline.\nAs another stream in the ST area, end-to-end methods have been promising recently (Berard et al., 2016; Weiss et al., 2017; Berard et al., 2018). The rise of end-to-end ST can be traced back to the success of deep neural models (Duong et al., 2016). But, unlike other well-defined tasks in deep learning, annotated speech-to-translation data is scarce, which prevents well-trained ST models. A simple solution to this issue is data augmentation (Pino et al., 2019, 2020). This method is model-free but generating large-scale synthetic data is time consuming. As an alternative, researchers used multi-task learning (MTL) to robustly train the ST model so that it could benefit from additional guide signals (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Berard et al., 2018; Sperber et al., 2019; Dong et al., 2021). Generally, MTL requires a careful design of the loss functions and more complicated architectures.\nIn a similar way, more recent work pre-trains different components of the ST system, and consolidates them into one. For example, one can initialize the encoder with an ASR model, and initialize the decoder with the target-language side of an MT model (Berard et al., 2018; Bansal et al., 2019; Stoian et al., 2020). More sophisticated methods include better training and fine-tuning (Wang et al., 2020a,b), the shrink mechanism (Liu et al., 2020), the adversarial regularizer (Alinejad and Sarkar, 2020), and etc. Although pre-trained models have quickly become dominant in many NLP tasks, they are still found to underperform the cascaded model in ST. This motivates us to explore the reasons why this happens and methods to solve the problems accordingly."
    }, {
      "heading" : "3 Why is ST Encoding Difficult?",
      "text" : "Following previous work in end-to-end models (Berard et al., 2016; Weiss et al., 2017), we envision an encoding-decoding process in which an input sequence is encoded into a representation vector, and the vector is then decoded into an output sequence.\nIn such a scenario, all end-to-end ST, ASR and MT systems can be viewed as instances of the same architecture. Then, components of these systems can be pre-trained and re-used across them.\nAn underlying assumption here is that the ST encoder is doing something quite similar to what the MT (or ASR) encoder is doing. However, Sperber et al. (2018) find that the ASR model benefits from a small attention window, which is inconsistent with the MT model (Yang et al., 2018). To verify this, we compare the behavior of ST, ASR and MT encoders. We choose Transformer as the base architecture (Vaswani et al., 2017) and run experiments on the MuST-C En-De corpus. We report the results on the MuST-C En-De tst-COMMON test data. For stronger systems, we use Connectionist Temporal Classification (CTC) (Graves et al., 2006) as the auxiliary loss on the encoders when we train the ASR and ST systems (Watanabe et al., 2017; Karita et al., 2019; Bahar et al., 2019). The CTC loss forces the encoders to learn alignments between speech and transcription. It is necessary for the state-of-the-art performance (Watanabe et al., 2018).\nHere we define the localness of a word as the sum of the attention weights to the surrounding words (or features) within a fixed small window2. The window size is 10% of the sequence length. Figure 1(a) shows the localness of the attention weights for different layers of the encoders. We see that the ST and ASR encoders prefer local attention which indicates a kind of short-distance dependencies in processing acoustics feature sequences. Whereas the MT encoder generates a\n2Here we treat the attention weight of Transformer as a distribution over all positions.\nmore global distribution of attention weights for word sequences, especially when we stack more layers. This result arises a new question: Is local attention sufficient for speech translation?\nThen, we design another experiment to examine if the high localness in attention weights of the ASR and ST encoders is due to the bias imposed by CTC. In Figure 1(b), we use the CTC loss in the intermediate layer and show the average localness of the layers above or below CTC. The CTC loss demonstrates strong preference for locally attentive models. The upper-level layers act more like an MT encoder, that is, the layers with no CTC loss generates more global distributions. Taking this further, Figure 1(c) demonstrates a slightly higher BLEU score when we free more upper-level layers from the guide of CTC. Meanwhile, the word error rate (WER) increases because only lower parts of the model are learned in a standard manner of ASR.\nNow we have some hints: the ST encoder is not a simple substitution of the ASR encoder or the MT encoder. Rather, they are complementary to each other, that is, we need the ASR encoder to deal with the acoustic input, and the MT encoder to generate the representation vector that can work better with the decoder."
    }, {
      "heading" : "4 The Method",
      "text" : "In speech translation, we want the encoder to represent the input speech to some sort of decoderfriendly representations. We also want the encoder to be “natural” for pre-training. In the following, we describe, Stacked Acoustics-and-Textual Encoding (SATE), a new ST encoding method to meet these requirements, and improvements of it."
    }, {
      "heading" : "4.1 Stacked Acoustic-and-Textual Encoding",
      "text" : "Unlike previous work, the SATE method does not rely on a single encoder to receive the signal from both the CTC loss and the feedback of the decoder. Instead, it is composed of two encoders: the first does exactly the same thing as the ASR encoder (call it acoustic encoder), and the other generates a higher-level globally-attentive representation on top of the acoustic encoder (call it textual encoder).\nSee Figure 2 for the architecture of SATE. The acoustic encoder is trained by CTC in addition to the supervision signal from the translation loss. Let (x, ys, yt) be an ST training sample, where x is the input feature sequence of the speech, ys is the transcription of x, and yt is the translation in the target language. We define the output of the acoustic encoder as:\nhs = Es(x) (1)\nwhere Es(·) is the encoding function. Then, we add a Softmax layer on hs to predict the CTC label path π = (π1, · · · , πT ), where T is the length of the input sequence. The probability of path P(π|hs) is the product of the probability P(πt|hst ) at every time t based on conditionally independent assumption:\nP(π|hs) ≈ T∏ t P(πt|hst ) (2)\nCTC works by summing over the probability of all possible alignment paths Φ(ys) between x and ys , as follows:\nPCTC(ys|hs) = ∑\nπ∈Φ(ys)\nP(π|hs) (3)\nThen, the CTC loss is defined as:\nLCTC = − log PCTC(ys|hs; θCTC) (4)\nwhere θCTC is the model parameters of the acoustic encoder and the CTC output layer.\nThe acoustic encoder is followed by an adaptor. It receives hs and P (π|hs), and produces a new representation required by the textual encoder. Let A(·, ·) be the adaptor module. Its output is defined as:\nĥs = A(hs,P(π|hs)) (5)\nWe leave the design of the adaptor to Section 4.2. Furthermore, we stack the textual encoder on the adaptor. The output ht is defined as:\nht = Et(ĥs) (6)\nwhere Et(·) is the textual encoder. ht is fed into the decoder for computing the translation probability PTrans(yt|ht), as in standard MT systems. We define the translation loss as:\nLTrans = − log PTrans(yt|ht; θST) (7)\nwhere θST is all model parameters except for the CTC output layer.\nFinally, we interpolate LCTC and LTrans (with coefficient α) for the loss of the entire model:\nL = α · LCTC + (1− α) · LTrans (8)\nSince the textual encoder works for the decoder only, it is trained as an MT encoder. In this way, the acoustic and textual encoders can do what we would originally expect them to do: the acoustic encoder deals with the acoustic input (i.e., ASR encoding), and the textual encoder generates a representation for translation (i.e., MT encoding). Also, SATE is friendly to pre-training. One can simply use an ASR encoder as the acoustic encoder, and use an MT encoder as the textual encoder. Note that SATE is in general a cascaded model, in response to the pioneering work in ST (Ney, 1999). It can be seen as cascading the ASR and MT systems in an end-to-end fashion."
    }, {
      "heading" : "4.2 The Adaptor",
      "text" : "Now we turn to the design of the adaptor. Note that the pre-trained MT encoder assumes that the input is a word embedding sequence. Simply stacking the MT encoder and the ASR encoder obviously\ndoes not work well. For this reason, the adaptor fits the output of the ASR encoder (i.e., the acoustic encoder) to what an MT encoder would like to see. We follow two principles in designing the adaptor: adaptive and informative.\nWe need an adaptive representation to make the input of the textual encoder similar to that of the MT encoder. To this end, we generate the soft contextual representation that shares the same latent space with the embedding layer of the MT encoder.\nAs shown in Eq. (2), the CTC output P(πt|hst ) indicates the alignment probability over the vocabulary at time t. Instead of replacing the representation by the embedding of the most-likely token (Liu et al., 2020), we employ a soft token which is the expectation of the embedding over the distribution from CTC. Let W e be the embedding matrix of the textual encoder, we define the soft representation hssoft as:\nhssoft = P(π|hs) ·W e (9)\nAlso, an informative representation should contain information in the original input (Peters et al., 2018). The output acoustic representation of the ASR encoder generally involves paralinguistic information, such as emotion, accent, and emphasis. They are not expressed in the form of text explicitly but might be helpful for translation. For example, the generation of the declarative or exclamatory sentences depends on the emotions of the speakers.\nWe introduce a single-layer neural network to learn to map the acoustic representation to the latent space of the textual encoder, which preserves the acoustic information:\nhsmap = ReLU(W map · hs + bmap) (10)\nwhereWmap and bmap are the trainable parameters. The final output of the adaptor is defined to be:\nA(hs, P (π|hs)) = λ · hsmap + (1− λ) · hssoft (11)\nwhere λ is the weight of hsmap and set to 0.5 by default. Figure 3 shows the architecture of the adaptor.\nNote that, in the adaptor, we do not change the sequence length for textual encoding because such a way is simple for implementation and shows satisfactory results in our experiments. Although there is a length inconsistency issue, the sequence representation of the speech should be similar with the\ncorrespond transcription. Shrinking the sequence simply results in information incompleteness. We will investigate this issue in the future."
    }, {
      "heading" : "4.3 Multi-teacher Knowledge Distillation",
      "text" : "Another improvement here is that we develop a multi-teacher knowledge distillation (MTKD) method to preserve the pre-trained knowledge during fine-tuning (Hinton et al., 2015).\nThe ST model mimics the teacher distribution by minimizing the cross-entropy loss between the teacher and student (Liu et al., 2019). For a training sample (x, ys, yt), we define two loss functions:\nLKD CTC = − T∑\nm=1 |V |∑ k=1 Q(πm = vk|x; θASR)\n× log P(πm = vk|x; θCTC) (12)\nLKD Trans = − |yt|∑ n=1 |V |∑ k=1 Q(ytn = vk|ys; θMT)\n× log P(ytn = vk|x; θST) (13)\nwhere vk is the word indexed by k and V is the vocabulary shared among the ST, ASR, and MT models. Q(·|·) is the teacher distribution and P(·|·) is the student distribution. θASR, θCTC, θMT and θST are the model parameters.\nWe can rewrite Eq. (8) to obtain a new loss: L = α · ( β · LCTC + (1− β) · LKD CTC ) +(1− α) ·( γ · LTrans + (1− γ) · LKD Trans ) (14)\nwhere both β and γ are the hyper-parameters that balance the preference between the teacher distribution and the ground truth."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Datasets and Preprocessing",
      "text" : "We consider restricted and unrestricted settings on speech translation tasks. We run experiments on the LibriSpeech English-French (En-Fr) (Kocabiyikoglu et al., 2018) and MuST-C EnglishGerman (En-De) (Gangi et al., 2019) corpora, which correspond to the low-resource and highresource datasets respectively. Available ASR and MT data is only from the ST data under the restricted setting. For comparison in practical scenarios, the unrestricted setting allows the additional data for ASR and MT models.\nLibriSpeech En-Fr Followed previous work, we use the clean speech translation training set of 100 hours, including 45K utterances and doubled translations of Google Translate. We select the model on the dev set (1,071 utterances) and report results on the test set (2,048 utterances).\nMuST-C En-De MuST-C is a multilingual speech translation corpus extracted from the TED talks. We run the experiments on the English-German speech translation dataset of 400 hours speech with 230K utterances. We select the model on the dev set (1,408 utterances) and report results on the tstCOMMON set (2,641 utterances).\nUnrestricted Setting We use the additional ASR and MT data for pre-training. The 960 hours LibriSpeech ASR corpus is used for the English ASR model. We extract 10M sentences pairs from the WMT14 English-French and 18M sentence pairs from the Opensubtitle20183 English-German translation datasets.\nPreprocessing Followed the preprocessing recipes of ESPnet (Inaguma et al., 2020), we remove the utterances of more than 3,000 frames and augment speech data by speed perturbation with factors of 0.9, 1.0, and 1.1. The 80-channel log-mel filterbank coefficients with 3-dimensional pitch features are extracted for speech data. We use the lower-cased transcriptions without punctuations. The text is tokenized using the scripts of Moses (Koehn et al., 2007). We learn Byte-Pair Encoding (Sennrich et al., 2016) subword segmentation with 10,000 merge operations based on a shared source and target vocabulary for all datasets.\n3http://opus.nlpl.eu/OpenSubtitles-v2018.php"
    }, {
      "heading" : "5.2 Model Settings",
      "text" : "All experiments are implemented based on the ESPnet toolkit4. We use the Adam optimizer with β1 = 0.9, β2 = 0.997 and adopt the default learning schedule in ESPnet. We apply dropout with a rate of 0.1 and label smoothing ls = 0.1 for regularization.\nFor reducing the computational cost, the input speech features are processed by two convolutional layers, which have a stride of 2 × 2 and downsample the sequence by a factor of 4 (Weiss et al., 2017). The encoder consists of 12 layers for both the ASR and vanilla ST models, and 6 layers for the MT model. The encoder of SATE includes an acoustic encoder of 12 layers and a textual encoder of 6 layers. The decoder consists of 6 layers for all models. The weight of CTC objective α for multitask learning is set to 0.3 for all ASR and ST models. The coefficients β and γ are set to 0.5 in Eq. (14) for the MTKD method.\nUnder the restricted setting, we employ the Transformer architecture, where each layer comprises 256 hidden units, 4 attention heads, and 2048 feed-forward size. For the unrestricted setting, we use the superior architecture Conformer (Gulati et al., 2020) on the ASR and ST tasks and widen the model by increasing the hidden size to 512 and attention heads to 8. The ASR5 and MT models pre-train with the additional data and fine-tune the model parameters with the task-specific data.\nDuring inference, we average the model parameters on the best 5 checkpoints based on the performance of the development set. We use beam search with a beam size of 4 for all models. Different from previous work, we report the case-sensitive SacreBLEU6 (Post, 2018) for future standardization comparison across papers."
    }, {
      "heading" : "5.3 Results",
      "text" : "Results on MuST-C En-De Table 2 summaries the experimental results on the MuST-C En-De task. Under the restricted setting, the cascaded ST model translates the output of the ASR model, which degrades the performance compared with the MT model that translates from the reference transcription. The performance of the E2E ST baseline with pre-training is only slightly lower than the cascaded counterpart. SATE outperforms the baseline\n4https://github.com/espnet/espnet 5We use the pre-trained ASR model offered by ESPnet. 6BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a\n+version.1.4.14\nmodel significantly. This demonstrates the superiority of stacked acoustic and textual encoding for the speech translation task. Incorporating the pretrained ASR and MT models into SATE releases the encoding burden of the model and achieves a remarkable improvement. The MTKD method provides a strong supervised signal and forces the model to preserve the pre-trained knowledge. Furthermore, we utilize the SpecAugment (Park et al., 2019) which is applied in the input speech features for better generalization and robustness7. It yields a remarkable improvement of 1.9 BLEU points over the cascaded baseline and achieves a new state-ofthe-art performance.\nUnder the unrestricted setting, the large-scale ASR and MT data is available, whereas the ST data is scarce. This leads to the cascaded method outperforms the vanilla E2E method with a huge margin of 4.5 BLEU points. The pre-training only slightly closes the gap due to the modeling deficiency and representation inconsistency. SATE incorporates the pre-trained models fully, which achieves a significant improvement of 3.7 BLEU points. With the MTKD and SpecAugment methods, we achieve a comparable performance of 28.1 BLEU points. To our knowledge, we are the first to develop an end-to-end ST system that achieves comparable performance with the cascaded counterpart when large-scale ASR and MT data is available. Results on LibriSpeech En-Fr Table 3 summaries the experimental results on the LibriSpeech En-Fr task. Different from the MuST-C corpus,\n7It is a fair comparison because the ASR model in the cascaded ST system also trains with the SpecAugment.\nit is of small magnitude with clean speech data. This results in that the performance of the vanilla E2E baseline is even better than the cascaded counterpart under the restricted setting. Furthermore, pre-training helps the model achieve an improvement of 0.8 BLEU points over the cascaded baseline. More interestingly, SATE without pre-training outperforms the above methods significantly, even achieves a slight improvement than the MT model. A possible reason is that the diverse acoustic representation is fed to the textual encoder, which improves the robustness of the model. This demonstrates the superiority of our method.\nCombining our proposed methods yields a substantial improvement of 2.0 BLEU points over the cascaded baseline. It is a new state-of-the-art result of 18.3 BLEU points. Also, we outperform the cascaded counterpart by 0.2 BLEU points on the unrestricted task."
    }, {
      "heading" : "6 Analysis",
      "text" : ""
    }, {
      "heading" : "6.1 Model Performance vs. Speedup",
      "text" : "In Table 4, we summarize the performance and inference speedup based on the real time factor (RTF). The vanilla E2E ST model yields an inference speedup of 1.91× than the cascaded counterpart and demonstrates the low latency of the end-to-end methods. We increase the encoder layers for comparison with SATE under the similar model parameters. However, there is a remarkable gap of 0.5 or 0.6 BLEU points, with or without pre-training.\nOur method not only improves the performance of 1.9 BLEU points but also reaches up to 1.69× speedup than the cascaded baseline. This encourages the application of the end-to-end ST model in practical scenarios."
    }, {
      "heading" : "6.2 Effects of Pre-trained Modules",
      "text" : "The effects of the pre-trained modules are shown in Table 5. The model performance drops significantly without the pre-trained ASR encoder, especially on the MuST-C corpus that contains noisy speech. The model parameters of pre-trained MT model are updated for adapting the output representation of the random initialized acoustic encoder. This results in the catastrophic forgetting problem (Goodfellow et al., 2015). The effect of the pretrained MT model is more remarkable on the LibriSpeech corpus due to the modeling burden on the translation. The benefit of the pre-trained MT decoder is larger than the MT encoder. This is contrary to the previous conclusions that the MT encoder helps the performance significantly (Li et al., 2020). A possible reason is that the pre-trained\nASR encoder provides a rich representation and acts as part of the MT encoder, this leads to lower performance degradation when the textual encoder trains from scratch.\nEach pre-trained module has a great effect on the final performance. With the complete integration of the pre-trained modules, the model parameters are updated slightly, which preserves the pre-trained knowledge."
    }, {
      "heading" : "6.3 Effects of The Adaptor",
      "text" : "We show the effects of the adaptor in Table 6. The straight connection which omits the representation inconsistency issue results in the lower benefit of pre-training. Although the soft representation aims at generating the adaptive representation, there is no obvious improvement on the MuST-C corpus. A possible reason is that the noisy speech inputs produce the misalignment probabilities, which disturbs the textual encoding. The mapping method achieves a slight improvement by transforming the acoustic representation to the textual representation. Fusing the soft and mapping representation enriches the information and avoids the representation inconsistency issue, which achieves the best performances."
    }, {
      "heading" : "6.4 Impact on Localness",
      "text" : "We show the encoder localness of the vanilla E2E ST model and SATE model with pre-training in\nFigure 4. As mentioned above, the vanilla ST model inherits the preference of ASR, which focuses on short-distance dependencies. SATE initializes with the pre-trained ASR and MT encoders, which stacks acoustic and textual encoding. The complementary behaviors of the pre-trained models benefit the translation, that is, the lower layers act like an ASR encoder while the upper layers capture global representation like an MT encoder."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we investigate the difficulty of speech translation and shed light on the reasons why pretraining has been challenging in ST. This inspires us to propose a Stacked Acoustic-and-Textual Encoding method, which is straightforward to incorporate the pre-trained models into ST. We also introduce an adaptor module and a multi-teacher knowledge distillation method for bridging the gap between pre-training and fine-tuning.\nResults on the LibriSpeech and MuST-C corpora demonstrate the superiority of our method. Furthermore, we achieve comparable or even better performance than the cascaded counterpart when large-scale ASR and MT data is available."
    }, {
      "heading" : "8 Acknowledgement",
      "text" : "This work was supported in part by the National Science Foundation of China (Nos. 61876035 and 61732005), the National Key R&D Program of China (No. 2019QY1801), and the Ministry of Science and Technology of the PRC (Nos. 2019YFF0303002 and 2020AAA0107900). The authors would like to thank anonymous reviewers for their comments."
    } ],
    "references" : [ {
      "title" : "Effectively pretraining a speech translation decoder with machine translation data",
      "author" : [ "Ashkan Alinejad", "Anoop Sarkar." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020,",
      "citeRegEx" : "Alinejad and Sarkar.,? 2020",
      "shortCiteRegEx" : "Alinejad and Sarkar.",
      "year" : 2020
    }, {
      "title" : "Tied multitask learning for neural speech translation",
      "author" : [ "Antonios Anastasopoulos", "David Chiang." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Anastasopoulos and Chiang.,? 2018",
      "shortCiteRegEx" : "Anastasopoulos and Chiang.",
      "year" : 2018
    }, {
      "title" : "A comparative study on end-to-end speech to text translation",
      "author" : [ "Parnia Bahar", "Tobias Bieschke", "Hermann Ney." ],
      "venue" : "IEEE Automatic Speech Recognition and Understanding Workshop, ASRU 2019, Singapore, December 14-18,",
      "citeRegEx" : "Bahar et al\\.,? 2019",
      "shortCiteRegEx" : "Bahar et al\\.",
      "year" : 2019
    }, {
      "title" : "Pretraining on high-resource speech recognition improves low-resource speech-to-text translation",
      "author" : [ "Sameer Bansal", "Herman Kamper", "Karen Livescu", "Adam Lopez", "Sharon Goldwater." ],
      "venue" : "Proceedings of the 2019 Conference of the",
      "citeRegEx" : "Bansal et al\\.,? 2019",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2019
    }, {
      "title" : "Endto-end automatic speech translation of audiobooks",
      "author" : [ "Alexandre Berard", "Laurent Besacier", "Ali Can Kocabiyikoglu", "Olivier Pietquin." ],
      "venue" : "2018 IEEE International Conference on Acoustics, Speech and Signal Processing,",
      "citeRegEx" : "Berard et al\\.,? 2018",
      "shortCiteRegEx" : "Berard et al\\.",
      "year" : 2018
    }, {
      "title" : "Listen and translate: A proof of concept for end-to-end speech-to-text translation",
      "author" : [ "Alexandre Berard", "Olivier Pietquin", "Christophe Servan", "Laurent Besacier." ],
      "venue" : "CoRR, abs/1612.01744.",
      "citeRegEx" : "Berard et al\\.,? 2016",
      "shortCiteRegEx" : "Berard et al\\.",
      "year" : 2016
    }, {
      "title" : "Consecutive decoding for speech-to-text translation",
      "author" : [ "Qianqian Dong", "Mingxuan Wang", "Hao Zhou", "Shuang Xu", "Bo Xu", "Lei Li." ],
      "venue" : "The Thirty-fifth AAAI Conference on Artificial Intelligence, AAAI.",
      "citeRegEx" : "Dong et al\\.,? 2021",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2021
    }, {
      "title" : "An attentional model for speech translation without transcription",
      "author" : [ "Long Duong", "Antonios Anastasopoulos", "David Chiang", "Steven Bird", "Trevor Cohn." ],
      "venue" : "NAACL HLT 2016, The 2016 Conference of the North American Chapter of",
      "citeRegEx" : "Duong et al\\.,? 2016",
      "shortCiteRegEx" : "Duong et al\\.",
      "year" : 2016
    }, {
      "title" : "Must-c: a multilingual speech translation corpus",
      "author" : [ "Mattia Antonino Di Gangi", "Roldano Cattoni", "Luisa Bentivogli", "Matteo Negri", "Marco Turchi." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the",
      "citeRegEx" : "Gangi et al\\.,? 2019",
      "shortCiteRegEx" : "Gangi et al\\.",
      "year" : 2019
    }, {
      "title" : "An empirical investigation of catastrophic forgetting in gradient-based neural networks",
      "author" : [ "Ian J. Goodfellow", "Mehdi Mirza", "Da Xiao", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2015
    }, {
      "title" : "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "author" : [ "Alex Graves", "Santiago Fernández", "Faustino J. Gomez", "Jürgen Schmidhuber." ],
      "venue" : "Machine Learning, Proceedings",
      "citeRegEx" : "Graves et al\\.,? 2006",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2006
    }, {
      "title" : "Conformer: Convolution-augmented transformer for speech",
      "author" : [ "Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang" ],
      "venue" : null,
      "citeRegEx" : "Gulati et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Gulati et al\\.",
      "year" : 2020
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey E. Hinton", "Oriol Vinyals", "Jeffrey Dean." ],
      "venue" : "CoRR, abs/1503.02531.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Espnet-st: All-inone speech translation toolkit",
      "author" : [ "Hirofumi Inaguma", "Shun Kiyono", "Kevin Duh", "Shigeki Karita", "Nelson Yalta", "Tomoki Hayashi", "Shinji Watanabe." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association",
      "citeRegEx" : "Inaguma et al\\.,? 2020",
      "shortCiteRegEx" : "Inaguma et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving transformer-based end-to-end speech recognition with connectionist temporal classification and language model integration",
      "author" : [ "Tomohiro Nakatani" ],
      "venue" : "In Interspeech 2019,",
      "citeRegEx" : "Nakatani.,? \\Q2019\\E",
      "shortCiteRegEx" : "Nakatani.",
      "year" : 2019
    }, {
      "title" : "Augmenting librispeech with french translations: A multimodal corpus for direct speech translation evaluation",
      "author" : [ "Ali Can Kocabiyikoglu", "Laurent Besacier", "Olivier Kraif." ],
      "venue" : "Proceedings of the Eleventh International Con-",
      "citeRegEx" : "Kocabiyikoglu et al\\.,? 2018",
      "shortCiteRegEx" : "Kocabiyikoglu et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning light-weight translation models from deep transformer",
      "author" : [ "Bei Li", "Ziyang Wang", "Hui Liu", "Quan Du", "Tong Xiao", "Chunliang Zhang", "Jingbo Zhu." ],
      "venue" : "CoRR, abs/2012.13866.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "End-to-end speech translation with knowledge distillation",
      "author" : [ "Yuchen Liu", "Hao Xiong", "Jiajun Zhang", "Zhongjun He", "Hua Wu", "Haifeng Wang", "Chengqing Zong." ],
      "venue" : "Interspeech 2019, 20th Annual Conference of the International",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Bridging the modality gap for speech-to-text translation",
      "author" : [ "Yuchen Liu", "Junnan Zhu", "Jiajun Zhang", "Chengqing Zong." ],
      "venue" : "CoRR, abs/2010.14920.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Statistical phrase-based speech translation",
      "author" : [ "Lambert Mathias", "William Byrne." ],
      "venue" : "2006 IEEE International Conference on Acoustics Speech and Signal Processing, ICASSP 2006,",
      "citeRegEx" : "Mathias and Byrne.,? 2006",
      "shortCiteRegEx" : "Mathias and Byrne.",
      "year" : 2006
    }, {
      "title" : "Speech translation: coupling of recognition and translation",
      "author" : [ "Hermann Ney." ],
      "venue" : "Proceedings of the 1999 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP ’99, Phoenix, Arizona, USA, March 15-",
      "citeRegEx" : "Ney.,? 1999",
      "shortCiteRegEx" : "Ney.",
      "year" : 1999
    }, {
      "title" : "Specaugment: A simple data augmentation method for automatic speech recognition",
      "author" : [ "Daniel S. Park", "William Chan", "Yu Zhang", "ChungCheng Chiu", "Barret Zoph", "Ekin D. Cubuk", "Quoc V. Le." ],
      "venue" : "Interspeech 2019, 20th Annual",
      "citeRegEx" : "Park et al\\.,? 2019",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Harnessing indirect training data for end-to-end automatic speech translation: Tricks of the trade",
      "author" : [ "Juan Pino", "Liezl Puzon", "Jiatao Gu", "Xutai Ma", "Arya D McCarthy", "Deepak Gopinath." ],
      "venue" : "arXiv preprint arXiv:1909.06515.",
      "citeRegEx" : "Pino et al\\.,? 2019",
      "shortCiteRegEx" : "Pino et al\\.",
      "year" : 2019
    }, {
      "title" : "Selftraining for end-to-end speech translation",
      "author" : [ "Juan Pino", "Qiantong Xu", "Xutai Ma", "Mohammad Javad Dousti", "Yun Tang." ],
      "venue" : "Interspeech 2020, 21st Annual Conference of the International Speech Communication Asso-",
      "citeRegEx" : "Pino et al\\.,? 2020",
      "shortCiteRegEx" : "Pino et al\\.",
      "year" : 2020
    }, {
      "title" : "A call for clarity in reporting BLEU scores",
      "author" : [ "Matt Post." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, WMT 2018, Belgium, Brussels, October 31 - November 1, 2018, pages 186–191. Association",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "Using word latice information for a tighter coupling in speech translation systems",
      "author" : [ "Tanja Schultz", "Szu-Chen Jou", "Stephan Vogel", "Shirin Saleem." ],
      "venue" : "Eighth International Conference on Spoken Language Processing.",
      "citeRegEx" : "Schultz et al\\.,? 2004",
      "shortCiteRegEx" : "Schultz et al\\.",
      "year" : 2004
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Attention-passing models for robust and data-efficient end-to-end speech translation",
      "author" : [ "Matthias Sperber", "Graham Neubig", "Jan Niehues", "Alex Waibel." ],
      "venue" : "Trans. Assoc. Comput. Linguistics, 7:313–325.",
      "citeRegEx" : "Sperber et al\\.,? 2019",
      "shortCiteRegEx" : "Sperber et al\\.",
      "year" : 2019
    }, {
      "title" : "Selfattentional acoustic models",
      "author" : [ "Matthias Sperber", "Jan Niehues", "Graham Neubig", "Sebastian Stüker", "Alex Waibel." ],
      "venue" : "Interspeech 2018, 19th Annual Conference of the International Speech Communication Association, Hyderabad,",
      "citeRegEx" : "Sperber et al\\.,? 2018",
      "shortCiteRegEx" : "Sperber et al\\.",
      "year" : 2018
    }, {
      "title" : "Analyzing ASR pretraining for low-resource speech-to-text translation",
      "author" : [ "Mihaela C. Stoian", "Sameer Bansal", "Sharon Goldwater." ],
      "venue" : "2020 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP",
      "citeRegEx" : "Stoian et al\\.,? 2020",
      "shortCiteRegEx" : "Stoian et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Bridging the gap between pre-training and fine-tuning for end-toend speech translation",
      "author" : [ "Chengyi Wang", "Yu Wu", "Shujie Liu", "Zhenglu Yang", "Ming Zhou." ],
      "venue" : "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI",
      "citeRegEx" : "Wang et al\\.,? 2020a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Curriculum pretraining for end-to-end speech translation",
      "author" : [ "Chengyi Wang", "Yu Wu", "Shujie Liu", "Ming Zhou", "Zhenglu Yang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL",
      "citeRegEx" : "Wang et al\\.,? 2020b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Hybrid ctc/attention architecture for end-to-end speech recognition",
      "author" : [ "Shinji Watanabe", "Takaaki Hori", "Suyoun Kim", "John R. Hershey", "Tomoki Hayashi." ],
      "venue" : "IEEE J. Sel. Top. Signal Process., 11(8):1240–1253.",
      "citeRegEx" : "Watanabe et al\\.,? 2017",
      "shortCiteRegEx" : "Watanabe et al\\.",
      "year" : 2017
    }, {
      "title" : "Sequence-to-sequence models can directly translate foreign speech",
      "author" : [ "Ron J. Weiss", "Jan Chorowski", "Navdeep Jaitly", "Yonghui Wu", "Zhifeng Chen." ],
      "venue" : "Interspeech 2017, 18th Annual Conference of the International",
      "citeRegEx" : "Weiss et al\\.,? 2017",
      "shortCiteRegEx" : "Weiss et al\\.",
      "year" : 2017
    }, {
      "title" : "Modeling localness for self-attention networks",
      "author" : [ "Baosong Yang", "Zhaopeng Tu", "Derek F. Wong", "Fandong Meng", "Lidia S. Chao", "Tong Zhang." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Pro-",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards making the most of BERT in neural machine translation",
      "author" : [ "Jiacheng Yang", "Mingxuan Wang", "Hao Zhou", "Chengqi Zhao", "Weinan Zhang", "Yong Yu", "Lei Li." ],
      "venue" : "The Thirty-Fourth AAAI Conference on Artificial In-",
      "citeRegEx" : "Yang et al\\.,? 2020",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Researchers typically use pre-trained Automatic Speech Recognition (ASR) and Machine Translation (MT) models to boost ST systems (Berard et al., 2018).",
      "startOffset" : 129,
      "endOffset" : 150
    }, {
      "referenceID" : 3,
      "context" : "For example, one can initialize the ST encoder using a large-scale ASR model (Bansal et al., 2019).",
      "startOffset" : 77,
      "endOffset" : 98
    }, {
      "referenceID" : 4,
      "context" : "Since the ST encoder is initialized by the pre-trained ASR encoder (Berard et al., 2018), it fails to model large contexts in the utterance.",
      "startOffset" : 67,
      "endOffset" : 88
    }, {
      "referenceID" : 36,
      "context" : "But a large scope of representation learning is necessary for translation (Yang et al., 2018).",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 37,
      "context" : "In addition, we develop a multi-teacher knowledge distillation method to robustly train the ST encoder and preserve the pretrained knowledge during fine-tuning (Yang et al., 2020).",
      "startOffset" : 160,
      "endOffset" : 179
    }, {
      "referenceID" : 20,
      "context" : "The earliest of these models were cascaded: they treated ST as a pipeline of running an ASR system and an MT system sequentially (Ney, 1999; Mathias and Byrne, 2006; Schultz et al., 2004).",
      "startOffset" : 129,
      "endOffset" : 187
    }, {
      "referenceID" : 19,
      "context" : "The earliest of these models were cascaded: they treated ST as a pipeline of running an ASR system and an MT system sequentially (Ney, 1999; Mathias and Byrne, 2006; Schultz et al., 2004).",
      "startOffset" : 129,
      "endOffset" : 187
    }, {
      "referenceID" : 26,
      "context" : "The earliest of these models were cascaded: they treated ST as a pipeline of running an ASR system and an MT system sequentially (Ney, 1999; Mathias and Byrne, 2006; Schultz et al., 2004).",
      "startOffset" : 129,
      "endOffset" : 187
    }, {
      "referenceID" : 5,
      "context" : "As another stream in the ST area, end-to-end methods have been promising recently (Berard et al., 2016; Weiss et al., 2017; Berard et al., 2018).",
      "startOffset" : 82,
      "endOffset" : 144
    }, {
      "referenceID" : 35,
      "context" : "As another stream in the ST area, end-to-end methods have been promising recently (Berard et al., 2016; Weiss et al., 2017; Berard et al., 2018).",
      "startOffset" : 82,
      "endOffset" : 144
    }, {
      "referenceID" : 4,
      "context" : "As another stream in the ST area, end-to-end methods have been promising recently (Berard et al., 2016; Weiss et al., 2017; Berard et al., 2018).",
      "startOffset" : 82,
      "endOffset" : 144
    }, {
      "referenceID" : 7,
      "context" : "The rise of end-to-end ST can be traced back to the success of deep neural models (Duong et al., 2016).",
      "startOffset" : 82,
      "endOffset" : 102
    }, {
      "referenceID" : 35,
      "context" : "As an alternative, researchers used multi-task learning (MTL) to robustly train the ST model so that it could benefit from additional guide signals (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Berard et al., 2018; Sperber et al., 2019; Dong et al., 2021).",
      "startOffset" : 148,
      "endOffset" : 263
    }, {
      "referenceID" : 1,
      "context" : "As an alternative, researchers used multi-task learning (MTL) to robustly train the ST model so that it could benefit from additional guide signals (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Berard et al., 2018; Sperber et al., 2019; Dong et al., 2021).",
      "startOffset" : 148,
      "endOffset" : 263
    }, {
      "referenceID" : 4,
      "context" : "As an alternative, researchers used multi-task learning (MTL) to robustly train the ST model so that it could benefit from additional guide signals (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Berard et al., 2018; Sperber et al., 2019; Dong et al., 2021).",
      "startOffset" : 148,
      "endOffset" : 263
    }, {
      "referenceID" : 28,
      "context" : "As an alternative, researchers used multi-task learning (MTL) to robustly train the ST model so that it could benefit from additional guide signals (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Berard et al., 2018; Sperber et al., 2019; Dong et al., 2021).",
      "startOffset" : 148,
      "endOffset" : 263
    }, {
      "referenceID" : 6,
      "context" : "As an alternative, researchers used multi-task learning (MTL) to robustly train the ST model so that it could benefit from additional guide signals (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Berard et al., 2018; Sperber et al., 2019; Dong et al., 2021).",
      "startOffset" : 148,
      "endOffset" : 263
    }, {
      "referenceID" : 4,
      "context" : "For example, one can initialize the encoder with an ASR model, and initialize the decoder with the target-language side of an MT model (Berard et al., 2018; Bansal et al., 2019; Stoian et al., 2020).",
      "startOffset" : 135,
      "endOffset" : 198
    }, {
      "referenceID" : 3,
      "context" : "For example, one can initialize the encoder with an ASR model, and initialize the decoder with the target-language side of an MT model (Berard et al., 2018; Bansal et al., 2019; Stoian et al., 2020).",
      "startOffset" : 135,
      "endOffset" : 198
    }, {
      "referenceID" : 30,
      "context" : "For example, one can initialize the encoder with an ASR model, and initialize the decoder with the target-language side of an MT model (Berard et al., 2018; Bansal et al., 2019; Stoian et al., 2020).",
      "startOffset" : 135,
      "endOffset" : 198
    }, {
      "referenceID" : 18,
      "context" : ", 2020a,b), the shrink mechanism (Liu et al., 2020), the adversarial regularizer (Alinejad and Sarkar, 2020), and etc.",
      "startOffset" : 33,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : ", 2020), the adversarial regularizer (Alinejad and Sarkar, 2020), and etc.",
      "startOffset" : 37,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "Following previous work in end-to-end models (Berard et al., 2016; Weiss et al., 2017), we envision an encoding-decoding process in which an input sequence is encoded into a representation vector, and the vector is then decoded into an output sequence.",
      "startOffset" : 45,
      "endOffset" : 86
    }, {
      "referenceID" : 35,
      "context" : "Following previous work in end-to-end models (Berard et al., 2016; Weiss et al., 2017), we envision an encoding-decoding process in which an input sequence is encoded into a representation vector, and the vector is then decoded into an output sequence.",
      "startOffset" : 45,
      "endOffset" : 86
    }, {
      "referenceID" : 36,
      "context" : "(2018) find that the ASR model benefits from a small attention window, which is inconsistent with the MT model (Yang et al., 2018).",
      "startOffset" : 111,
      "endOffset" : 130
    }, {
      "referenceID" : 31,
      "context" : "We choose Transformer as the base architecture (Vaswani et al., 2017) and run experiments on the MuST-C En-De corpus.",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 10,
      "context" : "For stronger systems, we use Connectionist Temporal Classification (CTC) (Graves et al., 2006) as the auxiliary loss on the encoders when we train the ASR and ST systems (Watanabe et al.",
      "startOffset" : 73,
      "endOffset" : 94
    }, {
      "referenceID" : 34,
      "context" : ", 2006) as the auxiliary loss on the encoders when we train the ASR and ST systems (Watanabe et al., 2017; Karita et al., 2019; Bahar et al., 2019).",
      "startOffset" : 83,
      "endOffset" : 147
    }, {
      "referenceID" : 2,
      "context" : ", 2006) as the auxiliary loss on the encoders when we train the ASR and ST systems (Watanabe et al., 2017; Karita et al., 2019; Bahar et al., 2019).",
      "startOffset" : 83,
      "endOffset" : 147
    }, {
      "referenceID" : 20,
      "context" : "Note that SATE is in general a cascaded model, in response to the pioneering work in ST (Ney, 1999).",
      "startOffset" : 88,
      "endOffset" : 99
    }, {
      "referenceID" : 18,
      "context" : "Instead of replacing the representation by the embedding of the most-likely token (Liu et al., 2020), we employ a soft token which is the expectation of the embedding over the distribution from CTC.",
      "startOffset" : 82,
      "endOffset" : 100
    }, {
      "referenceID" : 22,
      "context" : "Also, an informative representation should contain information in the original input (Peters et al., 2018).",
      "startOffset" : 85,
      "endOffset" : 106
    }, {
      "referenceID" : 12,
      "context" : "Another improvement here is that we develop a multi-teacher knowledge distillation (MTKD) method to preserve the pre-trained knowledge during fine-tuning (Hinton et al., 2015).",
      "startOffset" : 154,
      "endOffset" : 175
    }, {
      "referenceID" : 17,
      "context" : "The ST model mimics the teacher distribution by minimizing the cross-entropy loss between the teacher and student (Liu et al., 2019).",
      "startOffset" : 114,
      "endOffset" : 132
    }, {
      "referenceID" : 15,
      "context" : "We run experiments on the LibriSpeech English-French (En-Fr) (Kocabiyikoglu et al., 2018) and MuST-C EnglishGerman (En-De) (Gangi et al.",
      "startOffset" : 61,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : ", 2018) and MuST-C EnglishGerman (En-De) (Gangi et al., 2019) corpora, which correspond to the low-resource and highresource datasets respectively.",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 13,
      "context" : "Preprocessing Followed the preprocessing recipes of ESPnet (Inaguma et al., 2020), we remove the utterances of more than 3,000 frames and augment speech data by speed perturbation with factors of 0.",
      "startOffset" : 59,
      "endOffset" : 81
    }, {
      "referenceID" : 27,
      "context" : "We learn Byte-Pair Encoding (Sennrich et al., 2016) subword segmentation with 10,000 merge operations based on a shared source and target vocabulary for all datasets.",
      "startOffset" : 28,
      "endOffset" : 51
    }, {
      "referenceID" : 35,
      "context" : "For reducing the computational cost, the input speech features are processed by two convolutional layers, which have a stride of 2 × 2 and downsample the sequence by a factor of 4 (Weiss et al., 2017).",
      "startOffset" : 180,
      "endOffset" : 200
    }, {
      "referenceID" : 11,
      "context" : "For the unrestricted setting, we use the superior architecture Conformer (Gulati et al., 2020) on the ASR and ST tasks and widen the model by increasing the hidden size to 512 and attention heads to 8.",
      "startOffset" : 73,
      "endOffset" : 94
    }, {
      "referenceID" : 25,
      "context" : "Different from previous work, we report the case-sensitive SacreBLEU6 (Post, 2018) for future standardization comparison across papers.",
      "startOffset" : 70,
      "endOffset" : 82
    }, {
      "referenceID" : 21,
      "context" : "Furthermore, we utilize the SpecAugment (Park et al., 2019) which is applied in the input speech features for better generalization and robustness7.",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 9,
      "context" : "This results in the catastrophic forgetting problem (Goodfellow et al., 2015).",
      "startOffset" : 52,
      "endOffset" : 77
    }, {
      "referenceID" : 16,
      "context" : "This is contrary to the previous conclusions that the MT encoder helps the performance significantly (Li et al., 2020).",
      "startOffset" : 101,
      "endOffset" : 118
    } ],
    "year" : 2021,
    "abstractText" : "Encoder pre-training is promising in end-toend Speech Translation (ST), given the fact that speech-to-translation data is scarce. But ST encoders are not simple instances of Automatic Speech Recognition (ASR) or Machine Translation (MT) encoders. For example, we find that ASR encoders lack the global context representation, which is necessary for translation, whereas MT encoders are not designed to deal with long but locally attentive acoustic sequences. In this work, we propose a Stacked Acoustic-and-Textual Encoding (SATE) method for speech translation. Our encoder begins with processing the acoustic sequence as usual, but later behaves more like an MT encoder for a global representation of the input sequence. In this way, it is straightforward to incorporate the pre-trained models into the system. Also, we develop an adaptor module to alleviate the representation inconsistency between the pre-trained ASR encoder and MT encoder, and develop a multi-teacher knowledge distillation method to preserve the pre-training knowledge. Experimental results on the LibriSpeech En-Fr and MuST-C EnDe ST tasks show that our method achieves state-of-the-art BLEU scores of 18.3 and 25.2. To our knowledge, we are the first to develop an end-to-end ST system that achieves comparable or even better BLEU performance than the cascaded ST counterpart when large-scale ASR and MT data is available1.",
    "creator" : "LaTeX with hyperref"
  }
}