{
  "name" : "2021.acl-long.447.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots Matters",
    "authors" : [ "Mengjie Zhao", "Yi Zhu", "Ehsan Shareghi", "Ivan Vulić", "Roi Reichart", "Anna Korhonen", "Hinrich Schütze" ],
    "emails" : [ "mzhao@cis.lmu.de,", "yz568@cam.ac.uk,", "iv250@cam.ac.uk,", "alk23@cam.ac.uk,", "ehsan.shareghi@monash.edu,", "roiri@technion.ac.il" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5751–5767\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5751"
    }, {
      "heading" : "1 Introduction",
      "text" : "Multilingual pretrained encoders like multilingual BERT (mBERT; Devlin et al. (2019)) and XLMR (Conneau et al., 2020) are the top performers in crosslingual tasks such as natural language inference (Conneau et al., 2018), document classification (Schwenk and Li, 2018; Artetxe and Schwenk, 2019), and argument mining (ToledoRonen et al., 2020). They enable transfer learning through language-agnostic representations in crosslingual setups (Hu et al., 2020).\nA widely explored transfer scenario is zero-shot crosslingual transfer (Pires et al., 2019; Conneau and Lample, 2019; Artetxe and Schwenk, 2019),\n* Equal contribution. 1Code and resources are available at https://github.\ncom/fsxlt\nwhere a pretrained encoder is finetuned on abundant task data in the source language (e.g., English) and then directly evaluated on target-language test data, achieving surprisingly good performance (Wu and Dredze, 2019; Hu et al., 2020). However, there is evidence that zero-shot performance reported in the literature has large variance and is often not reproducible (Keung et al., 2020a; Rios et al., 2020); the results in languages distant from English fall far short of those similar to English (Hu et al., 2020; Liang et al., 2020).\nLauscher et al. (2020) stress the importance of few-shot crosslingual transfer instead, where the encoder is first finetuned on a source language and then further finetuned with a small amount (10–100) of examples (few shots) of the target language. The few shots substantially improve model performance of the target language with negligible annotation costs (Garrette and Baldridge, 2013; Hedderich et al., 2020).\nIn this work, however, we demonstrate that the gains from few-shot transfer exhibit a high degree of sensitivity to the selection of few shots. For example, different choices for the few shots can yield a performance variance of over 10% accuracy in a standard document classification task. Motivated by this, we propose to fix the few shots for fair comparisons between different crosslingual transfer methods, and provide a benchmark resembling the standard “N -wayK-shot” few-shot learning configuration (Fei-Fei et al., 2006; Koch et al., 2015). We also evaluate and compare several stateof-the-art (SotA) few-shot finetuning techniques, in order to understand their performance and susceptibility to the variance related to few shots.\nWe also demonstrate that the effectiveness of few-shot crosslingual transfer depends on the type of downstream task. For syntactic tasks such as named-entity recognition, the few shots can improve results by up to ≈20 F1 points. For chal-\nlenging tasks like adversarial paraphrase identification, the few shots do not help and even sometimes lead to worse performance than zero-shot transfer. To understand these phenomena, we conduct additional in-depth analyses, and find that the models tend to utilize shallow lexical hints (Geirhos et al., 2020) in the target language, rather than leveraging abstract crosslingual semantic features learned from the source language.\nOur contributions: 1) We show that few-shot crosslingual transfer is prone to large variations in task performance; this property hinders unbiased assessments of the effectiveness of different fewshot methods. 2) To remedy this issue, we publish fixed and standardized few shots to support fair comparisons and reproducibility. 3) We empirically verify that few-shot crosslingual transfer has different performance impact on structurally different tasks; we provide in-depth analyses concerning the source of performance gains. 4) We analyze several SotA few-shot learning methods, and show that they underperform simple full model finetuning. We hope that our work will shed new light on the potential and current difficulties of few-shot learning in crosslingual setups."
    }, {
      "heading" : "2 Background and Related Work",
      "text" : "Zero-/Few-Shot Crosslingual Transfer. Multilingual pretrained encoders show strong zero-shot crosslingual transfer (ZS-XLT) ability in various NLP tasks (Pires et al., 2019; Hsu et al., 2019; Artetxe and Schwenk, 2019). In order to guide and measure the progress, standardized benchmarks like XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020) have been developed.\nRecently, Lauscher et al. (2020) and Hedderich et al. (2020) extended the focus on few-shot crosslingual transfer (FS-XLT): They assume the availability of a handful of labeled examples in a target language,2 which are used to further finetune a source-trained model. The extra few shots bring large performance gains at low annotation cost. In this work, we systematically analyze this recent FS-XLT scenario.\nFS-XLT resembles the intermediate-task transfer (STILT) approach (Phang et al., 2018; Pruksachatkun et al., 2020). In STILT, a pretrained encoder is finetuned on a resource-rich intermedi-\n2According to Garrette and Baldridge (2013), it is possible to collect ≈100 POS-annotated sentences in two hours even for low-resource languages such as Malagasy.\nate task, and then finetuned on a (resource-lean) target task. Likewise, FS-XLT focuses on transferring knowledge and general linguistic intelligence (Yogatama et al., 2019), although such transfer is between languages in the same task instead of between different tasks.\nFew-shot learning was first explored in computer vision (Miller et al., 2000; Fei-Fei et al., 2006; Koch et al., 2015); the aim there is to learn new concepts with only few images. Methods like prototypical networks (Snell et al., 2017) and modelagnostic meta-learning (MAML; Finn et al. (2017)) have also been applied to many monolingual (typically English) NLP tasks such as relation classification (Han et al., 2018; Gao et al., 2019), namedentity recognition (Hou et al., 2020a), word sense disambiguation (Holla et al., 2020), and text classification (Yu et al., 2018; Yin, 2020; Yin et al., 2020; Bansal et al., 2020; Gupta et al., 2020). However, recent few-shot learning methods in computer vision consisting of two simple finetuning stages, first on base-class images and then on new-class few shots, have been shown to outperform MAML and achieve SotA scores (Wang et al., 2020; Chen et al., 2020; Tian et al., 2020; Dhillon et al., 2020). Inspired by this work, we compare various fewshot finetuning methods from computer vision in the context of FS-XLT.\nTask Performance Variance. Deep neural networks’ performance on NLP tasks is bound to exhibit large variance. Reimers and Gurevych (2017) and Dror et al. (2019) stress the importance of reporting score distributions instead of a single score for fair(er) comparisons. Dodge et al. (2020), Mosbach et al. (2021), and Zhang et al. (2021) show that finetuning pretrained encoders with different random seeds yields performance with large variance. In this work, we examine a specific source of variance: We show that the choice of the few shots in crosslingual transfer learning also introduces large variance in performance; consequently, we offer standardized few shots for more controlled and fair comparisons."
    }, {
      "heading" : "3 Method",
      "text" : "Following Lauscher et al. (2020) and Hedderich et al. (2020), our FS-XLT method comprises two stages. First, we conduct source-training: The pretrained mBERT is finetuned with abundant annotated data in the source language. Similar to Hu et al. (2020), Liang et al. (2020) and due to\nthe abundant labeled data for many NLP tasks, we choose English as the source in our experiments. Directly evaluating the source-trained model after this stage corresponds to the widely studied ZS-XLT scenario. The second stage is targetadapting: The source-trained model from previous stage is adapted to a target language using few shots. We discuss details of sampling the few shots in §4. The development set of the target language is used for model selection in this stage."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : "We consider three types of tasks requiring varying degrees of semantic and syntactic knowledge transfer: Sequence classification (CLS), namedentity recognition (NER), and part-of-speech tagging (POS) in up to 40 typologically diverse languages (cf., Appendix §B)."
    }, {
      "heading" : "4.1 Datasets and Selection of Few Shots",
      "text" : "For the CLS tasks, we sample few shots from four multilingual datasets: News article classification (MLDoc; Schwenk and Li (2018)); Amazon review classification (MARC; Keung et al. (2020b)); natural language inference (XNLI; Conneau et al. (2018); Williams et al. (2018)); and crosslingual paraphrase adversaries from word scrambling (PAWSX; Zhang et al. (2019); Yang et al. (2019)). We use treebanks in Universal Dependencies (Nivre et al., 2020) for POS, and WikiANN dataset (Pan et al., 2017; Rahimi et al., 2019) for NER. Table 1 reports key information about the datasets.\nWe adopt the conventional few-shot sampling strategy (Fei-Fei et al., 2006; Koch et al., 2015; Snell et al., 2017), and conduct “N -way K-shot” sampling from the datasets; N is the number of classes and K refers to the number of shots per class. A group of N -way K-shot data is referred to as a bucket. We set N equal to the number of labels |T |. Following Wang et al. (2020), we sample 40 buckets for each target (i.e., non-English)\nlanguage of a task to get a reliable estimation of model performance.\nCLS Tasks. For MLDoc and MARC, each language has a train/dev/test split. We sample the buckets without replacement from the training set of each target language, so that buckets are disjoint from each other. Target languages in XNLI and PAWSX only have dev/test splits. We sample the buckets from the dev set; the remaining data serves as a single new dev set for model selection during target-adapting. For all tasks, we use K ∈ {1, 2, 4, 8}.\nPOS and NER. For the two structured prediction tasks, “N -way K-shot” is not well-defined because each sentence contains one or more labeled tokens. We use a similar sampling principle as with CLS, where N is the size of the label set for each language and task, but K is set to the minimum number of occurrences for each label. In particular, we utilize the Minimum-Including Algorithm (Hou et al., 2020b,a) to satisfy the following criteria when sampling a bucket: 1) each label appears at least K times, and 2) at least one label will appear less than K times if any sentence is removed from the bucket. Appendix §C gives sampling details. In contrast to sampling for CLS, we do not enforce samples from different buckets to be disjoint due to the small amount of data in some low-resource languages. We only use K ∈ {1, 2, 4} and exclude K = 8, as 8-shot buckets already have lots of labeled tokens, and thus (arguably) might not be considered few-shot."
    }, {
      "heading" : "4.2 Training Setup",
      "text" : "We use the pretrained cased mBERT model (Devlin et al., 2019), and rely on the PyTorch-based (Paszke et al., 2019) HuggingFace Transformers repository (Wolf et al., 2019) in all experiments.\nFor source-training, we finetune the pretrained encoder for 10 epochs with batch size 32. For target-adapting to every target language, the fewshot data is a sampled bucket in this language, and we finetune on the bucket for 50 epochs with early-stopping of 10 epochs. The batch size is set to the number of shots in the bucket. Each target-adapting experiment is repeated 40 times using the 40 buckets. We use the Adam optimizer (Kingma and Ba, 2015) with default parameters in both stages with learning rates searched over {1e−5, 3e−5, 5e−5, 7e−5}. For CLS tasks, we use mBERT’s [CLS] token as the final represen-\ntation. For NER and POS, following Devlin et al. (2019), we use a linear classifier layer on top of the representation of each tokenized word, which is its last wordpiece (He and Choi, 2020).\nWe set the maximum sequence length to 128 after wordpiece tokenization (Wu et al., 2016), in all experiments. Further implementation details are shown in our Reproducibility Checklist in Appendix §A."
    }, {
      "heading" : "5 Results and Discussion",
      "text" : ""
    }, {
      "heading" : "5.1 Source-Training Results",
      "text" : "The ZS-XLT performance from English (EN) to target languages of the four CLS tasks are shown in the K = 0 column in Table 2. For NER and POS, the results are shown in Figure 2.\nFor XTREME tasks (XNLI, PAWSX, NER, POS), our implementation delivers results comparable to Hu et al. (2020). For MLDoc, our results are comparable to (Dong and de Melo, 2019; Wu and Dredze, 2019; Eisenschlos et al., 2019). It is worth noting that reproducing the exact results is challenging, as suggested by Keung et al. (2020a). For MARC, our zero-shot results are worse than Keung et al. (2020b)’s who use the dev set of each target language for model selection while we use EN dev, following the common true ZS-XLT setup."
    }, {
      "heading" : "5.2 Target-Adapting Results",
      "text" : "Variance of Few-Shot Transfer. We hypothesize that FS-XLT suffers from large variance (Dodge et al., 2020) due to the large model complexity and small amount of data in a bucket. To test this empirically, we first conduct two experiments on MLDoc and MARC. First, for a fixed random seed, we repeat 1-shot target-adapting 40 times using different 1-shot buckets in German (DE) and Spanish (ES). Second, for a fixed 1-shot bucket, we repeat the same experiment 40 times using random seeds\nin {0 . . . 39}. Figure 1 presents the dev set performance distribution of the 40 runs with 40 random seeds (top) and 40 1-shot buckets (bottom).\nWith exactly the same training data, using different random seeds yields a 1–2 accuracy difference of FS-XLT (Figure 1 top). A similar phenomenon has been observed in finetuning monolingual encoders (Dodge et al., 2020) and multilingual encoders with ZS-XLT (Keung et al., 2020a; Wu and Dredze, 2020b; Xia et al., 2020); we show this observation also holds for FS-XLT. The key takeaway is that varying the buckets is a more severe problem. It causes much larger variance (Figure 1 bottom): The maximum accuracy difference is ≈6 for DE MARC and ≈10 for ES MLDoc. This can be due to the fact that difficulty of individual examples varies in a dataset (Swayamdipta et al., 2020), resulting in different amounts of information encoded in buckets.\nThis large variance could be an issue when comparing different few-shot learning algorithms. The bucket choice is a strong confounding factor that may obscure the strength of a promising few-shot technique. Therefore, for fair comparison, it is necessary to work with a fixed set of few shots. We propose to fix the sampled buckets for unbiased comparison of different FS-XLT methods. We publish the sampled buckets from the six multilingual datasets as a fixed and standardized few-shot evaluation benchmark.\nIn what follows, each FS-XLT experiment is repeated 40 times using 40 different buckets with the same fixed random seed; we report mean and standard deviation. As noted, the variance due to random seeds is smaller (cf., Figure 1) and has been well studied before (Reimers and Gurevych, 2017; Dodge et al., 2020). In this work, we thus focus our attention and limited computing resources on understanding the impact of buckets, the newly detected source of variance. However, we encourage practitioners to report results with both factors considered in the future.\nDifferent Numbers of Shots. A comparison concerning the number of shots (K), based on the few-shot results in Table 2 and Figure 2, reveals that the buckets largely improve model performance on a majority of tasks (MLDoc, MARC, POS, NER) over zero-shot results. This is in line with prior work (Lauscher et al., 2020; Hedderich et al., 2020) and follows the success of work on using bootstrapped data (Chaudhary et al., 2019; Sherborne\net al., 2020). In general, we observe that: 1) 1-shot buckets bring the largest relative performance improvement over ZS-XLT; 2) the gains follow the increase ofK, but with diminishing returns; 3) the performance variance across the 40 buckets decreases as K increases. These observations are more pronounced for POS and NER; e.g., 1-shot EN to Urdu (UR) POS transfer shows gains of ≈22 F1 points (52.40 with zero-shot, 74.95 with 1-shot).\nFor individual runs, we observe that models in FS-XLT tend to overfit the buckets quickly at small K values. For example, in around 32% of NER 1- shot buckets, the model achieves the best dev score right after the first epoch; continuing the training only degrades performance. Similar observations hold for semantic tasks like MARC, where in 10 out of 40 DE 1-shot buckets, the dev set performance peaks at epoch 1 (cf. learning curve in Appendix §D Figure 6). This suggests the necessity of running the target-adapting experiments on multiple buckets if reliable conclusions are to be drawn.\nDifferent Downstream Tasks. The models for different tasks present various levels of sensitiv-\nity to FS-XLT. Among the CLS tasks that require semantic reasoning, FS-XLT benefits MLDoc the most. This is not surprising given the fact that keyword matching can largely solve MLDoc (Artetxe et al., 2020a,b): A few examples related to target language keywords are expected to significantly improve performance. FS-XLT also yields prominent gains on the Amazon review classification dataset MARC. Similar to MLDoc, we hypothesize that just matching a few important opinion and sentiment words (Liu, 2012) in the target language brings large gains already. We provide further qualitative analyses in §5.4.\nXNLI and PAWSX behave differently from MLDoc and MARC. XNLI requires higher level semantic reasoning on pairs of sentences. FSXLT performance improves modestly (XNLI) or even decreases (PAWSX-ES) compared to ZSXLT, even with large K. PAWSX requires a model to distinguish adversarially designed nonparaphrase sentence pairs with large lexical overlap like “Flights from New York to Florida” and “Flights from Florida to New York” (Zhang et al., 2019). This poses a challenge for FS-XLT, given the small amount of target language information in the buckets. Therefore, when buckets are small (e.g., K = 1) and for challenging semantic tasks like PAWSX, the buckets do not substantially help. Annotating more shots in the target language is an intuitive solution. Designing task-specific pretraining/finetuning objectives could also be promising (Klein and Nabi, 2020; Ram et al., 2021).\nUnlike CLS tasks, POS and NER benefit from FS-XLT substantially. We speculate that there are two reasons: 1) Both tasks often require little to no high-level semantic understanding or reasoning; 2) due to i.i.d. sampling, train/dev/test splits are likely to have overlapping vocabulary, and the labels in the buckets can easily propagate to dev and test. We delve deeper into these conjectures in §5.4.\nDifferent Languages. For languages that are more distant from EN, e.g., with different scripts, small lexical overlap, or fewer common typological features (Pires et al., 2019; Wu and Dredze, 2020a), FS-XLT introduces crucial lexical and structural information to guide the update of embedding and transformer layers in mBERT.\nWe present several findings based on the NER and POS results for a typologically diverse language sample. Figure 2 shows that for languages with non-Latin scripts (different from EN), despite\n40\nSame Script = Yes\nSame Script = No\ntheir small to non-existent lexical overlap3 and diverging typological features (see Appendix §D Tables 9 and 14), the performance boosts are generally larger than those in the same-script target languages: 6.2 vs. 3.0 average gain in NER and 11.4 vs. 5.4 in POS for K = 1. This clearly manifests the large information discrepancy between target-language buckets and source-language data. EN data is less relevant to these languages, so they obtain very limited gain from source-training, reflected by their low ZS-XLT scores. With a small amount of target-language knowledge in the buckets, the performance is improved dramatically, highlighting the effectiveness of FS-XLT.\nTable 3 shows that, besides script form, lexical overlap and the number of linguistic features com-\n3We define lexical overlap as |V |L∩|V |EN|V |EN where V denotes vocabulary. |V |L is computed with the 40 buckets of a target language L.\nmon with EN4 also contribute directly to FS-XLT performance difference among languages: There is a moderate negative correlation between F1 score gains vs. the two factors when considered independently for both syntactic tasks: The fewer overlaps/features a target language shares with EN, the larger the gain FS-XLT achieves.\nThis again stresses the importance of buckets – they contain target-language-specific knowledge about a task that cannot be obtained by ZS-XLT, which solely relies on language similarity. Interestingly, Pearson’s ρ indicates that common linguistic features are much less linearly correlated with FSXLT gains in NER than in POS."
    }, {
      "heading" : "5.3 Importance of Source-Training",
      "text" : "Table 4 reports the performance drop when directly carrying out target-adapting, without any prior source-training of mBERT. We show the scores for MLDoc and PAWSX as a simple and a challenging CLS task, respectively. For NER and POS, we select two high- (Russian (RU), ES), mid- (Vietnamese (VI), Turkish (TR)), and low-resource languages (Tamil (TA), Marathi (MR)) each.5\nThe results clearly indicate that omitting the\n4Following Pires et al. (2019), we use six WALS features: 81A (Order of Subject, Object and Verb), 85A (Order of Adposition and Noun), 86A (Order of Genitive and Noun), 87A (Order of Adjective and Noun), 88A (Order of Demonstrative and Noun), and 89A (Order of Numeral and Noun).\n5The categorization based on resource availability is according to WikiSize (Wu and Dredze, 2020a).\nJaccard Index of Buckets and Improved Predictions (FA)\nsource-training stage yields large performance drops. Even larger variance is also observed in this scenario (cf. Appendix §D Table 11). Therefore, the model indeed learns, when trained on the source language, some transferable crosslingual features that are beneficial to target languages, both for semantic and syntactic tasks."
    }, {
      "heading" : "5.4 Importance of Lexical Features",
      "text" : "We now investigate the sources of gains brought by FS-XLT over ZS-XLT.\nFor syntactic tasks, we take Persian (FA) POS as an example. Figure 3 visualizes the lexical overlap, measured by the Jaccard index, of 10 1-shot buckets (rows) and the improved word-label predictions introduced by target-adapting on each of the buckets (columns). In more detail, for column c, we collect the set (denoted as Cc) of all test set words whose label is incorrectly predicted by the zeroshot model, but correctly predicted by the model trained on the c-th bucket. For row i, we denote with Bi the set of words occurring in bucket i. The figure shows in cell (i, k) the Jaccard index of Bi and Ck. The bright color (i.e., higher lexical overlap) on the diagonal reflects that the improvements\nintroduced by a bucket are mainly6 those wordlabel predictions that are lexically more similar to the bucket than to other buckets.\nWe also investigate the question: How many word-label predictions that are improved after FSXLT occur in the bucket, i.e., in the training data? Figure 4 plots this for the 40 1-shot buckets in FA, UR, and Hindi (HI). We see that many test words do occur in the bucket (shown in orange), in line with recent findings (Lewis et al., 2021; Elangovan et al., 2021). These analyses shed light on why the buckets benefit NER/POS – which heavily rely on lexical information – more than higher level semantic tasks.\nFor the CLS task MARC, which requires un-\n6Note that the sampled buckets for POS are not completely disjoint (cf. sampling strategy in §4).\nderstanding product reviews, Figure 5 visualizes the confusion matrices of test set predictions for DE and Chinese (ZH) zero- and 1-shot models; axis ticks are review scores in {1, 2, 3, 4, 5}. The squares on the diagonals in the two left heatmaps show that parameter initialization on EN is a good basis for well-performing ZS-XLT: This is particularly true for DE, which is linguistically closer to EN. Two extreme review scores – 1 (for DE) and 5 (for ZH) – have the largest confusions. The two right heatmaps show that improvements brought by the 1-shot buckets are mainly achieved by correctly predicting more cases of the two extreme review scores: 2→ 1 (DE) and 4→ 5 (ZH). But the more challenging cases (reviews with scores 2, 3, 4), which require non-trivial reasoning, are not significantly improved, or even become worse.\nWe inspect examples that are incorrectly predicted by the few-shot model (predicting 1), but are correctly predicted by the zero-shot model (predicting 2). Specifically, we compute the difference of where [CLS] attends to, before and after adapting the model on a 1-shot DE bucket. We extract and average attentions computed by the 12 heads from the topmost transformer layer.\nTable 5 shows that “nicht” (“not”) draws high attention change from [CLS]. “Nicht” (i.e., negation) by itself is not a reliable indicator of sentiment, so giving the lowest score to reviews solely because they contain “nicht” is not a good strategy. The following review is classified as 1 by the 1-shot model, but 2 is the gold label (as the review is not entirely negative):\n“Die Uhr ging nicht einmal eine Minute ... Optisch allerdings sehr schön.” (“The clock didn’t even work one minute ... Visually, however, very nice.”)\nPretrained multilingual encoders are shown to learn and store “language-agnostic” features (Pires et al., 2019; Zhao et al., 2020); §5.3 shows that source-training mBERT on EN substantially benefits other languages, even for difficult semantic tasks like PAWSX. Conditioning on such languageagnostic features, we expect that the buckets should lead to good understanding and reasoning capabilities for a target language. However, plain few-shot finetuning still relies heavily on unintended shallow\nlexical cues and shortcuts (Niven and Kao, 2019; Geirhos et al., 2020) that generalize poorly. Other open research questions for future work arise: How do we overcome this excessive reliance on lexical features? How can we leverage language-agnostic features with few shots? Our standardized buckets, baseline results, and analyses are the initial step towards researching and answering these questions."
    }, {
      "heading" : "5.5 Target-Adapting Methods",
      "text" : "SotA few-shot learning methods (Chen et al., 2019; Wang et al., 2020; Tian et al., 2020; Dhillon et al., 2020) from computer vision consist of two stages: 1) training on base-class images, and 2) few-shot finetuning using new-class images. Source-training and target-adapting stages of FS-XLT, albeit among languages, follow an approach very similar to these methods. Therefore, we test their effectiveness for crosslingual transfer. These methods are built upon cosine similarity that imparts inductive bias about distance and is more effective than a fullyconnected classifier layer (FC) with smallK (Wang et al., 2020). Following (Chen et al., 2019; Wang et al., 2020; Tian et al., 2020), we freeze the embedding and transformer layers of mBERT, and explore four variants of the target-adapting stage using MARC.\nCOS+Pooler. We randomly initialize a trainable weight matrix W ∈ Rh×c where h is the hidden dimension size and c is the number of classes. Rewriting W as [w1, . . . ,wi, . . . ,wc], we compute the logits of an input sentence representation x ∈ Rh (from mBERT) belonging to class i as\nα · x ᵀwi\n‖x‖2 · ‖wi‖2 ,\nwhere α is a scaling hyperparameter, set to 10 in all experiments. During training, W and mBERT’s pooler layer containing a linear layer and a tanh non-linearity are updated.\nFC+Pooler. During training, we update the linear classifier layer and mBERT’s pooler layer.\nFC only. During training, we only update the linear classifier layer. This variant largely reduces model complexity and exhibit lower variance when K is small.\nFC(reset)+Pooler. Similar to FC+Pooler, but the source-trained linear classifier layer is randomly re-initialized before training.\nTable 6 shows the performance of these methods along with full model finetuning (without freezing). FC+Pooler performs the best among the\nfour for both K = 1 and K = 8 in all languages. However, it underperforms the full model finetuning, especially when K = 8. FC only is sub-optimal; yet the decrease in comparison to FC+Pooler is small, highlighting that EN-trained mBERT is a strong feature extractor. COS+Pooler and FC(reset)+Pooler perform considerably worse than the other two methods and zero-shot transfer – presumably because their new parameters need to be trained from scratch with few shots.\nWe leave further exploration of other possibilities of exploiting crosslingual features through collapse-preventing regularization (Aghajanyan et al., 2021) or contrastive learning (Gunel et al., 2021) to future work. Integrating prompting (Brown et al., 2020; Schick and Schütze, 2020; Gao et al., 2020; Liu et al., 2021) – a strong performing few-shot learning methodology for NLP – into the crosslingual transfer learning pipeline is also a promising direction."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "We have presented an extensive study of few-shot crosslingual transfer. The focus of the study has been on an empirically detected performance variance in few-shot scenarios: The models exhibit a high level of sensitivity to the choice of few shots. We analyzed and discussed the major causes of this variance across six diverse tasks for up to 40 languages. Our results show that large language models tend to overfit to few shots quickly and mostly rely on shallow lexical features present in the few shots, though they have been trained with abundant data in English. Moreover, we have empirically validated that state-of-the-art few-shot learning methods in computer vision do not outperform a conceptually simple alternative: Full model finetuning.\nOur study calls for more rigor and accurate reporting of the results of few-shot crosslingual transfer experiments. They should include score distributions over standardized and fixed few shots. To\naid this goal, we have created and provided such fixed few shots as a standardized benchmark for six multilingual datasets.\nFew-shot learning is promising for crosslingual transfer, because it mirrors how people acquire new languages, and that the few-shot data annotation is feasible. In future work, we will investigate more sophisticated techniques and extend the work to more NLP tasks."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was funded by the European Research Council: ERC NonSequeToR (#740516) and ERC LEXICAL (#648909). We thank the anonymous reviewers and Fei Mi for their helpful suggestions."
    }, {
      "heading" : "A Reproducibility Checklist",
      "text" : "A.1 mBERT Architecture and Number of Parameters\nWe use the “bert-base-multilingual-cased” model7. It contains 12 Transformer blocks with 768 hidden dimensions. Each block has 12 self attention heads. The model is pretrained on the concatenation of the Wikipedia dump of 104 languages.\nThere are about 179 million parameters in mBERT. For all the tasks, we use a linear output layer. Denoting the output dimension of a task as m, e.g., m = 2 for PAWSX. Then we have in total 179 million + 768×m + m parameters for the task.\nA.2 Computing Infrastructure\nAll experiments are conducted on GeForce GTX 1080Ti. In the source-training stage, we use 4 GPUs with per-GPU batch size 32. In the targetadapting stage, we use a single GPU and the batch size is equal to the number of examples in a bucket.\nA.3 Evaluation Metrics and Validation Performance\nWe follow the standard evaluation metrics used in XTREME (Hu et al., 2020) and they are shown in Table 1; evaluation functions in scikit-learn (Pedregosa et al., 2011) and seqeval (https://github.com/ chakki-works/seqeval) are used. Link to code: code/utils/eval meters.py.\nThe validation performance of the Englishtrained models are shown in the first row of Table 7; the optimal learning rate for each task is shown in the second row.\nFor all the FS-XLT experiments, we enclosed the validation scores in https://github.com/fsxlt/ running-logs.\nA.4 Hyperparameter Search\nFor both source-training and target-adapting, the only hyperparameter we search is learning rate (from {1e− 5, 3e− 5, 5e− 5, 7e− 5}) to reduce\n7https://github.com/google-research/ bert/blob/master/multilingual.md\nAlgorithm 1: Minimum-including Require: # of shot K, language data D, label set LD 1: Initialize a bucket S = {}, Count`j = 0 (∀`j ∈ LD) 2: for ` in LD do\nwhile Count` < K do From D, randomly sample a\n(x(i),y(i)) pair that y(i) includes ` Add (x(i),y(i)) to S Update all Count`j (∀`j ∈ LD)\n3: for each (x(i),y(i)) in S do Remove (x(i),y(i)) from S Update all Count`j (∀`j ∈ LD) if any Count`j < K then\nPut (x(i),y(i)) back to S Update all Count`j (∀`j ∈ LD)\n4: Return S\nthe sensitivity of our results to hyperparameter selection.\nA.5 Datasets and Preprocessing For tasks (XNLI, PAWSX, POS, NER) covered in XTREME (Hu et al., 2020), we utilize the provided preprocessed datasets. Our MLDoc dataset is obtained from https://github.com/ facebookresearch/MLDoc. We retrieve MARC from docs.opendata.aws/amazon-reviews-ml/ readme.html. Table 8 shows example entries of the datasets. It is worth noting that MARC is a single sentence review classification task, however, we put the “review title” and “product category” in the “Text B” field, following Keung et al. (2020b).\nWe utilize the tokenizer in the HuggingFace Transformers package (Wolf et al., 2019) to preprocess all the texts. In all experiments, we use 128 maximum sequence length and truncate from the end of a sentence if its length exceeds the limit."
    }, {
      "heading" : "B Languages",
      "text" : "We work on 40 languages in total. They are shown in Table 9, together with their ISO 639-1 codes, writing script, and language features from WALs (https://wals.info/) used in our experiments."
    }, {
      "heading" : "C Minimum-Including Algorithm",
      "text" : "We utilize the Minimum-including Algorithm from Hou et al. (2020a,b) for sampling the buckets of POS and NER which have several labels in a sentence. Denoting as x a sentence that consists of an array of words (x1, . . . , xn), and the array y that consists of a series of labels (y1, . . . , yn). We sample the buckets by using Algorithm 1. Note that we\nsample with replacement for POS and NER."
    }, {
      "heading" : "D Additional Results",
      "text" : "D.1 Learning Curve Figure 6 visualizes the averaged learning curve of 10 out of 40 German 1-shot MARC buckets for which the best dev performance is obtained at epoch 1.\nD.2 Numerical Values The numerical values of the POS and NER FS-XLT results are shown in Table 13 and Table 12. The absolute performances of few-shot transfer without English source-training are shown in Table 11. The lexical overlap of target languages with EN for NER and POS is shown in Table 14."
    }, {
      "heading" : "38 83 127 462 290 32 39 59 314 555",
      "text" : ""
    }, {
      "heading" : "60 262 283 322 73 63 163 190 395 189",
      "text" : ""
    }, {
      "heading" : "136 401 266 174 23 143 284 219 270 84",
      "text" : ""
    }, {
      "heading" : "255 543 112 70 20 269 416 126 125 65",
      "text" : ""
    }, {
      "heading" : "599 316 36 33 16 570 262 45 56 67",
      "text" : ""
    }, {
      "heading" : "6 25 75 357 537 16 27 42 245 670",
      "text" : ""
    }, {
      "heading" : "4 69 237 525 165 22 87 176 471 244",
      "text" : ""
    }, {
      "heading" : "24 259 497 196 24 65 298 369 218 50",
      "text" : ""
    }, {
      "heading" : "45 630 250 64 11 176 554 162 80 28",
      "text" : ""
    }, {
      "heading" : "292 584 78 27 19 526 361 43 31 40",
      "text" : "K=0 K=1 K=2 K=4 EN 95.39 - - - AF 86.60 91.10± 1.11 92.12± 1.15 93.50± 0.56 AR 66.55 75.64± 1.09 77.01± 0.84 78.52± 0.67 BG 87.02 91.01± 0.97 91.97± 0.90 93.18± 0.56 DE 86.38 89.38± 0.90 90.21± 0.50 91.32± 0.43 EL 81.89 89.69± 1.05 90.53± 0.89 91.58± 0.72 ES 86.64 90.05± 1.01 91.19± 0.74 92.31± 0.52 ET 79.17 81.69± 1.09 83.05± 0.98 84.39± 0.56 EU 49.51 68.44± 2.47 71.94± 1.78 75.89± 1.20 FA 65.73 80.82± 2.14 82.81± 1.79 84.95± 1.16 FI 74.49 78.25± 1.22 79.65± 0.85 81.32± 0.82 FR 82.54 89.55± 1.08 90.84± 0.64 91.66± 0.60 HE 76.79 80.40± 1.42 82.42± 1.06 83.98± 0.83 HI 64.29 78.87± 1.26 80.80± 0.80 81.97± 0.92 HU 75.10 84.44± 1.40 86.31± 0.90 88.61± 0.67 ID 70.80 72.68± 1.08 73.64± 0.78 74.34± 0.75 IT 85.97 88.77± 0.87 89.93± 0.50 90.77± 0.59 JA 47.60 75.84± 1.68 78.46± 1.31 80.42± 0.98 KO 42.29 57.43± 1.36 59.92± 1.18 62.37± 1.22 MR 58.70 71.60± 2.52 74.89± 1.95 77.21± 1.77 NL 88.35 88.97± 0.73 89.55± 0.79 90.83± 0.54 PT 86.45 88.18± 0.70 88.98± 0.66 89.78± 0.38 RU 86.36 89.07± 0.76 89.85± 0.57 91.13± 0.51 TA 53.51 62.84± 2.69 66.30± 1.56 69.36± 1.13 TE 67.48 71.46± 2.58 75.72± 1.94 78.84± 1.44 TR 57.58 64.01± 1.53 66.02± 1.28 67.73± 0.82 UR 52.40 74.95± 2.15 78.53± 1.38 79.57± 1.24 VI 54.96 64.79± 2.33 69.39± 1.73 72.36± 1.51 ZH 63.01 74.15± 1.96 76.62± 1.39 79.42± 0.83\nTable 12: Zero- (column K=0) and few- (columns K>0) shot cross-lingual transfer results (%) on POS test set.\nK=0 K=1 K=2 K=4 EN 83.65 - - - AF 78.36 79.07± 1.47 79.69± 1.40 80.24± 1.16 AR 39.91 54.44± 6.74 60.51± 4.30 63.61± 2.65 BG 78.59 78.65± 0.38 78.70± 0.39 78.87± 0.48 BN 64.17 66.37± 1.69 66.66± 1.57 65.98± 2.11 DE 79.00 79.33± 0.71 79.61± 0.76 79.74± 0.73 EL 75.20 74.93± 0.79 75.18± 0.95 75.40± 0.93 ES 77.16 79.19± 1.97 80.28± 1.71 80.90± 1.94 ET 71.88 72.58± 1.17 73.60± 1.65 74.60± 1.59 EU 55.35 59.60± 3.32 61.59± 3.84 64.68± 2.96 FA 40.73 59.20± 5.34 68.55± 4.04 71.13± 3.45 FI 68.43 71.43± 2.61 73.92± 2.44 75.81± 2.15 FR 80.38 80.54± 0.93 81.08± 0.85 81.22± 0.93 HE 56.36 58.24± 2.25 59.43± 2.29 60.27± 2.43 HI 65.84 67.16± 1.61 67.56± 2.18 68.29± 1.76 HU 71.28 72.23± 1.33 73.03± 1.44 74.14± 1.61 ID 60.10 77.87± 6.31 78.57± 4.14 81.07± 1.50 IT 80.30 80.68± 0.79 81.00± 0.92 80.90± 1.12 JA 7.16 20.71± 7.07 28.23± 5.32 32.93± 6.03 JV 61.18 67.80± 4.72 69.79± 3.37 72.12± 3.34 KA 61.26 61.62± 1.09 62.25± 1.56 63.68± 1.66 KK 40.29 50.42± 5.49 54.97± 6.81 62.94± 4.55 KO 46.50 47.25± 1.36 48.69± 1.82 51.76± 2.30 ML 46.77 47.83± 2.30 49.51± 3.01 51.41± 3.31 MR 54.70 55.78± 2.54 57.22± 2.43 59.18± 3.13 MS 68.61 71.04± 3.07 74.51± 4.28 76.25± 3.04 MY 42.45 43.55± 3.88 46.03± 4.48 47.81± 4.28 NL 82.77 82.73± 0.43 82.83± 0.54 82.82± 0.46 PT 79.28 79.89± 0.99 80.39± 0.98 80.49± 0.95 RU 65.20 67.30± 2.38 68.78± 2.73 71.34± 2.82 SW 68.36 71.07± 4.28 70.08± 3.15 74.33± 5.25 TA 46.12 47.81± 1.81 49.86± 2.99 52.23± 2.63 TE 50.02 52.57± 1.91 54.02± 2.65 55.75± 2.72 TH 1.53 4.56± 4.87 6.08± 4.88 5.87± 4.14 TL 69.23 72.34± 2.25 72.63± 2.43 73.55± 2.25 TR 65.78 69.37± 2.24 69.53± 2.07 72.33± 2.85 UR 40.77 58.48± 6.51 63.38± 4.88 66.49± 4.64 VI 64.67 68.77± 3.54 69.64± 3.63 71.08± 3.28 YO 35.48 53.55± 6.19 58.22± 5.47 65.46± 7.10 ZH 13.95 32.84± 7.10 40.34± 5.32 48.49± 4.30\nTable 13: Zero- (column K=0) and few- (columns K>0) shot cross-lingual transfer results (%) on NER test set."
    } ],
    "references" : [ {
      "title" : "Better fine-tuning by reducing representational collapse",
      "author" : [ "Armen Aghajanyan", "Akshat Shrivastava", "Anchit Gupta", "Naman Goyal", "Luke Zettlemoyer", "Sonal Gupta." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Aghajanyan et al\\.,? 2021",
      "shortCiteRegEx" : "Aghajanyan et al\\.",
      "year" : 2021
    }, {
      "title" : "On the cross-lingual transferability of monolingual representations",
      "author" : [ "Mikel Artetxe", "Sebastian Ruder", "Dani Yogatama." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623–4637, Online. Asso-",
      "citeRegEx" : "Artetxe et al\\.,? 2020a",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2020
    }, {
      "title" : "A call for more rigor in unsupervised cross-lingual learning",
      "author" : [ "Mikel Artetxe", "Sebastian Ruder", "Dani Yogatama", "Gorka Labaka", "Eneko Agirre." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Artetxe et al\\.,? 2020b",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2020
    }, {
      "title" : "Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond",
      "author" : [ "Mikel Artetxe", "Holger Schwenk." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:597–610.",
      "citeRegEx" : "Artetxe and Schwenk.,? 2019",
      "shortCiteRegEx" : "Artetxe and Schwenk.",
      "year" : 2019
    }, {
      "title" : "Learning to few-shot learn across diverse",
      "author" : [ "Trapit Bansal", "Rishikesh Jha", "Andrew McCallum" ],
      "venue" : null,
      "citeRegEx" : "Bansal et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2020
    }, {
      "title" : "A little annotation does a lot of good: A study in bootstrapping low-resource named entity recognizers",
      "author" : [ "Aditi Chaudhary", "Jiateng Xie", "Zaid Sheikh", "Graham Neubig", "Jaime Carbonell." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods",
      "citeRegEx" : "Chaudhary et al\\.,? 2019",
      "shortCiteRegEx" : "Chaudhary et al\\.",
      "year" : 2019
    }, {
      "title" : "A closer look at few-shot classification",
      "author" : [ "Wei-Yu Chen", "Yen-Cheng Liu", "Zsolt Kira", "YuChiang Frank Wang", "Jia-Bin Huang." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "A new metabaseline for few-shot learning",
      "author" : [ "Yinbo Chen", "Xiaolong Wang", "Zhuang Liu", "Huijuan Xu", "Trevor Darrell." ],
      "venue" : "arXiv preprint arXiv:2003.04390.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Alexis Conneau", "Guillaume Lample." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 7059–7069.",
      "citeRegEx" : "Conneau and Lample.,? 2019",
      "shortCiteRegEx" : "Conneau and Lample.",
      "year" : 2019
    }, {
      "title" : "Xnli: Evaluating crosslingual sentence representations",
      "author" : [ "Alexis Conneau", "Ruty Rinott", "Guillaume Lample", "Adina Williams", "Samuel R. Bowman", "Holger Schwenk", "Veselin Stoyanov." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Conneau et al\\.,? 2018",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "A baseline for few-shot image classification",
      "author" : [ "Guneet Singh Dhillon", "Pratik Chaudhari", "Avinash Ravichandran", "Stefano Soatto." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Dhillon et al\\.,? 2020",
      "shortCiteRegEx" : "Dhillon et al\\.",
      "year" : 2020
    }, {
      "title" : "Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping",
      "author" : [ "Jesse Dodge", "Gabriel Ilharco", "Roy Schwartz", "Ali Farhadi", "Hannaneh Hajishirzi", "Noah Smith" ],
      "venue" : null,
      "citeRegEx" : "Dodge et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Dodge et al\\.",
      "year" : 2020
    }, {
      "title" : "A robust selflearning framework for cross-lingual text classification",
      "author" : [ "Xin Dong", "Gerard de Melo." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Dong and Melo.,? 2019",
      "shortCiteRegEx" : "Dong and Melo.",
      "year" : 2019
    }, {
      "title" : "Deep dominance - how to properly compare deep neural models",
      "author" : [ "Rotem Dror", "Segev Shlomov", "Roi Reichart." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2773–2785, Florence, Italy. Associa-",
      "citeRegEx" : "Dror et al\\.,? 2019",
      "shortCiteRegEx" : "Dror et al\\.",
      "year" : 2019
    }, {
      "title" : "MultiFiT: Efficient multi-lingual language model fine-tuning",
      "author" : [ "Julian Eisenschlos", "Sebastian Ruder", "Piotr Czapla", "Marcin Kadras", "Sylvain Gugger", "Jeremy Howard." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Eisenschlos et al\\.,? 2019",
      "shortCiteRegEx" : "Eisenschlos et al\\.",
      "year" : 2019
    }, {
      "title" : "Memorization vs",
      "author" : [ "Aparna Elangovan", "Jiayuan He", "Karin Verspoor." ],
      "venue" : "generalization : Quantifying data leakage in NLP performance evaluation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Elangovan et al\\.,? 2021",
      "shortCiteRegEx" : "Elangovan et al\\.",
      "year" : 2021
    }, {
      "title" : "Oneshot learning of object categories",
      "author" : [ "L. Fei-Fei", "R. Fergus", "P. Perona." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(4):594–611.",
      "citeRegEx" : "Fei.Fei et al\\.,? 2006",
      "shortCiteRegEx" : "Fei.Fei et al\\.",
      "year" : 2006
    }, {
      "title" : "Model-agnostic meta-learning for fast adaptation of deep networks",
      "author" : [ "Chelsea Finn", "Pieter Abbeel", "Sergey Levine." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Re-",
      "citeRegEx" : "Finn et al\\.,? 2017",
      "shortCiteRegEx" : "Finn et al\\.",
      "year" : 2017
    }, {
      "title" : "Making pre-trained language models better few-shot learners",
      "author" : [ "Tianyu Gao", "Adam Fisch", "Danqi Chen." ],
      "venue" : "arXiv preprint arXiv:2012.15723.",
      "citeRegEx" : "Gao et al\\.,? 2020",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2020
    }, {
      "title" : "FewRel 2.0: Towards more challenging few-shot relation classification",
      "author" : [ "Tianyu Gao", "Xu Han", "Hao Zhu", "Zhiyuan Liu", "Peng Li", "Maosong Sun", "Jie Zhou" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Gao et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning a part-of-speech tagger from two hours of annotation",
      "author" : [ "Dan Garrette", "Jason Baldridge." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Garrette and Baldridge.,? 2013",
      "shortCiteRegEx" : "Garrette and Baldridge.",
      "year" : 2013
    }, {
      "title" : "Shortcut learning in deep neural networks",
      "author" : [ "Robert Geirhos", "Jörn-Henrik Jacobsen", "Claudio Michaelis", "Richard Zemel", "Wieland Brendel", "Matthias Bethge", "Felix A. Wichmann." ],
      "venue" : "Nature Machine Intelligence, 2(11):665–673.",
      "citeRegEx" : "Geirhos et al\\.,? 2020",
      "shortCiteRegEx" : "Geirhos et al\\.",
      "year" : 2020
    }, {
      "title" : "Supervised contrastive learning for pre-trained language model fine-tuning",
      "author" : [ "Beliz Gunel", "Jingfei Du", "Alexis Conneau", "Veselin Stoyanov." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Gunel et al\\.,? 2021",
      "shortCiteRegEx" : "Gunel et al\\.",
      "year" : 2021
    }, {
      "title" : "Effective few-shot classification with transfer learning",
      "author" : [ "Aakriti Gupta", "Kapil Thadani", "Neil O’Hare" ],
      "venue" : "In Proceedings of the 28th International Conference on Computational Linguistics,",
      "citeRegEx" : "Gupta et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2020
    }, {
      "title" : "FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation",
      "author" : [ "Xu Han", "Hao Zhu", "Pengfei Yu", "Ziyun Wang", "Yuan Yao", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Meth-",
      "citeRegEx" : "Han et al\\.,? 2018",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2018
    }, {
      "title" : "Establishing Strong Baselines for the New Decade: Sequence Tagging, Syntactic and Semantic Parsing with BERT",
      "author" : [ "Han He", "Jinho D. Choi." ],
      "venue" : "Proceedings of the 33rd International Florida Artificial Intelligence Research Society Conference,",
      "citeRegEx" : "He and Choi.,? 2020",
      "shortCiteRegEx" : "He and Choi.",
      "year" : 2020
    }, {
      "title" : "Transfer learning and distant supervision for multilingual transformer models: A study on African languages",
      "author" : [ "Michael A. Hedderich", "David Adelani", "Dawei Zhu", "Jesujoba Alabi", "Udia Markus", "Dietrich Klakow." ],
      "venue" : "Proceedings of the 2020 Con-",
      "citeRegEx" : "Hedderich et al\\.,? 2020",
      "shortCiteRegEx" : "Hedderich et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning to learn to disambiguate: Meta-learning for few-shot word sense disambiguation",
      "author" : [ "Nithin Holla", "Pushkar Mishra", "Helen Yannakoudakis", "Ekaterina Shutova." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020,",
      "citeRegEx" : "Holla et al\\.,? 2020",
      "shortCiteRegEx" : "Holla et al\\.",
      "year" : 2020
    }, {
      "title" : "Few-shot slot tagging with collapsed dependency transfer and label-enhanced task-adaptive projection network",
      "author" : [ "Yutai Hou", "Wanxiang Che", "Yongkui Lai", "Zhihan Zhou", "Yijia Liu", "Han Liu", "Ting Liu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the As-",
      "citeRegEx" : "Hou et al\\.,? 2020a",
      "shortCiteRegEx" : "Hou et al\\.",
      "year" : 2020
    }, {
      "title" : "Fewjoint: A few-shot learning benchmark for joint language understanding",
      "author" : [ "Yutai Hou", "Jiafeng Mao", "Yongkui Lai", "Cheng Chen", "Wanxiang Che", "Zhigang Chen", "Ting Liu." ],
      "venue" : "CoRR, abs/2009.08138.",
      "citeRegEx" : "Hou et al\\.,? 2020b",
      "shortCiteRegEx" : "Hou et al\\.",
      "year" : 2020
    }, {
      "title" : "Zero-shot reading comprehension by crosslingual transfer learning with multi-lingual language representation model",
      "author" : [ "Tsung-Yuan Hsu", "Chi-Liang Liu", "Hung-yi Lee." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Hsu et al\\.,? 2019",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2019
    }, {
      "title" : "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation",
      "author" : [ "Junjie Hu", "Sebastian Ruder", "Aditya Siddhant", "Graham Neubig", "Orhan Firat", "Melvin Johnson." ],
      "venue" : "Proceedings of the 37th International",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "Don’t use English dev: On the zero-shot cross-lingual evaluation of contextual embeddings",
      "author" : [ "Phillip Keung", "Yichao Lu", "Julian Salazar", "Vikas Bhardwaj." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Keung et al\\.,? 2020a",
      "shortCiteRegEx" : "Keung et al\\.",
      "year" : 2020
    }, {
      "title" : "The multilingual Amazon reviews corpus",
      "author" : [ "Phillip Keung", "Yichao Lu", "György Szarvas", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4563–4568, Online. As-",
      "citeRegEx" : "Keung et al\\.,? 2020b",
      "shortCiteRegEx" : "Keung et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "ICLR (Poster).",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Contrastive selfsupervised learning for commonsense reasoning",
      "author" : [ "Tassilo Klein", "Moin Nabi." ],
      "venue" : "In",
      "citeRegEx" : "Klein and Nabi.,? 2020",
      "shortCiteRegEx" : "Klein and Nabi.",
      "year" : 2020
    }, {
      "title" : "Siamese neural networks for one-shot image recognition",
      "author" : [ "Gregory Koch", "Richard Zemel", "Ruslan Salakhutdinov." ],
      "venue" : "ICML 2015 Deep Learning Workshop.",
      "citeRegEx" : "Koch et al\\.,? 2015",
      "shortCiteRegEx" : "Koch et al\\.",
      "year" : 2015
    }, {
      "title" : "From zero to hero: On the limitations of zero-shot language transfer with multilingual transformers",
      "author" : [ "Anne Lauscher", "Vinit Ravishankar", "Ivan Vulić", "Goran Glavaš." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Lauscher et al\\.,? 2020",
      "shortCiteRegEx" : "Lauscher et al\\.",
      "year" : 2020
    }, {
      "title" : "Question and answer test-train overlap in open-domain question answering datasets",
      "author" : [ "Patrick Lewis", "Pontus Stenetorp", "Sebastian Riedel." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Lewis et al\\.,? 2021",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2021
    }, {
      "title" : "XGLUE: A new benchmark datasetfor cross-lingual pre-training, understanding and generation",
      "author" : [ "Wu", "Shuguang Liu", "Fan Yang", "Daniel Campos", "Rangan Majumder", "Ming Zhou." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Sentiment analysis and opinion mining",
      "author" : [ "Bing Liu." ],
      "venue" : "Synthesis lectures on human language technologies, 5(1):1–167.",
      "citeRegEx" : "Liu.,? 2012",
      "shortCiteRegEx" : "Liu.",
      "year" : 2012
    }, {
      "title" : "Gpt understands, too",
      "author" : [ "Xiao Liu", "Yanan Zheng", "Zhengxiao Du", "Ming Ding", "Yujie Qian", "Zhilin Yang", "Jie Tang." ],
      "venue" : "arXiv preprint arXiv:2103.10385.",
      "citeRegEx" : "Liu et al\\.,? 2021",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Learning from one example through shared densities on transforms",
      "author" : [ "Erik G Miller", "Nicholas E Matsakis", "Paul A Viola." ],
      "venue" : "Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No. PR00662), volume 1,",
      "citeRegEx" : "Miller et al\\.,? 2000",
      "shortCiteRegEx" : "Miller et al\\.",
      "year" : 2000
    }, {
      "title" : "On the stability of fine-tuning {bert}: Misconceptions, explanations, and strong baselines",
      "author" : [ "Marius Mosbach", "Maksym Andriushchenko", "Dietrich Klakow." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Mosbach et al\\.,? 2021",
      "shortCiteRegEx" : "Mosbach et al\\.",
      "year" : 2021
    }, {
      "title" : "Probing neural network comprehension of natural language arguments",
      "author" : [ "Timothy Niven", "Hung-Yu Kao." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4658–4664, Florence, Italy. Association",
      "citeRegEx" : "Niven and Kao.,? 2019",
      "shortCiteRegEx" : "Niven and Kao.",
      "year" : 2019
    }, {
      "title" : "Universal Dependencies v2: An evergrowing multilingual treebank collection",
      "author" : [ "Joakim Nivre", "Marie-Catherine de Marneffe", "Filip Ginter", "Jan Hajič", "Christopher D. Manning", "Sampo Pyysalo", "Sebastian Schuster", "Francis Tyers", "Daniel Zeman" ],
      "venue" : null,
      "citeRegEx" : "Nivre et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Nivre et al\\.",
      "year" : 2020
    }, {
      "title" : "Crosslingual name tagging and linking for 282 languages",
      "author" : [ "Xiaoman Pan", "Boliang Zhang", "Jonathan May", "Joel Nothman", "Kevin Knight", "Heng Ji." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Pan et al\\.,? 2017",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2017
    }, {
      "title" : "Pytorch: An imperative style, high-performance deep learning library",
      "author" : [ "jani", "Sasank Chilamkurthy", "Benoit Steiner", "Lu Fang", "Junjie Bai", "Soumith Chintala" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "jani et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "jani et al\\.",
      "year" : 2019
    }, {
      "title" : "Scikit-learn: Machine learning",
      "author" : [ "F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay" ],
      "venue" : null,
      "citeRegEx" : "Pedregosa et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pedregosa et al\\.",
      "year" : 2011
    }, {
      "title" : "Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks",
      "author" : [ "Jason Phang", "Thibault Févry", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:1811.01088.",
      "citeRegEx" : "Phang et al\\.,? 2018",
      "shortCiteRegEx" : "Phang et al\\.",
      "year" : 2018
    }, {
      "title" : "How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996– 5001, Florence, Italy",
      "author" : [ "Telmo Pires", "Eva Schlinger", "Dan Garrette." ],
      "venue" : "Association for Computa-",
      "citeRegEx" : "Pires et al\\.,? 2019",
      "shortCiteRegEx" : "Pires et al\\.",
      "year" : 2019
    }, {
      "title" : "Intermediate-task transfer learning with pretrained language models: When and why",
      "author" : [ "Yada Pruksachatkun", "Jason Phang", "Haokun Liu", "Phu Mon Htut", "Xiaoyi Zhang", "Richard Yuanzhe Pang", "Clara Vania", "Katharina Kann", "Samuel R. Bowman" ],
      "venue" : null,
      "citeRegEx" : "Pruksachatkun et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Pruksachatkun et al\\.",
      "year" : 2020
    }, {
      "title" : "Massively multilingual transfer for NER",
      "author" : [ "Afshin Rahimi", "Yuan Li", "Trevor Cohn." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 151–164, Florence, Italy. Association for Computational Linguis-",
      "citeRegEx" : "Rahimi et al\\.,? 2019",
      "shortCiteRegEx" : "Rahimi et al\\.",
      "year" : 2019
    }, {
      "title" : "Few-shot question answering by pretraining span selection",
      "author" : [ "Ori Ram", "Yuval Kirstain", "Jonathan Berant", "Amir Globerson", "Omer Levy." ],
      "venue" : "arXiv preprint arXiv:2101.00438.",
      "citeRegEx" : "Ram et al\\.,? 2021",
      "shortCiteRegEx" : "Ram et al\\.",
      "year" : 2021
    }, {
      "title" : "Reporting score distributions makes a difference: Performance study of LSTM-networks for sequence tagging",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Reimers and Gurevych.,? 2017",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2017
    }, {
      "title" : "Subword segmentation and a single bridge language affect zero-shot neural machine translation",
      "author" : [ "Annette Rios", "Mathias Müller", "Rico Sennrich." ],
      "venue" : "Proceedings of the Fifth Conference on Machine Translation, pages 526–535, Online. Association for",
      "citeRegEx" : "Rios et al\\.,? 2020",
      "shortCiteRegEx" : "Rios et al\\.",
      "year" : 2020
    }, {
      "title" : "It’s not just size that matters: Small language models are also few-shot learners",
      "author" : [ "Timo Schick", "Hinrich Schütze." ],
      "venue" : "arXiv preprint arXiv:2009.07118.",
      "citeRegEx" : "Schick and Schütze.,? 2020",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2020
    }, {
      "title" : "A corpus for multilingual document classification in eight languages",
      "author" : [ "Holger Schwenk", "Xian Li." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Paris, France. European Language Resources",
      "citeRegEx" : "Schwenk and Li.,? 2018",
      "shortCiteRegEx" : "Schwenk and Li.",
      "year" : 2018
    }, {
      "title" : "Bootstrapping a crosslingual semantic parser",
      "author" : [ "Tom Sherborne", "Yumo Xu", "Mirella Lapata." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 499–517, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Sherborne et al\\.,? 2020",
      "shortCiteRegEx" : "Sherborne et al\\.",
      "year" : 2020
    }, {
      "title" : "Prototypical networks for few-shot learning",
      "author" : [ "Jake Snell", "Kevin Swersky", "Richard Zemel." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30, pages 4077–4087. Curran Associates, Inc.",
      "citeRegEx" : "Snell et al\\.,? 2017",
      "shortCiteRegEx" : "Snell et al\\.",
      "year" : 2017
    }, {
      "title" : "Dataset cartography: Mapping and diagnosing datasets with training dynamics",
      "author" : [ "Swabha Swayamdipta", "Roy Schwartz", "Nicholas Lourie", "Yizhong Wang", "Hannaneh Hajishirzi", "Noah A. Smith", "Yejin Choi." ],
      "venue" : "Proceedings of the 2020 Conference on",
      "citeRegEx" : "Swayamdipta et al\\.,? 2020",
      "shortCiteRegEx" : "Swayamdipta et al\\.",
      "year" : 2020
    }, {
      "title" : "Rethinking few-shot image classification: A good embedding is all you need? In Computer Vision – ECCV 2020, pages 266–282, Cham",
      "author" : [ "Yonglong Tian", "Yue Wang", "Dilip Krishnan", "Joshua B. Tenenbaum", "Phillip Isola." ],
      "venue" : "Springer International Pub-",
      "citeRegEx" : "Tian et al\\.,? 2020",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2020
    }, {
      "title" : "Multilingual argument mining: Datasets and analysis",
      "author" : [ "Orith Toledo-Ronen", "Matan Orbach", "Yonatan Bilu", "Artem Spector", "Noam Slonim." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 303–317, Online. As-",
      "citeRegEx" : "Toledo.Ronen et al\\.,? 2020",
      "shortCiteRegEx" : "Toledo.Ronen et al\\.",
      "year" : 2020
    }, {
      "title" : "Frustratingly simple few-shot object detection",
      "author" : [ "Xin Wang", "Thomas Huang", "Joseph Gonzalez", "Trevor Darrell", "Fisher Yu." ],
      "venue" : "Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Re-",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Williams et al\\.,? 2018",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "Huggingface’s transformers: State-of-the-art natural language",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "R’emi Louf", "Morgan Funtowicz", "Jamie Brew" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT",
      "author" : [ "Shijie Wu", "Mark Dredze." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Wu and Dredze.,? 2019",
      "shortCiteRegEx" : "Wu and Dredze.",
      "year" : 2019
    }, {
      "title" : "Are all languages created equal in multilingual bert? In Proceedings of the 5th Workshop on Representation Learning for NLP, RepL4NLP@ACL 2020, Online, July 9, 2020, pages 120–130",
      "author" : [ "Shijie Wu", "Mark Dredze." ],
      "venue" : "Association for Computational Lin-",
      "citeRegEx" : "Wu and Dredze.,? 2020a",
      "shortCiteRegEx" : "Wu and Dredze.",
      "year" : 2020
    }, {
      "title" : "Do explicit alignments robustly improve multilingual encoders? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4471–4482, Online",
      "author" : [ "Shijie Wu", "Mark Dredze." ],
      "venue" : "Association for Computa-",
      "citeRegEx" : "Wu and Dredze.,? 2020b",
      "shortCiteRegEx" : "Wu and Dredze.",
      "year" : 2020
    }, {
      "title" : "Which *BERT? A survey organizing contextualized encoders",
      "author" : [ "Patrick Xia", "Shijie Wu", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7516–7533, Online. As-",
      "citeRegEx" : "Xia et al\\.,? 2020",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2020
    }, {
      "title" : "PAWS-X: A cross-lingual adversarial dataset for paraphrase identification",
      "author" : [ "Yinfei Yang", "Yuan Zhang", "Chris Tar", "Jason Baldridge." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Meta-learning for few-shot natural language processing: A survey",
      "author" : [ "Wenpeng Yin" ],
      "venue" : null,
      "citeRegEx" : "Yin.,? \\Q2020\\E",
      "shortCiteRegEx" : "Yin.",
      "year" : 2020
    }, {
      "title" : "Universal natural language processing with limited annotations: Try few-shot textual entailment as a start",
      "author" : [ "Wenpeng Yin", "Nazneen Fatema Rajani", "Dragomir Radev", "Richard Socher", "Caiming Xiong." ],
      "venue" : "Proceedings of the 2020 Conference on",
      "citeRegEx" : "Yin et al\\.,? 2020",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning and evaluating general linguistic intelligence",
      "author" : [ "Dani Yogatama", "Cyprien de Masson d’Autume", "Jerome Connor", "Tomas Kocisky", "Mike Chrzanowski", "Lingpeng Kong", "Angeliki Lazaridou", "Wang Ling", "Lei Yu", "Chris Dyer", "Phil Blunsom" ],
      "venue" : null,
      "citeRegEx" : "Yogatama et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Yogatama et al\\.",
      "year" : 2019
    }, {
      "title" : "Diverse few-shot text classification with multiple metrics",
      "author" : [ "Mo Yu", "Xiaoxiao Guo", "Jinfeng Yi", "Shiyu Chang", "Saloni Potdar", "Yu Cheng", "Gerald Tesauro", "Haoyu Wang", "Bowen Zhou." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chap-",
      "citeRegEx" : "Yu et al\\.,? 2018",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2018
    }, {
      "title" : "Revisiting fewsample {bert} fine-tuning",
      "author" : [ "Tianyi Zhang", "Felix Wu", "Arzoo Katiyar", "Kilian Q Weinberger", "Yoav Artzi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "PAWS: Paraphrase adversaries from word scrambling",
      "author" : [ "Yuan Zhang", "Jason Baldridge", "Luheng He." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Inducing languageagnostic multilingual representations",
      "author" : [ "Wei Zhao", "Steffen Eger", "Johannes Bjerva", "Isabelle Augenstein." ],
      "venue" : "arXiv preprint arXiv:2008.09112.",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    }, {
      "title" : "2020b). We utilize the tokenizer in the HuggingFace Transformers package (Wolf et al., 2019) to preprocess all the texts. In all experiments, we use 128 maximum sequence length and truncate",
      "author" : [ "“Text B" ],
      "venue" : null,
      "citeRegEx" : "B.,? \\Q2019\\E",
      "shortCiteRegEx" : "B.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "(2019)) and XLMR (Conneau et al., 2020) are the top performers in crosslingual tasks such as natural language inference (Conneau et al.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 10,
      "context" : ", 2020) are the top performers in crosslingual tasks such as natural language inference (Conneau et al., 2018), document classification (Schwenk and Li, 2018; Artetxe and Schwenk, 2019), and argument mining (ToledoRonen et al.",
      "startOffset" : 88,
      "endOffset" : 110
    }, {
      "referenceID" : 59,
      "context" : ", 2018), document classification (Schwenk and Li, 2018; Artetxe and Schwenk, 2019), and argument mining (ToledoRonen et al.",
      "startOffset" : 33,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : ", 2018), document classification (Schwenk and Li, 2018; Artetxe and Schwenk, 2019), and argument mining (ToledoRonen et al.",
      "startOffset" : 33,
      "endOffset" : 82
    }, {
      "referenceID" : 33,
      "context" : "They enable transfer learning through language-agnostic representations in crosslingual setups (Hu et al., 2020).",
      "startOffset" : 95,
      "endOffset" : 112
    }, {
      "referenceID" : 52,
      "context" : "A widely explored transfer scenario is zero-shot crosslingual transfer (Pires et al., 2019; Conneau and Lample, 2019; Artetxe and Schwenk, 2019),",
      "startOffset" : 71,
      "endOffset" : 144
    }, {
      "referenceID" : 9,
      "context" : "A widely explored transfer scenario is zero-shot crosslingual transfer (Pires et al., 2019; Conneau and Lample, 2019; Artetxe and Schwenk, 2019),",
      "startOffset" : 71,
      "endOffset" : 144
    }, {
      "referenceID" : 3,
      "context" : "A widely explored transfer scenario is zero-shot crosslingual transfer (Pires et al., 2019; Conneau and Lample, 2019; Artetxe and Schwenk, 2019),",
      "startOffset" : 71,
      "endOffset" : 144
    }, {
      "referenceID" : 68,
      "context" : ", English) and then directly evaluated on target-language test data, achieving surprisingly good performance (Wu and Dredze, 2019; Hu et al., 2020).",
      "startOffset" : 109,
      "endOffset" : 147
    }, {
      "referenceID" : 33,
      "context" : ", English) and then directly evaluated on target-language test data, achieving surprisingly good performance (Wu and Dredze, 2019; Hu et al., 2020).",
      "startOffset" : 109,
      "endOffset" : 147
    }, {
      "referenceID" : 34,
      "context" : "However, there is evidence that zero-shot performance reported in the literature has large variance and is often not reproducible (Keung et al., 2020a; Rios et al., 2020); the results in languages distant from English fall far short of those similar to English (Hu et al.",
      "startOffset" : 130,
      "endOffset" : 170
    }, {
      "referenceID" : 57,
      "context" : "However, there is evidence that zero-shot performance reported in the literature has large variance and is often not reproducible (Keung et al., 2020a; Rios et al., 2020); the results in languages distant from English fall far short of those similar to English (Hu et al.",
      "startOffset" : 130,
      "endOffset" : 170
    }, {
      "referenceID" : 33,
      "context" : ", 2020); the results in languages distant from English fall far short of those similar to English (Hu et al., 2020; Liang et al., 2020).",
      "startOffset" : 98,
      "endOffset" : 135
    }, {
      "referenceID" : 18,
      "context" : "Motivated by this, we propose to fix the few shots for fair comparisons between different crosslingual transfer methods, and provide a benchmark resembling the standard “N -wayK-shot” few-shot learning configuration (Fei-Fei et al., 2006; Koch et al., 2015).",
      "startOffset" : 216,
      "endOffset" : 257
    }, {
      "referenceID" : 38,
      "context" : "Motivated by this, we propose to fix the few shots for fair comparisons between different crosslingual transfer methods, and provide a benchmark resembling the standard “N -wayK-shot” few-shot learning configuration (Fei-Fei et al., 2006; Koch et al., 2015).",
      "startOffset" : 216,
      "endOffset" : 257
    }, {
      "referenceID" : 23,
      "context" : "To understand these phenomena, we conduct additional in-depth analyses, and find that the models tend to utilize shallow lexical hints (Geirhos et al., 2020) in the target language, rather than leveraging abstract crosslingual semantic features learned from the source language.",
      "startOffset" : 135,
      "endOffset" : 157
    }, {
      "referenceID" : 52,
      "context" : "Multilingual pretrained encoders show strong zero-shot crosslingual transfer (ZS-XLT) ability in various NLP tasks (Pires et al., 2019; Hsu et al., 2019; Artetxe and Schwenk, 2019).",
      "startOffset" : 115,
      "endOffset" : 180
    }, {
      "referenceID" : 32,
      "context" : "Multilingual pretrained encoders show strong zero-shot crosslingual transfer (ZS-XLT) ability in various NLP tasks (Pires et al., 2019; Hsu et al., 2019; Artetxe and Schwenk, 2019).",
      "startOffset" : 115,
      "endOffset" : 180
    }, {
      "referenceID" : 3,
      "context" : "Multilingual pretrained encoders show strong zero-shot crosslingual transfer (ZS-XLT) ability in various NLP tasks (Pires et al., 2019; Hsu et al., 2019; Artetxe and Schwenk, 2019).",
      "startOffset" : 115,
      "endOffset" : 180
    }, {
      "referenceID" : 33,
      "context" : "In order to guide and measure the progress, standardized benchmarks like XTREME (Hu et al., 2020) and XGLUE (Liang et al.",
      "startOffset" : 80,
      "endOffset" : 97
    }, {
      "referenceID" : 51,
      "context" : "FS-XLT resembles the intermediate-task transfer (STILT) approach (Phang et al., 2018; Pruksachatkun et al., 2020).",
      "startOffset" : 65,
      "endOffset" : 113
    }, {
      "referenceID" : 53,
      "context" : "FS-XLT resembles the intermediate-task transfer (STILT) approach (Phang et al., 2018; Pruksachatkun et al., 2020).",
      "startOffset" : 65,
      "endOffset" : 113
    }, {
      "referenceID" : 75,
      "context" : "Likewise, FS-XLT focuses on transferring knowledge and general linguistic intelligence (Yogatama et al., 2019), although such transfer is between languages in the same task instead of between different tasks.",
      "startOffset" : 87,
      "endOffset" : 110
    }, {
      "referenceID" : 44,
      "context" : "Few-shot learning was first explored in computer vision (Miller et al., 2000; Fei-Fei et al., 2006; Koch et al., 2015); the aim there is to learn new concepts with only few images.",
      "startOffset" : 56,
      "endOffset" : 118
    }, {
      "referenceID" : 18,
      "context" : "Few-shot learning was first explored in computer vision (Miller et al., 2000; Fei-Fei et al., 2006; Koch et al., 2015); the aim there is to learn new concepts with only few images.",
      "startOffset" : 56,
      "endOffset" : 118
    }, {
      "referenceID" : 38,
      "context" : "Few-shot learning was first explored in computer vision (Miller et al., 2000; Fei-Fei et al., 2006; Koch et al., 2015); the aim there is to learn new concepts with only few images.",
      "startOffset" : 56,
      "endOffset" : 118
    }, {
      "referenceID" : 61,
      "context" : "Methods like prototypical networks (Snell et al., 2017) and modelagnostic meta-learning (MAML; Finn et al.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 26,
      "context" : "(2017)) have also been applied to many monolingual (typically English) NLP tasks such as relation classification (Han et al., 2018; Gao et al., 2019), namedentity recognition (Hou et al.",
      "startOffset" : 113,
      "endOffset" : 149
    }, {
      "referenceID" : 21,
      "context" : "(2017)) have also been applied to many monolingual (typically English) NLP tasks such as relation classification (Han et al., 2018; Gao et al., 2019), namedentity recognition (Hou et al.",
      "startOffset" : 113,
      "endOffset" : 149
    }, {
      "referenceID" : 30,
      "context" : ", 2019), namedentity recognition (Hou et al., 2020a), word sense disambiguation (Holla et al.",
      "startOffset" : 33,
      "endOffset" : 52
    }, {
      "referenceID" : 29,
      "context" : ", 2020a), word sense disambiguation (Holla et al., 2020), and text classification (Yu et al.",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 65,
      "context" : "ever, recent few-shot learning methods in computer vision consisting of two simple finetuning stages, first on base-class images and then on new-class few shots, have been shown to outperform MAML and achieve SotA scores (Wang et al., 2020; Chen et al., 2020; Tian et al., 2020; Dhillon et al., 2020).",
      "startOffset" : 221,
      "endOffset" : 300
    }, {
      "referenceID" : 7,
      "context" : "ever, recent few-shot learning methods in computer vision consisting of two simple finetuning stages, first on base-class images and then on new-class few shots, have been shown to outperform MAML and achieve SotA scores (Wang et al., 2020; Chen et al., 2020; Tian et al., 2020; Dhillon et al., 2020).",
      "startOffset" : 221,
      "endOffset" : 300
    }, {
      "referenceID" : 63,
      "context" : "ever, recent few-shot learning methods in computer vision consisting of two simple finetuning stages, first on base-class images and then on new-class few shots, have been shown to outperform MAML and achieve SotA scores (Wang et al., 2020; Chen et al., 2020; Tian et al., 2020; Dhillon et al., 2020).",
      "startOffset" : 221,
      "endOffset" : 300
    }, {
      "referenceID" : 12,
      "context" : "ever, recent few-shot learning methods in computer vision consisting of two simple finetuning stages, first on base-class images and then on new-class few shots, have been shown to outperform MAML and achieve SotA scores (Wang et al., 2020; Chen et al., 2020; Tian et al., 2020; Dhillon et al., 2020).",
      "startOffset" : 221,
      "endOffset" : 300
    }, {
      "referenceID" : 47,
      "context" : "We use treebanks in Universal Dependencies (Nivre et al., 2020) for POS, and WikiANN dataset (Pan et al.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 48,
      "context" : ", 2020) for POS, and WikiANN dataset (Pan et al., 2017; Rahimi et al., 2019) for NER.",
      "startOffset" : 37,
      "endOffset" : 76
    }, {
      "referenceID" : 54,
      "context" : ", 2020) for POS, and WikiANN dataset (Pan et al., 2017; Rahimi et al., 2019) for NER.",
      "startOffset" : 37,
      "endOffset" : 76
    }, {
      "referenceID" : 18,
      "context" : "We adopt the conventional few-shot sampling strategy (Fei-Fei et al., 2006; Koch et al., 2015; Snell et al., 2017), and conduct “N -way K-shot” sampling from the datasets; N is the number of classes and K refers to the number of shots per class.",
      "startOffset" : 53,
      "endOffset" : 114
    }, {
      "referenceID" : 38,
      "context" : "We adopt the conventional few-shot sampling strategy (Fei-Fei et al., 2006; Koch et al., 2015; Snell et al., 2017), and conduct “N -way K-shot” sampling from the datasets; N is the number of classes and K refers to the number of shots per class.",
      "startOffset" : 53,
      "endOffset" : 114
    }, {
      "referenceID" : 61,
      "context" : "We adopt the conventional few-shot sampling strategy (Fei-Fei et al., 2006; Koch et al., 2015; Snell et al., 2017), and conduct “N -way K-shot” sampling from the datasets; N is the number of classes and K refers to the number of shots per class.",
      "startOffset" : 53,
      "endOffset" : 114
    }, {
      "referenceID" : 11,
      "context" : "We use the pretrained cased mBERT model (Devlin et al., 2019), and rely on the PyTorch-based (Paszke et al.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 67,
      "context" : ", 2019) HuggingFace Transformers repository (Wolf et al., 2019) in all experiments.",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 36,
      "context" : "We use the Adam optimizer (Kingma and Ba, 2015) with default parameters in both stages with learning rates searched over {1e−5, 3e−5, 5e−5, 7e−5}.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 27,
      "context" : "(2019), we use a linear classifier layer on top of the representation of each tokenized word, which is its last wordpiece (He and Choi, 2020).",
      "startOffset" : 122,
      "endOffset" : 141
    }, {
      "referenceID" : 68,
      "context" : "For MLDoc, our results are comparable to (Dong and de Melo, 2019; Wu and Dredze, 2019; Eisenschlos et al., 2019).",
      "startOffset" : 41,
      "endOffset" : 112
    }, {
      "referenceID" : 16,
      "context" : "For MLDoc, our results are comparable to (Dong and de Melo, 2019; Wu and Dredze, 2019; Eisenschlos et al., 2019).",
      "startOffset" : 41,
      "endOffset" : 112
    }, {
      "referenceID" : 13,
      "context" : "We hypothesize that FS-XLT suffers from large variance (Dodge et al., 2020) due to the large model complexity and small amount of data in a bucket.",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 13,
      "context" : "A similar phenomenon has been observed in finetuning monolingual encoders (Dodge et al., 2020) and multilingual encoders with ZS-XLT (Keung et al.",
      "startOffset" : 74,
      "endOffset" : 94
    }, {
      "referenceID" : 34,
      "context" : ", 2020) and multilingual encoders with ZS-XLT (Keung et al., 2020a; Wu and Dredze, 2020b; Xia et al., 2020); we show this observation also holds for FS-XLT.",
      "startOffset" : 46,
      "endOffset" : 107
    }, {
      "referenceID" : 70,
      "context" : ", 2020) and multilingual encoders with ZS-XLT (Keung et al., 2020a; Wu and Dredze, 2020b; Xia et al., 2020); we show this observation also holds for FS-XLT.",
      "startOffset" : 46,
      "endOffset" : 107
    }, {
      "referenceID" : 71,
      "context" : ", 2020) and multilingual encoders with ZS-XLT (Keung et al., 2020a; Wu and Dredze, 2020b; Xia et al., 2020); we show this observation also holds for FS-XLT.",
      "startOffset" : 46,
      "endOffset" : 107
    }, {
      "referenceID" : 62,
      "context" : "This can be due to the fact that difficulty of individual examples varies in a dataset (Swayamdipta et al., 2020), resulting in different amounts of information encoded in buckets.",
      "startOffset" : 87,
      "endOffset" : 113
    }, {
      "referenceID" : 56,
      "context" : ", Figure 1) and has been well studied before (Reimers and Gurevych, 2017; Dodge et al., 2020).",
      "startOffset" : 45,
      "endOffset" : 93
    }, {
      "referenceID" : 13,
      "context" : ", Figure 1) and has been well studied before (Reimers and Gurevych, 2017; Dodge et al., 2020).",
      "startOffset" : 45,
      "endOffset" : 93
    }, {
      "referenceID" : 39,
      "context" : "This is in line with prior work (Lauscher et al., 2020; Hedderich et al., 2020) and follows the success of work on using bootstrapped data (Chaudhary et al.",
      "startOffset" : 32,
      "endOffset" : 79
    }, {
      "referenceID" : 28,
      "context" : "This is in line with prior work (Lauscher et al., 2020; Hedderich et al., 2020) and follows the success of work on using bootstrapped data (Chaudhary et al.",
      "startOffset" : 32,
      "endOffset" : 79
    }, {
      "referenceID" : 42,
      "context" : "Similar to MLDoc, we hypothesize that just matching a few important opinion and sentiment words (Liu, 2012) in the target language brings large gains already.",
      "startOffset" : 96,
      "endOffset" : 107
    }, {
      "referenceID" : 78,
      "context" : "PAWSX requires a model to distinguish adversarially designed nonparaphrase sentence pairs with large lexical overlap like “Flights from New York to Florida” and “Flights from Florida to New York” (Zhang et al., 2019).",
      "startOffset" : 196,
      "endOffset" : 216
    }, {
      "referenceID" : 37,
      "context" : "Designing task-specific pretraining/finetuning objectives could also be promising (Klein and Nabi, 2020; Ram et al., 2021).",
      "startOffset" : 82,
      "endOffset" : 122
    }, {
      "referenceID" : 55,
      "context" : "Designing task-specific pretraining/finetuning objectives could also be promising (Klein and Nabi, 2020; Ram et al., 2021).",
      "startOffset" : 82,
      "endOffset" : 122
    }, {
      "referenceID" : 52,
      "context" : ", with different scripts, small lexical overlap, or fewer common typological features (Pires et al., 2019; Wu and Dredze, 2020a), FS-XLT introduces crucial lexical and structural information to guide the update of embedding and transformer layers in mBERT.",
      "startOffset" : 86,
      "endOffset" : 128
    }, {
      "referenceID" : 69,
      "context" : ", with different scripts, small lexical overlap, or fewer common typological features (Pires et al., 2019; Wu and Dredze, 2020a), FS-XLT introduces crucial lexical and structural information to guide the update of embedding and transformer layers in mBERT.",
      "startOffset" : 86,
      "endOffset" : 128
    }, {
      "referenceID" : 69,
      "context" : "(5)The categorization based on resource availability is according to WikiSize (Wu and Dredze, 2020a).",
      "startOffset" : 78,
      "endOffset" : 100
    }, {
      "referenceID" : 40,
      "context" : "We see that many test words do occur in the bucket (shown in orange), in line with recent findings (Lewis et al., 2021; Elangovan et al., 2021).",
      "startOffset" : 99,
      "endOffset" : 143
    }, {
      "referenceID" : 17,
      "context" : "We see that many test words do occur in the bucket (shown in orange), in line with recent findings (Lewis et al., 2021; Elangovan et al., 2021).",
      "startOffset" : 99,
      "endOffset" : 143
    }, {
      "referenceID" : 52,
      "context" : "Pretrained multilingual encoders are shown to learn and store “language-agnostic” features (Pires et al., 2019; Zhao et al., 2020); §5.",
      "startOffset" : 91,
      "endOffset" : 130
    }, {
      "referenceID" : 79,
      "context" : "Pretrained multilingual encoders are shown to learn and store “language-agnostic” features (Pires et al., 2019; Zhao et al., 2020); §5.",
      "startOffset" : 91,
      "endOffset" : 130
    }, {
      "referenceID" : 46,
      "context" : "However, plain few-shot finetuning still relies heavily on unintended shallow lexical cues and shortcuts (Niven and Kao, 2019; Geirhos et al., 2020) that generalize poorly.",
      "startOffset" : 105,
      "endOffset" : 148
    }, {
      "referenceID" : 23,
      "context" : "However, plain few-shot finetuning still relies heavily on unintended shallow lexical cues and shortcuts (Niven and Kao, 2019; Geirhos et al., 2020) that generalize poorly.",
      "startOffset" : 105,
      "endOffset" : 148
    }, {
      "referenceID" : 6,
      "context" : "SotA few-shot learning methods (Chen et al., 2019; Wang et al., 2020; Tian et al., 2020; Dhillon et al., 2020) from computer vision consist of two stages: 1) training on base-class images, and 2) few-shot finetuning using new-class images.",
      "startOffset" : 31,
      "endOffset" : 110
    }, {
      "referenceID" : 65,
      "context" : "SotA few-shot learning methods (Chen et al., 2019; Wang et al., 2020; Tian et al., 2020; Dhillon et al., 2020) from computer vision consist of two stages: 1) training on base-class images, and 2) few-shot finetuning using new-class images.",
      "startOffset" : 31,
      "endOffset" : 110
    }, {
      "referenceID" : 63,
      "context" : "SotA few-shot learning methods (Chen et al., 2019; Wang et al., 2020; Tian et al., 2020; Dhillon et al., 2020) from computer vision consist of two stages: 1) training on base-class images, and 2) few-shot finetuning using new-class images.",
      "startOffset" : 31,
      "endOffset" : 110
    }, {
      "referenceID" : 12,
      "context" : "SotA few-shot learning methods (Chen et al., 2019; Wang et al., 2020; Tian et al., 2020; Dhillon et al., 2020) from computer vision consist of two stages: 1) training on base-class images, and 2) few-shot finetuning using new-class images.",
      "startOffset" : 31,
      "endOffset" : 110
    }, {
      "referenceID" : 65,
      "context" : "upon cosine similarity that imparts inductive bias about distance and is more effective than a fullyconnected classifier layer (FC) with smallK (Wang et al., 2020).",
      "startOffset" : 144,
      "endOffset" : 163
    }, {
      "referenceID" : 6,
      "context" : "Following (Chen et al., 2019; Wang et al., 2020; Tian et al., 2020), we freeze the embedding and transformer layers of mBERT, and",
      "startOffset" : 10,
      "endOffset" : 67
    }, {
      "referenceID" : 65,
      "context" : "Following (Chen et al., 2019; Wang et al., 2020; Tian et al., 2020), we freeze the embedding and transformer layers of mBERT, and",
      "startOffset" : 10,
      "endOffset" : 67
    }, {
      "referenceID" : 63,
      "context" : "Following (Chen et al., 2019; Wang et al., 2020; Tian et al., 2020), we freeze the embedding and transformer layers of mBERT, and",
      "startOffset" : 10,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "We leave further exploration of other possibilities of exploiting crosslingual features through collapse-preventing regularization (Aghajanyan et al., 2021) or contrastive learning (Gunel et al.",
      "startOffset" : 131,
      "endOffset" : 156
    }, {
      "referenceID" : 24,
      "context" : ", 2021) or contrastive learning (Gunel et al., 2021) to future work.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 58,
      "context" : "Integrating prompting (Brown et al., 2020; Schick and Schütze, 2020; Gao et al., 2020; Liu et al., 2021) – a strong performing few-shot learning methodology for NLP – into the crosslingual transfer learning pipeline is also a promising direction.",
      "startOffset" : 22,
      "endOffset" : 104
    }, {
      "referenceID" : 20,
      "context" : "Integrating prompting (Brown et al., 2020; Schick and Schütze, 2020; Gao et al., 2020; Liu et al., 2021) – a strong performing few-shot learning methodology for NLP – into the crosslingual transfer learning pipeline is also a promising direction.",
      "startOffset" : 22,
      "endOffset" : 104
    }, {
      "referenceID" : 43,
      "context" : "Integrating prompting (Brown et al., 2020; Schick and Schütze, 2020; Gao et al., 2020; Liu et al., 2021) – a strong performing few-shot learning methodology for NLP – into the crosslingual transfer learning pipeline is also a promising direction.",
      "startOffset" : 22,
      "endOffset" : 104
    } ],
    "year" : 2021,
    "abstractText" : "Few-shot crosslingual transfer has been shown to outperform its zero-shot counterpart with pretrained encoders like multilingual BERT. Despite its growing popularity, little to no attention has been paid to standardizing and analyzing the design of few-shot experiments. In this work, we highlight a fundamental risk posed by this shortcoming, illustrating that the model exhibits a high degree of sensitivity to the selection of few shots. We conduct a largescale experimental study on 40 sets of sampled few shots for six diverse NLP tasks across up to 40 languages. We provide an analysis of success and failure cases of few-shot transfer, which highlights the role of lexical features. Additionally, we show that a straightforward full model finetuning approach is quite effective for few-shot transfer, outperforming several state-of-the-art few-shot approaches. As a step towards standardizing few-shot crosslingual experimental designs, we make our sampled few shots publicly available.1",
    "creator" : "LaTeX with hyperref"
  }
}