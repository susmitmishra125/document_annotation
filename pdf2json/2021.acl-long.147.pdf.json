{
  "name" : "2021.acl-long.147.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Poisoning Knowledge Graph Embeddings via Relation Inference Patterns",
    "authors" : [ "Peru Bhardwaj", "John Kelleher", "Luca Costabello", "Declan O’Sullivan" ],
    "emails" : [ "peru.bhardwaj@adaptcentre.ie" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1875–1888\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1875"
    }, {
      "heading" : "1 Introduction",
      "text" : "Knowledge graph embeddings (KGE) are increasingly deployed in domains with high stake decision making like healthcare and finance (Noy et al., 2019), where it is critical to identify the potential security vulnerabilities that might cause failure. But the research on adversarial vulnerabilities of KGE models has received little attention. We study the adversarial vulnerabilities of KGE models through data poisoning attacks. These attacks craft input perturbations at training time that aim to subvert the learned model’s predictions at test time.\nPoisoning attacks have been proposed for models that learn from other graph modalities (Xu et al., 2020) but they cannot be applied directly to KGE models. This is because they rely on gradients of\n∗Equal contribution by last authors.\nall possible entries in a dense adjacency matrix and thus, do not scale to large knowledge graphs with multiple relations. The main challenge in designing poisoning attacks for KGE models is the large combinatorial search space of candidate perturbations which is of the order of millions for benchmark knowledge graphs with thousands of nodes. Two recent studies (Zhang et al., 2019a; Pezeshkpour et al., 2019) attempt to address this problem through random sampling of candidate perturbations (Zhang et al., 2019a) or through a vanilla auto-encoder that reconstructs discrete entities and relations from latent space (Pezeshkpour et al., 2019). However, random sampling depends on the number of candidates being sampled and the auto-encoder proposed in Pezeshkpour et al. (2019) is only applicable to multiplicative KGE models.\nIn this work, we propose to exploit the inductive abilities of KGE models to craft poisoned examples against the model. The inductive abilities of KGE models are expressed through different connectiv-\nity patterns like symmetry, inversion and composition between relations in the knowledge graph. We refer to these as inference patterns. We focus on the task of link prediction using KGE models and consider the adversarial goal of degrading the predicted rank of target missing facts. To degrade the ranks of target facts, we propose to carefully select a set of decoy facts and exploit the inference patterns to improve performance on this decoy set. Figure 1 shows an example of the use of composition pattern to degrade KGE model’s performance.\nWe explore a collection of heuristic approaches to select the decoy triples and craft adversarial perturbations that use different inference patterns to improve the model’s predictive performance on these decoy triples. Our solution addresses the challenge of large candidate space by breaking down the search space into smaller steps - (i) determining adversarial relations; (ii) determining the decoy entities that most likely violate an inference pattern; and (iii) determining remaining adversarial entities in the inference pattern that are most likely to improve the rank of decoy triples.\nWe evaluate the proposed attacks on four stateof-art KGE models with varied inductive abilities - DistMult, ComplEx, ConvE and TransE. We use two publicly available benchmark datasets for link prediction - WN18RR and FB15k-237. Comparison against the state-of-art poisoning attacks for KGE models shows that our proposed attacks outperform them in all cases. We find that the attacks based on symmetry pattern perform the best and generalize across all model-dataset combinations.\nThus, the main contribution of our research is an effective method to generate data poisoning attacks, which is based on inference patterns captured by KGE models. Through a novel reformulation of the problem of poisoning KGE models, we overcome the existing challenge in the scalability of poisoning attacks for KGE models. Furthermore, the extent of effectiveness of the attack relying on an inference pattern indicates the KGE model’s sensitivity to that pattern. Thus, our proposed poisoning attacks help in understanding the KGE models."
    }, {
      "heading" : "2 Problem Formulation",
      "text" : "For a set of entities E and a set of relations R, a knowledge graph is a collection of triples represented as KG = {(s, r, o) | s, o ∈ E and r ∈ R}, where s, r, o represent the subject, relation and object in a triple. A Knowledge Graph Em-\nbedding (KGE) model encodes entities and relations to a low-dimensional continuous vector space es, er, eo ∈ Rk where k is the embedding dimension. To do so, it uses a scoring function f : E × R × E → R which depends on the entity and relation embeddings to assign a score to each triple fsro = f(es, er, eo). Table 1 shows the scoring functions of state-of-art KGE models studied in this research. The embeddings are learned such that the scores for true (existing) triples in the knowledge graph are higher than the scores for false (non-existing) triples in the knowledge graph.\nMultiplicative vs Additive Interactions: The scoring functions of KGE models exhibit multiplicative or additive interactions (Chandrahas et al., 2018). The multiplicative models score triples through multiplicative interactions of subject, relation and object embeddings. The scoring function for these models can be expressed as fsro = e > r F(es, eo) where the function F measures the compatibility between the subject and object embeddings and varies across different models within this family. DistMult, ComplEx and ConvE have such interactions. On the other hand, additive models score triples through additive interactions of subject, relation and object embeddings. The scoring function for such models can be expressed as fsro = −\n∥∥M1r(es) + er −M2r(eo)∥∥ where es, eo ∈ RkE , er ∈ RkR and Mr ∈ RkE×kR is the projection matrix from entity space RkE to relation space RkR . TransE has additive interactions.\nInductive Capacity of KGE models: The general intuition behind the design of the scoring functions of KGE models is to capture logical properties between relations from the observed facts in the knowledge graph. These logical properties or inference patterns can then be used to make downstream inferences about entities and relations. For example, the relation is owned by\nis inverse of the relation owns, and when the fact (Account42, is owned by,Karl) is true, then the fact (Karl, owns, Account42) is also true and vice versa. A model that can capture inversion pattern can thus predict missing facts about owns based on observed facts about is owned by. The most studied inference patterns in the current literature are symmetry, inversion and composition since they occur very frequently in real-world knowledge graphs. In this work, we use these patterns to investigate the adversarial vulnerability of KGE models.\nLink Prediction: Since most of the existing knowledge graphs are incomplete, a standard use case of KGE models is to predict missing triples in the KG. This task is evaluated by an entity ranking procedure. Given a test triple (s, r, o), the subject entity is replaced by each entity from E in turn. These replacements are referred to as synthetic negatives. The KGE model’s scoring function is used to predict scores of these negative triples. The scores are then sorted in descending order and the rank of the correct entity is determined. These steps are repeated for the object entity of the triple.\nThe state-of-art evaluation metrics for this task are (i) MR which is the mean of the predicted ranks, (ii) MRR which is the mean of the reciprocals of predicted ranks and (iii) Hits@n which count the proportion of correct entities ranked in top-n. In the filtered setting (Bordes et al., 2013), negative triples that already exist in the training, validation or test set are filtered out. That is, their scores are ignored while computing the ranks. Depending on the domain of use, either subject or object or both ranks of the test triple are used to determine the model’s confidence1 in predicting a missing link.\nPoisoning Attacks on KGE models: We study poisoning attacks for the task of link prediction using KGE models. We focus on targeted attacks where the attacker targets a specific set of missing triples instead of the overall model performance. We use the notation (s, r, o) for the target triple; in this case, s, o are the target entities and r is the target relation. The goal of an adversarial attacker is to degrade the ranks of missing triples which are predicted highly plausible by the model. The rank of a highly plausible target triple can be degraded by improving the rank of less plausible decoy triples. For a target triple (s, r, o), the decoy triple for degrading the rank on object side would be (s, r, o′) and the decoy triple for degrading the\n1KGE models do not provide model uncertainty estimates.\nrank on subject side would be (s′, r, o). Thus, the aim of the adversarial attacker is to select decoy triples from the set of valid synthetic negatives and craft adversarial edits to improve their ranks. The attacker does not add the decoy triple itself as an adversarial edit, rather chooses the adversarial edits that would improve the rank of a missing decoy triple through an inference pattern.\nThreat Model: To ensure reliable vulnerability analysis, we use a white-box attack setting where the attacker has full knowledge of the target KGE model (Joseph et al., 2019). They cannot manipulate the model architecture or learned embeddings directly; but only through addition of triples to the training data. We focus on adversarial additions which are more challenging to design than adversarial deletions for sparse knowledge graphs2.\nAs in prior studies (Pezeshkpour et al., 2019; Zhang et al., 2019a), the attacker is restricted to making edits only in the neighbourhood of target entities. They are also restricted to 1 decoy triple for each entity of the target triple. Furthermore, because of the use of filtered settings for KGE evaluation, the attacker cannot add the decoy triple itself to the training data (which intuitively would be a way to improve the decoy triple’s rank)."
    }, {
      "heading" : "3 Poisoning Knowledge Graph Embeddings through Relation Inference Patterns",
      "text" : "Since the inference patterns on the knowledge graph specify a logic property between the relations, they can be expressed as Horn Clauses which is a subset of FOL formulae. For example, a property represented in the form ∀x, y : (x, owns, y)⇒ (y, is owned by, x) means that two entities linked by relation owns are also likely to be linked by the inverse relation is owned by. In this expression, the right hand side of the implication⇒ is referred to as the head and the left hand side as the body of the clause. Using such expressions, we define the three inference patterns used in our research.\nDefinition 3.1. The symmetry pattern Ps is expressed as ∀x, y : (x, r, y) ⇒ (y, r, x). Here, the relation r is symmetric relation.\n2For every target triple, the possible number of adversarial additions in the neighbourhood of each entity are E × R. For the benchmark dataset FB15k-237, this is of the order of millions; whereas the maximum number of candidates for adversarial deletion are of the order of thousands.\nDefinition 3.2. The inversion pattern Pi is expressed as ∀x, y : (x, ri, y) ⇒ (y, r, x). Here, the relations ri and r are inverse of each other.\nDefinition 3.3. The composition pattern Pc is expressed as ∀x, y, z : (x, r1, z) ∧ (z, r2, y) ⇒ (x, r, y). Here, the relation r is a composition of r1 and r2 ; and the ∧ is the conjunction operator from relational logic.\nThe mapping G : V → E of variables V in the above expressions to entities E is called a grounding. For example, we can map the logic expression ∀x, y : (x, owns, y) ⇒ (y, is owned by, x) to the grounding (Karl, owns, Account42) ⇒ (Account42, is owned by,Karl). Thus, a KGE model that captures the inversion pattern will assign a high prediction confidence to the head atom when the body of the clause exists in the graph.\nIn the above expressions, the decoy triple becomes the head atom and adversarial edits are the triples in the body of the expression. Since the decoy triple is an object or subject side negative of the target triple, the attacker already knows the relation in the head atom. They now want to determine (i) the adversarial relations in the body of the expression; (ii) the decoy entities which will most likely violate the inference pattern for the chosen relations and; (iii) the remaining entities in the body of the expression which will improve the prediction on the chosen decoy triple. Notice that the attacker needs all three steps for composition pattern only; for inversion pattern, only the first two steps are needed; and for symmetry pattern, only the second step is needed. Below we describe each step in detail. A computational complexity analysis of all the steps is available in Appendix A."
    }, {
      "heading" : "3.1 Step1: Determine Adversarial Relations",
      "text" : "Expressing the relation patterns as logic expressions is based on relational logic and assumes that the relations are constants. Thus, we use an algebraic approach to determine the relations in the head and body of a clause. Given the target relation r, we determine the adversarial relations using an algebraic model of inference (Yang et al., 2015).\nInversion: If an atom (x, r, y) holds true, then for the learned embeddings in multiplicative models, we can assume ex ◦ er ≈ ey; where ◦ denotes the Hadamard (element-wise) product. If the atom (y, ri, x) holds true as well, then we can also assume ey ◦eri ≈ ex. Thus, er ◦eri ≈ 1 for inverse relations r and ri when embeddings are learned\nfrom multiplicative models. We obtain a similar expression er + eri ≈ 0 when embeddings are learned from additive models.\nThus, to determine adversarial relations for inversion pattern, we use the pre-trained embeddings to select ri that minimizes | erieTr − 1 | for multiplicative models; and ri that minimizes | eri+er | for additive models.\nComposition: If two atoms (x, r1, y) and (y, r2, z) hold true, then for multiplicative models, ex ◦ er1 ≈ ey and ey ◦ er2 ≈ ez. Therefore, ex ◦ (er1 ◦er2) ≈ ez . Hence, relation r is a composition of r1 and r2 if er1 ◦ er2 ≈ er. Similarly, for embeddings from additive models, we can model composition as er1 + er2 ≈ er.\nThus, to determine adversarial relations for composition pattern, we use pre-trained embeddings to obtain all possible compositions of (r1, r2). For multiplicative models, we use er1 ◦ er2 and for additive models we use er1 + er2 . From these, we choose the relation pair for which the Euclidean distance between the composed relation embeddings and the target relation embedding er is minimum."
    }, {
      "heading" : "3.2 Step2: Determine Decoy Entities",
      "text" : "We consider three different heuristic approaches to select the decoy entity - soft truth score, ranks predicted by the KGE model and cosine distance.\nSoft Logical Modelling of Inference Patterns Once the adversarial relations are determined, we can express the grounding for symmetry, inversion and composition patterns for the decoy triples. We discuss only object side decoy triple for brevity -\nGs : (o′, r, s)⇒ (s, r, o′) Gi : (o′, ri, s)⇒ (s, r, o′)\nGc : (s, r1, o′′) ∧ (o′′, r2, o′)⇒ (s, r, o′)\nIf the model captures Ps, Pi or Pc to assign high rank to the target triple, then the head atom (s, r, o′) of a grounding that violates this pattern is a suitable decoy triple. Adding the body of this grounding to the knowledge graph would improve the model performance on decoy triple through Ps, Pi or Pc.\nTo determine the decoy triple this way, we need a measure of the degree to which a grounding satisfies an inference pattern. We call this measure the soft truth score φ : G → [0, 1] - it provides the truth value of a logic expression indicating the degree to which the expression is true. We model the soft truth score of grounded patterns using t-norm based fuzzy logics (Hájek, 1998).\nThe score fsro of an individual atom (i.e. triple) is computed using the KGE model’s scoring function. We use the sigmoid function σ(x) = 1/(1 + exp(−x)) to map this score to a continuous truth value in the range (0, 1). Hence, the soft truth score for an individual atom is φ(s, r, o) = σ(fsro). The soft truth score for the grounding of a pattern can then be expressed through logical composition (e.g. ∧ and⇒) of the scores of individual atoms in the grounding. We follow (Guo et al., 2016, 2018) and define the following compositions for logical conjunction (∧), disjunction (∨), and negation (¬):\nφ(a ∧ b) = φ(a) · φ(b), φ(a ∨ b) = φ(a) + φ(b)− φ(a) · φ(b), φ(¬a) = 1− φ(a).\nHere, a and b are two logical expressions, which can either be single triples or be constructed by combining triples with logical connectives. If a is a single triple (s, r, o), we have φ(a) = φ(s, r, o). Given these compositions, the truth value of any logical expression can be calculated recursively (Guo et al., 2016, 2018).\nThus, we obtain the following soft truth scores for the groundings of symmetry, inversion and composition patterns Gs, Gi and Gc -\nφ(Gs) = φ(o′, r, s) · φ(s, r, o′)− φ(o′, r, s) + 1 φ(Gi) = φ(o′, ri, s) · φ(s, r, o′)− φ(o′, ri, s) + 1. φ(Gc) = φ(s, r1, o′′) · φ(o′′, r2, o′) · φ(s, r, o′)\n− φ(s, r1, o′′) · φ(o′′, r2, o′) + 1\nTo select the decoy triple (s, r, o′) for symmetry and inversion, we score all possible groundings using φ(Gs) and φ(Gi). The head atom of grounding with minimum score is chosen as decoy triple.\nFor composition pattern, the soft truth score φ(Gc) for candidate decoy triples (s, r, o′) contains two entities (o′, o′′) to be identified. Thus, we use a greedy approach to select the decoy entity o′. We use the pre-trained embeddings to group the entities o′′ into k clusters using K-means clustering and determine a decoy entity with minimum soft truth score for each cluster. We then select the decoy entity o′ with minimum score across the k clusters.\nKGE Ranks: We use the ranking protocol from KGE evaluation to rank the target triple against valid subject and object side negatives (s′, r, o) and (s, r, o′). For each side, we select the negative triple that is ranked just below the target triple (that\nis, negative rank = target rank + 1). These are suitable as decoy because their predicted scores are likely not very different from the target triple’s score. Thus, the model’s prediction confidence for these triples might be effectively manipulated through adversarial additions. This is in contrast to very low ranked triples as decoy; where the model has likely learnt a low score with high confidence.\nCosine Distance: A high rank for the target triple (s, r, o) against queries (s, r, ?) and (?, r, o) indicates that es, eo are similar to the embeddings of other subjects and objects related by r in the training data. Thus, a suitable heuristic for selecting decoy entities s′ and o′ is to choose ones whose embeddings are dissimilar to es, eo. Since these entities are not likely to occur in the neighbourhood of o and s, they will act adversarially to reduce the rank of target triple. Thus, we select decoy entities s′ and o′ that have maximum cosine distance from target entities s and o respectively."
    }, {
      "heading" : "3.3 Step3: Determine Adversarial Entities",
      "text" : "This step is only needed for the composition pattern because the body for this pattern has two adversarial triples. Given the decoy triple in the head of the composition expression, we select the body of the expression that would maximize the rank of the decoy triple. We use the soft-logical model defined in Step 2 for selecting decoy triples. The soft truth score for composition grounding of decoy triple is given by φ(Gt) = φ(s, r1, o′′) · φ(o′′, r2, o′) · φ(s, r, o′) − φ(s, r1, o′′) · φ(o′′, r2, o′) + 1. We select the entity o′′ with maximum score because this entity satisfies the composition pattern for the decoy triple and is thus likely to improve the decoy triple’s ranks on addition to the knowledge graph."
    }, {
      "heading" : "4 Evaluation",
      "text" : "The aim of our evaluation is to assess the effectiveness of proposed attacks in degrading the predictive performance of KGE models on missing triples that are predicted true. We use the state-of-art evaluation protocol for data poisoning attacks (Xu et al., 2020). We train a clean model on the original data; then generate the adversarial edits and add them to the dataset; and finally retrain a new model on this poisoned data. All hyperparameters for training on original and poisoned data remain the same.\nWe evaluate four models with varying inductive abilities - DistMult, ComplEx, ConvE and TransE; on two publicly available benchmark datasets for link prediction3- WN18RR and FB15k-237. We filter out triples from the validation and test set that contain unseen entities. To assess the attack effectiveness in degrading performance on triples predicted as true, we need a set of triples that are predicted as true by the model. Thus, we select as target triples, a subset of the original test set where each triple is ranked ≤ 10 by the original model. Table 3 provides an overview of dataset statistics and the number of target triples selected.\nBaselines: We compare the proposed methods against the following baselines -\nRandom n: Random edits in the neighbourhood of each entity of the target triple.\nRandom g1: Global random edits in the knowledge graph which are not restricted to the neighbourhood of entities in the target triple and have 1 edit per decoy triple (like symmetry and inversion).\nRandom g2: Global random edits in the knowledge graph which are not restricted to the neigh-\n3https://github.com/TimDettmers/ConvE\nbourhood of entities in the target triple and have 2 edits per decoy triple (like composition).\nZhang et al.: Poisoning attack from (Zhang et al., 2019a) for edits in the neighbourhood of subject of the target triple. We extend it for both subject and object to match our evaluation protocol. Further implementation details available in Appendix B.2.\nCRIAGE: Poisoning attack from (Pezeshkpour et al., 2019). We use the publicly available implementation and the default attack settings4. The method was proposed for edits in the neighbourhood of object of the target triple. We extend it for both entities to match our evaluation protocol and to ensure fair evaluation.\nImplementation: For every attack, we filter out adversarial edit candidates that already exist in the graph.We also remove duplicate adversarial edits for different targets before adding them to the original dataset. For Step 2 of the composition attack with ground truth, we use the elbow method to determine the number of clusters for each model-data combination. Further details on KGE model training, computing resources and number of clusters are available in Appendix B. The source code to reproduce our experiments is available on GitHub5."
    }, {
      "heading" : "4.1 Results",
      "text" : "Table 4 and 5 show the reduction in MRR and Hits@1 due to different attacks on the WN18RR and FB15k-237 datasets. We observe that the proposed adversarial attacks outperform the random baselines and the state-of-art poisoning attacks for all KGE models on both datasets.\nWe see that the attacks based on symmetry inference pattern perform the best across all modeldataset combinations. This indicates the sensitivity of KGE models to symmetry pattern. For DistMult, ComplEx and ConvE, this sensitivity can be explained by the symmetric nature of the scoring functions of these models. That is, the models assign either equal or similar scores to triples that are symmetric opposite of each other. In the case of TransE, the model’s sensitivity to symmetry pattern is explained by the translation operation in scoring function. The score of target (s, r, o) is a translation from subject to object embedding through the relation embedding. Symmetry attack adds the adversarial triple (o′, r, s) where the relation is same\n4https://github.com/pouyapez/criage 5https://github.com/PeruBhardwaj/\nInferenceAttack\nas the target relation, but target subject is the object of adversarial triple. Now, the model learns the embedding of s as a translation from o′ through relation r. This adversarially modifies the embedding of s and in turn, the score of (s, r, o).\nWe see that inversion and composition attacks also perform better than baselines in most cases, but not as good as symmetry. This is particularly true for FB15k-237 where the performance for these patterns is similar to random baselines. For the composition pattern, it is likely that the model has stronger bias for shorter and simpler patterns like symmetry and inversion than for composition. This makes it harder to deceive the model through composition than through symmetry or inverse. Furthermore, FB15k-237 has high connectivity (Dettmers et al., 2018) which means that a KGE model relies on a high number of triples to learn target triples’ ranks. Thus, poisoning KGE models for FB15k237 will likely require more adversarial triples per target triple than that considered in this research.\nThe inversion pattern is likely ineffective on the benchmark datasets because these datasets do not have any inverse relations (Dettmers et al., 2018; Toutanova and Chen, 2015). This implies that our attacks cannot identify the inverse of the target triple’s relation in Step 1. We investigate this hypothesis further in Appendix D, and evaluate the attacks on WN18 dataset where the inverse relations have not been filtered out. This means that the KGE model can learn the inversion pattern and\nthe inversion attacks can identify the inverse of the target relation. In this setting, we find that the inversion attacks outperform other attacks against ComplEx on WN18, indicating the sensitivity of ComplEx to the inversion pattern when the dataset contains inverse relations.\nAn exception in the results is the composition pattern on TransE where the model performance improves instead of degrading on the target triples. This is likely due to the model’s sensitivity to composition pattern such that adding this pattern improves the performance on all triples, including target triples. To verify this, we checked the change in ranks of decoy triples and found that composition attacks on TransE improve these ranks too. Results for this experiment are available in Appendix C. This behaviour of composition also indicates that the selection of adversarial entities in Step 3 of the composition attacks can be improved. It also explains why the increase is more significant for WN18RR than FB15k-237 - WN18RR does not have any composition relations but FB15k-237 does; so adding these to WN18RR shows significant improvement in performance. We aim to investigate these and more hypotheses about the proposed attacks in future work."
    }, {
      "heading" : "5 Related Work",
      "text" : "KGE models can be categorized into tensor factorization models like DistMult (Yang et al., 2015) and ComplEx (Trouillon et al., 2016), neural archi-\ntectures like ConvE (Dettmers et al., 2018) and translational models like TransE (Bordes et al., 2013). We refer the reader to (Cai et al., 2018) for a comprehensive survey. Due to the black-box nature of KGE models, there is an emerging literature on understanding these models. (Pezeshkpour et al., 2019) and (Zhang et al., 2019a) are most closely related to our work as they propose other data poisoning attacks for KGE models.\nMinervini et al. (2017) and Cai and Wang (2018) use adversarial regularization in latent space and adversarial training to improve predictive performance on link prediction. But these adversarial samples are not in the input domain and aim to improve instead of degrade model performance. Poisoning attacks have also been proposed for models for undirected and single relational graph data like Graph Neural Networks (Zügner et al., 2018; Dai et al., 2018) and Network Embedding models (Bojchevski and Günnemann, 2019). A survey of poisoning attacks for graph data is available in (Xu et al., 2020). But the attacks for these models cannot be applied directly to KGE models because they require gradients of a dense adjacency matrix.\nIn the literature besides adversarial attacks, Lawrence et al. (2020), Nandwani et al. (2020) and Zhang et al. (2019b) generate post-hoc explanations to understand KGE model predictions. Trouillon et al. (2019) study the inductive abilities of KGE models as binary relation properties for controlled inference tasks with synthetic datasets. Allen et al. (2021) interpret the structure of knowl-\nedge graph embeddings by comparison with word embeddings. On the theoretical side, Wang et al. (2018) study the expressiveness of various bilinear KGE models and Gutiérrez-Basulto and Schockaert (2018) study the ability of KGE models to learn hard rules expressed as ontological knowledge.\nThe soft-logical model of inference patterns in this work is inspired by the literature on injecting logical rules into KGE models. Guo et al. (2016) and Guo et al. (2018) enforce soft logical rules by modelling the triples and rules in a unified framework and jointly learning embeddings from them. Additionally, our algebraic model of inference patterns, which is used to select adversarial relations, is related to approaches for graph traversal in latent vector space discussed in Yang et al. (2015); Guu et al. (2015); Arakelyan et al. (2021)."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We propose data poisoning attacks against KGE models based on inference patterns like symmetry, inversion and composition. Our experiments show that the proposed attacks outperform the state-ofart attacks. Since the attacks rely on relation inference patterns, they can also be used to understand the KGE models. This is because if a KGE model is sensitive to a relation inference pattern, then that pattern should be an effective adversarial attack. We observe that the attacks based on symmetry pattern generalize across all KGE models which indicates their sensitivity to this pattern.\nIn the future, we aim to investigate hypotheses about the effect of input graph connectivity and existence of specific inference patterns in datasets. We note that such investigation of inference pattern attacks will likely be influenced by the choice of datasets. In this paper, we have used benchmark datasets for link prediction. While there are intuitive assumptions about the inference patterns on these datasets, there is no study that formally measures and characterizes the existence of these patterns. This makes it challenging to verify the claims made about the inductive abilities of KGE models, not only by our proposed attacks but also by new KGE models proposed in the literature.\nThus, a promising step in understanding knowledge graph embeddings is to propose datasets and evaluation tasks that test varying degrees of specific inductive abilities. These will help evaluate new models and serve as a testbed for poisoning attacks. Furthermore, specifications of model performance on datasets with different inference patterns will improve the usability of KGE models in high-stake domains like healthcare and finance.\nIn addition to understanding model behaviour, the sensitivity of state-of-art KGE models to simple inference patterns indicates that these models can introduce security vulnerabilities in pipelines that use knowledge graph embeddings. Thus, another promising direction for future work is towards mitigating the security vulnerabilities of KGE models. Some preliminary ideas for this research can look into adversarial training; or training an ensemble of different KGE scoring functions; or training an ensemble from subsets of the training dataset. Since our experiments show that state-of-art KGE models are sensitive to symmetry pattern, we call for future research to investigate neural architectures that generalize beyond symmetry even though their predictive performance for link prediction on benchmark datasets might not be the best."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This research was conducted with the financial support of Accenture Labs and Science Foundation Ireland (SFI) at the ADAPT SFI Research Centre at Trinity College Dublin. The ADAPT SFI Centre for Digital Content Technology is funded by Science Foundation Ireland through the SFI Research Centres Programme and is co-funded under the European Regional Development Fund (ERDF) through Grant No. 13/RC/2106 P2.\nBroader Impact\nWe study the problem of generating data poisoning attacks on KGE models. Data poisoning attacks identify the vulnerabilities in learning algorithms that could be exploited by an adversary to manipulate the model’s behaviour (Joseph et al., 2019; Biggio and Roli, 2018). Such manipulation can lead to unintended model behaviour and failure. Identifying these vulnerabilities for KGE models is critical because of their increasing use in domains that need high stakes decision making like heathcare (Bendtsen and Petrovski, 2019) and finance (Hogan et al., 2020; Noy et al., 2019). In this way, our research is directed towards minimizing the negative consequences of deploying state-ofart KGE models in our society. This honours the ACM code of Ethics of contributing to societal well-being and acknowledging that all people are stakeholders in computing. At the same time, we aim to safeguard the KGE models against potential harm from adversaries and thus honour the ACM code of avoiding harm due to computing systems.\nArguably, because we study vulnerabilities by attacking the KGE models, the proposed attacks can be used by an actual adversary to manipulate the model behaviour of deployed systems. This paradox of an arms race is universal across security research (Biggio and Roli, 2018). For our research, we have followed the principle of proactive security as recommended by Joseph et al. (2019) and Biggio and Roli (2018). As opposed to reactive security measures where learning system designers develop countermeasures after the system is attacked, a proactive approach anticipates such attacks, simulates them and designs countermeasures before the systems are deployed. Thus, by revealing the vulnerabilities of KGE models, our research provides an opportunity to fix them.\nBesides the use case of security, our research can be used in understanding the inductive abilities of KGE models, which are black-box and hard to interpret. We design attacks that rely on the inductive assumptions of a model to be able to deceive that model. Thus, theoretically, the effectiveness of attacks based on one inference pattern over another indicates the model’s reliance on one inference pattern over another. However, as we discussed in our paper, realistically, it is challenging to make such claims about the inductive abilities of KGE models because the inference patterns in benchmark datasets are not well defined.\nThus, we would encourage further work to evaluate our proposed attacks by designing benchmark tasks and datasets that measure specific inductive abilities of models. This will not only be useful for evaluating the proposed attacks here, but also for understanding the inductive abilities of existing KGE models. This in turn, can guide the community to design better models. In this direction, we encourage researchers proposing new KGE models to evaluate not only the predictive performance on benchmark datasets, but also the claims made on inductive abilities of these models and their robustness to violations of these implicit assumptions."
    }, {
      "heading" : "A Computational Complexity Analysis",
      "text" : "Lets say E is the set of entities andR is the set of relations. The number of target triples to attack is t and the specific target triple is (s, r, o). Here, we discuss the computational complexity of the three steps of the proposed attacks -\nDetermine Adversarial Relations: In this step, we determine the inverse relation or the composition relation of a target triple. To select inverse relation, we needR computations for every target triple. Selecting composition relation requires the composition operation R2 times per target triple. To avoid repetition, we pre-compute the inverse and composition relations for all target triples. This gives the complexity O(R2) for inverse relation. For composition relation, we compute compositions of all relation pairs and then select the adversarial pair by comparison with target relation. This gives O(R2 +R) complexity for composition.\nDetermine Decoy Entity: The three heuristics to compute the decoy entity are soft-truth score, KGE ranks and cosine distance. For symmetry and inversion, the soft truth score requires 2 forward calls to the model for one decoy entity. For composition, if the number of clusters is k, the soft truth score requires 3k forward calls to the model. To select decoy entities based on KGE ranks, we require one forward call for each decoy entity. For cosine distance, we compute the similarity of s and o to all entities via two calls to Pytorch’s F.cosine similarity. Once the heuristic scores are computed, there is an additional complexity of O(E) to select the entity with minimum\nscore. Thus, the complexity for decoy selection is O(tE) for all heuristics except soft truth score on composition where it is O(ktE).\nDetermine Adversarial Entity: This step requires three forward calls to the KGE model because the ground truth score needs to be computed. Thus, the complexity for this step is O(tE).\nBased on the discussion above, the overall computational complexity is O(tE) for symmetry attacks and O(R2 + tE) for inversion attacks. For composition attacks, it isO(R2+R+ktE) for soft truth score and O(R2 +R + tE) for KGE ranks and cosine distance.\nB Implementation Details\nB.1 Training KGE models Our codebase6 for KGE model training is based on the codebase from (Dettmers et al., 2018)7. We use the 1-K training protocol but without reciprocal relations. Each training step alternates through batches of (s,r) and (o,r) pairs and their labels. The model implementation uses an if-statement for the forward pass conditioned on the input batch mode.\nFor TransE scoring function, we use L2 norm and a margin value of 9.0. The loss function used for all models is Pytorch’s BCELosswithLogits. For regularization, we use label smoothing and L2 regularization for TransE; and input dropout with label smoothing for remaining models. We also use hidden dropout and feature dropout for ConvE.\nWe do not use early stopping to ensure same hyperparameters for original and poisoned KGE models. We used an embedding size of 200 for all models on both datasets. For ComplEx, this becomes an embedding size of 400 because of the real and imaginary parts of the embeddings. All hyperparameters are tuned manually based on suggestions from state-of-art implementations of KGE models (Ruffinelli et al., 2020; Dettmers et al., 2018). The hyperparameter values for all model dataset combinations are available in the codebase. Table 6 shows the MRR and Hits@1 for the original KGE models on WN18RR and FB15k-237.\nFor re-training the model on poisoned dataset, we use the same hyperparameters as the original model. We run all model training, adversarial attacks and evaluation on a shared HPC cluster with Nvidia RTX 2080ti, Tesla K40 and V100 GPUs.\n6https://github.com/PeruBhardwaj/ InferenceAttack\n7https://github.com/TimDettmers/ConvE\nB.2 Baseline Implementation Details\nOne of the baselines in our evaluation is the attack from (Zhang et al., 2019a). It proposed edits in the neighbourhood of subject of the target triple. We extend it for both subject and object to match our evaluation protocol. Since no public implementation is available, we implement our own.\nThe attack is based on computing a perturbation score for all possible candidate additions. Since the search space for candidate additions is of the order E ×R, the attack uses random down sampling to filter out the candidates. The percent of triples down sampled are not reported in the original paper and the implementation is not available. So, in this paper, we pick a high and a low value of the percentage of triples down sampled and generate adversarial edits for both fractions. The high and low percent values that were used to select candidate adversarial additions for WN18RR are DistMult: (20.0, 5.0); ComplEx: (20.0, 5.0); ConvE: (2.0, 0.1); TransE: (20.0, 5.0). For FB15k-237, these values are DistMult: (20.0, 5.0); ComplEx: (15.0, 5.0); ConvE: (0.3, 0.1); TransE: (20.0, 5.0)\nThus, we generate two poisoned datasets from the attack - one that used a high number of candi-\ndates and another that used a low number of candidates. We train two separate KGE models on these datasets to assess attack performance. Table 7 shows the MRR of the original model; and poisoned KGE models from attack with high and low downsampling percents. The results reported for this attack’s performance in Section 4.1 are the better of the two results (which show more degradation in performance) for each combination.\nB.3 Attack Implementation Details\nOur proposed attacks involve three steps to generate the adversarial additions for all target triples. For step1 of selection of adversarial relations, we pre-compute the inversion and composition relations for all target triples. Step2 and Step3 are computed for each target triple in a for loop. These steps involve forward calls to KGE models to score adversarial candidates. For this, we use a vectorized implementation similar to KGE evaluation protocol. We also filter out the adversarial candidates that already exist in the training set. We further filter out any duplicates from the set of adversarial triples generated for all target triples.\nFor the composition attacks with soft-truth score, we use the KMeans clustering implementation from scikit− learn. We use the elbow method on the grid [5, 20, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500] to select the number of clusters. The number of clusters selected for WN18RR are DistMult: 300, ComplEx: 100, ConvE: 300, TransE: 50. For FB15k-237, the numbers are DistMult:\n200, ComplEx: 300, ConvE: 300, TransE: 100."
    }, {
      "heading" : "C Analysis on Decoy Triples",
      "text" : "The proposed attacks are designed to generate adversarial triples that improve the KGE model performance on decoy triples (s, r, o′) and (s′, r, o). In this section, we analyze whether the performance of KGE models improves or degrades over decoy triples after poisoning. For the decoy triples on object side (s, r, o′), we compute the change in object side MRR relative to the original object side MRR of these triples. Similarly, for the decoy triples on subject side (s′, r, o), we compute the change in subject side MRR relative to the original subject side MRR of these decoy triples. Figure 2 shows plots for the mean change in MRR of object and subject side decoy triples.\nWe observed in Section 4.1 that the composition attacks against TransE on WN18RR improved the performance on target triples instead of degrading it. In Figure 2, we notice that composition attacks against TransE are effective in improving the ranks of decoy triples on both WN18RR and FB15k-237. This evidence supports the argument made in the main paper - it is likely that the composition attack does not work against TransE for WN18RR because the original dataset does not contain any composition relations; thus adding this pattern improves model’s performance on all triples instead of just the target triples because of the sensitivity of TransE to composition pattern."
    }, {
      "heading" : "D Analysis on WN18",
      "text" : "The inversion attacks identify the relation that the KGE model might have learned as inverse of the target triple’s relation. But the benchmark datasets WN18RR and FB15k-237 do not contain inverse relations, and a KGE model trained on these clean datasets would not be vulnerable to inversion attacks. Thus, we perform additional evaluation on the WN18 dataset where triples with inverse relations have not been removed. Table 8 shows the results for different adversarial attacks on WN18.\nWe see that the symmetry based attack is most effective for DistMult, ConvE and TransE. This indicates the sensitivity of these models to the symmetry pattern even when inverse relations are present in the dataset. For DistMult and ConvE, this is likely due to the symmetric nature of their scoring functions; and for TransE, this is likely because of the translation operation as discussed in Section 4.1. On the ComplEx model, we see that though the symmetry attacks are more effective than random baselines, the inversion attacks are the most effective. This indicates that the ComplEx model is most sensitive to the inversion pattern when the input dataset contains inverse relations."
    }, {
      "heading" : "E Analysis of Runtime Efficiency",
      "text" : "In this section, we compare the runtime efficiency of the baseline and proposed attacks. Table 9 shows the time taken (in seconds) to select the adversarial triples using different attack strategies for all\nmodels on WN18 dataset. Similar patterns were observed for attack execution on other datasets.\nFor CRIAGE, the reported time does not include the time taken to train the auto-encoder model. Similarly, for soft-truth based composition attacks, the reported time does not include the time taken to pre-compute the clusters. We observe that the proposed attacks are more efficient than the baseline Zhang et al. attack which requires a combinatorial search over the canidate adversarial triples; and have comparable efficiency to CRIAGE. Among the different proposed attacks, composition attacks based on soft-truth score take more time than others because they select the decoy entity by computing the soft-truth score for multiple clusters."
    } ],
    "references" : [ {
      "title" : "Interpreting knowledge graph relation representation from word embeddings",
      "author" : [ "Carl Allen", "Ivana Balazevic", "Timothy Hospedales." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Allen et al\\.,? 2021",
      "shortCiteRegEx" : "Allen et al\\.",
      "year" : 2021
    }, {
      "title" : "Complex query answering with neural link predictors",
      "author" : [ "Erik Arakelyan", "Daniel Daza", "Pasquale Minervini", "Michael Cochez." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Arakelyan et al\\.,? 2021",
      "shortCiteRegEx" : "Arakelyan et al\\.",
      "year" : 2021
    }, {
      "title" : "How data and ai are helping unlock the secrets of disease",
      "author" : [ "Claus Bendtsen", "Slavé Petrovski." ],
      "venue" : "AstraZeneca Blog.",
      "citeRegEx" : "Bendtsen and Petrovski.,? 2019",
      "shortCiteRegEx" : "Bendtsen and Petrovski.",
      "year" : 2019
    }, {
      "title" : "Wild patterns: Ten years after the rise of adversarial machine learning",
      "author" : [ "Battista Biggio", "Fabio Roli." ],
      "venue" : "Pattern Recognition, 84:317–331.",
      "citeRegEx" : "Biggio and Roli.,? 2018",
      "shortCiteRegEx" : "Biggio and Roli.",
      "year" : 2018
    }, {
      "title" : "Adversarial attacks on node embeddings via graph poisoning",
      "author" : [ "Aleksandar Bojchevski", "Stephan Günnemann." ],
      "venue" : "International Conference on Machine Learning, pages 695–704. PMLR.",
      "citeRegEx" : "Bojchevski and Günnemann.,? 2019",
      "shortCiteRegEx" : "Bojchevski and Günnemann.",
      "year" : 2019
    }, {
      "title" : "Translating embeddings for modeling multirelational data",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Alberto GarciaDuran", "Jason Weston", "Oksana Yakhnenko." ],
      "venue" : "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger,",
      "citeRegEx" : "Bordes et al\\.,? 2013",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2013
    }, {
      "title" : "A comprehensive survey of graph embedding: Problems, techniques, and applications",
      "author" : [ "Hongyun Cai", "Vincent W Zheng", "Kevin ChenChuan Chang." ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering.",
      "citeRegEx" : "Cai et al\\.,? 2018",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2018
    }, {
      "title" : "KBGAN: Adversarial learning for knowledge graph embeddings",
      "author" : [ "Liwei Cai", "William Yang Wang." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-",
      "citeRegEx" : "Cai and Wang.,? 2018",
      "shortCiteRegEx" : "Cai and Wang.",
      "year" : 2018
    }, {
      "title" : "Towards understanding the geometry of knowledge graph embeddings",
      "author" : [ "Chandrahas", "Aditya Sharma", "Partha Talukdar." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Chandrahas et al\\.,? 2018",
      "shortCiteRegEx" : "Chandrahas et al\\.",
      "year" : 2018
    }, {
      "title" : "Adversarial attack on graph structured data",
      "author" : [ "Hanjun Dai", "Hui Li", "Tian Tian", "Xin Huang", "Lin Wang", "Jun Zhu", "Le Song." ],
      "venue" : "International conference on machine learning, pages 1115–1124. PMLR.",
      "citeRegEx" : "Dai et al\\.,? 2018",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2018
    }, {
      "title" : "Convolutional 2d knowledge graph embeddings",
      "author" : [ "Tim Dettmers", "Pasquale Minervini", "Pontus Stenetorp", "Sebastian Riedel." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.",
      "citeRegEx" : "Dettmers et al\\.,? 2018",
      "shortCiteRegEx" : "Dettmers et al\\.",
      "year" : 2018
    }, {
      "title" : "Jointly embedding knowledge graphs and logical rules",
      "author" : [ "Shu Guo", "Quan Wang", "Lihong Wang", "Bin Wang", "Li Guo." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 192–202, Austin, Texas. Associa-",
      "citeRegEx" : "Guo et al\\.,? 2016",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2016
    }, {
      "title" : "Knowledge graph embedding with iterative guidance from soft rules",
      "author" : [ "Shu Guo", "Quan Wang", "Lihong Wang", "Bin Wang", "Li Guo." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.",
      "citeRegEx" : "Guo et al\\.,? 2018",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2018
    }, {
      "title" : "From knowledge graph embedding to ontology embedding? an analysis of the compatibility between vector space representations and rules. In Principles of Knowledge Representation and Reasoning",
      "author" : [ "Vı́ctor Gutiérrez-Basulto", "Steven Schockaert" ],
      "venue" : null,
      "citeRegEx" : "Gutiérrez.Basulto and Schockaert.,? \\Q2018\\E",
      "shortCiteRegEx" : "Gutiérrez.Basulto and Schockaert.",
      "year" : 2018
    }, {
      "title" : "Traversing knowledge graphs in vector space",
      "author" : [ "Kelvin Guu", "John Miller", "Percy Liang." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 318–327, Lisbon, Portugal. Association for Compu-",
      "citeRegEx" : "Guu et al\\.,? 2015",
      "shortCiteRegEx" : "Guu et al\\.",
      "year" : 2015
    }, {
      "title" : "Metamathematics of Fuzzy Logic, volume 4",
      "author" : [ "Petr Hájek." ],
      "venue" : "Springer Science & Business Media.",
      "citeRegEx" : "Hájek.,? 1998",
      "shortCiteRegEx" : "Hájek.",
      "year" : 1998
    }, {
      "title" : "Knowledge graphs",
      "author" : [ "Rula", "Lukas Schmelzeisen", "Juan F. Sequeda", "Steffen Staab", "Antoine Zimmermann." ],
      "venue" : "CoRR, abs/2003.02320.",
      "citeRegEx" : "Rula et al\\.,? 2020",
      "shortCiteRegEx" : "Rula et al\\.",
      "year" : 2020
    }, {
      "title" : "Adversarial Machine Learning",
      "author" : [ "Anthony D. Joseph", "Blaine Nelson", "Benjamin I.P. Rubinstein", "J.D. Tygar." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Joseph et al\\.,? 2019",
      "shortCiteRegEx" : "Joseph et al\\.",
      "year" : 2019
    }, {
      "title" : "Explaining neural matrix factorization with gradient rollback",
      "author" : [ "Carolin Lawrence", "T. Sztyler", "Mathias Niepert." ],
      "venue" : "ArXiv, abs/2010.05516.",
      "citeRegEx" : "Lawrence et al\\.,? 2020",
      "shortCiteRegEx" : "Lawrence et al\\.",
      "year" : 2020
    }, {
      "title" : "Adversarial sets for regularising neural link predictors",
      "author" : [ "Pasquale Minervini", "Thomas Demeester", "Tim Rocktäschel", "Sebastian Riedel." ],
      "venue" : "Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI 2017,",
      "citeRegEx" : "Minervini et al\\.,? 2017",
      "shortCiteRegEx" : "Minervini et al\\.",
      "year" : 2017
    }, {
      "title" : "OxKBC: Outcome explanation for factorization based knowledge base completion",
      "author" : [ "Yatin Nandwani", "Ankesh Gupta", "Aman Agrawal", "Mayank Singh Chauhan", "Parag Singla", "Mausam." ],
      "venue" : "Automated Knowledge Base Construction.",
      "citeRegEx" : "Nandwani et al\\.,? 2020",
      "shortCiteRegEx" : "Nandwani et al\\.",
      "year" : 2020
    }, {
      "title" : "Industry-scale knowledge graphs: Lessons and challenges",
      "author" : [ "Natasha Noy", "Yuqing Gao", "Anshu Jain", "Anant Narayanan", "Alan Patterson", "Jamie Taylor." ],
      "venue" : "Commun. ACM, 62(8):36–43.",
      "citeRegEx" : "Noy et al\\.,? 2019",
      "shortCiteRegEx" : "Noy et al\\.",
      "year" : 2019
    }, {
      "title" : "Investigating robustness and interpretability of link prediction via adversarial modifications",
      "author" : [ "Pouya Pezeshkpour", "Yifan Tian", "Sameer Singh." ],
      "venue" : "Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).",
      "citeRegEx" : "Pezeshkpour et al\\.,? 2019",
      "shortCiteRegEx" : "Pezeshkpour et al\\.",
      "year" : 2019
    }, {
      "title" : "You can teach an old dog new tricks! on training knowledge graph embeddings",
      "author" : [ "Daniel Ruffinelli", "Samuel Broscheit", "Rainer Gemulla." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Ruffinelli et al\\.,? 2020",
      "shortCiteRegEx" : "Ruffinelli et al\\.",
      "year" : 2020
    }, {
      "title" : "Observed versus latent features for knowledge base and text inference",
      "author" : [ "Kristina Toutanova", "Danqi Chen." ],
      "venue" : "Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality, pages 57–66, Beijing, China. Association",
      "citeRegEx" : "Toutanova and Chen.,? 2015",
      "shortCiteRegEx" : "Toutanova and Chen.",
      "year" : 2015
    }, {
      "title" : "On inductive abilities of latent factor models for relational learning",
      "author" : [ "Théo Trouillon", "Éric Gaussier", "Christopher R. Dance", "Guillaume Bouchard." ],
      "venue" : "J. Artif. Int. Res., 64(1):21–53.",
      "citeRegEx" : "Trouillon et al\\.,? 2019",
      "shortCiteRegEx" : "Trouillon et al\\.",
      "year" : 2019
    }, {
      "title" : "Complex embeddings for simple link prediction",
      "author" : [ "Théo Trouillon", "Johannes Welbl", "Sebastian Riedel", "Éric Gaussier", "Guillaume Bouchard." ],
      "venue" : "International Conference on Machine Learning, pages 2071–2080.",
      "citeRegEx" : "Trouillon et al\\.,? 2016",
      "shortCiteRegEx" : "Trouillon et al\\.",
      "year" : 2016
    }, {
      "title" : "On multi-relational link prediction with bilinear models",
      "author" : [ "Yanjie Wang", "Rainer Gemulla", "Hui Li." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Adversarial attacks and defenses in images, graphs and text: A review",
      "author" : [ "Han Xu", "Yao Ma", "Hao-Chen Liu", "Debayan Deb", "Hui Liu", "Ji-Liang Tang", "Anil K Jain." ],
      "venue" : "International Journal of Automation and Computing, 17(2):151–178.",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Embedding entities and relations for learning and inference in knowledge bases",
      "author" : [ "Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA,",
      "citeRegEx" : "Yang et al\\.,? 2015",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "Data poisoning attack against knowledge graph embedding",
      "author" : [ "Hengtong Zhang", "Tianhang Zheng", "Jing Gao", "Chenglin Miao", "Lu Su", "Yaliang Li", "Kui Ren." ],
      "venue" : "International Joint Conference on Artificial Intelligence.",
      "citeRegEx" : "Zhang et al\\.,? 2019a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Interaction embeddings for prediction and explanation in knowledge graphs",
      "author" : [ "Wen Zhang", "Bibek Paudel", "Wei Zhang", "Abraham Bernstein", "Huajun Chen." ],
      "venue" : "Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining,",
      "citeRegEx" : "Zhang et al\\.,? 2019b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Adversarial attacks on neural networks for graph data",
      "author" : [ "Daniel Zügner", "Amir Akbarnejad", "Stephan Günnemann." ],
      "venue" : "International Conference on Knowledge Discovery & Data Mining, pages 2847–2856.",
      "citeRegEx" : "Zügner et al\\.,? 2018",
      "shortCiteRegEx" : "Zügner et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "Knowledge graph embeddings (KGE) are increasingly deployed in domains with high stake decision making like healthcare and finance (Noy et al., 2019), where it is critical to identify the potential security vulnerabilities that might cause failure.",
      "startOffset" : 130,
      "endOffset" : 148
    }, {
      "referenceID" : 28,
      "context" : "Poisoning attacks have been proposed for models that learn from other graph modalities (Xu et al., 2020) but they cannot be applied directly to KGE models.",
      "startOffset" : 87,
      "endOffset" : 104
    }, {
      "referenceID" : 30,
      "context" : "Two recent studies (Zhang et al., 2019a; Pezeshkpour et al., 2019) attempt to address this problem through random sampling of candidate perturbations (Zhang et al.",
      "startOffset" : 19,
      "endOffset" : 66
    }, {
      "referenceID" : 22,
      "context" : "Two recent studies (Zhang et al., 2019a; Pezeshkpour et al., 2019) attempt to address this problem through random sampling of candidate perturbations (Zhang et al.",
      "startOffset" : 19,
      "endOffset" : 66
    }, {
      "referenceID" : 30,
      "context" : ", 2019) attempt to address this problem through random sampling of candidate perturbations (Zhang et al., 2019a) or through a vanilla auto-encoder that reconstructs discrete entities and relations from latent space (Pezeshkpour et al.",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 22,
      "context" : ", 2019a) or through a vanilla auto-encoder that reconstructs discrete entities and relations from latent space (Pezeshkpour et al., 2019).",
      "startOffset" : 111,
      "endOffset" : 137
    }, {
      "referenceID" : 8,
      "context" : "tiplicative or additive interactions (Chandrahas et al., 2018).",
      "startOffset" : 37,
      "endOffset" : 62
    }, {
      "referenceID" : 5,
      "context" : "In the filtered setting (Bordes et al., 2013), negative triples that already exist in the training, validation or test set are filtered out.",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 17,
      "context" : "Threat Model: To ensure reliable vulnerability analysis, we use a white-box attack setting where the attacker has full knowledge of the target KGE model (Joseph et al., 2019).",
      "startOffset" : 153,
      "endOffset" : 174
    }, {
      "referenceID" : 22,
      "context" : "As in prior studies (Pezeshkpour et al., 2019; Zhang et al., 2019a), the attacker is restricted to making edits only in the neighbourhood of target",
      "startOffset" : 20,
      "endOffset" : 67
    }, {
      "referenceID" : 30,
      "context" : "As in prior studies (Pezeshkpour et al., 2019; Zhang et al., 2019a), the attacker is restricted to making edits only in the neighbourhood of target",
      "startOffset" : 20,
      "endOffset" : 67
    }, {
      "referenceID" : 29,
      "context" : "Given the target relation r, we determine the adversarial relations using an algebraic model of inference (Yang et al., 2015).",
      "startOffset" : 106,
      "endOffset" : 125
    }, {
      "referenceID" : 15,
      "context" : "We model the soft truth score of grounded patterns using t-norm based fuzzy logics (Hájek, 1998).",
      "startOffset" : 83,
      "endOffset" : 96
    }, {
      "referenceID" : 28,
      "context" : "We use the state-of-art evaluation protocol for data poisoning attacks (Xu et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 88
    }, {
      "referenceID" : 30,
      "context" : ": Poisoning attack from (Zhang et al., 2019a) for edits in the neighbourhood of subject of the target triple.",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 10,
      "context" : "Furthermore, FB15k-237 has high connectivity (Dettmers et al., 2018) which means that a KGE model relies on a high number of triples to learn target triples’ ranks.",
      "startOffset" : 45,
      "endOffset" : 68
    }, {
      "referenceID" : 10,
      "context" : "The inversion pattern is likely ineffective on the benchmark datasets because these datasets do not have any inverse relations (Dettmers et al., 2018; Toutanova and Chen, 2015).",
      "startOffset" : 127,
      "endOffset" : 176
    }, {
      "referenceID" : 24,
      "context" : "The inversion pattern is likely ineffective on the benchmark datasets because these datasets do not have any inverse relations (Dettmers et al., 2018; Toutanova and Chen, 2015).",
      "startOffset" : 127,
      "endOffset" : 176
    }, {
      "referenceID" : 29,
      "context" : "KGE models can be categorized into tensor factorization models like DistMult (Yang et al., 2015) and ComplEx (Trouillon et al.",
      "startOffset" : 77,
      "endOffset" : 96
    }, {
      "referenceID" : 26,
      "context" : ", 2015) and ComplEx (Trouillon et al., 2016), neural archi-",
      "startOffset" : 20,
      "endOffset" : 44
    }, {
      "referenceID" : 10,
      "context" : "tectures like ConvE (Dettmers et al., 2018) and translational models like TransE (Bordes et al.",
      "startOffset" : 20,
      "endOffset" : 43
    }, {
      "referenceID" : 5,
      "context" : ", 2018) and translational models like TransE (Bordes et al., 2013).",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 30,
      "context" : ", 2019) and (Zhang et al., 2019a) are most closely related to our work as they propose other",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 32,
      "context" : "Poisoning attacks have also been proposed for models for undirected and single relational graph data like Graph Neural Networks (Zügner et al., 2018; Dai et al., 2018) and Network Embedding models (Bojchevski and Günnemann, 2019).",
      "startOffset" : 128,
      "endOffset" : 167
    }, {
      "referenceID" : 9,
      "context" : "Poisoning attacks have also been proposed for models for undirected and single relational graph data like Graph Neural Networks (Zügner et al., 2018; Dai et al., 2018) and Network Embedding models (Bojchevski and Günnemann, 2019).",
      "startOffset" : 128,
      "endOffset" : 167
    }, {
      "referenceID" : 4,
      "context" : ", 2018) and Network Embedding models (Bojchevski and Günnemann, 2019).",
      "startOffset" : 37,
      "endOffset" : 69
    }, {
      "referenceID" : 28,
      "context" : "A survey of poisoning attacks for graph data is available in (Xu et al., 2020).",
      "startOffset" : 61,
      "endOffset" : 78
    }, {
      "referenceID" : 17,
      "context" : "identify the vulnerabilities in learning algorithms that could be exploited by an adversary to manipulate the model’s behaviour (Joseph et al., 2019; Biggio and Roli, 2018).",
      "startOffset" : 128,
      "endOffset" : 172
    }, {
      "referenceID" : 3,
      "context" : "identify the vulnerabilities in learning algorithms that could be exploited by an adversary to manipulate the model’s behaviour (Joseph et al., 2019; Biggio and Roli, 2018).",
      "startOffset" : 128,
      "endOffset" : 172
    }, {
      "referenceID" : 2,
      "context" : "Identifying these vulnerabilities for KGE models is critical because of their increasing use in domains that need high stakes decision making like heathcare (Bendtsen and Petrovski, 2019) and finance (Hogan et al.",
      "startOffset" : 157,
      "endOffset" : 187
    }, {
      "referenceID" : 21,
      "context" : "Identifying these vulnerabilities for KGE models is critical because of their increasing use in domains that need high stakes decision making like heathcare (Bendtsen and Petrovski, 2019) and finance (Hogan et al., 2020; Noy et al., 2019).",
      "startOffset" : 200,
      "endOffset" : 238
    }, {
      "referenceID" : 3,
      "context" : "This paradox of an arms race is universal across security research (Biggio and Roli, 2018).",
      "startOffset" : 67,
      "endOffset" : 90
    } ],
    "year" : 2021,
    "abstractText" : "We study the problem of generating data poisoning attacks against Knowledge Graph Embedding (KGE) models for the task of link prediction in knowledge graphs. To poison KGE models, we propose to exploit their inductive abilities which are captured through the relationship patterns like symmetry, inversion and composition in the knowledge graph. Specifically, to degrade the model’s prediction confidence on target facts, we propose to improve the model’s prediction confidence on a set of decoy facts. Thus, we craft adversarial additions that can improve the model’s prediction confidence on decoy facts through different inference patterns. Our experiments demonstrate that the proposed poisoning attacks outperform state-of-art baselines on four KGE models for two publicly available datasets. We also find that the symmetry pattern based attacks generalize across all model-dataset combinations which indicates the sensitivity of KGE models to this pattern.",
    "creator" : "LaTeX with hyperref"
  }
}