{
  "name" : "2021.acl-long.210.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Ruddit: Norms of Offensiveness for English Reddit Comments",
    "authors" : [ "Rishav Hada", "Sohi Sudhir", "Pushkar Mishra", "Helen Yannakoudakis", "Saif M. Mohammad", "Ekaterina Shutova" ],
    "emails" : [ "rishavhada@gmail.com,", "sohigre@gmail.com,", "pushkarmishra@fb.com,", "helen.yannakoudakis@kcl.ac.uk,", "saif.mohammad@nrc-cnrc.gc.ca,", "e.shutova@uva.nl" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2700–2717\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2700\nOn social media platforms, hateful and offensive language negatively impact the mental well-being of users and the participation of people from diverse backgrounds. Automatic methods to detect offensive language have largely relied on datasets with categorical labels. However, comments can vary in their degree of offensiveness. We create the first dataset of English language Reddit comments that has fine-grained, real-valued scores between -1 (maximally supportive) and 1 (maximally offensive). The dataset was annotated using Best–Worst Scaling, a form of comparative annotation that has been shown to alleviate known biases of using rating scales. We show that the method produces highly reliable offensiveness scores. Finally, we evaluate the ability of widely-used neural models to predict offensiveness scores on this new dataset."
    }, {
      "heading" : "1 Introduction",
      "text" : "Social media platforms serve as a medium for exchange of ideas on a range of topics, from the personal to the political. This exchange can, however, be disrupted by offensive or hateful language. Such language is pervasive online (Statista, 2020b), and exposure to it may have numerous negative consequences for the victim’s mental health (Munro, 2011). Automated offensive language detection has thus been gaining interest in the NLP community, as a promising direction to better understand the nature and spread of such content.\nThere are several challenges in the automatic detection of offensive language (Wiedemann et al., 2018). The NLP community has adopted various definitions for offensive language, classifying it into specific categories. For example, Waseem and\n∗Both authors contributed equally.\nHovy (2016) classified comments as racist, sexist, neither; Davidson et al. (2017) as hate-speech, offensive but not hate-speech, neither offensive nor hate-speech and Founta et al. (2018) as abusive, hateful, normal, spam. Schmidt and Wiegand (2017); Fortuna and Nunes (2018); Mishra et al. (2019); Kiritchenko and Nejadgholi (2020) summarize the different definitions. However, these categories have significant overlaps with each other, creating ill-defined boundaries, thus introducing ambiguity and annotation inconsistency (Founta et al., 2018). A further challenge is that after encountering several highly offensive comments, an annotator might find subsequent moderately offensive comments to not be offensive (de-sensitization) (Kurrek et al., 2020; Soral et al., 2018).\nAt the same time, existing approaches do not take into account that comments can be offensive to a different degree. Knowing the degree of offensiveness of a comment has practical implications, when taking action against inappropriate behaviour online, as it allows for a more fine-grained analysis and prioritization in moderation.\nThe representation of the offensive class in a dataset is often boosted using different strategies. The most common strategy used is key-word based sampling. This results in datasets that are rich in explicit offensive language (language that is unambiguous in its potential to be offensive, such as those using slurs or swear words (Waseem et al., 2017)) but lack cases of implicit offensive language (language with its true offensive nature obscured due to lack of unambiguous swear words, usage of sarcasm or offensive analogies, and others (Waseem et al., 2017; Wiegand et al., 2021)) (Waseem, 2016; Wiegand et al., 2019). Further, Wiegand et al. (2019) show that key-word based sampling often results in spurious correlations (e.g., sports-related expressions such as announcer and sport occur very frequently in offensive tweets).\nLastly, existing datasets consider offensive comments in isolation from the wider conversation of which they are a part. Offensive language is, however, inherently a social phenomenon and its analysis has much to gain from taking the conversational context into account (Gao and Huang, 2017).\nIn this paper, we present the first dataset of 6000 English language Reddit comments that has finegrained, real-valued scores between -1 (maximally supportive) and 1 (maximally offensive) – normative offensiveness ratings for the comments. For the first time, we use comparative annotations to detect offensive language. In its simplest form, comparative annotations involve giving the annotators two instances at a time, and asking which exhibits the property of interest to a greater extent. This alleviates several annotation biases present in standard rating scales, such as scale-region bias (Presser and Schuman, 1996; Asaadi et al., 2019), and improves annotation consistency (Kiritchenko and Mohammad, 2017). However, instead of needing to annotate N instances, one now needs to annotate N2 instance pairs—which can be prohibitive. Thus, we annotate our dataset using an efficient form of comparative annotation called Best–Worst Scaling (BWS) (Louviere, 1991; Louviere et al., 2015; Kiritchenko and Mohammad, 2016, 2017).\nBy eliminating different offensiveness categories, treating offensiveness as a continuous dimension, and eliciting comparative judgments from the annotators (based on their understanding of what is offensive), we alleviate the issues regarding category definitions and arbitrary category boundaries discussed earlier. By obtaining real-valued offensiveness scores, different thresholds can be used in downstream applications to handle varying degrees of offensiveness appropriately. By framing the task as a comparative annotation task, we obtain consistent and reliable annotations. We also greatly mitigate issues of annotator de-sensitization as one will still be able to recognize if one comment is more offensive than another, even if they think both comments are not that offensive.\nIn contrast to existing resources, which provide annotations for individual comments, our dataset includes conversational context for each comment (i.e. the Reddit thread in which the comment occurred). We conduct quantitative and qualitative analyses of the dataset to obtain insights into how emotions, identity terms, swear words, are related to offensiveness. Finally, we benchmark several\nwidely-used neural models in their ability to predict offensiveness scores on this new dataset.1"
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Offensive Language Datasets",
      "text" : "Surveys by Schmidt and Wiegand (2017); Fortuna and Nunes (2018); Mishra et al. (2019); Vidgen and Derczynski (2020) discuss various existing datasets and their compositions in detail. Waseem and Hovy (2016); Davidson et al. (2017); Founta et al. (2018) created datasets based on Twitter data. Due to prevalence of the non-offensive class in naturallyoccurring data (Waseem, 2016; Founta et al., 2018), the authors devised techniques to boost the presence of the offensive class in the dataset. Waseem and Hovy (2016) used terms frequently occurring in offensive tweets, while Davidson et al. (2017) used a list of hate-related terms to extract offensive tweets from the Twitter search API. Park et al. (2018), Wiegand et al. (2019), and Davidson et al. (2019) show that the Waseem and Hovy (2016) dataset exhibits topic bias and author bias due to the employed sampling strategy. Founta et al. (2018) boosted the representation of offensive class in their dataset by analysing the sentiment of the tweets and checking for the presence of offensive terms. In our work, we employ a hybrid approach, selecting our data in three ways: specific topics, emotion-related key-words, and random sampling.\nPast work has partitioned offensive comments into explicitly offensive (those that include profanity—swear words, taboo words, or hate terms) and implicitly offensive (those that do not include profanity) (Waseem et al., 2017; Caselli et al., 2020a; Wiegand et al., 2021). Some other past work has defined explicitly and implicitly offensive instances a little differently: Sap et al. (2020) considered factors such as obviousness, intent to offend and biased implications, Breitfeller et al. (2019) considered factors such as the context and the person annotating the instance, and Razo and Kübler (2020) considered the kind of lexicon used. Regardless of the exact definition, implicit offensive language, due to a lack of lexical cues, is harder to classify not only for computational models, but also for humans. In our work, we consider implicitly offensive comments as those offensive comments that do not contain any swear words.\n1Dataset and code available at: https://github.com/hadarishav/Ruddit.\nWulczyn et al. (2016, 2017) created three different datasets from Wikipedia Talk pages, focusing on aggression, personal attacks and toxicity. The comments were sampled at random from a large dump of English Wikipedia, and boosted by including comments from blocked users. For the personal attacks dataset, Wulczyn et al. (2016) used two different kinds of labels: ED (empirical distribution), OH (one hot). In case of ED, the comments were assigned real-valued scores between 0 and 1 representing the fraction of annotators who considered the comment a personal attack. While these labels were introduced to create a separation between the nature of comments with a score of 1.0 and those with a score of 0.6 (which would otherwise be classified as attacks), they are discrete. In our work, using the BWS comparative annotation setup, we assign fine-grained continuous scores to comments to denote their degree of offensiveness."
    }, {
      "heading" : "2.2 Best–Worst Scaling (BWS)",
      "text" : "BWS was proposed by Louviere (1991). Kiritchenko and Mohammad (2017) have experimentally shown that BWS produces more reliable finegrained scores than the scores acquired utilizing rating scales. In the BWS annotation setup, the annotators are given an n-tuple (where n > 1, and commonly n = 4), and asked which item is the best and which is the worst (best and worst correspond to the highest and the lowest with respect to a property of interest). Best–worst annotations are particularly efficient when using 4-tuples, as each annotation results in inequalities for 5 of the 6 item pairs. For example, a 4-tuple with items A, B, C, and D, where A is the best, and D is the worst, results in inequalities: A>B, A>C, A>D, B>D, and C>D. Real-valued scores of associations are calculated between the items and the property of interest from the best–worst annotations for a set of 4-tuples (Orme, 2009; Flynn and Marley, 2014). The scores can be used to rank items by the degree of association with the property of interest. Within the NLP community, BWS has thus far been used only for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko et al., 2014), phrase sentiment composition (Kiritchenko and Mohammad, 2016), and tweet-emotion intensity (Mohammad and Bravo-Marquez, 2017; Mohammad and Kiritchenko, 2018). Using BWS, we create the first\ndataset with degree of offensiveness scores for social media comments."
    }, {
      "heading" : "3 Data collection and sampling",
      "text" : "We extracted Reddit data from the Pushshift repository (Baumgartner et al., 2020) using Google BigQuery. Reddit is a social news aggregation, web content rating, and discussion website. It contains forums called subreddits dedicated to specific topics. Users can make a post on the subreddit to start a discussion. Users can comment on existing posts or comments to participate in the discussion. As users can also reply to a comment, the entire discussion has a hierarchical structure called the comment thread. We divided the extracted comments into 3 categories based on their subreddit source: 1. Topics (50%): Contains comments from\ntopic-focused subreddits: AskMen, AskReddit, TwoXChromosomes, vaxxhappened, worldnews, worldpolitics. These subreddits were chosen to cover a diverse range of topics. AskReddit, vaxxhappened, worldnews, worldpolitics discuss generic themes. TwoXChromosomes contains women’s perspectives on various topics and AskMen contains men’s perspectives.\n2. ChangeMyView (CMV) (25%): The CMV subreddit (with over a million users) has posts and comments on controversial topics.\n3. Random (25%): Contains comments from random subreddits.\nWe selected 808 posts from the subreddits based on criteria such as date, thread length, and post length. (Further details in the Appendix A.1.) We took the first 25 and the last 25 comments per post (skipping comments that had [DELETED] or [REMOVED] as comment body). The first responses are likely to be most relevant to the post. The final comments indicate how the discussion ended. We sampled 6000 comments from this set for annotation.\nThe goal of the sampling was to increase the proportion of offensive and emotional comments. Emotions are highly representative of one’s mental state, which in turn are associated with their behaviour (Poria et al., 2019). For example, Jay and Janschewitz (2008) show that people tend to swear when they are angry, frustrated or anxious.\nStudies have shown that the primary dimensions of emotion are valence, arousal, and dominance (VAD) (Osgood et al., 1957; Russell, 1980,\n2003). Valence is the positive–negative or pleasure– displeasure dimension. Arousal is the excited– calm or active–passive dimension. Dominance is powerful–weak or ‘have full control’–‘have no control’ dimension (Mohammad, 2018). To boost the representation of offensive and emotional comments in our dataset, we up-sampled comments that included low-valence (highly negative) words and those that included high-arousal words (as per the NRC VAD lexicon (Mohammad, 2018)). The manually constructed NRC VAD lexicon includes 20,000 English words, each with a real-valued score between 0 and 1 in the V, A, D dimensions.\nIn order to do this upsampling, we first defined the valence score of each comment as the average valence score of the negative words within the comment (A negative word is defined as a word with a valence score less than 0.25 in the VAD lexicon.). Similarly, we defined the arousal score for a comment as the average arousal score of high-arousal words in each comment (A high-arousal word is defined as a word with an arousal score greater than 0.75.)2\nWe selected comments from the comment pool such that 50% of the comments were from the Topics category, 25% of the comments from the CMV category and 25% of the comments from the Random category. Within each category, 33% of the comments were those that had the lowest valence scores, 33% of the comments were those that had the highest arousal scores, and the remaining were chosen at random."
    }, {
      "heading" : "4 Annotation",
      "text" : "The perception of ‘offensiveness’ of a comment can vary from person to person. Therefore, we used crowdsourcing to annotate our data. Crowdsourcing helps us get an aggregation of varied perspectives rather than expert opinions which can leave out offensiveness in a comment that lies outside the ‘typical’ offensiveness norms (Blackwell et al., 2017). We carried out all the annotation tasks on Amazon Mechanical Turk (AMT). Due to the strong language, an adult content warning was issued for the task. Reddit is most popular in the US, which accounts for 50% of its desktop traffic (Statista, 2020a). Therefore, we restricted annotators to those residing in the US. To maintain the\n2In some initial pilot experiments, we found this approach of sampling low valence and high arousal comments to result in the highest number of offensive comments.\nquality of annotations, only annotators with high approval rate were allowed to participate."
    }, {
      "heading" : "4.1 Annotation with Best–Worst Scaling",
      "text" : "We followed the procedure described in Kiritchenko and Mohammad (2016) to obtain BWS annotations. Annotators were presented with 4 comments (4-tuple) at a time and asked to select the comment that is most offensive (least supportive) and the comment that is least offensive (most supportive). We randomly generated 2N distinct 4-tuples (where N is the number of comments in the dataset), such that each comment was seen in eight different 4-tuples and no two 4-tuples had more than 2 items in common. We used the script provided by Kiritchenko and Mohammad (2016) to obtain the 4-tuples to be annotated.3\nKiritchenko and Mohammad (2016) show that in a word-level sentiment task, using just three annotations per 4-tuple produces highly reliable results. However, since we work with long comments and a relatively more difficult task, we got each tuple annotated by 6 annotators. Since each comment is seen in 8 different 4-tuples, we obtain 8 X 6 = 48 judgements per comment."
    }, {
      "heading" : "4.2 Annotation Task and Process",
      "text" : "In our instructions to the annotators, we defined offensive language as comments that include but are not limited to [being hurtful (with or without the usage of abusive words)/ being intentionally harmful/ treating someone improperly/ harming the ‘self-concept’ of another person/ aggressive outbursts/ name calling/ showing anger and hostility/ bullying/ hurtful sarcasm]. We also encouraged the annotators to follow their instincts. By framing the task in terms of comparisons and providing a broad definition of offensiveness, we avoided introducing artificial categories and elicit responses guided by their intuition of the language.\nDetailed annotation instructions are made publicly available (Figure 6 in Appendix A.2).4 A sample questionnaire is shown in Figure 7 in Appendix A.2. For quality control purposes, we manually annotated around 5% of the data ourselves beforehand. We will refer to these instances as gold questions. The gold questions were interspersed with the other questions. If a worker’s accuracy on\n3http://saifmohammad.com/WebPages/ BestWorst.html\n4AMT task interface with instructions: https:// hadarishav.github.io/Ruddit/\nthe gold questions fell below 70%, they were refused further annotation and all of their annotations were discarded. The discarded annotations were published again for re-annotation. We received a total of 95,255 annotations by 725 crowd workers.\nThe BWS responses were converted to scores using a simple counting procedure (Orme, 2009; Flynn and Marley, 2014). For each item, the score is the proportion of times the item is chosen as the most offensive minus the proportion of times the item is chosen as the least offensive. We release the aggregated annotations as well as the individual annotations of Ruddit, to allow further work on examining and understanding the variability.5"
    }, {
      "heading" : "4.3 Annotation Reliability",
      "text" : "We cannot use standard inter-annotator agreement measures to ascertain the quality of comparative annotations. The disagreement that arises in tuples having two items that are close together in their degree of offensiveness is a useful signal for BWS (helping it give similar scores to the two items). The quality of annotations can be measured by measuring the reproducibility of the end result – if repeated manual annotations from multiple annotators can produce similar rankings and scores, then, one can be confident about the quality of annotations received. To assess this reproducibility, we computed average split-half reliability (SHR) values over 100 trials. SHR is a commonly used approach to determine consistency in psychological studies.\nFor computing SHR values, the annotations for each 4-tuple were randomly split in two halves. Using these two splits, two sets of rankings were determined. We then calculated the correlation values between these two sets. This procedure was repeated 100 times and the correlations were averaged. A high correlation value indicates that the annotations are of good quality. Table 1 shows the SHR for our annotations. SHR scores of over 0.8 indicate substantial reliability.\n5We provide the comment IDs and not the comment body, in accordance to the GDPR regulations. Comment body can be extracted using the Reddit API."
    }, {
      "heading" : "5 Data Analysis",
      "text" : "In this section, we analyze various aspects of the data, including: the distribution of scores, the association with identity terms, the relationship with emotion dimensions, the relationship with data source, and the role of swear words.\nDistribution of Scores Figure 1 shows a histogram of frequency of comments vs. degree of offensiveness, over 40 equi-spaced score bins of size 0.05. We observe a normal distribution.\nTo analyze the data, we placed the comments in 5 equi-spaced score bins of size 0.4 (bin 1: −1.0 to −0.6, bin 2: −0.6 to −0.2, and so on). Table 2 shows some comments from the dataset (more examples can be found in Appendix A.3 Table 6). We observed that bin 1 primarily contains supportive comments while bin 2 shows a transition from supportive to neutral comments. Bin 3 is dominated by neutral comments but as the score increases the comments become potentially offensive and bins 4 & 5 predominantly contain offensive comments. It is interesting to note that bin 4 contains some instances of implicit offensive language such as\n‘You look like a lesbian mechanic who has a shell collection’. In their paper, Wiegand et al. (2021) explore the category of such “implicity abusive comparisons”, in depth. More examples of implicitly offensive comments present in our dataset can be found in table 2 and table 6 (in Appendix A.3).\nTo explore whether specific bins capture spe-\ncific topics or key-words, we calculated Pointwise Mutual Information (PMI) scores of all the unique words in the comments (excluding stop words) with the five score bins. Table 3 shows the top scoring words for each bin. We observed that bins 1, 2, and 3 exhibit a strong association with supportive or neutral words, while bins 4 and 5 show a strong association with swear words and identity terms commonly found in offensive contexts.\nIdentity terms A common criticism of the existing offensive language datasets is that in those datasets, certain identity terms (particularly those referring to minority groups) occur mainly in texts that are offensive (Sap et al., 2019; Davidson et al., 2019; Wiegand et al., 2019; Park et al., 2018; Dixon et al., 2018). This leads to high association of targeted minority groups (such as Muslims, females, black people and others) with the offensive class(es). This bias, in turn, is captured by the computational models trained on such datasets. As mentioned earlier, in Ruddit, certain words such as gay, trans, male, female, black, white were found to exhibit a relatively higher association with the offensive bins than with the supportive bins. In order to probe the effect of this on the computational models, we created a variant of Ruddit by\nreplacing all the identity terms (from the list given in Appendix A.4) in the comments with the [group] token and observed the effect on the models’ performance. We refer to this variant of the dataset as the identity-agnostic dataset. We analyse the models’ performance in the next section.\nOffensiveness vs. emotion As discussed earlier, our emotions impact the words we use in text. We examined this relationship quantitatively using Ruddit and the NRC VAD Lexicon (which has intensity scores along the valence, arousal, and dominance dimensions). We first identified sets of words in the VAD lexicon that have high valence scores (>0.75), low valence scores (<0.25), high arousal scores (>0.75), low arousal scores (<0.25), high dominance scores (>0.75), and low dominance scores (<0.25), respectively. We will refer to them as the high and low intensity V/A/D words. For each comment in Ruddit, we calculated three scores that captured the intensities of the high V, A, D words (the averages of the intensities of the high V/A/D words in the comment) and three scores that captured the intensities of the low V, A, D words (the averages of the intensities of the low V/A/D words in the comment). We then determined the correlation between each of these six scores and the degree of offensiveness. See Table 4. From the table, we can observe that high valence, low arousal, high dominance and low dominance show no correlation with offensiveness whereas low valence and high arousal are somewhat correlated.\nIn our dataset of 6000 comments, 33% (1990) comments are those that have the lowest valence scores (referred to as low valence comments), 33% (1990) of the comments are those that have the highest arousal scores (referred to as high arousal com-\nments) and the remaining 34% (2020) comments were chosen at random (referred to as random comments). In Figure 2, we see the distribution of comments from each type over the 5 score bins. We observe that the majority of comments from all types are situated in the center. High arousal comments are skewed towards the offensive end of the scale. Random comments are heavily skewed towards the supportive end of the scale while low valence comments are slightly skewed towards the supportive end. Figure 3 shows the distribution of comment types within each bin. We can clearly see that high arousal and low valence comments dominate the bins on the offensive end of the scale while random comments dominate the bins on the supportive end. Therefore, from both analyses, we can infer that the low valence and high arousal emotion dimensions are useful signals for determining the offensiveness of a comment.\nOffensiveness vs. data source As mentioned earlier, comments in our dataset come from three different sources - Topics, CMV, and Random. Figure 4 shows the distribution of comments from each source over the score bins. We observed that comments from Topics have near equal representation on both sides of the scale, while for the other two sources, comments are more prevalent in the\nsupportive bins. The higher representation of comments from Topics than the other two sources in the offensive bins, is likely due to the fact that the Topics category includes subreddits such as worldnews and worldpolitics. Discussions on these subreddits covers controversial topics and lead to the usage of offensive language. We observed that worldnews and worldpolitics indeed have high representation in the offensive bins (Figure 9 in Appendix A.4).\nSwear words We identified 868 comments in our dataset that contain at least one swear word from the cursing lexicon (Wang et al., 2014). Comments containing swear words can have a wide range of offensiveness scores. To visualize the distribution, we plot a histogram of the comments containing swear words vs. degree of offensiveness (see Figure 8 in Appendix A.4). The distribution is skewed towards the offensive end of the scale. An interesting observation is that some comments with low offensiveness scores contain phrases using swear words to express enthusiasm or to lay more emphasis, for example ‘Hell yes’, ‘sure as hell love it’, ‘uncomfortable as shit’ and others. To study the impact of comments containing swear words on\ncomputational models, we created another variant of Ruddit in which we removed all the comments containing at least one swear word. We refer to this variant as the no-swearing dataset. This dataset contains 5132 comments. We analyse the models’ performance on this dataset in the next section.\nOffensiveness in different score ranges It is possible that comments in the middle region of the scale may be more difficult for the computational models. Thus, we created a subset of Ruddit containing comments with scores from −0.5 to 0.5. We call this subset (of 5151 comments), the reduced-range dataset. We discuss the models’ performance on this dataset in the next section."
    }, {
      "heading" : "6 Computational Modeling",
      "text" : "In this section, we present benchmark experiments on Ruddit and its variants by implementing some commonly used model architectures. The task of the models was to predict the offensiveness score of a given comment. We performed 5-fold crossvalidation for each of the models.6"
    }, {
      "heading" : "6.1 Models",
      "text" : "Bidirectional LSTM We fed pre-trained 300 dimensional GloVe word embeddings (Pennington et al., 2014) to a 2-layered BiLSTM to obtain a sentence representation (using a concatenation of the last hidden state from the forward and backward direction). This sentence representation was then passed to a linear layer with a tanh activation to produce a score between −1 and 1. We used Mean Squared Error (MSE) loss as the objective function, Adam with 0.001 learning rate as the optimizer, hidden dimension of 256, batch size of 32, and a dropout of 0.5. The model was trained for 7 epochs.\nBERT We fine-tuned BERTbase (Devlin et al., 2019). We added a regression head containing a linear layer to the pre-trained model. We used MSE loss as the objective function, batch size of 16, and learning rate of 2e− 5 (other hyperparameters same as (Devlin et al., 2019)). We used the AdamW optimizer with a linear learning rate scheduler with no warm up steps. The model was trained for 3 epochs. (More details in Appendix A.5.)\n6Since we have a linear regression task, we created folds using sorted stratification (Lowe, 2016) to ensure that the distribution of all the partitions is similar.\nHateBERT HateBERT (Caselli et al., 2020b) is a version of BERT pretrained for abusive language detection in English. HateBERT was trained on RAL-E, a large dataset of English language Reddit comments from communities banned for being offensive or hateful. HateBERT has been shown to outperform the general purpose BERT model on the offensive language detection task when finetuned on popular datasets such as OffensEval 2019 (Zampieri et al., 2019), AbusEval (Caselli et al., 2020a), and HatEval (Basile et al., 2019).\nWe fine-tuned HateBERT on Ruddit and its variants. The experimental setup for this model is the same as that described for the BERT model."
    }, {
      "heading" : "6.2 Results and Analysis",
      "text" : "We report Pearson correlation (r) and MSE, averaged over all folds. The performance of the models on Ruddit and its variants is shown in the Table 5. Note that the performance values on the noswearing and the reduced-range datasets are not directly comparable to the performance values on the full Ruddit as their score range is different. We can see that on all the datasets, the HateBERT model performs the best, followed by the BERT model. Interestingly, the model performance (for all models) does not change substantially when trained on Ruddit or the identity-agnostic dataset. This indicates that the computational models are not learning to benefit from the association of certain identity terms with a specific range of scores on the offensiveness scale.7\nThe models show a performance drop on the no-swearing dataset, which suggests that swear words are useful indicators of offensiveness and that the comments containing them are easier to classify. Yet, the fact that the models still obtain performance of up to 0.8 (r) demonstrates that they necessitate and are able to learn other types of offensiveness features. It is also worth mentioning that even if they encounter swear words in a comment, the task is not simply to label the comment as offensive but to provide a suitable score.\nFinally, the models obtained the performance of up to 0.78 (r) on the reduced-range dataset, which shows that even if the comments from the extreme ends of the offensiveness scale are removed, Ruddit still presents an interesting and feasible offensiveness scoring task.\n7It should be noted that since the list of identity terms and the cursing lexicon we use is not exhaustive, our conclusions are only limited to the scope of the respective lists.\nError Analysis Figure 5 shows the squared error values of the 3 models over the offensiveness score range in Ruddit. As expected, for all the models, the error in predictions is lower on both the extreme ends of the scale than in the middle region. Comments with very high or very low offensiveness scores are rich in obvious linguistic cues, making it easier for the computational models to predict scores. Most of the not-obvious, indirect implicitly offensive, and neutral comments should be present in the middle region of the offensiveness scale, making them more difficult for the models. It is interesting to observe that HateBERT, unlike the other two models, does not have high error values for samples within the score range 0.25–0.75. This indicates that HateBERT is efficient in dealing with offensive language that does not lie in the extreme offensive end. BiLSTM seems relatively less accurate for samples in the supportive range (−0.75 to −0.25). This could be attributed to the less complex model architecture and the usage of GloVe word embeddings."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We presented the first dataset of online comments annotated for their degree of offensiveness. We used a comparative annotation technique called Best–Worst Scaling, which addresses the limitations of traditional rating scales. We showed that\nthe ratings obtained are highly reliable (SHR Pearson r ≈ 0.88). We performed data analysis to gain insight into the relation of emotions, data sources, identity terms, and swear words with the offensiveness scores. We showed that low valence and high arousal comments have a higher correlation with the offensiveness scores. Finally, we presented benchmark experiments to predict the offensiveness score of a comment, on our dataset. We found that computational models are not benefiting from the association of identity terms with specific range of scores on the offensiveness scale. In future work, it would be interesting to explore the use of conversational context in computational modeling of offensiveness, as well as studying the interaction between offensiveness and emotions in more depth. We make our dataset freely available to the research community."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This research was funded by the Facebook Online Safety Benchmark Research award for the project “A Benchmark and Evaluation Framework for Abusive Language Detection.”\nEthical Considerations\nWe create Ruddit to study, understand and explore the nature of offensive language. Any such dataset might also be used to create automatic offensive\nlanguage detection systems. While we realise the importance of such systems, we also accept that any moderation of online content is a threat to free speech. Offensive language datasets or automatic systems can be misused to stifle disagreeing voices. Our intent is solely to learn more about the use of offensive language, learn about the various degrees of offensive language, explore how computational models can be enabled to watch and contain offensive language, and encourage others to do so. We follow the format provided by Bender and Friedman (2018) to discuss the ethical considerations for our dataset.\nInstitutional Review: This research was funded by the Facebook Online Safety Benchmark Research award. The primary objective of this research award is the creation of publicly available benchmarks to improve online safety. This award does not directly benefit Facebook in any way. This research was reviewed by Facebook for various aspects, in particular: • Legal Review: Evaluates whether the research\nto be undertaken or the research performed can violate intellectual property rights.\n• Policy and Ethics Review: Evaluates whether the research to be undertaken aligns with the best ethics practices. This includes several aspects such as mitigating harm to people involved, improving data privacy, and informed consent.\nData Redistribution / User Privacy: We extracted our data from the Pushshift Reddit dataset made publicly available by Baumgartner et al. (2020) for research purposes. The creators of the Pushshift Reddit dataset have provisions to delete comments from their dataset upon user’s request. We release data in a manner that is GDPR compliant. We do not provide any user-specific information. We release only the comment IDs and post IDs. Reddit’s Terms of Service do not prohibit the distribution of ids.8 The researchers using the dataset need to retrieve the data using the Reddit API.\nSpeaker and Annotator Demographic: No specific speaker demographic information is available for the comments included in Ruddit. According to the October 2020 survey published by Statista (Statista, 2020a), 50% of the Reddit’s desktop traffic is from the United States. They also state that from the internet users in the US, 21% from ages\n8https://www.reddit.com/wiki/api-terms\n18-24, 23% from ages 25-29 and 14% from ages 30-49 use Reddit.\nWe restricted annotators to those residing in the US. A total of 725 crowd-workers participated in the task. Apart from the country of residence, no other information is known about the annotators. The annotators are governed by AMT’s privacy policy.9 Pew Research Center conducted a demographic survey of AMT workers in 2016. In this survey, 3370 workers participated. They found out that 80% of the crowd-workers on AMT are from the US (PRC, 2020). More information about the workers who participated in their survey can be found in their article.\nIt is important to include the opinions of targeted minorities and marginalized groups when dealing with the annotation of offensive language (Kiritchenko and Nejadgholi, 2020; Blackwell et al., 2017). However, we did not have our data annotated by the specific target demographic because it poses certain challenges. For example: identification of the target of offensive language; finding people of the target demographic group who are willing to annotate offensive language; and others. Annotating such offensive data can be even more traumatizing for the members of the targeted minorities. Finally, Ruddit was created with the intention to look at wide ranging offensive language of various degrees as opposed to detecting offensive language towards specific target groups.\nAnnotation Guidelines: We created our annotation guidelines drawing inspiration from the community standards set for offensive language on several social media platforms. These standards are made after thorough research and feedback from the community. However, we are aware that the definitions in our guidelines are not representative of all possible perspectives. The degree of offensiveness scores that we provide in Ruddit are a representation of what the majority of our annotators think. We would like to emphasize that the scores provided are not the “correct” or the only appropriate value of offensiveness. Different individuals and demographic groups may find the same comment to be more or less offensive than the scores provided.\nImpact on Annotators: Annotation of harsh and offensive language might impact the mental health of the annotators negatively (Vidgen et al., 2019;\n9https://www.mturk.com/help\nRoberts, 2016, 2019; Kiritchenko and Nejadgholi, 2020). The following minimized negative mental impact on the annotators participating in our task: • The comments that we included in our dataset\nare pre-moderated by Reddit’s admins and subreddit specific moderators. Any comments that do not comply with Reddit’s content policy are not included.10\n• Our goal was to annotate posts one sees on social media (after content moderation). Unlike some past work, we do not limit the data to include only negative comments. We included a large sample of posts that one normally sees on social media, and annotated it for degree of supportiveness or degree of offensiveness.\n• AMT provides a checkbox where requesters can indicate that some content in the task may be offensive. These tasks are not shown to annotators who have specified so in their profile. We used the checkbox to indicate that this task has offensive content.\n• We explicitly warned the annotators about the content of annotation, and advised worker discretion.\n• We provided detailed annotation instructions and informed the annotators about how the annotations for offensive language will be used for studying and understanding offensive language.\n• The annotation of our data was crowdsourced, allowing for a large number of raters (725). This reduces the number of comments seen per rater. We also placed a limit on how many posts one may annotate. Annotators were not allowed to submit more than ∼ 5% of the total assignments.\n• There are just 25 comments in the top 10% of the offensiveness score range. Thus, most annotators (> 99.95%) do not see even one such comment.\nIdentity Terms: As discussed in section 5, in Ruddit, certain identity terms show a higher association with offensive comments than with the supportive comments. In order to address this, we created a variant of Ruddit, in which we replaced all the identity terms (from the list given in Appendix A.4) with the [group] token. We call this variant the identity-agnostic dataset. We release the code for creating this variant from the original dataset. We evaluated our computational models on this variant and observed that the models did not learn to benefit from the association\n10https://www.redditinc.com/policies/ content-policy\nof the identity terms with the offensive comments.\nComputational Models: The models reported in this paper are not intended to fully automate offensive content moderation or to make judgements about specific individuals. Owing to privacy concerns, we do not model user history to predict offensiveness scores (Mitchell et al., 2018).\nFeedback: We are aware that our dataset is subject to the inherent bias of the data, the sampling procedure and the opinion of the annotators who annotated it. Finally, we acknowledge that this is not a comprehensive listing of all the ethical considerations and limitations. We welcome feedback from the research community and anyone using our dataset."
    }, {
      "heading" : "A Supplemental Material",
      "text" : "A.1 Post and Comment Criteria\nWe selected the posts from the subreddits based on the following criteria:\n1. Date: To extract comments from posts that discuss current matters, we took comments from the time period of January, 2015 to September, 2019 (last available month at the time of extraction).\n2. Thread length: We chose posts with more than 150 comments and less than 5000 comments. This criteria ensured that the posts contained enough comments to capture meaningful discussion.\n3. Post length: We chose posts containing more than 5 words and less than 60 words in the post body. This was done to avoid posts that are too short to provide enough information or are too long and have a possibility of being spam.\n4. URL: Often, posts on Reddit contain URLs redirecting to images, videos, news articles and others. We limited our posts to those containing at most one URL to avoid issues arising due to missing context.\nFor each post, the hierarchical threads were reconstructed using the Anytree python library. We filtered comments from these posts based on the following criteria:\n1. Comment length: We chose comments containing more than 5 words and less than 150 words in the comment body. We did this to include comments that are neither too long (can be difficult to annotate) nor too short (not very valuable).\n2. No. of users: In the first and last 25 comments of the thread, we ensured participation of at least 4 users. This was done to ensure that the comments in our dataset are from a diverse set of users.\n3. URL: We chose comments with no URL in them. Comments with URL can be difficult to annotate as the URLs provide extra context for the comment.\nA.2 Annotation\nFigure 6 shows the detailed annotation instructions given to the crowd-workers for the task.\nA sample questionnaire for the final annotation task is shown in Figure 7.\nThe hourly compensation rate for annotators on Amazon Mechanical Turk was US$7.50/hr. The task received considerable attention with 725 participants in total.\nA.3 Sample data\nTable 6 contains comments from Ruddit grouped according to the 5 score bins.\nA.4 Data Analysis\nWe used the list of identity terms used by Dixon et al. (2018) with a few of our own additions. The terms used are lesbian, gay, bisexual, transgender, trans, queer, lgbt, lgbtq, homosexual, straight, heterosexual, male, female, nonbinary, african, africanamerican, black, white, european, hispanic, latino, latina, latinx, mexican, canadian, american, asian, indian, middle eastern, chinese, japanese, christian, muslim, jewish, buddhist, catholic, protestant, sikh, taoist, old, older, young, younger, teenage, millenial, middle aged, elderly, blind, deaf, paralyzed, atheist, feminist, islam, muslim, man, woman, boy, girl.\nFigure 8 shows a histogram of the comments containing swear words–degree of offensiveness, over 40 equi-spaced score bins of size 0.05.\nFigure 9 shows a distribution of comments within each of the 5 score bins over the subreddits that were included in the Topics category.\nA.5 Computational Modeling\nHyperparameter Tuning We tuned hyperparameters for the BERT and the BiLSTM models. We performed grid search cross-validation on Ruddit and used Pearson’s r to select the best hyperparameter setting. All experiments were performed on a fixed seed value of 12.\nFor the BiLSTM model, the batch size was fixed at 32 and the number of epochs was set to 7. The hyperparameter search space is as follows:\n• Number of Layers (N): 1, 2\n• Hidden size (H): 64, 128, 256\nFor the BERT model, the batch size was fixed at 16 and BERT tokenizer’s maximum length was set\nto 200. We tune hyperparameters on the settings that Devlin et al. (2019) found to work best on all tasks. The search space is as follows:\n• Learning rate: 2e− 5, 3e− 5, 5e− 5\n• Number of epochs: 3, 4\nWe reported the best setting for the models in section 6.1. The average r of the BERT and the BiLSTM models across all hyperparameter search\ntrials was 0.868± 0.005 and 0.827± 0.002 respectively.\nTraining Times We trained all our models on the Tesla T4 GPU. The number of GPU(s) used is 1. The number of trainable parameters and thus, the training time varied for each model. The approximate number of trainable parameters for each model is as follows:\n• BiLSTM (N = 2, H = 256): 7 million\n• BERT: 108 million\n• HateBERT: 109 million\nThe approximate average runtime for each model on the Ruddit dataset is as follows:\n• BiLSTM (N = 2, H = 256): 2 seconds per epoch\n• BERT: 3 minutes per epoch\n• HateBERT: 3.6 minutes per epoch"
    } ],
    "references" : [ {
      "title" : "Big BiRD: A large, fine-grained, bigram relatedness dataset for examining semantic composition",
      "author" : [ "Shima Asaadi", "Saif Mohammad", "Svetlana Kiritchenko." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Asaadi et al\\.,? 2019",
      "shortCiteRegEx" : "Asaadi et al\\.",
      "year" : 2019
    }, {
      "title" : "SemEval-2019 task 5: Multilingual detection of hate speech against immigrants",
      "author" : [ "Valerio Basile", "Cristina Bosco", "Elisabetta Fersini", "Debora Nozza", "Viviana Patti", "Francisco Manuel Rangel Pardo", "Paolo Rosso", "Manuela Sanguinetti" ],
      "venue" : null,
      "citeRegEx" : "Basile et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Basile et al\\.",
      "year" : 2019
    }, {
      "title" : "The pushshift reddit dataset",
      "author" : [ "Jason Baumgartner", "Savvas Zannettou", "Brian Keegan", "Megan Squire", "Jeremy Blackburn" ],
      "venue" : null,
      "citeRegEx" : "Baumgartner et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Baumgartner et al\\.",
      "year" : 2020
    }, {
      "title" : "Data statements for natural language processing: Toward mitigating system bias and enabling better science",
      "author" : [ "Emily M. Bender", "Batya Friedman." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:587–604.",
      "citeRegEx" : "Bender and Friedman.,? 2018",
      "shortCiteRegEx" : "Bender and Friedman.",
      "year" : 2018
    }, {
      "title" : "Classification and its consequences for online harassment: Design insights from heartmob",
      "author" : [ "Lindsay Blackwell", "Jill Dimond", "Sarita Schoenebeck", "Cliff Lampe." ],
      "venue" : "Proc. ACM Hum.-Comput. Interact., 1(CSCW).",
      "citeRegEx" : "Blackwell et al\\.,? 2017",
      "shortCiteRegEx" : "Blackwell et al\\.",
      "year" : 2017
    }, {
      "title" : "Finding microaggressions in the wild: A case for locating elusive phenomena in social media posts",
      "author" : [ "Luke Breitfeller", "Emily Ahn", "David Jurgens", "Yulia Tsvetkov." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Breitfeller et al\\.,? 2019",
      "shortCiteRegEx" : "Breitfeller et al\\.",
      "year" : 2019
    }, {
      "title" : "I feel offended, don’t be abusive! implicit/explicit messages in offensive and abusive language",
      "author" : [ "Tommaso Caselli", "Valerio Basile", "Jelena Mitrović", "Inga Kartoziya", "Michael Granitzer." ],
      "venue" : "Proceedings of the 12th Language Resources and Evaluation Con-",
      "citeRegEx" : "Caselli et al\\.,? 2020a",
      "shortCiteRegEx" : "Caselli et al\\.",
      "year" : 2020
    }, {
      "title" : "2020b. Hatebert: Retraining bert for abusive language detection in english",
      "author" : [ "Tommaso Caselli", "Valerio Basile", "Jelena Mitrović", "Michael Granitzer" ],
      "venue" : null,
      "citeRegEx" : "Caselli et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Caselli et al\\.",
      "year" : 2020
    }, {
      "title" : "Racial bias in hate speech and abusive language detection datasets",
      "author" : [ "Thomas Davidson", "Debasmita Bhattacharya", "Ingmar Weber." ],
      "venue" : "Proceedings of the Third Workshop on Abusive Language Online, pages 25–35, Florence, Italy. Association for Com-",
      "citeRegEx" : "Davidson et al\\.,? 2019",
      "shortCiteRegEx" : "Davidson et al\\.",
      "year" : 2019
    }, {
      "title" : "Automated hate speech detection and the problem of offensive language",
      "author" : [ "Thomas Davidson", "Dana Warmsley", "Michael Macy", "Ingmar Weber." ],
      "venue" : "Proceedings of the 11th International AAAI Conference on Web and Social Media, ICWSM ’17, pages",
      "citeRegEx" : "Davidson et al\\.,? 2017",
      "shortCiteRegEx" : "Davidson et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Measuring and mitigating unintended bias in text classification",
      "author" : [ "Lucas Dixon", "John Li", "Jeffrey Sorensen", "Nithum Thain", "Lucy Vasserman." ],
      "venue" : "Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, AIES ’18, page 67–73, New",
      "citeRegEx" : "Dixon et al\\.,? 2018",
      "shortCiteRegEx" : "Dixon et al\\.",
      "year" : 2018
    }, {
      "title" : "Best-worst scaling: theory and methods",
      "author" : [ "T.N. Flynn", "A.A.J. Marley." ],
      "venue" : "Stephane Hess and Andrew Daly, editors, Handbook of Choice Modelling, Chapters, chapter 8, pages 178–201. Edward Elgar Publishing.",
      "citeRegEx" : "Flynn and Marley.,? 2014",
      "shortCiteRegEx" : "Flynn and Marley.",
      "year" : 2014
    }, {
      "title" : "A survey on automatic detection of hate speech in text",
      "author" : [ "P. Fortuna", "S. Nunes." ],
      "venue" : "ACM Computing Surveys (CSUR), 51:1 – 30.",
      "citeRegEx" : "Fortuna and Nunes.,? 2018",
      "shortCiteRegEx" : "Fortuna and Nunes.",
      "year" : 2018
    }, {
      "title" : "Large scale crowdsourcing and characterization of twitter",
      "author" : [ "Antigoni-Maria Founta", "Constantinos Djouvas", "Despoina Chatzakou", "Ilias Leontiadis", "Jeremy Blackburn", "Gianluca Stringhini", "Athena Vakali", "Michael Sirivianos", "Nicolas Kourtellis" ],
      "venue" : null,
      "citeRegEx" : "Founta et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Founta et al\\.",
      "year" : 2018
    }, {
      "title" : "Detecting online hate speech using context aware models",
      "author" : [ "Lei Gao", "Ruihong Huang." ],
      "venue" : "pages 260– 266.",
      "citeRegEx" : "Gao and Huang.,? 2017",
      "shortCiteRegEx" : "Gao and Huang.",
      "year" : 2017
    }, {
      "title" : "The pragmatics of swearing",
      "author" : [ "T. Jay", "Kristin" ],
      "venue" : "Janschewitz",
      "citeRegEx" : "Jay and Kristin,? \\Q2008\\E",
      "shortCiteRegEx" : "Jay and Kristin",
      "year" : 2008
    }, {
      "title" : "Embracing ambiguity: A comparison of annotation methodologies for crowdsourcing word sense labels",
      "author" : [ "David Jurgens." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Jurgens.,? 2013",
      "shortCiteRegEx" : "Jurgens.",
      "year" : 2013
    }, {
      "title" : "Semeval-2012 task 2: Measuring degrees of relational similarity",
      "author" : [ "David A. Jurgens", "Peter D. Turney", "Saif M. Mohammad", "Keith J. Holyoak." ],
      "venue" : "Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceed-",
      "citeRegEx" : "Jurgens et al\\.,? 2012",
      "shortCiteRegEx" : "Jurgens et al\\.",
      "year" : 2012
    }, {
      "title" : "Bestworst scaling more reliable than rating scales: A case study on sentiment intensity annotation",
      "author" : [ "Svetlana Kiritchenko", "Saif Mohammad." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short",
      "citeRegEx" : "Kiritchenko and Mohammad.,? 2017",
      "shortCiteRegEx" : "Kiritchenko and Mohammad.",
      "year" : 2017
    }, {
      "title" : "Capturing reliable fine-grained sentiment associations by crowdsourcing and best–worst scaling",
      "author" : [ "Svetlana Kiritchenko", "Saif M. Mohammad." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Kiritchenko and Mohammad.,? 2016",
      "shortCiteRegEx" : "Kiritchenko and Mohammad.",
      "year" : 2016
    }, {
      "title" : "Towards ethics by design in online abusive content detection",
      "author" : [ "Svetlana Kiritchenko", "Isar Nejadgholi" ],
      "venue" : null,
      "citeRegEx" : "Kiritchenko and Nejadgholi.,? \\Q2020\\E",
      "shortCiteRegEx" : "Kiritchenko and Nejadgholi.",
      "year" : 2020
    }, {
      "title" : "Sentiment analysis of short informal text",
      "author" : [ "Svetlana Kiritchenko", "Xiaodan Zhu", "Saif Mohammad." ],
      "venue" : "The Journal of Artificial Intelligence Research (JAIR), 50.",
      "citeRegEx" : "Kiritchenko et al\\.,? 2014",
      "shortCiteRegEx" : "Kiritchenko et al\\.",
      "year" : 2014
    }, {
      "title" : "Towards a comprehensive taxonomy and large-scale annotated corpus for online slur usage",
      "author" : [ "Jana Kurrek", "Haji Mohammad Saleem", "Derek Ruths." ],
      "venue" : "Proceedings of the Fourth Workshop on Online Abuse and Harms, pages 138–149, Online. As-",
      "citeRegEx" : "Kurrek et al\\.,? 2020",
      "shortCiteRegEx" : "Kurrek et al\\.",
      "year" : 2020
    }, {
      "title" : "Best-worst scaling: A model for thelargest difference judgments",
      "author" : [ "J.J. Louviere." ],
      "venue" : "working paper.",
      "citeRegEx" : "Louviere.,? 1991",
      "shortCiteRegEx" : "Louviere.",
      "year" : 1991
    }, {
      "title" : "Best-Worst Scaling: Theory, Methods and Applications",
      "author" : [ "Jordan J. Louviere", "Terry N. Flynn", "A.A.J. Marley." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Louviere et al\\.,? 2015",
      "shortCiteRegEx" : "Louviere et al\\.",
      "year" : 2015
    }, {
      "title" : "Stratified validation splits for regression problems",
      "author" : [ "Scott C. Lowe" ],
      "venue" : null,
      "citeRegEx" : "Lowe.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lowe.",
      "year" : 2016
    }, {
      "title" : "Tackling online abuse: A survey of automated abuse detection methods",
      "author" : [ "Pushkar Mishra", "Helen Yannakoudakis", "Ekaterina Shutova." ],
      "venue" : "CoRR, abs/1908.06024.",
      "citeRegEx" : "Mishra et al\\.,? 2019",
      "shortCiteRegEx" : "Mishra et al\\.",
      "year" : 2019
    }, {
      "title" : "Model cards for model reporting",
      "author" : [ "Margaret Mitchell", "Simone Wu", "Andrew Zaldivar", "Parker Barnes", "Lucy Vasserman", "Ben Hutchinson", "Elena Spitzer", "Inioluwa Deborah Raji", "Timnit Gebru." ],
      "venue" : "CoRR, abs/1810.03993.",
      "citeRegEx" : "Mitchell et al\\.,? 2018",
      "shortCiteRegEx" : "Mitchell et al\\.",
      "year" : 2018
    }, {
      "title" : "Obtaining reliable human ratings of valence, arousal, and dominance for 20,000 English words",
      "author" : [ "Saif Mohammad." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 174–",
      "citeRegEx" : "Mohammad.,? 2018",
      "shortCiteRegEx" : "Mohammad.",
      "year" : 2018
    }, {
      "title" : "Emotion intensities in tweets",
      "author" : [ "Saif Mohammad", "Felipe Bravo-Marquez." ],
      "venue" : "Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 65–77, Vancouver, Canada. Association for Computational Linguistics.",
      "citeRegEx" : "Mohammad and Bravo.Marquez.,? 2017",
      "shortCiteRegEx" : "Mohammad and Bravo.Marquez.",
      "year" : 2017
    }, {
      "title" : "Understanding emotions: A dataset of tweets to study interactions between affect categories",
      "author" : [ "Saif Mohammad", "Svetlana Kiritchenko." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018),",
      "citeRegEx" : "Mohammad and Kiritchenko.,? 2018",
      "shortCiteRegEx" : "Mohammad and Kiritchenko.",
      "year" : 2018
    }, {
      "title" : "The protection of children online: A brief scoping review to identify vulnerable groups",
      "author" : [ "Emily Munro" ],
      "venue" : null,
      "citeRegEx" : "Munro.,? \\Q2011\\E",
      "shortCiteRegEx" : "Munro.",
      "year" : 2011
    }, {
      "title" : "Maxdiff analysis: Simple counting,individual-level logit, and hb",
      "author" : [ "B. Orme." ],
      "venue" : "sawtooth software, inc.",
      "citeRegEx" : "Orme.,? 2009",
      "shortCiteRegEx" : "Orme.",
      "year" : 2009
    }, {
      "title" : "The Measurement of meaning",
      "author" : [ "C.E. Osgood", "G.J. Suci", "P.H. Tenenbaum." ],
      "venue" : "University of Illinois Press, Urbana:.",
      "citeRegEx" : "Osgood et al\\.,? 1957",
      "shortCiteRegEx" : "Osgood et al\\.",
      "year" : 1957
    }, {
      "title" : "Reducing gender bias in abusive language detection",
      "author" : [ "Ji Ho Park", "Jamin Shin", "Pascale Fung." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2799–2804, Brussels, Belgium. Association",
      "citeRegEx" : "Park et al\\.,? 2018",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2018
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha,",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "author" : [ "Soujanya Poria", "Navonil Majumder", "Rada Mihalcea", "Eduard Hovy" ],
      "venue" : null,
      "citeRegEx" : "Poria et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Poria et al\\.",
      "year" : 2019
    }, {
      "title" : "Research in the crowdsourcing age, a case study",
      "author" : [ "PRC." ],
      "venue" : "Accessed: 2020-10-10.",
      "citeRegEx" : "PRC.,? 2020",
      "shortCiteRegEx" : "PRC.",
      "year" : 2020
    }, {
      "title" : "Questions and Answers in Attitude Surveys: Experiments on Question Form, Wording, and Context",
      "author" : [ "Stanley Presser", "Howard Schuman." ],
      "venue" : "SAGE Publications, Inc.",
      "citeRegEx" : "Presser and Schuman.,? 1996",
      "shortCiteRegEx" : "Presser and Schuman.",
      "year" : 1996
    }, {
      "title" : "Investigating sampling bias in abusive language detection",
      "author" : [ "Dante Razo", "Sandra Kübler." ],
      "venue" : "Proceedings of the Fourth Workshop on Online Abuse and Harms, pages 70–78, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Razo and Kübler.,? 2020",
      "shortCiteRegEx" : "Razo and Kübler.",
      "year" : 2020
    }, {
      "title" : "Chapter Eight: Commercial Content Moderation: Digital Laborers’ Dirty Work",
      "author" : [ "Sarah T. Roberts." ],
      "venue" : "Peter Lang.",
      "citeRegEx" : "Roberts.,? 2016",
      "shortCiteRegEx" : "Roberts.",
      "year" : 2016
    }, {
      "title" : "Behind the Screen: Content Moderation in the Shadows of Social Media",
      "author" : [ "Sarah T. Roberts." ],
      "venue" : "Yale University Press.",
      "citeRegEx" : "Roberts.,? 2019",
      "shortCiteRegEx" : "Roberts.",
      "year" : 2019
    }, {
      "title" : "Core affect and the psychological construction of emotion",
      "author" : [ "James Russell." ],
      "venue" : "Psychological review, 110:145–72.",
      "citeRegEx" : "Russell.,? 2003",
      "shortCiteRegEx" : "Russell.",
      "year" : 2003
    }, {
      "title" : "A circumplex model of affect",
      "author" : [ "James A. Russell." ],
      "venue" : "Journal of Personality and Social Psychology, 39(6):1161–1178.",
      "citeRegEx" : "Russell.,? 1980",
      "shortCiteRegEx" : "Russell.",
      "year" : 1980
    }, {
      "title" : "The risk of racial bias in hate speech detection",
      "author" : [ "Maarten Sap", "Dallas Card", "Saadia Gabriel", "Yejin Choi", "Noah A. Smith." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1668–1678, Florence,",
      "citeRegEx" : "Sap et al\\.,? 2019",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2019
    }, {
      "title" : "Social bias frames: Reasoning about social and power implications of language",
      "author" : [ "Maarten Sap", "Saadia Gabriel", "Lianhui Qin", "Dan Jurafsky", "Noah A Smith", "Yejin Choi." ],
      "venue" : "ACL.",
      "citeRegEx" : "Sap et al\\.,? 2020",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2020
    }, {
      "title" : "A survey on hate speech detection using natural language processing",
      "author" : [ "Anna Schmidt", "Michael Wiegand." ],
      "venue" : "Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media, pages 1–10, Valencia, Spain. Associa-",
      "citeRegEx" : "Schmidt and Wiegand.,? 2017",
      "shortCiteRegEx" : "Schmidt and Wiegand.",
      "year" : 2017
    }, {
      "title" : "Exposure to hate speech increases prejudice through desensitization",
      "author" : [ "Wiktor Soral", "M. Bilewicz", "M. Winiewski." ],
      "venue" : "Aggressive Behavior, 44:13V–146.",
      "citeRegEx" : "Soral et al\\.,? 2018",
      "shortCiteRegEx" : "Soral et al\\.",
      "year" : 2018
    }, {
      "title" : "Directions in abusive language training data: Garbage in, garbage out",
      "author" : [ "Bertie Vidgen", "Leon Derczynski" ],
      "venue" : null,
      "citeRegEx" : "Vidgen and Derczynski.,? \\Q2020\\E",
      "shortCiteRegEx" : "Vidgen and Derczynski.",
      "year" : 2020
    }, {
      "title" : "Challenges and frontiers in abusive content detection",
      "author" : [ "Bertie Vidgen", "Alex Harris", "Dong Nguyen", "Rebekah Tromble", "Scott Hale", "Helen Margetts." ],
      "venue" : "Proceedings of the Third Workshop on Abusive Language Online, pages 80–93, Florence, Italy.",
      "citeRegEx" : "Vidgen et al\\.,? 2019",
      "shortCiteRegEx" : "Vidgen et al\\.",
      "year" : 2019
    }, {
      "title" : "Cursing in english on twitter",
      "author" : [ "Wenbo Wang", "Lu Chen", "Krishnaprasad Thirunarayan", "Amit P. Sheth." ],
      "venue" : "Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing, CSCW ’14, page 415–425, New York,",
      "citeRegEx" : "Wang et al\\.,? 2014",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    }, {
      "title" : "Are you a racist or am I seeing things? annotator influence on hate speech detection on twitter",
      "author" : [ "Zeerak Waseem." ],
      "venue" : "Proceedings of the First Workshop on NLP and Computational Social Science, pages 138– 142, Austin, Texas. Association for Computational",
      "citeRegEx" : "Waseem.,? 2016",
      "shortCiteRegEx" : "Waseem.",
      "year" : 2016
    }, {
      "title" : "Understanding abuse: A typology of abusive language detection subtasks",
      "author" : [ "Zeerak Waseem", "Thomas Davidson", "Dana Warmsley", "Ingmar Weber." ],
      "venue" : "Proceedings of the First Workshop on Abusive Language Online, pages 78–84, Vancouver, BC, Canada.",
      "citeRegEx" : "Waseem et al\\.,? 2017",
      "shortCiteRegEx" : "Waseem et al\\.",
      "year" : 2017
    }, {
      "title" : "Hateful symbols or hateful people? predictive features for hate speech detection on twitter",
      "author" : [ "Zeerak Waseem", "Dirk Hovy." ],
      "venue" : "Proceedings of the NAACL Student Research Workshop, pages 88–93, San Diego, California. Association for Computa-",
      "citeRegEx" : "Waseem and Hovy.,? 2016",
      "shortCiteRegEx" : "Waseem and Hovy.",
      "year" : 2016
    }, {
      "title" : "Transfer learning from LDA to bilstm-cnn for offensive language detection in twitter",
      "author" : [ "Gregor Wiedemann", "Eugen Ruppert", "Raghav Jindal", "Chris Biemann." ],
      "venue" : "CoRR, abs/1811.02906.",
      "citeRegEx" : "Wiedemann et al\\.,? 2018",
      "shortCiteRegEx" : "Wiedemann et al\\.",
      "year" : 2018
    }, {
      "title" : "Implicitly abusive comparisons – a new dataset and linguistic analysis",
      "author" : [ "Michael Wiegand", "Maja Geulig", "Josef Ruppenhofer." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Vol-",
      "citeRegEx" : "Wiegand et al\\.,? 2021",
      "shortCiteRegEx" : "Wiegand et al\\.",
      "year" : 2021
    }, {
      "title" : "Detection of Abusive Language: the Problem of Biased Datasets",
      "author" : [ "Michael Wiegand", "Josef Ruppenhofer", "Thomas Kleinbauer." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Wiegand et al\\.,? 2019",
      "shortCiteRegEx" : "Wiegand et al\\.",
      "year" : 2019
    }, {
      "title" : "Ex machina: Personal attacks seen at scale",
      "author" : [ "Ellery Wulczyn", "Nithum Thain", "Lucas Dixon." ],
      "venue" : "CoRR, abs/1610.08914.",
      "citeRegEx" : "Wulczyn et al\\.,? 2016",
      "shortCiteRegEx" : "Wulczyn et al\\.",
      "year" : 2016
    }, {
      "title" : "SemEval-2019 task 6: Identifying and categorizing offensive language in social media (OffensEval)",
      "author" : [ "Marcos Zampieri", "Shervin Malmasi", "Preslav Nakov", "Sara Rosenthal", "Noura Farra", "Ritesh Kumar." ],
      "venue" : "Proceedings of the 13th International Work-",
      "citeRegEx" : "Zampieri et al\\.,? 2019",
      "shortCiteRegEx" : "Zampieri et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 32,
      "context" : "Such language is pervasive online (Statista, 2020b), and exposure to it may have numerous negative consequences for the victim’s mental health (Munro, 2011).",
      "startOffset" : 143,
      "endOffset" : 156
    }, {
      "referenceID" : 55,
      "context" : "There are several challenges in the automatic detection of offensive language (Wiedemann et al., 2018).",
      "startOffset" : 78,
      "endOffset" : 102
    }, {
      "referenceID" : 14,
      "context" : "ambiguity and annotation inconsistency (Founta et al., 2018).",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 53,
      "context" : "This results in datasets that are rich in explicit offensive language (language that is unambiguous in its potential to be offensive, such as those using slurs or swear words (Waseem et al., 2017)) but lack cases of implicit offensive language (language with its true offensive nature obscured due to lack of unambiguous swear words, usage of sarcasm or offensive analogies, and others (Waseem et al.",
      "startOffset" : 175,
      "endOffset" : 196
    }, {
      "referenceID" : 53,
      "context" : ", 2017)) but lack cases of implicit offensive language (language with its true offensive nature obscured due to lack of unambiguous swear words, usage of sarcasm or offensive analogies, and others (Waseem et al., 2017; Wiegand et al., 2021)) (Waseem, 2016; Wiegand et al.",
      "startOffset" : 197,
      "endOffset" : 240
    }, {
      "referenceID" : 56,
      "context" : ", 2017)) but lack cases of implicit offensive language (language with its true offensive nature obscured due to lack of unambiguous swear words, usage of sarcasm or offensive analogies, and others (Waseem et al., 2017; Wiegand et al., 2021)) (Waseem, 2016; Wiegand et al.",
      "startOffset" : 197,
      "endOffset" : 240
    }, {
      "referenceID" : 15,
      "context" : "Offensive language is, however, inherently a social phenomenon and its analysis has much to gain from taking the conversational context into account (Gao and Huang, 2017).",
      "startOffset" : 149,
      "endOffset" : 170
    }, {
      "referenceID" : 39,
      "context" : "rating scales, such as scale-region bias (Presser and Schuman, 1996; Asaadi et al., 2019), and improves annotation consistency (Kiritchenko and Mohammad, 2017).",
      "startOffset" : 41,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : "rating scales, such as scale-region bias (Presser and Schuman, 1996; Asaadi et al., 2019), and improves annotation consistency (Kiritchenko and Mohammad, 2017).",
      "startOffset" : 41,
      "endOffset" : 89
    }, {
      "referenceID" : 19,
      "context" : ", 2019), and improves annotation consistency (Kiritchenko and Mohammad, 2017).",
      "startOffset" : 45,
      "endOffset" : 77
    }, {
      "referenceID" : 24,
      "context" : "Thus, we annotate our dataset using an efficient form of comparative annotation called Best–Worst Scaling (BWS) (Louviere, 1991; Louviere et al., 2015; Kiritchenko and Mohammad, 2016, 2017).",
      "startOffset" : 112,
      "endOffset" : 189
    }, {
      "referenceID" : 25,
      "context" : "Thus, we annotate our dataset using an efficient form of comparative annotation called Best–Worst Scaling (BWS) (Louviere, 1991; Louviere et al., 2015; Kiritchenko and Mohammad, 2016, 2017).",
      "startOffset" : 112,
      "endOffset" : 189
    }, {
      "referenceID" : 52,
      "context" : "Due to prevalence of the non-offensive class in naturallyoccurring data (Waseem, 2016; Founta et al., 2018), the authors devised techniques to boost the presence of the offensive class in the dataset.",
      "startOffset" : 72,
      "endOffset" : 107
    }, {
      "referenceID" : 14,
      "context" : "Due to prevalence of the non-offensive class in naturallyoccurring data (Waseem, 2016; Founta et al., 2018), the authors devised techniques to boost the presence of the offensive class in the dataset.",
      "startOffset" : 72,
      "endOffset" : 107
    }, {
      "referenceID" : 53,
      "context" : "Past work has partitioned offensive comments into explicitly offensive (those that include profanity—swear words, taboo words, or hate terms) and implicitly offensive (those that do not include profanity) (Waseem et al., 2017; Caselli et al., 2020a; Wiegand et al., 2021).",
      "startOffset" : 205,
      "endOffset" : 271
    }, {
      "referenceID" : 6,
      "context" : "Past work has partitioned offensive comments into explicitly offensive (those that include profanity—swear words, taboo words, or hate terms) and implicitly offensive (those that do not include profanity) (Waseem et al., 2017; Caselli et al., 2020a; Wiegand et al., 2021).",
      "startOffset" : 205,
      "endOffset" : 271
    }, {
      "referenceID" : 56,
      "context" : "Past work has partitioned offensive comments into explicitly offensive (those that include profanity—swear words, taboo words, or hate terms) and implicitly offensive (those that do not include profanity) (Waseem et al., 2017; Caselli et al., 2020a; Wiegand et al., 2021).",
      "startOffset" : 205,
      "endOffset" : 271
    }, {
      "referenceID" : 33,
      "context" : "Real-valued scores of associations are calculated between the items and the property of interest from the best–worst annotations for a set of 4-tuples (Orme, 2009; Flynn and Marley, 2014).",
      "startOffset" : 151,
      "endOffset" : 187
    }, {
      "referenceID" : 12,
      "context" : "Real-valued scores of associations are calculated between the items and the property of interest from the best–worst annotations for a set of 4-tuples (Orme, 2009; Flynn and Marley, 2014).",
      "startOffset" : 151,
      "endOffset" : 187
    }, {
      "referenceID" : 18,
      "context" : "Within the NLP community, BWS has thus far been used only for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko et al.",
      "startOffset" : 106,
      "endOffset" : 128
    }, {
      "referenceID" : 17,
      "context" : ", 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko et al.",
      "startOffset" : 35,
      "endOffset" : 50
    }, {
      "referenceID" : 22,
      "context" : ", 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko et al., 2014), phrase sentiment composition (Kiritchenko and Mohammad, 2016), and tweet-emotion intensity (Mohammad and Bravo-Marquez, 2017; Mohammad and Kiritchenko, 2018).",
      "startOffset" : 77,
      "endOffset" : 103
    }, {
      "referenceID" : 20,
      "context" : ", 2014), phrase sentiment composition (Kiritchenko and Mohammad, 2016), and tweet-emotion intensity (Mohammad and Bravo-Marquez, 2017; Mohammad and Kiritchenko, 2018).",
      "startOffset" : 38,
      "endOffset" : 70
    }, {
      "referenceID" : 30,
      "context" : ", 2014), phrase sentiment composition (Kiritchenko and Mohammad, 2016), and tweet-emotion intensity (Mohammad and Bravo-Marquez, 2017; Mohammad and Kiritchenko, 2018).",
      "startOffset" : 100,
      "endOffset" : 166
    }, {
      "referenceID" : 31,
      "context" : ", 2014), phrase sentiment composition (Kiritchenko and Mohammad, 2016), and tweet-emotion intensity (Mohammad and Bravo-Marquez, 2017; Mohammad and Kiritchenko, 2018).",
      "startOffset" : 100,
      "endOffset" : 166
    }, {
      "referenceID" : 2,
      "context" : "We extracted Reddit data from the Pushshift repository (Baumgartner et al., 2020) using Google BigQuery.",
      "startOffset" : 55,
      "endOffset" : 81
    }, {
      "referenceID" : 37,
      "context" : "Emotions are highly representative of one’s mental state, which in turn are associated with their behaviour (Poria et al., 2019).",
      "startOffset" : 108,
      "endOffset" : 128
    }, {
      "referenceID" : 29,
      "context" : "Dominance is powerful–weak or ‘have full control’–‘have no control’ dimension (Mohammad, 2018).",
      "startOffset" : 78,
      "endOffset" : 94
    }, {
      "referenceID" : 29,
      "context" : "To boost the representation of offensive and emotional comments in our dataset, we up-sampled comments that included low-valence (highly negative) words and those that included high-arousal words (as per the NRC VAD lexicon (Mohammad, 2018)).",
      "startOffset" : 224,
      "endOffset" : 240
    }, {
      "referenceID" : 4,
      "context" : "Crowdsourcing helps us get an aggregation of varied perspectives rather than expert opinions which can leave out offensiveness in a comment that lies outside the ‘typical’ offensiveness norms (Blackwell et al., 2017).",
      "startOffset" : 192,
      "endOffset" : 216
    }, {
      "referenceID" : 33,
      "context" : "The BWS responses were converted to scores using a simple counting procedure (Orme, 2009; Flynn and Marley, 2014).",
      "startOffset" : 77,
      "endOffset" : 113
    }, {
      "referenceID" : 12,
      "context" : "The BWS responses were converted to scores using a simple counting procedure (Orme, 2009; Flynn and Marley, 2014).",
      "startOffset" : 77,
      "endOffset" : 113
    }, {
      "referenceID" : 45,
      "context" : "Identity terms A common criticism of the existing offensive language datasets is that in those datasets, certain identity terms (particularly those referring to minority groups) occur mainly in texts that are offensive (Sap et al., 2019; Davidson et al., 2019; Wiegand et al., 2019; Park et al., 2018; Dixon et al., 2018).",
      "startOffset" : 219,
      "endOffset" : 321
    }, {
      "referenceID" : 8,
      "context" : "Identity terms A common criticism of the existing offensive language datasets is that in those datasets, certain identity terms (particularly those referring to minority groups) occur mainly in texts that are offensive (Sap et al., 2019; Davidson et al., 2019; Wiegand et al., 2019; Park et al., 2018; Dixon et al., 2018).",
      "startOffset" : 219,
      "endOffset" : 321
    }, {
      "referenceID" : 57,
      "context" : "Identity terms A common criticism of the existing offensive language datasets is that in those datasets, certain identity terms (particularly those referring to minority groups) occur mainly in texts that are offensive (Sap et al., 2019; Davidson et al., 2019; Wiegand et al., 2019; Park et al., 2018; Dixon et al., 2018).",
      "startOffset" : 219,
      "endOffset" : 321
    }, {
      "referenceID" : 35,
      "context" : "Identity terms A common criticism of the existing offensive language datasets is that in those datasets, certain identity terms (particularly those referring to minority groups) occur mainly in texts that are offensive (Sap et al., 2019; Davidson et al., 2019; Wiegand et al., 2019; Park et al., 2018; Dixon et al., 2018).",
      "startOffset" : 219,
      "endOffset" : 321
    }, {
      "referenceID" : 11,
      "context" : "Identity terms A common criticism of the existing offensive language datasets is that in those datasets, certain identity terms (particularly those referring to minority groups) occur mainly in texts that are offensive (Sap et al., 2019; Davidson et al., 2019; Wiegand et al., 2019; Park et al., 2018; Dixon et al., 2018).",
      "startOffset" : 219,
      "endOffset" : 321
    }, {
      "referenceID" : 51,
      "context" : "Swear words We identified 868 comments in our dataset that contain at least one swear word from the cursing lexicon (Wang et al., 2014).",
      "startOffset" : 116,
      "endOffset" : 135
    }, {
      "referenceID" : 10,
      "context" : "We used MSE loss as the objective function, batch size of 16, and learning rate of 2e− 5 (other hyperparameters same as (Devlin et al., 2019)).",
      "startOffset" : 120,
      "endOffset" : 141
    }, {
      "referenceID" : 26,
      "context" : "Since we have a linear regression task, we created folds using sorted stratification (Lowe, 2016) to ensure that the distribution of all the partitions is similar.",
      "startOffset" : 85,
      "endOffset" : 97
    }, {
      "referenceID" : 59,
      "context" : "HateBERT has been shown to outperform the general purpose BERT model on the offensive language detection task when finetuned on popular datasets such as OffensEval 2019 (Zampieri et al., 2019), AbusEval (Caselli et al.",
      "startOffset" : 169,
      "endOffset" : 192
    }, {
      "referenceID" : 6,
      "context" : ", 2019), AbusEval (Caselli et al., 2020a), and HatEval (Basile et al.",
      "startOffset" : 18,
      "endOffset" : 41
    }, {
      "referenceID" : 38,
      "context" : "They found out that 80% of the crowd-workers on AMT are from the US (PRC, 2020).",
      "startOffset" : 68,
      "endOffset" : 79
    }, {
      "referenceID" : 28,
      "context" : "Owing to privacy concerns, we do not model user history to predict offensiveness scores (Mitchell et al., 2018).",
      "startOffset" : 88,
      "endOffset" : 111
    } ],
    "year" : 2021,
    "abstractText" : "Warning: This paper contains comments that may be offensive or upsetting. On social media platforms, hateful and offensive language negatively impact the mental well-being of users and the participation of people from diverse backgrounds. Automatic methods to detect offensive language have largely relied on datasets with categorical labels. However, comments can vary in their degree of offensiveness. We create the first dataset of English language Reddit comments that has fine-grained, real-valued scores between -1 (maximally supportive) and 1 (maximally offensive). The dataset was annotated using Best–Worst Scaling, a form of comparative annotation that has been shown to alleviate known biases of using rating scales. We show that the method produces highly reliable offensiveness scores. Finally, we evaluate the ability of widely-used neural models to predict offensiveness scores on this new dataset.",
    "creator" : "LaTeX with hyperref"
  }
}