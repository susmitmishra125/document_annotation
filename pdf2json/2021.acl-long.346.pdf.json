{
  "name" : "2021.acl-long.346.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Evaluation Examples Are Not Equally Informative: How Should That Change NLP Leaderboards?",
    "authors" : [ "Pedro Rodriguez", "Joe Barrow", "Alexander Hoyle", "John P. Lalor", "Robin Jia", "Jordan Boyd-Graber" ],
    "emails" : [ "me@pedro.ai", "jdbarrow@cs.umd.edu", "hoyle@umd.edu", "john.lalor@nd.edu", "robinjia@usc.edu", "jbg@umiacs.umd.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4486–4503\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4486"
    }, {
      "heading" : "1 Leaderboards are Shiny",
      "text" : "Leaderboard evaluations—for better or worse—are the de facto standard for measuring progress in question answering (Rajpurkar et al., 2016) and in many NLP tasks (Wang et al., 2019a). An unfortunate side effect of leaderboard popularity is SOTA-chasing, often at the expense of carefully inspecting data and models (Linzen, 2020). For example, the same “super-human” models that top question answering leaderboards (Najberg, 2018) often fail spectacularly (Feng et al., 2018; Wallace et al., 2019a) by learning non-generalizable statistical patterns (McCoy et al., 2019; Niven and Kao, 2019). Finally, focusing solely on metrics conflates progress on a specific task with progress on realworld NLP problems behind the task (Bender and Koller, 2020). Plainly, focusing on headline SOTA numbers “provide(s) limited value for scientific progress absent insight into what drives them” and where they fail (Lipton and Steinhardt, 2019).\n∗Work completed at University of Maryland.\nIn this work we take leaderboards “as they are,” and imagine how they might better support research. Leaderboards establish differences between models on a fixed task. Hence, leaderboards should enable and encourage the comparison of models and inspection of examples. And leaderboards should also signal when they have outlived their usefulness (Boyd-Graber and Börschinger, 2020)."
    }, {
      "heading" : "1.1 How to Direct Leaderboards’ Light",
      "text" : "To help focus attention on examples and models of interest, we propose Difficulty and Ability Discriminating (DAD) leaderboards that explicitly model both task and submissions jointly, rather than either in isolation.1 DAD’s underlying model is based on\n1Source code, data, and visualizations at irt.pedro.ai.\nItem Response Theory (Lord et al., 1968; Baker, 2001, IRT, reviewed in §2), a widely used (van Rijn et al., 2016) alternative in educational testing to simple summary statistics (Edgeworth, 1888).\nDAD can explicitly identify the difficulty and discriminability of items (Figure 1),2 which in turn can lead to a more nuanced ranking of models, identifying poor items, and better understanding of a dataset and task. Throughout the paper, we use the question answering (QA) benchmark SQuAD 2.0 (Rajpurkar et al., 2018). For example, DAD can identify questions that are challenging to models and questions that are wrong (incorrectly annotated). In addition to better understanding datasets, it is also helpful for efficiently selecting evaluation items to annotate. We conclude with recommendations for future leaderboards (§7) and discuss where IRT in NLP can go next (§8)."
    }, {
      "heading" : "2 A Generative Story for Leaderboards",
      "text" : "Leaderboards are a product of the metrics, evaluation data, and subjects (machine or human) who answer items (Figure 2). For concreteness, let’s assume that we have a question-answering task and two subjects: Ken, who is good at trivia, and Burt, who is not. In the simplest IRT models, each subject j has a random variable θj corresponding to their skill: Ken’s is big, Burt’s is small.\nBut you cannot know that until you start asking them questions of varying difficulty βi. Harder questions have a higher difficulty (“what is the airspeed of an unladen swallow”) than easy ones (“who is buried in Grant’s tomb”). The bigger the margin between a subject’s skill θj and an item’s difficulty βi, θj − βi, the more likely that subject j responds correctly pi,j(ri,j = 1). This is the simplest IRT model, which we call IRT-base.\n2Example and feasibility distribution in Appendix A. Interactive visualization linked from http://irt.pedro.ai.\nGenerally, given n test items X = (X1, . . . , Xn) and m subjects S = (S1, . . . , Sm), where each subject answers every item, we want to estimate subject skills and item difficulties. To discover the random variables that best explain the data, we turn to probabilistic inference (Pearl, 1988).\nTwo additional random variables further improve DAD: discriminability γi and feasibility λi. We first consider discriminability and the margin between a question’s difficulty βi and a subject’s skill θj . A discriminative question is challenging but can still be answered correctly by a strong subject. If Ken’s ability is higher than most items’ difficulty (θj −βi is large), item discriminability multiplies this gap by γi in a model called IRT-disc. Questions with low γi are low quality: they have annotation error or do not make sense.\nAnother way of capturing poor quality questions is the feasibility λi. For example, if the question “who was the first president” has the answer Rajendra Prasad, the question has an unstated implicit assumption that subjects must guess what country or company the question is about. In the model IRT-feas, if a large fraction of subjects all get an item wrong, everyone’s probability of getting the item right is capped at λi. In NLP terms, 1 − λi corresponds to the prevalence of annotation errors that lead to unsolvable items.\nHaving introduced all of the constituent elements of the model, we can now present the full generative model:\n1. For each subject j: (a) Draw skill θj ∼ N (µθ, τ−1θ ) 2. For each item i: (a) Draw difficulty βi ∼ N (µβ, τ−1β ) (b) Draw discriminability γi ∼ N (µγ , τ−1γ ) (c) Draw feasibility λi ∼ U[0, 1] 3. Draw subject i response on item j, rij ∼ pij(rij | θj , βi, λi) =\npij(rij = 1|θj) = λi\n1 + e−γi(θj−βi) . (1)\nFor IRT-base, γi and λi are fixed to 1.0, while for IRT-disc, only λi is fixed.3\nMeans µθ, µβ, µγ are drawn from N (0, 106) and τθ, τβ, τγ from a Γ(1, 1) prior, as in Lalor et al. (2019) and recommended by Natesan et al. (2016).4\n3In psychometrics, IRT-base is called a Rasch (Rasch, 1960) or 1 parameter logistic (1PL) model, IRT-disc is a 2PL model, and IRT-feas is a 4PL model with guessing set to zero.\n4We differ by allowing γ < 0 to identify bad items.\nBecause it is difficult to completely codify skill and difficulty into a single number, we can rewrite the exponent in Equation 1 as a sum over dimensions −γi( ∑︁ k θj,k − βi,k), where each dimension captures the interaction between an item’s difficulty and a subject’s skill. For example, perhaps Burt could better exploit artifacts in one dimension (their skill for θj,k=5 is high but everywhere else is low) while Ken might not know much about a particular topic like potent potables (θj,k=2 is low but everywhere else is high). We call this model IRT-vec.5 Multidimensional IRT models (Reckase, 2009) could—in addition to better modeling difficulty—also cluster items for interpretation; we briefly experiment with this (Appendix F), but leave more to future work (§8)."
    }, {
      "heading" : "2.1 Examples are Not Equally Useful",
      "text" : "IRT’s fundamental assumption is that not all items and subjects are equal. This explains why leaderboards can fail while having “normal looking” accuracies. As a thought experiment, consider a dataset that is one third easy (βi ∈ [0, 1]), one third medium difficulty (βi ∈ [2, 3]), and one third hard (βi ∈ [6, 7]). Suppose that Ken has skill θk = 4 while Burt has skill θb = 2. A standard leaderboard would say that Ken has higher accuracy than Burt. But suppose there’s a new subject that wants to challenge Ken; they are not going to reliably dethrone Ken until their skill θc is greater than six.\nThis is a more mathematical formulation of the “easy” and “hard” dataset splits in question answering (Sugawara et al., 2018; Rondeau and Hazen, 2018; Sen and Saffari, 2020). In IRT-feas, this recapitulates the observation of Boyd-Graber and Börschinger (2020) that annotation error can hinder effective leaderboards. DAD helps systematize these observations and diagnose dataset issues."
    }, {
      "heading" : "2.2 Inference",
      "text" : "To estimate the latent parameters of our model, we use mean-field variational inference (Jordan et al., 1999). In variational inference, we propose a distribution over the latent variables, qϕ(·), that approximates the true but intractable posterior p(·). We then minimize the KL-divergence between these distributions, equivalent to maximizing the evidence lower-bound (ELBO) with respect to the variational parameters.\n5We do not incorporate feasibility into the IRT-vec model since it already improves over 1D models without it.\nIn our case, qϕ(·) is a mean-field distribution, which means it factorizes over each of the latent variables (the product is over the n × m subjectitem pairs)\nqϕ(θ,β,γ,µ, τ ) = q(µ)q(τ ) ∏︂ i,j q(θj)q(βi)q(γi) Specifically, for our key latent variables z ∈ {θ,β,γ}, the associated variational distributions are of the form q(z) = N (uz, t−1z ). Recall that in the generative distribution, each latent z is drawn from a N (µz, τ−1z ) whose parameters are also latent variables; for these variables, we use the variational distributions q(µz) = N (uµz , t−1µz ) and q(τz) = Γ(aτz , bτz). We optimize the ELBO with respect to the variational parameters\nϕ = {uz, tz,uµz , tµz ,aτz , bτz ,λ} for all z using ADAM (Kingma and Ba, 2015).\nWith DAD’s leaderboard IRT model introduced, we next discuss how leaderboard subjects are statistically compared and alternative methods—such as using IRT parameters—to evaluate whether two models are truly different."
    }, {
      "heading" : "3 Ranking and Comparing Subjects",
      "text" : "Fundamentally, the objective of comparative evaluations like leaderboards is to decide whether model A is better than model B. A thread of NLP has rightfully advocated for adding rigor to these decisions using statistics (Traub, 1997, Classical Testing Theory) where the objective is to infer a true score T from the observed test score X = T+E given a measurement error E, uniform across subjects. However, in educational testing—a field measuring skill and knowledge in humans—IRT is a primary measurement instrument (Hambleton, 1991, p. 2). A major motivation for IRT is that subjects of different skill have different errors. IRT explicitly accounts for the bandwidth-fidelity dilemma (McBride, 1976): items can either accurately measure a narrow ability range (fidelity) or inaccurately measure large ability ranges (bandwidth).6 This section and the next contrast methods for identifying the best model and advocate for IRT.\nImplicit in nearly all leaderboard evaluations is ranking models by a statistic such as the average accuracy. As we show in §4, naïve rankings are noisier than IRT rankings.\n6Estimation error of θ varies by position (Appendix E)."
    }, {
      "heading" : "4 IRT for Leaderboards",
      "text" : "Leaderboards should: (1) reliably and efficiently rank better models ahead of worse models (TagueSutcliffe, 1992; Voorhees, 2003) and (2) guide inspection of items and subjects (§5). The first ameliorates the unavoidable randomness of finite evaluations while the second enables error analysis (Wu et al., 2019) and model probing (Belinkov and Glass, 2019; Zhang et al., 2019). First we verify that IRT models accurately predict the responses of subjects (§4.2). Next, a ranking stability analysis shows that IRT has modestly better reliability than classical rankings (§4.2.3). Lastly, using IRT to actively sample items for annotation yields rankings with better correlation to complete test data (§4.4)."
    }, {
      "heading" : "4.1 Why a Linear Model Baseline",
      "text" : "At first blush, the differences between IRT and logistic regression are minimal, but we include the comparison to address natural questions from the NLP community: (1) do the idiosyncrasies of the IRT formulation hurt accuracy? (2) should we add features to better understand phenomena in the questions? (3) why not use deep models?\nThe next section argues that both IRT and logistic regression are accurate even without laboriously engineered task-specific features. Adding obvious features such as item words (e.g., questions) only minimally improves the accuracy. We explicitly omit less interpretable deep models since our goal is to make leaderboards more interpretable."
    }, {
      "heading" : "4.2 Response Prediction is Accurate",
      "text" : "Just as educational testing researchers validate IRT models by seeing if they predict subject responses correctly (American Educational Research Association, 2014), we validate how well DAD predicts whether SQuAD models get questions right.\nWe compare against a logistic regression linear model (LM) implemented with Vowpal Wabbit (Agarwal et al., 2014). Since integrating handcrafted features is easy, we incorporate features derived from subject IDs; item IDs; functions of the SQuAD question, answer, and title; and IRT parameters (details in Appendix B). As in IRT, logistic regression predicts whether a subject correctly responds to an item. Later, we discuss ways to integrate more features into IRT (§8)."
    }, {
      "heading" : "4.2.1 SQuAD Leaderboard Data",
      "text" : "Experiments are on the SQuAD 2.0 leaderboard. Development data are publicly available, and orga-\nnizers provide test set responses. There are 161 development subjects, 115 test subjects, and 11,873 items (1.9 million total pairs). Experiments that do not need test responses use all development subjects; those that do use the smaller test subset."
    }, {
      "heading" : "4.2.2 Evaluation Scheme",
      "text" : "Following prior work (Wu et al., 2020), we evaluate IRT and linear models by holding out 10% of responses and computing classification metrics.7 In SQuAD, predicting whether a response is correct is an imbalanced classification problem (80.4% of responses in the development set are correct). Thus, we use ROC AUC, macro F1, and accuracy."
    }, {
      "heading" : "4.2.3 IRT Response Prediction is Accurate",
      "text" : "IRT models that incorporate more priors into the generative story should be better, but are they? We compare four IRT models: IRT-base, IRT-disc, IRTfeas, and IRT-vec (§2). The more sophisticated models are better and all improve over the LM (Figure 3) and correlate well with each other (Appendix C). To be clear, while higher accuracy than LM is good, our goal is to validate that IRT models are accurate; later, we inspect model errors and identify annotation errors (§5)."
    }, {
      "heading" : "4.2.4 What Model Features are Predictive?",
      "text" : "Integrating additional features into Bayesian models is not trivial, so we instead use the flexibility of linear models to identify useful features. Our leaveone-in ablation compares features (Figure 3): the top ablations both use IRT features, further validating IRT parameters. The subject and item identifier features are also strongly predictive, but item is the stronger of the two. Text-based features are weaker, but this suggests future work to better integrate them into IRT models (§8)."
    }, {
      "heading" : "4.3 Ranking with IRT",
      "text" : "Leaderboards should produce reliable subject rankings: can DAD rank systems even with a tiny test set? Thus, we compare the correlation both of traditional average accuracy (§3) and IRT rankings on the whole test set compared to the rankings of the same metric on a smaller test set. Our first experiment (§4.3.1) examines the stability of existing items and subjects while the second (§4.4) investigates stability of “new” evaluation data using sampling strategies.\n7Everywhere else in the paper, we train on all responses.\nROC AUC\nMacro F1\nAccuracy"
    }, {
      "heading" : "4.3.1 IRT Rankings Have Better Reliability",
      "text" : "Rankings should be reliable within the same dataset (e.g., on dev set) and generalize to similar datasets (e.g., with a test dataset). To test the first, we measure the ranking stability of mutually exclusive samples of the development data (Buckley and Voorhees, 2000). To test the second, we measure the correlation between development set sample rankings to test set rankings (Voorhees, 1998).\nSpecifically, for a range of sample sizes8 we (1) sample two partitions of the data, (2) compute the classical ranking9 and the IRT ranking from a refit IRT-feas model, then (3) compute Kendall’s correlation (Kendall, 1938) between the samples for each ranking (details in Appendix D). In both cases IRT rankings have higher correlation than classical rankings (Figure 4, left). Since the benefit is strongest at low sample sizes, IRT can improve the reliability of small-scale evaluations.\nThe second experiment examines ranking generalization: IRT yields more reliable measures of subject skill, implying a greater consistency in subject rankings across evaluation settings. Figure 4 compares the development set sample rankings computed above to rankings obtained using subjects’ test set responses (with the same IRT model).\nAcross all sample sizes, subjects’ IRT ability estimated on the development set correlates well test set ability. Crucially, this is better than the corresponding classical metrics like accuracy (Appendix D quantifies the statistical significance of the difference), supporting our original motivation for using IRT.10\n8The sample size must be less than half the size of the development data so that we can obtain two samples.\n9For SQuAD, ordering by mean exact match score. 10Since the maximum trial size was limited, we train one final model with the full data, see Table 3 in the Appendix D."
    }, {
      "heading" : "4.4 IRT Improves Cold Start Reliability",
      "text" : "IRT can also guide the construction of tests. Just as IRT practitioners prepare tests for humans, we too construct tests for machines. In educational testing, collecting responses from humans is expensive; likewise, although questions are cheap in searchbased QA tasks (Nguyen et al., 2016; Kwiatkowski et al., 2019), annotating answers is expensive. Likewise, “grading” machine dialog responses is expensive and IRT helps (Sedoc and Ungar, 2020). To emulate this setting, we use computerized adaptive testing (Weiss and Kingsbury, 1984) to iteratively select SQuAD items to “annotate.”\nAs in human test preparation, we use existing annotations to infer item parameters and iteratively infer the ability of new subjects. This experiment splits m subjects into a training group (80%) and a testing group (20%). The training group represents subjects for which we have full item predictions and annotations; the testing group represents a new group of subjects that we need to rank. To efficiently rank, we should iteratively choose items to annotate that yield the most information about the ranking if all the data were annotated.\nThis experiment compares how well several item selection strategies work. For each selection method, we (1) choose a sample size, (2), sample from the development set, (3) compute the ranking of subjects, and (4) compute Kendall’s rank correlation (Figure 5).11\nWhich item selection strategies should we compare? As a baseline, we use naïve random sampling. Like prior work, we compare selecting items with the highest difficulty and the highest discriminability (Lalor et al., 2019) as well as the sum of the\n11We compute correlations with the complete development set on ten trials to build 95% confidence intervals.\n16 32 64 128 256 512 1,024 2,048 4,096 Development Set Sample Size\n16 32 64 128 256 512 1,024 2,048 4,096 Development Set Sample Size\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9 1.0 Ke nd al l R an k Co rr el at io\nn Dev Sample to Dev Sample\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nKe nd\nal l R\nan k\nCo rr\nel at\nio n\nDev Sample to Test\nIRT to IRT Acc to Acc\nCorrelation\nFigure 4: Compared to the final ranking over a large test set, how well does a small test set correlate? The left shows correlation between mutually exclusive development set samples and the right between development samples and the full test set. In both experiments (panes), ranking systems by IRT ability is more stable—across all sample sizes—than mean accuracy and thus more reliable (Kendall’s rank correlation is higher). Bands show 95% confidence intervals of rank correlations across ten trials per sample size.\ntwo.12 We propose that items should be selected according to their Fisher information content (Weiss, 1982)\nIi(θj) = (p′ij) 2\npij(1− pij) = γ2i pij(1− pij) (2)\nas derived by Lord et al. (1968, p. 70). Intuitively, if we do not yet know the true skill θj , we should pick items whose expected response we are most uncertain about. Our uncertainty (entropy) is maximized when the likelihood of a correct re-\n12We train an IRT-disc model to simplify sampling (e.g., avoiding a tradeoff between feasibility and discriminability).\nsponse pij is the same as the likelihood of an incorrect response 1− pij , which corresponds to the maximal value of Ii(θj); it is also sensible this value increases as discriminability γi increases.\nTo infer the maximally informative items, we estimate the ability θj of each subject using the currently selected items, use the ability to compute the information of each yet-to-be-annotated item for each subject, and then aggregate the informativeness\nInfo(i) = ∑︂ j Ii(θj) (3)\nby item i summed over subjects j. This approach is similar to uncertainty sampling and reduces to it for the IRT-base model (Lewis and Gale, 1994). We initially seed with the twenty-five most discriminative items (details in Appendix D).\nLike computerized adaptive testing (Moreno et al., 1984), Figure 5 shows that at lower sample sizes three of the IRT sampling methods are better than random sampling—difficulty does worse. The other IRT methods have comparable correlation. Thus, by using IRT, DAD can both improve rankings and guide annotation."
    }, {
      "heading" : "5 Qualitative Insights on Leaderboards",
      "text" : "DAD also helps qualitative analysis of items and subjects. First, IRT identifies overfitting and generalizes partitioning datasets by difficulty. Then we show that—like in educational testing—IRT identifies good and bad items."
    }, {
      "heading" : "5.1 Guiding Analysis with IRT",
      "text" : "Several works curate easy and hard QA subsets based on how many models answer correctly (Rondeau and Hazen, 2018) or heuristics (Sugawara et al., 2018). IRT can create similar subsets using IRT-feas, the best 1D model. Difficulty finds where subjects improve while discriminability and feasibility can surface items that may be invalid. For example, one low feasibility question (Figure 9) asks “what are two examples of types of Turing machines?” which has two problems: (1) the answer omits five types and (2) span-based evaluation precludes selecting non-contiguous types.\nAfter excluding items with negative discriminability—they are likely erroneous— we sort items into bins. We break both difficulty and discriminability into four bins—taking the 25th, 50th, and 75th percentiles—creating eight total bins. Then we select representative SQuAD subjects with their exact match scores (Figure 6). Let’s examine a feasible item with positive difficulty and discriminability like “what reform was attempted following the Nice treaty?”13 In this case, the annotator’s span is too long—resulting in almost no correct answers and a low fuzzy match (token F1). In contrast, one highly discriminative question succeeds because there are multiple plausible guesses to “who did the Normans team up with in Anatolia?”14 While both the Armenian state and Turkish forces are superficially plausible answers, only Turkish forces is correct; nonetheless, some models are fooled. Using IRT to guide subject analysis is helpful; next, we test how efficient it is in identifying annotation error.\n13A: “there was an attempt to reform the constitutional law of the EU and make it more transparent.” (Appendix Figure 10)\n14Example with statistics in Appendix Figure 11."
    }, {
      "heading" : "5.2 Identifying Annotation Error",
      "text" : "To test if IRT can identify annotation error, we inspect sixty SQuAD development set items. We select ten items from each of these groups: the most negative discriminability, discriminability nearest to zero, the highest discriminability, the least difficult, most difficult, and IRT model errors. For each, we annotate whether the item was correct, was “correct” yet flawed in some way, or simply wrong (Figure 7).15 Inter-annotator agreement between three authors on this three-way annotation with Krippendorff’s α (Krippendorff, 2004; Artstein and Poesio, 2008) is 0.344. Despite only modest agreement, just as in the development of education tests, negative discriminability is predictive of bad items. When discriminability is negative, then the probability of getting the answer right is higher when ability is lower, which is undesirable: Ken consistently loses to Burt on those items. This could identify bad items in evaluation sets for removal."
    }, {
      "heading" : "6 Related Work",
      "text" : "DAD draws together two primary threads: we use IRT to understand datasets, which has been applied to other NLP tasks, and apply it to improving leaderboards. Finally, we explore how the insights of IRT can improve not just the analysis of test sets but to improve the construction of test sets.\nIRT in NLP IRT is gaining traction in machine learning research (Martínez-Plumed et al., 2016, 2019) where automated metrics can be misleading (Sedoc et al., 2019): machine translation (Hopkins and May, 2013) and chatbot evaluation (Sedoc\n15Annotation guidelines provided in supplementary materials; Figure 7 uses the first set of annotations which were later augmented by two additional sets of annotations.\nand Ungar, 2020). Concurrent with our work, Vania et al. (2021) compare NLP test sets with IRT. Closest to our work in NLP is Otani et al. (2016), who rank machine translation subjects and compute correlations with gold scores. Similarly, MartínezPlumed and Hernández-Orallo (2020) use IRT on non-language AI video game benchmarks. Just as we use IRT to identify difficult or easy items, Lalor et al. (2016) create challenge sets for textual entailment. We test IRT as a way to guide annotation, but it can also train NLP models; for example, deep models learn “easy” examples faster (Lalor et al., 2018) and maintain test accuracy when training data are down-sampled (Lalor et al., 2019).\nImproving Leaderboards The rise NLP leaderboards has encouraged critical thought into improving them (Linzen, 2020), improving evaluation more broadly (Eger et al., 2020), and thoughtful consideration of their influence on the direction of research (Sculley et al., 2018; Dotan and Milli, 2020). DAD aims make leaderboard yardsticks (Hernandez-Orallo, 2020) more reliable, interpretable, and part of curating the benchmark itself. In line with our reliability goal, just as statistical tests should appear in publications (Dror et al., 2018; Dodge et al., 2019), they should be “freebies” for leaderboard participants (Ethayarajh and Jurafsky, 2020). Alternatively, Hou et al. (2019) posit that leaderboards could be automatically extracted from publications. How to aggregate multi-task benchmarks (Wang et al., 2019b,a; Fisch et al., 2019) and multi-metric benchmarks (Ma et al., 2021) is an open question which—although we do not address—is one use for IRT.\nThis work implicitly argues that leaderboards should be continually updated. As a (static) leaderboard ages, the task(s) overfit (Recht et al., 2019) which—although mitigable (Blum and Hardt, 2015; Anderson-Cook et al., 2019)—is best solved by continually collecting new data (Kiela et al., 2021). Ideally, new data should challenge models through adversarial collection (Wallace et al., 2019b; Nie et al., 2020) and related methods (Gardner et al., 2020). However, if making an easy leaderboard more difficult is possible, the leaderboard has outlived its helpfulness and should be retired (Voorhees, 2000).\nPart of our work centers on alternate task efficacy rankings, but this naïvely assumes that task efficacy is the sole use case of leaderboards. Indeed, focusing solely these factors can mislead the public (Paullada et al., 2020) and may not reflect human language capabilities (Schlangen, 2020). Leaderboards are also well positioned to provide incentive structures for participants to prioritize fairness (Bender and Friedman, 2018) and efficiency (Strubell et al., 2019; Schwartz et al., 2020; Min et al., 2021) or incorporate testing of specific capabilities (Ribeiro et al., 2020; Dunietz et al., 2020). To enable these more nuanced analyses, leaderboards should accept runnable models rather than static predictions (Ma et al., 2021).\nActive Learning Beyond IRT, the analysis of training dynamics and active learning (Settles, 2009) is helpful for actively sampling specific items or identifying low-quality items (Brodley and Friedl, 1999). For example, Swayamdipta et al. (2020) and Pleiss et al. (2020) propose alternative\ntraining dynamics-based methods for identifying difficult items as well annotation errors. Even closer to goals, Rahman et al. (2020) use active learning to build a test collection. Explicitly measuring how effectively examples separate the best subject from the rest allows test set curators to “focus on the bubble” (Boyd-Graber and Börschinger, 2020), prioritizing examples most likely to reveal interesting distinctions between submitted systems.\nAlternate Formulations IRT is an example of convergent evolution of models that predict subject action given an item. Ideal point models (Poole and Rosenthal, 2017) consider how a legislator (subject) will vote on a bill (item) and use a similar mathematical formulation. The venerable ELO model (Glickman and Jones, 1999) and modern extensions (Herbrich et al., 2007) predict whether a player (subject) will defeat an opponent (item) with, again, a similar mathematical model. Certain IRT models can also be formulated as nonlinear mixed models (Rijmen et al., 2003), where the item parameters are fixed effects and the latent subject parameters are random effects. This allows for comparisons between IRT models and other mixed effects models under a consistent framework. IRTbase and IRT-disc can be formulated as nonlinear mixed models, and IRT-feas can be formulated as a discrete mixture model over items. As we discuss further in the next section, DAD’s application of IRT can further be improved by adopting interpretable extensions of these models."
    }, {
      "heading" : "7 Conclusion",
      "text" : "This paper advocates incorporating decades of research in crafting education tests to improve how we evaluate the capabilities of NLP models. We propose and validate an alternate IRT ranking method for leaderboard evaluations, show it can guide annotation, detect annotation error, and naturally partition evaluation data. Just as educators moved from classical testing to IRT, the NLP community should consider future evaluations with IRT."
    }, {
      "heading" : "7.1 Limitations",
      "text" : "Although there is much to gain through IRT evaluation, there are limitations which make it hard to implement. First, it requires access to item-level responses for all examples for all subjects which are often only available to organizers. Second, Urbano (2016) notes that sampling mutually exclusive subsets has drawbacks—samples are not entirely\nindependent. Lastly, our work is a proof of concept using SQuAD 2.0 as a test bed and our results may not generalize."
    }, {
      "heading" : "8 Future Work",
      "text" : "We see a few directions for future work. First, this paper is intended to validate IRT and its usefulness as an active part of the leaderboard lifecycle; the natural next step is to implement it in a leaderboard. Second, our IRT models do not incorporate the item content (e.g., example text) to predict responses, but in principle could; Bayesian models with metadata (Card et al., 2018) and ideal point models from political science (Poole and Rosenthal, 1985) that incorporate bills and speeches do exactly this (Gerrish and Blei, 2011; Nguyen et al., 2015; Kraft et al., 2016). Analogously, IRT for leaderboards can and should also incorporate text from passages, questions, and answers to better model what makes questions difficult. Such a model can also predict which characteristics would create discriminating or difficult items. Lastly, multidimensional IRT models to evaluate multiple skills could aid multitask or multi-metric leaderboards like MRQA (Fisch et al., 2019) and Dynaboard (Ma et al., 2021)."
    }, {
      "heading" : "Acknowledgements",
      "text" : "For their work on early iterations of leaderboard visualizations, we thank Jacob Bremerman and Wei Wei Chi. For insightful discussions and ideas we thank Shi Feng, Doug Oard, João Sedoc, Mike Wu, and Patrick Lewis. We thank Peter Rankel for recommendations on statistical testing methods. For discussion and feedback on visualizations, we thank Leo Zhicheng Liu, Calvin Bao, and classmates in UMD’s Fall 2020 “Information Visualization” course. For suggestions on topic modeling, we thank Philip Resnik and Maria Antoniak. For feedback on prior versions of this paper, we thank our anonymous ACL reviewers and members of the UMD CLIP lab. Boyd-Graber and Rodriguez’s work at UMD were supported by NSF Grant IIS1822494. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the sponsor. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein."
    }, {
      "heading" : "A SQuAD Item Examples",
      "text" : "Figures 8, 9, 10, and 11 show previously discussed SQuAD examples (§5) in full. The SQuAD annotations from Figure 7 are included in supplementary materials and at irt.pedro.ai. On the same page, we provide a web interface for inspecting the parameters of the IRT models. Figure 12 shows the feasibility distribution corresponding to Figure 1."
    }, {
      "heading" : "B Logistic Regression Features",
      "text" : "The linear model (§4.2) includes features based on item IDs, subject IDs, textual features of the question, context, and answer, and topic model features. Table 1 lists the feature names from Figure 3 with descriptions of each. When IRT features or the statistics features are used, they include interaction terms with themselves.\nC IRT Model Type Correlation\nAlthough each IRT model differs in expressiveness, they should—in general—produce similar results. This is confirmed by computing the Kendall’s rank correlation between the subject abilities and item difficulties (Table 2)."
    }, {
      "heading" : "D Ranking Stability Experiments",
      "text" : "Here we provide further details for the ranking stability experiments (§4.2.3). First, we filter from the 161 subjects that have development set scores to the 115 that also have test set scores.16 In our simulation, we run 10 trials for every sample size; sample size begins at 100 and with steps of 100. In addition to these, we also run trials for sample sizes 25, 50, and 75. Since each sample can be no larger than half the dataset, we stop at half the dataset.\nD.1 Development and Test Set Correlations\nTable 3 uses a IRT-disc model since we noticed that in comparison IRT-feas overfit the data, yielding worse results. The correlations with the full data are all strong, but not the same. We conclude that—at least on SQuAD—IRT rankings are modestly more reliable than classical rankings.\nD.2 Statistical Significance of Difference in Kendall Tau Coefficients\nWhile Figure 4 shows a consistent difference in correlation between ranking methods, it is unclear whether this difference is statistically significant. We estimate the statistical significance of the difference through bootstrap sampling (Efron, 1994).\nSince the null case is no difference in correlation coefficients, we seek a symmetric sampling distribution centered at zero that represents a realistic density function. Each ranking stability experiment17 trial results in two lists of number pairs. The lists correspond to subject scores on two datasets;18 each number pair is the subject’s accuracy and IRT score. To create the bootstrap distribution, we (1) sample with replacement pairs from one list, (2) compute the correlation between\n16The SQuAD organizers curate the test set subjects to avoid overfit, garbage, or duplicate submissions.\n17One experiment for development sample to development sample and one for development sample to test set.\n18In the first experiment, development set samples; in the second, a development set sample and the full test set.\nthe resampled ranking and unused ranking when using accuracy versus IRT score, and (3) compute and store the IRT correlation score minus the accuracy correlation score. We repeat this process 1000 times for each of the 10 trials in the original experiment and aggregate all the differences to build the bootstrap distribution. For each sample size we compute the empirical P-Value on each trial which we show in box and whisker plots (Figure 13)."
    }, {
      "heading" : "E The IRT Statistical Test",
      "text" : "The IRT test differs in two substantial ways from other tests: (1) it does not assume that items are equally informative and (2) it does assume that the informativeness of items is a function of the subject’s skill θj . In the literature, this is closely connected to reliability (Tague-Sutcliffe, 1992) and each item provides information about the location of θj ; as we accumulate more evidence for the location of θj the error of estimation decreases. It is a well known result in IRT that standard error of estimate (SEE) σ(θ̂|θ) varies with respect to the agent location parameter θ (De Ayala, 2013, p. 30) and is connected to the Fisher information\nIi(θ) = (p′i) 2\npi(1− pi) (4)\nof each item. For a 2PL model, information\nIi(θ) = γ 2pi(1− pi) (5)\nis maximized when pi = (1 − pi). Since Fisher information is additive, the information of the evaluation set is maximal when items have a 50% chance of being responded to correctly. As derived by De Ayala (2013, p. 102), the standard error of estimation\nSEE(θ) = √︄ 1∑︁\ni Ii(θ) . (6)\nis computed by accumulating the information gained from each item. Given two subjects X and Y , one can use the probability distribution of score differences\nN(θY − θX ,SEE(θX)2 + SEE(θY )2) (7) to compute the probability that the difference in skill is greater than two standard errors which corresponds to an α ≤ .05 significance level."
    }, {
      "heading" : "F Multidimensional IRT Clustering",
      "text" : "While we achieve strong held-out accuracy with 10 dimensional IRT (IRT-vec), we had limited success in interpreting parameters. We use TSNE19 plots overlayed with features like item accuracy, the question’s Wikipedia page, if the question was answerable, length of questions, and topic model weights. Of these, item accuracy and answerability showed the most obvious patterns (Figure 14).\n19We use openTSNE (Poličar et al., 2019) with default parameters.\nWe repeated this approach with the multi-task question answering shared task MRQA (Fisch et al., 2019). However, instead of using 10 dimensions we use 6 to match the number of development set tasks in MRQA. Although questions in NarrativeQA standout (Figure 15), there is not a discernible pattern amongst the other tasks. We leave more sophisticated methods for making multidimensional IRT models interpretable to future work."
    }, {
      "heading" : "G Reproducibility Checklist",
      "text" : "Here we provide reproducibility details to complement our source code (https://irt.pedro.ai).\nG.1 Software and Parameters All IRT models are implemented in PyTorch (Paszke et al., 2019) and Pyro (Bingham et al., 2018). Linear models are trained with Vowpal Wabbit (Agarwal et al., 2014). The topic model that generates features for the linear model uses Mallet (McCallum, 2002).\nThe number of IRT model parameters is proportional to the number of subjects m and the number of items n. The IRT-base has one parameter per subject and one per item. The IRT-disc has one parameter per subject and two per item. The IRTfeas has one parameter per subject and three per item. The IRT-vec has ten parameters per subject and thirty per item.\nG.2 Hyperparameters We did not invest significant effort in hyperparameter tuning the IRT models and instead used the defaults in the py-irt software20 provided by Lalor et al. (2019). The IRT-base, IRT-disc, and IRT-feas models were trained for 1000 epochs with no early stopping conditions and a learning rate of 0.1 with ADAM (Kingma and Ba, 2015). The IRT-vec model was trained for 2500 epochs and used 10 dimensions.\n20github.com/jplalor/py-irt\nIn the linear model, we used a Hyperoptbased (Bergstra et al., 2013) tool provided by Vowpal Wabbit21 for hyper parameter search. For each LM, the tool spent 20 iterations optimizing the learning rate, L2 regularization, and number of bits against the logistic loss function. The learning rate was searched from 0.001 to 10 with loguniform sampling, L2 regularization from 1e− 8 to 1, and bits from 20 to 23 as categorical variables.\nThe topic model that generated features for the linear model used mallet, and we followed the recommendations of the software to set hyper param-\n21github.com/VowpalWabbit/vowpal_wabbit\neters.22 Specifically, we used an optimization interval of 10, removed stop words, trained for 1000 iterations, and used a document-topic threshold of 0.05. Each document was comprised of the Wikipedia page title and the question text.\nG.3 Computational Resources The majority of experiments were conducted on a single workstation with an Intel i7-7700K CPU, 47GB of RAM, and an Nvidia 1080Ti. The average runtime for the IRT-feas model on CPU is 113 seconds with a standard deviation of 2.31 over 5 trials. The average runtime of the IRT-vec model on GPU is 110 seconds with a standard deviation of 0.5 over 5 trials.\nSince each ranking stability experiment required (§4.3.1) re-training an IRT-feas model on each subset, we parallelized this experiment on a CPU cluster where each trial received two CPU cores and 16GB of RAM. In total, this included 520 trials which corresponds to twice that many trained IRT models since one model is trained on each subset of the data.\n22mallet.cs.umass.edu/topics.php"
    } ],
    "references" : [ {
      "title" : "A reliable effective terascale linear learning system",
      "author" : [ "Alekh Agarwal", "Olivier Chapelle", "Miroslav Dudík", "John Langford." ],
      "venue" : "Journal of Machine Learning Research, 15:1111–1133.",
      "citeRegEx" : "Agarwal et al\\.,? 2014",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2014
    }, {
      "title" : "How to host an effective data competition: Statistical advice for competition design and analysis",
      "author" : [ "Christine M Anderson-Cook", "Kary L Myers", "Lu Lu", "Michael L Fugate", "Kevin R Quinlan", "Norma Pawley." ],
      "venue" : "Statistical Analysis and Data Mining: The",
      "citeRegEx" : "Anderson.Cook et al\\.,? 2019",
      "shortCiteRegEx" : "Anderson.Cook et al\\.",
      "year" : 2019
    }, {
      "title" : "Inter-coder agreement for computational linguistics",
      "author" : [ "Ron Artstein", "Massimo Poesio." ],
      "venue" : "Computational Linguistics, 34(4):555–596.",
      "citeRegEx" : "Artstein and Poesio.,? 2008",
      "shortCiteRegEx" : "Artstein and Poesio.",
      "year" : 2008
    }, {
      "title" : "The Basics of Item Response Theory",
      "author" : [ "Frank B Baker." ],
      "venue" : "ERIC.",
      "citeRegEx" : "Baker.,? 2001",
      "shortCiteRegEx" : "Baker.",
      "year" : 2001
    }, {
      "title" : "Analysis methods in neural language processing: A survey",
      "author" : [ "Yonatan Belinkov", "James Glass." ],
      "venue" : "Transactions of the Association for Computational Linguistics, pages 49–72.",
      "citeRegEx" : "Belinkov and Glass.,? 2019",
      "shortCiteRegEx" : "Belinkov and Glass.",
      "year" : 2019
    }, {
      "title" : "Data statements for natural language processing: Toward mitigating system bias and enabling better science",
      "author" : [ "Emily M Bender", "Batya Friedman." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:587–604.",
      "citeRegEx" : "Bender and Friedman.,? 2018",
      "shortCiteRegEx" : "Bender and Friedman.",
      "year" : 2018
    }, {
      "title" : "Climbing towards NLU: On meaning, form, and understanding in the age of data",
      "author" : [ "Emily M Bender", "Alexander Koller." ],
      "venue" : "Proceedings of the Association for Computational Linguistics. Association for Computational Linguistics.",
      "citeRegEx" : "Bender and Koller.,? 2020",
      "shortCiteRegEx" : "Bender and Koller.",
      "year" : 2020
    }, {
      "title" : "Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures",
      "author" : [ "James Bergstra", "Daniel Yamins", "David Cox." ],
      "venue" : "Proceedings of the 30th International Conference on Machine Learning, volume 28",
      "citeRegEx" : "Bergstra et al\\.,? 2013",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2013
    }, {
      "title" : "Pyro: Deep Universal Probabilistic Programming",
      "author" : [ "Eli Bingham", "Jonathan P. Chen", "Martin Jankowiak", "Fritz Obermeyer", "Neeraj Pradhan", "Theofanis Karaletsos", "Rohit Singh", "Paul Szerlip", "Paul Horsfall", "Noah D. Goodman." ],
      "venue" : "Journal of Machine Learning Re-",
      "citeRegEx" : "Bingham et al\\.,? 2018",
      "shortCiteRegEx" : "Bingham et al\\.",
      "year" : 2018
    }, {
      "title" : "The ladder: A reliable leaderboard for machine learning competitions",
      "author" : [ "Avrim Blum", "Moritz Hardt." ],
      "venue" : "Proceedings of the International Conference of Machine Learning. PMLR.",
      "citeRegEx" : "Blum and Hardt.,? 2015",
      "shortCiteRegEx" : "Blum and Hardt.",
      "year" : 2015
    }, {
      "title" : "What question answering can learn from trivia nerds",
      "author" : [ "Jordan Boyd-Graber", "Benjamin Börschinger." ],
      "venue" : "Proceedings of the Association for Computational Linguistics. Association for Computational Linguistics.",
      "citeRegEx" : "Boyd.Graber and Börschinger.,? 2020",
      "shortCiteRegEx" : "Boyd.Graber and Börschinger.",
      "year" : 2020
    }, {
      "title" : "Identifying mislabeled training data",
      "author" : [ "Carla E Brodley", "Mark A Friedl." ],
      "venue" : "The journal of artificial intelligence research, 11(1):131–167.",
      "citeRegEx" : "Brodley and Friedl.,? 1999",
      "shortCiteRegEx" : "Brodley and Friedl.",
      "year" : 1999
    }, {
      "title" : "Evaluating evaluation measure stability",
      "author" : [ "Chris Buckley", "Ellen M Voorhees." ],
      "venue" : "Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval.",
      "citeRegEx" : "Buckley and Voorhees.,? 2000",
      "shortCiteRegEx" : "Buckley and Voorhees.",
      "year" : 2000
    }, {
      "title" : "Neural models for documents with metadata",
      "author" : [ "Dallas Card", "Chenhao Tan", "Noah A Smith." ],
      "venue" : "Proceedings of the Association for Computational Linguistics. Association for Computational Linguistics.",
      "citeRegEx" : "Card et al\\.,? 2018",
      "shortCiteRegEx" : "Card et al\\.",
      "year" : 2018
    }, {
      "title" : "The Theory and Practice of Item Response Theory",
      "author" : [ "Rafael Jaime De Ayala." ],
      "venue" : "Guilford Publications.",
      "citeRegEx" : "Ayala.,? 2013",
      "shortCiteRegEx" : "Ayala.",
      "year" : 2013
    }, {
      "title" : "Show your work: Improved reporting of experimental results",
      "author" : [ "Jesse Dodge", "Suchin Gururangan", "Dallas Card", "Roy Schwartz", "Noah A. Smith." ],
      "venue" : "Association for Computational Linguistics.",
      "citeRegEx" : "Dodge et al\\.,? 2019",
      "shortCiteRegEx" : "Dodge et al\\.",
      "year" : 2019
    }, {
      "title" : "Value-laden disciplinary shifts in machine learning",
      "author" : [ "Ravit Dotan", "Smitha Milli." ],
      "venue" : "Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency.",
      "citeRegEx" : "Dotan and Milli.,? 2020",
      "shortCiteRegEx" : "Dotan and Milli.",
      "year" : 2020
    }, {
      "title" : "The hitchhiker’s guide to testing statistical significance in natural language processing",
      "author" : [ "Rotem Dror", "Gili Baumer", "Segev Shlomov", "Roi Reichart." ],
      "venue" : "Proceedings of the Association for Computational Linguistics. Association for Computational Linguis-",
      "citeRegEx" : "Dror et al\\.,? 2018",
      "shortCiteRegEx" : "Dror et al\\.",
      "year" : 2018
    }, {
      "title" : "To test machine comprehension, start by defining comprehension",
      "author" : [ "Jesse Dunietz", "Gregory Burnham", "Akash Bharadwaj", "Owen Rambow", "Jennifer Chu-Carroll", "David Ferrucci." ],
      "venue" : "Proceedings of the Association for Computational Linguistics.",
      "citeRegEx" : "Dunietz et al\\.,? 2020",
      "shortCiteRegEx" : "Dunietz et al\\.",
      "year" : 2020
    }, {
      "title" : "The statistics of examinations",
      "author" : [ "F Y Edgeworth." ],
      "venue" : "Journal of the Royal Statistical Society, 51(3):599– 635.",
      "citeRegEx" : "Edgeworth.,? 1888",
      "shortCiteRegEx" : "Edgeworth.",
      "year" : 1888
    }, {
      "title" : "An introduction to the bootstrap",
      "author" : [ "Bradley Efron." ],
      "venue" : "Chapman & Hall, New York.",
      "citeRegEx" : "Efron.,? 1994",
      "shortCiteRegEx" : "Efron.",
      "year" : 1994
    }, {
      "title" : "Utility is in the eye of the user: A critique of NLP leaderboards",
      "author" : [ "Kawin Ethayarajh", "Dan Jurafsky." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing. Association for Computational Linguistics.",
      "citeRegEx" : "Ethayarajh and Jurafsky.,? 2020",
      "shortCiteRegEx" : "Ethayarajh and Jurafsky.",
      "year" : 2020
    }, {
      "title" : "Pathologies of neural models make interpretations difficult",
      "author" : [ "Shi Feng", "Eric Wallace", "Alvin Grissom II", "Mohit Iyyer", "Pedro Rodriguez", "Jordan Boyd-Graber." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing. Association for Com-",
      "citeRegEx" : "Feng et al\\.,? 2018",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2018
    }, {
      "title" : "MRQA 2019 shared task: Evaluating generalization in reading comprehension",
      "author" : [ "Adam Fisch", "Alon Talmor", "Robin Jia", "Minjoon Seo", "Eunsol Choi", "Danqi Chen." ],
      "venue" : "Proceedings of the 2nd Workshop on Machine Reading for Question Answering.",
      "citeRegEx" : "Fisch et al\\.,? 2019",
      "shortCiteRegEx" : "Fisch et al\\.",
      "year" : 2019
    }, {
      "title" : "Evaluating models’ local decision boundaries via contrast sets",
      "author" : [ "son F Liu", "Phoebe Mulcaire", "Qiang Ning", "Sameer Singh", "Noah A Smith", "Sanjay Subramanian", "Reut Tsarfaty", "Eric Wallace", "Ally Zhang", "Ben Zhou" ],
      "venue" : "In Findings of the Association",
      "citeRegEx" : "Liu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Predicting legislative roll calls from text",
      "author" : [ "Sean M Gerrish", "David M Blei." ],
      "venue" : "Proceedings of the International Conference of Machine Learning.",
      "citeRegEx" : "Gerrish and Blei.,? 2011",
      "shortCiteRegEx" : "Gerrish and Blei.",
      "year" : 2011
    }, {
      "title" : "Rating the chess rating system",
      "author" : [ "Mark E Glickman", "Albyn C Jones." ],
      "venue" : "Chance, 12.",
      "citeRegEx" : "Glickman and Jones.,? 1999",
      "shortCiteRegEx" : "Glickman and Jones.",
      "year" : 1999
    }, {
      "title" : "Fundamentals of item response theory",
      "author" : [ "Ronald Hambleton." ],
      "venue" : "Sage Publications, Newbury Park, Calif.",
      "citeRegEx" : "Hambleton.,? 1991",
      "shortCiteRegEx" : "Hambleton.",
      "year" : 1991
    }, {
      "title" : "TrueskillTM: A bayesian skill rating system",
      "author" : [ "Ralf Herbrich", "Tom Minka", "Thore Graepel." ],
      "venue" : "Proceedings of Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Herbrich et al\\.,? 2007",
      "shortCiteRegEx" : "Herbrich et al\\.",
      "year" : 2007
    }, {
      "title" : "AI evaluation: On broken yardsticks and measurement scales",
      "author" : [ "Jose Hernandez-Orallo." ],
      "venue" : "Workshop on Evaluating Evaluation of Ai Systems at AAAI.",
      "citeRegEx" : "Hernandez.Orallo.,? 2020",
      "shortCiteRegEx" : "Hernandez.Orallo.",
      "year" : 2020
    }, {
      "title" : "Models of translation competitions",
      "author" : [ "Mark Hopkins", "Jonathan May." ],
      "venue" : "Proceedings of the Association for Computational Linguistics. Association for Computational Linguistics.",
      "citeRegEx" : "Hopkins and May.,? 2013",
      "shortCiteRegEx" : "Hopkins and May.",
      "year" : 2013
    }, {
      "title" : "Identification of tasks, datasets, evaluation metrics, and numeric scores for scientific leaderboards construction",
      "author" : [ "Yufang Hou", "Charles Jochim", "Martin Gleize", "Francesca Bonin", "Debasis Ganguly." ],
      "venue" : "Proceedings of the Association for Computational",
      "citeRegEx" : "Hou et al\\.,? 2019",
      "shortCiteRegEx" : "Hou et al\\.",
      "year" : 2019
    }, {
      "title" : "An introduction to variational methods for graphical models",
      "author" : [ "Michael I. Jordan", "Zoubin Ghahramani", "Tommi S. Jaakkola", "Lawrence K. Saul." ],
      "venue" : "Machine Learning, 37(2):183–233.",
      "citeRegEx" : "Jordan et al\\.,? 1999",
      "shortCiteRegEx" : "Jordan et al\\.",
      "year" : 1999
    }, {
      "title" : "A new measure of rank correlation",
      "author" : [ "M G Kendall." ],
      "venue" : "Biometrika, 30(1/2):81–93.",
      "citeRegEx" : "Kendall.,? 1938",
      "shortCiteRegEx" : "Kendall.",
      "year" : 1938
    }, {
      "title" : "Dynabench: Rethinking benchmarking in NLP",
      "author" : [ "Bansal", "Christopher Potts", "Adina Williams." ],
      "venue" : "Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics.",
      "citeRegEx" : "Bansal et al\\.,? 2021",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2021
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "An embedding model for predicting Roll-Call votes",
      "author" : [ "Peter Kraft", "Hirsh Jain", "Alexander M Rush." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing. Association for Computational Linguistics.",
      "citeRegEx" : "Kraft et al\\.,? 2016",
      "shortCiteRegEx" : "Kraft et al\\.",
      "year" : 2016
    }, {
      "title" : "Content Analysis: an Introduction to its Methodology",
      "author" : [ "Klaus Krippendorff." ],
      "venue" : "Sage: Thousand Oaks, CA. Chapter 11.",
      "citeRegEx" : "Krippendorff.,? 2004",
      "shortCiteRegEx" : "Krippendorff.",
      "year" : 2004
    }, {
      "title" : "Natural questions: A benchmark for question answering research",
      "author" : [ "Uszkoreit", "Quoc Le", "Slav Petrov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:453–466.",
      "citeRegEx" : "Uszkoreit et al\\.,? 2019",
      "shortCiteRegEx" : "Uszkoreit et al\\.",
      "year" : 2019
    }, {
      "title" : "Understanding deep learning performance through an examination of test set difficulty: A psychometric case study",
      "author" : [ "John P Lalor", "Hao Wu", "Tsendsuren Munkhdalai", "Hong Yu." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Lalor et al\\.,? 2018",
      "shortCiteRegEx" : "Lalor et al\\.",
      "year" : 2018
    }, {
      "title" : "Building an evaluation scale using item response theory",
      "author" : [ "John P Lalor", "Hao Wu", "Hong Yu." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing. Association for Computational Linguistics.",
      "citeRegEx" : "Lalor et al\\.,? 2016",
      "shortCiteRegEx" : "Lalor et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning latent parameters without human response patterns: Item response theory with artificial crowds",
      "author" : [ "John P Lalor", "Hao Wu", "Hong Yu." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing. Association for Computational Linguis-",
      "citeRegEx" : "Lalor et al\\.,? 2019",
      "shortCiteRegEx" : "Lalor et al\\.",
      "year" : 2019
    }, {
      "title" : "A sequential algorithm for training text classifiers",
      "author" : [ "David D Lewis", "William A Gale." ],
      "venue" : "Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval. Springer-Verlag.",
      "citeRegEx" : "Lewis and Gale.,? 1994",
      "shortCiteRegEx" : "Lewis and Gale.",
      "year" : 1994
    }, {
      "title" : "How can we accelerate progress towards human-like linguistic generalization? In Proceedings of the Association for Computational Linguistics",
      "author" : [ "Tal Linzen." ],
      "venue" : "Association for Computational Linguistics.",
      "citeRegEx" : "Linzen.,? 2020",
      "shortCiteRegEx" : "Linzen.",
      "year" : 2020
    }, {
      "title" : "Troubling trends in machine learning scholarship",
      "author" : [ "Zachary C. Lipton", "Jacob Steinhardt." ],
      "venue" : "Queue, 17(1).",
      "citeRegEx" : "Lipton and Steinhardt.,? 2019",
      "shortCiteRegEx" : "Lipton and Steinhardt.",
      "year" : 2019
    }, {
      "title" : "Statistical theories of mental test scores",
      "author" : [ "F M Lord", "M R Novick", "Allan Birnbaum" ],
      "venue" : null,
      "citeRegEx" : "Lord et al\\.,? \\Q1968\\E",
      "shortCiteRegEx" : "Lord et al\\.",
      "year" : 1968
    }, {
      "title" : "Dynaboard: An evaluation-as-a-service platform for holistic nextgeneration benchmarking",
      "author" : [ "Zhiyi Ma", "Kawin Ethayarajh", "Tristan Thrush", "Somya Jain", "Ledell Wu", "Robin Jia", "Christopher Potts", "Adina Williams", "Douwe Kiela" ],
      "venue" : null,
      "citeRegEx" : "Ma et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2021
    }, {
      "title" : "Dual indicators to analyze AI benchmarks: Difficulty, discrimination, ability, and generality",
      "author" : [ "F Martínez-Plumed", "J Hernández-Orallo." ],
      "venue" : "IEEE Transactions on Computational Intelligence in AI and Games, 12(2):121–131.",
      "citeRegEx" : "Martínez.Plumed and Hernández.Orallo.,? 2020",
      "shortCiteRegEx" : "Martínez.Plumed and Hernández.Orallo.",
      "year" : 2020
    }, {
      "title" : "Making sense of item response theory in machine learning",
      "author" : [ "Fernando Martínez-Plumed", "Ricardo B C Prudêncio", "Adolfo Martínez-Usó", "José Hernández-Orallo." ],
      "venue" : "Proceedings of the Twenty-second European Conference on Artificial Intelligence.",
      "citeRegEx" : "Martínez.Plumed et al\\.,? 2016",
      "shortCiteRegEx" : "Martínez.Plumed et al\\.",
      "year" : 2016
    }, {
      "title" : "Item response theory in AI: Analysing machine learning classifiers at the instance level",
      "author" : [ "Fernando Martínez-Plumed", "Ricardo B C Prudêncio", "Adolfo Martínez-Usó", "José Hernández-Orallo." ],
      "venue" : "Artificial intelligence, 271:18–42.",
      "citeRegEx" : "Martínez.Plumed et al\\.,? 2019",
      "shortCiteRegEx" : "Martínez.Plumed et al\\.",
      "year" : 2019
    }, {
      "title" : "Bandwidth, fidelity, and adaptive tests",
      "author" : [ "James R. McBride." ],
      "venue" : "T. J. McConnell, Jr. (Ed.), CAT/C 2 1975: The second conference on computer-assisted test construction. Atlanta GA: Atlanta Public Schools.",
      "citeRegEx" : "McBride.,? 1976",
      "shortCiteRegEx" : "McBride.",
      "year" : 1976
    }, {
      "title" : "Mallet: A machine learning for language toolkit",
      "author" : [ "Andrew Kachites McCallum." ],
      "venue" : "http://mallet.cs.umass.edu.",
      "citeRegEx" : "McCallum.,? 2002",
      "shortCiteRegEx" : "McCallum.",
      "year" : 2002
    }, {
      "title" : "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
      "author" : [ "Tom McCoy", "Ellie Pavlick", "Tal Linzen." ],
      "venue" : "Proceedings of the Association for Computational Linguistics. Association for Computational Linguistics.",
      "citeRegEx" : "McCoy et al\\.,? 2019",
      "shortCiteRegEx" : "McCoy et al\\.",
      "year" : 2019
    }, {
      "title" : "EfficientQA competition: Systems, analyses and lessons learned",
      "author" : [ "Mehdad", "Wen-Tau Yih" ],
      "venue" : "NeurIPS",
      "citeRegEx" : "Mehdad and Yih.,? \\Q2021\\E",
      "shortCiteRegEx" : "Mehdad and Yih.",
      "year" : 2021
    }, {
      "title" : "Relationship between corresponding armed services vocational aptitude battery (ASVAB) and computerized adaptive testing (CAT) subtests",
      "author" : [ "Kathleen E Moreno", "C Douglas Wetzel", "James R McBride", "David J Weiss." ],
      "venue" : "Applied psychological",
      "citeRegEx" : "Moreno et al\\.,? 1984",
      "shortCiteRegEx" : "Moreno et al\\.",
      "year" : 1984
    }, {
      "title" : "Alibaba AI model tops humans in reading comprehension",
      "author" : [ "Adam Najberg" ],
      "venue" : null,
      "citeRegEx" : "Najberg.,? \\Q2018\\E",
      "shortCiteRegEx" : "Najberg.",
      "year" : 2018
    }, {
      "title" : "Bayesian prior choice in IRT estimation using MCMC and variational bayes",
      "author" : [ "Prathiba Natesan", "Ratna Nandakumar", "Tom Minka", "Jonathan D Rubright." ],
      "venue" : "Frontiers in psychology, 7:1422.",
      "citeRegEx" : "Natesan et al\\.,? 2016",
      "shortCiteRegEx" : "Natesan et al\\.",
      "year" : 2016
    }, {
      "title" : "MS MARCO: A human generated MAchine Reading COmprehension dataset",
      "author" : [ "Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng." ],
      "venue" : "Proceedings of the NIPS Workshop on Cognitive Computation:",
      "citeRegEx" : "Nguyen et al\\.,? 2016",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "Tea party in the house: A hierarchical ideal point topic model and its application to republican legislators in the 112th congress",
      "author" : [ "Viet-An Nguyen", "Jordan Boyd-Graber", "Philip Resnik", "Kristina Miler." ],
      "venue" : "Proceedings of the Association for Computational",
      "citeRegEx" : "Nguyen et al\\.,? 2015",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2015
    }, {
      "title" : "Adversarial NLI: A new benchmark for natural language understanding",
      "author" : [ "Yixin Nie", "Adina Williams", "Emily Dinan", "Mohit Bansal", "Jason Weston", "Douwe Kiela." ],
      "venue" : "Proceedings of the Association for Computational Linguistics. Association for Compu-",
      "citeRegEx" : "Nie et al\\.,? 2020",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2020
    }, {
      "title" : "Probing neural network comprehension of natural language arguments",
      "author" : [ "Timothy Niven", "Hung-Yu Kao." ],
      "venue" : "Proceedings of the Association for Computational Linguistics. Association for Computational Linguistics.",
      "citeRegEx" : "Niven and Kao.,? 2019",
      "shortCiteRegEx" : "Niven and Kao.",
      "year" : 2019
    }, {
      "title" : "IRT-based aggregation model of crowdsourced pairwise comparison for evaluating machine translations",
      "author" : [ "Naoki Otani", "Toshiaki Nakazawa", "Daisuke Kawahara", "Sadao Kurohashi." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Otani et al\\.,? 2016",
      "shortCiteRegEx" : "Otani et al\\.",
      "year" : 2016
    }, {
      "title" : "Pytorch: An imperative style, high-performance deep learning library",
      "author" : [ "jani", "Sasank Chilamkurthy", "Benoit Steiner", "Lu Fang", "Junjie Bai", "Soumith Chintala" ],
      "venue" : "In Proceedings of Advances in Neural Information Processing Systems",
      "citeRegEx" : "jani et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "jani et al\\.",
      "year" : 2019
    }, {
      "title" : "Data and its (dis)contents: A survey of dataset development and use in machine learning research",
      "author" : [ "Amandalynne Paullada", "Inioluwa Deborah Raji", "Emily M Bender", "Emily Denton", "Alex Hanna." ],
      "venue" : "Proceedings of the NeurIPS 2020 Workshop: ML",
      "citeRegEx" : "Paullada et al\\.,? 2020",
      "shortCiteRegEx" : "Paullada et al\\.",
      "year" : 2020
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference",
      "author" : [ "Judea Pearl." ],
      "venue" : "Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.",
      "citeRegEx" : "Pearl.,? 1988",
      "shortCiteRegEx" : "Pearl.",
      "year" : 1988
    }, {
      "title" : "Identifying mislabeled data using the area under the margin ranking",
      "author" : [ "Geoff Pleiss", "Tianyi Zhang", "Ethan R Elenberg", "Kilian Q Weinberger." ],
      "venue" : "Proceedings of Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Pleiss et al\\.,? 2020",
      "shortCiteRegEx" : "Pleiss et al\\.",
      "year" : 2020
    }, {
      "title" : "openTSNE: a modular python library for t-sne dimensionality reduction and embedding",
      "author" : [ "Pavlin G. Poličar", "Martin Stražar", "Blaž Zupan" ],
      "venue" : null,
      "citeRegEx" : "Poličar et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Poličar et al\\.",
      "year" : 2019
    }, {
      "title" : "A spatial model for legislative roll call analysis",
      "author" : [ "Keith T Poole", "Howard Rosenthal." ],
      "venue" : "American journal of political science, 29(2):357–384.",
      "citeRegEx" : "Poole and Rosenthal.,? 1985",
      "shortCiteRegEx" : "Poole and Rosenthal.",
      "year" : 1985
    }, {
      "title" : "Ideology & congress: A political economic history of roll call voting, 2 edition",
      "author" : [ "Keith T Poole", "Howard Rosenthal." ],
      "venue" : "Routledge, London, England.",
      "citeRegEx" : "Poole and Rosenthal.,? 2017",
      "shortCiteRegEx" : "Poole and Rosenthal.",
      "year" : 2017
    }, {
      "title" : "Efficient test collection construction via active learning",
      "author" : [ "Md Mustafizur Rahman", "Mucahid Kutlu", "Tamer Elsayed", "Matthew Lease." ],
      "venue" : "Proceedings of the 2020 ACM SIGIR on International Conference on Theory of Information Retrieval. Association",
      "citeRegEx" : "Rahman et al\\.,? 2020",
      "shortCiteRegEx" : "Rahman et al\\.",
      "year" : 2020
    }, {
      "title" : "Know what you don’t know: Unanswerable questions for squad",
      "author" : [ "Pranav Rajpurkar", "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the Association for Computational Linguistics. Association for Computational Linguistics.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2018",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2018
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing. Association for Computational Linguistics.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Studies in Mathematical Psychology: I",
      "author" : [ "Georg Rasch." ],
      "venue" : "Probabilistic Models for Some Intelligence and Attainment Tests. Studies in Mathematical Psychology: I. Probabilistic Models for Some Intelligence and Attainment Tests. Nielsen & Lydiche, Ox-",
      "citeRegEx" : "Rasch.,? 1960",
      "shortCiteRegEx" : "Rasch.",
      "year" : 1960
    }, {
      "title" : "Do ImageNet classifiers generalize to ImageNet? In Proceedings of the International Conference of Machine Learning",
      "author" : [ "Benjamin Recht", "Rebecca Roelofs", "Ludwig Schmidt", "Vaishaal Shankar." ],
      "venue" : "PMLR.",
      "citeRegEx" : "Recht et al\\.,? 2019",
      "shortCiteRegEx" : "Recht et al\\.",
      "year" : 2019
    }, {
      "title" : "Multidimensional item response theory models",
      "author" : [ "Mark D Reckase." ],
      "venue" : "Reckase, editor, Multidimensional Item Response Theory, pages 79–112. Springer New York, New York, NY.",
      "citeRegEx" : "Reckase.,? 2009",
      "shortCiteRegEx" : "Reckase.",
      "year" : 2009
    }, {
      "title" : "Beyond accuracy: Behavioral testing of NLP models with CheckList",
      "author" : [ "Marco Tulio Ribeiro", "Tongshuang Wu", "Carlos Guestrin", "Sameer Singh." ],
      "venue" : "Proceedings of the Association for Computational Linguistics. Association for Computational Linguis-",
      "citeRegEx" : "Ribeiro et al\\.,? 2020",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2020
    }, {
      "title" : "A nonlinear mixed model framework for item response theory",
      "author" : [ "Frank Rijmen", "Francis Tuerlinckx", "Paul De Boeck", "Peter Kuppens." ],
      "venue" : "Psychological methods, 8(2):185.",
      "citeRegEx" : "Rijmen et al\\.,? 2003",
      "shortCiteRegEx" : "Rijmen et al\\.",
      "year" : 2003
    }, {
      "title" : "Assessment of fit of item response theory models used in largescale educational survey assessments",
      "author" : [ "Peter W van Rijn", "Sandip Sinharay", "Shelby J Haberman", "Matthew S Johnson." ],
      "venue" : "Large-scale Assessments in Education, 4(1):10.",
      "citeRegEx" : "Rijn et al\\.,? 2016",
      "shortCiteRegEx" : "Rijn et al\\.",
      "year" : 2016
    }, {
      "title" : "Systematic error analysis of the Stanford question answering dataset",
      "author" : [ "Marc-Antoine Rondeau", "T J Hazen." ],
      "venue" : "Proceedings of the Workshop on Machine Reading for Question Answering. Association for Computational Linguistics.",
      "citeRegEx" : "Rondeau and Hazen.,? 2018",
      "shortCiteRegEx" : "Rondeau and Hazen.",
      "year" : 2018
    }, {
      "title" : "Targeting the benchmark: On methodology in current natural language processing research",
      "author" : [ "David Schlangen" ],
      "venue" : null,
      "citeRegEx" : "Schlangen.,? \\Q2020\\E",
      "shortCiteRegEx" : "Schlangen.",
      "year" : 2020
    }, {
      "title" : "Green AI",
      "author" : [ "Roy Schwartz", "Jesse Dodge", "Noah A Smith", "Oren Etzioni." ],
      "venue" : "Communications of the ACM, 63(12):54–63.",
      "citeRegEx" : "Schwartz et al\\.,? 2020",
      "shortCiteRegEx" : "Schwartz et al\\.",
      "year" : 2020
    }, {
      "title" : "Winner’s curse? on pace, progress, and empirical rigor",
      "author" : [ "D Sculley", "Jasper Snoek", "Alexander B Wiltschko", "A Rahimi." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Sculley et al\\.,? 2018",
      "shortCiteRegEx" : "Sculley et al\\.",
      "year" : 2018
    }, {
      "title" : "Chateval: A tool for chatbot evaluation",
      "author" : [ "Joao Sedoc", "Daphne Ippolito", "Arun Kirubarajan", "Jai Thirani", "Lyle Ungar", "Chris Callison-Burch." ],
      "venue" : "Conference of the North American Chapter of the Association for Computational Linguistics. Association for",
      "citeRegEx" : "Sedoc et al\\.,? 2019",
      "shortCiteRegEx" : "Sedoc et al\\.",
      "year" : 2019
    }, {
      "title" : "Item response theory for efficient human evaluation of chatbots",
      "author" : [ "João Sedoc", "Lyle Ungar." ],
      "venue" : "Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems. Association for Computational Linguistics.",
      "citeRegEx" : "Sedoc and Ungar.,? 2020",
      "shortCiteRegEx" : "Sedoc and Ungar.",
      "year" : 2020
    }, {
      "title" : "What do models learn from question answering datasets? In Proceedings of Empirical Methods in Natural Language Processing",
      "author" : [ "Priyanka Sen", "Amir Saffari." ],
      "venue" : "Association for Computational Linguistics.",
      "citeRegEx" : "Sen and Saffari.,? 2020",
      "shortCiteRegEx" : "Sen and Saffari.",
      "year" : 2020
    }, {
      "title" : "Active Learning Literature Survey",
      "author" : [ "Burr Settles." ],
      "venue" : "Technical Report 1648, University of Wisconsin– Madison.",
      "citeRegEx" : "Settles.,? 2009",
      "shortCiteRegEx" : "Settles.",
      "year" : 2009
    }, {
      "title" : "Energy and policy considerations for deep learning in NLP",
      "author" : [ "Emma Strubell", "Ananya Ganesh", "Andrew McCallum." ],
      "venue" : "Proceedings of the Association for Computational Linguistics. Association for Computational Linguistics.",
      "citeRegEx" : "Strubell et al\\.,? 2019",
      "shortCiteRegEx" : "Strubell et al\\.",
      "year" : 2019
    }, {
      "title" : "What makes reading comprehension questions easier? In Proceedings of Empirical Methods in Natural Language Processing",
      "author" : [ "Saku Sugawara", "Kentaro Inui", "Satoshi Sekine", "Akiko Aizawa." ],
      "venue" : "Association for Computational Linguistics.",
      "citeRegEx" : "Sugawara et al\\.,? 2018",
      "shortCiteRegEx" : "Sugawara et al\\.",
      "year" : 2018
    }, {
      "title" : "Evaluation metrics for machine reading comprehension: Prerequisite skills and readability",
      "author" : [ "Saku Sugawara", "Yusuke Kido", "Hikaru Yokono", "Akiko Aizawa." ],
      "venue" : "Proceedings of the Association for Computational Linguistics. Association for Compu-",
      "citeRegEx" : "Sugawara et al\\.,? 2017",
      "shortCiteRegEx" : "Sugawara et al\\.",
      "year" : 2017
    }, {
      "title" : "Dataset cartography: Mapping and diagnosing datasets with training dynamics",
      "author" : [ "Swabha Swayamdipta", "Roy Schwartz", "Nicholas Lourie", "Yizhong Wang", "Hannaneh Hajishirzi", "Noah A Smith", "Yejin Choi." ],
      "venue" : "Proceedings of Empirical Methods in Natural Lan-",
      "citeRegEx" : "Swayamdipta et al\\.,? 2020",
      "shortCiteRegEx" : "Swayamdipta et al\\.",
      "year" : 2020
    }, {
      "title" : "The pragmatics of information retrieval experimentation, revisited",
      "author" : [ "Jean Tague-Sutcliffe." ],
      "venue" : "Information processing & management, 28(4):467–490.",
      "citeRegEx" : "Tague.Sutcliffe.,? 1992",
      "shortCiteRegEx" : "Tague.Sutcliffe.",
      "year" : 1992
    }, {
      "title" : "Classical test theory in historical perspective",
      "author" : [ "Ross E Traub." ],
      "venue" : "Educational Measurement, 16:8–13.",
      "citeRegEx" : "Traub.,? 1997",
      "shortCiteRegEx" : "Traub.",
      "year" : 1997
    }, {
      "title" : "Test collection reliability: a study of bias and robustness to statistical assumptions via stochastic simulation",
      "author" : [ "Julián Urbano." ],
      "venue" : "Information Retrieval Journal, 19(3):313–350.",
      "citeRegEx" : "Urbano.,? 2016",
      "shortCiteRegEx" : "Urbano.",
      "year" : 2016
    }, {
      "title" : "Comparing test sets with item response theory",
      "author" : [ "Clara Vania", "Phu Mon Htut", "William Huang", "Dhara Mungra", "Richard Yuanzhe Pang", "Jason Phang", "Haokun Liu", "Kyunghyun Cho", "Samuel R. Bowman." ],
      "venue" : "Proceedings of the Association for Compu-",
      "citeRegEx" : "Vania et al\\.,? 2021",
      "shortCiteRegEx" : "Vania et al\\.",
      "year" : 2021
    }, {
      "title" : "Variations in relevance judgments and the measurement of retrieval effectiveness",
      "author" : [ "Ellen M Voorhees." ],
      "venue" : "Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval. Association for Computing Machinery.",
      "citeRegEx" : "Voorhees.,? 1998",
      "shortCiteRegEx" : "Voorhees.",
      "year" : 1998
    }, {
      "title" : "The TREC-8 question answering track report",
      "author" : [ "Ellen M Voorhees" ],
      "venue" : null,
      "citeRegEx" : "Voorhees.,? \\Q2000\\E",
      "shortCiteRegEx" : "Voorhees.",
      "year" : 2000
    }, {
      "title" : "Evaluating the evaluation: A case study using the TREC 2002 question answering track",
      "author" : [ "Ellen M Voorhees." ],
      "venue" : "Conference of the North American Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Voorhees.,? 2003",
      "shortCiteRegEx" : "Voorhees.",
      "year" : 2003
    }, {
      "title" : "Universal adversarial triggers for attacking and analyzing NLP",
      "author" : [ "Eric Wallace", "Shi Feng", "Nikhil Kandpal", "Matt Gardner", "Sameer Singh." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing. Association for Computational Linguis-",
      "citeRegEx" : "Wallace et al\\.,? 2019a",
      "shortCiteRegEx" : "Wallace et al\\.",
      "year" : 2019
    }, {
      "title" : "Trick me if you can: Human-inthe-loop generation of adversarial question answering examples",
      "author" : [ "Eric Wallace", "Pedro Rodriguez", "Shi Feng", "Jordan Boyd-Graber." ],
      "venue" : "Transactions of the Association for Computational Linguistics, pages 387–401.",
      "citeRegEx" : "Wallace et al\\.,? 2019b",
      "shortCiteRegEx" : "Wallace et al\\.",
      "year" : 2019
    }, {
      "title" : "SuperGLUE: A stickier benchmark for General-Purpose language understanding systems",
      "author" : [ "Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Proceedings of Advances in",
      "citeRegEx" : "Wang et al\\.,? 2019a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amapreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman." ],
      "venue" : "Proceedings of the International Conference on Learning Represen-",
      "citeRegEx" : "Wang et al\\.,? 2019b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving measurement quality and efficiency with adaptive testing",
      "author" : [ "David J Weiss." ],
      "venue" : "Applied psychological measurement, 6(4):473–492.",
      "citeRegEx" : "Weiss.,? 1982",
      "shortCiteRegEx" : "Weiss.",
      "year" : 1982
    }, {
      "title" : "Application of computerized adaptive testing to educational problems",
      "author" : [ "David J Weiss", "G Gage Kingsbury." ],
      "venue" : "Journal of educational measurement, 21(4):361–375.",
      "citeRegEx" : "Weiss and Kingsbury.,? 1984",
      "shortCiteRegEx" : "Weiss and Kingsbury.",
      "year" : 1984
    }, {
      "title" : "Variational item response theory: Fast, accurate, and expressive",
      "author" : [ "M Wu", "R Davis", "B Domingue", "C Piech", "Noah D Goodman." ],
      "venue" : "13th International Conference on Educational Data Mining.",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Errudite: Scalable, reproducible, and testable error analysis",
      "author" : [ "Tongshuang Wu", "Marco Tulio Ribeiro", "Jeffrey Heer", "Daniel Weld." ],
      "venue" : "Proceedings of the Association for Computational Linguistics. Association for Computational Linguistics.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Manifold: A Model-Agnostic framework for interpretation and diagnosis of machine learning models",
      "author" : [ "Jiawei Zhang", "Yang Wang", "Piero Molino", "Lezhi Li", "David S Ebert." ],
      "venue" : "IEEE transactions on visualization and computer graphics, 25(1):364–373.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 71,
      "context" : "question answering (Rajpurkar et al., 2016) and in many NLP tasks (Wang et al.",
      "startOffset" : 19,
      "endOffset" : 43
    }, {
      "referenceID" : 43,
      "context" : "An unfortunate side effect of leaderboard popularity is SOTA-chasing, often at the expense of carefully inspecting data and models (Linzen, 2020).",
      "startOffset" : 131,
      "endOffset" : 145
    }, {
      "referenceID" : 55,
      "context" : "For example, the same “super-human” models that top question answering leaderboards (Najberg, 2018) often fail spectacularly (Feng et al.",
      "startOffset" : 84,
      "endOffset" : 99
    }, {
      "referenceID" : 22,
      "context" : "For example, the same “super-human” models that top question answering leaderboards (Najberg, 2018) often fail spectacularly (Feng et al., 2018; Wallace et al., 2019a) by learning non-generalizable statistical patterns (McCoy et al.",
      "startOffset" : 125,
      "endOffset" : 167
    }, {
      "referenceID" : 97,
      "context" : "For example, the same “super-human” models that top question answering leaderboards (Najberg, 2018) often fail spectacularly (Feng et al., 2018; Wallace et al., 2019a) by learning non-generalizable statistical patterns (McCoy et al.",
      "startOffset" : 125,
      "endOffset" : 167
    }, {
      "referenceID" : 52,
      "context" : ", 2019a) by learning non-generalizable statistical patterns (McCoy et al., 2019; Niven and Kao, 2019).",
      "startOffset" : 60,
      "endOffset" : 101
    }, {
      "referenceID" : 60,
      "context" : ", 2019a) by learning non-generalizable statistical patterns (McCoy et al., 2019; Niven and Kao, 2019).",
      "startOffset" : 60,
      "endOffset" : 101
    }, {
      "referenceID" : 6,
      "context" : "Finally, focusing solely on metrics conflates progress on a specific task with progress on realworld NLP problems behind the task (Bender and Koller, 2020).",
      "startOffset" : 130,
      "endOffset" : 155
    }, {
      "referenceID" : 44,
      "context" : "Plainly, focusing on headline SOTA numbers “provide(s) limited value for scientific progress absent insight into what drives them” and where they fail (Lipton and Steinhardt, 2019).",
      "startOffset" : 151,
      "endOffset" : 180
    }, {
      "referenceID" : 10,
      "context" : "And leaderboards should also signal when they have outlived their usefulness (Boyd-Graber and Börschinger, 2020).",
      "startOffset" : 77,
      "endOffset" : 112
    }, {
      "referenceID" : 19,
      "context" : ", 2016) alternative in educational testing to simple summary statistics (Edgeworth, 1888).",
      "startOffset" : 72,
      "endOffset" : 89
    }, {
      "referenceID" : 64,
      "context" : "To discover the random variables that best explain the data, we turn to probabilistic inference (Pearl, 1988).",
      "startOffset" : 96,
      "endOffset" : 109
    }, {
      "referenceID" : 72,
      "context" : "In psychometrics, IRT-base is called a Rasch (Rasch, 1960) or 1 parameter logistic (1PL) model, IRT-disc is a 2PL model, and IRT-feas is a 4PL model with guessing set to zero.",
      "startOffset" : 45,
      "endOffset" : 58
    }, {
      "referenceID" : 74,
      "context" : "5 Multidimensional IRT models (Reckase, 2009) could—in addition to better modeling difficulty—also cluster items for interpretation; we briefly experiment with this (Appendix F), but leave more to future work (§8).",
      "startOffset" : 30,
      "endOffset" : 45
    }, {
      "referenceID" : 87,
      "context" : "This is a more mathematical formulation of the “easy” and “hard” dataset splits in question answering (Sugawara et al., 2018; Rondeau and Hazen, 2018; Sen and Saffari, 2020).",
      "startOffset" : 102,
      "endOffset" : 173
    }, {
      "referenceID" : 78,
      "context" : "This is a more mathematical formulation of the “easy” and “hard” dataset splits in question answering (Sugawara et al., 2018; Rondeau and Hazen, 2018; Sen and Saffari, 2020).",
      "startOffset" : 102,
      "endOffset" : 173
    }, {
      "referenceID" : 84,
      "context" : "This is a more mathematical formulation of the “easy” and “hard” dataset splits in question answering (Sugawara et al., 2018; Rondeau and Hazen, 2018; Sen and Saffari, 2020).",
      "startOffset" : 102,
      "endOffset" : 173
    }, {
      "referenceID" : 32,
      "context" : "To estimate the latent parameters of our model, we use mean-field variational inference (Jordan et al., 1999).",
      "startOffset" : 88,
      "endOffset" : 109
    }, {
      "referenceID" : 50,
      "context" : "IRT explicitly accounts for the bandwidth-fidelity dilemma (McBride, 1976): items can either accurately measure a narrow ability range (fidelity) or inaccurately measure large ability ranges (bandwidth).",
      "startOffset" : 59,
      "endOffset" : 74
    }, {
      "referenceID" : 104,
      "context" : "The first ameliorates the unavoidable randomness of finite evaluations while the second enables error analysis (Wu et al., 2019) and model probing (Belinkov and Glass, 2019; Zhang et al.",
      "startOffset" : 111,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : "We compare against a logistic regression linear model (LM) implemented with Vowpal Wabbit (Agarwal et al., 2014).",
      "startOffset" : 90,
      "endOffset" : 112
    }, {
      "referenceID" : 103,
      "context" : "Following prior work (Wu et al., 2020), we evaluate IRT and linear models by holding out 10% of responses and computing classification metrics.",
      "startOffset" : 21,
      "endOffset" : 38
    }, {
      "referenceID" : 12,
      "context" : "To test the first, we measure the ranking stability of mutually exclusive samples of the development data (Buckley and Voorhees, 2000).",
      "startOffset" : 106,
      "endOffset" : 134
    }, {
      "referenceID" : 94,
      "context" : "To test the second, we measure the correlation between development set sample rankings to test set rankings (Voorhees, 1998).",
      "startOffset" : 108,
      "endOffset" : 124
    }, {
      "referenceID" : 33,
      "context" : "(1) sample two partitions of the data, (2) compute the classical ranking9 and the IRT ranking from a refit IRT-feas model, then (3) compute Kendall’s correlation (Kendall, 1938) between the samples for each ranking (details in Appendix D).",
      "startOffset" : 162,
      "endOffset" : 177
    }, {
      "referenceID" : 57,
      "context" : "In educational testing, collecting responses from humans is expensive; likewise, although questions are cheap in searchbased QA tasks (Nguyen et al., 2016; Kwiatkowski et al., 2019), annotating answers is expensive.",
      "startOffset" : 134,
      "endOffset" : 181
    }, {
      "referenceID" : 83,
      "context" : "Likewise, “grading” machine dialog responses is expensive and IRT helps (Sedoc and Ungar, 2020).",
      "startOffset" : 72,
      "endOffset" : 95
    }, {
      "referenceID" : 102,
      "context" : "To emulate this setting, we use computerized adaptive testing (Weiss and Kingsbury, 1984) to iteratively select SQuAD items to “annotate.",
      "startOffset" : 62,
      "endOffset" : 89
    }, {
      "referenceID" : 41,
      "context" : "Like prior work, we compare selecting items with the highest difficulty and the highest discriminability (Lalor et al., 2019) as well as the sum of the",
      "startOffset" : 105,
      "endOffset" : 125
    }, {
      "referenceID" : 101,
      "context" : "12 We propose that items should be selected according to their Fisher information content (Weiss, 1982)",
      "startOffset" : 90,
      "endOffset" : 103
    }, {
      "referenceID" : 42,
      "context" : "This approach is similar to uncertainty sampling and reduces to it for the IRT-base model (Lewis and Gale, 1994).",
      "startOffset" : 90,
      "endOffset" : 112
    }, {
      "referenceID" : 54,
      "context" : "Like computerized adaptive testing (Moreno et al., 1984), Figure 5 shows that at lower sample sizes three of the IRT sampling methods are better than random sampling—difficulty does worse.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 78,
      "context" : "Several works curate easy and hard QA subsets based on how many models answer correctly (Rondeau and Hazen, 2018) or heuristics (Sugawara et al.",
      "startOffset" : 88,
      "endOffset" : 113
    }, {
      "referenceID" : 87,
      "context" : "Several works curate easy and hard QA subsets based on how many models answer correctly (Rondeau and Hazen, 2018) or heuristics (Sugawara et al., 2018).",
      "startOffset" : 128,
      "endOffset" : 151
    }, {
      "referenceID" : 82,
      "context" : ", 2016, 2019) where automated metrics can be misleading (Sedoc et al., 2019): machine translation (Hopkins and May, 2013) and chatbot evaluation (Sedoc",
      "startOffset" : 56,
      "endOffset" : 76
    }, {
      "referenceID" : 30,
      "context" : ", 2019): machine translation (Hopkins and May, 2013) and chatbot evaluation (Sedoc",
      "startOffset" : 29,
      "endOffset" : 52
    }, {
      "referenceID" : 39,
      "context" : "We test IRT as a way to guide annotation, but it can also train NLP models; for example, deep models learn “easy” examples faster (Lalor et al., 2018) and maintain test accuracy when training data are down-sampled (Lalor et al.",
      "startOffset" : 130,
      "endOffset" : 150
    }, {
      "referenceID" : 41,
      "context" : ", 2018) and maintain test accuracy when training data are down-sampled (Lalor et al., 2019).",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 43,
      "context" : "Improving Leaderboards The rise NLP leaderboards has encouraged critical thought into improving them (Linzen, 2020), improving evaluation more broadly (Eger et al.",
      "startOffset" : 101,
      "endOffset" : 115
    }, {
      "referenceID" : 81,
      "context" : ", 2020), and thoughtful consideration of their influence on the direction of research (Sculley et al., 2018; Dotan and Milli, 2020).",
      "startOffset" : 86,
      "endOffset" : 131
    }, {
      "referenceID" : 16,
      "context" : ", 2020), and thoughtful consideration of their influence on the direction of research (Sculley et al., 2018; Dotan and Milli, 2020).",
      "startOffset" : 86,
      "endOffset" : 131
    }, {
      "referenceID" : 29,
      "context" : "DAD aims make leaderboard yardsticks (Hernandez-Orallo, 2020) more reliable, interpretable, and part of curating the benchmark itself.",
      "startOffset" : 37,
      "endOffset" : 61
    }, {
      "referenceID" : 17,
      "context" : "In line with our reliability goal, just as statistical tests should appear in publications (Dror et al., 2018; Dodge et al., 2019), they should be “freebies” for leaderboard participants (Ethayarajh and Jurafsky, 2020).",
      "startOffset" : 91,
      "endOffset" : 130
    }, {
      "referenceID" : 15,
      "context" : "In line with our reliability goal, just as statistical tests should appear in publications (Dror et al., 2018; Dodge et al., 2019), they should be “freebies” for leaderboard participants (Ethayarajh and Jurafsky, 2020).",
      "startOffset" : 91,
      "endOffset" : 130
    }, {
      "referenceID" : 21,
      "context" : ", 2019), they should be “freebies” for leaderboard participants (Ethayarajh and Jurafsky, 2020).",
      "startOffset" : 64,
      "endOffset" : 95
    }, {
      "referenceID" : 23,
      "context" : "How to aggregate multi-task benchmarks (Wang et al., 2019b,a; Fisch et al., 2019) and multi-metric benchmarks (Ma et al.",
      "startOffset" : 39,
      "endOffset" : 81
    }, {
      "referenceID" : 46,
      "context" : ", 2019) and multi-metric benchmarks (Ma et al., 2021) is an open question which—although we do not address—is one use for IRT.",
      "startOffset" : 36,
      "endOffset" : 53
    }, {
      "referenceID" : 73,
      "context" : "As a (static) leaderboard ages, the task(s) overfit (Recht et al., 2019) which—although mitigable (Blum and Hardt, 2015; Anderson-Cook et al.",
      "startOffset" : 52,
      "endOffset" : 72
    }, {
      "referenceID" : 9,
      "context" : ", 2019) which—although mitigable (Blum and Hardt, 2015; Anderson-Cook et al., 2019)—is best solved by continually collecting new data (Kiela et al.",
      "startOffset" : 33,
      "endOffset" : 83
    }, {
      "referenceID" : 1,
      "context" : ", 2019) which—although mitigable (Blum and Hardt, 2015; Anderson-Cook et al., 2019)—is best solved by continually collecting new data (Kiela et al.",
      "startOffset" : 33,
      "endOffset" : 83
    }, {
      "referenceID" : 98,
      "context" : "Ideally, new data should challenge models through adversarial collection (Wallace et al., 2019b; Nie et al., 2020) and related methods (Gardner et al.",
      "startOffset" : 73,
      "endOffset" : 114
    }, {
      "referenceID" : 59,
      "context" : "Ideally, new data should challenge models through adversarial collection (Wallace et al., 2019b; Nie et al., 2020) and related methods (Gardner et al.",
      "startOffset" : 73,
      "endOffset" : 114
    }, {
      "referenceID" : 95,
      "context" : "However, if making an easy leaderboard more difficult is possible, the leaderboard has outlived its helpfulness and should be retired (Voorhees, 2000).",
      "startOffset" : 134,
      "endOffset" : 150
    }, {
      "referenceID" : 63,
      "context" : "Indeed, focusing solely these factors can mislead the public (Paullada et al., 2020) and may not reflect human language capabilities (Schlangen, 2020).",
      "startOffset" : 61,
      "endOffset" : 84
    }, {
      "referenceID" : 79,
      "context" : ", 2020) and may not reflect human language capabilities (Schlangen, 2020).",
      "startOffset" : 56,
      "endOffset" : 73
    }, {
      "referenceID" : 5,
      "context" : "Leaderboards are also well positioned to provide incentive structures for participants to prioritize fairness (Bender and Friedman, 2018) and efficiency (Strubell et al.",
      "startOffset" : 110,
      "endOffset" : 137
    }, {
      "referenceID" : 86,
      "context" : "Leaderboards are also well positioned to provide incentive structures for participants to prioritize fairness (Bender and Friedman, 2018) and efficiency (Strubell et al., 2019; Schwartz et al., 2020; Min et al., 2021) or incorporate testing of specific capabilities (Ribeiro et al.",
      "startOffset" : 153,
      "endOffset" : 217
    }, {
      "referenceID" : 80,
      "context" : "Leaderboards are also well positioned to provide incentive structures for participants to prioritize fairness (Bender and Friedman, 2018) and efficiency (Strubell et al., 2019; Schwartz et al., 2020; Min et al., 2021) or incorporate testing of specific capabilities (Ribeiro et al.",
      "startOffset" : 153,
      "endOffset" : 217
    }, {
      "referenceID" : 75,
      "context" : ", 2021) or incorporate testing of specific capabilities (Ribeiro et al., 2020; Dunietz et al., 2020).",
      "startOffset" : 56,
      "endOffset" : 100
    }, {
      "referenceID" : 18,
      "context" : ", 2021) or incorporate testing of specific capabilities (Ribeiro et al., 2020; Dunietz et al., 2020).",
      "startOffset" : 56,
      "endOffset" : 100
    }, {
      "referenceID" : 46,
      "context" : "To enable these more nuanced analyses, leaderboards should accept runnable models rather than static predictions (Ma et al., 2021).",
      "startOffset" : 113,
      "endOffset" : 130
    }, {
      "referenceID" : 85,
      "context" : "Active Learning Beyond IRT, the analysis of training dynamics and active learning (Settles, 2009) is helpful for actively sampling specific items or identifying low-quality items (Brodley and Friedl, 1999).",
      "startOffset" : 82,
      "endOffset" : 97
    }, {
      "referenceID" : 11,
      "context" : "Active Learning Beyond IRT, the analysis of training dynamics and active learning (Settles, 2009) is helpful for actively sampling specific items or identifying low-quality items (Brodley and Friedl, 1999).",
      "startOffset" : 179,
      "endOffset" : 205
    }, {
      "referenceID" : 10,
      "context" : "Explicitly measuring how effectively examples separate the best subject from the rest allows test set curators to “focus on the bubble” (Boyd-Graber and Börschinger, 2020), prioritizing examples most likely to reveal interesting distinctions between submitted systems.",
      "startOffset" : 136,
      "endOffset" : 171
    }, {
      "referenceID" : 68,
      "context" : "Ideal point models (Poole and Rosenthal, 2017) consider how a legislator (subject) will vote on a bill (item) and use a similar mathematical formulation.",
      "startOffset" : 19,
      "endOffset" : 46
    }, {
      "referenceID" : 26,
      "context" : "The venerable ELO model (Glickman and Jones, 1999) and modern extensions (Herbrich et al.",
      "startOffset" : 24,
      "endOffset" : 50
    }, {
      "referenceID" : 28,
      "context" : "The venerable ELO model (Glickman and Jones, 1999) and modern extensions (Herbrich et al., 2007) predict whether a player (subject) will defeat an opponent (item) with, again, a similar mathematical model.",
      "startOffset" : 73,
      "endOffset" : 96
    }, {
      "referenceID" : 76,
      "context" : "Certain IRT models can also be formulated as nonlinear mixed models (Rijmen et al., 2003), where the item parameters are fixed effects and the latent sub-",
      "startOffset" : 68,
      "endOffset" : 89
    }, {
      "referenceID" : 13,
      "context" : ", example text) to predict responses, but in principle could; Bayesian models with metadata (Card et al., 2018) and ideal point models from political science (Poole and Rosenthal, 1985) that incorporate bills and speeches do exactly this (Gerrish and Blei, 2011; Nguyen et al.",
      "startOffset" : 92,
      "endOffset" : 111
    }, {
      "referenceID" : 67,
      "context" : ", 2018) and ideal point models from political science (Poole and Rosenthal, 1985) that incorporate bills and speeches do exactly this (Gerrish and Blei, 2011; Nguyen et al.",
      "startOffset" : 54,
      "endOffset" : 81
    }, {
      "referenceID" : 25,
      "context" : ", 2018) and ideal point models from political science (Poole and Rosenthal, 1985) that incorporate bills and speeches do exactly this (Gerrish and Blei, 2011; Nguyen et al., 2015; Kraft et al., 2016).",
      "startOffset" : 134,
      "endOffset" : 199
    }, {
      "referenceID" : 58,
      "context" : ", 2018) and ideal point models from political science (Poole and Rosenthal, 1985) that incorporate bills and speeches do exactly this (Gerrish and Blei, 2011; Nguyen et al., 2015; Kraft et al., 2016).",
      "startOffset" : 134,
      "endOffset" : 199
    }, {
      "referenceID" : 36,
      "context" : ", 2018) and ideal point models from political science (Poole and Rosenthal, 1985) that incorporate bills and speeches do exactly this (Gerrish and Blei, 2011; Nguyen et al., 2015; Kraft et al., 2016).",
      "startOffset" : 134,
      "endOffset" : 199
    }, {
      "referenceID" : 23,
      "context" : "models to evaluate multiple skills could aid multitask or multi-metric leaderboards like MRQA (Fisch et al., 2019) and Dynaboard (Ma et al.",
      "startOffset" : 94,
      "endOffset" : 114
    } ],
    "year" : 2021,
    "abstractText" : "Leaderboards are widely used in NLP and push the field forward. While leaderboards are a straightforward ranking of NLP models, this simplicity can mask nuances in evaluation items (examples) and subjects (NLP models). Rather than replace leaderboards, we advocate a re-imagining so that they better highlight if and where progress is made. Building on educational testing, we create a Bayesian leaderboard model where latent subject skill and latent item difficulty predict correct responses. Using this model, we analyze the ranking reliability of leaderboards. Afterwards, we show the model can guide what to annotate, identify annotation errors, detect overfitting, and identify informative examples. We conclude with recommendations for future benchmark tasks. 1 Leaderboards are Shiny Leaderboard evaluations—for better or worse—are the de facto standard for measuring progress in question answering (Rajpurkar et al., 2016) and in many NLP tasks (Wang et al., 2019a). An unfortunate side effect of leaderboard popularity is SOTA-chasing, often at the expense of carefully inspecting data and models (Linzen, 2020). For example, the same “super-human” models that top question answering leaderboards (Najberg, 2018) often fail spectacularly (Feng et al., 2018; Wallace et al., 2019a) by learning non-generalizable statistical patterns (McCoy et al., 2019; Niven and Kao, 2019). Finally, focusing solely on metrics conflates progress on a specific task with progress on realworld NLP problems behind the task (Bender and Koller, 2020). Plainly, focusing on headline SOTA numbers “provide(s) limited value for scientific progress absent insight into what drives them” and where they fail (Lipton and Steinhardt, 2019). ∗Work completed at University of Maryland. −7.5 −5.5 −3.5 −1.5 0.5 2.5 4.5 6.5 8.5 0 2,000 −8 −6 −4 −2 0 2 4 6 8 10 Difficulty ( ) −10 −5 0 5 10 D is cr im in ab ili ty ( ) 0 2,000 −10.0 −8.0 −6.0 −4.0 −2.0 0.0 2.0 4.0 6.0 8.0 10.0 12.0 0.0 1.0 Feasibility (λ) Annotation Error Discriminative and Hard Discriminative",
    "creator" : "LaTeX with hyperref"
  }
}