{
  "name" : "2021.acl-long.393.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer",
    "authors" : [ "Yuanmeng Yan", "Rumei Li", "Sirui Wang", "Fuzheng Zhang", "Wei Wu", "Weiran Xu" ],
    "emails" : [ "yanyuanmeng@bupt.edu.cn", "xuweiran@bupt.edu.cn", "lirumei@meituan.com", "wangsirui@meituan.com", "zhangfuzheng@meituan.com", "wuwei30@meituan.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5065–5075\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5065"
    }, {
      "heading" : "1 Introduction",
      "text" : "Sentence representation learning plays a vital role in natural language processing tasks (Kiros et al., 2015; Hill et al., 2016; Conneau et al., 2017; Cer et al., 2018). Good sentence representations benefit a wide range of downstream tasks, especially for computationally expensive ones, including largescale semantic similarity comparison and information retrieval.\nRecently, BERT-based pre-trained language models have achieved high performance on many\n∗Work done during internship at Meituan Inc. The first two authors contribute equally. Weiran Xu is the corresponding author.\ndownstream tasks with additional supervision. However, the native sentence representations derived from BERT1 are proved to be of low-quality (Reimers and Gurevych, 2019; Li et al., 2020). As shown in Figure 1a, when directly adopt BERTbased sentence representations to semantic textual similarity (STS) tasks, almost all pairs of sentences achieved a similarity score between 0.6 to 1.0 , even if some pairs are regarded as completely unrelated by the human annotators. In other words, the BERT-derived native sentence representations are somehow collapsed (Chen and He, 2020), which means almost all sentences are mapped into a small area and therefore produce high similarity.\nSuch phenomenon is also observed in several previous works (Gao et al., 2019; Wang et al., 2019; Li et al., 2020). They find the word representation space of BERT is anisotropic, the high-frequency words are clustered and close to the origin, while low-frequency words disperse sparsely. When averaging token embeddings, those high-frequency words dominate the sentence representations, inducing biases against their real semantics 2. As a\n1Typically, we take the output of the [CLS] token or average token embeddings at the last few layers as the sentence representations.\n2We also empirically prove this hypothesis, please refer to Section 5.1 for more details.\nresult, it is inappropriate to directly apply BERT’s native sentence representations for semantic matching or text retrieval. Traditional methods usually fine-tune BERT with additional supervision. However, human annotation is costly and often unavailable in real-world scenarios.\nTo alleviate the collapse issue of BERT as well as reduce the requirement for labeled data, we propose a novel sentence-level training objective based on contrastive learning (He et al., 2020; Chen et al., 2020a,b). By encouraging two augmented views from the same sentence to be closer while keeping views from other sentences away, we reshape the BERT-derived sentence representation space and successfully solve the collapse issue (shown in Figure 1b). Moreover, we propose multiple data augmentation strategies for contrastive learning, including adversarial attack (Goodfellow et al., 2014; Kurakin et al., 2016), token shuffling, cutoff (Shen et al., 2020) and dropout (Hinton et al., 2012), that effectively transfer the sentence representations to downstream tasks. We name our approach ConSERT, a Contrastive Framework for SEntence Representation Transfer.\nConSERT has several advantages over previous approaches. Firstly, it introduces no extra structure or specialized implementation during inference. The parameter size of ConSERT keeps the same as BERT, making it easy to use. Secondly, compared with pre-training approaches, ConSERT is more efficient. With only 1,000 unlabeled texts drawn from the target distribution (which is easy to collect in real-world applications), we achieve 35% relative performance gain over BERT, and the training stage takes only a few minutes (1-2k steps) on a single V100 GPU. Finally, it includes several effective and convenient data augmentation methods with minimal semantic impact. Their effects are validated and analyzed in the ablation studies.\nOur contributions can be summarized as follows: 1) We propose a simple but effective sentence-level training objective based on contrastive learning. It mitigates the collapse of BERT-derived representations and transfers them to downstream tasks. 2) We explore various effective text augmentation strategies to generate views for contrastive learning and analyze their effects on unsupervised sentence representation transfer. 3) With only fine-tuning on unsupervised target datasets, our approach achieves significant improvement on STS tasks. When further incorporating with NLI supervision, our ap-\nproach achieves new state-of-the-art performance. We also show the robustness of our approach in data scarcity scenarios and intuitive analysis of the transferred representations.3"
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Sentence Representation Learning",
      "text" : "Supervised Approaches Several works use supervised datasets for sentence representation learning. Conneau et al. (2017) finds the supervised Natural Language Inference (NLI) task is useful to train good sentence representations. They use a BiLSTM-based encoder and train it on two NLI datasets, Stanford NLI (SNLI) (Bowman et al., 2015) and Multi-Genre NLI (MNLI) (Williams et al., 2018). Universal Sentence Encoder (Cer et al., 2018) adopts a Transformer-based architecture and uses the SNLI dataset to augment the unsupervised training. SBERT (Reimers and Gurevych, 2019) proposes a siamese architecture with a shared BERT encoder and is also trained on SNLI and MNLI datasets.\nSelf-supervised Objectives for Pre-training BERT (Devlin et al., 2019) proposes a bidirectional Transformer encoder for language model pre-training. It includes a sentence-level training objective, namely next sentence prediction (NSP), which predicts whether two sentences are adjacent or not. However, NSP is proved to be weak and has little contribution to the final performance (Liu et al., 2019). After that, various self-supervised objectives are proposed for pre-training BERT-like sentence encoders. CrossThought (Wang et al., 2020) and CMLM (Yang et al., 2020) are two similar objectives that recover masked tokens in one sentence conditioned on the representations of its contextual sentences. SLM (Lee et al., 2020) proposes an objective that reconstructs the correct sentence ordering given the shuffled sentences as the input. However, all these objectives need document-level corpus and are thus not applicable to downstream tasks with only short texts.\nUnsupervised Approaches BERT-flow (Li et al., 2020) proposes a flow-based approach that maps BERT embeddings to a standard Gaussian latent space, where embeddings are more suitable for comparison. However, this approach introduces\n3Our code is available at https://github.com/ yym6472/ConSERT.\nextra model structures and need specialized implementation, which may limit its application."
    }, {
      "heading" : "2.2 Contrastive Learning",
      "text" : "Contrastive Learning for Visual Representation Learning Recently, contrastive learning has become a very popular technique in unsupervised visual representation learning with solid performance (Chen et al., 2020a; He et al., 2020; Chen et al., 2020b). They believe that good representation should be able to identify the same object while distinguishing itself from other objects. Based on this intuition, they apply image transformations (e.g. cropping, rotation, cutout, etc.) to randomly generate two augmented versions for each image and make them close in the representation space. Such approaches can be regarded as the invariance modeling to the input samples. Chen et al. (2020a) proposes SimCLR, a simple framework for contrastive learning. They use the normalized temperature-scaled cross-entropy loss (NT-Xent) as the training loss, which is also called InfoNCE in the previous literature (Hjelm et al., 2018).\nContrastive Learning for Textual Representation Learning Recently, contrastive learning has been widely applied in NLP tasks. Many works use it for language model pre-training. IS-BERT (Zhang et al., 2020) proposes to add 1-D convolutional neural network (CNN) layers on top of BERT and train the CNNs by maximizing the mutual information (MI) between the global sentence embedding and its corresponding local contexts embeddings. CERT (Fang and Xie, 2020) adopts a similar structure as MoCo (He et al., 2020) and uses back-translation for data augmentation. However, the momentum encoder needs extra memory and back-translation may produce false positives. BERT-CT (Carlsson et al., 2021) uses two individual encoders for contrastive learning, which also needs extra memory. Besides, they only sample 7 negatives, resulting in low training efficiency. DeCLUTR (Giorgi et al., 2020) adopts the architecture of SimCLR and jointly trains the model with contrastive objective and masked language model objective. However, they only use spans for contrastive learning, which is fragmented in semantics. CLEAR (Wu et al., 2020) uses the same architecture and objectives as DeCLUTR. Both of them are used to pre-train the language model, which needs a large corpus and takes a lot of resources."
    }, {
      "heading" : "3 Approach",
      "text" : "In this section, we present ConSERT for sentence representation transfer. Given a BERT-like pretrained language model M and an unsupervised dataset D drawn from the target distribution, we aim at fine-tuning M on D to make the sentence representation more task-relevant and applicable to downstream tasks. We first present the general framework of our approach, then we introduce several data augmentation strategies for contrastive learning. Finally, we talk about three ways to further incorporate supervision signals."
    }, {
      "heading" : "3.1 General Framework",
      "text" : "Our approach is mainly inspired by SimCLR (Chen et al., 2020a). As shown in Figure 2, there are three major components in our framework:\n• A data augmentation module that generates different views for input samples at the token embedding layer.\n• A shared BERT encoder that computes sentence representations for each input text. During training, we use the average pooling of the token embeddings at the last layer to obtain sentence representations.\n• A contrastive loss layer on top of the BERT encoder. It maximizes the agreement between one representation and its corresponding version that is augmented from the same sentence while keeping it distant from other sentence representations in the same batch.\nFor each input text x, we first pass it to the data augmentation module, in which two transformations T1 and T2 are applied to generate two versions of token embeddings: ei = T1(x), ej = T2(x), where ei, ej ∈ RL×d, L is the sequence length and d is the hidden dimension. After that, both ei and ej will be encoded by multi-layer transformer blocks in BERT and produce the sentence representations ri and rj through average pooling.\nFollowing Chen et al. (2020a), we adopt the normalized temperature-scaled cross-entropy loss (NTXent) as the contrastive objective. During each training step, we randomly sample N texts from D to construct a mini-batch, resulting in 2N representations after augmentation. Each data point is trained to find out its counterpart among 2(N − 1) in-batch negative samples:\nLi,j = − log exp(sim(ri, rj)/τ)∑2N\nk=1 1[k 6=i] exp(sim(ri, rk)/τ) (1)\n, where sim(·) indicates the cosine similarity function, τ controls the temperature and 1 is the indicator. Finally, we average all 2N in-batch classification losses to obtain the final contrastive loss Lcon."
    }, {
      "heading" : "3.2 Data Augmentation Strategies",
      "text" : "We explore four different data augmentation strategies to generate views for contrastive learning, including adversarial attack (Goodfellow et al., 2014; Kurakin et al., 2016), token shuffling, cutoff (Shen et al., 2020) and dropout (Hinton et al., 2012), as illustrated in Figure 3.\nAdversarial Attack Adversarial training is generally used to improve the model’s robustness. They generate adversarial samples by adding a\nworst-case perturbation to the input sample. We implement this strategy with Fast Gradient Value (FGV) (Rozsa et al., 2016), which directly uses the gradient to compute the perturbation and thus is faster than two-step alternative methods. Note that this strategy is only applicable when jointly training with supervision since it relies on supervised loss to compute adversarial perturbations.\nToken Shuffling In this strategy, we aim to randomly shuffle the order of the tokens in the input sequences. Since the bag-of-words nature in the transformer architecture, the position encoding is the only factor about the sequential information. Thus, similar to Lee et al. (2020), we implement this strategy by passing the shuffled position ids to the embedding layer while keeping the order of the token ids unchanged.\nCutoff Shen et al. (2020) proposes a simple and efficient data augmentation strategy called cutoff. They randomly erase some tokens (for token cutoff), feature dimensions (for feature cutoff), or token spans (for span cutoff) in the L × d feature matrix. In our experiments, we only use token cutoff and feature cutoff and apply them to the token embeddings for view generation.\nDropout Dropout is a widely used regularization method that avoids overfitting. However, in our experiments, we also show its effectiveness as an augmentation strategy for contrastive learning. For this setting, we randomly drop elements in the token embedding layer by a specific probability and set their values to zero. Note that this strategy is different from Cutoff since each element is considered individually."
    }, {
      "heading" : "3.3 Incorporating Supervision Signals",
      "text" : "Besides unsupervised transfer, our approach can also be incorporated with supervised learning. We take the NLI supervision as an example. It is a sentence pair classification task, where the model are trained to distinguish the relation between two sentences among contradiction, entailment and neutral. The classification objective can be expressed as following:\nf = Concat(r1, r2, |r1 − r2|) Lce = CrossEntropy(Wf + b, y)\n(2)\n, where r1 and r2 denote two sentence representations.\nWe propose three ways for incorporating additional supervised signals:\n• Joint training (joint) We jointly train the model with the supervised and unsupervised objectives Ljoint = Lce + αLcon on NLI dataset. α is a hyper-parameter to balance two objectives.\n• Supervised training then unsupervised transfer (sup-unsup) We first train the model with Lce on NLI dataset, then use Lcon to finetune it on the target dataset.\n• Joint training then unsupervised transfer (joint-unsup) We first train the model with the Ljoint on NLI dataset, then use Lcon to fine-tune it on the target dataset."
    }, {
      "heading" : "4 Experiments",
      "text" : "To verify the effectiveness of our proposed approach, we conduct experiments on Semantic Textual Similarity (STS) tasks under the unsupervised and supervised settings."
    }, {
      "heading" : "4.1 Setups",
      "text" : "Dataset Following previous works(Reimers and Gurevych, 2019; Li et al., 2020; Zhang et al., 2020), we evaluate our approach on multiple STS datasets, including STS tasks 2012 - 2016 (STS12 - STS16) (Agirre et al., 2012, 2013, 2014, 2015, 2016), STS\nbenchmark (STSb) (Cer et al., 2017) and SICKRelatedness (SICK-R) (Marelli et al.). Each sample in these datasets contains a pair of sentences as well as a gold score between 0 and 5 to indicate their semantic similarity. For our unsupervised experiments, we mix the unlabeled texts from these datasets to fine-tune our model. We obtain all 7 datasets through the SentEval toolkit (Conneau and Kiela, 2018). The statistics is shown in Table 1.\nFor supervised experiments, we use the combination of SNLI (570k samples) (Bowman et al., 2015) and MNLI (430k samples) (Williams et al., 2018) to train our model. In the joint training setting, the NLI texts are also used for contrastive objectives.\nBaselines To show our effectiveness on unsupervised sentence representation transfer, we mainly select BERT-flow (Li et al., 2020) for comparison, since it shares the same setting as our approach. For unsupervised comparison, we use the average of GloVe embeddings, the BERT-derived native embeddings, CLEAR (Wu et al., 2020) (trained on BookCorpus and English Wikipedia corpus), ISBERT (Zhang et al., 2020) (trained on unlabeled texts from NLI datasets), BERT-CT (Carlsson et al., 2021) (trained on English Wikipedia corpus). For comparison with supervised methods, we select InferSent (Conneau et al., 2017), Universal Sentence Encoder (Cer et al., 2018), SBERT (Reimers and Gurevych, 2019) and BERT-CT (Carlsson et al., 2021) as baselines. They are all trained with NLI supervision.\nEvaluation When evaluating the trained model, we first obtain the representation of sentences by averaging the token embeddings at the last two layers4, then we report the spearman correlation between the cosine similarity scores of sentence representations and the human-annotated gold scores. When calculating spearman correlation, we merge all sentences together (even if some STS datasets have multiple splits) and calculate spearman correlation for only once5.\n4As shown in Li et al. (2020), averaging the last two layers of BERT achieves slightly better results than averaging the last one layer.\n5Note that such evaluation procedure is different from\nImplementation Details Our implementation is based on the Sentence-BERT6 (Reimers and Gurevych, 2019). We use both the BERT-base and BERT-large for our experiments. The max sequence length is set to 64 and we remove the default dropout layer in BERT architecture considering the cutoff and dropout data augmentation strategies used in our framework. The ratio of token cutoff and feature cutoff is set to 0.15 and 0.2 respectively, as suggested in Shen et al. (2020). The ratio of dropout is set to 0.2. The temperature τ of NT-Xent loss is set to 0.1, and the α is set to 0.15 for the joint training setting. We adopt Adam optimizer and set the learning rate to 5e-7. We use a linear learning rate warm-up over 10% of the training steps. The batch size is set to 96 in most of our experiments. We use the dev set of STSb to tune the hyperparameters (including the augmentation strategies) and evaluate the model every 200 steps during training. The best checkpoint on the dev set of STSb is saved for test. We further discuss the influence of the batch size and the temperature in the subsequent section."
    }, {
      "heading" : "4.2 Unsupervised Results",
      "text" : "For unsupervised evaluation, we load the pretrained BERT to initialize the BERT encoder in our framework. Then we randomly mix the unlabeled texts from 7 STS datasets and use them to fine-tune our model.\nSentEval toolkit, which calculates spearman correlation for each split and reports the mean or weighted mean scores.\n6https://github.com/UKPLab/ sentence-transformers\nThe results are shown in Table 2. We can observe that both BERT-flow and ConSERT can improve the representation space and outperform the GloVe and BERT baselines with unlabeled texts from target datasets. However, ConSERTlarge achieves the best performance among 6 STS datasets, significantly outperforming BERTlarge-flow with an 8% relative performance gain on average (from 70.76 to 76.45). Moreover, it is worth noting that ConSERTlarge even outperforms several supervised baselines (see Figure 3) like InferSent (65.01) and Universal Sentence Encoder (71.72), and keeps comparable to the strong supervised method SBERTlarge-NLI (76.55). For the BERTbase architecture, our approach ConSERTbase also outperforms BERTbase-flow with an improvement of 3.17 (from 69.57 to 72.74)."
    }, {
      "heading" : "4.3 Supervised Results",
      "text" : "For supervised evaluation, we consider the three settings described in Section 3.3. Note that in the joint setting, only NLI texts are used for contrastive learning, making it comparable to SBERT-NLI. We use the model trained under the joint setting as the initial checkpoint in the joint-unsup setting. We also re-implement the SBERT-NLI baselines and use them as the initial checkpoint in the sup-unsup setting.\nThe results are illustrated in Table 3. For the models trained with NLI supervision, we find that ConSERT joint consistently performs better than SBERT, revealing the effectiveness of our proposed contrastive objective as well as the data augmentation strategies. On average, ConSERTbase joint\nachieves a performance gain of 2.88 over the reimplemented SBERTbase-NLI, and ConSERTlarge joint achieves a performance gain of 2.70.\nWhen further performing representation transfer with STS unlabeled texts, our approach achieves even better performance. On average, ConSERTlarge joint-unsup outperforms the initial checkpoint ConSERTlarge joint with 1.84 performance gain, and outperforms the previous state-ofthe-art BERTlarge-flow with 2.92 performance gain. The results demonstrate that even for the models trained under supervision, there is still a huge potential of unsupervised representation transfer for improvement."
    }, {
      "heading" : "5 Qualitative Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Analysis of BERT Embedding Space",
      "text" : "To prove the hypothesis that the collapse issue is mainly due to the anisotropic space that is sensitive to the token frequency, we conduct experiments that mask the embeddings of several most frequent tokens when applying average pooling to calculate the sentence representations. The relation between the number of removed top-k frequent tokens and the average spearman correlation is shown in Figure 4.\nWe can observe that when removing a few top frequent tokens, the performance of BERT improves sharply on STS tasks. When removing\n34 most frequent tokens, the best performance is achieved (61.66), and there is an improvement of 7.8 from the original performance (53.86). For ConSERT, we find that removing a few most frequent tokens only results in a small improvement of less than 0.3. The results show that our approach reshapes the BERT’s original embedding space, reducing the influence of common tokens on sentence representations."
    }, {
      "heading" : "5.2 Effect of Data Augmentation Strategy",
      "text" : "In this section, we study the effect of data augmentation strategies for contrastive learning. We consider 5 options for each transformation, including None (i.e. doing nothing), Shuffle, Token Cutoff,\nFeature Cutoff, and Dropout, resulting in 5×5 combinations. Note that the Adversarial Attack strategy is not considered here, since it needs additional supervision to generate adversarial samples. All these experiments follow the unsupervised setting and use the BERTbase architecture.\nThe results can be found in Figure 5. We can make the following observations. First, Shuffle and Token Cutoff are the two most effective strategies (where Shuffle is slightly better than Token Cutoff), significantly outperforming Feature Cutoff and Dropout. This is probably because Shuffle and Token Cutoff are more related to the downstream STS tasks since they are directly operated on the token level and change the structure of the sentence to produce hard examples.\nSecondly, Feature Cutoff and Dropout also improve performance by roughly 4 points when compared with the None-None baseline. Moreover, we find they work well as a complementary strategy. Combining with another strategy like Shuffle may further improve the performance. When combined Shuffle with Feature Cutoff, we achieve the best result. We argue that Feature Cutoff and Dropout are useful in modeling the invariance of the internal noise for the sentence encoder, and thus improve the model’s robustness.\nFinally, we also observe that even without any data augmentation (the None-None combination), our contrastive framework can improve BERT’s performance on STS tasks (from 53.86 to 63.84). This None-None combination has no effect on maximizing agreement between views since the repre-\nsentations of augmented views are exactly the same. On the contrary, it tunes the representation space by pushing each representation away from others. We believe that the improvement is mainly due to the collapse phenomenon of BERT’s native representation space. To some extent, it also explains why our method works."
    }, {
      "heading" : "5.3 Performance under Few-shot Settings",
      "text" : "To validate the reliability and the robustness of ConSERT under the data scarcity scenarios, we conduct the few-shot experiments. We limit the number of unlabeled texts to 1, 10, 100, 1000, and 10000 respectively, and compare their performance with the full dataset.\nFigure 6 presents the results. For both the unsupervised and the supervised settings, our approach can make a huge improvement over the baseline with only 100 samples available. When the training samples increase to 1000, our approach can basically achieve comparable results with the models trained on the full dataset. The results reveal the robustness and effectiveness of our approach under the data scarcity scenarios, which is common in reality. With only a small amount of unlabeled texts drawn from the target data distribution, our approach can also tune the representation space and benefit the downstream tasks."
    }, {
      "heading" : "5.4 Influence of Temperature",
      "text" : "The temperature τ in NT-Xent loss (Equation 1) is used to control the smoothness of the distribution normalized by softmax operation and thus influences the gradients when backpropagation. A large temperature smooths the distribution while a small temperature sharpens the distribution. In our experiments, we explore the influence of temperature\nand present the result in Figure 7.\nAs shown in the figure, we find the performance is extremely sensitive to the temperature. Either too small or too large temperature will make our model perform badly. And the optimal temperature is obtained within a small range (from about 0.08 to 0.12). This phenomenon again demonstrates the collapse issue of BERT embeddings, as most sentences are close to each other, a large temperature may make this task too hard to learn. We select 0.1 as the temperature in most of our experiments."
    }, {
      "heading" : "5.5 Influence of Batch Size",
      "text" : "In some previous works of contrastive learning, it is reported that a large batch size benefits the final performance and accelerates the convergence of the model since it provides more in-batch negative samples for contrastive learning (Chen et al., 2020a). Those in-batch negative samples improve the training efficiency. We also analyze the influence of the batch size for unsupervised sentence representation transfer.\nThe results are illustrated in Table 4. We show both the spearman correlation and the corresponding training steps. We find that a larger batch size does achieve better performance. However, the improvement is not so significant. Meanwhile, a larger batch size does speed up the training process, but it also needs more GPU memories at the same time."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we propose ConSERT, a selfsupervised contrastive learning framework for transferring sentence representations to downstream tasks. The framework does not need extra structure and is easy to implement for any encoder. We demonstrate the effectiveness of our framework on various STS datasets, both our unsupervised and supervised methods achieve new state-of-the-art performance. Furthermore, few-shot experiments suggest that our framework is robust in the data scarcity scenarios. We also compare multiple combinations of data augmentation strategies and provide fine-grained analysis for interpreting how our approach works. We hope our work will provide a new perspective for future researches on sentence representation transfer."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Keqing He, Hongzhi Zhang and all anonymous reviewers for their helpful comments and suggestions. This work was partially supported by National Key R&D Program of China No. 2019YFF0303300 and Subject II No. 2019YFF0303302, DOCOMO Beijing Communications Laboratories Co., Ltd, MoE-CMCC “Artifical Intelligence” Project No. MCM20190701.\nBroader Impact\nSentence representation learning is a basic task in natural language processing and benefits many downstream tasks. This work proposes a contrastive learning based framework to solve the collapse issue of BERT and transfer BERT sentence representations to target data distribution. Our approach not only provides a new perspective about BERT’s representation space, but is also useful in practical applications, especially for data scarcity scenarios. When applying our approach, the user should collect a few unlabeled texts from target data distribution and use our framework to finetune BERT encoder in a self-supervised manner. Since our approach is self-supervised, no bias will be introduced from human annotations. Moreover, our data augmentation strategies also have little probability to introduce extra biases since they are all based on random sampling. However, it is still possible to introduce data biases from the unlabeled texts. Therefore, users should pay special attention to ensure that the training data is ethical, unbiased, and closely related to downstream tasks."
    } ],
    "references" : [ {
      "title" : "Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on",
      "author" : [ "Eneko Agirre", "Carmen Banea", "Claire Cardie", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Weiwei Guo", "Inigo Lopez-Gazpio", "Montse Maritxalar", "Rada Mihalcea" ],
      "venue" : null,
      "citeRegEx" : "Agirre et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2015
    }, {
      "title" : "Semeval-2014 task 10: Multilingual semantic textual similarity",
      "author" : [ "Eneko Agirre", "Carmen Banea", "Claire Cardie", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Weiwei Guo", "Rada Mihalcea", "German Rigau", "Janyce Wiebe." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Agirre et al\\.,? 2014",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2014
    }, {
      "title" : "Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation",
      "author" : [ "Eneko Agirre", "Carmen Banea", "Daniel Cer", "Mona Diab", "Aitor Gonzalez Agirre", "Rada Mihalcea", "German Rigau Claramunt", "Janyce Wiebe." ],
      "venue" : "In",
      "citeRegEx" : "Agirre et al\\.,? 2016",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2016
    }, {
      "title" : "Semeval-2012 task 6: A pilot on semantic textual similarity",
      "author" : [ "Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre." ],
      "venue" : "* SEM 2012: The First Joint Conference on Lexical and Computational Semantics–Volume 1: Proceedings of the",
      "citeRegEx" : "Agirre et al\\.,? 2012",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2012
    }, {
      "title" : " sem 2013 shared task: Semantic textual similarity",
      "author" : [ "Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor GonzalezAgirre", "Weiwei Guo." ],
      "venue" : "Second joint conference on lexical and computational semantics (* SEM), volume 1: proceedings of the Main confer-",
      "citeRegEx" : "Agirre et al\\.,? 2013",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2013
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Semantic re-tuning with contrastive tension",
      "author" : [ "Fredrik Carlsson", "Magnus Sahlgren", "Evangelia Gogoulou", "Amaru Cuba Gyllensten", "Erik Ylipää Hellqvist." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Carlsson et al\\.,? 2021",
      "shortCiteRegEx" : "Carlsson et al\\.",
      "year" : 2021
    }, {
      "title" : "Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
      "author" : [ "Daniel Cer", "Mona Diab", "Eneko Agirre", "Iñigo LopezGazpio", "Lucia Specia." ],
      "venue" : "Proceedings of the 11th International Workshop on Semantic Evalu-",
      "citeRegEx" : "Cer et al\\.,? 2017",
      "shortCiteRegEx" : "Cer et al\\.",
      "year" : 2017
    }, {
      "title" : "Universal sentence encoder for english",
      "author" : [ "Daniel Cer", "Yinfei Yang", "Sheng-yi Kong", "Nan Hua", "Nicole Limtiaco", "Rhomni St John", "Noah Constant", "Mario Guajardo-Cespedes", "Steve Yuan", "Chris Tar" ],
      "venue" : "In Proceedings of the 2018 Conference on Empirical",
      "citeRegEx" : "Cer et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Cer et al\\.",
      "year" : 2018
    }, {
      "title" : "A simple framework for contrastive learning of visual representations",
      "author" : [ "Ting Chen", "Simon Kornblith", "Mohammad Norouzi", "Geoffrey Hinton." ],
      "venue" : "arXiv preprint arXiv:2002.05709.",
      "citeRegEx" : "Chen et al\\.,? 2020a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Big selfsupervised models are strong semi-supervised learners",
      "author" : [ "Ting Chen", "Simon Kornblith", "Kevin Swersky", "Mohammad Norouzi", "Geoffrey Hinton" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring simple siamese representation learning",
      "author" : [ "Xinlei Chen", "Kaiming He." ],
      "venue" : "arXiv preprint arXiv:2011.10566.",
      "citeRegEx" : "Chen and He.,? 2020",
      "shortCiteRegEx" : "Chen and He.",
      "year" : 2020
    }, {
      "title" : "Senteval: An evaluation toolkit for universal sentence representations",
      "author" : [ "Alexis Conneau", "Douwe Kiela." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).",
      "citeRegEx" : "Conneau and Kiela.,? 2018",
      "shortCiteRegEx" : "Conneau and Kiela.",
      "year" : 2018
    }, {
      "title" : "Supervised learning of universal sentence representations from natural language inference data",
      "author" : [ "Alexis Conneau", "Douwe Kiela", "Holger Schwenk", "Loı̈c Barrault", "Antoine Bordes" ],
      "venue" : "In Proceedings of the 2017 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Conneau et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2017
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Cert: Contrastive self-supervised learning for language understanding",
      "author" : [ "Hongchao Fang", "Pengtao Xie." ],
      "venue" : "arXiv preprint arXiv:2005.12766.",
      "citeRegEx" : "Fang and Xie.,? 2020",
      "shortCiteRegEx" : "Fang and Xie.",
      "year" : 2020
    }, {
      "title" : "Representation degeneration problem in training natural language generation models",
      "author" : [ "Jun Gao", "Di He", "Xu Tan", "Tao Qin", "Liwei Wang", "TieYan Liu." ],
      "venue" : "arXiv preprint arXiv:1907.12009.",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "Declutr: Deep contrastive learning for unsupervised textual representations",
      "author" : [ "John M Giorgi", "Osvald Nitski", "Gary D. Bader", "Bo Wang." ],
      "venue" : "ArXiv, abs/2006.03659.",
      "citeRegEx" : "Giorgi et al\\.,? 2020",
      "shortCiteRegEx" : "Giorgi et al\\.",
      "year" : 2020
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy." ],
      "venue" : "arXiv preprint arXiv:1412.6572.",
      "citeRegEx" : "Goodfellow et al\\.,? 2014",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Momentum contrast for unsupervised visual representation learning",
      "author" : [ "Kaiming He", "Haoqi Fan", "Yuxin Wu", "Saining Xie", "Ross Girshick." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9729–9738.",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning distributed representations of sentences from unlabelled data",
      "author" : [ "Felix Hill", "Kyunghyun Cho", "Anna Korhonen." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Hill et al\\.,? 2016",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2016
    }, {
      "title" : "Improving neural networks by preventing coadaptation of feature detectors",
      "author" : [ "Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov." ],
      "venue" : "arXiv preprint arXiv:1207.0580.",
      "citeRegEx" : "Hinton et al\\.,? 2012",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning deep representations by mutual information estimation and maximization",
      "author" : [ "R Devon Hjelm", "Alex Fedorov", "Samuel LavoieMarchildon", "Karan Grewal", "Phil Bachman", "Adam Trischler", "Yoshua Bengio." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Hjelm et al\\.,? 2018",
      "shortCiteRegEx" : "Hjelm et al\\.",
      "year" : 2018
    }, {
      "title" : "Skip-thought vectors",
      "author" : [ "Ryan Kiros", "Yukun Zhu", "Russ R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "Advances in neural information processing systems, 28:3294–3302.",
      "citeRegEx" : "Kiros et al\\.,? 2015",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2015
    }, {
      "title" : "Adversarial examples in the physical world",
      "author" : [ "Alexey Kurakin", "Ian Goodfellow", "Samy Bengio." ],
      "venue" : "arXiv preprint arXiv:1607.02533.",
      "citeRegEx" : "Kurakin et al\\.,? 2016",
      "shortCiteRegEx" : "Kurakin et al\\.",
      "year" : 2016
    }, {
      "title" : "Slm: Learning a discourse language representation with sentence unshuffling",
      "author" : [ "Haejun Lee", "Drew A Hudson", "Kangwook Lee", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Lee et al\\.,? 2020",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "On the sentence embeddings from pre-trained language models",
      "author" : [ "Bohan Li", "Hao Zhou", "Junxian He", "Mingxuan Wang", "Yiming Yang", "Lei Li." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Sentencebert: Sentence embeddings using siamese bertnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Adversarial diversity and hard positive generation",
      "author" : [ "Andras Rozsa", "Ethan M Rudd", "Terrance E Boult." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 25–32.",
      "citeRegEx" : "Rozsa et al\\.,? 2016",
      "shortCiteRegEx" : "Rozsa et al\\.",
      "year" : 2016
    }, {
      "title" : "A simple but toughto-beat data augmentation approach for natural language understanding and generation",
      "author" : [ "Dinghan Shen", "Mingzhi Zheng", "Yelong Shen", "Yanru Qu", "Weizhu Chen." ],
      "venue" : "arXiv preprint arXiv:2009.13818.",
      "citeRegEx" : "Shen et al\\.,? 2020",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving neural language generation with spectrum control",
      "author" : [ "Lingxiao Wang", "Jing Huang", "Kevin Huang", "Ziniu Hu", "Guangtao Wang", "Quanquan Gu." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Cross-thought for sentence encoder pre-training",
      "author" : [ "Shuohang Wang", "Yuwei Fang", "Siqi Sun", "Zhe Gan", "Yu Cheng", "Jingjing Liu", "Jing Jiang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Williams et al\\.,? 2018",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "Clear: Contrastive learning for sentence representation",
      "author" : [ "Zhuofeng Wu", "Sinong Wang", "Jiatao Gu", "Madian Khabsa", "Fei Sun", "Hao Ma." ],
      "venue" : "arXiv preprint arXiv:2012.15466.",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Universal sentence representation learning with conditional masked language model",
      "author" : [ "Ziyi Yang", "Yinfei Yang", "Daniel Cer", "Jax Law", "Eric Darve." ],
      "venue" : "arXiv preprint arXiv:2012.14388.",
      "citeRegEx" : "Yang et al\\.,? 2020",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    }, {
      "title" : "An unsupervised sentence embedding method by mutual information maximization",
      "author" : [ "Yan Zhang", "Ruidan He", "Zuozhu Liu", "Kwan Hui Lim", "Lidong Bing." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "Sentence representation learning plays a vital role in natural language processing tasks (Kiros et al., 2015; Hill et al., 2016; Conneau et al., 2017; Cer et al., 2018).",
      "startOffset" : 89,
      "endOffset" : 168
    }, {
      "referenceID" : 20,
      "context" : "Sentence representation learning plays a vital role in natural language processing tasks (Kiros et al., 2015; Hill et al., 2016; Conneau et al., 2017; Cer et al., 2018).",
      "startOffset" : 89,
      "endOffset" : 168
    }, {
      "referenceID" : 13,
      "context" : "Sentence representation learning plays a vital role in natural language processing tasks (Kiros et al., 2015; Hill et al., 2016; Conneau et al., 2017; Cer et al., 2018).",
      "startOffset" : 89,
      "endOffset" : 168
    }, {
      "referenceID" : 8,
      "context" : "Sentence representation learning plays a vital role in natural language processing tasks (Kiros et al., 2015; Hill et al., 2016; Conneau et al., 2017; Cer et al., 2018).",
      "startOffset" : 89,
      "endOffset" : 168
    }, {
      "referenceID" : 28,
      "context" : "However, the native sentence representations derived from BERT1 are proved to be of low-quality (Reimers and Gurevych, 2019; Li et al., 2020).",
      "startOffset" : 96,
      "endOffset" : 141
    }, {
      "referenceID" : 26,
      "context" : "However, the native sentence representations derived from BERT1 are proved to be of low-quality (Reimers and Gurevych, 2019; Li et al., 2020).",
      "startOffset" : 96,
      "endOffset" : 141
    }, {
      "referenceID" : 11,
      "context" : "In other words, the BERT-derived native sentence representations are somehow collapsed (Chen and He, 2020), which means almost all sentences are mapped into a small area and therefore produce high similarity.",
      "startOffset" : 87,
      "endOffset" : 106
    }, {
      "referenceID" : 16,
      "context" : "Such phenomenon is also observed in several previous works (Gao et al., 2019; Wang et al., 2019; Li et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 113
    }, {
      "referenceID" : 31,
      "context" : "Such phenomenon is also observed in several previous works (Gao et al., 2019; Wang et al., 2019; Li et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 113
    }, {
      "referenceID" : 26,
      "context" : "Such phenomenon is also observed in several previous works (Gao et al., 2019; Wang et al., 2019; Li et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 113
    }, {
      "referenceID" : 18,
      "context" : "Moreover, we propose multiple data augmentation strategies for contrastive learning, including adversarial attack (Goodfellow et al., 2014; Kurakin et al., 2016), token shuffling, cutoff (Shen et al.",
      "startOffset" : 114,
      "endOffset" : 161
    }, {
      "referenceID" : 24,
      "context" : "Moreover, we propose multiple data augmentation strategies for contrastive learning, including adversarial attack (Goodfellow et al., 2014; Kurakin et al., 2016), token shuffling, cutoff (Shen et al.",
      "startOffset" : 114,
      "endOffset" : 161
    }, {
      "referenceID" : 30,
      "context" : ", 2016), token shuffling, cutoff (Shen et al., 2020) and dropout (Hinton et al.",
      "startOffset" : 33,
      "endOffset" : 52
    }, {
      "referenceID" : 21,
      "context" : ", 2020) and dropout (Hinton et al., 2012), that effectively transfer the sentence representations to downstream tasks.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 5,
      "context" : "They use a BiLSTM-based encoder and train it on two NLI datasets, Stanford NLI (SNLI) (Bowman et al., 2015) and Multi-Genre NLI (MNLI) (Williams et al.",
      "startOffset" : 86,
      "endOffset" : 107
    }, {
      "referenceID" : 33,
      "context" : ", 2015) and Multi-Genre NLI (MNLI) (Williams et al., 2018).",
      "startOffset" : 35,
      "endOffset" : 58
    }, {
      "referenceID" : 8,
      "context" : "Universal Sentence Encoder (Cer et al., 2018) adopts a Transformer-based architecture and uses the SNLI dataset to augment the unsupervised training.",
      "startOffset" : 27,
      "endOffset" : 45
    }, {
      "referenceID" : 28,
      "context" : "SBERT (Reimers and Gurevych, 2019) proposes a siamese architecture with a shared BERT encoder and is also trained on SNLI and MNLI datasets.",
      "startOffset" : 6,
      "endOffset" : 34
    }, {
      "referenceID" : 14,
      "context" : "Self-supervised Objectives for Pre-training BERT (Devlin et al., 2019) proposes a bidirectional Transformer encoder for language model pre-training.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 27,
      "context" : "However, NSP is proved to be weak and has little contribution to the final performance (Liu et al., 2019).",
      "startOffset" : 87,
      "endOffset" : 105
    }, {
      "referenceID" : 32,
      "context" : "CrossThought (Wang et al., 2020) and CMLM (Yang et al.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 35,
      "context" : ", 2020) and CMLM (Yang et al., 2020) are two similar objectives that recover masked tokens in one sentence conditioned on the representations of its contextual sentences.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 25,
      "context" : "SLM (Lee et al., 2020) proposes an objective that reconstructs the correct sentence ordering given the shuffled sentences as the input.",
      "startOffset" : 4,
      "endOffset" : 22
    }, {
      "referenceID" : 26,
      "context" : "Unsupervised Approaches BERT-flow (Li et al., 2020) proposes a flow-based approach that maps BERT embeddings to a standard Gaussian latent space, where embeddings are more suitable for comparison.",
      "startOffset" : 34,
      "endOffset" : 51
    }, {
      "referenceID" : 9,
      "context" : "Contrastive Learning for Visual Representation Learning Recently, contrastive learning has become a very popular technique in unsupervised visual representation learning with solid performance (Chen et al., 2020a; He et al., 2020; Chen et al., 2020b).",
      "startOffset" : 193,
      "endOffset" : 250
    }, {
      "referenceID" : 19,
      "context" : "Contrastive Learning for Visual Representation Learning Recently, contrastive learning has become a very popular technique in unsupervised visual representation learning with solid performance (Chen et al., 2020a; He et al., 2020; Chen et al., 2020b).",
      "startOffset" : 193,
      "endOffset" : 250
    }, {
      "referenceID" : 22,
      "context" : "They use the normalized temperature-scaled cross-entropy loss (NT-Xent) as the training loss, which is also called InfoNCE in the previous literature (Hjelm et al., 2018).",
      "startOffset" : 150,
      "endOffset" : 170
    }, {
      "referenceID" : 36,
      "context" : "IS-BERT (Zhang et al., 2020) proposes to add 1-D convolutional neural network (CNN) layers on top of BERT and train the CNNs by maximizing the mutual information (MI) between the global sentence embedding and its corresponding local contexts embeddings.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 15,
      "context" : "CERT (Fang and Xie, 2020) adopts a similar structure as MoCo (He et al.",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 19,
      "context" : "CERT (Fang and Xie, 2020) adopts a similar structure as MoCo (He et al., 2020) and uses back-translation for data augmentation.",
      "startOffset" : 61,
      "endOffset" : 78
    }, {
      "referenceID" : 6,
      "context" : "BERT-CT (Carlsson et al., 2021) uses two individual encoders for contrastive learning, which also needs extra memory.",
      "startOffset" : 8,
      "endOffset" : 31
    }, {
      "referenceID" : 17,
      "context" : "DeCLUTR (Giorgi et al., 2020) adopts the architecture of SimCLR and jointly trains the model with contrastive objective and masked language model objective.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 34,
      "context" : "CLEAR (Wu et al., 2020) uses the same architecture and objectives as DeCLUTR.",
      "startOffset" : 6,
      "endOffset" : 23
    }, {
      "referenceID" : 9,
      "context" : "Our approach is mainly inspired by SimCLR (Chen et al., 2020a).",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 18,
      "context" : "We explore four different data augmentation strategies to generate views for contrastive learning, including adversarial attack (Goodfellow et al., 2014; Kurakin et al., 2016), token shuffling, cutoff (Shen et al.",
      "startOffset" : 128,
      "endOffset" : 175
    }, {
      "referenceID" : 24,
      "context" : "We explore four different data augmentation strategies to generate views for contrastive learning, including adversarial attack (Goodfellow et al., 2014; Kurakin et al., 2016), token shuffling, cutoff (Shen et al.",
      "startOffset" : 128,
      "endOffset" : 175
    }, {
      "referenceID" : 30,
      "context" : ", 2016), token shuffling, cutoff (Shen et al., 2020) and dropout (Hinton et al.",
      "startOffset" : 33,
      "endOffset" : 52
    }, {
      "referenceID" : 21,
      "context" : ", 2020) and dropout (Hinton et al., 2012), as illustrated in Figure 3.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 29,
      "context" : "We implement this strategy with Fast Gradient Value (FGV) (Rozsa et al., 2016), which directly uses the gradient to compute the perturbation and thus is faster than two-step alternative methods.",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 28,
      "context" : "Dataset Following previous works(Reimers and Gurevych, 2019; Li et al., 2020; Zhang et al., 2020), we evaluate our approach on multiple STS datasets, including STS tasks 2012 - 2016 (STS12 - STS16) (Agirre et al.",
      "startOffset" : 32,
      "endOffset" : 97
    }, {
      "referenceID" : 26,
      "context" : "Dataset Following previous works(Reimers and Gurevych, 2019; Li et al., 2020; Zhang et al., 2020), we evaluate our approach on multiple STS datasets, including STS tasks 2012 - 2016 (STS12 - STS16) (Agirre et al.",
      "startOffset" : 32,
      "endOffset" : 97
    }, {
      "referenceID" : 36,
      "context" : "Dataset Following previous works(Reimers and Gurevych, 2019; Li et al., 2020; Zhang et al., 2020), we evaluate our approach on multiple STS datasets, including STS tasks 2012 - 2016 (STS12 - STS16) (Agirre et al.",
      "startOffset" : 32,
      "endOffset" : 97
    }, {
      "referenceID" : 7,
      "context" : ", 2012, 2013, 2014, 2015, 2016), STS benchmark (STSb) (Cer et al., 2017) and SICKRelatedness (SICK-R) (Marelli et al.",
      "startOffset" : 54,
      "endOffset" : 72
    }, {
      "referenceID" : 12,
      "context" : "We obtain all 7 datasets through the SentEval toolkit (Conneau and Kiela, 2018).",
      "startOffset" : 54,
      "endOffset" : 79
    }, {
      "referenceID" : 5,
      "context" : "For supervised experiments, we use the combination of SNLI (570k samples) (Bowman et al., 2015) and MNLI (430k samples) (Williams et al.",
      "startOffset" : 74,
      "endOffset" : 95
    }, {
      "referenceID" : 33,
      "context" : ", 2015) and MNLI (430k samples) (Williams et al., 2018) to train our model.",
      "startOffset" : 32,
      "endOffset" : 55
    }, {
      "referenceID" : 26,
      "context" : "Baselines To show our effectiveness on unsupervised sentence representation transfer, we mainly select BERT-flow (Li et al., 2020) for comparison, since it shares the same setting as our approach.",
      "startOffset" : 113,
      "endOffset" : 130
    }, {
      "referenceID" : 34,
      "context" : "For unsupervised comparison, we use the average of GloVe embeddings, the BERT-derived native embeddings, CLEAR (Wu et al., 2020) (trained on BookCorpus and English Wikipedia corpus), ISBERT (Zhang et al.",
      "startOffset" : 111,
      "endOffset" : 128
    }, {
      "referenceID" : 36,
      "context" : ", 2020) (trained on BookCorpus and English Wikipedia corpus), ISBERT (Zhang et al., 2020) (trained on unlabeled texts from NLI datasets), BERT-CT (Carlsson et al.",
      "startOffset" : 69,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : ", 2020) (trained on unlabeled texts from NLI datasets), BERT-CT (Carlsson et al., 2021) (trained on English Wikipedia corpus).",
      "startOffset" : 64,
      "endOffset" : 87
    }, {
      "referenceID" : 13,
      "context" : "For comparison with supervised methods, we select InferSent (Conneau et al., 2017), Universal Sentence Encoder (Cer et al.",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 8,
      "context" : ", 2017), Universal Sentence Encoder (Cer et al., 2018), SBERT (Reimers and Gurevych, 2019) and BERT-CT (Carlsson et al.",
      "startOffset" : 36,
      "endOffset" : 54
    }, {
      "referenceID" : 28,
      "context" : ", 2018), SBERT (Reimers and Gurevych, 2019) and BERT-CT (Carlsson et al.",
      "startOffset" : 15,
      "endOffset" : 43
    }, {
      "referenceID" : 6,
      "context" : ", 2018), SBERT (Reimers and Gurevych, 2019) and BERT-CT (Carlsson et al., 2021) as baselines.",
      "startOffset" : 56,
      "endOffset" : 79
    }, {
      "referenceID" : 28,
      "context" : "Implementation Details Our implementation is based on the Sentence-BERT6 (Reimers and Gurevych, 2019).",
      "startOffset" : 73,
      "endOffset" : 101
    }, {
      "referenceID" : 9,
      "context" : "In some previous works of contrastive learning, it is reported that a large batch size benefits the final performance and accelerates the convergence of the model since it provides more in-batch negative samples for contrastive learning (Chen et al., 2020a).",
      "startOffset" : 237,
      "endOffset" : 257
    } ],
    "year" : 2021,
    "abstractText" : "Learning high-quality sentence representations benefits a wide range of natural language processing tasks. Though BERT-based pretrained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks. In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised SEntence Representation Transfer, that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way. By making use of unlabeled texts, ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks. Experiments on STS datasets demonstrate that ConSERT achieves an 8% relative improvement over the previous state-of-the-art, even comparable to the supervised SBERT-NLI. And when further incorporating NLI supervision, we achieve new stateof-the-art performance on STS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios.",
    "creator" : "LaTeX with hyperref"
  }
}