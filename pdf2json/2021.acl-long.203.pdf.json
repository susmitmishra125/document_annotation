{
  "name" : "2021.acl-long.203.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Missing Modality Imagination Network for Emotion Recognition with Uncertain Missing Modalities",
    "authors" : [ "Jinming Zhao", "Ruichen Li", "Qin Jin" ],
    "emails" : [ "zhaojinming@ruc.edu.cn", "ruichen@ruc.edu.cn", "qjin@ruc.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2608–2618\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2608"
    }, {
      "heading" : "1 Introduction",
      "text" : "Automatic multimodal emotion recognition is very important to natural human-computer interactions (Fragopanagos and Taylor, 2002). It aims to understand and interpret human emotions expressed through multiple modalities such as speech content, voice tones and facial expression. Previous works have shown that these different modalities are complimentary for emotion expression, and proposed many effective multimodal fusion methods to improve the emotion recognition performance (Baltrušaitis et al., 2018; Tsai et al., 2019; Zhao et al., 2018). However, in real applications, many common causes can lead to the missing modality problem. For example, the camera is turned off or\n∗Equal Contribution †Corresponding Author\nblocked due to privacy issues; the speech content is unavailable due to automatic speech recognition errors; the voice and text are missing due to the silence of the user; or the faces cannot be detected due to lighting or occlusion issues as shown in Figure 1. Existing multimodal fusion models trained on full-modality samples usually fail when partial modalities are missing (Aguilar et al., 2019; Pham et al., 2019; Cai et al., 2018; Parthasarathy and Sundaram, 2020).\nThe missing modality problem has attracted more research attention in the past years, and the existing solutions for this problem are mainly based on learning joint multimodal representation so that all modality information can be encoded. Han et al. (Han et al., 2019) propose a joint training approach that implicitly fuses multimodal information from auxiliary modalities, which improves the monomodal emotion recognition performance. The recent cross-modality sequential translation-based methods proposed in (Pham et al., 2019; Wang et al., 2020) learn the joint multimodal representations via translating a source modality to multiple target modalities, which improves the performance\nof the source modality as input at the test time. However, these methods can only deal with the scenario where the source modality is input to the trained model. Different models need to be built for different missing modality cases1. Additionally, the sequential translation-based models require translation and generation of videos, audios, and text, which are difficult to train especially with limited training samples (Li et al., 2018; Pham et al., 2019).\nIn this work, we propose a novel unified model, Missing Modality Imagination Network (MMIN), to address the above issues. Specifically, the proposed MMIN learns the robust joint multimodal representations through cross-modality imagination with Cascade Residual Autoencoder (CRA) (Tran et al., 2017) and Cycle Consistency Learning (Zhu et al., 2017) based on sentence-level modalityspecific representations, as the sentence-level representation is more reasonable for modeling the cross-modality emotion correlation. The imagination module aims to predict the sentence-level emotional representation of the missing modality from the other available modalities. To the best of our knowledge, this is the first work that investigates a unified model for multimodal emotion recognition with uncertain missing-modality.\nExtensive experiments are carried out on two benchmark datasets, IEMOCAP and MSPIMPROV, under both uncertain missing-modality and full-modality conditions. The proposed MMIN model as a unified multimodal emotion recognition model can learn robust joint multimodal representations and outperforms the standard multimodal fusion models on both benchmark datasets under both the uncertain missing-modality and the fullmodality conditions. Furthermore, to evaluate the imagination ability of our MMIN model, we visualize the distributions of the imagined representations of the missing modalities and its ground-truth representations and find they are very similar, which demonstrates that MMIN can imagine the representations of the missing modalities based on the representations of the available modalities.\nIn summary, the main contributions of this work are: 1) We propose a unified model, Missing Modality Imagination Network (MMIN), to improve the robustness of emotion recognition systems under uncertain missing-modality testing con-\n1If there are audio(a),visual(v) and textual(t) three modalities, then the system needs 6 models trained under 6 missing modality conditions {a}, {v}, {t}, {a,v}, {a,t} and {v,t}, plus one model trained under the full-modality data.\nditions. 2) We design cross-modality imagination based on paired multimodal data and adopt Cascade Residual Autoencoder (CRA) and Cycle Consistency Learning to learn the robust joint multimodal representations. 3) Extensive experiments on two benchmark datasets demonstrate the effectiveness of the proposed model which improves the emotion recognition performance under both the uncertain missing-modality and the full-modality conditions."
    }, {
      "heading" : "2 Related Work",
      "text" : "Multimodal Emotion Recognition Many previous works have focused on fusing multimodal information to improve emotion recognition performance. Temporal attention-based methods are proposed to use the attention mechanism to selectively fuse different modalities based on the frame-level or word-level temporal sequence, such as Gated Multimodal Unit (GMU) (Aguilar et al., 2019), Multimodal Alignment Model (MMAN) (Xu et al., 2019) and Multi-modal Attention mechanism (cLSTM-MMA) (Pan et al., 2020). These methods use different uni-modal sub-networks to model the contextual representations for each modality and then use the multimodal attention mechanism to selectively fuse the representations of different modalities. Liang et al. (Liang et al., 2020) propose a semi-supervised multimodal (SSMM) emotion recognition model which uses cross-modality emotional distribution matching to leverages unlabeled data to learn the robust representations and achieves state-of-the-art performance. Missing Modality Problem Existing methods for missing modality problem can mainly be divided into three groups. The first group features the data augmentation approach, which randomly ablates the inputs to mimic missing modality cases (Parthasarathy and Sundaram, 2020). The second group is based on generative methods to directly predict the missing modalities given the available modalities (Li et al., 2018; Cai et al., 2018; Suo et al., 2019; Du et al., 2018). The third group aims to learn the joint multimodal representations that can contain related information from these modalities (Aguilar et al., 2019; Pham et al., 2019; Han et al., 2019; Wang et al., 2020).\nData augmentation methods: Parthasarathy et al. (Parthasarathy and Sundaram, 2020) propose a strategy to randomly ablate visual inputs during\ntraining at the clip or frame level to mimic realworld missing modality scenarios for audio-visual multimodal emotion recognition, which improves the recognition performance under missing modality conditions.\nGenerative methods: Tran et al. (Tran et al., 2017) propose Cascaded Residual Autoencoder (CRA) to utilize the residual mechanism over the autoencoder structure, which can take the corrupted data and estimate a function to well restore the incomplete data. Cai et al. (Cai et al., 2018) propose an encoder-decoder deep neural network to generate the missing modality (Positron Emission Tomography, PET) given the available modality (Magnetic Resonance Imaging, MRI), and the generated PET can provide complementary information to improve the detection and tracking of Alzheimers disease.\nLearning joint multimodal representations: Han et al. (Han et al., 2019) propose a joint training model that consists of two modality-specific encoders and one shared classifier, which implicitly fuse the audio and visual information as joint representations and improve the performance of the mono-modality emotion recognition. Pham et al. (Pham et al., 2019) propose a sequential translation-based model to learn the joint representation between the source modality and multiple target modalities. The hidden vectors of the source modality encoder work as the joint representations, which improve the emotion recognition performance of the source modality. Wang et al. (Wang et al., 2020) follow this translation-based method and propose a more efficient transformerbased translation model with parallel translation including textual features to acoustic features and textual features to visual features. Moreover, the above two translation-based models adopt the forward translation and backward translation training strategy to ensure that joint representations can retain maximal information from all modalities."
    }, {
      "heading" : "3 Method",
      "text" : "Given a set of video segments S, we use x = (xa, xv, xt) to represent the raw multimodal features for a video segment s ∈ S, where xa, xv and xt represent the raw features of acoustic, visual and textual modalities respectively. |S| represents the number of video segments in set S. We denote the target set Y = {yi}|S|i=1, yi ∈ {0, 1, . . . , C}, where yi is the target emotion category of the video\nsegment si and |C| is the number of emotion categories. Our proposed method aims to recognize the emotion category yi for every video segment si with full modalities, or with only partial modalities available, for the example shown in Figure 1, there exist only acoustic and textual modalities when visual modality is missing."
    }, {
      "heading" : "3.1 Missing Modality Imagination Network",
      "text" : "In order to learn robust joint multimodal representations, we propose a unified model, Missing Modality Imagination Network (MMIN), which can deal with different uncertain missing-modality conditions in real application scenarios. Figure 2 illustrates the framework of our proposed MMIN model which contains three main modules: 1) Modality Encoder Network for extracting modality-specific embeddings; 2) Imagination Module based on the Cascade Residual Autoencoder (CRA) and Cycle Consistency Learning for imagining the representations of missing modalities given the representations of the corresponding available modalities. The latent vectors of the autoencoders in CRA are collected to form the joint multimodal representations; 3) Emotion classifier for predicting the emotion category based on the joint multimodal representations. We introduce each module in details in the following subsections."
    }, {
      "heading" : "3.1.1 Modality Encoder Network",
      "text" : "The Modality Encoder Network is used to extract the modality-specific utterance-level embeddings based on the raw modality features x. As shown in Figure 2(b), we first pretrain the Modality Encoder Network in a multimodal emotion recognition model and it is further trained within MMIN model. We define the modality-specific embeddings of each modality as ha = EncA(xa), hv = EncV(xv), ht = EncT(xt), where EncA, EncV and EncT represent the acoustic, visual and textual encoders respectively, and ha, hv and ht represent the modality-specific embeddings generated by the corresponding encoders respectively."
    }, {
      "heading" : "3.1.2 Missing Modality Condition Creation",
      "text" : "Given a training sample with all three modalities (xa, xv, xt), there are 6 different possible missingmodality conditions as shown in Table 1. We can build a cross-modality pair (available,missing) under each missing-modality condition, where the available and missing mean the available modalities and the corresponding missing modalities respectively. In order to ensure a unified model that can handle various missing-modality conditions, we enforce a unified triplet input format for the modality encoder network as (xa, xv, xt). Under the missing-modality conditions, the raw features of the corresponding missing modalities are replaced by zero vectors. For example, the unified format input of the available modalities under the visual modality missing condition (case 1 in Table 1) is formatted as (xa, xvmiss, x\nt), where xvmiss refers to zero vectors.\nUnder the missing-modality training conditions, the input includes the cross-modality pairs referring to available modalities and missing modalities in the unified triplet format (as shown in Ta-\nble 1). The multimodal embeddings of these crossmodality pairs can be represented as (taking the visual modality missing condition as example):\nh = concat(ha, hvmiss, h t) ĥ = concat(hamiss, h v, htmiss)\n(1)\nwhere hamiss, h v miss and h t miss represent the modality-specific embedding when the corresponding modality is missing, which is produced by the corresponding modality encoder with input zero vectors."
    }, {
      "heading" : "3.1.3 Imagination Module",
      "text" : "We propose an autoencoder-based Imagination Module to predict the multimodal embeddings of the missing modalities given the multimodal embeddings of the available modalities. The Imagination Module is expected to learn the robust joint multimodal representations through the cross-modality imagination. As illustrated in Figure 2(a), we employ the Cascade Residual Autoencoder (CRA) (Tran et al., 2017) structure, which has sufficient learning capacity and more stable convergence than the standard autoencoder. The\nCRA structure is constructed by connecting a series of Residual Autoencoders (RAs). We further employ cycle consistency learning (Zhu et al., 2017; Wang et al., 2020) with a coupled net architecture with two independent networks to perform imagination in two directions, including the Forward (available → missing) and Backward (missing → available) imagination directions.\nTo be specific, we use a CRA model withB RAs and each RA is represented by φk, k = 1, 2, . . . , B, and the calculation of each RA can be defined as:\n{ ∆zk = φk(h), k = 1\n∆zk = φk(h+ ∑k−1 j=1 ∆zj), k > 1 (2)\nwhere h is the extracted multimodal embedding based on the available modalities in a unified crossmodality pair format (Eq.(1)) and ∆zk represents the output of the kth RA. Taking the visual modality missing condition as example (as shown in Figure 2(a)), the forward imagination aims to predict the multimodal embedding of the missing visual modality based on the available acoustic and textual modalities. The forward imagined multimodal embedding is expressed as:\nh ′ = imagineforward(h) = h+ B∑ k=1 ∆zk (3)\nwhere imagine(·) represents the function of the Imagination Module. The backward imagination aims to predict the multimodal embedding of the available modalities based on the forward imagined multimodal embedding h ′ (Eq.(3)). The backward imagined multimodal embedding is expressed as:\nh ′′ = imaginebackward(h ′ ) (4)"
    }, {
      "heading" : "3.1.4 Classifier",
      "text" : "We collect the latent vectors of each auto-encoder in the forward imagination module and concatenate them together to form the joint multimodal representation: R = concat(c1, c2, . . . , cB), where ck is the latent vector of the autoencoder in the kth RA. Based on the joint multimodal representation R, we calculate the probability distribution q as:\nq = softmax(fcls(R)) (5)\nwhere fcls(·) denotes the emotion classifier that consists of several fully-connected layers."
    }, {
      "heading" : "3.2 Joint Optimization",
      "text" : "The loss function for MMIN training includes three parts: the emotion recognition loss Lcls, forward imagination loss Lforward, and backward imagination loss Lbackward:\nLcls = − 1\n|S| |S|∑ i=1 H(p, q)\nLforward = 1\n|S| |S|∑ i=1 ∥∥∥ĥi − h′i∥∥∥2 2\nLbackward = 1\n|S| |S|∑ i=1 ∥∥∥hi − h′′i ∥∥∥2 2\n(6)\nwhere p is the true distribution of one-hot label and q is the prediction distribution calculated in Eq.(5). H(p, q) is the cross-entropy between distributions p and q. hi and ĥi are the ground-truth representations extracted by the modality encoder network as shown in Eq.(1). We combine all the three losses into the joint objective function as below to jointly optimize the model parameters:\nL = Lcls + λ1Lforward + λ2Lbackward (7)\nwhere λ1 and λ2 are weighting hyper parameters for Lforward and Lbackward respectively."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset",
      "text" : "We evaluate our proposed model on two benchmark multimodal emotion recognition datasets, Interactive Emotional Dyadic Motion Capture (IEMOCAP) (Busso et al., 2008) and MSP-IMPROV (Busso et al., 2016). The statistics of the two datasets are shown in Table 2.\nIEMOCAP contains recorded videos in 5 dyadic conversation sessions. In each session, there are multiple scripted plays and spontaneous dialogues between a male and a female speaker and 10 speakers in total in the database. We follow the emotional label processing in (Xu et al., 2019; Liang et al., 2020) to form the four-class emotion recognition setup.\nMSP-IMPROV contains recorded segments videos in dyadic conversation scenarios with 12 actors. We first remove videos that are shorter than 1 second. Then we select the videos in the “Other-improvised” group which are recorded during the improvisation scenarios with happy, anger, sadness, or neutral labels to form the four-class emotion recognition setup."
    }, {
      "heading" : "4.1.1 Missing-Modality Training Set",
      "text" : "We first define the original training set which contains all the three modalities as the full-modality training set. Based on the full-modality training set, we construct another training set that contains cross-modality pairs to simulate the possible missing-modality conditions and we define it as the missing-modality training set, which we use to train the proposed MMIN. Six different crossmodality pairs (Table 1) for each training sample are generated. Therefore, the number of the generated cross-modality pairs is six times as large as the number of the full-modality training samples."
    }, {
      "heading" : "4.1.2 Missing-Modality Testing Set",
      "text" : "We first define the original testing set which contains all the three modalities as the full-modality testing set. To evaluate the performance of the proposed MMIN under the uncertain missingmodality conditions, we construct six different missing modality testing subsets corresponding to the six possible missing modality conditions respectively. For example, in the inference stage, under the missing visual modality condition as shown in Figure 2(c), the raw feature of a missingmodality testing sample in the unified format is (xa, xvmiss, x\nt). We combine all the six missingmodality testing subsets together and denote it as the missing-modality testing set."
    }, {
      "heading" : "4.2 Raw Feature Extraction",
      "text" : "We follow feature extraction methods described in (Liang et al., 2020; Pan et al., 2020) and extract the frame-level raw features of each modality 2.\nAcoustic features: OpenSMILE toolkit (Eyben et al., 2010) with the configuration of “IS13 ComParE” is used to extract frame-level features, which have similar performance with the IS10 utterance-level acoustic features used in (Liang et al., 2020). We denote the features as “ComParE” and the feature vectors are in 130 dimensions.\nVisual features: We extract the facial expression features using a pretrained DenseNet (Huang\n2To facilitate fair comparison with the sequential translation-based missing modality method MCTN, we adopt frame-level features which can be directly used in the MCTN method\net al., 2017) which is trained based on the Facial Expression Recognition Plus (FER+) corpus (Barsoum et al., 2016). We denote the facial expression features as “Denseface”. The “Denseface” are frame-level sequential features based on the detected faces from the video frames, and the feature vectors are in 342 dimensions.\nTextual features: We extract contextual word embeddings using a pretrained BERT-large model (Devlin et al., 2019) which is one of the state-ofthe-art language representations. We denote the word embeddings as “Bert” and the features are in 1024 dimensions."
    }, {
      "heading" : "4.3 Higher-level Feature Encoder",
      "text" : "To generate more efficient sentence-level modalityspecific representations for the Imagination Module, we design different modality encoders for different modalities. Acoustic Modality Encoder (EncA): We apply a Long Short-term Memory (LSTM) network (Sak et al., 2014) to capture the temporal information based on the sequential frame-level raw acoustic features xa. Then we use max-pooling to get utterance-level acoustic embedding ha based on the LSTM hidden states. Visual Modality Encoder (EncV): We adopt a similar method with EncA on the sequential frame-level facial expression features xv and get utterance-level visual embedding hv. Textual Modality Encoder (EncT): We apply a TextCNN (Kim, 2014) to get the utterance-level textual embedding as ht based on the sequential word-level features xt."
    }, {
      "heading" : "4.4 Recognition Baselines",
      "text" : "Our baseline model takes the structure as shown in Figure 2(b), which is trained based on the fullmodality training set and we use it as our fullmodality baseline. To improve the system robustness against the missing modality problem, one intuitive solution is to add samples under the missingmodality conditions into the training set. We, therefore, pool the missing-modality training set and full-modality training set together to train the baseline model and use it as our augmented baseline."
    }, {
      "heading" : "4.5 Implementation Details",
      "text" : "Table 3 presents our implementation details. We use the 10-fold and 12-fold speaker-independent cross-validation to evaluate the models on IEMOCAP and MSP-IMPROV respectively. For the experiments on IEMOCAP, we take four sessions for\ntraining, and the remaining session is split by speakers into the validation and testing sets. For MSPIMPROV, we take the utterances of 10 speakers for training, the remaining 2 speakers are divided into validation set and testing set by speakers. We train the model with at most 100 epochs for each experiment. We select the best model on the validation set and report its performance on the testing set. To demonstrate the robustness of our models, we run each model three times to alleviate the influences of random initialization of parameters and apply a significance test for model comparison. All models are implemented with Pytorch deep learning toolkit and run on a single Nvidia GTX 1080Ti graphic card.\nFor the experiments on IEMOCAP, we use two evaluation metrics: weighted accuracy (WA) and unweighted accuracy (UA). Due to the imbalance of emotion categories on MSP-IMPROV, we use the f-score as the evaluation metric."
    }, {
      "heading" : "4.6 Full-modality Baseline Results",
      "text" : "We first compare our full-modality baseline with several state-of-the-art multimodal recognition models under the full-modality condition. Results in Table 4 show that our full-modality baseline outperforms other state-of-the-art models, which proves that our modality encoder network can extract effective representations for multimodal emotion recognition."
    }, {
      "heading" : "4.7 Uncertain Missing-Modality Results",
      "text" : "Table 5 presents the experimental results of our proposed MMIN model under different missingmodality testing conditions and full-modality testing condition. On IEMOCAP, comparing to the\n“full-modality baseline” results in Table 4, we see a significant performance drop under uncertain missing-modality testing conditions, which indicates that the model trained under the fullmodality condition is very sensitive to the missing modality problem. The intuitive solution “Augmented baseline”, which combines the missingmodality training set with the full-modality training set to train the baseline model, does significantly improves over the full-modality baseline under missing-modality testing conditions, which indicates that data augmentation can help alleviate the problem of data mismatch between training and testing. More notably, our proposed MMIN significantly outperforms both the full-modality baseline and the augmented baseline under every possible missing-modality testing condition. It also outperforms the two baselines under the full-modality testing condition, even though the MMIN model does not use the full-modality training data. These results indicate that our proposed MMIN model can learn robust joint multimodal representation so that it can achieve consistently better performance under both the different missing-modality and the full-modality testing conditions. This is because our proposed MMIN method not only has the data augmentation capability, but also can learn better joint representation, which can preserve information of other modalities.\nWe further analyze the performance under different missing modality conditions. Our MMIN model achieves significant improvement under one modality available conditions ({a}, {v}, or {t}) compared with the augmented baseline, especially for the weak modalities {a} and {v}. It brings some improvements as well over the augmented baseline even for the strong modality combinations, such as {a, t}. These experimental results indicate that the learned joint representation via MMIN did learn complementary info from the other modalities to compensate for the weak modalities.\nThe bottom block in Table 5 shows the performance comparison on the MSP-IMPROV dataset. Our proposed MMIN model again significantly outperforms the two baselines under different missing-modality and full-modality testing conditions, which demonstrates the good generalization ability of MMIN across different datasets.\nWe also compare to the MCTN (Pham et al., 2019) model which is the state-of-the-art model for the missing modality problem. As MCTN can-\nnot handle different missing-modality conditions in one unified model, so we have to train a particular model under each missing-modality condition3. The comparison results demonstrate that our proposed MMIN model not only can handle both the different missing-modality and the full-modality testing condition with a unified model, but also can consistently outperform the MCTN models under all missing-modality conditions."
    }, {
      "heading" : "4.8 Ablation Study",
      "text" : "We conduct experiments to ablate the contributions of different components in MMIN, including the structure of the imagination module and the cyclic consistency learning.\nStructure of the imagination module. We first investigate the impact of different network structures on the performance in the imagination module. Specifically, we compare the Autoencoder and the CRA structure in MMIN, and we adopt the same parameter scale to ensure the fairness of the comparison. As shown in Table 6, the performance of the imagination module with Autoencoder structure “MMIN-AE” is worse than that with the CRA structure under both different missing-modality and full-modality testing conditions. The performance comparison indicates that the CRA has a stronger imagination ability than the Autoencoder model.\nCycle Consistency Learning. To evaluate the impact of the cyclic consistency learning in MMIN,\n3We use features described in Sec. 4.3 and follow the training setting in (Pham et al., 2019) to conduct the MCTN experiments. The MCTN model cannot be evaluated under the full-modality testing condition because the target modalities cannot be None.\nwe conduct experiments using MMIN with or without cycle consistency learning. As shown in Table 6, the model trained without cycle consistency learning results in performance loss under all conditions, which indicates that the cycle consistency learning can enhance the imagination ability and learn more robust joint multimodal representations."
    }, {
      "heading" : "4.9 Analysis of MMIN Core Competence",
      "text" : "We conduct detailed experiments on IEMOCAP to demonstrate the joint representation learning ability and the imagination ability of our MMIN model. Joint representation learning ability: Since the joint representation is expected to retain information of multiple modalities, we conduct experiments to evaluate the joint representation learning ability of MMIN. We compare MMIN to the baseline model under the matched-modality condition in which the training data and the test data contain the same modalities. As shown in Table 7, comparing to the baseline model, MMIN achieves on par with or even better performance, which demonstrates that MMIN has the ability to learn effective joint multimodal representations. We also notice that the data-augmented model cannot beat the corresponding matching partial-modality baseline model, which indicates the data-augmented model cannot learn the joint representation. Imagination ability: Figure 3 visualizes the distribution of the ground-truth multimodal embeddings (ĥ in Figure 2) and MMIN imagined multimodal embeddings (h ′ in Figure 2) for a male speaker and female speaker using t-SNE (Maaten and Hinton, 2008). We observe that the distribution of\nthe ground-truth embeddings and imagined embeddings are very similar, although the distribution of visual modality embeddings deviates a little, it is mainly because the quality of the visual modality is poor in this dataset. It demonstrates that MMIN can imagine the representations of the missing modalities based on the available modalities."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we propose a novel unified multimodal emotion recognition model, Missing Modality Imagination Network (MMIN), to improve the emotion recognition performance under uncertain missing-modality conditions in real application scenarios. The proposed MMIN can learn the\nrobust joint multimodal representations through cross-modality imagination via the Cascade Residual Autoencoder and Cycle Consistency Learning. Extensive experiments on two public benchmark datasets demonstrate the effectiveness and robustness of our proposed model, which significantly outperforms other baselines under both uncertain missing-modality and full-modality conditions.\nIn the future work, we will explore ways to further improve the robust joint multimodal representation."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by the National Key R&D Program of China under Grant No. 2020AAA0108600, National Natural Science Foundation of China (No. 62072462), National Natural Science Foundation of China (No. 61772535), Beijing Natural Science Foundation (No. 4192028)."
    } ],
    "references" : [ {
      "title" : "Multimodal and multi-view models for emotion recognition",
      "author" : [ "Gustavo Aguilar", "Viktor Rozgic", "Weiran Wang", "Chao Wang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 991–1002.",
      "citeRegEx" : "Aguilar et al\\.,? 2019",
      "shortCiteRegEx" : "Aguilar et al\\.",
      "year" : 2019
    }, {
      "title" : "Multimodal machine learning: A survey and taxonomy",
      "author" : [ "Tadas Baltrušaitis", "Chaitanya Ahuja", "LouisPhilippe Morency." ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence, 41(2):423–443.",
      "citeRegEx" : "Baltrušaitis et al\\.,? 2018",
      "shortCiteRegEx" : "Baltrušaitis et al\\.",
      "year" : 2018
    }, {
      "title" : "Training deep networks for facial expression recognition with crowdsourced label distribution",
      "author" : [ "Emad Barsoum", "Cha Zhang", "Cristian Canton Ferrer", "Zhengyou Zhang." ],
      "venue" : "ICMI, ICMI ’16, page 279283, New York, NY, USA. Association for Com-",
      "citeRegEx" : "Barsoum et al\\.,? 2016",
      "shortCiteRegEx" : "Barsoum et al\\.",
      "year" : 2016
    }, {
      "title" : "Iemocap: interactive emotional dyadic motion capture database",
      "author" : [ "Carlos Busso", "Murtaza Bulut", "Chi-Chun Lee", "Abe Kazemzadeh", "Emily Mower", "Samuel Kim", "Jeannette N. Chang", "Sungbok Lee", "Shrikanth S. Narayanan." ],
      "venue" : "Language Re-",
      "citeRegEx" : "Busso et al\\.,? 2008",
      "shortCiteRegEx" : "Busso et al\\.",
      "year" : 2008
    }, {
      "title" : "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "author" : [ "Carlos Busso", "Srinivas Parthasarathy", "Alec Burmania", "Mohammed AbdelWahab", "Najmeh Sadoughi", "Emily Mower Provost." ],
      "venue" : "IEEE Transactions on Affective Comput-",
      "citeRegEx" : "Busso et al\\.,? 2016",
      "shortCiteRegEx" : "Busso et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep adversarial learning for multi-modality missing data completion",
      "author" : [ "Lei Cai", "Zhengyang Wang", "Hongyang Gao", "Dinggang Shen", "Shuiwang Ji." ],
      "venue" : "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data",
      "citeRegEx" : "Cai et al\\.,? 2018",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2018
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Semi-supervised deep generative modelling of incomplete multi-modality emotional data",
      "author" : [ "Changde Du", "Changying Du", "Hao Wang", "Jinpeng Li", "Wei-Long Zheng", "Bao-Liang Lu", "Huiguang He." ],
      "venue" : "Proceedings of the 26th ACM international confer-",
      "citeRegEx" : "Du et al\\.,? 2018",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2018
    }, {
      "title" : "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "author" : [ "Florian Eyben", "Martin Wöllmer", "Björn Schuller." ],
      "venue" : "ACM Multimedia, pages 1459–1462.",
      "citeRegEx" : "Eyben et al\\.,? 2010",
      "shortCiteRegEx" : "Eyben et al\\.",
      "year" : 2010
    }, {
      "title" : "Emotion recognition in human-computer interaction",
      "author" : [ "N Fragopanagos", "J.G. Taylor." ],
      "venue" : "IEEE Signal Processing Magazine, 18(1):32–80.",
      "citeRegEx" : "Fragopanagos and Taylor.,? 2002",
      "shortCiteRegEx" : "Fragopanagos and Taylor.",
      "year" : 2002
    }, {
      "title" : "Implicit fusion by joint audiovisual training for emotion recognition in mono modality",
      "author" : [ "Jing Han", "Zixing Zhang", "Zhao Ren", "Björn Schuller." ],
      "venue" : "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "citeRegEx" : "Han et al\\.,? 2019",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2019
    }, {
      "title" : "Densely connected convolutional networks",
      "author" : [ "Gao Huang", "Zhuang Liu", "Laurens Van Der Maaten", "Kilian Q Weinberger." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700–4708.",
      "citeRegEx" : "Huang et al\\.,? 2017",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2017
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "arXiv preprint arXiv:1408.5882.",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Video generation from text",
      "author" : [ "Yitong Li", "Martin Min", "Dinghan Shen", "David Carlson", "Lawrence Carin." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Semisupervised multi-modal emotion recognition with cross-modal distribution matching",
      "author" : [ "Jingjun Liang", "Ruichen Li", "Qin Jin." ],
      "venue" : "Proceedings of the 28th ACM International Conference on Multimedia, pages 2852–2861.",
      "citeRegEx" : "Liang et al\\.,? 2020",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2020
    }, {
      "title" : "Visualizing data using t-sne",
      "author" : [ "Laurens van der Maaten", "Geoffrey Hinton." ],
      "venue" : "Journal of machine learning research, 9(Nov):2579–2605.",
      "citeRegEx" : "Maaten and Hinton.,? 2008",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    }, {
      "title" : "Multi-modal attention for speech emotion recognition",
      "author" : [ "Zexu Pan", "Zhaojie Luo", "Jichen Yang", "Haizhou Li." ],
      "venue" : "Proc. Interspeech 2020, pages 364– 368.",
      "citeRegEx" : "Pan et al\\.,? 2020",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2020
    }, {
      "title" : "Training strategies to handle missing modalities for audio-visual expression recognition",
      "author" : [ "Srinivas Parthasarathy", "Shiva Sundaram." ],
      "venue" : "Companion Publication of the 2020 International Conference on Multimodal Interaction, pages 400–404.",
      "citeRegEx" : "Parthasarathy and Sundaram.,? 2020",
      "shortCiteRegEx" : "Parthasarathy and Sundaram.",
      "year" : 2020
    }, {
      "title" : "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "author" : [ "Hai Pham", "Paul Pu Liang", "Thomas Manzini", "LouisPhilippe Morency", "Barnabás Póczos." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial In-",
      "citeRegEx" : "Pham et al\\.,? 2019",
      "shortCiteRegEx" : "Pham et al\\.",
      "year" : 2019
    }, {
      "title" : "Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition",
      "author" : [ "Haşim Sak", "Andrew Senior", "Françoise Beaufays." ],
      "venue" : "arXiv preprint arXiv:1402.1128.",
      "citeRegEx" : "Sak et al\\.,? 2014",
      "shortCiteRegEx" : "Sak et al\\.",
      "year" : 2014
    }, {
      "title" : "Metric learning on healthcare data with incomplete modalities",
      "author" : [ "Qiuling Suo", "Weida Zhong", "Fenglong Ma", "Ye Yuan", "Jing Gao", "Aidong Zhang." ],
      "venue" : "IJCAI, pages 3534–3540.",
      "citeRegEx" : "Suo et al\\.,? 2019",
      "shortCiteRegEx" : "Suo et al\\.",
      "year" : 2019
    }, {
      "title" : "Missing modalities imputation via cascaded residual autoencoder",
      "author" : [ "Luan Tran", "Xiaoming Liu", "Jiayu Zhou", "Rong Jin." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1405–1414.",
      "citeRegEx" : "Tran et al\\.,? 2017",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2017
    }, {
      "title" : "Multimodal transformer for unaligned multimodal language sequences",
      "author" : [ "Yao-Hung Hubert Tsai", "Shaojie Bai", "Paul Pu Liang", "J Zico Kolter", "Louis-Philippe Morency", "Ruslan Salakhutdinov." ],
      "venue" : "Proceedings of the conference. Association for Com-",
      "citeRegEx" : "Tsai et al\\.,? 2019",
      "shortCiteRegEx" : "Tsai et al\\.",
      "year" : 2019
    }, {
      "title" : "Transmodality: An end2end fusion method with transformer for multimodal sentiment analysis",
      "author" : [ "Zilong Wang", "Zhaohong Wan", "Xiaojun Wan." ],
      "venue" : "Proceedings of The Web Conference 2020, pages 2514–2520.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning alignment for multimodal emotion recognition from speech",
      "author" : [ "Haiyang Xu", "Hui Zhang", "Kun Han", "Yun Wang", "Yiping Peng", "Xiangang Li." ],
      "venue" : "arXiv preprint arXiv:1909.05645.",
      "citeRegEx" : "Xu et al\\.,? 2019",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi-modal multi-cultural dimensional continues emotion recognition in dyadic interactions",
      "author" : [ "Jinming Zhao", "Ruichen Li", "Shizhe Chen", "Qin Jin." ],
      "venue" : "Proceedings of the 2018 on Audio/Visual Emotion Challenge and Workshop, pages 65–72.",
      "citeRegEx" : "Zhao et al\\.,? 2018",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2018
    }, {
      "title" : "Unpaired image-to-image translation using cycle-consistent adversarial networks",
      "author" : [ "Jun-Yan Zhu", "Taesung Park", "Phillip Isola", "Alexei A Efros." ],
      "venue" : "Proceedings of the IEEE international conference on computer vision, pages 2223–2232.",
      "citeRegEx" : "Zhu et al\\.,? 2017",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Automatic multimodal emotion recognition is very important to natural human-computer interactions (Fragopanagos and Taylor, 2002).",
      "startOffset" : 98,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "Previous works have shown that these different modalities are complimentary for emotion expression, and proposed many effective multimodal fusion methods to improve the emotion recognition performance (Baltrušaitis et al., 2018; Tsai et al., 2019; Zhao et al., 2018).",
      "startOffset" : 201,
      "endOffset" : 266
    }, {
      "referenceID" : 22,
      "context" : "Previous works have shown that these different modalities are complimentary for emotion expression, and proposed many effective multimodal fusion methods to improve the emotion recognition performance (Baltrušaitis et al., 2018; Tsai et al., 2019; Zhao et al., 2018).",
      "startOffset" : 201,
      "endOffset" : 266
    }, {
      "referenceID" : 25,
      "context" : "Previous works have shown that these different modalities are complimentary for emotion expression, and proposed many effective multimodal fusion methods to improve the emotion recognition performance (Baltrušaitis et al., 2018; Tsai et al., 2019; Zhao et al., 2018).",
      "startOffset" : 201,
      "endOffset" : 266
    }, {
      "referenceID" : 0,
      "context" : "Existing multimodal fusion models trained on full-modality samples usually fail when partial modalities are missing (Aguilar et al., 2019; Pham et al., 2019; Cai et al., 2018; Parthasarathy and Sundaram, 2020).",
      "startOffset" : 116,
      "endOffset" : 209
    }, {
      "referenceID" : 18,
      "context" : "Existing multimodal fusion models trained on full-modality samples usually fail when partial modalities are missing (Aguilar et al., 2019; Pham et al., 2019; Cai et al., 2018; Parthasarathy and Sundaram, 2020).",
      "startOffset" : 116,
      "endOffset" : 209
    }, {
      "referenceID" : 5,
      "context" : "Existing multimodal fusion models trained on full-modality samples usually fail when partial modalities are missing (Aguilar et al., 2019; Pham et al., 2019; Cai et al., 2018; Parthasarathy and Sundaram, 2020).",
      "startOffset" : 116,
      "endOffset" : 209
    }, {
      "referenceID" : 17,
      "context" : "Existing multimodal fusion models trained on full-modality samples usually fail when partial modalities are missing (Aguilar et al., 2019; Pham et al., 2019; Cai et al., 2018; Parthasarathy and Sundaram, 2020).",
      "startOffset" : 116,
      "endOffset" : 209
    }, {
      "referenceID" : 10,
      "context" : "(Han et al., 2019) propose a joint training approach that implicitly fuses multimodal information from auxiliary modalities, which improves the monomodal emotion recognition performance.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 18,
      "context" : "The recent cross-modality sequential translation-based methods proposed in (Pham et al., 2019; Wang et al., 2020) learn the joint multimodal representations via translating a source modality to multiple target modalities, which improves the performance",
      "startOffset" : 75,
      "endOffset" : 113
    }, {
      "referenceID" : 23,
      "context" : "The recent cross-modality sequential translation-based methods proposed in (Pham et al., 2019; Wang et al., 2020) learn the joint multimodal representations via translating a source modality to multiple target modalities, which improves the performance",
      "startOffset" : 75,
      "endOffset" : 113
    }, {
      "referenceID" : 13,
      "context" : "Additionally, the sequential translation-based models require translation and generation of videos, audios, and text, which are difficult to train especially with limited training samples (Li et al., 2018; Pham et al., 2019).",
      "startOffset" : 188,
      "endOffset" : 224
    }, {
      "referenceID" : 18,
      "context" : "Additionally, the sequential translation-based models require translation and generation of videos, audios, and text, which are difficult to train especially with limited training samples (Li et al., 2018; Pham et al., 2019).",
      "startOffset" : 188,
      "endOffset" : 224
    }, {
      "referenceID" : 21,
      "context" : "(Tran et al., 2017) and Cycle Consistency Learning (Zhu et al.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 26,
      "context" : ", 2017) and Cycle Consistency Learning (Zhu et al., 2017) based on sentence-level modalityspecific representations, as the sentence-level representation is more reasonable for modeling the cross-modality emotion correlation.",
      "startOffset" : 39,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "Temporal attention-based methods are proposed to use the attention mechanism to selectively fuse different modalities based on the frame-level or word-level temporal sequence, such as Gated Multimodal Unit (GMU) (Aguilar et al., 2019), Multimodal Alignment Model (MMAN) (Xu et al.",
      "startOffset" : 212,
      "endOffset" : 234
    }, {
      "referenceID" : 24,
      "context" : ", 2019), Multimodal Alignment Model (MMAN) (Xu et al., 2019) and Multi-modal Attention mechanism (cLSTM-MMA) (Pan et al.",
      "startOffset" : 43,
      "endOffset" : 60
    }, {
      "referenceID" : 16,
      "context" : ", 2019) and Multi-modal Attention mechanism (cLSTM-MMA) (Pan et al., 2020).",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 17,
      "context" : "The first group features the data augmentation approach, which randomly ablates the inputs to mimic missing modality cases (Parthasarathy and Sundaram, 2020).",
      "startOffset" : 123,
      "endOffset" : 157
    }, {
      "referenceID" : 13,
      "context" : "The second group is based on generative methods to directly predict the missing modalities given the available modalities (Li et al., 2018; Cai et al., 2018; Suo et al., 2019; Du et al., 2018).",
      "startOffset" : 122,
      "endOffset" : 192
    }, {
      "referenceID" : 5,
      "context" : "The second group is based on generative methods to directly predict the missing modalities given the available modalities (Li et al., 2018; Cai et al., 2018; Suo et al., 2019; Du et al., 2018).",
      "startOffset" : 122,
      "endOffset" : 192
    }, {
      "referenceID" : 20,
      "context" : "The second group is based on generative methods to directly predict the missing modalities given the available modalities (Li et al., 2018; Cai et al., 2018; Suo et al., 2019; Du et al., 2018).",
      "startOffset" : 122,
      "endOffset" : 192
    }, {
      "referenceID" : 7,
      "context" : "The second group is based on generative methods to directly predict the missing modalities given the available modalities (Li et al., 2018; Cai et al., 2018; Suo et al., 2019; Du et al., 2018).",
      "startOffset" : 122,
      "endOffset" : 192
    }, {
      "referenceID" : 0,
      "context" : "The third group aims to learn the joint multimodal representations that can contain related information from these modalities (Aguilar et al., 2019; Pham et al., 2019; Han et al., 2019; Wang et al., 2020).",
      "startOffset" : 126,
      "endOffset" : 204
    }, {
      "referenceID" : 18,
      "context" : "The third group aims to learn the joint multimodal representations that can contain related information from these modalities (Aguilar et al., 2019; Pham et al., 2019; Han et al., 2019; Wang et al., 2020).",
      "startOffset" : 126,
      "endOffset" : 204
    }, {
      "referenceID" : 10,
      "context" : "The third group aims to learn the joint multimodal representations that can contain related information from these modalities (Aguilar et al., 2019; Pham et al., 2019; Han et al., 2019; Wang et al., 2020).",
      "startOffset" : 126,
      "endOffset" : 204
    }, {
      "referenceID" : 23,
      "context" : "The third group aims to learn the joint multimodal representations that can contain related information from these modalities (Aguilar et al., 2019; Pham et al., 2019; Han et al., 2019; Wang et al., 2020).",
      "startOffset" : 126,
      "endOffset" : 204
    }, {
      "referenceID" : 17,
      "context" : "(Parthasarathy and Sundaram, 2020) propose a strategy to randomly ablate visual inputs during",
      "startOffset" : 0,
      "endOffset" : 34
    }, {
      "referenceID" : 21,
      "context" : "(Tran et al., 2017) propose Cascaded Residual Autoencoder (CRA) to utilize the residual mechanism over the autoencoder structure, which can take the corrupted data and estimate a function to well restore the incomplete data.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 5,
      "context" : "(Cai et al., 2018) propose an encoder-decoder deep neural network to generate the missing modality (Positron Emission Tomography, PET) given the available modality (Magnetic Resonance Imaging, MRI), and the generated PET can provide complementary information to improve the detection and tracking of Alzheimers disease.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 10,
      "context" : "(Han et al., 2019) propose a joint training model that consists of two modality-specific encoders and one shared classifier, which implicitly fuse the audio and visual information as joint representations and improve the performance of the mono-modality emotion recognition.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 18,
      "context" : "(Pham et al., 2019) propose a sequential translation-based model to learn the joint repre-",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 23,
      "context" : "(Wang et al., 2020) follow this translation-based method and propose a more efficient transformerbased translation model with parallel translation including textual features to acoustic features and textual features to visual features.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 21,
      "context" : "As illustrated in Figure 2(a), we employ the Cascade Residual Autoencoder (CRA) (Tran et al., 2017) structure, which has sufficient learning capacity and more stable convergence than the standard autoencoder.",
      "startOffset" : 80,
      "endOffset" : 99
    }, {
      "referenceID" : 26,
      "context" : "We further employ cycle consistency learning (Zhu et al., 2017; Wang et al., 2020) with a coupled net architecture with two independent networks to perform imagination in two directions, including the Forward (available → missing) and Backward (missing → available) imagination directions.",
      "startOffset" : 45,
      "endOffset" : 82
    }, {
      "referenceID" : 23,
      "context" : "We further employ cycle consistency learning (Zhu et al., 2017; Wang et al., 2020) with a coupled net architecture with two independent networks to perform imagination in two directions, including the Forward (available → missing) and Backward (missing → available) imagination directions.",
      "startOffset" : 45,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : "We evaluate our proposed model on two benchmark multimodal emotion recognition datasets, Interactive Emotional Dyadic Motion Capture (IEMOCAP) (Busso et al., 2008) and MSP-IMPROV (Busso et al.",
      "startOffset" : 143,
      "endOffset" : 163
    }, {
      "referenceID" : 24,
      "context" : "We follow the emotional label processing in (Xu et al., 2019; Liang et al., 2020) to form the four-class emotion recognition setup.",
      "startOffset" : 44,
      "endOffset" : 81
    }, {
      "referenceID" : 14,
      "context" : "We follow the emotional label processing in (Xu et al., 2019; Liang et al., 2020) to form the four-class emotion recognition setup.",
      "startOffset" : 44,
      "endOffset" : 81
    }, {
      "referenceID" : 14,
      "context" : "We follow feature extraction methods described in (Liang et al., 2020; Pan et al., 2020) and extract the frame-level raw features of each modality 2.",
      "startOffset" : 50,
      "endOffset" : 88
    }, {
      "referenceID" : 16,
      "context" : "We follow feature extraction methods described in (Liang et al., 2020; Pan et al., 2020) and extract the frame-level raw features of each modality 2.",
      "startOffset" : 50,
      "endOffset" : 88
    }, {
      "referenceID" : 8,
      "context" : "Acoustic features: OpenSMILE toolkit (Eyben et al., 2010) with the configuration of “IS13 ComParE” is used to extract frame-level features, which have similar performance with the IS10 utterance-level acoustic features used in (Liang et al.",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 14,
      "context" : ", 2010) with the configuration of “IS13 ComParE” is used to extract frame-level features, which have similar performance with the IS10 utterance-level acoustic features used in (Liang et al., 2020).",
      "startOffset" : 177,
      "endOffset" : 197
    }, {
      "referenceID" : 2,
      "context" : ", 2017) which is trained based on the Facial Expression Recognition Plus (FER+) corpus (Barsoum et al., 2016).",
      "startOffset" : 87,
      "endOffset" : 109
    }, {
      "referenceID" : 6,
      "context" : "Textual features: We extract contextual word embeddings using a pretrained BERT-large model (Devlin et al., 2019) which is one of the state-ofthe-art language representations.",
      "startOffset" : 92,
      "endOffset" : 113
    }, {
      "referenceID" : 19,
      "context" : "Acoustic Modality Encoder (EncA): We apply a Long Short-term Memory (LSTM) network (Sak et al., 2014) to capture the temporal information based on the sequential frame-level raw acoustic features xa.",
      "startOffset" : 83,
      "endOffset" : 101
    }, {
      "referenceID" : 12,
      "context" : "Textual Modality Encoder (EncT): We apply a TextCNN (Kim, 2014) to get the utterance-level textual embedding as ht based on the sequential word-level features xt.",
      "startOffset" : 52,
      "endOffset" : 63
    }, {
      "referenceID" : 18,
      "context" : "We also compare to the MCTN (Pham et al., 2019) model which is the state-of-the-art model for the missing modality problem.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 18,
      "context" : "3 and follow the training setting in (Pham et al., 2019) to conduct the MCTN experiments.",
      "startOffset" : 37,
      "endOffset" : 56
    }, {
      "referenceID" : 15,
      "context" : "Imagination ability: Figure 3 visualizes the distribution of the ground-truth multimodal embeddings (ĥ in Figure 2) and MMIN imagined multimodal embeddings (h ′ in Figure 2) for a male speaker and female speaker using t-SNE (Maaten and Hinton, 2008).",
      "startOffset" : 224,
      "endOffset" : 249
    } ],
    "year" : 2021,
    "abstractText" : "Multimodal fusion has been proved to improve emotion recognition performance in previous works. However, in real-world applications, we often encounter the problem of missing modality, and which modalities will be missing is uncertain. It makes the fixed multimodal fusion fail in such cases. In this work, we propose a unified model, Missing Modality Imagination Network (MMIN), to deal with the uncertain missing modality problem. MMIN learns robust joint multimodal representations, which can predict the representation of any missing modality given available modalities under different missing modality conditions. Comprehensive experiments on two benchmark datasets demonstrate that the unified MMIN model significantly improves emotion recognition performance under both uncertain missing-modality testing conditions and fullmodality ideal testing condition. The code will be available at https://github.com/AIM3RUC/MMIN.",
    "creator" : "LaTeX with hyperref package"
  }
}