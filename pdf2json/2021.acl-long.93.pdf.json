{
  "name" : "2021.acl-long.93.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Uncovering Constraint-Based Behavior in Neural Models via Targeted Fine-Tuning",
    "authors" : [ "Forrest Davis", "Marten van Schijndel" ],
    "emails" : [ "fd252@cornell.edu", "mv443@cornell.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1159–1171\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1159\nA growing body of literature has focused on detailing the linguistic knowledge embedded in large, pretrained language models. Existing work has shown that non-linguistic biases in models can drive model behavior away from linguistic generalizations. We hypothesized that competing linguistic processes within a language, rather than just non-linguistic model biases, could obscure underlying linguistic knowledge. We tested this claim by exploring a single phenomenon in four languages: English, Chinese, Spanish, and Italian. While human behavior has been found to be similar across languages, we find cross-linguistic variation in model behavior. We show that competing processes in a language act as constraints on model behavior and demonstrate that targeted fine-tuning can re-weight the learned constraints, uncovering otherwise dormant linguistic knowledge in models. Our results suggest that models need to learn both the linguistic constraints in a language and their relative ranking, with mismatches in either producing non-human-like behavior."
    }, {
      "heading" : "1 Introduction",
      "text" : "Ever larger pretrained language models continue to demonstrate success on a variety of NLP benchmarks (e.g., Devlin et al., 2019; Brown et al., 2020). One common approach for understanding why these models are successful is centered on inferring what linguistic knowledge such models acquire (e.g., Linzen et al., 2016; Hewitt and Manning, 2019; Hu et al., 2020; Warstadt et al., 2020a). Linguistic knowledge alone, of course, does not fully account for model behavior; non-linguistic heuristics have also been shown to drive model behavior (e.g., sentence length; see McCoy et al., 2019; Warstadt et al., 2020b). Nevertheless, when looking across a variety of experimental methods,\nmodels appear to acquire some grammatical knowledge (see Warstadt et al., 2019).\nHowever, investigations of linguistic knowledge in language models are limited by the overwhelming prominence of work solely on English (though see Gulordava et al., 2018; Ravfogel et al., 2018; Mueller et al., 2020). Prior work has shown nonlinguistic biases of neural language models mimic English-like linguistic structure, limiting the generalizability of claims founded on English data (e.g., Dyer et al., 2019; Davis and van Schijndel, 2020b). In the present study, we show via cross-linguistic comparison, that knowledge of competing linguistic constraints can obscure underlying linguistic knowledge.\nOur investigation is centered on a single discourse phenomena, implicit causality (IC) verbs, in four languages: English, Chinese, Spanish, and Italian. When an IC verb occurs in a sentence, interpretations of pronouns are affected:\n(1) a. Lavender frightened Kate because she was so terrifying.\nb. Lavender admired Kate because she was so amazing.\nIn (1), both Lavender and Kate agree in gender with she, so both are possible antecedents. However, English speakers overwhelmingly interpret she as referring to Lavender in (1-a) and Kate in (1-b). Verbs that have a subject preference (e.g., frightened) are called subject-biased IC verbs, and verbs with an object preference (e.g., admired) are called object-biased IC verbs.\nIC has been a rich source of psycholinguistic investigation (e.g., Garvey and Caramazza, 1974; Hartshorne, 2014; Williams, 2020). Current accounts of IC ground the phenomenon within the linguistic signal without the need for additional pragmatic inferences by comprehenders (e.g., Ro-\nhde et al., 2011; Hartshorne et al., 2013). Recent investigations of IC in neural language models confirms that the IC bias of English is learnable, at least to some degree, from text data alone (Davis and van Schijndel, 2020a; Upadhye et al., 2020). The ability of models trained on other languages to acquire an IC bias, however, has not been explored. Within the psycholinguistic literature, IC has been shown to be remarkably consistent crosslinguistically (see Hartshorne et al., 2013; Ngo and Kaiser, 2020). That is, IC verbs have been attested in a variety of languages. Given the crosslinguistic consistency of IC, then, models trained on other languages should also demonstrate an IC bias. However, using two popular model types, BERT based (Devlin et al., 2019) and RoBERTa based (Liu et al., 2019),1 we find that models only acquired a human-like IC bias in English and Chinese but not in Spanish and Italian.\nWe relate this to a crucial difference in the presence of a competing linguistic constraint affecting pronouns in the target languages. Namely, Spanish and Italian have a well studied process called pro drop, which allows for subjects to be ‘empty’ (Rizzi, 1986). An English equivalent would be “(she) likes BERT” where she can be elided. While IC verbs increase the probability of a pronoun that refers to a particular antecedent, pro drop disprefers any overt pronoun in subject position (i.e. the target location in our study). That is, both processes are in direct competition in our experiments. As a result, Spanish and Italian models are susceptible to overgeneralizing any learned pro-drop knowledge, favoring no pronouns rather than IC-conditioned pronoun generation.\nTo exhibit an IC bias, models of Spanish and Italian have two tasks: learn the relevant constraints (i.e. IC and pro drop) and the relative ranking of these constraints. We find that the models learn both constraints, but, critically, instantiate the wrong ranking, favoring pro drop to an IC bias. Using fine-tuning to demote pro drop, we are able to uncover otherwise dormant IC knowledge in Spanish and Italian. Thus, the apparent failure of the Spanish and Italian models to pattern like English and Chinese is not evidence on its own of a model’s inability to acquire the requisite linguistic\n1These model types were chosen for ease of access to existing models. Pretrained, large auto-regressive models are largely restricted to English, and prior work suggests that LSTMs are limited in their ability to acquire an IC bias in English (Davis and van Schijndel, 2020a).\nknowledge, but is in fact evidence that models are unable to adjudicate between competing linguistic constraints in a human-like way. In English and Chinese, the promotion of a pro-drop process via fine-tuning has the opposing effect, diminishing an IC bias in model behavior. As such, our results indicate that non-human like behavior can be driven by failure either to learn the underlying linguistic constraints or to learn the relevant constraint ranking."
    }, {
      "heading" : "2 Related Work",
      "text" : "This work is intimately related to the growing body of literature investigating linguistic knowledge in large, pretrained models. Largely, this literature articulates model knowledge via isolated linguistic phenomena, such as subject-verb agreement (e.g., Linzen et al., 2016; Mueller et al., 2020), negative polarity items (e.g., Marvin and Linzen, 2018; Warstadt et al., 2019), and discourse and pragmatic structure (including implicit causality; e.g., Ettinger, 2020; Schuster et al., 2020; Jeretic et al., 2020; Upadhye et al., 2020). Our study differs, largely, in framing model linguistic knowledge as sets of competing constraints, which privileges the interaction between linguistic phenomena.\nPrior work has noted competing generalizations influencing model behavior via the distinction of non-linguistic vs. linguistic biases (e.g., McCoy et al., 2019; Davis and van Schijndel, 2020a; Warstadt et al., 2020b). The findings in Warstadt et al. (2020b), that linguistic knowledge is represented within a model much earlier than attestation in model behavior, bears resemblance to our claims. We find that linguistic knowledge can, in fact, lie dormant due to other linguistic processes in a language, not just due to non-linguistic preferences. Our findings suggest that some linguistic knowledge may never surface in model behavior, though further work is needed on this point.\nIn the construction of our experiments, we were inspired by synthetic language studies which probe the underlying linguistic capabilities of language models (e.g., McCoy et al., 2018; Ravfogel et al., 2019). We made use of synthetically modified language data that accentuated, or weakened, evidence for certain linguistic processes. The goal of such modification in our work is quite similar both to work which attempts to remove targeted linguistic knowledge in model representations (e.g., Ravfogel et al., 2020; Elazar et al., 2021) and to work which\ninvestigates the representational space of models via priming (Prasad et al., 2019; Misra et al., 2020). In the present study, rather than identifying isolated linguistic knowledge or using priming to study relations between underlying linguistic representations, we ask how linguistic representations interact to drive model behavior."
    }, {
      "heading" : "3 Models",
      "text" : "Prior work on IC in neural language models has been restricted to autoregressive models for ease of comparison to human results (e.g., Upadhye et al., 2020). In the present study, we focused on two popular non-autoregressive language model variants, BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). We used existing models available via HuggingFace (Wolf et al., 2020).\nMultilingual models have been claimed to perform worse on targeted linguistics tasks than monolingual models (e.g., Mueller et al., 2020). We confirmed this claim by evaluating mBERT which exhibited no IC bias in any language.2 Thus, we focus in the rest of this paper on monolingual models (summarized in Table 1). For English, we used the BERT base uncased model and the RoBERTa base model. For Chinese, we evaluated BERT and RoBERTa models from Cui et al. (2020). For Spanish, we used BETO (Cañete et al., 2020) and RuPERTa (Romero, 2020). For Italian, we evaluated an uncased Italian BERT 3 as well as two RoBERTa based models, UmBERTo (Parisi et al., 2020) and GilBERTo (Ravasio and Di Perna, 2020).\n2Results are provided in Appendix B 3https://huggingface.co/dbmdz/bert-base-italian-uncased"
    }, {
      "heading" : "4 Experimental Stimuli and Measures",
      "text" : "Our list of target verbs was derived from existing psycholinguistic studies of IC verbs.4 For English, we used the IC verbs from Ferstl et al. (2011).\nEach verb in the human experiment was coded for IC bias based on continuations of sentence fragments (e.g., Kate accused Bill because ...). For Spanish, we used the IC verbs from Goikoetxea et al. (2008), which followed a similar paradigm as Ferstl et al. (2011) for English. Participants were given sentence fragments and asked to complete the sentence and circle their intended referent. The study reported the percent of subject continuations for 100 verbs, from which we used the 61 verbs which had a significant IC bias (i.e. excluding verbs with no significant subject or object bias).\nFor Italian, we used the 40 IC verbs reported in Mannetti and De Grada (1991). Human participants were given ambiguous completed sentences with no overt pronoun like “John feared Michael because of the kind of person (he) is” and were asked to judge who the null pronoun referred to, with the average number of responses that gave the subject as the antecedent reported.5 For Chinese, we used 59 IC verbs reported in Hartshorne et al. (2013), which determined average subject bias per verb in a similar way as Mannetti and De Grada (1991) (i.e. judgments of antecedent preferences given ambiguous sentences, this time with overt pronouns).6\nWe generated stimuli using 14 pairs of stereotypical male and female nouns (e.g., man vs. woman, husband vs. wife) in each language, rather than rely on proper names as was done in the human experiments. The models we investigated are bidirectional, so we used a neutral right context, was there, for English and Spanish, where human ex-\n4All stimuli, as well as code for reproducing the results of the paper are available at https://github.com/ forrestdavis/ImplicitCausality . For each language investigated, the stimuli were evaluated for grammaticality by native speakers with academic training in linguistics.\n5Specifically, Mannetti and De Grada (1991) grouped the verbs into four categories and reported the average per category as well as individual verb results for the most biased verbs and the negative/positive valency verbs. Additionally, figures showing average responses across various conditions was reported for one of the categories. From the combination of this information, the average scores for all but two verbs were able to be determined. The remaining two verbs were assigned the reported average score of their stimuli group.\n6In Hartshorne et al. (2013), 60 verbs were reported, but after consultation with a native speaker with academic training in linguistics, one verb was excluded due to perceived ungrammaticality of the construction.\nperiments provided no right context.7 For Italian we utilized the full sentences investigated in the human experiments. The Chinese human experiment also used full sentences, but relied on nonce words (i.e. novel, constructed words like sliktopoz), so we chose instead to generate sentences like the English and Spanish ones. All stimuli had subjects and objects that differed in gender, such that all nouns occurred in subject or object position (i.e. the stimuli were fully balanced for gender):\n(2) the man admired the woman because [MASK] was there.8\nThe mismatch in gender forced the choice of pronoun to be unambiguous. For each stimulus, we gathered the scores assigned to the third person singular male and female pronouns (e.g., he and she).9 Our measures were grouped by antecedent type (i.e. the pronoun refers to the subject or the object) and whether the verb was object-biased or subject-biased. For example, BERT assigns to (2) a score of 0.01 for the subject antecedent (i.e. he) and 0.97 for the object (i.e. she), in line with the object-bias of admire.\n7Using here, outside, or inside as the right context produces qualitatively the same patterns.\n8The model-specific mask token was used. Additionally, all models were uncased, with the exception of RoBERTa, so lower cased stimuli were used.\n9In spoken Chinese, the male and female pronouns are homophonous. They are, however, distinguished in writing."
    }, {
      "heading" : "5 Models Inconsistently Capture Implicit Causality",
      "text" : "As exemplified in (1), repeated below, IC verb bias modulates the preference for pronouns.\n(3) a. Lavender frightened Kate because she was so terrifying.\nb. Lavender admired Kate because she was so amazing.\nAn object-biased IC verb (e.g., admired) should increase the likelihood of pronouns that refer to the object, and a subject-biased IC verb (e.g., frightened) should increase the likelihood of reference to the subject. Given that all the investigated stimuli were disambiguated by gender, we categorized our results by the antecedent of the pronoun and the IC verb bias. We first turn to English and Chinese, which showed an IC bias in line with existing work on IC bias in autoregressive English models (e.g., Upadhye et al., 2020; Davis and van Schijndel, 2020a). We then detail the results for Spanish and Italian, where only very limited, if any, IC bias was observed."
    }, {
      "heading" : "5.1 English and Chinese",
      "text" : "The results for English and Chinese are given in Figure 1 and detailed in Appendix B. All models demonstrated a greater preference for pronouns referring to the object after an object-biased IC verb\nthan after a subject-biased IC verb.10 Additionally, they had greater preferences for pronouns referring to the subject after a subject-biased IC verb than after a object-biased IC verb. That is, all models showed the expected IC-bias effect. Generally, there was an overall greater preference for referring to the object, in line with a recency bias, with the exception of RoBERTa, where subject-biased IC verbs neutralized the recency effect."
    }, {
      "heading" : "5.2 Spanish and Italian",
      "text" : "The results for Spanish and Italian are given in Figure 2 and detailed in Appendix B. In stark contrast to the models of English and Chinese, an IC bias was either not demonstrated or was only weakly attested. For Spanish, BETO showed a greater preference for pronouns referencing the object after an object-biased IC verb than after a subject-biased IC verb. There was no corresponding IC effect for pronouns referring to the subject, and RuPERTa (a RoBERTa based model) had no IC effect at all.\nItalian BERT and GilBERTo (a RoBERTa based model) had no significant effect of IC-verb on pronouns referring to the object. There was a significant, albeit very small, increased score for pronouns referring to the subject after a subject-biased IC verb in line with a weak subject-IC bias. Similarly, UmBERTo (a RoBERTa based model) had significant, yet tiny IC effects, where object-biased IC verbs increased the score of pronouns referring to objects compared to subject-biased IC verbs (conversely with pronouns referring to the subject).\nAny significant effects in Spanish and Italian were much smaller than their counterparts in English (as is visually apparent between Figure 1 and Figure 2), and each of the Spanish and Italian models failed to demonstrate at least one of the IC effects."
    }, {
      "heading" : "6 Pro Drop and Implicit Causality: Competing Constraints",
      "text" : "We were left with an apparent mismatch between models of English and Chinese and models of Spanish and Italian. In the former, an IC verb bias modulated pronoun preferences. In the latter, the same\n10Throughout the paper, statistical significance was determined by two-way t-tests evaluating the difference between pronouns referring to objects after subject-biased and objectbiased IC verbs, and similarly for pronouns referring to the subject. The threshold for statistical significance was p = 0.0009, after adjusting for the 54 statistical tests conducted in the paper.\nIC verb bias was comparably absent. Recall that, for humans, the psycholinguistic literature suggests that IC bias is, in fact, quite consistent across languages (see Hartshorne et al., 2013).\nWe found a possible reason for why the two sets of models behave so differently by carefully considering the languages under investigation. Languages can be thought of as systems of competing linguistic constraints (e.g., Optimality Theory; Prince and Smolensky, 2004). Spanish and Italian exhibit pro drop and typical grammatical sentences often lack overt pronouns in subject position, opting instead to rely on rich agreement systems to disambiguate the intended subject at the verb (Rizzi, 1986). This constraint competes with IC, which favors pronouns that refer to either the subject or the object. Chinese also allows for empty arguments (both subjects and objects), typically called discourse pro-drop (Huang, 1984).11 As the name suggests, however, this process is more discourse constrained than the process in Spanish and Italian. For example, in Chinese, the empty subject can only refer to the subject of the preceding sentence (see Liu, 2014). As a means of comparison, in surveying three Universal Dependencies datasets,12 8% of nsubj (or nsubj:pass) relations were pronouns for Chinese, while only 2% and 3% were pronouns in Spanish and Italian respectively. English lies on the opposite end of the continuum, requiring overt pronouns in the absence of other nominals (cf. He likes NLP and *Likes NLP).\nTherefore, it’s possible that the presence of competing constraints in Spanish and Italian obscured the underlying IC knowledge: one constraint preferring pronouns which referred to the subject or object and the other constraint penalizing overt pronouns in subject positions (i.e. the target position masked in our experiments). In the following sections, we removed or otherwise demoted the dominance of each model’s pro-drop constraint for Spanish and Italian, and introduced or promoted a pro-drop like constraint in English and Chinese. We found that the degree of IC bias in model behavior could be controlled by the presence, or absence, of a competing pro-drop constraint."
    }, {
      "heading" : "6.1 Methodology",
      "text" : "We constructed two classes of dataset to fine-tune the models on. The first aimed to demote the pro-\n11Other names common to the literature include topic drop, radical pro drop, and rampant pro drop.\n12Chinese GSD, Italian ISDT, and Spanish AnCora.\ndrop constraint in Spanish and Italian. The second aimed to inject a pro-drop constraint into English and Chinese. For both we relied on Universal Dependencies datasets. For Spanish, we used the AnCora Spanish newswire corpus (Taulé et al., 2008), for Italian we used ISDT (Bosco et al., 2013) and VIT (Delmonte et al., 2007), for English we used the English Web Treebank (Silveira et al., 2014), and for Chinese, we used the Traditional Chinese Universal Dependencies Treebank annotated by Google (GSD) and the Chinese Parallel Universal Dependencies (PUD) corpus from the 2017 CoNLL shared task (Zeman et al., 2017).\nFor demoting pro drop, we found finite (i.e. inflected) verbs that did not have a subject relation in the corpora.13 We then added a pronoun, matching the person and number information given on the verb, alternating the gender. For Italian, this amounted to a dataset of 3798 sentences with a total of 4608 pronouns (2,284 he or she) added. For parity with Italian, we restricted Spanish to a dataset of the first 4000 sentences, which had 5,559 pronouns (3,573 he or she) added. For the addition of a pro-drop constraint in English and Chinese, we found and removed pronouns that bore a subject relation to a verb. This amounted to 935 modified sentences and 1083 removed pronouns (774 he or she) in Chinese and 4000 modified sentences\n13In particular, verbs that lacked any nsubj, nsubj:pass, expl, expl:impers, or expl:pass dependents\nand 5984 removed pronouns (2188 he or she) in English.14\nFor each language, 500 unmodified sentences were used for validation, and unchanged versions of all the sentences were kept and used to fine-tune the models as a baseline to ensure that there was nothing about the data themselves that changed the IC-bias of the models. Moreover, the fine-tuning data was filtered to ensure that no verbs evaluated in our test data were included. Fine-tuning proceeded using HuggingFace’s API. Each model was finetuned with a masked language modeling objective for 3 epochs with a learning rate of 5e-5, following the fine-tuning details in (Devlin et al., 2019).15"
    }, {
      "heading" : "6.2 Demoting Pro Drop: Spanish and Italian",
      "text" : "As a baseline, we fine-tuned the Spanish and Italian models on unmodified versions of all the data we used for demoting pro drop. The baseline results are given in Figure 3. We found the same qualitative effects detailed in Section 5.2, confirming that the data used for fine-tuning on their own did not produce model behavior in line with an IC bias.\nWe turn now to our main experimental manipu14A fuller breakdown of the fine-tuning data is given in Appendix A with the full training and evaluation data given on our Github. We restricted English to the first 4000 sentences for parity with Italian/Spanish. Using the full set of sentences resulted in qualitatively the same pattern. We used the maximum number of sentences we could take from Chinese UD.\n15We provide a Colab script for reproducing all fine-tuned models on our Github.\nlation: fine-tuning the Spanish and Italian models on sentences that exhibit the opposite of a pro-drop effect. It is worth repeating that the fine-tuning data shared no verbs or sentence frames with our test data. The results are given in Figure 4. Strikingly, an object-biased IC effect (pronouns referring to the object were more likely after objectbiased IC verbs than subject-biased IC verbs) was observed for Italian BERT and GilBERTo despite no such effect being observed in the base models. Moreover, both models showed a more than doubled subject-biased IC verb effect. UmBERTo also showed increased IC effects, as compared to the base models. Similarly for Spanish, a subjectbiased IC verb effect materialized for BETO when no corresponding effect was observed with the base model. The object-biased IC verb effect remained similar to what was reported in Section 5.2. For RuPERTa, which showed no IC knowledge in the initial investigation, no IC knowledge surfaced after fine-tuning. We conclude that RuPERTa has no underlying knowledge of IC, though further work should investigate this claim.\nTaken together these results indicate that simply fine-tuning on a small number of sentences can rerank the linguistic constraints influencing model behavior and uncover other linguistic knowledge (in our case an underlying IC-bias). That is, model behavior can hide linguistic knowledge not just because of non-linguistic heuristics, but also due\nto over-zealously learning one isolated aspect of linguistic structure at the expense of another."
    }, {
      "heading" : "6.3 Promoting Pro Drop: English and Chinese",
      "text" : "Next, we fine-tune a pro-drop constraint into models of English and Chinese. Recall that both models showed an IC effect, for both object-biased and subject-biased IC verbs. Moreover, both languages lack the pro-drop process found in Spanish and Italian (though Chinese allows null arguments).\nAs with Spanish and Italian, we fine-tuned the English and Chinese models on unmodified versions of the training sentences as a baseline (i.e. the sentences kept their pronouns) with the results given in Figure 5. There was no qualitative difference from the IC effects noted in Section 5.1. That is, for both English and Chinese, pronouns referring to the object were more likely after objectbiased IC verbs than after subject-biased IC verbs, and conversely pronouns referring to the subject were more likely after subject-biased than objectbiased IC verbs.\nThe results after fine-tuning the models on data mimicking a Spanish and Italian like pro-drop process (i.e. no pronouns in subject position) are given in Figure 6 and detailed in Appendix B. Despite fine-tuning on only 0.0004% and 0.003% of the data RoBERTa and BERT were trained on, respectively, the IC effects observed in Section 5.1 were severely diminished in English. However,\nthe subject-biased IC verb effect remained robust in both models. For Chinese BERT, the subjectbiased IC verb effect in the base model was lost and the object-biased IC verb effect was reduced. The subject-biased IC verb effect was similarly attenuated in Chinese RoBERTa. However, the object-biased IC verb effect remained.\nFor both languages, exposure to relatively little pro-drop data weakened the IC effect in behavior and even removed it in the case of subject-biased IC verbs in Chinese BERT. This result strengthens our claim that competition between learned linguistic constraints can obscure underlying linguistic knowledge in model behavior."
    }, {
      "heading" : "7 Discussion",
      "text" : "The present study investigated the ability of RoBERTa and BERT models to demonstrate knowledge of implicit causality across four languages (recall the contrast between Lavender frightened Kate and Lavender admired Kate in (1)). Contrary to humans, who show consistent subject and objectbiased IC verb preferences across languages (see Hartshorne et al., 2013), BERT and RoBERTa models of Spanish and Italian failed to demonstrate the full IC bias found in English and Chinese BERT and RoBERTa models (with our English results supporting prior work on IC bias in neural models and extending it to non-autoregressive models; Upadhye et al., 2020; Davis and van Schijndel, 2020a). Following standard behavioral probing (e.g., Linzen et al., 2016), this mismatch could be interpreted as evidence of differences in linguistic knowledge across languages. That is, model behavior in Spanish and Italian was inconsistent with predictions from the psycholinguistic IC literature, suggesting that these models lack knowledge of implicit causality. However, we found that to be an incorrect inference; the models did have underlying knowledge of IC.\nOther linguistic processes influence pronouns in Spanish and Italian, and we showed that competition between multiple distinct constraints affects model behavior. One constraint (pro drop) decreases the probability of overt pronouns in subject position, while the other (IC) increases the probability of pronouns that refer to particular antecedents (subject-biased verbs like frightened favoring subjects and object-biased verbs like admired favoring objects). Models of Spanish and Italian, then, must learn not only these two con-\nstraints, but also their ranking (i.e. should the model generate a pronoun as IC dictates, or generate no pronoun in line with pro drop). By fine-tuning the models on data contrary to pro drop (i.e. with overt pronouns in subject position), we uncovered otherwise hidden IC knowledge. Moreover, we found that fine-tuning a pro-drop constraint into English and Chinese greatly diminished IC’s influence on model behavior (with as little as 0.0004% of a models original training data).\nTaken together, we conclude that there are two ways of understanding mismatches between model linguistic behavior and human linguistic behavior. Either a model fails to learn the necessary linguistic constraint, or it succeeds in learning the constraint but fails to learn the correct interaction with other constraints. Existing literature points to a number of reasons a model may be unable to learn a linguistic representation, including the inability to learn mappings between form and meaning and the lack of embodiment (e.g., Bender and Koller, 2020; Bisk et al., 2020). We suggest that researchers should re-conceptualize linguistic inference on the part of neural models as inference of constraints and constraint ranking in order to better understand model behavior. We believe such framing will open additional connections with linguistic theory and psycholinguistics. Minimally, we believe targeted fine-tuning for constraint re-ranking may provide a general method both to understand what linguistic knowledge these models possess and to aid in making their linguistic behavior more human-like."
    }, {
      "heading" : "8 Conclusion and Future Work",
      "text" : "The present study provided evidence that model behavior can be meaningfully described, and understood, with reference to competing constraints. We believe that this is a potentially fruitful way of reasoning about model linguistic knowledge. Possible future directions include pairing our behavioral analyses with representational probing in order to more explicitly link model representations and model behavior (e.g., Ettinger et al., 2016; Hewitt and Liang, 2019) or exploring constraint competition in different models, like GPT-2 which has received considerable attention for its apparent linguistic behavior (e.g., Hu et al., 2020) and its ability to predict neural responses (e.g., Schrimpf et al., 2020)."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank members of the C.Psyd Lab, the Cornell NLP group, and the Stanford NLP Group, who gave valuable feedback on earlier forms of this work. Thanks also to the anonymous reviewers whose comments improved the paper."
    }, {
      "heading" : "A Additional Fine-tuning Training Information",
      "text" : "The full breakdown of pronouns added or removed in the fine-tuning training data are detailed below. English can be found in Table 2, Chinese can be found in Table 3, Spanish can be found in Table 4, and Italian can be found in Table 5."
    }, {
      "heading" : "B Expanded Results (including mBERT)",
      "text" : "The full details of the pairwise t-tests conducted for the present study are given below (including the results for mBERT). The results for English models are in Table 6, for Chinese models Table 7, for Spanish models Table 8, and Italian models Table 9."
    } ],
    "references" : [ {
      "title" : "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
      "author" : [ "Emily M. Bender", "Alexander Koller." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185–5198, Online. As-",
      "citeRegEx" : "Bender and Koller.,? 2020",
      "shortCiteRegEx" : "Bender and Koller.",
      "year" : 2020
    }, {
      "title" : "Experience Grounds Language",
      "author" : [ "Yonatan Bisk", "Ari Holtzman", "Jesse Thomason", "Jacob Andreas", "Yoshua Bengio", "Joyce Chai", "Mirella Lapata", "Angeliki Lazaridou", "Jonathan May", "Aleksandr Nisnevich", "Nicolas Pinto", "Joseph Turian." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Bisk et al\\.,? 2020",
      "shortCiteRegEx" : "Bisk et al\\.",
      "year" : 2020
    }, {
      "title" : "Converting Italian treebanks: Towards an Italian Stanford dependency treebank",
      "author" : [ "Cristina Bosco", "Simonetta Montemagni", "Maria Simi." ],
      "venue" : "Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 61–69,",
      "citeRegEx" : "Bosco et al\\.,? 2013",
      "shortCiteRegEx" : "Bosco et al\\.",
      "year" : 2013
    }, {
      "title" : "Language Models are Few-Shot Learners",
      "author" : [ "Amodei." ],
      "venue" : "arXiv:2005.14165 [cs].",
      "citeRegEx" : "Amodei.,? 2020",
      "shortCiteRegEx" : "Amodei.",
      "year" : 2020
    }, {
      "title" : "Spanish Pre-Trained BERT Model and Evaluation Data",
      "author" : [ "José Cañete", "Gabriel Chaperon", "Rodrigo Fuentes", "JouHui Ho", "Hojin Kang", "Jorge Pérez." ],
      "venue" : "PML4DC at ICLR 2020.",
      "citeRegEx" : "Cañete et al\\.,? 2020",
      "shortCiteRegEx" : "Cañete et al\\.",
      "year" : 2020
    }, {
      "title" : "Revisiting PreTrained Models for Chinese Natural Language Processing",
      "author" : [ "Yiming Cui", "Wanxiang Che", "Ting Liu", "Bing Qin", "Shijin Wang", "Guoping Hu." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 657–668,",
      "citeRegEx" : "Cui et al\\.,? 2020",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2020
    }, {
      "title" : "Discourse structure interacts with reference but not syntax in neural language models",
      "author" : [ "Forrest Davis", "Marten van Schijndel." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Davis and Schijndel.,? 2020a",
      "shortCiteRegEx" : "Davis and Schijndel.",
      "year" : 2020
    }, {
      "title" : "Recurrent Neural Network Language Models Always Learn English-Like Relative Clause Attachment",
      "author" : [ "Forrest Davis", "Marten van Schijndel." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1979–",
      "citeRegEx" : "Davis and Schijndel.,? 2020b",
      "shortCiteRegEx" : "Davis and Schijndel.",
      "year" : 2020
    }, {
      "title" : "VIT – Venice Italian Treebank: Syntactic and Quantitative Features",
      "author" : [ "Rodolfo Delmonte", "Antonella Bristot", "Sara Tonelli." ],
      "venue" : "Sixth International Workshop on Treebanks and Linguistic Theories, volume 1, pages 43–54.",
      "citeRegEx" : "Delmonte et al\\.,? 2007",
      "shortCiteRegEx" : "Delmonte et al\\.",
      "year" : 2007
    }, {
      "title" : "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "A Critical Analysis of Biased Parsers in Unsupervised Parsing",
      "author" : [ "Chris Dyer", "Gábor Melis", "Phil Blunsom." ],
      "venue" : "arXiv:1909.09428 [cs].",
      "citeRegEx" : "Dyer et al\\.,? 2019",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2019
    }, {
      "title" : "Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals",
      "author" : [ "Yanai Elazar", "Shauli Ravfogel", "Alon Jacovi", "Yoav Goldberg." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 9:160–175.",
      "citeRegEx" : "Elazar et al\\.,? 2021",
      "shortCiteRegEx" : "Elazar et al\\.",
      "year" : 2021
    }, {
      "title" : "What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models",
      "author" : [ "Allyson Ettinger." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:34–48.",
      "citeRegEx" : "Ettinger.,? 2020",
      "shortCiteRegEx" : "Ettinger.",
      "year" : 2020
    }, {
      "title" : "Probing for semantic evidence of composition by means of simple classification tasks",
      "author" : [ "Allyson Ettinger", "Ahmed Elgohary", "Philip Resnik." ],
      "venue" : "Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP, pages 134–139. Associa-",
      "citeRegEx" : "Ettinger et al\\.,? 2016",
      "shortCiteRegEx" : "Ettinger et al\\.",
      "year" : 2016
    }, {
      "title" : "Implicit causality bias in English: A corpus of 300 verbs",
      "author" : [ "Evelyn C. Ferstl", "Alan Garnham", "Christina Manouilidou." ],
      "venue" : "Behavior Research Methods, 43(1):124–135.",
      "citeRegEx" : "Ferstl et al\\.,? 2011",
      "shortCiteRegEx" : "Ferstl et al\\.",
      "year" : 2011
    }, {
      "title" : "Implicit Causality in Verbs",
      "author" : [ "Catherine Garvey", "Alfonso Caramazza." ],
      "venue" : "Linguistic Inquiry, 5(3):459–464.",
      "citeRegEx" : "Garvey and Caramazza.,? 1974",
      "shortCiteRegEx" : "Garvey and Caramazza.",
      "year" : 1974
    }, {
      "title" : "Normative study of the implicit causality of 100 interpersonal verbs in Spanish",
      "author" : [ "Edurne Goikoetxea", "Gema Pascual", "Joana Acha." ],
      "venue" : "Behavior Research Methods, 40(3):760–772.",
      "citeRegEx" : "Goikoetxea et al\\.,? 2008",
      "shortCiteRegEx" : "Goikoetxea et al\\.",
      "year" : 2008
    }, {
      "title" : "Colorless Green Recurrent Networks Dream Hierarchically",
      "author" : [ "Kristina Gulordava", "Piotr Bojanowski", "Edouard Grave", "Tal Linzen", "Marco Baroni." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Gulordava et al\\.,? 2018",
      "shortCiteRegEx" : "Gulordava et al\\.",
      "year" : 2018
    }, {
      "title" : "What is implicit causality? Language, Cognition and Neuroscience, 29(7):804–824",
      "author" : [ "Joshua K. Hartshorne" ],
      "venue" : null,
      "citeRegEx" : "Hartshorne.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hartshorne.",
      "year" : 2014
    }, {
      "title" : "Are implicit causality pronoun resolution biases consistent across languages and cultures",
      "author" : [ "Joshua K. Hartshorne", "Yasutada Sudo", "Miki Uruwashi" ],
      "venue" : null,
      "citeRegEx" : "Hartshorne et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hartshorne et al\\.",
      "year" : 2013
    }, {
      "title" : "Designing and Interpreting Probes with Control Tasks",
      "author" : [ "John Hewitt", "Percy Liang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro-",
      "citeRegEx" : "Hewitt and Liang.,? 2019",
      "shortCiteRegEx" : "Hewitt and Liang.",
      "year" : 2019
    }, {
      "title" : "A Structural Probe for Finding Syntax in Word Representations",
      "author" : [ "John Hewitt", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Hewitt and Manning.,? 2019",
      "shortCiteRegEx" : "Hewitt and Manning.",
      "year" : 2019
    }, {
      "title" : "A Systematic Assessment of Syntactic Generalization in Neural Language Models",
      "author" : [ "Jennifer Hu", "Jon Gauthier", "Peng Qian", "Ethan Wilcox", "Roger Levy." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "On the Distribution and Reference of Empty Pronouns",
      "author" : [ "C.-T. James Huang." ],
      "venue" : "Linguistic Inquiry, 15(4):531–574.",
      "citeRegEx" : "Huang.,? 1984",
      "shortCiteRegEx" : "Huang.",
      "year" : 1984
    }, {
      "title" : "Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature and PRESupposition",
      "author" : [ "Paloma Jeretic", "Alex Warstadt", "Suvrat Bhooshan", "Adina Williams." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Jeretic et al\\.,? 2020",
      "shortCiteRegEx" : "Jeretic et al\\.",
      "year" : 2020
    }, {
      "title" : "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies",
      "author" : [ "Tal Linzen", "Emmanuel Dupoux", "Yoav Goldberg." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4(0):521–535.",
      "citeRegEx" : "Linzen et al\\.,? 2016",
      "shortCiteRegEx" : "Linzen et al\\.",
      "year" : 2016
    }, {
      "title" : "A Modular Theory of Radical Pro Drop",
      "author" : [ "Chi-Ming Louis Liu." ],
      "venue" : "Ph.D., Harvard University.",
      "citeRegEx" : "Liu.,? 2014",
      "shortCiteRegEx" : "Liu.",
      "year" : 2014
    }, {
      "title" : "Interpersonal verbs: Implicit causality of action verbs and contextual factors: Implicit causality of action verbs",
      "author" : [ "L. Mannetti", "E. De Grada." ],
      "venue" : "European Journal of Social Psychology, 21(5):429–443.",
      "citeRegEx" : "Mannetti and Grada.,? 1991",
      "shortCiteRegEx" : "Mannetti and Grada.",
      "year" : 1991
    }, {
      "title" : "Targeted Syntactic Evaluation of Language Models",
      "author" : [ "Rebecca Marvin", "Tal Linzen." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1192–1202. Association for Computational Linguistics.",
      "citeRegEx" : "Marvin and Linzen.,? 2018",
      "shortCiteRegEx" : "Marvin and Linzen.",
      "year" : 2018
    }, {
      "title" : "Revisiting the poverty of the stimulus: Hierarchical generalization without a hierarchical bias in recurrent neural networks",
      "author" : [ "R Thomas McCoy", "Robert Frank", "Tal Linzen." ],
      "venue" : "Proceedings of the 40th Annual Virtual Meeting of the Cognitive Science Soci-",
      "citeRegEx" : "McCoy et al\\.,? 2018",
      "shortCiteRegEx" : "McCoy et al\\.",
      "year" : 2018
    }, {
      "title" : "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
      "author" : [ "Tom McCoy", "Ellie Pavlick", "Tal Linzen." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428–",
      "citeRegEx" : "McCoy et al\\.,? 2019",
      "shortCiteRegEx" : "McCoy et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring BERT’s Sensitivity to Lexical Cues using Tests from Semantic Priming",
      "author" : [ "Kanishka Misra", "Allyson Ettinger", "Julia Rayz." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4625–4635, Online. Associa-",
      "citeRegEx" : "Misra et al\\.,? 2020",
      "shortCiteRegEx" : "Misra et al\\.",
      "year" : 2020
    }, {
      "title" : "Cross-Linguistic Syntactic Evaluation of Word Prediction Models",
      "author" : [ "Aaron Mueller", "Garrett Nicolai", "Panayiota PetrouZeniou", "Natalia Talmina", "Tal Linzen." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Mueller et al\\.,? 2020",
      "shortCiteRegEx" : "Mueller et al\\.",
      "year" : 2020
    }, {
      "title" : "Implicit Causality: A Comparison of English and Vietnamese Verbs",
      "author" : [ "Binh Ngo", "Elsi Kaiser." ],
      "venue" : "University of Pennsylvania Working Papers in Linguistics, 26.",
      "citeRegEx" : "Ngo and Kaiser.,? 2020",
      "shortCiteRegEx" : "Ngo and Kaiser.",
      "year" : 2020
    }, {
      "title" : "UmBERTo: An Italian Language Model trained with Whole Word Masking",
      "author" : [ "Loreto Parisi", "Simone Francia", "Paolo Magnani" ],
      "venue" : null,
      "citeRegEx" : "Parisi et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Parisi et al\\.",
      "year" : 2020
    }, {
      "title" : "Using Priming to Uncover the Organization of Syntactic Representations in Neural Language Models",
      "author" : [ "Grusha Prasad", "Marten van Schijndel", "Tal Linzen." ],
      "venue" : "Proceedings of the 23rd Conference on Computational Natural Language Learning",
      "citeRegEx" : "Prasad et al\\.,? 2019",
      "shortCiteRegEx" : "Prasad et al\\.",
      "year" : 2019
    }, {
      "title" : "Optimality Theory: Constraint Interaction in Generative Grammar",
      "author" : [ "Alan Prince", "Paul Smolensky." ],
      "venue" : "Blackwell Pub., Malden, MA.",
      "citeRegEx" : "Prince and Smolensky.,? 2004",
      "shortCiteRegEx" : "Prince and Smolensky.",
      "year" : 2004
    }, {
      "title" : "GilBERTo: An Italian pretrained language model based on RoBERTa",
      "author" : [ "Giulio Ravasio", "Leonardo Di Perna" ],
      "venue" : null,
      "citeRegEx" : "Ravasio and Perna.,? \\Q2020\\E",
      "shortCiteRegEx" : "Ravasio and Perna.",
      "year" : 2020
    }, {
      "title" : "Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection",
      "author" : [ "Shauli Ravfogel", "Yanai Elazar", "Hila Gonen", "Michael Twiton", "Yoav Goldberg." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Ravfogel et al\\.,? 2020",
      "shortCiteRegEx" : "Ravfogel et al\\.",
      "year" : 2020
    }, {
      "title" : "Studying the Inductive Biases of RNNs with Synthetic Variations of Natural Languages",
      "author" : [ "Shauli Ravfogel", "Yoav Goldberg", "Tal Linzen." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Ravfogel et al\\.,? 2019",
      "shortCiteRegEx" : "Ravfogel et al\\.",
      "year" : 2019
    }, {
      "title" : "Can LSTM Learn to Capture Agreement? The Case of Basque",
      "author" : [ "Shauli Ravfogel", "Yoav Goldberg", "Francis Tyers." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 98–107.",
      "citeRegEx" : "Ravfogel et al\\.,? 2018",
      "shortCiteRegEx" : "Ravfogel et al\\.",
      "year" : 2018
    }, {
      "title" : "Null Objects in Italian and the Theory of pro",
      "author" : [ "Luigi Rizzi." ],
      "venue" : "Linguistic Inquiry, 17(3):501–557.",
      "citeRegEx" : "Rizzi.,? 1986",
      "shortCiteRegEx" : "Rizzi.",
      "year" : 1986
    }, {
      "title" : "Anticipating explanations in relative clause processing",
      "author" : [ "H. Rohde", "R. Levy", "A. Kehler." ],
      "venue" : "Cognition, 118(3):339–358.",
      "citeRegEx" : "Rohde et al\\.,? 2011",
      "shortCiteRegEx" : "Rohde et al\\.",
      "year" : 2011
    }, {
      "title" : "RuPERTa: The Spanish RoBERTa",
      "author" : [ "Manuel Romero" ],
      "venue" : null,
      "citeRegEx" : "Romero.,? \\Q2020\\E",
      "shortCiteRegEx" : "Romero.",
      "year" : 2020
    }, {
      "title" : "Artificial Neural Networks Accurately Predict Language Processing in the Brain",
      "author" : [ "Martin Schrimpf", "Idan Blank", "Greta Tuckute", "Carina Kauf", "Eghbal A. Hosseini", "Nancy Kanwisher", "Joshua Tenenbaum", "Evelina Fedorenko." ],
      "venue" : "bioRxiv, page",
      "citeRegEx" : "Schrimpf et al\\.,? 2020",
      "shortCiteRegEx" : "Schrimpf et al\\.",
      "year" : 2020
    }, {
      "title" : "Harnessing the linguistic signal to predict scalar inferences",
      "author" : [ "Sebastian Schuster", "Yuxing Chen", "Judith Degen." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5387–5403, Online. Association for",
      "citeRegEx" : "Schuster et al\\.,? 2020",
      "shortCiteRegEx" : "Schuster et al\\.",
      "year" : 2020
    }, {
      "title" : "A gold standard dependency corpus for English",
      "author" : [ "Natalia Silveira", "Timothy Dozat", "Marie-Catherine de Marneffe", "Samuel Bowman", "Miriam Connor", "John Bauer", "Christopher D. Manning." ],
      "venue" : "Proceedings of the Ninth International Conference",
      "citeRegEx" : "Silveira et al\\.,? 2014",
      "shortCiteRegEx" : "Silveira et al\\.",
      "year" : 2014
    }, {
      "title" : "AnCora: Multilevel annotated corpora for catalan and spanish",
      "author" : [ "Mariona Taulé", "M. Antònia Martı", "Marta Recasens" ],
      "venue" : "In Proceedings of the Sixth International Conference on Language Resources and Evaluation",
      "citeRegEx" : "Taulé et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Taulé et al\\.",
      "year" : 2008
    }, {
      "title" : "Predicting Reference: What do Language Models Learn about Discourse Models",
      "author" : [ "Shiva Upadhye", "Leon Bergen", "Andrew Kehler" ],
      "venue" : "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Upadhye et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Upadhye et al\\.",
      "year" : 2020
    }, {
      "title" : "BLiMP: The Benchmark of Linguistic Minimal Pairs for English",
      "author" : [ "Alex Warstadt", "Alicia Parrish", "Haokun Liu", "Anhad Mohananey", "Wei Peng", "Sheng-Fu Wang", "Samuel R. Bowman." ],
      "venue" : "Transactions of the Association for Computational Linguistics,",
      "citeRegEx" : "Warstadt et al\\.,? 2020a",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning Which Features Matter: RoBERTa Acquires a Preference for Linguistic Generalizations (Eventually)",
      "author" : [ "Alex Warstadt", "Yian Zhang", "Xiaocheng Li", "Haokun Liu", "Samuel R. Bowman." ],
      "venue" : "Proceedings of the 2020 Conference on Empiri-",
      "citeRegEx" : "Warstadt et al\\.,? 2020b",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2020
    }, {
      "title" : "Language Experience Predicts Pronoun Comprehension in Implicit Causality Sentences",
      "author" : [ "Elyce Dominique Williams." ],
      "venue" : "Master’s, University of North Carolina at Chapel Hill.",
      "citeRegEx" : "Williams.,? 2020",
      "shortCiteRegEx" : "Williams.",
      "year" : 2020
    }, {
      "title" : "Transformers: State-of-the-Art Natural Language Processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "One common approach for understanding why these models are successful is centered on inferring what linguistic knowledge such models acquire (e.g., Linzen et al., 2016; Hewitt and Manning, 2019; Hu et al., 2020; Warstadt et al., 2020a).",
      "startOffset" : 141,
      "endOffset" : 235
    }, {
      "referenceID" : 22,
      "context" : "One common approach for understanding why these models are successful is centered on inferring what linguistic knowledge such models acquire (e.g., Linzen et al., 2016; Hewitt and Manning, 2019; Hu et al., 2020; Warstadt et al., 2020a).",
      "startOffset" : 141,
      "endOffset" : 235
    }, {
      "referenceID" : 49,
      "context" : "One common approach for understanding why these models are successful is centered on inferring what linguistic knowledge such models acquire (e.g., Linzen et al., 2016; Hewitt and Manning, 2019; Hu et al., 2020; Warstadt et al., 2020a).",
      "startOffset" : 141,
      "endOffset" : 235
    }, {
      "referenceID" : 50,
      "context" : "Linguistic knowledge alone, of course, does not fully account for model behavior; non-linguistic heuristics have also been shown to drive model behavior (e.g., sentence length; see McCoy et al., 2019; Warstadt et al., 2020b).",
      "startOffset" : 153,
      "endOffset" : 224
    }, {
      "referenceID" : 40,
      "context" : "ing prominence of work solely on English (though see Gulordava et al., 2018; Ravfogel et al., 2018; Mueller et al., 2020).",
      "startOffset" : 41,
      "endOffset" : 121
    }, {
      "referenceID" : 32,
      "context" : "ing prominence of work solely on English (though see Gulordava et al., 2018; Ravfogel et al., 2018; Mueller et al., 2020).",
      "startOffset" : 41,
      "endOffset" : 121
    }, {
      "referenceID" : 18,
      "context" : "IC has been a rich source of psycholinguistic investigation (e.g., Garvey and Caramazza, 1974; Hartshorne, 2014; Williams, 2020).",
      "startOffset" : 60,
      "endOffset" : 128
    }, {
      "referenceID" : 51,
      "context" : "IC has been a rich source of psycholinguistic investigation (e.g., Garvey and Caramazza, 1974; Hartshorne, 2014; Williams, 2020).",
      "startOffset" : 60,
      "endOffset" : 128
    }, {
      "referenceID" : 48,
      "context" : "investigations of IC in neural language models confirms that the IC bias of English is learnable, at least to some degree, from text data alone (Davis and van Schijndel, 2020a; Upadhye et al., 2020).",
      "startOffset" : 144,
      "endOffset" : 198
    }, {
      "referenceID" : 33,
      "context" : "Within the psycholinguistic literature, IC has been shown to be remarkably consistent crosslinguistically (see Hartshorne et al., 2013; Ngo and Kaiser, 2020).",
      "startOffset" : 106,
      "endOffset" : 157
    }, {
      "referenceID" : 9,
      "context" : "However, using two popular model types, BERT based (Devlin et al., 2019) and RoBERTa based (Liu et al.",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 41,
      "context" : "Namely, Spanish and Italian have a well studied process called pro drop, which allows for subjects to be ‘empty’ (Rizzi, 1986).",
      "startOffset" : 113,
      "endOffset" : 126
    }, {
      "referenceID" : 32,
      "context" : "Largely, this literature articulates model knowledge via isolated linguistic phenomena, such as subject-verb agreement (e.g., Linzen et al., 2016; Mueller et al., 2020),",
      "startOffset" : 119,
      "endOffset" : 168
    }, {
      "referenceID" : 45,
      "context" : ", 2019), and discourse and pragmatic structure (including implicit causality; e.g., Ettinger, 2020; Schuster et al., 2020; Jeretic et al., 2020; Upadhye et al., 2020).",
      "startOffset" : 47,
      "endOffset" : 166
    }, {
      "referenceID" : 24,
      "context" : ", 2019), and discourse and pragmatic structure (including implicit causality; e.g., Ettinger, 2020; Schuster et al., 2020; Jeretic et al., 2020; Upadhye et al., 2020).",
      "startOffset" : 47,
      "endOffset" : 166
    }, {
      "referenceID" : 48,
      "context" : ", 2019), and discourse and pragmatic structure (including implicit causality; e.g., Ettinger, 2020; Schuster et al., 2020; Jeretic et al., 2020; Upadhye et al., 2020).",
      "startOffset" : 47,
      "endOffset" : 166
    }, {
      "referenceID" : 39,
      "context" : "In the construction of our experiments, we were inspired by synthetic language studies which probe the underlying linguistic capabilities of language models (e.g., McCoy et al., 2018; Ravfogel et al., 2019).",
      "startOffset" : 157,
      "endOffset" : 206
    }, {
      "referenceID" : 11,
      "context" : "The goal of such modification in our work is quite similar both to work which attempts to remove targeted linguistic knowledge in model representations (e.g., Ravfogel et al., 2020; Elazar et al., 2021) and to work which",
      "startOffset" : 152,
      "endOffset" : 202
    }, {
      "referenceID" : 35,
      "context" : "investigates the representational space of models via priming (Prasad et al., 2019; Misra et al., 2020).",
      "startOffset" : 62,
      "endOffset" : 103
    }, {
      "referenceID" : 31,
      "context" : "investigates the representational space of models via priming (Prasad et al., 2019; Misra et al., 2020).",
      "startOffset" : 62,
      "endOffset" : 103
    }, {
      "referenceID" : 9,
      "context" : "In the present study, we focused on two popular non-autoregressive language model variants, BERT (Devlin et al., 2019) and RoBERTa (Liu",
      "startOffset" : 97,
      "endOffset" : 118
    }, {
      "referenceID" : 4,
      "context" : "For Spanish, we used BETO (Cañete et al., 2020) and RuPERTa (Romero, 2020).",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 34,
      "context" : "For Italian, we evaluated an uncased Italian BERT 3 as well as two RoBERTa based models, UmBERTo (Parisi et al., 2020) and GilBERTo (Ravasio and Di Perna, 2020).",
      "startOffset" : 97,
      "endOffset" : 118
    }, {
      "referenceID" : 36,
      "context" : "Languages can be thought of as systems of competing linguistic constraints (e.g., Optimality Theory; Prince and Smolensky, 2004).",
      "startOffset" : 75,
      "endOffset" : 128
    }, {
      "referenceID" : 41,
      "context" : "rely on rich agreement systems to disambiguate the intended subject at the verb (Rizzi, 1986).",
      "startOffset" : 80,
      "endOffset" : 93
    }, {
      "referenceID" : 23,
      "context" : "and objects), typically called discourse pro-drop (Huang, 1984).",
      "startOffset" : 50,
      "endOffset" : 63
    }, {
      "referenceID" : 47,
      "context" : "For Spanish, we used the AnCora Spanish newswire corpus (Taulé et al., 2008), for Italian we used ISDT (Bosco et al.",
      "startOffset" : 56,
      "endOffset" : 76
    }, {
      "referenceID" : 2,
      "context" : ", 2008), for Italian we used ISDT (Bosco et al., 2013) and VIT (Delmonte et al.",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 8,
      "context" : ", 2013) and VIT (Delmonte et al., 2007), for English we used the English Web Treebank (Silveira et al.",
      "startOffset" : 16,
      "endOffset" : 39
    }, {
      "referenceID" : 46,
      "context" : ", 2007), for English we used the English Web Treebank (Silveira et al., 2014),",
      "startOffset" : 54,
      "endOffset" : 77
    }, {
      "referenceID" : 9,
      "context" : "Each model was finetuned with a masked language modeling objective for 3 epochs with a learning rate of 5e-5, following the fine-tuning details in (Devlin et al., 2019).",
      "startOffset" : 147,
      "endOffset" : 168
    }, {
      "referenceID" : 1,
      "context" : "linguistic representation, including the inability to learn mappings between form and meaning and the lack of embodiment (e.g., Bender and Koller, 2020; Bisk et al., 2020).",
      "startOffset" : 121,
      "endOffset" : 171
    }, {
      "referenceID" : 20,
      "context" : "Possible future directions include pairing our behavioral analyses with representational probing in order to more explicitly link model representations and model behavior (e.g., Ettinger et al., 2016; Hewitt and Liang, 2019) or exploring constraint competition in different models, like GPT-2 which has received considerable attention for its apparent linguistic behavior (e.",
      "startOffset" : 171,
      "endOffset" : 224
    } ],
    "year" : 2021,
    "abstractText" : "A growing body of literature has focused on detailing the linguistic knowledge embedded in large, pretrained language models. Existing work has shown that non-linguistic biases in models can drive model behavior away from linguistic generalizations. We hypothesized that competing linguistic processes within a language, rather than just non-linguistic model biases, could obscure underlying linguistic knowledge. We tested this claim by exploring a single phenomenon in four languages: English, Chinese, Spanish, and Italian. While human behavior has been found to be similar across languages, we find cross-linguistic variation in model behavior. We show that competing processes in a language act as constraints on model behavior and demonstrate that targeted fine-tuning can re-weight the learned constraints, uncovering otherwise dormant linguistic knowledge in models. Our results suggest that models need to learn both the linguistic constraints in a language and their relative ranking, with mismatches in either producing non-human-like behavior.",
    "creator" : "LaTeX with hyperref"
  }
}