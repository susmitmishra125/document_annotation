{
  "name" : "2021.acl-long.356.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "TWAG: A Topic-guided Wikipedia Abstract Generator",
    "authors" : [ "Fangwei Zhu", "Shangqing Tu", "Jiaxin Shi", "Juanzi Li", "Lei Hou", "Tong Cui" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4623–4635\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4623"
    }, {
      "heading" : "1 Introduction",
      "text" : "Wikipedia, one of the most popular crowd-sourced online knowledge bases, has been widely used as the valuable resources in natural language processing tasks such as knowledge acquisition (Lehmann et al., 2015) and question answering (Hewlett et al., 2016; Rajpurkar et al., 2016) due to its high quality and wide coverage. Within a Wikipedia article, its abstract is the overview of the whole content, and thus becomes the most frequently used part in various tasks. However, the abstract is often contributed by experts, which is labor-intensive and prone to be incomplete.\nIn this paper, we aim to automatically generate Wikipedia abstracts based on the related documents\n∗ Corresponding Author\ncollected from referred websites or search engines, which is essentially a multi-document summarization problem. This problem is studied in both extractive and abstractive manners.\nThe extractive models attempt to select relevant textual units from input documents and combine them into a summary. Graph-based representations are widely exploited to capture the most salient textual units and enhance the quality of the final summary (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Wan, 2008). Recently, there also emerge neural extractive models (Yasunaga et al., 2017; Yin et al., 2019) utilizing the graph convolutional network (Kipf and Welling, 2017) to better capture inter-document relations. However, these models are not suitable for Wikipedia abstract generation. The reason is that the input documents collected from various sources are often noisy and lack intrinsic relations (Sauper and Barzilay, 2009), which makes the relation graph hard to build.\nThe abstractive models aim to distill an informative and coherent summary via sentence-fusion and paraphrasing (Filippova and Strube, 2008; Banerjee et al., 2015; Bing et al., 2015), but achieve little success due to the limited scale of datasets. Liu et al. (2018) proposes an extractive-then-abstractive model and contributes WikiSum, a large-scale dataset for Wikipedia abstract generation, inspiring a branch of further studies (Perez-Beltrachini et al., 2019; Liu and Lapata, 2019; Li et al., 2020).\nThe above models generally view the abstract as plain text, ignoring the fact that Wikipedia abstracts describe certain entities, and the structure of Wikipedia articles could help generate comprehensive abstracts. We observe that humans tend to describe entities in a certain domain from several topics when writing Wikipedia abstracts. As illustrated in Figure 1, the abstract of the Arctic Fox contains its adaption, biology taxonomy and geographical distribution, which is consistent with\nthe content table. Therefore, given an entity in a specific domain, generating abstracts from corresponding topics would reduce redundancy and produce a more complete summary.\nIn this paper, we try to utilize the topical information of entities within its domain (Wikipedia categories) to improve the quality of the generated abstract. We propose a novel two-stage Topic-guided Wikipedia Abstract Generation model (TWAG). TWAG first divides input documents by paragraph and assigns a topic for each paragraph with a classifier-based topic detector. Then, it generates the abstract in a sentence-wise manner, i.e., predicts the topic distribution of each abstract sentence to determine its topic-aware representation, and decodes the sentence with a Pointer-Generator network (See et al., 2017).\nWe evaluate TWAG on the WikiCatSum (PerezBeltrachini et al., 2019) dataset, a subset of the WikiSum containing three distinct domains. Experimental results show that it significantly improves the quality of abstract compared with several strong baselines.\nIn conclusion, the contributions of our work are as follows:\n• We propose TWAG, a two-stage neural abstractive Wikipedia abstract generation model utilizing the topic information in Wikipedia, which is capable of generating comprehensive abstracts.\n• We simulate the way humans recognize entities, using a classifier to divide input documents into topics, and then perform topic-\naware abstract generation upon the predicted topic distribution of each abstract sentence.\n• Our experiment results against 4 distinct baselines prove the effectiveness of TWAG."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Multi-document Summarization",
      "text" : "Multi-document summarization is a classic and challenging problem in natural language processing, which aims to distill an informative and coherent summary from a set of input documents. Compared with single-document summarization, the input documents may contain redundant or even contradictory information (Radev, 2000).\nEarly high-quality multi-document summarization datasets are annotated by humans, e.g., datasets for Document Understanding Conference (DUC) and Text Analysis Conference (TAC). These datasets are too small to build neural models, and most of the early works take an extractive method, attempting to build graphs with interparagraph relations and choose the most salient textual units. The graph could be built with various information, e.g., TF-IDF similarity (Erkan and Radev, 2004), discourse relation (Mihalcea and Tarau, 2004), document-sentence two-layer relations (Wan, 2008), multi-modal (Wan and Xiao, 2009) and query information (Cai and Li, 2012). Recently, there emerge attempts to incorporate neural models, e.g., Yasunaga et al. (2017) builds a discourse graph and represents textual units upon the graph convolutional network (GCN) (Kipf and Welling, 2017), and Yin et al. (2019) adopts the entity linking technique to capture global dependencies between sentences and ranks the sentences with a neural graph-based model.\nIn contrast, early abstractive models using sentence-fusion and paraphrasing (Filippova and Strube, 2008; Banerjee et al., 2015; Bing et al., 2015) achieve less success. Inspired by the recent success of single-document abstractive models (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Huang et al., 2020), some works (Liu et al., 2018; Zhang et al., 2018) try to transfer single-document models to multi-document settings to alleviate the limitations of small-scale datasets. Specifically, Liu et al. (2018) defines Wikipedia generation problem and contributes the large-scale WikiSum dataset. Fabbri et al. (2019) constructs a middle-scale dataset named Multi-\nNews and proposes an extractive-then-abstractive model by appending a sequence-to-sequence model after the extractive step. Li et al. (2020) models inter-document relations with explicit graph representations, and incorporates pre-trained language models to better handle long input documents."
    }, {
      "heading" : "2.2 Wikipedia-related Text Generation",
      "text" : "Sauper and Barzilay (2009) is the first work focusing on Wikipedia generation, which uses Integer Linear Programming (ILP) to select the useful sentences for Wikipedia abstracts. Banerjee and Mitra (2016) further evaluates the coherence of selected sentences to improve the linguistic quality.\nLiu et al. (2018) proposes a two-stage extractivethen-abstractive model, which first picks paragraphs according to TF-IDF weights from web sources, then generates the summary with a transformer model by viewing the input as a long flat sequence. Inspired by this work, Perez-Beltrachini et al. (2019) uses a convolutional encoder and a hierarchical decoder, and utilizes the Latent Dirichlet Allocation model (LDA) to render the decoder topic-aware. HierSumm (Liu and Lapata, 2019) adopts a learning-based model for the extractive stage, and computes the attention between paragraphs to model the dependencies across multiple paragraphs. However, these works view Wikipedia abstracts as plain text and do not explore the underlying topical information in Wikipedia articles.\nThere are also works that focus on generating other aspects of Wikipedia text. Biadsy et al. (2008) utilizes the key-value pairs in Wikipedia infoboxes to generate high-quality biographies. Hayashi et al. (2021) investigates the structure of Wikipedia and builds an aspect-based summarization dataset by manually labeling aspects and identifying the aspect of input paragraphs with a finetuned RoBERTa model (Liu et al., 2019). Our model also utilizes the structure of Wikipedia, but we generate the compact abstract rather than individual aspects, which requires the fusion of aspects and poses a greater challenge to understand the connection and difference among topics."
    }, {
      "heading" : "3 Problem Definition",
      "text" : "Definition 1 Wikipedia abstract generation accepts a set of paragraphs1 D = {d1, d2, . . . , dn}\n1The input documents can be represented by textual units with different granularity, and we choose paragraph as it normally expresses relatively complete and compact semantics.\nof size n as input, and outputs a Wikipedia abstract S = (s1, s2, . . . , sm) with m sentences. The goal is to find an optimal abstract S∗ that best concludes the input, i.e.,\nS∗ = argmax S P (S|D) (1)\nPrevious works generally view S as plain text, ignoring the semantics in Wikipedia articles. Before introducing our idea, let’s review how Wikipedia organizes articles.\nWikipedia employs a hierarchical open category system to organize millions of articles, and we name the top-level category as domain. As for a Wikipedia article, we concern three parts, i.e., the abstract, the content table, and textual contents. Note that the content table is composed of several section labels {l}, pairing with corresponding textual contents {p}. As illustrated in Figure 1, the content table indicates different aspects (we call them topics) of the article, and the abstract semantically corresponds to these topics, telling us that topics could benefit the abstract generation.\nHowever, general domains like Person or Animal consist millions of articles with diverse content tables, making it not feasible to simply treat section labels as topics. Considering that articles in specific domains often share several salient topics, we manually merge similar section labels to convert the sections titles to a set of topics. Formally, the topic set is denoted as T = {T1, T2, ..., Tnt} of size nt, where each topic Ti = {l1i , l2i , . . . , lmi }.\nNow, our task can be expressed with a topical objective, i.e.,\nDefinition 2 Given the input paragraphs D, we introduce the latent topics Z = {z1, z2, . . . , zn}, where zi ∈ T is the topic of i-th input paragraph di, and our objective of Wikipedia abstract generation is re-written as\nS∗ = argmax Z P (Z|D) argmax S P (S|D,Z). (2)\nTherefore, the abstract generation could be completed with two sub-tasks, i.e., topic detection to optimize argmaxZ P (Z|D) and topic-aware abstract generation to optimize argmaxS P (S|D,Z)."
    }, {
      "heading" : "4 The Proposed Method",
      "text" : "As shown in Figure 2, our proposed TWAG adopts a two-stage structure. First, we train a topic detector based on existing Wikipedia articles to predict the topic of input paragraphs. Second, we group the\ninput paragraphs by detected topics to encode them separately, and generate the abstract in a sentencewise manner. In each step, we predict the topic distribution of the current sentence, fuse it with the global hidden state to get the topic-aware representation, and generate the sentence with a copy-based decoder. Next, we will detail each module."
    }, {
      "heading" : "4.1 Topic Detection",
      "text" : "The topic detector aims to annotate input paragraphs with their optimal corresponding topics. To formalize, given the input paragraphs D, Det returns its corresponding topics Z = {z1, z2, . . . , zn}, i.e.,\nZ = Det(D) (3)\nWe view topic detection as a classification problem. For each paragraph d ∈ D, we encode it with ALBERT(Lan et al., 2019) and then predict its topic z with a fully-connected layer, i.e.,\nd = ALBERT(d) (4)\nz = argmax(linear(d)) (5)\nwhere d is the vector representation of d, and we fine-tuned the ALBERT model on a pretrained version."
    }, {
      "heading" : "4.2 Topic-aware Abstract Generation",
      "text" : "Topic-aware abstract generator utilizes the input paragraphs D and the detected topics Z to generate\nthe abstract. Specifically, it contains three modules: a topic encoder to encode the input paragraphs into topical representations, a topic predictor to predict the topic distribution of abstract sentences and generate the topic-aware sentence representation, and a sentence decoder to generate abstract sentences based on the topic-aware representations."
    }, {
      "heading" : "4.2.1 Topic Encoder",
      "text" : "Given the input paragraphs D and the detected topics Z , we concatenate all paragraphs belonging to the same topic Tk to form a topic-specific text group (TTG) Gk, which contains salient information about a certain topic of an entity:\nGk = concat({di|zi = Tk}). (6)\nTo further capture hidden semantics, we use a bidirectional GRU to encode the TTGs:\ngk,Uk = BiGRU(Gk). (7)\ngk is the final hidden state of the Gk, and Uk = (u1,u2, . . . ,unGk ) represents the hidden state of each token in Gk, where nGk denotes the number of tokens in Gk."
    }, {
      "heading" : "4.2.2 Topic Predictor",
      "text" : "After encoding the topics into hidden states, TWAG tackles the decoding process in a sentence-wise manner:\nargmax S\nP (S|D,Z) = m∏\ni=1\nargmax si\nP (si|D,Z, s<i) (8)\nTo generate the abstract S, we first predict the topic distribution of every sentence si with a GRU decoder. At each time step t, the topic predictor produces a global hidden state ht, and then estimates the probability distribution qt over topics.\nht = GRU(ht−1, et−1) (9) qt = softmax(linear(ht)) (10)\nwhere et−1 denotes the topical information in the last step. e0 is initialized as an all-zero vector, and et could be derived from qt in two ways.\nThe first way named hard topic, is to directly select the topic with the highest probability, and take its corresponding representation, i.e.,\nehardt = gargmaxi(qi). (11)\nThe second way named soft topic, is to view every sentence as a mixture of different topics, and take the weighted sum over topic representations, i.e.,\nesoftt = qt ·G (12)\nwhere G = (g1,g2, . . . ,gnt) is the matrix of topic representations. With the observation that Wikipedia abstract sentences normally contain mixed topics, we choose the soft topic mechanism for our model (see Section 5.3 for details).\nFinally, we compute the topic-aware hidden state rt by adding up ht and et, which serves as the initial hidden state of sentence decoder:\nrt = ht + et (13)\nAdditionally, a stop confirmation is executed at each time step:\npstop = σ(linear(ht)) (14)\nwhere σ represents the sigmoid function. If pstop > 0.5, TWAG will terminate the decoding process and no more abstract sentences will be generated."
    }, {
      "heading" : "4.2.3 Sentence Decoder",
      "text" : "Our sentence decoder adopts the Pointer-Generator network (See et al., 2017), which picks tokens both from input paragraphs and vocabulary.\nTo copy a token from the input paragraphs, the decoder requires the token-wise hidden states U = (u1,u2, . . . ,unu) of all nu input tokens, which is obtained by concatenating the token-wise hidden states of all TTGs, i.e.,\nU = [U1,U2, . . . ,Unu ] (15)\nFor the k-th token, the decoder computes an attention distribution ak over tokens in the input paragraphs, where each element aik could be viewed as the probability of the i-th token being selected,\naik = softmax(tanh(Wuui+Wssk+ba)) (16)\nwhere sk denotes the decoder hidden state with s0 = rt to incorporate the topic-aware representation, and Wu,Ws,ba are trainable parameters.\nTo generate a token from the vocabulary, we first use the attention mechanism to calculate the weighted sum of encoder hidden states, known as the context vector,\nc∗k = ∑ i aikui. (17)\nwhich is further fed into a two-layer network to obtain the probability distribution over vocabulary,\nPvoc = softmax(linear(linear([sk, c∗k]))). (18)\nTo switch between these two mechanisms, pgen is computed from context vector c∗k, decoder hidden state sk and decoder input xk:\npgen = σ(W T c c ∗ k +W T s sk +W T x xk +bp) (19)\nwhere σ represents the sigmoid function and WTc ,W T s ,W T x and bp are trainable parameters. The final probability distribution of words is2\nP (w) = pgenPvoc(w) + (1− pgen) ∑\ni:|wwi=w aik (20)"
    }, {
      "heading" : "4.3 Training",
      "text" : "The modules for topic detection and abstract generation are trained separately."
    }, {
      "heading" : "4.3.1 Topic Detector Training",
      "text" : "Since there are no public benchmarks for assigning input paragraphs with Wikipedia topics, we construct the dataset with existing Wikipedia articles. In each domain, we collect all the label-content pairs {(l, p)} (defined in Section 3), and split the content into paragraphs p = (d1, d2, . . . , dnp) to form a set of label-paragraph pairs {(l, d)}. Afterwards, we choose all pairs (l, d) whose section label l belongs to a particular topic T ∈ T to complete the dataset construction, i.e., the topicparagraph set {(T, d)}. Besides, a NOISE topic is\n2wwi means the token corresponding to ui.\nset up in each domain, which refers to meaningless text like scripts and advertisements, and the corresponding paragraphs are obtained by utilizing regular expressions to match obvious noisy texts. The details are reported in Appendix A.\nNote that the dataset for abstract generation is collected from non-Wikipedia websites (refer to Section 5 for details). These two datasets are independent of each other, which prevents potential data leakage.\nIn the training step, we use the negative loglikelihood loss to optimize the topic detector."
    }, {
      "heading" : "4.3.2 Abstract Generator Training",
      "text" : "The loss of topic-aware abstract generation step consists of two parts: the first part is the average loss of sentence decoder for each abstract sentence Lsent, and the second part is the cross-entropy loss of stop confirmation Lstop.\nFollowing (See et al., 2017), we compute the loss of an abstract sentence by averaging the negative log likelihood of every target word in that sentence, and achieve Lsent via averaging over all m sentences,\nLsent = 1\nm m∑ t=1\n( 1\nnst nst∑ i=1\n− logP (wi) ) (21)\nwhere nst is the length of the t-th sentence of the abstract. As for Lstop, we adopt the cross-entropy loss, i.e.,\nLstop = −ys log(pstop)− (1− ys) log(1− pstop) (22)\nwhere ys = 1 when t > m and ys = 0 otherwise."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Experimental Settings",
      "text" : "Dataset. To evaluate the overall performance of our model, we use the WikiCatSum dataset proposed by (Perez-Beltrachini et al., 2019), which contains three distinct domains (Company, Film and Animal) in Wikipedia. Each domain is split into train (90%), validation (5%) and test (5%) set.\nWe build the dataset for training and evaluating the topic detector from the 2019-07-01 English Wikipedia full dump. For each record in the WikiCatSum dataset, we find the article with the same title in Wikipedia dump, and pick all section labelcontent pairs {(l, p)} in that article. We remove all hyperlinks and graphics in contents, split the contents into paragraphs with the spaCy library,\nand follow the steps in Section 4.3.1 to complete dataset construction. Finally, we conduct an 8:1:1 split for train, validation and test.\nTable 1 presents the detailed parameters of used datasets.\nEvaluation Metrics. We evaluate the performance of our model with ROUGE scores (Lin, 2004), which is a common metric in comparing generated and standard summaries. Considering that we do not constrain the length of generated abstracts, we choose ROUGE F1 score that combines precision and recall to eliminate the tendency of favoring long or short results.\nImplementation Details. We use the opensource PyTorch and transformers library to implement our model. All models are trained on NVIDIA GeForce RTX 2080.\nIn topic detection, we choose the top 20 frequent section labels in each domain and manually group them into different topics (refer to the Appendix A for details). For training, we use the pretrained albert-base-v2 model in the transformers library, keep its default parameters and train the module for 4 epochs with a learning rate of 3e-5.\nFor abstract generation, we use a single-layer BiGRU network to encode the TTGs into hidden states of 512 dimensions. The first 400 tokens of input paragraphs are retained and transformed into GloVe (Pennington et al., 2014) embedding of 300 dimensions. The vocabulary size is 50000 and out-of-vocabulary tokens are represented with the average embedding of its adjacent 10 tokens. This module is trained for 10 epochs, the learning rate is 1e-4 for the first epoch and 1e-5 for the rest.\nBefore evaluation, we remove sentences that have an overlap of over 50% with other sentences to reduce redundancy.\nBaselines. We compare our proposed TWAG with the following strong baselines:\n• TF-S2S (Liu et al., 2018) uses a Transformer decoder and compresses key-value pairs in self-attention with a convolutional layer.\n• CV-S2D+T (Perez-Beltrachini et al., 2019) uses a convolutional encoder and a two-layer hierarchical decoder, and introduces LDA to model topical information.\n• HierSumm (Liu and Lapata, 2019) utilizes the attention mechanism to model inter-\nparagraph relations and then enhances the document representation with graphs.\n• BART (Lewis et al., 2020) is a pretrained sequence-to-sequence model that achieved success on various sequence prediction tasks.\nWe fine-tune the pretrained BART-base model on our dataset and set beam size to 5 for all models using beam search at test time. The parameters we use for training and evaluation are identical to these in corresponding papers."
    }, {
      "heading" : "5.2 Results and Analysis",
      "text" : "Table 2 shows the ROUGE F1 scores of different models. In all three domains, TWAG outperforms other baselines. Our model surpasses other models on ROUGE-1 score by a margin of about 10%, while still retaining advantage on ROUGE-2 and ROUGE-L scores. In domain Company, our model boosts the ROUGE-L F1 score by about 30%, considering that ROUGE-L score is computed upon the longest common sequence, the highest ROUGE-L score indicates that abstracts generated by TWAG have the highest holistic quality.\nWhile CVS2D+T and BART retain reasonable scores, TF-S2S and HierSumm do not reach the scores they claim in their papers. Notice that the WikiCatSum dataset is a subset of WikiSum, which is used as the training dataset of these two models, we infer that TF-S2S and HierSumm require more training data to converge, and suffer from under-\nfitting due to the dataset scale. This phenomenon also proves that TWAG is data-efficient."
    }, {
      "heading" : "5.3 Ablation Study",
      "text" : "Learning Rate of Topic Detector. We tried two learning rates when training the topic detector module. A learning rate of 1e-7 would result in a precision of 0.922 in evaluation, while a learning rate of 3e-5 would result in a precision of 0.778. However, choosing the former learning rate causes a drop of about 10% in all ROUGE scores, which is the reason why we use the latter one in our full model.\nWe infer that human authors occasionally make mistakes, assigning paragraphs into section labels that belong to other topics. A topic detector with low learning rate overfits these mistakes, harming the overall performance of our model.\nSoft or Hard Topic. To further investigate the effectiveness of TWAG’s soft topic mechanism, we compare the results of soft and hard topic and report them in Table 4, from which we can see that hard topic does quite poorly in this task.\nA possible reason is that some sentences in the\nstandard abstract express more than one topic. Assigning one topic to each sentence will result in semantic loss and thus harm the quality of generated abstract, while the soft topic could better simulate the human writing style.\nNumber of Section Labels. The number of section labels nt plays a key role in our model: a small nt would not be informative enough to build topics, while a large one would induce noise. We can see from Figure 3 that the frequency of section labels is long-tailed, thus retaining only a small portion is able to capture the major part of information. Ta-\nble 5 records the experiment results we conducted on domain Company. nt = 20 reaches a peak on ROUGE 1, 2 and L scores, indicating that 20 is a reasonable number of section labels."
    }, {
      "heading" : "5.4 Case Study",
      "text" : "Table 3 shows the generated Wikipedia abstracts by different models about film Majina There. We\ncan see that the gold abstract contains information about three topics: basic information (region, director, and producer), actors, and music.\nAmong the models, TF-S2S produces an abstract with a proper pattern but contains wrong information and BART misses the musical information topic. CV-S2D+T, HierSumm, and our TWAG model both cover all three topics in the gold abstract, however, CV-S2D+T makes several factual errors like the release date and actors and HierSumm suffers from redundancy. TWAG covers all three topics in the gold abstract and discovers extra facts, proving itself to be competent in generating comprehensive abstracts."
    }, {
      "heading" : "5.5 Human Evaluation",
      "text" : "We follow the experimental setup of (PerezBeltrachini et al., 2019) and conduct a human evaluation consisting of two parts. A total of 45 examples (15 from each domain) are randomly selected from the test set for evaluation.\nThe first part is a question-answering (QA) scheme proposed in (Clarke and Lapata, 2010) in order to examine factoid information in summaries. We create 2-5 questions3 based on the golden sum-\n3Example questions are listed in the Appendix C, and the whole evaluation set is included in the our code repository.\nmary which covers the appeared topics, and invite 3 participants to answer the questions by taking automatically-generated summaries as background information. The more questions a summary can answer, the better it is. To quantify the results, we assign a score of 1/0.5/0.1/0 to a correct answer, a partially correct answer, a wrong answer and those cannot be answered, and report the average score over all questions. Notice that we give a score of 0.1 even if the participants answer the question incorrectly, because a wrong answer indicates the summary covers a certain topic and is superior to missing information. Results in Table 6 shows that 1) taking summaries generated by TWAG is capable of answering more questions and giving the correct answer, 2) TF-S2S and HierSumm perform poorly in domain Film and Animal, which is possibly a consequence of under-fitting in small datasets.\nThe second part is an evaluation over linguistic quality. We ask the participants to read different generated summaries from 3 perspectives and give a score of 1-5 (larger scores indicates higher quality): Completeness (does the summary contain sufficient information?), Fluency (is the summary fluent and grammatical?) and Succinctness (does the summary avoid redundant sentences?) Specifically, 3 participants are assigned to evaluate each model, and the average scores are taken as the fi-\nnal results. Table 7 presents the comparison results, from which we can see that, the linguistic quality of TWAG model outperforms other baseline models, validating its effectiveness."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we propose a novel topic-guided abstractive summarization model TWAG for generating Wikipedia abstracts. It investigates the section labels of Wikipedia, dividing the input document into different topics to improve the quality of generated abstract. This approach simulates the way how human recognize entities, and experimental results show that our model obviously outperforms existing state-of-the-art models which view Wikipedia abstracts as plain text. Our model also demonstrates its high data efficiency. In the future, we will try to incorporate pretrained language models into the topic-aware abstract generator module, and apply the topic-aware model to other texts rich in topical information like sports match reports."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the anonymous reviewers for their insightful comments. This work is supported by the National Key Research and Development Program of China (2017YFB1002101), NSFC Key Project (U1736204) and a grant from Huawei Inc.\nEthical Considerations\nTWAG could be applied to applications like automatically writing new Wikipedia abstracts or other texts rich in topical information. It can also help human writers to examine whether they have missed information about certain important topics.\nThe benefits of using our model include saving human writers’ labor and making abstracts more comprehensive. There are also important considerations when using our model. Input texts may violate copyrights when inadequately collected, and misleading texts may lead to factual mistakes in generated abstracts. To mitigate the risks, researches on how to avoid copyright issues when collecting documents from the Internet would help."
    }, {
      "heading" : "A Topic Allocation",
      "text" : "For each domain, we sort section labels by frequency and choose the top nt = 20 frequent section labels, then manually allocate them into different topics. Section labels with little semantic information like Reference and Notes are discarded in allocation to reduce noise. Table 8 shows how we allocate section labels into topics in domain Company, Film and Animal.\nAn additional NOISE topic is added to each domain to detect website noises. We build training records for NOISE by finding noise text in the training set of WikiCatSum by regular expressions. For example, we view all text containing “cookie”, “href” or text that seems to be a reference as noise."
    }, {
      "heading" : "B Trivia about Baselines",
      "text" : "We use BART-base as the baseline for comparison because BART-large performs poorly in experiments. BART-large starts generating redundant results when using only 4% training data, and its training loss also decreases much slower than\nBART-base. We infer that BART-large may overfit on training data, and BART-base is more competent to be the baseline."
    }, {
      "heading" : "C Human Evaluation Example",
      "text" : "Table 9 shows an example of gold summary, its corresponding question set and system outputs. The full dataset we used for human evaluation can be found in our code repository."
    } ],
    "references" : [ {
      "title" : "Wikiwrite: generating wikipedia articles automatically",
      "author" : [ "Siddhartha Banerjee", "Prasenjit Mitra." ],
      "venue" : "Proceedings of the 25th International Joint Conference on Artificial Intelligence, pages 2740–2746.",
      "citeRegEx" : "Banerjee and Mitra.,? 2016",
      "shortCiteRegEx" : "Banerjee and Mitra.",
      "year" : 2016
    }, {
      "title" : "Multi-document abstractive summarization using ilp based multi-sentence compression",
      "author" : [ "Siddhartha Banerjee", "Prasenjit Mitra", "Kazunari Sugiyama." ],
      "venue" : "Proceedings of the 24th International Conference on Artificial Intelligence, pages 1208–1214.",
      "citeRegEx" : "Banerjee et al\\.,? 2015",
      "shortCiteRegEx" : "Banerjee et al\\.",
      "year" : 2015
    }, {
      "title" : "An unsupervised approach to biography production using wikipedia",
      "author" : [ "Fadi Biadsy", "Julia Hirschberg", "Elena Filatova." ],
      "venue" : "Proceedings of ACL-08: HLT, pages 807–815.",
      "citeRegEx" : "Biadsy et al\\.,? 2008",
      "shortCiteRegEx" : "Biadsy et al\\.",
      "year" : 2008
    }, {
      "title" : "Abstractive multidocument summarization via phrase selection and merging",
      "author" : [ "Lidong Bing", "Piji Li", "Yi Liao", "Wai Lam", "Weiwei Guo", "Rebecca J Passonneau." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Bing et al\\.,? 2015",
      "shortCiteRegEx" : "Bing et al\\.",
      "year" : 2015
    }, {
      "title" : "Mutually reinforced manifold-ranking based relevance propagation model for query-focused multi-document summarization",
      "author" : [ "Xiaoyan Cai", "Wenjie Li." ],
      "venue" : "IEEE transactions on audio, speech, and language processing, 20(5):1597–1607.",
      "citeRegEx" : "Cai and Li.,? 2012",
      "shortCiteRegEx" : "Cai and Li.",
      "year" : 2012
    }, {
      "title" : "Discourse constraints for document compression",
      "author" : [ "James Clarke", "Mirella Lapata." ],
      "venue" : "Computational Linguistics, 36(3):411–441.",
      "citeRegEx" : "Clarke and Lapata.,? 2010",
      "shortCiteRegEx" : "Clarke and Lapata.",
      "year" : 2010
    }, {
      "title" : "Lexrank: Graph-based lexical centrality as salience in text summarization",
      "author" : [ "Günes Erkan", "Dragomir R Radev." ],
      "venue" : "Journal of artificial intelligence research, 22:457–479.",
      "citeRegEx" : "Erkan and Radev.,? 2004",
      "shortCiteRegEx" : "Erkan and Radev.",
      "year" : 2004
    }, {
      "title" : "Multi-news: A largescale multi-document summarization dataset and abstractive hierarchical model",
      "author" : [ "Alexander Richard Fabbri", "Irene Li", "Tianwei She", "Suyi Li", "Dragomir Radev." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Fabbri et al\\.,? 2019",
      "shortCiteRegEx" : "Fabbri et al\\.",
      "year" : 2019
    }, {
      "title" : "Sentence fusion via dependency graph compression",
      "author" : [ "Katja Filippova", "Michael Strube." ],
      "venue" : "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 177– 185.",
      "citeRegEx" : "Filippova and Strube.,? 2008",
      "shortCiteRegEx" : "Filippova and Strube.",
      "year" : 2008
    }, {
      "title" : "Bottom-up abstractive summarization",
      "author" : [ "Sebastian Gehrmann", "Yuntian Deng", "Alexander M Rush." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4098–4109.",
      "citeRegEx" : "Gehrmann et al\\.,? 2018",
      "shortCiteRegEx" : "Gehrmann et al\\.",
      "year" : 2018
    }, {
      "title" : "Wikiasp: A dataset for multi-domain aspectbased summarization",
      "author" : [ "Hiroaki Hayashi", "Prashant Budania", "Peng Wang", "Chris Ackerson", "Raj Neervannan", "Graham Neubig." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 9:211–225.",
      "citeRegEx" : "Hayashi et al\\.,? 2021",
      "shortCiteRegEx" : "Hayashi et al\\.",
      "year" : 2021
    }, {
      "title" : "Wikireading: A novel large-scale language understanding task over wikipedia",
      "author" : [ "Daniel Hewlett", "Alexandre Lacoste", "Llion Jones", "Illia Polosukhin", "Andrew Fandrianto", "Jay Han", "Matthew Kelcey", "David Berthelot." ],
      "venue" : "Proceedings of the 54th Annual Meet-",
      "citeRegEx" : "Hewlett et al\\.,? 2016",
      "shortCiteRegEx" : "Hewlett et al\\.",
      "year" : 2016
    }, {
      "title" : "Knowledge graph-augmented abstractive summarization with semantic-driven cloze reward",
      "author" : [ "Luyang Huang", "Lingfei Wu", "Lu Wang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5094–",
      "citeRegEx" : "Huang et al\\.,? 2020",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas N. Kipf", "Max Welling." ],
      "venue" : "Proceedings of the 5th International Conference on Learning Representations.",
      "citeRegEx" : "Kipf and Welling.,? 2017",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2017
    }, {
      "title" : "Albert: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Lan et al\\.,? 2019",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2019
    }, {
      "title" : "Dbpedia–a large-scale, multilingual knowledge base extracted from wikipedia",
      "author" : [ "Jens Lehmann", "Robert Isele", "Max Jakob", "Anja Jentzsch", "Dimitris Kontokostas", "Pablo N Mendes", "Sebastian Hellmann", "Mohamed Morsey", "Patrick Van Kleef", "Sören Auer" ],
      "venue" : null,
      "citeRegEx" : "Lehmann et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lehmann et al\\.",
      "year" : 2015
    }, {
      "title" : "Bart: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text summarization branches out, pages 74–81.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Generating wikipedia by summarizing long sequences",
      "author" : [ "Peter J Liu", "Mohammad Saleh", "Etienne Pot", "Ben Goodrich", "Ryan Sepassi", "Lukasz Kaiser", "Noam Shazeer." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Hierarchical transformers for multi-document summarization",
      "author" : [ "Yang Liu", "Mirella Lapata." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5070– 5081.",
      "citeRegEx" : "Liu and Lapata.,? 2019",
      "shortCiteRegEx" : "Liu and Lapata.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv, pages arXiv–1907.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Textrank: Bringing order into text",
      "author" : [ "Rada Mihalcea", "Paul Tarau." ],
      "venue" : "Proceedings of the 2004 conference on empirical methods in natural language processing, pages 404–411.",
      "citeRegEx" : "Mihalcea and Tarau.,? 2004",
      "shortCiteRegEx" : "Mihalcea and Tarau.",
      "year" : 2004
    }, {
      "title" : "A deep reinforced model for abstractive summarization",
      "author" : [ "Romain Paulus", "Caiming Xiong", "Richard Socher." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Paulus et al\\.,? 2018",
      "shortCiteRegEx" : "Paulus et al\\.",
      "year" : 2018
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Generating summaries with topic templates and structured convolutional decoders",
      "author" : [ "Laura Perez-Beltrachini", "Yang Liu", "Mirella Lapata." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5107–5116.",
      "citeRegEx" : "Perez.Beltrachini et al\\.,? 2019",
      "shortCiteRegEx" : "Perez.Beltrachini et al\\.",
      "year" : 2019
    }, {
      "title" : "A common theory of information fusion from multiple text sources step one: cross-document structure",
      "author" : [ "Dragomir Radev." ],
      "venue" : "1st SIGdial workshop on Discourse and dialogue, pages 74–83.",
      "citeRegEx" : "Radev.,? 2000",
      "shortCiteRegEx" : "Radev.",
      "year" : 2000
    }, {
      "title" : "Squad: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Automatically generating wikipedia articles: a structureaware approach",
      "author" : [ "Christina Sauper", "Regina Barzilay." ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Sauper and Barzilay.,? 2009",
      "shortCiteRegEx" : "Sauper and Barzilay.",
      "year" : 2009
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J Liu", "Christopher D Manning." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1073–1083.",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "An exploration of document impact on graph-based multi-document summarization",
      "author" : [ "Xiaojun Wan." ],
      "venue" : "Proceedings of the 2008 conference on empirical methods in natural language processing, pages 755– 762.",
      "citeRegEx" : "Wan.,? 2008",
      "shortCiteRegEx" : "Wan.",
      "year" : 2008
    }, {
      "title" : "Graph-based multi-modality learning for topic-focused multidocument summarization",
      "author" : [ "Xiaojun Wan", "Jianguo Xiao." ],
      "venue" : "Proceedings of the 21st international jont conference on Artifical intelligence, pages 1586–1591.",
      "citeRegEx" : "Wan and Xiao.,? 2009",
      "shortCiteRegEx" : "Wan and Xiao.",
      "year" : 2009
    }, {
      "title" : "Graph-based neural multi-document summarization",
      "author" : [ "Michihiro Yasunaga", "Rui Zhang", "Kshitijh Meelu", "Ayush Pareek", "Krishnan Srinivasan", "Dragomir Radev." ],
      "venue" : "Proceedings of the 21st Conference on Computational Natural Language Learning,",
      "citeRegEx" : "Yasunaga et al\\.,? 2017",
      "shortCiteRegEx" : "Yasunaga et al\\.",
      "year" : 2017
    }, {
      "title" : "Graph-based neural sentence ordering",
      "author" : [ "Yongjing Yin", "Linfeng Song", "Jinsong Su", "Jiali Zeng", "Chulun Zhou", "Jiebo Luo." ],
      "venue" : "Proceedings of the 28th International Joint Conference on Artificial Intelligence, pages 5387–5393.",
      "citeRegEx" : "Yin et al\\.,? 2019",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards a neural network approach to abstractive multi-document summarization",
      "author" : [ "Jianmin Zhang", "Jiwei Tan", "Xiaojun Wan." ],
      "venue" : "arXiv preprint arXiv:1804.09010.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Wikipedia, one of the most popular crowd-sourced online knowledge bases, has been widely used as the valuable resources in natural language processing tasks such as knowledge acquisition (Lehmann et al., 2015) and question answering (Hewlett et al.",
      "startOffset" : 187,
      "endOffset" : 209
    }, {
      "referenceID" : 11,
      "context" : ", 2015) and question answering (Hewlett et al., 2016; Rajpurkar et al., 2016) due to its high quality and wide coverage.",
      "startOffset" : 31,
      "endOffset" : 77
    }, {
      "referenceID" : 26,
      "context" : ", 2015) and question answering (Hewlett et al., 2016; Rajpurkar et al., 2016) due to its high quality and wide coverage.",
      "startOffset" : 31,
      "endOffset" : 77
    }, {
      "referenceID" : 6,
      "context" : "Graph-based representations are widely exploited to capture the most salient textual units and enhance the quality of the final summary (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Wan, 2008).",
      "startOffset" : 136,
      "endOffset" : 196
    }, {
      "referenceID" : 21,
      "context" : "Graph-based representations are widely exploited to capture the most salient textual units and enhance the quality of the final summary (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Wan, 2008).",
      "startOffset" : 136,
      "endOffset" : 196
    }, {
      "referenceID" : 29,
      "context" : "Graph-based representations are widely exploited to capture the most salient textual units and enhance the quality of the final summary (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Wan, 2008).",
      "startOffset" : 136,
      "endOffset" : 196
    }, {
      "referenceID" : 31,
      "context" : "Recently, there also emerge neural extractive models (Yasunaga et al., 2017; Yin et al., 2019) utilizing the graph convolutional network (Kipf and Welling, 2017) to better capture inter-document relations.",
      "startOffset" : 53,
      "endOffset" : 94
    }, {
      "referenceID" : 32,
      "context" : "Recently, there also emerge neural extractive models (Yasunaga et al., 2017; Yin et al., 2019) utilizing the graph convolutional network (Kipf and Welling, 2017) to better capture inter-document relations.",
      "startOffset" : 53,
      "endOffset" : 94
    }, {
      "referenceID" : 13,
      "context" : ", 2019) utilizing the graph convolutional network (Kipf and Welling, 2017) to better capture inter-document relations.",
      "startOffset" : 50,
      "endOffset" : 74
    }, {
      "referenceID" : 27,
      "context" : "The reason is that the input documents collected from various sources are often noisy and lack intrinsic relations (Sauper and Barzilay, 2009), which makes the relation graph hard to build.",
      "startOffset" : 115,
      "endOffset" : 142
    }, {
      "referenceID" : 8,
      "context" : "The abstractive models aim to distill an informative and coherent summary via sentence-fusion and paraphrasing (Filippova and Strube, 2008; Banerjee et al., 2015; Bing et al., 2015), but achieve little success due to the limited scale of datasets.",
      "startOffset" : 111,
      "endOffset" : 181
    }, {
      "referenceID" : 1,
      "context" : "The abstractive models aim to distill an informative and coherent summary via sentence-fusion and paraphrasing (Filippova and Strube, 2008; Banerjee et al., 2015; Bing et al., 2015), but achieve little success due to the limited scale of datasets.",
      "startOffset" : 111,
      "endOffset" : 181
    }, {
      "referenceID" : 3,
      "context" : "The abstractive models aim to distill an informative and coherent summary via sentence-fusion and paraphrasing (Filippova and Strube, 2008; Banerjee et al., 2015; Bing et al., 2015), but achieve little success due to the limited scale of datasets.",
      "startOffset" : 111,
      "endOffset" : 181
    }, {
      "referenceID" : 24,
      "context" : "(2018) proposes an extractive-then-abstractive model and contributes WikiSum, a large-scale dataset for Wikipedia abstract generation, inspiring a branch of further studies (Perez-Beltrachini et al., 2019; Liu and Lapata, 2019; Li et al., 2020).",
      "startOffset" : 173,
      "endOffset" : 244
    }, {
      "referenceID" : 19,
      "context" : "(2018) proposes an extractive-then-abstractive model and contributes WikiSum, a large-scale dataset for Wikipedia abstract generation, inspiring a branch of further studies (Perez-Beltrachini et al., 2019; Liu and Lapata, 2019; Li et al., 2020).",
      "startOffset" : 173,
      "endOffset" : 244
    }, {
      "referenceID" : 28,
      "context" : ", predicts the topic distribution of each abstract sentence to determine its topic-aware representation, and decodes the sentence with a Pointer-Generator network (See et al., 2017).",
      "startOffset" : 163,
      "endOffset" : 181
    }, {
      "referenceID" : 25,
      "context" : "Compared with single-document summarization, the input documents may contain redundant or even contradictory information (Radev, 2000).",
      "startOffset" : 121,
      "endOffset" : 134
    }, {
      "referenceID" : 6,
      "context" : ", TF-IDF similarity (Erkan and Radev, 2004), discourse relation (Mihalcea and Tarau, 2004), document-sentence two-layer relations (Wan, 2008), multi-modal (Wan and Xiao, 2009) and query information (Cai and Li, 2012).",
      "startOffset" : 20,
      "endOffset" : 43
    }, {
      "referenceID" : 21,
      "context" : ", TF-IDF similarity (Erkan and Radev, 2004), discourse relation (Mihalcea and Tarau, 2004), document-sentence two-layer relations (Wan, 2008), multi-modal (Wan and Xiao, 2009) and query information (Cai and Li, 2012).",
      "startOffset" : 64,
      "endOffset" : 90
    }, {
      "referenceID" : 29,
      "context" : ", TF-IDF similarity (Erkan and Radev, 2004), discourse relation (Mihalcea and Tarau, 2004), document-sentence two-layer relations (Wan, 2008), multi-modal (Wan and Xiao, 2009) and query information (Cai and Li, 2012).",
      "startOffset" : 130,
      "endOffset" : 141
    }, {
      "referenceID" : 30,
      "context" : ", TF-IDF similarity (Erkan and Radev, 2004), discourse relation (Mihalcea and Tarau, 2004), document-sentence two-layer relations (Wan, 2008), multi-modal (Wan and Xiao, 2009) and query information (Cai and Li, 2012).",
      "startOffset" : 155,
      "endOffset" : 175
    }, {
      "referenceID" : 4,
      "context" : ", TF-IDF similarity (Erkan and Radev, 2004), discourse relation (Mihalcea and Tarau, 2004), document-sentence two-layer relations (Wan, 2008), multi-modal (Wan and Xiao, 2009) and query information (Cai and Li, 2012).",
      "startOffset" : 198,
      "endOffset" : 216
    }, {
      "referenceID" : 13,
      "context" : "(2017) builds a discourse graph and represents textual units upon the graph convolutional network (GCN) (Kipf and Welling, 2017), and Yin et al.",
      "startOffset" : 104,
      "endOffset" : 128
    }, {
      "referenceID" : 8,
      "context" : "In contrast, early abstractive models using sentence-fusion and paraphrasing (Filippova and Strube, 2008; Banerjee et al., 2015; Bing et al., 2015) achieve less success.",
      "startOffset" : 77,
      "endOffset" : 147
    }, {
      "referenceID" : 1,
      "context" : "In contrast, early abstractive models using sentence-fusion and paraphrasing (Filippova and Strube, 2008; Banerjee et al., 2015; Bing et al., 2015) achieve less success.",
      "startOffset" : 77,
      "endOffset" : 147
    }, {
      "referenceID" : 3,
      "context" : "In contrast, early abstractive models using sentence-fusion and paraphrasing (Filippova and Strube, 2008; Banerjee et al., 2015; Bing et al., 2015) achieve less success.",
      "startOffset" : 77,
      "endOffset" : 147
    }, {
      "referenceID" : 28,
      "context" : "Inspired by the recent success of single-document abstractive models (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Huang et al., 2020), some works (Liu et al.",
      "startOffset" : 69,
      "endOffset" : 151
    }, {
      "referenceID" : 22,
      "context" : "Inspired by the recent success of single-document abstractive models (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Huang et al., 2020), some works (Liu et al.",
      "startOffset" : 69,
      "endOffset" : 151
    }, {
      "referenceID" : 9,
      "context" : "Inspired by the recent success of single-document abstractive models (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Huang et al., 2020), some works (Liu et al.",
      "startOffset" : 69,
      "endOffset" : 151
    }, {
      "referenceID" : 12,
      "context" : "Inspired by the recent success of single-document abstractive models (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Huang et al., 2020), some works (Liu et al.",
      "startOffset" : 69,
      "endOffset" : 151
    }, {
      "referenceID" : 18,
      "context" : ", 2020), some works (Liu et al., 2018; Zhang et al., 2018) try to transfer single-document models to multi-document settings to alleviate the limitations of small-scale datasets.",
      "startOffset" : 20,
      "endOffset" : 58
    }, {
      "referenceID" : 33,
      "context" : ", 2020), some works (Liu et al., 2018; Zhang et al., 2018) try to transfer single-document models to multi-document settings to alleviate the limitations of small-scale datasets.",
      "startOffset" : 20,
      "endOffset" : 58
    }, {
      "referenceID" : 19,
      "context" : "HierSumm (Liu and Lapata, 2019) adopts a learning-based model for the extractive stage, and computes the attention between paragraphs to model the dependencies across multiple paragraphs.",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 20,
      "context" : "(2021) investigates the structure of Wikipedia and builds an aspect-based summarization dataset by manually labeling aspects and identifying the aspect of input paragraphs with a finetuned RoBERTa model (Liu et al., 2019).",
      "startOffset" : 203,
      "endOffset" : 221
    }, {
      "referenceID" : 14,
      "context" : "For each paragraph d ∈ D, we encode it with ALBERT(Lan et al., 2019) and then predict its topic z with a fully-connected layer, i.",
      "startOffset" : 50,
      "endOffset" : 68
    }, {
      "referenceID" : 28,
      "context" : "Our sentence decoder adopts the Pointer-Generator network (See et al., 2017), which picks tokens both from input paragraphs and vocabulary.",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 28,
      "context" : "Following (See et al., 2017), we compute the loss of an abstract sentence by averaging the negative log likelihood of every target word in that sentence, and achieve Lsent via averaging over all m sentences,",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 24,
      "context" : "To evaluate the overall performance of our model, we use the WikiCatSum dataset proposed by (Perez-Beltrachini et al., 2019), which contains three distinct domains (Company, Film and Animal) in Wikipedia.",
      "startOffset" : 92,
      "endOffset" : 124
    }, {
      "referenceID" : 17,
      "context" : "We evaluate the performance of our model with ROUGE scores (Lin, 2004), which is a common metric in comparing generated and standard summaries.",
      "startOffset" : 59,
      "endOffset" : 70
    }, {
      "referenceID" : 23,
      "context" : "The first 400 tokens of input paragraphs are retained and transformed into GloVe (Pennington et al., 2014) embedding of 300 dimensions.",
      "startOffset" : 81,
      "endOffset" : 106
    }, {
      "referenceID" : 18,
      "context" : "• TF-S2S (Liu et al., 2018) uses a Transformer decoder and compresses key-value pairs in self-attention with a convolutional layer.",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 24,
      "context" : "• CV-S2D+T (Perez-Beltrachini et al., 2019) uses a convolutional encoder and a two-layer hierarchical decoder, and introduces LDA to model topical information.",
      "startOffset" : 11,
      "endOffset" : 43
    }, {
      "referenceID" : 19,
      "context" : "• HierSumm (Liu and Lapata, 2019) utilizes the attention mechanism to model inter-",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 16,
      "context" : "• BART (Lewis et al., 2020) is a pretrained sequence-to-sequence model that achieved success on various sequence prediction tasks.",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 5,
      "context" : "The first part is a question-answering (QA) scheme proposed in (Clarke and Lapata, 2010) in order to examine factoid information in summaries.",
      "startOffset" : 63,
      "endOffset" : 88
    } ],
    "year" : 2021,
    "abstractText" : "Wikipedia abstract generation aims to distill a Wikipedia abstract from web sources and has met significant success by adopting multidocument summarization techniques. However, previous works generally view the abstract as plain text, ignoring the fact that it is a description of a certain entity and can be decomposed into different topics. In this paper, we propose a two-stage model TWAG that guides the abstract generation with topical information. First, we detect the topic of each input paragraph with a classifier trained on existing Wikipedia articles to divide input documents into different topics. Then, we predict the topic distribution of each abstract sentence, and decode the sentence from topic-aware representations with a Pointer-Generator network. We evaluate our model on the WikiCatSum dataset, and the results show that TWAG outperforms various existing baselines and is capable of generating comprehensive abstracts. Our code and dataset can be accessed at https://github.com/THU-KEG/TWAG",
    "creator" : "LaTeX with hyperref"
  }
}