{
  "name" : "2021.acl-long.548.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Cross-replication Reliability - An Empirical Approach to Interpreting Inter-rater Reliability",
    "authors" : [ "Ka Wong", "Praveen Paritosh", "Lora Aroyo" ],
    "emails" : [ "danicky@gmail.com", "pkp@google.com", "l.m.aroyo@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7053–7065\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n7053"
    }, {
      "heading" : "1 Introduction",
      "text" : "Much content analysis and linguistics research is based on data generated by human beings (henceforth, annotators or raters) asked to make some kind of judgment. These judgments involve systematic interpretation of textual, visual, or audible matter (e.g. newspaper articles, television programs, advertisements, public speeches, and other multimodal data). When relying on human observers, researchers must worry about the quality of the data — specifically, their reliability (Krippendorff, 2004). Are the annotations collected reproducible, or are they the result of human idiosyncrasies?\nRespectable scholarly journals typically require reporting quantitative evidence for the inter-rater reliability (IRR) of the data (Hallgren, 2012). Cohen’s kappa (Cohen, 1960) or Krippendorff’s alpha (Hayes and Krippendorff, 2007) is expected to be\nabove a certain threshold to be worthy of publication, typically 0.6 (Landis and Koch, 1977). Similar IRR requirements for human annotations data have been followed across many fields. In this paper we refer to this absolute interpretation of IRR as the Landis-Koch approach (Fig. 1).\nThis approach has been foundational in guiding the development of widely used and shared datasets and resources. Meanwhile, the landscape of human annotations collection has witnessed a tectonic shift in recent years. Driven by the datahungry success of machine learning (LeCun et al., 2015; Schaekermann et al., 2020), there has been an explosive growth in the use of crowdsourcing for building datasets and benchmarks (Snow et al., 2008; Kochhar et al., 2010). We identify three paradigm shifts in the scope of and methodologies for data collection that make the Landis-Koch approach not as useful in today’s settings.\nA rise in annotator diversity In the precrowdsourcing era lab settings, data were typically annotated by two graduate students following detailed guidelines and working with balanced corpora. Over the past two decades, however, the bulk of data are annotated by crowd workers with high cultural and training variances.\nA rise in task diversity There has been an increasing amount of subjective tasks with genuine ambiguity: judging toxicity of online discussions (Aroyo et al., 2019), in which the IRR values range\nbetween 0.2 and 0.4; judging emotions expressed by faces (Cowen and Keltner, 2017), in which more than 80% of the IRR values are below 0.6; and A/B testing of user satisfaction or preference evaluations (Kohavi and Longbotham, 2017), where IRR values are typically between 0.3 and 0.5.\nA rise in imbalanced datasets Datasets are no longer balanced intentionally. Many high-stakes human judgements concern rare events with substantial tail risks: event security, disease diagnostics, financial fraud, etc. In all of these cases, a single rare event can be the source of considerable cost. High class imbalance has led to many complaints of IRR interpretability (Byrt et al., 1993; Feinstein and Cicchetti, 1990; Cicchetti and Feinstein, 1990).\nEach of these changes individually has a profound impact on data reliability. Together, they have caused a shift from data-from-the-lab to datafrom-the-wild, for which the Landis-Koch approach to interpreting IRR is admittedly too rigid and too stringent. Meanwhile, we have seen a drop in the reliance on reliability. Machine learning, crowdsourcing, and data research papers and tracks have abandoned the use and reporting of IRR for human labeled data, despite calls for it (Paritosh, 2012). The most cited recent datasets and benchmarks used by the community such as SQuAD (Rajpurkar et al., 2016), ImageNet (Deng et al., 2009), Freebase (Bollacker et al., 2008), have never reported IRR values. This would have been unthinkable twenty years ago. More importantly, this is happening against the backdrop of a reproducibility crisis in artificial intelligence (Hutson, 2018).\nWith the decline of the usage of IRR, we have seen a rise of ad hoc, misguided quality metrics that took its place, including 1) agreement-%, 2) accuracy relative to consensus, 3) accuracy relative to “ground truth.” This is dangerous, as IRR is still our best bet for ensuring data reliability. How can we ensure its continued importance in this new era of data collection?\nThis paper is an attempt to address this problem by proposing an empirical alternative to interpreting IRR. Instead of relying on an absolute scale, we benchmark an experiment’s IRR against two baseline measures, to be found in a replication. Replication here is defined as re-annotating the same set of items with a slight change in the experimental setup, e.g., annotator population, annotation guidelines, etc. By fixing the underlying corpus, we can\nensure the baseline measures are sensitive to the experiment on hand. The first baseline measure is the annotator reliability in the replication. The second measure is the annotator reliability between the replications. In Section 3, we present a novel way of measuring this. We call it cross-kappa (κx). It is an extension of Cohen’s (1960) kappa and is designed to measure annotator agreement between two replications in a chance-corrected manner.\nWe present in Appendix A the International Replication (IRep) dataset,1 a large-scale crowdsourced dataset of four million judgements of human facial expressions in videos. The dataset consists of three replications in Mexico City, Budapest, and Kuala Lumpur.2 Our analysis in Section 4 shows this empirical approach enables meaningful interpretation of IRR. In Section 5, we argue xRR is a sensible way of measuring the goodness of crowdsourced datasets, where high reliability is unattainable. While we only illustrate comparing annotator populations in this paper, the methodology behind the xRR framework is general and can apply to similarly replicated datasets, e.g., via change of annotation guidelines."
    }, {
      "heading" : "2 Related Work",
      "text" : "To position our research, we present a brief summary of the literature in two areas: metrics for measuring annotator agreement and their shortcomings (Section 2.1), comparing replications of an experiment (Section 2.2)."
    }, {
      "heading" : "2.1 Annotator Agreement",
      "text" : "Artstein and Poesio (2008) present a comprehensive survey of the literature on IRR metrics used in linguistics. Popping (1988) compare an astounding 43 measures for nominal data (mostly applicable to reliability of data generated by only two observers). Since then, Cohen’s (1960) kappa and its variants (Carletta et al., 1997; Cohen, 1968) have become the de facto standard for measuring agreement in computational linguistics.\nOne of the strongest criticisms of kappa is its lack of interpretability when facing class imbalance. This problem is known as the kappa paradox (Feinstein and Cicchetti, 1990; Byrt et al., 1993;\n1https://github.com/google-research-datasets/replicationdataset\n2On this task, raters received average hourly wages of $12, $20, and $14 USD in Mexico City, Budapest, and Kuala Lumpur respectively. See Appendix A for annotation setup.\nWarrens, 2010), or the ‘base rates’ problem (Uebersax, 1987). Bruckner and Yoder (2006) show class imbalance imposes practical limits on kappa and suggest one to interpret kappa in relation to the class imbalance of the underyling data. Others have proposed measures that are more robust against class imbalance (Gwet, 2008; Spitznagel and Helzer, 1985; Stewart and Rey, 1988). Pontius Jr and Millones (2011) even suggest abandoning the use of kappa altogether."
    }, {
      "heading" : "2.2 Agreement Between Replications",
      "text" : "Replications are often being compared, but it is done at the level of per-item mean scores. Cowen and Keltner (2017) measure the correlation between the mean scores of two geographical rater pools. They use Spearman’s (1904) correction for attenuation (discussed later in this paper) with splithalf reliability. Snow et al. (2008) measure the Pearson correlations between the score of a single expert and the mean score of a group of nonexperts, and vice versa. In this comparison the authors do not correct for correlation attenuation, hence the reported correlations may be strongly biased. Bias aside, correlation is not suitable for tasks with non-interval data or task with missing data. In this paper, we propose a general methodology for measuring rater agreement between replications with the same kind of generality, flexibility, and ease of use as IRR."
    }, {
      "heading" : "3 Cross-replication Reliability (xRR)",
      "text" : "Data reliability can be assessed when a set of items are annotated multiple times. When this is done by a single rater, intra-rater reliability assesses a person’s agreement with oneself. When this is done by two or more raters, inter-rater reliability (IRR) assesses the agreement between raters in an experiment. We propose to extend IRR to measure a similar notion of rater-rater agreement, but where the raters are taken from two different experiments. We call it cross-replication reliability (xRR). These replications can be a result of re-labeling the same items with a different rater pool, annotation template, or on a different platform, etc.\nWe begin with a general definition of Cohen’s (1960) kappa. We extend it to cross-kappa (κx) to measure cross-replication reliability. We then use this foundation to define normalized κx to measure similarity between two replications."
    }, {
      "heading" : "3.1 Kappa and Its Generalizations",
      "text" : "The class of IRR measures is quite diverse, covering many different experimental scenarios, e.g., different numbers of raters, rating scales, agreement definitions, assumptions about rater interchangeability, etc. Out of all such coefficients, Cohen’s (1960) kappa has a distinct property that makes it most suitable for the task on hand. Unlike Scott’s pi (Scott, 1955), Fleiss’s kappa (Fleiss, 1971), Krippendorf’s alpha (Krippendorff, 2004), and many others, Cohen’s (1960) kappa allows for two different marginal distributions. This stems from Cohen’s belief that two raters do not necessarily share the same marginal distribution, hence they should not be treated interchangeably. When we compare replications, e.g., two rater populations, we are deliberately changing some underlying conditions of the experiment, hence it is safer to assume the marginal distributions will not be the same. Within either replication, however, we rely on the rater interchangeability assumption. We think this more accurately reflects the current practice in crowdsourcing, where each rater contributes a limited number of responses in an experiment, and hence raters are operationally interchangeable.\nCohen’s (1960) kappa was invented to compare two raters classifying n items into a fixed number of categories. Since its publication, it has been generalized to accommodate multiple raters (Light, 1971; Berry and Mielke Jr, 1988), and to cover different types of annotation scales: ordinal (Cohen, 1968), interval (Berry and Mielke Jr, 1988; Janson and Olsson, 2001), multivariate (Berry and Mielke Jr, 1988), and any arbitrary distance function (Artstein and Poesio, 2008). In this paper we focus on Janson and Olsson’s (2001) generalization, which the authors denote with the lowercase Greek letter iota (ι). It extends kappa to accommodate interval data with multiple raters, and is expressed in terms of pairwise disagreement:\nι = 1− do de . (1)\ndo in this formula represents the observed portion of disagreement and is defined as:\ndo =\n[ n ( b\n2 )]−1∑ r<s n∑ i D(xri, xsi), (2)\nwhere n is the number of items, b the number of annotators, i the item index, r and s the annotator\nindexes; ∑\nr<s is the sum over all r and s such that 1 <= r < s <= b. D() is a pairwise disagreement defined as:\nD(xri, xsi) = (xri − xsi)2 (3)\nfor interval data, and\nD(xri, xsi) =\n{ 0, xri = xsi\n1, otherwise (4)\nfor categorical data. Note we are dropping Janson and Olsson’s multivariate reference in D() and focusing on the univariate case. de in the denominator represents the expected portion of disagreement and is defined as:\nde =\n[ n2 ( b\n2 )]−1∑ r<s n∑ i n∑ j D(xri, xsj). (5)\nJanson and Olsson’s expression in Eq. 1 is based on Berry and Mielke Jr (1988). While the latter use absolute distance for interval data, the former use squared distance instead. We follow Janson and Olsson’s approach because squared distance leads to desirable properties and familiar interpretation of coefficients (Fleiss and Cohen, 1973; Krippendorff, 1970). Squared distance is also used in alpha (Krippendorff, 2004). Berry and Mielke Jr (1988) show if b = 2 and the scale is categorical, ι in Eq. 1 reduces to Cohen’s (1960) kappa. For other rating scales such as ratio, rank, readers should refer to Krippendorff (2004) for additional distance functions. The equations for do and de are unaffected by the choice of D()."
    }, {
      "heading" : "3.2 Definition of κx",
      "text" : "Here we present κx as a novel reliability coefficient for measuring the rater agreement between two replications. In Janson and Olsson’s generalized kappa above, the disagreement is measured within pairs of annotations taken from the same experiment. In order to extend it to measure crossreplication agreement, we construct annotation pairs such that the two annotations are taken from different replications. We do not consider annotation pairs from the same replication. We define cross-kappa, κx(X,Y ), as a reliability coefficient between replications X and Y :\nκx(X,Y ) = 1− do(X,Y )\nde(X,Y ) , (6)\nwhere\ndo(X,Y ) = 1\nnRS n∑ i=1 R∑ r=1 S∑ s=1 D(xri, ysi), (7)\nand\nde(X,Y ) = 1\nn2RS n∑ i=1 n∑ j=1 R∑ r=1 S∑ s=1 D(xri, ysj),\n(8) where x and y denote annotations from replications X and Y respectively, n is the number of items, R and S the numbers of annotations per item in replications X and Y respectively. In this definition, the observed disagreement is obtained by averaging disagreement observed in nRS pairs of annotations, where each pair contains two annotations on the same item taken from two different replications. Expected disagreement is obtained by averaging over all possible n2RS cross-replication annotation pairs. When each replication has only 1 annotation per item, and the data is categorical, it is easy to show κx reduces to Cohen’s (1960) kappa. κx is a kappa-like measure, and will have properties similar to kappa’s. κx is bounded between 0 and 1 in theory, though in practice it may be slightly negative for small sample sizes. κx = 0 means there is no discernible agreement between raters from two replications, beyond what would be expected by chance. κx = 1 means all raters between two replications are in perfect agreement with each other, which also implies perfect agreement within either replication."
    }, {
      "heading" : "3.3 κx with Missing Data",
      "text" : "As presented, the two replications can have different numbers of annotations per item. However, within either replication, the number of annotations per item is assumed to be fixed. We recognize this may not always be the case. In practice, items within an experiment can receive varying numbers of annotations (i.e., missing data). We now show how to calculate κx with missing data.\nWhen computing IRR with missing data, weights can be used to account for varying numbers of annotations within each item. Janson and Olsson (2004) propose a weighting scheme for iota in Eq. 1. Instead, we follow the tradition of Krippendorff (2004) in weighting each annotation equally in computing do and de. That amounts to the following scheme. In do, we first normalize within each item, then we take a weighted average over\nall items, with weights proportional to the combined numbers of annotations per item. In de, no weighting is required.\nSince R and S can now vary from item to item, we index them using R(∗) and S(∗) to denote that they are functions of the underlying items. We rewrite do and de as:\ndo(X,Y ) = n∑ i=1 R(i) + S(i) R+ S R(i)∑ r=1 S(i)∑ s=1 D(xri, ysi)\nR(i) · S(i) (9)\nand\nde(X,Y ) = 1\nR · S n∑ i=1 n∑ j=1 R(i)∑ r=1 S(j)∑ s=1 D(xri, ysj),\n(10) with\nR = n∑ i R(i), S = n∑ j S(j), (11)\nwhere R is the total number of annotations in replications X , R(i) the number annotations on item i in replication X , r = 1, 2, . . . , R(i) (on item i in replication X); and similarly for S, S(j), and s with respect to replication Y . ∑R(i) r=1 and ∑S(j) s=1 in Eq. 9 and 10 are inner summations, where i and j are indexes from the outer summations. Without missing data, R(i) = R for all i, and S(j) = S for all j, then R = nR, S = nS, reducing Eq. 9 and 10 to Eq. 7 and 8."
    }, {
      "heading" : "3.4 Normalization of κx",
      "text" : "xRR is modeled closely after IRR in order to serve as its baseline. As IRR measures the agreement between raters, so does xRR. In other words, κx is really a measure of rater agreement, not a measure of experimental similarity per se. This distinction is important. If we want to measure how well we replicate an experiment, we need to measure its disagreement with the replication in relationship to their own internal disagreements. The departure between inter-experiment and intra-experiment disagreements is important in measuring experimental similarity.\nThis calls for a normalization that considers κx in relation to IRR. First, we take inspirations from Spearman’s correction for attenuation (Spearman, 1904):\nρxy = rxy√ reliabilityx √ reliabilityy , (12)\nwhere rxy is the observed Pearson product-moment correlation between x and y (variables observed with measurement errors), ρxy is an estimate of their true, unobserved correlation (in the absence of measurement errors), and reliabilityx and reliabilityy are the reliabilities of x and y respectively. Eq. 12 is Spearman’s attempt to correct for the negative bias in rxy caused by the observation errors in x and y.3\nEq. 12 is relevant here because of the close connection between Cohen’s (1960) kappa and the Pearson correlation, rxy. In the dichotomous case, if the two marginal distributions are the same, Cohen’s (1960) kappa is equivalent to the Pearson correlation (Cohen, 1960, 1968). In the multicategory case, Cohen (1968) generalizes this equivalence to weighted kappa, under the conditions of equal marginals and a specific quadratic weighting scheme.\nBased on this strong connection, we propose replacing rxy in Eq. 12 with κx and define normalized κx as:\nnormalizedκx = κx(X,Y )√\nIRRX √ IRRY . (13)\nDefined this way, one would expect normalized κx to behave like ρxy. That is indeed the case. When we apply both measures to the IRep dataset, we obtain a Pearson correlation of 0.99 between them (see Section 4.5). This leads to two insights. First, we can interpret normalized κx like a disattenuated correlation, ρxy (see (Muchinsky, 1996) for a rigorous interpretation). Second, normalized κx approximates the true correlation between two experiments’ item-level mean scores.\nDespite their affinity, ρxy is not a substitute for normalized κx for measuring experimental similarity. Normalized κx is more general as it can accommodate non-interval scales and missing data."
    }, {
      "heading" : "3.5 Connection between xRR and IRR",
      "text" : "By connecting normalized κx to ρxy, we can also learn a lot about κx itself. To the extent that normalized κx approximates ρxy, we can rewrite Eq. 13 as:\nκx(X,Y ) ≈ ρxy √ IRRX √ IRRY . (14)\nThis formulation shows κx behaves like a product of ρxy and the geometric mean of the two IRRs.\n3Spearman relied on the assumption that the errors are uncorrelated with each other and with x and y.\nThis has important consequences, as we can deduce the following. 1) Holding constant the mean scores, and hence ρxy, the lower the IRRs, the lower the κx. Intra-experiment disagreement inflates interexperiment disagreement. 2) In theory ρxy ≤ 1.0,4 hence κx is capped by the greater of the two IRRs. I.e., Intra-experiment agreement presents a ceiling to inter-experiment agreement. 3) If x and y are identically distributed, e.g., in a perfect replication, ρxy = 1 and κx(X,Y ) = IRRX = IRRY . Thus, when a low reliability experiment is replicated perfectly, κx will be as low, whereas normalized κx will be 1. This explains why normalized κx is more suitable for measuring experimental similarity.\nIn this section, we propose κx as a measure of rater agreement between two replications, and normalized κx is as an experimental similarity metric. In the next section, we apply them in conjunction with IRR to illustrate how we can gain deeper insights into experiment reliabilities by triangulating these measures."
    }, {
      "heading" : "4 Applying xRR to the IRep Dataset",
      "text" : "As a standalone measure, IRR captures the reliability of an experiment by encapsulating many of its facets: class imbalance, item difficulty, guideline clarity, rater qualification, task ambiguity, etc. As such, it is difficult to compare the IRR of different experiments, or to interpret their individual values, because IRR is tangled with all the aforementioned design parameters. For example, we cannot attribute a low IRR to rater qualification without first isolating other design parameters. This is the problem we try to solve with xRR by contextualizing IRR with meaningful baselines via a replication. We will demonstrate this by applying this technique to the IRep Dataset (Appendix A). We focus on a subset of 5 emotions for illustration purposes, with the rest of the reliability values provided in Appendix B. In our analysis, IRR is measured with Cohen’s (1960) kappa and xRR with κx. We will refer to them interchangeably."
    }, {
      "heading" : "4.1 IRR Variability Across Emotions",
      "text" : "First we illustrate in Fig. 2 that different emotions within the same city can have very different IRR. For instance, the labels awe and love in Mexico City have an IRR of 0.1208 and 0.597 respectively (Table 1). Awe and love are completely different\n4Spearman’s correction can occasionally produce a correlation above 1.0 (Muchinsky, 1996).\nemotions with different levels of class imbalance and ambiguity, and without controlling for these differences, the gap in their reliabilities is not unexpected. That is exactly the problem about comparing IRRs – such comparisons are not meaningful. We need something directly comparable to awe in order to interpret its low IRR. If we do not compare emotions, and just consider awe using the LandisKoch scale, that would not be helpful either. We would not be able to tell if its low IRR is a result of poor guidelines, general ambiguity in emotion detection, or ambiguity specific to awe. It’s more meaningful to compare replications of awe itself."
    }, {
      "heading" : "4.2 IRR Variability Across Replications",
      "text" : "While the aforementioned variation in IRR between emotions is expected, IRR of the same emotion can vary greatly between replications as well. Fig. 3 shows two contrasting examples. On the one hand, the IRR of love is consistent across replications. On the other hand, the IRR of contemplation varies a lot. We know the IRR variation in contemplation is strictly attributed to rater pool differences because the samples, platforms and annotation templates are the same across experiments. Such variation in IRR will be missed entirely by sampling based approaches for error-bars (e.g. standard error, bootstrap), which assume a fixed rater population."
    }, {
      "heading" : "4.3 Cross-replication Rater Agreement",
      "text" : "As shown, replication can facilitate comparisons of IRR by producing meaningful baselines. However, IRR is an internal property of a dataset, it does not allow us to compare two datasets directly. To\nthat end, we can apply κx to quantify the rater agreement between two datasets, as IRR quantifies the rater agreement within a dataset. Interestingly, not only is κx useful for comparing two datasets, but it also serves as another baseline for interpreting their IRRs.\nIRR is a step toward ensuring reproducibility, so naturally we wonder how much of the observed IRR is tied to the specific experiment and how much of it generalizes? This is of particular concern when raters are sampled in a clustered manner, e.g., crowd workers from the same geographical region, grad students sharing the same office. We rarely make sure raters are diverse and representative of the larger population. High IRR can be the result of a homogeneous rater group, limiting the generality of the results. In the context of the IRep dataset, that two cities having similar IRRs does not imply their raters agree with each other at a comparable level, or at all. We will demonstrate this with two contrasting examples.\nMexico City and Budapest both have a moderate IRR for sadness, 0.5147 and 0.5175 respectively, and their κx is nearly the same at 0.4709 (Fig. 4). This gives us confidence that the high IRR of sadness generalizes beyond the specific rater pools. In contrast, on contentment Mexico City and Kuala Lumpur have comparable levels of IRR, 0.4494 and 0.6363 respectively, but their κx is an abysmal -0.0344 5 (Fig. 5). In other words, the rater agreement on contentment is limited to within-pool observations only. This serves as an important reminder that IRR is a property of a specific experimental setup and may or may not generalize beyond that. κx allows us to ensure the internal agreement has external validity."
    }, {
      "heading" : "4.4 Replication Similarity",
      "text" : "κx is a step towards comparing two replications, but it is not a good standalone measure of replication similarity. To do that, we must also account for both replications’ internal agreements, e.g., via normalized κx in Eq. 13. Fig. 6 shows an example. Mexico City and Budapest have a low κx of 0.0817 on awe. On the surface, this low agreement may seem attributable to differences between the rater pools. However, there is a similarly low IRR in either city: 0.1208 in Mexico City, and 0.117 in Budapest. After accounting for IRR, normalized κx is much higher at 0.6872 (Table 2), indicating a decent replication similarity between the two cities.\n5Negative xRR value due to estimation error."
    }, {
      "heading" : "4.5 Connection to ρxy",
      "text" : "We apply Spearman’s correction for attenuation in Eq. 12 to all 31 emotion labels in 3 replication pairs. The resulting ρxy is plotted against the corresponding normalized κx in Fig. 7. Both measures are strongly correlated with a Pearson correlation of 0.99. This justifies interpreting normalized κx as a disattenuated correlation like ρxy."
    }, {
      "heading" : "5 Measuring the Quality of a Crowdsourced Dataset",
      "text" : "The IRep dataset is replicated and is conducive to xRR analysis. However, in practice most datasets are not replicated. Is xRR still useful? We present a specific use case of xRR in this section and argue that it is worth replicating a crowdsourced dataset in order to evaluate its quality."
    }, {
      "heading" : "5.1 Data Target",
      "text" : "Given a set of items, it is possible that annotations of the highest attainable quality still fail to meet the Landis-Koch requirements. Task subjectivity and class imbalance together impose a practical limit on kappa (Bruckner and Yoder, 2006). In these situations, the experimenter can forgo a data collection effort for reliability reasons. Alternatively, the experimenter may believe that data of sufficiently high quality can still have scientific merits, regardless of reliability. If so, what guidance can we use to ensure the highest quality data, especially when collecting data via crowdsourcing? This paper is heavily motivated by this question.\nxRR allows us to interpret IRR not on an absolute scale, but against a replication, a reference of sorts. By judging a crowdsourced dataset against a reference, we can decide if its meets a certain quality bar, albeit a relative one. In the IRep dataset,\nall replications are of equal importance. However, in practice, we can often define a trusted source as our target. This trusted source can consist of linguists, medical experts, calibrated crowd workers, or the experimenters themselves. They should have enough expertise knowledge and an adequate understanding of the task. The critical criterion in choosing a target is its ability to remove common quality concerns such as rater qualification and guideline effectiveness."
    }, {
      "heading" : "5.2 Internal Agreements",
      "text" : "By replicating a random subset of a crowdsourced dataset with trusted annotators,6 one can compare the two IRRs and make sure they are at a similar level. If the crowd IRR is much higher, that may be an indication of collusion, or a set of overly simplistic guidelines that have deviated from the experiment fidelity (Sameki et al., 2015). If the crowd IRR is much lower, it may just be a reflection of annotator diversity, or it can mean under-defined guidelines, unequal annotator qualifications, etc. Further investigation is needed to ensure the discrepancy is reasonable and appropriate."
    }, {
      "heading" : "5.3 Mutual Agreement",
      "text" : "Suppose the two IRRs are similar, that is not to say that both datasets are similar. Both groups of annotators can have high internal agreement amongst themselves, but the two groups can agree on different sets of items. If our goal is to collect crowdsourced data that closely mirror the target, then we have to measure their mutual agreement, in addition to comparing their internal agreements. Recall from Section 3.5 that if an experiment is replicated perfectly, κx should be identical to the two IRRs. Or more concisely, normalized κx should be equal to 1. Thus a high normalized κx can assure us that the crowdsourced annotators are functioning as an extension of the trusted annotators, based on which we form our expectations."
    }, {
      "heading" : "5.4 Relation to Gold Data",
      "text" : "At a glance, this approach seems similar to the common practice of measuring the accuracy of crowdsourced data against the ground truth (Resnik et al., 2006; Hripcsak and Wilcox, 2002). However, they are actually fundamentally different approaches. κx is rooted in the reliability literature that does not rely on the existence of a correct\n62 or more ratings per item are needed to measure the IRR.\nanswer. The authors argue this is an unrealistic assumption for many crowdsourcing tasks, where the input involves some subjective judgement. Accuracy itself is also a flawed metric for annotations data due to its inability to handle data uncertainty. For instance, when the reliability of the gold data is less than perfect, accuracy can never reach 1.0. Furthermore, accuracy is not chance-corrected, so it tends to inflate with class imbalance."
    }, {
      "heading" : "5.5 Extending an Existing Dataset",
      "text" : "The aforementioned technique can also measure the quality of a dataset extension. The main challenge in extending an existing dataset is to ensure the new data is consistent with the old. The state-of-the-art method in computer vision is frequency matching – ensuring the same proportion of yes/no votes in each image class. Recht et al. (2019) extended ImageNet7 using this technique, concluding there is a 11% - 14% drop in accuracy across a broad range of models. While frequency matching controls the distribution of some statistics, the impact of the new platform is uncontrolled for. Engstrom et al. (2020) pointed out a bias in this sampling technique. Overall, it is difficult to assess how well we are extending a dataset. To that end, xRR can be of help. A high normalized κx and a comparable IRR in the new data can give us confidence in the uniformity and continuity in the data collection."
    }, {
      "heading" : "6 Discussion",
      "text" : "There has been a tectonic shift in the scope of and methodologies for annotations data collection due to the rise of crowdsourcing and machine learning. In many of these tasks, a high reliability is often difficult to attain, even under favorable circumstances. The rigid Landis-Koch scale has resulted in a decrease in the usage and reporting of IRR in most widely used datasets and benchmarks. Instead of abandoning IRR, we should adapt it to new ways of measuring data quality. The xRR framework presents a first-principled way of doing so. It is a more empirical approach that utilizes a replication as a reference point. It is based on two metrics – κx for measuring cross-replication rater agreement – and normalized κx for measuring replication similarity.\nWe opensource a large-scale replication dataset of facial expression judgements analyzed with the proposed framework. We show this framework can\n7http://www.image-net.org/\nbe used to guide our crowdsourcing data collection efforts. This is the beginning of a long line of inquiry. We outline future work and limitations below:\nConfidence intervals for κx Confidence intervals for κx and normalized κx are required for hypothesis testing. Though one can use the blockbootstrap for an empirical estimate, large sample behavior of these metrics needs to be studied.\nSensitivity of κx with high class-imbalance The xRR framework sidesteps the effect of classimbalance by comparing replications on the same item set. Further analysis needs to confirm the sensitivity of κx metrics in high class-imbalance.\nOptimization of κx computation Our method requires constructing many pairs of observations: n2RS. This may get prohibitively expensive, when the number of items is large. Using algebraic simplification and dynamic programming, this can be made much more efficient.\nAlternative normalizations of κx We provided one particular normalization technique, but it may not suit all applications. For example, when comparing crowd annotations to expert annotations, one can consider, κx/IRRexpert.\nAlternative xRR coefficients Our proposed xRR coefficient, κx, is based on Cohen’s (1960) kappa for its assumption about rater noninterchangeability. It may be useful to consider Krippendorff’s alpha and other agreement statistics as alternatives for other assumptions and statistical characteristics.\nWe hope this paper and dataset will spark research on these questions and increase reporting of reliability measures for human annotated data."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We like to thank Gautam Prasad and Alan Cowen for their work on collecting and sharing the IRep dataset and opensourcing it."
    }, {
      "heading" : "A The IRep Dataset: Facial Expressions Replication Dataset",
      "text" : "The IRep Dataset is a human annotated dataset with a list of 30 emotion labels from a set of emotion classes defined in Cowen and Keltner (2017) plus one additional label ‘unsure’. During the data collection process, raters used the set of 30 facial expression labels to annotate their perception of the facial expression present in a video. As the purpose of this dataset is to illustrate how replications of human labeling experiments can be used to determine the overall quality of the resulting annotations, we have omitted the reference to the actual video content. The annotated videos are thus referred to as ‘items’ with a set of indices (item IDs), e.g. item 1, item 2, etc, stored in a CSV format. Raters are referred to as Rater 1 or Rater 2 across all rater pools. They are just placeholders and do not imply particular individuals (Table 3).\nThe dataset contains 3,939,418 annotations on 38,499 unique items. The size of the dataset is 15MB, and the dataset is released on GitHub: https://github. com/google-research-datasets/ replication-dataset. To produce the replications for this labeling experiment, we used rater pools in three different cities - Budapest, Kuala Lumpur and Mexico City - on the same labeling platform. In Table 4 we show the distribution of items and ratings across the different rater pools.\nB IRR, xRR, and normalized xRR values for the IRep dataset\nIn Table 5 we report the IRR, κx, and normalized κx obtained from the entire IRep dataset."
    } ],
    "references" : [ {
      "title" : "Crowdsourcing subjective tasks: The case study of understanding toxicity in online discussions",
      "author" : [ "Lora Aroyo", "Lucas Dixon", "Nithum Thain", "Olivia Redfield", "Rachel Rosen." ],
      "venue" : "Companion Proceedings of The 2019 World Wide Web Conference,",
      "citeRegEx" : "Aroyo et al\\.,? 2019",
      "shortCiteRegEx" : "Aroyo et al\\.",
      "year" : 2019
    }, {
      "title" : "Inter-coder agreement for computational linguistics",
      "author" : [ "Ron Artstein", "Massimo Poesio." ],
      "venue" : "Computational Linguistics, 34(4):555–596.",
      "citeRegEx" : "Artstein and Poesio.,? 2008",
      "shortCiteRegEx" : "Artstein and Poesio.",
      "year" : 2008
    }, {
      "title" : "A generalization of cohen’s kappa agreement measure to interval measurement and multiple raters",
      "author" : [ "Kenneth J. Berry", "Paul W. Mielke Jr." ],
      "venue" : "Educational and Psychological Measurement, 48(4):921– 933.",
      "citeRegEx" : "Berry and Jr.,? 1988",
      "shortCiteRegEx" : "Berry and Jr.",
      "year" : 1988
    }, {
      "title" : "Freebase: A collaboratively created graph database for structuring human knowledge",
      "author" : [ "Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor." ],
      "venue" : "Proceedings of the 2008 ACM SIGMOD International Conference on Management",
      "citeRegEx" : "Bollacker et al\\.,? 2008",
      "shortCiteRegEx" : "Bollacker et al\\.",
      "year" : 2008
    }, {
      "title" : "Interpreting kappa in observational research: Baserate matters",
      "author" : [ "Cornelia Taylor Bruckner", "Paul Yoder." ],
      "venue" : "American journal on mental retardation, 111(6):433–441.",
      "citeRegEx" : "Bruckner and Yoder.,? 2006",
      "shortCiteRegEx" : "Bruckner and Yoder.",
      "year" : 2006
    }, {
      "title" : "Bias, prevalence and kappa",
      "author" : [ "Ted Byrt", "Bishop Janet", "John Carlin." ],
      "venue" : "J Clin Epidemiol, 46(5):423–",
      "citeRegEx" : "Byrt et al\\.,? 1993",
      "shortCiteRegEx" : "Byrt et al\\.",
      "year" : 1993
    }, {
      "title" : "The reliability of a dialogue structure coding",
      "author" : [ "Jean Carletta", "Amy Isard", "Stephen Isard", "Jacqueline C Kowtko", "Gwyneth Doherty-Sneddon", "Anne H. Anderson" ],
      "venue" : null,
      "citeRegEx" : "Carletta et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Carletta et al\\.",
      "year" : 1997
    }, {
      "title" : "High agreement but low kappa: Ii",
      "author" : [ "Domenic V Cicchetti", "Alvan R Feinstein." ],
      "venue" : "resolving the paradoxes. Journal of clinical epidemiology, 43(6):551–558.",
      "citeRegEx" : "Cicchetti and Feinstein.,? 1990",
      "shortCiteRegEx" : "Cicchetti and Feinstein.",
      "year" : 1990
    }, {
      "title" : "A coefficient of agreement for nominal scales",
      "author" : [ "Jacob Cohen." ],
      "venue" : "Educational and psychological measurement, 20(1):37–46.",
      "citeRegEx" : "Cohen.,? 1960",
      "shortCiteRegEx" : "Cohen.",
      "year" : 1960
    }, {
      "title" : "Weighted kappa: nominal scale agreement provision for scaled disagreement or partial credit",
      "author" : [ "Jacob Cohen." ],
      "venue" : "Psychological bulletin, 70(4):213.",
      "citeRegEx" : "Cohen.,? 1968",
      "shortCiteRegEx" : "Cohen.",
      "year" : 1968
    }, {
      "title" : "Self-report captures 27 distinct categories of emotion bridged by continuous gradients",
      "author" : [ "Alan Cowen", "Dacher Keltner." ],
      "venue" : "Proceedings of the National Academy of Sciences of the United States of America, 114.",
      "citeRegEx" : "Cowen and Keltner.,? 2017",
      "shortCiteRegEx" : "Cowen and Keltner.",
      "year" : 2017
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "J. Deng", "W. Dong", "R. Socher", "L. Li", "Kai Li", "Li Fei-Fei." ],
      "venue" : "2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248– 255.",
      "citeRegEx" : "Deng et al\\.,? 2009",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2009
    }, {
      "title" : "Identifying statistical bias in dataset replication",
      "author" : [ "Logan Engstrom", "Andrew Ilyas", "Shibani Santurkar", "Dimitris Tsipras", "Jacob Steinhardt", "Aleksander Madry." ],
      "venue" : "International Conference on Machine Learning, pages 2922–2932. PMLR.",
      "citeRegEx" : "Engstrom et al\\.,? 2020",
      "shortCiteRegEx" : "Engstrom et al\\.",
      "year" : 2020
    }, {
      "title" : "High agreement but low kappa: I",
      "author" : [ "Alvan R Feinstein", "Domenic V Cicchetti." ],
      "venue" : "the problems of two paradoxes. Journal of clinical epidemiology, 43(6):543–549.",
      "citeRegEx" : "Feinstein and Cicchetti.,? 1990",
      "shortCiteRegEx" : "Feinstein and Cicchetti.",
      "year" : 1990
    }, {
      "title" : "Measuring nominal scale agreement among many raters",
      "author" : [ "J.L. Fleiss." ],
      "venue" : "Psychological Bulletin, 76(5):378–382.",
      "citeRegEx" : "Fleiss.,? 1971",
      "shortCiteRegEx" : "Fleiss.",
      "year" : 1971
    }, {
      "title" : "The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability",
      "author" : [ "Joseph L Fleiss", "Jacob Cohen." ],
      "venue" : "Educational and psychological measurement, 33(3):613– 619.",
      "citeRegEx" : "Fleiss and Cohen.,? 1973",
      "shortCiteRegEx" : "Fleiss and Cohen.",
      "year" : 1973
    }, {
      "title" : "Computing inter-rater reliability and its variance in the presence of high agreement",
      "author" : [ "Kilem Li Gwet." ],
      "venue" : "British Journal of Mathematical and Statistical Psychology, 61(1):29–48.",
      "citeRegEx" : "Gwet.,? 2008",
      "shortCiteRegEx" : "Gwet.",
      "year" : 2008
    }, {
      "title" : "Computing inter-rater reliability for observational data: An overview and tutorial",
      "author" : [ "Kevin A Hallgren." ],
      "venue" : "Tutorials in quantitative methods for psychology, 8(1):23–34.",
      "citeRegEx" : "Hallgren.,? 2012",
      "shortCiteRegEx" : "Hallgren.",
      "year" : 2012
    }, {
      "title" : "Answering the call for a standard reliability measure for coding data",
      "author" : [ "Andrew F Hayes", "Klaus Krippendorff." ],
      "venue" : "Communication methods and measures, 1(1):77–89.",
      "citeRegEx" : "Hayes and Krippendorff.,? 2007",
      "shortCiteRegEx" : "Hayes and Krippendorff.",
      "year" : 2007
    }, {
      "title" : "Reference standards, judges, and comparison subjects: roles for experts in evaluating system performance",
      "author" : [ "George Hripcsak", "Adam Wilcox." ],
      "venue" : "Journal of the American Medical Informatics Association, 9(1):1–15.",
      "citeRegEx" : "Hripcsak and Wilcox.,? 2002",
      "shortCiteRegEx" : "Hripcsak and Wilcox.",
      "year" : 2002
    }, {
      "title" : "Artificial intelligence faces reproducibility crisis",
      "author" : [ "Matthew Hutson" ],
      "venue" : null,
      "citeRegEx" : "Hutson.,? \\Q2018\\E",
      "shortCiteRegEx" : "Hutson.",
      "year" : 2018
    }, {
      "title" : "A measure of agreement for interval or nominal multivariate observations",
      "author" : [ "Harald Janson", "Ulf Olsson." ],
      "venue" : "Educational and Psychological Measurement, 61(2):277–289.",
      "citeRegEx" : "Janson and Olsson.,? 2001",
      "shortCiteRegEx" : "Janson and Olsson.",
      "year" : 2001
    }, {
      "title" : "A measure of agreement for interval or nominal multivariate observations by different sets of judges",
      "author" : [ "Harald Janson", "Ulf Olsson." ],
      "venue" : "Educational and Psychological Measurement, 64(1):62–70.",
      "citeRegEx" : "Janson and Olsson.,? 2004",
      "shortCiteRegEx" : "Janson and Olsson.",
      "year" : 2004
    }, {
      "title" : "The anatomy of a large-scale human computation engine",
      "author" : [ "Shailesh Kochhar", "Stefano Mazzocchi", "Praveen Paritosh." ],
      "venue" : "Proceedings of the ACM SIGKDD workshop on Human Computation, pages 10–17.",
      "citeRegEx" : "Kochhar et al\\.,? 2010",
      "shortCiteRegEx" : "Kochhar et al\\.",
      "year" : 2010
    }, {
      "title" : "Online controlled experiments and a/b testing",
      "author" : [ "Ron Kohavi", "Roger Longbotham." ],
      "venue" : "Encyclopedia of machine learning and data mining, 7(8):922–929.",
      "citeRegEx" : "Kohavi and Longbotham.,? 2017",
      "shortCiteRegEx" : "Kohavi and Longbotham.",
      "year" : 2017
    }, {
      "title" : "Bivariate agreement coefficients for reliability of data",
      "author" : [ "Klaus Krippendorff." ],
      "venue" : "Sociological methodology, 2:139–150.",
      "citeRegEx" : "Krippendorff.,? 1970",
      "shortCiteRegEx" : "Krippendorff.",
      "year" : 1970
    }, {
      "title" : "Content analysis: An introduction to its methodology",
      "author" : [ "Klaus Krippendorff." ],
      "venue" : "Sage publications.",
      "citeRegEx" : "Krippendorff.,? 2004",
      "shortCiteRegEx" : "Krippendorff.",
      "year" : 2004
    }, {
      "title" : "The measurement of observer agreement for categorical data",
      "author" : [ "J. Richard Landis", "Gary G. Koch." ],
      "venue" : "Biometrics, 33(1):159–74.",
      "citeRegEx" : "Landis and Koch.,? 1977",
      "shortCiteRegEx" : "Landis and Koch.",
      "year" : 1977
    }, {
      "title" : "Deep learning",
      "author" : [ "Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton." ],
      "venue" : "nature, 521(7553):436–444.",
      "citeRegEx" : "LeCun et al\\.,? 2015",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 2015
    }, {
      "title" : "Measures of response agreement for qualitative data: Some generalizations and alternatives",
      "author" : [ "Richard J. Light." ],
      "venue" : "Psychological Bulletin, 76(5):365– 377.",
      "citeRegEx" : "Light.,? 1971",
      "shortCiteRegEx" : "Light.",
      "year" : 1971
    }, {
      "title" : "The correction for attenuation",
      "author" : [ "Paul M Muchinsky." ],
      "venue" : "Educational and psychological measurement, 56(1):63–75.",
      "citeRegEx" : "Muchinsky.,? 1996",
      "shortCiteRegEx" : "Muchinsky.",
      "year" : 1996
    }, {
      "title" : "Human computation must be reproducible",
      "author" : [ "Praveen Paritosh." ],
      "venue" : "WWW 2012, Lyon.",
      "citeRegEx" : "Paritosh.,? 2012",
      "shortCiteRegEx" : "Paritosh.",
      "year" : 2012
    }, {
      "title" : "Death to kappa: birth of quantity disagreement and allocation disagreement for accuracy assessment",
      "author" : [ "Robert Gilmore Pontius Jr", "Marco Millones." ],
      "venue" : "International Journal of Remote Sensing, 32(15):4407–4429.",
      "citeRegEx" : "Jr and Millones.,? 2011",
      "shortCiteRegEx" : "Jr and Millones.",
      "year" : 2011
    }, {
      "title" : "On agreement indices for nominal data",
      "author" : [ "Roel Popping." ],
      "venue" : "Sociometric research, pages 90–105. Springer.",
      "citeRegEx" : "Popping.,? 1988",
      "shortCiteRegEx" : "Popping.",
      "year" : 1988
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning, pages 5389–5400",
      "author" : [ "Benjamin Recht", "Rebecca Roelofs", "Ludwig Schmidt", "Vaishaal Shankar." ],
      "venue" : "PMLR.",
      "citeRegEx" : "Recht et al\\.,? 2019",
      "shortCiteRegEx" : "Recht et al\\.",
      "year" : 2019
    }, {
      "title" : "Using intrinsic and extrinsic metrics to evaluate accuracy and facilitation in computerassisted coding",
      "author" : [ "Philip Resnik", "Michael Niv", "Michael Nossal", "Gregory Schnitzer", "Jean Stoner", "Andrew Kapit", "Richard Toren." ],
      "venue" : "Perspectives in Health Informa-",
      "citeRegEx" : "Resnik et al\\.,? 2006",
      "shortCiteRegEx" : "Resnik et al\\.",
      "year" : 2006
    }, {
      "title" : "Rigorously collecting commonsense judgments for complex question-answer content",
      "author" : [ "Mehrnoosh Sameki", "Aditya Barua", "Praveen Paritosh." ],
      "venue" : "Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, volume 3.",
      "citeRegEx" : "Sameki et al\\.,? 2015",
      "shortCiteRegEx" : "Sameki et al\\.",
      "year" : 2015
    }, {
      "title" : "The ai bookie bet: Will machine learning outgrow human labeling",
      "author" : [ "Mike Schaekermann", "Christopher Homan", "Lora Aroyo", "Praveen Paritosh", "Kurt Bollacker", "Chris Welty" ],
      "venue" : "AI Magazine,",
      "citeRegEx" : "Schaekermann et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Schaekermann et al\\.",
      "year" : 2020
    }, {
      "title" : "Reliability of content analysis: The case of nominal scale coding",
      "author" : [ "W Scott." ],
      "venue" : "Public Opinion Quarterly, (19):321—-325.",
      "citeRegEx" : "Scott.,? 1955",
      "shortCiteRegEx" : "Scott.",
      "year" : 1955
    }, {
      "title" : "Cheap and fast – but is it good? evaluating non-expert annotations for natural language tasks",
      "author" : [ "Rion Snow", "Brendan O’Connor", "Daniel Jurafsky", "Andrew Ng" ],
      "venue" : "In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Snow et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Snow et al\\.",
      "year" : 2008
    }, {
      "title" : "The proof and measurement of association between two things",
      "author" : [ "C. Spearman." ],
      "venue" : "The American Journal of Psychology, 15(1).",
      "citeRegEx" : "Spearman.,? 1904",
      "shortCiteRegEx" : "Spearman.",
      "year" : 1904
    }, {
      "title" : "A proposed solution to the base rate problem in the kappa statistic",
      "author" : [ "Edward L. Spitznagel", "John E. Helzer." ],
      "venue" : "Archives of General Psychiatry, 42(7):725– 728.",
      "citeRegEx" : "Spitznagel and Helzer.,? 1985",
      "shortCiteRegEx" : "Spitznagel and Helzer.",
      "year" : 1985
    }, {
      "title" : "A partial solution to the base rate problem of the k statistic",
      "author" : [ "Gavin W. Stewart", "Joseph M. Rey." ],
      "venue" : "Archives of general psychiatry, 45(5):504–505.",
      "citeRegEx" : "Stewart and Rey.,? 1988",
      "shortCiteRegEx" : "Stewart and Rey.",
      "year" : 1988
    }, {
      "title" : "Diversity of decision-making models and the measurement of interrater agreement",
      "author" : [ "John S Uebersax." ],
      "venue" : "Psychological bulletin, 101(1):140.",
      "citeRegEx" : "Uebersax.,? 1987",
      "shortCiteRegEx" : "Uebersax.",
      "year" : 1987
    }, {
      "title" : "A formal proof of a paradox associated with cohen’s kappa",
      "author" : [ "Matthijs J Warrens." ],
      "venue" : "Journal of Classification, 27(3):322–332.",
      "citeRegEx" : "Warrens.,? 2010",
      "shortCiteRegEx" : "Warrens.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "When collecting annotations and labeled data from humans, a standard practice is to use inter-rater reliability (IRR) as a measure of data goodness (Hallgren, 2012).",
      "startOffset" : 148,
      "endOffset" : 164
    }, {
      "referenceID" : 26,
      "context" : "When relying on human observers, researchers must worry about the quality of the data — specifically, their reliability (Krippendorff, 2004).",
      "startOffset" : 120,
      "endOffset" : 140
    }, {
      "referenceID" : 17,
      "context" : "Are the annotations collected reproducible, or are they the result of human idiosyncrasies? Respectable scholarly journals typically require reporting quantitative evidence for the inter-rater reliability (IRR) of the data (Hallgren, 2012).",
      "startOffset" : 223,
      "endOffset" : 239
    }, {
      "referenceID" : 8,
      "context" : "Cohen’s kappa (Cohen, 1960) or Krippendorff’s alpha (Hayes and Krippendorff, 2007) is expected to be Figure 1: Agreement measures for categorical data (Landis and Koch, 1977)",
      "startOffset" : 14,
      "endOffset" : 27
    }, {
      "referenceID" : 18,
      "context" : "Cohen’s kappa (Cohen, 1960) or Krippendorff’s alpha (Hayes and Krippendorff, 2007) is expected to be Figure 1: Agreement measures for categorical data (Landis and Koch, 1977)",
      "startOffset" : 52,
      "endOffset" : 82
    }, {
      "referenceID" : 27,
      "context" : "Cohen’s kappa (Cohen, 1960) or Krippendorff’s alpha (Hayes and Krippendorff, 2007) is expected to be Figure 1: Agreement measures for categorical data (Landis and Koch, 1977)",
      "startOffset" : 151,
      "endOffset" : 174
    }, {
      "referenceID" : 28,
      "context" : "Driven by the datahungry success of machine learning (LeCun et al., 2015; Schaekermann et al., 2020), there has been an explosive growth in the use of crowdsourcing for building datasets and benchmarks (Snow et al.",
      "startOffset" : 53,
      "endOffset" : 100
    }, {
      "referenceID" : 38,
      "context" : "Driven by the datahungry success of machine learning (LeCun et al., 2015; Schaekermann et al., 2020), there has been an explosive growth in the use of crowdsourcing for building datasets and benchmarks (Snow et al.",
      "startOffset" : 53,
      "endOffset" : 100
    }, {
      "referenceID" : 40,
      "context" : ", 2020), there has been an explosive growth in the use of crowdsourcing for building datasets and benchmarks (Snow et al., 2008; Kochhar et al., 2010).",
      "startOffset" : 109,
      "endOffset" : 150
    }, {
      "referenceID" : 23,
      "context" : ", 2020), there has been an explosive growth in the use of crowdsourcing for building datasets and benchmarks (Snow et al., 2008; Kochhar et al., 2010).",
      "startOffset" : 109,
      "endOffset" : 150
    }, {
      "referenceID" : 0,
      "context" : "A rise in task diversity There has been an increasing amount of subjective tasks with genuine ambiguity: judging toxicity of online discussions (Aroyo et al., 2019), in which the IRR values range",
      "startOffset" : 144,
      "endOffset" : 164
    }, {
      "referenceID" : 10,
      "context" : "4; judging emotions expressed by faces (Cowen and Keltner, 2017), in which more than 80% of the IRR values are below 0.",
      "startOffset" : 39,
      "endOffset" : 64
    }, {
      "referenceID" : 24,
      "context" : "6; and A/B testing of user satisfaction or preference evaluations (Kohavi and Longbotham, 2017), where IRR values are typically between 0.",
      "startOffset" : 66,
      "endOffset" : 95
    }, {
      "referenceID" : 5,
      "context" : "High class imbalance has led to many complaints of IRR interpretability (Byrt et al., 1993; Feinstein and Cicchetti, 1990; Cicchetti and Feinstein, 1990).",
      "startOffset" : 72,
      "endOffset" : 153
    }, {
      "referenceID" : 13,
      "context" : "High class imbalance has led to many complaints of IRR interpretability (Byrt et al., 1993; Feinstein and Cicchetti, 1990; Cicchetti and Feinstein, 1990).",
      "startOffset" : 72,
      "endOffset" : 153
    }, {
      "referenceID" : 7,
      "context" : "High class imbalance has led to many complaints of IRR interpretability (Byrt et al., 1993; Feinstein and Cicchetti, 1990; Cicchetti and Feinstein, 1990).",
      "startOffset" : 72,
      "endOffset" : 153
    }, {
      "referenceID" : 31,
      "context" : "Machine learning, crowdsourcing, and data research papers and tracks have abandoned the use and reporting of IRR for human labeled data, despite calls for it (Paritosh, 2012).",
      "startOffset" : 158,
      "endOffset" : 174
    }, {
      "referenceID" : 34,
      "context" : "used by the community such as SQuAD (Rajpurkar et al., 2016), ImageNet (Deng et al.",
      "startOffset" : 36,
      "endOffset" : 60
    }, {
      "referenceID" : 11,
      "context" : ", 2016), ImageNet (Deng et al., 2009), Freebase (Bollacker et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 3,
      "context" : ", 2009), Freebase (Bollacker et al., 2008), have never reported IRR values.",
      "startOffset" : 18,
      "endOffset" : 42
    }, {
      "referenceID" : 20,
      "context" : "ing against the backdrop of a reproducibility crisis in artificial intelligence (Hutson, 2018).",
      "startOffset" : 80,
      "endOffset" : 94
    }, {
      "referenceID" : 6,
      "context" : "Since then, Cohen’s (1960) kappa and its variants (Carletta et al., 1997; Cohen, 1968) have become the de facto standard for measuring agreement in computational linguistics.",
      "startOffset" : 50,
      "endOffset" : 86
    }, {
      "referenceID" : 9,
      "context" : "Since then, Cohen’s (1960) kappa and its variants (Carletta et al., 1997; Cohen, 1968) have become the de facto standard for measuring agreement in computational linguistics.",
      "startOffset" : 50,
      "endOffset" : 86
    }, {
      "referenceID" : 44,
      "context" : "7055 Warrens, 2010), or the ‘base rates’ problem (Uebersax, 1987).",
      "startOffset" : 49,
      "endOffset" : 65
    }, {
      "referenceID" : 16,
      "context" : "Others have proposed measures that are more robust against class imbalance (Gwet, 2008; Spitznagel and Helzer, 1985; Stewart and Rey, 1988).",
      "startOffset" : 75,
      "endOffset" : 139
    }, {
      "referenceID" : 42,
      "context" : "Others have proposed measures that are more robust against class imbalance (Gwet, 2008; Spitznagel and Helzer, 1985; Stewart and Rey, 1988).",
      "startOffset" : 75,
      "endOffset" : 139
    }, {
      "referenceID" : 43,
      "context" : "Others have proposed measures that are more robust against class imbalance (Gwet, 2008; Spitznagel and Helzer, 1985; Stewart and Rey, 1988).",
      "startOffset" : 75,
      "endOffset" : 139
    }, {
      "referenceID" : 39,
      "context" : "Unlike Scott’s pi (Scott, 1955), Fleiss’s kappa (Fleiss, 1971), Krippendorf’s alpha (Krippendorff, 2004), and many others, Cohen’s (1960) kappa allows for two different marginal distributions.",
      "startOffset" : 18,
      "endOffset" : 31
    }, {
      "referenceID" : 14,
      "context" : "Unlike Scott’s pi (Scott, 1955), Fleiss’s kappa (Fleiss, 1971), Krippendorf’s alpha (Krippendorff, 2004), and many others, Cohen’s (1960) kappa allows for two different marginal distributions.",
      "startOffset" : 48,
      "endOffset" : 62
    }, {
      "referenceID" : 26,
      "context" : "Unlike Scott’s pi (Scott, 1955), Fleiss’s kappa (Fleiss, 1971), Krippendorf’s alpha (Krippendorff, 2004), and many others, Cohen’s (1960) kappa allows for two different marginal distributions.",
      "startOffset" : 84,
      "endOffset" : 104
    }, {
      "referenceID" : 29,
      "context" : "Since its publication, it has been generalized to accommodate multiple raters (Light, 1971; Berry and Mielke Jr, 1988), and to cover different types of annotation scales: ordinal (Cohen, 1968), interval (Berry and Mielke Jr, 1988; Janson and Olsson, 2001), multivariate (Berry and Mielke Jr, 1988), and any arbitrary distance function (Artstein and Poesio, 2008).",
      "startOffset" : 78,
      "endOffset" : 118
    }, {
      "referenceID" : 9,
      "context" : "Since its publication, it has been generalized to accommodate multiple raters (Light, 1971; Berry and Mielke Jr, 1988), and to cover different types of annotation scales: ordinal (Cohen, 1968), interval (Berry and Mielke Jr, 1988; Janson and Olsson, 2001), multivariate (Berry and Mielke Jr, 1988), and any arbitrary distance function (Artstein and Poesio, 2008).",
      "startOffset" : 179,
      "endOffset" : 192
    }, {
      "referenceID" : 21,
      "context" : "Since its publication, it has been generalized to accommodate multiple raters (Light, 1971; Berry and Mielke Jr, 1988), and to cover different types of annotation scales: ordinal (Cohen, 1968), interval (Berry and Mielke Jr, 1988; Janson and Olsson, 2001), multivariate (Berry and Mielke Jr, 1988), and any arbitrary distance function (Artstein and Poesio, 2008).",
      "startOffset" : 203,
      "endOffset" : 255
    }, {
      "referenceID" : 1,
      "context" : "Since its publication, it has been generalized to accommodate multiple raters (Light, 1971; Berry and Mielke Jr, 1988), and to cover different types of annotation scales: ordinal (Cohen, 1968), interval (Berry and Mielke Jr, 1988; Janson and Olsson, 2001), multivariate (Berry and Mielke Jr, 1988), and any arbitrary distance function (Artstein and Poesio, 2008).",
      "startOffset" : 335,
      "endOffset" : 362
    }, {
      "referenceID" : 15,
      "context" : "We follow Janson and Olsson’s approach because squared distance leads to desirable properties and familiar interpretation of coefficients (Fleiss and Cohen, 1973; Krippendorff, 1970).",
      "startOffset" : 138,
      "endOffset" : 182
    }, {
      "referenceID" : 25,
      "context" : "We follow Janson and Olsson’s approach because squared distance leads to desirable properties and familiar interpretation of coefficients (Fleiss and Cohen, 1973; Krippendorff, 1970).",
      "startOffset" : 138,
      "endOffset" : 182
    }, {
      "referenceID" : 26,
      "context" : "Squared distance is also used in alpha (Krippendorff, 2004).",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 41,
      "context" : "First, we take inspirations from Spearman’s correction for attenuation (Spearman, 1904):",
      "startOffset" : 71,
      "endOffset" : 87
    }, {
      "referenceID" : 30,
      "context" : "First, we can interpret normalized κx like a disattenuated correlation, ρxy (see (Muchinsky, 1996) for a rigorous interpretation).",
      "startOffset" : 81,
      "endOffset" : 98
    }, {
      "referenceID" : 4,
      "context" : "Task subjectivity and class imbalance together impose a practical limit on kappa (Bruckner and Yoder, 2006).",
      "startOffset" : 81,
      "endOffset" : 107
    }, {
      "referenceID" : 37,
      "context" : "If the crowd IRR is much higher, that may be an indication of collusion, or a set of overly simplistic guidelines that have deviated from the experiment fidelity (Sameki et al., 2015).",
      "startOffset" : 162,
      "endOffset" : 183
    }, {
      "referenceID" : 36,
      "context" : "At a glance, this approach seems similar to the common practice of measuring the accuracy of crowdsourced data against the ground truth (Resnik et al., 2006; Hripcsak and Wilcox, 2002).",
      "startOffset" : 136,
      "endOffset" : 184
    }, {
      "referenceID" : 19,
      "context" : "At a glance, this approach seems similar to the common practice of measuring the accuracy of crowdsourced data against the ground truth (Resnik et al., 2006; Hripcsak and Wilcox, 2002).",
      "startOffset" : 136,
      "endOffset" : 184
    } ],
    "year" : 2021,
    "abstractText" : "When collecting annotations and labeled data from humans, a standard practice is to use inter-rater reliability (IRR) as a measure of data goodness (Hallgren, 2012). Metrics such as Krippendorff’s alpha or Cohen’s kappa are typically required to be above a threshold of 0.6 (Landis and Koch, 1977). These absolute thresholds are unreasonable for crowdsourced data from annotators with high cultural and training variances, especially on subjective topics. We present a new alternative to interpreting IRR that is more empirical and contextualized. It is based upon benchmarking IRR against baseline measures in a replication, one of which is a novel cross-replication reliability (xRR) measure based on Cohen’s (1960) kappa. We call this approach the xRR framework. We opensource a replication dataset of 4 million human judgements of facial expressions and analyze it with the proposed framework. We argue this framework can be used to measure the quality of crowdsourced datasets.",
    "creator" : "LaTeX with hyperref"
  }
}