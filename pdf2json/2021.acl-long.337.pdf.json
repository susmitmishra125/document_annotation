{
  "name" : "2021.acl-long.337.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Hierarchy-aware Label Semantics Matching Network for Hierarchical Text Classification",
    "authors" : [ "Haibin Chen", "Qianli Ma", "Zhenxi Lin", "Jiangyue Yan", "Ee-Peng Lim" ],
    "emails" : [ "chen@foxmail.com", "qianlima@scut.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4370–4379\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4370"
    }, {
      "heading" : "1 Introduction",
      "text" : "Hierarchical text classification (HTC) is widely used in Natural Language Processing (NLP), such as news categorization (Lewis et al., 2004) and scientific paper classification (Kowsari et al., 2017). HTC is a particular multi-label text classification problem, which introduces hierarchies to organize label structure. As depicted in Figure 1, HTC models predict multiple labels in a given label hierarchy, which generally construct one or multiple paths from coarse-grained labels to fine-grained labels in a top-down manner (Aixin Sun and Ee-Peng Lim, 2001). Generally speaking, fine-grained labels are the most appropriate labels for describing the input text. Coarse-grained labels are generally the parent nodes of coarse- or fine-grained labels, expressing a more general concept. The key challenges of\n∗*Corresponding author\nHTC are to model the large-scale, imbalanced, and structured label hierarchy (Mao et al., 2019).\nInput Text: \"Global debt is set to reach $200 trillion ...\"\nExisting work in HTC has introduced various methods to use hierarchical information in a holistic way. To capture the holistic label correlation features, some researchers proposed a hierarchyaware global model to exploit the prior probability of label dependencies through Graph Convolution Networks (GCN) and TreeLSTM (Zhou et al., 2020). Some researchers also introduced more label correlation features such as label semantic similarity and label co-occurrence (Lu et al., 2020). They followed the traditional way to transform HTC into multiple binary classifiers for every label (Fürnkranz et al., 2008). However, they ignored the interaction between text semantics and label semantics (Fürnkranz et al., 2008; Wang et al., 2019), which is highly useful for classification (Chen et al., 2020). Hence, their models may not be sufficient to model complex label dependencies and provide comparable text-label classification scores (Wang et al., 2019).\nA natural strategy for modeling the interaction between text semantics and label semantics is to introduce a text-label joint embedding by label attention (Xiao et al., 2019) or autoencoders (Yeh et al., 2017). Label attention-based methods adopted a\nself-attention mechanism to identify label-specific information (Xiao et al., 2019). Autoencoder-based methods extended the vanilla Canonical Correlated Autoencoder (Yeh et al., 2017) to a ranking-based autoencoder architecture to produce comparable text-label scores (Wang et al., 2019). However, these methods assume all the labels are independent without fully considering the correlation between coarse-grained labels and fine-grained labels, which cannot be simply transferred to HTC models (Zhou et al., 2020).\nIn this paper, we formulate the interaction between text and label as a semantic matching problem and propose a Hierarchy-aware Label Semantics Matching Network (HiMatch). The principal idea is that the text representations should be semantically similar to the target label representations (especially fine-grained labels), while they should be semantically far away from the incorrect label representations. First, we adopt a text encoder and a label encoder (shown in Figure 2) to extract textual semantics and label semantics, respectively. Second, inspired by the methods of learning common embeddings (Wang et al., 2019), we project both textual semantics and label semantics into a text-label joint embedding space where correlations between text and labels are exploited. In this joint embedding space, we introduce a joint embedding loss between text semantics and target label semantics to learn a text-label joint embedding. After that, we apply a matching learning loss to capture text-label matching relationships in a hierarchy-aware manner. In this way, the finegrained labels are semantically closest to the text semantics, followed by the coarse-grained labels, while the incorrect labels should be semantically far away from the text semantics. Hence, we propose a hierarchy-aware matching learning method to capture different matching relationships through different penalty margins on semantic distances. Finally, we employ the textual representations guided by the joint embedding loss and matching learning loss to perform the hierarchical text classification.\nThe major contributions of this paper are:\n1. By considering the text-label semantics matching relationship, we are the first to formulate HTC as a semantic matching problem rather than merely multiple binary classification tasks.\n2. We propose a hierarchy-aware label semantics matching network (HiMatch), in which we introduce a joint embedding loss and a matching learn-\ning loss to learn the text-label semantics matching relationship in a hierarchy-aware manner.\n3. Extensive experiments (with/without BERT) on various datasets show that our model achieves state-of-the-art results."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Hierarchical Text Classification",
      "text" : "Hierarchical text classification is a particular multilabel text classification problem, where the classification results are assigned to one or more nodes of a taxonomic hierarchy. Existing state-of-the-art methods focus on encoding hierarchy constraint in a global view such as directed graph and tree structure. Zhou et al. (2020) proposed a hierarchyaware global model to exploit the prior probability of label dependencies. Lu et al. (2020) introduced three kinds of label knowledge graphs, i.e., taxonomy graph, semantic similarity graph, and cooccurrence graph to benefit hierarchical text classification. They regarded hierarchical text classification as multiple binary classification tasks (Fürnkranz et al., 2008). The limitation is that these models did not consider the interaction of label semantics and text semantics. Therefore, they failed to capture complex label dependencies and can not provide comparable text-label classification scores (Wang et al., 2019), which leads to restricted performance (Chen et al., 2020). Hence, it is crucial to exploit the relationship between text and label semantics, and help the model distinguish target labels from incorrect labels in a comparable and hierarchy-aware manner. We perform matching learning in a joint embedding of text and label to solve these problems in this work."
    }, {
      "heading" : "2.2 Exploit Joint Embedding of Text and Label",
      "text" : "To determine the correlation between text and label, researchers proposed various methods to exploit a text-label joint embedding such as (Xiao et al., 2019) or Autoencoder (Yeh et al., 2017). In the field of multi-label text classification, Xiao et al. (2019) proposed a Label-Specific Attention Network (LSAN) to learn a text-label joint embedding by label semantic and document semantic. Wang et al. (2019) extended vanilla Canonical Correlated AutoEncoder (Yeh et al., 2017) to a ranking-based autoencoder architecture to produce comparable label scores. However, they did not fully consider label semantics and holistic label correlation\namong fine-grained labels, coarse-grained labels, and incorrect labels. In addition, we can not simply transfer these multi-label classification methods to HTC due to the constraint of hierarchy (Zhou et al., 2020)."
    }, {
      "heading" : "3 Proposed Method",
      "text" : "In this section, we will describe the details about our Hierarchy-aware Label Semantics Matching Network. Figure 2 shows the overall architecture of our proposed model."
    }, {
      "heading" : "3.1 Text Encoder",
      "text" : "In the HTC task, given the input sequence xseq = {x1, ..., xn}, the model will predict the label y = {y1, ..., yk} where n is the number of words and k is the number of label sets. The label with a probability higher than a fixed threshold (0.5) will be regarded as the prediction result. The sequence of token embeddings is firstly fed into a bidirectional GRU layer to extract contextual feature H = {h1, ..., hn}. Then, CNN layers with top-k max-pooling are adopted for generating key n-gram features T ∈ Rk×dcnn where dcnn indicates the output dimension of the CNN layer.\nFollowing the previous work (Zhou et al., 2020), we further introduce a hierarchy-aware text feature propagation module to encode label hierarchy information. We define a hierarchy label structure\nas a directed graph G = ( Vt, ←− E , −→ E ) , where Vt indicates the set of hierarchy structure nodes. ←− E are built from the top-down hierarchy paths representing the prior statistical probability from parent nodes to children nodes. −→ E are built from the bottom-up hierarchy paths representing the connection relationship from children nodes to parent nodes. The feature size of graph adjacency matrix ← E and → E is ∈ Rk×k, where k is the number of label sets. Text feature propagation module firstly projects text features T to node inputs Vt by a linear transformation Wproj ∈ Rk×dcnn×dt , where dt represents the hierarchy structure node dimension from text feature. Then a Graph Convolution Network (GCN) is adopted to explicitly combine text semantics with prior hierarchical information←− E and −→ E :\nSt = σ (←− E · Vt ·Wg1 + −→ E · Vt ·Wg2 ) (1)\nwhere σ is the activation function ReLU. Wg1,Wg2 ∈ Rdt×dt are the weight matrix of GCN. St is the text representation aware of prior hierarchy paths."
    }, {
      "heading" : "3.2 Label Encoder",
      "text" : "In the HTC task, the hierarchical label structure can be regarded as a directed graph G = ( Vl, ←− E , −→ E ) ,\nwhere Vl indicates the set of hierarchy structure nodes with label representation. The graph G in label encoder shares the same structure ←− E and −→ E with the graph in text encoder. Given the total label set y = {y1, ..., yk} as input, we create label embeddings Vl ∈ Rdl by averaging of pre-trained label embeddings first. Then GCN could be utilized as label encoder:\nSl = σ (←− E · Vl ·Wg3 + −→ E · Vl ·Wg4 ) (2)\nwhere σ is the activation function ReLU. Wg3,Wg4 ∈ Rdl×dl are the weight matrix of GCN. Sl is the label representation aware of prior hierarchy paths. It must be noted that the weight matrix and input representation of the label encoder are different with those in the text encoder."
    }, {
      "heading" : "3.3 Label Semantics Matching",
      "text" : ""
    }, {
      "heading" : "3.3.1 Joint Embedding Learning",
      "text" : "In this section, we will introduce the methods of learning a text-label joint embedding and hierarchyaware matching relationship. For joint embedding learning, firstly, we project text semantics St and label semantics Sl into a common latent space as follows:\nΦt = FFNt (St) , (3)\nΦl = FFNl (Sl) (4)\nwhere FFNt and FFNl are independent two-layer feedforward neural networks. Φt,Φl ∈ Rdϕ represent text semantics and label semantics in joint embedding space, respectively. dϕ indicates the dimension of joint embedding.\nIn order to align the two independent semantic representations in the latent space, we employ the mean squared loss between text semantics and target labels semantics:\nLjoint = ∑\np∈P (y) ∥∥Φt − Φpl ∥∥22 (5) where P (y) is target label sets. Ljoint aims to minimize the common embedding loss between input text and target labels."
    }, {
      "heading" : "3.3.2 Hierarchy-aware Matching Learning",
      "text" : "Based on the text-label joint embedding loss, the model only captures the correlations between text semantics and target labels semantics, while correlations among different granular labels are ignored.\nIn the HTC task, it is expected that the matching relationship between text semantics and fine-grained labels should be the closest, followed by coarsegrained labels. Text semantics and incorrect labels semantics should not be related.\nInsight of these, we propose a hierarchy-aware matching loss Lmatch to incorporate the correlations among text semantics and different labels semantics. Lmatch aims to penalize the small semantic distance between text semantics and incorrect labels semantics with a margin γ:\nLmatch = max ( 0, D ( Φt,Φ p l ) −D (Φt,Φnl ) + γ ) (6)\nwhere Φpl represents target labels semantics and Φnl represents incorrect labels semantics. We use L2-normalized euclidean distance for metricD and γ is a margin constant for margin-based triplet loss. We take the average of all the losses between every label pairs as the margin loss.\nHierarchy-aware Margin Due to the large label sets in the HTC task, it is time-consuming to calculate every label’s matching loss. Therefore, we propose hierarchy-aware sampling to alleviate the problem. Specifically, we sample all parent labels (coarse-grained labels), one sibling label, and one random incorrect label for every fine-grained label to obtain its negative label sets n ∈ N(y). It is also unreasonable to assign the same margin for different label pairs since the label semantics similarity is quite different in a large structured label hierarchy. Our basic idea is that the semantics relationship should be closer if two labels are closer\nin the hierarchical structure. Firstly, the text semantics should match fine-grained labels the most, which is exploited in joint embedding learning. Then we regard the pair with the smallest semantic distance (d1) as a positive pair and regard other textlabel matching pairs as negative pairs. As depicted in the schema figure 3, compared with the positive pair, the semantics matching distance between text and coarse-grained target labels (d2) should be larger. The incorrect sibling labels have a certain semantic relationship with the target labels. Hence, the semantics matching distance between text and the incorrect sibling labels of fine-grained labels (d3) should be further larger, while the semantics matching distance between text and other incorrect labels (d4) should be the largest. We introduce hierarchy-aware penalty margins γ1, γ2, γ3, γ4 to model the comparable relationship. The penalty margin is smaller if we expect the semantic matching distance to be smaller. We neglect γ1 because the matching relationships between text semantics and fine-grained labels are exploited in joint embedding learning. γ2, γ3, γ4 are penalty margins compared with the matching relationships between text semantics and fine-grained labels semantics. We introduce two hyperparameters α, β to measure different matching relationships of γ:\nγ2 = αγ; γ3 = βγ; γ4 = γ (7)\nwhere 0 < α < β < 1. The proposed loss captures the relative semantics similarity rankings among target labels and incorrect labels in a hierarchyaware manner."
    }, {
      "heading" : "3.4 Classification Learning and Objective Function",
      "text" : "We find that it is easier to overfit for classification learning if we perform classification learning in the text-label joint embedding directly. Hence, we use the text semantics representation St guided by joint embedding loss and matching learning loss to perform classification learning. St is fed into a fully connected layer to get the label probability ŷ for prediction.\nThe overall objective function includes a crossentropy category loss, joint embedding loss and hierarchy-aware matching loss:\nL = Lcls(y, ŷ) + λ1Ljoint + λ2Lmatch (8)\nwhere y and ŷ are the ground-truth label and output probability, respectively. λ1, λ2 are the hyperparameters for balancing the joint embedding loss and\nmatching learning loss. We minimize the above function by gradient descent during training."
    }, {
      "heading" : "4 Experiment",
      "text" : ""
    }, {
      "heading" : "4.1 Experiment Setup",
      "text" : "Datasets To evaluate the effectiveness of our model, we conduct experiments on three widelystudied datasets for hierarchical multi-label text classification. Statistics of these datasets are listed in Table 1. RCV1-V2 (Lewis et al., 2004) is a news categorization corpora, and WOS (Kowsari et al., 2017) includes abstracts of published papers from Web of Science. EURLEX57K is a large hierarchical multi-label text classification (LMTC) dataset that contains 57k English EU legislative documents, and is tagged with about 4.3k labels from the European Vocabulary (Chalkidis et al., 2019). The label sets are split into zero-shot labels, few-shot labels, and frequent labels. Few-shot labels are labels whose frequencies in the training set are less than or equal to 50. Frequent labels are labels whose frequencies in the training set are more than 50. The label setting is the same as previous work (Lu et al., 2020). In EURLEX57K, the corpora are only tagged with fine-grained labels, and the parent labels of fine-grained labels are not tagged as the target labels.\nEvaluation Metric On RCV1-V2 and WOS datasets, we measure the experimental results by Micro-F1 and Macro-F1. Micro-F1 takes the overall precision and recall of all the instances into account, while Macro-F1 equals the average F1score of labels. We report the results of two ranking metrics on large hierarchical multi-label text classification dataset EURLEX-57K, including Recall@5 and nDCG@5. The ranking metrics are preferable for EURLEX-57K since it does not introduce a significant bias towards frequent labels (Lu et al., 2020).\nImplementation Details We initialize the word embeddings with 300D pre-trained GloVe vectors\n(Pennington et al., 2014). Then we use a one-layer BiGRU with hidden dimension 100 and used 100 filters with kernel size [2,3,4] to setup the CNNs. The dimension of the text propagation feature and graph convolution weight matrix are both 300. The hidden size of joint embedding is 200. The matching margin γ is set to 0.2 on RCV1-V2 and WOS datasets, and set to 0.5 on EURLEX-57K dataset. We set the value of hierarchy-aware penalty hyperparameters α, β to 0.01 and 0.5, respectively. The loss balancing factor λ1, λ2 are set to 1. For fair comparisons with previous work (Lu et al., 2020; Chalkidis et al., 2019) on EURLEX-57K dataset, firstly, we do not use CNN layer and text feature propagation module. Secondly, to adapt to the zeroshot settings, the prediction is generated by the dot product similarity between text semantics and label semantics. Our model is optimized by Adam with a learning rate of 1e-4.\nFor pretrained language model BERT (Devlin et al., 2018), we use the top-level representation hCLS of BERT’s special CLS token to perform classification. To combine our model with BERT, we replace the text encoder of HiMatch with BERT, and the label representations are initiated by pretrained BERT embedding. The batch size is set to 16, and the learning rate is 2e-5.\nComparison Models On RCV1-V2 and WOS datasets, we compare our model with three types of strong baselines: 1) Text classification baselines: TextRCNN (Lai et al., 2015), TextRCNN with label attention (TextRCNN-LA) (Zhou et al., 2020), and SGM (Yang et al., 2018). 2) Hierarchy-aware models: HE-AGCRCNN (Peng et al., 2019), HMCN (Mao et al., 2019), Htrans (Banerjee et al., 2019), HiLAP-RL (Mao et al., 2019) which introduced reinforcement learning to simulate the assignment process, HiAGM (Zhou et al., 2020) which exploited the prior probability of label dependecies through Graph Convolution Network and TreeLSTM. 3) Pretrained language model: a more powerful pretrained language model BERT (Devlin et al., 2018) than tradition text classification models when fine-tuned on downstream tasks.\nOn EURLEX-57K dataset, we compare our model with strong baselines with/without zeroshot settings such as BIGRU-ATT, BIGRU-LWAN (Chalkidis et al., 2019) which introduced labelwise attention. The models starting with “ZERO” make predictions by calculating similarity scores between text and label semantics for zero-shot set-\ntings. AGRU-KAMG (Lu et al., 2020) is a stateof-the-art model which introduced various label knowledge."
    }, {
      "heading" : "4.2 Experiment Results",
      "text" : "Table 2, 3 and 4 report the performance of our approaches against other methods. HiAGM is an effective baseline on RCV1-V2 and WOS due to the introduction of holistic label information. However, they ignored the semantic relationship between text and labels. Our model achieves the best results by capturing the matching relationships among text and labels in a hierarchy-aware manner, which achieves stronger performances especially on Macro-F1. The improvements show that our model can make better use of structural information to help imbalanced HTC classification.\nThe pretrained language model BERT is an effective method when fine-tuned on downstream tasks. Compared with the results regarding HTC\nas multiple binary classifiers, our results show that the full use of structured label hierarchy can bring great improvements to BERT model on RCV1-V2 and WOS datasets.\nOn EURLEX57K dataset, our model achieves the best results on different matrics except for zeroshot labels. The largest improvements come from few-shot labels. AGRU-KAMG achieves the best results on zero-shot labels by fusing various knowledge such as label semantics similarities and label co-occurrence. However, our model performs semantics matching among seen labels based on training corpora, which is not designed for a specific zero-shot learning task."
    }, {
      "heading" : "4.3 Analysis",
      "text" : ""
    }, {
      "heading" : "4.3.1 Ablation Study",
      "text" : "In this section, we investigate to study the independent effect of each component in our proposed model. Firstly, we validate the influence of two proposed losses, and the hierarchy-aware sampling. The results are reported in Table 5. The results show that F1 will decrease with removing joint embedding loss or matching learning loss. Joint embedding loss has a great influence since label semantics matching relies on the joint embedding. Besides, in the hierarchy-aware margin subsection, we perform hierarchy-aware sampling by sampling coarse-grained labels, incorrect sibling labels, and other incorrect labels as negative label sets. When we remove hierarchy-aware sampling and replace it with random sampling, the results will decrease, which shows the effectiveness of hierarchy-aware sampling."
    }, {
      "heading" : "4.3.2 Hyperparameters Study",
      "text" : "To study the influence of the hyperparameters γ, α, and β, we conduct seven experiments on RCV1V2 dataset. The results are reported in Table 6. The first experiment is the best hyperparameters of our model. Then we fine-tune the matching learning margin γ in experiments two and three. We\nfind that a proper margin γ = 0.2 is beneficial for matching learning compared with a large or small margin. Furthermore, we validate the effectiveness of the hierarchy-aware margin. In experiment four, the performance will decrease if we violate the hierarchical structure by setting a large penalty margin for coarse-grained labels, and setting a small penalty margin for incorrect sibling labels. In experiment five, the performance has a relatively larger decrease if we set α = 1 and β = 1, which ignores hierarchical structure completely. We speculate that the penalty margin that violates the hierarchical structure will affect the results, since the semantics relationship should be closer if the labels are closer in the hierarchical structure. Moreover, we validate the effectiveness of different penalty margins among different granular labels. In experiments six and seven, the results will degrade if we ignore the relationships between coarse-grained target labels and incorrect sibling labels, by setting the same margin for α and\nβ. Therefore, it is necessary to set a small penalty margin for coarse-grained target labels, and a larger penalty margin for incorrect sibling labels."
    }, {
      "heading" : "4.3.3 T-SNE Visualization of Joint Embedding",
      "text" : "We plot the T-SNE projection of the text representations and label representations in the joint embedding in Figure 4. Figure a) is a part of the hierarchical label structure in RCV1-V2. Label C171 and C172 are fine-grained labels, and label C17 is coarse-grained label of C171 and C172. GWELF and E61 are other labels with different semantics with C17, C171 and C172. In Figure b), by introducing joint embedding loss, we can see that the text representations are close to their corresponding label representations. Furthermore, the text representations of labels C171 and C172 are close to the label representation of their coarse-grained label C17. However, the text representations of different labels may overlap, since the matching relationships among different labels are ignored. In Figure c), by introducing both joint embedding loss and matching learning loss, the text representations of different labels are more separable. Other unrelated text representations and label representations\nsuch as labels GWELF, E61 are far away from C17, C171, C172. Besides, the text representations of semantically similar labels (C171 and C172) are far away relatively compared with Figure b). The T-SNE visualization shows that our model can capture the semantics relationship among texts, coarsegrained labels, fine-grained labels and unrelated labels."
    }, {
      "heading" : "4.3.4 Performance Study on Label Granularity",
      "text" : "We analyze the performance with different label granularity based on their hierarchical levels. We compute level-based Micro-F1 and Macro-F1 scores of the RCV1-V2 dataset on TextRCNN, HiAGM, and our model in Figure 5. On RCV1-V2 dataset, both the second and third hierarchical levels contain fine-grained labels (leaf nodes). The second level has the largest number of labels and contains confusing labels with similar concepts, so its Micro-F1 is relatively low. Both the second and third levels contain some long-tailed labels, so their Macro-F1 are relatively low. Figure 5 shows that our model achieves a better performance than other models on all levels, especially among deep levels. The results demonstrate that our model has a better ability to capture the hierarchical label semantic, especially on fine-grained labels with a complex hierarchical structure."
    }, {
      "heading" : "4.3.5 Computational Complexity",
      "text" : "In this part, we compare the computational complexity between HiAGM and our model. For time complexity, the training time of HiMatch is 1.11 times that of HiAGM with batch size 64. For space complexity during training, HiMatch has 37.4M parameters, while HiAGM has 27.8M. The increase mainly comes from the label encoder with large\nlabel sets. However, during testing, the time and space complexity of HiMatch is the same as HiAGM. The reason is that only the classification results are needed, and we can remove the joint embedding. HiMatch achieves new state-of-the-art results, and we believe that the increase of computational complexity is acceptable."
    }, {
      "heading" : "5 Conclusion",
      "text" : "Here we present a novel hierarchical text classification model called HiMatch that can capture semantic relationships among texts and labels at different abstraction levels. Instead of treating HTC as multiple binary classification tasks, we consider the text-label semantics matching relationship and formulate it as a semantic matching problem. We learn a joint semantic embedding between text and labels. Finally, we propose a hierarchy-aware matching strategy to model different matching relationships among coarse-grained labels, fine-grained labels and incorrect labels. In future work, we plan to extend our model to the zero-shot learning scenario."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the anonymous reviewers for their helpful feedbacks. The work described in this paper was partially funded by the National Natural Science Foundation of China (Grant No. 61502174, and 61872148), the Natural Science Foundation of Guangdong Province (Grant No. 2017A030313355, 2019A1515010768 and 2021A1515011496), the Guangzhou Science and Technology Planning Project (Grant No. 201704030051, and 201902010020), the Key R&D Program of Guangdong Province (No. 2018B010107002) and the Fundamental Research Funds for the Central Universities."
    } ],
    "references" : [ {
      "title" : "Hierarchical text classification and evaluation",
      "author" : [ "Aixin Sun", "Ee-Peng Lim." ],
      "venue" : "Proceedings 2001 IEEE International Conference on Data Mining, pages 521–528.",
      "citeRegEx" : "Sun and Lim.,? 2001",
      "shortCiteRegEx" : "Sun and Lim.",
      "year" : 2001
    }, {
      "title" : "Hierarchical transfer learning for multi-label text classification",
      "author" : [ "Siddhartha Banerjee", "Cem Akkaya", "Francisco PerezSorrosal", "Kostas Tsioutsiouliklis." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Banerjee et al\\.,? 2019",
      "shortCiteRegEx" : "Banerjee et al\\.",
      "year" : 2019
    }, {
      "title" : "Large-scale multi-label text classification on EU legislation",
      "author" : [ "Ilias Chalkidis", "Emmanouil Fergadiotis", "Prodromos Malakasiotis", "Ion Androutsopoulos." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Chalkidis et al\\.,? 2019",
      "shortCiteRegEx" : "Chalkidis et al\\.",
      "year" : 2019
    }, {
      "title" : "Hyperbolic interaction model for hierarchical multi-label classification",
      "author" : [ "Boli Chen", "Xin Huang", "Lin Xiao", "Zixin Cai", "Liping Jing." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7496–7503.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Multilabel classification via calibrated label ranking",
      "author" : [ "Johannes Fürnkranz", "Eyke Hüllermeier", "Eneldo Loza Mencı́a", "Klaus Brinker" ],
      "venue" : null,
      "citeRegEx" : "Fürnkranz et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Fürnkranz et al\\.",
      "year" : 2008
    }, {
      "title" : "Hdltex: Hierarchical deep learning for text classification",
      "author" : [ "K. Kowsari", "D.E. Brown", "M. Heidarysafa", "K. Jafari Meimandi", "M.S. Gerber", "L.E. Barnes." ],
      "venue" : "2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA),",
      "citeRegEx" : "Kowsari et al\\.,? 2017",
      "shortCiteRegEx" : "Kowsari et al\\.",
      "year" : 2017
    }, {
      "title" : "Recurrent convolutional neural networks for text classification",
      "author" : [ "Siwei Lai", "Liheng Xu", "Kang Liu", "Jun Zhao." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 29.",
      "citeRegEx" : "Lai et al\\.,? 2015",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2015
    }, {
      "title" : "Rcv1: A new benchmark collection for text categorization research",
      "author" : [ "David D. Lewis", "Yiming Yang", "Tony G. Rose", "Fan Li." ],
      "venue" : "5:361–397.",
      "citeRegEx" : "Lewis et al\\.,? 2004",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2004
    }, {
      "title" : "Multi-label few/zero-shot learning with knowledge aggregated from multiple label graphs",
      "author" : [ "Jueqing Lu", "Lan Du", "Ming Liu", "Joanna Dipnall." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Lu et al\\.,? 2020",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2020
    }, {
      "title" : "Hierarchical text classification with reinforced label assignment",
      "author" : [ "Yuning Mao", "Jingjing Tian", "Jiawei Han", "Xiang Ren." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Mao et al\\.,? 2019",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2019
    }, {
      "title" : "Hierarchical taxonomy-aware and attentional graph capsule rcnns for large-scale multi-label text classification",
      "author" : [ "Hao Peng", "Jianxin Li", "Qiran Gong", "Senzhang Wang", "Lifang He", "Bo Li", "Lihong Wang", "Philip S. Yu." ],
      "venue" : "CoRR, abs/1906.04898.",
      "citeRegEx" : "Peng et al\\.,? 2019",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2019
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 conference",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Ranking-based autoencoder for extreme multi-label classification",
      "author" : [ "Bingyu Wang", "Li Chen", "Wei Sun", "Kechen Qin", "Kefeng Li", "Hui Zhou." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Label-specific document representation for multi-label text classification",
      "author" : [ "Lin Xiao", "Xin Huang", "Boli Chen", "Liping Jing." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Xiao et al\\.,? 2019",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2019
    }, {
      "title" : "SGM: Sequence generation model for multi-label classification",
      "author" : [ "Pengcheng Yang", "Xu Sun", "Wei Li", "Shuming Ma", "Wei Wu", "Houfeng Wang." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 3915–3926, Santa",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning deep latent space for multi-label classification",
      "author" : [ "Chih-Kuan Yeh", "Wei-Chieh Wu", "Wei-Jen Ko", "YuChiang Frank Wang." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 31.",
      "citeRegEx" : "Yeh et al\\.,? 2017",
      "shortCiteRegEx" : "Yeh et al\\.",
      "year" : 2017
    }, {
      "title" : "Hierarchy-aware global model for hierarchical text classification",
      "author" : [ "Jie Zhou", "Chunping Ma", "Dingkun Long", "Guangwei Xu", "Ning Ding", "Haoyu Zhang", "Pengjun Xie", "Gongshen Liu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Hierarchical text classification (HTC) is widely used in Natural Language Processing (NLP), such as news categorization (Lewis et al., 2004) and scientific paper classification (Kowsari et al.",
      "startOffset" : 120,
      "endOffset" : 140
    }, {
      "referenceID" : 6,
      "context" : ", 2004) and scientific paper classification (Kowsari et al., 2017).",
      "startOffset" : 44,
      "endOffset" : 66
    }, {
      "referenceID" : 10,
      "context" : "∗*Corresponding author HTC are to model the large-scale, imbalanced, and structured label hierarchy (Mao et al., 2019).",
      "startOffset" : 100,
      "endOffset" : 118
    }, {
      "referenceID" : 17,
      "context" : "To capture the holistic label correlation features, some researchers proposed a hierarchyaware global model to exploit the prior probability of label dependencies through Graph Convolution Networks (GCN) and TreeLSTM (Zhou et al., 2020).",
      "startOffset" : 217,
      "endOffset" : 236
    }, {
      "referenceID" : 9,
      "context" : "Some researchers also introduced more label correlation features such as label semantic similarity and label co-occurrence (Lu et al., 2020).",
      "startOffset" : 123,
      "endOffset" : 140
    }, {
      "referenceID" : 5,
      "context" : "They followed the traditional way to transform HTC into multiple binary classifiers for every label (Fürnkranz et al., 2008).",
      "startOffset" : 100,
      "endOffset" : 124
    }, {
      "referenceID" : 5,
      "context" : "However, they ignored the interaction between text semantics and label semantics (Fürnkranz et al., 2008; Wang et al., 2019), which is highly useful for classification (Chen et al.",
      "startOffset" : 81,
      "endOffset" : 124
    }, {
      "referenceID" : 13,
      "context" : "However, they ignored the interaction between text semantics and label semantics (Fürnkranz et al., 2008; Wang et al., 2019), which is highly useful for classification (Chen et al.",
      "startOffset" : 81,
      "endOffset" : 124
    }, {
      "referenceID" : 3,
      "context" : ", 2019), which is highly useful for classification (Chen et al., 2020).",
      "startOffset" : 51,
      "endOffset" : 70
    }, {
      "referenceID" : 13,
      "context" : "Hence, their models may not be sufficient to model complex label dependencies and provide comparable text-label classification scores (Wang et al., 2019).",
      "startOffset" : 134,
      "endOffset" : 153
    }, {
      "referenceID" : 14,
      "context" : "A natural strategy for modeling the interaction between text semantics and label semantics is to introduce a text-label joint embedding by label attention (Xiao et al., 2019) or autoencoders (Yeh et al.",
      "startOffset" : 155,
      "endOffset" : 174
    }, {
      "referenceID" : 14,
      "context" : "4371 self-attention mechanism to identify label-specific information (Xiao et al., 2019).",
      "startOffset" : 69,
      "endOffset" : 88
    }, {
      "referenceID" : 16,
      "context" : "Autoencoder-based methods extended the vanilla Canonical Correlated Autoencoder (Yeh et al., 2017) to a ranking-based autoencoder architecture to produce comparable text-label scores (Wang et al.",
      "startOffset" : 80,
      "endOffset" : 98
    }, {
      "referenceID" : 13,
      "context" : ", 2017) to a ranking-based autoencoder architecture to produce comparable text-label scores (Wang et al., 2019).",
      "startOffset" : 92,
      "endOffset" : 111
    }, {
      "referenceID" : 17,
      "context" : "However, these methods assume all the labels are independent without fully considering the correlation between coarse-grained labels and fine-grained labels, which cannot be simply transferred to HTC models (Zhou et al., 2020).",
      "startOffset" : 207,
      "endOffset" : 226
    }, {
      "referenceID" : 13,
      "context" : "Second, inspired by the methods of learning common embeddings (Wang et al., 2019), we project both textual semantics and label semantics into a text-label joint embedding space where correlations between text and labels are exploited.",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 5,
      "context" : "They regarded hierarchical text classification as multiple binary classification tasks (Fürnkranz et al., 2008).",
      "startOffset" : 87,
      "endOffset" : 111
    }, {
      "referenceID" : 13,
      "context" : "Therefore, they failed to capture complex label dependencies and can not provide comparable text-label classification scores (Wang et al., 2019), which leads to restricted performance (Chen et al.",
      "startOffset" : 125,
      "endOffset" : 144
    }, {
      "referenceID" : 3,
      "context" : ", 2019), which leads to restricted performance (Chen et al., 2020).",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 14,
      "context" : "To determine the correlation between text and label, researchers proposed various methods to exploit a text-label joint embedding such as (Xiao et al., 2019) or Autoencoder (Yeh et al.",
      "startOffset" : 138,
      "endOffset" : 157
    }, {
      "referenceID" : 16,
      "context" : "(2019) extended vanilla Canonical Correlated AutoEncoder (Yeh et al., 2017) to a ranking-based autoencoder architecture to produce comparable label scores.",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 17,
      "context" : "HTC due to the constraint of hierarchy (Zhou et al., 2020).",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 17,
      "context" : "Following the previous work (Zhou et al., 2020), we further introduce a hierarchy-aware text feature propagation module to encode label hierarchy information.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 8,
      "context" : "RCV1-V2 (Lewis et al., 2004) is a news categorization corpora, and WOS (Kowsari et al.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 6,
      "context" : ", 2004) is a news categorization corpora, and WOS (Kowsari et al., 2017) includes abstracts of published papers from Web of Science.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 2,
      "context" : "3k labels from the European Vocabulary (Chalkidis et al., 2019).",
      "startOffset" : 39,
      "endOffset" : 63
    }, {
      "referenceID" : 9,
      "context" : "The label setting is the same as previous work (Lu et al., 2020).",
      "startOffset" : 47,
      "endOffset" : 64
    }, {
      "referenceID" : 9,
      "context" : "The ranking metrics are preferable for EURLEX-57K since it does not introduce a significant bias towards frequent labels (Lu et al., 2020).",
      "startOffset" : 121,
      "endOffset" : 138
    }, {
      "referenceID" : 9,
      "context" : "For fair comparisons with previous work (Lu et al., 2020; Chalkidis et al., 2019) on EURLEX-57K dataset, firstly, we do not use CNN layer and text feature propagation module.",
      "startOffset" : 40,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : "For fair comparisons with previous work (Lu et al., 2020; Chalkidis et al., 2019) on EURLEX-57K dataset, firstly, we do not use CNN layer and text feature propagation module.",
      "startOffset" : 40,
      "endOffset" : 81
    }, {
      "referenceID" : 4,
      "context" : "For pretrained language model BERT (Devlin et al., 2018), we use the top-level representation hCLS of BERT’s special CLS token to perform classification.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 7,
      "context" : "Comparison Models On RCV1-V2 and WOS datasets, we compare our model with three types of strong baselines: 1) Text classification baselines: TextRCNN (Lai et al., 2015), TextRCNN with label attention (TextRCNN-LA) (Zhou et al.",
      "startOffset" : 149,
      "endOffset" : 167
    }, {
      "referenceID" : 17,
      "context" : ", 2015), TextRCNN with label attention (TextRCNN-LA) (Zhou et al., 2020), and SGM (Yang et al.",
      "startOffset" : 53,
      "endOffset" : 72
    }, {
      "referenceID" : 10,
      "context" : ", 2019), HMCN (Mao et al., 2019), Htrans (Banerjee et al.",
      "startOffset" : 14,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : ", 2019), Htrans (Banerjee et al., 2019), HiLAP-RL (Mao et al.",
      "startOffset" : 16,
      "endOffset" : 39
    }, {
      "referenceID" : 10,
      "context" : ", 2019), HiLAP-RL (Mao et al., 2019) which introduced reinforcement learning to simulate the assignment process, HiAGM (Zhou et al.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 17,
      "context" : ", 2019) which introduced reinforcement learning to simulate the assignment process, HiAGM (Zhou et al., 2020) which exploited the prior probability of label dependecies through Graph Convolution Network and TreeLSTM.",
      "startOffset" : 90,
      "endOffset" : 109
    }, {
      "referenceID" : 4,
      "context" : "3) Pretrained language model: a more powerful pretrained language model BERT (Devlin et al., 2018) than tradition text classification models when fine-tuned on downstream tasks.",
      "startOffset" : 77,
      "endOffset" : 98
    }, {
      "referenceID" : 2,
      "context" : "On EURLEX-57K dataset, we compare our model with strong baselines with/without zeroshot settings such as BIGRU-ATT, BIGRU-LWAN (Chalkidis et al., 2019) which introduced labelwise attention.",
      "startOffset" : 127,
      "endOffset" : 151
    }, {
      "referenceID" : 9,
      "context" : "AGRU-KAMG (Lu et al., 2020) is a stateof-the-art model which introduced various label knowledge.",
      "startOffset" : 10,
      "endOffset" : 27
    } ],
    "year" : 2021,
    "abstractText" : "Hierarchical text classification is an important yet challenging task due to the complex structure of the label hierarchy. Existing methods ignore the semantic relationship between text and labels, so they cannot make full use of the hierarchical information. To this end, we formulate the text-label semantics relationship as a semantic matching problem and thus propose a hierarchy-aware label semantics matching network (HiMatch). First, we project text semantics and label semantics into a joint embedding space. We then introduce a joint embedding loss and a matching learning loss to model the matching relationship between the text semantics and the label semantics. Our model captures the text-label semantics matching relationship among coarse-grained labels and fine-grained labels in a hierarchy-aware manner. The experimental results on various benchmark datasets verify that our model achieves state-of-the-art results.",
    "creator" : "LaTeX with hyperref"
  }
}