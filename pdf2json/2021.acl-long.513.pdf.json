{
  "name" : "2021.acl-long.513.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Multi-hop Graph Convolutional Network with High-order Chebyshev Approximation for Text Reasoning",
    "authors" : [ "Shuoran Jiang", "Qingcai Chen", "Xin Liu", "Baotian Hu", "Lisai Zhang" ],
    "emails" : [ "lisaizhang2016}@gmail.com", "hubaotian}@hit.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6563–6573\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6563"
    }, {
      "heading" : "1 Introduction",
      "text" : "Graph neural networks (GNNs) are usually used to learn the node representations in Euclidean space from graph data, which have been developed to one of the hottest research topics in recent years (Zhang, 2020). The primitive GNNs relied on recursive propagation on graphs, which takes a long time to train (Zhang et al., 2019b). One major variant of GNNs, graph convolutional networks (GCNs) (Kipf and Welling, 2017; Yao et al., 2019), takes spectral filtering to replace recursive message passing and needs only a shallow network to convergent, which have been used in various NLP tasks. For example, Yao et al. (2019) constructed the text as a graph and input it to a GCN. This method achieved better results than conventional deep learning models in text classification. Afterward, the GCNs\n∗corresponding author: Qingcai Chen\nhave became popular in more tasks, such as word embedding (Zhang et al., 2020b), semantic analysis (Zhang et al., 2019a), document summarization (Wang et al., 2020), knowledge graph (Wang et al., 2018), etc.\nThe spectral graph convolution in Yao’s GCN is a localized first-order Chebyshev approximation. It is equal to a stack of 1-step Markov chain (MC) layer and fully connected (FC) layer. Unlike the multi-step Markov chains, the message propagation in vanilla GCN lacks the node probability transitions. As a result, the multi-hop graph reasoning is very tardy in GCN and easily causes the suspended animation problem (Zhang and Meng, 2019). However, the probability transition on the graph is useful to improve the efficiency in learning contextual dependencies. In many NLP tasks (like the question answering (QA) system and entity relation extraction), the features of the two nodes need to be aligned. As an example, Figure 1 shows a simple graph where the node n4 is a pronoun of node n1. In this example, the adjacency matrix is masked on nodes n2, n3, n5 to demonstrate the message passing between n1 and n4. Figure 1 (c) and (d) plot the processes of feature alignment on two nodes without and with probability transitions respectively. In this example, the feature alignment process without probability transition needs 10 more steps than which with probability transition. It is shown that encoding the multi-hop dependencies through the spectral graph filtering in GCN usually requires a deep network. However, as well known that the deep neural network (DNN) is tough to train and easily causes the over-fitting problem (Rong et al., 2019).\nSome newest studies to improve the multi-hop graph reasoning include graph attention networks (GATs) (Veličković et al., 2018), graph residual neural network (GRESNET) (Zhang and Meng, 2019), graph diffusive neural network (DIFNET)\n(Zhang, 2020), TGMC-S (Zhang et al., 2020c) and Graph Transformer Networks (Yun et al., 2019; Zhang and Zhang, 2020). GATs enhance the graph reasoning by implicitly re-defining the graph structure with the attention on the 1-hop neighbors, but there is equilibrial optimization on the whole graph. GRESNET solves the suspended animation problem by creating extensively connected highways to involve raw node features and intermediate representations throughout all the model layers. However, the multi-hop dependencies are still reasoned at a slow pace. DIFNET introduces a new neuron unit, i.e., GDU (gated diffusive unit), to model and update the hidden node states at each layer. DIFNET replaces the spectral filtering with a recursive module and realizes the neural gate learning and graph residual learning. But the time cost is aggravated in DIFNET compared with GCN. TGMCS stacks GCN layers on adjacent matrices with different hops of traffic networks. Different from the ground-truth traffic network in TGMC-S, it is hard to construct the multi-hop word-word relationships objectively from the text. TGMC-S hadn’t given a way to improve the multi-hop message passing in GCN.\nTransformers (Vaswani et al., 2017) and corresponding pre-trained models (Xu et al., 2019) could be thought of as fully-connected graph neural networks that contain the multi-hop dependencies. They figure out the contextual dependencies on the fully-connected graph with the attention mechanism. The message propagation in transformers follows the relations self-adaptively learned from\ninput sequence instead of the fixed graph structures. Publications have shown that transformers outperform GCNs in many NLP tasks. Graph Transformer (Dwivedi and Bresson, 2020) generalizes the Transformer to arbitrary graphs, and improves inductive learning from Laplacian eigenvectors on graph topology. However, due to the connections scale quadratically growth with node number N in graphs, things get out of hand for very large N . Additionally, the fully-connected graph is not an interpretable architecture in practical tasks. For example, whether Transformers are the best choice to bring the text in linguistic theory? 1\nTo improve the efficiency and performance of multi-hop graph reasoning in spectral graph convolution, we proposed a new graph convolutional network with high-order dynamic Chebyshev approximation (HDGCN). A prime ChebNet and a high-order dynamic (HD) ChebNet are firstly applied to implement this Chebyshev approximation. These two sub-networks work like a trade-off on low-pass signals (direct dependencies) and highpass signals (multi-hop dependencies) respectively. The prime ChebNet takes the same frame as the convolutional layer in vanilla GCN. It mainly extracts information from direct neighbors in local contexts. The HD-ChebNet aggregates messages from multi-hop neighbors following the transition direction adaptively learned by the attention mechanism. The standard self-attention (Vaswani et al., 2017) has a O ( N2 )\ncomputation complexity and it is hard to be applied on long sequence. Even the existing sparsity attention methods, like the StarTransformer (Guo et al., 2019) and Extended Transformer Construction (ETC) (Ainslie et al., 2020), have reduced the quadratic dependence limit of sequence length to linear dependence, but the fullyconnected graph structure cannot be kept. We design a multi-vote-based cross-attention (MVCAttn) mechanism. The MVCAttn scales the computation complexity O(N2) in self-attention to O(N).\nThe main contributions of this paper are listed below:\n• To improve the efficiency and performance of multi-hop reasoning in spectral graph convolution, we propose a novel graph convolutional network with high-order dynamic Chebyshev Approximation (HDGCN).\n1https://towardsdatascience.com/transformers-are-graphneural-networks-bca9f75412aa\n• To avoid the over-smoothing problem in HD-ChebNet, we propose a multi-vote based cross-attention (MVCAttn) mechanism, which adaptively learn the direction of node probability transition. MVCAttn is a variant of the attention mechanism with the property of linear computation complexity.\n• The experimental results show that the proposed model outperforms compared SOTA models on four transductive and inductive NLP tasks."
    }, {
      "heading" : "2 Related Work",
      "text" : "Our work draws supports from the vanilla GCN and the attention mechanism, so we first give a glance at the paradigm of these models in this section."
    }, {
      "heading" : "2.1 Graph Convolutional Network",
      "text" : "The GCN model proposed by (Kipf and Welling, 2017) is the one we interested, and it is defined on graph G = {V, E}, where V is the node set and E is the edge set. The edge (vi, vj) ∈ E represents a link between nodes vi and vj . The graph signals are attributed as X ∈ R|V|×d, and the graph relations E can be defined as an adjacency matrix A ∈ R|V|×|V| (binary or weighted).\nEach convolutional layer in GCN is a 1st Chebyshev approximation on spectral graph convolution, and its layer-wise propagation rule in neural network is defined as:\nH(l+1) = σ ( ÃH(l)W(l) ) , L ≥ l ≥ 0\nÃ = (D + IN ) − 1 2 (A + IN ) (D + IN ) − 1 2 , (1)\nwhere H(0) = X, Ã is the normalized adjacency matrix and σ is a non-linear activation function.\nThe node embeddings output from the last convolutional layer are fed into a softmax classifier for node or graph classification, and the loss function L can be defined as the cross-entropy error. The weight set {W(l)}Ll=0 can be jointly optimized by minimizing L via gradient descent."
    }, {
      "heading" : "2.2 Self-Attention Is a Dynamic GCN",
      "text" : "The attention mechanism is an effective way to extract task-relevant features from inputs, and it helps the model to make better decisions (Lee et al., 2019). It has various approaches to compute the attention score from features, and the scaled dot-product attention proposed in Transformers\n(Vaswani et al., 2017) is the most popular one.\nZ = softmax (\nXWqWkXT√ dk ) ︸ ︷︷ ︸\nA\nXWv\n(2)\nwhere X ∈ RN×d is the input sequence, and weights Wq ∈ Rd×dk , Wk ∈ Rdk×d, Wv ∈ Rd×dv are used to transform sequence to queries, keys and values.\nAs showed in Equation 2, the attention scores A can be viewed as a dynamic adjacency matrix on sequence X. This process in self-attention is similar to the graph convolutional layer defined in Equation 1. The only difference is that the adjacency matrix in Equation 2 is adaptively learned from input instead of prior graph structures."
    }, {
      "heading" : "3 Method",
      "text" : "In our model, the input graph G = (V, E) takes the same form as the one in GCN. The nodes are attributed as X ∈ R|V|×d, and the adjacency matrix A ∈ R|V|×|V| (binary or weighted) is defined on graph edges E .\nThe spectral graph convolution in Fourier domain is defined as,\ngθ ? x = Ugθ\n( Λ̃ ) UTx (3)\nwhere x ∈ Rd is the signal on a node, U is the matrix of eigenvectors on normalized graph Laplacian L = IN −D− 1 2 AD− 1 2 = UΛUT , and the filter gθ(Λ̃) is a function of the eigenvalues on normalized L̃ in Fourier domain.\nThe K-th (K > 2) order truncation of Chebyshev polynomials on this spectral graph convolution is,\ngθ ? x ≈ K∑ i=0 θiUTi ( Λ̃ ) UTx (4)\nwhere T0 ( Λ̃ ) = I, T1 = Λ̃, Ti>1 ( Λ̃ ) =\n2Λ̃Ti−1 ( Λ̃ ) − Ti−2 ( Λ̃ )\n. To replace the parameters {θi}Ki=1 with another parameter set {θ(i)}K/2i=1 , the Kth-order Chebyshev polynomials in Equation 4 are approximated as:\ngθ ? x ≈ K/2∑ k=0 ( UΛ̃UT )2k ( I−UΛ̃UT ) xθ(k)\n≈ K/2∑ k=1 Ã2kÃxθ(i)\n(5)\nwhere the Ã is normalized adjacency matrix (as defined in Equation 1). As the node state transition Ã2k causes the over-smoothing problem (Li et al., 2018; Nt and Maehara, 2019), we take the dynamic pairwise relationship Ad self-adaptively learned by the attention mechanism to turn the direction of node state transition.\nThe powers of adjacency matrix Ã2k in Equation 5 can cause the over smoothing problem, we replace the Ã2k with ÃkAkd.\nIn our implementation, the first-order and higherorder Chebyshev polynomials in Equation 5 is approximated with a prime Chebyshev network (ChebNet) and high-order dynamic Chebyshev networks (HD-ChebNets) respectively. We generalize the graph convolution on Kth-order dynamic Chebyshev approximation (Equation 5) to the layerwise propagation as follows,\nH ≈ K/2∑ k=0 Z(k),\nZ(0) = σ ( ÃXW(0) ) ︸ ︷︷ ︸ Prime ChebNet ,\nZ(k) = σ ( Ã ( A\n(k) d Z (k)W (k) d\n) W(k) ) ︸ ︷︷ ︸\nUnit in HD-ChebNet\n,\n(6)\nwhere k is the order and W(0), W(k), W(k)d are nonlinear filters on node signals. For the convenience of writing, we just define the first layer of HDGCN."
    }, {
      "heading" : "3.1 Prime ChebNet",
      "text" : "We consider the same convolutional architecture as the one in GCN to implement the prime ChebNet,\nand it mainly aggregates messages from the direct dependencies.\nZ(0) = σ ( ÃXW(0) ) , (7)\nwhere W(0) ∈ Rd×d and Ã is the normalized symmetric adjacency matrix."
    }, {
      "heading" : "3.2 High-order Dynamic (HD) ChebNet",
      "text" : "As the multi-hop neighbors can be interacted via the 1-hop neighbors, we take the Z(0) output from the prime ChebNet as input of the HD-ChebNet. The multi-vote based cross-attention (MVCAttn) mechanism first adaptively learns the direction of node probability transition A(k)d , its schematic is showed in Figure 2 (b). MVCAttn has two phases - graph information aggregation and diffusion.\nGraph Information Aggregation coarsens the node embeddings Z(k−1) to a small supernode set S(k) ∈ RM×d, M |V|.\nThe first step is multi-vote projection (MVProj). In which node embeddings Z(k−1) are projected to multiple votes V(k) ∈ R|V|×M×d, and these votes are aggregated to supernode set S(k) = {s(k)m }Mm=1.\ns(k)m = MVProj ( Z(k−1) ) = norm\n |V|∑ v=1 z(k−1)v W V m  (8) where |V| ≥ v ≥ 1, M ≥ m ≥ 1, WVm ∈ Rdk×dk is the projection weight and norm () represents the LayerNorm operation.\nNext, the forward cross-attention (FCAttn) up-\ndates the supernode values as: Ŝ(k) = FCAttn ( Z(k),S(k) ) = A\n(k) f Z (k−1)Wfv\nA (k) f = Softmax\n( Z(k−1)WfkWfqS (k)\n√ d\n) (9)\nwhere Wfk ∈ Rdk×dc , Wfq ∈ Rdc×dk and Wfv ∈ Rdk×dk .\nGraph Information Diffusion feeds the supernodes Ŝ(k) back to update node set Z(k). With the node embeddings Z(k−1) and supernode embeddings Ŝ(k), the backward cross-attention (BCAttn) is defined as,\nZ(k) = BCAttn ( S̃(k),Z(k−1) ) = A\n(k) b Z (k−1)Wbv\nA (k) b = Softmax\n( Ŝ(k)WbqWbkZ\n(k−1) √ d\n) (10)\nwhere Wbq ∈ Rdk×da , Wbk ∈ Rda×dk and Wbv ∈ Rdk×dk .\nThe last step is adding the probability transition with Ã. The output of k-th order HD-ChebNet (Equation A) is,\nẐ(k) = σ ( ÃZ(k)W(k) ) (11)\nFinally, the outputs from the prime ChebNet and HD-ChebNets are integrated as the node embeddings,\nH = norm Z(0) + K/2∑ k=1 Ẑ(k)  . (12)"
    }, {
      "heading" : "3.3 Classifier Layer",
      "text" : "Node Classification The node representations H output from the last graph convolutional layer are straightforward fed into a Softmax classifier for node classification.\nŷv = Softmax (MLP (hv)) (13)\nGraph Classification The representation on the whole graph is constructed via a readout layer on the outputs H,\nhv = σ (f1 (hv)) tanh (f2 (hv))\nhg = 1\n|V| |V|∑ v=1 hv + Maxpool ( h1 · · ·h|V| ) (14)\nwhere denotes the Hadamard product and f1(), f2() are two non-linear functions.\nThe graph representation hg ∈ Rd is fed into the Softmax classifier to predict the graph label.\nŷg = Softmax (hg) (15)\nAll parameters are optimized by minimizing the cross-entropy function:\nL = − 1 N N∑ n=1 yn/g log(ŷn/g) (16)"
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we evaluate HDGCN on transductive and inductive NLP tasks of text classification, aspect-based sentiment classification, natural language inference, and node classification. In experiment, each layer of HDGCN is fixed with K = 6 order Chebyshev approximation and the model stacks L = 1 layer. The dimension of input node embeddings is d = 300 of GlVe or d = 768 of pre-trained BERT, and the hyper-parameter dk = 64, da = 64. So the weights W(0) ∈ Rd×64, Wld,W(k) ∈ R64×64 and Wfk,Wfq,Wbq,Wbk ∈ R64×64. The number of super-nodes is set as M = 10. Our model is optimized with adaBelief (Zhuang et al., 2020) with a learning rate 1e − 5. The schematics about the HDGCN is shown in Figure 2.\nTo analyze the effectiveness of MVCAttn in avoiding over-smoothing, we report the results of ablation study - HDGCN-static in Table 1, 2 5. The ablation model - HDGCN-static is an implementation of Equation 5, in which the node state transition is determined by the static adjacency matrix Ã2k."
    }, {
      "heading" : "4.1 Text Classification",
      "text" : "The first experiment is designed to evaluate the performance of HDGCN on the text graph classification. Four small-scale text datasets2 - MR, R8, R52, Ohsumed, and four large-scale text datasets - AG’s News3, SST-1, SST-24, Yelp-F5 are used in this task. The graph structures are built on word-word co-occurrences in a sliding window\n2https://github.com/yao8839836/text gcn 3http://groups.di.unipi.it/ gulli/AG corpus of news articles.html 4https://nlp.stanford.edu/sentiment/treebank.html 5https://www.yelp.com/dataset\n(width=3 and unweighted) on individual documents. HDGCN is initialized with word embeddings pre-trained by 300-d GloVe and 768-d BERTbase on small and large scale datasets respectively. The baselines include TextCNN, TextRNN, fastText, SWEM, TextGCN, GraphCNN, TextING, minCUT, BERT-base, DRNN, CNN-NSU, CapNets, LK-MTL, TinyBERT, Star-Transformer.\nTable 1 shows the test accuracies on four smallscale English datasets, in which HDGCN ranks top with accuracies 86.50%, 98.45%, 96.57%, 73.97% respectively. HDGCN beats the best baselines achieved by TextING (the newest GNN model) and the fine-tuned BERT-base model. Our ablation model HDGCN-static also achieves higher accuracies than the newest GNN models - TextING and minCUT. Therefore, the outperformance of HDGCN verifies that (1) the node probability transition in high-order Chebyshev approximation improves the spectral graph convolution; (2) the MVCAttn mechanism in high-order ChebNet further raises the effectiveness by avoiding the oversmoothing problem.\nTable 2 shows the test accuracies of HDGCN and other SOTA models on large-scale English datasets. HDGCN achieves the best results 95.5%, 53.9%, 69.6% on AG, SST-1, Yelp-F respectively, and performs a slight gap 0.3% with the top-1 baseline (TinyBERT) on SST-2. These results support that\nHDGCN outperforms the fully-connected graph module in Transformers and corresponding pretrained models. Additionally, these comparisons also demonstrates that the combination of prior graph structures and self-adaptive graph structures in graph convolution is able to improve the multihop graph reasoning."
    }, {
      "heading" : "4.2 Multi-hop Graph Reasoning in Text Graph",
      "text" : "In the second experiment, we make a case study on the MR dataset to visualize how the HDGCN improve multi-hop graph reasoning. Here, we take the positive comment ”inside the film’s conflict powered plot there is a decent moral trying to get out, but it’s not that , it’s the tension that keeps you in your seat Affleck and Jackson are good sparring partners” as an example.\nFirst, the word interactions on prior graph structure Ã (word-word co-occurrence in a sliding window with width=3) is showed in Figure 3. We can see that the word mainly interacts with its consecutive neighbors. It is hard for the vanilla GCN to encode multi-hop and non-consecutive word-word interactions as the example shown in Figure 1.\nFigure 4 shows the node interactions from node embeddings Z(0) to supernodes Ŝ(1) and the graph diffusion from Ŝ(1) to node embeddings Z(1). In which, the supernode S4 puts greater attention on the segment - it’s the tension that keeps you in your seat. This segment determines its positive\npolarity significantly. The other supernodes S1, S2, S3, S5 just aggregate messages from the global context evenly. Next, the messages aggregated in supernodes S1 ∼ S5 are mainly diffused to four tokens - conflict, decent, moral, you. That verifies the self-adaptively learned graph structure A\n(1) f ×A (1) b by the MVCAttn improves the multihop graph reasoning on nodes - conflict, decent, moral, you. From the perspective of semantics, these four words determine the positive sentiment in this comment significantly.\nFigure 5 shows the message aggregation from node embeddings Z(1) to supernodes Ŝ(2) and the message diffusion from Ŝ(2) to node embeddings Z(2). We can see that the supernode S4 puts greater attention on another segment - there is a decent moral young to get out, which also contributes to the sentiment polarity. Then messages aggregated to supernodes S1 ∼ S5 are diffused to all words evenly. The backward interactions from supern-\nodes S1 ∼ S5 to all graph nodes do not have visible differences. These results demonstrate that the multi-hop graph reasoning in HDGCN just needs one graph convolutional layer to attain the stationary state."
    }, {
      "heading" : "4.3 Aspect-based Sentiment Classification",
      "text" : "The third experiment evaluates HDGCN’s performance on the task of aspect-based sentiment classification. This task aims to identify whether the sentiment polarities of aspect are explicitly given in sentences (Zhao et al., 2020). The datasets used in this task include TWITTER, LAP14, REST14, REST15, REST16 (Zhao et al., 2020). The details about the statistics on these datasets are shown in Figure 6. The SOTA comparison models include AOA, TNet-LF, ASCNN, ASGCN-DT, ASGCNDG, AEN-BERT, BERT-PT, SDGCN-BERT.\nEach sample in this task includes a sentence pair, an aspect, and a label. The sentence pair and the aspect are concatenated into one long sentence, and the text graph is preprocessed with the dependency\ntree on this sentence. HDGCN is tested twice with word embeddings initialized by pre-trained 300-d GloVe and 768-d BERT-base respectively.\nTable 3 shows the test accuracies and microF1 scores on 5 datasets, where HDGCN achieves new state-of-the-art results on TWITTER, REST14, REST15, REST16, and a top-3 result on the LAP14. As shown in Figure 6 that the LAP14 has the maximum percentage of long sentences among all datasets. A shallow network in HDGCN does not outperform the SOTA result on the LAP14. Additionally, compared with the newest ASGCN and attention-based AOA, HDGCN achieves the best results on TWITTER, LAP14, REST15, REST16 (Acc) and performs very close with the highest accuracy on REST14 and macro-F1 score on REST16. Above comparison supports that the matching between aspect and sentence pair in HDGCN is more accurate than the newest GNN and attention-based models, which verifies that the multi-hop graph reasoning is improved in HDGCN."
    }, {
      "heading" : "4.4 Natural Language Inference",
      "text" : "The fourth experiment evaluates HDGCN’s performance on the Stanford natural language inference (SNLI) task (Bowman et al., 2015). This task aims to predict the semantic relationship is entailment or contradiction or neutral between a premise sentence and a hypothesis sentence. All the comparison methods include fine-tuned BERTbase, MT-DNN (Liu et al., 2020), SMART (Jiang et al., 2020), and CA-MTL (Pilault et al., 2021).\nIn this task, the premise and hypothesis sentences are concatenated and constructed into a long sentence. Which is preprocessed to a text graph with the dependency tree. The word embeddings in HDGCN were initialized from the pre-trained 768-d BERT-base.\nAll test accuracies are shown in Table 4, where HDGCN achieves the new state-of-the-art results\non the 10% data. As the MT-DNN, SMART and CA-MTL are all fine-tuned on multi-task learning, they perform better than HDGCN in low resource regimes (0.1% and 1.0% of the data). HDGCN just uses 0.02× more parameters than the BERT-base, and it outperforms the later model on all scales of data. These results verify that the combination of prior graph structure and self-adaptive graph structure in HDGCN performs comparably with the fully-adaptive graph structures in Transformers and BERT-based multi-task learning models."
    }, {
      "heading" : "4.5 Graph Node Classification",
      "text" : "The fifth experiment evaluates the effectiveness of HDGCN on the node classification task. We use three standard citation network benchmark datasets - Cora, Citeseer, and Pubmed, to compare the test accuracies on transductive node classification. In the three datasets, the nodes represent the documents and edges (undirected) represent citations. The node features correspond to elements of a bagof-words representation of a document (Veličković et al., 2018). We also use the PPI dataset to compare the results on inductive node classification, which consists of graphs corresponding to different human tissues. The baselines for comparison include GCN, GAT, Graph-Bert, GraphNAS, LoopyNet, HGCN, GRACE, GCNII. The results of our evaluation are recorded in Table 5.\nHDGCN achieves the new state-of-the-art results on Cora, Citeseer and Pubmed, and performs equally best with the newest GCNII on PPI. Our\nablation model, HDGCN-static, also achieves close results with the newest GNNs on Cora, Citeseer, Pubmed, but it performs poorly on PPI. Which verifies that the high-order Chebyshev approximation of spectral graph convolution has more serious over-smoothing problem in inductive node classification than transductive node classification. All comparisons in this experiment demonstrate the effectiveness of MVCAttn to avoid the oversmoothing problem."
    }, {
      "heading" : "5 Conclusions",
      "text" : "This study proposes a multi-hop graph convolutional network on high-order dynamic Chebyshev approximation (HDGCN) for text reasoning. To improve the multi-hop graph reasoning, each convolutional layer in HDGCN fuses low-pass signals (direct dependencies saved in fixed graph structures) and high-pass signals (multi-hop dependencies adaptively learned by MVCAttn) simultaneously. We also firstly propose the multi-votes based cross-attention (MVCAttn) mechanism to alleviate the over-smoothing in high-order Chebyshev approximation, and it just costs the linear computation complexity. Our experimental results demonstrate that HDGCN outperforms compared SOTA models on multiple transductive and inductive NLP tasks."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is supported by Natural Science Foundation of China (Grant No.61872113, 62006061), Strategic Emerging Industry Development Special Funds of Shenzhen (Grant No.XMHT20190108009), the Tencent Group Science and Technology Planning Project of Shenzhen (Grant No.JCYJ20190806112210067) and Shenzhen Foundational Research Funding (Grant No.JCYJ20200109113403826)."
    }, {
      "heading" : "A Appendices",
      "text" : "Here, we give the completed proof about our high-order Chebyshev approximation on the spectral graph convolution. We exhibit how to deduce the spectral graph convolution to 4th-order Chebyshev polynomials as follows.\ngθ ? x = UgθU Tx\nwhere gθ = gθ(Λ̃) is the graph filter defined in spectral domain. gθ ≈ θ0 + θ1Λ̃ + θ2 ( 2Λ̃2 − 1 ) + θ3 ( 4Λ̃3 − 3Λ̃ ) = θ0 + θ1Λ̃ + 2θ2Λ̃ 2 − θ2 + 4θ3Λ̃3 − 3θ3Λ̃\nSo,\nUgθU Tx ≈ θ0x+ θ1UΛ̃UTx+ 2θ2UΛ̃2UTx− θ2x+ 4θ3UΛ̃3UTx− 3θ3UΛ̃UTx\n= θ0x+ θ1UΛ̃U Tx+ 2θ2UΛ̃U TUΛ̃UTx− θ2x+ 4θ3UΛ̃UTUΛ̃UTUΛ̃UTx− 3θ3UΛ̃UTx\n= UΛ̃UTUΛ̃UT( θ0\nUΛ̃UTUΛ̃UT +\nθ1\nUΛ̃UT + 2θ2 −\nθ2\nUΛ̃UTUΛ̃UT + 4θ3UΛ̃U T − 3 θ3 UΛ̃UT )x\n= UΛ̃UTUΛ̃UT (\nθ0 − θ2 UΛ̃UTUΛ̃UT + θ1 − 3θ3 UΛ̃UT\n) x+ UΛ̃UTUΛ̃UT ( 2θ2 + 4θ3UΛ̃U T ) x\n= ( (θ0 − θ2) + (θ1 − 3θ3)UΛ̃UT ) x+ UΛ̃UTUΛ̃UT ( 2θ2 + 4θ3UΛ̃U T ) x\nLet assume θ(0) = θ0 − θ2 = −θ1 + 3θ3, θ(1) = 2θ1 = −4θ3,\nUgθU Tx ≈ θ(0) ( I−UΛ̃UT ) x+ UΛ̃UTUΛ̃UTθ(1) ( I−UΛ̃UT ) x\n≈ θ(0)Ãx+ Ã2θ(1)Ãx\nTo avoid the over-smoothing problem in the node state transition Ã2, the graph structure Ã is approximated by the dynamic adjacency matrix Ad self-adaptively learned with attention mechanism. This way have the hidden pairwise interactions to improve the multi-hop graph reasoning in high-order Chebyshev polynomials. Therefore, we define the layer-wise propagation of multi-hop graph convolutional network as follows.\nH ≈ K/2∑ k=0 Z(k),\nZ(0) = σ ( ÃXW(0) ) ︸ ︷︷ ︸ Prime ChebNet ,\nZ(k) = σ ( Ã ( A\n(k) d Z (k−1)W (k) d\n) W(k) ) ︸ ︷︷ ︸\nHD-ChebNet\nwhere X is the input features, and we introduce two nonlinear filterings W(k) and W(k)d on node signals."
    } ],
    "references" : [ {
      "title" : "Etc: Encoding long and structured inputs in transformers",
      "author" : [ "Joshua Ainslie", "Santiago Ontanon", "Chris Alberti", "Vaclav Cvicek", "Zachary Fisher", "Philip Pham", "Anirudh Ravula", "Sumit Sanghai", "Qifan Wang", "Li Yang." ],
      "venue" : "Proceedings of the 2020 Conference",
      "citeRegEx" : "Ainslie et al\\.,? 2020",
      "shortCiteRegEx" : "Ainslie et al\\.",
      "year" : 2020
    }, {
      "title" : "Mincut pooling in graph neural networks",
      "author" : [ "Filippo Maria Bianchi", "Daniele Grattarola", "Cesare Alippi." ],
      "venue" : "arXiv preprint arXiv:1907.00481.",
      "citeRegEx" : "Bianchi et al\\.,? 2019",
      "shortCiteRegEx" : "Bianchi et al\\.",
      "year" : 2019
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Hyperbolic graph convolutional neural networks",
      "author" : [ "Ines Chami", "Rex Ying", "Christopher Ré", "Jure Leskovec." ],
      "venue" : "NeurIPS, 32:4869.",
      "citeRegEx" : "Chami et al\\.,? 2019",
      "shortCiteRegEx" : "Chami et al\\.",
      "year" : 2019
    }, {
      "title" : "Simple and deep graph convolutional networks",
      "author" : [ "Ming Chen", "Zhewei Wei", "Zengfeng Huang", "Bolin Ding", "Yaliang Li." ],
      "venue" : "In ICML.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "In NAACL.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "A generalization of transformer networks to graphs",
      "author" : [ "Vijay Prakash Dwivedi", "Xavier Bresson." ],
      "venue" : "arXiv preprint arXiv:2012.09699.",
      "citeRegEx" : "Dwivedi and Bresson.,? 2020",
      "shortCiteRegEx" : "Dwivedi and Bresson.",
      "year" : 2020
    }, {
      "title" : "Graphnas: Graph neural architecture search with reinforcement learning",
      "author" : [ "Yang Gao", "Hong Yang", "Peng Zhang", "Chuan Zhou", "Yue Hu." ],
      "venue" : "arXiv preprint arXiv:1904.09981.",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "Startransformer",
      "author" : [ "Qipeng Guo", "Xipeng Qiu", "Pengfei Liu", "Yunfan Shao", "Xiangyang Xue", "Zheng Zhang." ],
      "venue" : "In NAACL, pages 1315–1325.",
      "citeRegEx" : "Guo et al\\.,? 2019",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2019
    }, {
      "title" : "Smart: Robust and efficient fine-tuning for pretrained natural language models through principled regularized optimization",
      "author" : [ "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Tuo Zhao." ],
      "venue" : "In ACL.",
      "citeRegEx" : "Jiang et al\\.,? 2020",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Tinybert: Distilling bert for natural language understanding",
      "author" : [ "Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu." ],
      "venue" : "In EMNLP.",
      "citeRegEx" : "Jiao et al\\.,? 2020",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 2020
    }, {
      "title" : "Is bert really robust? natural language attack on text classification and entailment",
      "author" : [ "Di Jin", "Zhijing Jin", "Joey Tianyi Zhou", "Peter Szolovits." ],
      "venue" : "arXiv preprint arXiv:1907.11932, 2.",
      "citeRegEx" : "Jin et al\\.,? 2019",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2019
    }, {
      "title" : "Bag of tricks for efficient text classification",
      "author" : [ "Armand Joulin", "Édouard Grave", "Piotr Bojanowski", "Tomáš Mikolov." ],
      "venue" : "In EACL, pages 427–431.",
      "citeRegEx" : "Joulin et al\\.,? 2017",
      "shortCiteRegEx" : "Joulin et al\\.",
      "year" : 2017
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas N Kipf", "Max Welling." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Kipf and Welling.,? 2017",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2017
    }, {
      "title" : "Attention models in graphs: A survey",
      "author" : [ "John Boaz Lee", "Ryan A Rossi", "Sungchul Kim", "Nesreen K Ahmed", "Eunyee Koh." ],
      "venue" : "TKDD, 13(6):1–25.",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "Deeper insights into graph convolutional networks for semi-supervised learning",
      "author" : [ "Qimai Li", "Zhichao Han", "Xiao-Ming Wu." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Initializing convolutional filters with semantic features for text classification",
      "author" : [ "Shen Li", "Zhe Zhao", "Tao Liu", "Renfen Hu", "Xiaoyong Du." ],
      "venue" : "In EMNLP.",
      "citeRegEx" : "Li et al\\.,? 2017",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "The microsoft toolkit of multi-task deep neural networks for natural language understanding",
      "author" : [ "Xiaodong Liu", "Yu Wang", "Jianshu Ji", "Hao Cheng", "Xueyun Zhu", "Emmanuel Awa", "Pengcheng He", "Weizhu Chen", "Hoifung Poon", "Guihong Cao" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Revisiting graph neural networks: All we have is low-pass filters",
      "author" : [ "Hoang Nt", "Takanori Maehara." ],
      "venue" : "arXiv preprint arXiv:1905.09550.",
      "citeRegEx" : "Nt and Maehara.,? 2019",
      "shortCiteRegEx" : "Nt and Maehara.",
      "year" : 2019
    }, {
      "title" : "Conditionally adaptive multi-task learning: Improving transfer learning in nlp using fewer parameters & less data",
      "author" : [ "Jonathan Pilault", "Amine Elhattami", "Christopher Pal." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Pilault et al\\.,? 2021",
      "shortCiteRegEx" : "Pilault et al\\.",
      "year" : 2021
    }, {
      "title" : "Dropedge: Towards deep graph convolutional networks on node classification",
      "author" : [ "Yu Rong", "Wenbing Huang", "Tingyang Xu", "Junzhou Huang." ],
      "venue" : "In ICLR.",
      "citeRegEx" : "Rong et al\\.,? 2019",
      "shortCiteRegEx" : "Rong et al\\.",
      "year" : 2019
    }, {
      "title" : "Attentional encoder network for targeted sentiment classification",
      "author" : [ "Youwei Song", "Jiahai Wang", "Tao Jiang", "Zhiyue Liu", "Yanghui Rao." ],
      "venue" : "arXiv preprint arXiv:1902.09314.",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "In NIPS, pages 6000–6010.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Graph attention networks",
      "author" : [ "Petar Veličković", "Guillem Cucurull", "Arantxa Casanova", "Adriana Romero", "Pietro Liò", "Yoshua Bengio." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Veličković et al\\.,? 2018",
      "shortCiteRegEx" : "Veličković et al\\.",
      "year" : 2018
    }, {
      "title" : "Disconnected recurrent neural networks for text categorization",
      "author" : [ "Baoxin Wang." ],
      "venue" : "In ACL.",
      "citeRegEx" : "Wang.,? 2018",
      "shortCiteRegEx" : "Wang.",
      "year" : 2018
    }, {
      "title" : "Heterogeneous graph neural networks for extractive document summarization",
      "author" : [ "Danqing Wang", "Pengfei Liu", "Yining Zheng", "Xipeng Qiu", "Xuan-Jing Huang." ],
      "venue" : "In ACL.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Cross-lingual knowledge graph alignment via graph convolutional networks",
      "author" : [ "Zhichun Wang", "Qingsong Lv", "Xiaohan Lan", "Yu Zhang." ],
      "venue" : "In EMNLP.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning what to share: Leaky multi-task network for text classification",
      "author" : [ "Liqiang Xiao", "Honglun Zhang", "Wenqing Chen", "Yongkun Wang", "Yaohui Jin." ],
      "venue" : "In Coling.",
      "citeRegEx" : "Xiao et al\\.,? 2018",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2018
    }, {
      "title" : "Bert post-training for review reading comprehension and aspect-based sentiment analysis",
      "author" : [ "Hu Xu", "Bing Liu", "Lei Shu", "Philip Yu." ],
      "venue" : "In NACL.",
      "citeRegEx" : "Xu et al\\.,? 2019",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2019
    }, {
      "title" : "Investigating capsule networks with dynamic routing for text classification",
      "author" : [ "Min Yang", "Wei Zhao", "Jianbo Ye", "Zeyang Lei", "Zhou Zhao", "Soufei Zhang." ],
      "venue" : "In EMNLP.",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Graph convolutional networks for text classification",
      "author" : [ "Liang Yao", "Chengsheng Mao", "Yuan Luo." ],
      "venue" : "In AAAI, volume 33, pages 7370–7377.",
      "citeRegEx" : "Yao et al\\.,? 2019",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2019
    }, {
      "title" : "Graph transformer networks",
      "author" : [ "Seongjun Yun", "Minbyul Jeong", "Raehyun Kim", "Jaewoo Kang", "Hyunwoo J Kim." ],
      "venue" : "Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Yun et al\\.,? 2019",
      "shortCiteRegEx" : "Yun et al\\.",
      "year" : 2019
    }, {
      "title" : "Aspect-based sentiment classification with aspectspecific graph convolutional networks",
      "author" : [ "Chen Zhang", "Qiuchi Li", "Dawei Song." ],
      "venue" : "In EMNLPIJCNLP.",
      "citeRegEx" : "Zhang et al\\.,? 2019a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Text graph transformer for document classification",
      "author" : [ "Haopeng Zhang", "Jiawei Zhang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8322–8327, Online. Association for Computational",
      "citeRegEx" : "Zhang and Zhang.,? 2020",
      "shortCiteRegEx" : "Zhang and Zhang.",
      "year" : 2020
    }, {
      "title" : "Get rid of suspended animation problem: Deep diffusive neural network on graph semi-supervised classification",
      "author" : [ "Jiawei Zhang." ],
      "venue" : "arXiv preprint arXiv:2001.07922.",
      "citeRegEx" : "Zhang.,? 2020",
      "shortCiteRegEx" : "Zhang.",
      "year" : 2020
    }, {
      "title" : "Gresnet: Graph residual network for reviving deep gnns from suspended animation",
      "author" : [ "Jiawei Zhang", "Lin Meng." ],
      "venue" : "CoRR, abs/1909.05729.",
      "citeRegEx" : "Zhang and Meng.,? 2019",
      "shortCiteRegEx" : "Zhang and Meng.",
      "year" : 2019
    }, {
      "title" : "Graph-bert: Only attention is needed for learning graph representations",
      "author" : [ "Jiawei Zhang", "Haopeng Zhang", "Congying Xia", "Li Sun." ],
      "venue" : "arXiv preprint arXiv:2001.05140.",
      "citeRegEx" : "Zhang et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Bayesian graph convolutional neural networks for semi-supervised classification",
      "author" : [ "Yingxue Zhang", "Soumyasundar Pal", "Mark Coates", "Deniz Ustebay." ],
      "venue" : "In AAAI, volume 33, pages 5829–5836.",
      "citeRegEx" : "Zhang et al\\.,? 2019b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Every document owns its structure: Inductive text classification via graph neural networks",
      "author" : [ "Yufeng Zhang", "Xueli Yu", "Zeyu Cui", "Shu Wu", "Zhongzhen Wen", "Liang Wang." ],
      "venue" : "In ACL.",
      "citeRegEx" : "Zhang et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Network-wide traffic flow estimation with insufficient volume detection and crowdsourcing data",
      "author" : [ "Zhengchao Zhang", "Meng Li", "Xi Lin", "Yinhai Wang." ],
      "venue" : "Transportation Research Part C: Emerging Technologies, 121:102870.",
      "citeRegEx" : "Zhang et al\\.,? 2020c",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Modeling sentiment dependencies with graph convolutional networks for aspect-level sentiment classification",
      "author" : [ "Pinlong Zhao", "Linlin Hou", "Ou Wu." ],
      "venue" : "Knowledge-Based Systems.",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep graph contrastive representation learning",
      "author" : [ "Yanqiao Zhu", "Yichen Xu", "Feng Yu", "Qiang Liu", "Shu Wu", "Liang Wang." ],
      "venue" : "arXiv preprint arXiv:2006.04131.",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    }, {
      "title" : "Adabelief optimizer: Adapting stepsizes by the belief in observed gradients",
      "author" : [ "Juntang Zhuang", "Tommy Tang", "Yifan Ding", "Sekhar C Tatikonda", "Nicha Dvornek", "Xenophon Papademetris", "James Duncan." ],
      "venue" : "NeurIPS, 33.",
      "citeRegEx" : "Zhuang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhuang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 34,
      "context" : "Graph neural networks (GNNs) are usually used to learn the node representations in Euclidean space from graph data, which have been developed to one of the hottest research topics in recent years (Zhang, 2020).",
      "startOffset" : 196,
      "endOffset" : 209
    }, {
      "referenceID" : 37,
      "context" : "The primitive GNNs relied on recursive propagation on graphs, which takes a long time to train (Zhang et al., 2019b).",
      "startOffset" : 95,
      "endOffset" : 116
    }, {
      "referenceID" : 13,
      "context" : "One major variant of GNNs, graph convolutional networks (GCNs) (Kipf and Welling, 2017; Yao et al., 2019), takes spectral filtering to replace recursive message passing and needs only a shallow network to convergent, which have been used in various NLP tasks.",
      "startOffset" : 63,
      "endOffset" : 105
    }, {
      "referenceID" : 30,
      "context" : "One major variant of GNNs, graph convolutional networks (GCNs) (Kipf and Welling, 2017; Yao et al., 2019), takes spectral filtering to replace recursive message passing and needs only a shallow network to convergent, which have been used in various NLP tasks.",
      "startOffset" : 63,
      "endOffset" : 105
    }, {
      "referenceID" : 38,
      "context" : "∗corresponding author: Qingcai Chen have became popular in more tasks, such as word embedding (Zhang et al., 2020b), semantic analysis (Zhang et al.",
      "startOffset" : 94,
      "endOffset" : 115
    }, {
      "referenceID" : 32,
      "context" : ", 2020b), semantic analysis (Zhang et al., 2019a), document summarization (Wang et al.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 25,
      "context" : ", 2019a), document summarization (Wang et al., 2020), knowledge graph (Wang et al.",
      "startOffset" : 33,
      "endOffset" : 52
    }, {
      "referenceID" : 35,
      "context" : "As a result, the multi-hop graph reasoning is very tardy in GCN and easily causes the suspended animation problem (Zhang and Meng, 2019).",
      "startOffset" : 114,
      "endOffset" : 136
    }, {
      "referenceID" : 20,
      "context" : "However, as well known that the deep neural network (DNN) is tough to train and easily causes the over-fitting problem (Rong et al., 2019).",
      "startOffset" : 119,
      "endOffset" : 138
    }, {
      "referenceID" : 23,
      "context" : "Some newest studies to improve the multi-hop graph reasoning include graph attention networks (GATs) (Veličković et al., 2018), graph residual neural network (GRESNET) (Zhang and Meng, 2019), graph diffusive neural network (DIFNET)",
      "startOffset" : 101,
      "endOffset" : 126
    }, {
      "referenceID" : 35,
      "context" : ", 2018), graph residual neural network (GRESNET) (Zhang and Meng, 2019), graph diffusive neural network (DIFNET)",
      "startOffset" : 49,
      "endOffset" : 71
    }, {
      "referenceID" : 39,
      "context" : "(Zhang, 2020), TGMC-S (Zhang et al., 2020c) and Graph Transformer Networks (Yun et al.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 31,
      "context" : ", 2020c) and Graph Transformer Networks (Yun et al., 2019; Zhang and Zhang, 2020).",
      "startOffset" : 40,
      "endOffset" : 81
    }, {
      "referenceID" : 33,
      "context" : ", 2020c) and Graph Transformer Networks (Yun et al., 2019; Zhang and Zhang, 2020).",
      "startOffset" : 40,
      "endOffset" : 81
    }, {
      "referenceID" : 22,
      "context" : "Transformers (Vaswani et al., 2017) and corresponding pre-trained models (Xu et al.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 28,
      "context" : ", 2017) and corresponding pre-trained models (Xu et al., 2019) could be thought of as fully-connected graph neural networks that contain the multi-hop dependencies.",
      "startOffset" : 45,
      "endOffset" : 62
    }, {
      "referenceID" : 6,
      "context" : "Graph Transformer (Dwivedi and Bresson, 2020) generalizes the Transformer to arbitrary graphs, and improves inductive learning from Laplacian eigenvectors on graph topology.",
      "startOffset" : 18,
      "endOffset" : 45
    }, {
      "referenceID" : 22,
      "context" : "The standard self-attention (Vaswani et al., 2017) has a O ( N2 ) computation complexity and it is hard to be applied on long sequence.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 8,
      "context" : "Even the existing sparsity attention methods, like the StarTransformer (Guo et al., 2019) and Extended Transformer Construction (ETC) (Ainslie et al.",
      "startOffset" : 71,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : ", 2019) and Extended Transformer Construction (ETC) (Ainslie et al., 2020), have reduced the quadratic dependence limit of sequence length to linear dependence, but the fullyconnected graph structure cannot be kept.",
      "startOffset" : 52,
      "endOffset" : 74
    }, {
      "referenceID" : 13,
      "context" : "The GCN model proposed by (Kipf and Welling, 2017) is the one we interested, and it is defined on graph G = {V, E}, where V is the node set and E is the edge set.",
      "startOffset" : 26,
      "endOffset" : 50
    }, {
      "referenceID" : 14,
      "context" : "The attention mechanism is an effective way to extract task-relevant features from inputs, and it helps the model to make better decisions (Lee et al., 2019).",
      "startOffset" : 139,
      "endOffset" : 157
    }, {
      "referenceID" : 22,
      "context" : "It has various approaches to compute the attention score from features, and the scaled dot-product attention proposed in Transformers (Vaswani et al., 2017) is the most popular one.",
      "startOffset" : 134,
      "endOffset" : 156
    }, {
      "referenceID" : 15,
      "context" : "As the node state transition Ã2k causes the over-smoothing problem (Li et al., 2018; Nt and Maehara, 2019), we take the dynamic pairwise relationship Ad self-adaptively learned by the attention mechanism to turn the direction of node state transition.",
      "startOffset" : 67,
      "endOffset" : 106
    }, {
      "referenceID" : 18,
      "context" : "As the node state transition Ã2k causes the over-smoothing problem (Li et al., 2018; Nt and Maehara, 2019), we take the dynamic pairwise relationship Ad self-adaptively learned by the attention mechanism to turn the direction of node state transition.",
      "startOffset" : 67,
      "endOffset" : 106
    }, {
      "referenceID" : 42,
      "context" : "Our model is optimized with adaBelief (Zhuang et al., 2020) with a learning rate 1e − 5.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 38,
      "context" : "Table 1: Test accuracy (%) on small-scale English datasets, where the results labeled with ? are cited from (Zhang et al., 2020b).",
      "startOffset" : 108,
      "endOffset" : 129
    }, {
      "referenceID" : 12,
      "context" : "Model AG SST-1 SST-2 Yelp-F fastText (Joulin et al., 2017) 92.",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 40,
      "context" : "The results labeled with ? are cited from (Zhao et al., 2020).",
      "startOffset" : 42,
      "endOffset" : 61
    }, {
      "referenceID" : 40,
      "context" : "This task aims to identify whether the sentiment polarities of aspect are explicitly given in sentences (Zhao et al., 2020).",
      "startOffset" : 104,
      "endOffset" : 123
    }, {
      "referenceID" : 40,
      "context" : "The datasets used in this task include TWITTER, LAP14, REST14, REST15, REST16 (Zhao et al., 2020).",
      "startOffset" : 78,
      "endOffset" : 97
    }, {
      "referenceID" : 2,
      "context" : "The fourth experiment evaluates HDGCN’s performance on the Stanford natural language inference (SNLI) task (Bowman et al., 2015).",
      "startOffset" : 107,
      "endOffset" : 128
    }, {
      "referenceID" : 17,
      "context" : "All the comparison methods include fine-tuned BERTbase, MT-DNN (Liu et al., 2020), SMART (Jiang et al.",
      "startOffset" : 63,
      "endOffset" : 81
    }, {
      "referenceID" : 9,
      "context" : ", 2020), SMART (Jiang et al., 2020), and CA-MTL (Pilault et al.",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 23,
      "context" : "The node features correspond to elements of a bagof-words representation of a document (Veličković et al., 2018).",
      "startOffset" : 87,
      "endOffset" : 112
    }, {
      "referenceID" : 13,
      "context" : "Transductive Inductive (ACC, %) (micro-F1) Model Cora Citeseer Pubmed PPI GCN (Kipf and Welling, 2017) 85.",
      "startOffset" : 78,
      "endOffset" : 102
    } ],
    "year" : 2021,
    "abstractText" : "Graph convolutional network (GCN) has become popular in various natural language processing (NLP) tasks with its superiority in longterm and non-consecutive word interactions. However, existing single-hop graph reasoning in GCN may miss some important nonconsecutive dependencies. In this study, we define the spectral graph convolutional network with the high-order dynamic Chebyshev approximation (HDGCN), which augments the multi-hop graph reasoning by fusing messages aggregated from direct and long-term dependencies into one convolutional layer. To alleviate the over-smoothing in high-order Chebyshev approximation, a multi-vote based crossattention (MVCAttn) with linear computation complexity is also proposed. The empirical results on four transductive and inductive NLP tasks and the ablation study verify the efficacy of the proposed model. Our source code is available at https://github.com/ MathIsAll/HDGCN-pytorch.",
    "creator" : "LaTeX with hyperref"
  }
}