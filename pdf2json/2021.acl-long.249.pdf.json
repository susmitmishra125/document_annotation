{
  "name" : "2021.acl-long.249.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "MultiMET: A Multimodal Dataset for Metaphor Understanding",
    "authors" : [ "Dongyu Zhang", "Minghao Zhang", "Heting Zhang", "Liang Yang", "Hongfei LIN" ],
    "emails" : [ "hflin@dlut.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3214–3225\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3214"
    }, {
      "heading" : "1 Introduction",
      "text" : "Metaphor is frequently employed in human language and its ubiquity in everyday communication has been established in empirical studies (Cameron, 2003; Steen, 2010; Shutova et al., 2010). Since Lakoff and Johnson (1980) introduced conceptual metaphor theory (CMT), metaphor has been regarded as not only a linguistic, but also a cognitive phenomenon for structuring human thought. Individuals use one usually concrete concept in metaphors to render another usually abstract one for reasoning and communication. For example,\nin the metaphorical utterance “knowledge is treasure,” knowledge is viewed in terms of treasure to express that knowledge can be valuable. According to CMT, metaphor involves the mapping process by which a target domain is conceptualized or understood in terms of a source domain.\nAs a means of cognition and communication, metaphor can occur in more modes than text alone. Multimodal information in which vision/audio content is integrated with the text can also contribute to metaphoric conceptualization (Forceville and Urios-Aparisi, 2009; Ventola et al., 2004). A multimodal metaphor is defined as a mapping of domains from different modes such as text and image, text and sound, or image and sound (Forceville and Urios-Aparisi, 2009). For example, in Figure 1 (a), the metaphorical message of fire in the sky is conveyed by a mapping between the target domain “sky” (sunset) and the source domain “fire” from two modalities. Figure 1 (b) offers another example with the metaphor of lungs made from cigarettes so a relation is triggered between two different entities, lung and cigarette, with the perceptual idea that smoking causes lung cancer. The source domain “cigarette” comes from the image, while the target domain “lung” appears in both text and image. Understanding multimodal metaphor\nrequires decoding metaphorical messages and involves many cognitive efforts such as identifying the semantic relationship between two domains (Coulson and Van Petten, 2002; Yang et al., 2013), interpreting authorial intent from multimodal messages (Evan Nelson, 2008), analyzing the sentiment metaphors convey (Ervas, 2019), which might be difficult for computers to do.\nQualitative studies have investigated the interplay between different modes underlying the understanding of multimodal metaphors in communicative environments such as advertisements (Forceville et al., 2017; Urios-Aparisi, 2009), movies (Forceville, 2016; Kappelhoff and Müller, 2011), songs (Forceville and Urios-Aparisi, 2009; Way and McKerrell, 2017), and cartoons (Refaie, 2003; Xiufeng, 2013). In particular, with the development of mass communication, texts nowadays are often combined with other modalities such as images and videos to achieve a vivid, appealing, persuasive, or aesthetic effect for the audience. This rapidly growing trend toward multimodality requires a shift to extend metaphor studies from monomodality to multimodality, as well as from theory-driven analysis to data-driven empirical testing for in-depth metaphor understanding.\nDespite the potential and importance of multimodal information for metaphor research, there has been little work on the automatic understanding of multimodal metaphors. While a number of approaches to metaphor processing have been proposed with a focus on text in the NLP community (Shutova et al., 2010; Mohler et al., 2013; Jang et al., 2015, 2017; Shutova et al., 2017; Pramanick et al., 2018; Liu et al., 2020), multimodal metaphors have not received the full attention they deserve, partly due to the severe lack of multimodal metaphor datasets with their challenging and timeand labor-consuming creation.\nTo overcome the above limitations, we propose a novel multimodal metaphor dataset (MultiMET) consisting of text-image pairs (text and its corresponding image counterparts) manually annotated for metaphor understanding. MultiMET will expand metaphor understanding from monomodality to multimodality and help to improve the performance of automatic metaphor comprehension systems by investigating multimodal cues. Our main contributions are as follows:\n• We create a novel multimodal dataset consisting of 10,437 text-image pair samples from\na range of resources including social media (Twitter and Facebook), and advertisements. MultiMET will be released publicly for research.\n• We present fine-grain manual multimodal annotations of the occurrence of metaphors, metaphor category, what sentiment metaphors evoke, and author intent. The quality control and agreement analyses for multiple annotators are described.\n• We quantitatively show the role of textual and visual modalities for metaphor detection; whether and to what extent metaphor affects the distribution of sentiment and intention, which quantitatively explores the mechanism of multimodal metaphor.\n• We propose three tasks to evaluate finegrained multimodal metaphor understanding abilities, including metaphor detection, sentiment analysis, and intent detection in multimodal metaphor. A range of baselines with benchmark results are reported to show the potential and usefulness of the MultiMET for future research."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Metaphor Datasets",
      "text" : "Although datasets of multimodal metaphors are scarce, a variety of monomodal datasets for metaphor studies have been created in recent years. Table 1 lists these datasets with their properties.\nNumerous text metaphor datasets have been published for metaphor processing in the NLP community including several popular ones, e.g., the VU Amsterdam Metaphor Corpus (VUAMC) (Steen, 2010), TroFi Example Base (Birke and Sarkar, 2006), and MOH-X (Mohammad et al., 2016). The largest one, VUAMC, consists of over 10,000 samples spread across 16,000 sentences, while others contain less than 5,000 samples. However, most existing metaphor datasets contain only textual data. Image metaphor datasets are few and they are pretty limited in the size and the scope of the data, such as VisMet (Steen, 2018), which is a visual metaphor online resource consisting of only 353 image samples. Although Shutova et al. (2016) constructed both text and image samples, their images were obtained by using a given phrase and queried Google\nimages. In that way, words and images in their work may be not suitably presented by each other.\nThe cognitive nature of metaphor implies that not only one modal isolation, but rather integrated multimodal information may contribute to metaphor expression and understanding, which makes our dataset MultiMET, which is large scale and contains both natural text and image messages and their annotations, different from existing datasets and more important for metaphor studies."
    }, {
      "heading" : "2.2 Metaphor Understanding",
      "text" : "Automatic metaphor understanding requires accomplishing certain tasks to decode metaphorical messages. In this paper, we focus on three important tasks for NLP in understanding metaphor: metaphor detection, sentiment analysis, and author intent detection. There has been increasing interest in NLP in various approaches to metaphor detection based on monomodal text. Early metaphor studies have focused on hand-constructed knowledge and machine learning techniques (Mason, 2004; Turney et al., 2011; Tsvetkov et al., 2014; Hovy et al., 2013). Others have also used distributional clustering (Shutova et al., 2013) and unsupervised approaches (Shutova et al., 2017; Mao et al., 2018). More recently, deep learning models have been explored to understand metaphor. However, little has been explored in multimodal metaphor detection except by Shutova et al. (2016), who are among the very few to explore the fusion of textual and image modalities to detect multimodal metaphor. Their results demonstrate the positive effect of combining textual and image features for metaphor detection.\nHowever, in their work, image features are extracted from a small size of constructed examples rather than natural samples of texts integrated with images, like MultiMET in our work. In addi-\ntion, apart from multimodal metaphor detection, the tasks related to metaphor understanding like sentiment detection and author intent detection in multimodal metaphor also have rarely been studied, although there exist similar multimodal studies in different tasks (Wang et al., 2017; Zadeh et al., 2017; Kruk et al., 2019)."
    }, {
      "heading" : "3 The MultiMET Dataset",
      "text" : ""
    }, {
      "heading" : "3.1 Data Collection",
      "text" : "With the goal of creating a large-scale multimodal metaphor dataset to support research on understanding metaphors, we collect data that contains both text and image from a range of sources including social media (Twitter and Facebook), and advertisements. Table 2 shows an overview for the statistics of the dataset.\nSocial Media. To collect potential metaphorical samples from Twitter and Facebook, we retrieved posts by querying hashtags metaphor or metaphorical. We collected publicly available Twitter and Facebook posts using Twitter and Facebook APIs complying with Twitter and Facebook’s terms of service. What the author labels as metaphorical is not always aligned with the actual definition of metaphor in our study. To collect metaphors whose nature accorded with what we define as multimodal metaphors, we re-annotated “metaphorical or literal” in the below section to potential Twitter and Facebook posts that other authors annotated as metaphor with hashtags.\nAdvertisements. Based on our review of linguistic literature on multimodal metaphor, we focused on an important source that is the main context of study: advertisements. Metaphorical messages abound in advertisements , which offer a natural and rich resource of data on metaphor and how textual and visual factors combine and interact (Sobrino, 2017; Forceville et al., 2017). We collected\npotential metaphorical samples of advertising from a large, publicly released dataset of 64,832 image advertisements that contain both images and inside text (Ye et al., 2019). To obtain the textual information, we extracted inside text from images using the API provided by Baidu AI. After that, human annotators rectified the extracted inaccurate text, removed any blurred text, and obtained text + image pairs from advertisements."
    }, {
      "heading" : "3.2 Data Filter",
      "text" : "For text data, we removed external links and mentions (@username); we removed non-English text using the LANGID (Lui and Baldwin, 2012) library to label each piece of data with a language tag; we removed strange symbols such as emojis; we removed “metaphor” or “metaphoric” when they were regular words rather than hashtags, because explicit metaphorical expressions are not our interest (e.g., “This metaphor is very appropriate”); we removed text with fewer than 3 words or more than 40 words. For image data, we removed textbased images (all the words are in the image), as well as images with low resolution. Because this task is about multimodal metaphor, it is necessary to maintain consistency of data between models. In other words, either both the image data and the text data should be removed, or neither. In addition, in the de-duplication step, we considered removal\nonly when both text and images were repeated."
    }, {
      "heading" : "3.3 Annotation Model",
      "text" : "We annotated the text-image pairs with the occurrence of metaphors (literal or metaphorical); (if metaphorical) relations of target and source domain (target/source: target/source vocabulary in text or verbalized target/source vocabulary in image); target/source modality (text, image, or text + image), metaphor category (text-dominant, imagedominant, or complementary); sentiment category (the sentiment metaphors evoke, namely very negative, negative, neutral, positive, or very positive), and author intents (descriptive, expressive, persuasive, or other). The annotation model was AnnotationModel = (Occurrence, Target, Source, TargetModality, SourceModality, MetaphorCategory, SentimentCategory, Intent, DataSource). Figure 3 is an annotation example."
    }, {
      "heading" : "3.4 Metaphor Annotation",
      "text" : "Metaphor category. There are a variety of ways in which texts and images are combined in multimodal content (Hendricks et al., 2016; Chen et al., 2017). Based on our review of the literature and observation of the samples in our dataset, we follow Tasić and Stamenković (2015) and divide multimodal metaphor into three categories: text dominant, image dominant, and complementary. Sometimes metaphors are expressed through texts with a mapping between source and target domains while the accompanying images serve as a visual illustration of the metaphors in the text, which is text dominant. As in Figure 2 (a), the text itself is sufficient to convey metaphorical information and can be identified as metaphorical expressions. “Highway” is a visual illustration of the source domain in a textual modality. By contrast, in the image dominant category, images play the dominant role in conveying metaphorical information and they provide sufficient information for readers to under-\nstand the metaphors. In Figure 2 (b), where we see the metaphorical message “Beetle (cars) are blood cells,” the text enriches the understanding of metaphorical meaning by adding an explanation “your heart beats faster” to the visual manifestation. The complementary category involves a roughly equal role of texts and images in rendering metaphorical information. The understanding of metaphor depends on the interaction of and balance between different modalities. If texts and images are interpreted separately, metaphors cannot be understood. In Figure 2 (c), when people read the text, “A kitten is kissing a flower,” and the inside text “Butterflies are not insects,” they do not realize the metaphorical use until they observe the butterfly in the corresponding image and infer that the target “butterfly” is expressed in term of the source “flower”.\nMetaphorical or literal. Our annotations focus on the dimension of expression, which involves identification of metaphorical and literal expressions by verbal means and visual means (Forceville, 1996; Phillips and McQuarrie, 2004). The metaphor annotation takes place at the relational level, which involves the identification of metaphorical relations between source and target domain expressions. For text modality, source and target domain expressions mean source and target domain words used in metaphorical texts. For image modality, source and target domain expressions mean words’ verbalized source and target domain in the visual modality. That is, the annotation of metaphorical relations represented in the modality of image involve the verbalization of the metaphor’s domains. Annotations involve naming and labeling what is linguistically familiar. Unlike text modality, which relies on explicit linguistic cues, for image modality, metaphorical relations are annotated based on perceptions of visual unities, and they determine the linguistic familiarity of images as well as existing words in the metaphor’s domains. Following Šorm and Steen (2018), annotators identified the metaphorical text+image pairs by looking at the incongruous units and explaining one non-reversible “A is B” identity relation, where two domains were expressed by different modalites."
    }, {
      "heading" : "3.5 Intent and Sentiment Annotation",
      "text" : "Interpreting authorial intent from multimodal messages in metaphor seems to be important for under-\nstanding metaphors. As mentioned above, within CMT, the essence of metaphor is using one thing from a source domain to express and describe another from a target domain. This implies that one important intent of creating metaphor could be to enable readers to understand the entities being described better. “Perceptual resemblance” is a major means of triggering a metaphorical relation between two different entities (Forceville and Urios-Aparisi, 2009). We name it descriptive intent, which involves visual and textual representations regarding the object, event, concept, information, action or character, etc. Moreover, in modern times, the increasing ubiquity of multimodal metaphors means that people cannot ignore its power of persuasion (Urios-Aparisi, 2009). People often leverage metaphor in communication environments such as advertisements and social media to persuade readers to buy or do things. We name this intent as persuasive. In addition, inspired by a variety of arousing, humorous, or aesthetic effects of metaphors (Christmann et al., 2011), the expressive is included in our intent annotation within the enlarged definition: expressing attitude, thought, emotion, feeling, attachment, etc. Based on these factors as well as investigation of the samples in our datasets, we generalized their taxonomy and listed the categories of the author intent in metaphor as descriptive,persuasive, expressive, and others.\nNumerous studies show that metaphorical language frequently expresses sentiments or emotions implicitly (Goatly, 2007; Kövecses, 1995, 2003). Compared to literal expressions, metaphors elicit more emotional activation of the human brain in the same context (Citron and Goldberg, 2014). Thus we also added the sentiment in our annotation, to test whether the sentiment impact of metaphors is stronger than literary messages from a multimodal perspective. The sentiment was placed in one of the five categories of very negative, negative, neutral, positive, or very positive."
    }, {
      "heading" : "3.6 Annotation Process",
      "text" : "We took two independent annotation approaches for two different types of tasks: selecting types of sentiment and intent and the annotation of metaphor. To select the options for sentiment and intent, we used a majority vote through CrowdFlower, the crowdsourcing platform. The participants were randomly presented with both the text and vision components with the instruction on the\ntop of each text + image pair for options. The annotation of metaphors includes metaphor occurrence, metaphor category and domain relation annotation. For metaphor annotation, we used expert annotators to complete the challenging annotation task, which required relatively deep understanding of metaphorical units and the complete task of verbalization of domains in image. The annotator team comprised five annotators who are postgraduate student researchers majoring in computational linguistics with metaphor study backgrounds. The annotators formed groups of two, plus one extra person. Using cross-validation, the two-member groups annotated, and the fifth person intervened if they disagreed."
    }, {
      "heading" : "3.7 Quality Control and Inner Agreement",
      "text" : "Annotations of multimodal metaphors rely on annotators’ opinions and introspection, which might be subjective. Thus we took corresponding, different measures for different types of annotations to achieve high-quality annotation. To select options, we established strict criteria for the choice of category. Each text-image pair was annotated by at least 10 annotators and we used a majority vote through CrowdFlower, the crowdsourcing platform. Following Shutova (2017), we chose the category of annotated options on which 70% or more annotators agreed as the answer to each question (final decision) to provide high confidence of annotation. For metaphor annotation, we added a guideline course, detailed instruction, and many samples, and we held regular meetings to discuss annotation problems and matters that needed attention. The guidelines changed three times when new problems emerged or good improvement methods were found. The kappa score, κ, was used to measure inter-annotator agreements (Fleiss, 1971). The agreement on the identification of literal or metaphorical was κ = 0.67; identification of text dominant, image dominant or complementary was\nκ = 0.79; the identification of source and target domain relation was κ = 0.58, which means they are substantially reliable."
    }, {
      "heading" : "4 Dataset Analysis",
      "text" : "Metaphor Category. We analyzed the role of textual and visual modalities to detect metaphors. From Figure 4 (a), we can see a complementary category among the three kinds of multimodal metaphors, which requires the interplay of textual and visual modality to understand the metaphorical meaning. It accounts for the largest proportion of metaphors, followed by the text-dominant and image-dominant categories. It shows the contribution of visual factors, which are similarly important in detecting metaphors. We therefore present a quantitative study of the role of textual and visual modalities in metaphor detection through human annotations and confirm the role and contribution of visuals in metaphor occurrence in natural language.\nAuthor Intent. Figure 4 (b) shows that expressive and persuasive intentions occur most frequently in the metaphorical data. However, descriptive intention occurs most frequently in the non-metaphorical data. This suggests that on the one hand, we are more likely to use metaphorical expressions when expressing our feelings, expressing emotions, or trying to persuade others. On the other hand, we tend to use literal expressions to make relatively objective statements.\nSentiment. Figure 4 (c) shows that there are some differences in the distribution of sentiment between the metaphorical data and non-metaphorical data. In the non-metaphorical data, neutral sentiment accounted for the largest proportion of 51%, followed by positive sentiment (33%), strong positive sentiment (7%), negative sentiment (7%), and strong negative sentiment (2%). In the metaphorical data, positive sentiment accounted for the largest proportion of 42%, followed by neutral sen-\ntiments (39%), strong positive sentiment (8%), negative sentiment (8%), and strong negative sentiment (3%). It turns out that there are more non-neutral sentiments in metaphor expression than in nonmetaphorical expression, and that metaphors are more frequently used to convey sentiments. Our findings accord with the results of previous studies on monomodal textual metaphors that metaphors convey more sentiments or emotions than literary text (Mohammad et al., 2016). We confirm the stronger emotional impact of metaphors than literary messages from a multimodal perspective.\nIn positive sentiment, the most common words in the source domain are person, face, and flower; the most common words in the target domain are love, life, and success. In negative sentiment, heart, food, and smoke are the most common words in the source domain, and the world, disaster, and life are the most common words in the target domain. This shows that sentiment tendency can influence the category in the source and target domains to some extent."
    }, {
      "heading" : "5 Experiment",
      "text" : "For the dataset constructed for this paper, we propose three tasks and provide their baselines, namely multimodal metaphor detection, multimodal metaphor sentiment analysis, and multimodal metaphor author intent detection.\nWe used the model shown in Figure 5 to detect metaphors, metaphorical sentiments, and metaphorical intentions. For text input, we used a text encoder to encode the text and to get the feature vector of the text. This paper used two different methods to encode the text, namely the pre-trained Bidirectional Encoder Representations from Transformers (BERT) model (Devlin et al., 2019) and Bidirectional Long-Short Term Memory (Bi-LSTM) networks (Medsker and Jain, 2001). Similarly, for image input, we used an image encoder to extract image features. We used three different image pre-\ntraining models: VGG16 (Simonyan and Zisserman, 2014), ResNet50 (He et al., 2016), and EfficientNet (Tan and Le, 2019). These methods have been widely used by researchers in feature extraction for various tasks.\nAfter obtaining the text feature vector and the image feature vector, we used four different feature fusion methods to combine the vectors, namely concatenation (Suryawanshi et al., 2020), elementwise multiply (Mai et al., 2020), element-wise add (Cai et al., 2019), and maximum (Das, 2019). Finally, we inputted the fusion vector into a fully connected layer and obtained the probabilities of different categories through the softmax activation function."
    }, {
      "heading" : "5.1 Experiment Settings",
      "text" : "We used Pytorch (Paszke et al., 2019) to build the model. The pre-trained models are available in Pytorch. The word embeddings have been trained on a Wikipedia dataset by Glove (Pennington et al., 2014). In the training process, we did not update the parameters in the pre-training models. When the model gradually tended to converge, we updated the parameters of the pre-training models with training data to avoid overfitting. We used the Adam optimizer (Kingma and Ba, 2014) to optimize the loss function, and the training method of gradient clipping (Zhang et al., 2019) to avoid gradient explosion. Other hyper-parameter settings are shown in Table 3."
    }, {
      "heading" : "5.2 Results",
      "text" : "The classification results are shown in Table 4. “Random” means that random predictions were made using the data as a baseline. In general, the model performed best on metaphor detection, followed by metaphor intention detection, and finally metaphor sentiment detection. For image and\nmultimodal classification, the ResNet50 performed best, followed by VGG16, and finally EfficientNet. Because ResNet solved the problem of gradient disappearance through the method of residual connection, the classification performance was better than VGG16 and EfficientNet. For text and multimodal classification, BERT performed better than Bi-LSTM. BERT has been fully trained in a largescale corpus, using transfer learning technology to fine-tune our three tasks and data, so it can achieve better performance. From the perspective of different features, multimodal features perform best, followed by text-only features, and finally image-only features. Multimodal fusion helps to improve the classification performance by 6%. This shows that the combination of image and text features is indeed helpful for the detection and understanding of metaphors, especially the detection of sentiments and intentions in metaphors. In addition, the importance of text modal data is explained. Without text description, it is difficult to detect metaphors correctly using only visual modal data.\nTo verify the influence of feature fusion on classification, we compared four different feature fusion methods. The results are shown in Table 5. The concatenate method to merge image and text features produces the highest accuracy. It shows that concatenate can make full use of the complementarity between different modal data, eliminate the noise generated by the fusion of different modal data, and improve the detection effect. In contrast,\nthe other three fusion methods cannot effectively eliminate the influence of noise introduced by different modal data, and it therefore interferes with the training of the model. Overall, the multimode model that combines the BERT text function and the ResNet50 image function through the concatenation method performs best on our three tasks."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This paper presents the creation of a novel resource, a large-scale multimodal metaphor dataset, MultiMET, with manual fine-gained annotation for metaphor understanding and research. Our dataset enables the quantitative study of the interplay of multimodalities for metaphor detection and confirms the contribution of visuals in metaphor occurrence in natural language. It also offers a set of baseline results of various tasks and shows the importance of combining multimodal cues for metaphor understanding. We hope MultiMET provides future researchers with valuable multimodal training data for the challenging tasks of multimodal metaphor processing and understanding ranging from metaphor detection to sentiment analysis of metaphor. We also hope that MultiMET will help to expand metaphor research from monomodality to multimodality and improve the performance of automatic metaphor understanding systems and contribute to the in-depth understanding and research development of metaphors. The dataset will be publicly available for research.\nEthical Considerations\nThis research was granted ethical approval by our Institutional Review Board (Approval code: DUTIEE190725 01). We collected publicly available Twitter and Facebook data using Twitter and Facebook APIs complying with Twitter and Facebook’s terms of service. We did not store any personal data (e.g., user IDs, usernames) and we annotated the data without knowledge of individual identities.\nWe annotated all our data using two independent approaches (expert based and crowdsourcing based) for two different types of tasks: the annotation of metaphor and the selection of types of sentiment and intent. For metaphor annotation, a deep understanding of metaphorical units was necessary. This challenging task was completed by five researchers who involved in this project. To annotate sentiment and intent, we used CrowdFlower, the crowdsourcing platform. To ensure that crowd workers were fairly compensated, we paid them at an hourly rate of 15 USD per hour, which is a fair and reasonable rate of pay for crowdsourcing (Whiting et al., 2019). We launched small pilots through CrowdFlower. The pilot for sentiment options took on average 43 seconds, and crowd workers were thus paid 0.18 USD per judgment, in accordance with an hourly wage of 15 USD. At the same time, the annotation of author intent took on average 23 seconds, and we thus paid 0.10 USD per judgment, corresponding to an hourly wage of 15 USD."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank the anonymous reviewers for their insightful and valuable comments. This work is supported by NSFC Programs (No.62076051)."
    } ],
    "references" : [ {
      "title" : "A clustering approach for nearly unsupervised recognition of nonliteral language",
      "author" : [ "Julia Birke", "Anoop Sarkar." ],
      "venue" : "Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 329–336, Trento,",
      "citeRegEx" : "Birke and Sarkar.,? 2006",
      "shortCiteRegEx" : "Birke and Sarkar.",
      "year" : 2006
    }, {
      "title" : "Multimodal sarcasm detection in twitter with hierarchical fusion model",
      "author" : [ "Yitao Cai", "Huiyu Cai", "Xiaojun Wan." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2506–2515, Florence, Italy.",
      "citeRegEx" : "Cai et al\\.,? 2019",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2019
    }, {
      "title" : "Metaphor in educational discourse",
      "author" : [ "Lynne Cameron." ],
      "venue" : "A&C Black, London, UK.",
      "citeRegEx" : "Cameron.,? 2003",
      "shortCiteRegEx" : "Cameron.",
      "year" : 2003
    }, {
      "title" : "Show, adapt and tell: Adversarial training of cross-domain image captioner",
      "author" : [ "Tseng-Hung Chen", "Yuan-Hong Liao", "Ching-Yao Chuang", "Wan-Ting Hsu", "Jianlong Fu", "Min Sun." ],
      "venue" : "Proceedings of the 2017 IEEE International Conference on Com-",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "The aesthetic paradox in processing conventional and non-conventional metaphors: A reaction time study",
      "author" : [ "Ursula Christmann", "Lena Wimmer", "Norbert Groeben." ],
      "venue" : "Scientific Study of Literature, 1(2):199–240.",
      "citeRegEx" : "Christmann et al\\.,? 2011",
      "shortCiteRegEx" : "Christmann et al\\.",
      "year" : 2011
    }, {
      "title" : "Metaphorical sentences are more emotionally engaging than their literal counterparts",
      "author" : [ "Francesca MM Citron", "Adele E Goldberg." ],
      "venue" : "Journal of Cognitive Neuroscience, 26(11):2585–2595.",
      "citeRegEx" : "Citron and Goldberg.,? 2014",
      "shortCiteRegEx" : "Citron and Goldberg.",
      "year" : 2014
    }, {
      "title" : "Conceptual integration and metaphor: An event-related potential study",
      "author" : [ "Seana Coulson", "Cyma Van Petten." ],
      "venue" : "Memory & Cognition, 30(6):958–968.",
      "citeRegEx" : "Coulson and Petten.,? 2002",
      "shortCiteRegEx" : "Coulson and Petten.",
      "year" : 2002
    }, {
      "title" : "A multimodal approach to sarcasm detection on social media",
      "author" : [ "Dipto Das." ],
      "venue" : "Ph.D. thesis, Missouri State University.",
      "citeRegEx" : "Das.,? 2019",
      "shortCiteRegEx" : "Das.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Metaphor, ignorance and the sentiment of (ir) rationality",
      "author" : [ "Francesca Ervas." ],
      "venue" : "Synthese, pages 1–25.",
      "citeRegEx" : "Ervas.,? 2019",
      "shortCiteRegEx" : "Ervas.",
      "year" : 2019
    }, {
      "title" : "Multimodal synthesis and the voice of the multimedia author in a japanese efl context",
      "author" : [ "Mark Evan Nelson." ],
      "venue" : "Innovation in Language Learning and Teaching, 2(1):65–82.",
      "citeRegEx" : "Nelson.,? 2008",
      "shortCiteRegEx" : "Nelson.",
      "year" : 2008
    }, {
      "title" : "Measuring nominal scale agreement among many raters",
      "author" : [ "Joseph L Fleiss." ],
      "venue" : "Psychological Bulletin, 76(5):378–382.",
      "citeRegEx" : "Fleiss.,? 1971",
      "shortCiteRegEx" : "Fleiss.",
      "year" : 1971
    }, {
      "title" : "Pictorial metaphor in advertising",
      "author" : [ "Charles Forceville." ],
      "venue" : "Psychology Press, East Sussex, UK.",
      "citeRegEx" : "Forceville.,? 1996",
      "shortCiteRegEx" : "Forceville.",
      "year" : 1996
    }, {
      "title" : "Visual and multimodal metaphor in film",
      "author" : [ "Charles Forceville." ],
      "venue" : "Embodied metaphors in film, television, and video games: Cognitive approaches, pages 17–32. Routledge, Abingdon, USA.",
      "citeRegEx" : "Forceville.,? 2016",
      "shortCiteRegEx" : "Forceville.",
      "year" : 2016
    }, {
      "title" : "Multimodal metaphor, volume 11",
      "author" : [ "Charles Forceville", "Eduardo Urios-Aparisi." ],
      "venue" : "Walter de Gruyter, Berlin, Germany.",
      "citeRegEx" : "Forceville and Urios.Aparisi.,? 2009",
      "shortCiteRegEx" : "Forceville and Urios.Aparisi.",
      "year" : 2009
    }, {
      "title" : "Visual and multimodal metaphor in advertising: Cultural perspectives",
      "author" : [ "Charles Forceville" ],
      "venue" : "Styles of Communication, 9(2):26–41.",
      "citeRegEx" : "Forceville,? 2017",
      "shortCiteRegEx" : "Forceville",
      "year" : 2017
    }, {
      "title" : "Washing the brain: Metaphor and hidden ideology, volume 23",
      "author" : [ "Andrew Goatly." ],
      "venue" : "John Benjamins Publishing, Amsterdam, Netherlands.",
      "citeRegEx" : "Goatly.,? 2007",
      "shortCiteRegEx" : "Goatly.",
      "year" : 2007
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2016, pages 770–778, Las Vegas, USA.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep compositional captioning: Describing novel object categories without paired training data",
      "author" : [ "Lisa Anne Hendricks", "Subhashini Venugopalan", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko", "Trevor Darrell." ],
      "venue" : "Proceedings of the IEEE",
      "citeRegEx" : "Hendricks et al\\.,? 2016",
      "shortCiteRegEx" : "Hendricks et al\\.",
      "year" : 2016
    }, {
      "title" : "Identifying metaphorical word use with tree kernels",
      "author" : [ "Dirk Hovy", "Shashank Srivastava", "Sujay Kumar Jauhar", "Mrinmaya Sachan", "Kartik Goyal", "Huying Li", "Whitney Sanders", "Eduard Hovy." ],
      "venue" : "Proceedings of the 1st Workshop on Metaphor in NLP, pages",
      "citeRegEx" : "Hovy et al\\.,? 2013",
      "shortCiteRegEx" : "Hovy et al\\.",
      "year" : 2013
    }, {
      "title" : "Finding structure in figurative language: Metaphor detection with topic-based frames",
      "author" : [ "Hyeju Jang", "Keith Maki", "Eduard Hovy", "Carolyn Rose." ],
      "venue" : "Proceedings of the 18th Annual SIGDIAL Meeting on Discourse and Dialogue, pages 320–330,",
      "citeRegEx" : "Jang et al\\.,? 2017",
      "shortCiteRegEx" : "Jang et al\\.",
      "year" : 2017
    }, {
      "title" : "Metaphor detection in discourse",
      "author" : [ "Hyeju Jang", "Seungwhan Moon", "Yohan Jo", "Carolyn Rose." ],
      "venue" : "Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 384–392, Prague, Czech Republic.",
      "citeRegEx" : "Jang et al\\.,? 2015",
      "shortCiteRegEx" : "Jang et al\\.",
      "year" : 2015
    }, {
      "title" : "Embodied meaning construction: Multimodal metaphor and expressive movement in speech, gesture, and feature film",
      "author" : [ "Hermann Kappelhoff", "Cornelia Müller." ],
      "venue" : "Metaphor and the Social World, 1(2):121–153.",
      "citeRegEx" : "Kappelhoff and Müller.,? 2011",
      "shortCiteRegEx" : "Kappelhoff and Müller.",
      "year" : 2011
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv e-prints, page arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Anger: Its language, conceptualization, and physiology in the light of crosscultural evidence",
      "author" : [ "Zoltán Kövecses." ],
      "venue" : "Language and the Cognitive Construal of the World, pages 181–196.",
      "citeRegEx" : "Kövecses.,? 1995",
      "shortCiteRegEx" : "Kövecses.",
      "year" : 1995
    }, {
      "title" : "Metaphor and emotion: Language, culture, and body in human feeling",
      "author" : [ "Zoltán Kövecses." ],
      "venue" : "Cambridge University Press, Cambridge, UK.",
      "citeRegEx" : "Kövecses.,? 2003",
      "shortCiteRegEx" : "Kövecses.",
      "year" : 2003
    }, {
      "title" : "Integrating text and image: Determining multimodal document intent in instagram posts",
      "author" : [ "Julia Kruk", "Jonah Lubin", "Karan Sikka", "Xiao Lin", "Dan Jurafsky", "Ajay Divakaran." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Kruk et al\\.,? 2019",
      "shortCiteRegEx" : "Kruk et al\\.",
      "year" : 2019
    }, {
      "title" : "Metaphors we live by",
      "author" : [ "George Lakoff", "Mark Johnson." ],
      "venue" : "University of Chicago press, Chicago, USA.",
      "citeRegEx" : "Lakoff and Johnson.,? 1980",
      "shortCiteRegEx" : "Lakoff and Johnson.",
      "year" : 1980
    }, {
      "title" : "Metaphor detection using contextual word embeddings from transformers",
      "author" : [ "Jerry Liu", "Nathan O’Hara", "Alexander Rubin", "Rachel Draelos", "Cynthia Rudin" ],
      "venue" : "In Proceedings of the 2nd Workshop on Figurative Language Processing,",
      "citeRegEx" : "Liu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "langid",
      "author" : [ "Marco Lui", "Timothy Baldwin." ],
      "venue" : "py: An off-the-shelf language identification tool. In Proceedings of the ACL 2012 System Demonstrations, pages 25–30, Jeju Island, Korea.",
      "citeRegEx" : "Lui and Baldwin.,? 2012",
      "shortCiteRegEx" : "Lui and Baldwin.",
      "year" : 2012
    }, {
      "title" : "Multi-fusion residual memory network for multimodal human sentiment comprehension",
      "author" : [ "Sijie Mai", "Haifeng Hu", "Jia Xu", "Songlong Xing." ],
      "venue" : "IEEE Transactions on Affective Computing, pages 1–15.",
      "citeRegEx" : "Mai et al\\.,? 2020",
      "shortCiteRegEx" : "Mai et al\\.",
      "year" : 2020
    }, {
      "title" : "Word embedding and wordnet based metaphor identification and interpretation",
      "author" : [ "Rui Mao", "Chenghua Lin", "Frank Guerin." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
      "citeRegEx" : "Mao et al\\.,? 2018",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2018
    }, {
      "title" : "Cormet: a computational, corpus-based conventional metaphor extraction system",
      "author" : [ "Zachary J Mason." ],
      "venue" : "Computational Linguistics, 30(1):23–44.",
      "citeRegEx" : "Mason.,? 2004",
      "shortCiteRegEx" : "Mason.",
      "year" : 2004
    }, {
      "title" : "Recurrent neural networks",
      "author" : [ "Larry R Medsker", "LC Jain." ],
      "venue" : "Design and Applications, 5:1–391.",
      "citeRegEx" : "Medsker and Jain.,? 2001",
      "shortCiteRegEx" : "Medsker and Jain.",
      "year" : 2001
    }, {
      "title" : "Metaphor as a medium for emotion: An empirical study",
      "author" : [ "Saif Mohammad", "Ekaterina Shutova", "Peter Turney." ],
      "venue" : "Proceedings of the 15th Joint Conference on Lexical and Computational Semantics, pages 23–33, Berlin, Germany.",
      "citeRegEx" : "Mohammad et al\\.,? 2016",
      "shortCiteRegEx" : "Mohammad et al\\.",
      "year" : 2016
    }, {
      "title" : "Semantic signatures for example-based linguistic metaphor detection",
      "author" : [ "Michael Mohler", "David Bracewell", "Marc Tomlinson", "David Hinote." ],
      "venue" : "Proceedings of the 1st Workshop on Metaphor in NLP, pages 27–35, Atlanta, USA.",
      "citeRegEx" : "Mohler et al\\.,? 2013",
      "shortCiteRegEx" : "Mohler et al\\.",
      "year" : 2013
    }, {
      "title" : "Introducing the lcc metaphor datasets",
      "author" : [ "Michael Mohler", "Mary Brunson", "Bryan Rink", "Marc Tomlinson." ],
      "venue" : "Proceedings of the 10th International Conference on Language Resources and Evaluation, pages 4221–4227, Portorož, Slovenia.",
      "citeRegEx" : "Mohler et al\\.,? 2016",
      "shortCiteRegEx" : "Mohler et al\\.",
      "year" : 2016
    }, {
      "title" : "Pytorch: An imperative style, high-performance deep learning",
      "author" : [ "Adam Paszke", "Sam Gross", "Francisco Massa", "Adam Lerer", "James Bradbury", "Gregory Chanan", "Trevor Killeen", "Zeming Lin", "Natalia Gimelshein", "Luca Antiga" ],
      "venue" : null,
      "citeRegEx" : "Paszke et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Paszke et al\\.",
      "year" : 2019
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1532–1543, Doha, Qatar.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Beyond visual metaphor: A new typology of visual rhetoric in advertising",
      "author" : [ "Barbara J Phillips", "Edward F McQuarrie." ],
      "venue" : "Marketing Theory, 4(12):113–136.",
      "citeRegEx" : "Phillips and McQuarrie.,? 2004",
      "shortCiteRegEx" : "Phillips and McQuarrie.",
      "year" : 2004
    }, {
      "title" : "An lstm-crf based approach to token-level metaphor detection",
      "author" : [ "Malay Pramanick", "Ashim Gupta", "Pabitra Mitra." ],
      "venue" : "Proceedings of the Workshop on Figurative Language Processing 2018, pages 67– 75, New Orleans, USA.",
      "citeRegEx" : "Pramanick et al\\.,? 2018",
      "shortCiteRegEx" : "Pramanick et al\\.",
      "year" : 2018
    }, {
      "title" : "Understanding visual metaphor: The example of newspaper cartoons",
      "author" : [ "Elisabeth El Refaie." ],
      "venue" : "Visual Communication, 2(1):75–95.",
      "citeRegEx" : "Refaie.,? 2003",
      "shortCiteRegEx" : "Refaie.",
      "year" : 2003
    }, {
      "title" : "Annotation of linguistic and conceptual metaphor",
      "author" : [ "Ekaterina Shutova." ],
      "venue" : "Handbook of linguistic annotation, pages 1073–1100. Springer, New York, USA.",
      "citeRegEx" : "Shutova.,? 2017",
      "shortCiteRegEx" : "Shutova.",
      "year" : 2017
    }, {
      "title" : "Black holes and white rabbits: Metaphor identification with visual features",
      "author" : [ "Ekaterina Shutova", "Douwe Kiela", "Jean Maillard." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Shutova et al\\.,? 2016",
      "shortCiteRegEx" : "Shutova et al\\.",
      "year" : 2016
    }, {
      "title" : "Multilingual metaphor processing: Experiments with semi-supervised and unsupervised learning",
      "author" : [ "Ekaterina Shutova", "Lin Sun", "Elkin Darı́o Gutiérrez", "Patricia Lichtenstein", "Srini Narayanan" ],
      "venue" : null,
      "citeRegEx" : "Shutova et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Shutova et al\\.",
      "year" : 2017
    }, {
      "title" : "Metaphor identification using verb and noun clustering",
      "author" : [ "Ekaterina Shutova", "Lin Sun", "Anna Korhonen." ],
      "venue" : "Proceedings of the 23rd International Conference on Computational Linguistics, pages 1002– 1010, Beijing, China.",
      "citeRegEx" : "Shutova et al\\.,? 2010",
      "shortCiteRegEx" : "Shutova et al\\.",
      "year" : 2010
    }, {
      "title" : "Statistical metaphor processing",
      "author" : [ "Ekaterina Shutova", "Simone Teufel", "Anna Korhonen." ],
      "venue" : "Computational Linguistics, 39(2):301–353.",
      "citeRegEx" : "Shutova et al\\.,? 2013",
      "shortCiteRegEx" : "Shutova et al\\.",
      "year" : 2013
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman." ],
      "venue" : "arXiv e-prints, page arXiv:1409.1556.",
      "citeRegEx" : "Simonyan and Zisserman.,? 2014",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2014
    }, {
      "title" : "Multimodal metaphor and metonymy in advertising, volume 2",
      "author" : [ "Paula Pérez Sobrino." ],
      "venue" : "John Benjamins Publishing Company, Amsterdam, Netherlands.",
      "citeRegEx" : "Sobrino.,? 2017",
      "shortCiteRegEx" : "Sobrino.",
      "year" : 2017
    }, {
      "title" : "Towards a method for visual metaphor identification",
      "author" : [ "Esther Šorm", "Gerard Steen." ],
      "venue" : "Visual metaphor: Structure and process, volume 18, pages 47–88. John Benjamins Publishing Company, Amsterdam, Netherlands.",
      "citeRegEx" : "Šorm and Steen.,? 2018",
      "shortCiteRegEx" : "Šorm and Steen.",
      "year" : 2018
    }, {
      "title" : "A method for linguistic metaphor identification: From MIP to MIPVU, volume 14",
      "author" : [ "Gerard Steen." ],
      "venue" : "John Benjamins Publishing, Amsterdam, Netherlands.",
      "citeRegEx" : "Steen.,? 2010",
      "shortCiteRegEx" : "Steen.",
      "year" : 2010
    }, {
      "title" : "Visual metaphor: Structure and process, volume 18",
      "author" : [ "Gerard J Steen." ],
      "venue" : "John Benjamins Publishing Company, Amsterdam, Netherlands.",
      "citeRegEx" : "Steen.,? 2018",
      "shortCiteRegEx" : "Steen.",
      "year" : 2018
    }, {
      "title" : "Multimodal meme dataset (multioff) for identifying offensive content in image and text",
      "author" : [ "Shardul Suryawanshi", "Bharathi Raja Chakravarthi", "Mihael Arcan", "Paul Buitelaar." ],
      "venue" : "Proceedings of the 2nd Workshop on Trolling, Aggression and Cyberbully-",
      "citeRegEx" : "Suryawanshi et al\\.,? 2020",
      "shortCiteRegEx" : "Suryawanshi et al\\.",
      "year" : 2020
    }, {
      "title" : "Efficientnet: Rethinking model scaling for convolutional neural networks",
      "author" : [ "Mingxing Tan", "Quoc Le." ],
      "venue" : "Proceedings of International Conference on Machine Learning 2019, pages 6105–6114, California, USA.",
      "citeRegEx" : "Tan and Le.,? 2019",
      "shortCiteRegEx" : "Tan and Le.",
      "year" : 2019
    }, {
      "title" : "The interplay of words and images in expressing multimodal metaphors in comics",
      "author" : [ "Miloš Tasić", "Dušan Stamenković." ],
      "venue" : "Procedia-Social and Behavioral Sciences, 212:117–122.",
      "citeRegEx" : "Tasić and Stamenković.,? 2015",
      "shortCiteRegEx" : "Tasić and Stamenković.",
      "year" : 2015
    }, {
      "title" : "Metaphor detection with cross-lingual model transfer",
      "author" : [ "Yulia Tsvetkov", "Leonid Boytsov", "Anatole Gershman", "Eric Nyberg", "Chris Dyer." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
      "citeRegEx" : "Tsvetkov et al\\.,? 2014",
      "shortCiteRegEx" : "Tsvetkov et al\\.",
      "year" : 2014
    }, {
      "title" : "Literal and metaphorical sense identification through concrete and abstract context",
      "author" : [ "Peter Turney", "Yair Neuman", "Dan Assaf", "Yohai Cohen." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 680–",
      "citeRegEx" : "Turney et al\\.,? 2011",
      "shortCiteRegEx" : "Turney et al\\.",
      "year" : 2011
    }, {
      "title" : "Interaction of multimodal metaphor and metonymy in tv commercials: Four case studies",
      "author" : [ "Eduardo Urios-Aparisi." ],
      "venue" : "Multimodal Metaphor, 11:95– 116.",
      "citeRegEx" : "Urios.Aparisi.,? 2009",
      "shortCiteRegEx" : "Urios.Aparisi.",
      "year" : 2009
    }, {
      "title" : "Perspectives on multimodality, volume 6",
      "author" : [ "Eija Ventola", "Cassily Charles", "Martin Kaltenbacher." ],
      "venue" : "John Benjamins Publishing, Amsterdam, Netherlands.",
      "citeRegEx" : "Ventola et al\\.,? 2004",
      "shortCiteRegEx" : "Ventola et al\\.",
      "year" : 2004
    }, {
      "title" : "Select-additive learning: Improving generalization in multimodal sentiment analysis",
      "author" : [ "Haohan Wang", "Aaksha Meghawat", "Louis-Philippe Morency", "Eric P Xing." ],
      "venue" : "Proceeding of 2017 IEEE International Conference on Multimedia and Expo,",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Music as multimodal discourse: Semiotics, power and protest",
      "author" : [ "Lyndon CS Way", "Simon McKerrell." ],
      "venue" : "Bloomsbury Publishing, London, UK.",
      "citeRegEx" : "Way and McKerrell.,? 2017",
      "shortCiteRegEx" : "Way and McKerrell.",
      "year" : 2017
    }, {
      "title" : "Fair work: Crowd work minimum wage with one line of code",
      "author" : [ "Mark E Whiting", "Grant Hugh", "Michael S Bernstein." ],
      "venue" : "Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, pages 197–206, Hilversum, The Netherlands.",
      "citeRegEx" : "Whiting et al\\.,? 2019",
      "shortCiteRegEx" : "Whiting et al\\.",
      "year" : 2019
    }, {
      "title" : "The conceptual integration model of multimodal metaphor construction: A case study of a political cartoon",
      "author" : [ "Zhao Xiufeng." ],
      "venue" : "Foreign Languages Research, 5:1–8.",
      "citeRegEx" : "Xiufeng.,? 2013",
      "shortCiteRegEx" : "Xiufeng.",
      "year" : 2013
    }, {
      "title" : "Contextual effects on conceptual blending in metaphors: an event-related potential study",
      "author" : [ "Fan-Pei Gloria Yang", "Kailyn Bradley", "Madiha Huq", "Dai-Lin Wu", "Daniel C Krawczyk." ],
      "venue" : "Journal of Neurolinguistics, 26(2):312–326.",
      "citeRegEx" : "Yang et al\\.,? 2013",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2013
    }, {
      "title" : "Interpreting the rhetoric of visual advertisements",
      "author" : [ "Keren Ye", "Narges Honarvar Nazari", "James Hahn", "Zaeem Hussain", "Mingda Zhang", "Adriana Kovashka." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1–16.",
      "citeRegEx" : "Ye et al\\.,? 2019",
      "shortCiteRegEx" : "Ye et al\\.",
      "year" : 2019
    }, {
      "title" : "Tensor fusion network for multimodal sentiment analysis",
      "author" : [ "Amir Zadeh", "Minghai Chen", "Soujanya Poria", "Erik Cambria", "Louis-Philippe Morency." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Zadeh et al\\.,? 2017",
      "shortCiteRegEx" : "Zadeh et al\\.",
      "year" : 2017
    }, {
      "title" : "Crowd-sourcing a high-quality dataset for metaphor identification in tweets",
      "author" : [ "Omnia Zayed", "John P McCrae", "Paul Buitelaar." ],
      "venue" : "Proceedings of the 2nd Conference on Language, Data and Knowledge, pages 1–17, Leipzig, Germany.",
      "citeRegEx" : "Zayed et al\\.,? 2019",
      "shortCiteRegEx" : "Zayed et al\\.",
      "year" : 2019
    }, {
      "title" : "Why gradient clipping accelerates training: A theoretical justification for adaptivity",
      "author" : [ "Jingzhao Zhang", "Tianxing He", "Suvrit Sra", "Ali Jadbabaie." ],
      "venue" : "arXiv e-prints, page arXiv:1905.11881.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Metaphor is frequently employed in human language and its ubiquity in everyday communication has been established in empirical studies (Cameron, 2003; Steen, 2010; Shutova et al., 2010).",
      "startOffset" : 135,
      "endOffset" : 185
    }, {
      "referenceID" : 50,
      "context" : "Metaphor is frequently employed in human language and its ubiquity in everyday communication has been established in empirical studies (Cameron, 2003; Steen, 2010; Shutova et al., 2010).",
      "startOffset" : 135,
      "endOffset" : 185
    }, {
      "referenceID" : 45,
      "context" : "Metaphor is frequently employed in human language and its ubiquity in everyday communication has been established in empirical studies (Cameron, 2003; Steen, 2010; Shutova et al., 2010).",
      "startOffset" : 135,
      "endOffset" : 185
    }, {
      "referenceID" : 14,
      "context" : "Multimodal information in which vision/audio content is integrated with the text can also contribute to metaphoric conceptualization (Forceville and Urios-Aparisi, 2009; Ventola et al., 2004).",
      "startOffset" : 133,
      "endOffset" : 191
    }, {
      "referenceID" : 58,
      "context" : "Multimodal information in which vision/audio content is integrated with the text can also contribute to metaphoric conceptualization (Forceville and Urios-Aparisi, 2009; Ventola et al., 2004).",
      "startOffset" : 133,
      "endOffset" : 191
    }, {
      "referenceID" : 63,
      "context" : "volves many cognitive efforts such as identifying the semantic relationship between two domains (Coulson and Van Petten, 2002; Yang et al., 2013), interpreting authorial intent from multimodal messages (Evan Nelson, 2008), analyzing the sentiment",
      "startOffset" : 96,
      "endOffset" : 145
    }, {
      "referenceID" : 9,
      "context" : "metaphors convey (Ervas, 2019), which might be difficult for computers to do.",
      "startOffset" : 17,
      "endOffset" : 30
    }, {
      "referenceID" : 57,
      "context" : "Qualitative studies have investigated the interplay between different modes underlying the understanding of multimodal metaphors in communicative environments such as advertisements (Forceville et al., 2017; Urios-Aparisi, 2009),",
      "startOffset" : 182,
      "endOffset" : 228
    }, {
      "referenceID" : 13,
      "context" : "movies (Forceville, 2016; Kappelhoff and Müller, 2011), songs (Forceville and Urios-Aparisi, 2009; Way and McKerrell, 2017), and cartoons (Refaie, 2003; Xiufeng, 2013).",
      "startOffset" : 7,
      "endOffset" : 54
    }, {
      "referenceID" : 22,
      "context" : "movies (Forceville, 2016; Kappelhoff and Müller, 2011), songs (Forceville and Urios-Aparisi, 2009; Way and McKerrell, 2017), and cartoons (Refaie, 2003; Xiufeng, 2013).",
      "startOffset" : 7,
      "endOffset" : 54
    }, {
      "referenceID" : 14,
      "context" : "movies (Forceville, 2016; Kappelhoff and Müller, 2011), songs (Forceville and Urios-Aparisi, 2009; Way and McKerrell, 2017), and cartoons (Refaie, 2003; Xiufeng, 2013).",
      "startOffset" : 62,
      "endOffset" : 123
    }, {
      "referenceID" : 60,
      "context" : "movies (Forceville, 2016; Kappelhoff and Müller, 2011), songs (Forceville and Urios-Aparisi, 2009; Way and McKerrell, 2017), and cartoons (Refaie, 2003; Xiufeng, 2013).",
      "startOffset" : 62,
      "endOffset" : 123
    }, {
      "referenceID" : 41,
      "context" : "movies (Forceville, 2016; Kappelhoff and Müller, 2011), songs (Forceville and Urios-Aparisi, 2009; Way and McKerrell, 2017), and cartoons (Refaie, 2003; Xiufeng, 2013).",
      "startOffset" : 138,
      "endOffset" : 167
    }, {
      "referenceID" : 62,
      "context" : "movies (Forceville, 2016; Kappelhoff and Müller, 2011), songs (Forceville and Urios-Aparisi, 2009; Way and McKerrell, 2017), and cartoons (Refaie, 2003; Xiufeng, 2013).",
      "startOffset" : 138,
      "endOffset" : 167
    }, {
      "referenceID" : 45,
      "context" : "While a number of approaches to metaphor processing have been proposed with a focus on text in the NLP community (Shutova et al., 2010; Mohler et al., 2013; Jang et al., 2015, 2017; Shutova et al., 2017; Pramanick et al., 2018; Liu et al., 2020), multimodal metaphors have not received the full attention they deserve, partly due to the severe lack of multimodal metaphor datasets with their challenging and timeand labor-consuming creation.",
      "startOffset" : 113,
      "endOffset" : 245
    }, {
      "referenceID" : 35,
      "context" : "While a number of approaches to metaphor processing have been proposed with a focus on text in the NLP community (Shutova et al., 2010; Mohler et al., 2013; Jang et al., 2015, 2017; Shutova et al., 2017; Pramanick et al., 2018; Liu et al., 2020), multimodal metaphors have not received the full attention they deserve, partly due to the severe lack of multimodal metaphor datasets with their challenging and timeand labor-consuming creation.",
      "startOffset" : 113,
      "endOffset" : 245
    }, {
      "referenceID" : 44,
      "context" : "While a number of approaches to metaphor processing have been proposed with a focus on text in the NLP community (Shutova et al., 2010; Mohler et al., 2013; Jang et al., 2015, 2017; Shutova et al., 2017; Pramanick et al., 2018; Liu et al., 2020), multimodal metaphors have not received the full attention they deserve, partly due to the severe lack of multimodal metaphor datasets with their challenging and timeand labor-consuming creation.",
      "startOffset" : 113,
      "endOffset" : 245
    }, {
      "referenceID" : 40,
      "context" : "While a number of approaches to metaphor processing have been proposed with a focus on text in the NLP community (Shutova et al., 2010; Mohler et al., 2013; Jang et al., 2015, 2017; Shutova et al., 2017; Pramanick et al., 2018; Liu et al., 2020), multimodal metaphors have not received the full attention they deserve, partly due to the severe lack of multimodal metaphor datasets with their challenging and timeand labor-consuming creation.",
      "startOffset" : 113,
      "endOffset" : 245
    }, {
      "referenceID" : 28,
      "context" : "While a number of approaches to metaphor processing have been proposed with a focus on text in the NLP community (Shutova et al., 2010; Mohler et al., 2013; Jang et al., 2015, 2017; Shutova et al., 2017; Pramanick et al., 2018; Liu et al., 2020), multimodal metaphors have not received the full attention they deserve, partly due to the severe lack of multimodal metaphor datasets with their challenging and timeand labor-consuming creation.",
      "startOffset" : 113,
      "endOffset" : 245
    }, {
      "referenceID" : 50,
      "context" : ", the VU Amsterdam Metaphor Corpus (VUAMC) (Steen, 2010), TroFi Example Base (Birke and Sarkar,",
      "startOffset" : 43,
      "endOffset" : 56
    }, {
      "referenceID" : 51,
      "context" : "Image metaphor datasets are few and they are pretty limited in the size and the scope of the data, such as VisMet (Steen, 2018), which is a visual metaphor online resource consisting of only 353 image samples.",
      "startOffset" : 114,
      "endOffset" : 127
    }, {
      "referenceID" : 0,
      "context" : "TroFi (Birke and Sarkar, 2006) 3,737 (44%) Text WSJ metaphor (metaphoricity) VUAMC (Steen, 2010) 16,000 (12.",
      "startOffset" : 6,
      "endOffset" : 30
    }, {
      "referenceID" : 50,
      "context" : "TroFi (Birke and Sarkar, 2006) 3,737 (44%) Text WSJ metaphor (metaphoricity) VUAMC (Steen, 2010) 16,000 (12.",
      "startOffset" : 83,
      "endOffset" : 96
    }, {
      "referenceID" : 55,
      "context" : "5%) Text BNC Baby metaphor TSV (Tsvetkov et al., 2014) 3,334 (50%) Text Web metaphor, affect LCC (Mohler et al.",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 36,
      "context" : ", 2014) 3,334 (50%) Text Web metaphor, affect LCC (Mohler et al., 2016) 16,265 (19%) Text ClueWeb09 metaphor MOH (Mohammad et al.",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 34,
      "context" : ", 2016) 16,265 (19%) Text ClueWeb09 metaphor MOH (Mohammad et al., 2016) 1,639 (25%) Text WordNet metaphor Zayed’s Tweets (Zayed et al.",
      "startOffset" : 49,
      "endOffset" : 72
    }, {
      "referenceID" : 66,
      "context" : ", 2016) 1,639 (25%) Text WordNet metaphor Zayed’s Tweets (Zayed et al., 2019) 2,500 (54%) Text Twitter metaphor Visual Met (Steen, 2018) 353 (100%) Image Adv, Arts, Cartoons metaphor Shutova et al.",
      "startOffset" : 57,
      "endOffset" : 77
    }, {
      "referenceID" : 51,
      "context" : ", 2019) 2,500 (54%) Text Twitter metaphor Visual Met (Steen, 2018) 353 (100%) Image Adv, Arts, Cartoons metaphor Shutova et al.",
      "startOffset" : 53,
      "endOffset" : 66
    }, {
      "referenceID" : 32,
      "context" : "Early metaphor studies have focused on hand-constructed knowledge and machine learning techniques (Mason, 2004; Turney et al., 2011; Tsvetkov et al., 2014; Hovy et al., 2013).",
      "startOffset" : 98,
      "endOffset" : 174
    }, {
      "referenceID" : 56,
      "context" : "Early metaphor studies have focused on hand-constructed knowledge and machine learning techniques (Mason, 2004; Turney et al., 2011; Tsvetkov et al., 2014; Hovy et al., 2013).",
      "startOffset" : 98,
      "endOffset" : 174
    }, {
      "referenceID" : 55,
      "context" : "Early metaphor studies have focused on hand-constructed knowledge and machine learning techniques (Mason, 2004; Turney et al., 2011; Tsvetkov et al., 2014; Hovy et al., 2013).",
      "startOffset" : 98,
      "endOffset" : 174
    }, {
      "referenceID" : 19,
      "context" : "Early metaphor studies have focused on hand-constructed knowledge and machine learning techniques (Mason, 2004; Turney et al., 2011; Tsvetkov et al., 2014; Hovy et al., 2013).",
      "startOffset" : 98,
      "endOffset" : 174
    }, {
      "referenceID" : 46,
      "context" : "Others have also used distributional clustering (Shutova et al., 2013) and unsupervised approaches (Shutova et al.",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 44,
      "context" : ", 2013) and unsupervised approaches (Shutova et al., 2017; Mao et al., 2018).",
      "startOffset" : 36,
      "endOffset" : 76
    }, {
      "referenceID" : 31,
      "context" : ", 2013) and unsupervised approaches (Shutova et al., 2017; Mao et al., 2018).",
      "startOffset" : 36,
      "endOffset" : 76
    }, {
      "referenceID" : 48,
      "context" : "Metaphorical messages abound in advertisements , which offer a natural and rich resource of data on metaphor and how textual and visual factors combine and interact (Sobrino, 2017; Forceville et al., 2017).",
      "startOffset" : 165,
      "endOffset" : 205
    }, {
      "referenceID" : 64,
      "context" : "potential metaphorical samples of advertising from a large, publicly released dataset of 64,832 image advertisements that contain both images and inside text (Ye et al., 2019).",
      "startOffset" : 158,
      "endOffset" : 175
    }, {
      "referenceID" : 29,
      "context" : "For text data, we removed external links and mentions (@username); we removed non-English text using the LANGID (Lui and Baldwin, 2012) library to label each piece of data with a language tag; we removed strange symbols such as emojis; we removed “metaphor” or “metaphoric” when",
      "startOffset" : 112,
      "endOffset" : 135
    }, {
      "referenceID" : 18,
      "context" : "There are a variety of ways in which texts and images are combined in multimodal content (Hendricks et al., 2016; Chen et al., 2017).",
      "startOffset" : 89,
      "endOffset" : 132
    }, {
      "referenceID" : 3,
      "context" : "There are a variety of ways in which texts and images are combined in multimodal content (Hendricks et al., 2016; Chen et al., 2017).",
      "startOffset" : 89,
      "endOffset" : 132
    }, {
      "referenceID" : 14,
      "context" : "“Perceptual resemblance” is a major means of triggering a metaphorical relation between two different entities (Forceville and Urios-Aparisi, 2009).",
      "startOffset" : 111,
      "endOffset" : 147
    }, {
      "referenceID" : 57,
      "context" : "Moreover, in modern times, the increasing ubiquity of multimodal metaphors means that people cannot ignore its power of persuasion (Urios-Aparisi, 2009).",
      "startOffset" : 131,
      "endOffset" : 152
    }, {
      "referenceID" : 4,
      "context" : "In addition, inspired by a variety of arousing, humorous, or aesthetic effects of metaphors (Christmann et al., 2011), the expres-",
      "startOffset" : 92,
      "endOffset" : 117
    }, {
      "referenceID" : 16,
      "context" : "Numerous studies show that metaphorical language frequently expresses sentiments or emotions implicitly (Goatly, 2007; Kövecses, 1995, 2003).",
      "startOffset" : 104,
      "endOffset" : 140
    }, {
      "referenceID" : 5,
      "context" : "Compared to literal expressions, metaphors elicit more emotional activation of the human brain in the same context (Citron and Goldberg, 2014).",
      "startOffset" : 115,
      "endOffset" : 142
    }, {
      "referenceID" : 11,
      "context" : "The kappa score, κ, was used to measure inter-annotator agreements (Fleiss, 1971).",
      "startOffset" : 67,
      "endOffset" : 81
    }, {
      "referenceID" : 34,
      "context" : "findings accord with the results of previous studies on monomodal textual metaphors that metaphors convey more sentiments or emotions than literary text (Mohammad et al., 2016).",
      "startOffset" : 153,
      "endOffset" : 176
    }, {
      "referenceID" : 8,
      "context" : "This paper used two different methods to encode the text, namely the pre-trained Bidirectional Encoder Representations from Transformers (BERT) model (Devlin et al., 2019) and Bidirectional Long-Short Term Memory (Bi-LSTM) networks (Medsker and Jain, 2001).",
      "startOffset" : 150,
      "endOffset" : 171
    }, {
      "referenceID" : 33,
      "context" : ", 2019) and Bidirectional Long-Short Term Memory (Bi-LSTM) networks (Medsker and Jain, 2001).",
      "startOffset" : 68,
      "endOffset" : 92
    }, {
      "referenceID" : 17,
      "context" : "man, 2014), ResNet50 (He et al., 2016), and EfficientNet (Tan and Le, 2019).",
      "startOffset" : 21,
      "endOffset" : 38
    }, {
      "referenceID" : 52,
      "context" : "image feature vector, we used four different feature fusion methods to combine the vectors, namely concatenation (Suryawanshi et al., 2020), elementwise multiply (Mai et al.",
      "startOffset" : 113,
      "endOffset" : 139
    }, {
      "referenceID" : 30,
      "context" : ", 2020), elementwise multiply (Mai et al., 2020), element-wise add (Cai et al.",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : ", 2020), element-wise add (Cai et al., 2019), and maximum (Das, 2019).",
      "startOffset" : 26,
      "endOffset" : 44
    }, {
      "referenceID" : 37,
      "context" : "We used Pytorch (Paszke et al., 2019) to build the model.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 23,
      "context" : "We used the Adam optimizer (Kingma and Ba, 2014) to optimize the loss function, and the training method of gradient clipping (Zhang et al.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 67,
      "context" : "We used the Adam optimizer (Kingma and Ba, 2014) to optimize the loss function, and the training method of gradient clipping (Zhang et al., 2019) to avoid gradient explosion.",
      "startOffset" : 125,
      "endOffset" : 145
    } ],
    "year" : 2021,
    "abstractText" : "Metaphor involves not only a linguistic phenomenon, but also a cognitive phenomenon structuring human thought, which makes understanding it challenging. As a means of cognition, metaphor is rendered by more than texts alone, and multimodal information in which vision/audio content is integrated with the text can play an important role in expressing and understanding metaphor. However, previous metaphor processing and understanding has focused on texts, partly due to the unavailability of large-scale datasets with ground truth labels of multimodal metaphor. In this paper, we introduce MultiMET, a novel multimodal metaphor dataset to facilitate understanding metaphorical information from multimodal text and image. It contains 10,437 text-image pairs from a range of sources with multimodal annotations of the occurrence of metaphors, domain relations, sentiments metaphors convey, and author intents. MultiMET opens the door to automatic metaphor understanding by investigating multimodal cues and their interplay. Moreover, we propose a range of strong baselines and show the importance of combining multimodal cues for metaphor understanding. MultiMET will be released publicly for research.",
    "creator" : "LaTeX with hyperref"
  }
}