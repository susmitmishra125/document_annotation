{
  "name" : "2021.acl-long.472.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "BASS: Boosting Abstractive Summarization with Unified Semantic Graph",
    "authors" : [ "Wenhao Wu", "Wei Li", "Xinyan Xiao", "Jiachen Liu", "Ziqiang", "Sujian Li", "Hua Wu", "Haifeng Wang" ],
    "emails" : [ "waynewu@pku.edu.cn", "lisujian@pku.edu.cn", "hua@baidu.com", "wanghaifeng@baidu.com", "zqcao@suda.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6052–6067\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6052"
    }, {
      "heading" : "1 Introduction",
      "text" : "Nowadays, the sequence-to-sequence (Seq2Seq) based summarization models have gained unprecedented popularity (Rush et al., 2015; See et al., 2017; Lewis et al., 2020). However, complex summarization scenarios such as long-document or multi-document summarization (MDS), still bring great challenges to Seq2Seq models (Cohan et al., 2018; Liu et al., 2018). In a long document numerous details and salient content may distribute evenly (Sharma et al., 2019) while multiple documents may contain repeated, redundant or contradictory information (Radev, 2000). These problems make Seq2Seq models struggle with content selection and organization which mainly depend\n∗Work is done during an internship at Baidu Inc. †Corresponding author.\non the long source sequence (Shao et al., 2017). Thus, how to exploit deep semantic structure in the complex text input is a key to further promote summarization performance.\nCompared with sequence, graph can aggregate relevant disjoint context by uniformly representing them as nodes and their relations as edges. This greatly benefits global structure learning and longdistance relation modeling. Several previous works have attempted to leverage sentence-relation graph to improve long sequence summarization, where nodes are sentences and edges are similarity or dis-\ncourse relations between sentences (Li et al., 2020). However, the sentence-relation graph is not flexible for fine-grained (such as entities) information aggregation and relation modeling. Some other works also proposed to construct local knowledge graph by OpenIE to improve Seq2Seq models (Fan et al., 2019; Huang et al., 2020). However, the OpenIE-based graph only contains sparse relations between partially extracted phrases, which cannot reflect the global structure and rich relations of the overall sequence.\nFor better modeling the long-distance relations and global structure of a long sequence, we propose to apply a phrase-level unified semantic graph to facilitate content selection and organization. Based on fine-grained phrases extracted from dependency parsing, our graph is suitable for information aggregation with the help of coreference resolution that substantially compresses the input and benefits content selection. Furthermore, relations between phrases play an important role in organizing the salient content when generating summaries. For example, in Figure 1 the phrases “Albert Einstein”, “the great prize” and “explanation of the of the photoelectric” which distribute in different sentences are easily aggregated through their semantic relations to compose the final summary sentence.\nWe further propose a graph-based encoderdecoder model based on the unified semantic graph. The graph-encoder effectively encodes long sequences by explicitly modeling the relations between phrases and capturing the global structure based on the semantic graph. Besides, several graph augmentation methods are also applied during graph encoding to tap the potential semantic relations. For the decoding procedure, the graph decoder incorporates the graph structure by graph propagate attention to guide the summary generation process, which can help select salient content and organize them into a coherent summary.\nWe conduct extensive experiments on both the long-document summarization dataset BIGPATENT and MDS dataset WikiSUM to validate the effectiveness of our model. Experiment results demonstrate that our graph-based model significantly improves the performance of both longdocument and multi-document summarization over several strong baselines. Our main contributions are summarized as follows:\n• We present the unified semantic graph which aggregates co-referent phrases distributed\nin context for better modeling the longdistance relations and global structure in longdocument summarization and MDS.\n• We propose a graph-based encoder-decoder model to improve both the document representation and summary generation process of the Seq2Seq architecture by leveraging the graph structure.\n• Automatic and human evaluation on both long-document summarization and MDS outperform several strong baselines and validate the effectiveness of our graph-based model."
    }, {
      "heading" : "2 Related Works",
      "text" : ""
    }, {
      "heading" : "2.1 Abstractive Summarization",
      "text" : "Abstractive summarization aims to generate a fluent and concise summary for the given input document (Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Bražinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al., 2020). Their works also emphasize the importance of modeling cross-document relations in MDS."
    }, {
      "heading" : "2.2 Structure Enhanced Summarization",
      "text" : "Explicit structures play an important role in recent deep learning-based extractive and abstractive summarization methods (Li et al., 2018a,b; Liu et al., 2019a). Different structures benefit summarization models from different aspects. Constituency parsing greatly benefits content selection\nand compression for extractive models. Cao et al. (2015) propose to extract salient sentences based on their constituency parsing trees. Xu and Durrett (2019) and Desai et al. (2020) jointly select and compress salient content based on syntax structure and syntax rules. Dependency parsing helps summarization models in semantic understanding. Jin et al. (2020) incorporate semantic dependency graphs of input sentences to help the summarization models generate sentences with better semantic relevance . Besides sentence-level structures, document-level structures also attract a lot of attention. Fernandes et al. (2019) build a simple graph consisting of sentences, tokens and POS for summary generation. By incorporating RST trees, Xu et al. (2020) propose a discourse-aware model to extract sentences. Similarly, structures from semantic analysis also help. Liu et al. (2015) and Liao et al. (2018) propose to guide summarization with Abstract Meaning Representation (AMR) for a better comprehension of the input context. (Li and Zhuge, 2019) propose semantic link networks based MDS but without graph neural networks. Recently, the local knowledge graph by OpenIE attracts great attention. Leveraging OpenIE extracted tuples, Fan et al. (2019) compress and reduce redundancy in multi-document inputs in MDS. Their work mainly focus on the efficiency in processing long sequences. Huang et al. (2020) utilize OpenIEbased graph for boosting the faithfulness of the generated summaries. Compared with their work, our phrase-level semantic graph focus on modeling long-distance relations and semantic structures."
    }, {
      "heading" : "3 Unified Semantic Graph",
      "text" : "In this section, we introduce the definition and construction of the unified semantic graph."
    }, {
      "heading" : "3.1 Graph Definition",
      "text" : "The unified semantic graph is a heterogeneous graph defined as G = (V,E), where V and E are the set of nodes and edges. Every node in V represents a concept merged from co-referent phrases.\nFor example, in Figure 1 the node “Albert Einstein” is merged from phases “Albert Einstein” and “his” which indicate the same person by coreference resolution. Defined as a heterogeneous graphG, every node v ∈ V and every edge eij ∈ E in our graph belongs to a type of phrase and dependency parsing relation, respectively. Determined by the type of phrases merged from, nodes are categorized into three different types: Noun phrase (N), Verb phrase (V), Other phrase (O). We neglect dependency relations in edges as they mainly indicate sentence syntax. Instead, the meta-paths (Sun et al., 2011) in the unified semantic graph convey various semantic relations. Notice that most O such as adjective phrases, adverb phrases function as modifiers, and the meta-path O-N indicates modification relation. The meta-path N-N between Noun phrases represents appositive relation or appositional relation. Furthermore, two-hop meta-path represents more complex semantic relations in graph. For example, N-V-N like [Albert Einstein]-[won]-[the physics Nobel Prize] indicates SVO (subject–verb–object) relation. It is essential to effectively model the two-hop meta-path for complex semantic relation modeling."
    }, {
      "heading" : "3.2 Graph Construction",
      "text" : "To construct the semantic graph, we extract phrases and their relations from sentences by first merging tokens into phrases and then merging co-referent phrases into nodes. We employ CoreNLP (Manning et al., 2014) to obtain coreference chains of the input sequence and the dependency parsing tree of each sentence. Based on the dependency parsing tree, we merge consecutive tokens that form a complete semantic unit into a phrase. Afterwards, we merge the same phrases from different positions and phrases in the same coreference chain to form the nodes in the semantic graph.\nThe final statistics of the unified semantic graph on WikiSUM are illustrated in table 1, which indicates that the scale of the graph expands moderately with the inputs. This also demonstrates how the unified semantic graph compresses long-text information."
    }, {
      "heading" : "4 Summarization Model",
      "text" : "In this section, we introduce our graph-based abstractive summarization model, which mainly consists of a graph encoder and a graph decoder, as shown in Figure 2. In the encoding stage, our\nmodel takes a document or the concatenation of a set of documents as text input (represented as x = {xk}), and encodes it by a text encoder to obtain a sequence of local token representations. The graph encoder further takes the unified semantic graph as graph input (represented as G = (V,E) in section 3.1), and explicitly model the semantic relations in graph to obtain global graph representations. Based on several novel graph-augmentation methods, the graph encoder also effectively taps the implicit semantic relations across the text input. In the decoding stage, the graph decoder leverages the graph structure to guide the summary generation process by a novel graph-propagate attention, which facilitates salient content selection and organization for generating more informative and coherent summaries."
    }, {
      "heading" : "4.1 Text Encoder",
      "text" : "To better represent local features in sequence, we apply the pre-trained language model RoBERTa (Liu et al., 2019b) as our text encoder. As the maximum positional embedding length of RoBERTa is 512, we extend the positional embedding length and randomly initialize the extended part. To be specific, in every layer, the representation of every node is only updated by it’s neighbors by self attention."
    }, {
      "heading" : "4.2 Graph Encoder",
      "text" : "After we obtain token representations by the text encoder, we further model the graph structure to obtain node representations. We initialize node representations in the graph based on token representations and the token-to-node alignment information from graph construction. After initialization, we apply graph encoding layers to model the explicit semantic relations features and additionally apply several graph augmentation methods to learn the implicit structure conveyed by the graph. Node Initialization Similar to graph construction in section 3.2, we initialize graph representations following the two-level merging, token merging and phrase merging. The token merging compresses and abstracts local token features into higher-level phrase representations. The phrase merging aggregates co-referent phrases in a wide context, which captures long-distance and crossdocument relations. To be simple, these two merging steps are implemented by average pooling. Graph Encoding Layer Following previous works in graph-to-sequence learning (KoncelKedziorski et al., 2019; Yao et al., 2020), we apply Transformer layers for graph modeling by applying the graph adjacent matrix as self-attention mask. Graph Augmentation Following previous works (Bastings et al., 2017; Koncel-Kedziorski et al., 2019), we add reverse edges and self-loop edges in graph as the original directed edges are\nnot enough for learning backward information. For better utilizing the properties of the united semantic graph, we further propose two novel graph augmentation methods. Supernode As the graph becomes larger, noises introduced by imperfect graph construction also increase, which may cause disconnected sub-graphs. To strengthen the robustness of graph modeling and learn better global representations, we add a special supernode connected with every other node in the graph to increase the connectivity. Shortcut Edges Indicated by previous works, graph neural networks are weak at modeling multihop relations (Abu-El-Haija et al., 2019). However, as mentioned in section 3.1, the meta-paths of length two represent rich semantic structures that require further modeling the two-hop relations between nodes. As illustrated in Figure 2, in a N-VN meta-path [Albert Einstein]-[was]-[a theoretical physicist], the relations [Albert Einstein]-[was] and [was]-[a theoretical physicist] are obviously less important than the two-hop relation [Albert Einstein]- [a theoretical physicist]. Therefore we add shortcut edges between every node and its twohop relation neighbors, represented as blue edges in Figure 2. We have also attempted other complex methods such as MixHop (Abu-El-Haija et al., 2019), but we find shortcut edges are more efficient and effective. The effectiveness of these graph augmentation methods has also been validated in section 6.2."
    }, {
      "heading" : "4.3 Graph Decoding Layer",
      "text" : "Token and node representations benefit summary generation in different aspects. Token representations are better at capturing local features while graph representations provide global and abstract features. For leveraging both representations, we apply a stack of Transformer-based graph decoding layers as the decoder which attends to both representations and fuse them for generating summaries. Let yl−1t denotes the representation of t-th summary token output by (l − 1)-th graph decoding layer. For the graph attention, we apply multi-head attention using yl−1t as query and node representations V = {vj} as keys and values:\nαt,j = (yl−1t WQ)(vjWK) T\n√ dhead\n(1)\nwhere WQ,WK ∈ Rd×d are parameter weights, αt,j denote the salient score for node j to yl−1t .\nWe then calculate the global graph vector gt as weighted sum over values of nodes: gt =∑\nj Softmax(αt,j)(vjWV ) where WV ∈ Rd×d is a learnable parameter. We also obtain contextualized text vector ct similar to the procedure above by calculating multi-head attention between yl−1t and token representations. Afterwards, we use a graph fusion layer which is a feed-forward neural network to fuse the concatenation of the two features: dlt = W T d ([gt, ct]), where Wd ∈ R2d×d is the linear transformation parameter and dlt is the hybrid representation of tokens and graph. After layer-norm and feed-forward layer, the l-th graph decoding layer output ylt is used as the input of the next layer and also used for generating the tth token in the final layer. Graph-propagate Attention When applying multi-head attention to graph, it only attends to node representations linearly, neglecting the graph structure. Inspired by Klicpera et al. (2019), we propose the graph-propagate attention to leverage the graph structure to guide the summary generation process. By further utilizing semantic structure, the decoder is more efficient in selecting and organizing salient content. Without extra parameters, the graph-propagation attention can be conveniently applied to the conventional multi-head attention for structure-aware learning.\nGraph-propagate attention consists of two steps: salient score prediction and score propagation. In the first step, we predict the salient score for every node linearly. We apply the output of multi-head attention αt ∈ R|v|×C in Equation 1 as salient scores, where |v| is the number of nodes in the graph andC is the number of attention heads. C is regarded as C digits or channels of the salient score for every node. We then make the salient score structureaware through score propagation. Though PageRank can propagate salient scores over the entire graph, it leads to over-smoothed scores, as in every summary decoding step only parts of the content are salient. Therefore, for each node we only propagate its salient score p times in the graph, aggregating at most p-hop relations. Let β0t = αt denotes the initial salient score predicted in previous step, the salient score after p-th propagation is:\nβpt = ωÂβ p−1 t + (1− ω)β0t (2)\nwhere Â = AD−1 is a degree-normalized adjacent matrix of the graph1, and ω ∈ (0, 1] is the teleport\n1Adjacent matrix A contains self-loop and reverse edges.\nprobability which defines the salient score has the probability ω to propagate towards the neighbor nodes and 1− ω to restart from initial. The graphpropagation procedure can also be formulated as:\nβpt = (ω pÂp + (1− ω)( p−1∑ i=0 ωiÂi))αt (3)\nAfter p steps of salient score propagation, the graph vector is then calculated by weighted sum of node values:\ng ′ t = ∑ j Softmax(βpt,j)(vjW V ) (4)\nwhere for the convenience of expression, the concatenation of multi-head is omitted. The output of fusing g ′ t and ct is then applied to generate the tth summary token as mentioned before."
    }, {
      "heading" : "5 Experiment Setup",
      "text" : "In this section, we describe the datasets of our experiments and various implementation details."
    }, {
      "heading" : "5.1 Summarization Datasets",
      "text" : "We evaluate our model on a SDS dataset and an MDS dataset, namely BIGPATENT (Sharma et al., 2019) and WikiSUM (Liu et al., 2018). BIGPATENT is a large-scale patent document summarization dataset with an average input of 3572.8 words and a reference with average length of 116.5 words. BIGPATENT is a highly abstractive summarization dataset with salient content evenly distributed in the input. We follow the standard splits of Sharma et al. (2019) for training, validation, and testing (1,207,222/67,068/67,072). WikiSUM is a large-scale MDS dataset. Following Liu and Lapata (2019a), we treat the generation of lead Wikipedia sections as an MDS task. To be specific, we directly utilize the preprocessed results from Liu and Lapata (2019a), which split source documents into multiple paragraphs and rank the paragraphs based on their titles to select top-40 paragraphs as source input. The average length of each paragraph and the target summary are 70.1 tokens and 139.4 tokens, respectively. We concatenate all the paragraphs as the input sequence. We use the standard splits of Liu and Lapata (2019a) for training, validation, and testing (1,579,360/38,144/38,205)."
    }, {
      "heading" : "5.2 Implementation Details",
      "text" : "We train all the abstractive models by max likelihood estimation with label smoothing (label smoothing factor 0.1). As we fine-tune the pretrained language model RoBERTa as text encoder, we apply two different Adam optimizers (Kingma and Ba, 2015) with β1 = 0.9 and β2 = 0.998 to train the pre-trained part and other parts of the model (Liu and Lapata, 2019b). The learning rate and warmup steps are 2e-3 and 20,000 for the pretrained part and 0.1 and 10,000 for other parts. As noticed from experiments, when the learning rate is high, graph-based models suffer from unstable training caused by the gradient explosion in the text encoder. Gradient clipping with a very small maximum gradient norm (0.2 in our work) solves this problem. All the models are trained for 300,000 steps on BIGPATENT and WikiSUM with 8 GPUs (NVIDIA Tesla V100). We apply dropout (with the probability of 0.1) before all linear layers. In our model, the number of graph-encoder layers and graph-decoder layers are set as 2 and 6, respectively. The hidden size of both graph encoding and graph decoding layers is 768 in alignment with RoBERTa, and the feed-forward size is 2048 for parameter efficiency. For graph-propagation attention, the parameter ω is 0.9, and the propagation steps p is 2. During decoding, we apply beam search with beam size 5 and length penalty with factor 0.9. Trigram blocking is used to reduce repetitions."
    }, {
      "heading" : "6 Results",
      "text" : ""
    }, {
      "heading" : "6.1 Automatic Evaluation",
      "text" : "We evaluate the quality of generated summaries using ROUGE F1(Lin, 2004) and BERTScore (Zhang\net al., 2020b). For ROUGE, we report unigram and bigram overlap between system summaries and reference summaries (ROUGE-1, ROUGE-2). We report sentence-level ROUGE-L for the BIGPATENT dataset and summary-level ROUGE-L for the WikiSUM for a fair comparison with previous works. We also report BERTScore 2 F1, a better metric at evaluating semantic similarity between system summaries and reference summaries. Results on MDS Table 2 summarizes the evaluation results on the WikiSUM dataset. We compare our model with several strong abstractive and extractive baselines. As listed in the top block, Lead and LexRank (Erkan and Radev, 2004) are two classic extractive methods. The second block shows the results of several different abstractive methods. TransS2S is the Transformer-based encoderdecoder model. By replacing the Transformer encoder in TransS2S with BERT (Devlin et al., 2019) or RoBERTa and training with two optimizers (Liu and Lapata, 2019b), we obtain two strong baselines BERTS2S and RoBERTaS2S. T-DMCA is the best model presented by Liu et al. (2018) for summarizing long sequence. HT is the best model presented by Liu and Lapata (2019a) with the hierarchical Transformer encoder and a flat Transformer decoder. GraphSum, presented by Li et al. (2020), leverages paragraph-level explicit graph by the graph encoder and decoder, which gives the current best performance on WikiSUM. We report the\n2We apply roberta-large L17 no-idf version as the metric model and rescale with baseline setting according to suggestions on https://github.com/Tiiiger/bert score.\nbest results of GraphSum with RoBERTa and the input length is about 2400 tokens. The last block reports the results of our model BASS with the input lengths of 2400 and 3000. Compared with all the baselines, our model BASS achieves great improvements on all the four metrics. The results demonstrates the effectiveness of our phrase-level semantic graph comparing with other RoBERTa based models, RoBERTaS2S (without graph) and GraphSum (sentence-relation graph). Furthermore, the phrase-level semantic graph improves the semantic relevance of the generated summaries and references, as the BERTScore improvements of BASS is obvious. Results on SDS Table 3 shows our experiment results along with other SDS baselines. Similar to WikiSUM, we also report LexRank, TransS2S, and RoBERTaS2S. Besides, we report the performance of several other baselines. ORACLE is the upper-bound of current extrative models. Seq2seq is based on LSTM encoder-decoder with attention mechanism (Bahdanau et al., 2015). Pointer and Pointer+cov are pointer-generation (See et al., 2017) with and without coverage mechanism, respectively. FastAbs (Chen and Bansal, 2018) is an abstractive method by jointly training sentence extraction and compression. TLM (Pilault et al., 2020) is a recent long-document summarization method based on language model. We also report the performances of recent pretrianing-based SOTA\ntext generation models BART (large) and Peaguasus (base) on BIGPATENT, which both contain a parameter size of 406M . The last block shows the results of our model, which contains a parameter size of 201M . The results show that BASS consistently outperforms RoBERTaS2S, and comparable with current large SOTA models with only half of the parameter size. This further demonstrates the effectiveness of our graph-augmented model on long-document summarization."
    }, {
      "heading" : "6.2 Model Analysis",
      "text" : "For a thorough understanding of BASS, we conduct several experiments on the WikiSUM test set, including the effects of the graph structure and input length. We also validate the effectiveness of the graph-augmentation methods in graph encoder and the graph-propagation attention in graph decoder by ablation studies. Graph Structure Analysis To analyze how the unified semantic graph benefits summarization learning, we conduct ablation studies on the graph structures. Illustrated in Table 4, after removing explicit relations between phrases by fully connecting all the nodes, the R-1 metric drops obviously which indicates the relations between phrases improve the informativeness of generated summaries. After further removing phrase merging, we observe a performance decrease in all the metrics, which indicates the long-distance relations benefit both the informativeness and fluency of summary. Ablation Study The experimental results of removing supernode and shortcut edges from the unified semantic graph prove the effectiveness of graph augmentation methods in the graph encoder. Experimental results without the gaph-propagation attention confirms that the structure of the unified semantic graph is also beneficial for decoding. Overall, the performance of the model drops the most when removing shortcut edges which indicates the rich potential information is beneficial for summarization. Finally, after removing all the graph-relevant components, performance dramatically drops on all the metrics. Length Comparison According to Liu et al. (2018), input length affects the summarization performance seriously for Seq2Seq models as most of them are not efficient at handling longer sequences. The basic TransS2S achieves its best performance at the input length of 800, while longer input hurts performance. Several previous models achieve bet-\nter performance when utilizing longer sequences. As illustrated in Figure 3, the performance of HT remains stable when the input length is longer than 800. Leveraging the power of sentence-level graph, GraphSum achieves the best performance at 2,400 but its performance begins to decrease when the input length reaches 3000. Unlike previous methods, ROUGE-1 of BASS significantly increased in 3000 indicates that the unified semantic graph benefits salient information selection even though the input length is extreme. Abastractiveness Analysis We also study the abstractiveness of BASS and other summarization systems on WikiSUM. We calculate the average novel n-grams to the source input, which reflects the abstractiveness of a summarization system (See et al., 2017). Illustrated in Figure 4, BASS generates more abstract summaries comparing to recent models, GraphSum, HT, and weaker than RoBERTaS2S. Summarized from observation, we draw to a conclusion that RoBERTaS2S usually generates context irrelevant contents due to the strong pretrained RoBERTa encoder but a randomly initialized decoder that relays on the long-text input poorly. Graph-based decoders of BASS and GraphSum alleviate this phenomenon."
    }, {
      "heading" : "6.3 Human Evaluation",
      "text" : "In addition to the above automatic evaluations, we also conduct human evaluations to assess the performance of systems. Because the patent dataset BIGPATENT contains lots of terminologies and requires professional background knowledge for annotators, we select WikiSUM as the dataset for evaluations. As Wikipedia entries can be summarized in many different aspects, annotators will naturally favor systems with longer outputs. Thus we first filter instances that the summaries of different systems are significantly different in lengths\nand then randomly select 100 instances. We invite 2 annotators to assess the summaries of different models independently.\nAnnotators evaluate the overall quality of summaries by ranking them taking into account the following criterias: (1) Informativeness: whether the summary conveys important and faithful facts of the input? (2) Fluency: whether the summary is fluent, grammatical, and coherent? (3) Succinctness: whether the summary is concise and dose not describe too many details? Summaries with the same quality get the same order. All systems get score 2,1,-1,2 for ranking 1,2,3,4 respectively. The rating of each system is averaged by the scores of all test instances.\nThe results of our system and the other three strong baselines are shown in Table 6. The percentage of rankings and the overall scores are both reported. Summarized from the results, our model BASS is able to generate higher quality summaries. Some examples are also shown in the appendix. Specifically, BASS generates fluent and concise summaries containing more salient content compared with other systems. The human evaluation results further validate the effectiveness of our semantic graph-based model."
    }, {
      "heading" : "7 Conclusion and Future Work",
      "text" : "In this paper, we propose to leverage the unified semantic graph to improve the performance of neural abstractive models for long-document summarization and MDS. We further present a graph-based encoder-decoder model to improve both the document representation and summary generation process by leveraging the graph structure. Experiments\non both long-document summarization and MDS show that our model outperforms several strong baselines, which demonstrates the effectiveness of our graph-based model and the superiority of the unified semantic graph for long-input abstractive summarization. Though remarkable achievements have been made by neural network-based summarization systems, they still do not actually understand languages and semantics. Incorporating language structures in deep neural networks as prior knowledge is a straightforward and effective way to help summarization systems, as proved by this work and previous works."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was partially supported by National Key R&D Program of China (No. 2020YFB1406701) and National Natural Science Foundation of China (No. 61876009)."
    }, {
      "heading" : "A Graph Construction",
      "text" : "Given a document set with n documents D = {d1, ...dn} and each document di ∈ D contains ki sentences. Algorithm 1 gives the details of constructing the unified semantic graph based on dependency parsing.\nWe apply CoreNLP for both coreference resolution and dependency parsing. We first extract coreference chains from every document and merge coreference chains with overlap phrases. We memorize all the coreference chains in set C, where each chain c = {p1, ..., pkc} ∈ C contains a set of co-referent phrases. We then parse every sentence in every document into a dependency parsing tree Ts. Afterwords we refines the tree by following\nAlgorithm 1: Construct Unified Semantic Graphs\nInput: Documents set D = {d1, ..., dn}, document di ∈ D, di = {s1, ..., ski}\nOutput: The unified semantic graph G 1 . Coreference Resolution 2 C ← ∅ 3 foreach d ∈ D do 4 cd ← COREFERNCE RESOLUSION(d) 5 C ← COREFERNCE MERGE(C, cd) 6 end 7 . Dependency Parsing 8 T ← ∅ 9 foreach d ∈ D do\n10 foreach s ∈ d do 11 Ts ← DEPENDENCY PARSE(s) 12 Ts ← IDENTIFY NODE TYPES(Ts) 13 Ts ← REMOVE PUNCTUATION(Ts) 14 Ts ←MERGE COREF PHRASE(Ts, C) 15 Ts ←MERGE NODES(Ts) 16 T ← T ⋃ {Ts} 17 end 18 end 19 . Initialize Graph 20 G = (V, E),V ← ∅, E ← ∅ 21 foreach tree T = (VT , ET ) ∈ T do 22 V ← V ⋃ {VT }\n23 E ← E ⋃ {ET } 24 end 25 . Merge Co-referent Nodes 26 foreach corefernce chain c ∈ C do 27 (V, E)←MERGE PHRASE(c,V, E) 28 end 29 G ← (V, E) 30 return G\noperations:\n• IDENTIFY NODE TYPES: after dependency parsing, each node in the tree is attached with a POS tag. We associate every node with its POS tag for future merging operations.\n• PRUNE PUNCTUATION: we remove all the punctuation nodes and their edges.\n• MERGE COREFE PHRASE: since a coreference chain contains a set of phrases but a dependency parsing tree is based on tokens, we first obtain phrases in coreference chains for the future convenience in merging coreferent phrases. For every phrase pi in a co-reference chain c, we merge the corresponding tokens of pi to form the target phrase pi in the tree. The merging operation is carried out by removing edges between the nodes and represent the tokens as a unified node.\n• NODE MERGE: after obtaining phrases in coreference chains, we merge other token nodes into concise phrases. This procedure is carried out by traveling every dependency graph in depth-first, and merge the tokens into a phrase if they satisfy the merging conditions. Overall, we merge consecutive tokens that form a complete semantic unit into a phrases.\nAfter we extract all the phrases, we merge all the same phrases and phrases in the same coreference chain by MERGE PHRASE and return the final semantic graph."
    }, {
      "heading" : "B Case Study",
      "text" : "We select several cases from human evaluation and demonstrate them to show the overall quality of systems. In each table, there are four blocks present the input article (Article), the reference summary (Reference Summary), the output summary of a strong baseline GraphSum (Baseline) and the output summary of our model BASS (BASS), separately. The original input article is the concatenation of several document paragraphs by the “||” symbol containing 1600 tokens in maximum. We only show the salient part of the input article due to the paragraph constraints. Spans in highlight indicate the salient contents. Spans in red indicate the unfaithful content, irrelevant content or repeats a system generated. The case in Table 7 describes an American ice hockey player “Colleen Coyne”. The important fact, “won a gold medal at the 1998 winter Olympics”, is well captured by BASS, however, the baseline model only mentions she “was a member” neglecting the substantial achievement. The case in Table 8 introduces the play “Colleen Coyne” which based on the four\nnovels of “Leonardo Padura” is difficult to summarize, as the relation between “Colleen Coyne”, “Leonardo Padura” and the name list of the four novels cross different documents and a long-span. The baseline model confuses with the name of stars and fails to list the names of four books. The Reference Summary in Table 9 is not informative enough to give a precise description of what is “Cetacean Intelligence”. Though BASS does not introduce the definition of “Cetacean”, it clearly describes the categories of “Cetacean Intelligence” which is more essential to the topic. In Table 10, BASS and Baseline generate summaries with similar content, but BASS provides more details such as, “righthanded”, distributed in different documents. In the case describing Broadcast, in Table 11, while the Baseline generates irrelevant titles of editors, BASS describes essential characters of the magazine. Though all the models apply trigram-block to avoid repeats, Table 12 shows that sometimes the Baseline still generates repeated n-grams while this seldomly happens on BASS."
    } ],
    "references" : [ {
      "title" : "MixHop: Higher-order graph convolutional architectures via sparsified neighborhood mixing",
      "author" : [ "Sami Abu-El-Haija", "Bryan Perozzi", "Amol Kapoor", "Nazanin Alipourfard", "Kristina Lerman", "Hrayr Harutyunyan", "Greg Ver Steeg", "Aram Galstyan." ],
      "venue" : "Pro-",
      "citeRegEx" : "Abu.El.Haija et al\\.,? 2019",
      "shortCiteRegEx" : "Abu.El.Haija et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised opinion summarization with noising and denoising",
      "author" : [ "Reinald Kim Amplayo", "Mirella Lapata." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1934–1945, Online. Association for Computa-",
      "citeRegEx" : "Amplayo and Lapata.,? 2020",
      "shortCiteRegEx" : "Amplayo and Lapata.",
      "year" : 2020
    }, {
      "title" : "Neural machine translation by jointly",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Graph convolutional encoders for syntax-aware neural machine translation",
      "author" : [ "Jasmijn Bastings", "Ivan Titov", "Wilker Aziz", "Diego Marcheggiani", "Khalil Sima’an" ],
      "venue" : "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Bastings et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Bastings et al\\.",
      "year" : 2017
    }, {
      "title" : "Unsupervised opinion summarization as copycat-review generation",
      "author" : [ "Arthur Bražinskas", "Mirella Lapata", "Ivan Titov." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5151–5169, Online. As-",
      "citeRegEx" : "Bražinskas et al\\.,? 2020",
      "shortCiteRegEx" : "Bražinskas et al\\.",
      "year" : 2020
    }, {
      "title" : "Ranking with recursive neural networks and its application to multi-document summarization",
      "author" : [ "Ziqiang Cao", "Furu Wei", "Li Dong", "Sujian Li", "Ming Zhou." ],
      "venue" : "Proceedings of the TwentyNinth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Cao et al\\.,? 2015",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep communicating agents for abstractive summarization",
      "author" : [ "Asli Celikyilmaz", "Antoine Bosselut", "Xiaodong He", "Yejin Choi." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Celikyilmaz et al\\.,? 2018",
      "shortCiteRegEx" : "Celikyilmaz et al\\.",
      "year" : 2018
    }, {
      "title" : "Fast abstractive summarization with reinforce-selected sentence rewriting",
      "author" : [ "Yen-Chun Chen", "Mohit Bansal." ],
      "venue" : "CoRR, abs/1805.11080.",
      "citeRegEx" : "Chen and Bansal.,? 2018",
      "shortCiteRegEx" : "Chen and Bansal.",
      "year" : 2018
    }, {
      "title" : "Multi-document summarization with determinantal point processes and contextualized representations",
      "author" : [ "Sangwoo Cho", "Chen Li", "Dong Yu", "Hassan Foroosh", "Fei Liu." ],
      "venue" : "Proceedings of the 2nd Workshop on New Frontiers in Summarization,",
      "citeRegEx" : "Cho et al\\.,? 2019",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2019
    }, {
      "title" : "Meansum: A neural model for unsupervised multi-document abstractive summarization",
      "author" : [ "Eric Chu", "Peter J. Liu." ],
      "venue" : "Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA,",
      "citeRegEx" : "Chu and Liu.,? 2019",
      "shortCiteRegEx" : "Chu and Liu.",
      "year" : 2019
    }, {
      "title" : "A discourse-aware attention model for abstractive summarization of long documents",
      "author" : [ "Arman Cohan", "Franck Dernoncourt", "Doo Soon Kim", "Trung Bui", "Seokhwan Kim", "Walter Chang", "Nazli Goharian." ],
      "venue" : "Proceedings of the 2018 Conference of",
      "citeRegEx" : "Cohan et al\\.,? 2018",
      "shortCiteRegEx" : "Cohan et al\\.",
      "year" : 2018
    }, {
      "title" : "Compressive summarization with plausibility and salience modeling",
      "author" : [ "Shrey Desai", "Jiacheng Xu", "Greg Durrett." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6259–6274, Online. As-",
      "citeRegEx" : "Desai et al\\.,? 2020",
      "shortCiteRegEx" : "Desai et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Unified language model pre-training for natural language understanding and generation",
      "author" : [ "Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon." ],
      "venue" : "Advances in Neural Infor-",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Lexrank: Graph-based lexical centrality as salience in text summarization",
      "author" : [ "Günes Erkan", "Dragomir R Radev." ],
      "venue" : "Journal of artificial intelligence research, 22:457–479.",
      "citeRegEx" : "Erkan and Radev.,? 2004",
      "shortCiteRegEx" : "Erkan and Radev.",
      "year" : 2004
    }, {
      "title" : "Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model",
      "author" : [ "Alexander Fabbri", "Irene Li", "Tianwei She", "Suyi Li", "Dragomir Radev." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Fabbri et al\\.,? 2019",
      "shortCiteRegEx" : "Fabbri et al\\.",
      "year" : 2019
    }, {
      "title" : "Using local knowledge graph construction to scale Seq2Seq models to multidocument inputs",
      "author" : [ "Angela Fan", "Claire Gardent", "Chloé Braud", "Antoine Bordes." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Fan et al\\.,? 2019",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2019
    }, {
      "title" : "Structured neural summarization",
      "author" : [ "Patrick Fernandes", "Miltiadis Allamanis", "Marc Brockschmidt." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
      "citeRegEx" : "Fernandes et al\\.,? 2019",
      "shortCiteRegEx" : "Fernandes et al\\.",
      "year" : 2019
    }, {
      "title" : "Bottom-up abstractive summarization",
      "author" : [ "Sebastian Gehrmann", "Yuntian Deng", "Alexander Rush." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4098–4109, Brussels, Belgium. Association",
      "citeRegEx" : "Gehrmann et al\\.,? 2018",
      "shortCiteRegEx" : "Gehrmann et al\\.",
      "year" : 2018
    }, {
      "title" : "Knowledge graph-augmented abstractive summarization with semantic-driven cloze reward",
      "author" : [ "Luyang Huang", "Lingfei Wu", "Lu Wang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5094–",
      "citeRegEx" : "Huang et al\\.,? 2020",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Semsum: Semantic dependency guided neural abstractive summarization",
      "author" : [ "Hanqi Jin", "Tianming Wang", "Xiaojun Wan." ],
      "venue" : "AAAI, pages 8026– 8033.",
      "citeRegEx" : "Jin et al\\.,? 2020",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Predict then propagate: Graph neural networks meet personalized pagerank",
      "author" : [ "Johannes Klicpera", "Aleksandar Bojchevski", "Stephan Günnemann." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May",
      "citeRegEx" : "Klicpera et al\\.,? 2019",
      "shortCiteRegEx" : "Klicpera et al\\.",
      "year" : 2019
    }, {
      "title" : "Text Generation from Knowledge Graphs with Graph Transformers",
      "author" : [ "Rik Koncel-Kedziorski", "Dhanush Bekal", "Yi Luan", "Mirella Lapata", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the",
      "citeRegEx" : "Koncel.Kedziorski et al\\.,? 2019",
      "shortCiteRegEx" : "Koncel.Kedziorski et al\\.",
      "year" : 2019
    }, {
      "title" : "Adapting the neural encoder-decoder framework from single to multi-document summarization",
      "author" : [ "Logan Lebanoff", "Kaiqiang Song", "Fei Liu." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Lebanoff et al\\.,? 2018",
      "shortCiteRegEx" : "Lebanoff et al\\.",
      "year" : 2018
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Leveraging graph to improve abstractive multi-document summarization",
      "author" : [ "Wei Li", "Xinyan Xiao", "Jiachen Liu", "Hua Wu", "Haifeng Wang", "Junping Du." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving neural abstractive document summarization with explicit information selection modeling",
      "author" : [ "Wei Li", "Xinyan Xiao", "Yajuan Lyu", "Yuanzhuo Wang." ],
      "venue" : "Proceedings of the 2018 Conference on",
      "citeRegEx" : "Li et al\\.,? 2018a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving neural abstractive document summarization with structural regularization",
      "author" : [ "Wei Li", "Xinyan Xiao", "Yajuan Lyu", "Yuanzhuo Wang." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Bel-",
      "citeRegEx" : "Li et al\\.,? 2018b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Abstractive multidocument summarization based on semantic link network",
      "author" : [ "Wei Li", "Hai Zhuge." ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering.",
      "citeRegEx" : "Li and Zhuge.,? 2019",
      "shortCiteRegEx" : "Li and Zhuge.",
      "year" : 2019
    }, {
      "title" : "Abstract Meaning Representation for multi-document summarization",
      "author" : [ "Kexin Liao", "Logan Lebanoff", "Fei Liu." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 1178–1190, Santa Fe, New Mexico, USA. As-",
      "citeRegEx" : "Liao et al\\.,? 2018",
      "shortCiteRegEx" : "Liao et al\\.",
      "year" : 2018
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text summarization branches out, pages 74–81.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Toward abstractive summarization using semantic representations",
      "author" : [ "Fei Liu", "Jeffrey Flanigan", "Sam Thomson", "Norman Sadeh", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Liu et al\\.,? 2015",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Generating wikipedia by summarizing long sequences",
      "author" : [ "Peter J. Liu", "Mohammad Saleh", "Etienne Pot", "Ben Goodrich", "Ryan Sepassi", "Lukasz Kaiser", "Noam Shazeer." ],
      "venue" : "6th International Conference on Learning Representations, ICLR 2018, Vancou-",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Hierarchical transformers for multi-document summarization",
      "author" : [ "Yang Liu", "Mirella Lapata." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5070–5081, Florence, Italy. Association for Compu-",
      "citeRegEx" : "Liu and Lapata.,? 2019a",
      "shortCiteRegEx" : "Liu and Lapata.",
      "year" : 2019
    }, {
      "title" : "Text summarization with pretrained encoders",
      "author" : [ "Yang Liu", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
      "citeRegEx" : "Liu and Lapata.,? 2019b",
      "shortCiteRegEx" : "Liu and Lapata.",
      "year" : 2019
    }, {
      "title" : "Single document summarization as tree induction",
      "author" : [ "Yang Liu", "Ivan Titov", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "The Stanford CoreNLP natural language processing toolkit",
      "author" : [ "Christopher Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven Bethard", "David McClosky." ],
      "venue" : "Proceedings of 52nd Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Manning et al\\.,? 2014",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "A deep reinforced model for abstractive summarization",
      "author" : [ "Romain Paulus", "Caiming Xiong", "Richard Socher." ],
      "venue" : "CoRR, abs/1705.04304.",
      "citeRegEx" : "Paulus et al\\.,? 2017",
      "shortCiteRegEx" : "Paulus et al\\.",
      "year" : 2017
    }, {
      "title" : "On extractive and abstractive neural document summarization with transformer language models",
      "author" : [ "Jonathan Pilault", "Raymond Li", "Sandeep Subramanian", "Chris Pal." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Pilault et al\\.,? 2020",
      "shortCiteRegEx" : "Pilault et al\\.",
      "year" : 2020
    }, {
      "title" : "ProphetNet: Predicting future n-gram for sequence-to-SequencePre-training",
      "author" : [ "Weizhen Qi", "Yu Yan", "Yeyun Gong", "Dayiheng Liu", "Nan Duan", "Jiusheng Chen", "Ruofei Zhang", "Ming Zhou." ],
      "venue" : "Findings of the Association for Computational Linguistics:",
      "citeRegEx" : "Qi et al\\.,? 2020",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2020
    }, {
      "title" : "A common theory of information fusion from multiple text sources step one: Cross-document structure",
      "author" : [ "Dragomir Radev." ],
      "venue" : "1st SIGdial Workshop on Discourse and Dialogue, pages 74–83, Hong Kong, China. Association for Computational Lin-",
      "citeRegEx" : "Radev.,? 2000",
      "shortCiteRegEx" : "Radev.",
      "year" : 2000
    }, {
      "title" : "Leveraging pre-trained checkpoints for sequence generation tasks",
      "author" : [ "Sascha Rothe", "Shashi Narayan", "Aliaksei Severyn." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:264–280.",
      "citeRegEx" : "Rothe et al\\.,? 2020",
      "shortCiteRegEx" : "Rothe et al\\.",
      "year" : 2020
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Alexander M. Rush", "Sumit Chopra", "Jason Weston." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 379–389, Lisbon, Portugal.",
      "citeRegEx" : "Rush et al\\.,? 2015",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2015
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J. Liu", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073–",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "Generating long and diverse responses with neural conversation models",
      "author" : [ "Louis Shao", "Stephan Gouws", "Denny Britz", "Anna Goldie", "Brian Strope", "Ray Kurzweil." ],
      "venue" : "CoRR, abs/1701.03185.",
      "citeRegEx" : "Shao et al\\.,? 2017",
      "shortCiteRegEx" : "Shao et al\\.",
      "year" : 2017
    }, {
      "title" : "BIGPATENT: A large-scale dataset for abstractive and coherent summarization",
      "author" : [ "Eva Sharma", "Chen Li", "Lu Wang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2204–2213, Florence, Italy.",
      "citeRegEx" : "Sharma et al\\.,? 2019",
      "shortCiteRegEx" : "Sharma et al\\.",
      "year" : 2019
    }, {
      "title" : "Pathsim: Meta path-based top-k similarity search in heterogeneous information networks",
      "author" : [ "Yizhou Sun", "Jiawei Han", "Xifeng Yan", "Philip S Yu", "Tianyi Wu." ],
      "venue" : "Proceedings of the VLDB Endowment, 4(11):992–1003.",
      "citeRegEx" : "Sun et al\\.,? 2011",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2011
    }, {
      "title" : "Neural extractive text summarization with syntactic compression",
      "author" : [ "Jiacheng Xu", "Greg Durrett." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Xu and Durrett.,? 2019",
      "shortCiteRegEx" : "Xu and Durrett.",
      "year" : 2019
    }, {
      "title" : "Discourse-aware neural extractive text summarization",
      "author" : [ "Jiacheng Xu", "Zhe Gan", "Yu Cheng", "Jingjing Liu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5021–5031, Online. Association for Computa-",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Interactive variance attention based online spoiler detection for time-sync comments",
      "author" : [ "Wenmian Yang", "Weijia Jia", "Wenyuan Gao", "Xiaojie Zhou", "Yutao Luo." ],
      "venue" : "Proceedings of the 28th ACM International Conference on Information and Knowledge",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Heterogeneous graph transformer for graphto-sequence learning",
      "author" : [ "Shaowei Yao", "Tianming Wang", "Xiaojun Wan." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020,",
      "citeRegEx" : "Yao et al\\.,? 2020",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2020
    }, {
      "title" : "PEGASUS: pre-training with extracted gap-sentences for abstractive summarization",
      "author" : [ "Jingqing Zhang", "Yao Zhao", "Mohammad Saleh", "Peter J. Liu." ],
      "venue" : "Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18",
      "citeRegEx" : "Zhang et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Bertscore: Evaluating text generation with BERT",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,",
      "citeRegEx" : "Zhang et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Article: broadcast ( magazine ) content s.v.p. chief content officer derek t. dingle s.v.p. / executive editor-at-large alfred a. edmond jr",
      "author" : [ "James Tolbert Hearn" ],
      "venue" : null,
      "citeRegEx" : "Hearn,? \\Q1947\\E",
      "shortCiteRegEx" : "Hearn",
      "year" : 1947
    } ],
    "referenceMentions" : [ {
      "referenceID" : 44,
      "context" : "Nowadays, the sequence-to-sequence (Seq2Seq) based summarization models have gained unprecedented popularity (Rush et al., 2015; See et al., 2017; Lewis et al., 2020).",
      "startOffset" : 109,
      "endOffset" : 166
    }, {
      "referenceID" : 45,
      "context" : "Nowadays, the sequence-to-sequence (Seq2Seq) based summarization models have gained unprecedented popularity (Rush et al., 2015; See et al., 2017; Lewis et al., 2020).",
      "startOffset" : 109,
      "endOffset" : 166
    }, {
      "referenceID" : 25,
      "context" : "Nowadays, the sequence-to-sequence (Seq2Seq) based summarization models have gained unprecedented popularity (Rush et al., 2015; See et al., 2017; Lewis et al., 2020).",
      "startOffset" : 109,
      "endOffset" : 166
    }, {
      "referenceID" : 10,
      "context" : "However, complex summarization scenarios such as long-document or multi-document summarization (MDS), still bring great challenges to Seq2Seq models (Cohan et al., 2018; Liu et al., 2018).",
      "startOffset" : 149,
      "endOffset" : 187
    }, {
      "referenceID" : 33,
      "context" : "However, complex summarization scenarios such as long-document or multi-document summarization (MDS), still bring great challenges to Seq2Seq models (Cohan et al., 2018; Liu et al., 2018).",
      "startOffset" : 149,
      "endOffset" : 187
    }, {
      "referenceID" : 47,
      "context" : "In a long document numerous details and salient content may distribute evenly (Sharma et al., 2019) while multiple documents may contain repeated, redundant or contradictory information (Radev, 2000).",
      "startOffset" : 78,
      "endOffset" : 99
    }, {
      "referenceID" : 42,
      "context" : ", 2019) while multiple documents may contain repeated, redundant or contradictory information (Radev, 2000).",
      "startOffset" : 94,
      "endOffset" : 107
    }, {
      "referenceID" : 26,
      "context" : "6053 course relations between sentences (Li et al., 2020).",
      "startOffset" : 40,
      "endOffset" : 57
    }, {
      "referenceID" : 16,
      "context" : "Some other works also proposed to construct local knowledge graph by OpenIE to improve Seq2Seq models (Fan et al., 2019; Huang et al., 2020).",
      "startOffset" : 102,
      "endOffset" : 140
    }, {
      "referenceID" : 19,
      "context" : "Some other works also proposed to construct local knowledge graph by OpenIE to improve Seq2Seq models (Fan et al., 2019; Huang et al., 2020).",
      "startOffset" : 102,
      "endOffset" : 140
    }, {
      "referenceID" : 45,
      "context" : "Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018).",
      "startOffset" : 86,
      "endOffset" : 174
    }, {
      "referenceID" : 18,
      "context" : "Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018).",
      "startOffset" : 86,
      "endOffset" : 174
    }, {
      "referenceID" : 39,
      "context" : "Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018).",
      "startOffset" : 86,
      "endOffset" : 174
    }, {
      "referenceID" : 6,
      "context" : "Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018).",
      "startOffset" : 86,
      "endOffset" : 174
    }, {
      "referenceID" : 35,
      "context" : "have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al.",
      "startOffset" : 70,
      "endOffset" : 113
    }, {
      "referenceID" : 43,
      "context" : "have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al.",
      "startOffset" : 70,
      "endOffset" : 113
    }, {
      "referenceID" : 14,
      "context" : "In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019).",
      "startOffset" : 61,
      "endOffset" : 102
    }, {
      "referenceID" : 8,
      "context" : "In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019).",
      "startOffset" : 61,
      "endOffset" : 102
    }, {
      "referenceID" : 24,
      "context" : "tion (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Bražinskas et al.",
      "startOffset" : 25,
      "endOffset" : 67
    }, {
      "referenceID" : 51,
      "context" : "tion (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Bražinskas et al.",
      "startOffset" : 25,
      "endOffset" : 67
    }, {
      "referenceID" : 9,
      "context" : ", 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Bražinskas et al., 2020; Amplayo and Lapata, 2020).",
      "startOffset" : 54,
      "endOffset" : 124
    }, {
      "referenceID" : 4,
      "context" : ", 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Bražinskas et al., 2020; Amplayo and Lapata, 2020).",
      "startOffset" : 54,
      "endOffset" : 124
    }, {
      "referenceID" : 1,
      "context" : ", 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Bražinskas et al., 2020; Amplayo and Lapata, 2020).",
      "startOffset" : 54,
      "endOffset" : 124
    }, {
      "referenceID" : 33,
      "context" : "After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al.",
      "startOffset" : 48,
      "endOffset" : 87
    }, {
      "referenceID" : 15,
      "context" : "After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al.",
      "startOffset" : 48,
      "endOffset" : 87
    }, {
      "referenceID" : 34,
      "context" : ", 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 99
    }, {
      "referenceID" : 26,
      "context" : ", 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 99
    }, {
      "referenceID" : 36,
      "context" : "Explicit structures play an important role in recent deep learning-based extractive and abstractive summarization methods (Li et al., 2018a,b; Liu et al., 2019a).",
      "startOffset" : 122,
      "endOffset" : 161
    }, {
      "referenceID" : 48,
      "context" : "Instead, the meta-paths (Sun et al., 2011) in the unified semantic graph convey various semantic relations.",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 38,
      "context" : "We employ CoreNLP (Manning et al., 2014) to obtain coreference chains of the input sequence and the dependency parsing tree of each sentence.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 37,
      "context" : "To better represent local features in sequence, we apply the pre-trained language model RoBERTa (Liu et al., 2019b) as our text encoder.",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 52,
      "context" : "Graph Encoding Layer Following previous works in graph-to-sequence learning (KoncelKedziorski et al., 2019; Yao et al., 2020), we apply Transformer layers for graph modeling by applying the graph adjacent matrix as self-attention mask.",
      "startOffset" : 76,
      "endOffset" : 125
    }, {
      "referenceID" : 3,
      "context" : "Graph Augmentation Following previous works (Bastings et al., 2017; Koncel-Kedziorski et al., 2019), we add reverse edges and self-loop edges in graph as the original directed edges are",
      "startOffset" : 44,
      "endOffset" : 99
    }, {
      "referenceID" : 23,
      "context" : "Graph Augmentation Following previous works (Bastings et al., 2017; Koncel-Kedziorski et al., 2019), we add reverse edges and self-loop edges in graph as the original directed edges are",
      "startOffset" : 44,
      "endOffset" : 99
    }, {
      "referenceID" : 0,
      "context" : "Shortcut Edges Indicated by previous works, graph neural networks are weak at modeling multihop relations (Abu-El-Haija et al., 2019).",
      "startOffset" : 106,
      "endOffset" : 133
    }, {
      "referenceID" : 47,
      "context" : "We evaluate our model on a SDS dataset and an MDS dataset, namely BIGPATENT (Sharma et al., 2019) and WikiSUM (Liu et al.",
      "startOffset" : 76,
      "endOffset" : 97
    }, {
      "referenceID" : 21,
      "context" : "As we fine-tune the pretrained language model RoBERTa as text encoder, we apply two different Adam optimizers (Kingma and Ba, 2015) with β1 = 0.",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 35,
      "context" : "998 to train the pre-trained part and other parts of the model (Liu and Lapata, 2019b).",
      "startOffset" : 63,
      "endOffset" : 86
    }, {
      "referenceID" : 31,
      "context" : "We evaluate the quality of generated summaries using ROUGE F1(Lin, 2004) and BERTScore (Zhang",
      "startOffset" : 61,
      "endOffset" : 72
    }, {
      "referenceID" : 14,
      "context" : "and LexRank (Erkan and Radev, 2004) are two classic extractive methods.",
      "startOffset" : 12,
      "endOffset" : 35
    }, {
      "referenceID" : 12,
      "context" : "By replacing the Transformer encoder in TransS2S with BERT (Devlin et al., 2019) or RoBERTa and training with two optimizers (Liu and Lapata, 2019b), we obtain two strong baselines BERTS2S and RoBERTaS2S.",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 35,
      "context" : ", 2019) or RoBERTa and training with two optimizers (Liu and Lapata, 2019b), we obtain two strong baselines BERTS2S and RoBERTaS2S.",
      "startOffset" : 52,
      "endOffset" : 75
    }, {
      "referenceID" : 2,
      "context" : "Seq2seq is based on LSTM encoder-decoder with attention mechanism (Bahdanau et al., 2015).",
      "startOffset" : 66,
      "endOffset" : 89
    }, {
      "referenceID" : 45,
      "context" : "Pointer and Pointer+cov are pointer-generation (See et al., 2017) with and without coverage mechanism, respectively.",
      "startOffset" : 47,
      "endOffset" : 65
    }, {
      "referenceID" : 7,
      "context" : "FastAbs (Chen and Bansal, 2018) is an abstractive method by jointly training sentence extraction and compression.",
      "startOffset" : 8,
      "endOffset" : 31
    }, {
      "referenceID" : 40,
      "context" : "TLM (Pilault et al., 2020) is a recent long-document summarization method based on language model.",
      "startOffset" : 4,
      "endOffset" : 26
    }, {
      "referenceID" : 45,
      "context" : "novel n-grams to the source input, which reflects the abstractiveness of a summarization system (See et al., 2017).",
      "startOffset" : 96,
      "endOffset" : 114
    } ],
    "year" : 2021,
    "abstractText" : "Abstractive summarization for long-document or multi-document remains challenging for the Seq2Seq architecture, as Seq2Seq is not good at analyzing long-distance relations in text. In this paper, we present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. Further, a graph-based encoder-decoder model is proposed to improve both the document representation and summary generation process by leveraging the graph structure. Specifically, several graph augmentation methods are designed to encode both the explicit and implicit relations in the text while the graphpropagation attention mechanism is developed in the decoder to select salient content into the summary. Empirical results show that the proposed architecture brings substantial improvements for both long-document and multidocument summarization tasks.ive summarization for long-document or multi-document remains challenging for the Seq2Seq architecture, as Seq2Seq is not good at analyzing long-distance relations in text. In this paper, we present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. Further, a graph-based encoder-decoder model is proposed to improve both the document representation and summary generation process by leveraging the graph structure. Specifically, several graph augmentation methods are designed to encode both the explicit and implicit relations in the text while the graphpropagation attention mechanism is developed in the decoder to select salient content into the summary. Empirical results show that the proposed architecture brings substantial improvements for both long-document and multidocument summarization tasks.",
    "creator" : "LaTeX with hyperref"
  }
}