{
  "name" : "2021.acl-long.36.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Introducing Orthogonal Constraint in Structural Probes",
    "authors" : [ "Tomasz Limisiewicz", "David Mareček" ],
    "emails" : [ "marecek}@ufal.mff.cuni.cz" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 428–442\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n428"
    }, {
      "heading" : "1 Introduction",
      "text" : "Latent representations of neural networks encode specific linguistic features. Recently, a lot of focus was devoted to interpret these representations and analyze structures captured by the deep models. One of the most popular analysis methods is probing (Belinkov et al., 2017; Blevins et al., 2018; Linzen et al., 2016; Liu et al., 2019). The pre-trained model’s 1 parameters are fixed, and its latent states or outputs are then fed into a simple neural network optimized to solve an auxiliary task, e.g., semantic, syntactic parsing, anaphora resolution, morphosyntactic tagging, etc. The amount of language information stored in the representations can be evaluated by measuring the specific language task’s performance.\n1Typically models for language modeling or machine translation are analyzed.\nProbing experiments usually involve classification tasks. Lately, Hewitt and Manning (2019) proposed Structural Probes, which use regression as an optimization objective. They train a linear projection layer to approximate: 1. dependency tree distances between words2 by the Euclidean distance between transformed vectors; 2. the tree depth of a word by the norm of its vector.\nIn Figure 1, we visualize our Orthogonal Structural Probe. A linear transformation is replaced by an Orthogonal Transformation (rotation of the embedding space), and product-wise multiplication of rotated vectors by a Scaling Vector to get the final projections. Our motivation is to obtain an embedding space that is isomorphic with the original one, and the impact of each dimension can be evaluated\n2Tree distance is the length of the tree path between two tokens\nby analyzing Scaling Vector’s weights. We elaborate on mathematical properties and training details in Section 3.\nIn addition to dependency trees used by Hewitt and Manning (2019), we introduce new structural tasks related to lexical hypernymy and word’s position in the sentence. We also employ a control task, in which we evaluate the memorization of randomly generated trees. Orthogonal Structural Probes let us optimize for multiple objectives jointly by keeping a shared Orthogonal Transformation matrix and changing task-specific Scaling Vectors.\nWe will answer the following questions:\n1. Do our Orthogonal Structural Probes achieve comparable or better performance to the Structural Probes of Hewitt and Manning (2019)?\n2. Can we find other phenomena such as lexical hypernymy and a word’s absolute position in a sentence using Orthogonal Structural Probe? How vulnerable are the probes to memorizing random data?\n3. Is it possible to effectively train Orthogonal Structural Probes jointly for multiple auxiliary objectives, i.e., depth and distance, or multiple types of structures mentioned in the previous question?\n4. Can we identify particular dimensions of the embedding space that encode particular linguistic structures? Are there any superfluous dimensions?\n5. If yes, what is the relationship between subspaces encoding distinct structures?"
    }, {
      "heading" : "2 Related Work",
      "text" : "Basic linguistic features can be easily extracted from the contextual representations (Liu et al., 2019). Probing was intensively used to investigate the representation of morphological information (mainly POS tags) in hidden states of machine translation systems and language models (Belinkov et al., 2017; Peters et al., 2018; Tenney et al., 2019b). Besides the work of Hewitt and Manning (2019), probing for dependency syntax was performed by Tenney et al. (2019a) and Blevins et al. (2018). They utilize a binary classifier to predict dependency edges. In work contemporary to ours,\nRavichander et al. (2020) employ a softmax classifier to show that BERT can be successfully probed for hypernymy.\nThere is an ongoing debate on which probe architectures offer a good insight into underlying representations. Zhang and Bowman (2018) showed that a POS tagger on top of a frozen randomly initialized LSTM model achieves unexpectedly high results. In the work of Hewitt and Liang (2019), the multilayer perceptron probes display similar accuracy for predicting POS tags as for randomly assigned tags. These symptoms underscore how crucial it is to carefully consider the probe’s architecture to avoid reaching spurious conclusions. It is good practice to monitor additional aspects of the probe beyond performance on a linguistic task, such as selectivity (Hewitt and Liang, 2019), or complexity (Pimentel et al., 2020). The recent state of knowledge is summarized in surveys on probing (Belinkov and Glass, 2019) and interpretation of BERT’s representations (Rogers et al., 2020).\nOrthogonality has been applied broadly in the field of deep learning, especially to cope with exploding/vanishing gradient problem in recurrent neural networks (Arjovsky et al., 2016; Jing et al., 2017a; Wisdom et al., 2016). In this work, we use regularization to enforce the orthogonality of a dense layer. In literature, such an approach is called “soft constraint” (Bansal et al., 2018; Vorontsov et al., 2017). Alternatively, “hard constraint” assumes parameterization of a network such that the transformation of latent states is orthogonal by definition (Arjovsky et al., 2016; Jing et al., 2017b). There are a few examples of orthogonality applications in NLP: in RNN language model (Dangovski et al., 2019); in Performer (Choromanski et al., 2020), which is a more efficient counterpart of Transformer (Vaswani et al., 2017). Best to our knowledge, we are the first to use orthogonal transformation in probing."
    }, {
      "heading" : "3 Method",
      "text" : "In this section, we first review the structural probing proposed by Hewitt and Manning (2019) and then introduce our Orthogonal Structural Probe."
    }, {
      "heading" : "3.1 Structural Probes",
      "text" : "In the previous work, a linear transformation is optimized to transform the contextual word representations produced by a pre-trained neural model (e.g. BERT Devlin et al. (2019), ELMo Peters et al.\n(2018)). The squared L2 norm of the differences between transformed word vectors approximate the tree distance between them:\ndB(hi, hj) 2 = (B(hi − hj))T (B(hi − hj)), (1)\nwhere B is the Linear Transformation matrix and hi, hj are the vector representations of words at positions i and j.\nThe probe is optimized to approximate the distance between tokens in the dependency tree (dT ) by gradient descent objective:\nmin B\n1\ns2 ∑ i,j ∣∣dT (wi, wj)− dB(hi, hj)2∣∣, (2) where s is the length of a sentence.\nMoreover, the same work introduced depth probes, where vectors were linearly transformed so that the squared L2 length of the mapping approximate the token’s depth in a dependency tree:\n||hi||2B = (Bhi)T (Bhi) (3)\nGradient descent objective is analogical:\nmin B\n1\ns ∑ i ∣∣‖wi‖T − ‖hi‖2B∣∣ (4)"
    }, {
      "heading" : "3.2 Orthogonal Structural Probes",
      "text" : "We introduce orthogonality to structural probes. For that purpose, we perform the singular value decomposition of the matrix B\nB = U ·D · V T , (5)\nwhere the matrices U and V are orthogonal, and D is diagonal. Notably, when we substitute B with U ·D · V T in Eq. (1), the matrix U cancels out. It can be easily shown by rearranging the variables in the equation:3\ndB(hi, hj) 2\n= (DV T (hi − hj))T (DV T (hi − hj)) (6)\nWe can replace the diagonal matrix D with a vector d̄ and use element-wise product (we will call d̄ the Scaling Vector). Finally, we get the following equation for Orthogonal Distance Probe:\ndd̄V T (hi, hj) 2\n= (d̄ V T (hi − hj))T (d̄ V T (hi − hj)) (7)\n3A complete derivation can be found in the appendix.\nThe same reasoning can be applied to Eq. (3) to obtain Orthogonal Depth Probe:\n||hi||2d̄V T = (d̄ V Thi) T (d̄ V Thi) (8)\nWe showed that Orthogonal Structural Probe is mathematically equivalent to Standard Structural Probe."
    }, {
      "heading" : "3.3 Multitask Training",
      "text" : "Orthogonal Structural Probe can be easily adapted to multitask probing for a set of objectives O. We use one shared Orthogonal Transformation and different Scaling Vectors for each task. In one batch, we compute a loss for a specific objective. For each batch (with objective o ∈ O), a forward pass consists of multiplication by a shared orthogonal matrix V T and product-wise multiplication by a designated vector d̄o. All the batches are shuffled together in a training epoch."
    }, {
      "heading" : "3.4 Orthogonality Regularization",
      "text" : "We use Double Soft Orthogonality Regularization (DSO) proposed by Bansal et al. (2018) to coerce orthogonality of the matrix V during training:\nλODSO(V ) = λO(||V TV −I||2F +||V V T−I||2F ) (9) || · ||F stands for the Frobenius norm of a matrix."
    }, {
      "heading" : "3.5 Sparsity Regularization",
      "text" : "In further experiments, we investigate the effects of sparsity in Scaling Vector. For that purpose, we compute the L1 norm and add it to the training loss.\nλS‖d̄‖1 (10)"
    }, {
      "heading" : "3.6 Training Objective",
      "text" : "Altogether, the loss equation in Orthogonal Distance Probe for objective o ∈ O is the following:\nLo,dist. = 1\ns2 ∑ i,j ∣∣dT (wi, wj)− dd̄oV T (hi, hj)2∣∣+ +λODSO(V ) + λS‖d̄o‖1\n(11)\nAnd in Orthogonal Depth Probe:\nLo,depth = 1\ns ∑ i ∣∣‖wi‖T − ‖hi‖2d̄oV T ∣∣+ +λODSO(V ) + λS‖d̄o‖1 (12)\nThe loss is normalized by the number of predictions in a sentence and averaged across a batch."
    }, {
      "heading" : "4 Experiments",
      "text" : "We train probes on top of each of 24 layers of English BERT large cased model (Devlin et al., 2019) implemented by HuggingFace (Wolf et al., 2020). We optimize for the approximation of depth and distance in four types of structures: syntactic dependency, lexical hypernymy, absolute position in a sentence, and randomly generated trees. In the following subsection, we expand upon these structures."
    }, {
      "heading" : "4.1 Data and Objectives",
      "text" : "In our experiments, we use training, evaluation, and test sentences from Universal Dependencies English Web Treebank (Silveira et al., 2014). Depending on the objective, we reveal only partial relevant annotation from the dataset.\nDependency Syntax We probe for syntactic structure in Universal Dependencies parse trees (Nivre et al., 2020). Dependency trees are annotated in English Web Treebank. We focus on distances between words in dependency trees and their depth, i.e., distance from the syntactic root.\nLexical Hypernymy We introduce probing for lexical information. We optimize probes to approximate the distance between pairs of words in the hypernymy tree and the depth for each word. For that purpose, we use the tree from WordNet (Miller, 1995). We consider lexical distances between pairs of nouns and pairs of verbs in sentences and lexical depth for each noun and verb. We provide gold POS information and look up synset by a lemmatized form of a word to avoid ambiguity.\nPosition in a Sentence Probing for the sentence index of a word and positional difference between pairs of words.\nRandom Structures We probe for randomly generated trees. When we jointly optimize for depth and distance, we keep the same randomly generated tree. This control task allows us to determine the extent to which our probes memorize the structures and thus over-fit to the training data."
    }, {
      "heading" : "4.2 Training",
      "text" : "We use batches of size 12 and an initial training rate of 0.02. We use learning rate decay and earlystopping mechanism: if validation loss does not achieve a new minimum after an epoch, the learning rate is divided by 10. After three consecutive\nlearning rate updates not resulting in a new minimum, the training is stopped.\nOrthogonality Regularization In our experiments, we took λO equal to 0.05.4 The regularization converged early during the gradient optimization. Hence we can assume that matrix V is orthogonal.\nSparsity Regularization By default λS = 0. Only in the experiments described in Section 5.1, we use sparsity regularization by setting λS to a positive value (0.005, 0.05, or 0.1) when DSO drops below 1.5 during the training. This mechanism prevents weakening orthogonality constraint in early epochs.\nAdditional details of the training are described in the appendix. The code is available at GitHub: https://github.com/Tom556/ OrthogonalTransformerProbing."
    }, {
      "heading" : "4.3 Evaluation",
      "text" : "We assess Spearman’s rank correlation between gold and predicted values. We report the average correlations for the sentences with lengths from 5 to 50 in the same way as Hewitt and Manning (2019).\nOur Orthogonal Structural Probes are trained jointly for multiple objectives (Section 3.3). We evaluate the effect of multitasking testing different configurations: A) separate probing for each objective; B) joint probing for distance and depth in the same structure type; C) joint probing for distance in all structures; D) joint probing for depths in all structures; E) probing for all objectives together. We compare the results with two baselines: I) optimizing only Scaling Vector; II) Structural Probes."
    }, {
      "heading" : "4.4 Dimensionality of Scaling Vector",
      "text" : "We hypothesize that the orthogonality regularization allows us to find embedding subspace capable of representing a particular linguistic structure. In Section 5.1, we examine the performance of lower-rank projections and ask whether further restrictions of dimensionality affect the results. In Section 5.2 we analyze interactions between subspaces related to a particular objective in a joint probing setting.\n4We experimentally checked that ten times smaller and ten times larger values of λO do not affect orthogonality of matrix V and lead to the same results.\nwithin 95% confidence interval based on Student’s t-test (Student, 1908) are marked in bold."
    }, {
      "heading" : "5 Results",
      "text" : "We compare Spearman’s correlations between predicted values and gold tree depths and distances in Table 1. The correlations obtained from Orthogonal Structural Probes are high for linguistic structures: from 0.803 for lexical distance to 0.882 for lexical depth. Predicted positional depths and distances nearly match gold values.\nCorrelation on training data for random structures is very weak, hinting that the probes do not memorize structures during training but extract them from the model’s representations. The correlation for distances is higher than for depth. We hypothesize it is because the probes learn some basic tree properties.5\nThe results obtained by Orthogonal Structural Probes are close to those of Structural Probes. For dependency distance, the difference is not statistically significant. Notably, correlations on training set for randomly generated trees decreased. It suggests that Orthogonal Structural Probes are less vulnerable to memorization. In multitask probing,\n5For instance, when the distances between nodes X and Y, and Y and Z are both 1, then the distance between X and Z needs to be 2\ncorrelation evenly decreases across all tasks. While selectivity (the difference between average correlation for dependency, lexical, and positional objectives and random objectives) increases from 0.673 to 0.726. Optimizing only a Scaling Vector gives distinctly lower correlations. These results emphasize the necessity of changing the coordinate system to amplify the dimensions encoding linguistic information.\nIn Fig. 2 (upper), we observe that the performance varies throughout the layers, confirming previous observations by Hewitt and Manning (2019) and Tenney et al. (2019a). The mid-upper layers tend to be more syntactic, and the mid-lower ones are more lexical. Predicting word position is more accurate in the lower layers, dropping significantly toward the last layers. It is due to the fact that in BERT, positional embeddings are added before the first layer. Random structure probes maintain steady results across all the layers."
    }, {
      "heading" : "5.1 Dimensionality",
      "text" : "We observe that orthogonality constraint is quite effective in restricting the probe’s rank. In most of our experiments, the majority of Scaling Vector parameters converged to zero. It allows selecting subspaces encoding particular linguistic features. We want to answer whether such subspace has enough capacity for each probing task. For that purpose, we zero out the dimensions with corresponding Scaling Vector weights closer to zero\nthan = 10−4.6 Their elimination does not affect the results; correlations in Table 2 and Table 1 column A are practically equal. The dimensionality reduction is the strongest for lexical and positional depth probes, where subspaces with the rank of 19 and 20 respectively encode the structures as well as the whole embedding space with 1024 dimensions (Fig. 2, lower). The number of selected dimensions is the highest in probing for random structures. This is because a large capacity is required for memorization.\nAnother question we pose is whether it would be adequate to shrink the subspace even further. For each objective, we choose and drop a random portion of parameters to examine how it would affect the predictions. We conduct a procedure similar to cross-validation, i.e., we repeatedly drop disjoint and exhaustive sets of dimensions and average results for each set at the end.7 Table 2 shows that dimension dropping had the largest impact on positional probes: −0.458 for depth; the decrease is low for lexical distance – only −0.083. It suggests that the information necessary for the latter objective is more dispersed than for the former one.\nSparsity Regularization We use sparsity regularization of Scaling Vector to examine whether dimensionality can be reduced more intelligently. The strength of regularization is regulated by value\n6In the appendix, we show that dimension selection is not sensitive to the selection of low 10−30 < < 10−3.\n7When we drop 25% of dimensions, we randomly choose four sets. Each dimension is exactly in one set.\nof λs ∈ {0.005, 0.05, 0.1}. We observe that for some objectives (dependency depth, positional depth, and positional distance), the relevant information is captured in a small number of dimensions. Remarkably, only one dimension of embedding space can achieve 0.822 correlation with dependency depths. We conjecture that if it is possible to achieve a high correlation with sparse subspaces, information on the phenomenon is focal in the model (concentrated in few dimensions). For the objectives with focal information, results decrease sharply when random dimensions are dropped because the probability of dropping important coordinates is high. On the other end of the spectrum, we can identify the objective for which information is spread – lexical distance. The dropping of random dimensions only moderately decreases correlation, as there are no especially essential coordinates. Probing with sparsity regularization produces subspaces of relatively large size.\nSparsity regularization also positively affects control objectives, decreasing correlations with distances and depths of randomly generated structures, indicating that regularized probes are less prone to memorization.\nNotably, Torroba Hennigen et al. (2020) proposed a method for selecting embeddings’ dimensions relevant to particular linguistic phenomena. In our setting, thanks to the Orthogonal Transformation, we are not constrained to analyzing the dimensions of just one coordinate system."
    }, {
      "heading" : "5.2 Separation of Information",
      "text" : "Another outcome of joint training was the ability to examine relationships between subspaces for each of the objectives. Figure 3 shows histograms of the dimensions selected in lexical and dependency probes. Each bin of the histogram corresponds to 10 coordinates. The height of a bar (in one color) represents how many were selected for a specific task. The dimensions on the x-axis are ordered by the weighted absolute values of Scaling Vectors.8\nWe found that in layers 6 and 16 (they achieve the highest correlation in lexical and dependency, respectively), the histograms are disjoint, indicating that the layers’ representations of dependency syntax and lexical hypernymy are orthogonal to each other in the embedding space. The orthogonality is less visible in the first layer and disappears almost entirely in the top one. In most layers, depth subspace is included in distance subspace for the same structural type. This behavior was expected as distance probing is more complex and therefore requires more capacity.\nIn Fig. 4 we present histograms for additional tasks at the model’s 16th layer. The positional subspace has a sizable intersection with the syntactic one, yet only a few common dimensions with the lexical subspace. The connection can be attributed to the fact that dependency edges can often be inferred from words’ relative positions. Probing for random structures is interlinked with other objectives. The sizes of shared subspaces for each pair can be found in Table 3. Histograms and tables for other sets of tasks are presented in the appendix."
    }, {
      "heading" : "6 Discussion",
      "text" : "The introduction of an orthogonal constraint is a core element of our analysis. The constraint assures that no dimension is enhanced or diminished in the transformation and allows interpreting the magnitude of values in the Scaling Vector as the relevance of each dimension for the objectives.\nIn an Orthogonal Structural Probe, the sufficient rank of a transformation is learned during the optimization. The rank regularization is a prerequisite to disentangle the information encoded by the probe (Section 5.2). The natural question\n8We weight the values before sorting to keep together non-zero dimensions of each Scaling Vector, i.e., dependency depth values are multiplied by 1000, dependency distance 100, lexical depth by 10. The weighting is performed only for visualization; the separation of linguistic information can be observed independently in Table 3.\nis whether such analysis can be performed by reducing the rank of Structural Probe with another regularizer and decomposing linear transformation after the optimization. We argue that it is not possible both in joint and separate probing:\n• In joint probing for multiple tasks: one Scaling Vector is shared for all the tasks. It is not possible to attribute the dimensions to a specific task.\n• In separate probing for each task: the decomposition leads to different orthogonal matrices. Hence, the dimensions of distinct Scaling Vectors do not correspond to each other."
    }, {
      "heading" : "6.1 Limitations",
      "text" : "We focus on syntax annotated in Universal Dependencies and lexical hypernymy encoded in WordNet. We do not claim that there is no correlation between syntactic and lexical information in BERT, just that the topologies of those two structures are encoded separately. It is entirely possible that we could find dimensions overlap when probing for syntax and lexicon in differently annotated datasets.\nConversely to Structural Probes, our reformulation of the loss (in Eq. (12) and Eq. (11)) is not convex. We thank one of the anonymous ACL reviewers for pointing it out. Nevertheless, we show that despite non-convexity, our Orthogonal Structural Probes achieve similar results to Structural Probes and are more selective."
    }, {
      "heading" : "7 Conclusions",
      "text" : "We have expanded structural probing to new types of auxiliary tasks and introduced a new setting, Orthogonal Structural Probe, in which probes can be optimized jointly. We found out that:\n1. Results of Orthogonal Structural Probes are on par with Standard Structural Probes on linguistic tasks. Orthogonal Structural Probes are less vulnerable to memorization.\n2. In addition to syntactic dependencies Orthogonal Structural Probes can be efficiently trained to approximate dependency and depth in WordNet hypernymy trees and positional order.\n3. Orthogonal Structural Probes can be trained jointly for multiple objectives. In most cases,\nthe performance moderately drops, and selectivity increases. The number of parameters decreases in comparison to training many separate probes.\n4. Usually, information necessary for each objective is stored in a subspace of relatively low rank (19 - 263). We can further reduce dimensionality by applying sparsity regularization. For a few objectives (e.g., positional depth, dependency depth), the information is hugely focal, and the performance can fall markedly when just 25% randomly selected dimensions are dropped.\n5. We have found that in most of BERT’s layers, the subspace encoding linguistic hypernymy is separated from the subspace encoding dependency syntax and subspace encoding word’s position."
    }, {
      "heading" : "7.1 Further work",
      "text" : "Our method can be adjusted for multitask and multilingual settings. Following the observation that the orthogonal transformation can map distributions of embeddings in typologically close languages (Mikolov et al., 2013; Vulić et al., 2020). We think that joint training for many languages may be possible by keeping the same Scaling Vector and adding a separate Orthogonal Transformation per language, fulfilling the role of orthogonal mappings. Another leg of research would be analyzing probes for other linguistic structures, for instance, derivation trees."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Ondřej Dušek, Greg Durrett, and anonymous reviewers of ACL for valuable comments on previous versions of this paper. This work has been supported by grant 18-02196S of the Czech Science Foundation and by grant 338521 of the Charles University Grant Agency. We have been using language resources and tools developed, stored and distributed by the LINDAT/CLARIAH-CZ project of the Ministry of Education, Youth and Sports of the Czech Republic (project LM2018101)."
    }, {
      "heading" : "A Technical Details",
      "text" : "The Orthogonal Structural Probe is trained to minimize L1 loss between predicted and gold distances and depths. The loss is normalized by the number of predictions in a sentence and averaged across a batch of size 12. Optimization is conducted with Adam (Kingma and Ba, 2014) with initial learning rate 0.02 and meta parameters: β1 = 0.9, β2 = 0.999, and = 10−8. We use learning rate decay and early-stopping mechanism: if validation loss does not achieve a new minimum after an epoch, learning rate is divided by 10. After three consecutive learning rate updates not resulting in a new minimum, the training is stopped.\nTo alleviate sharp jumps in training loss that we observed mainly in training of Depth Probes, we clip each gradient’s norm at c = 1.5.\nWe implemented the network in TensorFlow 2 (Abadi et al., 2015). The code is available at GitHub: https://github.com/Tom556/ OrthogonalTransformerProbing.\nA.1 Orthogonal Regularization\nIn order to coerce orthogonality of matrix V we add DSO to the loss. Bansal et al. (2018) showed that for convolutional neural network applied to image processing, a simpler regularization – SO is more powerful.\nλOSO(V ) = λO||V TV − I||2F (13)\nIn our experiments, DSO led to faster convergence. Fig. 5 shows values of orthogonality penalty during the training. Taking into account the properties of the Frobenius norm, we observe that V matrix is close to orthogonal already after initial epochs.\nA.2 Sparsity Regularization\nFig. 6 presents values of sparsity penalty during the training. The regularization is applied only after the orthogonality penalty drops below 1.5.\nA.3 Number of Parameters\nThe number of Orthogonal Structural Probe’s parameters is given by equation:\nNParamsOrtho = D 2 emb +Demb ·Nobj , (14)\nwhere Demb is dimensionality of the embeddings and Nobj is a number of jointly probed objectives. Therefore, our biggest probes on top of\nBERT Large for all eight objectives have 10242 + 1024 · 8 = 1, 056, 768 parameters. It is more than in Structural Probes of Hewitt and Manning (2019). Nevertheless, our probes have less degrees of freedom, because we use Orthogonal Transformation instead of Linear Transformation.\nDoFOrtho = Demb · (Demb − 1)\n2 +Demb ·Nobj\n(15) In the case of joint training for all objectives, the number of degrees of freedom equals to 523, 766.\nA.4 Computation Time We have trained Orthogonal Structural Probes on GPU a core GeForce GTX 1080 Ti. Approximate run times of specific configurations:\n• separate probing for depth ∼ 3 minutes\n• separate probing for distance ∼ 5 minutes\n• joint probing for distance and depth in the same structure type ∼ 7 minutes\n• joint probing for depths in all structures ∼ 13 minutes\n• joint probing for distance in all structures ∼ 18 minutes\n• probing for all objectives together ∼ 35 minutes"
    }, {
      "heading" : "B Derivation of Orthogonal Structural Probe Equation",
      "text" : "Eq. (6) with intermediate steps:\ndB(hi, hj) 2\n= (UDV T (hi − hj))T (UDV T (hi − hj)) = (hi − hj)TV DTUTUDV T (hi − hj) = (hi − hj)TV DTDV T (hi − hj) = (DV T (hi − hj))T (DV T (hi − hj)) (16)"
    }, {
      "heading" : "C Dataset Description",
      "text" : "Universal Dependencies English Web Treebank (Silveira et al., 2014) is available at https: //github.com/UniversalDependencies/UD_ English-EWT. It consist of: 12, 543 test, 2, 002 dev, and 2, 077 test sentences."
    }, {
      "heading" : "D Application in Dependency Parsing",
      "text" : "We have computed the UAS of dependency trees predicted based on dependency probes. We employ the algorithm for extraction of directed dependency trees proposed by Kulmizev et al. (2020). Our innovation to the method is that we optimize distance and depth probes jointly during one optimization.\nIn line with the previous studies, we show that Orthogonal Structural Probes can be employed for parsing. Table 4 presents Unlabeled Attachment Scores achieved by different multi-task configurations. Joint probing for dependency distance and depth allows us to extract a directed dependency tree in just one optimization. Best to our knowledge, it has not been tried before. Analogically to Spearman’s correlation, UAS drops when more objectives are used in optimization. However, even joint probing for all eight objectives is capable of producing trees with 75.66% UAS."
    }, {
      "heading" : "E Scaling Vector Properties",
      "text" : "In this appendix, we elaborate on the properties of Scaling Vectors parameters in the multi-task probing.\nE.1 Parameters Distribution The distribution of values in Scaling Vector (Fig. 7) shows that the majority of parameters converge to zero. They are within 10−40 to 10−30 margin after training. Therefore, the significant dimensions are clearly identifiable.\nE.2 Separation of Information (Continued) On the following pages, we present dimension overlap histograms and tables, as in Section 5.2, for the remaining pairs of objectives."
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-scale machine learning on heterogeneous systems",
      "author" : [ "Paul Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda Viégas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng." ],
      "venue" : "Software",
      "citeRegEx" : "Tucker et al\\.,? 2015",
      "shortCiteRegEx" : "Tucker et al\\.",
      "year" : 2015
    }, {
      "title" : "Unitary evolution recurrent neural networks",
      "author" : [ "Martin Arjovsky", "Amar Shah", "Yoshua Bengio." ],
      "venue" : "International Conference on Machine Learning, pages 1120–1128.",
      "citeRegEx" : "Arjovsky et al\\.,? 2016",
      "shortCiteRegEx" : "Arjovsky et al\\.",
      "year" : 2016
    }, {
      "title" : "Can We Gain More from Orthogonality Regularizations in Training Deep Networks? In S",
      "author" : [ "Nitin Bansal", "Xiaohan Chen", "Zhangyang Wang." ],
      "venue" : "Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances",
      "citeRegEx" : "Bansal et al\\.,? 2018",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2018
    }, {
      "title" : "What do neural machine translation models learn about morphology? In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol",
      "author" : [ "Yonatan Belinkov", "Nadir Durrani", "Fahim Dalvi", "Hassan Sajjad", "James Glass" ],
      "venue" : null,
      "citeRegEx" : "Belinkov et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Belinkov et al\\.",
      "year" : 2017
    }, {
      "title" : "Analysis methods in neural language processing: A survey",
      "author" : [ "Yonatan Belinkov", "James Glass." ],
      "venue" : "Transactions of the Association for Computational Linguistics (TACL), 7:49–72.",
      "citeRegEx" : "Belinkov and Glass.,? 2019",
      "shortCiteRegEx" : "Belinkov and Glass.",
      "year" : 2019
    }, {
      "title" : "Deep RNNs encode soft hierarchical syntax",
      "author" : [ "Terra Blevins", "Omer Levy", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 14–19, Melbourne, Australia. Asso-",
      "citeRegEx" : "Blevins et al\\.,? 2018",
      "shortCiteRegEx" : "Blevins et al\\.",
      "year" : 2018
    }, {
      "title" : "Rotational unit of memory: a novel representation unit for RNNs with scalable applications",
      "author" : [ "Rumen Dangovski", "Li Jing", "Preslav Nakov", "Mićo Tatalović", "Marin Soljačić." ],
      "venue" : "Transaction of the Association of Computational Linguistics, 7:121–138.",
      "citeRegEx" : "Dangovski et al\\.,? 2019",
      "shortCiteRegEx" : "Dangovski et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Designing and interpreting probes with control tasks",
      "author" : [ "John Hewitt", "Percy Liang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods",
      "citeRegEx" : "Hewitt and Liang.,? 2019",
      "shortCiteRegEx" : "Hewitt and Liang.",
      "year" : 2019
    }, {
      "title" : "A structural probe for finding syntax in word representations",
      "author" : [ "John Hewitt", "Christopher D. Manning." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Hewitt and Manning.,? 2019",
      "shortCiteRegEx" : "Hewitt and Manning.",
      "year" : 2019
    }, {
      "title" : "Gated orthogonal recurrent units: On learning to forget",
      "author" : [ "Li Jing", "Caglar Gulcehre", "John Peurifoy", "Yichen Shen", "Max Tegmark", "Marin Soljačić", "Y. Bengio." ],
      "venue" : "Neural Computation, 31.",
      "citeRegEx" : "Jing et al\\.,? 2017a",
      "shortCiteRegEx" : "Jing et al\\.",
      "year" : 2017
    }, {
      "title" : "Tunable efficient unitary neural networks (EUNN) and their application to RNNs",
      "author" : [ "Li Jing", "Yichen Shen", "Tena Dubcek", "John Peurifoy", "Scott Skirlo", "Yann LeCun", "Max Tegmark", "Marin Soljačić." ],
      "venue" : "Proceedings of the 34th International Conference",
      "citeRegEx" : "Jing et al\\.,? 2017b",
      "shortCiteRegEx" : "Jing et al\\.",
      "year" : 2017
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Do neural language models show preferences for syntactic formalisms",
      "author" : [ "Artur Kulmizev", "Vinit Ravishankar", "Mostafa Abdou", "Joakim Nivre" ],
      "venue" : "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Kulmizev et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Kulmizev et al\\.",
      "year" : 2020
    }, {
      "title" : "Assessing the ability of LSTMs to learn syntax-sensitive dependencies",
      "author" : [ "Tal Linzen", "Emmanuel Dupoux", "Yoav Goldberg." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:521– 535.",
      "citeRegEx" : "Linzen et al\\.,? 2016",
      "shortCiteRegEx" : "Linzen et al\\.",
      "year" : 2016
    }, {
      "title" : "Linguistic knowledge and transferability of contextual representations",
      "author" : [ "Nelson F. Liu", "Matt Gardner", "Yonatan Belinkov", "Matthew E. Peters", "Noah A. Smith." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploiting similarities among languages for machine translation",
      "author" : [ "Tomas Mikolov", "Quoc V. Le", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Wordnet: A lexical database for english",
      "author" : [ "George A. Miller." ],
      "venue" : "Commun. ACM, 38(11):39–41.",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "Universal Dependencies v2: An evergrowing multilingual treebank collection",
      "author" : [ "Joakim Nivre", "Marie-Catherine de Marneffe", "Filip Ginter", "Jan Hajič", "Christopher D. Manning", "Sampo Pyysalo", "Sebastian Schuster", "Francis Tyers", "Daniel Zeman" ],
      "venue" : null,
      "citeRegEx" : "Nivre et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Nivre et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Pareto probing: Trading off accuracy for complexity",
      "author" : [ "Tiago Pimentel", "Naomi Saphra", "Adina Williams", "Ryan Cotterell." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3138–3153, On-",
      "citeRegEx" : "Pimentel et al\\.,? 2020",
      "shortCiteRegEx" : "Pimentel et al\\.",
      "year" : 2020
    }, {
      "title" : "On the systematicity of probing contextualized word representations: The case of hypernymy in BERT",
      "author" : [ "Abhilasha Ravichander", "Eduard Hovy", "Kaheer Suleman", "Adam Trischler", "Jackie Chi Kit Cheung." ],
      "venue" : "Proceedings of the Ninth Joint Con-",
      "citeRegEx" : "Ravichander et al\\.,? 2020",
      "shortCiteRegEx" : "Ravichander et al\\.",
      "year" : 2020
    }, {
      "title" : "A primer in bertology: What we know about how bert works",
      "author" : [ "Anna Rogers", "Olga Kovaleva", "Anna Rumshisky." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:842–866.",
      "citeRegEx" : "Rogers et al\\.,? 2020",
      "shortCiteRegEx" : "Rogers et al\\.",
      "year" : 2020
    }, {
      "title" : "A gold standard dependency corpus for English",
      "author" : [ "Natalia Silveira", "Timothy Dozat", "Marie-Catherine de Marneffe", "Samuel Bowman", "Miriam Connor", "John Bauer", "Christopher D. Manning." ],
      "venue" : "Proceedings of the Ninth International Conference",
      "citeRegEx" : "Silveira et al\\.,? 2014",
      "shortCiteRegEx" : "Silveira et al\\.",
      "year" : 2014
    }, {
      "title" : "The probable error of a mean",
      "author" : [ "Student." ],
      "venue" : "Biometrika, pages 1–25.",
      "citeRegEx" : "Student.,? 1908",
      "shortCiteRegEx" : "Student.",
      "year" : 1908
    }, {
      "title" : "BERT rediscovers the classical NLP pipeline",
      "author" : [ "Ian Tenney", "Dipanjan Das", "Ellie Pavlick." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593– 4601, Florence, Italy. Association for Computational",
      "citeRegEx" : "Tenney et al\\.,? 2019a",
      "shortCiteRegEx" : "Tenney et al\\.",
      "year" : 2019
    }, {
      "title" : "What do you learn from context? probing for sentence structure in contextu",
      "author" : [ "Ian Tenney", "Patrick Xia", "Berlin Chen", "Alex Wang", "Adam Poliak", "R Thomas McCoy", "Najoung Kim", "Benjamin Van Durme", "Sam Bowman", "Dipanjan Das", "Ellie Pavlick" ],
      "venue" : null,
      "citeRegEx" : "Tenney et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Tenney et al\\.",
      "year" : 2019
    }, {
      "title" : "Intrinsic probing through dimension selection",
      "author" : [ "Lucas Torroba Hennigen", "Adina Williams", "Ryan Cotterell." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 197–216, Online. As-",
      "citeRegEx" : "Hennigen et al\\.,? 2020",
      "shortCiteRegEx" : "Hennigen et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "On orthogonality and learning recurrent networks with long term dependencies",
      "author" : [ "Eugene Vorontsov", "Chiheb Trabelsi", "Samuel Kadoury", "Chris Pal." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings",
      "citeRegEx" : "Vorontsov et al\\.,? 2017",
      "shortCiteRegEx" : "Vorontsov et al\\.",
      "year" : 2017
    }, {
      "title" : "Are all good word vector spaces isomorphic? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3178–3192, Online",
      "author" : [ "Ivan Vulić", "Sebastian Ruder", "Anders Søgaard." ],
      "venue" : "Association for Computa-",
      "citeRegEx" : "Vulić et al\\.,? 2020",
      "shortCiteRegEx" : "Vulić et al\\.",
      "year" : 2020
    }, {
      "title" : "Fullcapacity unitary recurrent neural networks",
      "author" : [ "Scott Wisdom", "Thomas Powers", "John Hershey", "Jonathan Le Roux", "Les Atlas." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 29, pages 4880–4888. Curran Associates,",
      "citeRegEx" : "Wisdom et al\\.,? 2016",
      "shortCiteRegEx" : "Wisdom et al\\.",
      "year" : 2016
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Language modeling teaches you more than translation does: Lessons learned through auxiliary syntactic task analysis",
      "author" : [ "Kelly Zhang", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neu-",
      "citeRegEx" : "Zhang and Bowman.,? 2018",
      "shortCiteRegEx" : "Zhang and Bowman.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "One of the most prominent approaches is structural probing (Hewitt and Manning, 2019), where a linear projection of word embeddings is performed in order to approximate the topology of dependency structures.",
      "startOffset" : 59,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : "One of the most popular analysis methods is probing (Belinkov et al., 2017; Blevins et al., 2018; Linzen et al., 2016; Liu et al., 2019).",
      "startOffset" : 52,
      "endOffset" : 136
    }, {
      "referenceID" : 5,
      "context" : "One of the most popular analysis methods is probing (Belinkov et al., 2017; Blevins et al., 2018; Linzen et al., 2016; Liu et al., 2019).",
      "startOffset" : 52,
      "endOffset" : 136
    }, {
      "referenceID" : 14,
      "context" : "One of the most popular analysis methods is probing (Belinkov et al., 2017; Blevins et al., 2018; Linzen et al., 2016; Liu et al., 2019).",
      "startOffset" : 52,
      "endOffset" : 136
    }, {
      "referenceID" : 15,
      "context" : "One of the most popular analysis methods is probing (Belinkov et al., 2017; Blevins et al., 2018; Linzen et al., 2016; Liu et al., 2019).",
      "startOffset" : 52,
      "endOffset" : 136
    }, {
      "referenceID" : 15,
      "context" : "Basic linguistic features can be easily extracted from the contextual representations (Liu et al., 2019).",
      "startOffset" : 86,
      "endOffset" : 104
    }, {
      "referenceID" : 3,
      "context" : "Probing was intensively used to investigate the representation of morphological information (mainly POS tags) in hidden states of machine translation systems and language models (Belinkov et al., 2017; Peters et al., 2018; Tenney et al., 2019b).",
      "startOffset" : 178,
      "endOffset" : 244
    }, {
      "referenceID" : 19,
      "context" : "Probing was intensively used to investigate the representation of morphological information (mainly POS tags) in hidden states of machine translation systems and language models (Belinkov et al., 2017; Peters et al., 2018; Tenney et al., 2019b).",
      "startOffset" : 178,
      "endOffset" : 244
    }, {
      "referenceID" : 8,
      "context" : "It is good practice to monitor additional aspects of the probe beyond performance on a linguistic task, such as selectivity (Hewitt and Liang, 2019), or complexity (Pimentel et al.",
      "startOffset" : 124,
      "endOffset" : 148
    }, {
      "referenceID" : 20,
      "context" : "It is good practice to monitor additional aspects of the probe beyond performance on a linguistic task, such as selectivity (Hewitt and Liang, 2019), or complexity (Pimentel et al., 2020).",
      "startOffset" : 164,
      "endOffset" : 187
    }, {
      "referenceID" : 4,
      "context" : "The recent state of knowledge is summarized in surveys on probing (Belinkov and Glass, 2019) and interpretation of BERT’s representations (Rogers et al.",
      "startOffset" : 66,
      "endOffset" : 92
    }, {
      "referenceID" : 22,
      "context" : "The recent state of knowledge is summarized in surveys on probing (Belinkov and Glass, 2019) and interpretation of BERT’s representations (Rogers et al., 2020).",
      "startOffset" : 138,
      "endOffset" : 159
    }, {
      "referenceID" : 1,
      "context" : "Orthogonality has been applied broadly in the field of deep learning, especially to cope with exploding/vanishing gradient problem in recurrent neural networks (Arjovsky et al., 2016; Jing et al., 2017a; Wisdom et al., 2016).",
      "startOffset" : 160,
      "endOffset" : 224
    }, {
      "referenceID" : 10,
      "context" : "Orthogonality has been applied broadly in the field of deep learning, especially to cope with exploding/vanishing gradient problem in recurrent neural networks (Arjovsky et al., 2016; Jing et al., 2017a; Wisdom et al., 2016).",
      "startOffset" : 160,
      "endOffset" : 224
    }, {
      "referenceID" : 31,
      "context" : "Orthogonality has been applied broadly in the field of deep learning, especially to cope with exploding/vanishing gradient problem in recurrent neural networks (Arjovsky et al., 2016; Jing et al., 2017a; Wisdom et al., 2016).",
      "startOffset" : 160,
      "endOffset" : 224
    }, {
      "referenceID" : 2,
      "context" : "In literature, such an approach is called “soft constraint” (Bansal et al., 2018; Vorontsov et al., 2017).",
      "startOffset" : 60,
      "endOffset" : 105
    }, {
      "referenceID" : 29,
      "context" : "In literature, such an approach is called “soft constraint” (Bansal et al., 2018; Vorontsov et al., 2017).",
      "startOffset" : 60,
      "endOffset" : 105
    }, {
      "referenceID" : 1,
      "context" : "Alternatively, “hard constraint” assumes parameterization of a network such that the transformation of latent states is orthogonal by definition (Arjovsky et al., 2016; Jing et al., 2017b).",
      "startOffset" : 145,
      "endOffset" : 188
    }, {
      "referenceID" : 11,
      "context" : "Alternatively, “hard constraint” assumes parameterization of a network such that the transformation of latent states is orthogonal by definition (Arjovsky et al., 2016; Jing et al., 2017b).",
      "startOffset" : 145,
      "endOffset" : 188
    }, {
      "referenceID" : 6,
      "context" : "There are a few examples of orthogonality applications in NLP: in RNN language model (Dangovski et al., 2019); in Performer (Choromanski et al.",
      "startOffset" : 85,
      "endOffset" : 109
    }, {
      "referenceID" : 28,
      "context" : ", 2020), which is a more efficient counterpart of Transformer (Vaswani et al., 2017).",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 7,
      "context" : "We train probes on top of each of 24 layers of English BERT large cased model (Devlin et al., 2019) implemented by HuggingFace (Wolf et al.",
      "startOffset" : 78,
      "endOffset" : 99
    }, {
      "referenceID" : 23,
      "context" : "1 Data and Objectives In our experiments, we use training, evaluation, and test sentences from Universal Dependencies English Web Treebank (Silveira et al., 2014).",
      "startOffset" : 139,
      "endOffset" : 162
    }, {
      "referenceID" : 18,
      "context" : "Dependency Syntax We probe for syntactic structure in Universal Dependencies parse trees (Nivre et al., 2020).",
      "startOffset" : 89,
      "endOffset" : 109
    }, {
      "referenceID" : 17,
      "context" : "For that purpose, we use the tree from WordNet (Miller, 1995).",
      "startOffset" : 47,
      "endOffset" : 61
    }, {
      "referenceID" : 24,
      "context" : "Each row’s optimal result is underlined (except baseline I); results within 95% confidence interval based on Student’s t-test (Student, 1908) are marked in bold.",
      "startOffset" : 126,
      "endOffset" : 141
    }, {
      "referenceID" : 16,
      "context" : "Following the observation that the orthogonal transformation can map distributions of embeddings in typologically close languages (Mikolov et al., 2013; Vulić et al., 2020).",
      "startOffset" : 130,
      "endOffset" : 172
    }, {
      "referenceID" : 30,
      "context" : "Following the observation that the orthogonal transformation can map distributions of embeddings in typologically close languages (Mikolov et al., 2013; Vulić et al., 2020).",
      "startOffset" : 130,
      "endOffset" : 172
    } ],
    "year" : 2021,
    "abstractText" : "With the recent success of pre-trained models in NLP, a significant focus was put on interpreting their representations. One of the most prominent approaches is structural probing (Hewitt and Manning, 2019), where a linear projection of word embeddings is performed in order to approximate the topology of dependency structures. In this work, we introduce a new type of structural probing, where the linear projection is decomposed into 1. isomorphic space rotation; 2. linear scaling that identifies and scales the most relevant dimensions. In addition to syntactic dependency, we evaluate our method on novel tasks (lexical hypernymy and position in a sentence). We jointly train the probes for multiple tasks and experimentally show that lexical and syntactic information is separated in the representations. Moreover, the orthogonal constraint makes the Structural Probes less vulnerable to memorization.",
    "creator" : "LaTeX with hyperref"
  }
}