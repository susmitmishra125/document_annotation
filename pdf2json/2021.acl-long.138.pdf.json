{
  "name" : "2021.acl-long.138.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Joint Model for Dropped Pronoun Recovery and Conversational Discourse Parsing in Chinese Conversational Speech",
    "authors" : [ "Jingxuan Yang", "Kerui Xu", "Jun Xu", "Si Li", "Sheng Gao", "Jun Guo", "Nianwen Xue", "Ji-Rong Wen" ],
    "emails" : [ "guojun}@bupt.edu.cn", "junxu@ruc.edu.cn,", "jirong.wen@gmail.com,", "xuen@brandeis.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1752–1763\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1752"
    }, {
      "heading" : "1 Introduction",
      "text" : "Pronouns are often dropped in Chinese conversations as the identity of the pronoun can be inferred from the context (Kim, 2000; Yang et al., 2015) without causing the sentence to be incomprehensible. The task of dropped pronoun recovery (DPR) aims to locate the position of the dropped pronoun and identify its type. Conversational discourse parsing (CDP) is another important task that aims to analyze the discourse relations among utterances\n∗ Corresponding author\nin a conversation, and plays a vital role in understanding multi-turn conversations.\nExisting work regards DPR and CDP as two independent tasks and tackles them separately. As an early attempt of DPR, Yang et al. (2015) employ a Maximum Entropy classifier to predict the position and type of dropped pronouns. Zhang et al. (2019) and Yang et al. (2019) attempt to recover the dropped pronouns by modeling the referents with deep neural networks. More recently, Yang et al. (2020) attempt to jointly predict all dropped pronouns in a conversation snippet by modeling dependencies between pronouns with general conditional random fields. A major shortcoming of these DPR methods is that they overlook the discourse relation (e.g., reply, question) between conversa-\ntional utterances when exploiting the context of the dropped pronoun. At the same time, previous CDP methods (Li et al., 2014; Afantenos et al., 2015; Shi and Huang, 2019) first predict the relation for each utterance pair and then construct the discourse structure for the conversation with a decoding algorithm. The effectiveness of these methods are compromised since the utterances might be incomplete when they have dropped pronouns.\nTo overcome these shortcomings, we propose a novel neural model called DiscProReco to perform DPR and CDP jointly. Figure 1 is a Chinese conversation snippet between two speakers A and B that illustrates the advantages of such a joint approach. In this example, a pronoun “你 (you)” is dropped in utterance B3. It is critical for the DPR model to know that both utterances B2 and B3 are in reply to the utterance A2, when recovering this dropped pronoun. Methods which ignore the structure (“(B3 expands B2) replies A2”) will more likely consider the utterance B3 to be semantically similar to A2, and wrongly recover the pronoun as “我 (I)”.\nGiven a pro-drop utterance and its context, DiscProReco parses the discourse structure of the conversation and recovers the dropped pronouns in the utterance in four steps: (i) Each utterance is parsed into its dependency structure and fed into a directed GCN to output the syntactic token states. The utterance state is then obtained by aggregating the token states in the utterance. (ii) The utterance states of a conversation are fed into a biaffine classifier to predict the discourse relation between each utterance pair and the discourse structure of the conversation is constructed. (iii) Taking the discourse structure as input, another (multirelational) GCN updates the utterance states and fuses them into the token states for each utterance to produce discourse-aware token representations. (iv) Based on the discourse structure-aware context representation, a pronoun recovery module is designed to recover the dropped pronouns in the utterances. When training this model, all components are jointly optimized by parameter sharing so that CDP and DPR can benefit each other. As there is no public dataset annotated with both dropped pronouns and conversational discourse structures, we also construct Structure Parsing-enhanced Dropped Pronoun Recovery (SPDPR) corpus, which is the first corpus annotated with both types of information. Experimental results show that DiscProReco outperforms all baselines of CDP and DPR.\nContributions: This work makes the following contributions: (i) We propose a unified framework DiscProReco to jointly perform CDP and DPR, and show that these two tasks can benefit each other. (ii) We construct a new large-scale dataset SPDPR (Section 4) which supports fair comparison across different methods and facilitates future research on both DPR and CDP. (iii) We present experimental results which show that DiscProReco with its joint learning mechanism realizes knowledge sharing between its CDP and DPR components and results in improvements for both tasks (Section 5). The code and SPDPR dataset is available at https:// github.com/ningningyang/DiscProReco."
    }, {
      "heading" : "2 Problem Formulation",
      "text" : "We first introduce the problem formulation of these two tasks. Following the practices in (Yang et al., 2015, 2019, 2020), we formulate DPR as a sequence labeling problem. DPR aims to recover the dropped pronouns in an utterance by assigning one of 17 labels to each token that indicates the type of pronoun that is dropped before the token (Yang et al., 2015). CDP is the task of constructing the conversational discourse structure by predicting the discourse relation (Xue et al., 2016) among utterances. The discourse relations may characterize one utterance as agreeing with, responding to, or indicate understanding of another utterance in the conversational context.\nLet us denote an input pro-drop utterance of n tokens as X = (w1, w2, · · · , wn), and its contextual utterances as C = (X1,X2, · · · ,Xm) where the ith contextual utterance Xi is a sequence of li tokens: Xi = (wi,1, · · · , wi,li). Our task aims to (1) model the distribution P(Xj|Xi,C) to predict the relation between each pair of utterances (i.e., (Xi,Xj)) for CDP, and (2) model Ŷ = argmaxY P(Y|X,C) to predict the recovered pronoun sequence Ŷ for the input utterance X. Each element of Ŷ is chosen from one of the T possible labels from Y = {y1, · · · , yT−1}∪{None} to indicate whether a pronoun is dropped before the corresponding token in utterance X and the type of the dropped pronoun. The label “None” means no pronoun is dropped before this token."
    }, {
      "heading" : "3 The DiscProReco Framework",
      "text" : ""
    }, {
      "heading" : "3.1 Model Overview",
      "text" : "The architecture of DiscProReco is illustrated in Figure 2. Given a pro-drop utterance X and its\ncontext C, DiscProReco first represents tokens of these utterances as d-dimensional pre-trained word embeddings (Li et al., 2018), and then feed them into a BiGRU (Chung et al., 2014) network, to represent sequential token states X ∈ Rn×d and C ∈ Rm×lm×d as the concatenation of forward and backward hidden states outputted from BiGRU. The syntactic dependency encoding layer then revises the sequential token states by exploiting the syntactic dependencies between tokens in the same utterance using a directed GCN and generates utterance representations. After that, the biaffine relation prediction layer predicts the relation between each pair of utterances. The discourse structure then is constructed based on the utterance nodes and the predicted relations. The discourse structure encoding layer further encodes the inter-utterance discourse structures with a multi-relational GCN, and employs the discourse-based utterance representations to revise the syntactic token states. Finally, the pronoun recovery layer explores the referent semantics from the context C and predicts the dropped pronouns in each utterance."
    }, {
      "heading" : "3.2 Syntactic Dependency Encoding Layer",
      "text" : "As the sequential token states overlook longdistance dependencies among tokens in a utterance, this layer takes in the sequential token states X and C, and revises them as syntactic token states as HX and HC by exploring the syntactic dependencies\nbetween the tokens based on a directed GCN. Specifically, for each input utterance in X and C, we first extract syntactic dependencies between the tokens with Stanford’s Stanza dependency parser (Qi et al., 2020). Using the output of the dependency parser, we construct a syntactic dependency graph for each utterance in which the nodes represents the tokens and the edges correspond to the extracted syntactic dependencies between the tokens. Following the practices of (Marcheggiani and Titov, 2017; Vashishth et al., 2018), three types of edges are defined in the graph. The node states are initialized by the sequential token states X and C, and then message passing is performed over the constructed graph using the directed GCN (Kipf and Welling, 2017), referred to as SynGCN. The syntactic dependency representation of token wi,n after (k + 1)-th GCN layer is defined as:\nhk+1wi,n = ReLU (∑ u∈N+(wi,n) g k e · ( Wkeh k u + b k e )) ,\nwhere Wke ∈ Rd×d and bke ∈ Rd are the edgespecific parameters, N+(wi,n) = N (wi,n) ∪ {wi,n} is the set of wi,n’s neighbors including itself, and ReLU(·) = max(0, ·) is the Rectified Linear Unit. gke is an edge-wise gating mechanism which incorporates the edge importance as:\ngke = σ ( ŵkeh k u + b̂ k e ) ,\nwhere ŵke ∈ R1×d and b̂ke ∈ R are independent\ntrainable parameters for each layer, and σ(·) is the sigmoid function. The revised syntactic token states HX and HC of the pro-drop utterance and context are outputted for subsequent discourse structure prediction and pronoun recovery."
    }, {
      "heading" : "3.3 Biaffine Relation Prediction Layer",
      "text" : "For conversational discourse parsing, we jointly predict the arc s(arc)i,j and relation s (rel) i,j between each pair of utterances utilizing the biaffine attention mechanism proposed in (Dozat and Manning, 2017). Given the syntactic token states HX and HC, we make an average aggregation on these token states of each utterance Xi to obtain the syntactic utterance representation hXi .\nFor a pair of utterances (Xi,Xj) in the conversation snippet, we feed the representations of these two utterances into a biaffine function to predict the probability of an arc from Xi to Xj as:\nr (arc head) i = MLP (arc head)(hXi),\nr (arc dep) j = MLP (arc dep)(hXj ),\ns (arc) i,j = r (arc head) i U (arc)r (arc dep) j + r\n(arc head)T i u (arc),\nwhere MLP is the multi-layer perceptron that transforms the original utterance representation hXi and hXj into head or dependent-specific utterance states r(arc head)i and r (arc dep) j . U (arc) and u(arc) are weight matrix and bias term used to determine the probability of a arc.\nOne distinctive characteristics of conversational discourse parsing is that the head of each dependent utterance must be chosen from the utterances before the dependent utterance. Thus we add an upper triangular mask operation on the results of arc prediction to regularize the predicted arc head:\ns(arc) = mask(s(arc)).\nWe minimize the cross-entropy of gold headdependent pair of utterances as: lossarc = − m∑ j=1 δ(Xj |Xi,C) log(Parc(Xj |Xi,C)),\nParc(Xj |Xi,C) = softmax(s(arc)i ).\nAfter obtaining the predicted directed unlabeled arc between each utterance pair, we calculate the score distribution s(rel)i,j ∈ Rk of each arc Xi → Xj , in which the t-th element indicates the score of\nthe t-th relation as the arc label prediction function in (Dozat and Manning, 2017). In the training phase, we also minimize the cross-entropy between gold relation labels and the predicted relations between utterances as: lossrel = − ∑n j=1 δ(Xj |Xi,C) log(Prel(Xj |Xi,C)),\nPrel(Xj |Xi,C) = softmax(s (rel) i,j )."
    }, {
      "heading" : "3.4 Discourse Structure Encoding Layer",
      "text" : "After the relations are predicted, we construct the discourse structure as a multi-relational graph in which each node indicates an utterance, and each edge represents the relation between a pair of utterances. In order to utilize the discourse information in dropped pronoun recovery process, we first encode the discourse structure, and then utilize the discourse information-based utterance representations to improve token states which are used to model the pronoun referent.\nSpecifically, we apply a multiple relational GCN (Vashishth et al., 2020), referred to as RelGCN, over the graph to encode the discourse structure based utterance representations R and utilize the updated representations to further revise syntactic token states HX and HC for outputting discourse structure based token states ZX and ZC. The node states of the graph are initialized as the average aggregation of token states of corresponding utterances. The representation of utterance Xi in the (k + 1)-th layer is updated by incorporating the discourse relation state hkrel as: rk+1i = f (∑ (j,rel)∈N (Xi) Prel(Xj |Xi,C)W k λ(rel)φ ( rkj ,h k rel )) ,\nwhere rkj and h k rel denote the updated representation of utterance j and relation rel after the k-th GCN layers, and Wkλ(rel) ∈ Rd×d is a relationtype specific parameter. Following the practice of (Vashishth et al., 2020), we take the composition operator φ as multiplication in this work. Please note that we take in the label distribution Prel(Xj |Xi,C) from the relation prediction layer and compute the weighted sum of each kind of relation to update the utterance representation, rather than taking the hard predicted relation by applying an argmax operation over the distribution.\nAfter encoding the constructed discourse structure with a message passing process, we obtain the discourse relation-augmented utterance representations R, and then utilize the updated utterance\nrepresentations to revise the syntactic token states with a linear feed-forward network:\nzwi,n = W1 · [ hk+1wi,n ; r k+1 i ] + b1,\nwhere hk+1wi,n refers to the token state of wi,n outputted from the (k + 1)-th layer of SynGCN, rk+1i refers to the state of the corresponding utterance that the token belongs to, outputted from the (k + 1)-th layer of RelGCN. The operation thus augments syntactic token states HX and HC with discourse information-based utterance representation to obtain discourse context-based token states ZX = (zw1 , . . . ,zwn) and ZC = (zw1,i , . . . ,zwi,li ), which will be used to model the referent semantics of the dropped pronoun in the dropper pronoun recovery layer."
    }, {
      "heading" : "3.5 Pronoun Recovery Layer",
      "text" : "This layer takes in the revised token representations ZX and ZC, and attempts to find tokens in context C that describe the referent of the dropped pronoun in the pro-drop utterance X with an attention mechanism. The referent representation is then captured as the weighted sum of discourse context-based token states as:\nawi,i′,n′ = softmax(W2 ( zwi zwi′,n′ ) + b2),\nrwi = m∑ i′=1 li′∑ n′=1 awi,i′,n′ · zwi′,n′ .\nThen we concatenate the referent representation rwi with the syntactic token representation h k+1 wi to predict the dropped pronoun category as follows:\nhrwi = tanh ( W3 · [ hk+1wi ; rwi ] + b3 ) ,\nP (yi|wi, C) = softmax (W4 · hrwi + b4) .\nThe objective of dropped pronoun recovery aims to minimize cross-entropy between the predicted label distributions and the annotated labels for all sentences as: lossdp = − ∑ q∈Q li∑ i=1 δ (yi|wi,C) log (P (yi|wi,C)) ,\nwhere Q represents all training instances, li represents the number of words in pro-drop utterance; δ (yi|wi,C) represents the annotated label of wi."
    }, {
      "heading" : "3.6 Training Objective",
      "text" : "We train our DiscProReco by jointly optimizing the objective of both discourse relation prediction and dropped pronoun recovery. The total training objective is defined as:\nloss = α · (lossarc+ losslabel) + β · lossdp, (1)\nwhere α and β are weights of CDP objective function and DPR objective function respectively."
    }, {
      "heading" : "4 The SPDPR Dataset",
      "text" : "To verify the effectiveness of DiscProReco, we need a conversational corpus containing the annotation of both dropped pronouns and discourse relations. To our knowledge, there is no such a public available corpus. Therefore, we constructed the first Structure Parsing-enhanced Dropped Pronoun Recovery (SPDPR) dataset by annotating the discourse structure information on a popular dropped pronoun recovery dataset (i.e., Chinese SMS).\nThe Chinese SMS/Chat dataset consists of 684 multi-party chat files and is a popular benchmark for dropped pronoun recovery (Yang et al., 2015). In this study, we set the size of the context snippet to be 8 utterances which include the current pro-drop utterance plus 5 utterances before and 2 utterances after. When performing discourse relation annotation we ask three linguistic experts to independently choose a head utterance for the current utterance from its context and annotate the discourse relation between them according to a set of 8 pre-defined relations (see Appendix A). The inter-annotator agreement for discourse relation annotation is 0.8362, as measured by Fleiss’s Kappa. The resulting SPDPR dataset consists of 292,455 tokens and 40,280 utterances, averaging 4,949 utterance pairs per relation, with a minimum of 540 pairs for the least frequent relation and a maximum of 12,252 for the most frequent relation. The SPDPR dataset also annotates 31,591 dropped pronouns (except the “None” category)."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Experimental Settings",
      "text" : "In this work, 300-dimensional pre-trained embeddings (Li et al., 2018) were input to the BiGRU encoder, and 500-dimensional hidden states were uitilized. For SynGCN and RelGCN, we set the number of GCN layers as 1 and 3 respectively, and augment them with a dropout rate of 0.5. The\nStanza dependency parser (Qi et al., 2020) returns 41 kinds of dependency edges. We remove 13 types of them which connects the punctuation with other tokens, and irrelevant to referent description. During training, we utilized Adam optimizer (Kingma and Ba, 2015) with a 0.005 learning rate and trained our model for 30 epochs. The model performed best on the validation set is used to make predictions on the test set. We repeat each experiment 10 times and records the average results."
    }, {
      "heading" : "5.2 Dropped Pronoun Recovery",
      "text" : "Datasets and Evaluation Metrics We tested the performance of DiscProReco for DPR on three datasets: (1) TC section of OntoNotes Release 5.0, which is a transcription of Chinese telephone conversations, and is released in the CoNLL 2012 Shared Task. (2) BaiduZhidao, which is a question answering corpus (Zhang et al., 2019). Ten types of concrete pronouns were annotated according to the pre-defined guidelines. These two benchmarks do not contain the discourse structure information and are mainly used to evaluate the effectiveness of our model for DPR task. (3) The SPDPR dataset, which contains 684 conversation files annotated with dropped pronouns and discourse relations. Following practice in (Yang et al., 2015, 2019), we reserve the same 16.7% of the training instances as the development set, and a separate test set was used to evaluate the models. The statistics of the three datasets are shown in Appendix B.\nSame as existing efforts (Yang et al., 2015, 2019), we use Precision(P), Recall(R) and F-score(F) as metrics when evaluating the performance of dropped pronoun models. Baselines We compared DiscProReco against ex-\nisting baselines, including: (1) MEPR (Yang et al., 2015), which leverages a Maximum Entropy classifier to predict the type of dropped pronoun before each token; (2) NRM (Zhang et al., 2019), which employs two MLPs to predict the position and type of a dropped pronoun separately; (3) BiGRU, which utilizes a bidirectional GRU to encode each token in a pro-drop sentence and then makes prediction; (4) NDPR (Yang et al., 2019), which models the referents of dropped pronouns from a large context with a structured attention mechanism; (5) Transformer-GCRF (Yang et al., 2020), which jointly recovers the dropped pronouns in a conversational snippet with general conditional random fields; (6) XLM-RoBERTa-NDPR, which utilizes the pre-trained multilingual masked language model (Conneau et al., 2020) to encode the pro-drop utterance and its context, and then employs the attention mechanism in NDPR to model the referent semantics.\nWe also compare two variants of DiscProReco: (1) DiscProReco (XLM-R-w/o RelGCN), which replaces the BiGRU encoder with the pre-trained XLM-RoBERTa model, removes the RelGCN layer, and only utilizes SynGCN to encode syntactic token representations for predicting the dropped pronouns. (2) DiscProReco(XLM-R) which uses the pre-trained XLM-RoBERTa model as an encoder to replace the BiGRU network in our proposed model. Experimental Results Table 1 reports the results of DiscProReco and the baseline methods on DPR. Please note that for the baseline methods, we directly used the numbers originally reported in the corresponding papers. From the results, we observed that our variant model DiscProReco(XLM-\nR-w/o RelGCN) outperforms existing baselines on three datasets by all evaluation metrics, which prove the effectiveness of our system as a standalone model for recovering dropped pronouns. We attribute this to the ability of our model to consider long-distance syntactic dependencies between tokens in the same utterance. Note that the results for feature-based baseline MEPR (Yang et al., 2015) on OntoNotes, and BaiduZhidao are not available because several essential features cannot been obtained. However, our proposed DiscProReco still significantly outperforms DiscProReco (XLM-R-w/o RelGCN) as it achieved 3.26%, 1.40%, and 1.70% absolute improvements in terms of precision, recall and F-score respectively on SPDPR corpus. This shows that discourse relations between utterances are crucially important for modeling the referent of dropped pronouns and achieving better performance in dropped pronoun recovery. This is consistent with the observation in (Ghosal et al., 2019). The best results are achieved when our model uses uses the pretrained XLM-RoBERTa (i.e., DiscProReco(XLMR)). Note that discourse relations are not available for Ontonotes and BaiduZhidao datasets and thus we do not have joint learning results for these two data sets. Error Analysis We further investigated some typical mistakes made by our DiscProReco for DPR. Resolving DPR involves effectively modeling the referent of each dropped pronoun from the context to recover the dropped pronoun. As illustrate in Figure 3, both DiscProReco and NDPR model the referent from the context. The former outperforms the latter since it considers the conversation structure that the utterance B3 is a reply to A3 but not an expansion to the utterance B1. However, just modeling the referent from the context is insufficient. In Figure 3, the referent of the dropped pronoun\nwas correctly identified but the dropped pronoun is mistakenly identified as “(他们/they)”. This indicates that the model needs to be augmented with some additional knowledge, such as the difference between singular and plural pronouns."
    }, {
      "heading" : "5.3 Conversational Discourse Parsing",
      "text" : "Datasets and Evaluation Metrics We evaluated the effectiveness of our DiscProReco framework for CDP task on two datasets as: (1) STAC, which is a standard benchmark for discourse parsing on multi-party dialogue (Asher and Lascarides, 2005). The dataset contains 1,173 dialogues, 12,867 EDUs and 12,476 relations. Same as existing studies, we set aside 10% of the training dialogues as the validation data. (2) SPDPR, which is constructed in our work containing 684 dialogues and 39,596 annotated relations. Following (Shi and Huang, 2019), we also utilized micro-averaged F-score as the evaluation metric. Baselines We compared our DiscProReco with existing baseline methods: (1) MST (Afantenos et al., 2015): A approach that uses local information in two utterances to predict the discourse relation, and uses the Maximum Spanning Tree (MST) to construct the discourse structure; (2) ILP (Perret et al., 2016): Same as MST except that the MST algorithm is replaced with Integer Linear Programming (ILP); (3) Deep+MST: A neural network that encodes the discourse representations with GRU, and then uses MST to construct the discourse structure; (4) Deep+ILP: Same as Deep+MST except that the MST algorithm is replaced with Integer Linear Programming (ILP); (5) Deep+Greedy: Similar to Deep+MST and Deep+ILP except that this model uses a greedy decoding algorithm to select the parent for each utterance; (6) Deep Sequential (Shi and Huang, 2019): A deep sequential neural net-\nwork which predicts the discourse relation utilizing both local and global context.\nIn order to explore the effectiveness of joint learning scheme, we also make a comparison of our DiscProReco with its variant, referred to as DiscProReco(w/o DPR), which predict the discourse relation independently, without recovering the dropped pronouns. Experimental Results We list the experimental results of our approach and the baselines in Table 2. For the STAC dataset, we also reported the original results of the STAC benchmark from an existing paper (Shi and Huang, 2019), and apply our DiscProReco to this corpus. For the SPDPR dataset, we ran the baseline methods with the same parameter settings. From the results we can see that the variant of our approach DiscProReco (w/o DPR) outperforms the baselines of discourse parsing. We attribute this to the effectiveness of the biaffine attention mechanism for dependency parsing task (Yan et al., 2020; Ji et al., 2019). However, our approach DiscProReco still significantly outperforms all the compared models. We attribute this to the joint training of the CDP task and the DPR task. The parameter sharing mechanism makes these two tasks benefits each other. Note that the results for the joint model is not available for STAC as STAC is not annotated with dropped pronouns."
    }, {
      "heading" : "5.4 Interaction between DPR and CDP",
      "text" : "We also conducted experiments on SPDPR to study the quantitative interaction between DPR and CDP. Firstly, during the training process, we optimize our DiscProReco model utilizing the objective function in Eq. 1 until the CDP task achieves a specific Fscore (i.e., gradually increases from 30.64 to 50.38). Then we fix the CDP components and continue to optimize the components of DPR task. We conduct this experiment to explore the influence of CDP task on the DPR task. Secondly, we set the ratio between α and β in Eq. 1 varies from 0.25 to 1.25 and record the F-score of DPR and CDP respectively. We conduct this experiment to study the interanction between these two tasks by modifying their weights in the objective function.\nResults of these two experiments are shown in Figure 4. According to Figure 4 (a), the performance of DPR is increased in terms of all evaluation metrics as the F-score of CDP increases, which indicates that exploring the discourse relations between utterances benefits dropped pronoun\nrecovery. Moreover, Figure 4 (b) illustrate the performance of DPR and CDP when the ratio between α to β varies gradually. Results show that the performance of CDP remains stable, while the performance of DPR increases at beginning and then decrease sharply as the ratio increases, indicating that DiscProReco framework should pay more attention to DPR during the optimizing process."
    }, {
      "heading" : "6 Related Work",
      "text" : "Dropped pronoun recovery is a critical technique that can benefit many downstream applications (Wang et al., 2016, 2018; Su et al., 2019). Yang et al. (2015) for the first time proposed this task, and utilized a Maximum Entropy classifier to recover the dropped pronouns in text messages. Giannella et al. (2017) further employed a linear-chain CRF to jointly predict the position and type of the dropped pronouns in a single utterance using hand-crafted features. Due to the powerful semantic modeling capability of deep learning, Zhang et al. (2019); Yang et al. (2019) introduced neural network methods to recover the dropped pronoun by modeling its semantics from the context. All these methods represent the utterances without considering the relationship between utterances, which is important to identify the referents. Zero pronoun resolution is also a closely related line of research to DPR (Chen and Ng, 2016; Yin et al., 2017, 2018). The main difference between DPR and zero pronoun resolution task is that DPR considers both anaphoric and non-anaphoric pronouns, and doesn’t attempt to resolve it to a referent.\nExisting discourse parsing methods first predicted the probability of discourse relation, and then applied a decoding algorithm to construct the discourse structure (Muller et al., 2012; Li et al., 2014; Afantenos et al., 2015; Perret et al., 2016). A deep sequential model (Shi and Huang, 2019) was further presented to predict the discourse dependencies utilizing both local information of two utterances and the global information of existing constructed discourse structure. All these methods consider how to do relation prediction independently. However, in this work, we explore the connection between the CDP and DPR, and attempt to make these two tasks mutually enhance each other."
    }, {
      "heading" : "7 Conclusion",
      "text" : "This paper presents that dropped pronoun recovery and conversational discourse parsing are two strongly related tasks. To make them benefit from each other, we devise a novel framework called DiscProReco to tackle these two tasks simultaneously. The framework is trained in a joint learning paradigm, and the parameters for the two tasks are jointly optimized. To facilitate the study of the problem, we created a large-scale dataset called SPDPR which contains the annotations of both dropped pronouns and discourse relations. Experimental results demonstrated that DiscProReco outperformed all baselines on both tasks."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by the National Key R&D Program of China (2019YFE0198200), the National Natural Science Foundation of China (No. 61872338, No. 61832017), Beijing Academy of Artificial Intelligence (BAAI2019ZD0305), Beijing Outstanding Young Scientist Program NO. BJJWZYJH012019100020098 and BUPT Excellent Ph.D. Students Foundation (No.CX2020305)."
    }, {
      "heading" : "A Discourse Relations",
      "text" : "The discourse relation describes a participant may speak a utterance to agree with, respond to, or indicate understanding of another utterance in the conversational context. According to (Xue et al., 2016) , each utterance is assumed only related to one previous utterance. All relations are summarized as 6 types between same-participant utterance pairs, and 2 types between different-participant utterance pairs, as summarized in Table 3."
    }, {
      "heading" : "B Statistics of DPR Datasets",
      "text" : "The statistics of three dropped pronoun recovery benchmarks (i.e., SPDPR, TC section of OntoNotes and BaiduZhidao) are shown in Table 4."
    } ],
    "references" : [ {
      "title" : "Discourse parsing for multiparty chat dialogues",
      "author" : [ "Stergos D. Afantenos", "Eric Kow", "Nicholas Asher", "Jérémy Perret." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal,",
      "citeRegEx" : "Afantenos et al\\.,? 2015",
      "shortCiteRegEx" : "Afantenos et al\\.",
      "year" : 2015
    }, {
      "title" : "Logics of Conversation",
      "author" : [ "Nicholas Asher", "Alex Lascarides." ],
      "venue" : "Studies in natural language processing. Cambridge University Press.",
      "citeRegEx" : "Asher and Lascarides.,? 2005",
      "shortCiteRegEx" : "Asher and Lascarides.",
      "year" : 2005
    }, {
      "title" : "Chinese zero pronoun resolution with deep neural networks",
      "author" : [ "Chen Chen", "Vincent Ng." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long",
      "citeRegEx" : "Chen and Ng.,? 2016",
      "shortCiteRegEx" : "Chen and Ng.",
      "year" : 2016
    }, {
      "title" : "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "author" : [ "Junyoung Chung", "Çaglar Gülçehre", "KyungHyun Cho", "Yoshua Bengio." ],
      "venue" : "CoRR, abs/1412.3555.",
      "citeRegEx" : "Chung et al\\.,? 2014",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2014
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep biaffine attention for neural dependency parsing",
      "author" : [ "Timothy Dozat", "Christopher D. Manning." ],
      "venue" : "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. Open-",
      "citeRegEx" : "Dozat and Manning.,? 2017",
      "shortCiteRegEx" : "Dozat and Manning.",
      "year" : 2017
    }, {
      "title" : "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "author" : [ "Deepanway Ghosal", "Navonil Majumder", "Soujanya Poria", "Niyati Chhaya", "Alexander F. Gelbukh." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods",
      "citeRegEx" : "Ghosal et al\\.,? 2019",
      "shortCiteRegEx" : "Ghosal et al\\.",
      "year" : 2019
    }, {
      "title" : "Dropped personal pronoun recovery in chinese SMS",
      "author" : [ "Chris Giannella", "Ransom K. Winder", "Stacy Petersen." ],
      "venue" : "Nat. Lang. Eng., 23(6):905–927.",
      "citeRegEx" : "Giannella et al\\.,? 2017",
      "shortCiteRegEx" : "Giannella et al\\.",
      "year" : 2017
    }, {
      "title" : "Graphbased dependency parsing with graph neural networks",
      "author" : [ "Tao Ji", "Yuanbin Wu", "Man Lan." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Vol-",
      "citeRegEx" : "Ji et al\\.,? 2019",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2019
    }, {
      "title" : "Subject/object drop in the acquisition of korean: A cross-linguistic comparison",
      "author" : [ "Young Joo Kim." ],
      "venue" : "Journal of East Asian Linguistics, 9(4):325–351.",
      "citeRegEx" : "Kim.,? 2000",
      "shortCiteRegEx" : "Kim.",
      "year" : 2000
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas N. Kipf", "Max Welling." ],
      "venue" : "5th International Conference on Learning Representations, ICLR 2017, Toulon, France,",
      "citeRegEx" : "Kipf and Welling.,? 2017",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2017
    }, {
      "title" : "Recursive deep models for discourse parsing",
      "author" : [ "Jiwei Li", "Rumeng Li", "Eduard H. Hovy." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of",
      "citeRegEx" : "Li et al\\.,? 2014",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "Analogical reasoning on chinese morphological and semantic relations",
      "author" : [ "Shen Li", "Zhe Zhao", "Renfen Hu", "Wensi Li", "Tao Liu", "Xiaoyong Du." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018,",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Encoding sentences with graph convolutional networks for semantic role labeling",
      "author" : [ "Diego Marcheggiani", "Ivan Titov." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Den-",
      "citeRegEx" : "Marcheggiani and Titov.,? 2017",
      "shortCiteRegEx" : "Marcheggiani and Titov.",
      "year" : 2017
    }, {
      "title" : "Constrained decoding for text-level discourse parsing",
      "author" : [ "Philippe Muller", "Stergos D. Afantenos", "Pascal Denis", "Nicholas Asher." ],
      "venue" : "COLING 2012, 24th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Pa-",
      "citeRegEx" : "Muller et al\\.,? 2012",
      "shortCiteRegEx" : "Muller et al\\.",
      "year" : 2012
    }, {
      "title" : "Integer linear programming for discourse parsing",
      "author" : [ "Jérémy Perret", "Stergos D. Afantenos", "Nicholas Asher", "Mathieu Morey." ],
      "venue" : "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Perret et al\\.,? 2016",
      "shortCiteRegEx" : "Perret et al\\.",
      "year" : 2016
    }, {
      "title" : "Stanza: A python natural language processing toolkit for many human languages",
      "author" : [ "Peng Qi", "Yuhao Zhang", "Yuhui Zhang", "Jason Bolton", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Qi et al\\.,? 2020",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2020
    }, {
      "title" : "A deep sequential model for discourse parsing on multi-party dialogues",
      "author" : [ "Zhouxing Shi", "Minlie Huang." ],
      "venue" : "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Con-",
      "citeRegEx" : "Shi and Huang.,? 2019",
      "shortCiteRegEx" : "Shi and Huang.",
      "year" : 2019
    }, {
      "title" : "Improving multi-turn dialogue modelling with utterance rewriter",
      "author" : [ "Hui Su", "Xiaoyu Shen", "Rongzhi Zhang", "Fei Sun", "Pengwei Hu", "Cheng Niu", "Jie Zhou." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL",
      "citeRegEx" : "Su et al\\.,? 2019",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2019
    }, {
      "title" : "Dating documents using graph convolution networks",
      "author" : [ "Shikhar Vashishth", "Shib Sankar Dasgupta", "Swayambhu Nath Ray", "Partha P. Talukdar." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Vashishth et al\\.,? 2018",
      "shortCiteRegEx" : "Vashishth et al\\.",
      "year" : 2018
    }, {
      "title" : "Composition-based multirelational graph convolutional networks",
      "author" : [ "Shikhar Vashishth", "Soumya Sanyal", "Vikram Nitin", "Partha P. Talukdar." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-",
      "citeRegEx" : "Vashishth et al\\.,? 2020",
      "shortCiteRegEx" : "Vashishth et al\\.",
      "year" : 2020
    }, {
      "title" : "Translating pro-drop languages with reconstruction models",
      "author" : [ "Longyue Wang", "Zhaopeng Tu", "Shuming Shi", "Tong Zhang", "Yvette Graham", "Qun Liu." ],
      "venue" : "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18),",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "A novel approach to dropped pronoun translation",
      "author" : [ "Longyue Wang", "Zhaopeng Tu", "Xiaojun Zhang", "Hang Li", "Andy Way", "Qun Liu." ],
      "venue" : "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Annotating the discourse and dialogue structure of SMS message conversations",
      "author" : [ "Nianwen Xue", "Qishen Su", "Sooyoung Jeong." ],
      "venue" : "Proceedings of the 10th Linguistic Annotation Workshop held in conjunction with ACL 2016, LAW@ACL 2016, August",
      "citeRegEx" : "Xue et al\\.,? 2016",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2016
    }, {
      "title" : "A graph-based model for joint chinese word segmentation and dependency parsing",
      "author" : [ "Hang Yan", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "Trans. Assoc. Comput. Linguistics, 8:78–92.",
      "citeRegEx" : "Yan et al\\.,? 2020",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2020
    }, {
      "title" : "Recovering dropped pronouns in chinese conversations via modeling their referents",
      "author" : [ "Jingxuan Yang", "Jianzhuo Tong", "Si Li", "Sheng Gao", "Jun Guo", "Nianwen Xue." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Transformer-gcrf: Recovering chinese dropped pronouns with general conditional random fields",
      "author" : [ "Jingxuan Yang", "Kerui Xu", "Jun Xu", "Si Li", "Sheng Gao", "Jun Guo", "Ji-Rong Wen", "Nianwen Xue." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical",
      "citeRegEx" : "Yang et al\\.,? 2020",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    }, {
      "title" : "Recovering dropped pronouns from chinese text messages",
      "author" : [ "Yaqin Yang", "Yalin Liu", "Nianwen Xue." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natu-",
      "citeRegEx" : "Yang et al\\.,? 2015",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "Chinese zero pronoun resolution with deep memory network",
      "author" : [ "Qingyu Yin", "Yu Zhang", "Weinan Zhang", "Ting Liu." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark,",
      "citeRegEx" : "Yin et al\\.,? 2017",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep reinforcement learning for chinese zero pronoun resolution",
      "author" : [ "Qingyu Yin", "Yu Zhang", "Weinan Zhang", "Ting Liu", "William Yang Wang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018,",
      "citeRegEx" : "Yin et al\\.,? 2018",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural recovery machine for chinese dropped pronoun",
      "author" : [ "Weinan Zhang", "Ting Liu", "Qingyu Yin", "Yu Zhang." ],
      "venue" : "Frontiers Comput. Sci., 13(5):1023–1033.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Pronouns are often dropped in Chinese conversations as the identity of the pronoun can be inferred from the context (Kim, 2000; Yang et al., 2015) without causing the sentence to be incomprehensible.",
      "startOffset" : 116,
      "endOffset" : 146
    }, {
      "referenceID" : 28,
      "context" : "Pronouns are often dropped in Chinese conversations as the identity of the pronoun can be inferred from the context (Kim, 2000; Yang et al., 2015) without causing the sentence to be incomprehensible.",
      "startOffset" : 116,
      "endOffset" : 146
    }, {
      "referenceID" : 12,
      "context" : "At the same time, previous CDP methods (Li et al., 2014; Afantenos et al., 2015; Shi and Huang, 2019) first predict the relation for each utterance pair and then construct the discourse structure for the conversation with a decoding algorithm.",
      "startOffset" : 39,
      "endOffset" : 101
    }, {
      "referenceID" : 0,
      "context" : "At the same time, previous CDP methods (Li et al., 2014; Afantenos et al., 2015; Shi and Huang, 2019) first predict the relation for each utterance pair and then construct the discourse structure for the conversation with a decoding algorithm.",
      "startOffset" : 39,
      "endOffset" : 101
    }, {
      "referenceID" : 18,
      "context" : "At the same time, previous CDP methods (Li et al., 2014; Afantenos et al., 2015; Shi and Huang, 2019) first predict the relation for each utterance pair and then construct the discourse structure for the conversation with a decoding algorithm.",
      "startOffset" : 39,
      "endOffset" : 101
    }, {
      "referenceID" : 24,
      "context" : "CDP is the task of constructing the conversational discourse structure by predicting the discourse relation (Xue et al., 2016) among utterances.",
      "startOffset" : 108,
      "endOffset" : 126
    }, {
      "referenceID" : 13,
      "context" : "context C, DiscProReco first represents tokens of these utterances as d-dimensional pre-trained word embeddings (Li et al., 2018), and then feed them into a BiGRU (Chung et al.",
      "startOffset" : 112,
      "endOffset" : 129
    }, {
      "referenceID" : 3,
      "context" : ", 2018), and then feed them into a BiGRU (Chung et al., 2014) network, to represent sequential token states X ∈ Rn×d and C ∈ Rm×lm×d as the concatenation of forward and backward hidden states outputted from BiGRU.",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 14,
      "context" : "Following the practices of (Marcheggiani and Titov, 2017; Vashishth et al., 2018), three types of edges are defined in the graph.",
      "startOffset" : 27,
      "endOffset" : 81
    }, {
      "referenceID" : 20,
      "context" : "Following the practices of (Marcheggiani and Titov, 2017; Vashishth et al., 2018), three types of edges are defined in the graph.",
      "startOffset" : 27,
      "endOffset" : 81
    }, {
      "referenceID" : 11,
      "context" : "The node states are initialized by the sequential token states X and C, and then message passing is performed over the constructed graph using the directed GCN (Kipf and Welling, 2017), referred to as SynGCN.",
      "startOffset" : 160,
      "endOffset" : 184
    }, {
      "referenceID" : 5,
      "context" : "For conversational discourse parsing, we jointly predict the arc s i,j and relation s (rel) i,j between each pair of utterances utilizing the biaffine attention mechanism proposed in (Dozat and Manning, 2017).",
      "startOffset" : 183,
      "endOffset" : 208
    }, {
      "referenceID" : 5,
      "context" : "After obtaining the predicted directed unlabeled arc between each utterance pair, we calculate the score distribution s i,j ∈ Rk of each arc Xi → Xj , in which the t-th element indicates the score of the t-th relation as the arc label prediction function in (Dozat and Manning, 2017).",
      "startOffset" : 258,
      "endOffset" : 283
    }, {
      "referenceID" : 21,
      "context" : "Specifically, we apply a multiple relational GCN (Vashishth et al., 2020), referred to as RelGCN, over the graph to encode the discourse structure based utterance representations R and utilize the updated representations to further revise syntactic token states HX and HC for outputting discourse structure based token states ZX and ZC.",
      "startOffset" : 49,
      "endOffset" : 73
    }, {
      "referenceID" : 21,
      "context" : "Following the practice of (Vashishth et al., 2020), we take the composition operator φ as multiplication in this work.",
      "startOffset" : 26,
      "endOffset" : 50
    }, {
      "referenceID" : 28,
      "context" : "The Chinese SMS/Chat dataset consists of 684 multi-party chat files and is a popular benchmark for dropped pronoun recovery (Yang et al., 2015).",
      "startOffset" : 124,
      "endOffset" : 143
    }, {
      "referenceID" : 13,
      "context" : "In this work, 300-dimensional pre-trained embeddings (Li et al., 2018) were input to the BiGRU encoder, and 500-dimensional hidden states were uitilized.",
      "startOffset" : 53,
      "endOffset" : 70
    }, {
      "referenceID" : 17,
      "context" : "Stanza dependency parser (Qi et al., 2020) returns 41 kinds of dependency edges.",
      "startOffset" : 25,
      "endOffset" : 42
    }, {
      "referenceID" : 10,
      "context" : "During training, we utilized Adam optimizer (Kingma and Ba, 2015) with a 0.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 31,
      "context" : "(2) BaiduZhidao, which is a question answering corpus (Zhang et al., 2019).",
      "startOffset" : 54,
      "endOffset" : 74
    }, {
      "referenceID" : 28,
      "context" : "Baselines We compared DiscProReco against existing baselines, including: (1) MEPR (Yang et al., 2015), which leverages a Maximum Entropy classifier to predict the type of dropped pronoun before each token; (2) NRM (Zhang et al.",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 31,
      "context" : ", 2015), which leverages a Maximum Entropy classifier to predict the type of dropped pronoun before each token; (2) NRM (Zhang et al., 2019), which employs two MLPs to predict the position and type of a dropped pronoun separately; (3) BiGRU, which utilizes a bidirectional GRU to encode each token in a pro-drop sentence and then makes prediction; (4) NDPR (Yang et al.",
      "startOffset" : 120,
      "endOffset" : 140
    }, {
      "referenceID" : 26,
      "context" : ", 2019), which employs two MLPs to predict the position and type of a dropped pronoun separately; (3) BiGRU, which utilizes a bidirectional GRU to encode each token in a pro-drop sentence and then makes prediction; (4) NDPR (Yang et al., 2019), which models the referents of dropped pronouns from a large context with a structured attention mechanism; (5) Transformer-GCRF (Yang et al.",
      "startOffset" : 224,
      "endOffset" : 243
    }, {
      "referenceID" : 27,
      "context" : ", 2019), which models the referents of dropped pronouns from a large context with a structured attention mechanism; (5) Transformer-GCRF (Yang et al., 2020), which jointly recovers the dropped pronouns in a conversational snippet with general conditional random fields; (6) XLM-RoBERTa-NDPR, which utilizes the pre-trained multilingual masked language model (Conneau et al.",
      "startOffset" : 137,
      "endOffset" : 156
    }, {
      "referenceID" : 4,
      "context" : ", 2020), which jointly recovers the dropped pronouns in a conversational snippet with general conditional random fields; (6) XLM-RoBERTa-NDPR, which utilizes the pre-trained multilingual masked language model (Conneau et al., 2020) to encode the pro-drop utterance and its context, and then employs the attention mechanism in NDPR to model the referent semantics.",
      "startOffset" : 209,
      "endOffset" : 231
    }, {
      "referenceID" : 28,
      "context" : "Note that the results for feature-based baseline MEPR (Yang et al., 2015) on OntoNotes, and BaiduZhidao are not available because several essential features cannot been obtained.",
      "startOffset" : 54,
      "endOffset" : 73
    }, {
      "referenceID" : 6,
      "context" : "This is consistent with the observation in (Ghosal et al., 2019).",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 1,
      "context" : "Datasets and Evaluation Metrics We evaluated the effectiveness of our DiscProReco framework for CDP task on two datasets as: (1) STAC, which is a standard benchmark for discourse parsing on multi-party dialogue (Asher and Lascarides, 2005).",
      "startOffset" : 211,
      "endOffset" : 239
    }, {
      "referenceID" : 18,
      "context" : "Following (Shi and Huang, 2019), we also utilized micro-averaged F-score as the evaluation metric.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "Baselines We compared our DiscProReco with existing baseline methods: (1) MST (Afantenos et al., 2015): A approach that uses local information in two utterances to predict the discourse relation, and uses the Maximum Spanning Tree (MST) to construct the discourse structure; (2) ILP (Perret et al.",
      "startOffset" : 78,
      "endOffset" : 102
    }, {
      "referenceID" : 16,
      "context" : ", 2015): A approach that uses local information in two utterances to predict the discourse relation, and uses the Maximum Spanning Tree (MST) to construct the discourse structure; (2) ILP (Perret et al., 2016): Same as MST except that the MST algorithm is replaced with Integer Linear Programming (ILP); (3) Deep+MST: A neural network that encodes the discourse representations with GRU, and then uses MST to construct the discourse structure; (4) Deep+ILP: Same as Deep+MST except that the MST algorithm is replaced with Integer Linear Programming (ILP); (5) Deep+Greedy: Similar to Deep+MST and Deep+ILP except that this model uses a greedy decoding algorithm to select the parent for each utterance; (6) Deep Sequential (Shi and Huang, 2019): A deep sequential neural net-",
      "startOffset" : 188,
      "endOffset" : 209
    }, {
      "referenceID" : 18,
      "context" : ", 2016): Same as MST except that the MST algorithm is replaced with Integer Linear Programming (ILP); (3) Deep+MST: A neural network that encodes the discourse representations with GRU, and then uses MST to construct the discourse structure; (4) Deep+ILP: Same as Deep+MST except that the MST algorithm is replaced with Integer Linear Programming (ILP); (5) Deep+Greedy: Similar to Deep+MST and Deep+ILP except that this model uses a greedy decoding algorithm to select the parent for each utterance; (6) Deep Sequential (Shi and Huang, 2019): A deep sequential neural net-",
      "startOffset" : 521,
      "endOffset" : 542
    }, {
      "referenceID" : 18,
      "context" : "For the STAC dataset, we also reported the original results of the STAC benchmark from an existing paper (Shi and Huang, 2019), and apply our DiscProReco to this corpus.",
      "startOffset" : 105,
      "endOffset" : 126
    }, {
      "referenceID" : 25,
      "context" : "biaffine attention mechanism for dependency parsing task (Yan et al., 2020; Ji et al., 2019).",
      "startOffset" : 57,
      "endOffset" : 92
    }, {
      "referenceID" : 8,
      "context" : "biaffine attention mechanism for dependency parsing task (Yan et al., 2020; Ji et al., 2019).",
      "startOffset" : 57,
      "endOffset" : 92
    }, {
      "referenceID" : 19,
      "context" : "Dropped pronoun recovery is a critical technique that can benefit many downstream applications (Wang et al., 2016, 2018; Su et al., 2019).",
      "startOffset" : 95,
      "endOffset" : 137
    }, {
      "referenceID" : 2,
      "context" : "Zero pronoun resolution is also a closely related line of research to DPR (Chen and Ng, 2016; Yin et al., 2017, 2018).",
      "startOffset" : 74,
      "endOffset" : 117
    }, {
      "referenceID" : 15,
      "context" : "1760 Existing discourse parsing methods first predicted the probability of discourse relation, and then applied a decoding algorithm to construct the discourse structure (Muller et al., 2012; Li et al., 2014; Afantenos et al., 2015; Perret et al., 2016).",
      "startOffset" : 170,
      "endOffset" : 253
    }, {
      "referenceID" : 12,
      "context" : "1760 Existing discourse parsing methods first predicted the probability of discourse relation, and then applied a decoding algorithm to construct the discourse structure (Muller et al., 2012; Li et al., 2014; Afantenos et al., 2015; Perret et al., 2016).",
      "startOffset" : 170,
      "endOffset" : 253
    }, {
      "referenceID" : 0,
      "context" : "1760 Existing discourse parsing methods first predicted the probability of discourse relation, and then applied a decoding algorithm to construct the discourse structure (Muller et al., 2012; Li et al., 2014; Afantenos et al., 2015; Perret et al., 2016).",
      "startOffset" : 170,
      "endOffset" : 253
    }, {
      "referenceID" : 16,
      "context" : "1760 Existing discourse parsing methods first predicted the probability of discourse relation, and then applied a decoding algorithm to construct the discourse structure (Muller et al., 2012; Li et al., 2014; Afantenos et al., 2015; Perret et al., 2016).",
      "startOffset" : 170,
      "endOffset" : 253
    }, {
      "referenceID" : 18,
      "context" : "A deep sequential model (Shi and Huang, 2019) was further presented to predict the discourse dependencies utilizing both local information of two utterances and the global information of existing constructed discourse structure.",
      "startOffset" : 24,
      "endOffset" : 45
    } ],
    "year" : 2021,
    "abstractText" : "In this paper, we present a neural model for joint dropped pronoun recovery (DPR) and conversational discourse parsing (CDP) in Chinese conversational speech. We show that DPR and CDP are closely related, and a joint model benefits both tasks. We refer to our model as DiscProReco, and it first encodes the tokens in each utterance in a conversation with a directed Graph Convolutional Network (GCN). The token states for an utterance are then aggregated to produce a single state for each utterance. The utterance states are then fed into a biaffine classifier to construct a conversational discourse graph. A second (multi-relational) GCN is then applied to the utterance states to produce a discourse relation-augmented representation for the utterances, which are then fused together with token states in each utterance as input to a dropped pronoun recovery layer. The joint model is trained and evaluated on a new Structure Parsing-enhanced Dropped Pronoun Recovery (SPDPR) dataset that we annotated with both two types of information. Experimental results on the SPDPR dataset and other benchmarks show that DiscProReco significantly outperforms the state-of-the-art baselines of both tasks.",
    "creator" : "LaTeX with hyperref"
  }
}