{
  "name" : "2021.acl-long.503.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "BERTGEN: Multi-task Generation through BERT",
    "authors" : [ "Faidon Mitzalis", "Ozan Caglayan", "Pranava Madhyastha", "Lucia Specia" ],
    "emails" : [ "phaedonmit@gmail.com,", "o.caglayan@ic.ac.uk", "pranava@ic.ac.uk", "lspecia@ic.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6440–6455\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6440"
    }, {
      "heading" : "1 Introduction",
      "text" : "Recent work in unsupervised and self-supervised pre-training has revolutionised the field of natural language understanding (NLU), resulting in high performance ceilings across multiple tasks (Devlin et al., 2019; Yang et al., 2019; Dong et al., 2019). The recent success of language model pre-training with masked language modelling (MLM) such as BERT (Devlin et al., 2019) further paved the way for more complex approaches that combine language pre-training with images (Tan and Bansal, 2019; Su et al., 2020; Lu et al., 2020), video (Sun et al., 2019), and speech (Chuang et al., 2020). Most of these approaches follow a task-specific fine-tuning step after the model is pre-trained.\nHowever, there has been little work on exploiting pre-trained MLMs for natural language generation (NLG) tasks. Previous work argues that the MLM objective is ill-suited for generation tasks such as machine translation (Yang et al., 2019; Rothe et al.,\n2020). Recent work in this direction has predominantly investigated the use of pre-trained models to either initialise Transformer-based encoderdecoder models (Imamura and Sumita, 2019; Clinchant et al., 2019; Yang et al., 2020; Rothe et al., 2020) or to distill knowledge for sequence generation tasks (Chen et al., 2020).\nIn this work, we present BERTGEN, which extends BERT in a generative setting (§ 2.1). This results in a single generator – without a separation between the encoder and the decoder – capable of consuming multiple input modalities and generating in multiple languages. The latter features are achieved by transferring knowledge from state-of-the-art pretrained models, namely VL-BERT (Su et al., 2020) and multilingual BERT (M-BERT) (Devlin et al., 2019). We train BERTGEN on various tasks, including image captioning, machine translation and multimodal machine translation, and datasets in four different languages (§ 2.2).\nBased on a number of experiments, our findings (§ 3) show that BERTGEN (i) is surprisingly versatile as it is capable of describing images and performing translation in unimodal and multimodal settings, across all languages, (ii) generalises well across zero-shot image captioning, multimodal machine translation, and out-of-domain news translation tasks, and finally (iii) is parameter efficient when compared to state-of-the-art models for each of the tasks combined together."
    }, {
      "heading" : "2 Method",
      "text" : "In this section, we describe BERTGEN and the tasks we explore. We then detail the baselines and SoTA systems that we compare against."
    }, {
      "heading" : "2.1 Model",
      "text" : "This section details the main aspects of BERTGEN that distinguish it from the existing work on vision & language pre-training.\nInitialisation. We take advantage of the previous successes in large-scale pre-training and propose a hybrid initialisation for BERTGEN (Figure 2). This involves using the VL-BERT (Su et al., 2020) checkpoint and initialising the word embeddings, the Transformer weights and the MLM head with M-BERT (Devlin et al., 2019). We conjecture that this primes BERTGEN to be aware of the visual modality and of multiple languages. This is simply due to VL-BERT being pre-trained on English monolingual and image captioning corpora, as well as M-BERT offering a 119K WordPiece vocabulary, trained on the entire Wikipedia in 104 languages1.\n1We adopt the ‘BERT-Base, Multilingual Cased’ version from the Transformers toolkit (Wolf et al., 2020).\nInput configuration. While BERTGEN is potentially capable of modeling a variety of generative tasks, we focus on three particular tasks, namely machine translation (MT), multimodal MT (MMT) and image captioning (IC). Therefore, depending on the task, the input configuration of the model may change during both training and testing. To clarify further, let us first denote a sequence of embeddings representing a source sentence by x(i) = [x\n(i) 1 , · · · , x (i) m ], its target translation by\ny(i) = [y (i) 1 , · · · , y (i) n ], and a collection of k regional visual features extracted from an associated image by v(i) = [v(i)1 , · · · , v (i) k ]. Figure 1 depicts BERTGEN when processing a sample from the MMT task. This task’s input configuration is a triplet that involves all the three sequences i.e. {x(i),y(i),v(i)}. Using this notation, the MT and IC tasks’ configurations would correspond to {x(i),y(i)} and {v(i),y(i)}, respectively.\nVisual embeddings. We follow VL-BERT and represent images as a collection of k features v(i) defined for regions of interest (RoI). After preextracting the 2048-dimensional RoI features using the bottom-up-top-down object detector (Anderson et al., 2018), we keep between 10 and 100 (i.e. k ∈ [10, 100]) of them depending on the confidence score. The final visual embedding for an RoI is obtained by summing its feature vector and its geometric embedding (i.e. the projection of the\nbounding box coordinates). When encoding the non-visual positions, the same RoI feature vector for the full image is repeated (see Figure 1). We note that we do not fine-tune the object detector during training.\nSequence unrolling. An important aspect of BERTGEN is that it does not explicitly distinguish between the encoder and the decoder blocks usually seen in sequence-to-sequence models. This is accomplished by formalising both encoding and generation using the MLM framework. Formally, let us consider the MMT task and define the maximum log-likelihood objective for a given triplet {x(i),v(i),y(i)} where the target y(i) has n tokens:\nL(i) = n∑\nt=1\nlog ( P (y\n(i) t |x(i);v(i);y (i) <t) ) (1)\nIn a typical sequence-to-sequence model, each logprobability term would be computed by a decoder within the forward-pass of the same training example. In contrast, BERTGEN explicitly unrolls the example n times, forming n new training examples. In other words, each conditional term in Equation 1 is observed independently within an epoch of training. Therefore, sequence unrolling has a data augmentation effect since a training corpus with D examples is approximately augmented by a factor of the average length of the target sequences. Moreover, the unified encoder-decoder formalism halves the number of parameters, making BERTGEN parameter efficient.\nSelf attention. Given that a single Transformer (Vaswani et al., 2017) performs both encoding and decoding, sequence unrolling affects selfattention as well (Figure 3). First, all positions attend to each other for a given unrolled example i.e. the attention is bi-directional. Second, since each unrolled case is an independent example, the\nself-attentive representations of early positions are naturally re-computed, in contrast to typical Transformer decoders. Finally, due to how inputs/outputs are represented in a single stream and encoded through shared self-attention, BERTGEN enforces an inductive bias towards a truly multi-modal and multi-lingual representation space.\nTarget language specifiers. Finally, to select the language during generation, input sequences begin with special target language specifiers (Ha et al., 2016; Johnson et al., 2017) (Figure 1). The specifier is task-agnostic, i.e. the same specifier [DE] is used both when captioning into German and when translating into German.\nTraining & hyper-parameters. We extend2 the base configuration of VL-BERT which is a Transformer with 12 self-attention layers and 12 heads. The model and feed-forward dimensions are 768 and 3072, respectively. On a single 32GB V100 GPU, one epoch (§ 3) takes approximately two days to complete as we could only fit one example per task (i.e. batch size equal to 13) into the memory3. We use AdamW optimiser (Loshchilov and Hutter, 2019) with base learning rate set to 1.3×10−5. The learning rate is warmed up in the first 16K steps and then decays linearly. We set the weight decay to 10−4. During training, we let the model update the positional embeddings as BERTGEN needs to learn new positions not covered by VL-BERT pretraining. The final model has ∼89.3M parameters excluding the word embeddings.\nDecoding. At test time, we incrementally add the most likely prediction (i.e. greedy search) into the previously masked position (Figure 1) and shift the [MASK] token right by one. The reason we chose greedy over beam search is because the latter would make decoding much slower due to self-attentive representations being re-computed. The decoding ends when [STOP] is predicted."
    }, {
      "heading" : "2.2 Tasks & Systems",
      "text" : "To evaluate BERTGEN’s generative abilities, we explore a diverse set of tasks: image captioning, textonly MT and multimodal MT. Table 1 summarises the training statistics for the various datasets we use.\n2https://github.com/ImperialNLP/BertGen 3With careful optimisation of the training code and mixed precision multi-GPU training, the training time can be substantially reduced."
    }, {
      "heading" : "2.2.1 Image Captioning",
      "text" : "Image captioning (IC) involves describing images in a specified natural language. We train BERTGEN for English, German and Turkish captioning tasks. Specifically, we use the FLICKR30K dataset (Young et al., 2014) that provides 29K training images, each with five English captions collected through crowd-sourcing. The validation and test sets contain approximately 1K images each. We use the MULTI30K dataset (Elliott et al., 2016), which annotates FLICKR30K images with five German captions. Finally, we use the TASVIRET dataset (Unal et al., 2016) which provides two Turkish captions for each of the 8,092 images in the FLICKR8K dataset (Rashtchian et al., 2010). Since FLICKR8K is a subset of FLICKR30K, we create a new split of TASVIRET to avoid data leakage between training and test splits. The resulting training, validation and test splits contain 6914, 543, and 543 images, respectively.\nTo evaluate BERTGEN’s performance on IC, we compare it against previous work with strong performance on COCO (Chen et al., 2015) and FLICKR30K. More precisely, ADAPTIVE ATTENTION (SENTINEL) (Lu et al., 2017), which uses a sentinel token to distinguish between visual and non-visual representations, and NEURAL BABY TALK (NBT), which follows a slot-filling approach through explicit object region information (Lu et al., 2018)."
    }, {
      "heading" : "2.2.2 Multimodal Machine Translation",
      "text" : "Multimodal Machine Translation (MMT) attempts to improve MT quality by incorporating information from modalities other than language (Sulubacak et al., 2020). In our case, we train BERTGEN for EN↔DE and EN↔FR MMT tasks and use the MULTI30K dataset, the main dataset for image-informed translation, which provides caption translations for FLICKR30K images in German and French. To evaluate BERTGEN on MMT tasks, we use the original 2016 test set which contains 1,000 examples.\nFor a comprehensive comparison with previous work, we train a SoTA recurrent MMT (Caglayan et al., 2020) solely on the MULTI30K dataset, which applies a secondary (visual) attention in the decoder over the RoI features i.e. the same features that are also used by BERTGEN (§ 2.1). There are two GRU (Cho et al., 2014) layers in both the encoder and the decoder and the embedding & hidden dimensions in the model are set to 200 and 320, respectively. Each model has ∼5.6M parameters excluding the word embeddings.\nBesides the state-of-the-art constrained recurrent MMT model described above, we further compare BERTGEN – which is trained on various other MT and IC corpora – to an unconstrained Transformer-based MMT trained on ∼9M additional EN→DE sentences (Libovický, 2019)4 in addition to MULTI30K."
    }, {
      "heading" : "2.2.3 Text-only Machine Translation",
      "text" : "We incorporate six text-only MT tasks into our training protocol. We use EN↔DE and EN↔FR MT datasets from IWSLT’14 (Cettolo et al., 2012) which consists of TED Talks’ subtitles and their translations. We take the prepare-iwslt14 recipe from FAIRSEQ (Ott et al., 2019) to prepare the dev and test sets. This yields an EN↔DE test set of 6,750 sentences which consists of dev2010, dev2012.TEDX, tst2010, tst2011 and tst2012. Similarly, the EN↔FR test set consists of dev2010, tst2010, tst2011 and tst2012, which amounts to 4,493 sentences.\nFor EN↔TR directions, we use the SETIMES2 (Tiedemann, 2012) news dataset for training. For development and test sets, we take the official WMT test sets (Bojar et al., 2018), namely, newstest2016 and newstest2017 as the development\n4We obtained test set outputs from the author and preprocessed with M-BERT tokeniser to ensure comparability.\nset (6,007 sentences), and newstest2018 (6,000 sentences) as the test set. Both IWSLT and SETIMES2 corpora are medium-scale resources often used in MT research community, and have much harder test sets than the MMT and IC tasks, due to a significant domain shift.\nFinally, for each translation direction, we train a Transformer NMT model (Vaswani et al., 2017) using the IWSLT-DE-EN recipe of the FAIRSEQ toolkit (Ott et al., 2019). This recipe has six encoders and six decoders, each equipped with 4-head self-attention layers. The model and feed-forward dimensions are set to 512 and 1024, respectively. Each model has ∼31.5M parameters excluding the word embeddings. Since BERTGEN is a general purpose multilingual and multimodal generator, we expect it to perform in the same ballpark as these strong NMT baselines, but not necessarily be SoTA compared to novel & sophisticated NMT models, which also make use of a lot more training data."
    }, {
      "heading" : "3 Results and Findings",
      "text" : "We train BERTGEN on lowercased sentences for 45 epochs, after which the overall performance on the tasks reached a plateau. We define one BERTGEN epoch as a single pass over all of the training data for the MULTI30K EN→DE MMT task and denote this task as the reference task. We use greedy search for all systems that we trained and merge back the word pieces before evaluation. We compute tokenised5 BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and CIDEr (Vedantam et al., 2015) using cococaption6. In what follows, we provide detailed quantitative and qualitative findings."
    }, {
      "heading" : "3.1 Image Captioning",
      "text" : "Table 2 provides an overview of BERTGEN’s image captioning performance on different test sets and languages. First of all, on English FLICKR30K, BERTGEN is clearly able to outperform strong captioning models (§ 2.2.1) SENTINEL (Lu et al., 2017) and NBT (Lu et al., 2018), even though they use beam search for decoding. On COCO (Chen et al., 2015), an image captioning corpus much larger and diverse than FLICKR30K, we evaluate BERTGEN on Karpathy’s test split (Karpathy and FeiFei, 2015) and notice that the scores are reasonable\n5Since M-BERT is aggressive on splitting apostrophes and hyphens, our results may slightly differ from other work.\n6https://github.com/tylin/coco-caption\ngiven that BERTGEN is not trained on COCO: our model lags behind NBT (w/ beam search) by 6.7 METEOR.\nFor zero-shot French captioning (F30K FR), we resort to the reference MMT translations from the MULTI30K EN→FR task, as there are no human references for French. Although this is problematic as the metrics will penalise captions that are not translations of English captions, we provide the scores to show that the zero-shot outputs are valid descriptions. We note that the low range of scores reported here is also due to having one reference caption instead of five references7 as in FLICKR30K. Finally we report results for our custom Turkish split (§ 2.2.1) (F30K TR) and German (F30K DE). Even though there are no comparable results in the literature for these three tasks, we demonstrate through some qualitative examples that BERTGEN produces sensible outputs.\nQualitative examples. We now focus on a few examples to examine the multilingual image captioning ability of BERTGEN in action (Table 3). For the first image, all captions are almost the same as the image has few salient points. For the second image however, we observe much more variation across captions, in line with the complexity of the scene. We are particularly surprised by the zeroshot French captioning performance, a task that BERTGEN is not trained for at all. Upon manual inspection, we noticed that the captions are often short, objective gists of the images. These observations also hold for the captions generated for the\n7As a reference, evaluating English captions using one reference at a time, yields 7.9 BLEU on average, compared to 27.0 BLEU in Table 2.\nCOCO test set, as we can see in the third example. A set of additional examples in the Appendix shows that BERTGEN does not simply retrieve caption translations learned from the EN→FR task. Overall, both quantitative and qualitative results provide evidence of the utility of multimodal and multilingual initialisation as well as the efficacy of knowledge transfer across different tasks for image captioning."
    }, {
      "heading" : "3.2 Multimodal Machine Translation",
      "text" : "Table 4 summarises BERTGEN’s performance on MMT. First of all, BERTGEN consistently outperforms the Transformer-based FAIRSEQ NMT models and the recurrent MMT (Caglayan et al., 2020) models on both the EN→DE and the EN→FR language pairs. Furthermore, BERTGEN is also substantially better than a state-of-the-art unconstrained MMT (Libovický, 2019) model trained on a ∼6x larger parallel corpus.\nAdversarial evaluation. Following Elliott (2018), we probe BERTGEN’s ability for integrating multiple modalities effectively. Specifically, we decode translations by shuffling {image, source caption} mappings so that the images do not correspond to the sentences to be translated. The EN→DE results showed that the incongruence leads to 1.1 and 0.9 point drops in BLEU and METEOR, respectively. For EN→FR, the drops are much more prominent with 3.1 and 2.3 points again for BLEU and METEOR. This indicates that the features are not ignored at all, unlike in (Caglayan et al., 2019), where they showed that sequence-to-sequence MMT models can learn to ignore the images when the linguistic signal is sufficient to perform the task.\nZero-shot performance. The results in Table 4 show the surprising ability of BERTGEN to perform MMT on directions unseen during training.\nMoreover, the zero-shot performance surpasses strong MMT and NMT systems by up to 2 and 3.3 METEOR for DE→FR and FR→DE, respectively. Similar to the image captioning results, this demonstrates the potential of BERTGEN to generalise over a variety of language pairs and tasks."
    }, {
      "heading" : "3.3 Machine Translation",
      "text" : "First, we compare BERTGEN’s performance to each task-specific FAIRSEQ system. According to Table 5, we observe that the translation quality of BERTGEN is generally superior compared to the strong FAIRSEQ systems, especially in METEOR, where BERTGEN leads in all pairs.\nSecond, we look at the learning efficiency by comparing the training curves between BERTGEN and each task-specific FAIRSEQ system (Figure 4). Here, the x axis represents how many times the spe-\ncific task’s training set has been seen by the models. BERTGEN is trained for 45 reference epochs (§ 3), and this corresponds to only a few complete passes over the training sets of NMT tasks8. This is in contrast to the single-task systems that usually require a large number of epochs for convergence. We notice a general trend and observe that BERTGEN tends to outperform single-task systems usually after only a few passes over the corresponding training set. Many factors could be contributing to this observation such as sequence unrolling, multitasking, shared input space or relevant inductive biases transferred from M-BERT. We partly address these in the ablation studies (§ 3.4) and leave further investigation to future work.\nZero-shot performance. We use the DE↔FR test set from the WMT’19 shared task on news translation (Barrault et al., 2019) to assess the zeroshot translation capability of BERTGEN. This test set includes 1,701 sentences from news data regarding European Elections. We compare our results to two shared task systems, namely TARTU (baseline) and MSRA (state-of-the-art) (Barrault et al., 2019), after re-tokenising them accordingly with M-BERT9. Although BERTGEN is expected to obtain lower scores than the dedicated WMT systems due to the domain mismatch of the test set, we consider both the quantitative (Table 6) and the qualitative results (Table 7) extremely encouraging."
    }, {
      "heading" : "3.4 Ablation Studies",
      "text" : ""
    }, {
      "heading" : "3.4.1 Impact of initialisation",
      "text" : "We train single-task MMT systems on the MULTI30K EN→DE language pair. Specifically, we begin with a baseline system which is initialised with random weights. We then train a second baseline where only the visual processing layers are\n8For example, only ∼3 passes over SETIMES EN→TR. 9TARTU is the baseline and MSRA is the best performing\nsystem for the shared task\ntransferred from VL-BERT. Finally, we train a third baseline that is initialised similar to BERTGEN, i.e. using the hybrid initialisation (§ 2.1). Figure 5 compares the validation BLEU scores of these three systems. We observe that the benefits of knowledge transfer from pre-trained models are incrementally positive, however, BERTGEN’s hybrid initialisation outperforms the other two ablations."
    }, {
      "heading" : "3.4.2 Impact of multi-task training",
      "text" : "We now remove the multi-tasking aspect from BERTGEN to investigate the extent to which the performance improvements are related to other tasks. Similar to § 3.4.1, we focus on the MULTI30K EN→DE MMT task and train a single-task, hybridinitialised BERTGEN. Figure 6 compares the validation BLEU scores obtained by the default BERTGEN and the single-task variant. We observe that BERTGEN benefits from multi-task training and, more importantly, does not seem to exhibit patterns\nof catastrophic forgetting (French, 1999). Based on these observations, we expect similar model behavior to hold for other tasks."
    }, {
      "heading" : "4 Related Work",
      "text" : ""
    }, {
      "heading" : "4.1 Multimodal multilingual pre-training",
      "text" : "Research in NLP and related fields has been increasingly focusing on transfer learning approaches where a model is first pre-trained on a data-rich task, and then transferred to downstream tasks (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019). This framework presumably allows the model to capture useful inductive biases that generalise to a variety of NLP tasks, often after performing a task-specific fine-tuning (Raffel et al., 2020). Of these, the most relevant studies to our work are BERT (Devlin et al., 2019) and its multilingual version M-BERT, which pre-train a Transformer (Vaswani et al., 2017) on large monolingual corpora using the masked language modelling (MLM) objective.\nRecent research has also attempted to combine linguistic inputs with other modalities such as vision and speech, to achieve a grounded understanding of meaning. Successful approaches including LXMERT (Tan and Bansal, 2019), VL-BERT (Su et al., 2020) and others (Lu et al., 2019; Li et al., 2020a,b) achieve this by combining BERT’s MLM objective with auxiliary tasks such as masked region classification and image sentence matching, and pre-train their model on large-scale image captioning corpora (Chen et al., 2015; Sharma et al., 2018). Similarly, SpeechBERT extends BERT by jointly training on speech and text data (Chuang et al., 2020). Although SoTA results are reported by these approaches, they focus on unimodal and multimodal natural language understanding (NLU) tasks, with a strong emphasis in English. The backbone of BERTGEN combines VL-BERT (Su et al., 2020) with M-BERT (Devlin et al., 2019) to realise a multilingual and multimodal generator that can be used for a diverse set of generative tasks and languages rather than NLU tasks."
    }, {
      "heading" : "4.2 Pre-training for generative tasks",
      "text" : "Previous work has studied how to benefit from pre-trained BERT models in generative tasks such as NMT (Imamura and Sumita, 2019; Clinchant et al., 2019; Zhu et al., 2020). BERTGEN differs from these as it is not fine-tuned for a particular MT corpus and it exhibits multi-lingual and multimodal properties for general purpose generation.\nAnother related branch of work explores pretraining strategies specific to sequence-to-sequence tasks. This includes MASS (Song et al., 2019), which exploits an encoder-decoder framework with the MLM objective for task-specific generative pre-training and UniLM (Dong et al., 2019), which introduces uni-directional, bi-directional and sequence-to-sequence LM objectives by carefully adjusting the self-attention masks during training. Zhou et al. (2020) extend UniLM to vision & language pre-training using Conceptual Captions (Sharma et al., 2018) as the pre-training dataset. However, these models require a further fine-tuning step for generative tasks, unlike BERTGEN that is trained only once."
    }, {
      "heading" : "4.3 Multi-task learning for generation",
      "text" : "Several approaches exist for multi-task learning & generation (Dong et al., 2015; Luong et al., 2016) in NLP, especially in multilingual NMT, where tasks denote different language pairs (Zoph and\nKnight, 2016; Firat et al., 2016). The multi-task (and zero-shot) generation ability of BERTGEN is mostly inspired by Ha et al. (2016) and Johnson et al. (2017). Both of these introduced target language specifiers to select the output language when decoding translations from their model.\nOur multilingual & multimodal take on multitask generation is most similar to Kaiser et al. (2017), where a single Transformer model is trained on different tasks including image captioning, object classification, machine translation, speech recognition and parsing. However, their architecture depends on particular structures such as encoders, decoders, modality-specific networks and I/O mixers, unlike BERTGEN which does not require task-specific modules."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this paper, we presented BERTGEN, a novel generative, decoder-only model which extends BERT by combining multimodal and multilingual pretrained models. Our findings show that BERTGEN obtains strong performance on a variety of generative tasks and further generalises over unseen tasks. Importantly, our model demonstrates the potential for general-purpose (instead of task-specific) generation that is above and beyond the traditional pre-training and fine-tuning practices. BERTGEN is also parameter efficient as it has 89.3M total parameters and is trained on thirteen tasks encompassing MT, multimodal MT and image captioning. On the other hand, each of the single-task FAIRSEQ NMT baselines has 31.5M parameters.\nOur ablation studies show that BERTGEN is able to efficiently transfer relevant inductive biases from the pre-trained models and benefits from multi-task learning without suffering from catastrophic forgetting. We hope that these findings will motivate future research in exploiting more sophisticated pre-trained models in place of M-BERT and VLBERT and others."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This paper is a follow-up work to the MSc. Thesis of Faidon Mitzalis, co-supervised by Prof. Lucia Specia and Dr. Ozan Caglayan. Lucia Specia, Pranava Madhyastha and Ozan Caglayan received support from MultiMT project (H2020 ERC Starting Grant No. 678017). Lucia Specia also received support from the Air Force Office of Scientific Research (under award number FA8655-20-1-7006)."
    }, {
      "heading" : "A Qualitative Examples",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Bottom-up and top-down attention for image captioning and visual question answering",
      "author" : [ "Peter Anderson", "Xiaodong He", "Chris Buehler", "Damien Teney", "Mark Johnson", "Stephen Gould", "Lei Zhang." ],
      "venue" : "Proceedings of the IEEE conference on computer vi-",
      "citeRegEx" : "Anderson et al\\.,? 2018",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2018
    }, {
      "title" : "Findings of the 2018 conference on machine translation (WMT18)",
      "author" : [ "Ondřej Bojar", "Christian Federmann", "Mark Fishel", "Yvette Graham", "Barry Haddow", "Philipp Koehn", "Christof Monz." ],
      "venue" : "Proceedings of the Third Conference on Machine Trans-",
      "citeRegEx" : "Bojar et al\\.,? 2018",
      "shortCiteRegEx" : "Bojar et al\\.",
      "year" : 2018
    }, {
      "title" : "Simultaneous machine translation with visual context",
      "author" : [ "Ozan Caglayan", "Julia Ive", "Veneta Haralampieva", "Pranava Madhyastha", "Loı̈c Barrault", "Lucia Specia" ],
      "venue" : "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Caglayan et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Caglayan et al\\.",
      "year" : 2020
    }, {
      "title" : "Probing the need for visual context in multimodal machine translation",
      "author" : [ "Ozan Caglayan", "Pranava Madhyastha", "Lucia Specia", "Loı̈c Barrault" ],
      "venue" : "In Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Caglayan et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Caglayan et al\\.",
      "year" : 2019
    }, {
      "title" : "WIT3: Web inventory of transcribed and translated talks",
      "author" : [ "Mauro Cettolo", "Christian Girardi", "Marcello Federico." ],
      "venue" : "Proceedings of the 16th Annual conference of the European Association for Machine Translation, pages 261–268, Trento, Italy. Eu-",
      "citeRegEx" : "Cettolo et al\\.,? 2012",
      "shortCiteRegEx" : "Cettolo et al\\.",
      "year" : 2012
    }, {
      "title" : "Microsoft COCO Captions: Data Collection and Evaluation Server",
      "author" : [ "Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Dollar", "C. Lawrence Zitnick" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Distilling knowledge learned in BERT for text generation",
      "author" : [ "Yen-Chun Chen", "Zhe Gan", "Yu Cheng", "Jingzhou Liu", "Jingjing Liu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7893–7905, On-",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning phrase representations using RNN encoder–decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "SpeechBERT: An Audio-andText Jointly Learned Language Model for End-toEnd Spoken Question Answering",
      "author" : [ "Yung-Sung Chuang", "Chi-Liang Liu", "Hung yi Lee", "Lin shan Lee." ],
      "venue" : "Proc. Interspeech 2020, pages 4168–4172.",
      "citeRegEx" : "Chuang et al\\.,? 2020",
      "shortCiteRegEx" : "Chuang et al\\.",
      "year" : 2020
    }, {
      "title" : "On the use of BERT for neural machine translation",
      "author" : [ "Stephane Clinchant", "Kweon Woo Jung", "Vassilina Nikoulina." ],
      "venue" : "Proceedings of the 3rd Workshop on Neural Generation and Translation, pages 108–117, Hong Kong. Association for Com-",
      "citeRegEx" : "Clinchant et al\\.,? 2019",
      "shortCiteRegEx" : "Clinchant et al\\.",
      "year" : 2019
    }, {
      "title" : "Meteor universal: Language specific translation evaluation for any target language",
      "author" : [ "Michael Denkowski", "Alon Lavie." ],
      "venue" : "Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376–380, Baltimore, Maryland, USA. Association",
      "citeRegEx" : "Denkowski and Lavie.,? 2014",
      "shortCiteRegEx" : "Denkowski and Lavie.",
      "year" : 2014
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi-task learning for multiple language translation",
      "author" : [ "Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
      "citeRegEx" : "Dong et al\\.,? 2015",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2015
    }, {
      "title" : "Unified language model pre-training for natural language understanding and generation",
      "author" : [ "Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon." ],
      "venue" : "Advances in Neural Informa-",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Adversarial evaluation of multimodal machine translation",
      "author" : [ "Desmond Elliott." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2974–2978, Brussels, Belgium. Association for Computational Lin-",
      "citeRegEx" : "Elliott.,? 2018",
      "shortCiteRegEx" : "Elliott.",
      "year" : 2018
    }, {
      "title" : "Multi30K: Multilingual EnglishGerman image descriptions",
      "author" : [ "Desmond Elliott", "Stella Frank", "Khalil Sima’an", "Lucia Specia" ],
      "venue" : "In Proceedings of the 5th Workshop on Vision and Language,",
      "citeRegEx" : "Elliott et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Elliott et al\\.",
      "year" : 2016
    }, {
      "title" : "Multi-way, multilingual neural machine translation with a shared attention mechanism",
      "author" : [ "Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Firat et al\\.,? 2016",
      "shortCiteRegEx" : "Firat et al\\.",
      "year" : 2016
    }, {
      "title" : "Catastrophic forgetting in connectionist networks",
      "author" : [ "Robert M. French." ],
      "venue" : "Trends in Cognitive Sciences, 3(4):128–135.",
      "citeRegEx" : "French.,? 1999",
      "shortCiteRegEx" : "French.",
      "year" : 1999
    }, {
      "title" : "Toward Multilingual Neural Machine Translation with Universal Encoder and Decoder",
      "author" : [ "Thanh-Le Ha", "Jan Niehues", "Alex Waibel." ],
      "venue" : "Proceedings of the 13th International Conference on Spoken Language Translation.",
      "citeRegEx" : "Ha et al\\.,? 2016",
      "shortCiteRegEx" : "Ha et al\\.",
      "year" : 2016
    }, {
      "title" : "Recycling a pre-trained BERT encoder for neural machine translation",
      "author" : [ "Kenji Imamura", "Eiichiro Sumita." ],
      "venue" : "Proceedings of the 3rd Workshop on Neural Generation and Translation, pages 23–31, Hong Kong. Association for Computational Linguistics.",
      "citeRegEx" : "Imamura and Sumita.,? 2019",
      "shortCiteRegEx" : "Imamura and Sumita.",
      "year" : 2019
    }, {
      "title" : "Deep visualsemantic alignments for generating image descriptions",
      "author" : [ "Andrej Karpathy", "Li Fei-Fei." ],
      "venue" : "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 3128–3137.",
      "citeRegEx" : "Karpathy and Fei.Fei.,? 2015",
      "shortCiteRegEx" : "Karpathy and Fei.Fei.",
      "year" : 2015
    }, {
      "title" : "Unicoder-VL: A universal encoder for vision and language by cross-modal pretraining",
      "author" : [ "Gen Li", "Nan Duan", "Yuejian Fang", "Ming Gong", "Daxin Jiang." ],
      "venue" : "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-",
      "citeRegEx" : "Li et al\\.,? 2020a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Oscar: Object-Semantics Aligned Pretraining for Vision-Language Tasks",
      "author" : [ "Gao." ],
      "venue" : "Computer Vision – ECCV 2020, pages 121–137, Cham. Springer International Publishing.",
      "citeRegEx" : "Gao.,? 2020b",
      "shortCiteRegEx" : "Gao.",
      "year" : 2020
    }, {
      "title" : "Multimodality in Machine Translation",
      "author" : [ "Jindřich Libovický." ],
      "venue" : "Ph.D. thesis, Charles University, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics, Prague, Czechia.",
      "citeRegEx" : "Libovický.,? 2019",
      "shortCiteRegEx" : "Libovický.",
      "year" : 2019
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Knowing when to look: Adaptive attention via a visual sentinel for image captioning",
      "author" : [ "J. Lu", "C. Xiong", "D. Parikh", "R. Socher." ],
      "venue" : "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3242–3250.",
      "citeRegEx" : "Lu et al\\.,? 2017",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2017
    }, {
      "title" : "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "author" : [ "Jiasen Lu", "Dhruv Batra", "Devi Parikh", "Stefan Lee." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 13–23.",
      "citeRegEx" : "Lu et al\\.,? 2019",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2019
    }, {
      "title" : "12-in-1: Multi-task vision and language representation learning",
      "author" : [ "Jiasen Lu", "Vedanuj Goswami", "Marcus Rohrbach", "Devi Parikh", "Stefan Lee." ],
      "venue" : "The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Lu et al\\.,? 2020",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural baby talk",
      "author" : [ "Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7219–7228.",
      "citeRegEx" : "Lu et al\\.,? 2018",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-task Sequence to Sequence Learning",
      "author" : [ "Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Luong et al\\.,? 2016",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2016
    }, {
      "title" : "Learned in translation: Contextualized word vectors",
      "author" : [ "Bryan McCann", "James Bradbury", "Caiming Xiong", "Richard Socher." ],
      "venue" : "Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, page 6297–6308,",
      "citeRegEx" : "McCann et al\\.,? 2017",
      "shortCiteRegEx" : "McCann et al\\.",
      "year" : 2017
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chap-",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Re-",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Collecting image annotations using Amazon’s Mechanical Turk",
      "author" : [ "Cyrus Rashtchian", "Peter Young", "Micah Hodosh", "Julia Hockenmaier." ],
      "venue" : "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechan-",
      "citeRegEx" : "Rashtchian et al\\.,? 2010",
      "shortCiteRegEx" : "Rashtchian et al\\.",
      "year" : 2010
    }, {
      "title" : "Leveraging pre-trained checkpoints for sequence generation tasks",
      "author" : [ "Sascha Rothe", "Shashi Narayan", "Aliaksei Severyn." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:264–280.",
      "citeRegEx" : "Rothe et al\\.,? 2020",
      "shortCiteRegEx" : "Rothe et al\\.",
      "year" : 2020
    }, {
      "title" : "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
      "author" : [ "Piyush Sharma", "Nan Ding", "Sebastian Goodman", "Radu Soricut." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Sharma et al\\.,? 2018",
      "shortCiteRegEx" : "Sharma et al\\.",
      "year" : 2018
    }, {
      "title" : "MASS: Masked sequence to sequence pre-training for language generation",
      "author" : [ "Kaitao Song", "Xu Tan", "Tao Qin", "Jianfeng Lu", "TieYan Liu." ],
      "venue" : "Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Ma-",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "VL-BERT: Pretraining of Generic Visual-Linguistic Representations",
      "author" : [ "Weijie Su", "Xizhou Zhu", "Yue Cao", "Bin Li", "Lewei Lu", "Furu Wei", "Jifeng Dai." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Su et al\\.,? 2020",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2020
    }, {
      "title" : "Multimodal machine translation through visuals and speech",
      "author" : [ "Umut Sulubacak", "Ozan Caglayan", "Stig-Arne Grönroos", "Aku Rouhe", "Desmond Elliott", "Lucia Specia", "Jörg Tiedemann." ],
      "venue" : "Machine Translation, pages 1–51.",
      "citeRegEx" : "Sulubacak et al\\.,? 2020",
      "shortCiteRegEx" : "Sulubacak et al\\.",
      "year" : 2020
    }, {
      "title" : "Videobert: A joint model for video and language representation learning",
      "author" : [ "Chen Sun", "Austin Myers", "Carl Vondrick", "Kevin Murphy", "Cordelia Schmid." ],
      "venue" : "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7464–7473.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "LXMERT: Learning cross-modality encoder representations from transformers",
      "author" : [ "Hao Tan", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Tan and Bansal.,? 2019",
      "shortCiteRegEx" : "Tan and Bansal.",
      "year" : 2019
    }, {
      "title" : "Parallel data, tools and interfaces in OPUS",
      "author" : [ "Jörg Tiedemann." ],
      "venue" : "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12), pages 2214–2218, Istanbul, Turkey. European Language Resources Association",
      "citeRegEx" : "Tiedemann.,? 2012",
      "shortCiteRegEx" : "Tiedemann.",
      "year" : 2012
    }, {
      "title" : "Tasviret: A benchmark dataset for automatic turkish description generation from images",
      "author" : [ "Mesut Erhan Unal", "Begum Citamak", "Semih Yagcioglu", "Aykut Erdem", "Erkut Erdem", "Nazli Ikizler Cinbis", "Ruket Cakici." ],
      "venue" : "2016 24th Signal Processing",
      "citeRegEx" : "Unal et al\\.,? 2016",
      "shortCiteRegEx" : "Unal et al\\.",
      "year" : 2016
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Cider: Consensus-based image description evaluation",
      "author" : [ "Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4566–4575.",
      "citeRegEx" : "Vedantam et al\\.,? 2015",
      "shortCiteRegEx" : "Vedantam et al\\.",
      "year" : 2015
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards making the most of BERT in neural machine translation",
      "author" : [ "Jiacheng Yang", "Mingxuan Wang", "Hao Zhou", "Chengqi Zhao", "Weinan Zhang", "Yong Yu", "Lei Li." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages",
      "citeRegEx" : "Yang et al\\.,? 2020",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    }, {
      "title" : "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32. Curran",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "From image descriptions to visual",
      "author" : [ "Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier" ],
      "venue" : null,
      "citeRegEx" : "Young et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 2014
    }, {
      "title" : "Unified vision-language pre-training for image captioning and vqa",
      "author" : [ "Luowei Zhou", "Hamid Palangi", "Lei Zhang", "Houdong Hu", "Jason Corso", "Jianfeng Gao." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 34(07):13041–13049.",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    }, {
      "title" : "Incorporating BERT into neural machine translation",
      "author" : [ "Jinhua Zhu", "Yingce Xia", "Lijun Wu", "Di He", "Tao Qin", "Wengang Zhou", "Houqiang Li", "Tie-Yan Liu." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-source neural translation",
      "author" : [ "Barret Zoph", "Kevin Knight." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 30–34, San Diego, Cali-",
      "citeRegEx" : "Zoph and Knight.,? 2016",
      "shortCiteRegEx" : "Zoph and Knight.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Recent work in unsupervised and self-supervised pre-training has revolutionised the field of natural language understanding (NLU), resulting in high performance ceilings across multiple tasks (Devlin et al., 2019; Yang et al., 2019; Dong et al., 2019).",
      "startOffset" : 192,
      "endOffset" : 251
    }, {
      "referenceID" : 49,
      "context" : "Recent work in unsupervised and self-supervised pre-training has revolutionised the field of natural language understanding (NLU), resulting in high performance ceilings across multiple tasks (Devlin et al., 2019; Yang et al., 2019; Dong et al., 2019).",
      "startOffset" : 192,
      "endOffset" : 251
    }, {
      "referenceID" : 13,
      "context" : "Recent work in unsupervised and self-supervised pre-training has revolutionised the field of natural language understanding (NLU), resulting in high performance ceilings across multiple tasks (Devlin et al., 2019; Yang et al., 2019; Dong et al., 2019).",
      "startOffset" : 192,
      "endOffset" : 251
    }, {
      "referenceID" : 11,
      "context" : "The recent success of language model pre-training with masked language modelling (MLM) such as BERT (Devlin et al., 2019) further paved the way for more complex approaches that combine language pre-training with images (Tan and Bansal, 2019; Su et al.",
      "startOffset" : 100,
      "endOffset" : 121
    }, {
      "referenceID" : 42,
      "context" : ", 2019) further paved the way for more complex approaches that combine language pre-training with images (Tan and Bansal, 2019; Su et al., 2020; Lu et al., 2020), video (Sun et al.",
      "startOffset" : 105,
      "endOffset" : 161
    }, {
      "referenceID" : 39,
      "context" : ", 2019) further paved the way for more complex approaches that combine language pre-training with images (Tan and Bansal, 2019; Su et al., 2020; Lu et al., 2020), video (Sun et al.",
      "startOffset" : 105,
      "endOffset" : 161
    }, {
      "referenceID" : 27,
      "context" : ", 2019) further paved the way for more complex approaches that combine language pre-training with images (Tan and Bansal, 2019; Su et al., 2020; Lu et al., 2020), video (Sun et al.",
      "startOffset" : 105,
      "endOffset" : 161
    }, {
      "referenceID" : 41,
      "context" : ", 2020), video (Sun et al., 2019), and speech (Chuang et al.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 49,
      "context" : "Previous work argues that the MLM objective is ill-suited for generation tasks such as machine translation (Yang et al., 2019; Rothe et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 146
    }, {
      "referenceID" : 36,
      "context" : "Previous work argues that the MLM objective is ill-suited for generation tasks such as machine translation (Yang et al., 2019; Rothe et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 146
    }, {
      "referenceID" : 19,
      "context" : "Recent work in this direction has predominantly investigated the use of pre-trained models to either initialise Transformer-based encoderdecoder models (Imamura and Sumita, 2019; Clinchant et al., 2019; Yang et al., 2020; Rothe et al., 2020) or to distill knowledge for sequence generation tasks (Chen et al.",
      "startOffset" : 152,
      "endOffset" : 241
    }, {
      "referenceID" : 9,
      "context" : "Recent work in this direction has predominantly investigated the use of pre-trained models to either initialise Transformer-based encoderdecoder models (Imamura and Sumita, 2019; Clinchant et al., 2019; Yang et al., 2020; Rothe et al., 2020) or to distill knowledge for sequence generation tasks (Chen et al.",
      "startOffset" : 152,
      "endOffset" : 241
    }, {
      "referenceID" : 48,
      "context" : "Recent work in this direction has predominantly investigated the use of pre-trained models to either initialise Transformer-based encoderdecoder models (Imamura and Sumita, 2019; Clinchant et al., 2019; Yang et al., 2020; Rothe et al., 2020) or to distill knowledge for sequence generation tasks (Chen et al.",
      "startOffset" : 152,
      "endOffset" : 241
    }, {
      "referenceID" : 36,
      "context" : "Recent work in this direction has predominantly investigated the use of pre-trained models to either initialise Transformer-based encoderdecoder models (Imamura and Sumita, 2019; Clinchant et al., 2019; Yang et al., 2020; Rothe et al., 2020) or to distill knowledge for sequence generation tasks (Chen et al.",
      "startOffset" : 152,
      "endOffset" : 241
    }, {
      "referenceID" : 6,
      "context" : ", 2020) or to distill knowledge for sequence generation tasks (Chen et al., 2020).",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 39,
      "context" : "The latter features are achieved by transferring knowledge from state-of-the-art pretrained models, namely VL-BERT (Su et al., 2020) and multilingual BERT (M-BERT) (Devlin et al.",
      "startOffset" : 115,
      "endOffset" : 132
    }, {
      "referenceID" : 11,
      "context" : ", 2020) and multilingual BERT (M-BERT) (Devlin et al., 2019).",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 39,
      "context" : "This involves using the VL-BERT (Su et al., 2020) checkpoint and initialising the word embeddings, the Transformer weights and the MLM head with M-BERT (Devlin et al.",
      "startOffset" : 32,
      "endOffset" : 49
    }, {
      "referenceID" : 11,
      "context" : ", 2020) checkpoint and initialising the word embeddings, the Transformer weights and the MLM head with M-BERT (Devlin et al., 2019).",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 0,
      "context" : "After preextracting the 2048-dimensional RoI features using the bottom-up-top-down object detector (Anderson et al., 2018), we keep between 10 and 100 (i.",
      "startOffset" : 99,
      "endOffset" : 122
    }, {
      "referenceID" : 45,
      "context" : "Given that a single Transformer (Vaswani et al., 2017) performs both encoding and decoding, sequence unrolling affects selfattention as well (Figure 3).",
      "startOffset" : 32,
      "endOffset" : 54
    }, {
      "referenceID" : 18,
      "context" : "Finally, to select the language during generation, input sequences begin with special target language specifiers (Ha et al., 2016; Johnson et al., 2017) (Figure 1).",
      "startOffset" : 113,
      "endOffset" : 152
    }, {
      "referenceID" : 24,
      "context" : "We use AdamW optimiser (Loshchilov and Hutter, 2019) with base learning rate set to 1.",
      "startOffset" : 23,
      "endOffset" : 52
    }, {
      "referenceID" : 50,
      "context" : "Specifically, we use the FLICKR30K dataset (Young et al., 2014) that provides 29K training images, each with five English captions collected through crowd-sourcing.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 15,
      "context" : "We use the MULTI30K dataset (Elliott et al., 2016), which annotates FLICKR30K images with five German captions.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 44,
      "context" : "Finally, we use the TASVIRET dataset (Unal et al., 2016) which provides two Turkish captions for each of the 8,092 images in the FLICKR8K dataset (Rashtchian et al.",
      "startOffset" : 37,
      "endOffset" : 56
    }, {
      "referenceID" : 35,
      "context" : ", 2016) which provides two Turkish captions for each of the 8,092 images in the FLICKR8K dataset (Rashtchian et al., 2010).",
      "startOffset" : 97,
      "endOffset" : 122
    }, {
      "referenceID" : 5,
      "context" : "To evaluate BERTGEN’s performance on IC, we compare it against previous work with strong performance on COCO (Chen et al., 2015) and FLICKR30K.",
      "startOffset" : 109,
      "endOffset" : 128
    }, {
      "referenceID" : 25,
      "context" : "More precisely, ADAPTIVE ATTENTION (SENTINEL) (Lu et al., 2017), which uses a sentinel token to distinguish between visual and non-visual representations, and NEURAL BABY TALK (NBT), which follows a slot-filling approach through explicit object region information (Lu et al.",
      "startOffset" : 46,
      "endOffset" : 63
    }, {
      "referenceID" : 28,
      "context" : ", 2017), which uses a sentinel token to distinguish between visual and non-visual representations, and NEURAL BABY TALK (NBT), which follows a slot-filling approach through explicit object region information (Lu et al., 2018).",
      "startOffset" : 208,
      "endOffset" : 225
    }, {
      "referenceID" : 40,
      "context" : "Multimodal Machine Translation (MMT) attempts to improve MT quality by incorporating information from modalities other than language (Sulubacak et al., 2020).",
      "startOffset" : 133,
      "endOffset" : 157
    }, {
      "referenceID" : 2,
      "context" : "For a comprehensive comparison with previous work, we train a SoTA recurrent MMT (Caglayan et al., 2020) solely on the MULTI30K dataset, which applies a secondary (visual) attention in the decoder over the RoI features i.",
      "startOffset" : 81,
      "endOffset" : 104
    }, {
      "referenceID" : 7,
      "context" : "There are two GRU (Cho et al., 2014) layers in both the encoder and the decoder and the embedding & hidden dimensions in the model are set to 200 and 320, respectively.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 23,
      "context" : "Besides the state-of-the-art constrained recurrent MMT model described above, we further compare BERTGEN – which is trained on various other MT and IC corpora – to an unconstrained Transformer-based MMT trained on ∼9M additional EN→DE sentences (Libovický, 2019)4 in addition to MULTI30K.",
      "startOffset" : 245,
      "endOffset" : 262
    }, {
      "referenceID" : 4,
      "context" : "We use EN↔DE and EN↔FR MT datasets from IWSLT’14 (Cettolo et al., 2012) which consists of TED Talks’ subtitles and their translations.",
      "startOffset" : 49,
      "endOffset" : 71
    }, {
      "referenceID" : 31,
      "context" : "We take the prepare-iwslt14 recipe from FAIRSEQ (Ott et al., 2019) to prepare the dev and test sets.",
      "startOffset" : 48,
      "endOffset" : 66
    }, {
      "referenceID" : 43,
      "context" : "For EN↔TR directions, we use the SETIMES2 (Tiedemann, 2012) news dataset for training.",
      "startOffset" : 42,
      "endOffset" : 59
    }, {
      "referenceID" : 1,
      "context" : "For development and test sets, we take the official WMT test sets (Bojar et al., 2018), namely, newstest2016 and newstest2017 as the development",
      "startOffset" : 66,
      "endOffset" : 86
    }, {
      "referenceID" : 45,
      "context" : "Finally, for each translation direction, we train a Transformer NMT model (Vaswani et al., 2017) using the IWSLT-DE-EN recipe of the FAIRSEQ toolkit (Ott et al.",
      "startOffset" : 74,
      "endOffset" : 96
    }, {
      "referenceID" : 31,
      "context" : ", 2017) using the IWSLT-DE-EN recipe of the FAIRSEQ toolkit (Ott et al., 2019).",
      "startOffset" : 60,
      "endOffset" : 78
    }, {
      "referenceID" : 32,
      "context" : "We compute tokenised5 BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and CIDEr (Vedantam et al.",
      "startOffset" : 27,
      "endOffset" : 50
    }, {
      "referenceID" : 10,
      "context" : ", 2002), METEOR (Denkowski and Lavie, 2014) and CIDEr (Vedantam et al.",
      "startOffset" : 16,
      "endOffset" : 43
    }, {
      "referenceID" : 46,
      "context" : ", 2002), METEOR (Denkowski and Lavie, 2014) and CIDEr (Vedantam et al., 2015) using cococaption6.",
      "startOffset" : 54,
      "endOffset" : 77
    }, {
      "referenceID" : 28,
      "context" : ", 2017) and NBT (Lu et al., 2018), even though they use beam search for decoding.",
      "startOffset" : 16,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : "On COCO (Chen et al., 2015), an image captioning corpus much larger and diverse than FLICKR30K, we evaluate BERTGEN on Karpathy’s test split (Karpathy and FeiFei, 2015) and notice that the scores are reasonable",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 2,
      "context" : "forms the Transformer-based FAIRSEQ NMT models and the recurrent MMT (Caglayan et al., 2020) models on both the EN→DE and the EN→FR language pairs.",
      "startOffset" : 69,
      "endOffset" : 92
    }, {
      "referenceID" : 23,
      "context" : "strained MMT (Libovický, 2019) model trained on a ∼6x larger parallel corpus.",
      "startOffset" : 13,
      "endOffset" : 30
    }, {
      "referenceID" : 3,
      "context" : "This indicates that the features are not ignored at all, unlike in (Caglayan et al., 2019), where they showed that sequence-to-sequence MMT models can learn to ignore the images when the linguistic signal is sufficient to perform the task.",
      "startOffset" : 67,
      "endOffset" : 90
    }, {
      "referenceID" : 30,
      "context" : "Research in NLP and related fields has been increasingly focusing on transfer learning approaches where a model is first pre-trained on a data-rich task, and then transferred to downstream tasks (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019).",
      "startOffset" : 195,
      "endOffset" : 258
    }, {
      "referenceID" : 33,
      "context" : "Research in NLP and related fields has been increasingly focusing on transfer learning approaches where a model is first pre-trained on a data-rich task, and then transferred to downstream tasks (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019).",
      "startOffset" : 195,
      "endOffset" : 258
    }, {
      "referenceID" : 11,
      "context" : "Research in NLP and related fields has been increasingly focusing on transfer learning approaches where a model is first pre-trained on a data-rich task, and then transferred to downstream tasks (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019).",
      "startOffset" : 195,
      "endOffset" : 258
    }, {
      "referenceID" : 34,
      "context" : "This framework presumably allows the model to capture useful inductive biases that generalise to a variety of NLP tasks, often after performing a task-specific fine-tuning (Raffel et al., 2020).",
      "startOffset" : 172,
      "endOffset" : 193
    }, {
      "referenceID" : 11,
      "context" : "Of these, the most relevant studies to our work are BERT (Devlin et al., 2019) and its multilingual version M-BERT, which pre-train a Transformer (Vaswani et al.",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 45,
      "context" : ", 2019) and its multilingual version M-BERT, which pre-train a Transformer (Vaswani et al., 2017) on large monolingual corpora using the masked language modelling (MLM) objective.",
      "startOffset" : 75,
      "endOffset" : 97
    }, {
      "referenceID" : 42,
      "context" : "Successful approaches including LXMERT (Tan and Bansal, 2019), VL-BERT (Su et al.",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 39,
      "context" : "Successful approaches including LXMERT (Tan and Bansal, 2019), VL-BERT (Su et al., 2020) and others (Lu et al.",
      "startOffset" : 71,
      "endOffset" : 88
    }, {
      "referenceID" : 5,
      "context" : ", 2020a,b) achieve this by combining BERT’s MLM objective with auxiliary tasks such as masked region classification and image sentence matching, and pre-train their model on large-scale image captioning corpora (Chen et al., 2015; Sharma et al., 2018).",
      "startOffset" : 211,
      "endOffset" : 251
    }, {
      "referenceID" : 37,
      "context" : ", 2020a,b) achieve this by combining BERT’s MLM objective with auxiliary tasks such as masked region classification and image sentence matching, and pre-train their model on large-scale image captioning corpora (Chen et al., 2015; Sharma et al., 2018).",
      "startOffset" : 211,
      "endOffset" : 251
    }, {
      "referenceID" : 8,
      "context" : "Similarly, SpeechBERT extends BERT by jointly training on speech and text data (Chuang et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 39,
      "context" : "The backbone of BERTGEN combines VL-BERT (Su et al., 2020) with M-BERT (Devlin et al.",
      "startOffset" : 41,
      "endOffset" : 58
    }, {
      "referenceID" : 11,
      "context" : ", 2020) with M-BERT (Devlin et al., 2019) to realise a multilingual and multimodal generator that can be used for a diverse set of generative tasks and languages rather than NLU tasks.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 19,
      "context" : "Previous work has studied how to benefit from pre-trained BERT models in generative tasks such as NMT (Imamura and Sumita, 2019; Clinchant et al., 2019; Zhu et al., 2020).",
      "startOffset" : 102,
      "endOffset" : 170
    }, {
      "referenceID" : 9,
      "context" : "Previous work has studied how to benefit from pre-trained BERT models in generative tasks such as NMT (Imamura and Sumita, 2019; Clinchant et al., 2019; Zhu et al., 2020).",
      "startOffset" : 102,
      "endOffset" : 170
    }, {
      "referenceID" : 52,
      "context" : "Previous work has studied how to benefit from pre-trained BERT models in generative tasks such as NMT (Imamura and Sumita, 2019; Clinchant et al., 2019; Zhu et al., 2020).",
      "startOffset" : 102,
      "endOffset" : 170
    }, {
      "referenceID" : 38,
      "context" : "This includes MASS (Song et al., 2019), which exploits an encoder-decoder framework with the MLM objective for task-specific generative pre-training and UniLM (Dong et al.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 13,
      "context" : ", 2019), which exploits an encoder-decoder framework with the MLM objective for task-specific generative pre-training and UniLM (Dong et al., 2019), which introduces uni-directional, bi-directional and sequence-to-sequence LM objectives by carefully adjusting the self-attention masks during training.",
      "startOffset" : 128,
      "endOffset" : 147
    }, {
      "referenceID" : 37,
      "context" : "(2020) extend UniLM to vision & language pre-training using Conceptual Captions (Sharma et al., 2018) as the pre-training dataset.",
      "startOffset" : 80,
      "endOffset" : 101
    }, {
      "referenceID" : 12,
      "context" : "Several approaches exist for multi-task learning & generation (Dong et al., 2015; Luong et al., 2016) in NLP, especially in multilingual NMT, where tasks denote different language pairs (Zoph and Knight, 2016; Firat et al.",
      "startOffset" : 62,
      "endOffset" : 101
    }, {
      "referenceID" : 29,
      "context" : "Several approaches exist for multi-task learning & generation (Dong et al., 2015; Luong et al., 2016) in NLP, especially in multilingual NMT, where tasks denote different language pairs (Zoph and Knight, 2016; Firat et al.",
      "startOffset" : 62,
      "endOffset" : 101
    }, {
      "referenceID" : 53,
      "context" : ", 2016) in NLP, especially in multilingual NMT, where tasks denote different language pairs (Zoph and Knight, 2016; Firat et al., 2016).",
      "startOffset" : 92,
      "endOffset" : 135
    }, {
      "referenceID" : 16,
      "context" : ", 2016) in NLP, especially in multilingual NMT, where tasks denote different language pairs (Zoph and Knight, 2016; Firat et al., 2016).",
      "startOffset" : 92,
      "endOffset" : 135
    } ],
    "year" : 2021,
    "abstractText" : "We present BERTGEN, a novel generative, decoder-only model which extends BERT by fusing multimodal and multilingual pretrained models VL-BERT and M-BERT, respectively. BERTGEN is auto-regressively trained for language generation tasks, namely image captioning, machine translation and multimodal machine translation, under a multitask setting. With a comprehensive set of evaluations, we show that BERTGEN outperforms many strong baselines across the tasks explored. We also show BERTGEN’s ability for zero-shot language generation, where it exhibits competitive performance to supervised counterparts. Finally, we conduct ablation studies which demonstrate that BERTGEN substantially benefits from multi-tasking and effectively transfers relevant inductive biases from the pre-trained models.",
    "creator" : "LaTeX with hyperref"
  }
}