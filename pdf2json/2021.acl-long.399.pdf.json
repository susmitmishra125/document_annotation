{
  "name" : "2021.acl-long.399.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Structural Pre-training for Dialogue Comprehension",
    "authors" : [ "Zhuosheng Zhang", "Hai Zhao" ],
    "emails" : [ "zhangzs@sjtu.edu.cn,", "zhaohai@cs.sjtu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5134–5145\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5134"
    }, {
      "heading" : "1 Introduction",
      "text" : "Recent advances in large-scale pre-training language models (PrLMs) have achieved remarkable successes in a variety of natural language processing (NLP) tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019a; Yang et al., 2019; Clark et al., 2020; Zhang et al., 2020d). Providing fine-grained contextualized embedding, these pre-trained models are widely employed as encoders for various downstream NLP tasks. Although the PrLMs demonstrate superior perfor-\n∗ Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), Key Projects of National Natural Science Foundation of China (U1836222 and 61733011), Huawei-SJTU long term AI project, Cuttingedge Machine Reading Comprehension and Language Model. This work was supported by Huawei Noah’s Ark Lab.\nmance due to their strong representation ability from self-supervised pre-training, it is still challenging to effectively adapt task-related knowledge during the detailed task-specific training which is usually in a way of fine-tuning (Gururangan et al., 2020). Generally, those PrLMs handle the whole input text as a linear sequence of successive tokens and implicitly capture the contextualized representations of those tokens through self-attention. Such fine-tuning paradigm of exploiting PrLMs would be suboptimal to model dialogue task which holds exclusive text features that plain text for PrLM training may hardly embody. Therefore, we explore a fundamental way to alleviate this difficulty by improving the training of PrLM. This work devotes itself to designing the natural way of adapting the language modeling to the dialogue scenario motivated by the natural characteristics of dialogue contexts.\nAs an active research topic in the NLP field, multi-turn dialogue modeling has attracted great interest. The typical task is response selection (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018) that aims to select the appropriate response according to a given dialogue context containing a number of utterances, which is the focus in this work. How-\never, selecting a coherent and informative response for a given dialogue context remains a challenge. The multi-turn dialogue typically involves two or more speakers that engage in various conversation topics, intentions, thus the utterances are rich in interactions, e.g., with criss-cross discourse structures (Li et al., 2020a; Bai and Zhao, 2018; Qin et al., 2016, 2017). A critical challenge is the learning of rich and robust context representations and interactive relationships of dialogue utterances, so that the resulting model is capable of adequately capturing the semantics of each utterance, and the relationships among all the utterances inside the dialogue.\nInspired by the effectiveness for learning universal language representations of PrLMs, there are increasing studies that employ PrLMs for conversation modeling (Mehri et al., 2019; Zhang et al., 2020b; Rothe et al., 2020; Whang et al., 2020; Han et al., 2021). These studies typically model the response selection with only the context-response matching task and overlook many potential training signals contained in dialogue data. Although the PrLMs have learned contextualized semantic representation from token-level or sentence-level pre-training tasks like MLM, NSP, they all do not consider dialogue related features like speaker role, continuity and consistency. One obvious issue of these approaches is that the relationships between utterances are harder to capture using word-level semantics. Besides, some latent features, such as user intent and conversation topic, are under-discovered in existing works (Xu et al., 2021). Therefore, the response retrieved by existing dialogue systems supervised by the conventional way still faces critical challenges, including incoherence and inconsistency.\nIn this work, we present SPIDER (Structural PretraIned DialoguE Reader), a structural language modeling method to capture dialogue exclusive features. Motivated to efficiently and explicitly model the coherence among utterances and the key facts in each utterance, we propose two training objectives in analogy to the original BERT-like language model (LM) training: 1) utterance order restoration (UOR), which predicts the order of the permuted utterances in dialogue context; 2) sentence backbone regularization (SBR), which regularizes the model to improve the factual correctness of summarized subject-verb-object (SVO) triplets. Experimental results on widely used benchmarks show\nthat SPDER boosts the model performance for various multi-turn dialogue comprehension tasks including response selection and dialogue reasoning."
    }, {
      "heading" : "2 Background and Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Pre-trained Language Models",
      "text" : "Recent works have explored various architecture choices and training objectives for large-scale LM pre-training (Zhou et al., 2020b,a; Xu et al., 2020a,b; Li et al., 2021, 2020b). Most of the PrLMs are based on the encoder in Transformer, among which Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019b) is one of the most representative work. BERT uses multiple layers of stacked Transformer Encoder to obtain contextualized representations of the language at different levels. BERT has helped achieve great performance improvement in a broad range of NLP tasks (Bai and Zhao, 2018; Zhang et al., 2020a; Luo and Zhao, 2020; Zhang et al., 2021). Several subsequent variants have been proposed to further enhance the capacity of PrLMs, such as XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020), ELECTRA (Clark et al., 2020). For simplicity and convenient comparison with public studies, we select the most widely used BERT as the backbone in this work.\nThere are two ways of training PrLMs on dialogue scenarios, including open-domain pretraining and domain-adaptive post-training. Some studies perform training on open-domain conversational data like Reddit for response selection or generation tasks (Wolf et al., 2019; Zhang et al., 2020c; Henderson et al., 2020; Bao et al., 2020), but they are limited to the original pre-training tasks and ignore the dialogue related features. For domain-adaptive post-training, prior works have indicated that the order information would be important in the text representation, and the well-known next-sentence-prediction (Devlin et al., 2019b) and sentence-order-prediction (Lan et al., 2020) can be viewed as special cases of order prediction. Especially in the dialogue scenario, predicting the word order of utterance, as well as the utterance order in the context, has shown effectiveness in the dialogue generation task (Kumar et al., 2020; Gu et al., 2020b), where the order information is well recognized (Chen et al., 2019). However, there is little attention paid to dialogue comprehension tasks such as response selection (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018). The potential difficulty\nis that utterance order restoration involves much more ordering possibilities for utterances that may have a quite flexible order inside dialogue text than NSP and SOP which only handle the predication of two-class ordering.\nOur work is also profoundly related to auxiliary multi-task learning, whose common theme is to guide the language modeling Transformers with explicit knowledge and complementing objectives (Zhang et al., 2019; Sun et al., 2019b; Xu et al., 2020a). A most related work is Xu et al. (2020a), which introduces four self-supervised tasks including next session prediction, utterance restoration, incoherence detection and consistency discrimination. Our work differs from Xu et al. (2020a) by three sides. 1) Motivation: our method is designed for a general-purpose in broad dialogue comprehension tasks whose goals may be either utterancelevel discourse coherence or inner-utterance factual correctness, instead of only motivated for downstream context-response matching, whose goal is to measure if two sequences are related or not. 2) Technique: we propose both sides of intra- and inter- utterance objectives. In contrast, the four objectives proposed in Xu et al. (2020a) are natural variants of NSP in BERT, which are all utterancelevel. 3) Training: we empirically evaluate domainadaptive training and multi-task learning, instead of only employing multi-task learning, which requires many efforts of optimizing coefficients in the loss functions, which would be time-consuming.\nIn terms of factual backbone modeling, compared with the existing studies that enhance the PrLMs by annotating named entities or incorporating external knowledge graphs (Eric et al., 2017; Liu et al., 2018), the SVO triplets extracted in our sentence backbone predication objective (SBP) method, appear more widely in the text itself. Such triplets ensure the correctness of SVO and enable our model to discover the salient facts from the lengthy texts, sensing the intuition of “who did what”."
    }, {
      "heading" : "2.2 Multi-turn Dialogue Comprehension",
      "text" : "Multi-turn dialogue comprehension aims to teach machines to read dialogue contexts and solve tasks such as response selection (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018) and answering questions (Sun et al., 2019a; Cui et al., 2020), whose common application is building intelligent humancomputer interactive systems (Chen et al., 2017a;\nShum et al., 2018; Li et al., 2017; Zhu et al., 2018b). Early studies mainly focus on the matching between the dialogue context and question (Huang et al., 2019; Zhu et al., 2018a). Recently, inspired by the impressive performance of PrLMs, the mainstream is employing PrLMs to handle the whole input texts of context and question, as a linear sequence of successive tokens and implicitly capture the contextualized representations of those tokens through self-attention (Qu et al., 2019; Liu et al., 2020). Such a way of modeling would be suboptimal to capture the high-level relationships between utterances in the dialogue history. In this work, we are motivated to model the structural relationships between utterances from utterance order restoration and the factual correctness inside each utterance in the perspective of language modeling pre-training instead of heuristically stacking deeper model architectures."
    }, {
      "heading" : "3 Approach",
      "text" : "This section presents our proposed method SPIDER (Structural Pre-traIned DialoguE Reader). First, we will present the standard dialogue comprehension model as the backbone. Then, we will introduce our designed language modeling objectives for dialogue scenarios, including utterance order restoration (UOR) and sentence backbone regularization (SBR). In terms of model training, we employ two strategies, i.e., 1) domain adaptive post-training that first trains a language model based on newly proposed objectives and then finetunes the response selection task; 2) multi-task finetuning that trains the model for downstream tasks, along with LM objectives."
    }, {
      "heading" : "3.1 Transformer Encoder",
      "text" : "We first employ a pre-trained language model such as BERT (Devlin et al., 2019a) to obtain the initial word representations. The utterances and response are concatenated and then fed into the encoder. Given the context C and response R, we concatenate all utterances in the context and the response candidate as a single consecutive token sequence with special tokens separating them: X = {[CLS]R[SEP]U1[EOU] . . .[EOU]Un[SEP]}, where [CLS] and [SEP] are special tokens. [EOU] is the “End Of Utterance” tag designed for multiturn context. X is then fed into the BERT encoder, which is a deep multi-layer bidirectional Transformer, to obtain a contextualized\nrepresentation H . In detail, let X = {x1, . . . , xn} be the embedding of the sequence, which are features of encoding sentence words of length n. The input embeddings are then fed into the multi-head attention layer to obtain the contextual representations.\nThe embedding sequence X is processed to a multi-layer bidirectional Transformer for learning contextualized representations, which is defined as\nH = FFN(MultiHead(K,Q, V )), (1)\nwhere K,Q,V are packed from the input sequence representation X . As the common practice, we set K = Q = V in the implementation.\nFor the following part, we use H = {h1, . . . , hn} to denote the last-layer hidden states of the input sequence."
    }, {
      "heading" : "3.2 SPIDER Training Objectives",
      "text" : "To simulate the dialogue-like features, we propose two pre-training objectives in addition to the original LM objectives: 1) utterance order restoration, which predicts the order of the permuted utterances in dialogue context; 2) sentence backbone regularization, which regularizes the model to improve the factual correctness of summarized subject-verbobject triplets. The utterance manipulations are shown in Figure 2. The following subsections describe the objectives in turn."
    }, {
      "heading" : "3.2.1 Utterance Order Restoration",
      "text" : "Coherence is an essential aspect of conversation modeling. In a coherent discourse, utterances should respect specific orders of relations and logic. The ordering of utterances in a dialogue context determines the semantic of the conversation. Therefore, learning to order a set of disordered utterances\nin such a way that maximizes the discourse coherence will have a critical impact in learning the representation of dialogue contexts.\nHowever, most previous studies focused on semantic relevance between context and response candidate. Here we introduce utterance-level position modeling, i.e., utterance order restoration to encourage the model to be aware of the semantic connections among utterances in the context. The idea is similar to autoencoding (AE) which aims to reconstruct the original data from corrupted input (Yang et al., 2019). Given permuted dialogue contexts that comprise utterances in random orders, we maximize the expected log-likelihood of a sequence of the original ground-truth order.\nThe goal of the utterance order restoration is to organize randomly shuffled utterances of a conversation into a coherent dialogue context. We extract the hidden states of [EOU] from H as the representation of each utterance. Formally, given an utterance sequence denoted as C ′ = [Hu1 ;Hu2 ; . . . ;HuK ] with order o = [o1; o2; . . . ; oK ], where K means the number of maximum positions to be predicted. We expect an ordered context C∗ = [uo∗1 ;uo∗2 ; . . . ;uo∗K ] is the most coherent permutation of utterances.\nAs predicting the permuted orders is a more challenging optimization problem than NSP and SOP tasks due to the large searching space of permutations and causes slow convergence in preliminary experiments, we choose to only predict the order of the last few permuted utterances by a permutation ratio δ to control the maximum number of permutations: K ′ = K ∗ δ. The UOR training objective is then formed as:\nLuor = − K′∑ k=1 [ok log ôk] , (2)\nwhere ôk denotes the predicted order."
    }, {
      "heading" : "3.2.2 Sentence Backbone Regularization",
      "text" : "The sentence backbone regularization objective is motivated to guide the model to distinguish the internal relation of the fact triplets that are extracted from each utterance, which would be helpful to improve the ability to capture the key facts of the utterance as well as the correctness. First, we apply a fact extractor to conduct the dependency parsing of each sentence. After that, we extract the subject, the root verb, and the object tokens as an SVO triplet corresponding to each utterance. Inspired by Bordes et al. (2013) where the embedding of the tail entity should be close to the embedding of the head entity plus some vector that depends on the relationship, we assume that given the dialogue input, in the hidden representation space, the summation of the subject and the verb should be close to the object as much as possible, i.e.,\nhsubject + hverb → hobject. (3)\nConsequently, based on the sequence hidden states hi where i = 1, ..., Ly, we introduce a regularization for the extracted facts:\nLsbr = m∑ k=1 (1− cos(hsubjk +hverbk , hobjk)), (4)\nwherem is the total number of fact tuples extracted from the summary and k indicates the k-th triplet. “subjk”, “verbk”, and “objk” are indexes of the k-th fact tuple’s subject, verb, and object.\nIn our implementation, since PrLMs take subwords as input while the SVO extraction performs in word-level, we use the first-token hidden state as the representation of the original word following the way in Devlin et al. (2019a) for named entity recognition."
    }, {
      "heading" : "4 Use of SPIDER Objectives",
      "text" : "In this section, we introduce two training methods to take the newly proposed language modeling objectives into account, namely domain-adaptive post-training and multi-task fine-tuning, as illustrated in Figure 3."
    }, {
      "heading" : "4.1 Domain Adaptive Post-training",
      "text" : "Similar to BERT, we also adopt the masked language model (MLM) and the next sentence prediction (NSP) as LM-training tasks to enable our model to capture lexical and syntactic information\nfrom tokens in text. More details of the LM training tasks can be found from Devlin et al. (2019a). The overall post-training loss is the sum of the MLM, NSP, UOR, and SBR loss.\nOur full model is trained by a joint loss by combining both of the objectives above:\nL = λ1(Lmlm + Lnsp) + λ2Luor + λ3Lsbr, (5)\nwhere λ1, λ2, λ3 are hyper-parameters. After post-training the language model on the dialogue corpus, we load the pre-trained weights as the same way of using BERT (Devlin et al., 2019a), to fine-tune the downstream tasks such as response selection and dialogue reasoning as focused in this work (details in Section 5.1)."
    }, {
      "heading" : "4.2 Multi-task Fine-tuning",
      "text" : "Since our objectives can well share the same input as the downstream tasks, there is an efficient way of using multi-task fine-tuning (MTF) to directly train the task-specific models along with our SPIDER objectives. Therefore, we feed the permuted context to the dialogue comprehension model and combine the three losses for training:\nL = β1Ldm + β2Luor + β3Lsbr, (6)\nwhere β1, β2, β3 are hyper-parameters. In order to train a task-specific model for dialogue comprehension, the hidden states H will be fed into a classifier with a fully connected and softmax layer. We learn model g(·, ·) by minimizing cross entropy loss with dataset D. Let Θ denote the parameters, for binary classification like the response selection task, the objective function L(D,Θ) can be formulated as:\nLdm = − N∑ i=1 [yi log(g(ci, ri))+\n(1− yi) log(1− g(ci, ri))].\nwhere N denotes the number of examples. For multiple choice task like MuTual, the loss function is:\nLdm = − N∑ i=1 C∑ k=1 yi,c log(g(ci, ri,k)).\nwhere C is the number of choice."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Datasets",
      "text" : "We evaluated our model on two English datasets: Ubuntu Dialogue Corpus (Ubuntu) (Lowe et al., 2015) and Multi-Turn Dialogue Reasoning (MuTual) (Cui et al., 2020),1 and two Chinese datasets: Douban Conversation Corpus (Douban) (Wu et al., 2017) and E-commerce Dialogue Corpus (ECD) (Zhang et al., 2018)."
    }, {
      "heading" : "5.1.1 Ubuntu Dialogue Corpus",
      "text" : "Ubuntu (Lowe et al., 2015) consists of English multi-turn conversations about technical support collected from chat logs of the Ubuntu forum. The dataset contains 1 million context-response pairs, 0.5 million for validation and 0.5 million for testing. In training set, each context has one positive response generated by human and one negative response sampled randomly. In validation and test sets, for each context, there are 9 negative responses and 1 positive response.\n1Actually, MuTual is a retrieval-based dialogue corpus in form, but the theme is English listening comprehension exams, thus we regard as a reading comprehension corpus in this work. Because the test set of MuTual is not publicly available, we conducted the comparison with our baselines on the Dev set for convenience."
    }, {
      "heading" : "5.1.2 Douban Conversation Corpus",
      "text" : "Douban (Wu et al., 2017) is different from Ubuntu in the following ways. First, it is an open domain where dialogues are extracted from Douban Group. Second, response candidates on the test set are collected by using the last turn as the query to retrieve 10 response candidates and labeled by humans. Third, there could be more than one correct response for a context."
    }, {
      "heading" : "5.1.3 E-commerce Dialogue Corpus",
      "text" : "ECD (Zhang et al., 2018) dataset is extracted from conversations between customer and service staff on Taobao. It contains over 5 types of conversations based on over 20 commodities. There are also 1 million context-response pairs in the training set, 0.5 million in the validation set, and 0.5 million in the test set."
    }, {
      "heading" : "5.1.4 Multi-Turn Dialogue Reasoning",
      "text" : "MuTual (Cui et al., 2020) consists of 8860 manually annotated dialogues based on Chinese student English listening comprehension exams. For each context, there is one positive response and three negative responses. The difference compared to the above three datasets is that only MuTual is reasoning-based. There are more than 6 types of reasoning abilities reflected in MuTual."
    }, {
      "heading" : "5.2 Implementation Details",
      "text" : "For the sake of computational efficiency, the maximum number of utterances is specialized as 20. The concatenated context, response, [CLS] and [SEP] in one sample is truncated according to the “longest first” rule or padded to a certain length, which is 256 for MuTual and 384 for the other three datasets. For the hyper-parameters, we empirically set λ1 = λ2 = λ3 = β1 = β2 = 1.\nOur model is implemented using Pytorch and based on the Transformer Library.2 We use BERT (Devlin et al., 2019a) as our backbone model. AdamW (Loshchilov and Hutter, 2019) is used as our optimizer. The batch size is 24 for MuTual, and 64 for others. The initial learning rate is 4× 10−6 for MuTual and 3 × 10−5 for others. The ratio is set to 0.4 in our implementation by default. We run 3 epochs for MuTual and 2 epochs for others and select the model that achieves the best result in validation. The training epochs are 3 for DAP.\nOur domain adaptive post-training for the corresponding response selection tasks is based on the three large-scale dialogue corpus including Ubuntu, Douban, and ECD, respectively.3 The data statistics are in Table 1. Since domain adaptive post-training is time-consuming, following previous studies (Gu et al., 2020a), we use bert-baseuncased, and bert-base-chinese for the English and\n2Our source code is available at https://github. com/cooelf/SPIDER.\n3Since phrases are quite common in Chinese, making it inaccurate to calculate the SVO relations according to Eq. 3, thus we did not use the SBR objective for the two Chinese tasks in this work.\nChinese datasets, respectively. Because there is no appropriate domain data for the small-scale Mutual dataset, we only report the multi-task fine-tuning results with our SPIDER objectives, and also present the results with other PrLMs such as ELECTRA (Clark et al., 2020) for general comparison."
    }, {
      "heading" : "5.3 Baseline Models",
      "text" : "We include the following models for comparison: • Multi-turn matching models: Sequential Matching Network (SMN) (Wu et al., 2017), Deep Attention Matching Network (DAM) (Zhou et al., 2018), Deep Utterance Aggregation (DUA) (Zhang et al., 2018), Interaction-over-Interaction (IoI) (Tao et al., 2019b) have been stated in Section 2.2. Besides, Multi-Representation Fusion Network (MRFN) (Tao et al., 2019a) matches context and response with multiple types of representations. Multi-hop Selector Network (MSN) (Yuan et al., 2019) utilizes a multi-hop selector to filter necessary utterances and matches among them. • PrLMs-based models: BERT (Devlin et al., 2019b), SA-BERT (Gu et al., 2020a), and ELECTRA (Clark et al., 2020)."
    }, {
      "heading" : "5.4 Evaluation Metrics",
      "text" : "Following (Lowe et al., 2015; Wu et al., 2017), we calculate the proportion of true positive response among the top-k selected responses from the list of n available candidates for one context, denoted as Rn@k. Besides, additional conventional metrics of information retrieval are employed on Douban: Mean Average Precision (MAP) (Baeza-Yates et al.,\n1999), Mean Reciprocal Rank (MRR) (Voorhees et al., 1999), and precision at position 1 (P@1)."
    }, {
      "heading" : "5.5 Results",
      "text" : "Tables 2-3 show the results on the four benchmark datasets. We have the following observations:\n1) Generally, the previous models based on multi-turn matching networks perform worse than simple PrLMs-based ones, illustrating the power of contextualized representations in context-sensitive dialogue modeling. PrLM can perform even better when equipped with our SPIDER objectives, verifying the effectiveness of dialogue-aware language modeling, where inter-utterance position information and inner-utterance key facts are better exploited. Compared with SA-BERT that involves more complex architecture and more parameters by injecting extra speaker-aware embeddings, SPIDER keeps the same model size as the backbone BERT, and even surpasses SA-BERT on most of the metrics.\n2) In terms of the training methods, DAP generally works better than MTF, with the merits of two-step procedures including the pure LM-based post-training. According to the ablation study in Table 4, we see that both of the dialogue-aware LM objectives are essentially effective and combining them (SPIDER) gives the best performance, which verifies the necessity of modeling the utterance order and factual correctness. We also notice that UOR shows better performance than SBR in DAP, while gives relative descent in MFT. The most plau-\nsible reason would be that UOR would permute the utterances in the dialogue context which helps the language model learn the utterance in UOR. However, in MFT, the major objective is the downstream dialogue comprehension task. The permutation of the context would possibly bring some negative effects to the downstream task training."
    }, {
      "heading" : "5.6 Influence of Permutation Ratio",
      "text" : "For the UOR objective, a hyper-parameter δ is set to control the maximum number of permutations (as described in Section 3.2.1), which would possibly influence the overall model performance. To investigate the effect, we set the permutation ratio from [0, 20%, 40%, 60%, 80%, 100%]. The result is depicted in Figure 4, in which our model outperforms the baseline in general, showing that the permutation indeed strengthens the baseline."
    }, {
      "heading" : "5.7 Comparison with Different Context Length",
      "text" : "Context length can be measured by the number of turns and average utterance length in a conversation respectively. We split test instances from the Ubuntu dataset into several buckets and compare SPIDER with UOR with the BERT baseline. According to the results depicted in Figure 5, we observe that SPIDER performs much better on contexts with long utterances, and it also performs robustly and is significantly and consistently superior to the baseline. The results indicate the benefits of modeling the utterance order for dialogue comprehension."
    }, {
      "heading" : "5.8 Human Evaluation about Factual Correctness",
      "text" : "To compare the improvements of SPIDER over the baseline on factual correctness, we extract the error cases of the BERT baseline on MuTual (102 in total) and 42 (41.2%) are correctly answered\nby SPIDER. Among the 42 solved cases, 33/42 (78.6%) are entailed with SVO facts in contexts, indicating the benefits of factual correctness."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we focus on the task-related adaptation of the pre-trained language models and propose SPIDER (Structural Pre-traIned DialoguE Reader), a structural language modeling method to capture dialogue exclusive features. To explicitly model the coherence among utterances and the key facts in each utterance, we introduce two novel dialogue-aware language modeling tasks including utterance order restoration and sentence backbone regularization objectives. Experiments on widely-used multi-turn dialogue comprehension benchmark datasets show the superiority over baseline methods. Our work reveals a way to make better use of the structure learning of the contextualized representations from pre-trained language models and gives insights on how to adapt the language modeling training objectives in downstream tasks."
    } ],
    "references" : [ {
      "title" : "Deep enhanced representation for implicit discourse relation recognition",
      "author" : [ "Hongxiao Bai", "Hai Zhao." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 571– 583, Santa Fe, New Mexico, USA. Association for",
      "citeRegEx" : "Bai and Zhao.,? 2018",
      "shortCiteRegEx" : "Bai and Zhao.",
      "year" : 2018
    }, {
      "title" : "PLATO: Pre-trained dialogue generation model with discrete latent variable",
      "author" : [ "Siqi Bao", "Huang He", "Fan Wang", "Hua Wu", "Haifeng Wang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 85–96, Online.",
      "citeRegEx" : "Bao et al\\.,? 2020",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2020
    }, {
      "title" : "Translating embeddings for modeling multirelational data",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Alberto Garcı́aDurán", "Jason Weston", "Oksana Yakhnenko" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Bordes et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2013
    }, {
      "title" : "A survey on dialogue systems: Recent advances and new frontiers",
      "author" : [ "Hongshen Chen", "Xiaorui Liu", "Dawei Yin", "Jiliang Tang." ],
      "venue" : "ACM SIGKDD Explorations Newsletter. 19(2):25–35.",
      "citeRegEx" : "Chen et al\\.,? 2017a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural machine translation with reordering embeddings",
      "author" : [ "Kehai Chen", "Rui Wang", "Masao Utiyama", "Eiichiro Sumita." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1787–1799, Florence, Italy.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "ELECTRA: pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "MuTual: A dataset for multi-turn dialogue reasoning",
      "author" : [ "Leyang Cui", "Yu Wu", "Shujie Liu", "Yue Zhang", "Ming Zhou." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1406–1416, Online. Association for",
      "citeRegEx" : "Cui et al\\.,? 2020",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019a",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference",
      "citeRegEx" : "Devlin et al\\.,? 2019b",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Key-value retrieval networks for task-oriented dialogue",
      "author" : [ "Mihail Eric", "Lakshmi Krishnan", "Francois Charette", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 37–49, Saarbrücken, Germany.",
      "citeRegEx" : "Eric et al\\.,? 2017",
      "shortCiteRegEx" : "Eric et al\\.",
      "year" : 2017
    }, {
      "title" : "Speaker-aware BERT for multi-turn response selection in retrieval-based chatbots",
      "author" : [ "Jia-Chen Gu", "Tianda Li", "Quan Liu", "Zhen-Hua Ling", "Zhiming Su", "Si Wei", "Xiaodan Zhu." ],
      "venue" : "CIKM ’20: The 29th ACM International Conference on Information",
      "citeRegEx" : "Gu et al\\.,? 2020a",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2020
    }, {
      "title" : "Dialogbert: Discourse-aware response generation via learning to recover and rank utterances",
      "author" : [ "Xiaodong Gu", "Kang Min Yoo", "Jung-Woo Ha." ],
      "venue" : "arXiv preprint arXiv:2012.01775.",
      "citeRegEx" : "Gu et al\\.,? 2020b",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2020
    }, {
      "title" : "Don’t stop pretraining: Adapt language models to domains and tasks",
      "author" : [ "Suchin Gururangan", "Ana Marasović", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A. Smith." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the",
      "citeRegEx" : "Gururangan et al\\.,? 2020",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2020
    }, {
      "title" : "Finegrained post-training for improving retrieval-based dialogue systems",
      "author" : [ "Janghoon Han", "Taesuk Hong", "Byoungjae Kim", "Youngjoong Ko", "Jungyun Seo." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Asso-",
      "citeRegEx" : "Han et al\\.,? 2021",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2021
    }, {
      "title" : "ConveRT: Efficient and accurate conversational representations from transformers",
      "author" : [ "Matthew Henderson", "Iñigo Casanueva", "Nikola Mrkšić", "Pei-Hao Su", "Tsung-Hsien Wen", "Ivan Vulić." ],
      "venue" : "Findings of the Association for Computational Linguis-",
      "citeRegEx" : "Henderson et al\\.,? 2020",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2020
    }, {
      "title" : "Flowqa: Grasping flow in history for conversational machine comprehension",
      "author" : [ "Hsin-Yuan Huang", "Eunsol Choi", "Wen-tau Yih." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.",
      "citeRegEx" : "Huang et al\\.,? 2019",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep attentive ranking networks for learning to order sentences",
      "author" : [ "Pawan Kumar", "Dhanajit Brahma", "Harish Karnick", "Piyush Rai." ],
      "venue" : "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of",
      "citeRegEx" : "Kumar et al\\.,? 2020",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2020
    }, {
      "title" : "ALBERT: A lite BERT for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "8th International Conference on Learning Representations,",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "Alime assist: An intelligent assistant for creating an innovative e-commerce experience",
      "author" : [ "Feng-Lin Li", "Minghui Qiu", "Haiqing Chen", "Xiongwei Wang", "Xing Gao", "Jun Huang", "Juwei Ren", "Zhongzhou Zhao", "Weipeng Zhao", "Lei Wang" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "Molweni: A challenge multiparty dialoguesbased machine reading comprehension dataset with discourse structure",
      "author" : [ "Jiaqi Li", "Ming Liu", "Min-Yen Kan", "Zihao Zheng", "Zekun Wang", "Wenqiang Lei", "Ting Liu", "Bing Qin." ],
      "venue" : "Proceedings of the 28th Inter-",
      "citeRegEx" : "Li et al\\.,? 2020a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Task-specific objectives of pre-trained language models for dialogue adaptation",
      "author" : [ "Junlong Li", "Zhuosheng Zhang", "Hai Zhao", "Xi Zhou", "Xiang Zhou." ],
      "venue" : "arXiv preprint arXiv:2009.04984.",
      "citeRegEx" : "Li et al\\.,? 2020b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Text compression-aided transformer encoding",
      "author" : [ "Zuchao Li", "Zhuosheng Zhang", "Hai Zhao", "Rui Wang", "Kehai Chen", "Masao Utiyama", "Eiichiro Sumita." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence.",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Hisbert for conversational reading comprehension",
      "author" : [ "Chuang Liu", "Deyi Xiong", "Yuxiang Jia", "Hongying Zan", "Changjian Hu." ],
      "venue" : "2020 International Conference on Asian Language Processing (IALP), pages 147–152. IEEE.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Knowledge diffusion for neural dialogue generation",
      "author" : [ "Shuman Liu", "Hongshen Chen", "Zhaochun Ren", "Yang Feng", "Qun Liu", "Dawei Yin." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "RoBERTa: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "The Ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems",
      "author" : [ "Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau." ],
      "venue" : "Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse",
      "citeRegEx" : "Lowe et al\\.,? 2015",
      "shortCiteRegEx" : "Lowe et al\\.",
      "year" : 2015
    }, {
      "title" : "Bipartite flat-graph network for nested named entity recognition",
      "author" : [ "Ying Luo", "Hai Zhao." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6408– 6418, Online. Association for Computational Lin-",
      "citeRegEx" : "Luo and Zhao.,? 2020",
      "shortCiteRegEx" : "Luo and Zhao.",
      "year" : 2020
    }, {
      "title" : "Pretraining methods for dialog context representation learning",
      "author" : [ "Shikib Mehri", "Evgeniia Razumovskaia", "Tiancheng Zhao", "Maxine Eskenazi." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "Mehri et al\\.,? 2019",
      "shortCiteRegEx" : "Mehri et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "A stacking gated neural architecture for implicit discourse relation classification",
      "author" : [ "Lianhui Qin", "Zhisong Zhang", "Hai Zhao." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2263–2270, Austin,",
      "citeRegEx" : "Qin et al\\.,? 2016",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2016
    }, {
      "title" : "Adversarial connectiveexploiting networks for implicit discourse relation classification",
      "author" : [ "Lianhui Qin", "Zhisong Zhang", "Hai Zhao", "Zhiting Hu", "Eric Xing." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Qin et al\\.,? 2017",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT with history answer embedding for conversational question answering",
      "author" : [ "Chen Qu", "Liu Yang", "Minghui Qiu", "W. Bruce Croft", "Yongfeng Zhang", "Mohit Iyyer." ],
      "venue" : "Proceedings of the 42nd International ACM SIGIR Conference on Research",
      "citeRegEx" : "Qu et al\\.,? 2019",
      "shortCiteRegEx" : "Qu et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever." ],
      "venue" : "Technical report.",
      "citeRegEx" : "Radford et al\\.,? 2018",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Leveraging pre-trained checkpoints",
      "author" : [ "Sascha Rothe", "Shashi Narayan", "Aliaksei Severyn" ],
      "venue" : null,
      "citeRegEx" : "Rothe et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Rothe et al\\.",
      "year" : 2020
    }, {
      "title" : "DREAM: A chal",
      "author" : [ "Choi", "Claire Cardie" ],
      "venue" : null,
      "citeRegEx" : "Choi and Cardie.,? \\Q2019\\E",
      "shortCiteRegEx" : "Choi and Cardie.",
      "year" : 2019
    }, {
      "title" : "2019b. One time",
      "author" : [ "Dongyan Zhao", "Rui Yan" ],
      "venue" : null,
      "citeRegEx" : "Zhao and Yan.,? \\Q2019\\E",
      "shortCiteRegEx" : "Zhao and Yan.",
      "year" : 2019
    }, {
      "title" : "Sequential matching network: A",
      "author" : [ "jun Li" ],
      "venue" : null,
      "citeRegEx" : "Li.,? \\Q2017\\E",
      "shortCiteRegEx" : "Li.",
      "year" : 2017
    }, {
      "title" : "Learning an effective context-response matching model with self-supervised tasks for retrieval-based dialogues",
      "author" : [ "Ruijian Xu", "Chongyang Tao", "Daxin Jiang", "Xueliang Zhao", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "arXiv preprint arXiv:2009.06265.",
      "citeRegEx" : "Xu et al\\.,? 2020a",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Topicaware multi-turn dialogue modeling",
      "author" : [ "Yi Xu", "Hai Zhao", "Zhuosheng Zhang." ],
      "venue" : "The ThirtyFifth AAAI Conference on Artificial Intelligence (AAAI-21).",
      "citeRegEx" : "Xu et al\\.,? 2021",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2021
    }, {
      "title" : "Syntax-enhanced pre-trained model",
      "author" : [ "Zenan Xu", "Daya Guo", "Duyu Tang", "Qinliang Su", "Linjun Shou", "Ming Gong", "Wanjun Zhong", "Xiaojun Quan", "Nan Duan", "Daxin Jiang." ],
      "venue" : "arXiv preprint arXiv:2012.14116.",
      "citeRegEx" : "Xu et al\\.,? 2020b",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime G. Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le." ],
      "venue" : "Advances in Neural Information Processing Systems 32: Annual Con-",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi-hop selector network for multi-turn response selection in retrieval-based chatbots",
      "author" : [ "Chunyuan Yuan", "Wei Zhou", "Mingming Li", "Shangwen Lv", "Fuqing Zhu", "Jizhong Han", "Songlin Hu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in",
      "citeRegEx" : "Yuan et al\\.,? 2019",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2019
    }, {
      "title" : "DCMN+: Dual co-matching network for multi-choice reading comprehension",
      "author" : [ "Shuailiang Zhang", "Hai Zhao", "Yuwei Wu", "Zhuosheng Zhang", "Xi Zhou", "Xiang Zhou." ],
      "venue" : "Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-",
      "citeRegEx" : "Zhang et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "DIALOGPT : Largescale generative pre-training for conversational response generation",
      "author" : [ "Yizhe Zhang", "Siqi Sun", "Michel Galley", "Yen-Chun Chen", "Chris Brockett", "Xiang Gao", "Jianfeng Gao", "Jingjing Liu", "Bill Dolan." ],
      "venue" : "Proceedings of the 58th An-",
      "citeRegEx" : "Zhang et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "DIALOGPT : Largescale generative pre-training for conversational response generation",
      "author" : [ "Yizhe Zhang", "Siqi Sun", "Michel Galley", "Yen-Chun Chen", "Chris Brockett", "Xiang Gao", "Jianfeng Gao", "Jingjing Liu", "Bill Dolan." ],
      "venue" : "Proceedings of the 58th An-",
      "citeRegEx" : "Zhang et al\\.,? 2020c",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "ERNIE: Enhanced language representation with informative entities",
      "author" : [ "Zhengyan Zhang", "Xu Han", "Zhiyuan Liu", "Xin Jiang", "Maosong Sun", "Qun Liu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Modeling multiturn conversation with deep utterance aggregation",
      "author" : [ "Zhuosheng Zhang", "Jiangtong Li", "Pengfei Zhu", "Hai Zhao", "Gongshen Liu." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 3740–3752,",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Semantics-aware BERT for language understanding",
      "author" : [ "Zhuosheng Zhang", "Yuwei Wu", "Hai Zhao", "Zuchao Li", "Shuailiang Zhang", "Xi Zhou", "Xiang Zhou." ],
      "venue" : "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Inno-",
      "citeRegEx" : "Zhang et al\\.,? 2020d",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Retrospective reader for machine reading comprehension",
      "author" : [ "Zhuosheng Zhang", "Junjie Yang", "Hai Zhao." ],
      "venue" : "The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21).",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Parsing all: Syntax and semantics, dependencies and spans",
      "author" : [ "Junru Zhou", "Zuchao Li", "Hai Zhao." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4438–4449, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Zhou et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    }, {
      "title" : "LIMIT-BERT : Linguistics informed multi-task BERT",
      "author" : [ "Junru Zhou", "Zhuosheng Zhang", "Hai Zhao", "Shuailiang Zhang." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4450–4461, Online. Association for Computa-",
      "citeRegEx" : "Zhou et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-turn response selection for chatbots with deep attention matching network",
      "author" : [ "Xiangyang Zhou", "Lu Li", "Daxiang Dong", "Yi Liu", "Ying Chen", "Wayne Xin Zhao", "Dianhai Yu", "Hua Wu." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association",
      "citeRegEx" : "Zhou et al\\.,? 2018",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2018
    }, {
      "title" : "SDNet: Contextualized attention-based deep network for conversational question answering",
      "author" : [ "Chenguang Zhu", "Michael Zeng", "Xuedong Huang." ],
      "venue" : "arXiv preprint arXiv:1812.03593.",
      "citeRegEx" : "Zhu et al\\.,? 2018a",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2018
    }, {
      "title" : "Lingke: a finegrained multi-turn chatbot for customer service",
      "author" : [ "Pengfei Zhu", "Zhuosheng Zhang", "Jiangtong Li", "Yafang Huang", "Hai Zhao." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations,",
      "citeRegEx" : "Zhu et al\\.,? 2018b",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "Recent advances in large-scale pre-training language models (PrLMs) have achieved remarkable successes in a variety of natural language processing (NLP) tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019a; Yang et al., 2019; Clark et al., 2020; Zhang et al., 2020d).",
      "startOffset" : 159,
      "endOffset" : 284
    }, {
      "referenceID" : 33,
      "context" : "Recent advances in large-scale pre-training language models (PrLMs) have achieved remarkable successes in a variety of natural language processing (NLP) tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019a; Yang et al., 2019; Clark et al., 2020; Zhang et al., 2020d).",
      "startOffset" : 159,
      "endOffset" : 284
    }, {
      "referenceID" : 7,
      "context" : "Recent advances in large-scale pre-training language models (PrLMs) have achieved remarkable successes in a variety of natural language processing (NLP) tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019a; Yang et al., 2019; Clark et al., 2020; Zhang et al., 2020d).",
      "startOffset" : 159,
      "endOffset" : 284
    }, {
      "referenceID" : 41,
      "context" : "Recent advances in large-scale pre-training language models (PrLMs) have achieved remarkable successes in a variety of natural language processing (NLP) tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019a; Yang et al., 2019; Clark et al., 2020; Zhang et al., 2020d).",
      "startOffset" : 159,
      "endOffset" : 284
    }, {
      "referenceID" : 5,
      "context" : "Recent advances in large-scale pre-training language models (PrLMs) have achieved remarkable successes in a variety of natural language processing (NLP) tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019a; Yang et al., 2019; Clark et al., 2020; Zhang et al., 2020d).",
      "startOffset" : 159,
      "endOffset" : 284
    }, {
      "referenceID" : 48,
      "context" : "Recent advances in large-scale pre-training language models (PrLMs) have achieved remarkable successes in a variety of natural language processing (NLP) tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019a; Yang et al., 2019; Clark et al., 2020; Zhang et al., 2020d).",
      "startOffset" : 159,
      "endOffset" : 284
    }, {
      "referenceID" : 12,
      "context" : "mance due to their strong representation ability from self-supervised pre-training, it is still challenging to effectively adapt task-related knowledge during the detailed task-specific training which is usually in a way of fine-tuning (Gururangan et al., 2020).",
      "startOffset" : 236,
      "endOffset" : 261
    }, {
      "referenceID" : 26,
      "context" : "The typical task is response selection (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018) that aims to select the appropriate response according to a given dialogue context containing a number of utterances, which is the focus in this work.",
      "startOffset" : 39,
      "endOffset" : 95
    }, {
      "referenceID" : 47,
      "context" : "The typical task is response selection (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018) that aims to select the appropriate response according to a given dialogue context containing a number of utterances, which is the focus in this work.",
      "startOffset" : 39,
      "endOffset" : 95
    }, {
      "referenceID" : 19,
      "context" : ", with criss-cross discourse structures (Li et al., 2020a; Bai and Zhao, 2018; Qin et al., 2016, 2017).",
      "startOffset" : 40,
      "endOffset" : 102
    }, {
      "referenceID" : 0,
      "context" : ", with criss-cross discourse structures (Li et al., 2020a; Bai and Zhao, 2018; Qin et al., 2016, 2017).",
      "startOffset" : 40,
      "endOffset" : 102
    }, {
      "referenceID" : 28,
      "context" : "Inspired by the effectiveness for learning universal language representations of PrLMs, there are increasing studies that employ PrLMs for conversation modeling (Mehri et al., 2019; Zhang et al., 2020b; Rothe et al., 2020; Whang et al., 2020; Han et al., 2021).",
      "startOffset" : 161,
      "endOffset" : 260
    }, {
      "referenceID" : 44,
      "context" : "Inspired by the effectiveness for learning universal language representations of PrLMs, there are increasing studies that employ PrLMs for conversation modeling (Mehri et al., 2019; Zhang et al., 2020b; Rothe et al., 2020; Whang et al., 2020; Han et al., 2021).",
      "startOffset" : 161,
      "endOffset" : 260
    }, {
      "referenceID" : 34,
      "context" : "Inspired by the effectiveness for learning universal language representations of PrLMs, there are increasing studies that employ PrLMs for conversation modeling (Mehri et al., 2019; Zhang et al., 2020b; Rothe et al., 2020; Whang et al., 2020; Han et al., 2021).",
      "startOffset" : 161,
      "endOffset" : 260
    }, {
      "referenceID" : 13,
      "context" : "Inspired by the effectiveness for learning universal language representations of PrLMs, there are increasing studies that employ PrLMs for conversation modeling (Mehri et al., 2019; Zhang et al., 2020b; Rothe et al., 2020; Whang et al., 2020; Han et al., 2021).",
      "startOffset" : 161,
      "endOffset" : 260
    }, {
      "referenceID" : 39,
      "context" : "Besides, some latent features, such as user intent and conversation topic, are under-discovered in existing works (Xu et al., 2021).",
      "startOffset" : 114,
      "endOffset" : 131
    }, {
      "referenceID" : 8,
      "context" : "Most of the PrLMs are based on the encoder in Transformer, among which Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019b) is one of the most representative work.",
      "startOffset" : 134,
      "endOffset" : 156
    }, {
      "referenceID" : 0,
      "context" : "BERT has helped achieve great performance improvement in a broad range of NLP tasks (Bai and Zhao, 2018; Zhang et al., 2020a; Luo and Zhao, 2020; Zhang et al., 2021).",
      "startOffset" : 84,
      "endOffset" : 165
    }, {
      "referenceID" : 43,
      "context" : "BERT has helped achieve great performance improvement in a broad range of NLP tasks (Bai and Zhao, 2018; Zhang et al., 2020a; Luo and Zhao, 2020; Zhang et al., 2021).",
      "startOffset" : 84,
      "endOffset" : 165
    }, {
      "referenceID" : 27,
      "context" : "BERT has helped achieve great performance improvement in a broad range of NLP tasks (Bai and Zhao, 2018; Zhang et al., 2020a; Luo and Zhao, 2020; Zhang et al., 2021).",
      "startOffset" : 84,
      "endOffset" : 165
    }, {
      "referenceID" : 49,
      "context" : "BERT has helped achieve great performance improvement in a broad range of NLP tasks (Bai and Zhao, 2018; Zhang et al., 2020a; Luo and Zhao, 2020; Zhang et al., 2021).",
      "startOffset" : 84,
      "endOffset" : 165
    }, {
      "referenceID" : 41,
      "context" : "enhance the capacity of PrLMs, such as XLNet (Yang et al., 2019), RoBERTa (Liu et al.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 24,
      "context" : ", 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 17,
      "context" : ", 2019), ALBERT (Lan et al., 2020), ELECTRA (Clark et al.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 45,
      "context" : "Some studies perform training on open-domain conversational data like Reddit for response selection or generation tasks (Wolf et al., 2019; Zhang et al., 2020c; Henderson et al., 2020; Bao et al., 2020), but they are limited to the original pre-training tasks and ignore the dialogue related features.",
      "startOffset" : 120,
      "endOffset" : 202
    }, {
      "referenceID" : 14,
      "context" : "Some studies perform training on open-domain conversational data like Reddit for response selection or generation tasks (Wolf et al., 2019; Zhang et al., 2020c; Henderson et al., 2020; Bao et al., 2020), but they are limited to the original pre-training tasks and ignore the dialogue related features.",
      "startOffset" : 120,
      "endOffset" : 202
    }, {
      "referenceID" : 1,
      "context" : "Some studies perform training on open-domain conversational data like Reddit for response selection or generation tasks (Wolf et al., 2019; Zhang et al., 2020c; Henderson et al., 2020; Bao et al., 2020), but they are limited to the original pre-training tasks and ignore the dialogue related features.",
      "startOffset" : 120,
      "endOffset" : 202
    }, {
      "referenceID" : 8,
      "context" : "For domain-adaptive post-training, prior works have indicated that the order information would be important in the text representation, and the well-known next-sentence-prediction (Devlin et al., 2019b) and sentence-order-prediction (Lan et al.",
      "startOffset" : 180,
      "endOffset" : 202
    }, {
      "referenceID" : 17,
      "context" : ", 2019b) and sentence-order-prediction (Lan et al., 2020) can be viewed as special cases of order prediction.",
      "startOffset" : 39,
      "endOffset" : 57
    }, {
      "referenceID" : 16,
      "context" : "Especially in the dialogue scenario, predicting the word order of utterance, as well as the utterance order in the context, has shown effectiveness in the dialogue generation task (Kumar et al., 2020; Gu et al., 2020b), where the order information is well recognized (Chen et al.",
      "startOffset" : 180,
      "endOffset" : 218
    }, {
      "referenceID" : 11,
      "context" : "Especially in the dialogue scenario, predicting the word order of utterance, as well as the utterance order in the context, has shown effectiveness in the dialogue generation task (Kumar et al., 2020; Gu et al., 2020b), where the order information is well recognized (Chen et al.",
      "startOffset" : 180,
      "endOffset" : 218
    }, {
      "referenceID" : 4,
      "context" : ", 2020b), where the order information is well recognized (Chen et al., 2019).",
      "startOffset" : 57,
      "endOffset" : 76
    }, {
      "referenceID" : 26,
      "context" : "However, there is little attention paid to dialogue comprehension tasks such as response selection (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018).",
      "startOffset" : 99,
      "endOffset" : 155
    }, {
      "referenceID" : 47,
      "context" : "However, there is little attention paid to dialogue comprehension tasks such as response selection (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018).",
      "startOffset" : 99,
      "endOffset" : 155
    }, {
      "referenceID" : 9,
      "context" : "In terms of factual backbone modeling, compared with the existing studies that enhance the PrLMs by annotating named entities or incorporating external knowledge graphs (Eric et al., 2017; Liu et al., 2018), the SVO triplets extracted in our sentence backbone predication objective (SBP) method, appear more widely in the text itself.",
      "startOffset" : 169,
      "endOffset" : 206
    }, {
      "referenceID" : 23,
      "context" : "In terms of factual backbone modeling, compared with the existing studies that enhance the PrLMs by annotating named entities or incorporating external knowledge graphs (Eric et al., 2017; Liu et al., 2018), the SVO triplets extracted in our sentence backbone predication objective (SBP) method, appear more widely in the text itself.",
      "startOffset" : 169,
      "endOffset" : 206
    }, {
      "referenceID" : 26,
      "context" : "Multi-turn dialogue comprehension aims to teach machines to read dialogue contexts and solve tasks such as response selection (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018) and answering questions (Sun et al.",
      "startOffset" : 126,
      "endOffset" : 182
    }, {
      "referenceID" : 47,
      "context" : "Multi-turn dialogue comprehension aims to teach machines to read dialogue contexts and solve tasks such as response selection (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018) and answering questions (Sun et al.",
      "startOffset" : 126,
      "endOffset" : 182
    }, {
      "referenceID" : 6,
      "context" : ", 2018) and answering questions (Sun et al., 2019a; Cui et al., 2020), whose common application is building intelligent humancomputer interactive systems (Chen et al.",
      "startOffset" : 32,
      "endOffset" : 69
    }, {
      "referenceID" : 3,
      "context" : ", 2020), whose common application is building intelligent humancomputer interactive systems (Chen et al., 2017a; Shum et al., 2018; Li et al., 2017; Zhu et al., 2018b).",
      "startOffset" : 92,
      "endOffset" : 167
    }, {
      "referenceID" : 18,
      "context" : ", 2020), whose common application is building intelligent humancomputer interactive systems (Chen et al., 2017a; Shum et al., 2018; Li et al., 2017; Zhu et al., 2018b).",
      "startOffset" : 92,
      "endOffset" : 167
    }, {
      "referenceID" : 54,
      "context" : ", 2020), whose common application is building intelligent humancomputer interactive systems (Chen et al., 2017a; Shum et al., 2018; Li et al., 2017; Zhu et al., 2018b).",
      "startOffset" : 92,
      "endOffset" : 167
    }, {
      "referenceID" : 15,
      "context" : "Early studies mainly focus on the matching between the dialogue context and question (Huang et al., 2019; Zhu et al., 2018a).",
      "startOffset" : 85,
      "endOffset" : 124
    }, {
      "referenceID" : 53,
      "context" : "Early studies mainly focus on the matching between the dialogue context and question (Huang et al., 2019; Zhu et al., 2018a).",
      "startOffset" : 85,
      "endOffset" : 124
    }, {
      "referenceID" : 32,
      "context" : "Recently, inspired by the impressive performance of PrLMs, the mainstream is employing PrLMs to handle the whole input texts of context and question, as a linear sequence of successive tokens and implicitly capture the contextualized representations of those tokens through self-attention (Qu et al., 2019; Liu et al., 2020).",
      "startOffset" : 289,
      "endOffset" : 324
    }, {
      "referenceID" : 22,
      "context" : "Recently, inspired by the impressive performance of PrLMs, the mainstream is employing PrLMs to handle the whole input texts of context and question, as a linear sequence of successive tokens and implicitly capture the contextualized representations of those tokens through self-attention (Qu et al., 2019; Liu et al., 2020).",
      "startOffset" : 289,
      "endOffset" : 324
    }, {
      "referenceID" : 7,
      "context" : "We first employ a pre-trained language model such as BERT (Devlin et al., 2019a) to obtain the initial word representations.",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 41,
      "context" : "The idea is similar to autoencoding (AE) which aims to reconstruct the original data from corrupted input (Yang et al., 2019).",
      "startOffset" : 106,
      "endOffset" : 125
    }, {
      "referenceID" : 7,
      "context" : "After post-training the language model on the dialogue corpus, we load the pre-trained weights as the same way of using BERT (Devlin et al., 2019a), to fine-tune the downstream tasks such as response selection and dialogue reasoning as focused in this work (details in Section 5.",
      "startOffset" : 125,
      "endOffset" : 147
    }, {
      "referenceID" : 26,
      "context" : "We evaluated our model on two English datasets: Ubuntu Dialogue Corpus (Ubuntu) (Lowe et al., 2015) and Multi-Turn Dialogue Reasoning (MuTual) (Cui et al.",
      "startOffset" : 80,
      "endOffset" : 99
    }, {
      "referenceID" : 6,
      "context" : ", 2015) and Multi-Turn Dialogue Reasoning (MuTual) (Cui et al., 2020),1 and two Chinese datasets: Douban Conversation Corpus (Douban) (Wu et al.",
      "startOffset" : 51,
      "endOffset" : 69
    }, {
      "referenceID" : 47,
      "context" : ", 2017) and E-commerce Dialogue Corpus (ECD) (Zhang et al., 2018).",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 26,
      "context" : "Ubuntu (Lowe et al., 2015) consists of English multi-turn conversations about technical support collected from chat logs of the Ubuntu forum.",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 47,
      "context" : "ECD (Zhang et al., 2018) dataset is extracted from conversations between customer and service staff on Taobao.",
      "startOffset" : 4,
      "endOffset" : 24
    }, {
      "referenceID" : 6,
      "context" : "MuTual (Cui et al., 2020) consists of 8860 manually annotated dialogues based on Chinese student English listening comprehension exams.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 10,
      "context" : "Since domain adaptive post-training is time-consuming, following previous studies (Gu et al., 2020a), we use bert-baseuncased, and bert-base-chinese for the English and",
      "startOffset" : 82,
      "endOffset" : 100
    }, {
      "referenceID" : 5,
      "context" : "Because there is no appropriate domain data for the small-scale Mutual dataset, we only report the multi-task fine-tuning results with our SPIDER objectives, and also present the results with other PrLMs such as ELECTRA (Clark et al., 2020) for general comparison.",
      "startOffset" : 220,
      "endOffset" : 240
    }, {
      "referenceID" : 52,
      "context" : ", 2017), Deep Attention Matching Network (DAM) (Zhou et al., 2018), Deep Utterance Aggregation (DUA) (Zhang et al.",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 47,
      "context" : ", 2018), Deep Utterance Aggregation (DUA) (Zhang et al., 2018), Interaction-over-Interaction (IoI) (Tao et al.",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 42,
      "context" : "Multi-hop Selector Network (MSN) (Yuan et al., 2019) utilizes a multi-hop selector to filter necessary utterances and matches among them.",
      "startOffset" : 33,
      "endOffset" : 52
    }, {
      "referenceID" : 8,
      "context" : "• PrLMs-based models: BERT (Devlin et al., 2019b), SA-BERT (Gu et al.",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 10,
      "context" : ", 2019b), SA-BERT (Gu et al., 2020a), and ELECTRA (Clark et al.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 26,
      "context" : "Following (Lowe et al., 2015; Wu et al., 2017), we calculate the proportion of true positive response among the top-k selected responses from the list of n available candidates for one context, denoted as Rn@k.",
      "startOffset" : 10,
      "endOffset" : 46
    } ],
    "year" : 2021,
    "abstractText" : "Pre-trained language models (PrLMs) have demonstrated superior performance due to their strong ability to learn universal language representations from self-supervised pre-training. However, even with the help of the powerful PrLMs, it is still challenging to effectively capture task-related knowledge from dialogue texts which are enriched by correlations among speaker-aware utterances. In this work, we present SPIDER, Structural PretraIned DialoguE Reader, to capture dialogue exclusive features. To simulate the dialoguelike features, we propose two training objectives in addition to the original LM objectives: 1) utterance order restoration, which predicts the order of the permuted utterances in dialogue context; 2) sentence backbone regularization, which regularizes the model to improve the factual correctness of summarized subject-verb-object triplets. Experimental results on widely used dialogue benchmarks verify the effectiveness of the newly introduced self-supervised tasks.",
    "creator" : "LaTeX with hyperref"
  }
}