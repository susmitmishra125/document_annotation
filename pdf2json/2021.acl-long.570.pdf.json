{
  "name" : "2021.acl-long.570.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Including Signed Languages in Natural Language Processing",
    "authors" : [ "Kayo Yin", "Amit Moryossef", "Julie Hochgesang", "Yoav Goldberg", "Malihe Alikhani" ],
    "emails" : [ "kayoy@cs.cmu.edu,", "amitmoryossef@gmail.com", "julie.hochgesang@gallaudet.edu,", "yogo@cs.biu.ac.il,", "malihe@pitt.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7347–7360\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n7347"
    }, {
      "heading" : "1 Introduction",
      "text" : "Natural Language Processing (NLP) has revolutionized the way people interact with technology through the rise of personal assistants and machine translation systems, to name a few. However, the vast majority of NLP models require a spoken language input (speech or text), thereby excluding around 200 different signed languages and up to 70 million deaf people1 from modern language technologies.\n1According to World Federation of the Deaf https://wfdeaf.org/our-work/\nThroughout history, Deaf communities fought for the right to learn and use signed languages, as well as for the recognition of signed languages as legitimate languages (§2). Indeed, signed languages are sophisticated communication modalities that are at least as capable as spoken languages in all manners, linguistic and social. However, in a predominantly oral society, deaf people are constantly encouraged to use spoken languages through lipreading or text-based communication. The exclusion of signed languages from modern language technologies further suppresses signing in favor of spoken languages. This disregards the preferences of the Deaf communities who strongly prefer to communicate in signed languages both online and for in-person day-to-day interactions, among themselves and when interacting with spoken language communities (Padden and Humphries, 1988; Glickman and Hall, 2018). Thus, it is essential to make\nsigned languages accessible.\nTo date, a large amount of research on Sign Language Processing (SLP) has been focused on the visual aspect of signed languages, led by the Computer Vision (CV) community, with little NLP involvement (Figure 1). This is not unreasonable, given that a decade ago, we lacked the adequate CV tools to process videos for further linguistic analyses. However, like spoken languages, signed languages are fully-fledged systems that exhibit all the fundamental characteristics of natural languages (§3), and current SLP techniques fail to address or leverage the linguistic structure of signed languages (§4). This leads us to believe that NLP tools and theories are crucial to process signed languages. Given the recent advances in CV, this position paper argues that now is the time to incorporate linguistic insight into signed language modeling.\nSigned languages introduce novel challenges for NLP due to their visual-gestural modality, simultaneity, spatial coherence, and lack of written form. By working on signed languages, the community will gain a more holistic perspective on natural languages through a better understanding of how meaning is conveyed by the visual modality and how language is grounded in visuospatial concepts.\nMoreover, SLP is not only an intellectually appealing area but also an important research area with a strong potential to benefit signing communities. Examples of beneficial applications enabled by signed language technologies include better documentation of endangered sign languages; educational tools for sign language learners; tools for query and retrieval of information from signed language videos; personal assistants that react to signed languages; real-time automatic sign language interpretations, and more. Needless to say, in addressing this research area, researchers should work alongside and under the direction of deaf communities, and to the benefit of the signing communities’ interest above all (Harris et al., 2009).\nAfter identifying the challenges and open problems to successfully include signed languages in NLP (§5), we emphasize the need to: (1) develop a standardized tokenization method of signed languages with minimal information loss for its modeling; (2) extend core NLP technologies to signed languages to create linguistically-informed models; (3) collect signed language data of sufficient size that accurately represents the real world; (4)\ninvolve and collaborate with the Deaf communities at every step of research."
    }, {
      "heading" : "2 Background and Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 History of Signed Languages and Deaf Culture",
      "text" : "Over the course of modern history, spoken languages were dominant so much so that signed languages struggled to be recognized as languages in their own right and educators developed misconceptions that signed language acquisition may hinder the development of speech skills. For example, in 1880, a large international conference of deaf educators called the Second International Congress on Education of the Deaf banned teaching signed languages, favoring speech therapy instead. It was not until the seminal work on American Sign Language (ASL) by Stokoe (1960) that signed languages started gaining recognition as natural, independent, and well-defined languages, which then inspired other researchers to further explore signed languages as a research area. Nevertheless, antiquated notions that deprioritized signed languages continue to do harm and subjects many to linguistic neglect (Humphries et al., 2016). Several studies have shown that deaf children raised solely with spoken languages do not gain enough access to a first language during their critical period of language acquisition (Murray et al., 2020). This language deprivation can lead to life-long consequences on the cognitive, linguistic, socioemotional, and academic development of the deaf (Hall et al., 2017).\nSigned languages are the primary languages of communication for the Deaf2 and are at the heart of Deaf communities. Failing to recognize signed languages as fully-fledged natural language systems in their own right has had harmful effects in the past, and in an increasingly digitized world, the NLP community has an important responsibility to include signed languages in its research. NLP research should strive to enable a world in which all people, including the Deaf, have access to languages that fit their lived experience."
    }, {
      "heading" : "2.2 Sign Language Processing in the Literature",
      "text" : "Jaffe (1994); Ong and Ranganath (2005); Parton\n2When capitalized, “Deaf” refers to a community of deaf people who share a language and a culture, whereas the lowercase “deaf” refers to the audiological condition of not hearing.\n(2006) survey early works in SLP that were mostly limited to using sensors to capture fingerspelling and isolated signs, or use rules to synthesize signs from spoken language text, due to the lack of adequate CV technology at the time to process videos. This paper will instead focus on more recent visionbased and data-driven approaches that are nonintrusive and more powerful. The introduction of a continuous signed language benchmark dataset (Forster et al., 2014; Cihan Camgöz et al., 2018), coupled with the advent of deep learning for visual processing, lead to increased efforts to recognize signed expressions from videos. Recent surveys on SLP mostly review these different approaches for sign language recognition developed by the CV community (Koller, 2020; Rastgoo et al., 2020; Adaloglou et al., 2020).\nMeanwhile, signed languages have remained relatively overlooked in NLP literature (Figure 1). Bragg et al. (2019) argue the importance of an interdisciplinary approach to SLP, raising the importance of NLP involvement among other disciplines. We take this argument further by diving into the linguistic modeling challenges for signed languages and providing a roadmap of open questions to be addressed by the NLP community, in hopes of stimulating efforts from an NLP perspective towards research on signed languages."
    }, {
      "heading" : "3 Sign Language Lingusitics",
      "text" : "Signed languages consist of phonological, morphological, syntactic, and semantic levels of structure that fulfill the same social, cognitive, and communicative purposes as other natural languages. While spoken languages primarily channel the oralauditory modality, signed languages use the visualgestural modality, relying on the face, hands, body of the signer, and the space around them to create distinctions in meaning. We present the linguistic features of signed languages3 that must be taken into account during their modeling.\nPhonology Signs are composed of minimal units that combine manual features such as hand configuration, palm orientation, placement, contact, path movement, local movement, as well as non-manual features including eye aperture, head movement, and torso positioning4 (Liddell and Johnson, 1989;\n3We mainly refer to ASL, where most sign language research has been conducted, but not exclusively.\n4In this work, we focus on visual signed languages rather than tactile systems such as Pro-Tactile ASL which DeafBlind\nJohnson and Liddell, 2011; Brentari, 2011; Sandler, 2012). In both signed and spoken languages, not all possible phonemes are realized, and inventories of two languages’ phonemes/features may not overlap completely. Different languages are also subject to rules for the allowed combinations of features.\nSimultaneity Though an ASL sign takes about twice as long to produce than an English word, the rates of transmission of information between the two languages are similar (Bellugi and Fischer, 1972). One way signed languages compensate for the slower production rate of signs is through simultaneity: signed languages make use of multiple visual cues to convey different information simultaneously(Sandler, 2012). For example, the signer may produce the sign for ’cup’ on one hand while simultaneously pointing to the actual cup with the other to express “that cup”. Similarly to tone in spoken languages, the face and torso can convey additional affective information (Liddell et al., 2003; Johnston and Schembri, 2007). Facial expressions can modify adjectives, adverbs, and verbs; a head shake can negate a phrase or sentence; eye direction can help indicate referents.\nReferencing The signer can introduce referents in discourse either by pointing to their actual locations in space, or by assigning a region in the signing space to a non-present referent and by pointing to this region to refer to it (Rathmann and Mathur, 2011; Schembri et al., 2018). Signers can also establish relations between referents grounded in signing space by using directional signs or embodying the referents using body shift or eye gaze (Dudis, 2004; Liddell and Metzger, 1998). Spatial referencing also impacts morphology when the directionality of a verb depends on the location of the reference to its subject and/or object (de Beuzeville, 2008; Fenlon et al., 2018): for example, a directional verb can move from the location of its subject and ending at the location of its object. While the relation between referents and verbs in spoken language is more arbitrary, referent relations are usually grounded in signed languages. The visual space is heavily exploited to make referencing clear.\nAnother way anaphoric entities are referenced in sign language is by using classifiers or depicting signs (Supalla, 1986; Wilcox and Hafer, 2004; Roy, 2011) that help describe the characteristics of the\nAmericans sometimes prefer.\nreferent. Classifiers are typically one-handed signs that do not have a particular location or movement assigned to them, or derive features from meaningful discourse (Liddell et al., 2003), so they can be used to convey how the referent relates to other entities, describe its movement, and give more details. For example, to tell about a car swerving and crashing, one might use the hand classifier for a vehicle, move it to indicate swerving, and crash it with another entity in space.\nTo quote someone other than oneself, signers perform role shift (Cormier et al., 2015), where they may physically shift in space to mark the distinction, and take on some characteristics of the people they are representing. For example, to recount a dialogue between a taller and a shorter person, the signer may shift to one side and look up when taking the shorter person’s role, shift to the other side and look down when taking the taller person’s role.\nFingerspelling Fingerspelling is a result of language contact between a signed language and a surrounding spoken language written form (Battison, 1978; Wilcox, 1992; Brentari and Padden, 2001; Patrie and Johnson, 2011). A set of manual gestures correspond with a written orthography or phonetic system. Fingerspelling is often used to indicate names or places or new concepts from the spoken language but often have become integrated into the signed languages themselves as another linguistic strategy (Padden, 1998; Montemurro and Brentari, 2018)."
    }, {
      "heading" : "4 Current State of SLP",
      "text" : "In this section, we present the existing methods, resources, and tasks in SLP, and discuss their limitations to lay the ground for future research."
    }, {
      "heading" : "4.1 Representations of Signed Languages",
      "text" : "Representation is a significant challenge for SLP, as unlike spoken languages, signed languages have no widely adopted written form. Figure 2 illustrates each signed language representation we will describe below.\nVideos are the most straightforward representation of a signed language and can amply incorporate the information conveyed through sign. One major drawback of using videos is their high dimensionality: they usually include more information than needed for modeling, and are expensive to store, transmit, and encode. As facial features are\nessential in sign, anonymizing raw videos also remains an open problem, limiting the possibility of making these videos publicly available (Isard, 2020).\nPoses reduce the visual cues from videos to skeleton-like wireframe or mesh representing the location of joints. While motion capture equipment can often provide better quality pose estimation, it is expensive and intrusive, and estimating pose from videos is the preferred method currently (Pishchulin et al., 2012; Chen et al., 2017; Cao et al., 2019; Güler et al., 2018). Compared to video representations, accurate poses are lower in complexity and anonymized, while observing relatively low information loss. However, they remain a continuous, multidimensional representation that is not adapted to most NLP models.\nWritten notation systems represent signs as discrete visual features. Some systems are written linearly and others use graphemes in two dimensions. While various universal (Sutton, 1990; Prillwitz and Zienert, 1990) and language-specific notation systems (Stokoe Jr, 2005; Kakumasu, 1968; Bergman, 1979) have been proposed, no writing system has been adopted widely by any sign language community, and the lack of standard hinders the exchange and unification of resources and applications between projects. Figure 2 depicts two universal notation systems: SignWriting (Sutton, 1990), a two-dimensional pictographic system, and HamNoSys (Prillwitz and Zienert, 1990), a linear stream of graphemes that was designed to be readable by machines.\nGlossing is the transcription of signed languages sign-by-sign, where every sign has a unique identifier. While various sign language corpus projects have provided gloss annotation guidelines (Mesch and Wallin, 2015; Johnston and De Beuzeville, 2016; Konrad et al., 2018), again, there is no single agreed-upon standard. Linear gloss annotations are also an imprecise representation of signed language: they do not adequately capture all information expressed simultaneously through different cues (i.e. body posture, eye gaze) or spatial relations, which leads to an inevitable information loss up to a semantic level that affects downstream performance on SLP tasks (Yin and Read, 2020b)."
    }, {
      "heading" : "4.2 Existing Sign Language Resources",
      "text" : "Now, we introduce the different formats of resources and discuss how they can be used for signed language modeling.\nBilingual dictionaries for signed language (Mesch and Wallin, 2012; Fenlon et al., 2015; Crasborn et al., 2016; Gutierrez-Sigut et al., 2016) map a spoken language word or short phrase to a signed language video. One notable dictionary is, SpreadTheSign5 is a parallel dictionary containing around 23,000 words with up to 41 different spoken-signed language pairs and more than 500,000 videos in total. While dictionaries may help create lexical rules between languages, they do not demonstrate the grammar or the usage of signs in context.\nFingerspelling corpora usually consist of videos of words borrowed from spoken languages that are signed letter-by-letter. They can be synthetically created (Dreuw et al., 2006) or mined from online resources (Shi et al., 2018, 2019). However, they only capture one aspect of signed languages.\nIsolated sign corpora are collections of annotated single signs. They are synthesized (Ebling et al., 2018; Huang et al., 2018; Sincan and Keles, 2020; Hassan et al., 2020) or mined from online resources (Vaezi Joze and Koller, 2019; Li et al., 2020), and can be used for isolated sign language recognition or for contrastive analysis of minimal signing pairs (Imashev et al., 2020). However, like dictionaries, they do not describe relations between\n5https://www.spreadthesign.com/\nsigns nor do they capture coarticulation during signing, and are often limited in vocabulary size (20- 1000 signs)\nContinuous sign corpora contain parallel sequences of signs and spoken language. Available continuous sign corpora are extremely limited, containing 4-6 orders of magnitude fewer sentence pairs than similar corpora for spoken language machine translation (Arivazhagan et al., 2019). Moreover, while automatic speech recognition (ASR) datasets contain up to 50,000 hours of recordings (Pratap et al., 2020), the largest continuous sign language corpus contain only 1,150 hours, and only 50 of them are publicly available (Hanke et al., 2020). These datasets are usually synthesized (Databases, 2007; Crasborn and Zwitserlood, 2008; Ko et al., 2019; Hanke et al., 2020) or recorded in studio conditions (Forster et al., 2014; Cihan Camgöz et al., 2018), which does not account for noise in real-life conditions. Moreover, some contain signed interpretations of spoken language rather than naturallyproduced signs, which may not accurately represent native signing since translation is now a part of the discourse event.\nAvailability Unlike the vast amount and diversity of available spoken language resources that allow various applications, signed language resources are scarce and currently only support translation and production. Unfortunately, most of the signed language corpora discussed in the literature are either not available for use or available under heavy restrictions and licensing terms. Signed language data is especially challenging to anonymize due to the importance of facial and other physical features\nin signing videos, limiting its open distribution, and developing anonymization with minimal information loss, or accurate anonymous representations is a promising research problem."
    }, {
      "heading" : "4.3 Sign Language Processing Tasks",
      "text" : "The CV community has mainly led the research on SLP so far to focus on processing the visual features in signed language videos. As a result, current SLP methods do not fully address the linguistic complexity of signed languages. We survey common SLP tasks and limitations of current methods by drawing on linguistic theories of signed languages.\nDetection Sign language detection is the binary classification task to determine whether a signed language is being used or not in a given video frame. While recent detection models (Borg and Camilleri, 2019; Moryossef et al., 2020) achieve high performance, we lack well-annotated data that include interference and distractions with non-signing instances for proper evaluation. A similar task in spoken languages is voice activity detection (VAD) (Sohn et al., 1999; Ramırez et al., 2004), the detection of when a human voice is used in an audio signal. However, as VAD methods often rely on speech-specific representations such as spectrograms, they are not always applicable to videos.\nIdentification Sign language identification classifies which signed language is being used in a given video automatically. Existing works utilize the distribution of phonemes (Gebre et al., 2013) or activity maps in signing space (Monteiro et al., 2016) to identify the signed language in videos. However, these methods only rely on low-level visual features, while signed languages have several distinctive features on a linguistic level, such as lexical or structural differences (McKee and Kennedy, 2000; Kimmelman, 2014; Ferreira-Brito, 1984; Shroyer and Shroyer, 1984) which have not been explored for this task.\nSegmentation Segmentation consists of detecting the frame boundaries for signs or phrases in videos to divide them into meaningful units. Current methods resort to segmenting units loosely mapped to signed language units (Santemiz et al., 2009; Farag and Brock, 2019; Bull et al., 2020), and does not leverage reliable linguistic predictors of sentence boundaries such as prosody in signed languages (i.e. pauses, sign duration, facial expres-\nsions, eye apertures) (Sandler, 2010; Ormel and Crasborn, 2012).\nRecognition Sign language recognition (SLR) detects and label signs from a video, either on isolated (Imashev et al., 2020; Sincan and Keles, 2020) or continuous (Cui et al., 2017; Camgöz et al., 2018, 2020b) signs. Though some previous works have referred to this as “sign language translation”, recognition merely determines the associated label of each sign, without handling the syntax and morphology of the signed language (Padden, 1988) to create a spoken language output. Instead, SLR has often been used as an intermediate step during translation to produce glosses from signed language videos.\nTranslation Sign language translation (SLT) commonly refers to the translation of signed language to spoken language. Current methods either perform translation with glosses (Camgöz et al., 2018, 2020b; Yin and Read, 2020a,b; Moryossef et al., 2021) or on pose estimations and sign articulators from videos (Ko et al., 2019; Camgöz et al., 2020a), but do not, for instance, handle spatial relations and grounding in discourse to resolve ambiguous referents.\nProduction Sign language production consists of producing signed language from spoken language and often use poses as an intermediate representation to overcome challenges in animation. To overcome the challenges in generating videos directly, most efforts use poses as an intermediate representation, with the goal of either using computer animation or pose-to-video models to perform video production. Earlier methods generate and concatenate isolated signs (Stoll et al., 2018, 2020), while more recent methods (Saunders et al., 2020b,a; Zelinka and Kanis, 2020; Xiao et al., 2020) autoregressively decode a sequence of poses from an input text. Due to the lack of suitable automatic evaluation methods of generated signs, existing works resort to measuring back-translation quality, which cannot accurately capture the quality of the produced signs nor its usability in real-world settings. A better understanding of how distinctions in meaning are created in signed language may help develop a better evaluation method."
    }, {
      "heading" : "5 Towards Including Signed Languages in Natural Language Processing",
      "text" : "The limitations in the design of current SLP models often stem from the lack of exploring the linguistic possibilities of signed languages. We therefore invite the NLP community to collaborate with the CV community, for their expertise in visual processing, and signing communities and sign linguists, for their expertise in signed languages and the lived experiences of signers, in researching SLP. We believe that first, the development of known tasks in the standard NLP pipeline to signed languages will help us better understand how to model them, as well as provide valuable tools for higher-level applications. Although these tasks have been thoroughly researched for spoken languages, they pose interesting new challenges in a different modality. We also emphasize the need for real-world data to develop such methods, and a close collaboration with signing communities to have an accurate understanding of how signed language technologies can benefit signers, all the while respecting the Deaf community’s ownership of signed languages."
    }, {
      "heading" : "5.1 Building NLP Pipelines",
      "text" : "Although signed and spoken languages differ in modality, we argue that as both express the syntax, semantics, and pragmatics of natural languages, fundamental theories of NLP can and should be extended to signed languages. NLP applications often rely on low-level tools such as tokenizers and parsers, so we invite more research efforts on these core NLP tasks that often lay the foundation of other applications. We also discuss what considerations should be taken into account for their development to signed languages and raise open questions that should be addressed.\nTokenization The vast majority of NLP methods require a discrete input. To extend NLP technologies to signed languages, we must first and foremost be able to develop adequate tokenization tools that maps continuous signed language videos to a discrete, accurate representation with minimal information loss. While existing SLP systems and datasets often use glosses as discrete lexical units of signed phrases, this poses three significant problems: (1) linear, single-dimensional glosses cannot fully capture the spatial constructions of signed languages, which downgrades downstream performance (Yin and Read, 2020b); (2) glosses are language-specific and requiring new glossing\nmodels for each language is impractical given the scarcity of resources; (3) glosses lack standard across corpora which limits data sharing and adds significant overhead in modeling.\nWe thus urge the adoption of an efficient, universal, and standardized method for tokenization of signed languages, all the while considering: how do we define lexical units in signed languages? (Johnston and Schembri, 1999; Johnston, 2010) To what degree can phonological units of signed languages be mapped to lexical units? Should we model the articulators of signs separately or together? What are the cross-linguistic phonological differences to consider? To what extent can ideas used in automatic speech recognition be applied to signed languages?\nSyntactic Analysis Part-of-speech (POS) tagging and syntactic parsing are fundamental to understand the meaning of words in context. Yet, no such linguistic tools for automatic syntactic analyses exist. To develop such tools, we must first define to what extent POS tagging and syntactic parsing for spoken languages also generalize to signed languages - do we need a new set of POS and dependency tags for signed languages? How are morphological features expressed? What are the annotation guidelines to create datasets on syntax? Can we draw on linguistic theories to design features and rules that perform these tasks? Are there typologically similar spoken languages for some signed languages we can perform transfer learning with?\nNamed Entity Recognition (NER) Recognizing named entities and finding relationships between them are highligh important in information retrieval and classification. Named entities in signed languages can be produced by a fingerspelled sequence, a sign, or even through mouthing of the name while the referent is introduced through pointing. Bleicken et al. (2016) attempt NER in German Sign Language (DGS) to perform anonymization, but only do so indirectly, by either performing NER on the gold DGS gloss annotations and German translations or manually on the videos. We instead propose NER in a fully automated fashion while considering, what are the visual markers of named entities? How are they introduced and referenced? How are relationships between them established?\nCoreference Resolution Resolving coreference is crucial for language understanding. In signed languages, present referents, where the signer explicitly points to the entity in question, are relatively unambiguous. In contrast, non-present referents and classifiers are heavily grounded in the signing space, so good modeling of the spatial coherence in sign language is required. Evidence suggests that classic theoretical frameworks, such as discourse representation theory, may extend to signed languages (Steinbach and Onea, 2016). We pose the following questions: to what extent can automatic coreference resolution of spoken languages be applied to signed languages? How do we keep track of referents in space? How can we leverage spatial relations to resolve ambiguity?\nTowards Linguistically Informed and Multimodal SLP We highly encourage the collaboration of multimodal and SLP research communities to develop powerful SLP models informed by core NLP tools such as the ones discussed, all the while processing and relating information from both linguistic and visual modalities. On the one hand, theories and methods to reason multimodal messages can enhance the joint modeling of vision and language in signed languages. SLP is especially subject to three of the core technical challenges in multimodal machine learning (Baltrušaitis et al., 2018): translation - how do we map visual-gestural information to/from audio-oral and textual information? alignment - how do we relate signed language units to spoken language units? co-learning - can we transfer high-resource spoken language knowledge to signed language? On the other hand, meaning in spoken languages is not only conveyed through speech or text but also through the visual modality. Studying signed languages can give a better understanding of how to model co-speech gestures, spatial discourse relations, and conceptual grounding of language through vision."
    }, {
      "heading" : "5.2 Collect Real-World Data",
      "text" : "Data is essential to develop any of the core NLP tools previously described, and current efforts in SLP are often limited by the lack of adequate data. We discuss the considerations to keep in mind when building datasets, challenges of collecting such data, and directions to facilitate data collection.\nWhat is Good Signed Language Data? For SLP models to be deployable, they must be developed using data that represents the real world ac-\ncurately. What constitutes an ideal signed language dataset is an open question, we suggest including the following requirements: (1) a broad domain; (2) sufficient data and vocabulary size; (3) real-world conditions; (4) naturally produced signs; (5) a diverse signer demographic; (6) native signers; and when applicable, (7) dense annotations.\nTo illustrate the importance of data quality during modeling, we first take as an example a current benchmark for SLP, the RWTH-PHOENIXWeather 2014T dataset (Cihan Camgöz et al., 2018) of German Sign Language, that does not meet most of the above criteria: it is restricted to the weather domain (1); contains only around 8K segments with 1K unique signs (2); filmed in studio conditions (3); interpreted from German utterances (4); and signed by nine Caucasian interpreters (5,6). Although this dataset successfully addressed data scarcity issues at the time and successfully rendered results comparable and fueled competitive research, it does not accurately represent signed languages in the real world. On the other hand, the Public DGS Corpus (Hanke et al., 2020) is an open-domain (1) dataset consisting of 50 hours of natural signing (4) by 330 native signers from various regions in Germany (5,6), annotated with glosses, HamNoSys and German translations (7), meeting all but two requirements we suggest.\nWe train a gloss-to-text sign language translation transformer (Yin and Read, 2020b) on both datasets. On RWTH-PHOENIX-Weather 2014T, we obtain 22.17 BLEU on testing; on Public DGS Corpus, we obtain a mere 3.2 BLEU. Although Transformers achieve encouraging results on RWTH-PHOENIXWeather 2014T (Saunders et al., 2020b; Camgöz et al., 2020a), they fail on more realistic, opendomain data. These results reveal that firstly, for real-world applications, we need more data to train such types of models, and secondly, while available data is severely limited in size, less data-hungry and more linguistically-informed approaches may be more suitable. This experiment reveals how it is crucial to use data that accurately represent the complexity and diversity of signed languages to precisely assess what types of methods are suitable, and how well our models would deploy to the real world.\nChallenges of Data Collection Collecting and annotating signed data inline with the ideal requires more resources than speech or text data, taking up to 600 minutes per minute of an annotated signed\nlanguage video (Hanke et al., 2020). Moreover, annotation usually require a specific set of knowledge and skills, which makes recruiting or training qualified annotators challenging. Additionally, there is little existing signed language data in the wild that are open to use, especially from native signers that are not interpretations of speech. Therefore, data collection often requires significant efforts and costs of on-site recording as well.\nAutomating Annotation To collect more data that enables the development of deployable SLP models, one useful research direction is creating tools that can simplify or automate parts of the collection and annotation process. One of the largest bottleneck in obtaining more adequate signed language data is the amount of time and scarcity of experts required to perform annotation. Therefore, tools that perform automatic parsing, detection of frame boundaries, extraction of articulatory features, suggestions for lexical annotations, and allow parts of the annotation process to be crowdsourced to non-experts, to name a few, have a high potential to facilitate and accelerate the availability of good data."
    }, {
      "heading" : "5.3 Practice Deaf Collaboration",
      "text" : "Finally, when working with signed languages, it is vital to keep in mind who this technology should benefit, and what they need. Researchers in SLP must honor that signed languages belong to the Deaf community and avoid exploiting their language as a commodity (Bird, 2020).\nSolving Real Needs Many efforts in SLP have developed intrusive methods (e.g. requiring signers to wear special gloves), which are often rejected by signing communities and therefore have limited real-world value. Such efforts are often marketed to perform “sign language translation” when they, in fact, only identify fingerspelling or recognize a very limited set of isolated signs at best. These approaches oversimplify the rich grammar of signed languages, promote the misconception that signs are solely expressed through the hands, and are considered by the Deaf community as a manifestation of audism, where it is the signers who must make the extra effort to wear additional sensors to be understood by non-signers (Erard, 2017). In order to avoid such mistakes, we encourage close Deaf involvement throughout the research process to ensure that we direct our efforts towards applications that will be adopted by signers, and do not\nmake false assumptions about signed languages or the needs of signing communities.\nBuilding Collaboration Deaf collaborations and leadership are essential for developing signed language technologies to ensure they address the community’s needs and will be adopted, and that they do not rely on misconceptions or inaccuracies about signed language (Harris et al., 2009; Kusters et al., 2017). Hearing researchers cannot relate to the deaf experience or fully understand the context in which the tools being developed would be used, nor can they speak for the deaf. Therefore, we encourage the creation of a long-term collaborative environment between signed language researchers and users, so that deaf users can identify meaningful challenges, and provide insights on the considerations to take, while researchers cater to the signers’ needs as the field evolves. We also recommend reaching out to signing communities for reviewing papers on signed languages, to ensure an adequate evaluation of this type of research results published at ACL venues. There are several ways to connect with Deaf communities for collaboration: one can seek deaf students in their local community, reach out to schools for the deaf, contact deaf linguists, join a network of researchers of sign-related technologies6, and/or participate in deaf-led projects."
    }, {
      "heading" : "6 Conclusions",
      "text" : "We urge the inclusion of signed languages in NLP. We believe that the NLP community is wellpositioned, especially with the plethora of successful spoken language processing methods coupled with the recent advent of computer vision tools for videos, to bring the linguistic insight needed for better signed language models. We hope to see an increase in both the interests and efforts in collecting signed language resources and developing signed language tools while building a strong collaboration with signing communities."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank Marc Schulder, Claude Mauk, David Mortensen, Chaitanya Ahuja, Siddharth Dalmia, Shruti Palaskar and Graham Neubig as well as the anonymous reviewers for their helpful feedback and insightful discussions.\n6https://www.crest-network.com/"
    } ],
    "references" : [ {
      "title" : "A comprehensive study",
      "author" : [ "Nikolas Adaloglou", "Theocharis Chatzis", "Ilias Papastratis", "Andreas Stergioulas", "Georgios Th Papadopoulos", "Vassia Zacharopoulou", "George J Xydopoulos", "Klimnis Atzakas", "Dimitris Papazachariou", "Petros Daras" ],
      "venue" : null,
      "citeRegEx" : "Adaloglou et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Adaloglou et al\\.",
      "year" : 2020
    }, {
      "title" : "Construction of the literature graph in semantic scholar",
      "author" : [ "ters", "Joanna Power", "Sam Skjonsberg", "Lucy Wang", "Chris Wilhelm", "Zheng Yuan", "Madeleine van Zuylen", "Oren Etzioni" ],
      "venue" : "In Proceedings of the 2018 Conference of the North American Chap-",
      "citeRegEx" : "ters et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "ters et al\\.",
      "year" : 2018
    }, {
      "title" : "Massively multilingual neural machine translation in the wild: Findings and chal",
      "author" : [ "Naveen Arivazhagan", "Ankur Bapna", "Orhan Firat", "Dmitry Lepikhin", "Melvin Johnson", "Maxim Krikun", "Mia Xu Chen", "Yuan Cao", "George Foster", "Colin Cherry" ],
      "venue" : null,
      "citeRegEx" : "Arivazhagan et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Arivazhagan et al\\.",
      "year" : 2019
    }, {
      "title" : "Multimodal machine learning: A survey and taxonomy",
      "author" : [ "Tadas Baltrušaitis", "Chaitanya Ahuja", "LouisPhilippe Morency." ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence, 41(2):423–443.",
      "citeRegEx" : "Baltrušaitis et al\\.,? 2018",
      "shortCiteRegEx" : "Baltrušaitis et al\\.",
      "year" : 2018
    }, {
      "title" : "Lexical borrowing in American sign language",
      "author" : [ "Robbin Battison." ],
      "venue" : "ERIC.",
      "citeRegEx" : "Battison.,? 1978",
      "shortCiteRegEx" : "Battison.",
      "year" : 1978
    }, {
      "title" : "A comparison of sign language and spoken language",
      "author" : [ "Ursula Bellugi", "Susan Fischer." ],
      "venue" : "Cognition, 1(2-3):173–200.",
      "citeRegEx" : "Bellugi and Fischer.,? 1972",
      "shortCiteRegEx" : "Bellugi and Fischer.",
      "year" : 1972
    }, {
      "title" : "Signed Swedish",
      "author" : [ "Brita Bergman." ],
      "venue" : "National Swedish Board of Education [Skolöverstyr.]:.",
      "citeRegEx" : "Bergman.,? 1979",
      "shortCiteRegEx" : "Bergman.",
      "year" : 1979
    }, {
      "title" : "Pointing and verb modification: the expression of semantic roles in the auslan corpus",
      "author" : [ "Louise de Beuzeville." ],
      "venue" : "Workshop Programme, page 13. Citeseer.",
      "citeRegEx" : "Beuzeville.,? 2008",
      "shortCiteRegEx" : "Beuzeville.",
      "year" : 2008
    }, {
      "title" : "Decolonising speech and language technology",
      "author" : [ "Steven Bird." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 3504–3519, Barcelona, Spain (Online). International Committee on Computational Linguistics.",
      "citeRegEx" : "Bird.,? 2020",
      "shortCiteRegEx" : "Bird.",
      "year" : 2020
    }, {
      "title" : "Using a language technology infrastructure for German in order to anonymize German Sign Language corpus data",
      "author" : [ "Julian Bleicken", "Thomas Hanke", "Uta Salden", "Sven Wagner." ],
      "venue" : "Proceedings of the Tenth International Conference on Language",
      "citeRegEx" : "Bleicken et al\\.,? 2016",
      "shortCiteRegEx" : "Bleicken et al\\.",
      "year" : 2016
    }, {
      "title" : "Sign language detection ”in the wild” with recurrent neural networks",
      "author" : [ "Mark Borg", "Kenneth P Camilleri." ],
      "venue" : "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1637–1641. IEEE.",
      "citeRegEx" : "Borg and Camilleri.,? 2019",
      "shortCiteRegEx" : "Borg and Camilleri.",
      "year" : 2019
    }, {
      "title" : "Sign language phonology",
      "author" : [ "Diane Brentari." ],
      "venue" : "The handbook of phonological theory, pages 691–721.",
      "citeRegEx" : "Brentari.,? 2011",
      "shortCiteRegEx" : "Brentari.",
      "year" : 2011
    }, {
      "title" : "A language with multiple origins: Native and foreign vocabulary in american sign language",
      "author" : [ "Diane Brentari", "Carol Padden." ],
      "venue" : "Foreign vocabulary in sign language: A cross-linguistic investigation of word formation, pages 87–119.",
      "citeRegEx" : "Brentari and Padden.,? 2001",
      "shortCiteRegEx" : "Brentari and Padden.",
      "year" : 2001
    }, {
      "title" : "Automatic segmentation of sign language into subtitle-units",
      "author" : [ "Hannah Bull", "Michèle Gouiffès", "Annelies Braffort." ],
      "venue" : "European Conference on Computer Vision, pages 186–198. Springer.",
      "citeRegEx" : "Bull et al\\.,? 2020",
      "shortCiteRegEx" : "Bull et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural sign language translation",
      "author" : [ "Necati Cihan Camgöz", "Simon Hadfield", "Oscar Koller", "Hermann Ney", "Richard Bowden." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7784–7793.",
      "citeRegEx" : "Camgöz et al\\.,? 2018",
      "shortCiteRegEx" : "Camgöz et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-channel transformers for multi-articulatory sign language translation",
      "author" : [ "Necati Cihan Camgöz", "Oscar Koller", "Simon Hadfield", "Richard Bowden." ],
      "venue" : "European Conference on Computer Vision, pages 301–319.",
      "citeRegEx" : "Camgöz et al\\.,? 2020a",
      "shortCiteRegEx" : "Camgöz et al\\.",
      "year" : 2020
    }, {
      "title" : "Sign language transformers: Joint end-to-end sign language recognition and translation",
      "author" : [ "Necati Cihan Camgöz", "Oscar Koller", "Simon Hadfield", "Richard Bowden." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-",
      "citeRegEx" : "Camgöz et al\\.,? 2020b",
      "shortCiteRegEx" : "Camgöz et al\\.",
      "year" : 2020
    }, {
      "title" : "Openpose: Realtime multiperson 2d pose estimation using part affinity fields",
      "author" : [ "Z. Cao", "G. Hidalgo Martinez", "T. Simon", "S. Wei", "Y.A. Sheikh." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence.",
      "citeRegEx" : "Cao et al\\.,? 2019",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2019
    }, {
      "title" : "Adversarial posenet: A structure-aware convolutional network for human pose estimation",
      "author" : [ "Yu Chen", "Chunhua Shen", "Xiu-Shen Wei", "Lingqiao Liu", "Jian Yang." ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision, pages",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural sign language translation",
      "author" : [ "Necati Cihan Camgöz", "Simon Hadfield", "Oscar Koller", "Hermann Ney", "Richard Bowden." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Camgöz et al\\.,? 2018",
      "shortCiteRegEx" : "Camgöz et al\\.",
      "year" : 2018
    }, {
      "title" : "Rethinking constructed action",
      "author" : [ "Kearsy Cormier", "Sandra Smith", "Zed SevcikovaSehyr." ],
      "venue" : "Sign Language & Linguistics, 18(2):167–204.",
      "citeRegEx" : "Cormier et al\\.,? 2015",
      "shortCiteRegEx" : "Cormier et al\\.",
      "year" : 2015
    }, {
      "title" : "The corpus ngt: an online corpus for professionals and laymen",
      "author" : [ "Onno A Crasborn", "IEP Zwitserlood" ],
      "venue" : null,
      "citeRegEx" : "Crasborn and Zwitserlood.,? \\Q2008\\E",
      "shortCiteRegEx" : "Crasborn and Zwitserlood.",
      "year" : 2008
    }, {
      "title" : "Recurrent convolutional neural networks for continuous sign language recognition by staged optimization",
      "author" : [ "Runpeng Cui", "Hu Liu", "Changshui Zhang." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages",
      "citeRegEx" : "Cui et al\\.,? 2017",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2017
    }, {
      "title" : "Modeling image variability in appearance-based gesture recognition",
      "author" : [ "Philippe Dreuw", "Thomas Deselaers", "Daniel Keysers", "Hermann Ney." ],
      "venue" : "ECCV workshop on statistical methods in multiimage and video processing, pages 7–18.",
      "citeRegEx" : "Dreuw et al\\.,? 2006",
      "shortCiteRegEx" : "Dreuw et al\\.",
      "year" : 2006
    }, {
      "title" : "Body partitioning and real-space blends",
      "author" : [ "Paul G Dudis." ],
      "venue" : "Cognitive linguistics, 15(2):223–238.",
      "citeRegEx" : "Dudis.,? 2004",
      "shortCiteRegEx" : "Dudis.",
      "year" : 2004
    }, {
      "title" : "Why sign-language gloves don’t help deaf people",
      "author" : [ "Michael Erard." ],
      "venue" : "The Atlantic, 9.",
      "citeRegEx" : "Erard.,? 2017",
      "shortCiteRegEx" : "Erard.",
      "year" : 2017
    }, {
      "title" : "Learning motion disfluencies for automatic sign language segmentation",
      "author" : [ "Iva Farag", "Heike Brock." ],
      "venue" : "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7360–7364. IEEE.",
      "citeRegEx" : "Farag and Brock.,? 2019",
      "shortCiteRegEx" : "Farag and Brock.",
      "year" : 2019
    }, {
      "title" : "Building bsl signbank: The lemma dilemma revisited",
      "author" : [ "Jordan Fenlon", "Kearsy Cormier", "Adam Schembri." ],
      "venue" : "International Journal of Lexicography, 28(2):169–206.",
      "citeRegEx" : "Fenlon et al\\.,? 2015",
      "shortCiteRegEx" : "Fenlon et al\\.",
      "year" : 2015
    }, {
      "title" : "Modification of indicating verbs in british sign language: A corpus-based study",
      "author" : [ "Jordan Fenlon", "Adam Schembri", "Kearsy Cormier." ],
      "venue" : "Language, 94(1):84–118.",
      "citeRegEx" : "Fenlon et al\\.,? 2018",
      "shortCiteRegEx" : "Fenlon et al\\.",
      "year" : 2018
    }, {
      "title" : "Similarities & differences in two brazilian sign languages",
      "author" : [ "Lucinda Ferreira-Brito." ],
      "venue" : "Sign language studies, 42:45–56.",
      "citeRegEx" : "Ferreira.Brito.,? 1984",
      "shortCiteRegEx" : "Ferreira.Brito.",
      "year" : 1984
    }, {
      "title" : "Extensions of the sign language recognition and translation corpus rwth-phoenix-weather",
      "author" : [ "Jens Forster", "Christoph Schmidt", "Oscar Koller", "Martin Bellgardt", "Hermann Ney." ],
      "venue" : "LREC, pages 1911–1916.",
      "citeRegEx" : "Forster et al\\.,? 2014",
      "shortCiteRegEx" : "Forster et al\\.",
      "year" : 2014
    }, {
      "title" : "Automatic sign language identification",
      "author" : [ "Binyam Gebrekidan Gebre", "Peter Wittenburg", "Tom Heskes." ],
      "venue" : "2013 IEEE International Conference on Image Processing, pages 2626–2630. IEEE.",
      "citeRegEx" : "Gebre et al\\.,? 2013",
      "shortCiteRegEx" : "Gebre et al\\.",
      "year" : 2013
    }, {
      "title" : "Language deprivation and deaf mental health",
      "author" : [ "Neil S Glickman", "Wyatte C Hall." ],
      "venue" : "Routledge.",
      "citeRegEx" : "Glickman and Hall.,? 2018",
      "shortCiteRegEx" : "Glickman and Hall.",
      "year" : 2018
    }, {
      "title" : "Densepose: Dense human pose estimation in the wild",
      "author" : [ "Rıza Alp Güler", "Natalia Neverova", "Iasonas Kokkinos." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7297–7306.",
      "citeRegEx" : "Güler et al\\.,? 2018",
      "shortCiteRegEx" : "Güler et al\\.",
      "year" : 2018
    }, {
      "title" : "Lse-sign: A lexical database for spanish sign language",
      "author" : [ "Eva Gutierrez-Sigut", "Brendan Costello", "Cristina Baus", "Manuel Carreiras." ],
      "venue" : "Behavior Research Methods, 48(1):123–137.",
      "citeRegEx" : "Gutierrez.Sigut et al\\.,? 2016",
      "shortCiteRegEx" : "Gutierrez.Sigut et al\\.",
      "year" : 2016
    }, {
      "title" : "Language deprivation syndrome: A possible neurodevelopmental disorder with sociocultural origins",
      "author" : [ "Wyatte C Hall", "Leonard L Levin", "Melissa L Anderson." ],
      "venue" : "Social psychiatry and psychiatric epidemiology, 52(6):761–776.",
      "citeRegEx" : "Hall et al\\.,? 2017",
      "shortCiteRegEx" : "Hall et al\\.",
      "year" : 2017
    }, {
      "title" : "Extending the Public DGS Corpus in size and depth",
      "author" : [ "Thomas Hanke", "Marc Schulder", "Reiner Konrad", "Elena Jahn." ],
      "venue" : "Proceedings of the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Re-",
      "citeRegEx" : "Hanke et al\\.,? 2020",
      "shortCiteRegEx" : "Hanke et al\\.",
      "year" : 2020
    }, {
      "title" : "Research ethics in sign language communities",
      "author" : [ "Raychelle Harris", "Heidi M Holmes", "Donna M Mertens." ],
      "venue" : "Sign Language Studies, 9(2):104– 131.",
      "citeRegEx" : "Harris et al\\.,? 2009",
      "shortCiteRegEx" : "Harris et al\\.",
      "year" : 2009
    }, {
      "title" : "An isolated-signing RGBD dataset of 100 American Sign Language signs produced by fluent ASL signers",
      "author" : [ "Saad Hassan", "Larwan Berke", "Elahe Vahdani", "Longlong Jing", "Yingli Tian", "Matt Huenerfauth." ],
      "venue" : "Proceedings of the LREC2020 9th Workshop",
      "citeRegEx" : "Hassan et al\\.,? 2020",
      "shortCiteRegEx" : "Hassan et al\\.",
      "year" : 2020
    }, {
      "title" : "Video-based sign language recognition without temporal segmentation",
      "author" : [ "Jie Huang", "Wengang Zhou", "Qilin Zhang", "Houqiang Li", "Weiping Li." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.",
      "citeRegEx" : "Huang et al\\.,? 2018",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2018
    }, {
      "title" : "Avoiding linguistic neglect of deaf children",
      "author" : [ "Tom Humphries", "Poorna Kushalnagar", "Gaurav Mathur", "Donna Jo Napoli", "Carol Padden", "Christian Rathmann", "Scott Smith." ],
      "venue" : "Social Service Review, 90(4):589–619.",
      "citeRegEx" : "Humphries et al\\.,? 2016",
      "shortCiteRegEx" : "Humphries et al\\.",
      "year" : 2016
    }, {
      "title" : "A dataset for linguistic understanding, visual evaluation, and recognition of sign languages: The k-rsl",
      "author" : [ "Alfarabi Imashev", "Medet Mukushev", "Vadim Kimmelman", "Anara Sandygulova." ],
      "venue" : "Proceedings of the 24th Conference on Computational",
      "citeRegEx" : "Imashev et al\\.,? 2020",
      "shortCiteRegEx" : "Imashev et al\\.",
      "year" : 2020
    }, {
      "title" : "Approaches to the anonymisation of sign language corpora",
      "author" : [ "Amy Isard." ],
      "venue" : "Proceedings of the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community,",
      "citeRegEx" : "Isard.,? 2020",
      "shortCiteRegEx" : "Isard.",
      "year" : 2020
    }, {
      "title" : "Evolution of mechanical fingerspelling hands for people who are deaf-blind",
      "author" : [ "David L Jaffe." ],
      "venue" : "Journal of rehabilitation research and development, 31(3):236–244.",
      "citeRegEx" : "Jaffe.,? 1994",
      "shortCiteRegEx" : "Jaffe.",
      "year" : 1994
    }, {
      "title" : "Toward a phonetic representation of signs: Sequentiality and contrast",
      "author" : [ "Robert E Johnson", "Scott K Liddell." ],
      "venue" : "Sign Language Studies, 11(2):241–274.",
      "citeRegEx" : "Johnson and Liddell.,? 2011",
      "shortCiteRegEx" : "Johnson and Liddell.",
      "year" : 2011
    }, {
      "title" : "From archive to corpus: Transcription and annotation in the creation of signed language corpora",
      "author" : [ "Trevor Johnston." ],
      "venue" : "International journal of corpus linguistics, 15(1):106–131.",
      "citeRegEx" : "Johnston.,? 2010",
      "shortCiteRegEx" : "Johnston.",
      "year" : 2010
    }, {
      "title" : "Auslan corpus annotation guidelines",
      "author" : [ "Trevor Johnston", "Louise De Beuzeville." ],
      "venue" : "Auslan Corpus.",
      "citeRegEx" : "Johnston and Beuzeville.,? 2016",
      "shortCiteRegEx" : "Johnston and Beuzeville.",
      "year" : 2016
    }, {
      "title" : "Australian Sign Language (Auslan): An introduction to sign language linguistics",
      "author" : [ "Trevor Johnston", "Adam Schembri." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Johnston and Schembri.,? 2007",
      "shortCiteRegEx" : "Johnston and Schembri.",
      "year" : 2007
    }, {
      "title" : "On defining lexeme in a signed language",
      "author" : [ "Trevor Johnston", "Adam C Schembri." ],
      "venue" : "Sign language & linguistics, 2(2):115–185.",
      "citeRegEx" : "Johnston and Schembri.,? 1999",
      "shortCiteRegEx" : "Johnston and Schembri.",
      "year" : 1999
    }, {
      "title" : "Urubu sign language",
      "author" : [ "Jim Kakumasu." ],
      "venue" : "International journal of American linguistics, 34(4):275– 281.",
      "citeRegEx" : "Kakumasu.,? 1968",
      "shortCiteRegEx" : "Kakumasu.",
      "year" : 1968
    }, {
      "title" : "Information structure in russian sign language and sign language of the netherlands",
      "author" : [ "Vadim Kimmelman" ],
      "venue" : null,
      "citeRegEx" : "Kimmelman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kimmelman.",
      "year" : 2014
    }, {
      "title" : "Neural sign language translation based on human keypoint estimation",
      "author" : [ "Sang-Ki Ko", "Chang Jo Kim", "Hyedong Jung", "Choongsang Cho." ],
      "venue" : "Applied Sciences, 9(13):2683.",
      "citeRegEx" : "Ko et al\\.,? 2019",
      "shortCiteRegEx" : "Ko et al\\.",
      "year" : 2019
    }, {
      "title" : "Quantitative survey of the state of the art in sign language recognition",
      "author" : [ "Oscar Koller." ],
      "venue" : "arXiv preprint arXiv:2008.09918.",
      "citeRegEx" : "Koller.,? 2020",
      "shortCiteRegEx" : "Koller.",
      "year" : 2020
    }, {
      "title" : "Public dgs corpus: Annotation conventions",
      "author" : [ "Reiner Konrad", "Thomas Hanke", "Gabriele Langer", "Susanne König", "Lutz König", "Rie Nishio", "Anja Regen." ],
      "venue" : "Technical report, Project Note AP03-2018-01, DGS-Korpus project, IDGS, Hamburg University.",
      "citeRegEx" : "Konrad et al\\.,? 2018",
      "shortCiteRegEx" : "Konrad et al\\.",
      "year" : 2018
    }, {
      "title" : "Innovations in deaf studies: The role of deaf scholars",
      "author" : [ "Annelies Kusters", "Maartje De Meulder", "Dai O’Brien" ],
      "venue" : null,
      "citeRegEx" : "Kusters et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kusters et al\\.",
      "year" : 2017
    }, {
      "title" : "Word-level deep sign language recognition from video: A new large-scale dataset and methods comparison",
      "author" : [ "Dongxu Li", "Cristian Rodriguez", "Xin Yu", "Hongdong Li." ],
      "venue" : "The IEEE Winter Conference on Applications of Computer Vision, pages 1459–1469.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "American sign language: The phonological base",
      "author" : [ "Scott K Liddell", "Robert E Johnson." ],
      "venue" : "Sign language studies, 64(1):195–277.",
      "citeRegEx" : "Liddell and Johnson.,? 1989",
      "shortCiteRegEx" : "Liddell and Johnson.",
      "year" : 1989
    }, {
      "title" : "Gesture in sign language discourse",
      "author" : [ "Scott K Liddell", "Melanie Metzger." ],
      "venue" : "Journal of pragmatics, 30(6):657–697.",
      "citeRegEx" : "Liddell and Metzger.,? 1998",
      "shortCiteRegEx" : "Liddell and Metzger.",
      "year" : 1998
    }, {
      "title" : "Grammar, gesture, and meaning in American Sign Language",
      "author" : [ "Scott K Liddell" ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Liddell,? 2003",
      "shortCiteRegEx" : "Liddell",
      "year" : 2003
    }, {
      "title" : "Lexical comparison of signs from american, australian, british and new zealand sign languages",
      "author" : [ "David McKee", "Graeme Kennedy." ],
      "venue" : "The signs of language revisited: An anthology to honor Ursula Bellugi and Edward Klima, pages 49–76.",
      "citeRegEx" : "McKee and Kennedy.,? 2000",
      "shortCiteRegEx" : "McKee and Kennedy.",
      "year" : 2000
    }, {
      "title" : "From meaning to signs and back: Lexicography and the swedish sign language corpus",
      "author" : [ "Johanna Mesch", "Lars Wallin." ],
      "venue" : "Proceedings of the 5th Workshop on the Representation and Processing of Sign Languages: Interactions between Corpus and",
      "citeRegEx" : "Mesch and Wallin.,? 2012",
      "shortCiteRegEx" : "Mesch and Wallin.",
      "year" : 2012
    }, {
      "title" : "Gloss annotations in the swedish sign language corpus",
      "author" : [ "Johanna Mesch", "Lars Wallin." ],
      "venue" : "International Journal of Corpus Linguistics, 20(1):102– 120.",
      "citeRegEx" : "Mesch and Wallin.,? 2015",
      "shortCiteRegEx" : "Mesch and Wallin.",
      "year" : 2015
    }, {
      "title" : "Detecting and identifying sign languages through visual features",
      "author" : [ "Caio DD Monteiro", "Christy Maria Mathew", "Ricardo Gutierrez-Osuna", "Frank Shipman." ],
      "venue" : "2016 IEEE International Symposium on Multimedia (ISM), pages 287–290. IEEE.",
      "citeRegEx" : "Monteiro et al\\.,? 2016",
      "shortCiteRegEx" : "Monteiro et al\\.",
      "year" : 2016
    }, {
      "title" : "Emphatic fingerspelling as code-mixing in american sign language",
      "author" : [ "Kathryn Montemurro", "Diane Brentari." ],
      "venue" : "Proceedings of the Linguistic Society of America, 3(1):61–1.",
      "citeRegEx" : "Montemurro and Brentari.,? 2018",
      "shortCiteRegEx" : "Montemurro and Brentari.",
      "year" : 2018
    }, {
      "title" : "Realtime sign language detection using human pose estimation",
      "author" : [ "Amit Moryossef", "Ioannis Tsochantaridis", "Roee Aharoni", "Sarah Ebling", "Srini Narayanan." ],
      "venue" : "European Conference on Computer Vision, pages 237–248. Springer.",
      "citeRegEx" : "Moryossef et al\\.,? 2020",
      "shortCiteRegEx" : "Moryossef et al\\.",
      "year" : 2020
    }, {
      "title" : "Data augmentation for sign language gloss translation",
      "author" : [ "Amit Moryossef", "Kayo Yin", "Graham Neubig", "Yoav Goldberg." ],
      "venue" : "arXiv preprint arXiv:2105.07476.",
      "citeRegEx" : "Moryossef et al\\.,? 2021",
      "shortCiteRegEx" : "Moryossef et al\\.",
      "year" : 2021
    }, {
      "title" : "The importance of signed languages for deaf children and their families",
      "author" : [ "Joseph J Murray", "Wyatte C Hall", "Kristin Snoddon." ],
      "venue" : "The Hearing Journal, 73(3):30–32.",
      "citeRegEx" : "Murray et al\\.,? 2020",
      "shortCiteRegEx" : "Murray et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatic sign language analysis: A survey and the future beyond lexical meaning",
      "author" : [ "Sylvie CW Ong", "Surendra Ranganath." ],
      "venue" : "IEEE Computer Architecture Letters, 27(06):873–891.",
      "citeRegEx" : "Ong and Ranganath.,? 2005",
      "shortCiteRegEx" : "Ong and Ranganath.",
      "year" : 2005
    }, {
      "title" : "Prosodic correlates of sentences in signed languages: A literature review and suggestions for new types of studies",
      "author" : [ "Ellen Ormel", "Onno Crasborn." ],
      "venue" : "Sign Language Studies, 12(2):279–315.",
      "citeRegEx" : "Ormel and Crasborn.,? 2012",
      "shortCiteRegEx" : "Ormel and Crasborn.",
      "year" : 2012
    }, {
      "title" : "Interaction of Morphology and Syntax in American Sign Language",
      "author" : [ "C. Padden." ],
      "venue" : "Outstanding Disc Linguistics Series. Garland.",
      "citeRegEx" : "Padden.,? 1988",
      "shortCiteRegEx" : "Padden.",
      "year" : 1988
    }, {
      "title" : "The asl lexicon",
      "author" : [ "Carol A Padden." ],
      "venue" : "Sign language & linguistics, 1(1):39–60.",
      "citeRegEx" : "Padden.,? 1998",
      "shortCiteRegEx" : "Padden.",
      "year" : 1998
    }, {
      "title" : "Deaf in America",
      "author" : [ "Carol A Padden", "Tom Humphries." ],
      "venue" : "Harvard University Press.",
      "citeRegEx" : "Padden and Humphries.,? 1988",
      "shortCiteRegEx" : "Padden and Humphries.",
      "year" : 1988
    }, {
      "title" : "Sign language recognition and translation: A multidisciplined approach from the field of artificial intelligence",
      "author" : [ "Becky Sue Parton." ],
      "venue" : "Journal of deaf studies and deaf education, 11(1):94–101.",
      "citeRegEx" : "Parton.,? 2006",
      "shortCiteRegEx" : "Parton.",
      "year" : 2006
    }, {
      "title" : "Fingerspelled word recognition through rapid serial visual presentation: RSVP",
      "author" : [ "Carol J Patrie", "Robert E Johnson." ],
      "venue" : "DawnSignPress.",
      "citeRegEx" : "Patrie and Johnson.,? 2011",
      "shortCiteRegEx" : "Patrie and Johnson.",
      "year" : 2011
    }, {
      "title" : "Articulated people detection and pose estimation: Reshaping the future",
      "author" : [ "Leonid Pishchulin", "Arjun Jain", "Mykhaylo Andriluka", "Thorsten Thorm ä hlen", "Bernt Schiele." ],
      "venue" : "2012 IEEE Conference on Computer Vision and Pattern Recognition, pages",
      "citeRegEx" : "Pishchulin et al\\.,? 2012",
      "shortCiteRegEx" : "Pishchulin et al\\.",
      "year" : 2012
    }, {
      "title" : "MLS: A Large-Scale Multilingual Dataset for Speech Research",
      "author" : [ "Vineel Pratap", "Qiantong Xu", "Anuroop Sriram", "Gabriel Synnaeve", "Ronan Collobert." ],
      "venue" : "Proc. Interspeech 2020, pages 2757– 2761.",
      "citeRegEx" : "Pratap et al\\.,? 2020",
      "shortCiteRegEx" : "Pratap et al\\.",
      "year" : 2020
    }, {
      "title" : "Hamburg notation system for sign language: Development of a sign writing with computer application",
      "author" : [ "Siegmund Prillwitz", "Heiko Zienert." ],
      "venue" : "Current trends in European Sign Language Research. Proceedings of the 3rd European Congress on Sign Lan-",
      "citeRegEx" : "Prillwitz and Zienert.,? 1990",
      "shortCiteRegEx" : "Prillwitz and Zienert.",
      "year" : 1990
    }, {
      "title" : "Efficient voice activity detection algorithms using long-term speech information",
      "author" : [ "Javier Ramırez", "José C Segura", "Carmen Benıtez", "Angel De La Torre", "Antonio Rubio." ],
      "venue" : "Speech communication, 42(34):271–287.",
      "citeRegEx" : "Ramırez et al\\.,? 2004",
      "shortCiteRegEx" : "Ramırez et al\\.",
      "year" : 2004
    }, {
      "title" : "Sign language recognition: A deep survey",
      "author" : [ "Razieh Rastgoo", "Kourosh Kiani", "Sergio Escalera." ],
      "venue" : "Expert Systems with Applications, page 113794.",
      "citeRegEx" : "Rastgoo et al\\.,? 2020",
      "shortCiteRegEx" : "Rastgoo et al\\.",
      "year" : 2020
    }, {
      "title" : "A featural approach to verb agreement in signed languages",
      "author" : [ "Christian Rathmann", "Gaurav Mathur." ],
      "venue" : "Theoretical Linguistics, 37(3-4):197–208.",
      "citeRegEx" : "Rathmann and Mathur.,? 2011",
      "shortCiteRegEx" : "Rathmann and Mathur.",
      "year" : 2011
    }, {
      "title" : "Discourse in signed languages",
      "author" : [ "Cynthia B Roy." ],
      "venue" : "Gallaudet University Press.",
      "citeRegEx" : "Roy.,? 2011",
      "shortCiteRegEx" : "Roy.",
      "year" : 2011
    }, {
      "title" : "Prosody and syntax in sign languages",
      "author" : [ "Wendy Sandler." ],
      "venue" : "Transactions of the philological society, 108(3):298–328.",
      "citeRegEx" : "Sandler.,? 2010",
      "shortCiteRegEx" : "Sandler.",
      "year" : 2010
    }, {
      "title" : "The phonological organization of sign languages",
      "author" : [ "Wendy Sandler." ],
      "venue" : "Language and linguistics compass, 6(3):162–182.",
      "citeRegEx" : "Sandler.,? 2012",
      "shortCiteRegEx" : "Sandler.",
      "year" : 2012
    }, {
      "title" : "Automatic sign segmentation from continuous signing via multiple sequence alignment",
      "author" : [ "Pinar Santemiz", "Oya Aran", "Murat Saraclar", "Lale Akarun." ],
      "venue" : "2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops,",
      "citeRegEx" : "Santemiz et al\\.,? 2009",
      "shortCiteRegEx" : "Santemiz et al\\.",
      "year" : 2009
    }, {
      "title" : "Everybody sign now: Translating spoken language to photo realistic sign language video",
      "author" : [ "Ben Saunders", "Necati Cihan Camgöz", "Richard Bowden." ],
      "venue" : "arXiv preprint arXiv:2011.09846.",
      "citeRegEx" : "Saunders et al\\.,? 2020a",
      "shortCiteRegEx" : "Saunders et al\\.",
      "year" : 2020
    }, {
      "title" : "Progressive transformers for endto-end sign language production",
      "author" : [ "Ben Saunders", "Necati Cihan Camgöz", "Richard Bowden." ],
      "venue" : "European Conference on Computer Vision, pages 687–705.",
      "citeRegEx" : "Saunders et al\\.,? 2020b",
      "shortCiteRegEx" : "Saunders et al\\.",
      "year" : 2020
    }, {
      "title" : "Indicating verbs as typologically unique constructions: Reconsidering verb ‘agreement’in sign languages",
      "author" : [ "Adam Schembri", "Kearsy Cormier", "Jordan Fenlon." ],
      "venue" : "Glossa: a journal of general linguistics, 3(1).",
      "citeRegEx" : "Schembri et al\\.,? 2018",
      "shortCiteRegEx" : "Schembri et al\\.",
      "year" : 2018
    }, {
      "title" : "Fingerspelling recognition in the wild with iterative visual attention",
      "author" : [ "B. Shi", "A. Martinez Del Rio", "J. Keane", "D. Brentari", "G. Shakhnarovich", "K. Livescu." ],
      "venue" : "ICCV.",
      "citeRegEx" : "Shi et al\\.,? 2019",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2019
    }, {
      "title" : "American sign language fingerspelling recognition in the wild",
      "author" : [ "B. Shi", "A. Martinez Del Rio", "J. Keane", "J. Michaux", "G. Shakhnarovich D. Brentari", "K. Livescu." ],
      "venue" : "SLT.",
      "citeRegEx" : "Shi et al\\.,? 2018",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2018
    }, {
      "title" : "Signs across America: A look at regional differences in American Sign Language",
      "author" : [ "Edgar H Shroyer", "Susan P Shroyer." ],
      "venue" : "Gallaudet University Press.",
      "citeRegEx" : "Shroyer and Shroyer.,? 1984",
      "shortCiteRegEx" : "Shroyer and Shroyer.",
      "year" : 1984
    }, {
      "title" : "Autsl: A large scale multi-modal turkish sign language dataset and baseline methods",
      "author" : [ "Ozge Mercanoglu Sincan", "Hacer Yalim Keles." ],
      "venue" : "IEEE Access, 8:181340–181355.",
      "citeRegEx" : "Sincan and Keles.,? 2020",
      "shortCiteRegEx" : "Sincan and Keles.",
      "year" : 2020
    }, {
      "title" : "A statistical model-based voice activity detection",
      "author" : [ "Jongseo Sohn", "Nam Soo Kim", "Wonyong Sung." ],
      "venue" : "IEEE signal processing letters, 6(1):1–3.",
      "citeRegEx" : "Sohn et al\\.,? 1999",
      "shortCiteRegEx" : "Sohn et al\\.",
      "year" : 1999
    }, {
      "title" : "A DRT analysis of discourse referents and anaphora resolution in sign language",
      "author" : [ "Markus Steinbach", "Edgar Onea." ],
      "venue" : "Journal of Semantics, 33(3):409– 448.",
      "citeRegEx" : "Steinbach and Onea.,? 2016",
      "shortCiteRegEx" : "Steinbach and Onea.",
      "year" : 2016
    }, {
      "title" : "Sign Language Structure: An Outline of the Visual Communication Systems of the American Deaf",
      "author" : [ "Jr. Stokoe", "William C." ],
      "venue" : "The Journal of Deaf Studies and Deaf Education, 10(1):3–37.",
      "citeRegEx" : "Stokoe and C.,? 1960",
      "shortCiteRegEx" : "Stokoe and C.",
      "year" : 1960
    }, {
      "title" : "Sign language structure: An outline of the visual communication systems of the american deaf",
      "author" : [ "William C Stokoe Jr." ],
      "venue" : "Journal of deaf studies and deaf education, 10(1):3–37.",
      "citeRegEx" : "Jr.,? 2005",
      "shortCiteRegEx" : "Jr.",
      "year" : 2005
    }, {
      "title" : "Sign language production using neural machine translation and generative adversarial networks",
      "author" : [ "Stephanie Stoll", "Necati Cihan Camgöz", "Simon Hadfield", "Richard Bowden." ],
      "venue" : "Proceedings of the 29th British Machine Vision Conference (BMVC",
      "citeRegEx" : "Stoll et al\\.,? 2018",
      "shortCiteRegEx" : "Stoll et al\\.",
      "year" : 2018
    }, {
      "title" : "Text2sign: towards sign language production using neural machine translation and generative adversarial networks",
      "author" : [ "Stephanie Stoll", "Necati Cihan Camgöz", "Simon Hadfield", "Richard Bowden." ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "Stoll et al\\.,? 2020",
      "shortCiteRegEx" : "Stoll et al\\.",
      "year" : 2020
    }, {
      "title" : "The classifier system in american sign language",
      "author" : [ "Ted Supalla." ],
      "venue" : "Noun classes and categorization, 7:181–214.",
      "citeRegEx" : "Supalla.,? 1986",
      "shortCiteRegEx" : "Supalla.",
      "year" : 1986
    }, {
      "title" : "Lessons in sign writing",
      "author" : [ "Valerie Sutton." ],
      "venue" : "SignWriting.",
      "citeRegEx" : "Sutton.,? 1990",
      "shortCiteRegEx" : "Sutton.",
      "year" : 1990
    }, {
      "title" : "Ms-asl: A large-scale data set and benchmark for understanding american sign language",
      "author" : [ "Hamid Vaezi Joze", "Oscar Koller." ],
      "venue" : "The British Machine Vision Conference (BMVC).",
      "citeRegEx" : "Joze and Koller.,? 2019",
      "shortCiteRegEx" : "Joze and Koller.",
      "year" : 2019
    }, {
      "title" : "The phonetics of fingerspelling, volume 4",
      "author" : [ "Sherman Wilcox." ],
      "venue" : "John Benjamins Publishing.",
      "citeRegEx" : "Wilcox.,? 1992",
      "shortCiteRegEx" : "Wilcox.",
      "year" : 1992
    }, {
      "title" : "Rethinking classifiers",
      "author" : [ "Sherman Wilcox", "Sarah Hafer." ],
      "venue" : "emmorey, k.(ed.).(2003). perspectives on classifier constructions in sign languages. mahwah, nj: Lawrence erlbaum associates. 332 pages. hardcover.",
      "citeRegEx" : "Wilcox and Hafer.,? 2004",
      "shortCiteRegEx" : "Wilcox and Hafer.",
      "year" : 2004
    }, {
      "title" : "Skeleton-based chinese sign language recognition and generation for bidirectional communication between deaf and hearing people",
      "author" : [ "Qinkun Xiao", "Minying Qin", "Yuting Yin." ],
      "venue" : "Neural Networks, 125:41–55.",
      "citeRegEx" : "Xiao et al\\.,? 2020",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you sign: sign language translation with transformers",
      "author" : [ "Kayo Yin", "Jesse Read." ],
      "venue" : "Sign Language Recognition, Translation and Production (SLRTP) Workshop-Extended Abstracts, volume 4.",
      "citeRegEx" : "Yin and Read.,? 2020a",
      "shortCiteRegEx" : "Yin and Read.",
      "year" : 2020
    }, {
      "title" : "Better sign language translation with STMC-transformer",
      "author" : [ "Kayo Yin", "Jesse Read." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 5975–5989, Barcelona,",
      "citeRegEx" : "Yin and Read.,? 2020b",
      "shortCiteRegEx" : "Yin and Read.",
      "year" : 2020
    }, {
      "title" : "Neural sign language synthesis: Words are our glosses",
      "author" : [ "Jan Zelinka", "Jakub Kanis." ],
      "venue" : "The IEEE Winter Conference on Applications of Computer Vision, pages 3395–3403.",
      "citeRegEx" : "Zelinka and Kanis.,? 2020",
      "shortCiteRegEx" : "Zelinka and Kanis.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 71,
      "context" : "This disregards the preferences of the Deaf communities who strongly prefer to communicate in signed languages both online and for in-person day-to-day interactions, among themselves and when interacting with spoken language communities (Padden and Humphries, 1988; Glickman and Hall, 2018).",
      "startOffset" : 237,
      "endOffset" : 290
    }, {
      "referenceID" : 32,
      "context" : "This disregards the preferences of the Deaf communities who strongly prefer to communicate in signed languages both online and for in-person day-to-day interactions, among themselves and when interacting with spoken language communities (Padden and Humphries, 1988; Glickman and Hall, 2018).",
      "startOffset" : 237,
      "endOffset" : 290
    }, {
      "referenceID" : 37,
      "context" : "Needless to say, in addressing this research area, researchers should work alongside and under the direction of deaf communities, and to the benefit of the signing communities’ interest above all (Harris et al., 2009).",
      "startOffset" : 196,
      "endOffset" : 217
    }, {
      "referenceID" : 40,
      "context" : "languages continue to do harm and subjects many to linguistic neglect (Humphries et al., 2016).",
      "startOffset" : 70,
      "endOffset" : 94
    }, {
      "referenceID" : 35,
      "context" : "This language deprivation can lead to life-long consequences on the cognitive, linguistic, socioemotional, and academic development of the deaf (Hall et al., 2017).",
      "startOffset" : 144,
      "endOffset" : 163
    }, {
      "referenceID" : 30,
      "context" : "The introduction of a continuous signed language benchmark dataset (Forster et al., 2014; Cihan Camgöz et al., 2018), coupled with the advent of deep learning for visual processing, lead to increased efforts to recognize signed expressions from videos.",
      "startOffset" : 67,
      "endOffset" : 116
    }, {
      "referenceID" : 52,
      "context" : "Recent surveys on SLP mostly review these different approaches for sign language recognition developed by the CV community (Koller, 2020; Rastgoo et al., 2020; Adaloglou et al., 2020).",
      "startOffset" : 123,
      "endOffset" : 183
    }, {
      "referenceID" : 78,
      "context" : "Recent surveys on SLP mostly review these different approaches for sign language recognition developed by the CV community (Koller, 2020; Rastgoo et al., 2020; Adaloglou et al., 2020).",
      "startOffset" : 123,
      "endOffset" : 183
    }, {
      "referenceID" : 0,
      "context" : "Recent surveys on SLP mostly review these different approaches for sign language recognition developed by the CV community (Koller, 2020; Rastgoo et al., 2020; Adaloglou et al., 2020).",
      "startOffset" : 123,
      "endOffset" : 183
    }, {
      "referenceID" : 5,
      "context" : "Simultaneity Though an ASL sign takes about twice as long to produce than an English word, the rates of transmission of information between the two languages are similar (Bellugi and Fischer, 1972).",
      "startOffset" : 170,
      "endOffset" : 197
    }, {
      "referenceID" : 79,
      "context" : "ing space to a non-present referent and by pointing to this region to refer to it (Rathmann and Mathur, 2011; Schembri et al., 2018).",
      "startOffset" : 82,
      "endOffset" : 132
    }, {
      "referenceID" : 86,
      "context" : "ing space to a non-present referent and by pointing to this region to refer to it (Rathmann and Mathur, 2011; Schembri et al., 2018).",
      "startOffset" : 82,
      "endOffset" : 132
    }, {
      "referenceID" : 24,
      "context" : "bodying the referents using body shift or eye gaze (Dudis, 2004; Liddell and Metzger, 1998).",
      "startOffset" : 51,
      "endOffset" : 91
    }, {
      "referenceID" : 57,
      "context" : "bodying the referents using body shift or eye gaze (Dudis, 2004; Liddell and Metzger, 1998).",
      "startOffset" : 51,
      "endOffset" : 91
    }, {
      "referenceID" : 28,
      "context" : "Spatial referencing also impacts morphology when the directionality of a verb depends on the location of the reference to its subject and/or object (de Beuzeville, 2008; Fenlon et al., 2018): for example, a directional verb can move from the location of its subject and ending at the location of its object.",
      "startOffset" : 148,
      "endOffset" : 190
    }, {
      "referenceID" : 97,
      "context" : "Another way anaphoric entities are referenced in sign language is by using classifiers or depicting signs (Supalla, 1986; Wilcox and Hafer, 2004; Roy, 2011) that help describe the characteristics of the",
      "startOffset" : 106,
      "endOffset" : 156
    }, {
      "referenceID" : 101,
      "context" : "Another way anaphoric entities are referenced in sign language is by using classifiers or depicting signs (Supalla, 1986; Wilcox and Hafer, 2004; Roy, 2011) that help describe the characteristics of the",
      "startOffset" : 106,
      "endOffset" : 156
    }, {
      "referenceID" : 80,
      "context" : "Another way anaphoric entities are referenced in sign language is by using classifiers or depicting signs (Supalla, 1986; Wilcox and Hafer, 2004; Roy, 2011) that help describe the characteristics of the",
      "startOffset" : 106,
      "endOffset" : 156
    }, {
      "referenceID" : 20,
      "context" : "To quote someone other than oneself, signers perform role shift (Cormier et al., 2015), where they may physically shift in space to mark the distinction, and take on some characteristics of the people they are representing.",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 70,
      "context" : "into the signed languages themselves as another linguistic strategy (Padden, 1998; Montemurro and Brentari, 2018).",
      "startOffset" : 68,
      "endOffset" : 113
    }, {
      "referenceID" : 63,
      "context" : "into the signed languages themselves as another linguistic strategy (Padden, 1998; Montemurro and Brentari, 2018).",
      "startOffset" : 68,
      "endOffset" : 113
    }, {
      "referenceID" : 42,
      "context" : "mains an open problem, limiting the possibility of making these videos publicly available (Isard, 2020).",
      "startOffset" : 90,
      "endOffset" : 103
    }, {
      "referenceID" : 74,
      "context" : "While motion capture equipment can often provide better quality pose estimation, it is expensive and intrusive, and estimating pose from videos is the preferred method currently (Pishchulin et al., 2012; Chen et al., 2017; Cao et al., 2019; Güler et al., 2018).",
      "startOffset" : 178,
      "endOffset" : 260
    }, {
      "referenceID" : 18,
      "context" : "While motion capture equipment can often provide better quality pose estimation, it is expensive and intrusive, and estimating pose from videos is the preferred method currently (Pishchulin et al., 2012; Chen et al., 2017; Cao et al., 2019; Güler et al., 2018).",
      "startOffset" : 178,
      "endOffset" : 260
    }, {
      "referenceID" : 17,
      "context" : "While motion capture equipment can often provide better quality pose estimation, it is expensive and intrusive, and estimating pose from videos is the preferred method currently (Pishchulin et al., 2012; Chen et al., 2017; Cao et al., 2019; Güler et al., 2018).",
      "startOffset" : 178,
      "endOffset" : 260
    }, {
      "referenceID" : 33,
      "context" : "While motion capture equipment can often provide better quality pose estimation, it is expensive and intrusive, and estimating pose from videos is the preferred method currently (Pishchulin et al., 2012; Chen et al., 2017; Cao et al., 2019; Güler et al., 2018).",
      "startOffset" : 178,
      "endOffset" : 260
    }, {
      "referenceID" : 98,
      "context" : "While various universal (Sutton, 1990; Prillwitz and Zienert, 1990) and language-specific no-",
      "startOffset" : 24,
      "endOffset" : 67
    }, {
      "referenceID" : 76,
      "context" : "While various universal (Sutton, 1990; Prillwitz and Zienert, 1990) and language-specific no-",
      "startOffset" : 24,
      "endOffset" : 67
    }, {
      "referenceID" : 49,
      "context" : "tation systems (Stokoe Jr, 2005; Kakumasu, 1968; Bergman, 1979) have been proposed, no writing system has been adopted widely by any sign language community, and the lack of standard hinders the exchange and unification of resources and ap-",
      "startOffset" : 15,
      "endOffset" : 63
    }, {
      "referenceID" : 6,
      "context" : "tation systems (Stokoe Jr, 2005; Kakumasu, 1968; Bergman, 1979) have been proposed, no writing system has been adopted widely by any sign language community, and the lack of standard hinders the exchange and unification of resources and ap-",
      "startOffset" : 15,
      "endOffset" : 63
    }, {
      "referenceID" : 98,
      "context" : "Figure 2 depicts two universal notation systems: SignWriting (Sutton, 1990), a two-dimensional pictographic system, and HamNoSys (Prillwitz and Zienert, 1990), a linear stream of graphemes that was designed to be readable by machines.",
      "startOffset" : 61,
      "endOffset" : 75
    }, {
      "referenceID" : 76,
      "context" : "Figure 2 depicts two universal notation systems: SignWriting (Sutton, 1990), a two-dimensional pictographic system, and HamNoSys (Prillwitz and Zienert, 1990), a linear stream of graphemes that was designed to be readable by machines.",
      "startOffset" : 129,
      "endOffset" : 158
    }, {
      "referenceID" : 61,
      "context" : "While various sign language corpus projects have provided gloss annotation guidelines (Mesch and Wallin, 2015; Johnston and De Beuzeville, 2016; Konrad et al., 2018), again, there is no single agreed-upon standard.",
      "startOffset" : 86,
      "endOffset" : 165
    }, {
      "referenceID" : 53,
      "context" : "While various sign language corpus projects have provided gloss annotation guidelines (Mesch and Wallin, 2015; Johnston and De Beuzeville, 2016; Konrad et al., 2018), again, there is no single agreed-upon standard.",
      "startOffset" : 86,
      "endOffset" : 165
    }, {
      "referenceID" : 104,
      "context" : "body posture, eye gaze) or spatial relations, which leads to an inevitable information loss up to a semantic level that affects downstream performance on SLP tasks (Yin and Read, 2020b).",
      "startOffset" : 164,
      "endOffset" : 185
    }, {
      "referenceID" : 60,
      "context" : "Bilingual dictionaries for signed language (Mesch and Wallin, 2012; Fenlon et al., 2015; Crasborn et al., 2016; Gutierrez-Sigut et al., 2016)",
      "startOffset" : 43,
      "endOffset" : 141
    }, {
      "referenceID" : 27,
      "context" : "Bilingual dictionaries for signed language (Mesch and Wallin, 2012; Fenlon et al., 2015; Crasborn et al., 2016; Gutierrez-Sigut et al., 2016)",
      "startOffset" : 43,
      "endOffset" : 141
    }, {
      "referenceID" : 34,
      "context" : "Bilingual dictionaries for signed language (Mesch and Wallin, 2012; Fenlon et al., 2015; Crasborn et al., 2016; Gutierrez-Sigut et al., 2016)",
      "startOffset" : 43,
      "endOffset" : 141
    }, {
      "referenceID" : 23,
      "context" : "They can be synthetically created (Dreuw et al., 2006) or mined from online resources (Shi et al.",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 39,
      "context" : "They are synthesized (Ebling et al., 2018; Huang et al., 2018; Sincan and Keles, 2020; Hassan et al., 2020) or mined from online resources (Vaezi Joze and Koller, 2019; Li et al.",
      "startOffset" : 21,
      "endOffset" : 107
    }, {
      "referenceID" : 90,
      "context" : "They are synthesized (Ebling et al., 2018; Huang et al., 2018; Sincan and Keles, 2020; Hassan et al., 2020) or mined from online resources (Vaezi Joze and Koller, 2019; Li et al.",
      "startOffset" : 21,
      "endOffset" : 107
    }, {
      "referenceID" : 38,
      "context" : "They are synthesized (Ebling et al., 2018; Huang et al., 2018; Sincan and Keles, 2020; Hassan et al., 2020) or mined from online resources (Vaezi Joze and Koller, 2019; Li et al.",
      "startOffset" : 21,
      "endOffset" : 107
    }, {
      "referenceID" : 55,
      "context" : ", 2020) or mined from online resources (Vaezi Joze and Koller, 2019; Li et al., 2020), and can be used for isolated sign language recognition or for contrastive analysis of minimal signing pairs (Imashev et al.",
      "startOffset" : 39,
      "endOffset" : 85
    }, {
      "referenceID" : 41,
      "context" : ", 2020), and can be used for isolated sign language recognition or for contrastive analysis of minimal signing pairs (Imashev et al., 2020).",
      "startOffset" : 117,
      "endOffset" : 139
    }, {
      "referenceID" : 2,
      "context" : "continuous sign corpora are extremely limited, containing 4-6 orders of magnitude fewer sentence pairs than similar corpora for spoken language machine translation (Arivazhagan et al., 2019).",
      "startOffset" : 164,
      "endOffset" : 190
    }, {
      "referenceID" : 75,
      "context" : "datasets contain up to 50,000 hours of recordings (Pratap et al., 2020), the largest continuous sign language corpus contain only 1,150 hours, and only 50 of them are publicly available (Hanke et al.",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 36,
      "context" : ", 2020), the largest continuous sign language corpus contain only 1,150 hours, and only 50 of them are publicly available (Hanke et al., 2020).",
      "startOffset" : 122,
      "endOffset" : 142
    }, {
      "referenceID" : 30,
      "context" : ", 2020) or recorded in studio conditions (Forster et al., 2014; Cihan Camgöz et al., 2018), which does not account for noise in real-life conditions.",
      "startOffset" : 41,
      "endOffset" : 90
    }, {
      "referenceID" : 10,
      "context" : "While recent detection models (Borg and Camilleri, 2019; Moryossef et al., 2020) achieve high performance, we lack well-annotated data that include interference and distractions with non-signing instances for proper evaluation.",
      "startOffset" : 30,
      "endOffset" : 80
    }, {
      "referenceID" : 64,
      "context" : "While recent detection models (Borg and Camilleri, 2019; Moryossef et al., 2020) achieve high performance, we lack well-annotated data that include interference and distractions with non-signing instances for proper evaluation.",
      "startOffset" : 30,
      "endOffset" : 80
    }, {
      "referenceID" : 91,
      "context" : "spoken languages is voice activity detection (VAD) (Sohn et al., 1999; Ramırez et al., 2004), the detection of when a human voice is used in an audio signal.",
      "startOffset" : 51,
      "endOffset" : 92
    }, {
      "referenceID" : 77,
      "context" : "spoken languages is voice activity detection (VAD) (Sohn et al., 1999; Ramırez et al., 2004), the detection of when a human voice is used in an audio signal.",
      "startOffset" : 51,
      "endOffset" : 92
    }, {
      "referenceID" : 31,
      "context" : "Existing works utilize the distribution of phonemes (Gebre et al., 2013) or activity maps in signing space (Monteiro et al.",
      "startOffset" : 52,
      "endOffset" : 72
    }, {
      "referenceID" : 62,
      "context" : ", 2013) or activity maps in signing space (Monteiro et al., 2016) to identify the signed language in videos.",
      "startOffset" : 42,
      "endOffset" : 65
    }, {
      "referenceID" : 59,
      "context" : "However, these methods only rely on low-level visual features, while signed languages have several distinctive features on a linguistic level, such as lexical or structural differences (McKee and Kennedy, 2000; Kimmelman, 2014; Ferreira-Brito, 1984; Shroyer and Shroyer, 1984) which have not been explored for this task.",
      "startOffset" : 185,
      "endOffset" : 276
    }, {
      "referenceID" : 50,
      "context" : "However, these methods only rely on low-level visual features, while signed languages have several distinctive features on a linguistic level, such as lexical or structural differences (McKee and Kennedy, 2000; Kimmelman, 2014; Ferreira-Brito, 1984; Shroyer and Shroyer, 1984) which have not been explored for this task.",
      "startOffset" : 185,
      "endOffset" : 276
    }, {
      "referenceID" : 29,
      "context" : "However, these methods only rely on low-level visual features, while signed languages have several distinctive features on a linguistic level, such as lexical or structural differences (McKee and Kennedy, 2000; Kimmelman, 2014; Ferreira-Brito, 1984; Shroyer and Shroyer, 1984) which have not been explored for this task.",
      "startOffset" : 185,
      "endOffset" : 276
    }, {
      "referenceID" : 89,
      "context" : "However, these methods only rely on low-level visual features, while signed languages have several distinctive features on a linguistic level, such as lexical or structural differences (McKee and Kennedy, 2000; Kimmelman, 2014; Ferreira-Brito, 1984; Shroyer and Shroyer, 1984) which have not been explored for this task.",
      "startOffset" : 185,
      "endOffset" : 276
    }, {
      "referenceID" : 83,
      "context" : "Current methods resort to segmenting units loosely mapped to signed language units (Santemiz et al., 2009; Farag and Brock, 2019; Bull et al., 2020), and does not leverage reliable linguistic predictors of sentence boundaries such as prosody in signed languages (i.",
      "startOffset" : 83,
      "endOffset" : 148
    }, {
      "referenceID" : 26,
      "context" : "Current methods resort to segmenting units loosely mapped to signed language units (Santemiz et al., 2009; Farag and Brock, 2019; Bull et al., 2020), and does not leverage reliable linguistic predictors of sentence boundaries such as prosody in signed languages (i.",
      "startOffset" : 83,
      "endOffset" : 148
    }, {
      "referenceID" : 13,
      "context" : "Current methods resort to segmenting units loosely mapped to signed language units (Santemiz et al., 2009; Farag and Brock, 2019; Bull et al., 2020), and does not leverage reliable linguistic predictors of sentence boundaries such as prosody in signed languages (i.",
      "startOffset" : 83,
      "endOffset" : 148
    }, {
      "referenceID" : 41,
      "context" : "Recognition Sign language recognition (SLR) detects and label signs from a video, either on isolated (Imashev et al., 2020; Sincan and Keles, 2020) or continuous (Cui et al.",
      "startOffset" : 101,
      "endOffset" : 147
    }, {
      "referenceID" : 90,
      "context" : "Recognition Sign language recognition (SLR) detects and label signs from a video, either on isolated (Imashev et al., 2020; Sincan and Keles, 2020) or continuous (Cui et al.",
      "startOffset" : 101,
      "endOffset" : 147
    }, {
      "referenceID" : 22,
      "context" : ", 2020; Sincan and Keles, 2020) or continuous (Cui et al., 2017; Camgöz et al., 2018, 2020b) signs.",
      "startOffset" : 46,
      "endOffset" : 92
    }, {
      "referenceID" : 69,
      "context" : "Though some previous works have referred to this as “sign language translation”, recognition merely determines the associated label of each sign, without handling the syntax and morphology of the signed language (Padden, 1988) to create a spoken language output.",
      "startOffset" : 212,
      "endOffset" : 226
    }, {
      "referenceID" : 65,
      "context" : "perform translation with glosses (Camgöz et al., 2018, 2020b; Yin and Read, 2020a,b; Moryossef et al., 2021) or on pose estimations and sign articulators from videos (Ko et al.",
      "startOffset" : 33,
      "endOffset" : 108
    }, {
      "referenceID" : 51,
      "context" : ", 2021) or on pose estimations and sign articulators from videos (Ko et al., 2019; Camgöz et al., 2020a), but do not, for instance, handle spa-",
      "startOffset" : 65,
      "endOffset" : 104
    }, {
      "referenceID" : 15,
      "context" : ", 2021) or on pose estimations and sign articulators from videos (Ko et al., 2019; Camgöz et al., 2020a), but do not, for instance, handle spa-",
      "startOffset" : 65,
      "endOffset" : 104
    }, {
      "referenceID" : 105,
      "context" : ", 2018, 2020), while more recent methods (Saunders et al., 2020b,a; Zelinka and Kanis, 2020; Xiao et al., 2020) autoregressively decode a sequence of poses from an input text.",
      "startOffset" : 41,
      "endOffset" : 111
    }, {
      "referenceID" : 102,
      "context" : ", 2018, 2020), while more recent methods (Saunders et al., 2020b,a; Zelinka and Kanis, 2020; Xiao et al., 2020) autoregressively decode a sequence of poses from an input text.",
      "startOffset" : 41,
      "endOffset" : 111
    }, {
      "referenceID" : 104,
      "context" : "While existing SLP systems and datasets often use glosses as discrete lexical units of signed phrases, this poses three significant problems: (1) linear, single-dimensional glosses cannot fully capture the spatial constructions of signed languages, which downgrades downstream performance (Yin and Read, 2020b); (2) glosses are language-specific and requiring new glossing models for each language is impractical given the",
      "startOffset" : 289,
      "endOffset" : 310
    }, {
      "referenceID" : 48,
      "context" : "sal, and standardized method for tokenization of signed languages, all the while considering: how do we define lexical units in signed languages? (Johnston and Schembri, 1999; Johnston, 2010) To what degree can phonological units of signed languages be mapped to lexical units? Should we model the articulators of signs separately or together? What are the cross-linguistic phonological differences to consider? To what extent can ideas used in automatic speech recognition be applied to signed",
      "startOffset" : 146,
      "endOffset" : 191
    }, {
      "referenceID" : 45,
      "context" : "sal, and standardized method for tokenization of signed languages, all the while considering: how do we define lexical units in signed languages? (Johnston and Schembri, 1999; Johnston, 2010) To what degree can phonological units of signed languages be mapped to lexical units? Should we model the articulators of signs separately or together? What are the cross-linguistic phonological differences to consider? To what extent can ideas used in automatic speech recognition be applied to signed",
      "startOffset" : 146,
      "endOffset" : 191
    }, {
      "referenceID" : 92,
      "context" : "Evidence suggests that classic theoretical frameworks, such as discourse representation theory, may extend to signed languages (Steinbach and Onea, 2016).",
      "startOffset" : 127,
      "endOffset" : 153
    }, {
      "referenceID" : 3,
      "context" : "SLP is especially subject to three of the core technical challenges in multimodal machine learning (Baltrušaitis et al., 2018): translation - how do we map visual-gestural information to/from audio-oral and textual information? alignment - how do we relate signed language units to spoken language units? co-learning - can we transfer high-resource spoken language knowledge to signed language? On the other hand, meaning in spoken languages is not only conveyed through speech or text but also through the visual modality.",
      "startOffset" : 99,
      "endOffset" : 126
    }, {
      "referenceID" : 36,
      "context" : "On the other hand, the Public DGS Corpus (Hanke et al., 2020) is an open-domain (1)",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 85,
      "context" : "Although Transformers achieve encouraging results on RWTH-PHOENIXWeather 2014T (Saunders et al., 2020b; Camgöz et al., 2020a), they fail on more realistic, opendomain data.",
      "startOffset" : 79,
      "endOffset" : 125
    }, {
      "referenceID" : 15,
      "context" : "Although Transformers achieve encouraging results on RWTH-PHOENIXWeather 2014T (Saunders et al., 2020b; Camgöz et al., 2020a), they fail on more realistic, opendomain data.",
      "startOffset" : 79,
      "endOffset" : 125
    }, {
      "referenceID" : 8,
      "context" : "Deaf community and avoid exploiting their language as a commodity (Bird, 2020).",
      "startOffset" : 66,
      "endOffset" : 78
    }, {
      "referenceID" : 25,
      "context" : "These approaches oversimplify the rich grammar of signed languages, promote the misconception that signs are solely expressed through the hands, and are considered by the Deaf community as a manifestation of audism, where it is the signers who must make the extra effort to wear additional sensors to be understood by non-signers (Erard, 2017).",
      "startOffset" : 330,
      "endOffset" : 343
    }, {
      "referenceID" : 37,
      "context" : "Building Collaboration Deaf collaborations and leadership are essential for developing signed language technologies to ensure they address the community’s needs and will be adopted, and that they do not rely on misconceptions or inaccuracies about signed language (Harris et al., 2009; Kusters et al., 2017).",
      "startOffset" : 264,
      "endOffset" : 307
    }, {
      "referenceID" : 54,
      "context" : "Building Collaboration Deaf collaborations and leadership are essential for developing signed language technologies to ensure they address the community’s needs and will be adopted, and that they do not rely on misconceptions or inaccuracies about signed language (Harris et al., 2009; Kusters et al., 2017).",
      "startOffset" : 264,
      "endOffset" : 307
    } ],
    "year" : 2021,
    "abstractText" : "Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community to include signed languages as a research area with high social and scientific impact. We first discuss the linguistic properties of signed languages to consider during their modeling. Then, we review the limitations of current SLP models and identify the open challenges to extend NLP to signed languages. Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of research.",
    "creator" : "LaTeX with hyperref"
  }
}