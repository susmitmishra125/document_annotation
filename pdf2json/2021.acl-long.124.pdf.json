{
  "name" : "2021.acl-long.124.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Improving Formality Style Transfer with Context-Aware Rule Injection",
    "authors" : [ "Zonghai Yao", "Hong Yu" ],
    "emails" : [ "zonghaiyao@cs.umass.edu", "yu@uml.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1561–1570\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1561"
    }, {
      "heading" : "1 Introduction",
      "text" : "Many user-generated data deviate from standard language in vocabulary, grammar, and language style. For example, abbreviations, phonetic substitutions, Hashtags, acronyms, internet language, ellipsis, and spelling errors, etc are common in tweets (Ghani et al., 2019; Muller et al., 2019; Han et al., 2013; Liu et al., 2020). Such irregularity leads to a significant challenge in applying existing language models pre-trained on large-scale corpora dominated with regular vocabulary and grammar. One solution is using formality style transfer (FST) (Rao and Tetreault, 2018), which aims to transfer the input text’s style from the informal domain to the formal domain. This may improve the downstream NLP applications such as information extraction, text classification and question answering.\nA common challenge for FST is low resource (Wu et al., 2020; Malmi et al., 2020; Wang et al., 2020). Therefore, approaches that integrate external knowledge, such as rules, have been developed. However, existing work (Rao and Tetreault, 2018; Wang et al., 2019) deploy context-insensitive rule\ninjection methods (CIRI). As shown in Figure 1, when we try to use CIRI-based FST as the preprocessing for user-generated data in the sentiment classification task, according to the rule detection system, ”extro” has two suggested changes ”extra” or ”extrovert” and ”intro” corresponds to either ”introduction” or ”introvert.” The existing CIRI-based FST models would arbitrarily choose rules following first come first served (FCFS). As such, the input ”always, always they think I an extro, but Im a big intro actually” could be translated wrongly as ”they always think I am an extra, but actually, I am a big introduction.” This leads to the wrong sentiment classification since the FST result completely destroys the original input’s semantic meaning.\nIn this work, we propose Context-Aware Rule Injection (CARI), an end-to-end BERT-based encoder and decoder model that is able to learn to select optimal rules based on context. As shown in Figure 1, CARI chooses rules based on context. With CARI-based FST, pre-trained models can perform better on the downstream natural language processing (NLP) tasks. In this case, CARI outputs the correctly translated text ”they always think I am an extrovert, but actually, I am a big introvert,” which helps the BERT-based classification model have the correct sentiment classification.\nIn this study, we performed both intrinsic and extrinsic evaluation of existing FST models and compared them with the CARI model. The intrinsic evaluation results showed that CARI improved the state-of-the-art results from 72.7 and 77.2 to 74.31 and 78.05, respectively, on two domains of a FST benchmark dataset. For the extrinsic evaluation, we introduced several tweet sentiment analysis tasks. Considering that tweet data is typical informal user-generated data, and regular pre-trained models are usually pre-trained on formal English corpora, using FST as a preprocessing step of tweet data is expected to improve the performance of reg-\nular pre-trained models on tweet downstream tasks. We regard measuring such improvement as the extrinsic evaluation. The extrinsic evaluation results showed that using CARI model as the prepocessing step improved the performance for both BERT and RoBERTa on several downstream tweet sentiment classification tasks. Our contributions are as follows:\n1. We propose a new method, CARI, to integrate rules for pre-trained language models. CARI is context-aware and can be trained end-to-end with the downstream NLP applications. 2. We have achieved new state-of-the-art results for FST on the benchmark GYAFC dataset. 3. We are the first to evaluate FST methods with extrinsic evaluation and we show that CARI outperformed existing rule-based FST approaches for sentiment classification."
    }, {
      "heading" : "2 Related work",
      "text" : "Rule-based Formality Style Transfer In the past few years, style-transfer generation has attracted increasing attention in NLP research. Early work transfers between modern English and the Shakespeare style with a phrase-based machine translation system (Xu et al., 2012). Recently, style transfer has been more recognized as a controllable text generation problem (Hu et al., 2017), where the style may be designated as sentiment (Fu et al., 2018), tense (Hu et al., 2017), or even general syntax (Bao et al., 2019; Chen et al., 2019). Formality style transfer has been mostly driven by the Grammarly’s Yahoo Answers Formality Corpus (GYAFC) (Rao and Tetreault, 2018). Since it is a parallel corpus, FST usually takes a seq2seq-like approach (Niu et al., 2018; Xu et al., 2019). Existing research attempts to integrate the rules into the\nmodel because the GYAFC is low resource. However, rule matching and selection are context insensitive in previous methods (Wang et al., 2019). This paper focuses on developing methods for contextaware rule selection.\nEvaluating Style Transfer Previous work on style transfer (Xu et al., 2012; Jhamtani et al., 2017; Niu et al., 2017; Sennrich et al., 2016a) has repurposed the machine translation metric BLEU (Papineni et al., 2002) and the paraphrase metric PINC (Chen and Dolan, 2011) for evaluation. Xu et al. (2012) introduced three evaluation metrics based on cosine similarity, language model and logistic regression. They also introduced human judgments for adequacy, fluency and style (Xu et al., 2012; Niu et al., 2017). Rao and Tetreault (2018) evaluated formality, fluency and meaning on the GYAFC dataset. Recent work on the GYAFC dataset (Wang et al., 2019; Zhang et al., 2020) mostly used BLEU as the evaluation metrics for FST. However, all aforementioned work focused on intrinsic evaluations. Our work has in addition evaluated FST extrinsically for downstream NLP applications.\nLexical Normalisation Lexical normalisation (Han and Baldwin, 2011; Baldwin et al., 2015) is the task of translating non-canonical words into canonical ones. Like FST, lexical normalisation can also be used to preprocess user-generated data. The MoNoise model (van der Goot and van Noord, 2017) is a state-of-the-art model based on featurebased Random Forest. The model ranks candidates provided by modules such as a spelling checker (aspell), a n-gram based language model and word embeddings trained on millions of tweets. Unlike FST, MoNoise and other lexical normalisation models can not change data’s language style. In this study, we explore the importance of language style transfer for user-generated data by comparing the results of MoNoise and FST models on tweets NLP downstream tasks.\nImproving language models’ performance for user-generated data User-generated data often deviate from standard language. In addition to the formality style transfer, there are some other ways to solve this problem (Eisenstein, 2013). Finetuning on downstream tasks with a user-generated dataset is most straightforward, but this is not easy for many supervised tasks without a large amount of accurately labeled data. Another method is to fine-tune pre-trained models on the target domain\ncorpora (Gururangan et al., 2020). However, it also requires sizable training data, which could be resource expensive (Sohoni et al., 2019; Dai et al., 2019; Yao et al., 2020)."
    }, {
      "heading" : "3 Approach",
      "text" : "For the downstream NLP tasks where input is user-generated data, we first used the FST model for preprocessing, and then fine-tuned the pretrained models (BERT and RoBERTa) with both the original data Dori and the FST data DFST , which were concatenated with a special token [SEP ], forming an input like (Dori[SEP ]DFST ).\nFor the formality style transfer task, we use the BERT-initialized encoder paired with the BERTinitialized decoder (Rothe et al., 2020) as the Seq2Seq model. All weights were initialized from a public BERT-Base checkpoint (Devlin et al., 2019). The only variable that was initialized randomly is the encoder-decoder attention. Here, we describe CARI and several baseline methods of injecting rules into the Seq2Seq model."
    }, {
      "heading" : "3.1 No Rule (NR)",
      "text" : "First we fine-tuned the BERT model with only the original user-generated input. Given an informal input xi and formal output yi, we fine-tuned the model with {(xi, yi)}Mi=0, where M is the number of data."
    }, {
      "heading" : "3.2 Context Insensitive Methods",
      "text" : "For baseline models, we experimented with two state-of-the-art methods for injecting rules. We followed Rao and Tetreault (2018) to create a set of rules to convert original data xi to prepossessed data x′i by rules, and then fine-tune the model with parallel data {(x′i, yi)}Mi=0. This is called Rule Base (RB) method. The prepossessed data, however, serves as a Markov blanket, i.e., the system is unaware of the original data, provided that only the prepossessed one is given. Therefore, the rule detection system could easily make mistakes and introduce noise.\nWang et al. (2019) improved the RB by concatenating the original text xi with the text processed by rules x′i with a special token [SEP ] in between, forming a input like (xi [SEP ] x′i). In this way, the model can make use of a rule detection system but also recognize its errors during the fine-tuning. This is called Rule Concatenation (RCAT) method. However, both RB and RCAT\nmethods are context insensitive, the rules were selected arbitrarily. In Figure 1 CIRI part, ”extra” and ”introduction” were incorrectly selected. This greatly limits the performance of the rule-based methods."
    }, {
      "heading" : "3.3 Context-Aware Rule Injection (CARI)",
      "text" : "As shown in Figure 1, the input of CARI consists of the original sentence xi and supplementary information. Suppose that ri is an exhaustive list of the rules that are successfully matched on xi. We make ri = {(ti,j , ci,j , ai,j)}Nj=0, where N is the total number of matched rules in ri. Here, ti,j and ci,j are the corresponding matched text and context in the original sentence, respectively, for every matched rule in ri, and ai,j are the corresponding alternative texts for every matched rule in ri. Each supplementary information is composed of one alternative text ai,j and its corresponding context ci,j . We connect all the supplementary information with the special token [SEP ] and then connect it after the original input. In this way, we form an input like (xi [SEP ] ai,1, ci,1 [SEP ]... [SEP ] ai,j , ci,j). Finally, the concatenated sequence and the corresponding formal reference yi serve as a parallel text pair to fine-tune the Seq2Seq model. Like RCAT, CARI can also use rule detection system and recognize its errors during the fine-tuning. Furthermore, since we keep all rules in the input, CARI is able to dynamically identify which rule to use, maximizing the use of the rule detection system."
    }, {
      "heading" : "4 Experimental setup",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "For the intrinsic evaluation, we used the GYAFC dataset.1 It consists of handcrafted informal-formal sentence pairs in two domains, namely, Entertainment & Music (E&M) and Family & Relationship (F&R). Table 1 shows the statistics of the training, validation, and test sets for the GYAFC dataset. In the validation and test sets of GYAFC, each sentence has four references. For better exploring the data requirements of different methods to combine rules, we followed Zhang et al. (2020) and used the back translation method (Sennrich et al., 2016b) to obtain additional 100,000 data for training. For rule detection system, we used the grammarbot API,2, and Grammarly3 to help us create a set of rules.\n1https://github.com/raosudha89/GYAFC-corpus 2https://www.grammarbot.io/ 3https://www.grammarly.com/\nFor the extrinsic evaluation, we used two datasets for sentiment classification: SemEval2018 Task 1: Affect in Tweets EI-oc (Mohammad et al., 2018), and Task 3: Irony Detection in English Tweets (Van Hee et al., 2018). Table 1 shows the statistics of the training, validation, and test set for the two datasets. We normalized two tweet NLP classification datasets by translating word tokens of user mentions and web/url links into special tokens @USER and HTTPURL, respectively, and converting emotion icon tokens into corresponding strings."
    }, {
      "heading" : "4.2 Fine-tuning models",
      "text" : "We employed the transformers library (Wolf et al., 2019) to independently fine-tune the BERTbased encoder and decoder model for each method in 20,000 steps (intrinsic evaluation), and fine-tune the BERT-based and RoBERTa-based classification models for each tweet sentiment analysis task in 10,000 steps (extrinsic evaluation). We used the Adam algorithm (Kingma and Ba, 2014) to train our model with a batch size 32. We set the learning rate to 1e-5 and stop training if validation loss increases in two successive epoch. We computed the task performance every 1,000 steps on the validation set. Finally, we selected the best model checkpoint to compute the performance score on the test set. We repeated this fine-tuning process three times with different random seeds and reported each final test result as an average over the test scores from the three runs. During inference, we use beam search with a beam size of 4 and beam\nwidth of 6 to generate sentences. The whole experiment is carried out on 1 TITANX GPU. Each FST model finished training within 12 hours."
    }, {
      "heading" : "4.3 Intrinsic Evaluation Baselines",
      "text" : "We used two state-of-the-art models, which were also relevant to our methods, as the strong intrinsic baseline models.\nruleGPT Like RCAT, Wang et al. (2019) aimed to solve the problem of information loss and noise caused by directly using rules as normalization in preprocessing. They put forward the GPT (Radford et al., 2019) based methods to concatenate the original input sentence and the sentence preprocessed by the rule detection system. Like the CIRI methods (RB, RCAT), their methods could not make full use of rules since they were also context-insensitive when selecting rules.\nBT + M-Task + F-Dis Zhang et al. (2020) used three data augmentation methods, Back translation (Sennrich et al., 2016b), Formality discrimination, and Multi-task transfer to solve the low-resource problem. In our experiments, we also use the back translation method to obtain additional data because we want to verify the impact on the amount of training data required when using different methods to combine rules."
    }, {
      "heading" : "4.4 Extrinsic Evaluation Baselines",
      "text" : "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) are two typical regular language models pre-trained on large-scale regular formal text corpora, like BooksCorpus (Zhu et al., 2015) and English Wikipedia. The user-generated data, such as tweets, deviate from the formal text in vocabulary, grammar, and language style. As a result, regular language models often perform poorly on user-generated data. FST aims to generate a formal sentence given an informal one, while keeping its semantic meaning. A good FST result is expected to make regular language models perform better on user-generated data. For the extrinsic evaluation, we chose BERT and RoBERTa as the basic model. We introduced several tweet sentiment analysis tasks to explore the FST models’ ability to transfer the user-generated data from the informal domain to the formal domain. Ideally, FST results for tweet data can improve the performance of BERT and RoBERTa on tweet sentiment analysis tasks. We regard measuring such improvement as the extrinsic evaluations. Besides, tweet data have much unique information, like Emoji, Hashtags, ellipsis, etc., which are not available in the GYAFC dataset. So in the extrinsic evaluation result analysis, although the final scores of FST-BERT and FST-RoBERTa were good, we paid more attention to the improvement of their performance before and after using FST, rather than the scores.\nWe used two different kinds of state-of-the-art\nmethods as our extrinsic evaluation baselines.\nSeerNet and UCDCC We used the best results in the SemEval-2018 workshop as the first comparison method. For the task Affect in Tweets EI-o, the baseline is SeerNet (Duppada et al., 2018), and for the task Irony Detection in English Tweets, the baseline is UCDCC (Ghosh and Veale, 2018).\nMoNoise MoNoise (van der Goot and van Noord, 2017) is the state-of-the-art model for the lexical normalization (Baldwin et al., 2015), which aimed to translate non-canonical words into canonical ones. Like the FST model, MoNoise can also be used as the prepossessing step in tweet classification tasks to normalize tweet input. So we used MoNoise as another comparison method."
    }, {
      "heading" : "5 Experimental results",
      "text" : ""
    }, {
      "heading" : "5.1 Intrinsic Evaluation",
      "text" : "Figure 2 showed the validation performance on both the E&M and the F&R domain. Compared to\nthe NR, the RB did not significantly improve. As we discussed above, even though the rule detection system will bring some useful information, it will also make mistakes and introduce noise. RB has no access to the original data, so it cannot distinguish helpful information from noise and mistakes. On the contrary, both RCAT and CARI have access to the original data, so their results improved a lot compared with RB. CARI had a better result compared to the RCAT. This is because RCAT is context insensitive while CARI is context-aware when selecting rules to modify the original input. Therefore, CARI is able to learn to select optimal rules based on context, while RCAT may miss using many correct rules with its pipeline prepossessing step for rules.\nFigure 2 also showed the relationship between the different methods and the different training size. Compared with the NR method, the three methods which use rules can reach their best performance with smaller training size. This result showed the positive effect of adding rules in the low-resource situation of the GYAFC dataset. Moreover, CARI used larger training set to reach its best performance than RB and RCAT, since it needed more data to learn how to dynamically identify which rule to use.\nIn Table 4, we explored how large the context window size was appropriate for the CARI method on GYAFC dataset. The results showed that for both domains when the window size reaches two (taking two tokens each from the text before and after), Seq2Seq model can well match all rules with the corresponding position in the original input and\nselect the correct one to use."
    }, {
      "heading" : "5.2 Extrinsic Evaluation",
      "text" : "Table 2 showed the effectiveness of using the CARI as the preprocessing step for user-generated data on applying regular pre-trained models (BERT and RoBERTa) on the downstream NLP tasks.\nCompared with the previous state-of-the-art results (UCDCC and SeerNet), the results of using BERT and RoBERTa directly were often very poor, since BERT and RoBERTa were only pre-trained on regular text corpora. Tweet data has the very different vocabulary, grammar, and language style from the regular text corpora, so it is hard for BERT and RoBERTa to have good performance with small amount of fine-tuning data.\nThe results of RCAT and CARI showed that FST can help BERT and RoBERTa improve their performance on tweet data, because they can transfer tweets into more formal text while keeping the original intention as much as possible. CARI performed better than RCAT, which was also in line with the results of intrinsic evaluation. This result also showed the rationality of our extrinsic evaluation metrics.\nComparing the results of MoNoise with BERT and RoBERTa, the input prepossessed by MoNoise can not help the pre-trained model to improve effectively. We think that this is because the lexical normalization models represented by MoNoise only translate non-canonical words on tweet data into canonical ones. Therefore, MoNoise can basically solve the problem of different vocabulary between regular text corpora and user-generated data, but it can not effectively solve the problem of different grammar and language style. As a result, for BERT and RoBERTa, even though there is no Out-of-Vocabulary (OOV) problem in the input data processed by MoNoise, they still can not accurately understand the meaning of the input.\nThis result confirmed the previous view that lexical normalization on tweets is a lossy trans-\nlation task (Owoputi et al., 2013; Nguyen et al., 2020). On the contrary, the positive results of the FST methods also showed that FST is more suitable as the downstream task prepossessing step of user-generated data. Because FST models need to transfer the informal language style to a formal one while keeping its semantic meaning, which makes a good FST model can ideally handle all the problems from vocabulary, grammar, and language style. This can help most language models pre-trained on the regular corpus, like BERT and RoBERTa, perform better on user-generated data."
    }, {
      "heading" : "5.3 Manual Analysis",
      "text" : "The prior evaluation results reveal the relative performance differences between approaches. Here, we identify trends per and between approaches. We sample 50 informal sentences total from the datasets and then analyze the outputs from each model. We present several representative results in Table 5.\nExamples 1 and 2 showed that, for BERT and RoBERTa, FST models are more suitable for preprocessing user-generated data than lexical normalization models. In example 1, both methods can effectively deal with the problem at the vocabulary level (”2” to ”to,” ”ur” to ”your,” and ”U” to ”you”). However, in example 2, FST can further transform source data into a more familiar language style for BERT and RoBERTa, which is not available in the current lexical normalization methods such as MoNoise.\nExample 3 showed the importance of injecting rules into the FST models. The word ”idiodic” is a misspelling of ”idiotic,” which is an OOV. Therefore, without the help of rules, the model can not understand the source data’s meanings and produced the wrong final output ”I do not understand your question.”\nExample 4 showed the importance of context for rule selection. The word ”concern” provides the required context to understand that ”exo” refers to an ”extra” ticket. So the CARI-based model can choose the right one (”exo” to ”extra”).\nExamples 5 and 6 showed the shortcomings of CARI. In example 5, the rule detection system did not provide the information that the ”fidy center” should be ”50 Cent (American rapper)”, so CARI delivered the wrong result. Even though CARI helps mitigate the data low resource challenge, it faces the challenge on its own. CARI depends\non the quality of the rules, and in this case, no rule exists that links ”fidy” to ”50.” In example 6, CARI mistakenly selected the rule ”eat me,” but not ”eat it.” This example also demonstrates the data sparsity that CARI faces. Here ”eat me” is more commonly used than ”eat it.”"
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this work, we proposed the Context-Aware Rule Injection(CARI), an innovative method for formality style transfer (FST) by injecting multiple rules into an end-to-end BERT-based encoder and decoder model. The intrinsic evaluation showed our CARI method achieved the highest performance with previous metrics on the FST benchmark dataset. Besides, we were the first to evaluate FST methods with extrinsic evaluation and specifically on the sentiment classification tasks. The extrinsic evaluation results showed that using the CARI-based FST as the preprocessing step outperformed existing rule-based FST approaches. Our results showed the rationality of adding such extensive evaluation."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors are grateful to Hadi Amiri (University of Massachusetts, Lowell) for his expert help in processing Twitter data, and to UMass BioNLP Group for lots of meaningful discussions.\nThis work was supported in part by the Center for Intelligent Information Retrieval. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor."
    } ],
    "references" : [ {
      "title" : "Shared tasks of the 2015 workshop on noisy user-generated text: Twitter lexical normalization and named entity recognition",
      "author" : [ "Timothy Baldwin", "Marie-Catherine de Marneffe", "Bo Han", "Young-Bum Kim", "Alan Ritter", "Wei Xu." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Baldwin et al\\.,? 2015",
      "shortCiteRegEx" : "Baldwin et al\\.",
      "year" : 2015
    }, {
      "title" : "Generating sentences from disentangled syntactic and semantic spaces",
      "author" : [ "Yu Bao", "Hao Zhou", "Shujian Huang", "Lei Li", "Lili Mou", "Olga Vechtomova", "Xinyu Dai", "Jiajun Chen." ],
      "venue" : "arXiv preprint arXiv:1907.05789.",
      "citeRegEx" : "Bao et al\\.,? 2019",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2019
    }, {
      "title" : "Collecting highly parallel data for paraphrase evaluation",
      "author" : [ "David Chen", "William B Dolan." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 190–200.",
      "citeRegEx" : "Chen and Dolan.,? 2011",
      "shortCiteRegEx" : "Chen and Dolan.",
      "year" : 2011
    }, {
      "title" : "Controllable paraphrase generation with a syntactic exemplar",
      "author" : [ "Mingda Chen", "Qingming Tang", "Sam Wiseman", "Kevin Gimpel." ],
      "venue" : "arXiv preprint arXiv:1906.00565.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Transformer-xl: Attentive language models beyond a fixed-length context",
      "author" : [ "Zihang Dai", "Zhilin Yang", "Yiming Yang", "Jaime Carbonell", "Quoc V Le", "Ruslan Salakhutdinov." ],
      "venue" : "arXiv preprint arXiv:1901.02860.",
      "citeRegEx" : "Dai et al\\.,? 2019",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Seernet at semeval-2018 task 1: Domain adaptation for affect in tweets",
      "author" : [ "Venkatesh Duppada", "Royal Jain", "Sushant Hiray." ],
      "venue" : "arXiv preprint arXiv:1804.06137.",
      "citeRegEx" : "Duppada et al\\.,? 2018",
      "shortCiteRegEx" : "Duppada et al\\.",
      "year" : 2018
    }, {
      "title" : "What to do about bad language on the internet",
      "author" : [ "Jacob Eisenstein." ],
      "venue" : "Proceedings of the 2013 conference of the North American Chapter of the association for computational linguistics: Human language technologies, pages 359–369.",
      "citeRegEx" : "Eisenstein.,? 2013",
      "shortCiteRegEx" : "Eisenstein.",
      "year" : 2013
    }, {
      "title" : "Style transfer in text: Exploration and evaluation",
      "author" : [ "Zhenxin Fu", "Xiaoye Tan", "Nanyun Peng", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.",
      "citeRegEx" : "Fu et al\\.,? 2018",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2018
    }, {
      "title" : "Social media big data analytics: A survey",
      "author" : [ "Norjihan Abdul Ghani", "Suraya Hamid", "Ibrahim Abaker Targio Hashem", "Ejaz Ahmed." ],
      "venue" : "Computers in Human Behavior, 101:417–428.",
      "citeRegEx" : "Ghani et al\\.,? 2019",
      "shortCiteRegEx" : "Ghani et al\\.",
      "year" : 2019
    }, {
      "title" : "Ironymagnet at semeval-2018 task 3: A siamese network for irony detection in social media",
      "author" : [ "Aniruddha Ghosh", "Tony Veale." ],
      "venue" : "Proceedings of The 12th International Workshop on Semantic Evaluation, pages 570–575.",
      "citeRegEx" : "Ghosh and Veale.,? 2018",
      "shortCiteRegEx" : "Ghosh and Veale.",
      "year" : 2018
    }, {
      "title" : "Monoise: Modeling noise using a modular normalization system",
      "author" : [ "Rob van der Goot", "Gertjan van Noord." ],
      "venue" : "arXiv preprint arXiv:1710.03476.",
      "citeRegEx" : "Goot and Noord.,? 2017",
      "shortCiteRegEx" : "Goot and Noord.",
      "year" : 2017
    }, {
      "title" : "Don’t stop pretraining",
      "author" : [ "Suchin Gururangan", "Ana Marasović", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A Smith" ],
      "venue" : null,
      "citeRegEx" : "Gururangan et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2020
    }, {
      "title" : "Lexical normalisation of short text messages: Makn sens a# twitter",
      "author" : [ "Bo Han", "Timothy Baldwin." ],
      "venue" : "Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pages 368–378.",
      "citeRegEx" : "Han and Baldwin.,? 2011",
      "shortCiteRegEx" : "Han and Baldwin.",
      "year" : 2011
    }, {
      "title" : "Lexical normalization for social media text",
      "author" : [ "Bo Han", "Paul Cook", "Timothy Baldwin." ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology (TIST), 4(1):1–27.",
      "citeRegEx" : "Han et al\\.,? 2013",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2013
    }, {
      "title" : "Toward controlled generation of text",
      "author" : [ "Zhiting Hu", "Zichao Yang", "Xiaodan Liang", "Ruslan Salakhutdinov", "Eric P Xing." ],
      "venue" : "International Conference on Machine Learning, pages 1587–1596. PMLR.",
      "citeRegEx" : "Hu et al\\.,? 2017",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2017
    }, {
      "title" : "Shakespearizing modern language using copy-enriched sequence-to-sequence models",
      "author" : [ "Harsh Jhamtani", "Varun Gangal", "Eduard Hovy", "Eric Nyberg." ],
      "venue" : "arXiv preprint arXiv:1707.01161.",
      "citeRegEx" : "Jhamtani et al\\.,? 2017",
      "shortCiteRegEx" : "Jhamtani et al\\.",
      "year" : 2017
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Named entity location prediction combining twitter and web",
      "author" : [ "Yinan Liu", "Wei Shen", "Zonghai Yao", "Jianyong Wang", "Zhenglu Yang", "Xiaojie Yuan." ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised text style transfer with padded masked language models",
      "author" : [ "Eric Malmi", "Aliaksei Severyn", "Sascha Rothe." ],
      "venue" : "arXiv preprint arXiv:2010.01054.",
      "citeRegEx" : "Malmi et al\\.,? 2020",
      "shortCiteRegEx" : "Malmi et al\\.",
      "year" : 2020
    }, {
      "title" : "Semeval2018 task 1: Affect in tweets",
      "author" : [ "Saif Mohammad", "Felipe Bravo-Marquez", "Mohammad Salameh", "Svetlana Kiritchenko." ],
      "venue" : "Proceedings of the 12th international workshop on semantic evaluation, pages 1–17.",
      "citeRegEx" : "Mohammad et al\\.,? 2018",
      "shortCiteRegEx" : "Mohammad et al\\.",
      "year" : 2018
    }, {
      "title" : "Enhancing bert for lexical normalization",
      "author" : [ "Benjamin Muller", "Benoı̂t Sagot", "Djamé Seddah" ],
      "venue" : "In Proceedings of the 5th Workshop on Noisy Usergenerated Text (W-NUT",
      "citeRegEx" : "Muller et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Muller et al\\.",
      "year" : 2019
    }, {
      "title" : "Bertweet: A pre-trained language model for english tweets",
      "author" : [ "Dat Quoc Nguyen", "Thanh Vu", "Anh Tuan Nguyen." ],
      "venue" : "arXiv preprint arXiv:2005.10200.",
      "citeRegEx" : "Nguyen et al\\.,? 2020",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2020
    }, {
      "title" : "A study of style in machine translation: Controlling the formality of machine translation output",
      "author" : [ "Xing Niu", "Marianna Martindale", "Marine Carpuat." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Niu et al\\.,? 2017",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2017
    }, {
      "title" : "Bleu: a method for automatic",
      "author" : [ "Jing Zhu" ],
      "venue" : null,
      "citeRegEx" : "Zhu.,? \\Q2002\\E",
      "shortCiteRegEx" : "Zhu.",
      "year" : 2002
    }, {
      "title" : "Dear sir or",
      "author" : [ "cal report", "OpenAI. Sudha Rao", "Joel Tetreault" ],
      "venue" : null,
      "citeRegEx" : "report et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "report et al\\.",
      "year" : 2018
    }, {
      "title" : "Low-memory neural network training: A",
      "author" : [ "Ré" ],
      "venue" : null,
      "citeRegEx" : "2019.,? \\Q2019\\E",
      "shortCiteRegEx" : "2019.",
      "year" : 2019
    }, {
      "title" : "Harnessing pre-trained neural networks with rules for formality style transfer",
      "author" : [ "Yunli Wang", "Yu Wu", "Lili Mou", "Zhoujun Li", "Wenhan Chao." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Formality style transfer with shared latent space",
      "author" : [ "Yunli Wang", "Yu Wu", "Lili Mou", "Zhoujun Li", "Wenhan Chao." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 2236–2249.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Huggingface’s transformers: Stateof-the-art natural language processing",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "Rémi Louf", "Morgan Funtowicz" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "A dataset for low-resource stylized sequence-to-sequence generation",
      "author" : [ "Yu Wu", "Yunli Wang", "Shujie Liu." ],
      "venue" : "Thirty-Fourth AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Formality style transfer with hybrid textual annotations",
      "author" : [ "Ruochen Xu", "Tao Ge", "Furu Wei." ],
      "venue" : "arXiv preprint arXiv:1903.06353.",
      "citeRegEx" : "Xu et al\\.,? 2019",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2019
    }, {
      "title" : "Paraphrasing for style",
      "author" : [ "Wei Xu", "Alan Ritter", "William B Dolan", "Ralph Grishman", "Colin Cherry." ],
      "venue" : "Proceedings of COLING 2012, pages 2899–2914.",
      "citeRegEx" : "Xu et al\\.,? 2012",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2012
    }, {
      "title" : "Zero-shot entity linking with efficient long range sequence modeling",
      "author" : [ "Zonghai Yao", "Liangliang Cao", "Huapu Pan." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 2517–2522.",
      "citeRegEx" : "Yao et al\\.,? 2020",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2020
    }, {
      "title" : "Parallel data augmentation for formality style transfer",
      "author" : [ "Yi Zhang", "Tao Ge", "Xu Sun." ],
      "venue" : "arXiv preprint arXiv:2005.07522.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "author" : [ "Yukun Zhu", "Ryan Kiros", "Rich Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "Proceedings of the IEEE inter-",
      "citeRegEx" : "Zhu et al\\.,? 2015",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "For example, abbreviations, phonetic substitutions, Hashtags, acronyms, internet language, ellipsis, and spelling errors, etc are common in tweets (Ghani et al., 2019; Muller et al., 2019; Han et al., 2013; Liu et al., 2020).",
      "startOffset" : 147,
      "endOffset" : 224
    }, {
      "referenceID" : 23,
      "context" : "For example, abbreviations, phonetic substitutions, Hashtags, acronyms, internet language, ellipsis, and spelling errors, etc are common in tweets (Ghani et al., 2019; Muller et al., 2019; Han et al., 2013; Liu et al., 2020).",
      "startOffset" : 147,
      "endOffset" : 224
    }, {
      "referenceID" : 15,
      "context" : "For example, abbreviations, phonetic substitutions, Hashtags, acronyms, internet language, ellipsis, and spelling errors, etc are common in tweets (Ghani et al., 2019; Muller et al., 2019; Han et al., 2013; Liu et al., 2020).",
      "startOffset" : 147,
      "endOffset" : 224
    }, {
      "referenceID" : 19,
      "context" : "For example, abbreviations, phonetic substitutions, Hashtags, acronyms, internet language, ellipsis, and spelling errors, etc are common in tweets (Ghani et al., 2019; Muller et al., 2019; Han et al., 2013; Liu et al., 2020).",
      "startOffset" : 147,
      "endOffset" : 224
    }, {
      "referenceID" : 32,
      "context" : "A common challenge for FST is low resource (Wu et al., 2020; Malmi et al., 2020; Wang et al., 2020).",
      "startOffset" : 43,
      "endOffset" : 99
    }, {
      "referenceID" : 21,
      "context" : "A common challenge for FST is low resource (Wu et al., 2020; Malmi et al., 2020; Wang et al., 2020).",
      "startOffset" : 43,
      "endOffset" : 99
    }, {
      "referenceID" : 30,
      "context" : "A common challenge for FST is low resource (Wu et al., 2020; Malmi et al., 2020; Wang et al., 2020).",
      "startOffset" : 43,
      "endOffset" : 99
    }, {
      "referenceID" : 29,
      "context" : "However, existing work (Rao and Tetreault, 2018; Wang et al., 2019) deploy context-insensitive rule injection methods (CIRI).",
      "startOffset" : 23,
      "endOffset" : 67
    }, {
      "referenceID" : 34,
      "context" : "Early work transfers between modern English and the Shakespeare style with a phrase-based machine translation system (Xu et al., 2012).",
      "startOffset" : 117,
      "endOffset" : 134
    }, {
      "referenceID" : 16,
      "context" : "Recently, style transfer has been more recognized as a controllable text generation problem (Hu et al., 2017), where the style may be designated as sentiment (Fu et al.",
      "startOffset" : 92,
      "endOffset" : 109
    }, {
      "referenceID" : 9,
      "context" : ", 2017), where the style may be designated as sentiment (Fu et al., 2018), tense (Hu et al.",
      "startOffset" : 56,
      "endOffset" : 73
    }, {
      "referenceID" : 16,
      "context" : ", 2018), tense (Hu et al., 2017), or even general syntax (Bao et al.",
      "startOffset" : 15,
      "endOffset" : 32
    }, {
      "referenceID" : 33,
      "context" : "Since it is a parallel corpus, FST usually takes a seq2seq-like approach (Niu et al., 2018; Xu et al., 2019).",
      "startOffset" : 73,
      "endOffset" : 108
    }, {
      "referenceID" : 29,
      "context" : "However, rule matching and selection are context insensitive in previous methods (Wang et al., 2019).",
      "startOffset" : 81,
      "endOffset" : 100
    }, {
      "referenceID" : 34,
      "context" : "Evaluating Style Transfer Previous work on style transfer (Xu et al., 2012; Jhamtani et al., 2017; Niu et al., 2017; Sennrich et al., 2016a) has repurposed the machine translation metric BLEU (Papineni et al.",
      "startOffset" : 58,
      "endOffset" : 140
    }, {
      "referenceID" : 17,
      "context" : "Evaluating Style Transfer Previous work on style transfer (Xu et al., 2012; Jhamtani et al., 2017; Niu et al., 2017; Sennrich et al., 2016a) has repurposed the machine translation metric BLEU (Papineni et al.",
      "startOffset" : 58,
      "endOffset" : 140
    }, {
      "referenceID" : 25,
      "context" : "Evaluating Style Transfer Previous work on style transfer (Xu et al., 2012; Jhamtani et al., 2017; Niu et al., 2017; Sennrich et al., 2016a) has repurposed the machine translation metric BLEU (Papineni et al.",
      "startOffset" : 58,
      "endOffset" : 140
    }, {
      "referenceID" : 2,
      "context" : ", 2002) and the paraphrase metric PINC (Chen and Dolan, 2011) for evaluation.",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 34,
      "context" : "They also introduced human judgments for adequacy, fluency and style (Xu et al., 2012; Niu et al., 2017).",
      "startOffset" : 69,
      "endOffset" : 104
    }, {
      "referenceID" : 25,
      "context" : "They also introduced human judgments for adequacy, fluency and style (Xu et al., 2012; Niu et al., 2017).",
      "startOffset" : 69,
      "endOffset" : 104
    }, {
      "referenceID" : 29,
      "context" : "Recent work on the GYAFC dataset (Wang et al., 2019; Zhang et al., 2020) mostly used BLEU as the evaluation metrics for FST.",
      "startOffset" : 33,
      "endOffset" : 72
    }, {
      "referenceID" : 36,
      "context" : "Recent work on the GYAFC dataset (Wang et al., 2019; Zhang et al., 2020) mostly used BLEU as the evaluation metrics for FST.",
      "startOffset" : 33,
      "endOffset" : 72
    }, {
      "referenceID" : 14,
      "context" : "Lexical Normalisation Lexical normalisation (Han and Baldwin, 2011; Baldwin et al., 2015) is the task of translating non-canonical words into canonical ones.",
      "startOffset" : 44,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : "Lexical Normalisation Lexical normalisation (Han and Baldwin, 2011; Baldwin et al., 2015) is the task of translating non-canonical words into canonical ones.",
      "startOffset" : 44,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : "In addition to the formality style transfer, there are some other ways to solve this problem (Eisenstein, 2013).",
      "startOffset" : 93,
      "endOffset" : 111
    }, {
      "referenceID" : 13,
      "context" : "Another method is to fine-tune pre-trained models on the target domain corpora (Gururangan et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 104
    }, {
      "referenceID" : 4,
      "context" : "However, it also requires sizable training data, which could be resource expensive (Sohoni et al., 2019; Dai et al., 2019; Yao et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 140
    }, {
      "referenceID" : 35,
      "context" : "However, it also requires sizable training data, which could be resource expensive (Sohoni et al., 2019; Dai et al., 2019; Yao et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 140
    }, {
      "referenceID" : 6,
      "context" : "All weights were initialized from a public BERT-Base checkpoint (Devlin et al., 2019).",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 22,
      "context" : "For the extrinsic evaluation, we used two datasets for sentiment classification: SemEval2018 Task 1: Affect in Tweets EI-oc (Mohammad et al., 2018), and Task 3: Irony Detection in English",
      "startOffset" : 124,
      "endOffset" : 147
    }, {
      "referenceID" : 31,
      "context" : "We employed the transformers library (Wolf et al., 2019) to independently fine-tune the BERTbased encoder and decoder model for each method in 20,000 steps (intrinsic evaluation), and fine-tune the BERT-based and RoBERTa-based classification models for each tweet sentiment analysis task in 10,000 steps (extrinsic evaluation).",
      "startOffset" : 37,
      "endOffset" : 56
    }, {
      "referenceID" : 18,
      "context" : "We used the Adam algorithm (Kingma and Ba, 2014) to train our model with a batch size 32.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 20,
      "context" : ", 2018) and RoBERTa (Liu et al., 2019) are two typical regular language models pre-trained on large-scale regular formal text corpora, like BooksCorpus (Zhu et al.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 37,
      "context" : ", 2019) are two typical regular language models pre-trained on large-scale regular formal text corpora, like BooksCorpus (Zhu et al., 2015) and",
      "startOffset" : 121,
      "endOffset" : 139
    }, {
      "referenceID" : 7,
      "context" : "For the task Affect in Tweets EI-o, the baseline is SeerNet (Duppada et al., 2018), and for the task Irony Detection in English Tweets, the baseline is UCDCC (Ghosh and Veale, 2018).",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 11,
      "context" : ", 2018), and for the task Irony Detection in English Tweets, the baseline is UCDCC (Ghosh and Veale, 2018).",
      "startOffset" : 83,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "MoNoise MoNoise (van der Goot and van Noord, 2017) is the state-of-the-art model for the lexical normalization (Baldwin et al., 2015), which aimed to translate non-canonical words into canonical ones.",
      "startOffset" : 111,
      "endOffset" : 133
    }, {
      "referenceID" : 24,
      "context" : "This result confirmed the previous view that lexical normalization on tweets is a lossy translation task (Owoputi et al., 2013; Nguyen et al., 2020).",
      "startOffset" : 105,
      "endOffset" : 148
    } ],
    "year" : 2021,
    "abstractText" : "Models pre-trained on large-scale regular text corpora often do not work well for usergenerated data where the language styles differ significantly from the mainstream text. Here we present Context-Aware Rule Injection (CARI), an innovative method for formality style transfer (FST). CARI injects multiple rules into an end-to-end BERT-based encoder and decoder model. It learns to select optimal rules based on context. The intrinsic evaluation showed that CARI achieved the new highest performance on the FST benchmark dataset. Our extrinsic evaluation showed that CARI can greatly improve the regular pretrained models’ performance on several tweet sentiment analysis tasks.",
    "creator" : "LaTeX with hyperref"
  }
}