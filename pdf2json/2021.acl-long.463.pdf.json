{
  "name" : "2021.acl-long.463.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Automatic ICD Coding via Interactive Shared Representation Networks with Self-distillation Mechanism",
    "authors" : [ "Tong Zhou", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao", "Kun Niu", "Weifeng Chong", "Shengping Liu" ],
    "emails" : [ "niukun}@bupt.edu.cn", "pengfei.cao@nlpr.ia.ac.cn", "yubo.chen@nlpr.ia.ac.cn", "kliu@nlpr.ia.ac.cn", "jzhao@nlpr.ia.ac.cn", "chongweifeng@unisound.com", "liushengping@unisound.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5948–5957\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5948"
    }, {
      "heading" : "1 Introduction",
      "text" : "The International Classification of Diseases (ICD) is a healthcare classification system launched by the World Health Organization. It contains a unique code for each disease, symptom, sign and so on. Analyzing clinical data and monitoring health issues would become more convenient with the promotion of ICD codes (Shull, 2019) (Choi et al., 2016) (Avati et al., 2018). The ICD coding task aims at assigns proper ICD codes to a clinical note. It has drawn much attention due to the importance of ICD codes. This task is usually undertaken by experienced coders manually. However, the manually process is inclined to be labor-intensive and\n*Work was done during an internship at National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences.\nerror-prone (Adams et al., 2002). A knowledgeable coder with medical experience has to read the whole clinical note with thousands of words in medical terms and assigning multiple codes from a large number of candidate codes, such as 15,000 and 60,000 codes in the ninth version (ICD-9) and the tenth version (ICD-10) of ICD taxonomies. On the one hand, medical expert with specialized ICD coding skills is hard to train. On the other hand, it is a challenge task even for professional coders, due to the large candidate code set and tedious clinical notes. As statistics, the cost incurred by coding errors and the financial investment spent on improving coding quality are estimated to be $25 billion per year in the US (Lang, 2007).\nAutomatic ICD coding methods (Stanfill et al., 2010) have been proposed to resolve the deficiency of manual annotation, regarding it as a multi-label text classification task. As shown in Figure 1, given a plain clinical text, the model tries to predict all the standardized codes from ICD-9. Recently, neural networks were introduced (Mullenbach et al., 2018) (Falis et al., 2019) (Cao et al., 2020) to alleviate the deficiency of manual feature engineering process of traditional machine learning method (Larkey and Croft, 1996) (Perotte et al., 2014) in ICD coding task, and great progresses have been made. Although effective, those methods either ignore the\nlong-tail distribution of the code frequency or not target the noisy text in clinical note. In the following, we will introduce the two characteristics and the reasons why they are critical for the automatic ICD coding. Long-tail: The long-tail problem is unbalanced data distribution phenomenon. And this problem is particularly noticeable in accompanied by a large target label set.\nAccording to our statistics, the proportion of the top 10% high-frequency codes in MIMIC-III (Johnson et al., 2016) occupied 85% of total occurrence. And 22% of the codes have less than two annotated samples. This is intuitive because people usually catch a cold but seldom have cancer. Trained with these long-tail data, neural automatic ICD coding method would inclined to make wrong predictions with high-frequency codes. Fortunately, intrinsic relationships among different diseases could be utilized to mitigate the deficiency caused by long-tail. For example, Polyneuropathy in diabetes is a complication of diabetes, with a lower probability than other complications since the long term effect of vessel lesion reflect at nerve would come out in the late-stage. If a model could learn shared information between polyneuropathy in diabetes and more common diseases diabetes, the prediction space would range to a set of complication of diabetes. Further, utilizing the dynamic code co-occurrence, (the cascade relationship among complications of diabetes) the confidence of predicting polyneuropathy in diabetes is gradually increased with the occurrence of vessel blockages, angina pectoris, hypertorphy of kidney, respectively. Therefore, how to learn shared information with considering dynamic code co-occurrence characteristics, is a crucial and challenging issue.\nNoisy text: The noisy text problem means that plentiful of information showing in clinical notes are redundant or misleading for ICD coding task. Clinical notes are usually written by doctors and nurses with different writing styles, accompanied by polysemous abbreviations, abundant medication records and repetitive records of physical indicators. According to our statistics1, about 10% of words in a clinical note contribute to the code assign task, on average. Other words are abundant medication records and repetitive records of physical indicators. These words are not just redundant but also misleading to the ICD coding task. For\n1We randomly select 20 clinical notes in MIMIC-III and manually highlight the essential words.\nexample, two critical patients with entirely different diseases could take similar medicines and have similar physical indicators in the rescue course. We argue that the noisy clinical notes are hard to read for both humans and machines. Training with such noisy text would confuse the model about where to focus on, and make wrong decisions due to the semantic deviation. Therefore, another challenging problem is how to deal with the noisy text in ICD coding task.\nIn this paper, we propose an Interactive Shared Representation Network with Self-Distillation Mechanism (ISD) to address the above issues.\nTo mitigate the disadvantage caused by the longtail issue, we extract shared representations among high-frequency and low-frequency codes from clinical notes. Codes with different occurrence frequencies all make binary decisions based on shared information rather than individually learning attention distributions. Additional experiments indicate that those shared representations could extract common information relevant to ICD codes. Further, we process the shared representations to an interaction decoder for polishing. The decoder additional supervised by two code completion tasks to ensure the dynamic code co-occurrence patterns were learned.\nTo alleviate the noisy text issue, we further propose a self-distillation learning mechanism to ensure the extracted shared representations focus on the long clinical note’s noteworthy part. The teacher part makes predictions through constructed purified text with all crucial information; meanwhile, the student part takes the origin clinical note as a reference. The student is forced to learn the teacher’s shared representations with identical target codes.\nThe contributions of this paper are as follows:\n1) We propose a framework capable of dealing with the long-tail and noisy text issues in the ICD coding task simultaneously.\n2) To relieve the long-tail issue, we propose an interactive shared representation network, which can capture the internal connections among codes with different frequencies. To handle the noisy text, we devise a selfdistillation learning mechanism, guiding the model focus on important parts of clinical notes.\n3) Experiments on two widely used ICD coding datasets, MIMIC-II and MIMIC-III, show our\nmethod outperforms state-of-the-art methods in macro F1 with 4% and 2%, respectively. The source code is available at www.github. com/tongzhou21/ISD."
    }, {
      "heading" : "2 Related Work",
      "text" : "ICD coding is an important task in the limelight for decades. Feature based methods firstly brought to solve this task. (Larkey and Croft, 1996) explored traditional machine learning algorithms, including KNN, relevance feedback, and Bayesian applying to ICD coding. (Perotte et al., 2014) utilized SVM for classification in consideration of the hierarchy of ICD codes. With the popularity of neural networks, researchers have proven the effectiveness of CNN and LSTM in ICD coding task. (Mullenbach et al., 2018) propose a convolutional neural network with an attention mechanism to capture each code’s desire information in source text also exhibit interpretability. (Xie and Xing, 2018) develop tree LSTM to utilize code descriptions. To further improve the performance, customized structures were introduced to utilize the code cooccurrence and code hierarchy of ICD taxonomies. (Cao et al., 2020) embedded the ICD codes into hyperbolic space to explore their hierarchical nature and constructed a co-graph to import code co-occurrence prior. We argue that they capture code co-occurrence in a static manner rather than dynamic multi-hop relations. (Vu et al., 2020) consider learning attention distribution for each code and introduce hierarchical joint learning architecture to handle the tail codes. Taking advantage of a set of middle representations to deal with the long-tail issue is similar to our shared representation setting, while our method enables every label to choose its desire representation from shared attention rather than its upper-level node, with more flexibility.\nThe direct solution to deal with an imbalance label set is re-sampling the training data (Japkowicz and Stephen, 2002) (Shen et al., 2016) or reweighting the labels in the loss function (Wang\net al., 2017) (Huang et al., 2016). Some studies treat the classification of tail labels as few-shot learning task. (Song et al., 2019) use GAN to generate label-wise features according to ICD code descriptions. (Huynh and Elhamifar, 2020) proposed shared multi-attention for multi-label image labeling. Our work further constructs a label interaction module for label relevant shared representation to utilize dynamic label co-occurrence.\nLots of effects tried to normalize noisy texts before inputting to downstream tasks. (Vateekul and Koomsubha, 2016) (Joshi and Deshpande, 2018) apply pre-processing techniques on twitter data for sentiment classification. (Lourentzou et al., 2019) utilized seq2seq model for text normalization. Others targeted at noisy input in an end2end manner by designing customized architecture. (Sergio and Lee, 2020) (Sergio et al., 2020). Different from previous works on noisy text, our method neither need extra text processing nor bring in specific parameters."
    }, {
      "heading" : "3 Method",
      "text" : "This section describes our interactive shared representation learning mechanism and self-distillation learning paradigm for ICD coding. Figure 2 shows the architecture of interactive shared representation networks and manifest the inference workflow of our method. We first encode the source clinical note to the hidden state with a multi-scale convolution neural network. Then a shared attention module further extracts code relevant information shared among all codes. A multi-layer bidirectional Transformer decoder insert between the shared attention representation extraction module and code prediction, establishes connections among shared code relevant representations."
    }, {
      "heading" : "3.1 Multi-Scale Convolutional Encoder",
      "text" : "We employ convolutional neural networks (CNN) for source text representation because the computation complexity affected by the length of clinical notes is non-negligible, although other sequen-\ntial encoders such as recurrent neural networks or Transformer(Vaswani et al., 2017) could capture longer dependency of text, theoretically. CNN could encode local n-gram pattern, critical in text classification, and with high computational efficiency. The words in source text are first mapped into low-dimensional word embedding space, constitute a matrix E = {e1, e2, ..., eNx}. Note that Nx is the clinical note’s length, e is the word vector with dimension de. As shown in Eq. 1 and 2, we concatenate the convolutional representation from kernel set C = {c1, c2, ..., cS} with different size kc to hidden representation matrix H = {h1, h2, ..., hNx} with size Nx × dl:\nh cj i = tanh(Wc ∗ xi:i+kcj−1 + bcj ) (1)\nhi = {hc0i ;h c1 i ; ...;h cS i } (2)"
    }, {
      "heading" : "3.2 Shared Attention",
      "text" : "The label attention method tends to learn relevant document representations for each code. We argue that the attention of rare code could not be well learned due to lacking training data. Motivated by (Huynh and Elhamifar, 2020) we propose shared attention to bridge the gap between highfrequency and low-frequency codes by learning shared representations HS through attention. Code set with total number of Nl codes represents in\ncode embedding El = {el1, el2, ..., elNl} according to their text descriptions. A set of trainable shared queries for attention with sizeNq×dl is introduced, noted as Eq = {eq1, e q 2, ..., e q Nq }, where Nq is the total number of shared queries as a hyperparameter. Then Eq calculates shared attention representation HS = {hS1 , hS2 , ..., hSNq} with hidden representation H in Eq. 3 to 5:\nAttention(Q,K, V ) = softmax( QKT√ dk V ) (3)\nαi = Attention(e q i , H,H) (4)\nhSi = H · αi (5)\nIn ideal conditions, those shared representations reflect the code relevant information corresponding to the source text. We can predict codes through HS . Each code i has its right to choose a shared representation in HS for code-specific vector through the highest dot product score si.\nsi = max(HS · eli) (6)\nThe product score was further applying to calculate the final score ŷl through the sigmoid function.\nŷi = σ(si) (7)\nWith the supervision of binary cross-entropy loss function, the shared representation should have\nlearned to represent code relevant information.\nLpred = Nl∑ i=1 [−yilog(ŷi)− (1− yi)log(1− ŷi)]\n(8)"
    }, {
      "heading" : "3.3 Interactive Shared Attention",
      "text" : "Above shared attention mechanism lacks interaction among code relevant information, which is of great importance in the ICD coding task. We implement this interaction through a bidirectional multi-layer Transformer decoder D with an additional code completion task. The shared representation HS is considered the orderless sequential input of the decoder D. Each layer of the Transformer contains interaction among shared representation HS through self-attention and interaction between shared representation and source text through source sequential attention.\nTo make sure the decoder could model the dynamic code co-occurrence pattern, we propose two code set completion tasks, shown at the bottom of Figure 3.\n(1) Missing code completion: We construct a code sequence Ltgt of a real clinical note X in the training set, randomly masking one code lmis. The decoder takes this code sequence as input to predict the masked code.\nLmis = −logP (lmis|Ltgt \\ lmis ∪ lmask, X) (9)\n(2) Wrong code removal: Similar to the above task, we construct a code sequence Ltgt, but by randomly adding a wrong code lwro. The decoder is aiming to fade the wrong code’s representation with a special mask representation lmask.\nLrem = −logP (lmask|Ltgt ∪ lwro, X) (10)\nThe decoder could generate purificatory code relevant information with higher rationality with the above two tasks’ learning. The decoder is plugged to refine the shared representation HS to HS′, so the subsequent dot product score is calculated by HS′.\nsi = max(HS′ · eli) (11)"
    }, {
      "heading" : "3.4 Self-distillation Learning Mechanism",
      "text" : "We argue that learning the desired shared attention distribution over such a long clinical text is difficult, and the αi tends to be smooth, brings lots of unnecessary noise information. Therefore we propose a self-distillation learning mechanism showing in the\ngray dotted lines of Figure 3. With this mechanism, the model could learn superior intermediate representations from itself without introducing another trained model.\nConsidering a single clinical note X with target code set Ltgt for training, we derive two paths inputted to the model. The teacher’s training data consists of the text descriptions XLtgt = {X l1, X l2, ..., X lNltgt}. We handle those code descriptions separately through the encoder and concatenate them into a flat sequence of hidden state HLtgt = {H l1 ;H l2 ; ...;H lNltgt }, where Nltgt is the number of code in Ltgt, so the subsequent process in our model is not affected.\nWe optimize the teacher’s prediction result ŷtgti through binary cross-entropy loss. Ltgt = Nl∑ i=1 [−yilog(ŷtgti )− (1− yi)log(1− ŷ tgt i )]\n(12) Student takes origin clinical note Xas input and also have BCE loss to optimize. We assume that an origin clinical note with thousands of words contains all desired codes’ information, as well as less essential words. The teacher’s input contains all desired information that indicates codes to be predicted without any noise. Ideal shared representations obtained from attention are supposed to collect code relevant information only. Hence we treat the teacher’s share representation HLtgt as a perfect example to the student. A distillation loss encourages those two representation sequences to be similar.\ncosine(HA, HB) = N∑ i hAi · hBi ‖ hAi ‖ ‖ hBi ‖ (13)\nLdist = min{1− cosine(HS′, HLtgt′)} (14)\nSince we treat the shared representations without order restrict, every teacher have its rights to choose a suitable student, meanwhile, considering other teachers’ appropriateness. It implements with Hungarian algorithm (Kuhn, 1955) to calculates the cosine distance globally minimum. Where ′ denotes any shuffle version of the origin representation sequence."
    }, {
      "heading" : "3.5 Training",
      "text" : "The complete training pipeline of our method is shown in Figure 3. The final loss function is the\nweighting sum of the above losses.\nL = λpredLpred+λmisLmis + λremLrem+ λtgtLtgt + λdistLdist\n(15)"
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "For fair comparison, we follow the datasets used by previous work on ICD coding (Mullenbach et al., 2018) (Cao et al., 2020), including MIMICII (Jouhet et al., 2012) and MIMIC-III (Johnson et al., 2016). The third edition is the extension of II. Both datasets contain discharge summaries that are tagged manually with a set of ICD-9 codes. The dataset preprocessing process is consistent with (Mullenbach et al., 2018). For MIMIC-III full dataset, there are 47719, 1631, 3372 different patients’ discharge summaries for training, development, and testing, respectively. Totally 8921 unique codes occur in those three parts. MIMICIII 50 dataset only retains the most frequent codes appear in full setting, leave 8067, 1574, 1730 discharge summaries for training, development, and testing, respectively. MIMIC-II dataset contains 5031 unique codes divided into 20533 and 2282 clinical notes for training and testing, respectively."
    }, {
      "heading" : "4.2 Metrics and Parameter Settings",
      "text" : "As in previous works (Mullenbach et al., 2018), we evaluate our method using both the micro and macro, F1 and AUC metrics. As well as P@8 indicates the proportion of the correctly-predicted codes in the top-8 predicted codes. PyTorch (Paszke et al., 2019) is chosen for our method’s implementation. We perform a grid search over all hyperparameters for each dataset. The parameter selections are based on the tradeoff between validation performance and training efficiency. We set the word embedding size to 100. We build the vocabulary set using the CBOW Word2Vec method (Mikolov et al., 2013) to pre-train word embeddings based on words in all MIMIC data, resulting in the most frequent 52254 words included. The multi-scale convolution filter size is 5, 7, 9, 11, respectively. The size of each filter output is onequarter of the code embedding size. We set code embedding size to 128 and 256 for the MIMIC-II and MIMIC-III, respectively. The size of shared representation is 64. We utilize a two-layer Transformer for the interactive decoder. For the loss function, we set λmis = 0.5, λmis = 5e − 4, λrem = 5e− 4, λtgt = 0.5, and λdist = 1e− 3 to adjust the scale of different supervisory signals. We use Adam for optimization with an initial learning rate of 3e-4, and other settings keep the default."
    }, {
      "heading" : "4.3 Baselines",
      "text" : "We compare our method with the following baselines:\nHA-GRU: A Hierarchical Attention Gated Recurrent Unit model is proposed by (Baumel et al., 2017) to predict ICD codes on the MIMIC-II dataset.\nCAML & DR-CAML: (Mullenbach et al., 2018) proposed the Convolutional Attention Net-\nwork for Multi-Label Classification (CAML), which learning attention distribution for each label. DR-CAML indicates Description Regularized CAML, an extension incorporating the text description of codes.\nMSATT-KG: The Multi-Scale Feature Attention and Structured Knowledge Graph Propagation was proposed by (Xie et al., 2019) They capture variable n-gram features and select multi-scale features through densely connected CNN and a multiscale feature attention mechanism. GCN is also employed to capture the hierarchical relationships among medical codes.\nMultiResCNN: The Multi-Filter Residual Convolutional Neural Network was proposed by (Li and Yu, 2020). They utilize the multi-filter convolutional layer capture variable n-gram patterns and residual mechanism to enlarge the receptive field.\nHyperCore: Hyperbolic and Co-graph Representation was proposed by (Cao et al., 2020). They explicitly model code hierarchy through hyperbolic embedding and learning code co-occurrence thought GCN.\nLAAT & JointLAAT: (Vu et al., 2020) Label Attention model (LAAT) for ICD coding was proposed by (Vu et al., 2020), learning attention distributions over LSTM encoding hidden states for each code. JointLAAT is an extension of LAAT with hierarchical joint learning."
    }, {
      "heading" : "4.4 Compared with State-of-the-art Methods",
      "text" : "The left part of Table 1 and Table 2 show the results of our method on the MIMIC-III and MIMIC-II dataset with the whole ICD code set. Compared with previous methods generating attention distribution for each code, our method achieves better results on most metrics, indicating the shared attention mechanism’s effectiveness. It is noteworthy that the macro results have more significant improvement compare to micro than previous methods. Since the macro indicators are mainly affected by tail codes’ performance, our approach benefits\nfrom the interactive shared representations among codes with different frequencies.\nCompared with the static code interaction of cooccurrence implemented in (Cao et al., 2020), our method achieves higher scores, indicating that the dynamic code interaction module could capture more complex code interactive information other than limit steps of message passing in GCN.\nThe right part of Table 1 shows the results of our method on the MIMIC-III dataset with the most frequent 50 codes. It proved that our approach’s performance would not fall behind with a more balanced label set."
    }, {
      "heading" : "4.5 Ablation Experiments",
      "text" : "To investigate the effectiveness of our proposed components of the method, we also perform the ablation experiments on the MIMIC-III-full dataset. The ablation results are shown in Table 3, indicating that none of these models can achieve a comparable result with our full version. Demonstrate that all those factors contribute a certain improvement to our model.\n(1) Effectiveness of Self-distillation. Specifically, when we discard the whole self-distillation part (w/o self-distillation), the performance drops, demonstrate the effectiveness of the selfdistillation. To further investigate the contribution of the self-distillation module, whether the more training data we constructed, we retain the teacher path and remove the loss between shared representations (w/o distillation loss), the performance still slightly drops. It can be concluded that although the positive effects of the constructed training data in the teacher path, the distillation still plays a role.\n(2) Effectiveness of Shared Representation. When we remove the self-distillation mechanism (w/o self-distillation), the contribution of shared representation part can be deduced compared to the performance of CAML. Result showing our version still have 1.1% advantage in macro F1, indicating the effectiveness of shared representation.\n(3) Effectiveness of Code Completion Task. When we neglect the missing code completion task and wrong code removal task (w/o code completion tasks), the code interactive decoder optimizes with final prediction loss only. The performance is even worse than the model without the whole code interaction module (w/o co-occurrence decoder). It indicates that the additional code completion task is the guarantee of modeling dynamic code co-occurrence characteristics. Further compared with the model with label attention rather than our proposed shared representations (w/o shared representation), the performance even worse, showing the code completion task is also the guarantee of the effectiveness of shared representations. Without this self-supervised task, the shared information is obscure and the performance drops due to the join of dubiously oriented model parameters."
    }, {
      "heading" : "4.6 Discussion",
      "text" : "To further explore our proposed interactive shared attention mechanism, we conduct comparisons among various numbers of shared representations in our method. And visualization the attention distribution over source text of different shared representations, as well as the information they extracted.\n(1) The Analysis of Shared Representations Size. As shown in Table 4, both large or small size would harm the final performance. When the shared size is set to 1, the shared representation degrades into a global representation. A single vector compelled to predict multiple codes causes the performance drops, as Table 4 shows. We also initialize the shared embeddings with ICD’s hierarchical parent node. Specifically, there are 1159 unique first three characters in the raw ICD code set of MIMIC-III-full. We initialize those shared embeddings with the mean vector of their corresponding child codes. Although the hierarchical priori knowledge is introduced, the computation\ncomplexity and uneven node selection could cause the model to be hard to optimize and overfit high frequent parent nodes.\n(2) Visualization of Shared Attention Distribution. The attention distribution of different shared representations shown in Table 5 indicates that they have learned to focus on different source text patterns in the noisy clinical note to represent code relevant information.\n(3) The Analysis of Self-distillation. As shown in Table 6, the attention weights over clinical text learned by model with the training of selfdistillation mechanism are more sharp than origin learning process. In combination with Table 5, it can be concluded that the self-distillation mechanism could help the model more focus on the desire words of clinical text."
    }, {
      "heading" : "5 Conclusion",
      "text" : "This paper proposes an interactive shared representation network and a self-distillation mechanism for the automatic ICD coding task, to address the long-tail and noisy text issues. The shared representations can bridge the gap between the learning\nprocess of frequent and rare codes. And the code interaction module models the dynamic code cooccurrence characteristic, further improving the performance of tail codes. Moreover, to address the noisy text issue, the self-distillation learning mechanism helps the shared representations focus on code-related information in noisy clinical notes. Experimental results on two MIMIC datasets indicate that our proposed model significantly outperforms previous state-of-the-art methods."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is supported by the National Key Research and Development Program of China (No.2017YFB1002101), the National Natural Science Foundation of China (No. 61806201, 61976211). This work is also supported by Beijing Academy of Artificial Intelligence (BAAI2019QN0301), the Key Research Program of the Chinese Academy of Sciences (Grant NO. ZDBS-SSW-JSC006), independent research project of National Laboratory of Pattern Recognition and the CCF-Tencent Open Research Fund."
    } ],
    "references" : [ {
      "title" : "Addressing medical coding and billing part ii: a strategy for achieving compliance",
      "author" : [ "Diane L Adams", "Helen Norman", "Valentine J Burroughs." ],
      "venue" : "a risk management approach for reducing coding and billing errors. Journal of the National Medical Asso-",
      "citeRegEx" : "Adams et al\\.,? 2002",
      "shortCiteRegEx" : "Adams et al\\.",
      "year" : 2002
    }, {
      "title" : "Improving palliative care with deep learning",
      "author" : [ "Anand Avati", "Kenneth Jung", "Stephanie Harman", "Lance Downing", "Andrew Ng", "Nigam H Shah." ],
      "venue" : "BMC medical informatics and decision making, 18(4):122.",
      "citeRegEx" : "Avati et al\\.,? 2018",
      "shortCiteRegEx" : "Avati et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-label classification of patient notes a case study on icd code assignment",
      "author" : [ "Tal Baumel", "Jumana Nassour-Kassis", "Raphael Cohen", "Michael Elhadad", "Noémie Elhadad." ],
      "venue" : "arXiv preprint arXiv:1709.09587.",
      "citeRegEx" : "Baumel et al\\.,? 2017",
      "shortCiteRegEx" : "Baumel et al\\.",
      "year" : 2017
    }, {
      "title" : "Hypercore: Hyperbolic and co-graph representation for automatic icd coding",
      "author" : [ "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao", "Shengping Liu", "Weifeng Chong." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Cao et al\\.,? 2020",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2020
    }, {
      "title" : "Doctor ai: Predicting clinical events via recurrent neural networks",
      "author" : [ "Edward Choi", "Mohammad Taha Bahadori", "Andy Schuetz", "Walter F Stewart", "Jimeng Sun." ],
      "venue" : "Machine Learning for Healthcare Conference, pages 301–318.",
      "citeRegEx" : "Choi et al\\.,? 2016",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2016
    }, {
      "title" : "Ontological attention ensembles for capturing semantic concepts in icd code prediction from clinical text",
      "author" : [ "Matus Falis", "Maciej Pajak", "Aneta Lisowska", "Patrick Schrempf", "Lucas Deckers", "Shadia Mikhael", "Sotirios Tsaftaris", "Alison O’Neil" ],
      "venue" : null,
      "citeRegEx" : "Falis et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Falis et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning deep representation for imbalanced classification",
      "author" : [ "Chen Huang", "Yining Li", "Chen Change Loy", "Xiaoou Tang." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5375–5384.",
      "citeRegEx" : "Huang et al\\.,? 2016",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "A shared multiattention framework for multi-label zero-shot learning",
      "author" : [ "Dat Huynh", "Ehsan Elhamifar." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8776–8786.",
      "citeRegEx" : "Huynh and Elhamifar.,? 2020",
      "shortCiteRegEx" : "Huynh and Elhamifar.",
      "year" : 2020
    }, {
      "title" : "The class imbalance problem: A systematic study",
      "author" : [ "Nathalie Japkowicz", "Shaju Stephen." ],
      "venue" : "Intelligent data analysis, 6(5):429–449.",
      "citeRegEx" : "Japkowicz and Stephen.,? 2002",
      "shortCiteRegEx" : "Japkowicz and Stephen.",
      "year" : 2002
    }, {
      "title" : "Mimiciii, a freely accessible critical care database",
      "author" : [ "Alistair EW Johnson", "Tom J Pollard", "Lu Shen", "H Lehman Li-Wei", "Mengling Feng", "Mohammad Ghassemi", "Benjamin Moody", "Peter Szolovits", "Leo Anthony Celi", "Roger G Mark." ],
      "venue" : "Scien-",
      "citeRegEx" : "Johnson et al\\.,? 2016",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2016
    }, {
      "title" : "Twitter sentiment analysis system",
      "author" : [ "Shaunak Joshi", "Deepali Deshpande." ],
      "venue" : "arXiv preprint arXiv:1807.07752.",
      "citeRegEx" : "Joshi and Deshpande.,? 2018",
      "shortCiteRegEx" : "Joshi and Deshpande.",
      "year" : 2018
    }, {
      "title" : "Automated classification of free-text pathology reports for registration of incident cases of cancer",
      "author" : [ "Vianney Jouhet", "Georges Defossez", "Anita Burgun", "Pierre Le Beux", "P Levillain", "Pierre Ingrand", "Vincent Claveau" ],
      "venue" : null,
      "citeRegEx" : "Jouhet et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Jouhet et al\\.",
      "year" : 2012
    }, {
      "title" : "The hungarian method for the assignment problem",
      "author" : [ "Harold W Kuhn." ],
      "venue" : "Naval research logistics quarterly, 2(1-2):83–97.",
      "citeRegEx" : "Kuhn.,? 1955",
      "shortCiteRegEx" : "Kuhn.",
      "year" : 1955
    }, {
      "title" : "Consultant report-natural language processing in the health care industry",
      "author" : [ "Dee Lang." ],
      "venue" : "Cincinnati Children’s Hospital Medical Center, Winter, 6.",
      "citeRegEx" : "Lang.,? 2007",
      "shortCiteRegEx" : "Lang.",
      "year" : 2007
    }, {
      "title" : "Combining classifiers in text categorization",
      "author" : [ "Leah S Larkey", "W Bruce Croft." ],
      "venue" : "Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 289–297.",
      "citeRegEx" : "Larkey and Croft.,? 1996",
      "shortCiteRegEx" : "Larkey and Croft.",
      "year" : 1996
    }, {
      "title" : "Icd coding from clinical text using multi-filter residual convolutional neural network",
      "author" : [ "Fei Li", "Hong Yu." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8180–8187.",
      "citeRegEx" : "Li and Yu.,? 2020",
      "shortCiteRegEx" : "Li and Yu.",
      "year" : 2020
    }, {
      "title" : "Adapting sequence to sequence models for text normalization in social media",
      "author" : [ "Ismini Lourentzou", "Kabir Manghnani", "ChengXiang Zhai." ],
      "venue" : "Proceedings of the International AAAI Conference",
      "citeRegEx" : "Lourentzou et al\\.,? 2019",
      "shortCiteRegEx" : "Lourentzou et al\\.",
      "year" : 2019
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "arXiv preprint arXiv:1301.3781.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Explainable prediction of medical codes from clinical text",
      "author" : [ "James Mullenbach", "Sarah Wiegreffe", "Jon Duke", "Jimeng Sun", "Jacob Eisenstein." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Mullenbach et al\\.,? 2018",
      "shortCiteRegEx" : "Mullenbach et al\\.",
      "year" : 2018
    }, {
      "title" : "Pytorch: An imperative style, high-performance deep learning library",
      "author" : [ "Adam Paszke", "Sam Gross", "Francisco Massa", "Adam Lerer", "James Bradbury", "Gregory Chanan", "Trevor Killeen", "Zeming Lin", "Natalia Gimelshein", "Luca Antiga" ],
      "venue" : null,
      "citeRegEx" : "Paszke et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Paszke et al\\.",
      "year" : 2019
    }, {
      "title" : "Diagnosis code assignment: models and evaluation metrics",
      "author" : [ "Adler Perotte", "Rimma Pivovarov", "Karthik Natarajan", "Nicole Weiskopf", "Frank Wood", "Noémie Elhadad." ],
      "venue" : "Journal of the American Medical Informatics Association, 21(2):231–237.",
      "citeRegEx" : "Perotte et al\\.,? 2014",
      "shortCiteRegEx" : "Perotte et al\\.",
      "year" : 2014
    }, {
      "title" : "Stacked debert: All attention in incomplete data for text classification",
      "author" : [ "Gwenaelle Cunha Sergio", "Minho Lee." ],
      "venue" : "Neural Networks.",
      "citeRegEx" : "Sergio and Lee.,? 2020",
      "shortCiteRegEx" : "Sergio and Lee.",
      "year" : 2020
    }, {
      "title" : "Attentively embracing noise for robust latent representation in bert",
      "author" : [ "Gwenaelle Cunha Sergio", "Dennis Singh Moirangthem", "Minho Lee." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 3479–3491.",
      "citeRegEx" : "Sergio et al\\.,? 2020",
      "shortCiteRegEx" : "Sergio et al\\.",
      "year" : 2020
    }, {
      "title" : "Relay backpropagation for effective learning of deep convolutional neural networks",
      "author" : [ "Li Shen", "Zhouchen Lin", "Qingming Huang." ],
      "venue" : "European conference on computer vision, pages 467–482. Springer.",
      "citeRegEx" : "Shen et al\\.,? 2016",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2016
    }, {
      "title" : "Digital health and the state of interoperable electronic health records",
      "author" : [ "Jessica Germaine Shull." ],
      "venue" : "JMIR medical informatics, 7(4):e12712.",
      "citeRegEx" : "Shull.,? 2019",
      "shortCiteRegEx" : "Shull.",
      "year" : 2019
    }, {
      "title" : "Generalized zero-shot icd coding",
      "author" : [ "Congzheng Song", "Shanghang Zhang", "Najmeh Sadoughi", "Pengtao Xie", "Eric Xing." ],
      "venue" : "arXiv preprint arXiv:1909.13154.",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "A systematic literature review of automated clinical coding and classification systems",
      "author" : [ "Mary H Stanfill", "Margaret Williams", "Susan H Fenton", "Robert A Jenders", "William R Hersh." ],
      "venue" : "Journal of the American Medical Informatics Association,",
      "citeRegEx" : "Stanfill et al\\.,? 2010",
      "shortCiteRegEx" : "Stanfill et al\\.",
      "year" : 2010
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "A study of sentiment analysis using deep learning techniques on thai twitter data",
      "author" : [ "Peerapon Vateekul", "Thanabhat Koomsubha." ],
      "venue" : "2016 13th International Joint Conference on Computer Science and Software Engineering (JCSSE), pages 1–6. IEEE.",
      "citeRegEx" : "Vateekul and Koomsubha.,? 2016",
      "shortCiteRegEx" : "Vateekul and Koomsubha.",
      "year" : 2016
    }, {
      "title" : "A label attention model for icd coding from clinical text",
      "author" : [ "Thanh Vu", "Dat Quoc Nguyen", "Anthony Nguyen." ],
      "venue" : "arXiv preprint arXiv:2007.06351.",
      "citeRegEx" : "Vu et al\\.,? 2020",
      "shortCiteRegEx" : "Vu et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning to model the tail",
      "author" : [ "Yu-Xiong Wang", "Deva Ramanan", "Martial Hebert." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 7029–7039.",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "A neural architecture for automated icd coding",
      "author" : [ "Pengtao Xie", "Eric Xing." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 1066–1076.",
      "citeRegEx" : "Xie and Xing.,? 2018",
      "shortCiteRegEx" : "Xie and Xing.",
      "year" : 2018
    }, {
      "title" : "Ehr coding with multi-scale feature attention and structured knowledge graph propagation",
      "author" : [ "Xiancheng Xie", "Yun Xiong", "Philip S Yu", "Yangyong Zhu." ],
      "venue" : "Proceedings of the 28th ACM International Conference on Information and Knowledge Manage-",
      "citeRegEx" : "Xie et al\\.,? 2019",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "motion of ICD codes (Shull, 2019) (Choi et al., 2016) (Avati et al.",
      "startOffset" : 34,
      "endOffset" : 53
    }, {
      "referenceID" : 13,
      "context" : "ing coding quality are estimated to be $25 billion per year in the US (Lang, 2007).",
      "startOffset" : 70,
      "endOffset" : 82
    }, {
      "referenceID" : 18,
      "context" : "Recently, neural networks were introduced (Mullenbach et al., 2018) (Falis et al.",
      "startOffset" : 42,
      "endOffset" : 67
    }, {
      "referenceID" : 3,
      "context" : ", 2019) (Cao et al., 2020) to alleviate the deficiency of manual feature engineering process of traditional machine learning method (Larkey and Croft, 1996) (Perotte et al.",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 14,
      "context" : ", 2020) to alleviate the deficiency of manual feature engineering process of traditional machine learning method (Larkey and Croft, 1996) (Perotte et al.",
      "startOffset" : 113,
      "endOffset" : 137
    }, {
      "referenceID" : 20,
      "context" : ", 2020) to alleviate the deficiency of manual feature engineering process of traditional machine learning method (Larkey and Croft, 1996) (Perotte et al., 2014) in ICD coding task, and great progresses have been made.",
      "startOffset" : 138,
      "endOffset" : 160
    }, {
      "referenceID" : 9,
      "context" : "According to our statistics, the proportion of the top 10% high-frequency codes in MIMIC-III (Johnson et al., 2016) occupied 85% of total occurrence.",
      "startOffset" : 93,
      "endOffset" : 115
    }, {
      "referenceID" : 20,
      "context" : "(Perotte et al., 2014) utilized SVM for classification in consideration of the hierarchy of ICD codes.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 18,
      "context" : "(Mullenbach et al., 2018) propose a convolutional neural network with an attention mechanism to capture each code’s desire information in source text",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 31,
      "context" : "(Xie and Xing, 2018) develop tree LSTM to utilize code descriptions.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 3,
      "context" : "(Cao et al., 2020) embedded the ICD codes into hyperbolic space to explore their hierarchical nature and constructed a co-graph to import code co-occurrence prior.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 29,
      "context" : "(Vu et al., 2020) consider learning attention distribution for each code and introduce hierarchical joint learning architecture to handle the tail codes.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 8,
      "context" : "The direct solution to deal with an imbalance label set is re-sampling the training data (Japkowicz and Stephen, 2002) (Shen et al.",
      "startOffset" : 89,
      "endOffset" : 118
    }, {
      "referenceID" : 23,
      "context" : "The direct solution to deal with an imbalance label set is re-sampling the training data (Japkowicz and Stephen, 2002) (Shen et al., 2016) or reweighting the labels in the loss function (Wang et al.",
      "startOffset" : 119,
      "endOffset" : 138
    }, {
      "referenceID" : 30,
      "context" : ", 2016) or reweighting the labels in the loss function (Wang et al., 2017) (Huang et al.",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 25,
      "context" : "(Song et al., 2019) use GAN to generate label-wise features according to ICD code descriptions.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 7,
      "context" : "(Huynh and Elhamifar, 2020) proposed shared multi-attention for multi-label image labeling.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 28,
      "context" : "(Vateekul and Koomsubha, 2016) (Joshi and Deshpande, 2018) apply pre-processing techniques on twitter data for",
      "startOffset" : 0,
      "endOffset" : 30
    }, {
      "referenceID" : 10,
      "context" : "(Vateekul and Koomsubha, 2016) (Joshi and Deshpande, 2018) apply pre-processing techniques on twitter data for",
      "startOffset" : 31,
      "endOffset" : 58
    }, {
      "referenceID" : 16,
      "context" : "(Lourentzou et al., 2019) utilized seq2seq model for text normalization.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 27,
      "context" : "tial encoders such as recurrent neural networks or Transformer(Vaswani et al., 2017) could capture longer dependency of text, theoretically.",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 7,
      "context" : "Motivated by (Huynh and Elhamifar, 2020) we propose shared attention to bridge the gap between highfrequency and low-frequency codes by learning shared representations HS through attention.",
      "startOffset" : 13,
      "endOffset" : 40
    }, {
      "referenceID" : 12,
      "context" : "It implements with Hungarian algorithm (Kuhn, 1955) to calculates the cosine distance globally minimum.",
      "startOffset" : 39,
      "endOffset" : 51
    }, {
      "referenceID" : 18,
      "context" : "For fair comparison, we follow the datasets used by previous work on ICD coding (Mullenbach et al., 2018) (Cao et al.",
      "startOffset" : 80,
      "endOffset" : 105
    }, {
      "referenceID" : 3,
      "context" : ", 2018) (Cao et al., 2020), including MIMICII (Jouhet et al.",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 11,
      "context" : ", 2020), including MIMICII (Jouhet et al., 2012) and MIMIC-III (Johnson et al.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 18,
      "context" : "The dataset preprocessing process is consistent with (Mullenbach et al., 2018).",
      "startOffset" : 53,
      "endOffset" : 78
    }, {
      "referenceID" : 18,
      "context" : "As in previous works (Mullenbach et al., 2018), we evaluate our method using both the micro and macro, F1 and AUC metrics.",
      "startOffset" : 21,
      "endOffset" : 46
    }, {
      "referenceID" : 19,
      "context" : "PyTorch (Paszke et al., 2019) is chosen for our method’s implementation.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 17,
      "context" : "We build the vocabulary set using the CBOW Word2Vec method (Mikolov et al., 2013) to pre-train word embeddings based on words in all MIMIC data, resulting",
      "startOffset" : 59,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : "HA-GRU: A Hierarchical Attention Gated Recurrent Unit model is proposed by (Baumel et al., 2017) to predict ICD codes on the MIMIC-II dataset.",
      "startOffset" : 75,
      "endOffset" : 96
    }, {
      "referenceID" : 18,
      "context" : "CAML & DR-CAML: (Mullenbach et al., 2018) proposed the Convolutional Attention Net-",
      "startOffset" : 16,
      "endOffset" : 41
    }, {
      "referenceID" : 32,
      "context" : "MSATT-KG: The Multi-Scale Feature Attention and Structured Knowledge Graph Propagation was proposed by (Xie et al., 2019) They capture variable n-gram features and select multi-scale features through densely connected CNN and a multi-",
      "startOffset" : 103,
      "endOffset" : 121
    }, {
      "referenceID" : 15,
      "context" : "MultiResCNN: The Multi-Filter Residual Convolutional Neural Network was proposed by (Li and Yu, 2020).",
      "startOffset" : 84,
      "endOffset" : 101
    }, {
      "referenceID" : 3,
      "context" : "HyperCore: Hyperbolic and Co-graph Representation was proposed by (Cao et al., 2020).",
      "startOffset" : 66,
      "endOffset" : 84
    }, {
      "referenceID" : 29,
      "context" : "LAAT & JointLAAT: (Vu et al., 2020) Label Attention model (LAAT) for ICD coding was proposed by (Vu et al.",
      "startOffset" : 18,
      "endOffset" : 35
    }, {
      "referenceID" : 29,
      "context" : ", 2020) Label Attention model (LAAT) for ICD coding was proposed by (Vu et al., 2020), learning attention distributions over LSTM encoding hidden states for each code.",
      "startOffset" : 68,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : "Compared with the static code interaction of cooccurrence implemented in (Cao et al., 2020), our method achieves higher scores, indicating that the",
      "startOffset" : 73,
      "endOffset" : 91
    } ],
    "year" : 2021,
    "abstractText" : "The ICD coding task aims at assigning codes of the International Classification of Diseases in clinical notes. Since manual coding is very laborious and prone to errors, many methods have been proposed for the automatic ICD coding task. However, existing works either ignore the long-tail of code frequency or the noisy clinical notes. To address the above issues, we propose an Interactive Shared Representation Network with Self-Distillation mechanism. Specifically, an interactive shared representation network targets building connections among codes while modeling the cooccurrence, consequently alleviating the longtail problem. Moreover, to cope with the noisy text issue, we encourage the model to focus on the clinical note’s noteworthy part and extract valuable information through a self-distillation learning mechanism. Experimental results on two MIMIC datasets demonstrate the effectiveness of our method.",
    "creator" : "LaTeX with hyperref"
  }
}