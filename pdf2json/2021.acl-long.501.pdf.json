{
  "name" : "2021.acl-long.501.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "DYPLOC: Dynamic Planning of Content Using Mixed Language Models for Text Generation",
    "authors" : [ "Xinyu Hua", "Ashwin Sreevatsa", "Lu Wang" ],
    "emails" : [ "hua.x@northeastern.edu", "wangluxy}@umich.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6408–6423\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6408\nWe study the task of long-form opinion text generation, which faces at least two distinct challenges. First, existing neural generation models fall short of coherence, thus requiring efficient content planning. Second, diverse types of information are needed to guide the generator to cover both subjective and objective content. To this end, we propose DYPLOC, a generation framework that conducts dynamic planning of content while generating the output based on a novel design of mixed language models. To enrich the generation with diverse content, we further propose to use large pre-trained models to predict relevant concepts and to generate claims. We experiment with two challenging tasks on newly collected datasets: (1) argument generation with Reddit ChangeMyView, and (2) writing articles using New York Times’ Opinion section. Automatic evaluation shows that our model significantly outperforms competitive comparisons. Human judges further confirm that our generations are more coherent with richer content."
    }, {
      "heading" : "1 Introduction",
      "text" : "Opinion articles serve as an important media to convey the authors’ values, beliefs, and stances on important societal issues. Automatically generating long-form opinion articles has the potential of facilitating various tasks, such as essay writing and speech drafting, and it is the focus of this work. Though opinion generation has been investigated for constructing arguments (Hua and Wang, 2018), writing reviews (Ni and McAuley, 2018), and producing emotional dialogue responses (Song et al., 2019), those outputs are relatively short. While impressive progress in generation has been achieved by using large pre-trained Transformers (Radford et al., 2019; Lewis et al., 2020a), directly adopting\nthem for long-form opinion text generation poses distinct challenges.\nFirst, large models still fall short of producing coherent text due to the lack of efficient content control and planning (Ko and Li, 2020; Wu et al., 2020; Tan et al., 2021). A common solution is to use concatenated phrases or semantic representations to guide the generation process (Yao et al., 2019; Harkous et al., 2020; Ribeiro et al., 2020; Goldfarb-Tarrant et al., 2020), where content planning, including both content selection and ordering, is expected to be learned by attention mechanisms. However, attentions have only achieved limited improvements. Recent work also explores training a\nseparate planning module to produce sorted content, which is then fed into a generator (Fan et al., 2019; Hua and Wang, 2020; Goldfarb-Tarrant et al., 2020). Nonetheless, this strategy results in a disconnection between planning and realization, and the output is not guaranteed to respect the planning results (Castro Ferreira et al., 2019; Prabhumoye et al., 2020).\nThe second challenge for opinion generation resides in the diversity of information that is needed to produce an output with consistent stances and supported by pertinent facts. Though large models memorize significant amounts of knowledge, they cannot retrieve and operate with them precisely (Lewis et al., 2020b). Due to the argumentative nature of opinion text, simply including knowledge bases (Guan et al., 2020; Zhou et al., 2020) is insufficient to uphold the desired quality, as it requires the combination of subjective claims and objective evidence as supports.\nTo this end, we propose a novel generation framework, DYPLOC (dynamic planning of content), to conduct content selection and ordering as text is produced.1 Concretely, given a set of unordered content items, as displayed in Figure 1, we design mixed language models, with each implemented as a sequence-to-sequence model to encode one item and the input statement. At each decoding step, our system selects which items to reflect, and predicts a word based on probabilities marginalized over all language models. Crucially, our end-to-end trained framework (1) enables the generator to access multiple content items at all times and select content based on what has been generated so far, (2) can be directly built on large pre-trained Transformers, e.g., BART (Lewis et al., 2020a), with planning and generation modules jointly trained, and (3) outputs learned content selection scores to provide an interface for system decision interpretation.\nFurthermore, to ensure that our framework can be applied to a broad range of generation tasks, we design content items to cover three critical elements: entities and concepts that are central to many generation applications, and claims that are building blocks for opinion text. We show an example for counter-argument generation in Figure 1. Importantly, we employ BART to predict additional relevant concepts, derived from Concept-\n1Data and code are available at: xinyuhua.github. io/Resources/acl21/.\nNet (Speer et al., 2017), and generate claims, as central propositions, to enrich the generated text with both objective and subjective content.\nFor experiments, we collect two datasets: (1) posts from Reddit ChangeMyView for argument generation, and (2) articles from the New York Times Opinion section (Sandhaus, 2008) for opinion article writing. Our proposed framework outperforms competitive comparisons, such as finetuning BART with the same content items, based on automatic metrics of BLEU, ROUGE, and METEOR. Human assessment further confirms that our system outputs have richer content and are more coherent in both tasks.\nOur main contributions are summarized as below: • We present a dynamic content planning generation framework, which is directly built on top of BART. Our design of mixed language models overcomes the lack of control by existing models that use implicit planning with attentions or hard copying. •We propose content plan augmentation by automatically generating relevant concepts and claims. • We construct two opinion text generation datasets with content plans that capture prominent entities and concepts."
    }, {
      "heading" : "2 Related Work",
      "text" : "Neural Generation with Planning. Text planning is seen as a crucial step to guide the generation of high-quality, well-organized natural language text (McKeown, 1992; Reiter and Dale, 2000). Incorporating planning modules to neural text generator has attracted significant research interests (Shen et al., 2019; Moryossef et al., 2019; Puduppully et al., 2019), which proves to be especially beneficial for long-form output (Fan et al., 2019; Hua and Wang, 2019). More recently, large pre-trained Transformers have established new state-of-the-arts for a wide range of text generation tasks (Lewis et al., 2020a; Roller et al., 2020; Kale and Rastogi, 2020). But it is non-trivial to integrate planning modules into them. Existing approaches resort to decoupling planning and decoding stages (Hua and Wang, 2020; Kedzie and McKeown, 2020), which inevitably increases system complexities and potentially introduces cascading errors.\nWe take inspiration from the retrieval-augmented generation framework (Lewis et al., 2020b), which is designed to incorporate relevant documents for\nquestion answering. Our adaptation uses a trainable plan scoring module to reflect content selection and ordering, which is more suitable for long text generation and offers better interpretability. Concurrent work by Zhang et al. (2021) presents a mixtureof-expert decoder to tackle knowledge-grounded generation. However, their score distribution for language models is fixed across all decoding steps, whereas ours is updated as generation progresses and can better reflect the dynamic nature of content planning.\nControllable Text Generation. Another related line of research investigates the controllability of generation models (Wiseman et al., 2017), including conditioning over keywords (Keskar et al., 2019; Hua and Wang, 2020; Xu et al., 2020), syntactic structures (Casas et al., 2020; Goyal and Durrett, 2020), or semantic representations (Wen et al., 2015; Elder et al., 2018). Our work differs from all previous methods as we combine different types of content, covering both objective and subjective information, and attain fine-grained sentence-level control using a novel design of mixed conditional language models.\nOpinion Text Generation. Our model tackles opinion articles, which differs from traditional text generation systems that mostly concern fact-based generations (Gardent et al., 2017; Novikova et al., 2017; Puduppully et al., 2019). An extensive body of work has studied summarizing (Wang and Ling, 2016; Suhara et al., 2020; Bražinskas et al., 2020) or generating (Ni and McAuley, 2018; Li et al., 2019) reviews and building dialogue systems enhanced with emotions (Li et al., 2016; Song et al., 2019). More recently, developments are made in generating argumentative text (El Baff et al., 2019;\nHidey and McKeown, 2019), which primarily focus on constructing single sentence claims on a limited number of topics. In comparison, our model can handle substantially longer output with improved quality."
    }, {
      "heading" : "3 Model",
      "text" : "Task Formulation. Our opinion text generation framework takes as input a set of content items. Each content item consists of a title t, a set of entities Ei 2, such as {United States, 9/11 attacks}, and a set of core concepts Ci, such as {attack, knowledge}, that are often abstract notions. Our model first expands Ci by predicting additional relevant concepts C+i and optionally generates a pertinent claim mi, and then outputs the final text with multiple sentences as y = {yt}, to faithfully reflect the content items with a coherent structure. An overview of our system is illustrated in Figure 2.\nBelow we first describe the content item augmentation methods (§ 3.1), followed by our generator with mixed language models that condition on expanded content items (§ 3.2)."
    }, {
      "heading" : "3.1 Content Item Augmentation",
      "text" : "Concept Expansion. With limited number of entities and concepts as input, generation systems are often incapable of producing long text with rich content, resulting in hallucination (Wiseman et al., 2017; Tian et al., 2019). Therefore, from the often-abstract core concepts, we aim to predict more specific concepts that are also relevant to the given title. For instance, as displayed in Figure 1, for core concepts {make, happen} and\n2Note that i distinguishes the items. Their order is random.\nentities {Bill Clinton, 9/11 attacks}, we grow the input with more concrete concepts of {mistake, administration}.\nWe thus consider a concept expansion module g(·), which predicts additional relevant concepts, denoted as C+i , by conditioning on the original content item:\nC+i = g(t, Ei, Ci) (1)\nWhile g(·) can be any conditional predictor, our experiment shows that fine-tuned BART model performs best on our tasks, where it generates C+i word-by-word by consuming the content item.3 Training data construction is described in § 4.2.\nClaim Generation. As discussed in § 1, opinion text generation should be controlled with consistent propositions, which cannot be effectively expressed by disconnected concepts. Therefore, we argue that natural languages are more suitable for delivering central claims, since they better encode stylistic languages, e.g., persuasion strategies.\nConcretely, we fine-tune another BART model by taking in the title t and the entities Ei, which then produces a claim with nucleus sampling for decoding (Holtzman et al., 2020). In this work, we assume the subset of content items that can be used to generate claims is known. Possible future work includes predicting such subsets and filtering claims with quality measurement."
    }, {
      "heading" : "3.2 Content Realization via Mixed Conditioning",
      "text" : "After obtaining the augmented content items, we leverage the BART model to encode each of them as a sequence, as illustrated in Figure 2. Segmenter <s> is added to indicate the change of elements in a content item. Our encoders run over all items {xi} in parallel, from which we extract content item representations {hi}, based on the last layer’s hidden states of the first token.\nThe standard sequence-to-sequence (seq2seq) framework models output probabilities by taking a single sequence as input. It is challenging to extend seq2seq to consider multiple sequences simultaneously, and conduct content planning concurrently. Therefore, we introduce a plan scoring network,\n3We also exploited a model that uses the structure of knowledge bases, e.g., ConceptNet, for learning to expand concepts, but it yields lower precision and recall than fine-tuning BART does.\nd(xi|y<t), which learns to dynamically select and order content based on what has been produced previously while generating the outputs. As outlined in Figure 2, our generator is informed of all content items during generation. At each decoding step t, the probabilities of output words are estimated as a weighted sum of all content item-conditioned language models as follows:\np(yt|y<t) = ∑ i d(xi|y<t)p(yt|y<t,xi) (2) d(xi|y<t) = softmaxi(eit) (3)\nwhere p(yt|y<t,xi) corresponds to the i-th language model with xi as the input. Crucially, d(xi|y<t) determines the importance of xi when generating token yt and thus achieves the effect of content planning. We design a two-layer feedforward network to estimate eit:\neit = Wo tanh(Wd[hi; st]) (4)\nwhere hi denotes the representation of content item xi, st is the decoder state, and Wo and Wd are learnable parameters. Although mixed language models have been used by Lewis et al. (2020b) to include retrieved documents for question answering, their relevance scores are given by external retrieval models, whereas our plan scorer d(xi|y<t) is learned together with the generator.\nTraining and Decoding. Our model is end-to-end trained with both the standard cross-entropy loss Lgen over the tokens in the target generations and a separate loss Lplan for learning d(xi|y<t):\nL(θ) = Lgen(θ) + Lplan(θ) (5)\nTo create labels for Lplan, we leverage the correspondence between content items and target tokens, i.e., d(xi|y<t) is optimized to approach 1 if yi is in the sentence that derives xi, otherwise 0.4 Details about training data construction is in § 4.2.\nAt each decoding step, the individual language models, p(yt|y<t,xi), and the distribution scores, d(xi|y<t), are first calculated in parallel. We then decode each token greedily based on the mixed language models in an autoregressive way.\n4We also experimented with a training objective consisting of the generation loss only, but the performance degraded significantly. Future directions include removing the training signals for planning."
    }, {
      "heading" : "4 Experiment Setups",
      "text" : "We experiment with the tasks of argument generation and opinion article writing (§ 4.1). Both tasks require generating multi-sentence output, and contain a substantial amount of opinions and factual content. We describe the construction of initial content items and the training data for generating expanded concepts and claims in § 4.2. We present models for comparison in § 4.3. Finally, we provide implementation details in § 4.4."
    }, {
      "heading" : "4.1 Tasks and Datasets",
      "text" : "Argument Generation. We collect arguments from Reddit ChangeMyView5 (CMV) community, an online forum that features argumentative discussions. Each thread begins with an original post (OP) stating an opinion towards a controversial topic, e.g., “The U.S. is too big for one government”. High-quality replies that counter-argue with the OP and are labeled with community endorsement are collected in our prior work (Hua and Wang, 2020), covering content posted from 2013 to 2018. In this work, we extend the data collection to 2019. Our goal is to generate the entire reply (i.e., the target) given the OP title. Statistics about the CMV dataset are listed in Table 1. We reserve the most recent 1, 000 samples for test and another 1, 000 for validation.\nOpinion Article Writing. Our second task is to generate opinion articles, as collected from the New York Times (NYT) corpus (Sandhaus, 2008). We retain articles whose taxonomy labels include Top/Opinion. To ensure that articles can be processed by our computing resource, we only keep the ones with at most 20 sentences, representing 60% of all opinion articles. As shown in Table 1, NYT outputs tend to be significantly longer and contain less claims than CMV. Similarly, we keep 1, 000 examples each for test and validation sets."
    }, {
      "heading" : "4.2 Content Item Construction",
      "text" : "From target references, we describe how to automatically construct the input content items consisting of entities and core concepts, and how to collect training data to fine-tune BART to predict more specific concepts and additional claims. Prior work has demonstrated the benefits of incorporating knowledge bases for text generation (Clark et al., 2018; Puduppully et al., 2019; Guan et al., 2020). We\n5https://www.reddit.com/r/ changemyview/\nthus consider two sources of knowledge: (1) entities from Wikipedia, which are useful for modeling events and opinion targets, and (2) concept words from ConceptNet (Speer et al., 2017), that cover more related details. Note that our setup is generally applicable to other text generation tasks, as these input items can be obtained through standard NLP pipelines, as described below.\nEntity Linking. We first segment a reference into sentences. The ones with fewer than 5 tokens are discarded for content item construction. For the rest, we extract entity mentions using Stanford CoreNLP (Manning et al., 2014), and further include nominal noun phrases. For entity linking, we adopt CrossWiki (Spitkovsky and Chang, 2012), which can process our large-scale data within a reasonable amount of time. CrossWiki maps a mention to a list of frequently linked Wikipedia entries. We further manually verify and correct the linking results for the top 500 most frequent mentions.\nConcept Extraction. To identify concepts in a reference, we match the lemmatized unigrams and their part-of-speech (POS) tags against all ConceptNet entries. To create a reasonably challenging task, we only keep a subset of the matches for inclusion in the core concept set (i.e., Ci), with the rest used as C+i , to be generated by our concept expansion model. Furthermore, we conjecture that an opinion article author tends to start with highlevel topics that cover more abstract topical words. We thus leverage a lexicon (Brysbaert et al., 2014) with concreteness scores, ranging from 0 (abstract) to 5 (concrete), for over 40k English words. We keep concepts that are verbs or have a concreteness score lower than 3.0. Word coverage of references by using core concepts and additionally with aug-\nmented concepts are 13.2% and 16.9% on CMV respectively, and similarly on NYT (Table 1). Finally, we train a concept generator with BART to produce C+i , conditional on Ci, the title, and the entities.\nClaim Detection and Generation. Claims are indispensable for opinion articles. As described in § 3.1, we aim to enrich content items with claims targeting the given entities within the title’s context. To this end, we first train a claim detector by finetuning a BERTbase (Devlin et al., 2019) sequence classifier with a dataset consisting of sentences of claims and facts. Concretely, we collect 54, 802 claim sentences from Kialo6, a repository for debate arguments. We then sample 50, 000 sentences from Wikipedia, which are treated as facts. This classifier is applied on a reference, and sentences that are labeled as claims become the target for our claim generator.\nWe then learn a claim generator using BART, which takes in the title and the entities, and outputs the claim. We augment our training data with replies collected from 30 active subreddits related to political discussions, with details in Appendix A. In total, 80, 566 sentences, which contain at least one entity and are labeled by our classifier as claims, are kept to train the generator."
    }, {
      "heading" : "4.3 Baselines and Comparisons",
      "text" : "We compare with three baselines: (1) RETRIEVAL first calculates the TF-IDF weighted bag-of-words vectors for each content item, which is then used to query the training set sentences. The one with the highest cosine similarity is picked for each query, which are then ordered by a trained PointerNetwork (Vinyals et al., 2015) as described in Gong et al. (2016). (2) SENTPLANNER (Hua and Wang, 2019) is an LSTM-based seq2seq model with a separate sentence planning decoder, where the planner selects keyphrases by using attentions and the generator reflects the selections. We treat our entities and concepts as keyphrases to feed to this model. (3) SEQ2SEQ is a fine-tuned BART model, whose input is the original content items without augmentation, thus does not have access to the predicted concepts and claims.\nAdditionally, we consider a strong comparison SEQ2SEQFULL, by fine-tuning BART with the same augmented content items as inputs as in our model. The difference is that the content items are\n6https://www.kialo.com/\nconcatenated before being used as input."
    }, {
      "heading" : "4.4 Reproducibility",
      "text" : "We implement all models using the Huggingface Transformers library (Wolf et al., 2020) with PyTorch (Paszke et al., 2019). We use the base model for BART, which has 768 dimensional states and 6 layers for both encoder and decoder (140M parameters in total). Our newly added plan scoring network only contains 1.2M parameters, less than 1% of the pre-trained model. Our generation model is optimized using Adam (Kingma and Ba, 2014), with a batch size of 3. To improve efficiency, we adopt the mixed-precision (FP16) to train each model, using one NVIDIA Titan RTX GPU card with 24GB memory. The number of content items is limited to 10 per sample, and the numbers of entities and concepts per content item are capped at 20, respectively. We also truncate the target output to at most 200 tokens during training. Early stopping is applied over validation loss. Our model converges after being trained for 38 hours (19 epochs) on CMV, and 45 hours (15 epochs) on NYT. The best validation perplexity reaches about 6.1 after model convergence on both datasets."
    }, {
      "heading" : "5 Results",
      "text" : ""
    }, {
      "heading" : "5.1 Automatic Evaluation",
      "text" : "Here we report results on test sets with standard automatic metrics: BLEU (Papineni et al., 2002) measures the n-gram precision (here we consider up to bigrams); ROUGE (Lin, 2004), calculated based on n-gram recall; and METEOR (Denkowski and Lavie, 2014), which also accounts for synonyms. In Table 2, we first present the results when goldstandard concept expansion is used.\nOur proposed DYPLOC model achieves significantly higher performance across all metrics on both datasets. In particular, the substantial lead over SEQ2SEQFULL, which has access to the same content items as ours, indicates that dynamic content planning with mixed language models produces superior generations. Among comparison models, the gap between SEQ2SEQFULL and SEQ2SEQ shows the effectiveness of content item augmentation. We also observe a significant drop for baselines without using large models, highlighting the importance of pre-training.\nAblation Study. To verify the effect of each element in content items, we further train ablated models by removing concepts, claims, or entities. The\nresults are also displayed in Table 2. In general, scores decrease when using only partial content items, among which removing all concepts lead to the biggest performance drop, suggesting that entities and claims alone are insufficient to produce informative outputs.\nEffect of Hard Selection of Content Items. To test the necessity of using weighted-sum marginalization (Eq. 2), we experiment with two comparisons with hard selections, i.e., either randomly choosing a content item, or using the one with the highest predicted plan score (greedy selection). For both cases, we set the selected content item’s plan score as 1.0, with the rest of the candidates having a score of 0.0, to ensure the probabilities summed up to 1.0. As can be seen from the bottom two rows of Table 2, not surprisingly, random selection performs much worse. We observe that its generations lack coherence and fluency, implying the effectiveness of our learnable content planner. On the other hand, using greedily selected content items obtains comparable results with DYPLOC, where a weighted sum of content items is considered. Indeed, we find that DYPLOC’s plan scores are often sharp where one content item has much\nhigher weight than others, and in these scenarios, it is almost equivalent to the greedy selection setup.\nResults with Generated Concepts. Table 3 lists generation results with our system generated concepts as expansion. While all systems yield worse results compared to using gold-standard concepts, our DYPLOC still outperforms other models by substantial margins, showing its robustness when input concepts are noisy. Yet it also suggests the importance of having more accurate and comprehensive concept expansion, which should be explored in the future work."
    }, {
      "heading" : "5.2 Human Evaluation",
      "text" : "We hire three proficient English speakers to evaluate four key aspects of the generated outputs: (1) grammaticality; (2) coherence, measuring if the text is logical and cohesive; (3) relevance, gauging topic relatedness to the input title; and (4) content richness, assessing the specificity and whether there is enough details in the outputs. Each aspect is rated on a scale of 1 (worst) to 5 (best). In addi-\ntion, judges also rank the system outputs by their overall preferences. Detailed evaluation guideline is attached in Appendix C.\nWe randomly select 50 samples from the test sets for both tasks, and present outputs by SEQ2SEQ, SEQ2SEQFULL, and DYPLOC in random orders. Table 4 shows that DYPLOC receives higher scores across all aspects and tasks. In particular, the considerable differences in coherence and content richness indicate that our framework yields better content organization as well as retains more useful information. Overall, our system outputs are ranked best for 44.7% and 45.9% of the time in two tasks, significantly more than the comparisons.\nAnalysis on Argumentative Quality. In the ablation study, we find that our full model’s performance is similar to the version without having claims as input. We suspect this is because claims are often paraphrased or even not directly used when delivering an argument, which cannot be captured by the automatic metrics. To better understand how claims are used for generation, we randomly select 50 examples by DYPLOC and its variant without claims, and ask the same human judges to decide whether there is a clear central argument conveyed by each generated argument on\nCMV. We observe that 66.7% of the outputs by our full model are recognized as successfully delivering arguments with consistent stances, whereas only 61.3% are true for the model variant without claims. This gap confirms that claim drafts can indeed promote the argumentative quality as perceived by human readers."
    }, {
      "heading" : "6 Further Discussions",
      "text" : "Evaluation results on generation quality have shown the effectiveness of our mixed language models. In this section, we aim to further understand the behavior of the plan scoring network, d(x|y<t), such as how it affects the usage of content items for generation. Specifically, we adopt the following procedure to construct alignment between each sentence in the output and content items: for each token yt, we establish a mapping yt 7→ xi if xi is the most important item for producing yt, i.e., xi = argmaxx d(x|y<t), and d(xi|y<t) > 0.5. If all tokens in an entire sentence are mapped to the same xi, we consider this sentence is aligned to that content item. Based on this rule, we show sample output and corresponding alignments in Table 5.\nFor the rest of this section, we conduct analyses based on this alignment result. We first examine whether the model learns to utilize enough content items, i.e., high coverage. Then we provide insights on whether the generation faithfully reflects the argumentative claims using entailment relation labeling by human inspection.\nHow many content items are used by the output? Human judges have rated our model output to contain more relevant information (Table 4). We believe this can be attributed to the enhanced capacity to access and reflect the input data with dynamic content planning, as a result of mixed language models. To verify this hypothesis, we calculate the percentage of content items that are aligned to at least one output sentence. Figure 3 shows that, using our system, the coverage reaches 87.25% on CMV and 83.89% for NYT. If we replace the generated concepts with gold-standard concepts (as extracted from references) instead, the coverage exceeds 90% on both tasks. These observations indicate that our model can indeed adequately utilize the input data, with more accurate concepts further encouraging higher coverage.\nHow are claim content items realized? Claims are the central elements for opinion text construction. As mentioned in § 4.2, a subset of the content items are supplied with claim sentences. In order to examine whether they are realized as claim sentences in the outputs, we leverage the fine-tuned BERT classifier (§ 4.2) to label all output sentences. 90.96% of the sentences that are aligned to a claim element in the input are also labeled as claim on CMV. The percentage is only 69.41% for NYT, though, likely because the NYT opinion articles still contain more objective information.\nFurthermore, we conduct a human evaluation study to assess the semantic relations between\nclaim input and its aligned generated sentence. We randomly sample 50 outputs from test sets, and ask four human judges to read each. For each sample, we highlight one output sentence that is aligned to a content item with claim element. The judges determine a three-way (ENTAIL, NEUTRAL, CONTRADICTORY) entailment relation between the input claim (premise) and the output (hypothesis). Results show that ENTAIL accounts for 49.06% of all instances, while only 3.77% are deemed CONTRADICTORY. Upon inspection, the contradictory pairs are usually disagreements with regard to implicit sentiments, e.g., “Journalist is the most responsible for the problem” vs. “Media coverage is a good thing.”. This suggests that while our conditional language model achieves reasonable semantic control in most cases, it is still not guaranteed to capture more nuanced semantics encoded in opinions and arguments. Future work includes designing representations that can better model stances in opinions as well as argumentative structures."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We present a novel text generation framework that enables dynamic content planning based on mixed conditional language models. We further employ large models to augment system inputs with diverse content that covers both objective and subjective information. The experiments on two distinct opinion text generation tasks show that our proposed model compares favorably against strong comparisons based on fine-tuned BART models with the same input. Human evaluation further confirms that our model generations have richer information and better content organization."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This research is supported in part by National Science Foundation through Grant IIS-1813341. We thank three anonymous reviewers for their valuable suggestions on various aspects of this work.\nEthics Statement\nLarge models that are pre-trained on heterogeneous web data are shown to encode biases and can be potentially harmful for marginalized populations. Along with the improved controllability, we also recognize that our system might be misused to create fabricated or offensive content. We therefore advocate cautious and responsible practices in realworld deployment."
    }, {
      "heading" : "A Training Data Construction for Claim Generator",
      "text" : "We describe the claim generation model in § 4.2 for content item enrichment. Since both our CMV and NYT data focus on the politics domain, we leverage a collection of Reddit posts from politics related subreddits. The full list of subreddits are shown in Table 6. In total, we collect 1.6 million posts, which are split into sentences, among which we only keep the ones classified as claim by the BERTbase classifier and have at least one named entity."
    }, {
      "heading" : "B Additional Automatic Evaluation Results",
      "text" : "In § 5.1, we report results by automatic metrics using system predicted concepts in Table 3. Here we additionally show the results evaluated by ROUGE2 and average output lengths in Table 7."
    }, {
      "heading" : "C Human Evaluation Guideline",
      "text" : "We include the detailed human evaluation guidelines in Figure 4. Note that we collect 53 samples for annotation for each domain. The first three are for calibration only and not be included in the final results."
    }, {
      "heading" : "D Additional Sample Outputs",
      "text" : "Additional example content items and generations are demonstrated in Table 8 and Table 9.\nTitle I feel that humanity is going backwards due to a rise in internet use and lack of face to face communication. Link https://www.reddit.com/r/changemyview/comments/1xklhm"
    } ],
    "references" : [ {
      "title" : "Unsupervised opinion summarization as copycat-review generation",
      "author" : [ "Arthur Bražinskas", "Mirella Lapata", "Ivan Titov." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5151–5169, Online. As-",
      "citeRegEx" : "Bražinskas et al\\.,? 2020",
      "shortCiteRegEx" : "Bražinskas et al\\.",
      "year" : 2020
    }, {
      "title" : "Concreteness ratings for 40 thousand generally known english word lemmas",
      "author" : [ "Marc Brysbaert", "Amy Beth Warriner", "Victor Kuperman." ],
      "venue" : "Behavior research methods, 46(3):904–911.",
      "citeRegEx" : "Brysbaert et al\\.,? 2014",
      "shortCiteRegEx" : "Brysbaert et al\\.",
      "year" : 2014
    }, {
      "title" : "Syntax-driven iterative expansion language models for controllable text generation",
      "author" : [ "Noe Casas", "José A.R. Fonollosa", "Marta R. Costajussà." ],
      "venue" : "Proceedings of the Fourth Workshop on Structured Prediction for NLP, pages 1–10, Online. Association",
      "citeRegEx" : "Casas et al\\.,? 2020",
      "shortCiteRegEx" : "Casas et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural data-to-text generation: A comparison between pipeline and end-to-end architectures",
      "author" : [ "Thiago Castro Ferreira", "Chris van der Lee", "Emiel van Miltenburg", "Emiel Krahmer." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods",
      "citeRegEx" : "Ferreira et al\\.,? 2019",
      "shortCiteRegEx" : "Ferreira et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural text generation in stories using entity representations as context",
      "author" : [ "Elizabeth Clark", "Yangfeng Ji", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Clark et al\\.,? 2018",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2018
    }, {
      "title" : "Meteor universal: Language specific translation evaluation for any target language",
      "author" : [ "Michael Denkowski", "Alon Lavie." ],
      "venue" : "Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376–380, Baltimore, Maryland, USA. Association",
      "citeRegEx" : "Denkowski and Lavie.,? 2014",
      "shortCiteRegEx" : "Denkowski and Lavie.",
      "year" : 2014
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Computational argumentation synthesis as a language modeling task",
      "author" : [ "Roxanne El Baff", "Henning Wachsmuth", "Khalid Al Khatib", "Manfred Stede", "Benno Stein." ],
      "venue" : "Proceedings of the 12th International Conference on Natural Language Gen-",
      "citeRegEx" : "Baff et al\\.,? 2019",
      "shortCiteRegEx" : "Baff et al\\.",
      "year" : 2019
    }, {
      "title" : "E2E NLG challenge submission: Towards controllable generation of diverse natural language",
      "author" : [ "Henry Elder", "Sebastian Gehrmann", "Alexander O’Connor", "Qun Liu" ],
      "venue" : "In Proceedings of the 11th International Conference on Natural Language",
      "citeRegEx" : "Elder et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Elder et al\\.",
      "year" : 2018
    }, {
      "title" : "Strategies for structuring story generation",
      "author" : [ "Angela Fan", "Mike Lewis", "Yann Dauphin." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2650– 2660, Florence, Italy. Association for Computa-",
      "citeRegEx" : "Fan et al\\.,? 2019",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2019
    }, {
      "title" : "The WebNLG challenge: Generating text from RDF data",
      "author" : [ "Claire Gardent", "Anastasia Shimorina", "Shashi Narayan", "Laura Perez-Beltrachini." ],
      "venue" : "Proceedings of the 10th International Conference on Natural Language Generation, pages 124–133, San-",
      "citeRegEx" : "Gardent et al\\.,? 2017",
      "shortCiteRegEx" : "Gardent et al\\.",
      "year" : 2017
    }, {
      "title" : "Content planning for neural story generation with aristotelian rescoring",
      "author" : [ "Seraphina Goldfarb-Tarrant", "Tuhin Chakrabarty", "Ralph Weischedel", "Nanyun Peng." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Goldfarb.Tarrant et al\\.,? 2020",
      "shortCiteRegEx" : "Goldfarb.Tarrant et al\\.",
      "year" : 2020
    }, {
      "title" : "End-to-end neural sentence ordering using pointer network",
      "author" : [ "Jingjing Gong", "Xinchi Chen", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "arXiv preprint arXiv:1611.04953.",
      "citeRegEx" : "Gong et al\\.,? 2016",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural syntactic preordering for controlled paraphrase generation",
      "author" : [ "Tanya Goyal", "Greg Durrett." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 238– 252, Online. Association for Computational Linguis-",
      "citeRegEx" : "Goyal and Durrett.,? 2020",
      "shortCiteRegEx" : "Goyal and Durrett.",
      "year" : 2020
    }, {
      "title" : "A knowledge-enhanced pretraining model for commonsense story generation",
      "author" : [ "Jian Guan", "Fei Huang", "Zhihao Zhao", "Xiaoyan Zhu", "Minlie Huang." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:93–108.",
      "citeRegEx" : "Guan et al\\.,? 2020",
      "shortCiteRegEx" : "Guan et al\\.",
      "year" : 2020
    }, {
      "title" : "Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity",
      "author" : [ "Hamza Harkous", "Isabel Groves", "Amir Saffari." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 2410–2424,",
      "citeRegEx" : "Harkous et al\\.,? 2020",
      "shortCiteRegEx" : "Harkous et al\\.",
      "year" : 2020
    }, {
      "title" : "Fixed that for you: Generating contrastive claims with semantic edits",
      "author" : [ "Christopher Hidey", "Kathy McKeown." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Hidey and McKeown.,? 2019",
      "shortCiteRegEx" : "Hidey and McKeown.",
      "year" : 2019
    }, {
      "title" : "The curious case of neural text",
      "author" : [ "Yejin Choi" ],
      "venue" : null,
      "citeRegEx" : "Choi.,? \\Q2020\\E",
      "shortCiteRegEx" : "Choi.",
      "year" : 2020
    }, {
      "title" : "2019. Sentence-level content",
      "author" : [ "tics. Xinyu Hua", "Lu Wang" ],
      "venue" : null,
      "citeRegEx" : "Hua and Wang.,? \\Q2019\\E",
      "shortCiteRegEx" : "Hua and Wang.",
      "year" : 2019
    }, {
      "title" : "PAIR: Planning and",
      "author" : [ "Linguistics. Xinyu Hua", "Lu Wang" ],
      "venue" : null,
      "citeRegEx" : "Hua and Wang.,? \\Q2020\\E",
      "shortCiteRegEx" : "Hua and Wang.",
      "year" : 2020
    }, {
      "title" : "2020b. Retrieval-augmented generation",
      "author" : [ "Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich Küttler", "Mike Lewis", "Wen-tau Yih", "Tim Rocktäschel", "Sebastian Riedel", "Douwe Kiela" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "A persona-based neural conversation model",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Georgios Spithourakis", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Generating long and informative reviews with aspect-aware coarse-to-fine decoding",
      "author" : [ "Junyi Li", "Wayne Xin Zhao", "Ji-Rong Wen", "Yang Song." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1969–",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Association for Computational Linguistics",
      "author" : [ "Florence", "Italy" ],
      "venue" : null,
      "citeRegEx" : "1979 et al\\.,? \\Q1979\\E",
      "shortCiteRegEx" : "1979 et al\\.",
      "year" : 1979
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "The stanford corenlp natural language processing toolkit",
      "author" : [ "Christopher D Manning", "Mihai Surdeanu", "John Bauer", "Jenny Rose Finkel", "Steven Bethard", "David McClosky." ],
      "venue" : "Proceedings of 52nd annual meeting of the association for computational linguis-",
      "citeRegEx" : "Manning et al\\.,? 2014",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "Text generation",
      "author" : [ "Kathleen McKeown." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "McKeown.,? 1992",
      "shortCiteRegEx" : "McKeown.",
      "year" : 1992
    }, {
      "title" : "Step-by-step: Separating planning from realization in neural data-to-text generation",
      "author" : [ "Amit Moryossef", "Yoav Goldberg", "Ido Dagan." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Moryossef et al\\.,? 2019",
      "shortCiteRegEx" : "Moryossef et al\\.",
      "year" : 2019
    }, {
      "title" : "Personalized review generation by expanding phrases and attending on aspect-aware representations",
      "author" : [ "Jianmo Ni", "Julian McAuley." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Pa-",
      "citeRegEx" : "Ni and McAuley.,? 2018",
      "shortCiteRegEx" : "Ni and McAuley.",
      "year" : 2018
    }, {
      "title" : "Computer-intensive methods for testing hypotheses",
      "author" : [ "Eric W Noreen." ],
      "venue" : "Wiley New York.",
      "citeRegEx" : "Noreen.,? 1989",
      "shortCiteRegEx" : "Noreen.",
      "year" : 1989
    }, {
      "title" : "The E2E dataset: New challenges for endto-end generation",
      "author" : [ "Jekaterina Novikova", "Ondřej Dušek", "Verena Rieser." ],
      "venue" : "Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 201–206, Saarbrücken, Germany. Association",
      "citeRegEx" : "Novikova et al\\.,? 2017",
      "shortCiteRegEx" : "Novikova et al\\.",
      "year" : 2017
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Pytorch: An imperative style, high-performance deep learning library",
      "author" : [ "Adam Paszke", "Sam Gross", "Francisco Massa", "Adam Lerer", "James Bradbury", "Gregory Chanan", "Trevor Killeen", "Zeming Lin", "Natalia Gimelshein", "Luca Antiga" ],
      "venue" : null,
      "citeRegEx" : "Paszke et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Paszke et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring controllable text generation techniques",
      "author" : [ "Shrimai Prabhumoye", "Alan W Black", "Ruslan Salakhutdinov." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 1–14, Barcelona, Spain (Online). Interna-",
      "citeRegEx" : "Prabhumoye et al\\.,? 2020",
      "shortCiteRegEx" : "Prabhumoye et al\\.",
      "year" : 2020
    }, {
      "title" : "Data-to-text generation with entity modeling",
      "author" : [ "Ratish Puduppully", "Li Dong", "Mirella Lapata." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2023–2035, Florence, Italy. Association for Compu-",
      "citeRegEx" : "Puduppully et al\\.,? 2019",
      "shortCiteRegEx" : "Puduppully et al\\.",
      "year" : 2019
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Building natural language generation systems",
      "author" : [ "Ehud Reiter", "Robert Dale." ],
      "venue" : "Cambridge university press.",
      "citeRegEx" : "Reiter and Dale.,? 2000",
      "shortCiteRegEx" : "Reiter and Dale.",
      "year" : 2000
    }, {
      "title" : "Investigating pretrained language models for graph-to-text generation",
      "author" : [ "Leonardo FR Ribeiro", "Martin Schmitt", "Hinrich Schütze", "Iryna Gurevych." ],
      "venue" : "arXiv preprint arXiv:2007.08426.",
      "citeRegEx" : "Ribeiro et al\\.,? 2020",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2020
    }, {
      "title" : "Recipes for building an open-domain chatbot",
      "author" : [ "Stephen Roller", "Emily Dinan", "Naman Goyal", "Da Ju", "Mary Williamson", "Yinhan Liu", "Jing Xu", "Myle Ott", "Kurt Shuster", "Eric M Smith" ],
      "venue" : "arXiv preprint arXiv:2004.13637",
      "citeRegEx" : "Roller et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Roller et al\\.",
      "year" : 2020
    }, {
      "title" : "The new york times annotated corpus",
      "author" : [ "Evan Sandhaus." ],
      "venue" : "Linguistic Data Consortium, Philadelphia, 6(12):e26752.",
      "citeRegEx" : "Sandhaus.,? 2008",
      "shortCiteRegEx" : "Sandhaus.",
      "year" : 2008
    }, {
      "title" : "Select and attend: Towards controllable content selection in text generation",
      "author" : [ "Xiaoyu Shen", "Jun Suzuki", "Kentaro Inui", "Hui Su", "Dietrich Klakow", "Satoshi Sekine." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Shen et al\\.,? 2019",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2019
    }, {
      "title" : "Generating responses with a specific emotion in dialog",
      "author" : [ "Zhenqiao Song", "Xiaoqing Zheng", "Lu Liu", "Mu Xu", "Xuan-Jing Huang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3685–3695.",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "Conceptnet 5.5: An open multilingual graph of general knowledge",
      "author" : [ "Robyn Speer", "Joshua Chin", "Catherine Havasi" ],
      "venue" : "In Proceedings of the AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Speer et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Speer et al\\.",
      "year" : 2017
    }, {
      "title" : "A cross-lingual dictionary for English Wikipedia concepts",
      "author" : [ "Valentin I. Spitkovsky", "Angel X. Chang." ],
      "venue" : "Language Resources and Evaluation (LREC), Istanbul, Turkey.",
      "citeRegEx" : "Spitkovsky and Chang.,? 2012",
      "shortCiteRegEx" : "Spitkovsky and Chang.",
      "year" : 2012
    }, {
      "title" : "OpinionDigest: A simple framework for opinion summarization",
      "author" : [ "Yoshihiko Suhara", "Xiaolan Wang", "Stefanos Angelidis", "Wang-Chiew Tan." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5789–",
      "citeRegEx" : "Suhara et al\\.,? 2020",
      "shortCiteRegEx" : "Suhara et al\\.",
      "year" : 2020
    }, {
      "title" : "Progressive generation of long text with pretrained language models",
      "author" : [ "Bowen Tan", "Zichao Yang", "Maruan Al-Shedivat", "Eric Xing", "Zhiting Hu." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Tan et al\\.,? 2021",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2021
    }, {
      "title" : "Sticking to the facts: Confident decoding for faithful data-to-text generation",
      "author" : [ "Ran Tian", "Shashi Narayan", "Thibault Sellam", "Ankur P Parikh." ],
      "venue" : "arXiv preprint arXiv:1910.08684.",
      "citeRegEx" : "Tian et al\\.,? 2019",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2019
    }, {
      "title" : "Pointer networks",
      "author" : [ "Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 28, pages 2692–2700. Curran Associates, Inc.",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural network-based abstract generation for opinions and arguments",
      "author" : [ "Lu Wang", "Wang Ling." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Wang and Ling.,? 2016",
      "shortCiteRegEx" : "Wang and Ling.",
      "year" : 2016
    }, {
      "title" : "Semantically conditioned LSTM-based natural language generation for spoken dialogue systems",
      "author" : [ "Tsung-Hsien Wen", "Milica Gašić", "Nikola Mrkšić", "PeiHao Su", "David Vandyke", "Steve Young." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical",
      "citeRegEx" : "Wen et al\\.,? 2015",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2015
    }, {
      "title" : "Challenges in data-to-document generation",
      "author" : [ "Sam Wiseman", "Stuart Shieber", "Alexander Rush." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2253–2263, Copenhagen, Denmark. Association for",
      "citeRegEx" : "Wiseman et al\\.,? 2017",
      "shortCiteRegEx" : "Wiseman et al\\.",
      "year" : 2017
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "A controllable model of grounded response generation",
      "author" : [ "Zeqiu Wu", "Michel Galley", "Chris Brockett", "Yizhe Zhang", "Xiang Gao", "Chris Quirk", "Rik Koncel-Kedziorski", "Jianfeng Gao", "Hannaneh Hajishirzi", "Mari Ostendorf" ],
      "venue" : null,
      "citeRegEx" : "Wu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "MEGATRON-CNTRL: Controllable story generation with external knowledge using large-scale language models",
      "author" : [ "Peng Xu", "Mostofa Patwary", "Mohammad Shoeybi", "Raul Puri", "Pascale Fung", "Anima Anandkumar", "Bryan Catanzaro." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Planand-write: Towards better automatic storytelling",
      "author" : [ "Lili Yao", "Nanyun Peng", "Ralph Weischedel", "Kevin Knight", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7378–7385.",
      "citeRegEx" : "Yao et al\\.,? 2019",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2019
    }, {
      "title" : "Joint retrieval and generation training for grounded text generation",
      "author" : [ "Yizhe Zhang", "Siqi Sun", "Xiang Gao", "Yuwei Fang", "Chris Brockett", "Michel Galley", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "arXiv preprint arXiv:2105.06597.",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Improving conversational recommender systems via knowledge graph based semantic fusion",
      "author" : [ "Kun Zhou", "Wayne Xin Zhao", "Shuqing Bian", "Yuanhang Zhou", "Ji-Rong Wen", "Jingsong Yu." ],
      "venue" : "Proceedings of the 26th ACM SIGKDD International",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : "Though opinion generation has been investigated for constructing arguments (Hua and Wang, 2018), writing reviews (Ni and McAuley, 2018), and producing emotional dialogue responses (Song et al.",
      "startOffset" : 113,
      "endOffset" : 135
    }, {
      "referenceID" : 41,
      "context" : "Though opinion generation has been investigated for constructing arguments (Hua and Wang, 2018), writing reviews (Ni and McAuley, 2018), and producing emotional dialogue responses (Song et al., 2019), those outputs are relatively short.",
      "startOffset" : 180,
      "endOffset" : 199
    }, {
      "referenceID" : 35,
      "context" : "While impressive progress in generation has been achieved by using large pre-trained Transformers (Radford et al., 2019; Lewis et al., 2020a), directly adopting United_States, Intelligence knowledge, attack America was never prepared and had a bad intelligence system.",
      "startOffset" : 98,
      "endOffset" : 141
    }, {
      "referenceID" : 52,
      "context" : "First, large models still fall short of producing coherent text due to the lack of efficient content control and planning (Ko and Li, 2020; Wu et al., 2020; Tan et al., 2021).",
      "startOffset" : 122,
      "endOffset" : 174
    }, {
      "referenceID" : 45,
      "context" : "First, large models still fall short of producing coherent text due to the lack of efficient content control and planning (Ko and Li, 2020; Wu et al., 2020; Tan et al., 2021).",
      "startOffset" : 122,
      "endOffset" : 174
    }, {
      "referenceID" : 54,
      "context" : "A common solution is to use concatenated phrases or semantic representations to guide the generation process (Yao et al., 2019; Harkous et al., 2020; Ribeiro et al., 2020; Goldfarb-Tarrant et al., 2020), where content planning, including both content selection and ordering, is expected to be learned by attention mechanisms.",
      "startOffset" : 109,
      "endOffset" : 202
    }, {
      "referenceID" : 15,
      "context" : "A common solution is to use concatenated phrases or semantic representations to guide the generation process (Yao et al., 2019; Harkous et al., 2020; Ribeiro et al., 2020; Goldfarb-Tarrant et al., 2020), where content planning, including both content selection and ordering, is expected to be learned by attention mechanisms.",
      "startOffset" : 109,
      "endOffset" : 202
    }, {
      "referenceID" : 37,
      "context" : "A common solution is to use concatenated phrases or semantic representations to guide the generation process (Yao et al., 2019; Harkous et al., 2020; Ribeiro et al., 2020; Goldfarb-Tarrant et al., 2020), where content planning, including both content selection and ordering, is expected to be learned by attention mechanisms.",
      "startOffset" : 109,
      "endOffset" : 202
    }, {
      "referenceID" : 11,
      "context" : "A common solution is to use concatenated phrases or semantic representations to guide the generation process (Yao et al., 2019; Harkous et al., 2020; Ribeiro et al., 2020; Goldfarb-Tarrant et al., 2020), where content planning, including both content selection and ordering, is expected to be learned by attention mechanisms.",
      "startOffset" : 109,
      "endOffset" : 202
    }, {
      "referenceID" : 9,
      "context" : "tent, which is then fed into a generator (Fan et al., 2019; Hua and Wang, 2020; Goldfarb-Tarrant et al., 2020).",
      "startOffset" : 41,
      "endOffset" : 110
    }, {
      "referenceID" : 19,
      "context" : "tent, which is then fed into a generator (Fan et al., 2019; Hua and Wang, 2020; Goldfarb-Tarrant et al., 2020).",
      "startOffset" : 41,
      "endOffset" : 110
    }, {
      "referenceID" : 11,
      "context" : "tent, which is then fed into a generator (Fan et al., 2019; Hua and Wang, 2020; Goldfarb-Tarrant et al., 2020).",
      "startOffset" : 41,
      "endOffset" : 110
    }, {
      "referenceID" : 33,
      "context" : "Nonetheless, this strategy results in a disconnection between planning and realization, and the output is not guaranteed to respect the planning results (Castro Ferreira et al., 2019; Prabhumoye et al., 2020).",
      "startOffset" : 153,
      "endOffset" : 208
    }, {
      "referenceID" : 14,
      "context" : "edge bases (Guan et al., 2020; Zhou et al., 2020) is insufficient to uphold the desired quality, as it requires the combination of subjective claims and objective evidence as supports.",
      "startOffset" : 11,
      "endOffset" : 49
    }, {
      "referenceID" : 56,
      "context" : "edge bases (Guan et al., 2020; Zhou et al., 2020) is insufficient to uphold the desired quality, as it requires the combination of subjective claims and objective evidence as supports.",
      "startOffset" : 11,
      "endOffset" : 49
    }, {
      "referenceID" : 39,
      "context" : "posts from Reddit ChangeMyView for argument generation, and (2) articles from the New York Times Opinion section (Sandhaus, 2008) for opinion article writing.",
      "startOffset" : 113,
      "endOffset" : 129
    }, {
      "referenceID" : 26,
      "context" : "of high-quality, well-organized natural language text (McKeown, 1992; Reiter and Dale, 2000).",
      "startOffset" : 54,
      "endOffset" : 92
    }, {
      "referenceID" : 36,
      "context" : "of high-quality, well-organized natural language text (McKeown, 1992; Reiter and Dale, 2000).",
      "startOffset" : 54,
      "endOffset" : 92
    }, {
      "referenceID" : 40,
      "context" : "Incorporating planning modules to neural text generator has attracted significant research interests (Shen et al., 2019; Moryossef et al., 2019; Puduppully et al., 2019), which proves to be especially beneficial for long-form output (Fan et al.",
      "startOffset" : 101,
      "endOffset" : 169
    }, {
      "referenceID" : 27,
      "context" : "Incorporating planning modules to neural text generator has attracted significant research interests (Shen et al., 2019; Moryossef et al., 2019; Puduppully et al., 2019), which proves to be especially beneficial for long-form output (Fan et al.",
      "startOffset" : 101,
      "endOffset" : 169
    }, {
      "referenceID" : 34,
      "context" : "Incorporating planning modules to neural text generator has attracted significant research interests (Shen et al., 2019; Moryossef et al., 2019; Puduppully et al., 2019), which proves to be especially beneficial for long-form output (Fan et al.",
      "startOffset" : 101,
      "endOffset" : 169
    }, {
      "referenceID" : 9,
      "context" : ", 2019), which proves to be especially beneficial for long-form output (Fan et al., 2019; Hua and Wang, 2019).",
      "startOffset" : 71,
      "endOffset" : 109
    }, {
      "referenceID" : 18,
      "context" : ", 2019), which proves to be especially beneficial for long-form output (Fan et al., 2019; Hua and Wang, 2019).",
      "startOffset" : 71,
      "endOffset" : 109
    }, {
      "referenceID" : 19,
      "context" : "Existing approaches resort to decoupling planning and decoding stages (Hua and Wang, 2020; Kedzie and McKeown, 2020), which inevitably increases system complexities and potentially introduces cascading errors.",
      "startOffset" : 70,
      "endOffset" : 116
    }, {
      "referenceID" : 50,
      "context" : "Another related line of research investigates the controllability of generation models (Wiseman et al., 2017), includ-",
      "startOffset" : 87,
      "endOffset" : 109
    }, {
      "referenceID" : 19,
      "context" : "ing conditioning over keywords (Keskar et al., 2019; Hua and Wang, 2020; Xu et al., 2020), syntactic structures (Casas et al.",
      "startOffset" : 31,
      "endOffset" : 89
    }, {
      "referenceID" : 53,
      "context" : "ing conditioning over keywords (Keskar et al., 2019; Hua and Wang, 2020; Xu et al., 2020), syntactic structures (Casas et al.",
      "startOffset" : 31,
      "endOffset" : 89
    }, {
      "referenceID" : 2,
      "context" : ", 2020), syntactic structures (Casas et al., 2020; Goyal and Durrett, 2020), or semantic representations (Wen et al.",
      "startOffset" : 30,
      "endOffset" : 75
    }, {
      "referenceID" : 13,
      "context" : ", 2020), syntactic structures (Casas et al., 2020; Goyal and Durrett, 2020), or semantic representations (Wen et al.",
      "startOffset" : 30,
      "endOffset" : 75
    }, {
      "referenceID" : 49,
      "context" : ", 2020; Goyal and Durrett, 2020), or semantic representations (Wen et al., 2015; Elder et al., 2018).",
      "startOffset" : 62,
      "endOffset" : 100
    }, {
      "referenceID" : 8,
      "context" : ", 2020; Goyal and Durrett, 2020), or semantic representations (Wen et al., 2015; Elder et al., 2018).",
      "startOffset" : 62,
      "endOffset" : 100
    }, {
      "referenceID" : 10,
      "context" : "Our model tackles opinion articles, which differs from traditional text generation systems that mostly concern fact-based generations (Gardent et al., 2017; Novikova et al., 2017; Puduppully et al., 2019).",
      "startOffset" : 134,
      "endOffset" : 204
    }, {
      "referenceID" : 30,
      "context" : "Our model tackles opinion articles, which differs from traditional text generation systems that mostly concern fact-based generations (Gardent et al., 2017; Novikova et al., 2017; Puduppully et al., 2019).",
      "startOffset" : 134,
      "endOffset" : 204
    }, {
      "referenceID" : 34,
      "context" : "Our model tackles opinion articles, which differs from traditional text generation systems that mostly concern fact-based generations (Gardent et al., 2017; Novikova et al., 2017; Puduppully et al., 2019).",
      "startOffset" : 134,
      "endOffset" : 204
    }, {
      "referenceID" : 48,
      "context" : "An extensive body of work has studied summarizing (Wang and Ling, 2016; Suhara et al., 2020; Bražinskas et al., 2020) or generating (Ni and McAuley, 2018; Li et al.",
      "startOffset" : 50,
      "endOffset" : 117
    }, {
      "referenceID" : 44,
      "context" : "An extensive body of work has studied summarizing (Wang and Ling, 2016; Suhara et al., 2020; Bražinskas et al., 2020) or generating (Ni and McAuley, 2018; Li et al.",
      "startOffset" : 50,
      "endOffset" : 117
    }, {
      "referenceID" : 0,
      "context" : "An extensive body of work has studied summarizing (Wang and Ling, 2016; Suhara et al., 2020; Bražinskas et al., 2020) or generating (Ni and McAuley, 2018; Li et al.",
      "startOffset" : 50,
      "endOffset" : 117
    }, {
      "referenceID" : 28,
      "context" : ", 2020) or generating (Ni and McAuley, 2018; Li et al., 2019) reviews and building dialogue systems enhanced with emotions (Li et al.",
      "startOffset" : 22,
      "endOffset" : 61
    }, {
      "referenceID" : 22,
      "context" : ", 2020) or generating (Ni and McAuley, 2018; Li et al., 2019) reviews and building dialogue systems enhanced with emotions (Li et al.",
      "startOffset" : 22,
      "endOffset" : 61
    }, {
      "referenceID" : 21,
      "context" : ", 2019) reviews and building dialogue systems enhanced with emotions (Li et al., 2016; Song et al., 2019).",
      "startOffset" : 69,
      "endOffset" : 105
    }, {
      "referenceID" : 41,
      "context" : ", 2019) reviews and building dialogue systems enhanced with emotions (Li et al., 2016; Song et al., 2019).",
      "startOffset" : 69,
      "endOffset" : 105
    }, {
      "referenceID" : 16,
      "context" : "More recently, developments are made in generating argumentative text (El Baff et al., 2019; Hidey and McKeown, 2019), which primarily focus on constructing single sentence claims on a limited number of topics.",
      "startOffset" : 70,
      "endOffset" : 117
    }, {
      "referenceID" : 50,
      "context" : "With limited number of entities and concepts as input, generation systems are often incapable of producing long text with rich content, resulting in hallucination (Wiseman et al., 2017; Tian et al., 2019).",
      "startOffset" : 163,
      "endOffset" : 204
    }, {
      "referenceID" : 46,
      "context" : "With limited number of entities and concepts as input, generation systems are often incapable of producing long text with rich content, resulting in hallucination (Wiseman et al., 2017; Tian et al., 2019).",
      "startOffset" : 163,
      "endOffset" : 204
    }, {
      "referenceID" : 19,
      "context" : "High-quality replies that counter-argue with the OP and are labeled with community endorsement are collected in our prior work (Hua and Wang, 2020), covering content posted from 2013 to 2018.",
      "startOffset" : 127,
      "endOffset" : 147
    }, {
      "referenceID" : 4,
      "context" : "Prior work has demonstrated the benefits of incorporating knowledge bases for text generation (Clark et al., 2018; Puduppully et al., 2019; Guan et al., 2020).",
      "startOffset" : 94,
      "endOffset" : 158
    }, {
      "referenceID" : 34,
      "context" : "Prior work has demonstrated the benefits of incorporating knowledge bases for text generation (Clark et al., 2018; Puduppully et al., 2019; Guan et al., 2020).",
      "startOffset" : 94,
      "endOffset" : 158
    }, {
      "referenceID" : 14,
      "context" : "Prior work has demonstrated the benefits of incorporating knowledge bases for text generation (Clark et al., 2018; Puduppully et al., 2019; Guan et al., 2020).",
      "startOffset" : 94,
      "endOffset" : 158
    }, {
      "referenceID" : 42,
      "context" : "thus consider two sources of knowledge: (1) entities from Wikipedia, which are useful for modeling events and opinion targets, and (2) concept words from ConceptNet (Speer et al., 2017), that cover more related details.",
      "startOffset" : 165,
      "endOffset" : 185
    }, {
      "referenceID" : 25,
      "context" : "For the rest, we extract entity mentions using Stanford CoreNLP (Manning et al., 2014), and further include nominal noun phrases.",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 43,
      "context" : "adopt CrossWiki (Spitkovsky and Chang, 2012), which can process our large-scale data within a reasonable amount of time.",
      "startOffset" : 16,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "We thus leverage a lexicon (Brysbaert et al., 2014) with concreteness scores, ranging from 0 (abstract) to 5 (concrete), for over 40k English words.",
      "startOffset" : 27,
      "endOffset" : 51
    }, {
      "referenceID" : 6,
      "context" : "To this end, we first train a claim detector by finetuning a BERTbase (Devlin et al., 2019) sequence classifier with a dataset consisting of sentences of claims and facts.",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 47,
      "context" : "The one with the highest cosine similarity is picked for each query, which are then ordered by a trained PointerNetwork (Vinyals et al., 2015) as described in Gong et al.",
      "startOffset" : 120,
      "endOffset" : 142
    }, {
      "referenceID" : 31,
      "context" : "Here we report results on test sets with standard automatic metrics: BLEU (Papineni et al., 2002) measures the n-gram precision (here we consider up to bigrams); ROUGE (Lin, 2004), calculated based",
      "startOffset" : 74,
      "endOffset" : 97
    }, {
      "referenceID" : 24,
      "context" : ", 2002) measures the n-gram precision (here we consider up to bigrams); ROUGE (Lin, 2004), calculated based",
      "startOffset" : 78,
      "endOffset" : 89
    }, {
      "referenceID" : 5,
      "context" : "on n-gram recall; and METEOR (Denkowski and Lavie, 2014), which also accounts for synonyms.",
      "startOffset" : 29,
      "endOffset" : 56
    }, {
      "referenceID" : 29,
      "context" : "Our DYPLOC model statistically significantly outperforms all baselines and comparisons (randomization approximation test (Noreen, 1989), p < 0.",
      "startOffset" : 121,
      "endOffset" : 135
    } ],
    "year" : 2021,
    "abstractText" : "We study the task of long-form opinion text generation, which faces at least two distinct challenges. First, existing neural generation models fall short of coherence, thus requiring efficient content planning. Second, diverse types of information are needed to guide the generator to cover both subjective and objective content. To this end, we propose DYPLOC, a generation framework that conducts dynamic planning of content while generating the output based on a novel design of mixed language models. To enrich the generation with diverse content, we further propose to use large pre-trained models to predict relevant concepts and to generate claims. We experiment with two challenging tasks on newly collected datasets: (1) argument generation with Reddit ChangeMyView, and (2) writing articles using New York Times’ Opinion section. Automatic evaluation shows that our model significantly outperforms competitive comparisons. Human judges further confirm that our generations are more coherent with richer content.",
    "creator" : "LaTeX with hyperref"
  }
}