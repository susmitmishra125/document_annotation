{
  "name" : "2021.acl-long.421.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Matching Distributions between Model and Data: Cross-domain Knowledge Distillation for Unsupervised Domain Adaptation",
    "authors" : [ "Bo Zhang", "Xiaoming Zhang", "Yun Liu", "Lei Cheng", "Zhoujun Li" ],
    "emails" : [ "zhangnet@buaa.edu.cn,", "yolixs@buaa.edu.cn,", "gzliuyun@buaa.edu.cn,", "lizj@buaa.edu.cn,", "leicheng@sribd.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5423–5433\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5423"
    }, {
      "heading" : "1 Introduction",
      "text" : "Annotating sufficient training data is usually an expensive and time-consuming work for diverse application domains. Unsupervised Domain Adaptation (UDA) aims at solving this learning problem in the unlabeled target domain by utilizing the abundant knowledge in an existing domain called source domain, even when these domains may have different distributions. This technique has motivated research on cross-domain text classification (Chen et al., 2019; Ye et al., 2020; Gururangan et al., 2020). One of the important knowledge in the source domain is the labels of samples. Current methods mainly leverage the labeled source\n∗ Corresponding author.\ndata and unlabeled target data to learn the domaininvariant features (Tzeng et al., 2014; Ganin and Lempitsky, 2015) and the discriminative features (Saito et al., 2017; Ge et al., 2020) that are shared across different domains.\nUnfortunately, sometimes we are forbidden access to the source data, which are distributed on different devices and usually contain private information, e.g., user profile. Existing methods cannot solve the UDA problem without the source data yet. In addition, it is necessary to adapt the target domain with a flexible network architecture different from the source domain in terms of different deployment requirements for different domains. But most of works (Liang et al., 2020; Li et al., 2020) are required to share the same network architecture between different domains. In this paper, we propose a novel UDA setting: only a trained source model and a set of unlabeled target data are provided, and the target model is allowed to have different network architectures with the trained source model. It differs from the vanilla UDA in that a trained source model instead of source data is provided as supervision to the unlabeled target domain when learning to adapt the model. Such a setting satisfies privacy policy and effective delivery, and helps deploy the target model flexibly according to the target application.\nOur setting seems somewhat similar to Knowledge Distillation (KD) (Hinton et al., 2015), where a trained teacher model teaches a student model with different architecture on the same task over a set of unlabeled data. KD assumes that the empirical distribution of the data used for training the student model matches the distribution associated with the trained teacher model. Nevertheless, in our setting, the unlabeled data and teacher (source) model have different distributions. One of simple yet generic solution for our setting is to match the distributions between source and target domains under the process of distilling the knowledge. How-\never, it is quite challenging to reduce the shifts between a known distribution (e.g., a trained source model) and the empirical distribution of data (e.g., target data). Prior methods minimize a distance metric of domain discrepancy, such as Maximum Mean Discrepancy (MMD) (Tzeng et al., 2014) to match the distributions across domains in terms of the source and target data. Unfortunately, the empirical evaluation of these metrics is unavailable since we cannot access the source data.\nIn this paper, we propose a generic framework named Cross-domain Knowledge Distillation (CdKD). Specifically, we define a Joint Kernelized Stein Discrepancy (JKSD) that measures the largest discrepancy over the Hilbert space of functions between empirical sample expectations of target domain and source distribution expectations. Inspired by the works (Liu et al., 2016), the source distribution expectations are being zero via the effect of Stein operator such that we can evaluate the discrepancy of joint distributions without any source data. We embed JKSD criterion into deep network where multi-view features including activations, gradients and class probabilities in the source model are exploited to explore the domain-invariant and discriminative features across domains. In addition, we further maximize JKSD using adversarial strategy where the multi-view features are integrated into domain adaptation abundantly. Finally, CdKD is learnt by joint optimizing both KD objective (Hinton et al., 2015) and JKSD. The main contributions are outlined as,\n• We propose to investigate the problem of UDA without needing source data by exploring the distribution discrepancy between a source model and a set of target data. We adapt the target domain with different network architecture flexibly in terms of different deployment environments.\n• For the first time, the gradient information of the source domain is exploited to boost the UDA performance. Mu et al. (2020) shows a key intuition that per-sample gradients contain task-relevant discriminative information.\n• We experiment under two Amazon review datasets for cross-domain text classification, which demonstrates that CdKD still has obvious performance advantage in all settings though without needing any source data."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Unsupervised Domain Adaptation (UDA)",
      "text" : "UDA aims at learning a model which can generalize across different domains following different probability distributions. Existing works mainly focus on how to learn domain-invariant features and discriminative features that are shared across different domains. Moment Matching, e.g., Maximum Mean Discrepancy (MMD) (Tzeng et al., 2014) and adversarial learning (Ganin and Lempitsky, 2015) are commonly used to learn domain-invariant features by aligning the marginal distributions. To learn discriminative features for UDA, self-training methods (Saito et al., 2017; Zou et al., 2019) train the target classifier in terms of the pseudo labels of target data. These works committed to improve the quality of pseudo labels including introducing mutual learning (Ge et al., 2020) and dual information maximization (Ye et al., 2020). The other line of learning discriminative features is to match the conditional distributions across domains by aligning multiple domain-specific layers (Long et al., 2017, 2018) or making an explicit hypothesis between conditional distributions (Wang et al., 2018; Yu et al., 2019; Fang et al., 2020). STN (Yao et al., 2019) explores the class-conditional distributions to approximate the discrepancy between the conditional distributions via Soft-MMD. The work (Zhang et al., 2021) derives a novel criterion Conditional Mean Discrepancy (CMD) to measure the shifts between conditional distributions in tensorproduct Hilbert space directly.\nHowever, these methods assume the target users can access to the source data, which is unsafe and sometimes unpractical since source data may be private and decentralized. Therefore, the recent works propose to generalize a target model over a set of unlabeled target data only in terms of the supervision of a trained source model. SHOT (Liang et al., 2020) learns the target-specific feature extraction module by using both information maximization and self-training strategy. Li et al. (2020) improve the target model through target-style data based on generative adversarial network (GAN) where the GAN and the target model are collaborated without source data. Unfortunately, they require that the target model must share the same network architecture with the source model. Meanwhile, multi-view features in the source model including activation and gradient are not exploited which also contribute most to the domain adaptation."
    }, {
      "heading" : "2.2 Knowledge Distillation (KD)",
      "text" : "KD transfers the knowledge from a cumbersome model to a small model that is more suitable for deployment (Hinton et al., 2015). The general technique of KD involves using a teacher-student strategy, where a large deep teacher model trained for a given task teaches shallower student model on the same task (Yim et al., 2017; Chen et al., 2018). The teacher and student models are trained based on the same data. These KD methods make an assumption that the training data and the distribution associated with the teacher model are independent and identically distributed. However, sometimes we are required to train a student model in a new domain that the teacher model is not familiar, i.e, the domain shifts exist between the new domain and the domain that the teacher model is trained. The proposed CdKD is able to relieve the domain shifts adaptively during distilling the knowledge."
    }, {
      "heading" : "3 Methodology",
      "text" : "We address the unsupervised domain adaptation (UDA) task with only a trained source model and without access to source data. We consider K-way classification. Formally, in this novel setting, we are given a trained source model fs : X 7→ Y and a target domain Dt = {xi}mi=1 ⊂ X with m unlabeled samples. Here, the goal of Cross-domain Knowledge Distillation (CdKD) is to learn a target model ft : X 7→ Y and infer {yi}mi=1, with only Dt and fs available. The target model ft is allowed to have different network architecture with fs.\nCdKD is a special KD which consists of a trained teacher model fs, a student model ft and unlabeled data Dt as well. But it differs from KD in that the empirical distribution of Dt don’t match\nthe distribution associated with the trained model fs. Therefore, it is necessary to introduce distribution adaptation to eliminate the biases between the source and target domains during distilling the knowledge. Specifically, as shown in Figure 1(a), we first introduce KD to distill the knowledge to the target domain in terms of the class probabilities produced by the source model fs. Then, we introduce a novel criterion JKSD to match the joint distributions across domains by evaluating the shift between a known distribution and a set of data. This is the first work to explore the distribution discrepancy between a model and a set of data in UDA task."
    }, {
      "heading" : "3.1 Distilling Knowledge to Target Domain",
      "text" : "Given a target sample x ∈ Dt, the target model ft : X 7→ Y produces class probabilities by using a “softmax” output layer that converts the logits p = (p1, · · · , pK) into a probability ft(x) = (q1, · · · , qK),\nqi = exp(pi/T )∑ j exp(pj/T )\nwhere T is a temperature used for generating “softer” class probabilities. We optimize the target model ft by minimizing the following objective for knowledge distillation,\nLKD = − 1\nm ∑ x∈Dt fs(x) > log ft(x) (1)\nIn our paper, the setting of temperature follows the work (Hinton et al., 2015): a high temperature T is adopted to compute ft(x) during training, but after it has been trained it uses a temperature of 1."
    }, {
      "heading" : "3.2 Joint Kernelized Stein Discrepancy",
      "text" : "In traditional UDA setting, Joint Maximum Mean Discrepancy (JMMD) (Long et al., 2017) has been applied to measure the discrepancy in joint distributions of different domains, and it can be estimated empirically using finite samples of source and target domains. Specifically, suppose k : X ×X 7→ R and l : Y × Y 7→ R are the positive definite kernels with feature maps φ(·) : X 7→ F and ψ(·) : Y 7→ G for domains of X and Y , respectively that corresponds to reproducing kernel Hilbert space (RKHS) F and G . Let CPXY : G 7→ F be the uncentered cross covariance operator that be defined as CPXY = E(x,y)∼P [φ(x) ⊗ ψ(y)]. JMMD measures the shifts in joint distributions P (X,Y) and Q(X,Y) by\nJ(P,Q) = sup f⊗g∈H\nEQ(f(x)g(y))− EP (f(x)g(y))\n=‖CQXY − C P XY ‖F⊗G\nwhereH is a unit ball in F ⊗ G . In our setting, unfortunately, the empirical estimation of JMMD is unavailable since we cannot access the source data Ds directly (The empirical estimation of JMMD is in Appendix A.1). Kernelized stein discrepancy (KSD) as a statistical test for goodness-of-fit can test whether a set of samples are generated from a marginal probability (Chwialkowski et al., 2016; Liu et al., 2016). Inspired by KSD, we introduce Joint KSD (JKSD) to evaluate the discrepancy between a known distribution P (X,Y) and a set of data Q̂ = {xi,yi}mi=1 obtained from a distribution Q(X,Y).\nAssume the dimension of X is d (X = Rd), i.e., x = (x1, · · · , xd), ∀x ∈ X . We denote by F d = F ×· · ·F the Hilbert space of d×1 vector-valued functions f = {f1, · · · , fd}with fi ∈ F , and with an inner product 〈f, f ′〉Fd = ∑d i=1 〈fi, f ′i〉F for f ′ ∈ F d. We begin by defining a Stein operator AP : F d ⊗ G 7→ F d ⊗ G acting on functions f ∈ F d and g ∈ G\n(AP f⊗g)(x,y) = g(y) ( ∇xf(x) +f(x)∇x logP (x,y) )> 1d\n(2)\nwhere ∇x logP (x,y) = ∇xP (x,y)P (x,y) ∈ R d×1, ∇xf(x) = (∂f1(x)∂x1 , · · · , ∂fd(x) ∂xd\n) ∈ Rd×1 for x = (x1, · · · , xd) and 1d is a d× 1 vector with all elements equal to 1. The expectation of Stein operator AP over the distribution P is equal to 0\nEP (AP f ⊗ g)(x,y) = 0 (3)\nwhich can be proved easily by (Chwialkowski et al., 2016, Lemma 5.1). The Stein operator AP can be expressed by defining a function ξxy over the space F d ⊗ G that depends on gradients of the log-distribution and the kernel,\nξxy =∇xφ(x)⊗ ψ(y) +(∇x logP (x,y))φ(x)⊗ ψ(y)\n(4)\nThus, (AP f ⊗ g)(x,y) can be presented as an inner product, i.e., 〈f ⊗ g, ξxy〉Fd⊗G . Now, we can define JKSD and express it in the RKHS by replacing the term f(x)g(y) in J(P,Q) as our Stein operator,\nS(P,Q) := sup f⊗g∈H′\nEQ(AP f ⊗ g)(x,y)\n− EP (AP f ⊗ g)(x,y) = supEQ(AP f ⊗ g)(x,y) = sup 〈f ⊗ g,EQξxy〉Fd⊗G =‖EQξxy‖Fd⊗G\nwhere H′ is a unit ball in F d ⊗ G . This makes it clear why Eq. 3 is a desirable property: we can compute S(P,Q) by computing the HilbertSchmidt norm ‖EQξxy‖, without need to access the data obtained from P .\nWe can empirically estimate S2(P,Q) based on the known probability P and finite samples Q̂ = {(xi,yi)}mi=1 ∼ Q(X,Y) in term of kernel tricks as follows,\nŜ2(P,Q) = 1\nm2 tr(∇2KL+ 2ΥL+ ΩL) (5) (∇2K)i,j = 〈 ∇xiφ(xi),∇xjφ(xj) 〉 Fd\nΥi,j = (∇xik(xi,xj))>∇xj logP (xj ,yj) Ωi,j = k(xi,xj) ( ∇xi logP (xi,yi)>\n∇xj logP (xj ,yj) )\nwhere L = {l(yi,yj)} is the kernel gram matrix, 〈∇xφ(x),∇x′φ(x′)〉Fd = ∑d i=1 ∂k(x,x′) ∂xi∂x′i , all the matrices ∇2K, Υ, Ω and L are in Rm×m, and tr(M) is the trace of the matrix M. (Refer to Appendix A.2 for detail.)\nIn our experiments, we adopt Gaussian kernel k(x1,x2) = exp(− 1σ2 ‖x1 − x2‖\n2) where its derivative ∇x1k(x1,x2) ∈ Rd and (∇2K)i,j ∈ R can be computed numerically,\n∇x1k(x1,x2) = k(x1,x2) ( − 2 σ2 (x1 − x2) )\n(∇2K)i,j = k(x1,x2) ( 2d\nσ2 − 4‖x1 − x2‖ 2 σ4\n)\nRemark. Based on the virtue of goodness-fit test theory, we will have S(P,Q) = 0 if and only if P = Q (Chwialkowski et al., 2016). Instead of applying uniform weights as MMD does, JKSD applies non-uniform weights βi,j ,\nŜ2(P,Q) = ∑ i,j βi,jl(yi,yj)\nwhere βi,j = (∇2K + 2Υ + Ω)i,j is, in turn, determined by the activation-based and gradient-based features of the known probability P . JKSD computes a dynamic weight βi,j to decide whether the sample i shares the same label with other sample j in the target domain. Different from cluster-based methods (Liang et al., 2020), JKSD assigns each sample a label according to all the data in the target domain instead of the centroid of each category. The computation of centroid severely suffers from the noise due to the domain shifts. In contrast, our solution is more suitable for UDA because we avoid to use the untrusted intermediate results (i.e., the centroid of each category) to infer the labels."
    }, {
      "heading" : "3.3 Training",
      "text" : "The pipeline of our CdKD framework is shown in Figure 1(b). The source model parameterized by a DNN consists of two modules: a feature extractor Ts : X 7→ Zs and a classifier Gs : Zs 7→ Y , i.e., fs(x) = Gs(Ts(x)). The target model ft = Tt◦Gt also has two modules where we use parallel notations Tt(·; θT ) : X 7→ Zt and Gt(·; θG) : Zt 7→ Y for target model. Note here in our experiments, the dimension of the latent representations of source model is set equal to the target model, i.e., Zs = Zt = Rd. The extractors Ts and Tt are allowed to adopt different network architectures.\nThe input space X is usually highly sparse where the kernel function cannot capture sufficient features to measure the similarity. Therefore, we evaluate JKSD based on latent representations of target samples, i.e., Q̂ = {(z,y)|z = Tt(x),y = Gt(z),x ∈ Dt} ∼ Q(Z,Y). In Eq. 5, it is required to evaluate the joint probability P (Y = y,Z = z) = p(y|z)p(z) over a sample (z,y) obtained from Q̂. The probability p(y|z) that the sample follows conditional distribution of the source domain P (Y|Z) can be evaluated as p(y|z) = y>Gs(z). Similarly, the term p(z) represents the probability that the target representation z follows the marginal distribution P (Z) of the source domain. Since we cannot access the source\nmarginal distribution directly, we approximate it by evaluating the cosine similarity of the representations outputted from the source model and target model, i.e.,\np(z) = 1\n2 cos(z, Ts(x)) +\n1\n2\nwhere x = T−1t (z) is the sample corresponding to z for any z ∈ Q̂. Formally, the term∇z logP (z,y) in Eq. 5 can be computed as\n∇z logP (z,y) = 1 p(y|z) y>∇zGs(z) + ∇zp(z) p(z)\nwhere ∇zGs(z) ∈ RK×d is a Jacobian matrix of the target latent representation with respect to the source classifier Gs.\nWe propose to train the target model ft by jointly distilling the knowledge from the source domain and reducing the shifts in the joint distributions via JKSD,\nmin θT ,θG\nLKD + µŜ 2(P,Q)\nwhere µ > 0 is a tradeoff parameter for JKSD. In order to maximize the test power of JKSD, we require the class of functions h ∈ F d⊗G to be rich enough. Meanwhile, kernel-based metrics usually suffer from vanishing gradients for low-bandwidth kernels. We are enlightened by (Long et al., 2017) which introduces the adversarial training to circumvent these issues. Specifically, we multiple fully connected layers U and V parameterized by θU and θV to JKSD, i.e., k(xi,xj) and l(yi,yj) are replaced as k(U(xi), U(xj)) and l(V (yi), V (yj)) in Eq. 5. We maximize JKSD with respect to the new parameters θU and θV to maximize the test power of JKSD such that the samples in the target domain are made more discriminative by abundantly exploiting the activation and gradient features in the source domain. As shown in Figure 1(c), the target model ft can be optimized by the following adversarial objective,\nmin θT ,θG max θU ,θV\nLKD + µŜ 2(P,Q) (6)"
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Setup",
      "text" : "To testify its versatility, we evaluate the proposed model in two tasks including UDA and knowledge distillation.\nAmazon-Review1 is a benchmark dataset for domain adaptation in text classification task. Two versions of Amazon Review datasets are used to evaluate models. The work provides a simplified Amazon-Review dataset (Amazon-Feature) collected from four distinct domains: Books (B), DVD (D), Electronics (E) and Kitchen (K). Each domain comprises 4,000 samples with 400d feature representations and 2 categories (positive and negative). Zhang et al. (2021) collected a larger dataset called Amazon-Text from Amazon-Review with the same domains in Amazon-Feature to test the model performance for large-scale transfer learning. The review texts are divided into two categories according to user rating, i.e., positive (5 stars) and negative (1 star). There are 10,000 original review texts in each category and 20,000 texts in each domain. The notation S→T represents the transfer learning from the source domain S to the target domain T.\nBaselines. For the bulk of experiments the following baselines are evaluated. The Source-Only model is trained only over source domain and tested over target-domain data while Train-on-Target model is trained and tested over target-domain data directly. We compare with conventional domain adaptation methods: Transfer Component Analysis (TCA) (Pan et al., 2010), Balanced Distribution Adaptation (BDA) (Wang et al., 2017), Geodesic Flow Kernel (GFK) (Gong et al., 2012), Deep Domain Confusion (DDC) (Tzeng et al., 2014), Domain Adversarial Neural Networks (RevGrad) (Ganin and Lempitsky, 2015) and Dynamic Adversarial Adaptation Network (DAAN) (Yu et al., 2019). We compare with SHOT (Liang et al., 2020) for the UDA task without the source data.\n1http://jmcauley.ucsd.edu/data/amazon/\nWe also compare with the knowledge distillation method (KD) (Hinton et al., 2015) in our setting.\nIn our experiments, three different extractors are selected. For Amazon-Feature dataset, the extractor is simply modeled as a typical 3-layer fully connected network (MLP) to transform 400d inputs into 50d latent feature vectors. Two types of networks are leveraged for Amazon-Text dataset to encode the original review texts, i.e., TextCNN and BertGRU. TextCNN (Kim, 2014) is a text convolutional network that consists of 150 convolutional filters with 3 different window sizes. We also evaluate the performance of cross-domain text classification on a pre-trained language model, i.e., BERT (Devlin et al., 2019). We freeze BERT model and construct a 2-layer bi-directional GRU (Cho et al., 2014) to learn from the representations produced by BERT. The classifier is modeled as a 2-layer fully connected network for all the settings. For CdKD, we consider to learn the source model fs by minimizing the standard cross-entropy loss. We randomly specify a 0.7/0.3 split in the source dataset and generate the optimal source model based on the validation split. U and V are modeled as weight matrices.\nWe implement all deep methods based on Pytorch framework, and BERT model is implemented and pre-trained by pytorch-transformers2. We adopt Gaussian kernel with bandwidth set to median pairwise squared distances on the training data (Gretton et al., 2012). The temperature T is set to 10 during training. We use AdamW optimizer (Loshchilov and Hutter, 2019) with batch size of 128 and the learning rate annealing strategy in (Long et al., 2017): it is adjusted during back propagation using the following formula:\n2https://github.com/huggingface/ transformers\nModels E→B K→B B→D E→D K→D B→E D→E D→K Avg\nTextCNN 69.5 67.4 79.7 72.9 71.2 70.2 64.6 65.5 70.1 BertGRU 83.8 84.4 91.3 87.0 88.6 84.8 79.1 79.7 84.8\nKD (Hinton et al., 2015) 83.1 81.8 87.0 86.3 85.8 82.6 78.5 78.2 82.9 CdKD (our) 83.8 83.5 87.9 86.7 86.6 83.9 82.3 81.8 84.6\nηp = η0\n(1+10p)0.75 where p is the training progress\nlinearly changing from 0 to 1 and η0 is set to 0.001. We apply the same strategy in (Ganin and Lempitsky, 2015) to adjust the factor µ dynamically, i.e., we gradually change it from 0 to 1 by a progressive schedule: µp = 21+exp(−10p) − 1."
    }, {
      "heading" : "4.2 Results",
      "text" : "In the first experiment, we compare with the conventional domain adaptation methods where the source model and target model share the same network architectures. The classification accuracy results on the Amazon-Feature dataset for domain adaptation based on MLP are shown in Table 1. Some of the observations and analysis are listed as follows. (1) The performance of traditional UDA methods (e.g., TCA, GFK and BDA) is worse than Source-Only model, i.e., negative transfer learning occurs in all transfer tasks. These models directly define kernel over sparse input vectors such that the kernel function cannot capture sufficient features to measure the similarity. The deep transfer methods outperform all the traditional methods, suggesting that embedding domain adaptation modules into\ndeep network can reduce domain discrepancy significantly. (2) The average accuracy of CdKD is slightly 1.0% higher than other deep transfer methods (DDC, RevGrad, DAAN and SHOT) overall. It verifies the positive effect of transferring the knowledge from trained source model without accessing the source data.\nTable 2 shows the classification performance of deep UDA models based on TextCNN and BertGRU over a large dataset Amazon-Text. For TextCNN extractor, we have following analysis. CdKD achieves superior performance over prior methods by larger margins compared to small dataset Amazon-Feature. Compared to DDC and RevGrad that obtains the domain-invariant features, CdKD can learn discriminative information from the source model by minimizing JKSD criterion. SHOT assumes that the target outputs should be similar to one-hot encoding. However, the onehot encoding used in SHOT is noisy and untrusted due to the domain shifts. Different from SHOT, we match the joint distributions across domains in terms of multi-view features rather than only class probabilities when adapting the target model. By\ngoing from TextCNN to extremely deep BertGRU, we attain a more in-depth understanding of feature transferability. BertGRU-based models outperform TextCNN-based models significantly, which shows BERT enables learning more transferable representations for UDA. Our CdKD has a slight advantage compared to other models overall under the powerful transferability of BertGRU. It reveals the necessity of designing a moment matching approach to incorporate activation and gradient features into domain adaptation for reducing the losses caused by the lack of source data.\nIn the second experiment, we compare with the KD model where the knowledge in BertGRU is distilled to the TextCNN-based model. We generate the optimal BertGRU as the teacher model based on the source dataset. The TextCNN model uses BERT tokenizer tool to guarantee the same input space between two models. We randomly specify a 0.5/0.2/0.3 split in the target dataset where we train and select TextCNN-based model based on the train split and validation split respectively. The result is reported in Table 3 in terms of the test split. The average accuracy of CdKD is 1.6% higher than original KD and approaches to the teacher model BertGRU. Significantly, the accuracy scores of tasks D → E and D → K are higher than BertGRU. This is attributed to distribution adaptation where extra performance is also gained from JKSD besides the guidance of the teacher model."
    }, {
      "heading" : "4.3 Analysis",
      "text" : "Ablation Study. We conduct the ablation experiments to see the contributions of gradient information (g) and the adversarial strategy (a), which are evaluated with TextCNN extractor for UDA task. By ablating CdKD, we have two baselines of CdKD-g (w/o g) and CdKD-a (w/o a). For CdKD-g, we set the gradient of log-distribution ∇xj logP (xj ,yj) ∈ Rd×1 to a constant, i.e., 1 d(1, 1, ..., 1)\n> while we optimize CdKD without adversarial strategy for CdKD-a. From the results in Figure 2, CdKD-g and CdKD-a perform worse\nthan CdKD but still better than KD, suggesting that gradient information and the adversarial strategy both contribute to the improvements of our model. The gradient information is one type of important knowledge in the source domain, but all previous methods ignore its importance for UDA. Effects of Source Model Accuracy. Here we study how the performance of target model are influenced by the source model accuracy, which are analyzed based on B → E task using TextCNN extractor. We randomly obtain 9 optimal source models using different seeds over B dataset, and train CdKD and KD models based on different source models for B → E task. Figure 3 shows the classification accuracy of CdKD and KD by varying accuracy of source models tested over E dataset. CdKD obtains similar performance under different source models, indicating that CdKD is not very sensitive to the quality of source models. However, the curves of KD is unstable, i.e., the performance of KD is vulnerable to the impact of the source models, because different source models follow the different distributions. Obviously, JKSD plays a crucial role in determining the effects of alleviating this distribution discrepancy among different source models. Effects of Batch Size. Batch size is a key parameter to optimize JKSD metric because it is required to compute kernel over a min-batch of data. Figure 4 shows the classification accuracy of CdKD by varying batch size in {64, 128, 256, 512}. The experiment shows that CdKD is not sensitive to batch size when batch size is larger than 64, suggesting\nthat CdKD don’t need a very large batch size for accurate estimation of JKSD."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we shed a new light on the challenges of UDA without needing source data. Specifically, we provided a generic framework named CdKD to learn a classification model over a set of unlabeled target data by making use of the knowledge of the activation and gradient information in the trained source model. CdKD learned the collective knowledge across different domains including domain-invariant and discriminative features by matching the joint distributions between a trained source model and a set of target data. Experiments for cross-domain text classification testified that CdKD still achieves advantages for UDA task though without any source data and improves the performance of KD task when the trained teacher model doesn’t match the training data."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported in part by National Natural Science Foundation of China under Grant 62001309, in part by the Opening Project of Beijing Key Laboratory of Internet Culture and Digital Dissemination Research and in part by the Open Research Fund from Shenzhen Research Institute of Big Data (No. 2019ORF01012)."
    }, {
      "heading" : "A Appendices",
      "text" : "A.1 Empirical Evaluation of JMMD JMMD J(P,Q) measures the shifts in joint distributions P (X,Y) and Q(X,Y) by\nsup f⊗g∈H\nEQ(f(x)g(y))− EP (f(x)g(y))\n= supEQ (〈f ⊗ g, φ(x)⊗ ψ(y)〉) − EP (〈f ⊗ g, φ(x)⊗ ψ(y)〉)\n= sup 〈 f ⊗ g, CQXY − C P XY 〉 F⊗G\n= ∥∥∥CQXY − CPXY ∥∥∥\nF⊗G\nGiven a source domain Ds = {(xsi ,ysi )}ni=1 ∼ P (X,Y) and a target domain Dt = {(xtj ,ytj)}mj=1 ∼ Q(X,Y), the empirical estimation of JMMD is,\nĴ2(P,Q) = 1\nn2 tr(KssLss) +\n1\nm2 tr(KttLtt)\n− 2 mn\ntr(KstLts) (7)\nwhere (Kst)i,j = k(xsi ,x t j) and (Lst)i,j = l(ysi ,y t j) are gram matrices, and tr(A) is the trace\nof the matrix A. The Eq. 7 applies the source data Kst, Kss, Lst and Lss to compute the score of JMMD, which cannot adapt to our new setting obviously. Note here that the JMMD used in our paper is a simplified version of (Long et al., 2017), where we only consider two variables.\nA.2 Empirical Evaluation of JKSD Denote λxy = ∇xφ(x) + φ(x)(∇x logP (x,y)) where ξxy can be represented as ξxy = λxy⊗ψ(y). The empirical evaluation of JKSD can be computed as,\n‖EQξxy‖2 = 〈EQξxy,EQξxy〉 =EQEQ′ 〈 λxy ⊗ ψ(y), λx′y′ ⊗ ψ(y′) 〉 =EQEQ′ 〈 λxy, λx′y′ 〉 Fd 〈 ψ(y), ψ(y′) 〉 G\n=EQEQ′ 〈 λxy, λx′y′ 〉 Fd l(y,y′)\nwhere EQ′ [·] refers to E(x′,y′)∼Q[·]. For f = (f1, · · · , fd) ∈ F d and g = (g1, · · · , gd) ∈ F d, the inner product between f and g is defined as 〈f, g〉 = ∑d i=1 〈fi, gi〉F . Based on this definition, the inner product 〈∇xφ(x),∇x′φ(x′)〉Fd can be evaluated as\nd∑ i=1 〈 ∂φ(x) ∂xi , ∂φ(x′) ∂x′i 〉 F = d∑ i=1 ∂k(x,x′) ∂xi∂x′i\nSimilar to (Chwialkowski et al., 2016), we can compute h(x,y,x′,y′) = 〈 λxy, λx′y′ 〉 Fd as,\n∇x logP (x,y)>∇x′ logP (x′,y′)k(x,x′) +∇x logP (x,y)>∇x′k(x,x′) +∇x′ logP (x′,y′)>∇xk(x,x′) + 〈 ∇xφ(x),∇x′φ(x′) 〉 Fd\nThus, JKSD S2(P,Q) is the expectation of h(x,y,x′,y′)l(y,y′) over the distribution Q,\nS2(P,Q) = EQEQ′h(x,y,x′,y′)l(y,y′)\nGiven a set of samples Dt = {(xi,yi)}mi=1 ∼ Q(X,Y), we can evaluate S2(P,Q) as\n1\nm2 ∑ x,y ∑ x′,y′ h(x,y,x′,y′)l(y,y′)\nwhich can be represented in the matrix form as shown in Eq. 5."
    } ],
    "references" : [ {
      "title" : "Multisource cross-lingual model transfer: Learning what to share",
      "author" : [ "Xilun Chen", "Ahmed Hassan Awadallah", "Hany Hassan", "Wei Wang", "Claire Cardie." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Darkrank: Accelerating deep metric learning via cross sample similarities transfer",
      "author" : [ "Yuntao Chen", "Naiyan Wang", "Zhaoxiang Zhang." ],
      "venue" : "AAAI, pages 2852–2859.",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning phrase representations using RNN encoder–decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "A kernel test of goodness of fit",
      "author" : [ "Kacper Chwialkowski", "Heiko Strathmann", "Arthur Gretton." ],
      "venue" : "volume 48 of Proceedings of Machine Learning Research, pages 2606–2615, New York, New York, USA. PMLR.",
      "citeRegEx" : "Chwialkowski et al\\.,? 2016",
      "shortCiteRegEx" : "Chwialkowski et al\\.",
      "year" : 2016
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Dart: Domainadversarial residual-transfer networks for unsupervised cross-domain image classification",
      "author" : [ "Xianghong Fang", "Haoli Bai", "Ziyi Guo", "Bin Shen", "Steven Hoi", "Zenglin Xu." ],
      "venue" : "Neural Networks.",
      "citeRegEx" : "Fang et al\\.,? 2020",
      "shortCiteRegEx" : "Fang et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised domain adaptation by backpropagation",
      "author" : [ "Yaroslav Ganin", "Victor Lempitsky." ],
      "venue" : "International Conference on Machine Learning, pages 1180–1189.",
      "citeRegEx" : "Ganin and Lempitsky.,? 2015",
      "shortCiteRegEx" : "Ganin and Lempitsky.",
      "year" : 2015
    }, {
      "title" : "Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person reidentification",
      "author" : [ "Yixiao Ge", "Dapeng Chen", "Hongsheng Li." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,",
      "citeRegEx" : "Ge et al\\.,? 2020",
      "shortCiteRegEx" : "Ge et al\\.",
      "year" : 2020
    }, {
      "title" : "Geodesic flow kernel for unsupervised domain adaptation",
      "author" : [ "Boqing Gong", "Yuan Shi", "Fei Sha", "Kristen Grauman." ],
      "venue" : "2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 2066– 2073. IEEE.",
      "citeRegEx" : "Gong et al\\.,? 2012",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2012
    }, {
      "title" : "A kernel two-sample test",
      "author" : [ "Arthur Gretton", "Karsten M Borgwardt", "Malte J Rasch", "Bernhard Schölkopf", "Alexander Smola." ],
      "venue" : "Journal of Machine Learning Research, 13(Mar):723–773.",
      "citeRegEx" : "Gretton et al\\.,? 2012",
      "shortCiteRegEx" : "Gretton et al\\.",
      "year" : 2012
    }, {
      "title" : "Don’t stop pretraining: Adapt language models to domains and tasks",
      "author" : [ "Suchin Gururangan", "Ana Marasović", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A. Smith." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the",
      "citeRegEx" : "Gururangan et al\\.,? 2020",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2020
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey E Hinton", "Oriol Vinyals", "Jeffrey Dean." ],
      "venue" : "arXiv: Machine Learning.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751, Doha, Qatar. Association for Computational Lin-",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Model adaptation: Unsupervised domain adaptation without source data",
      "author" : [ "Rui Li", "Qianfen Jiao", "Wenming Cao", "Hau-San Wong", "Si Wu." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9641–9650.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation",
      "author" : [ "Jian Liang", "Dapeng Hu", "Jiashi Feng." ],
      "venue" : "International Conference on Machine Learning.",
      "citeRegEx" : "Liang et al\\.,? 2020",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2020
    }, {
      "title" : "A kernelized stein discrepancy for goodness-of-fit tests",
      "author" : [ "Qiang Liu", "Jason Lee", "Michael Jordan." ],
      "venue" : "International conference on machine learning, pages 276–284.",
      "citeRegEx" : "Liu et al\\.,? 2016",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Conditional adversarial domain adaptation",
      "author" : [ "Mingsheng Long", "Zhangjie Cao", "Jianmin Wang", "Michael I Jordan." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1640–1650.",
      "citeRegEx" : "Long et al\\.,? 2018",
      "shortCiteRegEx" : "Long et al\\.",
      "year" : 2018
    }, {
      "title" : "Deep transfer learning with joint adaptation networks",
      "author" : [ "Mingsheng Long", "Han Zhu", "Jianmin Wang", "Michael I Jordan." ],
      "venue" : "International conference on machine learning, pages 2208–2217.",
      "citeRegEx" : "Long et al\\.,? 2017",
      "shortCiteRegEx" : "Long et al\\.",
      "year" : 2017
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "7th International Conference on Learning Representations.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Gradients as features for deep representation learning",
      "author" : [ "Fangzhou Mu", "Yingyu Liang", "Yin Li." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020.",
      "citeRegEx" : "Mu et al\\.,? 2020",
      "shortCiteRegEx" : "Mu et al\\.",
      "year" : 2020
    }, {
      "title" : "Domain adaptation via transfer component analysis",
      "author" : [ "Sinno Jialin Pan", "Ivor W Tsang", "James T Kwok", "Qiang Yang." ],
      "venue" : "IEEE Transactions on Neural Networks, 22(2):199–210.",
      "citeRegEx" : "Pan et al\\.,? 2010",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2010
    }, {
      "title" : "Asymmetric tri-training for unsupervised domain adaptation",
      "author" : [ "Kuniaki Saito", "Yoshitaka Ushiku", "Tatsuya Harada." ],
      "venue" : "International Conference on Machine Learning, pages 2988–2997.",
      "citeRegEx" : "Saito et al\\.,? 2017",
      "shortCiteRegEx" : "Saito et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep domain confusion: Maximizing for domain invariance",
      "author" : [ "Eric Tzeng", "Judy Hoffman", "Ning Zhang", "Kate Saenko", "Trevor Darrell." ],
      "venue" : "arXiv preprint arXiv:1412.3474.",
      "citeRegEx" : "Tzeng et al\\.,? 2014",
      "shortCiteRegEx" : "Tzeng et al\\.",
      "year" : 2014
    }, {
      "title" : "Balanced distribution adaptation for transfer learning",
      "author" : [ "Jindong Wang", "Yiqiang Chen", "Shuji Hao", "Wenjie Feng", "Zhiqi Shen." ],
      "venue" : "2017 IEEE International Conference on Data Mining (ICDM), pages 1129–1134. IEEE.",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Visual domain adaptation with manifold embedded distribution alignment",
      "author" : [ "Jindong Wang", "Wenjie Feng", "Yiqiang Chen", "Han Yu", "Meiyu Huang", "Philip S Yu." ],
      "venue" : "Proceedings of the 26th ACM international conference on Multimedia, pages 402–",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Heterogeneous domain adaptation via soft transfer network",
      "author" : [ "Yuan Yao", "Yu Zhang", "Xutao Li", "Yunming Ye." ],
      "venue" : "Proceedings of the 27th ACM International Conference on Multimedia, page 1578–1586.",
      "citeRegEx" : "Yao et al\\.,? 2019",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2019
    }, {
      "title" : "Feature adaptation of pre-trained language models across languages and domains with robust self-training",
      "author" : [ "Hai Ye", "Qingyu Tan", "Ruidan He", "Juntao Li", "Hwee Tou Ng", "Lidong Bing." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Ye et al\\.,? 2020",
      "shortCiteRegEx" : "Ye et al\\.",
      "year" : 2020
    }, {
      "title" : "A gift from knowledge distillation: Fast optimization, network minimization and transfer learning",
      "author" : [ "Junho Yim", "Donggyu Joo", "Jihoon Bae", "Junmo Kim." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4133–",
      "citeRegEx" : "Yim et al\\.,? 2017",
      "shortCiteRegEx" : "Yim et al\\.",
      "year" : 2017
    }, {
      "title" : "Transfer learning with dynamic adversarial adaptation network",
      "author" : [ "Chaohui Yu", "Jindong Wang", "Yiqiang Chen", "Meiyu Huang." ],
      "venue" : "International Conference on Data Mining, pages 778–786. IEEE.",
      "citeRegEx" : "Yu et al\\.,? 2019",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2019
    }, {
      "title" : "Discriminative feature adaptation via conditional mean discrepancy for cross-domain text classification",
      "author" : [ "Bo Zhang", "Xiaoming Zhang", "Yun Liu", "Lei Chen." ],
      "venue" : "International Conference on Database Systems for Advanced Applications (DAS-",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Confidence regularized self-training",
      "author" : [ "Yang Zou", "Zhiding Yu", "Xiaofeng Liu", "BVK Kumar", "Jinsong Wang." ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision, pages 5982– 5991.",
      "citeRegEx" : "Zou et al\\.,? 2019",
      "shortCiteRegEx" : "Zou et al\\.",
      "year" : 2019
    }, {
      "title" : "The Eq. 7 applies the source data Kst, Kss, Lst and Lss to compute the score of JMMD, which cannot adapt to our new setting obviously. Note here that the JMMD used in our paper is a simplified version of (Long",
      "author" : [ "A. matrix" ],
      "venue" : null,
      "citeRegEx" : "matrix,? \\Q2017\\E",
      "shortCiteRegEx" : "matrix",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "This technique has motivated research on cross-domain text classification (Chen et al., 2019; Ye et al., 2020; Gururangan et al., 2020).",
      "startOffset" : 74,
      "endOffset" : 135
    }, {
      "referenceID" : 26,
      "context" : "This technique has motivated research on cross-domain text classification (Chen et al., 2019; Ye et al., 2020; Gururangan et al., 2020).",
      "startOffset" : 74,
      "endOffset" : 135
    }, {
      "referenceID" : 10,
      "context" : "This technique has motivated research on cross-domain text classification (Chen et al., 2019; Ye et al., 2020; Gururangan et al., 2020).",
      "startOffset" : 74,
      "endOffset" : 135
    }, {
      "referenceID" : 22,
      "context" : "data and unlabeled target data to learn the domaininvariant features (Tzeng et al., 2014; Ganin and Lempitsky, 2015) and the discriminative features (Saito et al.",
      "startOffset" : 69,
      "endOffset" : 116
    }, {
      "referenceID" : 6,
      "context" : "data and unlabeled target data to learn the domaininvariant features (Tzeng et al., 2014; Ganin and Lempitsky, 2015) and the discriminative features (Saito et al.",
      "startOffset" : 69,
      "endOffset" : 116
    }, {
      "referenceID" : 21,
      "context" : ", 2014; Ganin and Lempitsky, 2015) and the discriminative features (Saito et al., 2017; Ge et al., 2020) that are shared across different domains.",
      "startOffset" : 67,
      "endOffset" : 104
    }, {
      "referenceID" : 7,
      "context" : ", 2014; Ganin and Lempitsky, 2015) and the discriminative features (Saito et al., 2017; Ge et al., 2020) that are shared across different domains.",
      "startOffset" : 67,
      "endOffset" : 104
    }, {
      "referenceID" : 14,
      "context" : "most of works (Liang et al., 2020; Li et al., 2020) are required to share the same network architecture between different domains.",
      "startOffset" : 14,
      "endOffset" : 51
    }, {
      "referenceID" : 13,
      "context" : "most of works (Liang et al., 2020; Li et al., 2020) are required to share the same network architecture between different domains.",
      "startOffset" : 14,
      "endOffset" : 51
    }, {
      "referenceID" : 11,
      "context" : "Our setting seems somewhat similar to Knowledge Distillation (KD) (Hinton et al., 2015), where a trained teacher model teaches a student model with different architecture on the same task over a set of unlabeled data.",
      "startOffset" : 66,
      "endOffset" : 87
    }, {
      "referenceID" : 22,
      "context" : "Prior methods minimize a distance metric of domain discrepancy, such as Maximum Mean Discrepancy (MMD) (Tzeng et al., 2014) to match the distributions across domains in terms of the source and target data.",
      "startOffset" : 103,
      "endOffset" : 123
    }, {
      "referenceID" : 15,
      "context" : "Inspired by the works (Liu et al., 2016), the source distribution expectations are being zero via the effect of Stein operator such that we can evaluate the discrepancy of joint distributions without any source data.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 22,
      "context" : ", Maximum Mean Discrepancy (MMD) (Tzeng et al., 2014) and adversarial learning (Ganin and Lempitsky, 2015) are commonly used to learn domain-invariant features by aligning the marginal distributions.",
      "startOffset" : 33,
      "endOffset" : 53
    }, {
      "referenceID" : 6,
      "context" : ", 2014) and adversarial learning (Ganin and Lempitsky, 2015) are commonly used to learn domain-invariant features by aligning the marginal distributions.",
      "startOffset" : 33,
      "endOffset" : 60
    }, {
      "referenceID" : 21,
      "context" : "To learn discriminative features for UDA, self-training methods (Saito et al., 2017; Zou et al., 2019) train the target classifier in terms of the pseudo labels of target data.",
      "startOffset" : 64,
      "endOffset" : 102
    }, {
      "referenceID" : 30,
      "context" : "To learn discriminative features for UDA, self-training methods (Saito et al., 2017; Zou et al., 2019) train the target classifier in terms of the pseudo labels of target data.",
      "startOffset" : 64,
      "endOffset" : 102
    }, {
      "referenceID" : 7,
      "context" : "These works committed to improve the quality of pseudo labels including introducing mutual learning (Ge et al., 2020) and dual information maximization (Ye et al.",
      "startOffset" : 100,
      "endOffset" : 117
    }, {
      "referenceID" : 26,
      "context" : ", 2020) and dual information maximization (Ye et al., 2020).",
      "startOffset" : 42,
      "endOffset" : 59
    }, {
      "referenceID" : 24,
      "context" : ", 2017, 2018) or making an explicit hypothesis between conditional distributions (Wang et al., 2018; Yu et al., 2019; Fang et al., 2020).",
      "startOffset" : 81,
      "endOffset" : 136
    }, {
      "referenceID" : 28,
      "context" : ", 2017, 2018) or making an explicit hypothesis between conditional distributions (Wang et al., 2018; Yu et al., 2019; Fang et al., 2020).",
      "startOffset" : 81,
      "endOffset" : 136
    }, {
      "referenceID" : 5,
      "context" : ", 2017, 2018) or making an explicit hypothesis between conditional distributions (Wang et al., 2018; Yu et al., 2019; Fang et al., 2020).",
      "startOffset" : 81,
      "endOffset" : 136
    }, {
      "referenceID" : 25,
      "context" : "STN (Yao et al., 2019) explores the class-conditional distributions to approximate the discrepancy between the conditional distributions via Soft-MMD.",
      "startOffset" : 4,
      "endOffset" : 22
    }, {
      "referenceID" : 29,
      "context" : "The work (Zhang et al., 2021) derives a novel criterion Conditional Mean Discrepancy (CMD) to measure the shifts between conditional distributions in tensorproduct Hilbert space directly.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : "SHOT (Liang et al., 2020) learns the target-specific feature extraction module by using both information maximization and self-training strategy.",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 11,
      "context" : "KD transfers the knowledge from a cumbersome model to a small model that is more suitable for deployment (Hinton et al., 2015).",
      "startOffset" : 105,
      "endOffset" : 126
    }, {
      "referenceID" : 27,
      "context" : "The general technique of KD involves using a teacher-student strategy, where a large deep teacher model trained for a given task teaches shallower student model on the same task (Yim et al., 2017; Chen et al., 2018).",
      "startOffset" : 178,
      "endOffset" : 215
    }, {
      "referenceID" : 1,
      "context" : "The general technique of KD involves using a teacher-student strategy, where a large deep teacher model trained for a given task teaches shallower student model on the same task (Yim et al., 2017; Chen et al., 2018).",
      "startOffset" : 178,
      "endOffset" : 215
    }, {
      "referenceID" : 11,
      "context" : "In our paper, the setting of temperature follows the work (Hinton et al., 2015): a high temperature T is adopted to compute ft(x) during training, but after it has been trained it uses a temperature of 1.",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 17,
      "context" : "In traditional UDA setting, Joint Maximum Mean Discrepancy (JMMD) (Long et al., 2017) has been applied to measure the discrepancy in joint distributions of different domains, and it can be estimated",
      "startOffset" : 66,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : "Based on the virtue of goodness-fit test theory, we will have S(P,Q) = 0 if and only if P = Q (Chwialkowski et al., 2016).",
      "startOffset" : 94,
      "endOffset" : 121
    }, {
      "referenceID" : 14,
      "context" : "Different from cluster-based methods (Liang et al., 2020), JKSD assigns each sample a label according to all the data in the target domain instead of the centroid of each category.",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 17,
      "context" : "We are enlightened by (Long et al., 2017) which introduces the adversarial training to circumvent these issues.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 20,
      "context" : "We compare with conventional domain adaptation methods: Transfer Component Analysis (TCA) (Pan et al., 2010), Balanced Distribution Adaptation (BDA) (Wang et al.",
      "startOffset" : 90,
      "endOffset" : 108
    }, {
      "referenceID" : 23,
      "context" : ", 2010), Balanced Distribution Adaptation (BDA) (Wang et al., 2017), Geodesic Flow Kernel (GFK) (Gong et al.",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 8,
      "context" : ", 2017), Geodesic Flow Kernel (GFK) (Gong et al., 2012), Deep Domain Confusion (DDC) (Tzeng et al.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 22,
      "context" : ", 2012), Deep Domain Confusion (DDC) (Tzeng et al., 2014), Domain Adversarial Neural Networks (RevGrad) (Ganin and Lempitsky, 2015) and Dynamic Adversarial Adaptation Network (DAAN) (Yu et al.",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 6,
      "context" : ", 2014), Domain Adversarial Neural Networks (RevGrad) (Ganin and Lempitsky, 2015) and Dynamic Adversarial Adaptation Network (DAAN) (Yu et al.",
      "startOffset" : 54,
      "endOffset" : 81
    }, {
      "referenceID" : 28,
      "context" : ", 2014), Domain Adversarial Neural Networks (RevGrad) (Ganin and Lempitsky, 2015) and Dynamic Adversarial Adaptation Network (DAAN) (Yu et al., 2019).",
      "startOffset" : 132,
      "endOffset" : 149
    }, {
      "referenceID" : 14,
      "context" : "We compare with SHOT (Liang et al., 2020) for the UDA task without the source data.",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 11,
      "context" : "edu/data/amazon/ We also compare with the knowledge distillation method (KD) (Hinton et al., 2015) in our setting.",
      "startOffset" : 77,
      "endOffset" : 98
    }, {
      "referenceID" : 12,
      "context" : "TextCNN (Kim, 2014) is a text convolutional network that consists of 150 convolutional filters with 3 different window sizes.",
      "startOffset" : 8,
      "endOffset" : 19
    }, {
      "referenceID" : 2,
      "context" : "We freeze BERT model and construct a 2-layer bi-directional GRU (Cho et al., 2014) to learn from the representations produced by BERT.",
      "startOffset" : 64,
      "endOffset" : 82
    }, {
      "referenceID" : 9,
      "context" : "We adopt Gaussian kernel with bandwidth set to median pairwise squared distances on the training data (Gretton et al., 2012).",
      "startOffset" : 102,
      "endOffset" : 124
    }, {
      "referenceID" : 18,
      "context" : "We use AdamW optimizer (Loshchilov and Hutter, 2019) with batch size of 128 and the learning rate annealing strategy in (Long et al.",
      "startOffset" : 23,
      "endOffset" : 52
    }, {
      "referenceID" : 17,
      "context" : "We use AdamW optimizer (Loshchilov and Hutter, 2019) with batch size of 128 and the learning rate annealing strategy in (Long et al., 2017): it is adjusted during back propagation using the following formula:",
      "startOffset" : 120,
      "endOffset" : 139
    }, {
      "referenceID" : 6,
      "context" : "We apply the same strategy in (Ganin and Lempitsky, 2015) to adjust the factor μ dynamically, i.",
      "startOffset" : 30,
      "endOffset" : 57
    } ],
    "year" : 2021,
    "abstractText" : "Unsupervised Domain Adaptation (UDA) aims to transfer the knowledge of source domain to the unlabeled target domain. Existing methods typically require to learn to adapt the target model by exploiting the source data and sharing the network architecture across domains. However, this pipeline makes the source data risky and is inflexible for deploying the target model. This paper tackles a novel setting where only a trained source model is available and different network architectures can be adapted for target domain in terms of deployment environments. We propose a generic framework named Crossdomain Knowledge Distillation (CdKD) without needing any source data. CdKD matches the joint distributions between a trained source model and a set of target data during distilling the knowledge from the source model to the target domain. As a type of important knowledge in the source domain, for the first time, the gradient information is exploited to boost the transfer performance. Experiments on cross-domain text classification demonstrate that CdKD achieves superior performance, which verifies the effectiveness in this novel setting.",
    "creator" : "LaTeX with hyperref"
  }
}