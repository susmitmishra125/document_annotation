{
  "name" : "2021.acl-long.551.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic",
    "authors" : [ "Muhammad Abdul-Mageed", "AbdelRahim Elmadany", "El Moatez Billah Nagoudi" ],
    "emails" : [ "muhammad.mageed@ubc.ca", "a.elmadany@ubc.ca", "moatez.nagoudi@ubc.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7088–7105\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n7088"
    }, {
      "heading" : "1 Introduction",
      "text" : "Language models (LMs) exploiting self-supervised learning such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019a) have recently emerged as powerful transfer learning tools that help improve a very wide range of natural language processing (NLP) tasks. Multilingual LMs such as mBERT (Devlin et al., 2019) and XLM-RoBERTa (XLM-R) (Conneau et al., 2020) have also been introduced, but are usually outperformed by monolingual models pre-trained with larger vocabulary and bigger language-specific datasets (Virtanen et al., 2019; Antoun et al., 2020; Dadas et al., 2020;\n† All authors contributed equally.\nde Vries et al., 2019; Le et al., 2020; Martin et al., 2020; Nguyen and Tuan Nguyen, 2020).\nSince LMs are costly to pre-train, it is important to keep in mind the end goals they will serve once developed. For example, (i) in addition to their utility on ‘standard’ data, it is useful to endow them with ability to excel on wider real world settings such as in social media. Some existing LMs do not meet this need since they were trained on datasets that do not sufficiently capture the nuances of social media language (e.g., frequent use of abbreviations, emoticons, and hashtags; playful character repetitions; neologisms and informal language). It is also desirable to build models able to (ii) serve diverse communities (e.g., speakers of dialects of a given language), rather than focusing only on mainstream varieties. In addition, once created, models should be (iii) usable in energy efficient scenarios. This means that, for example, medium-to-large models with competitive performance should be preferred to large-to-mega models.\nA related issue is (iv) how LMs are evaluated. Progress in NLP hinges on our ability to carry out meaningful comparisons across tasks, on carefully designed benchmarks. Although several benchmarks have been introduced to evaluate LMs, the majority of these are either exclusively in English (e.g., DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), SuperGLUE (Wang et al., 2019)) or use machine translation in their training splits (e.g., XTREME (Hu et al., 2020)). Again, useful as these benchmarks are, this circumvents our ability to measure progress in real-world settings (e.g., training and evaluation on native vs. translated data) for both cross-lingual NLP and in monolingual, non-English environments.\nContext. Our objective is to showcase a scenario where we build LMs that meet all four needs listed above. That is, we describe novel LMs that (i) excel across domains, including social media, (ii) can serve diverse communities, and (iii) perform well compared to larger (more energy hungry) mod-\nels (iv) on a novel, standardized benchmark. We choose Arabic as the context for our work since it is a widely spoken language (∼ 400M native speakers), with a large number of diverse dialects differing among themselves and from the standard variety, Modern Standard Arabic (MSA). Arabic is also covered by the popular mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020), which provides us a setup for meaningful comparisons. That is, not only are we able to empirically measure monolingual vs. multilingual performance under robust conditions using our new benchmark, ARLUE, but we can also demonstrate how our base-sized models outperform (or at least are on par with) larger models (i.e., XLM-RLarge, which is ∼ 3.4× larger than our models). In the context of our work, we also show how the currently best-performing model dedicated to Arabic, AraBERT (Antoun et al., 2020), suffers from a number of issues. These include (a) not making use of easily accessible data across domains and, more seriously, (b) limited ability to handle Arabic dialects and (c) narrow evaluation. We rectify all these limitations.\nOur contributions. With our stated goals in mind, we introduce ARBERT and MARBERT, two Arabic-focused LMs exploiting largeto-massive diverse datasets. For evaluation, we also introduce a novel ARabic natural Language Understanding Evaluation benchmark (ARLUE). ARLUE is composed of 42 different datasets, making it by far the largest and most diverse Arabic NLP benchmark we know of. We arrange ARLUE into six coherent cluster tasks and methodically evaluate on each independent dataset as well as each cluster task, ultimately reporting a single ARLUE score. Our models establish new stateof-the-art (SOTA) on the majority of tasks, across all cluster tasks. Our goal is for ARLUE to serve the critical need for measuring progress on Arabic, and facilitate evaluation of multilingual and Arabic LMs. To summarize, we offer the following contributions:\n1. We develop ARBERT and MARBERT, two novel Arabic-specific Transformer LMs pre-trained on very large and diverse datasets to facilitate transfer learning on MSA as well as Arabic dialects.\n2. We introduce ARLUE, a new benchmark developed by collecting and standardizing splits\non 42 datasets across six different Arabic language understanding cluster tasks, thereby facilitating measurement of progress on Arabic and multilingual NLP.\n3. We fine-tune our new powerful models on ARLUE and provide an extensive set of comparisons to available models. Our models achieve new SOTA on all task clusters in 37 out of 48 individual datasets and a SOTA ARLUE score.\nThe rest of the paper is organized as follows: In Section 2, we provide an overview of Arabic LMs. Section 3 describes our Arabic pre-tained models. We evaluate our models on downstream tasks in Section 4, and present our benchmark ARLUE and evaluation on it in Section 5. Section 6 is an overview of related work. We conclude in Section 7. We now introduce existing Arabic LMs."
    }, {
      "heading" : "2 Arabic LMs",
      "text" : "The term Arabic refers to a collection of languages, language varieties, and dialects. The standard variety of Arabic is MSA, and there exists a large number of dialects that are usually defined at the level of the region or country (Abdul-Mageed et al., 2020a, 2021a,b). A number of Arabic LMs has been developed. The most notable among these is AraBERT (Antoun et al., 2020), which is trained with the same architecture as BERT (Devlin et al., 2019) and uses the BERTBase configuration. AraBERT is trained on 23GB of Arabic text, making ∼ 70M sentences and 3B words, from Arabic Wikipedia, the Open Source International dataset (OSIAN) (Zeroual et al., 2019) (3.5M news articles from 24 Arab countries), and 1.5B words Corpus from El-Khair (2016) (5M articles extracted from 10 news sources). Antoun et al. (2020) evaluate AraBERT on three Arabic downstream tasks. These are (1) sentiment analysis from six different datasets: HARD (Elnagar et al., 2018), ASTD (Nabil et al., 2015), ArsenTDLev (Baly et al., 2019), LABR (Aly and Atiya, 2013), and ArSaS (Elmadany et al., 2018). (2) NER, with the ANERcorp (Benajiba and Rosso, 2007), and (3) Arabic QA, on Arabic-SQuAD and ARCD (Mozannar et al., 2019) datasets. Another Arabic LM that was also introduced is ArabicBERT (Safaya et al., 2020), which is similarly based on BERT architecture. ArabicBERT was pretrained on two datasets only, Arabic Wikipedia and\nArabic OSACAR (Suárez et al., 2019). Since both of these datasets are already included in AraBERT, and Arabic OSACAR1 has significant duplicates, we compare to AraBERT only. GigaBERT (Lan et al., 2020), an Arabic and English LM designed with code-switching data in mind, was also introduced.2"
    }, {
      "heading" : "3 Our Models",
      "text" : ""
    }, {
      "heading" : "3.1 ARBERT",
      "text" : ""
    }, {
      "heading" : "3.1.1 Training Data",
      "text" : "We train ARBERT on 61GB of MSA text (6.5B tokens) from the following sources:\n• Books (Hindawi). We collect and preprocess 1, 800 Arabic books from the public Arabic bookstore Hindawi.3\n• El-Khair. This is a 5M news articles dataset from 10 major news sources covering eight Arab countries from El-Khair (2016).\n• Gigaword. We use Arabic Gigaword 5th Edition from the Linguistic Data Consortium (LDC).4 The dataset is a comprehensive archive of newswire text from multiple Arabic news sources.\n• OSCAR. This is the MSA and Egyptian Arabic portion of the Open Super-large Crawled Almanach coRpus (Suárez et al., 2019),5\na huge multilingual subset from Common Crawl6 obtained using language identification and filtering.\n• OSIAN. The Open Source International Arabic News Corpus (OSIAN) (Zeroual et al., 2019) consists of 3.5 million articles from 31 news sources in 24 Arab countries.\n• Wikipedia Arabic. We download and use the December 2019 dump of Arabic Wikipedia. We use WikiExtractor7 to extract articles and remove markup from the dump.\n1https://oscar-corpus.com. 2Since GigaBERT is very recent, we could not compare to it. However, we note that our pre-training datasets are much larger (i.e., 15.6B tokens for MARBERT vs. 4.3B Arabic tokens for GigaBERT) and more diverse.\n3https://www.hindawi.org/books/. 4https://catalog.ldc.upenn.edu/LDC2011T11. 5https://oscar-corpus.com/. 6https://commoncrawl.org. 7https://github.com/attardi/wikiextractor.\nWe provide relevant size and token count statistics about the datasets in Table 1."
    }, {
      "heading" : "3.1.2 Training Procedure",
      "text" : "Pre-processing. To prepare the raw data for pretraining, we perform light pre-processing. This helps retain a faithful representation of the naturally occurring text. We only remove diacritics and replace URLs, user mentions, and hashtags that may exist in any of the collections with the generic string tokens URL, USER, and HASHTAG, respectively. We do not perform any further preprocessing of the data before splitting the text off to wordPieces (Schuster and Nakajima, 2012). Multilingual models such as mBERT and XLM-R have 5K (out of 110K) and 14K (out of 250K) Arabic WordPieces, respectively, in their vocabularies. AraBERT employs a vocabulary of 60K (out of 64K).8 For ARBERT, we use a larger vocabulary of 100K WordPieces. For tokenization, we use the WordPiece tokenizer (Wu et al., 2016) provided by Devlin et al. (2019). Pre-training. For ARBERT, we follow Devlin et al. (2019)’s pre-training setup. To generate each training input sequence, we use the whole word masking, where 15% of the N input tokens are selected for replacement. These tokens are replaced 80% of the time with the [MASK] token, 10% with a random token, and 10% with the original token. We use the original implementation of BERT in the TensorFlow framework.9 As mentioned, we use the same network architecture as BERTBase: 12 layers, 768 hidden units, 12 heads, for a total of ∼ 163M parameters. We use a batch size of 256 sequences and a maximum sequence length of 128 tokens (256 sequences × 128 tokens = 32, 768 tokens/batch) for 8M steps, which is approximately 42 epochs over the 6.5B tokens. For all our models, we use a learning rate of 1e−4.\n8The empty 4K vocabulary bin is reserved for additional wordPieces, if needed.\n9https://github.com/google-research/bert.\nWe pre-train the model on one Google Cloud TPU with eight cores (v2.8) from TensorFlow Research Cloud (TFRC).10 Training took ∼ 16 days, for 42 epochs over all the tokens. Table 2 shows a comparison of ARBERT with mBERT, XLM-R, AraBERT, and MARBERT (see Section 3.2) in terms of data sources and size, vocabulary size, and model parameters."
    }, {
      "heading" : "3.2 MARBERT",
      "text" : "As we pointed out in Sections 1 and 2, Arabic has a large number of diverse dialects. Most of these dialects are under-studied due to rarity of resources. Multilingual models such as mBERT and XLM-R are trained on mostly MSA data, which is also the case for AraBERT and ARBERT. As such, these models are not best suited for downstream tasks involving dialectal Arabic. To treat this issue, we use a large Twitter dataset to pre-train a new model, MARBERT, from scratch as we describe next."
    }, {
      "heading" : "3.2.1 Training data",
      "text" : "To pre-train MARBERT, we randomly sample 1B Arabic tweets from a large in-house dataset of about 6B tweets. We only include tweets with at least three Arabic words, based on character string matching, regardless whether the tweet has non-Arabic string or not. That is, we do not remove non-Arabic so long as the tweet meets the three Arabic word criterion. The dataset makes up 128GB of text (15.6B tokens)."
    }, {
      "heading" : "3.2.2 Training Procedure",
      "text" : "Pre-processing. We employ the same preprocessing as ARBERT. Pre-training. We use the same network architecture as BERTBase, but without the next sentence prediction (NSP) objective since tweets are short.11 We use the same vocabulary size (100K wordPieces) as ARBERT, and MARBERT also has ∼ 160M parameters. We train MARBERT for 17M steps (∼ 36 epochs) with a batch size of 256 and a maximum sequence length of 128. Training took ∼ 40 days on one Google Cloud TPU (eight cores). We now present a comparison between our models and popular multilingual models as well as AraBERT.\n10https://www.tensorflow.org/tfrc. 11It was also shown that NSP is not crucial for model per-\nformance (Liu et al., 2019a)."
    }, {
      "heading" : "3.3 Model Comparison",
      "text" : "Our models compare to mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020) (base and large), and AraBERT (Antoun et al., 2020) in terms of training data size, vocabulary size, and overall model capacity as we summarize in Table 2. In terms of the actual Arabic variety involved, Devlin et al. (2019) train mBERT with Wikipedia Arabic data, which is MSA. XLM-R (Conneau et al., 2020) is trained on Common Crawl data, which likely involves a small amount of Arabic dialects. AraBERT is trained on MSA data only. ARBERT is trained on a large collection of MSA datasets. Unlike all other models, our MARBERT model is trained on Twitter data, which involves both MSA and diverse dialects. We now describe our fine-tuning setup."
    }, {
      "heading" : "3.4 Model Fine-Tuning",
      "text" : "We evaluate our models by fine-tuning them on a wide range of tasks, which we thematically organize into six clusters: (1) sentiment analysis (SA), (2) social meaning (SM) (i.e., age and gender, dangerous and hateful speech, emotion, irony, and sarcasm), (3) topic classification (TC), (4) dialect identification (DI), (5) named entity recognition (NER), and (6) question answering (QA). For all classification tasks reported in this paper, we compare our models to four other models: mBERT, XLMRBase, XLM-RLarge, and AraBERT. We note that XLM-RLarge is ∼ 3.4× larger than any of our own models (∼ 550M parameters vs. ∼ 160M). We offer two main types of evaluation: on (i) individual tasks, which allows us to compare to other works on each individual dataset (48 classification tasks on 42 datasets), and (ii) ARLUE clusters (six task clusters).\nFor all reported experiments, we follow the same light pre-processing we use for pre-training. For all individual tasks and ARLUE task clusters, we finetune on the respective training splits for 25 epochs, identifying the best epoch on development data, and reporting on both development and test data.12 We typically use the exact data splits provided by original authors of each dataset. Whenever no clear\n12A minority of datasets came with no development split from source, and so we identify and report the best epoch only on test data for these. This allows us to compare all the models under the same conditions (25 epochs) and report a fair comparison to the respective original works. For all ARLUE cluster tasks, we identify the best epoch exclusively on our development sets (shown in Table 10).\nsplits are available, or in cases where expensive cross-validation was used in source, we divide the data following a standard 80% training, 10% development, and 10% test split. For all experiments, whether on individual tasks or ARLUE task clusters, we use the Adam optimizer (?) with input sequence length of 256, a batch size of 32, and a learning rate of 2e−6. These values were identified in initial experiments based on development data of a few tasks.13 We now introduce individual tasks."
    }, {
      "heading" : "4 Individual Downstream Tasks",
      "text" : ""
    }, {
      "heading" : "4.1 Sentiment Analysis",
      "text" : "Datasets. We fine-tune the language models on all publicly available SA datasets we could find in addition to those we acquired directly from authors. In total, we have the following 17 MSA and DA datasets: AJGT (Alomari et al., 2017), AraNETSent (Abdul-Mageed et al., 2020b), AraSenTi-Tweet (Al-Twairesh et al., 2017), ArSarcasmSent (Farha and Magdy, 2020), ArSAS (Elmadany et al., 2018), ArSenDLev (Baly et al., 2019), ASTD (Nabil et al., 2015), AWATIF (Abdul-Mageed and Diab, 2012), BBNS & SYTS (Salameh et al., 2015), CAMelSent (Obeid et al., 2020), HARD (Elnagar et al., 2018), LABR (Aly and Atiya, 2013), TwitterAbdullah (Abdulla et al., 2013), TwitterSaad,14 and SemEval2017 (Rosenthal et al., 2017). Details about the datasets and their splits are in Section A.1. Baselines. We compare to the STOA listed in Table 3 and Table 4 captions. For all datasets with no baseline in Table 3, we consider AraBERT our baseline. Details about SA baselines are in Section A.2. Results. To facilitate comparison to previous works with the appropriate evaluation metrics, we\n13NER and QA are expetions, where we use sequence lengths of 128 and 384, respectively; a batch sizes of 16 for both; and a learning rate of 2e−6 and 3e−5, respectively.\n14www.kaggle.com/mksaad/arabic-sentiment-twitter."
    }, {
      "heading" : "Dataset (classes) SOTA mBERT XLM-RB XLM-RL AraBERT ARBERT MARBERT",
      "text" : ""
    }, {
      "heading" : "Dataset (classes) SOTA mBERT XLM-RB XLM-RL AraBERT ARBERT MARBERT",
      "text" : "split our results into two tables: We show results in F1PN in Table 3 and F1 in Table 4. We typically bold the best result on each dataset. Our models achieve best results in 13 out of the 17 classification tasks reported in the two tables combined, while XLM-R (which is a much larger model) outperforms our models in the 4 remaining tasks. We also note that XLM-R acquires better results than AraBERT in the majority of tasks, a trend that continues for the rest of tasks. Results also clearly show that MARBERT is more powerful than than ARBERT. This is due to MARBERT’s larger and more diverse pre-training data, especially that many of the SA datasets involve dialects and come from social media."
    }, {
      "heading" : "4.2 Social Meaning Tasks",
      "text" : "We collectively refer to a host of tasks as social meaning. These are age and gender detection; dangerous, hateful, and offensive speech detection; emotion detection; irony detection; and sarcasm detection. We now describe datasets we use for each of these tasks. Datasets. For both age and gender, we use"
    }, {
      "heading" : "Task (classes) SOTA mBERT XLM-RB XLM-RL AraBERT ARBERT MARBERT",
      "text" : "Arap-Tweet (Zaghouani and Charfi, 2018). We use AraDan (Alshehri et al., 2020) for dangerous speech. For offensive language and hate speech, we use the dataset released in the shared task (subtasks A and B) of offensive speech by Mubarak et al. (2020). We also use AraNETEmo (AbdulMageed et al., 2020b), IDAT@FIRE2019 (Ghanem et al., 2019), and ArSarcasm (Farha and Magdy, 2020) for emotion, irony and sarcasm, respectively. More information about these datasets and their splits is in Appendix B.1. Baselines. Baselines for social meaning tasks are the SOTA listed in Table 5 caption. Details about each baseline is in Appendix B.2. Results. As Table 5 shows, our models acquire best results on all eight tasks. Of these, MARBERT achieves best performance on seven tasks, while ARBERT is marginally better than MARBERT on one task (irony@FIRE2019). The sizeable gains MARBERT achieves reflects its superiority on social media tasks. On average, our models are 9.83 F1 better than all previous SOTA."
    }, {
      "heading" : "4.3 Topic Classification",
      "text" : "Classifying documents by topic is a classical task that still has practical utility. We use four TC datasets, as follows: Datasets. We fine-tune on Arabic News Text (ANT) (Chouigui et al., 2017) under three pretaining settings (title only, text only, and title+text.), Khaleej (Abbas et al., 2011), and OSAC (Saad and Ashour, 2010). Details about these datasets and the classes therein are in Appendix C.1. Baselines. Since, to the best of our knowledge, there are no published results exploiting deep learning on TC, we consider AraBERT a strong baseline. Results. As Table 6 shows, ARBERT acquires best results on both OSAC and Khaleej, and the title-only setting of ANT. AraBERT slightly outperforms our models on the text-only and title+text"
    }, {
      "heading" : "Dataset (classes) mBERT XLM-RB XLM-RL AraBERT ARBERT MARBERT",
      "text" : ""
    }, {
      "heading" : "Dataset (classes) Task SOTA mBERT XLM-RB XLM-RL AraBERT ARBERT MARBERT",
      "text" : "settings of ANT."
    }, {
      "heading" : "4.4 Dialect Identification",
      "text" : "Arabic dialect identification can be performed at different levels of granularity, including binary (i.e., MSA-DA), regional (e.g., Gulf, Levantine), country level (e.g., Algeria, Morocco), and recently province level (e.g., the Egyptian province of Cairo, the Saudi province of Al-Madinah) (Abdul-Mageed et al., 2020a, 2021b). Datasets. We fine-tune our models on the following datasets: Arabic Online Commentary (AOC) (Zaidan and Callison-Burch, 2014), ArSarcasmDia (Farha and Magdy, 2020),15 MADAR (sub-task 2) (Bouamor et al., 2019), NADI-2020 (Abdul-Mageed et al., 2020a), and QADI (Abdelali et al., 2020). Details about these datasets are in Table D.1. Baselines. Our baselines are marked in Table 7 caption. Details about the baselines are in Table D.2. Results. As Table 7 shows, our models outperform all SOTA as well as the baseline AraBERT across all classification levels with sizeable margins. These results reflect the powerful and diverse dialectal representation of MARBERT, enabling it to serve wider communities. Although ARBERT is developed mainly for MSA, it also outperforms all other models."
    }, {
      "heading" : "4.5 Named Entity Recognition",
      "text" : "We fine-tune the models on five NER datasets. Datasets. We use ACE03NW and ACE03BN (Mitchell et al., 2004), ACE04NW (Mitchell et al., 2004), ANERcorp (Benajiba and Rosso, 2007), and TW-NER (Darwish, 2013). Table E.1 shows the\n15ArSarcasmDia carries regional dialect labels."
    }, {
      "heading" : "Dataset SOTA mBERT XLM-RB XLM-RL AraBERT ARBERT MARBERT",
      "text" : "distribution of named entity classes across the five datasets. Baseline. We compare our results with SOTA presented by Khalifa and Shaalan (2019) and follow them in focusing on person (PER), location (LOC) and organization (ORG) named entity labels while setting other labels to the unnamed entity (O). Details about Khalifa and Shaalan (2019) SOTA models are in Appendix E.2. Results. As Table 8 shows, our models outperform SOTA on two out of the five NER datasets. We note that even though SOTA (Khalifa and Shaalan, 2019) employ a complex combination of CNNs and character-level LSTMs, which may explain their better results on two datasets, MARBERT still achieves highest performance on the social media dataset (TW-NER)."
    }, {
      "heading" : "4.6 Question Answering",
      "text" : "Datasets. We use ARCD (Mozannar et al., 2019) and the three human translated Arabic test sections of the XTREME benchmark (Hu et al., 2020): MLQA (Lewis et al., 2020), XQuAD (Artetxe et al., 2020), and TyDi QA (Artetxe et al., 2020). Details about these datasets are in Table F.1. Baselines. We compare to Antoun et al. (2020) and consider their system a baseline on ARCD. We follow the same splits they used where we fine-tune on Arabic SQuAD (Mozannar et al., 2019) and 50% of ARCD and test on the remaining 50% of ARCD (ARCD-test). For all other experiments, we fine-tune on the Arabic machine translated SQuAD (AR-XTREME) from the XTREME multilingual benchmark (Hu et al., 2020) and test on the human translated test sets listed above. Our baselines in these is Hu et al. (2020)’s mBERTBase model on gold (human) data. Results. As is standard, we report QA results in terms of both Exact Match (EM) and F1. We find that results with ARBERT and MARBERT on QA are not competitive, a clear discrepancy from what we have observed thus far on other tasks. We hypothesize this is because the two models are pre-trained with a sequence length of only 128, which does not allow them to sufficiently capture\nboth a question and its likely answer within the same sequence window during the pre-training.16 To rectify this, we further pre-train the stronger model, MARBERT, on the same MSA data as ARBERT in addition to AraNews dataset (Nagoudi et al., 2020) (8.6GB), but with a bigger sequence length of 512 tokens for 40 epochs. We call this further pre-trained model MARBERT-v2, noting it has 29B tokens. As Table 9 shows, MARBERTv2 acquires best performance on all but one test set, where XLM-RLarge marginally outperforms us (only in F1)."
    }, {
      "heading" : "5 ARLUE",
      "text" : ""
    }, {
      "heading" : "5.1 ARLUE Categories",
      "text" : "We concatenate the corresponding splits of the individual datasets to form ARLUE, which is a conglomerate of task clusters. That is, we concatenate all training data from each group of tasks into a single TRAIN, all development into a single DEV, and all test into a single TEST. One exception is the social meaning tasks whose data we keep independent (see ARLUESM below). Table 10 shows a summary of the ARLUE datasets.17 We now briefly describe how we merge individual datasets into ARLUE. ARLUESenti. To construct ARLUESenti, we collapse the labels very negative into negative, very positive into positive, and objective into neutral, and remove the mixed class. This gives us the 3 classes negative, positive, and neutral for ARLUESenti. Details are in Table A.1. ARLUESM. We refer to the different social meaning datasets collectively as ARLUESM. We do not merge these datasets to preserve the conceptual coherence specific to each of the tasks. Details about individual datasets in ARLUESM are in B.1. ARLUETopic. We straightforwardly merge the TC datasets to form ARLUETopic, without modifying any class labels. Details of ARLUETopic data are in Table C.1. ARLUEDia. We construct three ARLUEDia categories. Namely, we concatenate the AOC and AraSarcasmDia MSA-DA classes to form ARLUEDia-B (binary) and the region level classes from the same two datasets to acquire ARLUEDia-R (4-classes, region). We then merge the country\n16In addition, MARBERT is not trained on Wikipedia data from where some questions come.\n17Again, ARLUESM datasets are kept independent, but to provide a summary of all ARLUE datasets we collate the numbers in Table 10.\nclasses from the rest of datasets to get ARLUEDia-C (21-classes, country). Details are in Table D.1. ARLUENER & ARLUEQA. We straightforwardly concatenate all corresponding splits from the different NER and QA datasets to form ARLUENER and ARLUEQA, respectively. Details of each of these task clusters data are in Tables E.1 and F.1, respectively."
    }, {
      "heading" : "5.2 Evaluation on ARLUE",
      "text" : "We present results on each task cluster independently using the relevant metric for both the development split (Table 11) and test split (Table 12). Inspired by McCann et al. (2018) and Wang et al. (2018) who score NLP systems based on their performance on multiple datasets, we introduce an ARLUE score. The ARLUE score is simply the macro-average of the different scores across all task clusters, weighting each task equally. Following Wang et al. (2018), for tasks with multiple metrics (e.g., accuracy and F1), we use an unweighted average of the metrics as the score for the task when computing the overall macro-average. As Table 12 shows, our MARBERT-v2 model achieves the highest ARLUE score (77.40), followed by XLM-RL (76.55) and ARBERT (76.07). We also note that in spite of its superiority on social data, MARBERT ranks top 4. This is due to MARBERT suffering on the QA tasks (due to its short input sequence length), and to a lesser extent on NER and TC."
    }, {
      "heading" : "6 Related Work",
      "text" : "English and Multilingual LMs. Pre-trained LMs exploiting a self-supervised objective with masking such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) have revolutionized NLP. Multilingual versions of these models such as mBERT and XLM-RoBERTa (Conneau et al., 2020) were also pre-trained. Other models with different objectives and/or architectures such as ALBERT (Lan et al., 2019), T5 (Raffel et al., 2020) and its multilingual version, mT5 (Xue et al., 2021), and GPT3 (Brown et al., 2020) were also introduced. More information about BERT-inspired LMs can be found in Rogers et al. (2020). Non-English LMs. Several models dedicated to individual languages other than English have been developed. These include AraBERT (Antoun et al., 2020) and ArabicBERT (Safaya et al., 2020) for Arabic, Bertje for Dutch (de Vries et al., 2019), CamemBERT (Martin et al., 2020) and FlauBERT (Le et al., 2020) for French, PhoBERT for Vietnamese (Nguyen and Tuan Nguyen, 2020), and the models presented by Virtanen et al. (2019) for Finnish, Dadas et al. (2020) for Polish, and Malmsten et al. (2020) for Swedish. Pyysalo et al. (2020) also create monolingual LMs for 42 languages exploiting Wikipedia data. Our models contributed to this growing work of dedicated LMs, and has the advantage of covering a wide range of dialects. Our MARBERT and MARBERT-v2 models are also trained with a massive scale social media dataset, endowing them with a remarkable ability for real-world downstream tasks. NLP Benchmarks. In recent years, several NLP benchmarks were designed for comparative evaluation of pre-trained LMs. For English, McCann et al. (2018) introduced NLP Decathlon (DecaNLP) which combines 10 common NLP datasets/tasks. Wang et al. (2018) proposed GLUE, a popular benchmark for evaluating nine NLP tasks. Wang et al. (2019) also presented SuperGLUE, a more challenging benchmark than GLUE covering seven tasks. In the cross-lingual setting, Hu et al. (2020)\nprovide a Cross-lingual TRansfer Evaluation of Multilingual Encoders (XTREME) benchmark for the evaluation of cross-lingual transfer learning covering nine tasks for 40 languages (12 language families). ARLUE complements these benchmarking efforts, and is focused on Arabic and its dialects. ARLUE is also diverse (involves 42 datasets) and challenging (our best ARLUE score is at 77.40)."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We presented our efforts to develop two powerful Transformer-based language models for Arabic. Our models are trained on large-to-massive datasets that cover different domains and text genres, including social media. By pre-training MARBERT and MARBERT-v2 on dialectal Arabic, we aim at enabling downstream NLP technologies that serve wider and more diverse communities. Our best models perform better than (or on par with) XLMRLarge (∼ 3.4× larger than our models), and hence are more energy efficient at inference time. Our models are also significantly better than AraBERT,\nthe currently best-performing Arabic pre-trained LM. We also introduced AraLU, a large and diverse benchmark for Arabic NLU composed of 42 datasets thematically organized into six main task clusters. ARLUE fills a critical gap in Arabic and multilingual NLP, and promises to help propel innovation and facilitate meaningful comparisons in the field. Our models are publicly available. We also plan to publicly release our ARLUE benchmark. In the future, we plan to explore self-training our language models as a way to improve performance following Khalifa et al. (2021). We also plan to investigate developing more energy efficient models."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We gratefully acknowledges support from the Natural Sciences and Engineering Research Council of Canada, the Social Sciences and Humanities Research Council of Canada, Canadian Foundation for Innovation, Compute Canada and UBC ARCSockeye (https://doi.org/10.14288/SOCKEYE). We also thank the Google TFRC program for providing us with free TPU access."
    }, {
      "heading" : "Ethical Considerations",
      "text" : "Although our language models are pre-trained using datasets that were public at the time of collection, parts of these datasets might become private or get removed (e.g., tweets that are deleted by users). For this reason, we will not release or redistribute any of the pre-training datasets. Data coverage is another important consideration: Our datasets have wide coverage, and one of our contributions is offering models that can serve more diverse communities in better ways than existing models. However, our models may still carry biases that we have not tested for and hence we recommend they be used with caution. Finally, our models deliver better performance than larger-sized models and as such are more energy conserving. However, smaller models that can achieve simply ‘good enough’ results should also be desirable. This is part of our own future research, and the community at large is invited to develop novel methods that are more environment friendly."
    }, {
      "heading" : "Appendices",
      "text" : ""
    }, {
      "heading" : "A Sentiment Analysis",
      "text" : ""
    }, {
      "heading" : "A.1 SA Datasets",
      "text" : "• AJGT. The Arabic Jordanian General Tweets (AJGT) dataset (Alomari et al., 2017) covers MSA and Jordanian Arabic, with 900 positive and 900 negative posts.\n• AraNETSent. Abdul-Mageed et al. (2020b) collect 15 datasets in both MSA and dialects from Abdul-Mageed and Diab (2012) (AWATIF), Abdul-Mageed et al. (2014) (SAMAR), Abdulla et al. (2013); Nabil et al. (2015); Kiritchenko et al. (2016); Aly and Atiya (2013); Salameh et al. (2015); Rosenthal et al. (2017); Alomari et al. (2017); Mohammad et al. (2018), and Baly et al. (2019). These datasets carry both binary (negative and positive) and three-way (negative, neutral, and positive) labels, but Abdul-Mageed et al. (2020b) map them into binary sentiment only.\n• AraSenTi-Tweet. This comprises 17, 573 gold labeled MSA and Saudi Arabic tweets by Al-Twairesh et al. (2017).\n• ArSarcasmSent This sarcasm dataset is labeled with sentiment tags by Farha and Magdy (2020) who extract it from ASTD (Nabil et al., 2015) (10, 547 tweets) and SemEval-2017 Task 4 (Rosenthal et al., 2017) (8, 075 tweets).\n• ArSAS. This Arabic Speech Act and Sentiment (ArSAS) corpus (Elmadany et al., 2018) consists of tweets annotated with sentiment tags.\n• ArSenD-Lev. The Arabic Sentiment Twitter Dataset for LEVantine dialect (ArSenDLev) (Baly et al., 2019) has 4, 000 tweets retrieved from the Levant region.\n• ASTD. This is a collection of 10, 006 Egyptian tweets by Nabil et al. (2015).\n• AWATIF. This is an MSA dataset from newswire, Wikipedia, and web fora introduced by Abdul-Mageed and Diab (2012).\n• BBNS & SYTS. The BBN blog posts Sentiment (BBNS) and Syria Tweets\nSentiment (SYTS) are introduced by Salameh et al. (2015).\n• CAMelSent. Obeid et al. (2020) merge training and development data from ArSAS (Elmadany et al., 2018), ASTD (Nabil et al., 2015), SemEval (Rosenthal et al., 2017), and ArSenTD (Al-Twairesh et al., 2017) to create a new training dataset (∼ 24K) and evaluate on the independent test sets from each of these sources.\n• HARD. The Hotel Arabic Reviews Dataset (HARD) (Elnagar et al., 2018) consists of 93, 700 MSA and dialect hotel reviews.\n• LABR. The Large Arabic Book Review Corpus (Aly and Atiya, 2013) has 63, 257 book reviews from Goodreads,18 each rated with a 1-5 stars scale.\n• TwitterAbdullah.19 This is a dataset of 2, 000 MSA and Jordanian Arabic tweets manually labeled by Abdulla et al. (2013).\n• TwitterSaad. This dataset is collected using an emoji lexicon by Moatez Saad in 2019 and is available on Kaggle.20\n• SemEval-2017. This is the SemEval-2017 sentiment analysis in Arabic Twitter task datasetby Rosenthal et al. (2017)."
    }, {
      "heading" : "A.2 SA Baselines",
      "text" : "For SA, we compare to the following STOA:\n• Antoun et al. (2020). We compare to best results reported by the authors on five SA datasets: HARD, balanced ASTD (which we refer to as ASTD-B), ArSenTD-Lev, AJGT, and the unbalanced positive and negative classes for LABR. They split each dataset into 80/20 for Train/Test, respectively, and report in accuracy using the best epoch identified on test data. For a valid comparison, we follow their data splits and evaluation set up.\n• Obeid et al. (2020). They fine-tune mBERT and AraBERT on the merged CAMelsent\n18www.goodreads.com. 19For ease of reference, we assign a name to this and other\nunnamed datasets. 20www.kaggle.com/mksaad/arabic-sentiment-twittercorpus.\ndatasets and report in F1PN , which is the macro F1 score over the positive and negative classes only (while neglecting the neutral class).\n• Abdul-Mageed et al. (2020b). They finetune mBERT on the AraNETSent data and report results in F1 score on test data."
    }, {
      "heading" : "A.3 SA Evaluation on DEV",
      "text" : "Table A.2 shows results of SA on DEV for datasets where there is a development split."
    }, {
      "heading" : "Dataset (classes) mBERT XLM-RB XLM-RL AraBERT ARBERT MARBERT",
      "text" : ""
    }, {
      "heading" : "B Social Meaning",
      "text" : ""
    }, {
      "heading" : "B.1 SM Tasks & Datasets",
      "text" : "• Age and Gender. For both age and gender, we use the Arap-Tweet dataset (Zaghouani and Charfi, 2018), which covers 17 different countries from 11 Arab regions. We follow the 80-10-10 data split of AraNet (Abdul-Mageed et al., 2020b).\n• Dangerous Speech. We use the dangerous speech AraDang dataset from Alshehri et al. (2020), which is composed of tweets manually labeled with dangerous and safe tags.\nTask Dataset (classes) Classes TRAIN DEV TEST Age Arap-Tweet (3) { ≤ 24 yrs, 25− 34 yrs, ≥ 35 yrs } 1.3M 160.7K 160.7K Dangerous AraDang (2) {dangerous, not-dangerous} 3.5K 616 664 Emotion AraNETEmo (8) {ang, anticip, disg, fear, joy, sad, surp, trust} 190K 911 942 Gender Arap-Tweet (2) {female, male} 1.3M 160.7K 160.7K Hate Speech HS@OSACT (2) {hate, not-hate} 10K 1K 2K Irony FIRE2019 (2) {irony, not-irony} 3.6K - 404 Offensive OFF@OSACT (2) {offensive, not-offensive} 10K 1K 2K Sarcasm AraSarcasm (2) {sarcasm, not-sarcasm} 8.4K - 2.1K\nMore details about these datasets are in Table B.1."
    }, {
      "heading" : "B.2 SM Baselines",
      "text" : "• Age and Gender. We compare to AraNET Abdul-Mageed et al. (2020b) age and gender models, trained by fine-tuning mBERT. The authors report 51.42 and 65.30 F1 on age and gender, respectively.\n• Dangerous Speech. We compare to Alshehri et al. (2020), who report a best of 59.60 F1 on test with an mBERT model fined-tuned on emotion data.\n• Emotion. We compare to Abdul-Mageed et al. (2020b), who acquire 60.32 F1 on test with a fine-tuned mBERT.\n• Hate Speech. The best results on the offensive and hate speech shared task (Mubarak et al., 2020) are at 95 F1 score and are reported by Husain (2020), who employ heavy\n21http://edinburghnlp.inf.ed.ac.uk/workshops/OSACT4."
    }, {
      "heading" : "Task (classes) mBERT XLM-RB XLM-RL AraBERT ARBERT MARBERT",
      "text" : "Age (3) 56.33 59.70 53.63 57.67 58.60 62.19 Dangerous (2) 67.35 65.09 69.95 67.73 68.58 75.50 Emotion (8) 61.34 72.09 72.78 65.46 68.05 75.18 Gender (2) 68.06 71.10 71.23 67.61 69.97 72.81 Hate (2) 75.91 76.56 78.00 72.09 75.01 82.91 Irony (2) 81.08 83.12 81.29 79.12 84.83 86.77 Offensive (2) 84.04 85.26 86.72 87.21 88.77 91.68"
    }, {
      "heading" : "B.3 SM Evaluation on DEV",
      "text" : "Table B.2 shows results of the social meaning tasks on development splits."
    }, {
      "heading" : "C Topic Classification",
      "text" : ""
    }, {
      "heading" : "C.1 TC Datasets",
      "text" : ""
    }, {
      "heading" : "Dataset (classes) mBERT XLM-RB XLM-RL AraBERT ARBERT MARBERT",
      "text" : ""
    }, {
      "heading" : "C.2 TC Evaluation on DEV",
      "text" : "Results of TC tasks on DEV data are in Table C.2."
    }, {
      "heading" : "D Dialect Identification",
      "text" : ""
    }, {
      "heading" : "D.1 DIA Datasets",
      "text" : "We introduce each dataset briefly here and provide a description summary of all datasets in Table D.1.\n• Arabic Online Commentary (AOC). This is a repository of 3M Arabic comments on online news (Zaidan and Callison-Burch, 2014). It is labeled with MSA and three regional dialects (Egyptian, Gulf, and Levantine).\n• ArSarcasmDia. This dataset is developed by Farha and Magdy (2020) for sarcasm detection but also carries regional dialect labels from the set {Egyptian, Gulf, Levantine, Maghrebi}.\n• MADAR. Sub-task 2 of the MADAR shared task (Bouamor et al., 2019)22 is focused on user-level dialect identification with manuallycurated country labels (n=21).\n• NADI-2020. The first Nuanced Arabic Dialect Identification shared task (NADI 2020) (Abdul-Mageed et al., 2020a)23 targets country level (n=21) as well as province level (n=100) dialects.\n• QADI. The QCRI Arabic Dialect Identification (QADI) dataset (Abdelali et al., 2020) is labeled at the country level (n=18).\nDetails of the datasets are in Table D.1."
    }, {
      "heading" : "D.2 DIA Baselines",
      "text" : "• Elaraby and Abdul-Mageed (2018) report three levels of classification on AOC data: (1) MSA vs. DA (87.23 accuracy), (2) regional (i.e., Egyptian, Gulf, and Levantine) (87.81 accuracy), and (3) MSA, Egyptian, Gulf, and\n22https://camel.abudhabi.nyu.edu/madar-shared-task2019/.\n23https://github.com/UBC-NLP/nadi."
    }, {
      "heading" : "Dataset (classes) Task mBERT XLM-RB XLM-RL AraBERT ARBERT MARBERT",
      "text" : "MADAR(21) Country 33.75 34.54 33.28 33.47 39.24 40.61 AOC(4) Regoin 80.07 78.97 79.55 80.85 81.96 83.56 AOC(3) Regoin 87.07 86.80 88.21 88.46 89.57 91.56 AOC(2) Binary 87.89 87.63 88.38 88.76 89.32 89.66 NADI(21) Country 14.49 17.30 18.62 16.18 23.73 26.40 NADI(100) Province 02.32 03.91 4.00 03.04 06.05 05.23"
    }, {
      "heading" : "D.3 DIA Evaluation on DEV",
      "text" : "Table D.2 shows results of the dialect identification tasks on development splits."
    }, {
      "heading" : "E Named Entity Recognition",
      "text" : ""
    }, {
      "heading" : "E.1 NER datasets",
      "text" : "Table E.1 and Table E.2 show the data splits across our NER datasets, and the results of all our models on the development splits.\nDataset Tokens Train DEV Test ANERcorp 150.2K 95.5K 24.8K 29.9K ACE03BN 15.6K 11.6K 2K 2K ACE03NW 27K 21.3K 2.7K 3K ACE04BN 70.5K 56.5K 7K 7K TW-NER 74.8K 42.9K 7.4K 24.5K ARLUENER 338.3K 227.7K 44.1K 66.5K\nTable E.1: Distribution of the Arabic NER datasets."
    }, {
      "heading" : "Dataset (classes) mBERT XLM-RB XLM-RL AraBERT ARBERT MARBERT",
      "text" : "ANERcorp 86.20 87.24 89.64 90.24 83.24 80.86 ACE03NW 80.57 88.21 90.49 89.76 88.17 85.02 ACE03BN 80.35 80.36 83.39 81.05 90.91 79.05 ACE04NW 87.21 90.08 91.94 89.70 89.33 86.80 TW-NER 52.60 73.61 77.70 73.61 70.78 67.39\nTable E.2: NER results (F1) on DEV."
    }, {
      "heading" : "E.2 NER Baselines",
      "text" : "Khalifa and Shaalan (2019) apply CNNs and BiLSTMs and report F1 scores on test sets, as follows: 88.77 (ANERcorp), 91.47 (ACE03NW), 94.92 (ACE03BN), 91.20 (ACE04NW), and 65.34 (Twitter). We use their exact data splits."
    }, {
      "heading" : "F Question Answering Datasets",
      "text" : "• ARCD. Mozannar et al. (2019) use crowdsourcing to develop the Arabic Reading Comprehension Dataset. We use the same ARCD data splits used by Antoun et al. (2020).\n• MLQA. This MultiLingual Question Answering benchmark is proposed by Lewis et al. (2020). It consists of over 5K extractive question-answer instances in SQuAD format in seven languages, including Arabic.\n• XQuAD. This Cross-lingual Question Answering Dataset Artetxe et al. (2020) consists of 1, 190 question-answer pairs and 240 paragraphs from SQuAD v1.1 (Rajpurkar et al., 2016) translated into ten languages (including Arabic) by professional translators.\n• TyDi QA. The TyDi QA dataset Artetxe et al. (2020) is manually curated and covers 11 languages (including Arabic). We focus on the “Gold” passage task only.\nDataset TRAIN DEV TEST AR-XTREME 86.7K (MT) - - ARCD - - 1.4K (H) AR-MLQA - 517 (HT) 5.3K (HT) AR-XQuAD - - 1.2K (HT) AR-TyDi-QA 14.8K (H) - 921 (H) ARLUEQA 101.6K 517 11.6K\nTable F.1: Multilingual & Arabic QA datasets. H: Human Created. HT: Human Translated. MT: Machine Translated."
    } ],
    "references" : [ {
      "title" : "Evaluation of topic identification methods on arabic corpora",
      "author" : [ "Mourad Abbas", "Kamel Smaı̈li", "Daoud Berkani" ],
      "venue" : "JDIM,",
      "citeRegEx" : "Abbas et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Abbas et al\\.",
      "year" : 2011
    }, {
      "title" : "Arabic Dialect Identification in the Wild",
      "author" : [ "Ahmed Abdelali", "Hamdy Mubarak", "Younes Samih", "Sabit Hassan", "Kareem Darwish." ],
      "venue" : "Proceedings of the Sixth Arabic Natural Language Processing Workshop.",
      "citeRegEx" : "Abdelali et al\\.,? 2020",
      "shortCiteRegEx" : "Abdelali et al\\.",
      "year" : 2020
    }, {
      "title" : "Samar: Subjectivity and sentiment analysis for arabic social media",
      "author" : [ "Muhammad Abdul-Mageed", "Mona Diab", "Sandra Kübler." ],
      "venue" : "Computer Speech & Language, 28(1):20–37.",
      "citeRegEx" : "Abdul.Mageed et al\\.,? 2014",
      "shortCiteRegEx" : "Abdul.Mageed et al\\.",
      "year" : 2014
    }, {
      "title" : "AWATIF: A Multi-Genre Corpus for Modern Standard Arabic Subjectivity and Sentiment Analysis",
      "author" : [ "Muhammad Abdul-Mageed", "Mona T Diab." ],
      "venue" : "LREC, volume 515, pages 3907–3914. Citeseer.",
      "citeRegEx" : "Abdul.Mageed and Diab.,? 2012",
      "shortCiteRegEx" : "Abdul.Mageed and Diab.",
      "year" : 2012
    }, {
      "title" : "2021a. DiaLex: A benchmark for evaluating multi",
      "author" : [ "Muhammad Abdul-Mageed", "Shady Elbassuoni", "Jad Doughman", "AbdelRahim Elmadany", "El Moatez Billah Nagoudi", "Yorgo Zoughby", "Ahmad Shaher", "Iskander Gaba", "Ahmed Helal", "Mohammed El-Razzaz" ],
      "venue" : null,
      "citeRegEx" : "Abdul.Mageed et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Abdul.Mageed et al\\.",
      "year" : 2021
    }, {
      "title" : "2020a. NADI 2020: The first nuanced Arabic dialect identification",
      "author" : [ "Muhammad Abdul-Mageed", "Chiyu Zhang", "Houda Bouamor", "Nizar Habash" ],
      "venue" : null,
      "citeRegEx" : "Abdul.Mageed et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Abdul.Mageed et al\\.",
      "year" : 2020
    }, {
      "title" : "NADI 2021: The second nuanced Arabic dialect identification shared task",
      "author" : [ "Muhammad Abdul-Mageed", "Chiyu Zhang", "AbdelRahim Elmadany", "Houda Bouamor", "Nizar Habash." ],
      "venue" : "Proceedings of the Sixth Arabic Natural Language Process-",
      "citeRegEx" : "Abdul.Mageed et al\\.,? 2021b",
      "shortCiteRegEx" : "Abdul.Mageed et al\\.",
      "year" : 2021
    }, {
      "title" : "AraNet: A deep learning toolkit for Arabic social media",
      "author" : [ "Muhammad Abdul-Mageed", "Chiyu Zhang", "Azadeh Hashemi", "El Moatez Billah Nagoudi." ],
      "venue" : "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools,",
      "citeRegEx" : "Abdul.Mageed et al\\.,? 2020b",
      "shortCiteRegEx" : "Abdul.Mageed et al\\.",
      "year" : 2020
    }, {
      "title" : "Arabic sentiment analysis: Corpus-based and lexicon-based",
      "author" : [ "Nawaf Abdulla", "N Mahyoub", "M Shehab", "Mahmoud Al-Ayyoub." ],
      "venue" : "Proceedings of The IEEE conference on Applied Electrical Engineering and Computing Technologies (AEECT).",
      "citeRegEx" : "Abdulla et al\\.,? 2013",
      "shortCiteRegEx" : "Abdulla et al\\.",
      "year" : 2013
    }, {
      "title" : "Arasenti-tweet: A corpus for Arabic sentiment analysis of saudi tweets",
      "author" : [ "Nora Al-Twairesh", "Hend Al-Khalifa", "AbdulMalik AlSalman", "Yousef Al-Ohali." ],
      "venue" : "Procedia Computer Science, 117:63–72.",
      "citeRegEx" : "Al.Twairesh et al\\.,? 2017",
      "shortCiteRegEx" : "Al.Twairesh et al\\.",
      "year" : 2017
    }, {
      "title" : "Enabling deep learning of emotion with first-person seed expressions",
      "author" : [ "Hassan Alhuzali", "Muhammad Abdul-Mageed", "Lyle Ungar." ],
      "venue" : "Proceedings of the Second Workshop on Computational Modeling of People’s Opinions, Personality, and",
      "citeRegEx" : "Alhuzali et al\\.,? 2018",
      "shortCiteRegEx" : "Alhuzali et al\\.",
      "year" : 2018
    }, {
      "title" : "Arabic tweets sentimental analysis using machine learning",
      "author" : [ "Khaled Mohammad Alomari", "Hatem M ElSherif", "Khaled Shaalan." ],
      "venue" : "International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, pages",
      "citeRegEx" : "Alomari et al\\.,? 2017",
      "shortCiteRegEx" : "Alomari et al\\.",
      "year" : 2017
    }, {
      "title" : "Understanding and detecting dangerous speech in social media",
      "author" : [ "Ali Alshehri", "El Moatez Billah Nagoudi", "Muhammad Abdul-Mageed." ],
      "venue" : "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared",
      "citeRegEx" : "Alshehri et al\\.,? 2020",
      "shortCiteRegEx" : "Alshehri et al\\.",
      "year" : 2020
    }, {
      "title" : "LABR: A Large Scale Arabic book Reviews Dataset",
      "author" : [ "Mohamed Aly", "Amir Atiya." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pages 494–498.",
      "citeRegEx" : "Aly and Atiya.,? 2013",
      "shortCiteRegEx" : "Aly and Atiya.",
      "year" : 2013
    }, {
      "title" : "Arabert: Transformer-based model for arabic language understanding",
      "author" : [ "Wissam Antoun", "Fady Baly", "Hazem Hajj." ],
      "venue" : "Proceedings of the 4th",
      "citeRegEx" : "Antoun et al\\.,? 2020",
      "shortCiteRegEx" : "Antoun et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are few-shot",
      "author" : [ "Askell" ],
      "venue" : null,
      "citeRegEx" : "Askell,? \\Q2020\\E",
      "shortCiteRegEx" : "Askell",
      "year" : 2020
    }, {
      "title" : "2019. BERT: Pre-training",
      "author" : [ "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Toutanova.,? \\Q2019\\E",
      "shortCiteRegEx" : "Toutanova.",
      "year" : 2019
    }, {
      "title" : "Multi-Task Learning using AraBert for Offensive Language Detection",
      "author" : [ "Marc Djandji", "Fady Baly", "Wissam Antoun", "Hazem Hajj." ],
      "venue" : "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offen-",
      "citeRegEx" : "Djandji et al\\.,? 2020",
      "shortCiteRegEx" : "Djandji et al\\.",
      "year" : 2020
    }, {
      "title" : "2016. 1.5 billion words Arabic Corpus. arXiv preprint arXiv:1611.04033",
      "author" : [ "Ibrahim Abu El-Khair" ],
      "venue" : null,
      "citeRegEx" : "El.Khair.,? \\Q2016\\E",
      "shortCiteRegEx" : "El.Khair.",
      "year" : 2016
    }, {
      "title" : "Weighted combination of BERT and N-GRAM features for Nuanced Arabic Dialect Identification",
      "author" : [ "Abdellah El Mekki", "Ahmed Alami", "Hamza Alami", "Ahmed Khoumsi", "Ismail Berrada." ],
      "venue" : "Proceedings of the Fourth Arabic Natural Language",
      "citeRegEx" : "Mekki et al\\.,? 2020",
      "shortCiteRegEx" : "Mekki et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep models for Arabic dialect identification on benchmarked data",
      "author" : [ "Mohamed Elaraby", "Muhammad Abdul-Mageed." ],
      "venue" : "Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018), pages 263–274, Santa",
      "citeRegEx" : "Elaraby and Abdul.Mageed.,? 2018",
      "shortCiteRegEx" : "Elaraby and Abdul.Mageed.",
      "year" : 2018
    }, {
      "title" : "ArSAS: An Arabic Speech-Act and Sentiment Corpus of Tweets",
      "author" : [ "AbdelRahim Elmadany", "Hamdy Mubarak", "Walid Magdy." ],
      "venue" : "OSACT, 3:20.",
      "citeRegEx" : "Elmadany et al\\.,? 2018",
      "shortCiteRegEx" : "Elmadany et al\\.",
      "year" : 2018
    }, {
      "title" : "Hotel Arabic-Reviews Dataset Construction for Sentiment Analysis Applications",
      "author" : [ "Ashraf Elnagar", "Yasmin S Khalifa", "Anas Einea." ],
      "venue" : "Intelligent Natural Language Processing: Trends and Applications, pages 35–52. Springer.",
      "citeRegEx" : "Elnagar et al\\.,? 2018",
      "shortCiteRegEx" : "Elnagar et al\\.",
      "year" : 2018
    }, {
      "title" : "From Arabic Sentiment Analysis to Sarcasm Detection: The ArSarcasm Dataset",
      "author" : [ "Ibrahim Abu Farha", "Walid Magdy." ],
      "venue" : "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language",
      "citeRegEx" : "Farha and Magdy.,? 2020",
      "shortCiteRegEx" : "Farha and Magdy.",
      "year" : 2020
    }, {
      "title" : "IDAT@FIRE2019: Overview of the Track on Irony Detection in Arabic Tweets",
      "author" : [ "Bilal Ghanem", "Jihen Karoui", "Farah Benamara", "Véronique Moriceau", "Paolo Rosso." ],
      "venue" : ". In Mehta P., Rosso P., Majumder P., Mitra M. (Eds.) Working Notes of the",
      "citeRegEx" : "Ghanem et al\\.,? 2019",
      "shortCiteRegEx" : "Ghanem et al\\.",
      "year" : 2019
    }, {
      "title" : "ALT Submission for OSACT Shared Task on Offensive Language Detection",
      "author" : [ "Sabit Hassan", "Younes Samih", "Hamdy Mubarak", "Ahmed Abdelali", "Ammar Rashed", "Shammur Absar Chowdhury." ],
      "venue" : "Proceedings of the 4th Workshop on Open-Source",
      "citeRegEx" : "Hassan et al\\.,? 2020",
      "shortCiteRegEx" : "Hassan et al\\.",
      "year" : 2020
    }, {
      "title" : "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation",
      "author" : [ "Junjie Hu", "Sebastian Ruder", "Aditya Siddhant", "Graham Neubig", "Orhan Firat", "Melvin Johnson." ],
      "venue" : "Proceedings of the 37th International",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "OSACT4 Shared Task on Offensive Language Detection: Intensive Preprocessing-Based Approach",
      "author" : [ "Fatemah Husain." ],
      "venue" : "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offen-",
      "citeRegEx" : "Husain.,? 2020",
      "shortCiteRegEx" : "Husain.",
      "year" : 2020
    }, {
      "title" : "Self-training pre-trained language models for zero- and few-shot multi-dialectal Arabic sequence labeling",
      "author" : [ "Muhammad Khalifa", "Muhammad Abdul-Mageed", "Khaled Shaalan." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the As-",
      "citeRegEx" : "Khalifa et al\\.,? 2021",
      "shortCiteRegEx" : "Khalifa et al\\.",
      "year" : 2021
    }, {
      "title" : "Character convolutions for Arabic Named Entity Recognition with Long Short-Term Memory Networks",
      "author" : [ "Muhammad Khalifa", "Khaled Shaalan." ],
      "venue" : "Computer Speech & Language, 58:335–346.",
      "citeRegEx" : "Khalifa and Shaalan.,? 2019",
      "shortCiteRegEx" : "Khalifa and Shaalan.",
      "year" : 2019
    }, {
      "title" : "SemEval-2016 Task 7: Determining Sentiment Intensity of English and Arabic Phrases",
      "author" : [ "Svetlana Kiritchenko", "Saif Mohammad", "Mohammad Salameh." ],
      "venue" : "Proceedings of the 10th international workshop on semantic evaluation (SEMEVAL-2016),",
      "citeRegEx" : "Kiritchenko et al\\.,? 2016",
      "shortCiteRegEx" : "Kiritchenko et al\\.",
      "year" : 2016
    }, {
      "title" : "An Empirical Study of Pre-trained Transformers for Arabic Information Extraction",
      "author" : [ "Wuwei Lan", "Yang Chen", "Wei Xu", "Alan Ritter." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4727–",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "Albert: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "arXiv preprint arXiv:1909.11942.",
      "citeRegEx" : "Lan et al\\.,? 2019",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2019
    }, {
      "title" : "FlauBERT: Unsupervised Language Model Pre-training for French",
      "author" : [ "Hang Le", "Loı̈c Vial", "Jibril Frej", "Vincent Segonne", "Maximin Coavoux", "Benjamin Lecouteux", "Alexandre Allauzen", "Benoit Crabbé", "Laurent Besacier", "Didier Schwab" ],
      "venue" : null,
      "citeRegEx" : "Le et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2020
    }, {
      "title" : "MLQA: Evaluating Cross-lingual Extractive Question Answering",
      "author" : [ "Patrick Lewis", "Barlas Oğuz", "Ruty Rinott", "Sebastian Riedel", "Holger Schwenk." ],
      "venue" : "pages 7315–7330.",
      "citeRegEx" : "Lewis et al\\.,? 2020",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Incorporating Word and Subword Units in Unsupervised Machine",
      "author" : [ "Zihan Liu", "Yan Xu", "Genta Indra Winata", "Pascale Fung." ],
      "venue" : "Translation Using Language Model Rescoring.",
      "citeRegEx" : "Liu et al\\.,? 2019b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Playing with Words at the National Library of Sweden–Making a Swedish BERT",
      "author" : [ "Martin Malmsten", "Love Börjeson", "Chris Haffenden." ],
      "venue" : "arXiv preprint arXiv:2007.01658.",
      "citeRegEx" : "Malmsten et al\\.,? 2020",
      "shortCiteRegEx" : "Malmsten et al\\.",
      "year" : 2020
    }, {
      "title" : "CamemBERT: a Tasty French Language Model",
      "author" : [ "Louis Martin", "Benjamin Muller", "Pedro Javier Ortiz Suárez", "Yoann Dupont", "Laurent Romary", "Éric de la Clergerie", "Djamé Seddah", "Benoı̂t Sagot" ],
      "venue" : "In Proceedings of the 58th Annual Meet-",
      "citeRegEx" : "Martin et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Martin et al\\.",
      "year" : 2020
    }, {
      "title" : "The Natural Language Decathlon: Multitask Learning as Question Answering",
      "author" : [ "Bryan McCann", "Nitish Shirish Keskar", "Caiming Xiong", "Richard Socher." ],
      "venue" : "arXiv preprint arXiv:1806.08730.",
      "citeRegEx" : "McCann et al\\.,? 2018",
      "shortCiteRegEx" : "McCann et al\\.",
      "year" : 2018
    }, {
      "title" : "Tides extraction (ACE) 2003 multilingual training data",
      "author" : [ "Alexis Mitchell", "Stephanie Strassel", "Mark Przybocki", "J Davis", "George Doddington", "Ralph Grishman", "B Sundheim." ],
      "venue" : "Linguistic Data Consortium, Philadelphia Web Download.",
      "citeRegEx" : "Mitchell et al\\.,? 2004",
      "shortCiteRegEx" : "Mitchell et al\\.",
      "year" : 2004
    }, {
      "title" : "Semeval-2018 Task 1: Affect in Tweets",
      "author" : [ "S. Bravo-Marquez Mohammad", "M.F. Salameh", "S. Kiritchenko." ],
      "venue" : "Proceedings of International Workshop on Semantic Evaluation (SemEval-2018). Association for Computational Linguistics.",
      "citeRegEx" : "Mohammad et al\\.,? 2018",
      "shortCiteRegEx" : "Mohammad et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural Arabic Question Answering",
      "author" : [ "Hussein Mozannar", "Karl El Hajal", "Elie Maamary", "Hazem Hajj." ],
      "venue" : "Proceedings of the Fourth Arabic Natural Language Processing Workshop, Florence, Italy. Association for Computational Linguistics.",
      "citeRegEx" : "Mozannar et al\\.,? 2019",
      "shortCiteRegEx" : "Mozannar et al\\.",
      "year" : 2019
    }, {
      "title" : "Overview of OSACT4 Arabic Offensive Language Detection Shared Task",
      "author" : [ "Hamdy Mubarak", "Kareem Darwish", "Walid Magdy", "Tamer Elsayed", "Hend Al-Khalifa." ],
      "venue" : "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Pro-",
      "citeRegEx" : "Mubarak et al\\.,? 2020",
      "shortCiteRegEx" : "Mubarak et al\\.",
      "year" : 2020
    }, {
      "title" : "Astd: Arabic sentiment tweets dataset",
      "author" : [ "Mahmoud Nabil", "Mohamed Aly", "Amir F Atiya." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2515–2519.",
      "citeRegEx" : "Nabil et al\\.,? 2015",
      "shortCiteRegEx" : "Nabil et al\\.",
      "year" : 2015
    }, {
      "title" : "PhoBERT: Pre-trained language models for Vietnamese",
      "author" : [ "Dat Quoc Nguyen", "Anh Tuan Nguyen." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1037–1042, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Nguyen and Nguyen.,? 2020",
      "shortCiteRegEx" : "Nguyen and Nguyen.",
      "year" : 2020
    }, {
      "title" : "CAMeL Tools: An Open Source Python Toolkit for Arabic Natural Language Processing",
      "author" : [ "Ossama Obeid", "Nasser Zalmout", "Salam Khalifa", "Dima Taji", "Mai Oudah", "Bashar Alhafni", "Go Inoue", "Fadhl Eryani", "Alexander Erdmann", "Nizar Habash." ],
      "venue" : "In",
      "citeRegEx" : "Obeid et al\\.,? 2020",
      "shortCiteRegEx" : "Obeid et al\\.",
      "year" : 2020
    }, {
      "title" : "WikiBERT models: deep transfer learning for many languages",
      "author" : [ "Sampo Pyysalo", "Jenna Kanerva", "Antti Virtanen", "Filip Ginter." ],
      "venue" : "arXiv preprint arXiv:2006.01538.",
      "citeRegEx" : "Pyysalo et al\\.,? 2020",
      "shortCiteRegEx" : "Pyysalo et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring the Limits of Transfer Learning with a Unified Text-toText Transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "Journal of Machine Learning Re-",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "SQuAD: 100000+ Questions for Machine Comprehension of Text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Austin, Texas. Associa-",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "A Primer in BERTology: What we know about how BERT works",
      "author" : [ "Anna Rogers", "Olga Kovaleva", "Anna Rumshisky." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:842–866.",
      "citeRegEx" : "Rogers et al\\.,? 2020",
      "shortCiteRegEx" : "Rogers et al\\.",
      "year" : 2020
    }, {
      "title" : "SemEval-2017 task 4: Sentiment analysis in Twitter",
      "author" : [ "Sara Rosenthal", "Noura Farra", "Preslav Nakov." ],
      "venue" : "Proceedings of the 11th international workshop on semantic evaluation (SemEval-2017), pages 502– 518.",
      "citeRegEx" : "Rosenthal et al\\.,? 2017",
      "shortCiteRegEx" : "Rosenthal et al\\.",
      "year" : 2017
    }, {
      "title" : "OSAC: Open Source Arabic Corpora",
      "author" : [ "Motaz K Saad", "Wesam M Ashour." ],
      "venue" : "6th ArchEng Int. Symposiums, EEECS, volume 10.",
      "citeRegEx" : "Saad and Ashour.,? 2010",
      "shortCiteRegEx" : "Saad and Ashour.",
      "year" : 2010
    }, {
      "title" : "KUISAIL at SemEval-2020 task 12: BERTCNN for offensive speech identification in social media",
      "author" : [ "Ali Safaya", "Moutasem Abdullatif", "Deniz Yuret." ],
      "venue" : "Proceedings of the Fourteenth Workshop on Semantic Evaluation, pages 2054–2059, Barcelona",
      "citeRegEx" : "Safaya et al\\.,? 2020",
      "shortCiteRegEx" : "Safaya et al\\.",
      "year" : 2020
    }, {
      "title" : "Sentiment after Translation: A",
      "author" : [ "Mohammad Salameh", "Saif Mohammad", "Svetlana Kiritchenko" ],
      "venue" : null,
      "citeRegEx" : "Salameh et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Salameh et al\\.",
      "year" : 2015
    }, {
      "title" : "Japanese and Korean Voice Search",
      "author" : [ "Mike Schuster", "Kaisuke Nakajima." ],
      "venue" : "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5149–5152. IEEE.",
      "citeRegEx" : "Schuster and Nakajima.,? 2012",
      "shortCiteRegEx" : "Schuster and Nakajima.",
      "year" : 2012
    }, {
      "title" : "Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructure",
      "author" : [ "Pedro Javier Ortiz Suárez", "Benoı̂t Sagot", "Laurent Romary" ],
      "venue" : "In 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)",
      "citeRegEx" : "Suárez et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Suárez et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi-Dialect Arabic BERT for Country-Level Dialect Identification",
      "author" : [ "Bashar Talafha", "Mohammad Ali", "Muhy Eddin Za’ter", "Haitham Seelawi", "Ibraheem Tuffaha", "Mostafa Samir", "Wael Farhan", "Hussein T Al-Natsheh" ],
      "venue" : "In Proceedings of the Fifth Ara-",
      "citeRegEx" : "Talafha et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Talafha et al\\.",
      "year" : 2020
    }, {
      "title" : "Multilingual is not enough: BERT for Finnish",
      "author" : [ "Antti Virtanen", "Jenna Kanerva", "Rami Ilo", "Jouni Luoma", "Juhani Luotolahti", "Tapio Salakoski", "Filip Ginter", "Sampo Pyysalo." ],
      "venue" : "arXiv preprint arXiv:1912.07076.",
      "citeRegEx" : "Virtanen et al\\.,? 2019",
      "shortCiteRegEx" : "Virtanen et al\\.",
      "year" : 2019
    }, {
      "title" : "BERTje: A Dutch BERT Model",
      "author" : [ "Wietse de Vries", "Andreas van Cranenburgh", "Arianna Bisazza", "Tommaso Caselli", "Gertjan van Noord", "Malvina Nissim." ],
      "venue" : "arXiv preprint arXiv:1912.09582.",
      "citeRegEx" : "Vries et al\\.,? 2019",
      "shortCiteRegEx" : "Vries et al\\.",
      "year" : 2019
    }, {
      "title" : "Superglue: A stickier benchmark for general-purpose language understanding systems",
      "author" : [ "Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R Bowman." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Glue: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: An-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "mt5: A massively multilingual pre-trained text-to-text transformer",
      "author" : [ "Barua", "Colin Raffel." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Barua and Raffel.,? 2021",
      "shortCiteRegEx" : "Barua and Raffel.",
      "year" : 2021
    }, {
      "title" : "Arap-Tweet: A Large Multi-Dialect Twitter Corpus for Gender, Age and Language Variety Identification",
      "author" : [ "Wajdi Zaghouani", "Anis Charfi." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC",
      "citeRegEx" : "Zaghouani and Charfi.,? 2018",
      "shortCiteRegEx" : "Zaghouani and Charfi.",
      "year" : 2018
    }, {
      "title" : "Arabic Dialect Identification",
      "author" : [ "Omar F Zaidan", "Chris Callison-Burch." ],
      "venue" : "Computational Linguistics, 40(1):171–202.",
      "citeRegEx" : "Zaidan and Callison.Burch.,? 2014",
      "shortCiteRegEx" : "Zaidan and Callison.Burch.",
      "year" : 2014
    }, {
      "title" : "OSIAN: Open Source International Arabic News Corpus - Preparation and Integration into the CLARIN-infrastructure",
      "author" : [ "Imad Zeroual", "Dirk Goldhahn", "Thomas Eckart", "Abdelhak Lakhouaja." ],
      "venue" : "Proceedings of the Fourth Arabic Natural Language",
      "citeRegEx" : "Zeroual et al\\.,? 2019",
      "shortCiteRegEx" : "Zeroual et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi-task bidirectional transformer representations for irony detection",
      "author" : [ "Chiyu Zhang", "Muhammad Abdul-Mageed." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Zhang and Abdul.Mageed.,? 2019a",
      "shortCiteRegEx" : "Zhang and Abdul.Mageed.",
      "year" : 2019
    }, {
      "title" : "No Army, No Navy: BERT Semi-Supervised Learning of Arabic Dialects",
      "author" : [ "Chiyu Zhang", "Muhammad Abdul-Mageed." ],
      "venue" : "Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 279–284, Florence, Italy. Association",
      "citeRegEx" : "Zhang and Abdul.Mageed.,? 2019b",
      "shortCiteRegEx" : "Zhang and Abdul.Mageed.",
      "year" : 2019
    }, {
      "title" : "2020b) collect 15 datasets in both MSA and dialects from Abdul-Mageed and Diab (2012) (AWATIF)",
      "author" : [ "AraNETSent. Abdul-Mageed" ],
      "venue" : "Abdul-Mageed et al",
      "citeRegEx" : "Abdul.Mageed,? \\Q2014\\E",
      "shortCiteRegEx" : "Abdul.Mageed",
      "year" : 2014
    }, {
      "title" : "The Arabic Sentiment Twitter Dataset for LEVantine dialect (ArSenDLev) (Baly et al., 2019) has 4, 000 tweets retrieved from the Levant region",
      "author" : [ "• ArSenD-Lev" ],
      "venue" : null,
      "citeRegEx" : "ArSenD.Lev.,? \\Q2019\\E",
      "shortCiteRegEx" : "ArSenD.Lev.",
      "year" : 2019
    }, {
      "title" : "2020) merge training and development data from ArSAS (Elmadany et al., 2018)",
      "author" : [ "CAMelSent. Obeid" ],
      "venue" : "ASTD (Nabil et al.,",
      "citeRegEx" : "Obeid,? \\Q2015\\E",
      "shortCiteRegEx" : "Obeid",
      "year" : 2015
    }, {
      "title" : "The Large Arabic Book Review Corpus (Aly and Atiya, 2013) has 63, 257 book reviews from Goodreads,18 each rated with a 1-5 stars scale",
      "author" : [ "• LABR" ],
      "venue" : null,
      "citeRegEx" : "LABR.,? \\Q2013\\E",
      "shortCiteRegEx" : "LABR.",
      "year" : 2013
    }, {
      "title" : "We compare to best results reported by the authors on five SA datasets: HARD, balanced ASTD (which we refer to as ASTD-B), ArSenTD-Lev, AJGT",
      "author" : [ "STOA: • Antoun" ],
      "venue" : "For SA,",
      "citeRegEx" : "Antoun,? \\Q2020\\E",
      "shortCiteRegEx" : "Antoun",
      "year" : 2020
    }, {
      "title" : "They fine-tune mBERT and AraBERT on the merged CAMelsent 18www.goodreads.com",
      "author" : [ "Obeid" ],
      "venue" : "19For ease of reference,",
      "citeRegEx" : "Obeid,? \\Q2020\\E",
      "shortCiteRegEx" : "Obeid",
      "year" : 2020
    }, {
      "title" : "2020) who rank second in the shared task with a fine-tuned AraBERT",
      "author" : [ "Djandji" ],
      "venue" : null,
      "citeRegEx" : "Djandji,? \\Q2020\\E",
      "shortCiteRegEx" : "Djandji",
      "year" : 2020
    }, {
      "title" : "Sub-task 2 of the MADAR shared task (Bouamor et al., 2019)22 is focused on user-level dialect identification with manuallycurated country labels (n=21)",
      "author" : [ "• MADAR" ],
      "venue" : null,
      "citeRegEx" : "MADAR.,? \\Q2019\\E",
      "shortCiteRegEx" : "MADAR.",
      "year" : 2019
    }, {
      "title" : "2019b) developed the top ranked system in MADAR subtask 2, with 48.76 accuracy",
      "author" : [ "F1. • Zhang", "Abdul-Mageed" ],
      "venue" : null,
      "citeRegEx" : "Zhang and Abdul.Mageed,? \\Q2019\\E",
      "shortCiteRegEx" : "Zhang and Abdul.Mageed",
      "year" : 2019
    }, {
      "title" : "2020) developed NADI subtask 1 (country",
      "author" : [ "• Talafha" ],
      "venue" : null,
      "citeRegEx" : "Talafha,? \\Q2020\\E",
      "shortCiteRegEx" : "Talafha",
      "year" : 2020
    }, {
      "title" : "2020) developed NADI subtask 2 (province level) winning system",
      "author" : [ "• El Mekki" ],
      "venue" : null,
      "citeRegEx" : "Mekki,? \\Q2020\\E",
      "shortCiteRegEx" : "Mekki",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 35,
      "context" : ", 2019) and RoBERTa (Liu et al., 2019a) have recently emerged as powerful transfer learning tools that help improve a very wide range of natural language processing (NLP) tasks.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 61,
      "context" : ", 2018), GLUE (Wang et al., 2018), SuperGLUE (Wang et al.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 60,
      "context" : ", 2018), SuperGLUE (Wang et al., 2019)) or use machine translation in their training splits (e.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 14,
      "context" : "In the context of our work, we also show how the currently best-performing model dedicated to Arabic, AraBERT (Antoun et al., 2020), suffers from a number of issues.",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 14,
      "context" : "The most notable among these is AraBERT (Antoun et al., 2020), which is trained with the same architecture as BERT (Devlin et al.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 65,
      "context" : "AraBERT is trained on 23GB of Arabic text, making ∼ 70M sentences and 3B words, from Arabic Wikipedia, the Open Source International dataset (OSIAN) (Zeroual et al., 2019) (3.",
      "startOffset" : 149,
      "endOffset" : 171
    }, {
      "referenceID" : 22,
      "context" : "These are (1) sentiment analysis from six different datasets: HARD (Elnagar et al., 2018), ASTD (Nabil et al.",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 44,
      "context" : ", 2018), ASTD (Nabil et al., 2015), ArsenTDLev (Baly et al.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 13,
      "context" : ", 2019), LABR (Aly and Atiya, 2013), and ArSaS (Elmadany et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 21,
      "context" : ", 2019), LABR (Aly and Atiya, 2013), and ArSaS (Elmadany et al., 2018).",
      "startOffset" : 47,
      "endOffset" : 70
    }, {
      "referenceID" : 42,
      "context" : "(2) NER, with the ANERcorp (Benajiba and Rosso, 2007), and (3) Arabic QA, on Arabic-SQuAD and ARCD (Mozannar et al., 2019) datasets.",
      "startOffset" : 99,
      "endOffset" : 122
    }, {
      "referenceID" : 53,
      "context" : "Another Arabic LM that was also introduced is ArabicBERT (Safaya et al., 2020), which is similarly based on BERT architecture.",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 31,
      "context" : "GigaBERT (Lan et al., 2020), an Arabic and English LM designed with code-switching data in mind, was also introduced.",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 56,
      "context" : "This is the MSA and Egyptian Arabic portion of the Open Super-large Crawled Almanach coRpus (Suárez et al., 2019),5 a huge multilingual subset from Common Crawl6 obtained using language identification and filtering.",
      "startOffset" : 92,
      "endOffset" : 113
    }, {
      "referenceID" : 65,
      "context" : "The Open Source International Arabic News Corpus (OSIAN) (Zeroual et al., 2019) consists of 3.",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 55,
      "context" : "We do not perform any further preprocessing of the data before splitting the text off to wordPieces (Schuster and Nakajima, 2012).",
      "startOffset" : 100,
      "endOffset" : 129
    }, {
      "referenceID" : 35,
      "context" : "(11)It was also shown that NSP is not crucial for model performance (Liu et al., 2019a).",
      "startOffset" : 68,
      "endOffset" : 87
    }, {
      "referenceID" : 14,
      "context" : ", 2020) (base and large), and AraBERT (Antoun et al., 2020) in terms of training data size, vocabulary size, and overall model capacity as we summarize in Table 2.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 11,
      "context" : "In total, we have the following 17 MSA and DA datasets: AJGT (Alomari et al., 2017), AraNETSent (Abdul-Mageed et al.",
      "startOffset" : 61,
      "endOffset" : 83
    }, {
      "referenceID" : 7,
      "context" : ", 2017), AraNETSent (Abdul-Mageed et al., 2020b), AraSenTi-Tweet (Al-Twairesh et al.",
      "startOffset" : 20,
      "endOffset" : 48
    }, {
      "referenceID" : 9,
      "context" : ", 2020b), AraSenTi-Tweet (Al-Twairesh et al., 2017), ArSarcasmSent (Farha and Magdy, 2020), ArSAS (Elmadany et al.",
      "startOffset" : 25,
      "endOffset" : 51
    }, {
      "referenceID" : 23,
      "context" : ", 2017), ArSarcasmSent (Farha and Magdy, 2020), ArSAS (Elmadany et al.",
      "startOffset" : 23,
      "endOffset" : 46
    }, {
      "referenceID" : 21,
      "context" : ", 2017), ArSarcasmSent (Farha and Magdy, 2020), ArSAS (Elmadany et al., 2018), ArSenDLev (Baly et al.",
      "startOffset" : 54,
      "endOffset" : 77
    }, {
      "referenceID" : 44,
      "context" : ", 2019), ASTD (Nabil et al., 2015), AWATIF (Abdul-Mageed and Diab, 2012), BBNS & SYTS (Salameh et al.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 3,
      "context" : ", 2015), AWATIF (Abdul-Mageed and Diab, 2012), BBNS & SYTS (Salameh et al.",
      "startOffset" : 16,
      "endOffset" : 45
    }, {
      "referenceID" : 54,
      "context" : ", 2015), AWATIF (Abdul-Mageed and Diab, 2012), BBNS & SYTS (Salameh et al., 2015), CAMelSent (Obeid et al.",
      "startOffset" : 59,
      "endOffset" : 81
    }, {
      "referenceID" : 46,
      "context" : ", 2015), CAMelSent (Obeid et al., 2020), HARD (Elnagar et al.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 22,
      "context" : ", 2020), HARD (Elnagar et al., 2018), LABR (Aly and Atiya, 2013), TwitterAbdullah (Abdulla et al.",
      "startOffset" : 14,
      "endOffset" : 36
    }, {
      "referenceID" : 13,
      "context" : ", 2018), LABR (Aly and Atiya, 2013), TwitterAbdullah (Abdulla et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 8,
      "context" : ", 2018), LABR (Aly and Atiya, 2013), TwitterAbdullah (Abdulla et al., 2013), TwitterSaad,(14) and SemEval2017 (Rosenthal et al.",
      "startOffset" : 53,
      "endOffset" : 75
    }, {
      "referenceID" : 51,
      "context" : ", 2013), TwitterSaad,(14) and SemEval2017 (Rosenthal et al., 2017).",
      "startOffset" : 42,
      "endOffset" : 66
    }, {
      "referenceID" : 12,
      "context" : "We use AraDan (Alshehri et al., 2020) for dangerous speech.",
      "startOffset" : 14,
      "endOffset" : 37
    }, {
      "referenceID" : 24,
      "context" : ", 2020b), IDAT@FIRE2019 (Ghanem et al., 2019), and ArSarcasm (Farha and Magdy, 2020) for emotion, irony and sarcasm, respectively.",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 23,
      "context" : ", 2019), and ArSarcasm (Farha and Magdy, 2020) for emotion, irony and sarcasm, respectively.",
      "startOffset" : 23,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "), Khaleej (Abbas et al., 2011), and OSAC (Saad and Ashour, 2010).",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 64,
      "context" : "We fine-tune our models on the following datasets: Arabic Online Commentary (AOC) (Zaidan and Callison-Burch, 2014), ArSarcasmDia (Farha and Magdy, 2020),15 MADAR (sub-task 2) (Bouamor et al.",
      "startOffset" : 82,
      "endOffset" : 115
    }, {
      "referenceID" : 23,
      "context" : "We fine-tune our models on the following datasets: Arabic Online Commentary (AOC) (Zaidan and Callison-Burch, 2014), ArSarcasmDia (Farha and Magdy, 2020),15 MADAR (sub-task 2) (Bouamor et al.",
      "startOffset" : 130,
      "endOffset" : 153
    }, {
      "referenceID" : 40,
      "context" : "We use ACE03NW and ACE03BN (Mitchell et al., 2004), ACE04NW (Mitchell et al.",
      "startOffset" : 27,
      "endOffset" : 50
    }, {
      "referenceID" : 40,
      "context" : ", 2004), ACE04NW (Mitchell et al., 2004), ANERcorp (Benajiba and Rosso, 2007), and TW-NER (Darwish, 2013).",
      "startOffset" : 17,
      "endOffset" : 40
    }, {
      "referenceID" : 29,
      "context" : "We note that even though SOTA (Khalifa and Shaalan, 2019) employ a complex combination of CNNs and character-level LSTMs, which may explain their better results on two datasets, MARBERT still achieves highest performance on the social media dataset (TW-NER).",
      "startOffset" : 30,
      "endOffset" : 57
    }, {
      "referenceID" : 42,
      "context" : "We use ARCD (Mozannar et al., 2019) and the three human translated Arabic test sections of the XTREME benchmark (Hu et al.",
      "startOffset" : 12,
      "endOffset" : 35
    }, {
      "referenceID" : 26,
      "context" : ", 2019) and the three human translated Arabic test sections of the XTREME benchmark (Hu et al., 2020): MLQA (Lewis et al.",
      "startOffset" : 84,
      "endOffset" : 101
    }, {
      "referenceID" : 34,
      "context" : ", 2020): MLQA (Lewis et al., 2020), XQuAD (Artetxe et al.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 42,
      "context" : "We follow the same splits they used where we fine-tune on Arabic SQuAD (Mozannar et al., 2019) and 50% of ARCD and test on the remaining 50% of ARCD (ARCD-test).",
      "startOffset" : 71,
      "endOffset" : 94
    }, {
      "referenceID" : 26,
      "context" : "For all other experiments, we fine-tune on the Arabic machine translated SQuAD (AR-XTREME) from the XTREME multilingual benchmark (Hu et al., 2020) and test on the human translated test sets listed above.",
      "startOffset" : 130,
      "endOffset" : 147
    }, {
      "referenceID" : 26,
      "context" : "(2020), while rest of rows report models trained with AR-XTREME (Hu et al., 2020).",
      "startOffset" : 64,
      "endOffset" : 81
    }, {
      "referenceID" : 36,
      "context" : ", 2019) and RoBERTa (Liu et al., 2019b) have revolutionized NLP.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 32,
      "context" : "Other models with different objectives and/or architectures such as ALBERT (Lan et al., 2019), T5 (Raffel et al.",
      "startOffset" : 75,
      "endOffset" : 93
    }, {
      "referenceID" : 48,
      "context" : ", 2019), T5 (Raffel et al., 2020) and its multilingual version, mT5 (Xue et al.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 14,
      "context" : "These include AraBERT (Antoun et al., 2020) and ArabicBERT (Safaya et al.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 53,
      "context" : ", 2020) and ArabicBERT (Safaya et al., 2020) for Arabic, Bertje for Dutch (de Vries et al.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 38,
      "context" : ", 2019), CamemBERT (Martin et al., 2020) and FlauBERT (Le et al.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 33,
      "context" : ", 2020) and FlauBERT (Le et al., 2020) for French, PhoBERT for Vietnamese (Nguyen and Tuan Nguyen, 2020), and the models presented by Virtanen et al.",
      "startOffset" : 21,
      "endOffset" : 38
    } ],
    "year" : 2021,
    "abstractText" : "Pre-trained language models (LMs) are currently integral to many natural language processing systems. Although multilingual LMs were also introduced to serve many languages, these have limitations such as being costly at inference time and the size and diversity of non-English data involved in their pre-training. We remedy these issues for a collection of diverse Arabic varieties by introducing two powerful deep bidirectional transformer-based models, ARBERT and MARBERT. To evaluate our models, we also introduce ARLUE, a new benchmark for multi-dialectal Arabic language understanding evaluation. ARLUE is built using 42 datasets targeting six different task clusters, allowing us to offer a series of standardized experiments under rich conditions. When fine-tuned on ARLUE, our models collectively achieve new state-of-theart results across the majority of tasks (37 out of 48 classification tasks, on the 42 datasets). Our best model acquires the highest ARLUE score (77.40) across all six task clusters, outperforming all other models including XLMRLarge (∼ 3.4× larger size). Our models are publicly available at https://github.com/UBCNLP/marbert and ARLUE will be released through the same repository.",
    "creator" : "LaTeX with hyperref"
  }
}