{
  "name" : "2021.acl-long.375.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "StereoRel: Relational Triple Extraction from a Stereoscopic Perspective",
    "authors" : [ "Xuetao Tian", "Liping Jing", "Lu He", "Feng Liu" ],
    "emails" : [ "fliu}@bjtu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4851–4861\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4851"
    }, {
      "heading" : "1 Introduction",
      "text" : "Relational triple is a common structural representation of semantic facts. A triple is always in form of (subject, relation, object), where subject and object are two entities connected by a type of predefined semantic relation. Relational triple extraction from unstructured texts is critical to understanding massive text corpora and constructing largescale knowledge graph (Ren et al., 2017; Wei et al., 2020), which is widely concerned in recent years.\nEarly researches (Zhou et al., 2005; Chan and Roth, 2011; Zhang et al., 2017) first recognize entities and predict the relations for each entity pair. Such approaches suffer from error propagation problem and thus recent researches (Zheng et al., 2017; Zeng et al., 2018; Fu et al., 2019;\n∗Corresponding author: Liping Jing.\nNayak and Ng, 2020; Wei et al., 2020; Liu et al., 2020) try to build a jointly-decoding schema for entities and relations. However, relational triple extraction still faces the following challenging issues:\n• Information loss (I-IL). Information loss includes entity incompleteness (Zeng et al., 2020) and entity overlapping (Zeng et al., 2018; Wei et al., 2020). Entity incompleteness (I-IL-EI) refers to that only head or tail token rather than completed entity is recognized, while entity overlapping (I-IL-EO) is that one entity belonging to multiple triples cannot be marked.\n• Error propagation (I-EP). Error propagation comes from the prediction process with strict order. For examples, pipeline models (Zhang et al., 2017; Takanobu et al., 2019) recognize entities first and predict relations based on each specific entity pair. Generative models (Zeng et al., 2018, 2019) extract subject, object and relation with a predetermined order.\n• Ignoring the interaction between entity and relation (I-II). Subjects (or objects) in different predefined relations should have different recognition patterns, which are not modelled when ignoring the interaction between entity and relation.\nTo intuitively explore the above issues and address them, from a stereoscopic perspective, we map the relational triples of a text to a threedimensional (3-D) space, which is like a cube as Figure 1. The relational triples are actually some small cubes in the whole cube. Existing researches are actually to model the cube from different perspectives and further extract the triples. Based on the representation of triples in 3-D space, three operations (i.e. slice, projection and shrinkage)\nare defined as Figure 2, to understand why existing methods suffer from the above issues. Furthermore, we propose a novel model for relational triple extraction, which can simultaneously handle the above issues, named StereoRel. More precisely, the cube is modelled from three perspectives, including (x, z)-plane projection, (y, z)-plane projection and z-slices, which indicates the subjects, objects and their correspondences for each predefined relation. Correspondingly, the proposed method leverages three decoders to extract relational triples in a unified model.\nThis work has the following main contributions:\n• We provide a revealing insight into relational triple extraction from a stereoscopic perspective, where the occurrence of several challenging issues and shortcomings of existing methods are rationalized.\n• We propose a novel StereoRel model for relational triple extraction, which can simultaneously reduce information loss, avoid error propagation and not ignore the interaction between entity and relation.\n• Extensive experiments are conducted on five public datasets, demonstrating that the proposed model outperforms the recent advanced baselines."
    }, {
      "heading" : "2 Relational Triple Extraction from 3-D Perspective",
      "text" : "In form of (subject, relation, object), triples can naturally be mapped to a three-dimensional (3-D) space, which is elaborated in this section. Meanwhile, we define three operations (i.e. slice, projection and shrinkage) in 3-D space, to make it easy to understand the strengths and shortcomings of previous researches."
    }, {
      "heading" : "2.1 Triple Representation in 3-D Space",
      "text" : "Given a text L with length being |L| and a predefined relation set R having |R| relations, L may have several triples, that is, p([s, r, o]|L) where [·] represents a collection. Each triple consists of a subject (s), an object (o) and one relation (r) belonging to R. Subject is one entity, that is, ngram in L and so does object. To model p([s]|L) or p([o]|L), the common strategy is to leverage sequence tagging on L, which has some existing strategies, such as BMES tagging (Zhang and Yang, 2018; Li et al., 2020) and start-and-end binary tagging (Wei et al., 2020; Sui et al., 2020). Anyway, there is a tag set T , and thus p([s]|L) or p([o]|L) can be represented by a vector with length being |L| × |T |. Meanwhile, due to r ∈ R, modeling p([r]|L) can be taken as a classification task, which requires a vector with length being |R| to represent. Therefore, when modeling p([s, r, o]|L) by consid-\nering all possible connections, it should be equivalent to a cube with size being (|L| × |T |)2 × |R| in a 3-D space.\nAs shown in Figure 1, the line segments of the cube mapping on x-axis, y-axis and z-axis are respectively regarded as the representations of subjects, objects and relations, that is, p([s]|L), p([o]|L) and p([r]|L). Similarly, the rectangles of the cube mapping on (x, y)-plane, (x, z)-plane and (y, z)-plane are respectively regarded as p([s, o]|L), p([s, r]|L) and p([o, r]|L). Further, each triple is mapped to a small cube in the space. Based on the stereoscopic representation of relational triples, we define the following operations. Slice, denoted as sli(·). As shown in Figure 2(a), when some elements (i.e. subject, object or relation) are specified, the representation space will be reduced. The operation is like slicing the cube. For instances, a specific relation corresponds to a z-slice with size being (|L| × |T |)2 × 1. Both subject and object being specified leads to an xyslice with size being (m× |T |)× (n× |T |)× |R|, which can be seen as the intersection of an x-slice and a y-slice. If subject, object or relation are\nall specified, there is an xyz-slice with size being (m× |T |)× (n× |T |)× 1, that is, a triple. Projection, denoted as pro(·). As depicted in Figure 2(b), two types of projection are defined, cubeto-plane and plane-to-axis. The former seems to look at the whole cube from a certain plane. For example, in the projection from cube to (x, y)-plane, two triples with the same subject and object are indistinguishable. Similarly, in the projection from cube to (x, z)-plane, there is only subject and relation information but no object information. The later seems to look at a plane from a certain axis, such as (x, z)-plane to x-axis projection, where the subjects in different z-slices may have the same representation on x-axis. Hereafter, for easy reading, (x, z)-plane to x-axis and (y, z)-plane to y-axis projections are denoted as prox(·) and proy(·) respectively. The projection from cube to (x, y)plane is denoted as proxy(·). The rest ones are similar. Shrinkage, denoted as shr(·). In the cube representation, each token pair is represented by an xy-slice with size being |T | × |T | × |R|. Such an xy-slice can reflect all possible entity-tagging combinations of a token pair. As described in Figure 2(c), a shrinkage over a cube only represent whether the token pair satisfies one specific entitytagging combination, such as (start, start). Thus, the size of a shrinkage is |L| × |L| × |R|."
    }, {
      "heading" : "2.2 Analysis of Previous Researches",
      "text" : "As aforementioned, relational triple extraction faces three challenging issues: information loss (IIL-EI or I-IL-EO), error propagation (I-EP) and ignoring the interaction between entity and relation (I-II). It is clear to match them to the three operations in 3-D space as Table 1. sli(·) corresponds to the prediction process with strict order, and thus leads to error propagation. Without considering the nested entities in a text, proxz/yz/z(·) do not result in any problems, while prox/y/xy(·) is the\nopposite. Both prox/y(·) and proxy(·) lead to ignoring the interaction between entity and relation. Meanwhile, proxy(·) makes the triples with overlapped entities indistinguishable. The cube can be disassembled into |T | × |T | shr(·). Modeling only one shr(·) will cause entity incompleteness. To get deep insights on relational triple extraction, based on the correspondence between the operations and issues, we analyze previous researches as shown in Table 2.\nEarly researches (Zelenko et al., 2002; Zhou et al., 2005; Chan and Roth, 2011) adopt pipeline approaches, where the entities are recognized first and the relations for each entity pair are predicted. Arguing that such approaches neglect the inherent relevance between entity recognition and relation extraction, some solutions (Miwa and Bansal, 2016; Zhang et al., 2017; Takanobu et al., 2019) still extract entities and relations sequentially, but make two tasks share the same encoder. These methods model p([s, r, o]|L) as p([s] ∪ [o]|L) and p([si, r, oj ]|L, slixy(si, oj)),\nwhere p([s]∪[o]|L) = p(prox(proxz([s, r, o]))∪ proy(proyz([s, r, o]))|L). Therefore, pipeline paradigm suffers from I-EP and I-II issues. MHS (Bekoulis et al., 2018) is another two-stage method. The model recognizes entities firstly and extracts relational triples with a multi-head selection strategy on each subject, where prox/y(·) and slix(·) lead to I-EP and I-II issues respectively.\nIn the following researches on relational triple extraction, several methods with joint decoding schema are proposed. Specifically, NovelTagging (Zheng et al., 2017) and PA-Tagging (Dai et al., 2019) achieve joint decoding by designing a unified tagging scheme and convert relational triple extraction to an end-to-end sequence tagging problem. Such a tagging schema has to model p(proxy([s, r, o])|L) and thus suffers from I-IL-EO and I-II issues. CopyRE (Zeng et al., 2018) and CopyRRL (Zeng et al., 2019) leverage sequence-to-sequence model with copy mechanism. GraphRel (Fu et al., 2019) introduces graph convolutional network jointly learn entities\nand relations. Despite their initial success, the three methods only model p(shr([s, r, o])|L) and thus suffer from I-IL-EI issue. Sequence generation models, CopyRE and CopyRRL, predict triples one by one and model p(slixyz(si, ri, oi)) via p(sliz(ri)) × p(slixz(si, ri)|sliz(ri)) × p(slixyz(si, ri, oi)|slixz(si, ri)), which leads to I-EP issue. GraphRel cannot avoid I-IL-EO and I-II issues due to its utilizing proxy(·).\nRecently, to address I-IL issue, CopyMTL (Zeng et al., 2020) proposes a multi-task learning framework based on CopyRE, to simultaneously predict completed entities and capture relational triples. However, the model still does not solve I-EP issue. Meanwhile, entity recognition is implemented by modeling p([s] ∪ [o]|L) via a standalone module, which leads to I-II issue. Following sequence-to-sequence schema, WDec and PNDec (Nayak and Ng, 2020) design specific decoder block which can generate triples with completed entities. Such models ease I-II issue, but still suffers from I-EP issue since that it models p(slixyz(si, ri, oi)) via p(slix(si)) × p(slixy(si, oi)|slix(si)) × p(slixyz(si, ri, oi)|slixy(si, oi)). CasRel (Wei et al., 2020) regards relations as functions that map subjects to objects in a text. It is necessary to recognize subjects first and then objects, which leads to I-EP issue. To recognize subjects, p([s]|L) is modelled via p(prox(proxz([s, r, o]))|L), where prox(·) leads to I-II issue. Att-as-Rel (Liu et al., 2020) models the triples by multi-head attention, where completed entities are recognized by modeling p([s] ∪ [o]|L) separately and thus there is I-II issue. Similarly, TPLinker (Wang et al., 2020b) regards joint extraction as a token pair linking problem, where entity recognition is also modelled separately via p([s] ∪ [o]|L)."
    }, {
      "heading" : "3 The Proposed StereoRel Model",
      "text" : "To handle the above three issues simultaneously, we avoid to make the operations in Table 1 and try to model p([s, r, o]|L) via ∑|R| i [p([s, ri]|L) +p([ri, o]|L) + p(shr([s, ri, o])|L)]. As depicted in Figure 3, the proposed StereoRel model first leverages BERT encoder to extract the text representation for the original text. Then, for each predefined relation, the text representation is transformed to its subject and object spaces. Based on them, three decoders are built to separately model p(proxz([s, r, o])|L), p(proyz([s, r, o])|L) and\np(shr([s, r, o])|L). The first two will cause no issue and provide complete entities for the last shr(·) operation."
    }, {
      "heading" : "3.1 BERT Encoder",
      "text" : "To sufficiently capture the textual information, the encoder is built by a pre-trained language model, BERT (Devlin et al., 2019). BERT encoder tokenizes a text L using a predefined vocabulary and generates a corresponding sequence Ľ by concatenating a [CLS] token, the tokenized text and a [SEP] token. The detailed steps can be referred to (Devlin et al., 2019). BERT encoder will embed a text L into a matrix T ∈ R(|L|+2)×db , where db is the hidden size of BERT, and Tj can be seen as the word embedding of j-th token, j ∈ [0, |L|+ 1]. After this, for each relation ri, T is transformed to a new text representation Ti ∈ R(|L|+2)×dr by:\nTi = φ(TWi + bi), (1)\nwhere {Wi}|R|i=1 ∈ Rdb×dr , {bi} |R| i=1 ∈ R1×dr are trainable parameters and φ(·) is predetermined activation function."
    }, {
      "heading" : "3.2 Subject Decoder",
      "text" : "Subject decoder is to model ∑|R|\ni p([s, ri]|L), that is, (x, z)-plane projection p(proxz([s, r, o])|L), which recognizes the subjects for each predefined relation. For one specific relation ri, we transform its text representation to Tsubi ∈ R(|L|+2)×de in ri’s subject space with de being the hidden size. The transformation is implemented by\nTsubi = T sub−q i + T sub−k i + T sub−b i , (2)\nT sub−q/k/b i = φ(Ti W sub−q/k/b i + b sub−q/k/b i ),\n(3) where Tsub−qi , T sub−k i , T sub−b i are linear transformations on top of Ti. T sub−q i , T sub−k i will be used by shrinkage decoder, while Tsub−bi only works for subject decoder. {Wsub−qi } |R| i=1, {Wsub−ki } |R| i=1, {W sub−b i } |R| i=1 ∈ Rdr×de , {bsub−qi } |R| i=1, {b sub−k i } |R| i=1, {b sub−b i } |R| i=1 ∈ R1×de are trainable parameters and φ(·) is predetermined activation function. Based on Tsubi , all possible subjects in relation ri’s subject space are recognized by a sequential conditional random field (CRF) (Lafferty et al., 2001) layer with\n[Begin, Inside, Outside] tagging schema, where the probability of the final label sequence, ysubi = [y sub i1 , y sub i2 , ..., y sub i|L|], is modeled as follows:\nP(ysubi |L) = ∏|L| j=1 φj(y sub i(j−1), y sub ij |L)∑\ny′∈Y ∏|L| j=1 φj(y ′ j−1, y ′ j |L) , (4)\nφj(y, ŷ|L) = exp(Tsubwcrfsuby,ŷ + b crfsub y,ŷ ), (5)\nwhere Y denotes all possible label sequence of L. wcrfsuby,ŷ and b crfsub y,ŷ are trainable parameters corresponding to the label pair (y, ŷ)."
    }, {
      "heading" : "3.3 Object Decoder",
      "text" : "Object decoder is to model ∑|R|\ni p([ri, o]|L), that is, (y, z)-plane projection p(proyz([s, r, o])|L), which recognizes the objects for each predefined relation. Similar to subject decoder, the text representation in object space, Tobji ∈ R|R|×(|L|+2)×de , is obtained in object decoder as:\nTobji = T obj−q i + T obj−k i + T obj−b i , (6)\nT obj−q/k/b i = φ(Ti W obj−q/k/b i + b obj−q/k/b i ),\n(7)\nwhere Tobj−qi , T obj−k i , T obj−b i are linear transformations on top of Ti. {Wobj−qi } |R| i=1, {Wobj−ki } |R| i=1, {W obj−b i } |R| i=1 ∈ Rdr×de , {bobj−qi } |R| i=1, {b obj−k i } |R| i=1, {b obj−b i } |R| i=1 ∈ R1×de are trainable parameters. In like wise, objects of the i-th predefined relation are tagged as yobji = [y obj i1 , y obj i2 , ..., y obj i|L|] via another CRF layer as:\nP(yobji |L) = ∏|L| j=1 φj(y obj i(j−1), y obj ij |L)∑\ny′∈Y ∏|L| j=1 φj(y ′ j−1, y ′ j |L) , (8)\nφj(y, ŷ|L) = exp(Tobjw crfobj y,ŷ + b crfobj y,ŷ ), (9)\nwhere wcrfobjy,ŷ and b crfobj y,ŷ are trainable parameters."
    }, {
      "heading" : "3.4 Shrinkage Decoder",
      "text" : "To extract the correspondences between subjects and objects, shrinkage decoder is leveraged to model ∑|R| i p(shr([s, ri, o])|L), where each element of shr(·) denotes whether the corresponding token pair is one specific position of a (subject, object) pair, such as (start, start) or (end, end). To model this, a pair-wise classification function f is established as:\npshr ijj′ = f(Tsubij ,T obj ij′ ), (10)\nindicating the probability that j-token and j ′ -token is the specifc position of a (subject, object) pair, which satisfies the i-th predefined relation. We design the function as follows:\npshr ijj′ = ξ(psub→obj ijj′ , pobj→sub ijj′ ), (11)\npsub→obj ijj′ = softmaxj(ψ(T sub−q ij ,T obj−k ij′ )),\n(12)\npobj→sub ijj′ = softmaxj′ (ψ(T obj−q ij′ ,Tsub−kij )),\n(13) where ψ(·) is implemented by dot product or neural network to provide an initial probability. psub→obj\nijj′\nand pobj→sub ijj′\nrespectively indicate the probability distributions for a subject searching for its objects and an object searching for its subjects, which are integrated via a predetermined function ξ(·), such as minimum, maximum and multiplication."
    }, {
      "heading" : "3.5 Learning and Inference",
      "text" : "Subject and object decoders are learned by textlevel log-likelihood loss, while shrinkage decoder is learned by token-level binary cross-entropy loss. Thus, the unified model is learned by a combined loss function Ltotal = Lsub + Lobj + Lshr, where\nLsub = − |R|∑ i log(P(ysubi |L)), (14) Lobj = − |R|∑ i log(P(yobji |L)), (15)\nLshr = − |R|∑ i |L|∑ j |L|∑ j′ [ p̂shr ijj′ log(pshr ijj′ )+\n(1− p̂shr ijj′ ) log(1− pshr ijj′\n) ] . (16)\nThe relational triples can be inferred based on the three decoders. Concretely, for each predefined relation ri, the subjects and objects can be obtained by ysubi and y obj i respectively. For the subject sij and object oij′ with (j-th token, j ′ -th token) satisfying the specific position, if pshr ijj′\nis greater than a predetermined threshold δ, (sij , ri, oij′ ) will be extracted as a relational triple."
    }, {
      "heading" : "4 Experiments",
      "text" : "To evaluate the proposed StereoRel model, we conduct a performance comparison on five public datasets in this section."
    }, {
      "heading" : "4.1 Experimental Settings",
      "text" : "Evaluation Metrics and Datasets. Generally, the performance on relational triple extraction is evaluated by precision (Pre.), recall (Rec.) and F1-score (F1) , where a triple is regarded as correct if subject, relation and object are all matched. Notably, in previous works, there are two evaluation modes: Partial Match and Exact Match. The former holds that subject (or object) is correct as long as its head or tail is correct, while the latter requires it to be recognized completely. To properly compare our model with various baselines, benchmark datasets are selected for the two modes separately. Concretely, we utilize NYT (Riedel et al., 2010), WebNLG (Gardent et al., 2017), NYT10 (Takanobu et al., 2019) and NYT11 (Takanobu et al., 2019) datasets for Partial Match, while NYT (Riedel et al., 2010) and Wiki-KBP (Dai et al., 2019) datasets for Exact Match. The details are shown in Table 3. The splits of validation set are the same as previous researches. Implementation Details. For making a fair comparison, we utilize the cased BERT-base1 model in our experiments, which is the same as CasRel (Wei et al., 2020) and TPLinker (Wang et al., 2020b), and thus db = 768. Adam optimizer (Kingma and Ba, 2015) is utilized to train the proposed method with initial learning rate being 1e-5. The hidden size dr, de are set as 64, 32. The threshold δ is tuned for each relation and determined by the validation set. φ(·) is set as relu activation function. ψ(·) is set as dot product. ξ(·) is set as the multiplication function.\n1https://storage.googleapis.com/bert_ models/2018_10_18/cased_L-12_H-768_A-12. zip"
    }, {
      "heading" : "4.2 Performance Comparison",
      "text" : "We employ some recent advanced methods as baselines, mainly including the models analyzed in Table 2. Table 4, 5, 6 and 7 report the results of our method against the baselines for Partial Match evaluation mode, and Table 8 and 9 report the results for Exact Match. The models before CasRel do not employ BERT encoder and the rest does.\nAs aforementioned in Table 2, existing models do not handle three challenging issues simultaneously, while our proposed StereoRel model does. Among the baselines, Att-as-Rel is the first work to extract triples for each predefined relation with no I-IL and I-EP issues, and thus achieves a huge performance improvement compared with previous methods. Based on BERT encoder, the performance on relational triple extraction has been further improved by CasRel and TPLinker. Due\nto no I-EP issue, TPLinker outperforms CasRel. However, TPLinker still suffers from I-II issue. Our proposed StereoRel model further considers it and achieves a better performance. From the results, comparing with the second best baseline, the performance improvement of the existing best baseline on the five datasets were 2.5%, 0.1%, 1.4%, 0.1% and 1.5% respectively, in terms of F1-score. Our model obtains performance gain about 0.3%, 0.2%, 0.2%, 0.7% and 0.6% in terms of the best baseline. It can be seen that the improvement is satisfied."
    }, {
      "heading" : "5 Discussions and Perspectives",
      "text" : "For relational triple extraction, from the stereoscopic perspective, there are the following two aspects worthy of discussion. The first one is about learning strategy. Most of previous studies and ours employ binary cross-entropy loss to learn the models. However, since the label space of relational triple in 3-D space is huge, binary cross-entropy is available but not necessarily optimal. Meanwhile, cross-entropy is permutation-sensitive loss function (Sui et al., 2020), which is incompatible with generative models (Zeng et al., 2018, 2019, 2020) since it is necessary to predetermine extraction order of multiple triples. To this question, CGT (Ye et al., 2020) incorporates contrastive learning strategy and SPN (Sui et al., 2020) transforms relational\ntriple extraction into set prediction problem learned by bipartite matching loss. These ideas may be introduced in the future.\nThe second one is to recognize nested entities in relational triples. Nested entities are the entities among which there are substring relationships, like “U.N.” being a substring of “U.N. Ambassador”. Such entities definitely affect the overall performance on Exact Match mode. Take NYT dataset as an example, there are about 2.5% sentences containing nested entities. Nested entity recognition has been widely studied (Li et al.; Wang et al., 2020a), but most studies on relational triple extraction have not considered it. TPLinker (Wang et al., 2020b) provides a solution to recognize nested entities via a token pair tagging, but it ignores the interaction between entity and relation. For StereoRel model, although not focusing on nested entities, it has not been much affected. The reason is that StereoRel recognizes subjects and objects for each predefined relation separately. In this case, only 0.06% of nested entities in NYT cannot be marked. Anyway, modeling nested entities from the stereoscopic perspective is worth exploring in the future."
    }, {
      "heading" : "6 Conclusions",
      "text" : "Relational triple extraction is critical to understanding massive text corpora. However, existing studies\nface some challenging issues, including information loss, error propagation and ignoring the interaction between entity and relation. In this paper, aiming at simultaneously handling the above issues, we provide a revealing insight into relational triple extraction from a stereoscopic perspective, which rationalizes the occurrence of these issues and exposes the shortcomings of existing methods. Further, we propose a novel model leveraging three decoders to respectively extract subjects, objects and their correspondences for each predefined relation. Extensive experiments are conducted on five public datasets, demonstrating that the proposed model outperforms the recent advanced baselines."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported in part by the National Natural Science Foundation of China under Grant 61822601 and 61773050; the Beijing Natural Science Foundation under Grant Z180006; the Open Project Program Foundation of the Key Laboratory of Opto-Electronics Information Processing, Chinese Academy of Sciences(OEIP-O-202004); National Key Research and Development Project No. 2019YFB1405202."
    } ],
    "references" : [ {
      "title" : "Joint entity recognition and relation extraction as a multi-head selection problem",
      "author" : [ "Giannis Bekoulis", "Johannes Deleu", "Thomas Demeester", "Chris Develder." ],
      "venue" : "Expert Syst. Appl., 114:34–45.",
      "citeRegEx" : "Bekoulis et al\\.,? 2018",
      "shortCiteRegEx" : "Bekoulis et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploiting syntactico-semantic structures for relation extraction",
      "author" : [ "Yee Seng Chan", "Dan Roth." ],
      "venue" : "The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24",
      "citeRegEx" : "Chan and Roth.,? 2011",
      "shortCiteRegEx" : "Chan and Roth.",
      "year" : 2011
    }, {
      "title" : "Joint extraction of entities and overlapping relations using position-attentive sequence labeling",
      "author" : [ "Dai Dai", "Xinyan Xiao", "Yajuan Lyu", "Shan Dou", "Qiaoqiao She", "Haifeng Wang." ],
      "venue" : "The Thirty-Third AAAI Conference on Artificial Intelli-",
      "citeRegEx" : "Dai et al\\.,? 2019",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 17th Conference of the",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Graphrel: Modeling text as relational graphs for joint entity and relation extraction",
      "author" : [ "Tsu-Jui Fu", "Peng-Hsuan Li", "Wei-Yun Ma." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July",
      "citeRegEx" : "Fu et al\\.,? 2019",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2019
    }, {
      "title" : "Creating training corpora for NLG micro-planners",
      "author" : [ "Claire Gardent", "Anastasia Shimorina", "Shashi Narayan", "Laura Perez-Beltrachini." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancou-",
      "citeRegEx" : "Gardent et al\\.,? 2017",
      "shortCiteRegEx" : "Gardent et al\\.",
      "year" : 2017
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "Proceedings of the 3rd International Conference on Learning Representations.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira." ],
      "venue" : "ICML 2001, Williams College, Williamstown, MA, USA, June 28 - July 1, 2001,",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Recursively binary modification model for nested named entity recognition",
      "author" : [ "Bing Li", "Shifeng Liu", "Yifang Sun", "Wei Wang", "Xiang Zhao" ],
      "venue" : "In The ThirtyFourth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Li et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "FLAT: Chinese NER using flat-lattice transformer",
      "author" : [ "Xiaonan Li", "Hang Yan", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention as relation: Learning supervised multi-head self-attention for relation extraction",
      "author" : [ "Jie Liu", "Shaowei Chen", "Bingquan Wang", "Jiaxin Zhang", "Na Li", "Tong Xu." ],
      "venue" : "Proceedings of the TwentyNinth International Joint Conference on Artificial In-",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "End-to-end relation extraction using LSTMs on sequences and tree structures",
      "author" : [ "Makoto Miwa", "Mohit Bansal." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Vol-",
      "citeRegEx" : "Miwa and Bansal.,? 2016",
      "shortCiteRegEx" : "Miwa and Bansal.",
      "year" : 2016
    }, {
      "title" : "Effective modeling of encoder-decoder architecture for joint entity and relation extraction",
      "author" : [ "Tapas Nayak", "Hwee Tou Ng." ],
      "venue" : "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Arti-",
      "citeRegEx" : "Nayak and Ng.,? 2020",
      "shortCiteRegEx" : "Nayak and Ng.",
      "year" : 2020
    }, {
      "title" : "CoType: Joint extraction of typed entities and relations with knowledge bases",
      "author" : [ "Xiang Ren", "Zeqiu Wu", "Wenqi He", "Meng Qu", "Clare R. Voss", "Heng Ji", "Tarek F. Abdelzaher", "Jiawei Han." ],
      "venue" : "Proceedings of the 26th International Conference on World Wide",
      "citeRegEx" : "Ren et al\\.,? 2017",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2017
    }, {
      "title" : "Modeling relations and their mentions without labeled text",
      "author" : [ "Sebastian Riedel", "Limin Yao", "Andrew McCallum." ],
      "venue" : "Machine Learning and Knowledge Discovery in Databases, European Conference, ECML PKDD 2010, Barcelona, Spain, September",
      "citeRegEx" : "Riedel et al\\.,? 2010",
      "shortCiteRegEx" : "Riedel et al\\.",
      "year" : 2010
    }, {
      "title" : "Joint entity and relation extraction with set prediction networks",
      "author" : [ "Dianbo Sui", "Yubo Chen", "Kang Liu", "Jun Zhao", "Xiangrong Zeng", "Shengping Liu." ],
      "venue" : "CoRR, abs/2011.01675.",
      "citeRegEx" : "Sui et al\\.,? 2020",
      "shortCiteRegEx" : "Sui et al\\.",
      "year" : 2020
    }, {
      "title" : "A hierarchical framework for relation extraction with reinforcement learning",
      "author" : [ "Ryuichi Takanobu", "Tianyang Zhang", "Jiexi Liu", "Minlie Huang." ],
      "venue" : "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innova-",
      "citeRegEx" : "Takanobu et al\\.,? 2019",
      "shortCiteRegEx" : "Takanobu et al\\.",
      "year" : 2019
    }, {
      "title" : "Pyramid: A layered model for nested named entity recognition",
      "author" : [ "Jue Wang", "Lidan Shou", "Ke Chen", "Gang Chen." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020,",
      "citeRegEx" : "Wang et al\\.,? 2020a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Tplinker: Single-stage joint extraction of entities and relations through token pair linking",
      "author" : [ "Yucheng Wang", "Bowen Yu", "Yueyang Zhang", "Tingwen Liu", "Hongsong Zhu", "Limin Sun." ],
      "venue" : "Proceedings of the 28th International Conference on Com-",
      "citeRegEx" : "Wang et al\\.,? 2020b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "A novel cascade binary tagging framework for relational triple extraction",
      "author" : [ "Zhepei Wei", "Jianlin Su", "Yue Wang", "Yuan Tian", "Yi Chang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, On-",
      "citeRegEx" : "Wei et al\\.,? 2020",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2020
    }, {
      "title" : "Kernel methods for relation extraction",
      "author" : [ "Dmitry Zelenko", "Chinatsu Aone", "Anthony Richardella." ],
      "venue" : "Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, EMNLP 2002, Philadelphia, PA, USA, July 6-7,",
      "citeRegEx" : "Zelenko et al\\.,? 2002",
      "shortCiteRegEx" : "Zelenko et al\\.",
      "year" : 2002
    }, {
      "title" : "CopyMTL: Copy mechanism for joint extraction of entities and relations with multi-task learning",
      "author" : [ "Daojian Zeng", "Haoran Zhang", "Qianying Liu." ],
      "venue" : "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innova-",
      "citeRegEx" : "Zeng et al\\.,? 2020",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning the extraction order of multiple relational facts in a sentence with reinforcement learning",
      "author" : [ "Xiangrong Zeng", "Shizhu He", "Daojian Zeng", "Kang Liu", "Shengping Liu", "Jun Zhao." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in",
      "citeRegEx" : "Zeng et al\\.,? 2019",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2019
    }, {
      "title" : "Extracting relational facts by an end-to-end neural model with copy mechanism",
      "author" : [ "Xiangrong Zeng", "Daojian Zeng", "Shizhu He", "Kang Liu", "Jun Zhao." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018,",
      "citeRegEx" : "Zeng et al\\.,? 2018",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2018
    }, {
      "title" : "End-to-end neural relation extraction with global optimization",
      "author" : [ "Meishan Zhang", "Yue Zhang", "Guohong Fu." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, Septem-",
      "citeRegEx" : "Zhang et al\\.,? 2017",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2017
    }, {
      "title" : "Chinese NER using lattice LSTM",
      "author" : [ "Yue Zhang", "Jie Yang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 1554–1564.",
      "citeRegEx" : "Zhang and Yang.,? 2018",
      "shortCiteRegEx" : "Zhang and Yang.",
      "year" : 2018
    }, {
      "title" : "Joint extraction of entities and relations based on a novel tagging scheme",
      "author" : [ "Suncong Zheng", "Feng Wang", "Hongyun Bao", "Yuexing Hao", "Peng Zhou", "Bo Xu." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Zheng et al\\.,? 2017",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2017
    }, {
      "title" : "Exploring various knowledge in relation extraction",
      "author" : [ "Guodong Zhou", "Jian Su", "Jie Zhang", "Min Zhang." ],
      "venue" : "ACL 2005, 43rd Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, 25-30 June 2005, University",
      "citeRegEx" : "Zhou et al\\.,? 2005",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "Relational triple extraction from unstructured texts is critical to understanding massive text corpora and constructing largescale knowledge graph (Ren et al., 2017; Wei et al., 2020), which is widely concerned in recent years.",
      "startOffset" : 147,
      "endOffset" : 183
    }, {
      "referenceID" : 19,
      "context" : "Relational triple extraction from unstructured texts is critical to understanding massive text corpora and constructing largescale knowledge graph (Ren et al., 2017; Wei et al., 2020), which is widely concerned in recent years.",
      "startOffset" : 147,
      "endOffset" : 183
    }, {
      "referenceID" : 27,
      "context" : "Early researches (Zhou et al., 2005; Chan and Roth, 2011; Zhang et al., 2017) first recognize entities and predict the relations for each entity pair.",
      "startOffset" : 17,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "Early researches (Zhou et al., 2005; Chan and Roth, 2011; Zhang et al., 2017) first recognize entities and predict the relations for each entity pair.",
      "startOffset" : 17,
      "endOffset" : 77
    }, {
      "referenceID" : 24,
      "context" : "Early researches (Zhou et al., 2005; Chan and Roth, 2011; Zhang et al., 2017) first recognize entities and predict the relations for each entity pair.",
      "startOffset" : 17,
      "endOffset" : 77
    }, {
      "referenceID" : 21,
      "context" : "Information loss includes entity incompleteness (Zeng et al., 2020) and entity overlapping (Zeng et al.",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 24,
      "context" : "For examples, pipeline models (Zhang et al., 2017; Takanobu et al., 2019) recognize",
      "startOffset" : 30,
      "endOffset" : 73
    }, {
      "referenceID" : 16,
      "context" : "For examples, pipeline models (Zhang et al., 2017; Takanobu et al., 2019) recognize",
      "startOffset" : 30,
      "endOffset" : 73
    }, {
      "referenceID" : 25,
      "context" : "To model p([s]|L) or p([o]|L), the common strategy is to leverage sequence tagging on L, which has some existing strategies, such as BMES tagging (Zhang and Yang, 2018; Li et al., 2020) and start-and-end binary tagging (Wei et al.",
      "startOffset" : 146,
      "endOffset" : 185
    }, {
      "referenceID" : 8,
      "context" : "To model p([s]|L) or p([o]|L), the common strategy is to leverage sequence tagging on L, which has some existing strategies, such as BMES tagging (Zhang and Yang, 2018; Li et al., 2020) and start-and-end binary tagging (Wei et al.",
      "startOffset" : 146,
      "endOffset" : 185
    }, {
      "referenceID" : 19,
      "context" : ", 2020) and start-and-end binary tagging (Wei et al., 2020; Sui et al., 2020).",
      "startOffset" : 41,
      "endOffset" : 77
    }, {
      "referenceID" : 15,
      "context" : ", 2020) and start-and-end binary tagging (Wei et al., 2020; Sui et al., 2020).",
      "startOffset" : 41,
      "endOffset" : 77
    }, {
      "referenceID" : 20,
      "context" : "Early researches (Zelenko et al., 2002; Zhou et al., 2005; Chan and Roth, 2011) adopt pipeline approaches, where the entities are recognized first and the relations for each entity pair are predicted.",
      "startOffset" : 17,
      "endOffset" : 79
    }, {
      "referenceID" : 27,
      "context" : "Early researches (Zelenko et al., 2002; Zhou et al., 2005; Chan and Roth, 2011) adopt pipeline approaches, where the entities are recognized first and the relations for each entity pair are predicted.",
      "startOffset" : 17,
      "endOffset" : 79
    }, {
      "referenceID" : 1,
      "context" : "Early researches (Zelenko et al., 2002; Zhou et al., 2005; Chan and Roth, 2011) adopt pipeline approaches, where the entities are recognized first and the relations for each entity pair are predicted.",
      "startOffset" : 17,
      "endOffset" : 79
    }, {
      "referenceID" : 11,
      "context" : "Arguing that such approaches neglect the inherent relevance between entity recognition and relation extraction, some solutions (Miwa and Bansal, 2016; Zhang et al., 2017; Takanobu et al., 2019) still extract entities and relations sequentially, but make two tasks share the same encoder.",
      "startOffset" : 127,
      "endOffset" : 193
    }, {
      "referenceID" : 24,
      "context" : "Arguing that such approaches neglect the inherent relevance between entity recognition and relation extraction, some solutions (Miwa and Bansal, 2016; Zhang et al., 2017; Takanobu et al., 2019) still extract entities and relations sequentially, but make two tasks share the same encoder.",
      "startOffset" : 127,
      "endOffset" : 193
    }, {
      "referenceID" : 16,
      "context" : "Arguing that such approaches neglect the inherent relevance between entity recognition and relation extraction, some solutions (Miwa and Bansal, 2016; Zhang et al., 2017; Takanobu et al., 2019) still extract entities and relations sequentially, but make two tasks share the same encoder.",
      "startOffset" : 127,
      "endOffset" : 193
    }, {
      "referenceID" : 26,
      "context" : "Specifically, NovelTagging (Zheng et al., 2017) and PA-Tagging (Dai et al.",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 2,
      "context" : ", 2017) and PA-Tagging (Dai et al., 2019) achieve joint decoding by designing a unified tagging scheme and convert relational triple extraction to an end-to-end sequence tagging problem.",
      "startOffset" : 23,
      "endOffset" : 41
    }, {
      "referenceID" : 22,
      "context" : ", 2018) and CopyRRL (Zeng et al., 2019) leverage sequence-to-sequence model with copy mechanism.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 4,
      "context" : "GraphRel (Fu et al., 2019) introduces graph convolutional network jointly learn entities",
      "startOffset" : 9,
      "endOffset" : 26
    }, {
      "referenceID" : 21,
      "context" : "Recently, to address I-IL issue, CopyMTL (Zeng et al., 2020) proposes a multi-task learning framework based on CopyRE, to simultane-",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 12,
      "context" : "Following sequence-to-sequence schema, WDec and PNDec (Nayak and Ng, 2020) design specific decoder block which can generate triples with completed entities.",
      "startOffset" : 54,
      "endOffset" : 74
    }, {
      "referenceID" : 19,
      "context" : "CasRel (Wei et al., 2020) regards relations as functions that map subjects to objects in a text.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 10,
      "context" : "Att-as-Rel (Liu et al., 2020) models the triples by multi-head attention,",
      "startOffset" : 11,
      "endOffset" : 29
    }, {
      "referenceID" : 18,
      "context" : "Similarly, TPLinker (Wang et al., 2020b) regards joint extraction as a token pair linking problem, where entity recognition is also modelled separately via p([s] ∪ [o]|L).",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : "To sufficiently capture the textual information, the encoder is built by a pre-trained language model, BERT (Devlin et al., 2019).",
      "startOffset" : 108,
      "endOffset" : 129
    }, {
      "referenceID" : 3,
      "context" : "The detailed steps can be referred to (Devlin et al., 2019).",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : "Based on T i , all possible subjects in relation ri’s subject space are recognized by a sequential conditional random field (CRF) (Lafferty et al., 2001) layer with",
      "startOffset" : 130,
      "endOffset" : 153
    }, {
      "referenceID" : 14,
      "context" : "Concretely, we utilize NYT (Riedel et al., 2010), WebNLG (Gardent et al.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : ", 2010), WebNLG (Gardent et al., 2017), NYT10 (Takanobu et al.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 16,
      "context" : ", 2017), NYT10 (Takanobu et al., 2019) and NYT11 (Takanobu et al.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 16,
      "context" : ", 2019) and NYT11 (Takanobu et al., 2019) datasets for Partial Match, while NYT (Riedel et al.",
      "startOffset" : 18,
      "endOffset" : 41
    }, {
      "referenceID" : 14,
      "context" : ", 2019) datasets for Partial Match, while NYT (Riedel et al., 2010)",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 2,
      "context" : "and Wiki-KBP (Dai et al., 2019) datasets for Exact Match.",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 19,
      "context" : "For making a fair comparison, we utilize the cased BERT-base1 model in our experiments, which is the same as CasRel (Wei et al., 2020) and TPLinker (Wang et al.",
      "startOffset" : 116,
      "endOffset" : 134
    }, {
      "referenceID" : 18,
      "context" : ", 2020) and TPLinker (Wang et al., 2020b), and thus db = 768.",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 6,
      "context" : "Adam optimizer (Kingma and Ba, 2015) is utilized to train the proposed method with initial learning rate being 1e-5.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 15,
      "context" : "Meanwhile, cross-entropy is permutation-sensitive loss function (Sui et al., 2020), which is incompatible with generative models (Zeng et al.",
      "startOffset" : 64,
      "endOffset" : 82
    }, {
      "referenceID" : 15,
      "context" : ", 2020) incorporates contrastive learning strategy and SPN (Sui et al., 2020) transforms relational",
      "startOffset" : 59,
      "endOffset" : 77
    }, {
      "referenceID" : 17,
      "context" : "Nested entity recognition has been widely studied (Li et al.; Wang et al., 2020a), but most studies on relational triple extraction have not considered it.",
      "startOffset" : 50,
      "endOffset" : 81
    }, {
      "referenceID" : 18,
      "context" : "TPLinker (Wang et al., 2020b) provides a solution to recognize nested entities via a token pair tagging, but it ignores the interaction between entity and relation.",
      "startOffset" : 9,
      "endOffset" : 29
    } ],
    "year" : 2021,
    "abstractText" : "Relational triple extraction is critical to understanding massive text corpora and constructing large-scale knowledge graph, which has attracted increasing research interest. However, existing studies still face some challenging issues, including information loss, error propagation and ignoring the interaction between entity and relation. To intuitively explore the above issues and address them, in this paper, we provide a revealing insight into relational triple extraction from a stereoscopic perspective, which rationalizes the occurrence of these issues and exposes the shortcomings of existing methods. Further, a novel model is proposed for relational triple extraction, which maps relational triples to a three-dimension (3D) space and leverages three decoders to extract them, aimed at simultaneously handling the above issues. Extensive experiments are conducted on five public datasets, demonstrating that the proposed model outperforms the recent advanced baselines.",
    "creator" : "LaTeX with hyperref package"
  }
}