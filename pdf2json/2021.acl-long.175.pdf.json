{
  "name" : "2021.acl-long.175.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "SMURF: SeMantic and linguistic UndeRstanding Fusion for Caption Evaluation via Typicality Analysis",
    "authors" : [ "Joshua Feinglass", "Yezhou Yang" ],
    "emails" : [ "joshua.feinglass@asu.edu", "yz.yang@asu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2250–2260\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2250"
    }, {
      "heading" : "1 Introduction",
      "text" : "Visual captioning serves as a foundation for image/video understanding tools and relies on caption evaluation for identifying promising research directions. Rule-based caption evaluation approaches like the n-gram based CIDEr (Vedantam et al., 2015) and parsed semantic proposal based SPICE (Anderson et al., 2016) specifically are able to provide researchers with meaningful feedback on what their algorithm is lacking. However, ngram based methods are sensitive to stop words and sentence parsers are often inconsistent, leading to Liu et al. (2017) showing that neither method\n1SMURF source codes and data will be released at https: //github.com/JoshuaFeinglass/SMURF.\nfully captures either the fluency or the semantic meaning of text. More recently proposed metrics attempt to learn cues of caption quality by training models via image grounding techniques (Cui et al., 2018) or human and generated captions (Sellam et al., 2020). These approaches, however, lack generality, require domain specific training, and offer little insight for improving captioners, leading to none of the proposed models being adopted for use as a caption evaluation benchmark. We instead postulate that quality in semantics and descriptive language is universally recognizable.\nThe primary difficulty of caption evaluation is its cross-modal nature introducing ambiguity into the expected output, resulting in a ground truth that\nis no longer a single outcome, but a large set of potential outcomes of varying levels of quality. From this problem setting, the novel concept of “typicality” arises naturally. A desirable caption is one that is atypical enough linguistically that it uniquely describes the scene, follows typical natural language protocols, and matches a typical semantic description of a scene.\nLinguistically, the number of typical sequences is characterized by the entropy rate (Cover, 1999). Current work estimates the English language as having an entropy rate of only 1.44 bits/letter (Takahashi and Tanaka-Ishii, 2018), implying that the typical set of English is only a tiny fraction of the full space of potential text. Self-attention transformers are language models that are able to identify the distinguishing contextual features of this typical set and as a result have now become the staple of natural language understanding tasks. Here we define typicality based on the distance of a candidate text’s features from expected features of the typical set. We call this linguistic typicality estimation method Model-Integrated Meta-Analysis (MIMA) and use the function, fMIMA, to create referenceless fluency metrics attune to captioning needs. Rather than assuming a predefined evaluation task and introducing bias by fine-tuning the self-attention transformer, our method extracts the inherent properties of language learned by transformers (Devlin et al., 2019; Liu et al., 2019) by treating self-attention layers as probability distributions as demonstrated in Clark et al. (2019). Our approach represents the first integration of a fluency specific metric that demonstrably improves correlation with human judgment for caption evaluation.\nBy removing stop words from the candidate text, fMIMA is able to create a metric that assesses a relatively new fluency criteria in captioning: style. We refer to this metric as Stochastic Process Understanding Rating using Typical Sets (SPURTS). Style can be thought of as the instantiation of diction and is necessary for generating human-level quality captions. Stylized captions describe a much smaller set of media, leading to machines instead generating the most typical caption that is still semantically correct. This results in a significant gap between machine and human captioners that can be seen in diction-based examples such as the use of the common words like “dog” and “food” instead of more descriptive words like “Schnauzer” and\n“lasagna”. The other aspect of fluency assessed by fMIMA is grammar. Unlike style, grammar is not essential for caption quality, however, highly atypical syntax can potentially lead to awkward captions, so we develop a separate grammatical outlier penalty.\nWe then define a lightweight and reliable typicality based semantic similarity measure, Semantic Proposal Alikeness Rating using Concept Similarity (SPARCS), which complements our referenceless metrics and grounds them to the reference captions. By matching word sequences, current methods limit the scope of their evaluation. Instead, we take non-stopword unigrams and further coalesce them into concepts through stemming, then combine the reference texts, like in Yi et al. (2020), using a novel semantic typicality measure of the reference text’s concepts to evaluate the semantic similarity of a candidate and reference text.\nSPURTS and SPARCS can be used to assess system-level differences between captioners as shown in Figure 1. Based on this analysis, the M2 Transformer lags behind 2015 models in terms of similarity to human captions, even though both 2020 captioners achieved state-of-the-art results based on CIDEr standards. This difference becomes even more significant when you consider that the use of style makes it more difficult for a caption to be semantically correct. Human captions, M2 Transformer (Cornia et al., 2020), XTransformer (Pan et al., 2020), and Google (Vinyals et al., 2015) incur a total grammar outlier penalty of−44.93,−7.47,−7.56, and−4.46, respectively. In order to provide caption-level insight as well, we combine SPURTS, SPARCS, and our grammar outlier penalty into one metric - SeMantic and linguistic UndeRstanding Fusion (SMURF) - which rewards captions based on semantics and fluency. Contributions: Our key contributions are: 1. A novel and widely-applicable model metaanalysis technique, MIMA, which estimates the typicality of candidate text and which provides a means of assessing transformer robustness. 2. Three novel evaluation metrics useful for both caption-level and system-level evaluation: stylefocused SPURTS, semantic-focused SPARCS, and their combination which incorporates grammatical outliers as well, SMURF. 3. Experiments showing that SPARCS and SMURF achieve SOTA performance in their respective areas of semantic evaluation and humanmachine evaluation at both a system and caption-\nlevel. 4. Evidence showing that the performance of automatic evaluation metrics has been underestimated relative to voting-based human evaluation metrics."
    }, {
      "heading" : "2 Related Work",
      "text" : "Originally, popular rule-based metrics from machine translation that were mostly n-gram based, namely METEOR (Banerjee and Lavie, 2005), BLEU (Papineni et al., 2002), and ROUGE (Lin, 2004), were used for caption evaluation. Vedantam et al. (2015) introduced the more semantically sensitive CIDEr which uses tf-idf to identify distinguishing n-grams and then compares them using cosine similarity. SPICE (Anderson et al., 2016) greatly improved upon n-gram based approaches by using a sentence parser to generate semantic propositions. Word moving distance scores (Zhao et al., 2019; Kilickaya et al., 2017) have also been used for semantic evaluation with limited success. BERTScore (Zhang et al., 2019) used cosine similarity of embeddings from the self-attention transformer, BERT, and achieved state-of-the-art results on COCO but provided little interpretation of their approach.\nDomain specific training approaches have also been introduced with limited adoption. Cui et al. (2018); Jiang et al. (2019); Sharif et al. (2019) present a training approach for caption evaluation where an image grounding and/or caption based Turing test is learned based on training data from human and machine captioners. An adjusted BERTScore (Yi et al., 2020), BLEURT (Sellam et al., 2020), and NUBIA (Kane et al., 2020) utilize transformer embeddings for comparison between reference and candidate text, then perform caption dataset specific fine-tuning of the model downstream.\nThe importance of fluency in captioning has been widely recognized. Liu et al. (2017) attempted to integrate CIDEr and SPICE to create a cost function attune to both lexicographical and semantic qualities for captioning optimization. Cui et al. (2018) identified the presence of less frequent, distinguishing words within human-generated text in the COCO dataset. Mathews et al. (2018) recognized the importance of style in captions and integrated it into their model without sacrificing semantics.\nReferenceless evaluation, first proposed in Napoles et al. (2016) as a referenceless grammar\nerror correction (GEC) evaluation metric, has been recognized as an effective avenue for fluency evaluation as a whole (Asano et al., 2017), along with combined approaches (Choshen and Abend, 2018). More recently, Perception Score (Gu et al., 2021) outlined a general paradigm for training referenceless quality evaluation."
    }, {
      "heading" : "3 Our Approach",
      "text" : ""
    }, {
      "heading" : "3.1 Self-Attention Transformer Background",
      "text" : "First introduced in Vaswani et al. (2017), transformers are made of layers of parallel attention heads which extract contextual information about inputs using attention. They take in a sequence vector of tokenized words from candidate text, yn, add start and separator/end tokens, and pass the input through a series of separate linear transforms with parameters, p, to create query, key, and value vectors, denoted as qi,ki,vi, respectively. These vectors are then used to compute the attention weight parameters of the heads as shown:\nαij(y n, p) = exp(qTi kj)∑n l=1 exp(q T i kl) , (1)\noi(y n, p) = n∑ j=1 αijvj , (2)\nwhere αij and oi are each layer’s attention weights and output, respectively. Here αij(yn, p) is a joint distribution with marginal distributions αi(yn, p) =∑\nj αij(y n, p) and αj(yn, p) = ∑ i αij(y\nn, p). BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) are encoder-decoder instantiations of transformers, pretrained on fundamental language tasks over large corpora. Both BERT and RoBERTa have achieved state-of-the-art results in various language understanding tasks. In order to speed up inference time, many papers have employed knowledge distillation to reduce the number of parameters these transformers require while still preserving their inference capabilities (Sun et al., 2019; Sanh et al., 2019; Chen et al., 2020)."
    }, {
      "heading" : "3.2 Information Theory Background",
      "text" : "Transformers like BERT and RoBERTa take text tokenized into sub-word components as input, capturing both the syntax and morphology of the text. The text sequences used as training data, xn, can be modelled as a stationary ergodic stochastic process, {Xk}∞k=1, with instantiations limited to finite\nalphabet X and based on joint probability distribution, P (X1 = x1, ..., Xn = xn), whose transition predictability is governed by entropy rate, H(X ). The entropy of a distribution, or entropy rate in the case of a stochastic process, can be used to describe the number of instantiations expected to be observed from a random variable or process, referred to as the typical set. From the Asymptotic Equipartition Property (AEP), it is known that the size of the typical set of sequences is bounded by\n|A n| ≤ 2n(H(X )+ ), (3)\nwhere 2nH(X ) estimates the size of the typical set."
    }, {
      "heading" : "3.3 Model-Integrated Meta-Analysis",
      "text" : "We assume that a self-attention transformer learns to fill in words from a sentence by extracting features, F . The quality of a piece of text can then be assessed by determining the distance of features taken by the model from candidate text, Y n = yn, from the expected value of features taken from correctly written text, Xn = (xn∈A n), shown visually in Figure 2 and mathematically in Equation 4\nDtypical = dist(F | yn,E[F | (xn∈A n)]). (4)\nHere dist does not does not refer to a specific distance metric and is instead an unspecified norm that exists in some realizable projection space. We then postulate the existence of a surrogate function, fMIMA, which maps the sequence input and transformer parameter set, p, such that\nfMIMA(y n, p) ∝ −Dtypical, (5)\nresulting in a value indicating the typicality of a candidate input sequence. This value can be used to characterize the input for evaluation purposes."
    }, {
      "heading" : "3.4 Attention-Based Information Flow as MIMA Function",
      "text" : "We postulate that input text that differs more greatly from members of the typical set generates a greater “spark of interest” in a transformer, resulting in greater information flow through parts of the network as shown in Figure 3. Conversely, if the input text is similar to the positive examples the transformer trains on, less information flows in through the layer, indicating that the model has already captured information about the sequence previously. We formulate information flow in terms of the attention dimensions αi(yn, p), αj(yn, p), and their joint distribution αij(yn, p) as defined in Section 3.1. We consider information flow based on the redundancy between αi(yn, p) and αj(yn, p) and use normalized mutual information (MI):\nIflow(y n, p) =MI\n= 2∗H(αi(yn, p)) +H(αj(yn, p))−H(αij(yn, p))\nH(αi(yn, p)) +H(αj(yn, p)) ,\n(6) as defined in Witten and Frank (2005) to capture this redundancy.\nWe are interested in attention heads with large information flow values, but find empirically that heads with the largest information flow values depend very little on the input and simply function as all-pass layers. Thus, we downselect to a single attention head information flow value to obtain\nfMIMA(y n, p)\n= 1−medianlayer(maxhead[Iflow(yn, p)]). (7) Here, the max over a given layer’s attention heads captures the largest “spark of interest”. The median removes outlier layers that have largely invariant information flow values."
    }, {
      "heading" : "3.5 Caption Evaluation",
      "text" : "MIMA provides us with a foundation for computing the fluency of input text. We divide fluency into two categories: grammar and style. Grammar depends on the typicality of the sequence as a whole, fMIMA, and is computed using the distilled BERT model since it achieves the highest Pearson correlation in the grammar experiment from Table 1. Style depends on the distinctness, or atypicality, of the words directly associated with the image description, which we evaluate by removing the stop words from the text, then computing what we define as SPURTS as shown\nSPURTS = 1− fMIMA(yw/o, p), (8)\nwhere yw/o is the candidate sequence without stop words and fMIMA is computed using the distilled RoBERTa model since it performs well on out-ofdistribution text as shown in Figure 5.\nWe formulate semantic similarity using typicality as well. Assuming a comprehensive set of all valid captions for a single image were available, we consider the distribution of all concepts, S. Here we define concepts as the set stem terms that would remain if all stop words and affix/suffixes were removed from the text. The distribution of concepts sampled from such a set of captions, Sm, would have a typical set, Sβm, of the most relevant concepts. Thus, a valid caption that is representative of the image semantically and demonstrates fluency should contain concepts that are members of the typical set of concepts, Sβm, and be a member of the typical set of correctly formed language sequences defined in Section 3.2, A n, as shown in Figure 4.\nTo extract concepts from a caption, we use a stemmer on yw/s and estimate the typicality of each reference concept using the document frequency, df , of the concept across the available reference captions, gt(S), where gt is the function that maps concepts to a reference caption set. We then use an adjusted F1 score to determine the similarity\nbetween the reference concepts and candidate concepts.\nThe first portion of the F1 score is precision, corresponding to caption correctness. Our adjusted precision is P (C,S) = ∑ i dfgt(S)(Ci)\n|gt(S)|∑ i( dfgt(S)(Ci) |gt(S)| + I[dfgt(S)(Ci) = 0]) ,\n(9) where C is the candidate concept set and gt(S) is the reference caption set. Our approach equally weights correct and incorrect concepts if only one reference is used, but as the number increases, gradually decreases the importance of less common correct concepts.\nThe second portion of the F1 score is recall, corresponding to caption detail. Our adjusted recall is\nR(C,S) = ∑\ni dfgt(S)(Ci)∑ i dfgt(S)(Si) . (10)\nwhere a candidate concept set, C, which included all concepts from the reference set, S, would achieve a score of 1.\nWe then use the standard F1 score combination\nSPARCS = F1(C,S) = 2 ∗ P (C,S) ∗R(C,S) P (C,S) +R(C,S) .\n(11) To give an overall evaluation of performance, we fuse the proposed metrics. To begin, we standardize the output score distribution of human generated captions for each metric using the captions from the COCO Karpathy test split from Figure 1, metric′ = metric−E[metric(COCOtest)]σ(metric(COCOtest)) , creating SPARCS′, SPURTS′, and f ′MIMA. Utilizing the standardization, we use threshold, T = −1.96, corresponding to the left tail of a 95% confidence interval, to represent the lower bound of expected human captioning performance. We then use T to define a grammatical outlier penalty G= min(MIMA′−T, 0) and a style reward D= max(SPURTS′−T, 0). The quantities are combined as follows\nSMURF = { SPARCS′ +G if SPARCS′ < T, SPARCS′ +D +G otherwise. (12) It can be interpreted as applying a semantic threshold, then incorporating the style reward since style is only beneficial for caption quality if the caption\nis semantically correct. For all of our proposed metrics, a larger value corresponds to higher quality caption."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Preliminary Experiment",
      "text" : "We first seek to validate that our proposed fMIMA, extracted from the attention layers of BERT, RoBERTa, and their knowledge distilled versions, is proportional to the distance from the expected value of features of the typical set. To this end, we create an experiment where we can control the randomness of input text. We begin with 11 different paragraphs from unrelated Wikipedia articles. We extract all the words from the paragraphs and create a word set corpus. We then sample 25 sentences from the paragraphs randomly. Each sentence is iteratively degraded by substituting a fraction of the words with random words from the word set corpus. At each iteration step, the sentences are passed through the transformers and the value of fMIMA is computed. Eventually the sentence is incoherent and bears no resemblance to “natural” text. The process and results can be seen in Figure 5. The average fMIMA value for our information flow formulation shows a strong correlation with the degradation in both models up until about 10% of the tokens have been replaced, beyond which RoBERTa remains reliable but BERT does not, demonstrating RoBERTa’s superior robustness."
    }, {
      "heading" : "4.2 Datasets",
      "text" : "CoNLL-2014 The CoNLL-2014 competition (Ng et al., 2014) was a shared task of correcting grammatical errors of all types present in different sentences of an essay written by a learner of English as a second language. The essay consisted of 1312 separate sections to correct. A system-level human evaluation study of the grammatical quality of the\ncorrected sentences from 12 competition submissions was presented in Grundkiewicz et al. (2015). Participants were asked to rate how natural the corrected sentences sounded and did not have access to any reference sentence. Microsoft COCO 2014 We use the Microsoft COCO validation set (Chen et al., 2015), comprised of 40,504 images, for a system-level human correlation experiment. These images are annotated with five human-generated captions, one of which is used as a baseline caption candidate. Human evaluations of competition entries were collected using Amazon Mechanical Turk (AMT). These evaluations were framed as questions from which 2 primary dimensions of system-level caption quality were derived as a ground truth to rank competitors: M1 (percentage better than or equal to human description) and M2 (percentage passing the Turing Test). Three additional categories were also included as an experimental ablation study but were not considered in the final competition ranking. In total, 255,000 evaluations were collected. Flickr 8K We use the graded human quality scores for the 5,822 remapped captions from the Flickr 8k dataset (Hodosh et al., 2013) for a caption-level semantic human correlation study. The dataset was formed by selecting captions from one image and assigning them to another. These captions are then graded based on how well they align with the image using two different standards. The first standard is Expert Annotation, where human experts rate the image-caption pairing on a scale of 1 (caption and image unrelated) to 4 (caption describes image with no errors). Each caption-image pairing has 3 scores, which we combine by taking the average. The second standard is Crowd Flower Annotation, where at least 3 students vote yes or no on whether the caption and image are aligned. Composite Dataset An additional dataset for caption-level study of semantic human correlation from Aditya et al. (2018). It contains 11,095 human judgments (on a scale of 1-5) over Flickr 8K, Flickr 30K (Young et al., 2014), and COCO and in contrast to the Flickr 8K dataset, includes machine generated captions in addition to human reference captions as candidates. Each evaluation is either based purely on correctness or detailedness. PASCAL-50S Human evaluators were asked to identify which of two sentences, B or C, is more similar to reference sentence A. Unlike other caption datasets, human evaluators in Pascal-\n50S (Vedantam et al., 2015) did not have access to the original image. The captions for sentence A were sourced from a 1000 image subset of the UIUC PASCAL Sentence Dataset (Rashtchian et al., 2010) for which additional human captions were collected using AMT. Sentence B and C were sourced from both human and machine generated captions. The human captions were sourced from the original PASCAL dataset, resulting in four different pairing combinations: human-correct (HC), human-incorrect (HI), human-model (HM), and model-model (MM)."
    }, {
      "heading" : "4.3 System-Level Human Correlation",
      "text" : "System-level experiments evaluate how closely human evaluation and automatic evaluation models align in terms of their overall evaluation of captioning models. To confirm that fMIMA can capture grammar information, we replicate the experiment performed in Napoles et al. (2016) and show improved performance over previous benchmarks in Table 1. GLEU (Napoles et al., 2015), I-measure (Felice and Briscoe, 2015), and M2 (Dahlmeier and Ng, 2012) are reference-based while their proposed ER, LT, and LFM are referenceless and based on linguistic features like fMIMA.\nWe then benchmark our proposed caption evaluation metrics against the rule-based metrics used in the Microsoft COCO 2015 Captioning Competition, which still serve as the standard for caption evaluation, and the recall-idf configuration of BERTScore. We observe that the original COCO submissions and many of the original codebases for the submissions are not publicly available or do not provide pretrained models. Other authors attempt to reproduce the submissions using open source reimplementations that they have trained themselves, which will not be consistent with the submissions for which the human evaluations were\nperformed. Thus, we instead opt to use the 4 representative baseline caption sets (Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015) provided publicly by Cui et al. (2018), which include 3 competition submissions from open sourced models and 1 human caption baseline. These are guaranteed to be consistent with their work and reproducible. In Table 2, we show the COCO results for SPARCS, SPURTS, and SMURF.\nSMURF and BERTScore demonstrate the highest correlation with human judgment in this dataset. BERTScore’s performance is partially due to incorporation of idf dataset priors also used by CIDEr, which we do not utilize to keep our metrics as general and consistent as possible. To illustrate this point, we also report BERTScore’s correlation without idf weighting (BS-w/oidf) for this experiment. Despite its simplicity, SPARCS also performs well along with SPURTS. The rest of the metrics fail to adequately reflect human judgment."
    }, {
      "heading" : "4.4 Caption-Level Human Correlation",
      "text" : "Caption level experiments evaluate how closely human evaluation and automatic evaluation models align for each individual caption. We begin with the Pascal-50S dataset in Table 3. We follow the procedure used in Anderson et al. (2016) and use the first 5 sentence A entries of each image.\nThe Pascal-50S dataset is based on a direct comparison between the reference and candidate captions, which gives similarity based metrics a distinct advantage. As a result, SPARCS achieves the top score in this experiment. Another interesting result is the fact that SPURTS performs reasonably well in the human-machine category despite having\nno access to the reference sentence. This shows SPURTS effectiveness as a Turing Test at both a system and caption-level, independent of semantic information. The additional information provided by SPURTS to SMURF in the human-machine category actually improves its performance.\nTo evaluate our semantic metric specifically, we use the Flickr 8K and Composite dataset and follow the experiments specified in Anderson et al. (2016). However, we have discovered a flaw in previous comparisons between the correlation of automatic evaluation metrics with expert evaluation and interhuman correlation using the Flickr 8k dataset. Only a small subset of annotations between the Crowd Flower and Expert Annotations overlap, which often consists of ties causing the ranking metric to fail. To give a fair comparison, we also test the automatic metrics on a tie-free subset of the Flickr 8k data and use these results for human comparison. All of these results can be seen in Table 4.\nSPARCS outperforms other metrics in the Flickr 8k dataset. However, SPICE outperforms SPARCS on the Composite dataset. This is likely due to the fact that evaluations of “correctness” in the Composite dataset are based on semantic propositions and do not consider partial correctness.\nAdditionally, these new results show that automatic metrics can actually outperform voting-based human metrics in terms of their correlation with experts, further motivating their use. This warrants further study as some recent datasets opt to use voting-based human metrics due to their ease of collection (Levinboim et al., 2021)."
    }, {
      "heading" : "4.5 Generalization/Robustness Study",
      "text" : "We perform a caption-level generalizability and robustness case study on the most commonly used caption evaluation algorithms using the COCO validation set in Table 5. We define a critical fail-\nure, F , as a disparity of greater than 1 between system-level human (M2) and caption-level algorithm correlation of a reference evaluation metric and a tested evaluation metric for a given caption set of an image. The last column of Table 5 shows the likelihood of a critical failure occurring for each metric.\nIn a human study, we identify the primary cause of critical failure in the 20 most severe discrepancies in order to identify potential areas for improvement for each metric. We use SMURF as a reference evaluator for the other evaluators and SPICE as a reference for SMURF. The estimated probability of each of these failure causes is shown in the first three columns of Table 5.\nThe first failure cause, c1, refers to a scenario where the metric fails despite there being enough word overlap between the candidate and reference captions for a correct judgment to be made. This implies that the choice of words/sequences made by the metric for the comparison needs improvement. The second failure cause, c2, refers to the use of correct and distinct words or phrases by the human captioner that are not seen in the references. Lastly, we include the case where the reference evaluator may have incorrectly identified the correct caption ranking (according to the human annotator) as matching system-level human judgment. We refer to this as a reference failure, RF .\nThe focus of previous studies has been robustness to distractors (Sharif et al., 2019; Cui et al., 2018; Hodosh and Hockenmaier, 2016). We ob-\nserve no captions where this is a primary cause of failure. On the contrary, we find that each metric is highly susceptible to specific c1 scenarios: n-gram based: Both CIDEr and METEOR are sensitive to stopwords, leading to rewards for words or sequences that supply no additional information. SPICE: Semantic proposal formation or sentence parsing issues can lead to the metric unpredictably failing to recognize highly informative proposals. SMURF: The metric may fail to adequately reward additional information if the words used are too common, like ‘few’ or ‘some’."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "In this paper, we use information theory based typicality analysis to capture a new perspective on the problem of caption evaluation. Our analysis leads us to two caption evaluation metrics that capture separate dimensions of caption quality and a fused metric. We have performed experiments demonstrating their correlation with human judgment, showed how these methods could be used to perform multi-aspect system-level analysis of algorithm performance, and performed caption-level studies explaining why combining these two algorithms leads to more robust and generalizable evaluations. The underlying mechanism, MIMA, opens many new avenues for the analysis of selfattention transformers and potentially other models. Future work could also focus on optimal weighting between semantics and style."
    }, {
      "heading" : "6 Ethical Impact",
      "text" : "Harmful bias, especially towards gender (Hendricks et al., 2018), has been shown to be present in image caption datasets and is often further magnified by automatic captioners. Prior caption evaluation methods have the potential to further exacerbate the problem by rewarding such captions due to their reliance on dataset specific images or captions. Referenceless evaluations like our style metric, SPURTS, offer a preemptive approach for mitigating harmful dataset bias, like in Simpson’s Paradox (Mehrabi et al., 2019), by utilizing intrinsic properties of descriptive language learned by self-attention models over far larger and more diverse corpora. This gives the evaluator a more wholistic view of caption quality rather than viewing the world through the lens of a single visual dataset."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors acknowledge support from the NSF Project VR-K #1750082, the DARPA KAIROS program (LESTAT project), and the anonymous reviewers for their insightful discussion. Any opinions, findings, and conclusions in this publication are those of the authors and do not necessarily reflect the view of the funding agencies."
    } ],
    "references" : [ {
      "title" : "Image understanding using vision and reasoning through scene description graph",
      "author" : [ "Somak Aditya", "Yezhou Yang", "Chitta Baral", "Yiannis Aloimonos", "Cornelia Fermüller." ],
      "venue" : "Computer Vision and Image Understanding, 173:33–45.",
      "citeRegEx" : "Aditya et al\\.,? 2018",
      "shortCiteRegEx" : "Aditya et al\\.",
      "year" : 2018
    }, {
      "title" : "Spice: Semantic propositional image caption evaluation",
      "author" : [ "Peter Anderson", "Basura Fernando", "Mark Johnson", "Stephen Gould." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Anderson et al\\.,? 2016",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2016
    }, {
      "title" : "Reference-based metrics can be replaced with reference-less metrics in evaluating grammatical error correction systems",
      "author" : [ "Hiroki Asano", "Tomoya Mizumoto", "Kentaro Inui." ],
      "venue" : "Proceedings of the Eighth International Joint Conference on Natu-",
      "citeRegEx" : "Asano et al\\.,? 2017",
      "shortCiteRegEx" : "Asano et al\\.",
      "year" : 2017
    }, {
      "title" : "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
      "author" : [ "Satanjeev Banerjee", "Alon Lavie." ],
      "venue" : "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or sum-",
      "citeRegEx" : "Banerjee and Lavie.,? 2005",
      "shortCiteRegEx" : "Banerjee and Lavie.",
      "year" : 2005
    }, {
      "title" : "Microsoft coco captions: Data collection and evaluation server",
      "author" : [ "Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Dollár", "C Lawrence Zitnick." ],
      "venue" : "arXiv preprint arXiv:1504.00325.",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Distilling knowledge learned in bert for text generation",
      "author" : [ "Yen-Chun Chen", "Zhe Gan", "Yu Cheng", "Jingzhou Liu", "Jingjing Liu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7893–7905.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Referenceless measure of faithfulness for grammatical error correction",
      "author" : [ "Leshem Choshen", "Omri Abend." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Choshen and Abend.,? 2018",
      "shortCiteRegEx" : "Choshen and Abend.",
      "year" : 2018
    }, {
      "title" : "What does bert look at? an analysis of bert’s attention",
      "author" : [ "Kevin Clark", "Urvashi Khandelwal", "Omer Levy", "Christopher D Manning." ],
      "venue" : "arXiv preprint arXiv:1906.04341.",
      "citeRegEx" : "Clark et al\\.,? 2019",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "Meshed-memory transformer for image captioning",
      "author" : [ "Marcella Cornia", "Matteo Stefanini", "Lorenzo Baraldi", "Rita Cucchiara." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10578–10587.",
      "citeRegEx" : "Cornia et al\\.,? 2020",
      "shortCiteRegEx" : "Cornia et al\\.",
      "year" : 2020
    }, {
      "title" : "Elements of Information Theory",
      "author" : [ "Thomas M Cover." ],
      "venue" : "John Wiley & Sons.",
      "citeRegEx" : "Cover.,? 1999",
      "shortCiteRegEx" : "Cover.",
      "year" : 1999
    }, {
      "title" : "Learning to evaluate image captioning",
      "author" : [ "Y. Cui", "G. Yang", "A. Veit", "X. Huang", "S. Belongie." ],
      "venue" : "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5804–5812.",
      "citeRegEx" : "Cui et al\\.,? 2018",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2018
    }, {
      "title" : "A beamsearch decoder for grammatical error correction",
      "author" : [ "Daniel Dahlmeier", "Hwee Tou Ng." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages",
      "citeRegEx" : "Dahlmeier and Ng.,? 2012",
      "shortCiteRegEx" : "Dahlmeier and Ng.",
      "year" : 2012
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards a standard evaluation method for grammatical error detection and correction",
      "author" : [ "Mariano Felice", "Ted Briscoe." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Felice and Briscoe.,? 2015",
      "shortCiteRegEx" : "Felice and Briscoe.",
      "year" : 2015
    }, {
      "title" : "Human evaluation of grammatical error correction systems",
      "author" : [ "Roman Grundkiewicz", "Marcin Junczys-Dowmunt", "Edward Gillian." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 461–470.",
      "citeRegEx" : "Grundkiewicz et al\\.,? 2015",
      "shortCiteRegEx" : "Grundkiewicz et al\\.",
      "year" : 2015
    }, {
      "title" : "Perception score: A learned metric for open-ended text generation evaluation",
      "author" : [ "Jing Gu", "Qingyang Wu", "Zhou Yu." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 35(14):12902– 12910.",
      "citeRegEx" : "Gu et al\\.,? 2021",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2021
    }, {
      "title" : "Women also snowboard: Overcoming bias in captioning models",
      "author" : [ "Lisa Anne Hendricks", "Kaylee Burns", "Kate Saenko", "Trevor Darrell", "Anna Rohrbach." ],
      "venue" : "Proceedings of the European Conference on Computer Vision (ECCV), pages 771–787.",
      "citeRegEx" : "Hendricks et al\\.,? 2018",
      "shortCiteRegEx" : "Hendricks et al\\.",
      "year" : 2018
    }, {
      "title" : "Focused evaluation for image description with binary forcedchoice tasks",
      "author" : [ "Micah Hodosh", "Julia Hockenmaier." ],
      "venue" : "Proceedings of the 5th Workshop on Vision and Language, pages 19–28.",
      "citeRegEx" : "Hodosh and Hockenmaier.,? 2016",
      "shortCiteRegEx" : "Hodosh and Hockenmaier.",
      "year" : 2016
    }, {
      "title" : "Framing image description as a ranking task: Data, models and evaluation metrics",
      "author" : [ "Micah Hodosh", "Peter Young", "Julia Hockenmaier." ],
      "venue" : "Journal of Artificial Intelligence Research, 47:853–899.",
      "citeRegEx" : "Hodosh et al\\.,? 2013",
      "shortCiteRegEx" : "Hodosh et al\\.",
      "year" : 2013
    }, {
      "title" : "TIGEr: Text-to-image grounding for image caption evaluation",
      "author" : [ "Ming Jiang", "Qiuyuan Huang", "Lei Zhang", "Xin Wang", "Pengchuan Zhang", "Zhe Gan", "Jana Diesner", "Jianfeng Gao." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Jiang et al\\.,? 2019",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2019
    }, {
      "title" : "Nubia: Neural based interchangeability assessor for text generation",
      "author" : [ "Hassan Kane", "Muhammed Yusuf Kocyigit", "Ali Abdalla", "Pelkins Ajanoh", "Mohamed Coulibali." ],
      "venue" : "arXiv preprint arXiv:2004.14667.",
      "citeRegEx" : "Kane et al\\.,? 2020",
      "shortCiteRegEx" : "Kane et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep visualsemantic alignments for generating image descriptions",
      "author" : [ "Andrej Karpathy", "Li Fei-Fei" ],
      "venue" : null,
      "citeRegEx" : "Karpathy and Fei.Fei.,? \\Q2015\\E",
      "shortCiteRegEx" : "Karpathy and Fei.Fei.",
      "year" : 2015
    }, {
      "title" : "Re-evaluating automatic metrics for image captioning",
      "author" : [ "Mert Kilickaya", "Aykut Erdem", "Nazli Ikizler-Cinbis", "Erkut Erdem." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1,",
      "citeRegEx" : "Kilickaya et al\\.,? 2017",
      "shortCiteRegEx" : "Kilickaya et al\\.",
      "year" : 2017
    }, {
      "title" : "Quality estimation for image captions based on large-scale human evaluations",
      "author" : [ "Tomer Levinboim", "Ashish V. Thapliyal", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Levinboim et al\\.,? 2021",
      "shortCiteRegEx" : "Levinboim et al\\.",
      "year" : 2021
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text summarization branches out, pages 74–81.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Improved image captioning via policy gradient optimization of spider",
      "author" : [ "Siqi Liu", "Zhenhai Zhu", "Ning Ye", "Sergio Guadarrama", "Kevin Murphy." ],
      "venue" : "Proceedings of the IEEE international conference on computer vision, pages 873–881.",
      "citeRegEx" : "Liu et al\\.,? 2017",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2017
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Semstyle: Learning to generate stylised image captions using unaligned text",
      "author" : [ "A. Mathews", "L. Xie", "X. He." ],
      "venue" : "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8591–8600.",
      "citeRegEx" : "Mathews et al\\.,? 2018",
      "shortCiteRegEx" : "Mathews et al\\.",
      "year" : 2018
    }, {
      "title" : "A survey on bias and fairness in machine learning",
      "author" : [ "Ninareh Mehrabi", "Fred Morstatter", "Nripsuta Saxena", "Kristina Lerman", "Aram Galstyan." ],
      "venue" : "arXiv preprint arXiv:1908.09635.",
      "citeRegEx" : "Mehrabi et al\\.,? 2019",
      "shortCiteRegEx" : "Mehrabi et al\\.",
      "year" : 2019
    }, {
      "title" : "Ground truth for grammatical error correction metrics",
      "author" : [ "Courtney Napoles", "Keisuke Sakaguchi", "Matt Post", "Joel Tetreault." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Napoles et al\\.,? 2015",
      "shortCiteRegEx" : "Napoles et al\\.",
      "year" : 2015
    }, {
      "title" : "There’s no comparison: Referenceless evaluation metrics in grammatical error correction",
      "author" : [ "Courtney Napoles", "Keisuke Sakaguchi", "Joel Tetreault." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Napoles et al\\.,? 2016",
      "shortCiteRegEx" : "Napoles et al\\.",
      "year" : 2016
    }, {
      "title" : "The CoNLL-2014 shared task on grammatical error correction",
      "author" : [ "Hwee Tou Ng", "Siew Mei Wu", "Ted Briscoe", "Christian Hadiwinoto", "Raymond Hendy Susanto", "Christopher Bryant." ],
      "venue" : "Proceedings of the Eighteenth Conference on Computational Natural",
      "citeRegEx" : "Ng et al\\.,? 2014",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2014
    }, {
      "title" : "X-linear attention networks for image captioning",
      "author" : [ "Yingwei Pan", "Ting Yao", "Yehao Li", "Tao Mei." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10971– 10980.",
      "citeRegEx" : "Pan et al\\.,? 2020",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2020
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Collecting image annotations using amazon’s mechanical turk",
      "author" : [ "Cyrus Rashtchian", "Peter Young", "Micah Hodosh", "Julia Hockenmaier." ],
      "venue" : "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechan-",
      "citeRegEx" : "Rashtchian et al\\.,? 2010",
      "shortCiteRegEx" : "Rashtchian et al\\.",
      "year" : 2010
    }, {
      "title" : "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "arXiv preprint arXiv:1910.01108.",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "BLEURT: Learning robust metrics for text generation",
      "author" : [ "Thibault Sellam", "Dipanjan Das", "Ankur Parikh." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881–7892, Online. Association for Computa-",
      "citeRegEx" : "Sellam et al\\.,? 2020",
      "shortCiteRegEx" : "Sellam et al\\.",
      "year" : 2020
    }, {
      "title" : "Lceval: Learned composite metric for caption evaluation",
      "author" : [ "Naeha Sharif", "Lyndon White", "Mohammed Bennamoun", "Wei Liu", "Syed Afaq Ali Shah." ],
      "venue" : "International Journal of Computer Vision, 127(10):1586–1610.",
      "citeRegEx" : "Sharif et al\\.,? 2019",
      "shortCiteRegEx" : "Sharif et al\\.",
      "year" : 2019
    }, {
      "title" : "Patient knowledge distillation for bert model compression",
      "author" : [ "Siqi Sun", "Yu Cheng", "Zhe Gan", "Jingjing Liu." ],
      "venue" : "arXiv preprint arXiv:1908.09355.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Cross entropy of neural language models at infinity—a new bound of the entropy rate",
      "author" : [ "Shuntaro Takahashi", "Kumiko Tanaka-Ishii." ],
      "venue" : "Entropy, 20(11):839.",
      "citeRegEx" : "Takahashi and Tanaka.Ishii.,? 2018",
      "shortCiteRegEx" : "Takahashi and Tanaka.Ishii.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Cider: Consensus-based image description evaluation",
      "author" : [ "R. Vedantam", "C.L. Zitnick", "D. Parikh." ],
      "venue" : "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4566–4575.",
      "citeRegEx" : "Vedantam et al\\.,? 2015",
      "shortCiteRegEx" : "Vedantam et al\\.",
      "year" : 2015
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3156–3164.",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Data Mining: Practical Machine Learning Tools and Techniques",
      "author" : [ "Ian H. Witten", "Eibe Frank." ],
      "venue" : "Morgan Kaufmann, Amsterdam.",
      "citeRegEx" : "Witten and Frank.,? 2005",
      "shortCiteRegEx" : "Witten and Frank.",
      "year" : 2005
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio." ],
      "venue" : "International conference on machine learn-",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Improving image captioning evaluation by considering inter references variance",
      "author" : [ "Yanzhi Yi", "Hangyu Deng", "Jinglu Hu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 985–994, Online. Associ-",
      "citeRegEx" : "Yi et al\\.,? 2020",
      "shortCiteRegEx" : "Yi et al\\.",
      "year" : 2020
    }, {
      "title" : "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
      "author" : [ "Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 2:67–78.",
      "citeRegEx" : "Young et al\\.,? 2014",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 2014
    }, {
      "title" : "Bertscore: Evaluating text generation with bert",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q Weinberger", "Yoav Artzi." ],
      "venue" : "arXiv preprint arXiv:1904.09675.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance",
      "author" : [ "Wei Zhao", "Maxime Peyrard", "Fei Liu", "Yang Gao", "Christian M Meyer", "Steffen Eger." ],
      "venue" : "arXiv preprint arXiv:1909.02622.",
      "citeRegEx" : "Zhao et al\\.,? 2019",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 41,
      "context" : "Rule-based caption evaluation approaches like the n-gram based CIDEr (Vedantam et al., 2015) and parsed semantic proposal based SPICE (Anderson et al.",
      "startOffset" : 69,
      "endOffset" : 92
    }, {
      "referenceID" : 1,
      "context" : ", 2015) and parsed semantic proposal based SPICE (Anderson et al., 2016) specifically are able to provide researchers with meaningful feedback on what their algorithm is lacking.",
      "startOffset" : 49,
      "endOffset" : 72
    }, {
      "referenceID" : 4,
      "context" : "The ground truth captions are sourced from the Karpathy test split of the COCO dataset (Chen et al., 2015; Karpathy and Fei-Fei, 2015) with one used as a baseline for automatic captioners (Cornia et al.",
      "startOffset" : 87,
      "endOffset" : 134
    }, {
      "referenceID" : 21,
      "context" : "The ground truth captions are sourced from the Karpathy test split of the COCO dataset (Chen et al., 2015; Karpathy and Fei-Fei, 2015) with one used as a baseline for automatic captioners (Cornia et al.",
      "startOffset" : 87,
      "endOffset" : 134
    }, {
      "referenceID" : 8,
      "context" : ", 2015; Karpathy and Fei-Fei, 2015) with one used as a baseline for automatic captioners (Cornia et al., 2020; Pan et al., 2020; Vinyals et al., 2015).",
      "startOffset" : 89,
      "endOffset" : 150
    }, {
      "referenceID" : 32,
      "context" : ", 2015; Karpathy and Fei-Fei, 2015) with one used as a baseline for automatic captioners (Cornia et al., 2020; Pan et al., 2020; Vinyals et al., 2015).",
      "startOffset" : 89,
      "endOffset" : 150
    }, {
      "referenceID" : 42,
      "context" : ", 2015; Karpathy and Fei-Fei, 2015) with one used as a baseline for automatic captioners (Cornia et al., 2020; Pan et al., 2020; Vinyals et al., 2015).",
      "startOffset" : 89,
      "endOffset" : 150
    }, {
      "referenceID" : 10,
      "context" : "More recently proposed metrics attempt to learn cues of caption quality by training models via image grounding techniques (Cui et al., 2018) or human and generated captions (Sellam et al.",
      "startOffset" : 122,
      "endOffset" : 140
    }, {
      "referenceID" : 36,
      "context" : ", 2018) or human and generated captions (Sellam et al., 2020).",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 9,
      "context" : "Linguistically, the number of typical sequences is characterized by the entropy rate (Cover, 1999).",
      "startOffset" : 85,
      "endOffset" : 98
    }, {
      "referenceID" : 12,
      "context" : "self-attention transformer, our method extracts the inherent properties of language learned by transformers (Devlin et al., 2019; Liu et al., 2019) by treating self-attention layers as probability distributions as demonstrated in Clark et al.",
      "startOffset" : 108,
      "endOffset" : 147
    }, {
      "referenceID" : 26,
      "context" : "self-attention transformer, our method extracts the inherent properties of language learned by transformers (Devlin et al., 2019; Liu et al., 2019) by treating self-attention layers as probability distributions as demonstrated in Clark et al.",
      "startOffset" : 108,
      "endOffset" : 147
    }, {
      "referenceID" : 8,
      "context" : "Human captions, M2 Transformer (Cornia et al., 2020), XTransformer (Pan et al.",
      "startOffset" : 31,
      "endOffset" : 52
    }, {
      "referenceID" : 32,
      "context" : ", 2020), XTransformer (Pan et al., 2020), and Google (Vinyals et al.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 42,
      "context" : ", 2020), and Google (Vinyals et al., 2015) incur a total grammar outlier penalty of−44.",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 3,
      "context" : "Originally, popular rule-based metrics from machine translation that were mostly n-gram based, namely METEOR (Banerjee and Lavie, 2005), BLEU (Papineni et al.",
      "startOffset" : 109,
      "endOffset" : 135
    }, {
      "referenceID" : 33,
      "context" : "Originally, popular rule-based metrics from machine translation that were mostly n-gram based, namely METEOR (Banerjee and Lavie, 2005), BLEU (Papineni et al., 2002), and ROUGE (Lin, 2004), were used for caption evaluation.",
      "startOffset" : 142,
      "endOffset" : 165
    }, {
      "referenceID" : 24,
      "context" : ", 2002), and ROUGE (Lin, 2004), were used for caption evaluation.",
      "startOffset" : 19,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "SPICE (Anderson et al., 2016) greatly improved upon n-gram based approaches",
      "startOffset" : 6,
      "endOffset" : 29
    }, {
      "referenceID" : 48,
      "context" : "Word moving distance scores (Zhao et al., 2019; Kilickaya et al., 2017) have also been used for semantic evaluation with limited success.",
      "startOffset" : 28,
      "endOffset" : 71
    }, {
      "referenceID" : 22,
      "context" : "Word moving distance scores (Zhao et al., 2019; Kilickaya et al., 2017) have also been used for semantic evaluation with limited success.",
      "startOffset" : 28,
      "endOffset" : 71
    }, {
      "referenceID" : 45,
      "context" : "An adjusted BERTScore (Yi et al., 2020), BLEURT (Sellam et al.",
      "startOffset" : 22,
      "endOffset" : 39
    }, {
      "referenceID" : 36,
      "context" : ", 2020), BLEURT (Sellam et al., 2020), and NUBIA (Kane et al.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 20,
      "context" : ", 2020), and NUBIA (Kane et al., 2020) utilize transformer embeddings for comparison between reference and candidate text, then perform caption dataset specific fine-tuning of the model",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 2,
      "context" : "been recognized as an effective avenue for fluency evaluation as a whole (Asano et al., 2017), along with combined approaches (Choshen and Abend, 2018).",
      "startOffset" : 73,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : ", 2017), along with combined approaches (Choshen and Abend, 2018).",
      "startOffset" : 40,
      "endOffset" : 65
    }, {
      "referenceID" : 15,
      "context" : "More recently, Perception Score (Gu et al., 2021) outlined a general paradigm for training",
      "startOffset" : 32,
      "endOffset" : 49
    }, {
      "referenceID" : 26,
      "context" : ", 2019) and RoBERTa (Liu et al., 2019) are encoder-decoder instantiations of",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 38,
      "context" : "In order to speed up inference time, many papers have employed knowledge distillation to reduce the number of parameters these transformers require while still preserving their inference capabilities (Sun et al., 2019; Sanh et al., 2019; Chen et al., 2020).",
      "startOffset" : 200,
      "endOffset" : 256
    }, {
      "referenceID" : 35,
      "context" : "In order to speed up inference time, many papers have employed knowledge distillation to reduce the number of parameters these transformers require while still preserving their inference capabilities (Sun et al., 2019; Sanh et al., 2019; Chen et al., 2020).",
      "startOffset" : 200,
      "endOffset" : 256
    }, {
      "referenceID" : 5,
      "context" : "In order to speed up inference time, many papers have employed knowledge distillation to reduce the number of parameters these transformers require while still preserving their inference capabilities (Sun et al., 2019; Sanh et al., 2019; Chen et al., 2020).",
      "startOffset" : 200,
      "endOffset" : 256
    }, {
      "referenceID" : 31,
      "context" : "CoNLL-2014 The CoNLL-2014 competition (Ng et al., 2014) was a shared task of correcting grammatical errors of all types present in different sentences of an essay written by a learner of English as a second language.",
      "startOffset" : 38,
      "endOffset" : 55
    }, {
      "referenceID" : 4,
      "context" : "COCO validation set (Chen et al., 2015), comprised of 40,504 images, for a system-level human correlation experiment.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 18,
      "context" : "Flickr 8K We use the graded human quality scores for the 5,822 remapped captions from the Flickr 8k dataset (Hodosh et al., 2013) for a caption-level semantic human correlation study.",
      "startOffset" : 108,
      "endOffset" : 129
    }, {
      "referenceID" : 46,
      "context" : "It contains 11,095 human judgments (on a scale of 1-5) over Flickr 8K, Flickr 30K (Young et al., 2014), and COCO and in contrast to the Flickr 8K dataset, includes machine generated captions in addition to human reference captions as candidates.",
      "startOffset" : 82,
      "endOffset" : 102
    }, {
      "referenceID" : 34,
      "context" : "The captions for sentence A were sourced from a 1000 image subset of the UIUC PASCAL Sentence Dataset (Rashtchian et al., 2010) for which additional human captions were collected using AMT.",
      "startOffset" : 102,
      "endOffset" : 127
    }, {
      "referenceID" : 29,
      "context" : "GLEU (Napoles et al., 2015), I-measure (Felice and Briscoe, 2015), and M2 (Dahlmeier and Ng, 2012) are reference-based while their proposed ER, LT, and LFM are referenceless and based on linguistic features like",
      "startOffset" : 5,
      "endOffset" : 27
    }, {
      "referenceID" : 13,
      "context" : ", 2015), I-measure (Felice and Briscoe, 2015), and M2 (Dahlmeier and Ng, 2012) are reference-based while their proposed ER, LT, and LFM are referenceless and based on linguistic features like",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 11,
      "context" : ", 2015), I-measure (Felice and Briscoe, 2015), and M2 (Dahlmeier and Ng, 2012) are reference-based while their proposed ER, LT, and LFM are referenceless and based on linguistic features like",
      "startOffset" : 54,
      "endOffset" : 78
    }, {
      "referenceID" : 42,
      "context" : "sentative baseline caption sets (Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015) provided publicly by Cui et al.",
      "startOffset" : 32,
      "endOffset" : 99
    }, {
      "referenceID" : 44,
      "context" : "sentative baseline caption sets (Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015) provided publicly by Cui et al.",
      "startOffset" : 32,
      "endOffset" : 99
    }, {
      "referenceID" : 21,
      "context" : "sentative baseline caption sets (Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015) provided publicly by Cui et al.",
      "startOffset" : 32,
      "endOffset" : 99
    }, {
      "referenceID" : 23,
      "context" : "This warrants further study as some recent datasets opt to use voting-based human metrics due to their ease of collection (Levinboim et al., 2021).",
      "startOffset" : 122,
      "endOffset" : 146
    }, {
      "referenceID" : 37,
      "context" : "The focus of previous studies has been robustness to distractors (Sharif et al., 2019; Cui et al., 2018; Hodosh and Hockenmaier, 2016).",
      "startOffset" : 65,
      "endOffset" : 134
    }, {
      "referenceID" : 10,
      "context" : "The focus of previous studies has been robustness to distractors (Sharif et al., 2019; Cui et al., 2018; Hodosh and Hockenmaier, 2016).",
      "startOffset" : 65,
      "endOffset" : 134
    }, {
      "referenceID" : 17,
      "context" : "The focus of previous studies has been robustness to distractors (Sharif et al., 2019; Cui et al., 2018; Hodosh and Hockenmaier, 2016).",
      "startOffset" : 65,
      "endOffset" : 134
    }, {
      "referenceID" : 16,
      "context" : "Harmful bias, especially towards gender (Hendricks et al., 2018), has been shown to be present in image caption datasets and is often further magnified by automatic captioners.",
      "startOffset" : 40,
      "endOffset" : 64
    }, {
      "referenceID" : 28,
      "context" : "metric, SPURTS, offer a preemptive approach for mitigating harmful dataset bias, like in Simpson’s Paradox (Mehrabi et al., 2019), by utilizing intrinsic properties of descriptive language learned by self-attention models over far larger and more diverse corpora.",
      "startOffset" : 107,
      "endOffset" : 129
    } ],
    "year" : 2021,
    "abstractText" : "The open-ended nature of visual captioning makes it a challenging area for evaluation. The majority of proposed models rely on specialized training to improve human-correlation, resulting in limited adoption, generalizability, and explainabilty. We introduce “typicality”, a new formulation of evaluation rooted in information theory, which is uniquely suited for problems lacking a definite ground truth. Typicality serves as our framework to develop a novel semantic comparison, SPARCS, as well as referenceless fluency evaluation metrics. Over the course of our analysis, two separate dimensions of fluency naturally emerge: style, captured by metric SPURTS, and grammar, captured in the form of grammatical outlier penalties. Through extensive experiments and ablation studies on benchmark datasets, we show how these decomposed dimensions of semantics and fluency provide greater systemlevel insight into captioner differences. Our proposed metrics along with their combination, SMURF, achieve state-of-the-art correlation with human judgment when compared with other rule-based evaluation metrics1.",
    "creator" : "LaTeX with hyperref"
  }
}