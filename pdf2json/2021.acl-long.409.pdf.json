{
  "name" : "2021.acl-long.409.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Meta-Learning with Variational Semantic Memory for Word Sense Disambiguation",
    "authors" : [ "Yingjun Du", "Nithin Holla", "Xiantong Zhen", "Ekaterina Shutova" ],
    "emails" : [ "y.du@uva.nl", "nithin.holla7@gmail.com", "x.zhen@uva.nl", "C.G.M.Snoek@uva.nl", "e.shutova@uva.nl" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5254–5268\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5254"
    }, {
      "heading" : "1 Introduction",
      "text" : "Disambiguating word meaning in context is at the heart of any natural language understanding task or application, whether it is performed explicitly or implicitly. Traditionally, word sense disambiguation (WSD) has been defined as the task of explicitly labeling word usages in context with sense labels from a pre-defined sense inventory. The majority of approaches to WSD rely on (semi-)supervised learning (Yuan et al., 2016; Raganato et al., 2017a,b; Hadiwinoto et al., 2019; Huang et al., 2019; Scarlini et al., 2020; Bevilacqua and Navigli, 2020) and make use of training corpora manually annotated for word senses. Typically, these methods require a fairly large number of annotated training examples per word. This problem is exacerbated by the dramatic imbalances in\nsense frequencies, which further increase the need for annotation to capture a diversity of senses and to obtain sufficient training data for rare senses.\nThis motivated recent research on few-shot WSD, where the objective of the model is to learn new, previously unseen word senses from only a small number of examples. Holla et al. (2020a) presented a meta-learning approach to few-shot WSD, as well as a benchmark for this task. Meta-learning makes use of an episodic training regime, where a model is trained on a collection of diverse few-shot tasks and is explicitly optimized to perform well when learning from a small number of examples per task (Snell et al., 2017; Finn et al., 2017; Triantafillou et al., 2020). Holla et al. (2020a) have shown that meta-learning can be successfully applied to learn new word senses from as little as one example per sense. Yet, the overall model performance in settings where data is highly limited (e.g. one- or two-shot learning) still lags behind that of fully supervised models.\nIn the meantime, machine learning research demonstrated the advantages of a memory component for meta-learning in limited data settings (Santoro et al., 2016a; Munkhdalai and Yu, 2017a; Munkhdalai et al., 2018; Zhen et al., 2020). The memory stores general knowledge acquired in learning related tasks, which facilitates the acquisition of new concepts and recognition of previously unseen classes with limited labeled data (Zhen et al., 2020). Inspired by these advances, we introduce the first model of semantic memory for WSD in a meta-learning setting. In meta-learning, prototypes are embeddings around which other data points of the same class are clustered (Snell et al., 2017). Our semantic memory stores prototypical representations of word senses seen during training, generalizing over the contexts in which they are used. This rich contextual information aids in learning new senses of previously unseen words\nthat appear in similar contexts, from very few examples.\nThe design of our prototypical representation of word sense takes inspiration from prototype theory (Rosch, 1975), an established account of category representation in psychology. It stipulates that semantic categories are formed around prototypical members, new members are added based on resemblance to the prototypes and category membership is a matter of degree. In line with this account, our models learn prototypical representations of word senses from their linguistic context. To do this, we employ a neural architecture for learning probabilistic class prototypes: variational prototype networks, augmented with a variational semantic memory (VSM) component (Zhen et al., 2020).\nUnlike deterministic prototypes in prototypical networks (Snell et al., 2017), we model class prototypes as distributions and perform variational inference of these prototypes in a hierarchical Bayesian framework. Unlike deterministic memory access in memory-based meta-learning (Santoro et al., 2016b; Munkhdalai and Yu, 2017a), we access memory by Monte Carlo sampling from a variational distribution. Specifically, we first perform variational inference to obtain a latent memory variable and then perform another step of variational inference to obtain the prototype distribution. Furthermore, we enhance the memory update of vanilla VSM with a novel adaptive update rule involving a hypernetwork (Ha et al., 2016) that controls the weight of the updates. We call our approach β-VSM to denote the adaptive weight β for memory updates.\nWe experimentally demonstrate the effectiveness of this approach for few-shot WSD, advancing the state of the art in this task. Furthermore, we observe the highest performance gains on word senses with the least training examples, emphasizing the benefits of semantic memory for truly few-shot learning scenarios. Our analysis of the meaning prototypes acquired in the memory suggests that they are able to capture related senses of distinct words, demonstrating the generalization capabilities of our memory component. We make our code publicly available to facilitate further research.1"
    }, {
      "heading" : "2 Related work",
      "text" : "Word sense disambiguation Knowledge-based approaches to WSD (Lesk, 1986; Agirre et al.,\n1https://github.com/YDU-uva/VSM_WSD\n2014; Moro et al., 2014) rely on lexical resources such as WordNet (Miller et al., 1990) and do not require a corpus manually annotated with word senses. Alternatively, supervised learning methods treat WSD as a word-level classification task for ambiguous words and rely on sense-annotated corpora for training. Early supervised learning approaches trained classifiers with hand-crafted features (Navigli, 2009; Zhong and Ng, 2010) and word embeddings (Rothe and Schütze, 2015; Iacobacci et al., 2016) as input. Raganato et al. (2017a) proposed a benchmark for WSD based on the SemCor corpus (Miller et al., 1994) and found that supervised methods outperform the knowledgebased ones.\nNeural models for supervised WSD include LSTM-based (Hochreiter and Schmidhuber, 1997) classifiers (Kågebäck and Salomonsson, 2016; Melamud et al., 2016; Raganato et al., 2017b), nearest neighbour classifier with ELMo embeddings (Peters et al., 2018), as well as a classifier based on pretrained BERT representations (Hadiwinoto et al., 2019). Recently, hybrid approaches incorporating information from lexical resources into neural architectures have gained traction. GlossBERT (Huang et al., 2019) fine-tunes BERT with WordNet sense definitions as additional input. EWISE (Kumar et al., 2019) learns continuous sense embeddings as targets, aided by dictionary definitions and lexical knowledge bases. Scarlini et al. (2020) present a semi-supervised approach for obtaining sense embeddings with the aid of a lexical knowledge base, enabling WSD with a nearest neighbor algorithm. By further exploiting the graph structure of WordNet and integrating it with BERT, EWISER (Bevilacqua and Navigli, 2020) achieves the current state-of-the-art performance on the benchmark by Raganato et al. (2017a) – an F1 score of 80.1%.\nUnlike few-shot WSD, these works do not finetune the models on new words during testing. Instead, they train on a training set and evaluate on a test set where words and senses might have been seen during training.\nMeta-learning Meta-learning, or learning to learn (Schmidhuber, 1987; Bengio et al., 1991; Thrun and Pratt, 1998), is a learning paradigm where a model is trained on a distribution of tasks so as to enable rapid learning on new tasks. By solving a large number of different tasks, it aims to leverage the acquired knowledge to learn new, unseen tasks. The training set, referred to as the\nmeta-training set, consists of episodes, each corresponding to a distinct task. Every episode is further divided into a support set containing just a handful of examples for learning the task, and a query set containing examples for task evaluation. In the meta-training phase, for each episode, the model adapts to the task using the support set, and its performance on the task is evaluated on the corresponding query set. The initial parameters of the model are then adjusted based on the loss on the query set. By repeating the process on several episodes/tasks, the model produces representations that enable rapid adaptation to a new task. The test set, referred to as the meta-test set, also consists of episodes with a support and query set. The meta-test set corresponds to new tasks that were not seen during meta-training. During metatesting, the meta-trained model is first fine-tuned on a small number of examples in the support set of each meta-test episode and then evaluated on the accompanying query set. The average performance on all such query sets measures the few-shot learning ability of the model.\nMetric-based meta-learning methods (Koch et al., 2015; Vinyals et al., 2016; Sung et al., 2018; Snell et al., 2017) learn a kernel function and make predictions on the query set based on the similarity with the support set examples. Model-based methods (Santoro et al., 2016b; Munkhdalai and Yu, 2017a) employ external memory and make predictions based on examples retrieved from the memory. Optimization-based methods (Ravi and Larochelle, 2017; Finn et al., 2017; Nichol et al., 2018; Antoniou et al., 2019) directly optimize for generalizability over tasks in their training objective.\nMeta-learning has been applied to a range of tasks in NLP, including machine translation (Gu et al., 2018), relation classification (Obamuyide and Vlachos, 2019), text classification (Yu et al., 2018; Geng et al., 2019), hypernymy detection (Yu et al., 2020), and dialog generation (Qian and Yu, 2019). It has also been used to learn across distinct NLP tasks (Dou et al., 2019; Bansal et al., 2019) as well as across different languages (Nooralahzadeh et al., 2020; Li et al., 2020). Bansal et al. (2020) show that meta-learning during self-supervised pretraining of language models leads to improved fewshot generalization on downstream tasks.\nHolla et al. (2020a) propose a framework for few-shot word sense disambiguation, where the goal is to disambiguate new words during meta-\ntesting. Meta-training consists of episodes formed from multiple words whereas meta-testing has one episode corresponding to each of the test words. They show that prototype-based methods – prototypical networks (Snell et al., 2017) and first-order ProtoMAML (Triantafillou et al., 2020) – obtain promising results, in contrast with model-agnostic meta-learning (MAML) (Finn et al., 2017).\nMemory-based models Memory mechanisms (Weston et al., 2014; Graves et al., 2014; Krotov and Hopfield, 2016) have recently drawn increasing attention. In memory-augmented neural network (Santoro et al., 2016b), given an input, the memory read and write operations are performed by a controller, using soft attention for reads and least recently used access module for writes. Meta Network (Munkhdalai and Yu, 2017b) uses two memory modules: a key-value memory in combination with slow and fast weights for one-shot learning. An external memory was introduced to enhance recurrent neural network in Munkhdalai et al. (2019), in which memory is conceptualized as an adaptable function and implemented as a deep neural network. Semantic memory has recently been introduced by Zhen et al. (2020) for few-shot learning to enhance prototypical representations of objects, where memory recall is cast as a variational inference problem.\nIn NLP, Tang et al. (2016) use content and location-based neural attention over external memory for aspect-level sentiment classification. Das et al. (2017) use key-value memory for question answering on knowledge bases. Mem2Seq (Madotto et al., 2018) is an architecture for task-oriented dialog that combines attention-based memory with pointer networks (Vinyals et al., 2015). Geng et al. (2020) propose Dynamic Memory Induction Networks for few-shot text classification, which utilizes dynamic routing (Sabour et al., 2017) over a static memory module. Episodic memory has been used in lifelong learning on language tasks, as a means to perform experience replay (d’Autume et al., 2019; Han et al., 2020; Holla et al., 2020b)."
    }, {
      "heading" : "3 Task and dataset",
      "text" : "We treat WSD as a word-level classification problem where ambiguous words are to be classified into their senses given the context. In traditional WSD, the goal is to generalize to new contexts of word-sense pairs. Specifically, the test set consists of word-sense pairs that were seen during train-\ning. On the other hand, in few-shot WSD, the goal is to generalize to new words and senses altogether. The meta-testing phase involves further adapting the models (on the small support set) to new words that were not seen during training and evaluates them on new contexts (using the query set). It deviates from the standard N -way, K-shot classification setting in few-shot learning since the words may have a different number of senses and each sense may have different number of examples (Holla et al., 2020a), making it a more realistic few-shot learning setup (Triantafillou et al., 2020).\nDataset We use the few-shot WSD benchmark provided by Holla et al. (2020a). It is based on the SemCor corpus (Miller et al., 1994), annotated with senses from the New Oxford American Dictionary by Yuan et al. (2016). The dataset consists of words grouped into meta-training, metavalidation and meta-test sets. The meta-test set consists of new words that were not part of metatraining and meta-validation sets. There are four setups varying in the number of sentences in the support set |S| = 4, 8, 16, 32. |S| = 4 corresponds to an extreme few-shot learning scenario for most words, whereas |S| = 32 comes closer to the number of sentences per word encountered in standard WSD setups. For |S| = 4, 8, 16, 32, the number of unique words in the meta-training / meta-validation / meta-test sets is 985/166/270, 985/163/259, 799/146/197 and 580/85/129 respectively. We use the publicly available standard dataset splits.2\nEpisodes The meta-training episodes were created by first sampling a set of words and a fixed number of senses per word, followed by sampling example sentences for these word-sense pairs. This strategy allows for a combinatorially large number of episodes. Every meta-training episode has |S| sentences in both the support and query sets, and corresponds to the distinct task of disambiguating between the sampled word-sense pairs. The total number of meta-training episodes is 10, 000. In the meta-validation and meta-test sets, each episode corresponds to the task of disambiguating a single, previously unseen word between all its senses. For every meta-test episode, the model is fine-tuned on a few examples in the support set and its generalizability is evaluated on the query set. In contrast to\n2https://github.com/Nithin-Holla/ MetaWSD\nthe meta-training episodes, the meta-test episodes reflect a natural distribution of senses in the corpus, including class imbalance, providing a realistic evaluation setting."
    }, {
      "heading" : "4 Methods",
      "text" : ""
    }, {
      "heading" : "4.1 Model architectures",
      "text" : "We experiment with the same model architectures as Holla et al. (2020a). The model fθ, with parameters θ, takes words xi as input and produces a perword representation vector fθ(xi) for i = 1, ..., L where L is the length of the sentence. Sense predictions are only made for ambiguous words using the corresponding word representation.\nGloVe+GRU Single-layer bi-directional GRU (Cho et al., 2014) network followed by a single linear layer, that takes GloVe embeddings (Pennington et al., 2014) as input. GloVe embeddings capture all senses of a word. We thus evaluate a model’s ability to disambiguate from senseagnostic input.\nELMo+MLP A multi-layer perception (MLP) network that receives contextualized ELMo embeddings (Peters et al., 2018) as input. Their contextualised nature makes ELMo embeddings better suited to capture meaning variation than the static ones. Since ELMo is not fine-tuned, this model has the lowest number of learnable parameters.\nBERT Pretrained BERTBASE (Devlin et al., 2019) model followed by a linear layer, fully finetuned on the task. BERT underlies state-of-the-art approaches to WSD."
    }, {
      "heading" : "4.2 Prototypical Network",
      "text" : "Our few-shot learning approach builds upon prototypical networks (Snell et al., 2017), which is widely used for few-shot image classification and has been shown to be successful in WSD (Holla et al., 2020a). It computes a prototype zk = 1 K ∑ k fθ(xk) of each word sense (where K is the number of examples for each word sense) through an embedding function fθ, which is realized as the aforementioned architectures. It computes a distribution over classes for a query sample x given a distance function d(·, ·) as the softmax over its distances to the prototypes in the embedding space:\np(yi = k|x) = exp(−d(fθ(x), zk))∑ k′ exp(−d(fθ(x), zk′)) (1)\nHowever, the resulting prototypes may not be sufficiently representative of word senses as semantic categories when using a single deterministic vector, computed as the average of only a few examples. Such representations lack expressiveness and may not encompass sufficient intra-class variance, that is needed to distinguish between different fine-grained word senses. Moreover, large uncertainty arises in the single prototype due to the small number of samples."
    }, {
      "heading" : "4.3 Variational Prototype Network",
      "text" : "Variational prototype network (Zhen et al., 2020) (VPN) is a powerful model for learning latent representations from small amounts of data, where the prototype z of each class is treated as a distribution. Given a task with a support set S and query set Q, the objective of VPN takes the following form:\nLVPN = 1\n|Q| |Q|∑ i=1 [ 1 Lz Lz∑ lz=1 − log p(yi|xi, z(lz))\n+ λDKL[q(z|S)||p(z|xi)] ]\n(2) where q(z|S) is the variational posterior over z, p(z|xi) is the prior, and Lz is the number of Monte Carlo samples for z. The prior and posterior are assumed to be Gaussian. The re-parameterization trick (Kingma and Welling, 2013) is adopted to enable back-propagation with gradient descent, i.e., z(lz) = f(S, (lz)), (lz) ∼ N (0, I), f(·, ·) = (lz) ∗ µz + σz , where the mean µz and diagonal covariance σz are generated from the posterior inference network with S as input. The amortization technique is employed for the implementation of VPN. The posterior network takes the mean word representations in the support set S as input and returns the parameters of q(z|S). Similarly, the prior network produces the parameters of p(z|xi) by taking the query word representation xi ∈ Q as input. The conditional predictive log-likelihood is implemented as a cross-entropy loss."
    }, {
      "heading" : "4.4 β-Variational Semantic Memory",
      "text" : "In order to leverage the shared common knowledge between different tasks to improve disambiguation in future tasks, we incorporate variational semantic memory (VSM) as in Zhen et al. (2020). It consists of two main processes: memory recall, which retrieves relevant information that fits with specific tasks based on the support set of the current task;\nmemory update, which effectively collects new information from the task and gradually consolidates the semantic knowledge in the memory. We adopt a similar memory mechanism and introduce an improved update rule for memory consolidation.\nMemory recall The memory recall of VSM aims to choose the related content from the memory, and is accomplished by variational inference. It introduces latent memory m as an intermediate stochastic variable, and infers m from the addressed memory M . The approximate variational posterior q(m|M,S) over the latent memory m is obtained empirically by\nq(m|M,S) = |M |∑ a=1 γap(m|Ma), (3)\nwhere\nγa = exp\n( g(Ma, S) )∑ i exp ( g(Mi, S)\n) (4) g(·) is the dot product, |M | is the number of memory slots, Ma is the memory content at slot a and stores the prototype of samples in each class, and we take the mean representation of samples in S.\nThe variational posterior over the prototype then becomes:\nq̃(z|M,S) ≈ 1 Lm Lm∑ lm=1 q(z|m(lm), S), (5)\nwhere m(lm) is a Monte Carlo sample drawn from the distribution q(m|M,S), and lm is the number of samples. By incorporating the latent memory m from Eq. (3), we achieve the objective for varia-\ntional semantic memory as follows:\nLVSM = |Q|∑ i=1 [ − Eq(z|S,m) [ log p(yi|xi, z) ] + λzDKL [ q(z|S,m)||p(z|xi)\n] + λmDKL\n[ |M |∑ i γip(m|Mi)||p(m|S) ]] (6)\nwhere p(m|S) is the introduced prior over m, λz and λm are the hyperparameters. The overall computational graph of VSM is shown in Figure 1. Similarly, the posterior and prior over m are also assumed to be Gaussian and obtained by using amortized inference networks; more details are provided in Appendix A.1.\nMemory update The memory update is to be able to effectively absorb new useful information to enrich memory content. VSM employs an update rule as follows:\nMc ← βMc + (1− β)M̄c, (7)\nwhere Mc is the memory content corresponding to class c, M̄c is obtained using graph attention (Veličković et al., 2017), and β ∈ (0, 1) is a hyperparameter.\nAdaptive memory update Although VSM was shown to be promising for few-shot image classification, it can be seen from the experiments by Zhen et al. (2020) that different values of β have considerable influence on the performance. β determines the extent to which memory is updated at each iteration. In the original VSM, β is treated as a hyperparameter obtained by cross-validation, which is time-consuming and inflexible in dealing with different datasets. To address this problem, we propose an adaptive memory update rule by learning β from data using a lightweight hypernetwork (Ha et al., 2016). To be more specific, we obtain β by a function fβ(·) implemented as an MLP with a sigmoid activation function in the output layer. The hypernetwork takes M̄c as input and returns the value of β:\nβ = fβ(M̄c) (8)\nMoreover, to prevent the possibility of endless growth of memory value, we propose to scale down the memory value whenever ‖Mc‖2 > 1. This is\nachieved by scaling as follows:\nMc = Mc\nmax(1, ‖Mc‖2) (9)\nWhen we update memory, we feed the new obtained memory M̄c into the hypernetwork fβ(·) and output adaptive β for the update. We provide a more detailed implementation of β-VSM in Appendix A.1."
    }, {
      "heading" : "5 Experiments and results",
      "text" : "Experimental setup The size of the shared linear layer and memory content of each word sense is 64, 256, and 192 for GloVe+GRU, ELMo+MLP and BERT respectively. The activation function of the shared linear layer is tanh for GloVe+GRU and ReLU for the rest. The inference networks gφ(·) for calculating the prototype distribution and gψ(·) for calculating the memory distribution are all three-layer MLPs, with the size of each hidden layer being 64, 256, and 192 for GloVe+GRU, ELMo+MLP and BERT. The activation function of their hidden layers is ELU (Clevert et al., 2016), and the output layer does not use any activation function. Each batch during meta-training includes 16 tasks. The hypernetwork fβ(·) is also a threelayer MLP, with the size of hidden state consistent with that of the memory contents. The linear layer activation function is ReLU for the hypernetwork. For BERT and |S| = {4, 8}, λz = 0.001, λm = 0.0001 and learning rate is 5e−6; |S| = 16, λz = 0.0001, λm = 0.0001 and learning rate is 1e−6; |S| = 32, λz = 0.001, λm = 0.0001 and learning rate is 1e−5. Hyperparameters for other models are reported in Appendix A.2. All the hyperparameters are chosen using the meta-validation set. The number of slots in memory is consistent with the number of senses in the meta-training set – 2915 for |S| = 4 and 8; 2452 for |S| = 16; 1937 for |S| = 32. The evaluation metric is the wordlevel macro F1 score, averaged over all episodes in the meta-test set. The parameters are optimized using Adam (Kingma and Ba, 2014).\nWe compare our methods against several baselines and state-of-the-art approaches. The nearest neighbor classifier baseline (NearestNeighbor) predicts a query example’s sense as the sense of the support example closest in the word embedding space (ELMo and BERT) in terms of cosine distance. The episodic fine-tuning baseline (EF-ProtoNet) is one where only meta-testing is\nperformed, starting from a randomly initialized model. Prototypical network (ProtoNet) and ProtoFOMAML achieve the highest few-shot WSD performance to date on the benchmark of Holla et al. (2020a).\nResults In Table 1, we show the average macro F1 scores of the models, with their mean and standard deviation obtained over five independent runs. Our proposed β-VSM achieves the new state-ofthe-art performance on few-shot WSD with all the embedding functions, across all the setups with varying |S|. For GloVe+GRU, where the input is sense-agnostic embeddings, our model improves disambiguation compared to ProtoNet by 1.8% for |S| = 4 and by 2.4% for |S| = 32. With contextual embeddings as input, β-VSM with ELMo+MLP also leads to improvements compared to the previous best ProtoFOMAML for all |S|. Holla et al. (2020a) obtained state-of-the-art performance with BERT, and β-VSM further advances this, resulting in a gain of 0.9 – 2.2%. The consistent improvements with different embedding functions and support set sizes suggest that our β-VSM is effective for few-shot WSD for varying number of shots and senses as well as across model architectures."
    }, {
      "heading" : "6 Analysis and discussion",
      "text" : "To analyze the contributions of different components in our method, we perform an ablation study by comparing ProtoNet, VPN, VSM and β-VSM and present the macro F1 scores in Table 2.\nRole of variational prototypes VPN consistently outperforms ProtoNet with all embedding functions (by around 1% F1 score on average). The results indicate that the probabilistic prototypes provide more informative representations of word senses compared to deterministic vectors. The highest gains were obtained in case of GloVe+GRU (1.7% F1 score with |S| = 8), suggesting that probabilistic prototypes are particularly useful for models that rely on static word embeddings, as they capture uncertainty in contextual interpretation.\nRole of variational semantic memory We show the benefit of VSM by comparing it with VPN. VSM consistently surpasses VPN with all three embedding functions. According to our analysis, VSM makes the prototypes of different word senses more distinctive and distant from each other. The senses in memory provide more context information, enabling larger intra-class variations to be captured, and thus lead to improvements upon VPN.\nRole of adaptive β To demonstrate the effectiveness of the hypernetwork for adaptive β, we compare β-VSM with VSM where β is tuned by cross-validation. It can be seen from Table 2 that there is consistent improvement over VSM. Thus, the learned adaptive β acquires the ability to determine how much of the contents of memory needs to be updated based on the current new memory. βVSM enables the memory content of different word senses to be more representative by better absorbing information from data with adaptive update, resulting in improved performance.\nVariation of performance with the number of senses In order to further probe into the strengths of β-VSM, we analyze the macro F1 scores of the different models averaged over all the words in the meta-test set with a particular number of senses. In Figure 2, we show a bar plot of the scores obtained from BERT for |S| = 16. For words with a low number of senses, the task corresponds to a higher number of effective shots and vice versa. It can be seen that the different models perform roughly the same for words with fewer senses, i.e., 2 – 4. VPN is comparable to ProtoNet in its distribution of scores. But with semantic memory, VSM improves the performance on words with a higher number of senses. β-VSM further boosts the scores for such words on average. The same trend is observed for |S| = 8 (see Appendix A.3). Therefore, the improvements of β-VSM over ProtoNet come from tasks with fewer shots, indicating that VSM is particularly effective at disambiguation in low-shot scenarios.\nVisualization of prototypes To study the distinction between the prototype distributions of word senses obtained by β-VSM, VSM and VPN, we visualize them using t-SNE (Van der Maaten and Hinton, 2008). Figure 3 shows prototype distributions based on BERT for the word draw. Different colored ellipses indicate the distribution of its different senses obtained from the support set. Different colored points indicate the representations of the query examples. β-VSM makes the prototypes of different word senses of the same word more distinctive and distant from each other, with less overlap, compared to the other models. Notably, the representations of query examples are closer to their corresponding prototype distribution for βVSM, thereby resulting in improved performance. We also visualize the prototype distributions of similar vs. dissimilar senses of multiple words in Figure 4 (see Appendix A.4 for example sentences). The blue ellipse corresponds to the ‘set up’ sense of launch from the meta-test samples. Green and gray ellipses correspond to a similar sense of the words start and establish from the memory. We can see that they are close to each other. Orange and purple ellipses correspond to other senses of the words start and establish from the memory, and they are well separated. For a given query word, our model is thus able to retrieve related senses from the memory and exploit them to make its word sense distribution more representative and distinctive."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we presented a model of variational semantic memory for few-shot WSD. We use a variational prototype network to model the prototype of each word sense as a distribution. To leverage the shared common knowledge between tasks, we incorporate semantic memory into the probabilistic model of prototypes in a hierarchical Bayesian framework. VSM is able to acquire longterm, general knowledge that enables learning new senses from very few examples. Furthermore, we propose adaptive β-VSM which learns an adaptive memory update rule from data using a lightweight hypernetwork. The consistent new state-of-the-art performance with three different embedding functions shows the benefit of our model in boosting few-shot WSD.\nSince meaning disambiguation is central to many natural language understanding tasks, models based on semantic memory are a promising direction in NLP, more generally. Future work might investigate the role of memory in modeling meaning variation across domains and languages, as well as in tasks that integrate knowledge at different levels of linguistic hierarchy."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Implementation details In the meta-training phase, we implement β-VSM by end-to-end learning with stochastic neural networks. The inference network and hypernetwork are parameterized by a feed-forward multi-layer perceptrons (MLP). At meta-train time, we first extract the features of the support set via fθ(xS), where fθ is the feature extraction network and we use permutation-invariant instance-pooling operations to get the mean feature f̄sc of samples in\nthe c-th class. Then we get the memory Ma by using the support representation f̄sc of each class. The memory obtained Ma will be fed into a small three-layers MLP network gψ(·) to calculate the mean µm and variance σm of the memory distribution m, which is then used to sample the memory m by m ∼ N (µm, diag((σm)2)). The new memory M̄c is obtained by using graph attention. The nodes of the graph are a set of feature representations of the current task samples: Fc = {f0c , f1c , f2c , . . . , fNcc }, where fNcc ∈ Rd, Nc = |Sc ∪ Qc|, f0c = Mc, f i>0c = fθ(xic). Nc contains all samples including both the support and query set from the c-th category in the current task. When we update memory, we take the new obtained memory M̄c into the hypernetwork fβ(·) as input and output the adaptive β to update the memory using Equation 8. We calculate the prototype of the latent distribution, i.e., the mean µz and variance σz by another small three-layer MLP network gφ(·, ·), whose inputs are f̄sc and m. Then the prototype z(lz) is sampled from the distribution z(lz) ∼ N (µz, diag((σz)2)). By using the prototypical word sense of support samples and the feature embedding of query sample xi, we obtain the predictive value ŷi.\nAt meta-test time, we feed the support representation f̄ sc into the gψ(·) to generate the memory ma. Then, using the sampled memory ma and the support representation f̄sc , we obtain the distribution of prototypical word sense z. Finally, we make predictions for the query sample by using the query representation extracted from embedding function and the support prototype z.\nA.2 Hyperparameters and runtimes\nWe present our hyperparameters in Table 3. For Monte Carlo sampling, we set differentLZ andLM for the each embedding function and |S|, which are chosen using the validation set. Training time differs for different |S| and different embedding functions. Here we give the training time per epoch for |S| = 16. For GloVe+GRU, the approximate training time per epoch is 20 minutes; for ELMo+MLP it is 80 minutes; and for BERT, it is 60 minutes. The number of meta-learned parameters for GloVe+GRU is θ are 889, 920; for ELMo+MLP it is 262, 404; and for BERT it is θ are 107, 867, 328. We implemented all models using the PyTorch framework and trained them on an NVIDIA Tesla V100.\nA.3 Variation of performance with the number of senses\nTo further demonstrate that β-VSM achieves better performance in extremely data scarce scenarios, we also analyze variation of macro F1 scores with the number of senses for BERT and |S| = 8. In Figure 5, we observe a similar trend as with |S| = 16. β-VSM has an improved performance for words with many senses, which corresponds to a low-shot scenario. For example, with 8 senses, the task is essentially one-shot.\nA.4 Example sentences to visualize prototypes\nIn Table 4, we provide some example sentences used to generate the plots in Figure 4. These examples correspond to words launch, start and establish, and contain senses ‘set up’, ‘begin’ and ‘build up’.\nA.5 Results on the meta-validation set We provide the results on the on the meta-validation set in the Table 5, to better facilitate reproducibility."
    } ],
    "references" : [ {
      "title" : "Random walks for knowledge-based word sense disambiguation",
      "author" : [ "Eneko Agirre", "Oier López de Lacalle", "Aitor Soroa." ],
      "venue" : "Computational Linguistics, 40(1):57–84.",
      "citeRegEx" : "Agirre et al\\.,? 2014",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2014
    }, {
      "title" : "How to train your MAML",
      "author" : [ "Antreas Antoniou", "Harrison Edwards", "Amos Storkey." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Antoniou et al\\.,? 2019",
      "shortCiteRegEx" : "Antoniou et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to few-shot learn across diverse natural language classification tasks",
      "author" : [ "Trapit Bansal", "Rishikesh Jha", "Andrew McCallum." ],
      "venue" : "arXiv preprint arXiv:1911.03863.",
      "citeRegEx" : "Bansal et al\\.,? 2019",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2019
    }, {
      "title" : "Self-supervised meta-learning for few-shot natural language classification tasks",
      "author" : [ "Trapit Bansal", "Rishikesh Jha", "Tsendsuren Munkhdalai", "Andrew McCallum." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Bansal et al\\.,? 2020",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning a synaptic learning rule",
      "author" : [ "Y. Bengio", "S. Bengio", "J. Cloutier." ],
      "venue" : "IJCNN-91-Seattle International Joint Conference on Neural Networks, volume ii, pages 969 vol.2–.",
      "citeRegEx" : "Bengio et al\\.,? 1991",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 1991
    }, {
      "title" : "Breaking through the 80% glass ceiling: Raising the state of the art in word sense disambiguation by incorporating knowledge graph information",
      "author" : [ "Michele Bevilacqua", "Roberto Navigli." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association",
      "citeRegEx" : "Bevilacqua and Navigli.,? 2020",
      "shortCiteRegEx" : "Bevilacqua and Navigli.",
      "year" : 2020
    }, {
      "title" : "Learning phrase representations using RNN encoder–decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Fast and accurate deep network learning by exponential linear units (elus)",
      "author" : [ "Djork-Arné Clevert", "Thomas Unterthiner", "Sepp Hochreiter." ],
      "venue" : "4th",
      "citeRegEx" : "Clevert et al\\.,? 2016",
      "shortCiteRegEx" : "Clevert et al\\.",
      "year" : 2016
    }, {
      "title" : "Question answering on knowledge bases and text using universal schema and memory networks",
      "author" : [ "Rajarshi Das", "Manzil Zaheer", "Siva Reddy", "Andrew McCallum." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Das et al\\.,? 2017",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2017
    }, {
      "title" : "Episodic memory in lifelong language learning",
      "author" : [ "Cyprien de Masson d’Autume", "Sebastian Ruder", "Lingpeng Kong", "Dani Yogatama" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "d.Autume et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "d.Autume et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Investigating meta-learning algorithms for low-resource natural language understanding tasks",
      "author" : [ "Zi-Yi Dou", "Keyi Yu", "Antonios Anastasopoulos." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Dou et al\\.,? 2019",
      "shortCiteRegEx" : "Dou et al\\.",
      "year" : 2019
    }, {
      "title" : "Model-agnostic meta-learning for fast adaptation of deep networks",
      "author" : [ "Chelsea Finn", "Pieter Abbeel", "Sergey Levine." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Re-",
      "citeRegEx" : "Finn et al\\.,? 2017",
      "shortCiteRegEx" : "Finn et al\\.",
      "year" : 2017
    }, {
      "title" : "Dynamic memory induction networks for few-shot text classification",
      "author" : [ "Ruiying Geng", "Binhua Li", "Yongbin Li", "Jian Sun", "Xiaodan Zhu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1087–",
      "citeRegEx" : "Geng et al\\.,? 2020",
      "shortCiteRegEx" : "Geng et al\\.",
      "year" : 2020
    }, {
      "title" : "Induction networks for few-shot text classification",
      "author" : [ "Ruiying Geng", "Binhua Li", "Yongbin Li", "Xiaodan Zhu", "Ping Jian", "Jian Sun." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Geng et al\\.,? 2019",
      "shortCiteRegEx" : "Geng et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural turing machines",
      "author" : [ "Alex Graves", "Greg Wayne", "Ivo Danihelka." ],
      "venue" : "arXiv preprint arXiv:1410.5401.",
      "citeRegEx" : "Graves et al\\.,? 2014",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2014
    }, {
      "title" : "Meta-learning for lowresource neural machine translation",
      "author" : [ "Jiatao Gu", "Yong Wang", "Yun Chen", "Victor O.K. Li", "Kyunghyun Cho." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3622–3631,",
      "citeRegEx" : "Gu et al\\.,? 2018",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2018
    }, {
      "title" : "Hypernetworks",
      "author" : [ "David Ha", "Andrew Dai", "Quoc V Le." ],
      "venue" : "arXiv preprint arXiv:1609.09106.",
      "citeRegEx" : "Ha et al\\.,? 2016",
      "shortCiteRegEx" : "Ha et al\\.",
      "year" : 2016
    }, {
      "title" : "Improved word sense disambiguation using pre-trained contextualized word representations",
      "author" : [ "Christian Hadiwinoto", "Hwee Tou Ng", "Wee Chung Gan." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Hadiwinoto et al\\.,? 2019",
      "shortCiteRegEx" : "Hadiwinoto et al\\.",
      "year" : 2019
    }, {
      "title" : "Continual relation learning via episodic memory activation and reconsolidation",
      "author" : [ "Xu Han", "Yi Dai", "Tianyu Gao", "Yankai Lin", "Zhiyuan Liu", "Peng Li", "Maosong Sun", "Jie Zhou." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Han et al\\.,? 2020",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2020
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Learning to learn to disambiguate: Meta-learning for few-shot word sense disambiguation",
      "author" : [ "Nithin Holla", "Pushkar Mishra", "Helen Yannakoudakis", "Ekaterina Shutova." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Holla et al\\.,? 2020a",
      "shortCiteRegEx" : "Holla et al\\.",
      "year" : 2020
    }, {
      "title" : "Meta-learning with sparse experience replay for lifelong language learning",
      "author" : [ "Nithin Holla", "Pushkar Mishra", "Helen Yannakoudakis", "Ekaterina Shutova." ],
      "venue" : "arXiv preprint arXiv:2009.04891.",
      "citeRegEx" : "Holla et al\\.,? 2020b",
      "shortCiteRegEx" : "Holla et al\\.",
      "year" : 2020
    }, {
      "title" : "GlossBERT: BERT for word sense disambiguation with gloss knowledge",
      "author" : [ "Luyao Huang", "Chi Sun", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
      "citeRegEx" : "Huang et al\\.,? 2019",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "Embeddings for word sense disambiguation: An evaluation study",
      "author" : [ "Ignacio Iacobacci", "Mohammad Taher Pilehvar", "Roberto Navigli." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association",
      "citeRegEx" : "Iacobacci et al\\.,? 2016",
      "shortCiteRegEx" : "Iacobacci et al\\.",
      "year" : 2016
    }, {
      "title" : "Word sense disambiguation using a bidirectional LSTM",
      "author" : [ "Mikael Kågebäck", "Hans Salomonsson." ],
      "venue" : "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon (CogALex - V), pages 51–56, Osaka, Japan. The COLING 2016 Organizing Com-",
      "citeRegEx" : "Kågebäck and Salomonsson.,? 2016",
      "shortCiteRegEx" : "Kågebäck and Salomonsson.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Autoencoding variational bayes",
      "author" : [ "Diederik P Kingma", "Max Welling." ],
      "venue" : "arXiv preprint arXiv:1312.6114.",
      "citeRegEx" : "Kingma and Welling.,? 2013",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2013
    }, {
      "title" : "Siamese neural networks for one-shot image recognition",
      "author" : [ "Gregory Koch", "Richard Zemel", "Ruslan Salakhutdinov." ],
      "venue" : "ICML deep learning workshop, volume 2. Lille.",
      "citeRegEx" : "Koch et al\\.,? 2015",
      "shortCiteRegEx" : "Koch et al\\.",
      "year" : 2015
    }, {
      "title" : "Dense associative memory for pattern recognition",
      "author" : [ "Dmitry Krotov", "John J Hopfield." ],
      "venue" : "arXiv preprint arXiv:1606.01164.",
      "citeRegEx" : "Krotov and Hopfield.,? 2016",
      "shortCiteRegEx" : "Krotov and Hopfield.",
      "year" : 2016
    }, {
      "title" : "Zero-shot word sense disambiguation using sense definition embeddings",
      "author" : [ "Sawan Kumar", "Sharmistha Jat", "Karan Saxena", "Partha Talukdar." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "Kumar et al\\.,? 2019",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone",
      "author" : [ "Michael Lesk." ],
      "venue" : "Proceedings of the 5th Annual International Conference on Systems Documentation, SIGDOC ’86, page 24–26,",
      "citeRegEx" : "Lesk.,? 1986",
      "shortCiteRegEx" : "Lesk.",
      "year" : 1986
    }, {
      "title" : "Learn to cross-lingual transfer with meta graph learning across heterogeneous languages",
      "author" : [ "Zheng Li", "Mukul Kumar", "William Headden", "Bing Yin", "Ying Wei", "Yu Zhang", "Qiang Yang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Visualizing data using t-sne",
      "author" : [ "Laurens Van der Maaten", "Geoffrey Hinton." ],
      "venue" : "Journal of machine learning research, 9(11).",
      "citeRegEx" : "Maaten and Hinton.,? 2008",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    }, {
      "title" : "Mem2Seq: Effectively incorporating knowledge bases into end-to-end task-oriented dialog systems",
      "author" : [ "Andrea Madotto", "Chien-Sheng Wu", "Pascale Fung." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Vol-",
      "citeRegEx" : "Madotto et al\\.,? 2018",
      "shortCiteRegEx" : "Madotto et al\\.",
      "year" : 2018
    }, {
      "title" : "context2vec: Learning generic context embedding with bidirectional LSTM",
      "author" : [ "Oren Melamud", "Jacob Goldberger", "Ido Dagan." ],
      "venue" : "Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pages 51–61, Berlin,",
      "citeRegEx" : "Melamud et al\\.,? 2016",
      "shortCiteRegEx" : "Melamud et al\\.",
      "year" : 2016
    }, {
      "title" : "Wordnet: An on-line lexical database",
      "author" : [ "George A. Miller", "Richard Beckwith", "Christiane Fellbaum", "Derek Gross", "Katherine Miller." ],
      "venue" : "International Journal of Lexicography, 3:235–244.",
      "citeRegEx" : "Miller et al\\.,? 1990",
      "shortCiteRegEx" : "Miller et al\\.",
      "year" : 1990
    }, {
      "title" : "Using a semantic concordance for sense identification",
      "author" : [ "George A. Miller", "Martin Chodorow", "Shari Landes", "Claudia Leacock", "Robert G. Thomas." ],
      "venue" : "Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-",
      "citeRegEx" : "Miller et al\\.,? 1994",
      "shortCiteRegEx" : "Miller et al\\.",
      "year" : 1994
    }, {
      "title" : "Entity linking meets word sense disambiguation: a unified approach",
      "author" : [ "Andrea Moro", "Alessandro Raganato", "Roberto Navigli." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 2:231– 244.",
      "citeRegEx" : "Moro et al\\.,? 2014",
      "shortCiteRegEx" : "Moro et al\\.",
      "year" : 2014
    }, {
      "title" : "Metalearned neural memory",
      "author" : [ "Tsendsuren Munkhdalai", "Alessandro Sordoni", "Tong Wang", "Adam Trischler." ],
      "venue" : "Advanced in Neural Information Processing Systems.",
      "citeRegEx" : "Munkhdalai et al\\.,? 2019",
      "shortCiteRegEx" : "Munkhdalai et al\\.",
      "year" : 2019
    }, {
      "title" : "Meta networks",
      "author" : [ "Tsendsuren Munkhdalai", "Hong Yu." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 2554–2563, International Convention Centre, Syd-",
      "citeRegEx" : "Munkhdalai and Yu.,? 2017a",
      "shortCiteRegEx" : "Munkhdalai and Yu.",
      "year" : 2017
    }, {
      "title" : "Meta networks",
      "author" : [ "Tsendsuren Munkhdalai", "Hong Yu." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, Proceedings of Machine Learning Research, pages 2554–2563, International Convention Centre, Sydney, Australia.",
      "citeRegEx" : "Munkhdalai and Yu.,? 2017b",
      "shortCiteRegEx" : "Munkhdalai and Yu.",
      "year" : 2017
    }, {
      "title" : "Rapid adaptation with conditionally shifted neurons",
      "author" : [ "Tsendsuren Munkhdalai", "Xingdi Yuan", "Soroush Mehri", "Adam Trischler." ],
      "venue" : "International Conference on Machine Learning, pages 3664–3673. PMLR.",
      "citeRegEx" : "Munkhdalai et al\\.,? 2018",
      "shortCiteRegEx" : "Munkhdalai et al\\.",
      "year" : 2018
    }, {
      "title" : "Word sense disambiguation: A survey",
      "author" : [ "Roberto Navigli." ],
      "venue" : "ACM Computing Surveys, 41(2):1–69.",
      "citeRegEx" : "Navigli.,? 2009",
      "shortCiteRegEx" : "Navigli.",
      "year" : 2009
    }, {
      "title" : "On first-order meta-learning algorithms",
      "author" : [ "Alex Nichol", "Joshua Achiam", "John Schulman." ],
      "venue" : "arXiv preprint arXiv:1803.02999.",
      "citeRegEx" : "Nichol et al\\.,? 2018",
      "shortCiteRegEx" : "Nichol et al\\.",
      "year" : 2018
    }, {
      "title" : "Zero-shot cross-lingual transfer with meta learning",
      "author" : [ "Farhad Nooralahzadeh", "Giannis Bekoulis", "Johannes Bjerva", "Isabelle Augenstein." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Nooralahzadeh et al\\.,? 2020",
      "shortCiteRegEx" : "Nooralahzadeh et al\\.",
      "year" : 2020
    }, {
      "title" : "Model-agnostic meta-learning for relation classification with limited supervision",
      "author" : [ "Abiola Obamuyide", "Andreas Vlachos." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5873–5879, Florence,",
      "citeRegEx" : "Obamuyide and Vlachos.,? 2019",
      "shortCiteRegEx" : "Obamuyide and Vlachos.",
      "year" : 2019
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar. Asso-",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Domain adaptive dialog generation via meta learning",
      "author" : [ "Kun Qian", "Zhou Yu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2639–2649, Florence, Italy. Association for Computational Linguistics.",
      "citeRegEx" : "Qian and Yu.,? 2019",
      "shortCiteRegEx" : "Qian and Yu.",
      "year" : 2019
    }, {
      "title" : "Word sense disambiguation: A unified evaluation framework and empirical comparison",
      "author" : [ "Alessandro Raganato", "Jose Camacho-Collados", "Roberto Navigli." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association",
      "citeRegEx" : "Raganato et al\\.,? 2017a",
      "shortCiteRegEx" : "Raganato et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural sequence learning models for word sense disambiguation",
      "author" : [ "Alessandro Raganato", "Claudio Delli Bovi", "Roberto Navigli." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1156–1167,",
      "citeRegEx" : "Raganato et al\\.,? 2017b",
      "shortCiteRegEx" : "Raganato et al\\.",
      "year" : 2017
    }, {
      "title" : "Optimization as a model for few-shot learning",
      "author" : [ "Sachin Ravi", "Hugo Larochelle." ],
      "venue" : "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.",
      "citeRegEx" : "Ravi and Larochelle.,? 2017",
      "shortCiteRegEx" : "Ravi and Larochelle.",
      "year" : 2017
    }, {
      "title" : "Cognitive representations of semantic categories",
      "author" : [ "Eleanor Rosch." ],
      "venue" : "Journal of Experimental Psychology: General, 104:192–233.",
      "citeRegEx" : "Rosch.,? 1975",
      "shortCiteRegEx" : "Rosch.",
      "year" : 1975
    }, {
      "title" : "AutoExtend: Extending word embeddings to embeddings for synsets and lexemes",
      "author" : [ "Sascha Rothe", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
      "citeRegEx" : "Rothe and Schütze.,? 2015",
      "shortCiteRegEx" : "Rothe and Schütze.",
      "year" : 2015
    }, {
      "title" : "Dynamic routing between capsules",
      "author" : [ "Sara Sabour", "Nicholas Frosst", "Geoffrey E Hinton." ],
      "venue" : "Advances in Neural Information Processing Systems 30, pages 3856–3866.",
      "citeRegEx" : "Sabour et al\\.,? 2017",
      "shortCiteRegEx" : "Sabour et al\\.",
      "year" : 2017
    }, {
      "title" : "Metalearning with memory-augmented neural networks",
      "author" : [ "Adam Santoro", "Sergey Bartunov", "Matthew Botvinick", "Daan Wierstra", "Timothy Lillicrap." ],
      "venue" : "International conference on machine learning, pages 1842–1850. PMLR.",
      "citeRegEx" : "Santoro et al\\.,? 2016a",
      "shortCiteRegEx" : "Santoro et al\\.",
      "year" : 2016
    }, {
      "title" : "Metalearning with memory-augmented neural networks",
      "author" : [ "Adam Santoro", "Sergey Bartunov", "Matthew Botvinick", "Daan Wierstra", "Timothy Lillicrap." ],
      "venue" : "Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceed-",
      "citeRegEx" : "Santoro et al\\.,? 2016b",
      "shortCiteRegEx" : "Santoro et al\\.",
      "year" : 2016
    }, {
      "title" : "With more contexts comes better performance: Contextualized sense embeddings for all-round word sense disambiguation",
      "author" : [ "Bianca Scarlini", "Tommaso Pasini", "Roberto Navigli." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods",
      "citeRegEx" : "Scarlini et al\\.,? 2020",
      "shortCiteRegEx" : "Scarlini et al\\.",
      "year" : 2020
    }, {
      "title" : "Evolutionary principles in self-referential learning",
      "author" : [ "Jurgen Schmidhuber." ],
      "venue" : "on learning now to learn: The meta-meta-meta...-hook. Diploma thesis, Technische Universitat Munchen, Germany, 14 May.",
      "citeRegEx" : "Schmidhuber.,? 1987",
      "shortCiteRegEx" : "Schmidhuber.",
      "year" : 1987
    }, {
      "title" : "Prototypical networks for few-shot learning",
      "author" : [ "Jake Snell", "Kevin Swersky", "Richard Zemel." ],
      "venue" : "Advances in Neural Information Processing Systems 30, pages 4077–4087.",
      "citeRegEx" : "Snell et al\\.,? 2017",
      "shortCiteRegEx" : "Snell et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning to compare: Relation network for few-shot learning",
      "author" : [ "Flood Sung", "Yongxin Yang", "Li Zhang", "Tao Xiang", "Philip HS Torr", "Timothy M Hospedales." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages",
      "citeRegEx" : "Sung et al\\.,? 2018",
      "shortCiteRegEx" : "Sung et al\\.",
      "year" : 2018
    }, {
      "title" : "Aspect level sentiment classification with deep memory network",
      "author" : [ "Duyu Tang", "Bing Qin", "Ting Liu." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 214–224, Austin, Texas. Association for Com-",
      "citeRegEx" : "Tang et al\\.,? 2016",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to Learn",
      "author" : [ "Sebastian Thrun", "Lorien Pratt", "editors" ],
      "venue" : null,
      "citeRegEx" : "Thrun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Thrun et al\\.",
      "year" : 1998
    }, {
      "title" : "Meta-dataset: A dataset of datasets for learning",
      "author" : [ "Eleni Triantafillou", "Tyler Zhu", "Vincent Dumoulin", "Pascal Lamblin", "Utku Evci", "Kelvin Xu", "Ross Goroshin", "Carles Gelada", "Kevin Swersky", "Pierre-Antoine Manzagol", "Hugo Larochelle" ],
      "venue" : null,
      "citeRegEx" : "Triantafillou et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Triantafillou et al\\.",
      "year" : 2020
    }, {
      "title" : "Graph attention networks",
      "author" : [ "Petar Veličković", "Guillem Cucurull", "Arantxa Casanova", "Adriana Romero", "Pietro Lio", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1710.10903.",
      "citeRegEx" : "Veličković et al\\.,? 2017",
      "shortCiteRegEx" : "Veličković et al\\.",
      "year" : 2017
    }, {
      "title" : "Matching networks for one shot learning",
      "author" : [ "Oriol Vinyals", "Charles Blundell", "Timothy Lillicrap", "Koray Kavukcuoglu", "Daan Wierstra." ],
      "venue" : "Advances in Neural Information Processing Systems 29, pages 3630–3638.",
      "citeRegEx" : "Vinyals et al\\.,? 2016",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2016
    }, {
      "title" : "Pointer networks",
      "author" : [ "Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 28, pages 2692–2700. Curran Associates, Inc.",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Memory networks",
      "author" : [ "Jason Weston", "Sumit Chopra", "Antoine Bordes." ],
      "venue" : "arXiv preprint arXiv:1410.3916.",
      "citeRegEx" : "Weston et al\\.,? 2014",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2014
    }, {
      "title" : "Hypernymy detection for lowresource languages via meta learning",
      "author" : [ "Changlong Yu", "Jialong Han", "Haisong Zhang", "Wilfred Ng." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3651–3656,",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Diverse few-shot text classification with multiple metrics",
      "author" : [ "Mo Yu", "Xiaoxiao Guo", "Jinfeng Yi", "Shiyu Chang", "Saloni Potdar", "Yu Cheng", "Gerald Tesauro", "Haoyu Wang", "Bowen Zhou." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chap-",
      "citeRegEx" : "Yu et al\\.,? 2018",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2018
    }, {
      "title" : "Semi-supervised word sense disambiguation with neural models",
      "author" : [ "Dayu Yuan", "Julian Richardson", "Ryan Doherty", "Colin Evans", "Eric Altendorf." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics:",
      "citeRegEx" : "Yuan et al\\.,? 2016",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to learn variational semantic memory",
      "author" : [ "Xiantong Zhen", "Yingjun Du", "Huan Xiong", "Qiang Qiu", "Cees Snoek", "Ling Shao." ],
      "venue" : "Proceedings of NeurIPS.",
      "citeRegEx" : "Zhen et al\\.,? 2020",
      "shortCiteRegEx" : "Zhen et al\\.",
      "year" : 2020
    }, {
      "title" : "It makes sense: A wide-coverage word sense disambiguation system for free text",
      "author" : [ "Zhi Zhong", "Hwee Tou Ng." ],
      "venue" : "Proceedings of the ACL 2010 System Demonstrations, pages 78–83, Uppsala, Sweden. Association for Computational Linguistics.",
      "citeRegEx" : "Zhong and Ng.,? 2010",
      "shortCiteRegEx" : "Zhong and Ng.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 71,
      "context" : "The majority of approaches to WSD rely on (semi-)supervised learning (Yuan et al., 2016; Raganato et al., 2017a,b; Hadiwinoto et al., 2019; Huang et al., 2019; Scarlini et al., 2020; Bevilacqua and Navigli, 2020) and make use of training corpora manually annotated for word senses.",
      "startOffset" : 69,
      "endOffset" : 212
    }, {
      "referenceID" : 18,
      "context" : "The majority of approaches to WSD rely on (semi-)supervised learning (Yuan et al., 2016; Raganato et al., 2017a,b; Hadiwinoto et al., 2019; Huang et al., 2019; Scarlini et al., 2020; Bevilacqua and Navigli, 2020) and make use of training corpora manually annotated for word senses.",
      "startOffset" : 69,
      "endOffset" : 212
    }, {
      "referenceID" : 23,
      "context" : "The majority of approaches to WSD rely on (semi-)supervised learning (Yuan et al., 2016; Raganato et al., 2017a,b; Hadiwinoto et al., 2019; Huang et al., 2019; Scarlini et al., 2020; Bevilacqua and Navigli, 2020) and make use of training corpora manually annotated for word senses.",
      "startOffset" : 69,
      "endOffset" : 212
    }, {
      "referenceID" : 58,
      "context" : "The majority of approaches to WSD rely on (semi-)supervised learning (Yuan et al., 2016; Raganato et al., 2017a,b; Hadiwinoto et al., 2019; Huang et al., 2019; Scarlini et al., 2020; Bevilacqua and Navigli, 2020) and make use of training corpora manually annotated for word senses.",
      "startOffset" : 69,
      "endOffset" : 212
    }, {
      "referenceID" : 5,
      "context" : "The majority of approaches to WSD rely on (semi-)supervised learning (Yuan et al., 2016; Raganato et al., 2017a,b; Hadiwinoto et al., 2019; Huang et al., 2019; Scarlini et al., 2020; Bevilacqua and Navigli, 2020) and make use of training corpora manually annotated for word senses.",
      "startOffset" : 69,
      "endOffset" : 212
    }, {
      "referenceID" : 60,
      "context" : "tasks and is explicitly optimized to perform well when learning from a small number of examples per task (Snell et al., 2017; Finn et al., 2017; Triantafillou et al., 2020).",
      "startOffset" : 105,
      "endOffset" : 172
    }, {
      "referenceID" : 12,
      "context" : "tasks and is explicitly optimized to perform well when learning from a small number of examples per task (Snell et al., 2017; Finn et al., 2017; Triantafillou et al., 2020).",
      "startOffset" : 105,
      "endOffset" : 172
    }, {
      "referenceID" : 64,
      "context" : "tasks and is explicitly optimized to perform well when learning from a small number of examples per task (Snell et al., 2017; Finn et al., 2017; Triantafillou et al., 2020).",
      "startOffset" : 105,
      "endOffset" : 172
    }, {
      "referenceID" : 56,
      "context" : "In the meantime, machine learning research demonstrated the advantages of a memory component for meta-learning in limited data settings (Santoro et al., 2016a; Munkhdalai and Yu, 2017a; Munkhdalai et al., 2018; Zhen et al., 2020).",
      "startOffset" : 136,
      "endOffset" : 229
    }, {
      "referenceID" : 40,
      "context" : "In the meantime, machine learning research demonstrated the advantages of a memory component for meta-learning in limited data settings (Santoro et al., 2016a; Munkhdalai and Yu, 2017a; Munkhdalai et al., 2018; Zhen et al., 2020).",
      "startOffset" : 136,
      "endOffset" : 229
    }, {
      "referenceID" : 42,
      "context" : "In the meantime, machine learning research demonstrated the advantages of a memory component for meta-learning in limited data settings (Santoro et al., 2016a; Munkhdalai and Yu, 2017a; Munkhdalai et al., 2018; Zhen et al., 2020).",
      "startOffset" : 136,
      "endOffset" : 229
    }, {
      "referenceID" : 72,
      "context" : "In the meantime, machine learning research demonstrated the advantages of a memory component for meta-learning in limited data settings (Santoro et al., 2016a; Munkhdalai and Yu, 2017a; Munkhdalai et al., 2018; Zhen et al., 2020).",
      "startOffset" : 136,
      "endOffset" : 229
    }, {
      "referenceID" : 72,
      "context" : "tion of new concepts and recognition of previously unseen classes with limited labeled data (Zhen et al., 2020).",
      "startOffset" : 92,
      "endOffset" : 111
    }, {
      "referenceID" : 60,
      "context" : "In meta-learning, prototypes are embeddings around which other data points of the same class are clustered (Snell et al., 2017).",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 53,
      "context" : "The design of our prototypical representation of word sense takes inspiration from prototype theory (Rosch, 1975), an established account of category representation in psychology.",
      "startOffset" : 100,
      "endOffset" : 113
    }, {
      "referenceID" : 72,
      "context" : "this, we employ a neural architecture for learning probabilistic class prototypes: variational prototype networks, augmented with a variational semantic memory (VSM) component (Zhen et al., 2020).",
      "startOffset" : 176,
      "endOffset" : 195
    }, {
      "referenceID" : 60,
      "context" : "Unlike deterministic prototypes in prototypical networks (Snell et al., 2017), we model class proto-",
      "startOffset" : 57,
      "endOffset" : 77
    }, {
      "referenceID" : 57,
      "context" : "Unlike deterministic memory access in memory-based meta-learning (Santoro et al., 2016b; Munkhdalai and Yu, 2017a), we access",
      "startOffset" : 65,
      "endOffset" : 114
    }, {
      "referenceID" : 40,
      "context" : "Unlike deterministic memory access in memory-based meta-learning (Santoro et al., 2016b; Munkhdalai and Yu, 2017a), we access",
      "startOffset" : 65,
      "endOffset" : 114
    }, {
      "referenceID" : 17,
      "context" : "Furthermore, we enhance the memory update of vanilla VSM with a novel adaptive update rule involving a hypernetwork (Ha et al., 2016) that controls the weight of the updates.",
      "startOffset" : 116,
      "endOffset" : 133
    }, {
      "referenceID" : 36,
      "context" : "such as WordNet (Miller et al., 1990) and do not require a corpus manually annotated with word senses.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 43,
      "context" : "Early supervised learning approaches trained classifiers with hand-crafted features (Navigli, 2009; Zhong and Ng, 2010) and word embeddings (Rothe and Schütze, 2015; Iacobacci et al.",
      "startOffset" : 84,
      "endOffset" : 119
    }, {
      "referenceID" : 73,
      "context" : "Early supervised learning approaches trained classifiers with hand-crafted features (Navigli, 2009; Zhong and Ng, 2010) and word embeddings (Rothe and Schütze, 2015; Iacobacci et al.",
      "startOffset" : 84,
      "endOffset" : 119
    }, {
      "referenceID" : 54,
      "context" : "Early supervised learning approaches trained classifiers with hand-crafted features (Navigli, 2009; Zhong and Ng, 2010) and word embeddings (Rothe and Schütze, 2015; Iacobacci et al., 2016) as input.",
      "startOffset" : 140,
      "endOffset" : 189
    }, {
      "referenceID" : 24,
      "context" : "Early supervised learning approaches trained classifiers with hand-crafted features (Navigli, 2009; Zhong and Ng, 2010) and word embeddings (Rothe and Schütze, 2015; Iacobacci et al., 2016) as input.",
      "startOffset" : 140,
      "endOffset" : 189
    }, {
      "referenceID" : 37,
      "context" : "the SemCor corpus (Miller et al., 1994) and found that supervised methods outperform the knowledgebased ones.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 20,
      "context" : "Neural models for supervised WSD include LSTM-based (Hochreiter and Schmidhuber, 1997) classifiers (Kågebäck and Salomonsson, 2016;",
      "startOffset" : 52,
      "endOffset" : 86
    }, {
      "referenceID" : 48,
      "context" : ", 2017b), nearest neighbour classifier with ELMo embeddings (Peters et al., 2018), as well as a classifier based on pretrained BERT representations (Hadiwinoto et al.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 18,
      "context" : ", 2018), as well as a classifier based on pretrained BERT representations (Hadiwinoto et al., 2019).",
      "startOffset" : 74,
      "endOffset" : 99
    }, {
      "referenceID" : 23,
      "context" : "GlossBERT (Huang et al., 2019) fine-tunes BERT with WordNet sense definitions as additional input.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 5,
      "context" : "By further exploiting the graph structure of WordNet and integrating it with BERT, EWISER (Bevilacqua and Navigli, 2020) achieves the current state-of-the-art performance on the benchmark by Raganato et al.",
      "startOffset" : 90,
      "endOffset" : 120
    }, {
      "referenceID" : 59,
      "context" : "Meta-learning Meta-learning, or learning to learn (Schmidhuber, 1987; Bengio et al., 1991; Thrun and Pratt, 1998), is a learning paradigm where a model is trained on a distribution of tasks so as to enable rapid learning on new tasks.",
      "startOffset" : 50,
      "endOffset" : 113
    }, {
      "referenceID" : 4,
      "context" : "Meta-learning Meta-learning, or learning to learn (Schmidhuber, 1987; Bengio et al., 1991; Thrun and Pratt, 1998), is a learning paradigm where a model is trained on a distribution of tasks so as to enable rapid learning on new tasks.",
      "startOffset" : 50,
      "endOffset" : 113
    }, {
      "referenceID" : 28,
      "context" : "Metric-based meta-learning methods (Koch et al., 2015; Vinyals et al., 2016; Sung et al., 2018; Snell et al., 2017) learn a kernel function and make predictions on the query set based on the similarity",
      "startOffset" : 35,
      "endOffset" : 115
    }, {
      "referenceID" : 66,
      "context" : "Metric-based meta-learning methods (Koch et al., 2015; Vinyals et al., 2016; Sung et al., 2018; Snell et al., 2017) learn a kernel function and make predictions on the query set based on the similarity",
      "startOffset" : 35,
      "endOffset" : 115
    }, {
      "referenceID" : 61,
      "context" : "Metric-based meta-learning methods (Koch et al., 2015; Vinyals et al., 2016; Sung et al., 2018; Snell et al., 2017) learn a kernel function and make predictions on the query set based on the similarity",
      "startOffset" : 35,
      "endOffset" : 115
    }, {
      "referenceID" : 60,
      "context" : "Metric-based meta-learning methods (Koch et al., 2015; Vinyals et al., 2016; Sung et al., 2018; Snell et al., 2017) learn a kernel function and make predictions on the query set based on the similarity",
      "startOffset" : 35,
      "endOffset" : 115
    }, {
      "referenceID" : 57,
      "context" : "Model-based methods (Santoro et al., 2016b; Munkhdalai and Yu, 2017a) employ external memory and make predictions based on examples retrieved from the memory.",
      "startOffset" : 20,
      "endOffset" : 69
    }, {
      "referenceID" : 40,
      "context" : "Model-based methods (Santoro et al., 2016b; Munkhdalai and Yu, 2017a) employ external memory and make predictions based on examples retrieved from the memory.",
      "startOffset" : 20,
      "endOffset" : 69
    }, {
      "referenceID" : 16,
      "context" : "Meta-learning has been applied to a range of tasks in NLP, including machine translation (Gu et al., 2018), relation classification (Obamuyide and Vlachos, 2019), text classification (Yu et al.",
      "startOffset" : 89,
      "endOffset" : 106
    }, {
      "referenceID" : 46,
      "context" : ", 2018), relation classification (Obamuyide and Vlachos, 2019), text classification (Yu et al.",
      "startOffset" : 33,
      "endOffset" : 62
    }, {
      "referenceID" : 70,
      "context" : ", 2018), relation classification (Obamuyide and Vlachos, 2019), text classification (Yu et al., 2018; Geng et al., 2019), hypernymy detection (Yu et al.",
      "startOffset" : 84,
      "endOffset" : 120
    }, {
      "referenceID" : 14,
      "context" : ", 2018), relation classification (Obamuyide and Vlachos, 2019), text classification (Yu et al., 2018; Geng et al., 2019), hypernymy detection (Yu et al.",
      "startOffset" : 84,
      "endOffset" : 120
    }, {
      "referenceID" : 69,
      "context" : ", 2019), hypernymy detection (Yu et al., 2020), and dialog generation (Qian and Yu, 2019).",
      "startOffset" : 29,
      "endOffset" : 46
    }, {
      "referenceID" : 11,
      "context" : "It has also been used to learn across distinct NLP tasks (Dou et al., 2019; Bansal et al., 2019) as well as across different languages (Nooralahzadeh et al.",
      "startOffset" : 57,
      "endOffset" : 96
    }, {
      "referenceID" : 2,
      "context" : "It has also been used to learn across distinct NLP tasks (Dou et al., 2019; Bansal et al., 2019) as well as across different languages (Nooralahzadeh et al.",
      "startOffset" : 57,
      "endOffset" : 96
    }, {
      "referenceID" : 45,
      "context" : ", 2019) as well as across different languages (Nooralahzadeh et al., 2020; Li et al., 2020).",
      "startOffset" : 46,
      "endOffset" : 91
    }, {
      "referenceID" : 32,
      "context" : ", 2019) as well as across different languages (Nooralahzadeh et al., 2020; Li et al., 2020).",
      "startOffset" : 46,
      "endOffset" : 91
    }, {
      "referenceID" : 60,
      "context" : "They show that prototype-based methods – prototypical networks (Snell et al., 2017) and first-order ProtoMAML (Triantafillou et al.",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 64,
      "context" : ", 2017) and first-order ProtoMAML (Triantafillou et al., 2020) – obtain",
      "startOffset" : 34,
      "endOffset" : 62
    }, {
      "referenceID" : 12,
      "context" : "promising results, in contrast with model-agnostic meta-learning (MAML) (Finn et al., 2017).",
      "startOffset" : 72,
      "endOffset" : 91
    }, {
      "referenceID" : 68,
      "context" : "Memory-based models Memory mechanisms (Weston et al., 2014; Graves et al., 2014; Krotov and Hopfield, 2016) have recently drawn increas-",
      "startOffset" : 38,
      "endOffset" : 107
    }, {
      "referenceID" : 15,
      "context" : "Memory-based models Memory mechanisms (Weston et al., 2014; Graves et al., 2014; Krotov and Hopfield, 2016) have recently drawn increas-",
      "startOffset" : 38,
      "endOffset" : 107
    }, {
      "referenceID" : 29,
      "context" : "Memory-based models Memory mechanisms (Weston et al., 2014; Graves et al., 2014; Krotov and Hopfield, 2016) have recently drawn increas-",
      "startOffset" : 38,
      "endOffset" : 107
    }, {
      "referenceID" : 57,
      "context" : "In memory-augmented neural network (Santoro et al., 2016b), given an input, the memory read and write operations are performed by a controller, using soft attention for reads and least recently used access module for writes.",
      "startOffset" : 35,
      "endOffset" : 58
    }, {
      "referenceID" : 41,
      "context" : "Network (Munkhdalai and Yu, 2017b) uses two memory modules: a key-value memory in combination with slow and fast weights for one-shot learning.",
      "startOffset" : 8,
      "endOffset" : 34
    }, {
      "referenceID" : 34,
      "context" : "Mem2Seq (Madotto et al., 2018) is an architecture for task-oriented dialog that combines attention-based memory with pointer networks (Vinyals et al.",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 67,
      "context" : ", 2018) is an architecture for task-oriented dialog that combines attention-based memory with pointer networks (Vinyals et al., 2015).",
      "startOffset" : 111,
      "endOffset" : 133
    }, {
      "referenceID" : 55,
      "context" : "(2020) propose Dynamic Memory Induction Networks for few-shot text classification, which utilizes dynamic routing (Sabour et al., 2017) over a static memory module.",
      "startOffset" : 114,
      "endOffset" : 135
    }, {
      "referenceID" : 9,
      "context" : "Episodic memory has been used in lifelong learning on language tasks, as a means to perform experience replay (d’Autume et al., 2019; Han et al., 2020; Holla et al., 2020b).",
      "startOffset" : 110,
      "endOffset" : 172
    }, {
      "referenceID" : 19,
      "context" : "Episodic memory has been used in lifelong learning on language tasks, as a means to perform experience replay (d’Autume et al., 2019; Han et al., 2020; Holla et al., 2020b).",
      "startOffset" : 110,
      "endOffset" : 172
    }, {
      "referenceID" : 22,
      "context" : "Episodic memory has been used in lifelong learning on language tasks, as a means to perform experience replay (d’Autume et al., 2019; Han et al., 2020; Holla et al., 2020b).",
      "startOffset" : 110,
      "endOffset" : 172
    }, {
      "referenceID" : 21,
      "context" : "It deviates from the standard N -way, K-shot classification setting in few-shot learning since the words may have a different number of senses and each sense may have different number of examples (Holla et al., 2020a), making it a more realistic few-shot learning setup (Triantafillou et al.",
      "startOffset" : 196,
      "endOffset" : 217
    }, {
      "referenceID" : 64,
      "context" : ", 2020a), making it a more realistic few-shot learning setup (Triantafillou et al., 2020).",
      "startOffset" : 61,
      "endOffset" : 89
    }, {
      "referenceID" : 37,
      "context" : "It is based on the SemCor corpus (Miller et al., 1994), annotated",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 6,
      "context" : "GloVe+GRU Single-layer bi-directional GRU (Cho et al., 2014) network followed by a single linear layer, that takes GloVe embeddings (Pennington et al.",
      "startOffset" : 42,
      "endOffset" : 60
    }, {
      "referenceID" : 47,
      "context" : ", 2014) network followed by a single linear layer, that takes GloVe embeddings (Pennington et al., 2014) as input.",
      "startOffset" : 79,
      "endOffset" : 104
    }, {
      "referenceID" : 48,
      "context" : "ELMo+MLP A multi-layer perception (MLP) network that receives contextualized ELMo embeddings (Peters et al., 2018) as input.",
      "startOffset" : 93,
      "endOffset" : 114
    }, {
      "referenceID" : 10,
      "context" : "BERT Pretrained BERTBASE (Devlin et al., 2019) model followed by a linear layer, fully finetuned on the task.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 60,
      "context" : "Our few-shot learning approach builds upon prototypical networks (Snell et al., 2017), which is widely used for few-shot image classification and has been shown to be successful in WSD (Holla et al.",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 21,
      "context" : ", 2017), which is widely used for few-shot image classification and has been shown to be successful in WSD (Holla et al., 2020a).",
      "startOffset" : 107,
      "endOffset" : 128
    }, {
      "referenceID" : 27,
      "context" : "The re-parameterization trick (Kingma and Welling, 2013) is adopted to enable back-propagation with gradient descent, i.",
      "startOffset" : 30,
      "endOffset" : 56
    }, {
      "referenceID" : 65,
      "context" : "where Mc is the memory content corresponding to class c, M̄c is obtained using graph attention (Veličković et al., 2017), and β ∈ (0, 1) is a",
      "startOffset" : 95,
      "endOffset" : 120
    }, {
      "referenceID" : 17,
      "context" : "we propose an adaptive memory update rule by learning β from data using a lightweight hypernetwork (Ha et al., 2016).",
      "startOffset" : 99,
      "endOffset" : 116
    }, {
      "referenceID" : 7,
      "context" : "The activation function of their hidden layers is ELU (Clevert et al., 2016), and the output layer does not use any activation",
      "startOffset" : 54,
      "endOffset" : 76
    }, {
      "referenceID" : 26,
      "context" : "The parameters are optimized using Adam (Kingma and Ba, 2014).",
      "startOffset" : 40,
      "endOffset" : 61
    } ],
    "year" : 2021,
    "abstractText" : "A critical challenge faced by supervised word sense disambiguation (WSD) is the lack of large annotated datasets with sufficient coverage of words in their diversity of senses. This inspired recent research on few-shot WSD using meta-learning. While such work has successfully applied meta-learning to learn new word senses from very few examples, its performance still lags behind its fully-supervised counterpart. Aiming to further close this gap, we propose a model of semantic memory for WSD in a meta-learning setting. Semantic memory encapsulates prior experiences seen throughout the lifetime of the model, which aids better generalization in limited data settings. Our model is based on hierarchical variational inference and incorporates an adaptive memory update rule via a hypernetwork. We show our model advances the state of the art in few-shot WSD, supports effective learning in extremely data scarce (e.g. one-shot) scenarios and produces meaning prototypes that capture similar senses of distinct words.",
    "creator" : "LaTeX with hyperref"
  }
}