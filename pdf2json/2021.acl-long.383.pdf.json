{
  "name" : "2021.acl-long.383.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Personalized Transformer for Explainable Recommendation",
    "authors" : [ "Lei Li", "Yongfeng Zhang", "Li Chen" ],
    "emails" : [ "csleili@comp.hkbu.edu.hk", "lichen@comp.hkbu.edu.hk", "yongfeng.zhang@rutgers.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4947–4957\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4947"
    }, {
      "heading" : "1 Introduction",
      "text" : "Recent years have witnessed the successful application of natural language generation. Many of the applications in fact require certain degree of personalization, such as explainable recommendation (Zhang et al., 2014; Li et al., 2020c; Zhang and Chen, 2020), review generation (Dong et al., 2017), review summarization (Li et al., 2019), and conversational systems (Zhang et al., 2018; Chen et al., 2020). In these tasks, user and item IDs that distinguish one user/item from the others are crucial to\n1https://github.com/lileipisces/PETER\npersonalization. For example, in recommender systems, different users may care about different item features (e.g., style vs. quality), and different items may have different characteristics (e.g., fashionable vs. comfortable). The goal of explainable recommendation (Zhang and Chen, 2020) is to provide an explanation to a user for a recommended item, so as to justify how the recommendation might match his/her interests. That is, given a pair of user ID and item ID, the system needs to generate an explanation, such as “the style of the jacket is fashionable” (see the last column of Table 4 for more examples).\nTransformer (Vaswani et al., 2017), whose strong language modeling ability has been demonstrated on a variety of tasks (Radford et al., 2018; Devlin et al., 2019; Brown et al., 2020), however, is relatively under-explored for personalized natural language generation. Since IDs and words are in very different semantic spaces, it would be problematic to directly put them together for attention learning, because by doing so, the IDs are treated as words, but the IDs appear far less frequently than the words. For example, a paragraph of review (and thus hundreds of words) on e-commerce platform only corresponds to a single pair of user ID and item ID. As such, the IDs may be regarded as out-of-vocabulary tokens, to which the model is insensitive. As shown in Fig. 1(a), when generating an explanation for a user-item pair, standard Transformer relies heavily on the special <bos> token instead of the user or the item. This would result in identical explanations over different useritem pairs (see USR score in Table 2), deviating from our personalization goal.\nTo address this problem, we bridge IDs and words by designing an elegant task called context prediction, which maps IDs onto words to be generated by the explanation task. This in some way resembles one’s drafting-polishing process, where by predicting some words the context prediction\n[U se r] [It em ] <b os > th e ho te l is lo ca\nte d\nin th e he ar t of th e cit y an d th e cit y ce nt re is\nSource\n[User] [Item]\nthe hotel\nis located\nin the heart of\nthe city and the city centre is <eos>\nTa rg\net\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(a) Standard Transformer model, where the user and the item have no contribution to each generation step.\n[U se r] [It em ] <b os > th e po ol ar ea is ni ce an d th e gy m is ve ry we ll eq ui pp\ned\nSource\n[Rating] [Context]\nthe pool area\nis nice and the gym is very well equipped <eos> Ta rg et\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(b) Our PETER model, where the user and item IDs play significant roles in the generation steps.\nFigure 1: Attention visualization of two models when generating an explanation for the same user-item pair (see the first two columns). They are both from the last attention layer, so the target sequences are offset by one position for better illustration. The larger the attention weights, the lighter the cells.\ntask does the job of drafting. Then, the explanation generation task polishes these words so as to form a readable sentence. Meanwhile, we demonstrate that conducting recommendation task on the same model is also feasible, so we name it PETER, which stands for PErsonalized Transformer for Explainable Recommendation. As we can see in Fig. 1(b), when PETER generates an explanation for the same user-item pair, it can utilize the information of both the user and the item, which illustrates the effectiveness of our context prediction task.\nIn addition, PETER is flexible to incorporate item features that can help to guide its generation. This can be very useful when, for instance, a user proactively asks the system to explain certain feature(s) of a recommendation (Li et al., 2020c), e.g., price. Then, we would expect the model to generate a targeted explanation, such as “great jacket, especially for the price”. PETER is a small unpretrained Transformer with only 2 layers, yet it outperforms a fine-tuned BERT (Ni et al., 2019) on most metrics by a large margin, and takes less time to train, as shown in our experiments. This manifests the superiority of our model.\nIn summary, our key contributions are:\n• We propose PETER that makes recommendation and generates explanation simultaneously based on user and item IDs for explainable recommendation. To the best of our knowledge, we are the first to enable Transformer with personalized natural language generation.\n• We evaluate the generated explanations on not only text quality metrics (such as BLEU and ROUGE), but also metrics that particularly focus on explainability from the angle of item features. Extensive experiments show that our model can outperform state-of-the-art baselines on large datasets.\n• Our solution sheds light on a broader scope of fields that also need personalization (e.g., personalized conversational systems). In addition, it points out a way for Transformer to deal with heterogeneous inputs, e.g., text and images in multimodal artificial intelligence."
    }, {
      "heading" : "2 Related Work",
      "text" : "Explainable recommendation (Zhang et al., 2014; Zhang and Chen, 2020) has been studied from two major perspectives: human-computer interaction and machine learning. The former (Gedikli et al., 2014; Chen and Wang, 2017; Chen et al., 2019b) investigates how people perceive different styles of explanations, while the latter provides explanations by designing new explainable recommendation algorithms, to which our work is more related. There exist various types of explanation styles, such as pre-defined templates (Zhang et al., 2014; Li et al., 2020a), ranked sentences (Chen et al., 2019d; Li et al., 2021), image visualizations (Chen et al., 2019c), knowledge graph paths (Ai et al., 2018; Xian et al., 2019; Fu et al., 2020; Xian et al., 2020), reasoning rules (Shi et al.,\n2020; Chen et al., 2021; Zhu et al., 2021), etc., among which, recently, generated natural language explanations (Ni et al., 2019; Li et al., 2020c) have received much attention, mainly owing to the advancement of natural language generation technology and the availability of textual data on recommendation platforms such as e-commerce. However, previous works mostly rely on recurrent neural networks (RNN), e.g., LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Cho et al., 2014), leaving the potentially more effective Transformer under-explored, which motivates this work.\nTransformer (Vaswani et al., 2017) was first brought to machine translation with the architecture of encoder-decoder. Later works (Liu et al., 2018; Devlin et al., 2019) show that it remains effective, even when the encoder or the decoder is removed, reducing nearly half of the parameters. Under the paradigm of pre-training plus finetuning, Transformer’s effectiveness has been confirmed on a wide range of tasks, including both natural language understanding and generation (Radford et al., 2018; Devlin et al., 2019; Dong et al., 2019). Particularly, it is able to perform novel tasks, e.g., arithmetic, after scaling up both the model and the training data (Radford et al., 2019; Brown et al., 2020). However, it may not be friendly to researchers who do not possess large amounts of computing resources. Instead, our work explores small unpretrained models, as they are computationally cheaper and more flexible when being adapted to new applications, e.g., personalized generation.\nPersonalized generation usually involves the IDs of users and items. Previous approaches typically adopt multi-layer perceptron (MLP) to encode the IDs into a context vector, from which RNN can decode a word sequence. This strategy can be found in many applications, such as review generation (Dong et al., 2017), tip generation (Li et al., 2017) and explanation generation (Li et al., 2020c). However, it does not fit Transformer that relies entirely on self-attention. Probably because a proper solution to deal with heterogeneous inputs (i.e., IDs and words) is yet to be found, previous works with Transformer for personalized generation replace IDs with text segments, such as persona attributes (Zheng et al., 2020), movie titles (Zhou et al., 2020) and item features (Ni et al., 2019), which are in the same semantic space as the word sequence to be generated. In comparison, our solution is to design an effective task that can give the IDs linguistic\nmeanings, thus connecting IDs with words."
    }, {
      "heading" : "3 Problem Formulation",
      "text" : "The goal of our explanation task is to generate a natural language sentence Êu,i for a pair of user u and item i to justify why i is recommended to u. Meanwhile, our model PETER can also make recommendations by estimating a rating r̂u,i that predicts u’s preference towards i. At the testing stage, only user u and item i are used as inputs for producing both explanation and recommendation. When item features Fu,i are available, our model is flexible to incorporate them by simply concatenating them at the beginning of the explanation. In this case, the features are also needed in the testing stage. In the following, we will discuss both cases."
    }, {
      "heading" : "4 Methodology",
      "text" : "In this section, we present the details of our model PETER. First, we show how to encode different types of tokens in a sequence. Then, we briefly review Transformer and introduce our revised attention masking matrix. At last, we formulate the three tasks, i.e., explanation generation, context prediction and recommendation, and integrate them into a multi-task learning framework."
    }, {
      "heading" : "4.1 Input Representation",
      "text" : "We first introduce our way to encode heterogeneous inputs into vector representations. As shown in Fig. 2, the input to our model is a sequence, consisting\nof user ID u, item ID i, features Fu,i, and explanation Eu,i. The user and the item serve for the purpose of personalization, i.e., aiming to make the generated explanation reflect both the user’s interests and the item’s attributes. The features can guide the model to talk about certain topics. For instance, a conversational recommender system may explain a recommendation’s specialty to the user with the goal of knowing more about his/her preference (Chen et al., 2020). Since the features are not always available, in our experiments we test both cases (with and without them). When they are available, the input sequence can be represented as S = [u, i, f1, · · · , f|Fu,i|, e1, · · · , e|Eu,i|], where f1, · · · , f|Fu,i| are the features and e1, · · · , e|Eu,i| are the explanation’s word sequence. |Fu,i| denotes the number of features and |Eu,i| is the number of words in the explanation.\nClearly there are three types of tokens in the sequence S, i.e., users, items, and words (including features), for which we prepare three sets of randomly initialized token embeddings U, I and V respectively, besides the positional embeddings P that encode the position of each token in the sequence. Notice that, we do not add users and items to the vocabulary V , given that it costs more time to predict a word out of the huge amount of IDs (for example, millions of users and items in e-commerce). After performing embedding lookup, we can obtain the sequence’s token representation [u, i, f1, · · · , f|Fu,i|, e1, · · · , e|Eu,i|] and its positional representation [p1, · · · ,p|S|], where |S| is the length of the sequence. The input representation of the sequence is the addition of the corresponding token representation and positional representation, denoted as S0 = [s0,1, · · · , s0,|S|]."
    }, {
      "heading" : "4.2 Transformer and Attention Masking",
      "text" : "To enable the three tasks, we show how to modify the attention masking mechanism in Transformer (Vaswani et al., 2017). Transformer consists of L identical layers, each of which is composed of two sub-layers: multi-head self-attention and positionwise feed-forward network. The l-th layer encodes the previous layer’s output Sl−1 into Sl, where l ∈ [1, L]. In the multi-head self-attention sublayer, the computation of each attention head is also identical, and among the H heads of the l-th layer, the h-th head Al,h is computed as follows:\nAl,h = softmax( Ql,hK > l,h√\nd + M)Vl,h\nQl,h = Sl−1W Q l,h,Kl,h = Sl−1W K l,h, Vl,h = Sl−1W V l,h\nM = { 0, Allow to attend −∞, Prevent from attending\n(1)\nwhere Sl−1 ∈ R|S|×d is the (l − 1)-th layer’s output, WQl,h,W K l,h,W V l,h ∈ R d× d H are projection matrices, d denotes the dimension of embeddings, and M ∈ R|S|×|S| is the attention masking matrix.\nEach element in M controls whether a token in the sequence can attend to another. For example, in the unidirectional left-to-right language model (Radford et al., 2018), the lower triangular part of M is set to 0 and the remaining part −∞, so as to allow each token to attend to past tokens (including itself), but prevent it from attending to future tokens. We call it Left-to-Right Masking. As our model is not limited to the left-to-right explanation generation task, we modify the masking mechanism to accommodate the other two tasks (i.e., context prediction and recommendation). As shown in Fig. 3, the first two tokens u and i in the sequence can attend to each other, because both context prediction and recommendation tasks need them. To echo our model, we name it PETER Masking."
    }, {
      "heading" : "4.3 Explanation and Recommendation",
      "text" : "In the following, we perform the three tasks, after obtaining the sequence’s final representation SL = [sL,1, · · · , sL,|S|] from Transformer. The key challenge lies in the personalization of explanation generation task, for which we design the context prediction task. For both tasks, we apply a linear layer to the final representation of each token to map it into a |V|-sized vector. As an example,\nafter passing through this layer, sL,t becomes ct:\nct = softmax(WvsL,t + bv) (2)\nwhere Wv ∈ R|V|×d and bv ∈ R|V| are weight parameters. The vector ct represents the probability distribution over the vocabulary V , from which a word e with probability cet can be sampled.\nExplanation Generation: We adopt the Negative Log-Likelihood (NLL) as the explanation task’s loss function, and compute the mean of useritem pairs in the training set:\nLe = 1 |T | ∑\n(u,i)∈T\n1\n|Eu,i| |Eu,i|∑ t=1 − log cet2+|Fu,i|+t\n(3) where T denotes the training set. The probability cett is offset by 2 + |Fu,i| positions because the explanation is placed at the end of the sequence, and |Fu,i| = 0 when the features are unavailable.\nAt the testing stage, along with u, i, and Fu,i (if available), we feed the model a special begin-ofsequence token <bos>. From its resulting probability distribution c<bos>, the model can predict a word. For simplicity, among the many decoding methods, we opt for greedy decoding that samples the word with the largest probability. Then we can concatenate this predicted word at the end of the sequence to form a new input sequence for generating another word. We do this repeatedly until the model produces a special end-of-sequence token <eos>, or the generated explanation Êu,i reaches a pre-defined length.\nContext Prediction: As discussed earlier, when there is only one task of explanation generation, Transformer fails to make use of user ID and item ID, resulting in identical sentences. To address this issue, we design this task to map the IDs onto the words in the explanation, so as to build a connection between them. Since the first two positions (u and i) of the sequence are allowed to attend to each other, both of their final representations absorb the information of the user and the item. Thus, we can use either of them to perform this task. Here, we use the 2nd one for better illustration in Fig. 2. Again, we adopt NLL as the loss function:\nLc = 1 |T | ∑\n(u,i)∈T\n1\n|Eu,i| |Eu,i|∑ t=1 − log cet2 (4)\nwhere the difference from Eq. (3) is that all predicted words are from the 2nd position, which is why they are not sequentially ordered (see Fig. 2).\nRating Prediction: Recommendation can be seen as a prediction problem (Chen et al., 2021) where the goal is to predict a score r̂u,i based on the IDs of user u and item i. As both u and i in the sequence can attend to each other, their final representations capture the interaction between them. Next, we map the 1st representation sL,1 into a scalar (because the 2nd one is used for context prediction). To this end, we employ multi-layer perceptron (MLP) with one hidden layer as follows:\nr̂u,i = w rσ(WrsL,1 + b r) + br (5)\nwhere Wr ∈ Rd×d, br ∈ Rd, wr ∈ R1×d and br ∈ R are weight parameters, and σ(·) is the sigmoid function. Therefore, it can be seen that it is feasible to do both recommendation and explanation on Transformer. As recommendation is not the key focus of this paper, we leave its improvement in the future work. For this task, we use Mean Square Error (MSE) as the loss function:\nLr = 1 |T | ∑\n(u,i)∈T\n(ru,i − r̂u,i)2 (6)\nwhere ru,i is the ground-truth rating.\nMulti-task Learning: At last, we integrate the three tasks into a multi-task learning framework whose objective function is defined as:\nJ = min Θ (λeLe + λcLc + λrLr) (7)\nwhere Θ denotes all the trainable parameters in the model, and λe, λc and λr are regularization weights that balance the learning of different tasks. In this way, the model can be trained efficiently in an end-to-end manner."
    }, {
      "heading" : "5 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "5.1 Datasets",
      "text" : "For experimentation, we adopt three publicly available explainable recommendation datasets, and their data splits (Li et al., 2020c). During the splitting process, each dataset is randomly divided into training, validation and testing sets with ratio 8:1:1 for 5 times, and the training set holds at least one record for each user and each item. The three datasets are respectively from TripAdvisor\n(hotel), Amazon (movies & TV) and Yelp (restaurant). Each record in the datasets is comprised of a user ID, an item ID, a rating, an explanation, and a feature. The explanations are sentences extracted from user reviews. Each explanation contains at least one item feature, e.g., bedroom, which ensures the explanation quality. Statistics of the datasets are shown in Table 1. We can see that Yelp is much larger than the other two in terms of size, making it closer to the real-world situation where there are millions of users and items."
    }, {
      "heading" : "5.2 Evaluation Metrics",
      "text" : "To evaluate the recommendation performance, we adopt two commonly used metrics: Root Mean Square Error (RMSE) and Mean Absolute Error (MAE). As to explanation performance, we measure the generated explanations from two main perspectives: text quality and explainability. For the former, we adopt BLEU (Papineni et al., 2002) in machine translation and ROUGE (Lin, 2004) in text summarization, and report BLEU-1 and BLEU4, and Precision, Recall and F1 of ROUGE-1 and ROUGE-2. Though being widely used, BLUE and ROUGE are not flawless. For example, it is difficult for them to detect the problem of identical sentences generated by Transformer. These identical sentences might not be used as explanations, because they are less likely to well explain the special property of different recommendations. To quantitatively measure how severe the problem is, we adopt USR that computes the Unique Sentence Ratio of generated sentences (Li et al., 2020c).\nText quality, however, is not equal to explainbility. In the case of explainable recommendation, users may value more an explanation that justifies a recommendation’s advantages on certain features (Li et al., 2020c; Chen et al., 2019a). To this end, we adopt the other three metrics proposed by (Li et al., 2020c): Feature Matching Ratio (FMR), Feature Coverage Ratio (FCR) and Feature Diversity (DIV). FMR measures whether a generated\nexplanation contains the feature in the ground-truth. FCR is computed as the number of distinct features contained in all the generated explanations, divided by the total number of features in the whole dataset. DIV measures the intersection of features between any two generated explanations.\nFor RMSE, MAE and DIV, the lower, the better, while it is opposite for the rest of metrics."
    }, {
      "heading" : "5.3 Compared Methods",
      "text" : "We introduce baselines, first for explanation and then for recommendation. For the former, we divide the baselines into two groups, depending on whether the feature is used or not.\nThe following models leverage only user and item IDs to generate explanations (without feature). We denote our model without feature as PETER.\n• Transformer (Vaswani et al., 2017) performs the explanation generation task by treating user and item IDs as words. We also tested encoder-decoder Transformer, where the encoder encodes the IDs for the decoder to decode, but its results turned out to be the same, so we do not report it.\n• NRT (Li et al., 2017) can predict a rating and generate a tip simultaneously based on user and item IDs. We take the explanations in the datasets as tips. Moreover, we found that the model’s problem of generating identical sentences (as reported in Li et al., 2020c) is caused by the L2 regularization in its original design. For fair comparison, we removed it.\n• Att2Seq (Dong et al., 2017) is a review generation approach and we take the explanations as reviews. This model has an attention module, but we found that it makes the generated content unreadable in the task. To be fair, we removed it as well.\nWhen features are used, we denote our model as PETER+, and compare it with two recent models:\n• ACMLM (Ni et al., 2019) is a fine-tuned BERT (Devlin et al., 2019), where an attention layer is introduced to encode the features from both the user and the item. By predicting masked tokens, this model can produce diverse sentences.\n• NETE (Li et al., 2020c) is a tailored GRU (Cho et al., 2014) that incorporates a given\nfeature into the decoding process to generate template-like explanations. It can also make recommendations.\nFor recommendation, besides NRT and NETE, we include another two traditional methods:\n• PMF (Mnih and Salakhutdinov, 2007) is a standard probabilistic matrix factorization method that characterizes users and items by latent factors.\n• SVD++ (Koren, 2008) leverages a user’s interacted items to enhance the latent factors."
    }, {
      "heading" : "5.4 Implementation Details",
      "text" : "We train each model on the training set, tune the hyper-parameters on the validation set, and report the performance on the testing set. The results are averaged on the 5 data splits. We adopt the codes of ACMLM and NETE, and implement all the other methods. For NRT, Att2Seq, NETE and our PETER and PETER+, we set the size of vocabulary to 20,000 by keeping the most frequent words. We do not apply this to Transformer, otherwise users\nand items (regarded as words) may be filtered out. We set both the number of context words and the length of explanations to 15, because the mean length of explanations is approximately 13 (see Table 1). ACMLM adopts sub-words, so we do not apply the above two steps to it. We reuse the other default settings of the baselines.\nFor Transformer, PETER and PETER+, we set the embedding size d to 512 and the dimension of feed-forward network to 2,048, following (Vaswani et al., 2017), but the number of layers L and attention heads H are both 2. For our models PETER and PETER+, we set the regularization weights λe, λc and λr to 1.0, 1.0 and 0.1, respectively. We optimize the model via stochastic gradient descent (Robbins and Monro, 1951), and apply gradient clipping (Pascanu et al., 2013) with a threshold of\n1.0. The batch size is set to 128, and the learning rate 1.0. At each epoch, we save the model if it achieves the lowest loss on the validation set, but when there is no improvement, we decrease the learning rate by a factor of 0.25. When the latter happens for 5 times, we stop training and load the saved model for prediction."
    }, {
      "heading" : "6 Results and Analysis",
      "text" : ""
    }, {
      "heading" : "6.1 Quantitative Analysis on Explanations",
      "text" : "In Table 2, we compare the performance of explanation generation methods in two groups. We first analyze models that make use of item features (i.e., ACMLM, NETE and PETER+). Our PETER+ consistently and significantly outperforms ACMLM and NETE on the three datasets in terms of text quality (BLEU and ROUGE). This shows the effectiveness of our model in generating high-quality sentences. Notice that Li et al. (2020b) conducted a user survey and reported that NETE’s explanations were perceived useful by most participants. It suggests that our model’s explanations with better quality could also be very useful to real users.\nAgain, in terms of text quality, the performance gap between PETER+ and ACMLM (a fine-tuned BERT) is extremely large, because the latter’s generation is achieved by predicting masked tokens, which is quite different from word-by-word generation. This may explain why ACMLM produces diverse sentences (high USR), which, however, is less meaningful when text quality cannot be guaranteed. Furthermore, PETER+ beats both ACMLM and NETE on the explainability metric FMR that cares about whether a generated explanation mentions the feature in the ground-truth. This is quite useful in real-world applications when the system is asked to explain a particular feature. Regarding the other two explainability metrics FCR and DIV, PETER+ is also very competitive. ACMLM gains better performance on some cases, because at the training stage it is exposed to more features (from both the user and the item), which is unfair to both PETER+ and NETE.\nNext, we discuss the results of the models that\nonly leverage user and item IDs for generation. As it can be seen, Transformer generates identical explanations on each dataset, resulting in nearly 0 score on Unique Sentence Ratio (USR). Owing to the context prediction task, our PETER successfully addresses this issue, producing diverse (comparable USR) and high-quality (best BLEU4) sentences. In particular, on the largest dataset Yelp, it achieves the best performance on most of the metrics. This again demonstrates the effectiveness of our model. On Amazon and TripAdvisor, NRT and Att2Seq are very competitive, because we fixed their generation issues (see Section 5.3). In addition, the two datasets are small and thus the training samples are limited, so our model may underfit, which is why it does not always reach the best performance.\nBesides explanation performance, we also investigate the efficiency of different Transformer-based models. On the same machine (NVIDIA Tesla P40) and dataset (TripAdvisor), we compare the training minutes of ACMLM and our PETER+ in Table 3. Compared with ACMLM, our model takes less time to train (2.3 minutes per epoch), since it has only 2 layers and thus less parameters. But because it is unpretrained and learned from scratch, it needs more training epochs."
    }, {
      "heading" : "6.2 Qualitative Case Study on Explanations",
      "text" : "In Table 4, we present two examples generated by PETER and PETER+ on the TripAdvisor dataset. We can see that PETER generates distinct context words and explanations for different user-item pairs. This confirms that our proposed solution can indeed endow the user and item IDs with linguis-\ntic meanings, as well as achieving certain degree of personalization for natural language generation. Among the commonly used context words, e.g., the, there are some important features (underlined), according to which the model then generates an explanation that talks about them. Admittedly, there is still much room for improvement of the context prediction task, so as to more accurately predict the features in the ground-truth (e.g., rooms vs. pool in the first example). One alternative is to leverage the features to guide the model’s generation. This explains why PETER+ is able to generate an explanation that talks about rooms rather than pool, making it semantically closer to the ground-truth. It thus demonstrates our model’s flexibility in incorporating these features."
    }, {
      "heading" : "6.3 Recommendation Performance",
      "text" : "Table 5 presents the performance comparison of different recommendation methods. On the largest dataset Yelp with approximately 1.3 million records, our model PETER performs as good as the three competitive baselines (i.e., SVD++, NRT and NETE), which shows the rationale of our recommendation module. Since our model PETER has more parameters to learn, it may underfit on small datasets. This explains why it does not always perform the best on TripAdvisor and Amazon. When more training data are available to Transformer, usually the performance will become better, as evidenced by GPT-2 (Radford et al., 2019) and GPT-3 (Brown et al., 2020). Thus, we can expect our model to perform well in real-world applications, where the training data are bigger than the testing datasets, e.g., billion-scale users in Amazon."
    }, {
      "heading" : "6.4 Ablation Study",
      "text" : "In Table 6, we provide an ablation study conducted on the TripAdvisor dataset. After disabling the context prediction task Lc by setting λc = 0, the performances of both explainability and text quality drop dramatically, and the unique sentence ratio (USR) is nearly approaching Transformer’s (see Table 2). It hence confirms this task’s effectiveness.\nAs Lc is highly correlated with the recommendation task Lr via the user and item IDs (see Section 4.3), the removal of Lc leads to slight improvement on recommendation performance. We can also observe a reversed phenomenon when we disable Lr. When PETER masking is replaced by the Left-to-Right masking that prevents the model from accessing the item information, the recommendation performance drops sharply. Overall, PETER reaches an optimal situation, where its explainability, text quality and recommendation performance are all reasonably good."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We propose a simple and effective solution to address the personalized generation problem of Transformer, unleashing its language modeling power to generate explanations for recommender systems. Extensive experiments show that the solution is both effective and efficient. It opens up a new way of exploiting Transformer by designing good tasks instead of scaling up model size. There are various applications of personalized generation for which Transformer is still less explored. Our next step is to adopt our solution for personalized question answering systems and personalized conversational agents. We also plan to incorporate item images into the model, so as to generate visual explanations for recommendations, since “a picture is worth a thousand words”. Another meaningful extension is to adapt the model to cross-lingual explanation generation, because international platforms, e.g., Amazon, may serve users who speak different languages."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was partially supported by HKBU IRCMS/19-20/D05, RGC/HKBU12201620, and NSF IIS-1910154 and IIS-2007907. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsors."
    } ],
    "references" : [ {
      "title" : "Language models are few-shot",
      "author" : [ "Askell" ],
      "venue" : null,
      "citeRegEx" : "Askell,? \\Q2020\\E",
      "shortCiteRegEx" : "Askell",
      "year" : 2020
    }, {
      "title" : "Generate natural language explana",
      "author" : [ "Zhang. 2019a" ],
      "venue" : null,
      "citeRegEx" : "2019a.,? \\Q2019\\E",
      "shortCiteRegEx" : "2019a.",
      "year" : 2019
    }, {
      "title" : "Neural collaborative reasoning",
      "author" : [ "Zhang." ],
      "venue" : "In",
      "citeRegEx" : "Zhang.,? 2021",
      "shortCiteRegEx" : "Zhang.",
      "year" : 2021
    }, {
      "title" : "Li Chen and Feng Wang",
      "author" : [ "Proceedings of The Web Conference" ],
      "venue" : "2017. Explaining recom-",
      "citeRegEx" : "Conference,? 2021",
      "shortCiteRegEx" : "Conference",
      "year" : 2021
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "2019 Annual Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to generate product reviews from attributes",
      "author" : [ "Li Dong", "Shaohan Huang", "Furu Wei", "Mirella Lapata", "Ming Zhou", "Ke Xu." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume",
      "citeRegEx" : "Dong et al\\.,? 2017",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2017
    }, {
      "title" : "Unified language model pre-training for natural language understanding and generation",
      "author" : [ "Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon." ],
      "venue" : "Advances in Neural Informa-",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Fairness-aware explainable recommendation over knowledge graphs",
      "author" : [ "Zuohui Fu", "Yikun Xian", "Ruoyuan Gao", "Jieyu Zhao", "Qiaoying Huang", "Yingqiang Ge", "Shuyuan Xu", "Shijie Geng", "Chirag Shah", "Yongfeng Zhang" ],
      "venue" : null,
      "citeRegEx" : "Fu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2020
    }, {
      "title" : "How should i explain? a comparison of different explanation types for recommender systems",
      "author" : [ "Fatih Gedikli", "Dietmar Jannach", "Mouzhi Ge." ],
      "venue" : "International Journal of Human-Computer Studies, 72(4):367–382.",
      "citeRegEx" : "Gedikli et al\\.,? 2014",
      "shortCiteRegEx" : "Gedikli et al\\.",
      "year" : 2014
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Factorization meets the neighborhood: a multifaceted collaborative filtering model",
      "author" : [ "Yehuda Koren." ],
      "venue" : "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 426–434.",
      "citeRegEx" : "Koren.,? 2008",
      "shortCiteRegEx" : "Koren.",
      "year" : 2008
    }, {
      "title" : "Towards personalized review summarization via useraware sequence network",
      "author" : [ "Junjie Li", "Haoran Li", "Chengqing Zong." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6690–6697.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Caesar: context-aware explanation based on supervised attention for service recommendations",
      "author" : [ "Lei Li", "Li Chen", "Ruihai Dong." ],
      "venue" : "Journal of Intelligent Information Systems, pages 1–24.",
      "citeRegEx" : "Li et al\\.,? 2020a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards controllable explanation generation for recommender systems via neural template",
      "author" : [ "Lei Li", "Li Chen", "Yongfeng Zhang." ],
      "venue" : "Companion Proceedings of the Web Conference 2020, pages 198–202.",
      "citeRegEx" : "Li et al\\.,? 2020b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Generate neural template explanations for recommendation",
      "author" : [ "Lei Li", "Yongfeng Zhang", "Li Chen." ],
      "venue" : "Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pages 755–764.",
      "citeRegEx" : "Li et al\\.,? 2020c",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Extra: Explanation ranking datasets for explainable recommendation",
      "author" : [ "Lei Li", "Yongfeng Zhang", "Li Chen." ],
      "venue" : "Proceedings of the 44th International ACM SIGIR conference on Research and Development in Information Retrieval.",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Neural rating regression with abstractive tips generation for recommendation",
      "author" : [ "Piji Li", "Zihao Wang", "Zhaochun Ren", "Lidong Bing", "Wai Lam." ],
      "venue" : "Proceedings of the 40th International ACM SIGIR conference on Research and Development in Infor-",
      "citeRegEx" : "Li et al\\.,? 2017",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text summarization branches out, pages 74–81.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Generating wikipedia by summarizing long sequences",
      "author" : [ "Peter J Liu", "Mohammad Saleh", "Etienne Pot", "Ben Goodrich", "Ryan Sepassi", "Lukasz Kaiser", "Noam Shazeer." ],
      "venue" : "The Sixth International Conference on Learning Representations.",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Probabilistic matrix factorization",
      "author" : [ "Andriy Mnih", "Russ R Salakhutdinov." ],
      "venue" : "Advances in neural information processing systems, pages 1257–1264.",
      "citeRegEx" : "Mnih and Salakhutdinov.,? 2007",
      "shortCiteRegEx" : "Mnih and Salakhutdinov.",
      "year" : 2007
    }, {
      "title" : "Justifying recommendations using distantly-labeled reviews and fine-grained aspects",
      "author" : [ "Jianmo Ni", "Jiacheng Li", "Julian McAuley." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Ni et al\\.,? 2019",
      "shortCiteRegEx" : "Ni et al\\.",
      "year" : 2019
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "On the difficulty of training recurrent neural networks",
      "author" : [ "Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio." ],
      "venue" : "International conference on machine learning, pages 1310–1318.",
      "citeRegEx" : "Pascanu et al\\.,? 2013",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2013
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "A stochastic approximation method",
      "author" : [ "Herbert Robbins", "Sutton Monro." ],
      "venue" : "The annals of mathematical statistics, pages 400–407.",
      "citeRegEx" : "Robbins and Monro.,? 1951",
      "shortCiteRegEx" : "Robbins and Monro.",
      "year" : 1951
    }, {
      "title" : "Neural logic reasoning",
      "author" : [ "Shaoyun Shi", "Hanxiong Chen", "Weizhi Ma", "Jiaxin Mao", "Min Zhang", "Yongfeng Zhang." ],
      "venue" : "Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pages 1365–1374.",
      "citeRegEx" : "Shi et al\\.,? 2020",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Reinforcement knowledge graph reasoning for explainable recommendation",
      "author" : [ "Yikun Xian", "Zuohui Fu", "S Muthukrishnan", "Gerard De Melo", "Yongfeng Zhang." ],
      "venue" : "Proceedings of the 42nd International ACM SIGIR Conference on Research and",
      "citeRegEx" : "Xian et al\\.,? 2019",
      "shortCiteRegEx" : "Xian et al\\.",
      "year" : 2019
    }, {
      "title" : "Cafe: Coarse-to-fine neural symbolic reasoning for explainable recommendation",
      "author" : [ "Yikun Xian", "Zuohui Fu", "Handong Zhao", "Yingqiang Ge", "Xu Chen", "Qiaoying Huang", "Shijie Geng", "Zhou Qin", "Gerard De Melo", "Shan Muthukrishnan" ],
      "venue" : null,
      "citeRegEx" : "Xian et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Xian et al\\.",
      "year" : 2020
    }, {
      "title" : "Explainable recommendation: A survey and new perspectives",
      "author" : [ "Yongfeng Zhang", "Xu Chen." ],
      "venue" : "Foundations and Trends R",
      "citeRegEx" : "Zhang and Chen.,? 2020",
      "shortCiteRegEx" : "Zhang and Chen.",
      "year" : 2020
    }, {
      "title" : "Towards conversational search and recommendation: System ask, user respond",
      "author" : [ "Yongfeng Zhang", "Xu Chen", "Qingyao Ai", "Liu Yang", "W Bruce Croft." ],
      "venue" : "Proceedings of the 27th ACM International Conference on Information and Knowledge",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Explicit factor models for explainable recommendation based on phrase-level sentiment analysis",
      "author" : [ "Yongfeng Zhang", "Guokun Lai", "Min Zhang", "Yi Zhang", "Yiqun Liu", "Shaoping Ma." ],
      "venue" : "Proceedings of the 37th international ACM SIGIR conference on",
      "citeRegEx" : "Zhang et al\\.,? 2014",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2014
    }, {
      "title" : "A pre-training based personalized dialogue generation model with persona-sparse data",
      "author" : [ "Yinhe Zheng", "Rongsheng Zhang", "Minlie Huang", "Xiaoxi Mao." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, pages 9693–9700.",
      "citeRegEx" : "Zheng et al\\.,? 2020",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving conversational recommender systems via knowledge graph based semantic fusion",
      "author" : [ "Kun Zhou", "Wayne Xin Zhao", "Shuqing Bian", "Yuanhang Zhou", "Ji-Rong Wen", "Jingsong Yu." ],
      "venue" : "Proceedings of the 26th ACM SIGKDD International",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    }, {
      "title" : "Faithfully explainable recommendation via neural logic reasoning",
      "author" : [ "Yaxin Zhu", "Yikun Xian", "Zuohui Fu", "Gerard de Melo", "Yongfeng Zhang." ],
      "venue" : "2021 Annual Conference of the North American Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Zhu et al\\.,? 2021",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 32,
      "context" : "Many of the applications in fact require certain degree of personalization, such as explainable recommendation (Zhang et al., 2014; Li et al., 2020c; Zhang and Chen, 2020), review generation (Dong et al.",
      "startOffset" : 111,
      "endOffset" : 171
    }, {
      "referenceID" : 14,
      "context" : "Many of the applications in fact require certain degree of personalization, such as explainable recommendation (Zhang et al., 2014; Li et al., 2020c; Zhang and Chen, 2020), review generation (Dong et al.",
      "startOffset" : 111,
      "endOffset" : 171
    }, {
      "referenceID" : 30,
      "context" : "Many of the applications in fact require certain degree of personalization, such as explainable recommendation (Zhang et al., 2014; Li et al., 2020c; Zhang and Chen, 2020), review generation (Dong et al.",
      "startOffset" : 111,
      "endOffset" : 171
    }, {
      "referenceID" : 5,
      "context" : ", 2020c; Zhang and Chen, 2020), review generation (Dong et al., 2017), review summarization (Li et al.",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 11,
      "context" : ", 2017), review summarization (Li et al., 2019), and conversational systems (Zhang et al.",
      "startOffset" : 30,
      "endOffset" : 47
    }, {
      "referenceID" : 31,
      "context" : ", 2019), and conversational systems (Zhang et al., 2018; Chen et al., 2020).",
      "startOffset" : 36,
      "endOffset" : 75
    }, {
      "referenceID" : 30,
      "context" : "The goal of explainable recommendation (Zhang and Chen, 2020) is to provide an explanation to a user for a recommended item, so as to justify how the recommendation might match his/her interests.",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 27,
      "context" : "Transformer (Vaswani et al., 2017), whose strong language modeling ability has been demonstrated on a variety of tasks (Radford et al.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 23,
      "context" : ", 2017), whose strong language modeling ability has been demonstrated on a variety of tasks (Radford et al., 2018; Devlin et al., 2019; Brown et al., 2020), however, is relatively under-explored for personalized natural language generation.",
      "startOffset" : 92,
      "endOffset" : 155
    }, {
      "referenceID" : 4,
      "context" : ", 2017), whose strong language modeling ability has been demonstrated on a variety of tasks (Radford et al., 2018; Devlin et al., 2019; Brown et al., 2020), however, is relatively under-explored for personalized natural language generation.",
      "startOffset" : 92,
      "endOffset" : 155
    }, {
      "referenceID" : 14,
      "context" : "This can be very useful when, for instance, a user proactively asks the system to explain certain feature(s) of a recommendation (Li et al., 2020c), e.",
      "startOffset" : 129,
      "endOffset" : 147
    }, {
      "referenceID" : 20,
      "context" : "PETER is a small unpretrained Transformer with only 2 layers, yet it outperforms a fine-tuned BERT (Ni et al., 2019) on most metrics by a large margin, and takes less time to train, as shown in our experiments.",
      "startOffset" : 99,
      "endOffset" : 116
    }, {
      "referenceID" : 32,
      "context" : "Explainable recommendation (Zhang et al., 2014; Zhang and Chen, 2020) has been studied from two major perspectives: human-computer interaction and machine learning.",
      "startOffset" : 27,
      "endOffset" : 69
    }, {
      "referenceID" : 30,
      "context" : "Explainable recommendation (Zhang et al., 2014; Zhang and Chen, 2020) has been studied from two major perspectives: human-computer interaction and machine learning.",
      "startOffset" : 27,
      "endOffset" : 69
    }, {
      "referenceID" : 8,
      "context" : "The former (Gedikli et al., 2014; Chen and Wang, 2017; Chen et al., 2019b) investigates how people perceive different styles of explanations, while the latter provides explanations by designing new explainable recommendation algorithms, to which our work is more related.",
      "startOffset" : 11,
      "endOffset" : 74
    }, {
      "referenceID" : 32,
      "context" : "There exist various types of explanation styles, such as pre-defined templates (Zhang et al., 2014; Li et al., 2020a), ranked sentences (Chen et al.",
      "startOffset" : 79,
      "endOffset" : 117
    }, {
      "referenceID" : 12,
      "context" : "There exist various types of explanation styles, such as pre-defined templates (Zhang et al., 2014; Li et al., 2020a), ranked sentences (Chen et al.",
      "startOffset" : 79,
      "endOffset" : 117
    }, {
      "referenceID" : 15,
      "context" : ", 2020a), ranked sentences (Chen et al., 2019d; Li et al., 2021), image visualizations (Chen et al.",
      "startOffset" : 27,
      "endOffset" : 64
    }, {
      "referenceID" : 28,
      "context" : ", 2019c), knowledge graph paths (Ai et al., 2018; Xian et al., 2019; Fu et al., 2020; Xian et al., 2020), reasoning rules (Shi et al.",
      "startOffset" : 32,
      "endOffset" : 104
    }, {
      "referenceID" : 7,
      "context" : ", 2019c), knowledge graph paths (Ai et al., 2018; Xian et al., 2019; Fu et al., 2020; Xian et al., 2020), reasoning rules (Shi et al.",
      "startOffset" : 32,
      "endOffset" : 104
    }, {
      "referenceID" : 29,
      "context" : ", 2019c), knowledge graph paths (Ai et al., 2018; Xian et al., 2019; Fu et al., 2020; Xian et al., 2020), reasoning rules (Shi et al.",
      "startOffset" : 32,
      "endOffset" : 104
    }, {
      "referenceID" : 20,
      "context" : ", among which, recently, generated natural language explanations (Ni et al., 2019; Li et al., 2020c) have received much attention, mainly owing to the advancement of natural language generation technology and the availability of textual data on recommendation platforms such as e-commerce.",
      "startOffset" : 65,
      "endOffset" : 100
    }, {
      "referenceID" : 14,
      "context" : ", among which, recently, generated natural language explanations (Ni et al., 2019; Li et al., 2020c) have received much attention, mainly owing to the advancement of natural language generation technology and the availability of textual data on recommendation platforms such as e-commerce.",
      "startOffset" : 65,
      "endOffset" : 100
    }, {
      "referenceID" : 27,
      "context" : "Transformer (Vaswani et al., 2017) was first brought to machine translation with the architecture of encoder-decoder.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 18,
      "context" : "Later works (Liu et al., 2018; Devlin et al., 2019) show that it remains effective, even when the encoder or the decoder is removed, reducing nearly half of the parameters.",
      "startOffset" : 12,
      "endOffset" : 51
    }, {
      "referenceID" : 4,
      "context" : "Later works (Liu et al., 2018; Devlin et al., 2019) show that it remains effective, even when the encoder or the decoder is removed, reducing nearly half of the parameters.",
      "startOffset" : 12,
      "endOffset" : 51
    }, {
      "referenceID" : 23,
      "context" : "Under the paradigm of pre-training plus finetuning, Transformer’s effectiveness has been confirmed on a wide range of tasks, including both natural language understanding and generation (Radford et al., 2018; Devlin et al., 2019; Dong et al., 2019).",
      "startOffset" : 186,
      "endOffset" : 248
    }, {
      "referenceID" : 4,
      "context" : "Under the paradigm of pre-training plus finetuning, Transformer’s effectiveness has been confirmed on a wide range of tasks, including both natural language understanding and generation (Radford et al., 2018; Devlin et al., 2019; Dong et al., 2019).",
      "startOffset" : 186,
      "endOffset" : 248
    }, {
      "referenceID" : 6,
      "context" : "Under the paradigm of pre-training plus finetuning, Transformer’s effectiveness has been confirmed on a wide range of tasks, including both natural language understanding and generation (Radford et al., 2018; Devlin et al., 2019; Dong et al., 2019).",
      "startOffset" : 186,
      "endOffset" : 248
    }, {
      "referenceID" : 24,
      "context" : ", arithmetic, after scaling up both the model and the training data (Radford et al., 2019; Brown et al., 2020).",
      "startOffset" : 68,
      "endOffset" : 110
    }, {
      "referenceID" : 5,
      "context" : "This strategy can be found in many applications, such as review generation (Dong et al., 2017), tip generation (Li et al.",
      "startOffset" : 75,
      "endOffset" : 94
    }, {
      "referenceID" : 16,
      "context" : ", 2017), tip generation (Li et al., 2017) and explanation generation (Li et al.",
      "startOffset" : 24,
      "endOffset" : 41
    }, {
      "referenceID" : 14,
      "context" : ", 2017) and explanation generation (Li et al., 2020c).",
      "startOffset" : 35,
      "endOffset" : 53
    }, {
      "referenceID" : 33,
      "context" : ", IDs and words) is yet to be found, previous works with Transformer for personalized generation replace IDs with text segments, such as persona attributes (Zheng et al., 2020), movie titles (Zhou et al.",
      "startOffset" : 156,
      "endOffset" : 176
    }, {
      "referenceID" : 34,
      "context" : ", 2020), movie titles (Zhou et al., 2020) and item features (Ni et al.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 20,
      "context" : ", 2020) and item features (Ni et al., 2019), which are in the same semantic space as the word sequence to be generated.",
      "startOffset" : 26,
      "endOffset" : 43
    }, {
      "referenceID" : 27,
      "context" : "To enable the three tasks, we show how to modify the attention masking mechanism in Transformer (Vaswani et al., 2017).",
      "startOffset" : 96,
      "endOffset" : 118
    }, {
      "referenceID" : 23,
      "context" : "For example, in the unidirectional left-to-right language model (Radford et al., 2018), the lower triangular part of M is set to 0 and the remaining part −∞, so as to allow each token to attend to past tokens (including itself), but prevent it from attending to future tokens.",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 14,
      "context" : "For experimentation, we adopt three publicly available explainable recommendation datasets, and their data splits (Li et al., 2020c).",
      "startOffset" : 114,
      "endOffset" : 132
    }, {
      "referenceID" : 21,
      "context" : "For the former, we adopt BLEU (Papineni et al., 2002) in machine translation and ROUGE (Lin, 2004) in text summarization, and report BLEU-1 and BLEU4, and Precision, Recall and F1 of ROUGE-1 and ROUGE-2.",
      "startOffset" : 30,
      "endOffset" : 53
    }, {
      "referenceID" : 17,
      "context" : ", 2002) in machine translation and ROUGE (Lin, 2004) in text summarization, and report BLEU-1 and BLEU4, and Precision, Recall and F1 of ROUGE-1 and ROUGE-2.",
      "startOffset" : 41,
      "endOffset" : 52
    }, {
      "referenceID" : 14,
      "context" : "To quantitatively measure how severe the problem is, we adopt USR that computes the Unique Sentence Ratio of generated sentences (Li et al., 2020c).",
      "startOffset" : 129,
      "endOffset" : 147
    }, {
      "referenceID" : 14,
      "context" : "In the case of explainable recommendation, users may value more an explanation that justifies a recommendation’s advantages on certain features (Li et al., 2020c; Chen et al., 2019a).",
      "startOffset" : 144,
      "endOffset" : 182
    }, {
      "referenceID" : 14,
      "context" : "To this end, we adopt the other three metrics proposed by (Li et al., 2020c): Feature Matching Ratio (FMR), Feature Coverage Ratio (FCR) and Feature Diversity (DIV).",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 27,
      "context" : "• Transformer (Vaswani et al., 2017) performs the explanation generation task by treating user and item IDs as words.",
      "startOffset" : 14,
      "endOffset" : 36
    }, {
      "referenceID" : 16,
      "context" : "• NRT (Li et al., 2017) can predict a rating and generate a tip simultaneously based on user and item IDs.",
      "startOffset" : 6,
      "endOffset" : 23
    }, {
      "referenceID" : 5,
      "context" : "• Att2Seq (Dong et al., 2017) is a review generation approach and we take the explanations as reviews.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 20,
      "context" : "• ACMLM (Ni et al., 2019) is a fine-tuned BERT (Devlin et al.",
      "startOffset" : 8,
      "endOffset" : 25
    }, {
      "referenceID" : 4,
      "context" : ", 2019) is a fine-tuned BERT (Devlin et al., 2019), where an attention layer is introduced to encode the features from both the user and the item.",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 14,
      "context" : "• NETE (Li et al., 2020c) is a tailored GRU (Cho et al.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 19,
      "context" : "• PMF (Mnih and Salakhutdinov, 2007) is a standard probabilistic matrix factorization method that characterizes users and items by latent factors.",
      "startOffset" : 6,
      "endOffset" : 36
    }, {
      "referenceID" : 10,
      "context" : "• SVD++ (Koren, 2008) leverages a user’s interacted items to enhance the latent factors.",
      "startOffset" : 8,
      "endOffset" : 21
    }, {
      "referenceID" : 27,
      "context" : "For Transformer, PETER and PETER+, we set the embedding size d to 512 and the dimension of feed-forward network to 2,048, following (Vaswani et al., 2017), but the number of layers L and attention heads H are both 2.",
      "startOffset" : 132,
      "endOffset" : 154
    }, {
      "referenceID" : 25,
      "context" : "We optimize the model via stochastic gradient descent (Robbins and Monro, 1951), and apply gradient clipping (Pascanu et al.",
      "startOffset" : 54,
      "endOffset" : 79
    }, {
      "referenceID" : 22,
      "context" : "We optimize the model via stochastic gradient descent (Robbins and Monro, 1951), and apply gradient clipping (Pascanu et al., 2013) with a threshold of",
      "startOffset" : 109,
      "endOffset" : 131
    }, {
      "referenceID" : 24,
      "context" : "When more training data are available to Transformer, usually the performance will become better, as evidenced by GPT-2 (Radford et al., 2019) and GPT-3 (Brown et al.",
      "startOffset" : 120,
      "endOffset" : 142
    } ],
    "year" : 2021,
    "abstractText" : "Personalization of natural language generation plays a vital role in a large spectrum of tasks, such as explainable recommendation, review summarization and dialog systems. In these tasks, user and item IDs are important identifiers for personalization. Transformer, which is demonstrated with strong language modeling capability, however, is not personalized and fails to make use of the user and item IDs since the ID tokens are not even in the same semantic space as the words. To address this problem, we present a PErsonalized Transformer for Explainable Recommendation (PETER1), on which we design a simple and effective learning objective that utilizes the IDs to predict the words in the target explanation, so as to endow the IDs with linguistic meanings and to achieve personalized Transformer. Besides generating explanations, PETER can also make recommendations, which makes it a unified model for the whole recommendationexplanation pipeline. Extensive experiments show that our small unpretrained model outperforms fine-tuned BERT on the generation task, in terms of both effectiveness and efficiency, which highlights the importance and the nice utility of our design.",
    "creator" : "LaTeX with hyperref package"
  }
}