{
  "name" : "2021.acl-long.228.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Marginal Utility Diminishes: Exploring the Minimum Knowledge for BERT Knowledge Distillation",
    "authors" : [ "Yuanxin Liu", "Fandong Meng", "Zheng Lin", "Weiping Wang", "Jie Zhou" ],
    "emails" : [ "liuyuanxin@iie.ac.cn,", "linzheng@iie.ac.cn,", "wangweiping@iie.ac.cn,", "fandongmeng@tencent.com", "withtomzhou@tencent.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2928–2941\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2928"
    }, {
      "heading" : "1 Introduction",
      "text" : "Since the launch of BERT (Devlin et al., 2019), pre-trained language models (PLMs) have been advancing the state-of-the arts (SOTAs) in a wide range of NLP tasks. At the same time, the growing\n∗ Work was done when Yuanxin Liu was an intern at Pattern Recognition Center, WeChat AI, Tencent Inc, China.\n† Zheng Lin is the corresponding author.\nsize of PLMs has inspired a wave of research interest in model compression (Han et al., 2016) in the NLP community, which aims to facilitate the deployment of the powerful PLMs to resource-limited scenarios.\nKnowledge distillation (KD) (Hinton et al., 2015) is an effective technique in model compression. In conventional KD, the student model is trained to imitate the teacher’s prediction over classes, i.e., the soft labels. Subsequently, Romero et al. (2015) find that the intermediate representations in the teacher’s hidden layers can also serve as a useful source of knowledge. As an initial attempt to introduce this idea to BERT compression, PKD (Sun et al., 2019) proposed to distill representations of the [CLS] token in BERT’s hidden layers, and later studies (Jiao et al., 2020; Sun et al., 2020; Hou et al., 2020; Liu et al., 2021) extend the distillation of hidden state knowledge (HSK) to all the tokens.\nIn contrast to the previous work that attempts to increase the amount of HSK, in this paper we explore towards the opposite direction to “compress” HSK. We make the observation that although distilling HSK is helpful, the marginal utility diminishes quickly as the amount of HSK increases. To understand this effect, we conduct a series of analysis\nby compressing the HSK from three dimensions, namely depth, length and width (see Section 2.3 for detailed description). We first compress each single dimension and compare a variety of strategies to extract crucial knowledge. Then, we jointly compress the three dimensions using a set of compression configurations, which specify the amount of HSK assigned to each dimension. Figure 1 shows the results on QNLI dataset. We can find that 1) perceivable performance improvement can be obtained by extracting and distilling the crucial HSK, and 2) with only a tiny fraction of HSK the students can achieve the same performance as extensive HSK distillation.\nBased on the second finding, we further propose an efficient paradigm to distill HSK. Concretely, we run BERT over the training set to obtain and store a subset of HSK. This can be done on cloud devices with sufficient computational capability. Given a target device with limited resource, we can compress BERT and select the amount of HSK accordingly. Then, the compressed model can perform KD on either the cloud or directly on the target device using the selected HSK and the original training data, dispensing with the need to load the teacher model.\nIn summary, our maojor contributions are:\n• We observe the marginal utility diminishing effect of HSK in BERT KD. To our knowledge, we are the first attempt to systematically study knowledge compression in BERT KD.\n• We conduct exploratory studies on how to extract the crucial knowledge in HSK, based on which we obtain perceivable improvements over a widely-used HSK distillation strategy.\n• We propose an efficient KD paradigm based on the empirical findings. Experiments on the GLUE benchmark for NLU (Wang et al., 2019) show that, the proposal gives rise to training speedup of 2.7× ∼3.4× for TinyBERT and ROSITA on GPU and CPU1."
    }, {
      "heading" : "2 Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 BERT Architecture",
      "text" : "The backbone of BERT consists of an embedding layer and L identical Transformer (Vaswani et al., 2017) layers. The input to the embedding layer is a\n1The code is available at https://github.com/ llyx97/Marginal-Utility-Diminishes\ntext sequence x tokenized by WordPiece (Wu et al., 2016). There are two special tokens in x: [CLS] is inserted in the left-most position to aggregate the sequence representation and [SEP] is used to separate text segments. By summing up the token embedding, the position embedding and the segment embedding, the embedding layer outputs a sequence of vectors E = [ e1, · · · , e|x| ] ∈ R|x|×dH , where dH is the hidden size of the model. Then, E passes through the stacked Transformer layers, which can be formulated as:\nHl = Trml (Hl−1) , l ∈ [1, L] (1)\nwhere Hl = [ hl,1, · · · ,hl,|x| ] ∈ R|x|×dH is the outputs of the lth layer and H0 = E. Each Transformer layer is composed of two sub-layers: the multi-head self-attention layer and the feedforward network (FFN). Each sub-layer is followed by a sequence of dropout (Srivastava et al., 2014), residual connection (He et al., 2016) and layer normalization (Ba et al., 2016).\nFinally, for the tasks of NLU, a task-specific classifier is employed by taking as input the representation of [CLS] in the Lth layer."
    }, {
      "heading" : "2.2 BERT Compression with KD",
      "text" : "Knowledge distillation is a widely-used technique in model compression, where the compressed model (student) is trained under the guidance of the original model (teacher). This is achieved by minimizing the difference between the features produced by the teacher and the student:\nLKD = ∑\n(fS ,fT )\nL ( fS(x), fT (x) ) (2)\nwhere ( fS , fT ) is a pair of features from student and teacher respectively. L is the loss function and x is a data sample. In terms of BERT compression, the predicted probability over classes, the intermediate representations and the self-attention distributions can be used as the features to transfer. In this paper, we focus on the intermediate representations {Hl}Ll=0 (i.e., the HSK), which have shown to be a useful source of knowledge in BERT compression. The loss function is computed as the Mean Squared Error (MSE) in a layer-wise way:\nLHSK = L ′∑\nl=0\nMSE ( HSl W,H T g(l) ) (3)\nwhere L ′\nis the student’s layer number and g(l) is the layer mapping function to select teacher layers. W ∈ RdSH×dTH is the linear transformation to project the student’s representations HSl to the same size as the teacher’s representation HTl ."
    }, {
      "heading" : "2.3 HSK Compression",
      "text" : "According to Equation 3, the HSK from teacher can be stacked into a tensor ĤT =[ HTg(0), · · · ,H T g(L′ ) ] ∈ R(L ′ +1)×|x|×dTH , which consists of three structural dimensions, namely depth, length and width. For the depth dimension, ĤT can be compressed by eliminating entire layers. By dropping the representations corresponding to particular tokens, we compress the length dimension. When it comes to the width dimension, we set the eliminated activations to zero. We will discuss the strategies to compress each dimension later in Section 4."
    }, {
      "heading" : "3 Experimental Setups",
      "text" : ""
    }, {
      "heading" : "3.1 Datasets",
      "text" : "We perform experiments on seven tasks from the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019): CoLA (linguistic acceptability), SST-2 (sentiment analysis), RTE, QNLI, MNLI-m and MNLI-mm (natural language inference), MRPC and STS-B (semantic matching/similarity). Due to space limitation, we only report results on CoLA, SST-2, QNLI and MNLI for single-dimension HSK compression in Section 4, and results on the other three tasks are presented in Appendix E."
    }, {
      "heading" : "3.2 Evaluation",
      "text" : "Following (Devlin et al., 2019), for the dev set, we use Matthew’s correlation and Spearman correlation to evaluate the performance on CoLA and STS-B respectively. For the other tasks, we report the classification accuracy. We use the dev set to conduct our exploratory studies and the test set results are reported to compare HSK compression with the existing distillation strategy. For the test set of MRPC, we report the results of F1 score."
    }, {
      "heading" : "3.3 Implementation Details",
      "text" : "We take two representative KD-based methods, i.e., TinyBERT (Jiao et al., 2020) and ROSITA (Liu et al., 2021), as examples to conduct our analysis. TinyBERT is a compact version of BERT that is randomly initialized. It is trained with two-stage\nKD: first on the unlabeled general domain data and then on the task-specific training data. ROSITA replaces the first stage KD with structured pruning and matrix factorization, which can be seen as a direct transfer of BERT’s knowledge from the model parameters.\nWe focus on KD with the task-specific training data and do not use any data augmentation. For TinyBERT, the student model is initialized with the 4-layer general distillation model provided by Jiao et al. (2020) (denoted as TinyBERT4). For ROSITA, we first fine-tune BERTBASE on the downstream task and then compress it following Liu et al. (2021) to obtain a 6-layer student model (denoted as ROSITA6). The fine-tuned BERTBASE is used as the shared teacher for TinyBERT and ROSITA. Following Jiao et al. (2020), we first conduct HSK distillation as in Equation 3 (w/o distilling the self-attention distribution) and then distill the teacher’s predictions using crossentropy loss. All the results are averaged over three runs with different random seeds. The model architecture of the students and the hyperparameter settings can be seen in Appendix A and Appendix B respectively."
    }, {
      "heading" : "4 Single-Dimension Knowledge Compression",
      "text" : "Researches on model pruning have shown that the structural units in a model are of different levels of importance, and the unimportant ones can be dropped without affecting the performance. In this section, we investigate whether the same law holds for HSK compression in KD. We study the three dimensions separately and compare a variety of strategies to extract the crucial knowledge. When a certain dimension is compressed, the other two dimensions are kept to full scale."
    }, {
      "heading" : "4.1 Depth Compression",
      "text" : ""
    }, {
      "heading" : "4.1.1 Compression Strategies",
      "text" : "From the layer point of view, HSK compression can be divided into two steps. First, the layer mapping function g(l) selects one of the teacher layers for each student layer. This produces L ′ + 1 pairs of teacher-student features:[\n(HS0 ,H T g(0)), · · · , (H S L ′ ,HT g(L ′ ) ) ] . Second, a subset of these feature pairs are selected to perform HSK distillation.\nFor the first step, a simple but effective strategy\nis the uniform mapping function:\ng(l) = l × L L′ ,mod(L,L ′ ) = 0 (4)\nIn this way, the teacher layers are divided into L ′ blocks and the top layer of each block serves as the guidance in KD. Recently, Wang et al. (2020a) empirically show that the upper-middle layers of BERT, as compared with the top layer, are a better choice to guide the top layer of student in selfattention distillation. Inspired by this, we redesign Equation 4 to allow the top student layer to distill knowledge from an upper-middle teacher layer, and the lower layers follow the uniform mapping principle. This function can be formulated as:\ng(l, Ltop) = l × round(L top\nL′ ) (5)\nwhere Ltop is the teacher layer corresponding to the top student layer and round() is the roundingoff operation. Figure 2 gives an illustration of g(l, Ltop) with a 6-layer teacher and a 3-layer student. Specifically, for the 12-layer BERTBASE teacher, we select Ltop from {8, 10, 12}. For the second step, we simply keep the top ND feature pairs: {(HSl ,HTg(l,Ltop))} L ′ l=L′−ND+1."
    }, {
      "heading" : "4.1.2 Results and Analysis",
      "text" : "Figure 3 presents the results of depth compression with different layer mapping functions. We can find that: 1) For the g(l, 12) mapping function (the grey lines), depth compression generally has a negative impact on the students’ performance. Specially, the performance of ROSITA6 declines drastically when the number of layers is reduced to 1 ∼ 3. 2) In terms of the g(l, 10) and g(l, 8) mapping functions (the blue and orange lines), HSK distillation with only one or two layers can achieve comparable performance as using all the L ′ + 1 layers.\nOn the QNLI and MNLI datasets, the performance can even be improved by eliminating the lower layers. 3) In general, the student achieves better results with the redesigned layer mapping function in Equation 5 across the four tasks. This demonstrates that, like the self-attention knowledge, the most crucial HSK does not necessarily reside in the top BERT layer, which reveals a potential way to improve HSK distillation of BERT. 4) Compared with g(l, 8), the improvement brought by g(l, 10) is more stable across different tasks and student models. Therefore, we use the g(l, 10) layer mapping function when investigating the other two dimensions."
    }, {
      "heading" : "4.2 Length Compression",
      "text" : ""
    }, {
      "heading" : "4.2.1 Compression Strategies",
      "text" : "To compress the length dimension, we design a method to measure the tokens’ importance by using the teacher’s self-attention distribution. The intuition is that self-attention controls the information flow among tokens across layers, and thus the representations of the most attended tokens may contain crucial information.\nAssuming that the teacher has Ah attention heads, and the attention weights in the lth layer\nis ATl = { ATl,a }Ah a=1\n, where ATl,a ∈ R|x|×|x| is the attention matrix of the ath head. Each row of ATl,a is the attention distribution of a particular token to all the tokens. In our length compression strategy, the importance score of the tokens is the attention distribution of the [CLS] token (i.e., the first row in\nATl,a) averaged over the Ah heads:\nSl = 1\nAh Ah∑ a=1 ATl,a,1,Sl ∈ R|x| (6)\nTo match the depth of the student, we employ the layer mapping function in Equation 5 to select Sg(l,Ltop) for the lth student layer.\nThe length compression strategies examined in this section are summarized as:\nAtt is the attention-based strategy as described above. The layer mapping function to select S is the same as the one to select HSK, i.e., g(l, 10).\nAtt w/o [SEP] excludes the HSK of the special token [SEP]. The rationality of this operation will be explained in the following analysis.\nAtt (Ltop = 12) w/o [SEP] is different from Att w/o [SEP] in that it utilizes g(l, 12) to select S.\nLeft is a naive baseline that discards tokens from the tail of the text sequence. When the token number is reduced to 1, the student only distills the HSK from the [CLS] token."
    }, {
      "heading" : "4.2.2 Results and Analysis",
      "text" : "The length compression results are shown in Figure 4 and Figure 5. We can derive the following observations: 1) For all strategies, significant performance decline can only be observed when HSK length is compressed heavily (to less than 0.05 ∼ 0.30). In some cases, using a subset of tokens’ representation even leads to perceivable\nimprovement over the full length (e.g., ROSITA6 on CoLA and TinyBERT4 on SST-2 and QNLI). 2) The performance of Att is not satisfactory. When being applied to ROSITA6, the Att strategy underperforms the Left baseline. The results of Att in TinyBERT4, though better than those in ROSITA6, still lag behind the other strategies at the left-most points. 3) Excluding [SEP] in the Att strategy alleviates the drop in performance, especially when HSK length is compressed to less than 0.05. 4) As a general trend, further improvement over Att w/o [SEP] can be obtained by using g(l, 12) in the selection of S, which produces the most robust results among the four strategies.\nTo explain why the Att strategy performs poorly, we inspect into the tokens that receive the highest importance scores under Equation 6. We find that the special token [SEP] is dominant in most hidden layers. As shown in Figure 6, from the 4th ∼ 10th\nlayers, [SEP] is the most attended token for almost all training samples. Meanwhile, [SEP] frequently appears in the top three positions across all the layers. Similar phenomenon was found in Clark et al. (2019), where [SEP] receives high attention scores from itself and other tokens in the middle layers. Combining this phenomenon and the results in Figure 4 and Figure 5, it can be inferred that the representations of [SEP] is not a desirable source of knowledge for ROSITA and TinyBERT. We conjecture that this is because there exists some trivial patterns in the representations of [SEP], which prevents the student to extract the informative features that are more relevant to the task."
    }, {
      "heading" : "4.3 Width Compression",
      "text" : ""
    }, {
      "heading" : "4.3.1 Compression Strategies",
      "text" : "As discussed in Section 2.3, the width dimension is compressed by setting some activations in the intermediate representations to zero. Practically, we apply a binary mask M ∈ RdTH to the vectors in HTl , which gives rise to [ M hTl,1, · · · ,M hTl,|x| ] , where denotes the element-wise product. On this basis, we introduce and compare three masking designs for width compression:\nRand Mask randomly set the values in M to zero, where the total number of “0” is controlled by the compression ratio. This mask is static, i.e., hTl,i(∀i, l) for all the training samples share the same mask.\nUniform Mask is also a static mask. It is constructed by distributing “0” in a uniform way. For-\nmally, the mask M is defined as:\nMi = { 1, i ∈ I 0, otherwise\n(7)\nwhere I = { round ( i× d T H\nNW )}NW i=1 is the indices\nof the remained NW activations.\nMag Mask masks out the activations with low magnitude. Therefore, this mask is dynamic, i.e., every hTl,i(∀i, l) has its own M."
    }, {
      "heading" : "4.3.2 Results and Analysis",
      "text" : "The width compression results can be seen in Figure 7, from which we can obtain two findings. First, the masks reveal different patterns when combined with different student models. For ROSITA6, the performance of Rand Mask and Uniform Mask decreases sharply at 20% HSK width. In comparison, the performance change is not that significant when it comes to TinyBERT4. This suggests that TinyBERT4 is more robust to HSK width compression than ROSITA6. Second, the magnitude-based masking strategy obviously outperforms Rand Mask and Uniform Mask. As we compress the nonzero activations in HSK from 100% to 20%, the performance drop of Mag Mask is only marginal, indicating that there exists considerable knowledge redundancy in the width dimension."
    }, {
      "heading" : "5 Three-Dimension Joint Knowledge Compression",
      "text" : "With the findings in single-dimension compression, we are now at a position to investigate joint HSK compression from the three dimensions."
    }, {
      "heading" : "5.1 Measuring the Amount of HSK",
      "text" : "For every single dimension, measuring the amount of HSK is straightforward: using the number of layers, tokens and activations for depth, length and width respectively. In order to quantify the total amount of HSK (denoted as AHSK), we define one unit of AHSK as the amount of HSK in any hTl,i(∀l ∈ [0, L], i ∈ [1, |x|]). In other words, the AHSK of ĤT equals to (L ′ +1)×|x|. When HSK is compressed to ND layers, NL tokens and NW activations, the AHSK is ND ×NL × NW dTH ."
    }, {
      "heading" : "5.2 Compression Configurations & Strategies",
      "text" : "Formally, the triplet (ND, NL, NW ) defines a search space ∈ R(L ′ +1)×|x|×dTH of the configu-\nrations for three-dimension (3D) HSK compression, and we could have multiple combinations of (ND, NL, NW ) that satisfy a particular AHSK . In practice, we reconstruct the search space as:\nND ∈ [1, L′ + 1], NL ∈ [1, 50], NW dTH ∈ {0.1× i}10i=1\n(8) To study the student’s performance with different amounts of HSK, we sample a set of configurations for a range of AHSK , the statistics of which is summarized in Table 1. Details of the configurations\ncan be seen in Appendix C. To compress each single dimension in joint HSK compression, we utilize the most advantageous strategies that we found in Section 4. Specifically, Att (Ltop = 12) w/o [SEP] is used to compress length, Mag Mask is used to compress width and the g(l, Ltop) for depth compression is selected according to the performance of depth compression."
    }, {
      "heading" : "5.3 Results and Analysis",
      "text" : "The results of 3D joint HSK compression are presented in Figure 8 and Figure 9. As we can see, introducing HSK in KD brings consistent improvement to the conventional prediction distillation method. However, the marginal benefit quickly diminishes as more HSK is included. Typically, with less than 1% of HSK, the student models can achieve the same or better result as full-scale HSK distillation. Over a certain threshold of AHSK , the\nperformance begins to decrease. Among different tasks and student models, the gap between the best results (peaks on the blue lines) and full-scale HSK distillation varies from 0.3 (ROSITA6 on MNLI and STS-B) to 5.3 (TinyBERT4 on CoLA). The results also suggest that existing BERT distillation method (i.e., g(l, 12)) can be improved by simply compressing HSK: Numerous points of different configurations lie over the red stars.\nTable 2 presents the results of different KDbased BERT compression methods. For fair comparison, we do not include other methods described in Section 7, because they either distill different type of knowledge or use different student model structure. Here, we focus on comparing the performance with or without HSK compression given the same student model. We can see that except for the results of a few tasks on the test sets, HSK compression consistently promotes the performance of the baseline methods."
    }, {
      "heading" : "6 Improving Training Efficiency",
      "text" : "Existing BERT compression methods mostly focus on improving the inference efficiency. However, the teacher model is used to extract features throughout the training process, which suggests that the training efficiency still has room for improvement. As shown in Figure 10, the compressed models achieve considerable inference speedup, while the increase in training speed is relatively small. Moreover, for students with different sizes or architectures, the teacher should be deployed every time when training a new student. Intuitively, we can run the teacher once and reuse the features for all the students. In this way, we do not need to load the teacher model while training the student, and thereby increasing the training speed. We refer\nto this strategy as offline HSK distillation 2. To evaluate the training efficiency of the proposed KD paradigm, we compute the training time on the MNLI dataset. The results are presented in the left plots of Figure 10. As we can see, offline HSK distillation increases the training speed of the student models, as compared with online distillation. The speedup is consistent for different student models and devices.\nDespite the training speedup, however, loading and storing HSK increases the memory consumption. The full set of HSK can take up a large amount of space, especially for the pre-trained language models like BERT. Fortunately, our findings in the previous sections suggest that the student only requires a tiny fraction of HSK.\nTable 3 summarizes the actual memory consump-\n2In the literature (Gou et al., 2020), “offline distillation” also means the teacher parameters are fixed during KD, which is different from our definition here.\n(ND, NL, NW ) AHSK Feature Size (GB) Mag Mask Size (GB)\ntion of four configurations with different AHSK . As we can see, the full set of HSK for ROSITA6 takes up approximately 1 TB of memory space, which is only applicable to some high-end cloud servers. Compressing the HSK can reduce the size to GB level, which enables training on devices like personal computers. It is worth noticing that storing the dynamic Mag Mask is consuming, which typically accounts for more space than HSK. However, the binary masks can be further compressed using some data compression algorithms.\nBased on the above results and analysis, we summarize our paradigm for efficient HSK distillation as: First, the teacher BERT runs on the training data to obtain and store the features of HSK and predictions. This can be done on devices that have sufficient computing and memory resources. Then, according to the target application and device, we decide the student’s structure and the amount of HSK to distill. Finally, KD can be performed on a cloud server or directly on the target device."
    }, {
      "heading" : "7 Related Work",
      "text" : "KD is widely studied in BERT compression. In addition to distilling the teacher’s predictions as in Hinton et al. (2015), researches have shown that the student’s performance can be improved by using the representations from intermediate BERT layers (Sun et al., 2019; Liu et al., 2021; Hou et al., 2020) and the self-attention distributions (Jiao et al., 2020; Sun et al., 2020). Typically, the knowledge is extensively distilled in a layer-wise manner. To fully utilize BERT’s knowledge, some recent work also proposed to combine multiple teacher layers in BERT KD (Passban et al., 2021; Li et al., 2020) or KD on Transformer-based NMT models (Wu et al., 2020). In contrast to these studies that attempt to increase the amount knowledge, we study BERT KD from the compression point of view. Similar idea can be found in MiniLMs (Wang et al., 2020a,b), which only use the teacher’s knowledge to guide the last layer of student. However, they only con-\nsider knowledge from the layer dimension, while we investigate the three dimensions of HSK.\nWe explore a variety of strategies to determine feature importance for each single dimension. This is related to a line of studies called the attribution methods, which attempt to attribute a neural network’s prediction to the input features. The attention weights have also been investigated as an attribution method. However, prior work (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019; Brunner et al., 2020; Hao et al., 2020) finds that attention weights usually fail to correlate well with their contributions to the final prediction. This echoes with our finding that the original Att strategy performs poorly in length compression. However, the attention weights may play different roles in attribution and HSK distillation. Whether the findings in attribution are transferable to HSK distillation is still a problem that needs further investigation."
    }, {
      "heading" : "8 Conclusions and Future Work",
      "text" : "In this paper, we investigate the compression of HSK in BERT KD. We divide the HSK of BERT into three dimensions and explore a range of compression strategies for each single dimension. On this basis, we jointly compress the three dimensions and find that, with a tiny fraction of HSK, the student can achieve the same or even better performance as distilling the full-scale knowledge. Based on this finding, we propose a new paradigm to improve the training efficiency in BERT KD, which does not require loading the teacher model during training. The experiments show that the training speed can be increased by 2.7× ∼ 3.4× for two kinds of student models and two types of CPU and GPU devices.\nMost of the compression strategies investigated in this study are heuristic, which still have room for improvement. Therefore, a future direction of our work could be designing more advanced algorithm to search for the most useful HSK in BERT KD. Additionally, since HSK distillation in the pre-training stage is orders of magnitude time-consuming than task-specific distillation, the marginal utility diminishing effect in pre-training distillation is also a problem worth studying."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by National Natural Science Foundation of China (No. 61976207, No. 61906187)."
    }, {
      "heading" : "A Architecture of Student Models",
      "text" : "TinyBERT (Jiao et al., 2020) rescales the structure of BERT from the number of layers, the dimension of the Transformer layer outputs, and the hidden dimension of feed-forward networks. We use the 4-layer version (14.5M parameters) of TinyBERT that is released by Jiao et al. (2020).\nROSITA (Liu et al., 2021) compresses BERT from four structural dimensions, namely the layer, attention heads, the hidden dimension of the feedforward network and the rank of SVD to compress the embedding matrix. In practice, we scale the four dimensions to construct a 6-layer model ROSITA6 that has approximately the same size as TinyBERT4. ROSITA6 has 6 layers and 2 attention heads, and the FFN dimension and embedding matrix rank are 768 and 128 respectively."
    }, {
      "heading" : "B Hyperparameters",
      "text" : "Following Jiao et al. (2020), we first distill HSK and then distill the teacher’s predictions. The hyperparamers for HSK distillation basically follow Jiao et al. (2020), except that the training epoch of CoLA is changed from 50 to 30, the training epoch of QNLI is changed from 10 to 5, and the batch size for MNLI and QNLI is changed from 256 to 64. For prediction distillation, we use the linear decaying learning rate schedule. For each model and dataset, we tune the number of epoch (from {5, 10}) and learning rate (from {2e−5, 5e−5}) for the baseline method that use the uniform layer-wise strategy g(l, 12), and the hyperparameters are used for all the results with compressed HSK. Table 4 summarizes the hyperparameters."
    }, {
      "heading" : "C Configurations of 3D Compression Strategy",
      "text" : "As described in the paper, for each AHSK we can obtain a number of configurations. Specifically, when we use the ROSITA6 there are 13, 21, 45, 75, 112 configurations for AHSK = 1± 10%, 3± 10%, 5± 10%, 10± 10%, 50± 10% respectively. We randomly sample subsets of configurations in our experiments, the statistics of which is shown in\nTable 1. The configurations that exceed the layer constrain are excluded for TinyBERT4. The detailed configurations for different AHSK are summarized in Table 5."
    }, {
      "heading" : "D Experimental Settings for Efficiency Evaluation",
      "text" : "In Figure 9, we show the training and inference time of two models on two devices. The training time is computed as the time to run 500 training steps (i.e, batches of data). When it comes to inference, we run the models on the entire training set and dev set for GPU and CPU respectively. For training, the batch size is set to 64 and 16 for GPU and CPU respectively. For inference, we set the batch size to 128 and 1 for GPU and CPU respectively. The maximum sequence length is 128 for all the settings. For offline distillation, we use the configuration (1, 9, 0.1)."
    }, {
      "heading" : "E More Experimental Results",
      "text" : "E.1 Full Results of Depth Compression\nDepth compression results on all seven tasks are presented in Figure 11. Like the results on CoLA, SST-2, QNLI and MNLI, the results on MRPC, RTE and STS-B also suggest that the redesigned mapping functions (i.e., g(l, 8) and g(l, 10)) generally outperforms the original uniform mapping function g(l, 12), especially when HSK is compressed to one layer.\nE.2 Full Results of Length Compression\nLength compression results on all seven tasks are presented in Figure 12 and Figure 13. As we can see, the general trends on MRPC, RTE and STS-B are in accordance with the other four tasks, whose results are discussed in Section 4.2.2. Significant performance drop only occurs when length is compressed to less than 0.05 on MRPC, RTE and STSB. The strategy base on original attentinon weights (i.e., Att) performs poorly with small HSK length. In comparison, Att w/o [SEP] and Att (Ltop = 12) w/o [SEP] reduce the performance drop caused by length compression.\nFigure 14 shows the proportion of data samples where [SEP] is the top1 and top3 most attended token. We can see that for most data samples, [SEP] is among the top3 tokens and frequently appears as the top1 from the 4th ∼ 10th layers. This pattern is consistent across the seven tasks.\nE.3 Full Results of Width Compression Figure 15 shows the full results of width compression on all seven tasks. We can see that the gap between compression strategies is larger for ROSITA6, as compared with TinyBERT4. Among the three strategies, Mag Mask clearly outperforms Rand Mask and Uniform Mask.\n1±10% 3±5% 5±5% 10±5% 50±5%\n(2, 1, 0.5) (6, 1, 0.5) (1, 13, 0.4) (7, 3, 0.5) (6, 10, 0.8) (5, 2, 0.1) (6, 5, 0.1) (6, 2, 0.4) (1, 49, 0.2) (2, 36, 0.7) (1, 9, 0.1) (1, 29, 0.1) (5, 5, 0.2) (3, 8, 0.4) (3, 25, 0.7) (1, 2, 0.5) (1, 10, 0.3) (2, 24, 0.1) (3, 5, 0.7) (2, 27, 0.9) (1, 5, 0.2) (1, 5, 0.6) (1, 25, 0.2) (1, 25, 0.4) (6, 9, 0.9) (1, 1, 1.0) (2, 15, 0.1) (1, 6, 0.8) (2, 7, 0.7) (6, 12, 0.7) (3, 3, 0.1) (5, 2, 0.3) (7, 7, 0.1) (3, 16, 0.2) (6, 27, 0.3) (3, 1, 0.3) (5, 1, 0.6) (6, 8, 0.1) (7, 14, 0.1) (7, 24, 0.3) (1, 10, 0.1) (5, 6, 0.1) (4, 4, 0.3) (2, 8, 0.6) (4, 18, 0.7) (2, 5, 0.1) (3, 5, 0.2) (2, 6, 0.4) (5, 3, 0.7) (3, 21, 0.8) (1, 1, 0.9) (1, 30, 0.1) (2, 3, 0.8) (3, 7, 0.5) (5, 10, 1.0) (5, 1, 0.2) (5, 3, 0.2) (3, 8, 0.2) (4, 8, 0.3) (7, 23, 0.3) (1, 3, 0.3) (1, 6, 0.5) (4, 12, 0.1) (5, 7, 0.3) (5, 25, 0.4)"
    } ],
    "references" : [ {
      "title" : "Layer normalization",
      "author" : [ "Lei Jimmy Ba", "Jamie Ryan Kiros", "Geoffrey E. Hinton." ],
      "venue" : "CoRR, abs/1607.06450.",
      "citeRegEx" : "Ba et al\\.,? 2016",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2016
    }, {
      "title" : "On identifiability in transformers",
      "author" : [ "Gino Brunner", "Yang Liu", "Damian Pascual", "Oliver Richter", "Massimiliano Ciaramita", "Roger Wattenhofer." ],
      "venue" : "ICLR. OpenReview.net.",
      "citeRegEx" : "Brunner et al\\.,? 2020",
      "shortCiteRegEx" : "Brunner et al\\.",
      "year" : 2020
    }, {
      "title" : "What does bert look at? an analysis of bert’s attention",
      "author" : [ "Kevin Clark", "Urvashi Khandelwal", "Omer Levy", "Christopher D. Manning." ],
      "venue" : "BlackBoxNLP@ACL.",
      "citeRegEx" : "Clark et al\\.,? 2019",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL-HLT (1), pages 4171–4186. Association for Computational Linguistics.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Knowledge distillation: A survey",
      "author" : [ "Jianping Gou", "Baosheng Yu", "Stephen John Maybank", "Dacheng Tao." ],
      "venue" : "CoRR, abs/2006.05525.",
      "citeRegEx" : "Gou et al\\.,? 2020",
      "shortCiteRegEx" : "Gou et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding",
      "author" : [ "Song Han", "Huizi Mao", "William J. Dally." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Han et al\\.,? 2016",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2016
    }, {
      "title" : "Selfattention attribution: Interpreting information interactions inside transformer",
      "author" : [ "Yaru Hao", "Li Dong", "Furu Wei", "Ke Xu." ],
      "venue" : "CoRR, abs/2004.11207.",
      "citeRegEx" : "Hao et al\\.,? 2020",
      "shortCiteRegEx" : "Hao et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "CVPR, pages 770–778. IEEE Computer Society.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey E. Hinton", "Oriol Vinyals", "Jeffrey Dean." ],
      "venue" : "CoRR, abs/1503.02531.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Dynabert: Dynamic BERT with adaptive width and depth",
      "author" : [ "Lu Hou", "Zhiqi Huang", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Qun Liu." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Hou et al\\.,? 2020",
      "shortCiteRegEx" : "Hou et al\\.",
      "year" : 2020
    }, {
      "title" : "Tinybert: Distilling BERT for natural language understanding",
      "author" : [ "Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu." ],
      "venue" : "EMNLP (Findings), pages 4163–4174. Association for Computational Linguis-",
      "citeRegEx" : "Jiao et al\\.,? 2020",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT-EMD: many-to-many layer mapping for BERT compression with earth mover’s distance",
      "author" : [ "Jianquan Li", "Xiaokang Liu", "Honghong Zhao", "Ruifeng Xu", "Min Yang", "Yaohong Jin." ],
      "venue" : "EMNLP (1), pages 3009–3018. Association for Computational",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Rosita: Refined bert compression with integrated techniques",
      "author" : [ "Yuanxin Liu", "Zheng Lin", "Fengcheng Yuan." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Liu et al\\.,? 2021",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "ALP-KD: attention-based layer projection for knowledge distillation",
      "author" : [ "Peyman Passban", "Yimeng Wu", "Mehdi Rezagholizadeh", "Qun Liu." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Passban et al\\.,? 2021",
      "shortCiteRegEx" : "Passban et al\\.",
      "year" : 2021
    }, {
      "title" : "Fitnets: Hints for thin deep nets",
      "author" : [ "Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio." ],
      "venue" : "ICLR (Poster).",
      "citeRegEx" : "Romero et al\\.,? 2015",
      "shortCiteRegEx" : "Romero et al\\.",
      "year" : 2015
    }, {
      "title" : "Is attention interpretable? In ACL (1), pages 2931–2951",
      "author" : [ "Sofia Serrano", "Noah A. Smith." ],
      "venue" : "Association for Computational Linguistics.",
      "citeRegEx" : "Serrano and Smith.,? 2019",
      "shortCiteRegEx" : "Serrano and Smith.",
      "year" : 2019
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "J. Mach. Learn. Res., 15(1):1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Patient knowledge distillation for BERT model compression",
      "author" : [ "Siqi Sun", "Yu Cheng", "Zhe Gan", "Jingjing Liu." ],
      "venue" : "EMNLP/IJCNLP (1), pages 4322–4331. Association for Computational Linguistics.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Mobilebert: a compact task-agnostic BERT for resource-limited devices",
      "author" : [ "Zhiqing Sun", "Hongkun Yu", "Xiaodan Song", "Renjie Liu", "Yiming Yang", "Denny Zhou." ],
      "venue" : "ACL, pages 2158–2170. Association for Computational Linguistics.",
      "citeRegEx" : "Sun et al\\.,? 2020",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "NIPS, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman." ],
      "venue" : "ICLR (Poster). OpenReview.net.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Minilmv2: Multi-head selfattention relation distillation for compressing pretrained transformers",
      "author" : [ "Wenhui Wang", "Hangbo Bao", "Shaohan Huang", "Li Dong", "Furu Wei." ],
      "venue" : "CoRR, abs/2012.15828.",
      "citeRegEx" : "Wang et al\\.,? 2020a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers",
      "author" : [ "Wenhui Wang", "Furu Wei", "Li Dong", "Hangbo Bao", "Nan Yang", "Ming Zhou." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Wang et al\\.,? 2020b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is not not explanation",
      "author" : [ "Sarah Wiegreffe", "Yuval Pinter." ],
      "venue" : "EMNLP/IJCNLP (1), pages 11–20. Association for Computational Linguistics.",
      "citeRegEx" : "Wiegreffe and Pinter.,? 2019",
      "shortCiteRegEx" : "Wiegreffe and Pinter.",
      "year" : 2019
    }, {
      "title" : "Why skip if you can combine: A simple knowledge distillation technique for intermediate layers",
      "author" : [ "Yimeng Wu", "Peyman Passban", "Mehdi Rezagholizadeh", "Qun Liu." ],
      "venue" : "EMNLP (1), pages 1016–1021. Association for Computational Linguistics.",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Since the launch of BERT (Devlin et al., 2019), pre-trained language models (PLMs) have been advancing the state-of-the arts (SOTAs) in a wide range of NLP tasks.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 12,
      "context" : "Figure 1: The Acc variation of ROSITA (Liu et al., 2021) and TinyBERT (Jiao et al.",
      "startOffset" : 38,
      "endOffset" : 56
    }, {
      "referenceID" : 10,
      "context" : ", 2021) and TinyBERT (Jiao et al., 2020) on QNLI with the increase of HSK.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 5,
      "context" : "size of PLMs has inspired a wave of research interest in model compression (Han et al., 2016) in the NLP community, which aims to facilitate the deployment of the powerful PLMs to resource-limited scenarios.",
      "startOffset" : 75,
      "endOffset" : 93
    }, {
      "referenceID" : 8,
      "context" : "Knowledge distillation (KD) (Hinton et al., 2015) is an effective technique in model compression.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 17,
      "context" : "As an initial attempt to introduce this idea to BERT compression, PKD (Sun et al., 2019) proposed to distill representations of the [CLS] token in BERT’s hidden layers, and later studies (Jiao et al.",
      "startOffset" : 70,
      "endOffset" : 88
    }, {
      "referenceID" : 10,
      "context" : ", 2019) proposed to distill representations of the [CLS] token in BERT’s hidden layers, and later studies (Jiao et al., 2020; Sun et al., 2020; Hou et al., 2020; Liu et al., 2021) extend the distillation of hidden state knowledge (HSK) to all the tokens.",
      "startOffset" : 106,
      "endOffset" : 179
    }, {
      "referenceID" : 18,
      "context" : ", 2019) proposed to distill representations of the [CLS] token in BERT’s hidden layers, and later studies (Jiao et al., 2020; Sun et al., 2020; Hou et al., 2020; Liu et al., 2021) extend the distillation of hidden state knowledge (HSK) to all the tokens.",
      "startOffset" : 106,
      "endOffset" : 179
    }, {
      "referenceID" : 9,
      "context" : ", 2019) proposed to distill representations of the [CLS] token in BERT’s hidden layers, and later studies (Jiao et al., 2020; Sun et al., 2020; Hou et al., 2020; Liu et al., 2021) extend the distillation of hidden state knowledge (HSK) to all the tokens.",
      "startOffset" : 106,
      "endOffset" : 179
    }, {
      "referenceID" : 12,
      "context" : ", 2019) proposed to distill representations of the [CLS] token in BERT’s hidden layers, and later studies (Jiao et al., 2020; Sun et al., 2020; Hou et al., 2020; Liu et al., 2021) extend the distillation of hidden state knowledge (HSK) to all the tokens.",
      "startOffset" : 106,
      "endOffset" : 179
    }, {
      "referenceID" : 20,
      "context" : "Experiments on the GLUE benchmark for NLU (Wang et al., 2019) show that, the proposal gives rise to training speedup of 2.",
      "startOffset" : 42,
      "endOffset" : 61
    }, {
      "referenceID" : 19,
      "context" : "1 BERT Architecture The backbone of BERT consists of an embedding layer and L identical Transformer (Vaswani et al., 2017) layers.",
      "startOffset" : 100,
      "endOffset" : 122
    }, {
      "referenceID" : 16,
      "context" : "Each sub-layer is followed by a sequence of dropout (Srivastava et al., 2014), residual connection (He et al.",
      "startOffset" : 52,
      "endOffset" : 77
    }, {
      "referenceID" : 7,
      "context" : ", 2014), residual connection (He et al., 2016) and layer normalization (Ba et al.",
      "startOffset" : 29,
      "endOffset" : 46
    }, {
      "referenceID" : 20,
      "context" : "1 Datasets We perform experiments on seven tasks from the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019): CoLA (linguistic acceptability), SST-2 (sentiment analysis), RTE, QNLI, MNLI-m and MNLI-mm (natural language inference), MRPC and STS-B (semantic matching/similarity).",
      "startOffset" : 117,
      "endOffset" : 136
    }, {
      "referenceID" : 3,
      "context" : "2 Evaluation Following (Devlin et al., 2019), for the dev set, we use Matthew’s correlation and Spearman correlation to evaluate the performance on CoLA and STS-B respectively.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 12,
      "context" : ", 2020) and ROSITA (Liu et al., 2021), as examples to conduct our analysis.",
      "startOffset" : 19,
      "endOffset" : 37
    }, {
      "referenceID" : 4,
      "context" : "In the literature (Gou et al., 2020), “offline distillation” also means the teacher parameters are fixed during KD, which is different from our definition here.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 17,
      "context" : "(2015), researches have shown that the student’s performance can be improved by using the representations from intermediate BERT layers (Sun et al., 2019; Liu et al., 2021; Hou et al., 2020) and the self-attention distributions (Jiao et al.",
      "startOffset" : 136,
      "endOffset" : 190
    }, {
      "referenceID" : 12,
      "context" : "(2015), researches have shown that the student’s performance can be improved by using the representations from intermediate BERT layers (Sun et al., 2019; Liu et al., 2021; Hou et al., 2020) and the self-attention distributions (Jiao et al.",
      "startOffset" : 136,
      "endOffset" : 190
    }, {
      "referenceID" : 9,
      "context" : "(2015), researches have shown that the student’s performance can be improved by using the representations from intermediate BERT layers (Sun et al., 2019; Liu et al., 2021; Hou et al., 2020) and the self-attention distributions (Jiao et al.",
      "startOffset" : 136,
      "endOffset" : 190
    }, {
      "referenceID" : 10,
      "context" : ", 2020) and the self-attention distributions (Jiao et al., 2020; Sun et al., 2020).",
      "startOffset" : 45,
      "endOffset" : 82
    }, {
      "referenceID" : 18,
      "context" : ", 2020) and the self-attention distributions (Jiao et al., 2020; Sun et al., 2020).",
      "startOffset" : 45,
      "endOffset" : 82
    }, {
      "referenceID" : 13,
      "context" : "To fully utilize BERT’s knowledge, some recent work also proposed to combine multiple teacher layers in BERT KD (Passban et al., 2021; Li et al., 2020) or KD on Transformer-based NMT models (Wu et al.",
      "startOffset" : 112,
      "endOffset" : 151
    }, {
      "referenceID" : 11,
      "context" : "To fully utilize BERT’s knowledge, some recent work also proposed to combine multiple teacher layers in BERT KD (Passban et al., 2021; Li et al., 2020) or KD on Transformer-based NMT models (Wu et al.",
      "startOffset" : 112,
      "endOffset" : 151
    }, {
      "referenceID" : 24,
      "context" : ", 2020) or KD on Transformer-based NMT models (Wu et al., 2020).",
      "startOffset" : 46,
      "endOffset" : 63
    }, {
      "referenceID" : 23,
      "context" : "However, prior work (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019; Brunner et al., 2020; Hao et al., 2020) finds that attention weights usually fail to correlate well with their contributions to the final prediction.",
      "startOffset" : 20,
      "endOffset" : 113
    }, {
      "referenceID" : 15,
      "context" : "However, prior work (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019; Brunner et al., 2020; Hao et al., 2020) finds that attention weights usually fail to correlate well with their contributions to the final prediction.",
      "startOffset" : 20,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : "However, prior work (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019; Brunner et al., 2020; Hao et al., 2020) finds that attention weights usually fail to correlate well with their contributions to the final prediction.",
      "startOffset" : 20,
      "endOffset" : 113
    }, {
      "referenceID" : 6,
      "context" : "However, prior work (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019; Brunner et al., 2020; Hao et al., 2020) finds that attention weights usually fail to correlate well with their contributions to the final prediction.",
      "startOffset" : 20,
      "endOffset" : 113
    } ],
    "year" : 2021,
    "abstractText" : "Recently, knowledge distillation (KD) has shown great success in BERT compression. Instead of only learning from the teacher’s soft label as in conventional KD, researchers find that the rich information contained in the hidden layers of BERT is conducive to the student’s performance. To better exploit the hidden knowledge, a common practice is to force the student to deeply mimic the teacher’s hidden states of all the tokens in a layer-wise manner. In this paper, however, we observe that although distilling the teacher’s hidden state knowledge (HSK) is helpful, the performance gain (marginal utility) diminishes quickly as more HSK is distilled. To understand this effect, we conduct a series of analysis. Specifically, we divide the HSK of BERT into three dimensions, namely depth, length and width. We first investigate a variety of strategies to extract crucial knowledge for each single dimension and then jointly compress the three dimensions. In this way, we show that 1) the student’s performance can be improved by extracting and distilling the crucial HSK, and 2) using a tiny fraction of HSK can achieve the same performance as extensive HSK distillation. Based on the second finding, we further propose an efficient KD paradigm to compress BERT, which does not require loading the teacher during the training of student. For two kinds of student models and computing devices, the proposed KD paradigm gives rise to training speedup of 2.7× ∼3.4×.",
    "creator" : "LaTeX with hyperref"
  }
}