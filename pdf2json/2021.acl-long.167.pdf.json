{
  "name" : "2021.acl-long.167.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "IrEne: Interpretable Energy Prediction for Transformers",
    "authors" : [ "Qingqing Cao", "Yash Kumar Lal", "Harsh Trivedi", "Aruna Balasubramanian", "Niranjan Balasubramanian" ],
    "emails" : [ "qicao@cs.stonybrook.edu", "ylal@cs.stonybrook.edu", "hjtrivedi@cs.stonybrook.edu", "arunab@cs.stonybrook.edu", "niranjan@cs.stonybrook.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2145–2157\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2145"
    }, {
      "heading" : "1 Introduction",
      "text" : "Accurately measuring the energy consumption of NLP models is becoming ever more important. Models are growing exponentially, with billions, even approaching trillions, of parameters with correspondingly large resource consumption (e.g. GPT-3 (Brown et al., 2020) has 175 billion parameters and Switch Transformers can have 1.6 trillion parameters (Fedus et al., 2021)). Recent works have sought to estimate energy consumption and suggest ways to reduce the resulting costs and car-\nbon impacts (Strubell et al., 2019; Schwartz et al., 2019; Henderson et al., 2020; Anthony et al., 2020)\nUnfortunately, there are no easy-to-use and accurate solutions for measuring or predicting the energy consumption. On the one hand, measuring energy consumption directly through hardware power monitors is not feasible as it requires exclusive access to the hardware and detailed instrumentation. On the other hand, there are software models that predict energy as a function of resource utilization (Strubell et al., 2019; Henderson et al., 2020) but these energy prediction models are inaccurate (Cao et al., 2020). The inaccuracy stems from the prediction models not accounting for the complex interactions between energy consumption and resource utilization.\nIn this work, we focus on inference energy which can incur substantial costs especially for models that support high-volume web services. We ask how we can build an energy prediction method that is accurate, interpretable, and extensible. We make three contributions in answering this question.\nFirst, we frame the problem of interpretable energy prediction over a model tree abstraction. This abstraction represents the model as the root node that is composed from model-specific modules, which themselves are recursively composed from lower-level machine learning (ML) primitives, ones that are not model-specific. Given a model, the energy prediction problem is framed as the task of predicting the energy of all the nodes in its model tree abstraction. The result is that IrEne can predict not only the inference energy consumption of the entire model, but also of its components, making the energy prediction highly interpretable.\nSecond, we develop IrEne, that includes a multilevel prediction method that predicts energy in all nodes of the abstraction tree in a bottom-up fashion using resource utilization and model description features. For each of the leaf-nodes that are re-used\nin different models, the ML primitives, IrEne uses a separate regressor trained on ground-truth energy measurements. One simple way to get energy for all other higher-level nodes is to recursively sumup the values. While this works reasonably well (even better than a prior prediction model), direct summing of the raw predictions is sub-optimal because the error can propagate through the model tree thus making upper-level nodes estimation more erroneous. Instead, we learn a single regressor for all intermediate nodes, one that essentially adjusts the sum of children’s predicted energy values based on features of the children. Since IrEne is built on top of energy predictions of ML primitives that are not model specific, it is generalizable and can be used to predict the energy for previously unseen (Transformer-based) models.\nThird, to evaluate IrEne, we create an evaluation dataset with ground-truth energy measurements for multiple Transformer-based models at all levels in the model tree abstraction. Evaluations show that IrEne is more accurate – with an average modellevel energy error of 5 ∼ 7% compared against the ground-truth, while existing software-based method (Strubell et al., 2019) has over 55% error. The module-level energy errors are also substantially small showing that IrEne is both accurate and interpretable. Last, we also conduct multiple analyses that show the utility of IrEne for interpretable energy predictions."
    }, {
      "heading" : "2 Related work",
      "text" : "Over the last couple of years, there has been increased interest in the energy consumption of NLP models, starting with the work by Strubell et al. (Strubell et al., 2019). This work, and a follow up software framework called experimentimpact-tracker (Henderson et al., 2020) tracks the resource (i.e., CPU, GPU, memory) utilization of an NLP model and predicts energy consumption as a function of resources. However, our previous study shows that this type of resource utilization only modeling can be highly inaccurate (Cao et al., 2020). This is in part due to the complex relationship between resource utilization and energy consumption. Further, there are other activities that are not accounted via resource utilization such as data movement in GPU memory which can also cause significant energy footprint (Chen et al., 2016; Boroumand et al., 2018).\nOther works (Zhou et al., 2020; Schwartz et al.,\n2019) report the energy numbers through alternate metrics including dollar cost or in terms of floating point operations. However, these do not directly map to the energy consumption. Energy prediction of applications on mobile devices is a well-studied topic in the systems community (Pathak et al., 2011, 2012; Yoon et al., 2012; Cao et al., 2017) but these work require fine-grained understanding of the application. None of the existing systems predict energy for NLP applications."
    }, {
      "heading" : "3 Interpretable Energy Prediction",
      "text" : "In this section we first state our design goals, motivate the abstraction, and problem formulation for interpretable energy prediction."
    }, {
      "heading" : "3.1 Design Goals",
      "text" : "We design the energy prediction model with three design goals: (i) accurate prediction while incurring low profiling overheads; high overheads when measuring runtime resource utilization can hide the true energy costs of the NLP model, (ii) provide interpretable energy analysis of the components inside the NLP model, especially for analyzing energy bottlenecks; (iii) extensible and generalizable, in the sense that, they are trained once but can work on unseen NLP models to remain useful as new models emerge."
    }, {
      "heading" : "3.2 Model Tree Abstraction",
      "text" : "To achieve the above goals, we first need a representation of the NLP model that is at a suitable abstraction both from interpretability and generalization standpoints.\nOn the one hand, using only low-level abstractions such as the math operations can help with easy generalization to new models as their units are basic math (or other compute) operations that are building blocks of any model. However, they lack interpretability since they don’t directly convey the model architecture semantics. For example, a BERT (Devlin et al., 2019) model has matrix multiplications in both the self-attention and feed forward layers. Only having the energy of each matrix multiplication alone, without knowing which higher level logic units (i.e., either self-attention or feed forward layer) they belong to, does not help analyze if they are the bottlenecks for that particular unit. On the other hand, high-level abstractions preserve the architecture semantics and are interpretable for practitioners, but they don’t\neasily generalize to unseen models that may not have the same modules used for training.\nInstead, we use a model tree abstraction that represents the model nodes in three-levels: math level, machine learning (ML) level and module level. Math level nodes are a finite set of mathematical operations (like addition, subtraction, matrix multiplication etc); they form model-agnostic ML level nodes (such as Linear, LayerNorm etc.), which further can be used to construct complex module level nodes. Module level nodes are groups of lower ML level node operations that reflect the logic units of the NLP algorithms defined by model authors. The model tree abstraction is such that each parent node captures computation of all of its children nodes. Figure 1 shows an example of a one-layer BERT (Devlin et al., 2019) model (omitted math level nodes). The execution of the model tree nodes can be in parallel, but current systems have a fixed sequential order for executing the sibling nodes. In this work, we only focus on sequential execution. Note that the model tree doesn’t capture the order of execution. E.g., BertOutput appears right after BertIntermediate in BERT’s computation graph, but here they’ll be represented as siblings of the same parent BertLayer:0, and their energy will be treated separately. The parent node BertLayer:0 encapsulates the energy and computation of its children node BertIntermediate, BertOutput, and BertAttention, in no particular order."
    }, {
      "heading" : "3.3 Problem Definition",
      "text" : "With this new model tree abstraction, we formally state the problem of interpretable energy estimation\nof a NLP model. Given a model tree abstraction of a NLP model M consisting of a set of nodes N = {n|nml ∪ nmod} (nml is the set of ML level nodes, nmod is the set of module level nodes), for an input size I (a pair of batch size b and sequence length s) 1, we can predict the energy En for every node n in the model tree. The energy of root node is the energy for the entire model."
    }, {
      "heading" : "4 Interpretable Prediction with IrEne",
      "text" : "Figure 2 shows the IrEne architecture. IrEne takes the user-specified model and builds an energy predictor for a target hardware device. The model is run once on the target hardware and the runtime resource utilization is logged. During this run, IrEne uses code instrumentation and just-in-time (JIT) run-time tracing to break down the model into sub-components, and extracts a model tree representation (see details in §A).\nIrEne then provides interpretable energy analysis by predicting the energy for every node in the model tree in a bottom-up fashion. At the leaves, where the nodes correspond to the ML primitives, IrEne uses separate regression models for each type of ML primitive (e.g., one regressor for Linear Layer, another for LayerNorm etc.). For the intermediate nodes, their energy is predicted recursively using a single regressor that makes a weighted combination of the predicted energy values from its children. For both types of regressors, they use features that are derived from resource utilization (e.g. cpu utilization) and generalized node features\n1The batch size and input sequence length together decide the amount of input data to the model, therefore, they both affect the model energy consumption.\n(e.g. size of inputs) enabling accurate multi-level energy prediction.\nIrEne represents higher-level modules via generalizable features and the ML primitives. Even if the intermediate modules are model-specific (e.g. BertSelfAttention), the features are general, allowing IrEne to predict energy of unseen models.\nThe IrEne model is trained using ground-truth energy measurements of ML primitives and a handful of NLP models; we use a highly accurate hardware power monitor to measure ground truth energy (§A). Of course, one can use the power monitor to measure energy directly at runtime. However, this is cumbersome and requires physical access to the device which is not always feasible with cloudbased deployments. Further, the hardware meter only measures the total energy, which is not interpretable in terms of its components."
    }, {
      "heading" : "4.1 Multilevel energy prediction",
      "text" : "At the leaf-level, the energy prediction problem requires predicting the energy of ML primitives. As an offline step, IrEne first enumerates all relevant ML primitives and builds a specialized regressor for each primitive by training over ground truth data. In some cases, model developers can define their own ML primitives. We extract information about such custom primitives from the JIT trace.\nFormally, for a leaf node n with ML primitive i, we predict the energy of the node as:\nPMLie (n) = Wi ∗ feat(n) + bi (1) using primitive specific parameters Wi the weight vector and bi the bias. We learn these parameters using a mean squared error loss between predicted Pe(n) and ground-truth energy Ge(n).\nOur hierarchical tree representation gives a naturally interpretable way of propagating this prediction through the tree. Since each node represents total computation of its children nodes, the total energy from children nodes should also roughly correspond to that of the parent node. Formally,\nPe(n) = ∑\nc∈ child(n)\nPe(c) if n is non-leaf\n= PMLie (n) if n is leaf (2)\nWe call this baseline prediction model PredictedSum. This model is interpretable but naively summing up the energy values accumulates error going up the tree and results in noisy module-level predictions. To account for this, we use a weighted sum of child node energy, where the weights are learnt using node features. Formally,\nPe(n) = ∑\nc∈ child(n)\nα(c) ∗ Pe(c) if n is non-leaf\n= PMLie (n) if n is leaf\nα(c) = 1 + tanh(W ∗ feat(c) + b)/τ (3) where W and b are parameters and τ is a hyperparameter. Unlike ML primitives, here we have a single regressor with one set of weight vector (W) and bias scalar (b) parameters across all non-leaf nodes of any type. Note that this single regressor doesn’t predict node’s energy directly, but determines how much the predicted energy from its child node should be scaled before summing the children node energy. It does this recursively starting from the root, and hence encodes tree structure in its computation. We do not learn node-specific regressors because that does not allow generalizing to new models that may have different modules\nthan the ones during training. Since the method is essentially calibrating the sum of the energy values, regularizing the model so that the computed weights on the energy values to be around 1 helps the learning. We do this by equation 3, which makes the range of computed weights, α(c) to be within 1± τ . To supervise this model, we use the ground-truth energy from all the non-leaf nodes, and we train it in an end-to-end fashion. Formally,\nloss(n) = ∑\ns∈ subtree(n)\n( Pe(s)−Ge(s) )2 Ge(s)2 (4)\nWe scale the mean squared error with groundtruth energy, since scales of energy at different levels of the tree are vastly different. We refer to this model as the End2End regressor, since the error signal in energy prediction of any node backpropagates through the whole subtree. We use this training scheme in IrEne. In our evaluation (section 5), we perform an ablation study to show why the tree structure and the end-to-end regressor is crucial for accuracy."
    }, {
      "heading" : "4.2 Featurization",
      "text" : "We design two categories of energy-relevant features in IrEne : (i) the model features that reflect hardware-independent compute and memory information, and (ii) the resource features that capture how the models use hardware resources and cause energy activities. Table 1 shows the features used in IrEne. For the model description related information, we use features that characterize the compute, memory, and size of input etc. These are features that are independent of the underlying hardware. For resource features, we use utilization, usage and clock speed of hardware components including CPU, memory and GPU. Note that these two sets of features are extensible, meaning that one can add more either hardware-specific features or new model features. See Appendix sections A.2 and A.3 for details on how we obtain these features."
    }, {
      "heading" : "5 IrEne Evaluation",
      "text" : "Our evaluation is aimed at measuring the accuracy of IrEne relative to ground truth and the state-ofthe-art. We show the IrEne only causes 5-7% error for the model energy prediction. We also show that for a given Transformer model, IrEne can be used to find the energy bottlenecks and analyze the energy versus task performance trade-offs."
    }, {
      "heading" : "5.1 Setup",
      "text" : "Target Hardware: we use 2 GPU-equipped desktop PCs as the target hardware for running our models. See Table 2 for details. Software and models: We perform inference in Transformer models using PyTorch (Paszke et al., 2019) v1.7 through the HuggingFace Transformers (Wolf et al., 2020) library. The six models we study are — BERT-base (Devlin et al., 2019), RoBERTa-base (Liu et al., 2019), DistillBERT (Sanh et al., 2020), DistilGPT2 (Sanh et al., 2020; Radford et al., 2019), OpenAI GPT (Radford et al., 2018) and GPT2 (Radford et al., 2019). Software-based Measurement Baseline: For comparisons, we use the software-based energy measurements provided by the experiment-impacttracker (Henderson et al., 2020) which estimates energy as a function of the GPU, CPU, and memory utilization. The method computes energy by aggregating resource usage as follows: etotal = PUE ∑ p(pdramedram + pcpuecpu + pgpuegpu), where presource 2 are the percentages of each system resource used by the attributable processes relative to the total in-use resources and eresource is the energy usage of that resource. The constant\n2resources can be dram, cpu, gpu\nfor power usage effectiveness (PUE) compensates for extra energy used to cool or heat data centers."
    }, {
      "heading" : "5.2 Dataset and Evaluation Methodology",
      "text" : "For each model, we obtain the model tree and for each node in it, we associate ground-truth energy measurements using the power monitor and its resource features using low-overhead logging (Section A). For each node we run it repetitively for 20 seconds, since it often takes a very short time for one run (e.g. from 0.1 to 100 millisecond). We repeat this process for five rounds (the variations are within <1%) and record the average energy as the ground-truth for the node. We use 1 GPU to run all experiments. We record the start and end timestamp of the model program, and extract the energy values by comparing and aligning the timestamps from the resource profiler logs and power monitor logs. Ground Truth Energy: We measure ground truth energy using a emonPi power monitor (Hudson, 2021) which is open source. The emonPi uses a clip-on CT sensor to monitor the energy consumed by the computer which records the passthrough current and voltage every 170 ms. This allows us to accurately measure the power draw at a sub second granularity. We obtain current, voltage, and timestamp values from the power meter’s builtin serial port. The energy (e) consumed during a time period is then calculated using the sampled current (It) and voltage (Vt) values in that period: e = ∑ t VtIt.\nTo guarantee the consistency and reliability of the hardware energy measurements, we cool down the PCs after each experiment finishes to avoid potential overheating issue that can cause subsequent energy distortions. We measure the standby power consumption (when the CPU load is < 0.1%) and ensure before running the experiments that the PC does not draw more than the standby power. Further, no other application is running during our experiments.\nTo understand the scale of energy usage, Table 3 shows the estimated energy consumption (in kWh) using our ground truth measurement. We also show the cost of answering one million queries (in USD) when using a BERT-base model in a reading comprehension (over one passage), and in an end-toend setting (over 150 passages) ignoring retrieval compute. For reference, Google search handles millions of queries every minute (Kenshoo, 2019).\nEnergy Dataset: To evaluate the energy prediction, we create a dataset that cover a wide range of input sizes for the six studied Transformer models and the 24 BERT model variants (Turc et al., 2019). Each instance in the dataset can be of type ML, Module or Model level and is associated with features shown in Table 1 and hardware measured energy. We show the statistics of the dataset for BERT-base, DistilBERT and GPT2 in Table 4. Energy Error Metric: We measure the energy error percentage as 100×|PE−GE|/GE, where PE is the predicted energy and GE is the ground truth energy."
    }, {
      "heading" : "5.3 Energy Prediction Results",
      "text" : "We compare IrEne with the existing software measurement methods (Strubell et al., 2019; Henderson et al., 2020). We apply their method directly for all the models in our dataset. Note that their method is a fully-defined estimation model with a fixed set of parameters without any training. For IrEne experiments, we report cross-validated evaluation on the energy prediction dataset — leaving data from one model out of training set and evaluating on it, and then repeating the same for all the models.\n3based on the US national average as of May 2021 according to https://www.electricchoice.com/ electricity-prices-by-state.\nIrEne is accurate Table 5 shows the energy prediction errors at the model-level for all the models on the two PCs. The existing software-based baseline method from Strubell et al. (2019) incurs large energy prediction errors of over 50%.\nIrEne on the other hand incurs substantially lower errors, with at most 7.6% errors across the models, showing its value for reliable and accurate energy analysis. As seen from the cumulative distribution function for the model errors in Figure 3, all of IrEne’s errors are below 17% and nearly half of its errors are below 10%. We note here that our leave-one-model-out cross validation specifically evaluates the generalizability of IrEne.\nML and Module Levels Errors are also low. Table 7, 6 show a break down of the IrEne errors at the ML and module levels respectively. Accurately predicting ML level energy is key to accurate predictions for at the module level and higher, as the errors will accumulate up the model tree in IrEne. It turns out that we can indeed predict ML level energy with high-levels of accuracy — errors are lower than 1%, providing reliable values for the module level predictions. Note that even unseen models (ie ones evaluated in the test partition) will be made up of the same set of ML primitives (perhaps with different input and batch sizes). The results here cannot be directly generalized to unseen ML-primitives. Module level errors are higher and vary in range (5.4% to 16.7%) across different models. Module level errors also turn out to be higher than the model level errors. This is mainly because the module level errors are averages across all intermediate module level nodes in the model tree; some modules might have bigger errors, but these get calibrated by our End2End energy regressor. We further characterize these effects in IrEne ablation and validation analysis."
    }, {
      "heading" : "5.4 Feature Ablations",
      "text" : "Table 8 shows the contribution of model and resource features in IrEne energy prediction. We observe that resource features provide most of the benefits for energy estimation IrEne for all levels, confirming that resource information is important for energy prediction. Model features do not reduce ML level error because the error is already small, but they help further reduce the prediction errors for module and model levels and combining model and resource features together brings the average estimation errors further down to 8.5% and 5.5%."
    }, {
      "heading" : "5.5 Modeling Ablations",
      "text" : "To understand the impact of learning and the architectural choices of aggregating ML level energy into module level energy in IrEne affect the model accuracy, we build three (ablated) models: Is end-to-end learning necessary? To test this, we build a StepWise regressor that simply learns to predict the energy of parent node from the groundtruth energy of its child nodes at the training time. At the test time, it uses predicted energy generating predictions from ground up.\nPe(n) = ∑\nc∈ child(n)\nα(c) ∗Ge(c) Training\nPe(n) = ∑\nc∈ child(n)\nα(c) ∗ Pe(c) Testing (5)\nHere, α(c) and loss are still as defined in equation 3 and 4 respectively. However, unlike the IrEne (End2End) regressor, the errors in the prediction of root node, do not backpropogate to its prediction of descendant nodes i.e. there is no end-to-end training. Is tree-structure necessary? To test this, we build an Unstructured regressor that ignores the tree structure completely, and directly predicts the energy from the feature representation of nodes (Module and Model level) using linear regression as in equation (1). Unlike ML-level regressor though, here we need to use single set of parameters for common across the nodes. Is learning necessary? To test this, we use the PredictedSum model (equation 2). Recall this model also aggregates energy predictions over the tree-structure but has no parameters to train.\nTable 9 shows the ablation of IrEne with respect to different algorithmic choices of the module level energy aggregation. First, we find that the regressor that ignores the tree structure (Unstructured)\nperforms significantly worse than all other regressors that do consider it. Interestingly, learning without structure even performs worse than PredictedSum regressor that naively adds child energy without any learning, highlighting the importance of tree-structure. Further, learnt weighted sum outperforms PredictedSum regressor. In particular, End2End regressor performs better than StepWise regressor showing the importance of optimizing on whole tree in an end-to-end fashion."
    }, {
      "heading" : "5.6 Interpretable Energy Analysis",
      "text" : "In this section, we use the interpretable energy analysis from IrEne to show energy bottlenecks for given Transformer models, how energy varies for different model architectures, and how it can be used to effectively pick accuracy-energy trade-offs. Finding energy bottlenecks: We use IrEne to analyze the energy bottlenecks in Transformer models. For simplicity of analysis, we predict the energy for modules that are immediate parents of the ML level nodes and use it calculate the percentage of energy it contributes to the model overall. Table 10 shows the energy breakdown of two models: RoBERTabase and GPT2. We observe that self-attention layers in RoBERTa-base model consume 31% of the total energy while it is the feed forward layers in GPT2 that consume more than 59% of the energy. The module level energy breakdown of all models in Table 12 in Appendix C. We also present the full energy breakdown of the BERT-base model and annotate each node with predicted energy percentage in Figure 5 in the Appendix. Task accuracy versus energy tradeoffs:\nWe fine-tune BERT-24 models (Turc et al., 2019) on the Stanford Sentiment Treebank V2 (SST2) (Socher et al., 2013) using the default examples in the HuggingFace Transformers (Wolf et al., 2020) without any hyperparameter tuning. We evaluate the accuracy on the dev set of SST2. These\nmodels are not part of our energy prediction training data. We additionally exclude BERT-base from training data to show the extensibility of IrEne.\nGiven an energy budget, IrEne allows for selection of an optimal architecture that gets the highest accuracy for a task. In Figure 4, we see that it is possible for models to use more energy but return lower accuracy than other models which might use less energy. Similarly, given an accuracy target, we can choose an architecture with the lowest energy use. For example, for a target of 88% accuracy or above, there are many such models ranging from 4J all the way to 12J. Last, we point out that the trade-off curve based on the predicted energy mirrors that of the ground-truth well enough to be used as an accurate proxy."
    }, {
      "heading" : "6 Discussion",
      "text" : "This work focused on inference energy predictions of Transformers on a target hardware device.\nThe model tree abstraction is general and not tied to Transformer architectures nor to specific deep learning frameworks, it is extensible to other neural networks like LSTM and frameworks like TensorFlow. The abstraction is built from the computational graph and knowledge about the model architecture and underlying software. As long as these are available we can apply our methodology to other architectures as well.\nPredicting the training energy is an important and a more challenging problem. We believe our methodology can be extended. However, it will require tracking the energy of both forward and backward processes and even modeling other aspects training dynamics, for example, time to converge to specific accuracy.\nScaling to unseen hardware is an important and challenging area that needs further research. It requires both measuring the ground truth energy for a more diverse collection of hardware and designing proper hardware-specific features (i.e., L1 cache size, CPU cores, etc.). We believe IrEne’s methodology can be extended to calibrate software reported energy as a way to scale how we collect ground truths (as weak-supervision). In the future, we plan to study workloads on more hardware to choose proper features that capture the hardware energy differences."
    }, {
      "heading" : "7 Conclusions",
      "text" : "Energy consumption of NLP models is an important consideration from a cost perspective and increasingly, from an environmental impact perspective as well. Designing energy efficient and costeffective models requires both accurate and interpretable energy modeling. In this work, we showed that by carefully combining resource utilization with model description based features, we can develop a multi-level energy prediction model that is not only highly accurate but is also able to provide a break-down of how its different components contribute to its overall energy."
    }, {
      "heading" : "8 Acknowledgement",
      "text" : "This material is based upon work supported by the National Science Foundation under Grant No 2007362."
    }, {
      "heading" : "B Software Measurements Results",
      "text" : "We use experiment-impact-tracker (Henderson et al., 2020) to estimate software-based energy measurements for the models at a module level as well as ML level. Table 11 shows the percentage error in software based measurements for module level operations. We calculate a model’s module level error as average percentage error over runs for batch sizes 24 and 38, and sequence length 32 and 128. Getting granular ML level software energy corresponding to Strubell et al. (2019) requires modifying the existing framework which is non-trivial. We leave this to future work."
    }, {
      "heading" : "C Energy Breakdowns",
      "text" : "We show module level predicted energy breakdown of four Transformer models in Table 12, and show an abridged view of BERT-base-uncased tree annotated with predicted energy and distribution in Figure 5."
    } ],
    "references" : [ {
      "title" : "2019. BERT: Pre-training",
      "author" : [ "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Toutanova.,? \\Q2019\\E",
      "shortCiteRegEx" : "Toutanova.",
      "year" : 2019
    }, {
      "title" : "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
      "author" : [ "William Fedus", "Barret Zoph", "Noam Shazeer." ],
      "venue" : "arxiv.",
      "citeRegEx" : "Fedus et al\\.,? 2021",
      "shortCiteRegEx" : "Fedus et al\\.",
      "year" : 2021
    }, {
      "title" : "Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning",
      "author" : [ "Peter Henderson", "Jieru Hu", "Joshua Romoff", "Emma Brunskill", "Dan Jurafsky", "Joelle Pineau." ],
      "venue" : "arXiv:2002.05651 [cs].",
      "citeRegEx" : "Henderson et al\\.,? 2020",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2020
    }, {
      "title" : "emonPi - OpenEnergyMonitor",
      "author" : [ "Glyn Hudson" ],
      "venue" : null,
      "citeRegEx" : "Hudson.,? \\Q2021\\E",
      "shortCiteRegEx" : "Hudson.",
      "year" : 2021
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv:1907.11692 [cs].",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
      "author" : [ "jani", "Sasank Chilamkurthy", "Benoit Steiner", "Lu Fang", "Junjie Bai", "Soumith Chintala" ],
      "venue" : null,
      "citeRegEx" : "jani et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "jani et al\\.",
      "year" : 2019
    }, {
      "title" : "Where is the energy spent inside my app? fine grained energy accounting on smartphones with Eprof",
      "author" : [ "Abhinav Pathak", "Y. Charlie Hu", "Ming Zhang." ],
      "venue" : "Proceedings of the 7th ACM european conference on Computer Systems, EuroSys ’12, pages",
      "citeRegEx" : "Pathak et al\\.,? 2012",
      "shortCiteRegEx" : "Pathak et al\\.",
      "year" : 2012
    }, {
      "title" : "Fine-grained power modeling for smartphones using system call tracing",
      "author" : [ "Abhinav Pathak", "Y. Charlie Hu", "Ming Zhang", "Paramvir Bahl", "Yi-Min Wang." ],
      "venue" : "Proceedings of the sixth conference on Computer systems, EuroSys ’11, pages 153–168, New York,",
      "citeRegEx" : "Pathak et al\\.,? 2011",
      "shortCiteRegEx" : "Pathak et al\\.",
      "year" : 2011
    }, {
      "title" : "Scikit-learn: Machine learning",
      "author" : [ "F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay" ],
      "venue" : null,
      "citeRegEx" : "Pedregosa et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pedregosa et al\\.",
      "year" : 2011
    }, {
      "title" : "Scikit-learn: Machine learning in python",
      "author" : [ "Fabian Pedregosa", "Gaël Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg" ],
      "venue" : null,
      "citeRegEx" : "Pedregosa et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pedregosa et al\\.",
      "year" : 2011
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever." ],
      "venue" : "OpenAI Blog.",
      "citeRegEx" : "Radford et al\\.,? 2018",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI Blog.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "arXiv:1910.01108 [cs].",
      "citeRegEx" : "Sanh et al\\.,? 2020",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2020
    }, {
      "title" : "Green AI",
      "author" : [ "Roy Schwartz", "Jesse Dodge", "Noah A. Smith", "Oren Etzioni." ],
      "venue" : "arXiv:1907.10597 [cs, stat].",
      "citeRegEx" : "Schwartz et al\\.,? 2019",
      "shortCiteRegEx" : "Schwartz et al\\.",
      "year" : 2019
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 Conference on",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Energy and Policy Considerations for Deep Learning in NLP",
      "author" : [ "Emma Strubell", "Ananya Ganesh", "Andrew McCallum." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3645–3650, Florence, Italy.",
      "citeRegEx" : "Strubell et al\\.,? 2019",
      "shortCiteRegEx" : "Strubell et al\\.",
      "year" : 2019
    }, {
      "title" : "Well-Read Students Learn Better: On the Importance of Pre-training Compact Models",
      "author" : [ "Iulia Turc", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv:1908.08962 [cs]. ArXiv: 1908.08962.",
      "citeRegEx" : "Turc et al\\.,? 2019",
      "shortCiteRegEx" : "Turc et al\\.",
      "year" : 2019
    }, {
      "title" : "HuggingFace’s Transformers: State-of-the-art Natural Language Processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush." ],
      "venue" : "arXiv:1910.03771 [cs].",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "AppScope: application energy metering framework for android smartphones using kernel activity monitoring",
      "author" : [ "Chanmin Yoon", "Dongwon Kim", "Wonwoo Jung", "Chulkoo Kang", "Hojung Cha." ],
      "venue" : "Proceedings of the 2012 USENIX conference on",
      "citeRegEx" : "Yoon et al\\.,? 2012",
      "shortCiteRegEx" : "Yoon et al\\.",
      "year" : 2012
    }, {
      "title" : "HULK: An Energy Efficiency Benchmark Platform for Responsible Natural Language Processing",
      "author" : [ "Xiyou Zhou", "Zhiyu Chen", "Xiaoyong Jin", "William Yang Wang." ],
      "venue" : "arXiv:2002.05829 [cs].",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    }, {
      "title" : "Regressor Training Procedures We’ve implemented IrEne using SciKit Learn (Pedregosa et al., 2011a) and PyTorch (Paszke et al., 2019). We learn linear regressors for ML-level",
      "author" : [ "tors. A" ],
      "venue" : null,
      "citeRegEx" : "A.4,? \\Q2019\\E",
      "shortCiteRegEx" : "A.4",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Recent works have sought to estimate energy consumption and suggest ways to reduce the resulting costs and carbon impacts (Strubell et al., 2019; Schwartz et al., 2019; Henderson et al., 2020; Anthony et al., 2020)",
      "startOffset" : 122,
      "endOffset" : 214
    }, {
      "referenceID" : 14,
      "context" : "Recent works have sought to estimate energy consumption and suggest ways to reduce the resulting costs and carbon impacts (Strubell et al., 2019; Schwartz et al., 2019; Henderson et al., 2020; Anthony et al., 2020)",
      "startOffset" : 122,
      "endOffset" : 214
    }, {
      "referenceID" : 2,
      "context" : "Recent works have sought to estimate energy consumption and suggest ways to reduce the resulting costs and carbon impacts (Strubell et al., 2019; Schwartz et al., 2019; Henderson et al., 2020; Anthony et al., 2020)",
      "startOffset" : 122,
      "endOffset" : 214
    }, {
      "referenceID" : 16,
      "context" : "On the other hand, there are software models that predict energy as a function of resource utilization (Strubell et al., 2019; Henderson et al., 2020) but these energy prediction models are inaccurate (Cao et al.",
      "startOffset" : 103,
      "endOffset" : 150
    }, {
      "referenceID" : 2,
      "context" : "On the other hand, there are software models that predict energy as a function of resource utilization (Strubell et al., 2019; Henderson et al., 2020) but these energy prediction models are inaccurate (Cao et al.",
      "startOffset" : 103,
      "endOffset" : 150
    }, {
      "referenceID" : 16,
      "context" : "Evaluations show that IrEne is more accurate – with an average modellevel energy error of 5 ∼ 7% compared against the ground-truth, while existing software-based method (Strubell et al., 2019) has over 55% error.",
      "startOffset" : 169,
      "endOffset" : 192
    }, {
      "referenceID" : 2,
      "context" : "This work, and a follow up software framework called experimentimpact-tracker (Henderson et al., 2020) tracks the resource (i.",
      "startOffset" : 78,
      "endOffset" : 102
    }, {
      "referenceID" : 20,
      "context" : "Other works (Zhou et al., 2020; Schwartz et al., 2019) report the energy numbers through alternate metrics including dollar cost or in terms of floating point operations.",
      "startOffset" : 12,
      "endOffset" : 54
    }, {
      "referenceID" : 14,
      "context" : "Other works (Zhou et al., 2020; Schwartz et al., 2019) report the energy numbers through alternate metrics including dollar cost or in terms of floating point operations.",
      "startOffset" : 12,
      "endOffset" : 54
    }, {
      "referenceID" : 19,
      "context" : "Energy prediction of applications on mobile devices is a well-studied topic in the systems community (Pathak et al., 2011, 2012; Yoon et al., 2012; Cao et al., 2017) but these work require fine-grained understanding of the application.",
      "startOffset" : 101,
      "endOffset" : 165
    }, {
      "referenceID" : 5,
      "context" : ", 2019), RoBERTa-base (Liu et al., 2019), DistillBERT (Sanh et al.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 13,
      "context" : ", 2019), DistillBERT (Sanh et al., 2020), DistilGPT2 (Sanh et al.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 13,
      "context" : ", 2020), DistilGPT2 (Sanh et al., 2020; Radford et al., 2019), OpenAI GPT (Radford et al.",
      "startOffset" : 20,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : ", 2020), DistilGPT2 (Sanh et al., 2020; Radford et al., 2019), OpenAI GPT (Radford et al.",
      "startOffset" : 20,
      "endOffset" : 61
    }, {
      "referenceID" : 11,
      "context" : ", 2019), OpenAI GPT (Radford et al., 2018) and GPT2 (Radford et al.",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 2,
      "context" : "Software-based Measurement Baseline: For comparisons, we use the software-based energy measurements provided by the experiment-impacttracker (Henderson et al., 2020) which estimates energy as a function of the GPU, CPU, and memory utilization.",
      "startOffset" : 141,
      "endOffset" : 165
    }, {
      "referenceID" : 3,
      "context" : "Ground Truth Energy: We measure ground truth energy using a emonPi power monitor (Hudson, 2021) which is open source.",
      "startOffset" : 81,
      "endOffset" : 95
    }, {
      "referenceID" : 17,
      "context" : "Energy Dataset: To evaluate the energy prediction, we create a dataset that cover a wide range of input sizes for the six studied Transformer models and the 24 BERT model variants (Turc et al., 2019).",
      "startOffset" : 180,
      "endOffset" : 199
    }, {
      "referenceID" : 16,
      "context" : "We compare IrEne with the existing software measurement methods (Strubell et al., 2019; Henderson et al., 2020).",
      "startOffset" : 64,
      "endOffset" : 111
    }, {
      "referenceID" : 2,
      "context" : "We compare IrEne with the existing software measurement methods (Strubell et al., 2019; Henderson et al., 2020).",
      "startOffset" : 64,
      "endOffset" : 111
    }, {
      "referenceID" : 17,
      "context" : "We fine-tune BERT-24 models (Turc et al., 2019) on the Stanford Sentiment Treebank V2 (SST2) (Socher et al.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 15,
      "context" : ", 2019) on the Stanford Sentiment Treebank V2 (SST2) (Socher et al., 2013) using the default examples in the HuggingFace Transformers (Wolf et al.",
      "startOffset" : 53,
      "endOffset" : 74
    } ],
    "year" : 2021,
    "abstractText" : "Existing software-based energy measurements of NLP models are not accurate because they do not consider the complex interactions between energy consumption and model execution. We present IrEne, an interpretable and extensible energy prediction system that accurately predicts the inference energy consumption of a wide range of Transformer-based NLP models. IrEne constructs a model tree graph that breaks down the NLP model into modules that are further broken down into low-level machine learning (ML) primitives. IrEne predicts the inference energy consumption of the ML primitives as a function of generalizable features and fine-grained runtime resource usage. IrEne then aggregates these low-level predictions recursively to predict the energy of each module and finally of the entire model. Experiments across multiple Transformer models show IrEne predicts inference energy consumption of transformer models with an error of under 7% compared to the ground truth. In contrast, existing energy models see an error of over 50%. We also show how IrEne can be used to conduct energy bottleneck analysis and to easily evaluate the energy impact of different architectural choices. We release the code and data at https:// github.com/StonyBrookNLP/irene.",
    "creator" : "LaTeX with hyperref"
  }
}