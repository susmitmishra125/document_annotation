{
  "name" : "2021.acl-long.494.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Dual Graph Convolutional Networks for Aspect-based Sentiment Analysis",
    "authors" : [ "Ruifan Li", "Hao Chen", "Fangxiang Feng", "Zhanyu Ma", "Xiaojie WANG", "Eduard Hovy" ],
    "emails" : [ "xjwang}@bupt.edu.cn", "hovy@cmu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6319–6329\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6319"
    }, {
      "heading" : "1 Introduction",
      "text" : "Sentiment analysis has become a popular topic in natural language processing (Liu, 2012; Li and Hovy, 2017). Aspect-based sentiment analysis (ABSA) talks an entity-level oriented fine-grained sentiment analysis task that aims to determine sentiment polarities of given aspects in a sentence. In\n∗Corresponding author.\nFigure 1, the comment is about a restaurant review. The sentiment polarity of the two aspects “price” and “service” are positive and negative, respectively. Thus, ABSA can precisely identify user’s attitudes towards a certain aspect, rather than simply assigning a sentiment polarity for a sentence.\nThe key point in solving the ABSA task is to model the dependency relationship between an aspect and its corresponding opinion expressions. Nevertheless, there probably exist multiple aspects and different opinion expressions in a sentence. To judge the sentiment of a particular aspect, previous studies (Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018) have proposed various recurrent neural networks (RNNs) with attention mechanisms to generate aspect-specific sentence representations and have achieved appealing results. However, an inherent defect makes the attention mechanism vulnerable to noise in the sentence. Take Figure 1 as an example; for the aspect “service”, the opinion word “reasonable” may receive more attention than the opinion word “poor”. However, the “reasonable” refers to another aspect, i.e., “price”.\nMore recent efforts (Zhang et al., 2019; Sun et al., 2019b; Huang and Carley, 2019; Zhang and Qian, 2020; Chen et al., 2020; Liang et al., 2020; Wang et al., 2020; Tang et al., 2020) have been de-\nvoted to graph convolutional networks (GCNs) and graph attention networks (GATs) over dependency trees, which explicitly exploit the syntactic structure of a sentence. Consider the dependency tree in Figure 1; the syntactic dependency can establish connections between the words in a sentence. For example, a dependency relation exists between the aspect “price” and the opinion word “reasonable”. However, two challenges arise when applying syntactic dependency knowledge to the ABSA task: 1) the inaccuracy of the dependency parsing results and 2) GCNs over dependency trees do not work well as expected on datasets that are not sensitive to syntactic dependency due to the informal expression and complexity of online reviews.\nIn this paper, we propose a novel architecture, the dual graph convolution network (DualGCN), as shown in Figure 2, to solve the aforementioned challenges. For the first challenge, we use the probability matrix of all dependency arcs from a dependency parser to build a syntax-based graph convolutional network (SynGCN). The idea behind this approach is that the probability matrix representing dependencies between words contains rich syntactic information compared with the final discrete output of a dependency parser. For the second, we construct a semantic correlation-based graph convolutional network (SemGCN) by utilizing a self-attention mechanism. The idea behind this approach is that the attention matrix shaped by self-attending, also viewed as an edge-weighted directed graph, can represent semantic correlations between words. Moreover, motivated by the work of DGEDT (Tang et al., 2020), we utilize a BiAffine module to bridge relevant information between the SynGCN and SemGCN modules.\nFurthermore, we design two regularizers to enhance our DualGCN model. We observe that the semantically related terms of each word should not overlap. Therefore, we encourage the attention probability distributions over words to be orthogonal. To this end, we incorporate an orthogonal regularizer on the attention probability matrix for the SemGCN module. Moreover, the two representations learned from the SynGCN and SemGCN modules should contain significantly distinct information captured by the syntactic dependency and the semantic correlation. Therefore, we expect that the SemGCN module could learn semantic representations different from syntactic representations. Thus, we propose a differential regularizer between\nthe SynGCN and SemGCN modules. Our contributions are highlighted as follows:\n• We propose a DualGCN model for the ABSA task. Our DualGCN considers both the syntactic structure and the semantic correlation within a given sentence. Specifically, our DualGCN integrates the SynGCN and SemGCN networks through a mutual BiAffine module.\n• We propose orthogonal and differential regularizers. The orthogonal regularizer encourages the SemGCN network to learn an orthogonal semantic attention matrix, whereas the differential regularizer encourages the SemGCN network to learn semantic features distinct from the syntactic ones built from the SynGCN network.\n• We conduct extensive experiments on the SemEval 2014 and Twitter datasets. The experimental results demonstrate the effectiveness of our DualGCN model. Additionally, the source code and preprocessed datasets used in our work are provided on GitHub1."
    }, {
      "heading" : "2 Related Work",
      "text" : "Traditional sentiment analysis tasks are sentencelevel or document-level oriented. In contrast, ABSA is an entity-level oriented and a more finegrained task for sentiment analysis. Earlier methods (Titov and McDonald, 2008; Jiang et al., 2011; Kiritchenko et al., 2014; Vo and Zhang, 2015) are usually based on handcrafted features and fail to model the dependency between the given aspect and its context.\nRecently, various attention-based neural networks have been proposed to implicitly model the semantic relation of an aspect and its context to capture the opinion expression component (Wang et al., 2016; Tang et al., 2016a,b; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018; Li et al., 2018a; Tan et al., 2019). For instance, (Wang et al., 2016) proposed attentionbased LSTMs for aspect-level sentiment classification. (Tang et al., 2016b) and (Chen et al., 2017) both introduced a hierarchical attention network to identify important sentiment information related to the given aspect. (Fan et al., 2018) exploited a multi-grained attention mechanism to capture the word-level interaction between aspects and their context. (Tan et al., 2019) designed a dual attention\n1https://github.com/CCChenhao997/DualGCN-ABSA\nnetwork to recognize conflicting opinions. In addition, the pre-trained language model BERT (Devlin et al., 2019) has achieved remarkable performance in many NLP tasks, including ABSA. (Sun et al., 2019a) transformed ABSA task into a sentence pair classification task by constructing an auxiliary sentence. (Xu et al., 2019) proposed a post-training approach on the BERT to enhance the performance of fine-tuning stage for the ABSA task.\nAnother trend explicitly leverages syntactic knowledge. This type of knowledge helps to establish connections between the aspects and the other words in a sentence to learn syntax-aware feature representations of aspects. (Dong et al., 2014) proposed a recursive neural network to adaptively propagate the sentiment of words to the aspect along the dependency tree. (He et al., 2018) introduced an attention model that incorporated syntactic information to compute attention weights. (Phan and Ogunbona, 2020) utilized the syntactic relative distance to reduce the impact of irrelevant words.\nFollowing this line, a few works extend the GCN and GAT models by means of a syntactical dependency tree and develop several outstanding models (Zhang et al., 2019; Sun et al., 2019b; Huang and Carley, 2019; Wang et al., 2020; Tang et al., 2020). These works explicitly exploit the syntactic structure information to learn node representations from adjacent nodes. Thus, the dependency tree shortens the distance between the aspects and opinion words of a sentence and alleviates the problem of long-range dependency.\nMost recently, several works explore the idea of combining different types of graph for ABSA task. For instance, (Chen et al., 2020) combined a dependency graph and a latent graph to generate the aspect representation. (Zhang and Qian, 2020) observed the characteristics of word co-occurrence in linguistics and designed hierarchical syntactic and lexical graphs. (Liang et al., 2020) constructed aspect-focused and inter-aspect graphs to learn dependency feature of the key aspect words and sentiment relations between different aspects.\nIn this paper, we propose a GCN based method combining syntactic and semantic features. We use a dependency probability matrix with richer syntactic information and elaborately design orthogonal and differential regularizers to enhance the ability to precisely capture the semantic associations."
    }, {
      "heading" : "3 Graph Convolutional Network (GCN)",
      "text" : "Motivated by conventional convolutional neural networks (CNNs) and graph embedding, a GCN is an efficient CNN variant that operates directly on graphs (Kipf and Welling, 2017). For graph structured data, a GCN can apply the convolution operation on directly connected nodes to encode local information. Through the message passing of multilayer GCNs, each node in a graph can learn more global information. Given a graph with n nodes, the graph can be represented as an adjacency matrix A ∈ Rn×n. Most previous work (Zhang et al., 2019; Sun et al., 2019b) extend GCN models by encoding dependency trees and incorporating dependency paths between words. They build the adjacency matrix A over the syntactical dependency tree of a sentence. Thus, an element Aij in A indicates whether the i-th node is connected to the j-th node. Specifically, Aij = 1 if the i-th node is connected to the j-th node, and Aij = 0 otherwise. In addition, the adjacency matrix A, composed of 0 and 1, can be deemed as the final discrete output of a dependency parser. For the i-th node at the l-th layer, formally, its hidden state representation, denoted as hli, is updated by the following equation:\nhli = σ  n∑ j=1 AijW lhl−1j + b l  (1) where W l is a weight matrix, bl is a bias term, and σ is an activation function (e.g., ReLU)."
    }, {
      "heading" : "4 Proposed DualGCN",
      "text" : "Figure 2 provides an overview of DualGCN. In the ABSA task, a sentence-aspect pair (s, a) is given, where a = {a1, a2, ..., am} is an aspect. It is also a sub-sequence of the entire sentence s = {w1, w2, ..., wn}. Then, we utilize BiLSTM or BERT as sentence encoder to extract hidden contextual representations, respectively. For the BiLSTM encoder, we first obtain the word embeddings x = {x1, x2, ..., xn} of the sentence s from an embedding lookup table E ∈ R|V |×de , where |V | is the size of vocabulary and de denotes the dimensionality of word embeddings. Next, the word embeddings of the sentence are fed into a BiLSTM to produce hidden state vectors H = {h1, h2, ..., hn}, where hi ∈ R2d is the hidden state vector at time t from the BiLSTM. The dimensionality of a hidden state vector d is output by a unidirectional LSTM.\nFor the BERT encoder, we construct a sentenceaspect pair “[CLS] sentence [SEP] aspect [SEP]” as input to obtain aspect-aware hidden representations of the sentence. Moreover, in order to match the wordpiece-based representations of BERT with the result of syntactic dependency based on word, we expand dependencies of a word into its all of subwords. Then, the hidden representations of sentence are input into the SynGCN and SemGCN modules, respectively. A BiAffine module is then adopted for effective information flow. Finally, we aggregate all the aspect nodes’ representations from the SynGCN and SemGCN modules via pooling and concatenation to form the final aspect representation. Next, we elaborate on the details of our proposed DualGCN model."
    }, {
      "heading" : "4.1 Syntax-based GCN (SynGCN)",
      "text" : "The SynGCN module takes the syntactic encoding as input. To encode syntactic information, we utilize the probability matrix of all dependency arcs from a dependency parser. Compared to the final discrete output of a dependency parser, the dependency probability matrix could capture rich structural information by providing all latent syntactic\nstructures. Therefore, the dependency probability matrix is used to alleviate dependency parsing errors. Here, we use the state-of-the-art dependency parsing model LAL-Parser (Mrini et al., 2019).\nWith the syntactic encoding of an adjacency matrix Asyn ∈ Rn×n, the SynGCN module takes the hidden state vectors H from BiLSTM as initial node representations in the syntactic graph. The syntactic graph representation Hsyn = {hsyn1 , h syn 2 , ..., h syn n } is then obtained from the SynGCN module using Eq. (1). Here, hsyni ∈ Rd is a hidden representation of the ith node. Note that for aspect nodes, we use symbols {hsyna1 , h syn a2 , ..., h syn am} to denote their hidden representations."
    }, {
      "heading" : "4.2 Semantic-based GCN (SemGCN)",
      "text" : "Instead of utilizing additional syntactic knowledge, as in SynGCN, SemGCN obtains an attention matrix as an adjacency matrix via a self-attention mechanism. On the one hand, self-attention can capture the semantically related terms of each word in a sentence, which is more flexible than the syntactic structure. One the other hand, SemGCN can adapt to online reviews that are not sensitive to syntactic information.\nSelf-Attention Self-attention (Vaswani et al., 2017) computes the attention score of each pair of elements in parallel. In our DualGCN, we compute the attention score matrix Asem ∈ Rn×n using a self-attention layer. We then take the attention score matrix Asem as the adjacency matrix of our SemGCN module, which can be formulated as:\nAsem = softmax\n( QWQ × ( KWK )T √ d ) (2)\nwhere matricesQ andK are both equal to the graph representations of previous layer of our SemGCN module, while WQ and WK are learnable weight matrices. In addition, d is the dimensionality of the input node feature. Note that we use only one self-attention head to obtain an attention score matrix for a sentence. Similar to the SynGCN module, the SemGCN module obtains the graph representation Hsem. Additionally, we use the symbols {hsema1 , h sem a2 , ..., h sem am } to denote the hidden representations of all aspect nodes. BiAffine Module To effectively exchange relevant features between the SynGCN and SemGCN modules, we adopt a mutual BiAffine transformation as a bridge. We formulate the process as follows:\nHsyn′ = softmax ( HsynW1(H sem)T ) Hsem (3)\nHsem′ = softmax ( HsemW2(H syn)T ) Hsyn (4)\nwhere W1 and W2 are trainable parameters. Finally, we apply average pooling and concatenation operations on the aspect nodes of the SynGCN and SemGCN modules. Thus, we obtain the final feature representation for the ABSA task, i.e.,\nhsyna = f ( hsyna1 , h syn a2 , ..., h syn am ) (5)\nhsema = f ( hsema1 , h sem a2 , ..., h sem am ) (6) r = [hsyna , h sem a ] (7)\nwhere f(·) is an average pooling function applied over the aspect node representations. Then, the obtained representation r is fed into a linear layer, followed by a softmax function to produce a sentiment probability distribution p, i.e.,\np(a) = softmax (Wpr + bp) (8)\nwhere Wp and bp are the learnable weight and bias."
    }, {
      "heading" : "4.3 Regularizer",
      "text" : "To improve the semantic representation, we propose two regularizers for the SemGCN module, i.e., orthogonal and differential regularizers. Orthogonal Regularizer Intuitively, the related items of each word should be in different regions in a sentence, so the attention score distributions rarely overlap. Therefore, we expect a regularizer to encourage orthogonality among the attention score vectors of all words. Given an attention score matrix Asem ∈ Rn×n, the orthogonal regularizer is formulated as follows:\nRO = ‖AsemAsemT − I‖F (9)\nwhere I is an identity matrix. The subscript F denotes the Frobenius norm. As a result, each nondiagonal element of AsemAsemT is minimized to maintain the matrix Asem orthogonal. Differential Regularizer We expect that two types of feature representations learned from the SynGCN and SemGCN modules represent distinct information contained within the syntactic dependency trees and semantic correlations. Therefore, we adopt a differential regularizer between the two adjacency matrices of the SynGCN and SemGCN modules. Note that the regularizer is only restrictive to Asem and is given as\nRD = 1\n‖Asem −Asyn‖F . (10)"
    }, {
      "heading" : "4.4 Loss Function",
      "text" : "Our training goal is to minimize the following total objective function:\n`T = `C + λ1RO + λ2RD + λ3‖Θ‖2 (11)\nwhere λ1, λ2 and λ3 are regularization coefficients and Θ represents all trainable model parameters. `C is a standard cross-entropy loss and is defined for the ABSA task as follows:\n`C = − ∑\n(s,a)∈D ∑ c∈C log p(a) (12)\nwhere D contains all sentence-aspect pairs and C is the collection of distinct sentiment polarities."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Datasets",
      "text" : "We conduct experiments on three public standard datasets. The Restaurant and Laptop datasets\nare made public from the SemEval ABSA challenge (Pontiki et al., 2014). Following (Chen et al., 2017), we remove the instances using the “conflict” label. In addition, the Twitter dataset is a collection of tweets (Dong et al., 2014). All three datasets have three sentiment polarities: positive, negative and neutral. Each sentence in these datasets is annotated with marked aspects and their corresponding polarities. Statistics for the three datasets are shown in Table 1."
    }, {
      "heading" : "5.2 Implementation Details",
      "text" : "The LAL-Parser (Mrini et al., 2019), which is used for dependency parsing, provides an off-the-shelf parser2. For all the experiments, we use pretrained 300-dimensional Glove3 vectors (Pennington et al., 2014) to initialize the word embeddings. The dimensionality of the position (i.e., the relative position of each word in a sentence with respect to the aspect) embeddings and part-of-speech (POS) embeddings is set to 30. Thus, we concatenate the word, POS and position embeddings and then input them into a BiLSTM model, whose hidden size is set to 50. To alleviate overfitting, we apply dropout at a rate of 0.7 to the input word embeddings of the BiLSTM. The dropout rate of the SynGCN and SemGCN modules is set to 0.1, and the number of SynGCN and SemGCN layers is set to 2. All the model weights are initialized from a uniform distribution. We use the Adam optimizer with a learning rate of 0.002. The DualGCN model is trained in 50 epochs with a batch size of 16. The regularization coefficients, λ1 and λ2 are set to (0.2, 0.3), (0.2, 0.2) and (0.3, 0.2) for the three datasets, respectively, and λ3 is set to 10−4. For DualGCN+BERT, we use the bert-base-uncased4 English version. See our code for more details about BERT’s experiments. Additionally, following (Marcheggiani and Titov, 2017), we add a self-loop for each node in\n2https://github.com/KhalilMrini/LAL-Parser 3https://nlp.stanford.edu/projects/glove/ 4https://github.com/huggingface/transformers\nthe SynGCN and SemGCN modules."
    }, {
      "heading" : "5.3 Baseline Methods",
      "text" : "We compare DualGCN with state-of-the-art baselines. The models are briefly described as follows. 1) ATAE-LSTM (Wang et al., 2016) utilizes aspect embedding and the attention mechanism in aspectlevel sentiment classification. 2) IAN (Ma et al., 2017) employs two LSTMs and an interactive attention mechanism to generate representations for the aspect and sentence. 3) RAM (Chen et al., 2017) uses multiple attention and memory networks to learn the sentence representation. 4) MGAN (Fan et al., 2018) designs a multigrained attention mechanism to capture word-level interactions between the aspect and context. 5) TNet (Li et al., 2018b) transforms BiLSTM embeddings into target-specific embeddings and uses CNN to extract final embeddings for classification. 6) ASGCN (Zhang et al., 2019) first proposed using GCN to learn the aspect-specific representations for aspect-based sentiment classification. 7) CDT (Sun et al., 2019b) utilizes a GCN over a dependency tree to learn aspect representations with syntactic information. 8) BiGCN (Zhang and Qian, 2020) uses hierarchical graph structure to integrate word co-occurrence information and dependency type information. 9) kumaGCN (Chen et al., 2020) employs a latent graph structure to complement syntactic features. 10) InterGCN (Liang et al., 2020) utilizes a GCN over a dependency tree to learn aspect representations with syntactic information. 11) R-GAT (Wang et al., 2020) proposes a aspectoriented dependency tree structure and then encodes new dependency trees with a relational GAT. 12) DGEDT (Tang et al., 2020) proposes a dependency graph enhanced dual-transformer network by jointly considering flat representations and graphbased representations. 13) BERT (Devlin et al., 2019) is the vanilla BERT model by feeding the sentence-aspect pair and using the representation of [CLS] for predictions. 14) R-GAT+BERT (Wang et al., 2020) is the RGAT model that uses a pre-trained BERT to replace BiLSTM as an encoder. 15) DGEDT+BERT (Tang et al., 2020) is the DGEDT model that uses a pre-trained BERT to replace BiLSTM as an encoder."
    }, {
      "heading" : "5.4 Comparison Results",
      "text" : "To evaluate the ABSA models, we use the accuracy and macro-averaged F1-score as the main evaluation metrics. The main experimental results are reported in Table 2. Our DualGCN model consistently outperforms all attention-based and syntax-based methods on the Restaurant, Laptop and Twitter datasets. These results demonstrates that our DualGCN effectively integrates syntactic knowledge and semantic information. In addition, the DualGCN accurately fits datasets that contain formal, informal or complicated reviews. Compared to attention-based methods such as ATAELSTM, IAN and RAM, our DualGCN model utilizes syntactic knowledge to establish dependencies between words, so it can avoid noises introduced by the attention mechanism. Moreover, the syntaxbased methods, such as ASGCN, CDT, R-GAT and so on, achieve better performance than attentionbased methods, but they ignore the semantic correlation between words. However, when considering informal or complicated sentences, using only syntactic knowledge results in poor performance. In Table 2, on the other side, the results from the last group shows that the basic BERT outperforms most of the models based on static word embedding. Moreover, based on BERT, our DualGCN+BERT achieves better performance."
    }, {
      "heading" : "5.5 Ablation Study",
      "text" : "To further investigate the role of modules in the DualGCN model, we conduct extensive ablation studies. The results are reported in Table 2. The SynGCN-head model uses the discrete outputs of a dependency parser to construct the adjacency matrix of the GCNs. In contrast, SynGCN leverages the probability matrix generated in a dependency parser as the adjacency matrix. The SynGCN model outperforms the SynGCN-head on the Restaurant and Laptop datasets, which demonstrates that rich syntactic knowledge can alleviate dependency parsing errors. The SemGCN model utilizes a self-attention layer to construct the adjacency matrix of the semantic graph. This SemGCN model outperforms the SynGCN on the Twitter dataset because the reviews from Twitter, compared to those from Restaurant and Laptop datasets, are largely informal and insensitive to syntactic information. DualGCN w/o BiAffine means that we remove the BiAffine module so that the SynGCN and SemGCN modules cannot interact with each\nother. Therefore, the performance degrades substantially on the Restaurant and Laptop datasets. DualGCN w/o RO&RD indicates that we remove both the orthogonal and differential regularizers. Similarly, DualGCN w/o RO or RD denotes that we remove only one of the regularizers. The experimental results show that our two regularizers encourage the DualGCN to capture semantic correlations precisely. Overall, our DualGCN with all modules achieves the best performance."
    }, {
      "heading" : "5.6 Case Study",
      "text" : "Table 4 shows a few sample cases analyzed using different models. The notations P, N and O represent positive, negative and neutral sentiment, respectively. We highlight the aspect words in red and in blue. For the aspect “food” in the first sample, the attention-based methods, i.e., ATAELSTM and IAN, are prone to attend to the noisy word “dreadful”. Although the syntactic dependency can establish direct connections between an aspect and some words, no association exists between the aspect and the opinion words for complicated sentences. Take the second sample as an example; the aspect “apple os” is far from the opinion word “happy” in terms of syntactic distance. Thus, the SynGCN model fails. Additionally, in the third sample, feature representations of the key words “did not” are not captured by the SynGCN model. In contrast, the SemGCN model can attend to the semantic correlation between words. The last two samples demonstrate that our DualGCN, which fully considers the complementarity of syntactic knowledge and semantic information, can address complicated and informal sentences with the help of the orthogonal and differential regularizers."
    }, {
      "heading" : "5.7 Attention Visualization",
      "text" : "To investigate the effectiveness of the two regularizers in capturing the semantic correlations between words, we visualized the attention score matrix of the DualGCN w/o RO&RD and the intact DualGCN. Consider the sample sentence, i.e., “Web browsing is very quick with Safari browser.” with “Safari browser” as an aspect. As shown in Figure 3 (a), the attention score matrix is dense, and the related terms of each word overlap in the DualGCN w/o RO&RD model. This result is attributed to the lack of semantic constraints in the self-attention layers. The overlap of semantic correlations will lead to redundancy and noise during information propagation. The seventh and eighth rows of the\nattention score matrix are the attention probability distributions of “safari” and “browser”, respectively. The information to which “safari browser” pays attention is redundant and it does not pay more attention to the key opinion word “quick”. Thus, the DualGCN w/o RO&RD failed. In comparison, in Figure 3 (b), the attention score matrix produced by our DualGCN is relatively sparse. Both “safari” and “browser” are semantically related to “quick”, and their other attended items are also semantically reasonable. In addition, the attention scores of the related terms of each words tend to be distinct and precise due to the semantic constraints of these two regularizers. Therefore, our DualGCN model can readily predict the correct sentiment polarity of the aspect “safari browser”."
    }, {
      "heading" : "5.8 Impact of the DualGCN Layer Number",
      "text" : "To investigate the impact of the DualGCN layer number, we evaluate our DualGCN model with one to eight layers on the Restaurant and Laptop datasets. As shown in Figure 4, our model with two DualGCN layers performs the best. On one the hand, node representations cannot propagate far when the number of layers is small. On the other hand, if the number of layers is excessive, the model will become unstable due to the vanishing gradient and information redundancy."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we propose a DualGCN architecture to address the disadvantages of attention-based and dependency-based methods for ABSA tasks. Our\nDualGCN model integrates syntactic knowledge and semantic information by means of the SynGCN and SemGCN modules. Moreover, to effectively capture the semantic correlation between words, we propose orthogonal and differential regularizers in the SemGCN module. These regularizers can attend to the semantically related items with less overlap of each word and capture feature representations that differ from the syntactic structure. Extensive experiments on benchmark datasets show that our DualGCN model outperforms baselines."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported in part by the National Key R&D Program of China under Grant 2019YFF0303300 and Subject II under Grant 2019YFF0303302, in part by the National Natural Science Foundation of China under Grants 61906018 and 62076032, in part by the 111 Project under Grant B08004, and in part by the Fundamental Research Funds for the Central Universities under Grant 2021RC36."
    } ],
    "references" : [ {
      "title" : "Inducing target-specific latent structures for aspect sentiment classification",
      "author" : [ "Chenhua Chen", "Zhiyang Teng", "Yue Zhang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5596–5607, On-",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Recurrent attention network on memory for aspect sentiment analysis",
      "author" : [ "Peng Chen", "Zhongqian Sun", "Lidong Bing", "Wei Yang." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 452–461, Copen-",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Adaptive recursive neural network for target-dependent Twitter sentiment classification",
      "author" : [ "Li Dong", "Furu Wei", "Chuanqi Tan", "Duyu Tang", "Ming Zhou", "Ke Xu." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Dong et al\\.,? 2014",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2014
    }, {
      "title" : "Multi-grained attention network for aspect-level sentiment classification",
      "author" : [ "Feifan Fan", "Yansong Feng", "Dongyan Zhao." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3433–3442, Brussels, Bel-",
      "citeRegEx" : "Fan et al\\.,? 2018",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2018
    }, {
      "title" : "A position-aware bidirectional attention network for aspect-level sentiment analysis",
      "author" : [ "Shuqin Gu", "Lipeng Zhang", "Yuexian Hou", "Yin Song." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 774–784, Santa",
      "citeRegEx" : "Gu et al\\.,? 2018",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2018
    }, {
      "title" : "Effective attention modeling for aspect-level sentiment classification",
      "author" : [ "Ruidan He", "Wee Sun Lee", "Hwee Tou Ng", "Daniel Dahlmeier." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 1121–1131, Santa Fe,",
      "citeRegEx" : "He et al\\.,? 2018",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2018
    }, {
      "title" : "Syntaxaware aspect level sentiment classification with graph attention networks",
      "author" : [ "Binxuan Huang", "Kathleen Carley." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Huang and Carley.,? 2019",
      "shortCiteRegEx" : "Huang and Carley.",
      "year" : 2019
    }, {
      "title" : "Aspect level sentiment classification with attention-over-attention neural networks",
      "author" : [ "Binxuan Huang", "Yanglan Ou", "Kathleen M. Carley." ],
      "venue" : "CoRR, abs/1804.06536.",
      "citeRegEx" : "Huang et al\\.,? 2018",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2018
    }, {
      "title" : "Target-dependent Twitter sentiment classification",
      "author" : [ "Long Jiang", "Mo Yu", "Ming Zhou", "Xiaohua Liu", "Tiejun Zhao." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages",
      "citeRegEx" : "Jiang et al\\.,? 2011",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2011
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas N. Kipf", "Max Welling." ],
      "venue" : "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017.",
      "citeRegEx" : "Kipf and Welling.,? 2017",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2017
    }, {
      "title" : "NRC-Canada-2014: Detecting aspects and sentiment in customer reviews",
      "author" : [ "Svetlana Kiritchenko", "Xiaodan Zhu", "Colin Cherry", "Saif Mohammad." ],
      "venue" : "Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 437–",
      "citeRegEx" : "Kiritchenko et al\\.,? 2014",
      "shortCiteRegEx" : "Kiritchenko et al\\.",
      "year" : 2014
    }, {
      "title" : "Reflections on sentiment/opinion analysis",
      "author" : [ "Jiwei Li", "Eduard Hovy." ],
      "venue" : "A Practical Guide to Sentiment Analysis, pages 41–59, Cham. Springer International Publishing.",
      "citeRegEx" : "Li and Hovy.,? 2017",
      "shortCiteRegEx" : "Li and Hovy.",
      "year" : 2017
    }, {
      "title" : "Hierarchical attention based position-aware network for aspect-level sentiment analysis",
      "author" : [ "Lishuang Li", "Yang Liu", "AnQiao Zhou." ],
      "venue" : "Proceedings of the 22nd Conference on Computational Natural Language Learning, pages 181–189, Brussels, Belgium.",
      "citeRegEx" : "Li et al\\.,? 2018a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Transformation networks for target-oriented sentiment classification",
      "author" : [ "Xin Li", "Lidong Bing", "Wai Lam", "Bei Shi." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 946–",
      "citeRegEx" : "Li et al\\.,? 2018b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Jointly learning aspect-focused and inter-aspect relations with graph convolutional networks for aspect sentiment analysis",
      "author" : [ "Bin Liang", "Rongdi Yin", "Lin Gui", "Jiachen Du", "Ruifeng Xu." ],
      "venue" : "Proceedings of the 28th International Conference on Com-",
      "citeRegEx" : "Liang et al\\.,? 2020",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2020
    }, {
      "title" : "Sentiment analysis and opinion mining",
      "author" : [ "Bing Liu." ],
      "venue" : "Synthesis lectures on human language technologies, 5(1):1–167.",
      "citeRegEx" : "Liu.,? 2012",
      "shortCiteRegEx" : "Liu.",
      "year" : 2012
    }, {
      "title" : "Interactive attention networks for aspect-level sentiment classification",
      "author" : [ "Dehong Ma", "Sujian Li", "Xiaodong Zhang", "Houfeng Wang." ],
      "venue" : "Proceedings of the 26th International Joint Conference on Artificial Intelligence, IJCAI’17, page 4068–4074.",
      "citeRegEx" : "Ma et al\\.,? 2017",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2017
    }, {
      "title" : "Encoding sentences with graph convolutional networks for semantic role labeling",
      "author" : [ "Diego Marcheggiani", "Ivan Titov." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1506–1515, Copenhagen,",
      "citeRegEx" : "Marcheggiani and Titov.,? 2017",
      "shortCiteRegEx" : "Marcheggiani and Titov.",
      "year" : 2017
    }, {
      "title" : "Rethinking self-attention: An interpretable selfattentive encoder-decoder parser",
      "author" : [ "Khalil Mrini", "Franck Dernoncourt", "Trung Bui", "Walter Chang", "Ndapa Nakashole." ],
      "venue" : "arXiv preprint arXiv:1911.03875.",
      "citeRegEx" : "Mrini et al\\.,? 2019",
      "shortCiteRegEx" : "Mrini et al\\.",
      "year" : 2019
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha,",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Modelling context and syntactical features for aspectbased sentiment analysis",
      "author" : [ "Minh Hieu Phan", "Philip O. Ogunbona." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3211–3220, Online. As-",
      "citeRegEx" : "Phan and Ogunbona.,? 2020",
      "shortCiteRegEx" : "Phan and Ogunbona.",
      "year" : 2020
    }, {
      "title" : "SemEval-2014 task 4: Aspect based sentiment analysis",
      "author" : [ "Maria Pontiki", "Dimitris Galanis", "John Pavlopoulos", "Harris Papageorgiou", "Ion Androutsopoulos", "Suresh Manandhar." ],
      "venue" : "Proceedings of the 8th International Workshop on Semantic Evaluation",
      "citeRegEx" : "Pontiki et al\\.,? 2014",
      "shortCiteRegEx" : "Pontiki et al\\.",
      "year" : 2014
    }, {
      "title" : "Utilizing BERT for aspect-based sentiment analysis via constructing auxiliary sentence",
      "author" : [ "Chi Sun", "Luyao Huang", "Xipeng Qiu." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Sun et al\\.,? 2019a",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Aspect-level sentiment analysis via convolution over dependency tree",
      "author" : [ "Kai Sun", "Richong Zhang", "Samuel Mensah", "Yongyi Mao", "Xudong Liu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Sun et al\\.,? 2019b",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Recognizing conflict opinions in aspect-level sentiment classification with dual attention networks",
      "author" : [ "Xingwei Tan", "Yi Cai", "Changxi Zhu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th In-",
      "citeRegEx" : "Tan et al\\.,? 2019",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2019
    }, {
      "title" : "Aspect level sentiment classification with deep memory network",
      "author" : [ "Duyu Tang", "Bing Qin", "Ting Liu." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 214–224, Austin, Texas. Association for Com-",
      "citeRegEx" : "Tang et al\\.,? 2016a",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2016
    }, {
      "title" : "Aspect level sentiment classification with deep memory network",
      "author" : [ "Duyu Tang", "Bing Qin", "Ting Liu." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 214–224, Austin, Texas. Association for Com-",
      "citeRegEx" : "Tang et al\\.,? 2016b",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2016
    }, {
      "title" : "Dependency graph enhanced dualtransformer structure for aspect-based sentiment classification",
      "author" : [ "Hao Tang", "Donghong Ji", "Chenliang Li", "Qiji Zhou." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Tang et al\\.,? 2020",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2020
    }, {
      "title" : "Modeling online reviews with multi-grain topic models",
      "author" : [ "Ivan Titov", "Ryan McDonald." ],
      "venue" : "Proceedings of the 17th International Conference on World Wide Web, page 111–120, New York, NY, USA. Association for Computing Machinery.",
      "citeRegEx" : "Titov and McDonald.,? 2008",
      "shortCiteRegEx" : "Titov and McDonald.",
      "year" : 2008
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep learning for event-driven stock prediction",
      "author" : [ "Duy-Tin Vo", "Yue Zhang." ],
      "venue" : "Proceedings of IJCAI, BueNos Aires, Argentina.",
      "citeRegEx" : "Vo and Zhang.,? 2015",
      "shortCiteRegEx" : "Vo and Zhang.",
      "year" : 2015
    }, {
      "title" : "Relational graph attention network for aspect-based sentiment analysis",
      "author" : [ "Kai Wang", "Weizhou Shen", "Yunyi Yang", "Xiaojun Quan", "Rui Wang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3229–",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention-based LSTM for aspectlevel sentiment classification",
      "author" : [ "Yequan Wang", "Minlie Huang", "Xiaoyan Zhu", "Li Zhao." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 606–615, Austin,",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "BERT post-training for review reading comprehension and aspect-based sentiment analysis",
      "author" : [ "Hu Xu", "Bing Liu", "Lei Shu", "Philip Yu." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Xu et al\\.,? 2019",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2019
    }, {
      "title" : "Aspect-based sentiment classification with aspectspecific graph convolutional networks",
      "author" : [ "Chen Zhang", "Qiuchi Li", "Dawei Song." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna-",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Convolution over hierarchical syntactic and lexical graphs for aspect level sentiment analysis",
      "author" : [ "Mi Zhang", "Tieyun Qian." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3540–3549, On-",
      "citeRegEx" : "Zhang and Qian.,? 2020",
      "shortCiteRegEx" : "Zhang and Qian.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Sentiment analysis has become a popular topic in natural language processing (Liu, 2012; Li and Hovy, 2017).",
      "startOffset" : 77,
      "endOffset" : 107
    }, {
      "referenceID" : 12,
      "context" : "Sentiment analysis has become a popular topic in natural language processing (Liu, 2012; Li and Hovy, 2017).",
      "startOffset" : 77,
      "endOffset" : 107
    }, {
      "referenceID" : 33,
      "context" : "To judge the sentiment of a particular aspect, previous studies (Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018) have proposed various recurrent neural networks (RNNs) with attention mechanisms to generate aspect-specific sentence representations and have achieved appealing results.",
      "startOffset" : 64,
      "endOffset" : 194
    }, {
      "referenceID" : 26,
      "context" : "To judge the sentiment of a particular aspect, previous studies (Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018) have proposed various recurrent neural networks (RNNs) with attention mechanisms to generate aspect-specific sentence representations and have achieved appealing results.",
      "startOffset" : 64,
      "endOffset" : 194
    }, {
      "referenceID" : 17,
      "context" : "To judge the sentiment of a particular aspect, previous studies (Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018) have proposed various recurrent neural networks (RNNs) with attention mechanisms to generate aspect-specific sentence representations and have achieved appealing results.",
      "startOffset" : 64,
      "endOffset" : 194
    }, {
      "referenceID" : 1,
      "context" : "To judge the sentiment of a particular aspect, previous studies (Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018) have proposed various recurrent neural networks (RNNs) with attention mechanisms to generate aspect-specific sentence representations and have achieved appealing results.",
      "startOffset" : 64,
      "endOffset" : 194
    }, {
      "referenceID" : 4,
      "context" : "To judge the sentiment of a particular aspect, previous studies (Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018) have proposed various recurrent neural networks (RNNs) with attention mechanisms to generate aspect-specific sentence representations and have achieved appealing results.",
      "startOffset" : 64,
      "endOffset" : 194
    }, {
      "referenceID" : 8,
      "context" : "To judge the sentiment of a particular aspect, previous studies (Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018) have proposed various recurrent neural networks (RNNs) with attention mechanisms to generate aspect-specific sentence representations and have achieved appealing results.",
      "startOffset" : 64,
      "endOffset" : 194
    }, {
      "referenceID" : 5,
      "context" : "To judge the sentiment of a particular aspect, previous studies (Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018) have proposed various recurrent neural networks (RNNs) with attention mechanisms to generate aspect-specific sentence representations and have achieved appealing results.",
      "startOffset" : 64,
      "endOffset" : 194
    }, {
      "referenceID" : 28,
      "context" : "Moreover, motivated by the work of DGEDT (Tang et al., 2020), we utilize a BiAffine module to bridge relevant information between the SynGCN and SemGCN modules.",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 29,
      "context" : "Earlier methods (Titov and McDonald, 2008; Jiang et al., 2011; Kiritchenko et al., 2014; Vo and Zhang, 2015) are usually based on handcrafted features and fail to",
      "startOffset" : 16,
      "endOffset" : 108
    }, {
      "referenceID" : 9,
      "context" : "Earlier methods (Titov and McDonald, 2008; Jiang et al., 2011; Kiritchenko et al., 2014; Vo and Zhang, 2015) are usually based on handcrafted features and fail to",
      "startOffset" : 16,
      "endOffset" : 108
    }, {
      "referenceID" : 11,
      "context" : "Earlier methods (Titov and McDonald, 2008; Jiang et al., 2011; Kiritchenko et al., 2014; Vo and Zhang, 2015) are usually based on handcrafted features and fail to",
      "startOffset" : 16,
      "endOffset" : 108
    }, {
      "referenceID" : 31,
      "context" : "Earlier methods (Titov and McDonald, 2008; Jiang et al., 2011; Kiritchenko et al., 2014; Vo and Zhang, 2015) are usually based on handcrafted features and fail to",
      "startOffset" : 16,
      "endOffset" : 108
    }, {
      "referenceID" : 33,
      "context" : "Recently, various attention-based neural networks have been proposed to implicitly model the semantic relation of an aspect and its context to capture the opinion expression component (Wang et al., 2016; Tang et al., 2016a,b; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018; Li et al., 2018a; Tan et al., 2019).",
      "startOffset" : 184,
      "endOffset" : 352
    }, {
      "referenceID" : 17,
      "context" : "Recently, various attention-based neural networks have been proposed to implicitly model the semantic relation of an aspect and its context to capture the opinion expression component (Wang et al., 2016; Tang et al., 2016a,b; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018; Li et al., 2018a; Tan et al., 2019).",
      "startOffset" : 184,
      "endOffset" : 352
    }, {
      "referenceID" : 1,
      "context" : "Recently, various attention-based neural networks have been proposed to implicitly model the semantic relation of an aspect and its context to capture the opinion expression component (Wang et al., 2016; Tang et al., 2016a,b; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018; Li et al., 2018a; Tan et al., 2019).",
      "startOffset" : 184,
      "endOffset" : 352
    }, {
      "referenceID" : 4,
      "context" : "Recently, various attention-based neural networks have been proposed to implicitly model the semantic relation of an aspect and its context to capture the opinion expression component (Wang et al., 2016; Tang et al., 2016a,b; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018; Li et al., 2018a; Tan et al., 2019).",
      "startOffset" : 184,
      "endOffset" : 352
    }, {
      "referenceID" : 8,
      "context" : "Recently, various attention-based neural networks have been proposed to implicitly model the semantic relation of an aspect and its context to capture the opinion expression component (Wang et al., 2016; Tang et al., 2016a,b; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018; Li et al., 2018a; Tan et al., 2019).",
      "startOffset" : 184,
      "endOffset" : 352
    }, {
      "referenceID" : 5,
      "context" : "Recently, various attention-based neural networks have been proposed to implicitly model the semantic relation of an aspect and its context to capture the opinion expression component (Wang et al., 2016; Tang et al., 2016a,b; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018; Li et al., 2018a; Tan et al., 2019).",
      "startOffset" : 184,
      "endOffset" : 352
    }, {
      "referenceID" : 13,
      "context" : "Recently, various attention-based neural networks have been proposed to implicitly model the semantic relation of an aspect and its context to capture the opinion expression component (Wang et al., 2016; Tang et al., 2016a,b; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018; Li et al., 2018a; Tan et al., 2019).",
      "startOffset" : 184,
      "endOffset" : 352
    }, {
      "referenceID" : 25,
      "context" : "Recently, various attention-based neural networks have been proposed to implicitly model the semantic relation of an aspect and its context to capture the opinion expression component (Wang et al., 2016; Tang et al., 2016a,b; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018; Li et al., 2018a; Tan et al., 2019).",
      "startOffset" : 184,
      "endOffset" : 352
    }, {
      "referenceID" : 33,
      "context" : "For instance, (Wang et al., 2016) proposed attentionbased LSTMs for aspect-level sentiment classification.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : ", 2016b) and (Chen et al., 2017) both introduced a hierarchical attention network to identify important sentiment information related to the given aspect.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 4,
      "context" : "(Fan et al., 2018) exploited a multi-grained attention mechanism to capture the word-level interaction between aspects and their context.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 2,
      "context" : "tion, the pre-trained language model BERT (Devlin et al., 2019) has achieved remarkable performance in many NLP tasks, including ABSA.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 23,
      "context" : "(Sun et al., 2019a) transformed ABSA task into a sentence pair classification task by constructing an auxiliary sentence.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 34,
      "context" : "(Xu et al., 2019) proposed a post-training approach on the BERT to enhance the performance of fine-tuning stage for the ABSA task.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 3,
      "context" : "(Dong et al., 2014) proposed a recursive neural network to adaptively propagate the sentiment of words to the aspect along the dependency tree.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 21,
      "context" : "(Phan and Ogunbona, 2020) utilized the syntactic relative distance to reduce the impact of irrelevant words.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "For instance, (Chen et al., 2020) combined a dependency graph and a latent graph to generate the aspect representation.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 15,
      "context" : "(Liang et al., 2020) constructed aspect-focused and inter-aspect graphs to learn dependency feature of the key aspect words and sentiment relations between different aspects.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 10,
      "context" : "an efficient CNN variant that operates directly on graphs (Kipf and Welling, 2017).",
      "startOffset" : 58,
      "endOffset" : 82
    }, {
      "referenceID" : 35,
      "context" : "Most previous work (Zhang et al., 2019; Sun et al., 2019b) extend GCN models by",
      "startOffset" : 19,
      "endOffset" : 58
    }, {
      "referenceID" : 24,
      "context" : "Most previous work (Zhang et al., 2019; Sun et al., 2019b) extend GCN models by",
      "startOffset" : 19,
      "endOffset" : 58
    }, {
      "referenceID" : 19,
      "context" : "Here, we use the state-of-the-art dependency parsing model LAL-Parser (Mrini et al., 2019).",
      "startOffset" : 70,
      "endOffset" : 90
    }, {
      "referenceID" : 22,
      "context" : "are made public from the SemEval ABSA challenge (Pontiki et al., 2014).",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 1,
      "context" : "Following (Chen et al., 2017), we remove the instances using the “conflict” label.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 3,
      "context" : "In addition, the Twitter dataset is a collection of tweets (Dong et al., 2014).",
      "startOffset" : 59,
      "endOffset" : 78
    }, {
      "referenceID" : 20,
      "context" : "For all the experiments, we use pretrained 300-dimensional Glove3 vectors (Pennington et al., 2014) to initialize the word embeddings.",
      "startOffset" : 74,
      "endOffset" : 99
    }, {
      "referenceID" : 18,
      "context" : "Additionally, following (Marcheggiani and Titov, 2017), we add a self-loop for each node in",
      "startOffset" : 24,
      "endOffset" : 54
    }, {
      "referenceID" : 33,
      "context" : "1) ATAE-LSTM (Wang et al., 2016) utilizes aspect embedding and the attention mechanism in aspectlevel sentiment classification.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : "3) RAM (Chen et al., 2017) uses multiple attention and memory networks to learn the sentence representation.",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 4,
      "context" : "4) MGAN (Fan et al., 2018) designs a multigrained attention mechanism to capture word-level interactions between the aspect and context.",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 14,
      "context" : "5) TNet (Li et al., 2018b) transforms BiLSTM embeddings into target-specific embeddings and uses",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 35,
      "context" : "6) ASGCN (Zhang et al., 2019) first proposed using GCN to learn the aspect-specific representations for aspect-based sentiment classification.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 36,
      "context" : "8) BiGCN (Zhang and Qian, 2020) uses hierarchical graph structure to integrate word co-occurrence information and dependency type information.",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "9) kumaGCN (Chen et al., 2020) employs a latent graph structure to complement syntactic features.",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 15,
      "context" : "10) InterGCN (Liang et al., 2020) utilizes a GCN over a dependency tree to learn aspect representations with syntactic information.",
      "startOffset" : 13,
      "endOffset" : 33
    }, {
      "referenceID" : 32,
      "context" : "11) R-GAT (Wang et al., 2020) proposes a aspectoriented dependency tree structure and then encodes new dependency trees with a relational GAT.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 28,
      "context" : "12) DGEDT (Tang et al., 2020) proposes a dependency graph enhanced dual-transformer network by jointly considering flat representations and graphbased representations.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 2,
      "context" : "13) BERT (Devlin et al., 2019) is the vanilla BERT model by feeding the sentence-aspect pair and using the representation of [CLS] for predictions.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 32,
      "context" : "14) R-GAT+BERT (Wang et al., 2020) is the RGAT model that uses a pre-trained BERT to replace BiLSTM as an encoder.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 28,
      "context" : "15) DGEDT+BERT (Tang et al., 2020) is the DGEDT model that uses a pre-trained BERT to replace BiLSTM as an encoder.",
      "startOffset" : 15,
      "endOffset" : 34
    } ],
    "year" : 2021,
    "abstractText" : "Aspect-based sentiment analysis is a finegrained sentiment classification task. Recently, graph neural networks over dependency trees have been explored to explicitly model connections between aspects and opinion words. However, the improvement is limited due to the inaccuracy of the dependency parsing results and the informal expressions and complexity of online reviews. To overcome these challenges, in this paper, we propose a dual graph convolutional networks (DualGCN) model that considers the complementarity of syntax structures and semantic correlations simultaneously. Particularly, to alleviate dependency parsing errors, we design a SynGCN module with rich syntactic knowledge. To capture semantic correlations, we design a SemGCN module with self-attention mechanism. Furthermore, we propose orthogonal and differential regularizers to capture semantic correlations between words precisely by constraining attention scores in the SemGCN module. The orthogonal regularizer encourages the SemGCN to learn semantically correlated words with less overlap for each word. The differential regularizer encourages the SemGCN to learn semantic features that the SynGCN fails to capture. Experimental results on three public datasets show that our DualGCN model outperforms state-of-theart methods and verify the effectiveness of our model.",
    "creator" : "LaTeX with hyperref"
  }
}