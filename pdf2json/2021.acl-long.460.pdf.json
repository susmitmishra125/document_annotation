{
  "name" : "2021.acl-long.460.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Controversy and Conformity: from Generalized to Personalized Aggressiveness Detection",
    "authors" : [ "Kamil Kanclerz", "Alicja Figas", "Marcin Gruza", "Tomasz Kajdanowicz", "Jan Kocoń", "Daria Puchalska", "Przemysław Kazienko" ],
    "emails" : [ "kazienko}@pwr.edu.pl", "238442@student.pwr.edu.pl", "234800@student.pwr.edu.pl" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5915–5926\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5915"
    }, {
      "heading" : "1 Introduction",
      "text" : "Unfortunately, in the pursuit of knowledge on the Internet, one may come across content that they consider inappropriate for various reasons, such as being too aggressive. Many users notoriously come across content that offends them while surfing the Internet. This can cause discomfort and discourage from further expansion of knowledge. To avoid this, it is important to effectively filter out content that a given user may find unwanted. This poses a risk of erroneous assessment of whether a given text is considered inappropriate by a given person. For that purpose, we need to extend commonly applied generalizing solutions and develop personalized methods that take into account beliefs and preferences of the individual user. We expect this information can be obtained from the individual’s prior opinions about the offensiveness of some texts. Then, it is crucial to select the relevant texts that allow deriving as much information about users preferences as possible. Our new idea is to use some known, most controversial texts whose offensiveness is very ambiguous and depends more on subjective personal judgment. We examined how many documents has to be annotated by a given user to encapsulate their beliefs sufficiently and to improve personalized reasoning. Independently, we considered personal measures quantifying conformity of each individual. In other words, we measured to what extent a person evaluates documents similarly to others, i.e. \"is a part of the mainstream\". The conformity measures are used as input features for the classifier. This way, it is possible to find out the user beliefs based on their opinions regarding a relatively small number of texts. In this paper, we present novel methods of personalized aggressive content detection based on the representation of user opinion about aggressive texts. We propose: (1) conformity-based personalization, (2) class-based embeddings, and (3) annotation-based embeddings (Sec. 6). Our experiments were performed on the only relevant dataset Wikipedia Talk Labels: Aggression (Sec. 3). Having defined and calculated controversy of documents and conformity of users (Sec. 4), we validated our methods. The results revealed that additional individualized features: simple user conformity measures computed on few texts or embeddings of even four controversial texts significantly boost our personalized classification (Sec. 8). The gain provided by our personalized methods is greater for more controversial documents. This work is based on the results obtained in the article (Kocoń et al., 2021). In addition, in paper (Milkowski et al., 2021), we showed that the personalized approach is also effective for other subjective problems in NLP, such as\nrecognizing emotions elicited by text. The source code we used to conduct experiments and evaluation is publicly available in CLARIN-PL GitHub repository1."
    }, {
      "heading" : "2 Related work",
      "text" : "It is observable a steady increase in the number of offensive (Levmore and Nussbaum, 2010), hate (Breckheimer, 2001; Brown, 2018), aggressive, toxic, cyberbullying (Chen et al., 2012), or simply socially unacceptable online messages (Ljubešić et al., 2019). There are many definitions of offensive speech, which can be summarised as speech that targets specific social groups in a way that is harmful to them (Jacobs, 2002). Some countries, such as the USA, protect the rights to use this type of speech as an acceptable form of political expression (Heyman, 2008). In turn, the law prohibits hate speech in many EU countries (Rosenfeld, 2002). Such laws pose a challenge for operators of social networking sites and other online services to identify and moderate unacceptable content. Large companies such as Facebook and Google are often accused of not doing enough to ensure that their platforms are not used to attack other people (Ben-David and Fernández, 2016). On the other hand, attempts to automatically control content often lead to the accidental blocking of content that was not intended to offend anyone.\nAmbiguity of the definition of offensiveness is a serious problem. This inconsistency is visible in many reviews related to automatic detection of hate speech (Fortuna and Nunes, 2018; Schmidt and Wiegand, 2017; Alrehili, 2019; Poletto et al., 2020) or more specifically on aggressiveness detection (Sadiq et al., 2021; Modha et al., 2020).\nAutomatic recognition of offensive speech is the subject of many NLP workshops, such as Semeval 2019 (Zampieri et al., 2019b), GermEval 2018 (Wiegand et al., 2018), FIRE/HASOC 2019 (Mandl et al., 2019) or PolEval 2019 (Ptaszyński et al., 2019). Classic methods do not consider context and word order, e.g. the bag-of-words model (Zhang et al., 2010) or TF-IDF (Sahlgren et al., 2018). The representation may be extended with additional ontologies (Bloehdorn and Hotho, 2004) or WordNets (Scott and Matwin, 1998; Piasecki et al., 2009; Misiaszek et al., 2014; Janz et al., 2017; Kocoń et al., 2019b) and used with SVM (Razavi\n1https://github.com/CLARIN-PL/ controversy-conformity\net al., 2010) or logistic regression models (Waseem and Hovy, 2016; Sahlgren et al., 2018; Kocoń et al., 2018; Kocoń and Maziarz, 2021). New methods often use word embeddings (Wiegand et al., 2018; Bojanowski et al., 2017; Łukasz Augustyniak et al., 2021) (Wiegand et al., 2018; Bojanowski et al., 2017) mixed with character embeddings (Augustyniak et al., 2019), together with deep neural networks, e.g. CNN (Zampieri et al., 2019a) or LSTM (Yenala et al., 2017). The current state-of-the-art are Transformer-based architectures such as BERT (Devlin et al., 2019), ALBERT (Lan et al., 2019), XLNet (Yang et al., 2019) or RoBERTa (Liu et al., 2019). Nevertheless all these methods focus solely on the text itself. Any wider context has been considered very rarely, e.g. as time, thread or author’s social network features (Ziems et al., 2020).\nIn articles focused on detection of aggressiveness (Modha et al., 2018; Risch and Krestel, 2018; Safi Samghabadi et al., 2020), the most often used were datasets shared at the Workshops on Trolling, Aggression and Cyberbullying (TRAC) (Kumar et al., 2018, 2020) at LREC. Few others also used the Wikipedia Talk Labels: Aggression (Wulczyn et al., 2017b), where all individual annotations are available, not just the majority vote. Unfortunately, we have not found any other aggression dataset, for which this information would also be given. Moreover the authors focus mainly on the multilingual aspect of the aggression detection (Modha et al., 2018; Risch and Krestel, 2018; Safi Samghabadi et al., 2020). In addition to deep neural models, less complex methods such as logistic regression are also used (Modha et al., 2018; Risch and Krestel, 2018).\nTo the best of our knowledge, there are no work that dealt with the subjective problem of aggressiveness detection in the personalized way. The disagreement between annotators is usually measured by a single value, e.g. using Cohen’s kappa or Krippendorf’s alpha, and not investigated further. The researchers prefer a higher agreement level rather than controversy. Therefore, majority annotation is used in modeling, which to some extent leads to the loss of valuable information.\nThere are several studies focusing on the problem of the disagreement in data annotations. This provides valuable information not only about the annotators, but also about the instances by reflecting their ambiguity (Aroyo and Welty, 2013). There may be no single right label for every text.\nThe disagreement was used to divide annotators into polarized groups (Akhtar et al., 2020) or to filter out the spammers (Raykar and Yu, 2012; Soberón et al., 2013). In (Gao et al., 2019), attention was also drawn to the problem of conformity bias, where the reviewers tend to issue similar opinions. Less frequently, the disagreement is examined at the instance level, to measure its controversy or ambiguity, as in (Aroyo and Welty, 2013). For example, (Chklovski and Mihalcea, 2003) used confusion matrices in word sense tagging task to create and explore coarse sense clusters."
    }, {
      "heading" : "3 Dataset: Wikipedia Talk Labels",
      "text" : "We used the Wikipedia Talk Labels: Aggression data, gathered in the Wikipedia Detox project (Wulczyn et al., 2017b,a). Unlike other collections, it provides information about all annotations given by Crowdflower workers (not only the majority vote) for 100k+ comments from English Wikipedia. The assigned aggression score ranged from very aggressive (-3), via neutral (0), to very friendly (3). It was binarized to ’1 - aggressive’ for negative scores or ’0 - nonaggressive’ for neutral or friendly annotations. The dataset contained a suggested data split into train, dev and test set.\nTo enable our experiments, we removed annotations assigned by workers with less than 100 annotations in the train set, <20 in the dev set or <20 in the test set. Otherwise, we would not have data to extract user beliefs from and to perform personalization. We also removed users who did not assign any aggressive label in the dev set. Information about at least one text, that a specific user considered aggressive was crucial to model his individual perception of such content. Finally, there were 2,450 annotators left (Tab. 1), so we randomly divided them into 10 equal-sized folds.\nThe train set is used to calculate the representations (embeddings) of documents being classified. This is the only data exploited in the classic, generalizing approach (our baseline). The dev set provides information about user beliefs, i.e. their previous annotations. Individualized input features are extracted from dev data: (1) conformity measures and (2) personal embeddings in class-based and annotation-based personalization. Personalizationrelated calculations on the dev set refer to both training and testing procedure. The documents from the test set are embedded and classified by the trained model for the validation purposes."
    }, {
      "heading" : "4 Controversy and Conformity Measures",
      "text" : "For training and testing purposes, both controversy Contr for documents and conformity GConf, WConf for users are calculated within the dev set."
    }, {
      "heading" : "4.1 Controversy",
      "text" : "Controversy Contr(d) ∈ [0, 1] of document d is an entropy-based measure expressed in the following\nway:\nContr(d) = { 0, if n0d = nd ∨ n1d = nd − ∑\nc=0,1 ncd nd log2 ( ncd nd ) , otherwise\nwhere n0d, n 1 d is the number of negative and positive annotations assigned to document d, respectively; nd is the total number of document d’s annotations, nd = n0d + n 1 d; ncd nd\napproximates the probability that annotation of document d is of class c. Contr(d) = 0 means that all users annotated d the same, Contr(d) = 1 when 50% of users perceived it aggressive and 50% not.\nControversy Contr(d) is used to rank documents from the dev dataset. The most controversial texts (top k) are embedded in class-based or annotation-based personalization. Independently, controversy is computed within the test data in order to investigate differences in reasoning quality for more and less controversial documents."
    }, {
      "heading" : "4.2 General conformity",
      "text" : "General conformity GConf(a,C) ∈ [0, 1] of human a quantifies how often a belongs to the majority of annotators evaluating individual texts. It can be of different kind depending on the class C we consider:\nGConf(a,C) = ∑ d∈Aa 1{ld∈C ∧ ld=ld,a}∑\nd∈Aa 1{ld∈C} ,\nwhere Aa is the set of documents annotated by a; C denotes the conformity type related to the considered classes, i.e. C = {0}, {1} or {0, 1}; ld,a is the class label assigned by a to document d; ld is the d’s class label obtained by majority voting. In case of equal annotations for both classes document d is considered aggressive. GConf(a,C) = 1 when a annotated all documents d ∈ Aa the same like the others and no one annotated it otherwise.\nNote that depending on C, conformity can be calculated in three variants: for nonaggressive (C = {0}), aggressive (C = {1}) or any documents (C = {0, 1}) annotated by a. Such three conformity values are used as input features in conformity-based personalization, Sec. 7."
    }, {
      "heading" : "4.3 Weighted conformity",
      "text" : "Weighted conformity WConf(a,C) ∈ [0, 1] is similar to general conformity GConf(a,C) but it respects the size of the group the annotator belongs\nto, while evaluating the document. The larger the group with annotator a, the greater annotator a conformity:\nWConf(a,C) =\n∑ d∈A ∑ c∈C\nncd nd 1{ld,a=c}∑\nd∈Aa 1{ld,a∈C} ."
    }, {
      "heading" : "5 Controversy Analysis",
      "text" : "To have some insight into our data, we calculated controversy Contr(d) on each dataset (train/dev/test). Fig. 2 presents the distribution of annotations for controversy measure in the dev and test set. In both, the ratio of aggressive to nonaggressive documents is increasing and reaching 0.5 for the most controversial documents, i.e. Contr(d) = 1 resulting from the same number of aggressive and nonaggressive votes. The examples of such texts are following:\n\"Your behaviour is inappropriate and your reaction is ludicrous. Do they give out admin rights in cornflake packets now?\", n0d = n 1 d = 5.\n\"Far from being ridiculous, it is the recommended approach to follow on wikipedia. We don’t simply state what either side claims, rather we report on how they are viewed by neutral 3rd party sources. Take it to WP:NPOVN if you don’t believe me, rather than indulging in your continued disruptive habit of always having the WP:LASTWORD.\", n0d = n 1 d = 14.\nWe learned that classic methods based solely on content analysis (not personalized) perform worse, the more controversial the documents being tested, Fig. 6. It was the main inspiration for our personalized methods.\nWe also checked contribution of aggressive texts for the consecutive most controversial documents included in the personal user embeddings, Fig. 3."
    }, {
      "heading" : "6 Methods for Personalized Aggressiveness Detection",
      "text" : "We assume that personal beliefs can be expressed by user activity, i.e. their individual annotations. It means that we can use information about k documents previously annotated by the user in the form of their embeddings or user conformity measures. It leads us to three novel personalization methods: (1) conformity-based, (2) text-based, and (3) annotation-based, Fig. 4. According to our initial studies, the most informative were user annotations provided for most controversial documents.\nIn conformity-based personalization, we exploited simple conformity measures that represent the beliefs of one user in the aggregated way: GConf and WConf. Each of them can deliver three separate values: for only aggressive, only nonaggressive, and all texts. Finally, we examined input feature sets based on only GConf, only WConf, and on both, Sec. 7.\nWe also propose two versions of personal embeddings for previously annotated texts: class-based and annotation-based.\nThe class-based embedding consists of two fastText embeddings of k documents from the dev set that the user rated as (1) nonaggressive and (2) separately as aggressive, Fig. 4. Each of the two embeddings can aggregate any and different number of previous user annotations; the embedding size is static for every k. If the user has not annotated any texts of given class (e.g. aggressive), the embedding represents an empty string (zeros). Overall, it is a very rare case in our experiments, mostly happening for k = 1.\nThe annotation-based embeddings consider all k user annotations individually. For each such text d, we use the following features: (1) the embedding of the d’s content, (2) its controversy Contr(d), (3)\nthe percentage of users who rated d as nonaggressive, (4) the rating of the given user (0/1), and (5) the information on whether this rating is consistent with the the majority rating. Thus, we receive a relatively large number of input features: 300+k∗304.\nOur general personalized aggressiveness detection procedure is as follows:\n1. We ask users to annotate k most controversial documents from the pre-defined set (here dev).\n2. Information from the first step is used to extract individually-specific features reflecting personal user beliefs, i.e. conformity measures or embeddings of these k texts (classbased and annotation-based methods).\n3. A subset of the same users (upper rows in Fig. 1) annotate next documents. The data about their following annotations (embeddings of texts from train) together with data from step 2. are used to train the classifier.\n4. For some other users (lower rows in Fig. 1), we also collect their annotations (the test set). Together with the information about their individual preferences (step 2.) they are used for validation (testing) purposes only."
    }, {
      "heading" : "7 Experimental setup",
      "text" : "To validate our three personalized methods, we utilized Wikipedia Talk Labels: Aggression, see Sec. 3. We applied 10-fold cross-validation based on users. The first nine sets are used to train the model (upper rows in Fig. 1), while the remaining 10th set for testing (lower rows in Fig. 1). The results presented in plots are averaged over all ten folds.\nSince only dev texts with annotations are assumed to represent prior knowledge about users, they were used to test personalization scenarios for each of our three methods: class-based, annotation-based, and conformity-based. The last one was in three variants: only three GConf(a,C) measures (for C = {0}, {1}, {0, 1}), only three WConf(a,C) measures, all six conformity values. Thus, we analyzed five methods in total. For each of them, we considered: (1) different number k=1,2,..20 of texts d previously annotated by user a: d ∈ Aa (for conformity-based methods |Aa| = k, (2) different selection procedures for texts d ∈ Aa used to represent a’s beliefs (personalization): (2a) k most controversial texts d ∈ Aa,\n(2b) k class-balanced most controversial (like 2a but with class balancing), (2c) most aggressive d ∈ Aa (rank according to % of aggressive annotations among all for d), (2d) random selection of k texts d ∈ Aa. In total, we tested: 10 folds x (5 methods x 20 distinct k no. of texts x 4 selection + 1 baseline) = 4,010 models.\nThe logistic regression models were optimized during the training process by using the L2 regularization and the early stopping mechanism. Both of them aim to prevent overfitting and the early stopping mechanism additionally ensures that the model instance that achieved the best loss function score is preserved. The models were run on Intel Xeon Processor E5-2650 v4.\nWe also compared our personalized methods with the baseline, i.e. the commonly investigated approach generalizing user perception. It exploited only the evaluated text embeddings as the input.\nWe considered classification performance not only for the whole test set but also in its breakdown of 10 percentage buckets according to three independent rankings of test docs: (1) most controversial (Contr(d)), (2) with least conformity GConf(a, {0, 1}), averaged over all a ∈ Test an-\nnotating d, (3) least WConf(a, {0, 1}). Here, the measures were computed for the test set only, not for dev. It was used to investigate where our models more outperform the baseline. In order to generate text embeddings in each personalization method, we used the fastText library (Bojanowski et al., 2017; Joulin et al., 2017). It offers pre-trained word vectors for 157 languages, based on the continuous bag of words (CBOW) model in a 300-dimensional space, with character n-grams of length 5."
    }, {
      "heading" : "8 Validation of personalization methods",
      "text" : "Both class-based and annotation-based methods were tested using various rankings while selecting texts for personal embeddings: most controversial, class-balanced most controversial, most aggressive, and random. The conformity-based methods were evaluated in terms of the measure variant used: general conformity, weighted conformity, and both, all with random selection of texts."
    }, {
      "heading" : "8.1 Conformity-based Personalization",
      "text" : "The results for three conformity-based personalization methods, i.e. three different sets of input conformity features (Sec. 7) and various number k\nof texts used to calculate user conformity are shown in Fig. 5a. The greater k results in more precise evaluation of user conformity. It also directly and positively impacts on model performance, although gains for k > 15 are very small.\nAdditionally, we considered the performance for more and less controversial documents in the test set, Fig. 6a. It is clearly visible that the nonpersonalized method is completely lost for the most controversial documents. However, our conformitybased models lose relatively less. It appears that their gain (smaller loss) is greater for 30% most controversial texts. In other words, the greater controversy, the greater gain from personalization."
    }, {
      "heading" : "8.2 Class-based Embeddings",
      "text" : "Fig. 5b describes evaluation of class-based embeddings for various text selection approaches and different number of previously annotated texts. The performance was shown only for texts from the aggression class (the same plot shapes were for macro F1 and both classes). The models using the most controversial texts for selection reached the best results in 14 out of 20 cases (70%). The highest F1 score was achieved for only 4 texts representing user beliefs. It was greater than the model\nwithout any personalization by over 7pp."
    }, {
      "heading" : "8.3 Annotation-based Embeddings",
      "text" : "Annotation-based embeddings were tested for the same rankings as in Sec. 8.2, Fig. 5c. The most controversial texts used to generate user representations and feed the model provided the best results in 17 out of 20 cases (85%). The best performance was achieved while using 18 texts to represent user personal beliefs – then, the input consisted of 5,772 features. The F1 score of this model was greater than the baseline by over 10pp.\nThe greater gain compared to the not personalized method is exposed for 50% of the most controversial texts in the test set; the greatest for 10% of the most controversial – even 22.7 percentage points (twice better: 44.0% vs. 21.3%), Fig. 6b."
    }, {
      "heading" : "8.4 Comparison of personalization methods",
      "text" : "The best models from each personalization method, which were achieved for annotations of most controversial texts, are compared in Fig. 5d. Models based on annotation-based embeddings provided significantly better results than the others in 10 out of 20 cases of k values (50%). The conformitybased models performed better than other models in\n3 out of 20 cases (15%); it referred to the smallest number of texts considered (k = 1÷ 3). The highest value of F1 score was achieved by the model using 18 texts to represent user personal beliefs. However, this solution used 5,772 input features, whereas the much simpler conformity-based model with 306 input features was only 2.7 percentage points worse. Simultaneously, conformity-based model training time was 38.6 times faster than the annotation-based one, Fig. 7.\nPractically, we would like to avoid bothering the user with too many previous annotations, i.e. we may want to limit k to just a few, for example k = 4. Then, we should select k most controversial texts and use either class-based or conformity-based personalization. They learn just as fast but keep the same performance: 7.3 percentage points, 5.7 percentage points greater F1 for class aggressive, respectively, and 3.9 percentage points, 3.2 percentage points greater macro F1 (for both classes), respectively.\nThe worst performance was observed for models using class-based embeddings. The results of evaluation on all texts are presented in Fig. 5d.\nRandom selection of k texts for personalization is almost always worse than dedicated rankings, Fig. 6b,c. Most controversial texts turned out to be the best option that usually outperformed the most aggressive and class-balanced most controversial."
    }, {
      "heading" : "9 Discussion",
      "text" : "A valuable observation from our experiments is that already one document used to valuate user beliefs is enough to significantly improve reasoning, Fig. 5d. Anyway, more texts in personalization keep boosting the performance, but about 4-5 previously annotated most controversial documents seem to be a reasonable trade-off between reasoning quality\nand user annoyance. Annotation-based embeddings most precisely express user opinions, but it comes at the cost of linearly longer learning and demand for more samples. They also cannot easily adapt to different number k of personalization documents.\nWe decided to utilize very fast logistic regression model with fastText embeddings, since we wanted to examine thousands of models related to multiple scenarios, not all are presented here.\nWe belief our personalization methods establish a new research direction: how to effectively and efficiently embed user beliefs? We expect new methods will be developed for that purpose.\nOne of the most important postulate derived from our research is the demand for new datasets collections. We need annotations of individual humans rather than aggregated and agreed general beliefs received by majority voting, by annotator training, or by removal of controversial texts.\nBesides, our personalization methods may be applied to any NLP problem with inconsistencies between people. It especially refers to diverse emotions evoked by textual content, hate speech, detection of cyberbullying or offensive, toxic, abusive,\nharmful, or socially unaccepted content. The common problem of imbalanced classes in aggressiveness detection (Tab. 1, Fig. 12) will be addressed in future work."
    }, {
      "heading" : "10 Conclusions",
      "text" : "The main conclusion from our research is that the natural controversies associated with individual perceptions of contents should not be overlooked or reduced but rather directly exploited in personalized solutions. Ultimately, this reflects the diversity in our societies.\nOur three new personalization methods make use of texts previously annotated by a given user by means of conformity measures, class-based or annotation-based embeddings. Just a few documents are able to capture individual user beliefs, the more so, the more controversial documents they relate to. As a result, all our methods outperform classic solutions that generalize offensiveness understanding. The gain is greater for more controversial documents.\nThe personalization solutions can also be applied to other NLP problems, where the content tends to be subjectively perceived as hate speech, cyberbullying, abusive or offensive, as well as in prediction of emotions elicited by text (Kocoń et al., 2019a; Milkowski et al., 2021) and even in sentiment analysis (Kocoń et al., 2019; Kanclerz et al., 2020).\nWe keep working on testing of our methods on more resource-demanding but also more SOTA language representations: XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), and XLM-RoBERTa (Conneau et al., 2020)."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was financed by (1) the National Science Centre, Poland, project no. 2020/37/B/ST6/03806; (2) the Polish Ministry of Education and Science, CLARIN-PL Project; (3) the European Regional Development Fund as a part of the 2014-2020 Smart Growth Operational Programme, CLARIN - Common Language Resources and Technology Infrastructure, project no. POIR.04.02.0000C002/19."
    } ],
    "references" : [ {
      "title" : "Modeling annotator perspective and polarized opinions to improve hate speech detection",
      "author" : [ "Sohail Akhtar", "Valerio Basile", "Viviana Patti." ],
      "venue" : "Proceed-",
      "citeRegEx" : "Akhtar et al\\.,? 2020",
      "shortCiteRegEx" : "Akhtar et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatic hate speech detection on social media: A brief survey",
      "author" : [ "A. Alrehili." ],
      "venue" : "2019 IEEE/ACS 16th International Conference on Computer Systems and Applications (AICCSA), pages 1–6.",
      "citeRegEx" : "Alrehili.,? 2019",
      "shortCiteRegEx" : "Alrehili.",
      "year" : 2019
    }, {
      "title" : "Harnessing disagreement in crowdsourcing a relation extraction gold standard",
      "author" : [ "Lora Aroyo", "Chris Welty." ],
      "venue" : "Technical report, Technical Report.",
      "citeRegEx" : "Aroyo and Welty.,? 2013",
      "shortCiteRegEx" : "Aroyo and Welty.",
      "year" : 2013
    }, {
      "title" : "Comprehensive analysis of aspect term extraction methods using various text embeddings",
      "author" : [ "Łukasz Augustyniak", "Tomasz Kajdanowicz", "Przemysław Kazienko." ],
      "venue" : "Computer Speech & Language, 69:101217.",
      "citeRegEx" : "Augustyniak et al\\.,? 2021",
      "shortCiteRegEx" : "Augustyniak et al\\.",
      "year" : 2021
    }, {
      "title" : "Aspect detection using word and char embeddings with (bi) lstm and crf",
      "author" : [ "Łukasz Augustyniak", "Tomasz Kajdanowicz", "Przemysław Kazienko." ],
      "venue" : "2019 IEEE Second International Conference on Artificial Intelligence and Knowledge Engineering",
      "citeRegEx" : "Augustyniak et al\\.,? 2019",
      "shortCiteRegEx" : "Augustyniak et al\\.",
      "year" : 2019
    }, {
      "title" : "Hate speech and covert discrimination on social media: Monitoring the facebook pages of extreme-right political parties in spain",
      "author" : [ "Anat Ben-David", "Ariadna Matamoros Fernández." ],
      "venue" : "International Journal of Communication, 10:27.",
      "citeRegEx" : "Ben.David and Fernández.,? 2016",
      "shortCiteRegEx" : "Ben.David and Fernández.",
      "year" : 2016
    }, {
      "title" : "Text classification by boosting weak learners based on terms and concepts",
      "author" : [ "Stephan Bloehdorn", "Andreas Hotho." ],
      "venue" : "Fourth IEEE International Conference on Data Mining (ICDM’04), pages 331– 334. IEEE.",
      "citeRegEx" : "Bloehdorn and Hotho.,? 2004",
      "shortCiteRegEx" : "Bloehdorn and Hotho.",
      "year" : 2004
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:135–146.",
      "citeRegEx" : "Bojanowski et al\\.,? 2017",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2017
    }, {
      "title" : "A haven for hate: the foreign and domestic implications of protecting internet hate speech under the first amendment",
      "author" : [ "Peter J Breckheimer." ],
      "venue" : "S. Cal. L. Rev., 75:1493.",
      "citeRegEx" : "Breckheimer.,? 2001",
      "shortCiteRegEx" : "Breckheimer.",
      "year" : 2001
    }, {
      "title" : "What is so special about online (as compared to offline) hate speech? Ethnicities, 18(3):297–326",
      "author" : [ "Alexander Brown" ],
      "venue" : null,
      "citeRegEx" : "Brown.,? \\Q2018\\E",
      "shortCiteRegEx" : "Brown.",
      "year" : 2018
    }, {
      "title" : "Detecting offensive language in social media to protect adolescent online safety",
      "author" : [ "Ying Chen", "Yilu Zhou", "Sencun Zhu", "Heng Xu." ],
      "venue" : "2012 International Conference on Privacy, Security, Risk and Trust and 2012 International Confernece on Social",
      "citeRegEx" : "Chen et al\\.,? 2012",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2012
    }, {
      "title" : "Exploiting agreement and disagreement of human annotators for word sense disambiguation",
      "author" : [ "Timothy Chklovski", "Rada Mihalcea." ],
      "venue" : "In Proceedings of RANLP 2003.",
      "citeRegEx" : "Chklovski and Mihalcea.,? 2003",
      "shortCiteRegEx" : "Chklovski and Mihalcea.",
      "year" : 2003
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440–",
      "citeRegEx" : "Guzmán et al\\.,? 2020",
      "shortCiteRegEx" : "Guzmán et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "A survey on automatic detection of hate speech in text",
      "author" : [ "P. Fortuna", "S. Nunes." ],
      "venue" : "ACM Computing Surveys (CSUR), 51:1 – 30.",
      "citeRegEx" : "Fortuna and Nunes.,? 2018",
      "shortCiteRegEx" : "Fortuna and Nunes.",
      "year" : 2018
    }, {
      "title" : "Does my rebuttal matter? insights from a major NLP conference",
      "author" : [ "Yang Gao", "Steffen Eger", "Ilia Kuznetsov", "Iryna Gurevych", "Yusuke Miyao." ],
      "venue" : "CoRR, abs/1903.11367.",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "Hate speech, public discourse, and the first amendment",
      "author" : [ "Steven J Heyman." ],
      "venue" : "Oxford University Press, Forthcoming.",
      "citeRegEx" : "Heyman.,? 2008",
      "shortCiteRegEx" : "Heyman.",
      "year" : 2008
    }, {
      "title" : "Hate crime: Criminal law and identity politics: Author’s summary",
      "author" : [ "James B Jacobs." ],
      "venue" : "Theoretical Criminology, 6(4):481–484.",
      "citeRegEx" : "Jacobs.,? 2002",
      "shortCiteRegEx" : "Jacobs.",
      "year" : 2002
    }, {
      "title" : "plWordNet as a Basis for Large Emotive Lexicons of Polish",
      "author" : [ "Arkadiusz Janz", "Jan Kocoń", "Maciej Piasecki", "Zaśko-Zielińska Monika." ],
      "venue" : "LTC’17 8th Language and Technology Conference, Poznań, Poland. Fundacja Uniwersytetu im. Adama Mick-",
      "citeRegEx" : "Janz et al\\.,? 2017",
      "shortCiteRegEx" : "Janz et al\\.",
      "year" : 2017
    }, {
      "title" : "Bag of tricks for efficient text classification",
      "author" : [ "Armand Joulin", "Édouard Grave", "Piotr Bojanowski", "Tomáš Mikolov." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers,",
      "citeRegEx" : "Joulin et al\\.,? 2017",
      "shortCiteRegEx" : "Joulin et al\\.",
      "year" : 2017
    }, {
      "title" : "Cross-lingual deep neural transfer learning in sentiment analysis",
      "author" : [ "Kamil Kanclerz", "Piotr Miłkowski", "Jan Kocoń." ],
      "venue" : "Procedia Computer Science, 176:128–137.",
      "citeRegEx" : "Kanclerz et al\\.,? 2020",
      "shortCiteRegEx" : "Kanclerz et al\\.",
      "year" : 2020
    }, {
      "title" : "Classifier-based polarity propagation in a wordnet",
      "author" : [ "Jan Kocoń", "Arkadiusz Janz", "Maciej Piasecki." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).",
      "citeRegEx" : "Kocoń et al\\.,? 2018",
      "shortCiteRegEx" : "Kocoń et al\\.",
      "year" : 2018
    }, {
      "title" : "Mapping wordnet onto human brain connectome in emotion processing and semantic similarity recognition",
      "author" : [ "Jan Kocoń", "Marek Maziarz." ],
      "venue" : "Information Processing & Management, 58(3):102530.",
      "citeRegEx" : "Kocoń and Maziarz.,? 2021",
      "shortCiteRegEx" : "Kocoń and Maziarz.",
      "year" : 2021
    }, {
      "title" : "Multi-level sentiment analysis of polemo 2.0: Extended corpus of multi-domain consumer reviews",
      "author" : [ "Jan Kocoń", "Piotr Miłkowski", "Monika ZaśkoZielińska" ],
      "venue" : "In Proceedings of the 23rd Conference on Computational Natural Language Learning",
      "citeRegEx" : "Kocoń et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Kocoń et al\\.",
      "year" : 2019
    }, {
      "title" : "Offensive, aggressive, and hate speech analysis: from data-centric to human-centred approach",
      "author" : [ "Jan Kocoń", "Alicja Figas", "Marcin Gruza", "Daria Puchalska", "Tomasz Kajdanowicz", "Przemysław Kazienko." ],
      "venue" : "Information Processing & Management.",
      "citeRegEx" : "Kocoń et al\\.,? 2021",
      "shortCiteRegEx" : "Kocoń et al\\.",
      "year" : 2021
    }, {
      "title" : "Recognition of emotions, valence",
      "author" : [ "Jan Kocoń", "Arkadiusz Janz", "Piotr Miłkowski", "Monika Riegel", "Małgorzata Wierzba", "Artur Marchewka", "Agnieszka Czoska", "Damian Grimling", "Barbara Konat", "Konrad Juszczyk", "Katarzyna Klessa", "Maciej Piasecki" ],
      "venue" : null,
      "citeRegEx" : "Kocoń et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Kocoń et al\\.",
      "year" : 2019
    }, {
      "title" : "Albert: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "arXiv preprint arXiv:1909.11942.",
      "citeRegEx" : "Lan et al\\.,? 2019",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2019
    }, {
      "title" : "The offensive Internet: Speech, privacy, and reputation",
      "author" : [ "Saul Levmore", "Martha Craven Nussbaum." ],
      "venue" : "Harvard University Press.",
      "citeRegEx" : "Levmore and Nussbaum.,? 2010",
      "shortCiteRegEx" : "Levmore and Nussbaum.",
      "year" : 2010
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "The frenk datasets of socially unacceptable discourse in slovene and english",
      "author" : [ "Nikola Ljubešić", "Darja Fišer", "Tomaž Erjavec." ],
      "venue" : "International Conference on Text, Speech, and Dialogue, pages 103– 114. Springer.",
      "citeRegEx" : "Ljubešić et al\\.,? 2019",
      "shortCiteRegEx" : "Ljubešić et al\\.",
      "year" : 2019
    }, {
      "title" : "Overview of the HASOC Track at FIRE 2019: Hate speech and offensive content identification in indo-european languages",
      "author" : [ "Thomas Mandl", "Sandip Modha", "Prasenjit Majumder", "Daksh Patel", "Mohana Dave", "Chintak Mandlia", "Aditya Patel." ],
      "venue" : "Pro-",
      "citeRegEx" : "Mandl et al\\.,? 2019",
      "shortCiteRegEx" : "Mandl et al\\.",
      "year" : 2019
    }, {
      "title" : "Personal bias in prediction of emotions elicited by textual opinions",
      "author" : [ "Piotr Milkowski", "Marcin Gruza", "Kamil Kanclerz", "Przemyslaw Kazienko", "Damian Grimling", "Jan Kocoń." ],
      "venue" : "Proceedings of the Joint Conference of the 59th Annual Meeting of the",
      "citeRegEx" : "Milkowski et al\\.,? 2021",
      "shortCiteRegEx" : "Milkowski et al\\.",
      "year" : 2021
    }, {
      "title" : "Belief propagation method for word sentiment in wordnet 3.0",
      "author" : [ "Andrzej Misiaszek", "Przemysław Kazienko", "Marcin Kulisiewicz", "Łukasz Augustyniak", "Włodzimierz Tuligłowicz", "Adrian Popiel", "Tomasz Kajdanowicz" ],
      "venue" : "In Asian Conference on In-",
      "citeRegEx" : "Misiaszek et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Misiaszek et al\\.",
      "year" : 2014
    }, {
      "title" : "Filtering aggression from the multilingual social media feed",
      "author" : [ "Sandip Modha", "Prasenjit Majumder", "Thomas Mandl." ],
      "venue" : "Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018), pages 199–207, Santa Fe, New",
      "citeRegEx" : "Modha et al\\.,? 2018",
      "shortCiteRegEx" : "Modha et al\\.",
      "year" : 2018
    }, {
      "title" : "Detecting and visualizing hate speech in social media: A cyber watchdog for surveillance",
      "author" : [ "Sandip Modha", "Prasenjit Majumder", "Thomas Mandl", "Chintak Mandalia." ],
      "venue" : "Expert Systems with Applications, 161:113725.",
      "citeRegEx" : "Modha et al\\.,? 2020",
      "shortCiteRegEx" : "Modha et al\\.",
      "year" : 2020
    }, {
      "title" : "A wordnet from the ground up",
      "author" : [ "Maciej Piasecki", "Bernd Broda", "Stanislaw Szpakowicz." ],
      "venue" : "Oficyna Wydawnicza Politechniki Wrocławskiej Wrocław.",
      "citeRegEx" : "Piasecki et al\\.,? 2009",
      "shortCiteRegEx" : "Piasecki et al\\.",
      "year" : 2009
    }, {
      "title" : "Resources and benchmark corpora for hate speech detection: a systematic review",
      "author" : [ "Fabio Poletto", "Valerio Basile", "M. Sanguinetti", "Cristina Bosco", "V. Patti." ],
      "venue" : "LREC 2020.",
      "citeRegEx" : "Poletto et al\\.,? 2020",
      "shortCiteRegEx" : "Poletto et al\\.",
      "year" : 2020
    }, {
      "title" : "Results of the poleval 2019 shared task 6: First dataset and open shared task for automatic cyberbullying detection in polish twitter",
      "author" : [ "Michal Ptaszyński", "Agata Pieciukiewicz", "Paweł Dybala." ],
      "venue" : "Proceedings of the PolEval 2019 Workshop,",
      "citeRegEx" : "Ptaszyński et al\\.,? 2019",
      "shortCiteRegEx" : "Ptaszyński et al\\.",
      "year" : 2019
    }, {
      "title" : "Eliminating spammers and ranking annotators for crowdsourced labeling tasks",
      "author" : [ "Vikas C. Raykar", "Shipeng Yu." ],
      "venue" : "J. Mach. Learn. Res., 13(1):491–518.",
      "citeRegEx" : "Raykar and Yu.,? 2012",
      "shortCiteRegEx" : "Raykar and Yu.",
      "year" : 2012
    }, {
      "title" : "Offensive language detection",
      "author" : [ "Amir H. Razavi", "Diana Inkpen", "Sasha Uritsky", "Stan Matwin" ],
      "venue" : null,
      "citeRegEx" : "Razavi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Razavi et al\\.",
      "year" : 2010
    }, {
      "title" : "Aggression identification using deep learning and data augmentation",
      "author" : [ "Julian Risch", "Ralf Krestel." ],
      "venue" : "Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018), pages 150–158, Santa Fe, New Mexico, USA. Association",
      "citeRegEx" : "Risch and Krestel.,? 2018",
      "shortCiteRegEx" : "Risch and Krestel.",
      "year" : 2018
    }, {
      "title" : "Hate speech in constitutional jurisprudence: a comparative analysis",
      "author" : [ "Michel Rosenfeld." ],
      "venue" : "Cardozo L. Rev., 24:1523.",
      "citeRegEx" : "Rosenfeld.,? 2002",
      "shortCiteRegEx" : "Rosenfeld.",
      "year" : 2002
    }, {
      "title" : "Aggression detection through deep neural model on twitter",
      "author" : [ "Saima Sadiq", "Arif Mehmood", "Saleem Ullah", "Maqsood Ahmad", "Gyu Sang Choi", "Byung-Won On." ],
      "venue" : "Future Generation Computer Systems, 114:120 – 129.",
      "citeRegEx" : "Sadiq et al\\.,? 2021",
      "shortCiteRegEx" : "Sadiq et al\\.",
      "year" : 2021
    }, {
      "title" : "Aggression and misogyny detection using BERT: A multi-task approach",
      "author" : [ "Niloofar Safi Samghabadi", "Parth Patwa", "Srinivas PYKL", "Prerana Mukherjee", "Amitava Das", "Thamar Solorio." ],
      "venue" : "Proceedings of the Second Workshop on Trolling, Ag-",
      "citeRegEx" : "Samghabadi et al\\.,? 2020",
      "shortCiteRegEx" : "Samghabadi et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning representations for detecting abusive language",
      "author" : [ "Magnus Sahlgren", "Tim Isbister", "Fredrik Olsson." ],
      "venue" : "Proceedings of the 2nd Workshop on Abusive Language Online (ALW2), pages 115– 123, Brussels, Belgium. Association for Computa-",
      "citeRegEx" : "Sahlgren et al\\.,? 2018",
      "shortCiteRegEx" : "Sahlgren et al\\.",
      "year" : 2018
    }, {
      "title" : "A survey on hate speech detection using natural language processing",
      "author" : [ "Anna Schmidt", "Michael Wiegand." ],
      "venue" : "Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media, pages 1–10, Valencia, Spain. Associa-",
      "citeRegEx" : "Schmidt and Wiegand.,? 2017",
      "shortCiteRegEx" : "Schmidt and Wiegand.",
      "year" : 2017
    }, {
      "title" : "Text classification using wordnet hypernyms",
      "author" : [ "Sam Scott", "Stan Matwin." ],
      "venue" : "Usage of WordNet in Natural Language Processing Systems.",
      "citeRegEx" : "Scott and Matwin.,? 1998",
      "shortCiteRegEx" : "Scott and Matwin.",
      "year" : 1998
    }, {
      "title" : "Measuring crowd truth: Disagreement metrics combined with worker behavior filters",
      "author" : [ "Guillermo Soberón", "Lora Aroyo", "Chris Welty", "Oana Inel", "Hui Lin", "Manfred Overmeen." ],
      "venue" : "Proceedings of the 1st International Conference on Crowdsourcing the",
      "citeRegEx" : "Soberón et al\\.,? 2013",
      "shortCiteRegEx" : "Soberón et al\\.",
      "year" : 2013
    }, {
      "title" : "Hateful symbols or hateful people? predictive features for hate speech detection on Twitter",
      "author" : [ "Zeerak Waseem", "Dirk Hovy." ],
      "venue" : "Proceedings of the NAACL Student Research Workshop, pages 88–93, San Diego, California. Association for Computa-",
      "citeRegEx" : "Waseem and Hovy.,? 2016",
      "shortCiteRegEx" : "Waseem and Hovy.",
      "year" : 2016
    }, {
      "title" : "Overview of the germeval 2018 shared task on the identification of offensive language",
      "author" : [ "Michael Wiegand", "Melanie Siegel", "Josef Ruppenhofer." ],
      "venue" : "In",
      "citeRegEx" : "Wiegand et al\\.,? 2018",
      "shortCiteRegEx" : "Wiegand et al\\.",
      "year" : 2018
    }, {
      "title" : "Ex machina: Personal attacks seen at scale",
      "author" : [ "Ellery Wulczyn", "Nithum Thain", "Lucas Dixon." ],
      "venue" : "Proceedings of the 26th International Conference on World Wide Web, pages 1391–1399.",
      "citeRegEx" : "Wulczyn et al\\.,? 2017a",
      "shortCiteRegEx" : "Wulczyn et al\\.",
      "year" : 2017
    }, {
      "title" : "2017b. Wikipedia talk labels: Aggression",
      "author" : [ "Ellery Wulczyn", "Nithum Thain", "Lucas Dixon" ],
      "venue" : null,
      "citeRegEx" : "Wulczyn et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Wulczyn et al\\.",
      "year" : 2017
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems, pages 5753–5763.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep learning for detecting inappropriate content in text",
      "author" : [ "Harish Yenala", "Ashish Jhanwar", "Manoj Chinnakotla", "Jay Goyal." ],
      "venue" : "International Journal of Data Science and Analytics.",
      "citeRegEx" : "Yenala et al\\.,? 2017",
      "shortCiteRegEx" : "Yenala et al\\.",
      "year" : 2017
    }, {
      "title" : "Predicting the type and target of offensive posts in social media",
      "author" : [ "Marcos Zampieri", "Shervin Malmasi", "Preslav Nakov", "Sara Rosenthal", "Noura Farra", "Ritesh Kumar." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the",
      "citeRegEx" : "Zampieri et al\\.,? 2019a",
      "shortCiteRegEx" : "Zampieri et al\\.",
      "year" : 2019
    }, {
      "title" : "SemEval-2019 task 6: Identifying and categorizing offensive language in social media (OffensEval)",
      "author" : [ "Marcos Zampieri", "Shervin Malmasi", "Preslav Nakov", "Sara Rosenthal", "Noura Farra", "Ritesh Kumar." ],
      "venue" : "Proceedings of the 13th Interna-",
      "citeRegEx" : "Zampieri et al\\.,? 2019b",
      "shortCiteRegEx" : "Zampieri et al\\.",
      "year" : 2019
    }, {
      "title" : "Understanding bag-of-words model: a statistical framework",
      "author" : [ "Yin Zhang", "Rong Jin", "Zhi-Hua Zhou." ],
      "venue" : "International Journal of Machine Learning and Cybernetics, 1(1-4):43–52.",
      "citeRegEx" : "Zhang et al\\.,? 2010",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2010
    }, {
      "title" : "Aggressive, repetitive, intentional, visible, and imbalanced: Refining representations for cyberbullying classification",
      "author" : [ "Caleb Ziems", "Ymir Vigfusson", "Fred Morstatter." ],
      "venue" : "Proceedings of the International AAAI Conference on Web and Social Media,",
      "citeRegEx" : "Ziems et al\\.,? 2020",
      "shortCiteRegEx" : "Ziems et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "This work is based on the results obtained in the article (Kocoń et al., 2021).",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 31,
      "context" : "In addition, in paper (Milkowski et al., 2021), we showed that the personalized approach is also effective for other subjective problems in NLP, such as",
      "startOffset" : 22,
      "endOffset" : 46
    }, {
      "referenceID" : 27,
      "context" : "It is observable a steady increase in the number of offensive (Levmore and Nussbaum, 2010), hate (Breckheimer, 2001; Brown, 2018), aggressive, toxic, cyberbullying (Chen et al.",
      "startOffset" : 62,
      "endOffset" : 90
    }, {
      "referenceID" : 8,
      "context" : "It is observable a steady increase in the number of offensive (Levmore and Nussbaum, 2010), hate (Breckheimer, 2001; Brown, 2018), aggressive, toxic, cyberbullying (Chen et al.",
      "startOffset" : 97,
      "endOffset" : 129
    }, {
      "referenceID" : 9,
      "context" : "It is observable a steady increase in the number of offensive (Levmore and Nussbaum, 2010), hate (Breckheimer, 2001; Brown, 2018), aggressive, toxic, cyberbullying (Chen et al.",
      "startOffset" : 97,
      "endOffset" : 129
    }, {
      "referenceID" : 10,
      "context" : "It is observable a steady increase in the number of offensive (Levmore and Nussbaum, 2010), hate (Breckheimer, 2001; Brown, 2018), aggressive, toxic, cyberbullying (Chen et al., 2012), or simply socially unacceptable online messages (Ljubešić et al.",
      "startOffset" : 164,
      "endOffset" : 183
    }, {
      "referenceID" : 29,
      "context" : ", 2012), or simply socially unacceptable online messages (Ljubešić et al., 2019).",
      "startOffset" : 57,
      "endOffset" : 80
    }, {
      "referenceID" : 17,
      "context" : "There are many definitions of offensive speech, which can be summarised as speech that targets specific social groups in a way that is harmful to them (Jacobs, 2002).",
      "startOffset" : 151,
      "endOffset" : 165
    }, {
      "referenceID" : 16,
      "context" : "Some countries, such as the USA, protect the rights to use this type of speech as an acceptable form of political expression (Heyman, 2008).",
      "startOffset" : 125,
      "endOffset" : 139
    }, {
      "referenceID" : 41,
      "context" : "law prohibits hate speech in many EU countries (Rosenfeld, 2002).",
      "startOffset" : 47,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "Google are often accused of not doing enough to ensure that their platforms are not used to attack other people (Ben-David and Fernández, 2016).",
      "startOffset" : 112,
      "endOffset" : 143
    }, {
      "referenceID" : 14,
      "context" : "This inconsistency is visible in many reviews related to automatic detection of hate speech (Fortuna and Nunes, 2018; Schmidt and Wiegand, 2017; Alrehili, 2019; Poletto et al., 2020) or more specifically on aggressiveness detection",
      "startOffset" : 92,
      "endOffset" : 182
    }, {
      "referenceID" : 45,
      "context" : "This inconsistency is visible in many reviews related to automatic detection of hate speech (Fortuna and Nunes, 2018; Schmidt and Wiegand, 2017; Alrehili, 2019; Poletto et al., 2020) or more specifically on aggressiveness detection",
      "startOffset" : 92,
      "endOffset" : 182
    }, {
      "referenceID" : 1,
      "context" : "This inconsistency is visible in many reviews related to automatic detection of hate speech (Fortuna and Nunes, 2018; Schmidt and Wiegand, 2017; Alrehili, 2019; Poletto et al., 2020) or more specifically on aggressiveness detection",
      "startOffset" : 92,
      "endOffset" : 182
    }, {
      "referenceID" : 36,
      "context" : "This inconsistency is visible in many reviews related to automatic detection of hate speech (Fortuna and Nunes, 2018; Schmidt and Wiegand, 2017; Alrehili, 2019; Poletto et al., 2020) or more specifically on aggressiveness detection",
      "startOffset" : 92,
      "endOffset" : 182
    }, {
      "referenceID" : 55,
      "context" : "Automatic recognition of offensive speech is the subject of many NLP workshops, such as Semeval 2019 (Zampieri et al., 2019b), GermEval 2018 (Wiegand et al.",
      "startOffset" : 101,
      "endOffset" : 125
    }, {
      "referenceID" : 49,
      "context" : ", 2019b), GermEval 2018 (Wiegand et al., 2018), FIRE/HASOC 2019 (Mandl et al.",
      "startOffset" : 24,
      "endOffset" : 46
    }, {
      "referenceID" : 30,
      "context" : ", 2018), FIRE/HASOC 2019 (Mandl et al., 2019) or PolEval 2019 (Ptaszyński et al.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 56,
      "context" : "the bag-of-words model (Zhang et al., 2010) or TF-IDF (Sahlgren et al.",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 6,
      "context" : "The representation may be extended with additional ontologies (Bloehdorn and Hotho, 2004) or WordNets (Scott and Matwin, 1998; Piasecki et al.",
      "startOffset" : 62,
      "endOffset" : 89
    }, {
      "referenceID" : 46,
      "context" : "The representation may be extended with additional ontologies (Bloehdorn and Hotho, 2004) or WordNets (Scott and Matwin, 1998; Piasecki et al., 2009; Misiaszek et al., 2014; Janz et al., 2017; Kocoń et al., 2019b) and used with SVM (Razavi",
      "startOffset" : 102,
      "endOffset" : 213
    }, {
      "referenceID" : 35,
      "context" : "The representation may be extended with additional ontologies (Bloehdorn and Hotho, 2004) or WordNets (Scott and Matwin, 1998; Piasecki et al., 2009; Misiaszek et al., 2014; Janz et al., 2017; Kocoń et al., 2019b) and used with SVM (Razavi",
      "startOffset" : 102,
      "endOffset" : 213
    }, {
      "referenceID" : 32,
      "context" : "The representation may be extended with additional ontologies (Bloehdorn and Hotho, 2004) or WordNets (Scott and Matwin, 1998; Piasecki et al., 2009; Misiaszek et al., 2014; Janz et al., 2017; Kocoń et al., 2019b) and used with SVM (Razavi",
      "startOffset" : 102,
      "endOffset" : 213
    }, {
      "referenceID" : 18,
      "context" : "The representation may be extended with additional ontologies (Bloehdorn and Hotho, 2004) or WordNets (Scott and Matwin, 1998; Piasecki et al., 2009; Misiaszek et al., 2014; Janz et al., 2017; Kocoń et al., 2019b) and used with SVM (Razavi",
      "startOffset" : 102,
      "endOffset" : 213
    }, {
      "referenceID" : 48,
      "context" : ", 2010) or logistic regression models (Waseem and Hovy, 2016; Sahlgren et al., 2018; Kocoń et al., 2018; Kocoń and Maziarz, 2021).",
      "startOffset" : 38,
      "endOffset" : 129
    }, {
      "referenceID" : 44,
      "context" : ", 2010) or logistic regression models (Waseem and Hovy, 2016; Sahlgren et al., 2018; Kocoń et al., 2018; Kocoń and Maziarz, 2021).",
      "startOffset" : 38,
      "endOffset" : 129
    }, {
      "referenceID" : 21,
      "context" : ", 2010) or logistic regression models (Waseem and Hovy, 2016; Sahlgren et al., 2018; Kocoń et al., 2018; Kocoń and Maziarz, 2021).",
      "startOffset" : 38,
      "endOffset" : 129
    }, {
      "referenceID" : 22,
      "context" : ", 2010) or logistic regression models (Waseem and Hovy, 2016; Sahlgren et al., 2018; Kocoń et al., 2018; Kocoń and Maziarz, 2021).",
      "startOffset" : 38,
      "endOffset" : 129
    }, {
      "referenceID" : 49,
      "context" : "New methods often use word embeddings (Wiegand et al., 2018; Bojanowski et al., 2017; Łukasz Augustyniak et al., 2021) (Wiegand et al.",
      "startOffset" : 38,
      "endOffset" : 118
    }, {
      "referenceID" : 7,
      "context" : "New methods often use word embeddings (Wiegand et al., 2018; Bojanowski et al., 2017; Łukasz Augustyniak et al., 2021) (Wiegand et al.",
      "startOffset" : 38,
      "endOffset" : 118
    }, {
      "referenceID" : 49,
      "context" : ", 2021) (Wiegand et al., 2018; Bojanowski et al., 2017) mixed with character embeddings (Augustyniak et al.",
      "startOffset" : 8,
      "endOffset" : 55
    }, {
      "referenceID" : 7,
      "context" : ", 2021) (Wiegand et al., 2018; Bojanowski et al., 2017) mixed with character embeddings (Augustyniak et al.",
      "startOffset" : 8,
      "endOffset" : 55
    }, {
      "referenceID" : 4,
      "context" : ", 2017) mixed with character embeddings (Augustyniak et al., 2019), together with deep neural networks, e.",
      "startOffset" : 40,
      "endOffset" : 66
    }, {
      "referenceID" : 13,
      "context" : "The current state-of-the-art are Transformer-based architectures such as BERT (Devlin et al., 2019), ALBERT (Lan et al.",
      "startOffset" : 78,
      "endOffset" : 99
    }, {
      "referenceID" : 26,
      "context" : ", 2019), ALBERT (Lan et al., 2019), XLNet (Yang et al.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 52,
      "context" : ", 2019), XLNet (Yang et al., 2019) or RoBERTa (Liu et al.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 57,
      "context" : "as time, thread or author’s social network features (Ziems et al., 2020).",
      "startOffset" : 52,
      "endOffset" : 72
    }, {
      "referenceID" : 33,
      "context" : "In articles focused on detection of aggressiveness (Modha et al., 2018; Risch and Krestel, 2018; Safi Samghabadi et al., 2020), the most often used were datasets shared at the Workshops on Trolling, Aggression and Cyberbullying (TRAC) (Kumar",
      "startOffset" : 51,
      "endOffset" : 126
    }, {
      "referenceID" : 40,
      "context" : "In articles focused on detection of aggressiveness (Modha et al., 2018; Risch and Krestel, 2018; Safi Samghabadi et al., 2020), the most often used were datasets shared at the Workshops on Trolling, Aggression and Cyberbullying (TRAC) (Kumar",
      "startOffset" : 51,
      "endOffset" : 126
    }, {
      "referenceID" : 33,
      "context" : "Moreover the authors focus mainly on the multilingual aspect of the aggression detection (Modha et al., 2018; Risch and Krestel, 2018; Safi Samghabadi et al., 2020).",
      "startOffset" : 89,
      "endOffset" : 164
    }, {
      "referenceID" : 40,
      "context" : "Moreover the authors focus mainly on the multilingual aspect of the aggression detection (Modha et al., 2018; Risch and Krestel, 2018; Safi Samghabadi et al., 2020).",
      "startOffset" : 89,
      "endOffset" : 164
    }, {
      "referenceID" : 33,
      "context" : "complex methods such as logistic regression are also used (Modha et al., 2018; Risch and Krestel, 2018).",
      "startOffset" : 58,
      "endOffset" : 103
    }, {
      "referenceID" : 40,
      "context" : "complex methods such as logistic regression are also used (Modha et al., 2018; Risch and Krestel, 2018).",
      "startOffset" : 58,
      "endOffset" : 103
    }, {
      "referenceID" : 2,
      "context" : "This provides valuable information not only about the annotators, but also about the instances by reflecting their ambiguity (Aroyo and Welty, 2013).",
      "startOffset" : 125,
      "endOffset" : 148
    }, {
      "referenceID" : 0,
      "context" : "5917 The disagreement was used to divide annotators into polarized groups (Akhtar et al., 2020) or to filter out the spammers (Raykar and Yu, 2012; Soberón et al.",
      "startOffset" : 74,
      "endOffset" : 95
    }, {
      "referenceID" : 38,
      "context" : ", 2020) or to filter out the spammers (Raykar and Yu, 2012; Soberón et al., 2013).",
      "startOffset" : 38,
      "endOffset" : 81
    }, {
      "referenceID" : 47,
      "context" : ", 2020) or to filter out the spammers (Raykar and Yu, 2012; Soberón et al., 2013).",
      "startOffset" : 38,
      "endOffset" : 81
    }, {
      "referenceID" : 15,
      "context" : "In (Gao et al., 2019), attention was also drawn to the problem of conformity bias, where the reviewers tend to issue similar opinions.",
      "startOffset" : 3,
      "endOffset" : 21
    }, {
      "referenceID" : 2,
      "context" : "Less frequently, the disagreement is examined at the instance level, to measure its controversy or ambiguity, as in (Aroyo and Welty, 2013).",
      "startOffset" : 116,
      "endOffset" : 139
    }, {
      "referenceID" : 11,
      "context" : "For example, (Chklovski and Mihalcea, 2003) used confusion matrices in word sense tagging task to create and explore coarse sense clusters.",
      "startOffset" : 13,
      "endOffset" : 43
    }, {
      "referenceID" : 31,
      "context" : "lying, abusive or offensive, as well as in prediction of emotions elicited by text (Kocoń et al., 2019a; Milkowski et al., 2021) and even in sentiment analysis (Kocoń et al.",
      "startOffset" : 83,
      "endOffset" : 128
    }, {
      "referenceID" : 23,
      "context" : ", 2021) and even in sentiment analysis (Kocoń et al., 2019; Kanclerz et al., 2020).",
      "startOffset" : 39,
      "endOffset" : 82
    }, {
      "referenceID" : 20,
      "context" : ", 2021) and even in sentiment analysis (Kocoń et al., 2019; Kanclerz et al., 2020).",
      "startOffset" : 39,
      "endOffset" : 82
    }, {
      "referenceID" : 52,
      "context" : "We keep working on testing of our methods on more resource-demanding but also more SOTA language representations: XLNet (Yang et al., 2019),",
      "startOffset" : 120,
      "endOffset" : 139
    }, {
      "referenceID" : 28,
      "context" : "RoBERTa (Liu et al., 2019), and XLM-RoBERTa (Conneau et al.",
      "startOffset" : 8,
      "endOffset" : 26
    } ],
    "year" : 2021,
    "abstractText" : "There is content such as hate speech, offensive, toxic or aggressive documents, which are perceived differently by their consumers. They are commonly identified using classifiers solely based on textual content that generalize pre-agreed meanings of difficult problems. Such models provide the same results for each user, which leads to high misclassification rate observable especially for contentious, aggressive documents. Both document controversy and user nonconformity require new solutions. Therefore, we propose novel personalized approaches that respect individual beliefs expressed by either user conformity-based measures or various embeddings of their previous text annotations. We found that only a few annotations of most controversial documents are enough for all our personalization methods to significantly outperform classic, generalized solutions. The more controversial the content, the greater the gain. The personalized solutions may be used to efficiently filter unwanted aggressive content in the way adjusted to a given person.",
    "creator" : "LaTeX with hyperref"
  }
}