{
  "name" : "2021.acl-long.490.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Unleash GPT-2 Power for Event Detection",
    "authors" : [ "Amir Pouran", "Ben Veyseh", "Viet Dac Lai", "Franck Dernoncourt", "Thien Huu Nguyen" ],
    "emails" : [ "apouranb@cs.uoregon.edu,", "vietl@cs.uoregon.edu,", "thien@cs.uoregon.edu,", "franck.dernoncourt@adobe.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6271–6282\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6271"
    }, {
      "heading" : "1 Introduction",
      "text" : "An important task of Information Extraction (IE) involves Event Detection (ED) whose goal is to recognize and classify words/phrases that evoke events in text (i.e., event triggers). For instance, in the sentence “The organization donated 2 million dollars to humanitarian helps.”, ED systems should recognize “donated” as an event trigger of type Pay. We differentiate two subtasks in ED, i.e., Event Identification (EI): a binary classification problem to predict if a word in text is an event trigger or not, and Event Classification (EC): a multi-class classification problem to classify event triggers according to predefined event types.\nSeveral methods have been introduced for ED,\nextending from feature-based models (Ahn, 2006; Liao and Grishman, 2010a; Miwa et al., 2014) to advanced deep learning methods (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016c; Sha et al., 2018; Zhang et al., 2020b; Nguyen et al., 2021). Although deep learning models have achieved substantial improvement, their requirement of large training datasets together with the small sizes of existing ED datasets constitutes a major hurdle to build high-performing ED models. Recently, there have been some efforts to enlarge training data for ED models by exploiting unsupervised (Huang et al., 2016; Yuan et al., 2018) or distantly-supervised (Keith et al., 2017; Nguyen and Nguyen, 2018; Araki and Mitamura, 2018) techniques. The common strategy in these methods is to exploit unlabeled text data that are rich in event mentions to aid the expansion of training data for ED. In this work, we explore a novel approach for training data expansion in ED by leveraging the existing pre-trained language model GPT-2 (Radford et al., 2019) to automatically generate training data for models. Motivated by the promising performance of GPT models for text generation, we expect our approach to produce effective data for ED in different domains.\nSpecifically, we aim to fine-tune GPT-2 on existing training datasets so it can generate new sentences annotated with event triggers and/or event types, serving as additional training data for ED models. One direction to achieve this idea is to explicitly mark event triggers along with their event types in sentences of an existing ED dataset that can be used to fine-tune the GPT model for new data generation. However, one issue with this direction is that in existing ED datasets, numbers of examples for some rare event types might be small, potentially leading to the poor tuning performance of GPT and impairing the quality of generated examples for such rare events. In addition, large num-\nbers of event types in some ED datasets might make it more challenging for the fine-tuning of GPT to differentiate event types and produce high-quality data. To this end, instead of directly generating data for ED, we propose to use GPT-2 to only generate samples for the event identification task to simplify the generation and achieve data with better annotated labels (i.e., output sentences only are only marked with positions of event triggers). As such, to effectively leverage the generated EI data to improve ED performance, we propose a multitask learning framework to train the ED models on the combination of the generated EI data and the original ED data. In particular, for every event trigger candidate in a sentence, our framework seeks to perform two tasks, i.e., EI to predict a binary label for being an event trigger or not, and ED to predict the event type (if any) evoked by the word via a multi-class classification problem. An input encoder is shared for both tasks that allow training signals from both generated EI data and original ED data to contribute to the representation learning in the encoder (i.e., transferring knowledge in generated EI data to ED models).\nDespite the simplification to EI for better annotated labels of data, the generated sentences might still involve noises due to the inherent nature of the language generation, e.g., grammatically wrong sentences, inconsistent information, or incorrect event trigger annotations. As such, it is crucial to introduce mechanisms to filter the noises in generated data to enable effective transfer learning from generated EI data. To this end, prior works for GPTbased data generation for other tasks has attempted to directly remove noisy generated examples before actual usage for model training via some heuristic rules (Anaby-Tavor et al., 2020; Yang et al., 2020). However, heuristic rules are brittle and restricted in their coverage so they might overly filter the generated data or incorrectly retain some noisy generated samples. To address this issue, we propose to preserve all generated data for training and devise methods to explicitly limit impacts of noisy generated sentences in the models. In particular, we expect the inclusion of generated EI data into the training process for ED models might help to shift the representations of the models to better regions for ED. As such, we argue that this representation transition should only occur at a reasonable rate as drastic divergence of representations due to the generated data might be associated with\nnoises in the data. Motivated by this intuition, we propose a novel teacher-student framework for our multi-task learning problem where the teacher is trained on the original clean ED datasets to induce anchor representation knowledge for data. The student, on the other hand, will be trained on both generated EI data and original ED data to accomplish transfer learning. Here, the anchor knowledge from the teacher will be leveraged to guide the student to prevent drastic divergence of representation vectors for noisy information penalization. Consequently, we propose a novel anchor information to implement this idea, seeking to maintain the same level of differences between the generated and original data (in terms of representation vectors) for both the teacher and the student (i.e., generated-vsoriginal data difference as the anchor). At the core of this techniques involves the computation of distance/difference between samples in generated and original data. In this work, we envision two types of information that models should consider when computing such distances for our problem: (1) representation vectors of the models for the examples, and (2) event trigger likelihood scores of examples based on the models (i.e., two examples in the generated and original data are more similar if they both correspond to event triggers). As such, we propose to cast this distance computation problem of generated and original data into an Optimal Transport (OT) problem. OT is an established method to compute the optimal transportation between two data distributions based on the probability masses of data points and their pair-wise distances, thus facilitating the integration of the two criteria of event trigger likelihoods and representation vectors into the distance computation between data point sets.\nExtensive experiments and analysis reveal the effectiveness of the proposed approach for ED in different domains, establishing new state-of-theart performance on the ACE 2005, CySecED and RAMS datasets."
    }, {
      "heading" : "2 Model",
      "text" : "We formulate the task of Event Detection as a word-level classification problem as in prior work (Nguyen and Grishman, 2015; Ngo et al., 2020). Formally, given the sentence S = [w1, w2, . . . , wn] and the candidate trigger wordwt, the goal is to predict the event type l from a pre-defined set of event types L. Note that if the word wt is not a trigger word, the gold event type is None. Our proposed\napproach for this task consist of two stages: (1) Data Augmentation: to employ natural language generation to augment existing training datasets for ED, (2) Task Modeling: to propose a deep learning model for ED, exploiting available training data."
    }, {
      "heading" : "2.1 Data Augmentation",
      "text" : "As presented in the introduction, our motivation in this work is to explore a novel approach for training data augmentation for ED based on the powerful pre-trained language model for text generation GPT2. Our overall strategy involves using some existing training dataset O for ED (i.e., original data) to fine-tune GPT-2. The fine-tuned model is then employed to generate a new labeled training set G (i.e., synthetic data) that will be combined with the original data O to train models for ED.\nTo simplify the training data generation task and enhance the quality of the synthetic data, we seek to generate data only for the subtask EI of ED where synthesized sentences are annotated with positions of their event triggers (i.e., event types for triggers are not required for the generation to avoid the complication with rare event types for fine-tuning). To this end, we first enrich each sentence S ∈ O with positions of event triggers that it contains to facilitate the GPT fine-tuning process. Formally, assume that S = w1, w2, . . . , wn is a sentence of n words with only one event trigger word located at wt, the enriched sentence S′ for S would have the form: S′ = [BOS,w1, . . . , TRGs, wt, TRGe, . . . , wn, EOS] where TRGs and TRGe are special tokens to mark the position of the event trigger, and BOS and EOS are special tokens to identify the beginning and the end of the sentence. Next, the GPT-2 model will be fine-tuned on the enriched sentences S′ of O in an auto-regressive fashion (i.e., predicting the next token in S′ given prior ones). Finally, using the fine-tuned GPT-2, we generate a new dataset G of |O| sentences (|G| = |O|) to achieve a balanced size. Here, we ensure that only generated sentences that contain the special tokens TRGs and TRGe (i.e., involving event trigger words) are added into G, allowing us to identify the candidate trigger word in our word-level classification formulation for ED. As such, the combination A of the synthetic data G and the original data O (A = O ∪ G) will be leveraged to train our ED model in the next step.\nTo assess the quality of the synthetic data, we randomly select 200 sentences from G (generated\nby the fine-tuned GPT-2 model over the popular ACE 2005 training set for ED) and evaluate them regarding grammatical soundness, meaningfulness, and inclusion and correctness of annotated event triggers (i.e., whether the words between the tokens TRGs and TRGe evoke events or not). Among the sampled set, we find that 17% of the sentences contains at least one type of such errors."
    }, {
      "heading" : "2.2 Task Modeling",
      "text" : "This section describes our model for ED to overcome the noises in the generated data G for model training. As discussed in the introduction, we employ the Teacher-Student framework with multitask learning to achieve this goal. In the proposed framework, the teacher and student employs a base deep learning model with the same architecture and different parameters. Base Model: Following the prior work (Wang et al., 2019), our base model consists of the BERTbase model to represent each word wi in the input sentence S with a vector ei. Formally, the input sentence [[CLS], w1, w2, . . . , wn, [SEP ]] is fed into the BERTbase model and the hidden states of the last layer of BERT are taken as the contextualized embeddings of the input words, i.e., E = [e1, e2, . . . , en]. Note that ifwi contains more than one word-piece, the average of its word-piece embeddings is used for ei. In our experiments, we find that fixing the BERTbase parameters achieve higher performance. As such, to fine-tune the contextualized embeddings E for ED, we employ a Bi-directional Long Short-Term Memory (BiLSTM) network to consumes E; its hidden states, i.e., H = [h1, h2, . . . , hn], are then employed as the final representations for the words in S. Finally, to create the final vector V for ED prediction, the max-pooled representation of the sentence, i.e., h̄ = MAX POOL(h1, h2, . . . , hn), is concatenated with the representation of the trigger candidate, i.e., ht. V is consumed by a feed-forward network, whose last layer has |L| neurons, followed by a softmax layer to predict the distribution P (·|S, t) over possible event types in L. To train the model, we use negative log-likelihood as the loss function: Lpred = − logP (l|S, t) where l is the gold label.\nAs the synthetic sentences in G only involve information about positions of event triggers (i.e., no event types included), we cannot directly combine G with O to train ED models with the loss Lpred. To facilitate the integration of G into the training\nprocess, we introduce an auxiliary task of EI for the multi-task learning in the training process, seeking to predict the binary label laux for the trigger candidate wt in S, i.e., laux = 1 if wt is an event trigger. To perform this auxiliary task, we employ another feed-forward network, i.e., FFaux, which also consumes the overall vector V as input. This feedforward network has one neuron with the sigmoid activation function in the last layer to estimate the event trigger likelihood score: P (laux = 1|S, t) = FFaux(V ). Finally, to train the base model with the auxiliary task, we exploit the binary crossentropy loss: Laux = −(laux log(FFaux(V )) + (1− laux) log(1−FFaux(V ))). Note that the main ED task and the auxiliary EI task are done jointly in a single training process where the loss Lpred for ED is computed only for the original data O. The loss Laux, in contrast, will be obtained for both original and synthetic data in A.\nKnowledge Consistency: The generated data G is not noise-free. As such, training the ED model on A could lead to inferior performance. To address this issue, as discussed in the introduction, we propose to first learn the anchor knowledge from the original data O, then use that to lead the model training on A to prevent drastic divergence from the anchor knowledge (i.e., knowledge consistency promotion), thus constraining the noises. Hence, we propose a teacher-student network, in which the teacher is first trained on O to learn the anchor knowledge. The student network will be trained on A afterward leveraging the consistency guidance with the induced anchor knowledge from the teacher. We will also use the student network as the final model for our ED problem in this work.\nIn our framework, both teacher and student networks will be trained in the multi-task setting with ED and EI tasks. In particular, the training losses for both ED and EI will be computed based on O for the teacher (the loss to train the teacher is: Lpred + τLaux where τ is a trade-off parameter). In contrast, the combined data A will be used to compute the EI loss for the student while the ED loss for the student can only be computed on the original data O. As such, we propose to enforce the knowledge consistency between the two networks for both the main task ED and the auxiliary task EI during the training of the student model. First, to achieve the knowledge consistency for ED, we seek to minimize the KL divergence between the teacher-predicted label-probability distri-\nbution and the student-predicted label-probability distributions. Formally, for a sentence S ∈ O, the label-probability distributions of the teacher and the student, i.e., Pt(·|S, t) and Ps(·|S, t) respectively, are employed to compute the KL-divergence loss LKL = −Σl∈LPt(l|S, t) log(Pt(l|S,t)Ps(l|S,t)). By decreasing the KL-divergence during the student’s training, the model is encouraged to make similar predictions as the teacher for the same original sentence, thereby preventing noises to mislead the student. Note that different from traditional teacherstudent networks that employ KL to achieve knowledge distillation on unlabelled data (Hinton et al., 2015), the KL divergence in our model is leveraged to enforce knowledge consistency to prevent noises in labeled data automatically generated by GPT-2.\nSecond, for the auxiliary task EI, instead of enforcing the student-teacher knowledge consistency via similarity predictions, we argue that it will be more beneficial to leverage the difference between the original data O and the generated data G as an anchor knowledge to promote consistency. In particular, we expect that the student which is trained on A, should discern the same difference between G and O as the teacher which is trained only on the original data O. Formally, during student training, for each mini-batch, the distances between the original data and the generated data detected by the teacher and the student are denoted by dTO,G and dSO,G , respectively. To enforce the O-G distance consistency between the two networks, the following loss is added into the overall loss function: Ldist = |dTO,G−d S O,G |\n|B| , where |B| is the mini-batch size. The advantage of this novel knowledge consistency enforcement compared to the KL-divergence is that it explicitly exploits the different nature of the original and generated data to facilitate the mitigation of noises in the generated data.\nA remaining question for our proposed knowledge consistency concerns how to assess the difference between the original and the generated data from the perspective of the teacher, i.e., dTO,G , and the student networks, i.e., dSO,G . In this section, we will describe our method from the perspective of the student (the same method is employed for the teacher network). In particular, we define the difference between the original and the generated data as the cost of transforming O to G such that for the transformed data the model will make the same predictions as G. How can we compute the cost of such transformation? To answer this ques-\ntion, we propose to employ Optimal Transport (OT) which is an established method to find the efficient transportation (i.e., transformation with the lowest cost) of one probability distribution to another one. Formally, given the probability distributions p(x) and q(y) over the domains X and Y , and the cost function C(x, y) : X ×Y → R+ for mapping X to Y , OT finds the optimal joint distribution π∗(x, y) (over X × Y) with marginals p(x) and q(y), i.e., the cheapest transportation from p(x) to q(y), by solving the following problem:\nπ∗(x, y) = min π∈Π(x,y) ∫ Y ∫ X π(x, y)C(x, y)dxdy\ns.t. x ∼ p(x) and y ∼ q(y), (1)\nwhere Π(x, y) is the set of all joint distributions with marginals p(x) and q(y). Note that if the distributions p(x) and q(y) are discrete, the integrals in Equation 1 are replaced with a sum and the joint distribution π∗(x, y) is represented by a matrix whose entry (x, y) represents the probability of transforming the data point x ∈ X to y ∈ Y to convert the distribution p(x) to q(y). By solving the problem in Equation 11, the cost of transforming the discrete distribution p(x) to q(y) (i.e., Wasserstein distance DistW ) is defined as: DistW = Σx∈XΣy∈Yπ\n∗(x, y)C(x, y). In order to utilize OT to compute the transformation cost between O and G, i.e., dSO,G , we propose to define the domain X and Y as the representation spaces of the sentences in O and G, respectively, obtained from the student network. In particular, a data point x ∈ X represents a sentence Xo ∈ O. Similarly, a data point y ∈ Y stands for a sentence Yg ∈ G. To define the cost function C(x, y) for OT, we compute the Euclidean distance between the representation vectors of the sentences Xo and Yg (obtained by max-pooling over representations of their words): C(x, y) =\n∥∥h̄Xo − h̄Yg ∥∥ where h̄Xo = MAX POOL(h X o,1, . . . , h X o,|Xo|), h̄Yg = MAX POOL(h Y g,1, . . . , h Y g,|Yg |), and h X o,i and hYg,i are the representation vectors of the ith words of Xo and Yg, respectively, obtained from the student’s BiLSTM. Also, to define the discrete distribution p(x) for OT over X , we employ the event trigger likelihood ScoreXo for the trigger candidate of each sentence Xo in X that is returned by the feed-forward network FFSaux\n1It is worth mentioning that this problem is intractable so we solve its entropy-based approximation using the Sinkhorn algorithm (Peyre and Cuturi, 2019).\nfor the auxiliary task EI in the student model, i.e, ScoreXo = FF S aux(Xo). Afterward, we apply the softmax function over the scores of the original sentences in the current mini-batch to obtain p(x), i.e., p(x) = Softmax(ScoreXo ). Similarly, the discrete distribution q(y) is defined as q(y) = Softmax(ScoreYg ). To this end, by solving the OT problem in Equation 1 and obtaining the efficient transport plan π∗(x, y) using this setup, we can obtain the distance dSO,G . In the same way, the distance dTO,G can be computed using the representations and event trigger likelihoods from the teacher network. Note that in this way, we can integrate both representation vectors of sentences and event trigger likelihoods into the distance computation between data as motivated in the introduction.\nFinally, to train the student model, the following combined loss function is used in our framework: L = Lpred + αLaux + βLKL + γLdist, where α, β, and γ are the trade-off parameters."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Datasets, Baselines & Hyper-Parameters",
      "text" : "To evaluate the effectiveness of the proposed model, called the GPT-based data augmentation model for ED with OT (GPTEDOT), we conduct experiments on the following ED datasets:\nACE 2005 (Walker et al., 2006): This dataset annotates 599 documents for 33 event types that cover different text domains(e.g., news, weblog or conversation documents). We use the same preprocessing script and data split as prior works (Lai et al., 2020c; Tong et al., 2020b) to achieve fair comparisons. In particular, the data split involves 529/30/40 articles for train/dev/test sets respectively. For this dataset, we compare our model with prior state-of-the-art models reported in the recent works (Lai et al., 2020c; Tong et al., 2020b), including BERT-based models such as DMBERT, AD-DMBERT (Wang et al., 2019), DRMM, EKD (Tong et al., 2020b), and GatedGCN (Lai et al., 2020c).\nCySecED (Man Duc Trong et al., 2020): This dataset provides 8,014 event triggers for 30 event types from 300 articles of the cybersecurity domain (i.e., cybersecurity events). We follow the the same pre-processing and data split as the original work (Man Duc Trong et al., 2020) with 240/30/30 documents for the train/dev/test sets. To be consistent with other experiments and facilitate the data generation based on GPT-2, the experiments on Cy-\nSecED are conducted at the sentence level where inputs for models involve sentences. As such, we employ the state-of-the-art sentence-level models reported in (Man Duc Trong et al., 2020), i.e., DMBERT (Wang et al., 2019), BERT-ED (Yang et al., 2019), as the baselines for CySecED.\nRAMS (Ebner et al., 2020): This dataset annotates 9,124 event triggers for 38 event types. We use the official data split with 3,194, 399, and 400 documents for training, development, and testing respectively for RAMS. We also perform ED at the sentence level in this dataset. For the baselines, we utilize recent state-of-the-art BERT-based models for ED, i.e., DMBERT (Wang et al., 2019) and GatedGCN (Lai et al., 2020c). For a fair comparison, the performance of such baseline models is obtained via their official implementations from the original papers that are fine-tuned for RAMS.\nFor each dataset, we use its training and development data to fine-tune the GPT-2 model. We tune the hyperparameters for the proposed teacherstudent architecture using a random search. All the hyperparameters are selected based on the F1 scores on the development set of the ACE 2005 dataset. The same hyper-parameters from this finetuning are then applied for other datasets for consistency. In our model we use the small version of GPT-2 to generate data. In the base model, we use BERTbase, 300 dimensions in the hidden states of BiLSTM and 2 layers of feed-forward neural networks with 200 hidden dimensions to predict events. The trade-off parameters τ , α, β and γ are set to 0.1, 0.1, 0.05, and 0.08, respectively. The learning rate is set to 0.3 for the Adam optimizer and the batch size of 50 are employed during training. Finally, note that we do not update the BERT model for word embeddings in this work due to its better performance on the development data of ACE 2005."
    }, {
      "heading" : "3.2 Results",
      "text" : "Results of experiments on the ACE 2005 test set are shown in Table 1. The most important observation is that the proposed model GPTEDOT significantly outperforms all the baseline models (p < 0.01), thus showing the benefits of GPT-generated data and the teacher-student framework with knowledge consistency for ED in this work. In particular, compared to the BERT-based models that leverage data augmentation, i.e., AD-DMBERT (Wang et al., 2019) with semi-supervised and adversarial\nlearning, DRMM (Tong et al., 2020a) with imageenhanced models, and EKD (Tong et al., 2020b) with external open-domain event triggers, the better performance of GPTEDOT highlights the advantages of GPT-2 to generate data for ED models.\nResults of experiments on the CySecED test set are presented in Table 2. This table reveals that the teacher-student architecture GPTEDOT significantly improves the performance over previous state-of-the-art models for ED in cybersecurity domain. This is important as it shows that the proposed model is effective in different domains. In addition, our results also suggest that GPT-2 can be employed to generate effective data for ED in domains where data annotation for ED requires extensive domain expertise and expensive cost to obtain such as the cybersecurity events. Moreover, the higher margin of improvement for GPTEDOT on CySecED compared to the those on the ACE\n2005 dataset suggests the necessity of using more training data for ED in technical domains.\nFinally, results of experiments on the RAMS test set are reported in Table 3. Consistent with our experiments on ACE 2005 and CySecED, our proposed model achieve significantly higher performance than existing state-of-the-art models (p < 0.01), thus further confirming the advantages of GPTEDOT for ED."
    }, {
      "heading" : "3.3 Ablation Study",
      "text" : "This ablation study evaluates the effectiveness of different components in GPTEDOT for ED. First, for the importance of the generated data G from GPT-2 and the teacher-student architecture to mitigate noises, we examine the following baselines: (1) BaseO: The baseline is the base model trained only on the original dataO, thus being equivalent to the teacher model and not using the student model; and (2) BaseA: This baseline trains the base model on the combination of the original and generated data, i.e., A, using the multi-learning setting (i.e., the teacher model is excluded).\nSecond, for the multi-task learning design in the teacher network, we explore the following ablated models: (3) Teacher−A: This baseline removes the auxiliary task EI in the teacher from GPTEDOT. As such, the OT-based knowledge consistency for EI is also eliminated; (4) Teacher−M : In this model, the main task ED is utilize to train the teacher, so the corresponding KL-based knowledge consistency for ED is also removed.\nThird, for the design of the knowledge consistency losses in the student network, we evaluate the following baselines: (5) Student−OT : This ablated model eliminates the OT-based knowledge consistency loss for the auxiliary task EI in the student’s training of GPTEDOT (the auxiliary task is still employed for the teacher and the student); (6) Student−KL: For this model, the KL-based knowledge consistency for the main task ED is ignored in the student’s training; (7) Student+OT : In this baseline, we use OT for the knowledge consistency on both the main and the auxiliary tasks. Here, for the main task ED, the cost function C(x, y) for OT is still obtained via the Euclidean distances between representation vectors while the distributions p(x) and p(y) are based on the maximum probabilities of the label-probability distributions Ps(.|Xo, to) and Ps(Yg, tg) for the ED task; and (8) Student+KL: This baseline employs the KL di-\nvergence between models’ predicted distributions to enforce the teacher-student consistency for both the main task and the auxiliary task. To this end, for the auxiliary task EI, we convert the final activation of FFaux into a distribution with two data points (i.e., [FFaux(X), 1 − FFaux(X)]) to compute the KL divergence between the teacher and the student.\nFinally, for the importance of Euclidean distances and event trigger likelihoods in the OTbased distance between O and G for knowledge consistency in EI, we investigate two baselines: (9) OT−Rep: Here, to compute OT, we use constant cost between every pair of sentences, i.e., C(x, y) = 1 (i.e., ignoring representation-based distances); and (10) OT−Score: This model uses uniform distributions for p(x) and q(y) to compute the OT (i.e., ignoring event trigger likelihoods).\nWe report the performance of the models (on the ACE 2005 development set) for the ablation study in Table 4. There are several observations from this table. First, the generated data G and the teacher-student architecture are necessary for GPTEDOT to achieve the highest performance. In particular, comparing with BaseO, the better performance of GPTEDOT indicates the benefits of the GPT-generated data. Moreover, the better performance of BaseO over BaseA reveals that the simple combination of the synthetic and original data without any effective method to mitigate noises might be harmful. Second, the lower performance of Teacher−A and Teacher−M shows that both the auxiliary and the main task (i.e., multi-task learning) in the teacher are integral to produce the best performance. Third, the choice of methods to promote knowledge consistency is important and the proposed combination of KL and OT for the ED and EI tasks (respectively) are necessary. In particular, removing or replacing each of them with the other one (i.e., Student+OT and Student+KL) would de-\ncrease the performance significantly. Finally, in the proposed consistency method based on OT for EI, it is beneficial to employ both representation-level distances (i.e., OT−Rep) and models’ predictions for event trigger likelihoods (i.e., OT−Score) as removing any of them hurts the performance."
    }, {
      "heading" : "3.4 Analysis",
      "text" : "To provide more insights into the quality of the synthetic data G, we provide samples of sentences that are generated by the fine-tuned GPT-2 model on each dataset in Table 5. This table illustrates that the generated sentences also belong to the domains of the original data (i.e., the cybersecurity domain). As such, combining synthetic data with original data is promising for improving ED performance as demonstrated in our experiments.\nAs discussed earlier, the generated data G is not free of noise. In order to better understand the types of errors existing in generated sentences, we manually assess 200 sentences randomly selected from the set G generated by the fine-tuned GPT-2 model on the ACE 2005 dataset. We categorize the errors into five types and provide their proportions along with example for each error type in Table 6. This table shows that the majority of errors are due to missing labels (i.e., no special tokens TRGs and TRGe are generated) or incorrect labels (i.e., marked words are not event triggers of interested\ntypes) generated by the language model. Finally, to study the importance of the size of the generated data to augment training set for ED, we conduct an experiment in which different numbers of generated samples in G (for the ACE 2005 dataset) are combined with the original data O. The results are shown in Table 7. According to this table, the highest performance of the proposed model is achieved when the numbers of the generated and original data are equal. More specifically, decreasing the number of generated samples potentially limits the benefits of data augmentation. On the other hand, increasing the size of generated data might introduces extensive noises and become harmful to the ED models."
    }, {
      "heading" : "4 Related Work",
      "text" : "Early methods for ED have employed featurebased techniques (Ahn, 2006; Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao and Grishman, 2010a,b; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013; Miwa et al., 2014; Yang and Mitchell, 2016). Later, advanced deep learning methods (Nguyen and Grishman, 2015; Chen et al., 2015; Nguyen et al., 2016a,b; Sha et al., 2018; Zhang et al., 2019; Yang et al., 2019; Nguyen and Nguyen, 2019; Zhang et al., 2020b) have been applied for ED. One challenge for ED research is the limited size of existing datasets that hinder the training of effective models. Prior works have attempted to address this issue via unsupervised (Huang et al., 2016; Yuan et al., 2018), semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012; Ferguson et al., 2018), distantly supervised (Keith et al., 2017; Nguyen and Nguyen, 2018; Zeng et al., 2017; Araki and Mitamura, 2018), and few/zeroshot (Huang et al., 2018; Lai et al., 2020a,b) learn-\ning. In this work, we propose a novel method to augment training data for ED by exploiting the powerful language model GPT-2 to automatically generate new samples.\nLeveraging GPT-2 for augmenting training data has also been studied for other NLP tasks recently (e.g., relation extraction, commonsense reasoning) (Papanikolaou and Pierleoni, 2020; Zhang et al., 2020a; Yang et al., 2020; Madaan et al., 2020; Bosselut et al., 2019; Kumar et al., 2020; AnabyTavor et al., 2020; Peng et al., 2020). However, none of those works has explored GPT-2 for ED. In addition, existing methods only resort to heuristics to filter out noisy samples generated by GPT-2. In contrast, we propose a novel differentiable method capable of preventing noises from diverging representation vectors of the models for ED."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We propose a novel method for augmenting training data for ED using the samples generated by the language model GPT-2. To avoid noises in the generated data, we propose a novel teacher-student architecture in a multi-task learning framework. We introduce a mechanism for knowledge consistency enforcement to mitigate noises from generated data based on optimal transport. Experiments on various ED benchmark datasets demonstrate the effectiveness of the proposed method."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research has been supported by the Army Research Office (ARO) grant W911NF-21-1-0112 and the NSF grant CNS-1747798 to the IUCRC Center for Big Learning. This research is also based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA Contract No. 2019- 19051600006 under the Better Extraction from Text Towards Enhanced Retrieval (BETTER) Program. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ARO, ODNI, IARPA, the Department of Defense, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. This document does not contain technology or technical data controlled under either the\nU.S. International Traffic in Arms Regulations or the U.S. Export Administration Regulations."
    } ],
    "references" : [ {
      "title" : "The stages of event extraction",
      "author" : [ "David Ahn." ],
      "venue" : "Proceedings of the Workshop on Annotating and Reasoning about Time and Events.",
      "citeRegEx" : "Ahn.,? 2006",
      "shortCiteRegEx" : "Ahn.",
      "year" : 2006
    }, {
      "title" : "Do not have enough data? deep learning to the rescue! In Proceedings of the Association for the Advancement",
      "author" : [ "Ateret Anaby-Tavor", "Boaz Carmeli", "Esther Goldbraich", "Amir Kantor", "George Kour", "Segev Shlomov", "Naama Tepper", "Naama Zwerdling" ],
      "venue" : null,
      "citeRegEx" : "Anaby.Tavor et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Anaby.Tavor et al\\.",
      "year" : 2020
    }, {
      "title" : "Open-domain event detection using distant supervision",
      "author" : [ "Jun Araki", "Teruko Mitamura." ],
      "venue" : "Proceedings of the International Conference on Computational Linguistics (COLING).",
      "citeRegEx" : "Araki and Mitamura.,? 2018",
      "shortCiteRegEx" : "Araki and Mitamura.",
      "year" : 2018
    }, {
      "title" : "Comet: Commonsense transformers for automatic knowledge graph construction",
      "author" : [ "Antoine Bosselut", "Hannah Rashkin", "Maarten Sap", "Chaitanya Malaviya", "Asli Celikyilmaz", "Yejin Choi." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for",
      "citeRegEx" : "Bosselut et al\\.,? 2019",
      "shortCiteRegEx" : "Bosselut et al\\.",
      "year" : 2019
    }, {
      "title" : "Event extraction via dynamic multipooling convolutional neural networks",
      "author" : [ "Yubo Chen", "Liheng Xu", "Kang Liu", "Daojian Zeng", "Jun Zhao." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Exploiting document level information to improve event detection via recurrent neural networks",
      "author" : [ "Shaoyang Duan", "Ruifang He", "Wenli Zhao." ],
      "venue" : "Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP).",
      "citeRegEx" : "Duan et al\\.,? 2017",
      "shortCiteRegEx" : "Duan et al\\.",
      "year" : 2017
    }, {
      "title" : "Multi-sentence argument linking",
      "author" : [ "Seth Ebner", "Patrick Xia", "Ryan Culkin", "Kyle Rawlins", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Ebner et al\\.,? 2020",
      "shortCiteRegEx" : "Ebner et al\\.",
      "year" : 2020
    }, {
      "title" : "Semi-supervised event extraction with paraphrase clusters",
      "author" : [ "James Ferguson", "Colin Lockard", "Daniel S Weld", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Ferguson et al\\.,? 2018",
      "shortCiteRegEx" : "Ferguson et al\\.",
      "year" : 2018
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean." ],
      "venue" : "Proceedings of the Deep Learning Workshop at NeurIPS.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Using cross-entity inference to improve event extraction",
      "author" : [ "Yu Hong", "Jianfeng Zhang", "Bin Ma", "Jianmin Yao", "Guodong Zhou", "Qiaoming Zhu." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Hong et al\\.,? 2011",
      "shortCiteRegEx" : "Hong et al\\.",
      "year" : 2011
    }, {
      "title" : "Liberal event extraction and event schema induction",
      "author" : [ "Lifu Huang", "Taylor Cassidy", "Xiaocheng Feng", "Heng Ji", "Clare Voss", "Jiawei Han", "Avirup Sil." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Huang et al\\.,? 2016",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Zero-shot transfer learning for event extraction",
      "author" : [ "Lifu Huang", "Heng Ji", "Kyunghyun Cho", "Clare R. Voss." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Huang et al\\.,? 2018",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2018
    }, {
      "title" : "Bootstrapped training of event extraction classifiers",
      "author" : [ "Ruihong Huang", "Ellen Riloff." ],
      "venue" : "Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics (EACL).",
      "citeRegEx" : "Huang and Riloff.,? 2012",
      "shortCiteRegEx" : "Huang and Riloff.",
      "year" : 2012
    }, {
      "title" : "Refining event extraction through cross-document inference",
      "author" : [ "Heng Ji", "Ralph Grishman." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Ji and Grishman.,? 2008",
      "shortCiteRegEx" : "Ji and Grishman.",
      "year" : 2008
    }, {
      "title" : "Identifying civilians killed by police with distantly supervised entity-event extraction",
      "author" : [ "Katherine Keith", "Abram Handler", "Michael Pinkham", "Cara Magliozzi", "Joshua McDuffie", "Brendan O’Connor" ],
      "venue" : "In Proceedings of the Conference on Empirical",
      "citeRegEx" : "Keith et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Keith et al\\.",
      "year" : 2017
    }, {
      "title" : "Data augmentation using pre-trained transformer models",
      "author" : [ "Varun Kumar", "Ashutosh Choudhary", "Eunah Cho." ],
      "venue" : "arXiv preprint arXiv:2003.02245.",
      "citeRegEx" : "Kumar et al\\.,? 2020",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploiting the matching information in the support set for few shot event classification",
      "author" : [ "Viet Dac Lai", "Franck Dernoncourt", "Thien Huu Nguyen." ],
      "venue" : "Proceedings of the 24th Pacific-Asia Conference on Knowledge Discovery and Data Mining",
      "citeRegEx" : "Lai et al\\.,? 2020a",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2020
    }, {
      "title" : "Extensively matching for few-shot learning event detection",
      "author" : [ "Viet Dac Lai", "Franck Dernoncourt", "Thien Huu Nguyen." ],
      "venue" : "Proceedings of the 1st Joint Workshop on Narrative Understanding, Storylines, and Events (NUSE) at ACL 2020.",
      "citeRegEx" : "Lai et al\\.,? 2020b",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2020
    }, {
      "title" : "Event detection: Gate diversity and syntactic importance scores for graph convolution neural networks",
      "author" : [ "Viet Dac Lai", "Tuan Ngo Nguyen", "Thien Huu Nguyen." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Lai et al\\.,? 2020c",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2020
    }, {
      "title" : "Joint event extraction via structured prediction with global features",
      "author" : [ "Qi Li", "Heng Ji", "Liang Huang." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Li et al\\.,? 2013",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2013
    }, {
      "title" : "Filtered ranking for bootstrapping in event extraction",
      "author" : [ "Shasha Liao", "Ralph Grishman." ],
      "venue" : "Proceedings of the International Conference on Computational Linguistics (COLING).",
      "citeRegEx" : "Liao and Grishman.,? 2010a",
      "shortCiteRegEx" : "Liao and Grishman.",
      "year" : 2010
    }, {
      "title" : "Using document level cross-event inference to improve event extraction",
      "author" : [ "Shasha Liao", "Ralph Grishman." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Liao and Grishman.,? 2010b",
      "shortCiteRegEx" : "Liao and Grishman.",
      "year" : 2010
    }, {
      "title" : "Exploiting the ground-truth: An adversarial imitation based knowledge distillation approach for event detection",
      "author" : [ "Jian Liu", "Yubo Chen", "Kang Liu." ],
      "venue" : "Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI).",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Event detection via gated multilingual attention mechanism",
      "author" : [ "Jian Liu", "Yubo Chen", "Kang Liu", "Jun Zhao." ],
      "venue" : "Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI).",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploiting argument information to improve event detection via supervised attention mechanisms",
      "author" : [ "Shulin Liu", "Yubo Chen", "Kang Liu", "Jun Zhao." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Liu et al\\.,? 2017",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2017
    }, {
      "title" : "Distilling discrimination and generalization knowledge for event detection via deltarepresentation learning",
      "author" : [ "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Le Sun." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational",
      "citeRegEx" : "Lu et al\\.,? 2019",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2019
    }, {
      "title" : "Eigen: Event influence generation using pre-trained language models",
      "author" : [ "Aman Madaan", "Dheeraj Rajagopal", "Yiming Yang", "Abhilasha Ravichander", "Eduard Hovy", "Shrimai Prabhumoye." ],
      "venue" : "arXiv preprint arXiv:2010.11764.",
      "citeRegEx" : "Madaan et al\\.,? 2020",
      "shortCiteRegEx" : "Madaan et al\\.",
      "year" : 2020
    }, {
      "title" : "Introducing a new dataset for event detection in cybersecurity texts",
      "author" : [ "Hieu Man Duc Trong", "Duc Trong Le", "Amir Pouran Ben Veyseh", "Thuat Nguyen", "Thien Huu Nguyen." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Trong et al\\.,? 2020",
      "shortCiteRegEx" : "Trong et al\\.",
      "year" : 2020
    }, {
      "title" : "Event extraction as dependency parsing",
      "author" : [ "David McClosky", "Mihai Surdeanu", "Christopher Manning." ],
      "venue" : "BioNLP Shared Task Workshop.",
      "citeRegEx" : "McClosky et al\\.,? 2011",
      "shortCiteRegEx" : "McClosky et al\\.",
      "year" : 2011
    }, {
      "title" : "Comparable study of event extraction in newswire and biomedical domains",
      "author" : [ "Makoto Miwa", "Paul Thompson", "Ioannis Korkontzelos", "Sophia Ananiadou." ],
      "venue" : "Proceedings of the International Conference on Computational Linguistics (COLING).",
      "citeRegEx" : "Miwa et al\\.,? 2014",
      "shortCiteRegEx" : "Miwa et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning to select important context words for event detection",
      "author" : [ "Nghia Ngo", "Tuan Ngo Nguyen", "Thien Huu Nguyen." ],
      "venue" : "Proceedings of the 24th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD).",
      "citeRegEx" : "Ngo et al\\.,? 2020",
      "shortCiteRegEx" : "Ngo et al\\.",
      "year" : 2020
    }, {
      "title" : "Cross-task instance representation interactions and label dependencies for joint information extraction with graph convolutional networks",
      "author" : [ "Minh Van Nguyen", "Viet Dac Lai", "Thien Huu Nguyen." ],
      "venue" : "Proceedings of the Conference of the",
      "citeRegEx" : "Nguyen et al\\.,? 2021",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2021
    }, {
      "title" : "Who is killed by police: Introducing supervised attention for hierarchical lstms",
      "author" : [ "Minh Van Nguyen", "Thien Huu Nguyen." ],
      "venue" : "Proceedings of the International Conference on Computational Linguistics (COLING).",
      "citeRegEx" : "Nguyen and Nguyen.,? 2018",
      "shortCiteRegEx" : "Nguyen and Nguyen.",
      "year" : 2018
    }, {
      "title" : "Joint event extraction via recurrent neural networks",
      "author" : [ "Thien Huu Nguyen", "Kyunghyun Cho", "Ralph Grishman." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Nguyen et al\\.,? 2016a",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "A two-stage approach for extending event detection to new types via neural networks",
      "author" : [ "Thien Huu Nguyen", "Lisheng Fu", "Kyunghyun Cho", "Ralph Grishman." ],
      "venue" : "Proceedings of the 1st ACL Workshop on Representation Learning for NLP (RepL4NLP).",
      "citeRegEx" : "Nguyen et al\\.,? 2016b",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "Event detection and domain adaptation with convolutional neural networks",
      "author" : [ "Thien Huu Nguyen", "Ralph Grishman." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Nguyen and Grishman.,? 2015",
      "shortCiteRegEx" : "Nguyen and Grishman.",
      "year" : 2015
    }, {
      "title" : "Graph convolutional networks with argument-aware pooling for event detection",
      "author" : [ "Thien Huu Nguyen", "Ralph Grishman." ],
      "venue" : "Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI).",
      "citeRegEx" : "Nguyen and Grishman.,? 2018",
      "shortCiteRegEx" : "Nguyen and Grishman.",
      "year" : 2018
    }, {
      "title" : "New york university 2016 system for kbp event nugget: A deep learning approach",
      "author" : [ "Thien Huu Nguyen", "Adam Meyers", "Ralph Grishman." ],
      "venue" : "Proceedings of Text Analysis Conference (TAC).",
      "citeRegEx" : "Nguyen et al\\.,? 2016c",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "One for all: Neural joint modeling of entities and events",
      "author" : [ "Trung Minh Nguyen", "Thien Huu Nguyen." ],
      "venue" : "Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI).",
      "citeRegEx" : "Nguyen and Nguyen.,? 2019",
      "shortCiteRegEx" : "Nguyen and Nguyen.",
      "year" : 2019
    }, {
      "title" : "Dare: Data augmented relation extraction with gpt",
      "author" : [ "Yannis Papanikolaou", "Andrea Pierleoni" ],
      "venue" : null,
      "citeRegEx" : "Papanikolaou and Pierleoni.,? \\Q2020\\E",
      "shortCiteRegEx" : "Papanikolaou and Pierleoni.",
      "year" : 2020
    }, {
      "title" : "A unified model of phrasal and sentential evidence for information extraction",
      "author" : [ "Siddharth Patwardhan", "Ellen Riloff." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Patwardhan and Riloff.,? 2009",
      "shortCiteRegEx" : "Patwardhan and Riloff.",
      "year" : 2009
    }, {
      "title" : "Data augmentation for spoken language understanding via pretrained models",
      "author" : [ "Baolin Peng", "Chenguang Zhu", "Michael Zeng", "Jianfeng Gao." ],
      "venue" : "arXiv preprint arXiv:2004.13952.",
      "citeRegEx" : "Peng et al\\.,? 2020",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2020
    }, {
      "title" : "Computational optimal transport: With applications to data science",
      "author" : [ "Gabriel Peyre", "Marco Cuturi." ],
      "venue" : "Foundations and Trends in Machine Learning.",
      "citeRegEx" : "Peyre and Cuturi.,? 2019",
      "shortCiteRegEx" : "Peyre and Cuturi.",
      "year" : 2019
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Casie: Extracting cybersecurity event information from text",
      "author" : [ "Taneeya Satyapanich", "Francis Ferraro", "Tim Finin." ],
      "venue" : "Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI).",
      "citeRegEx" : "Satyapanich et al\\.,? 2020",
      "shortCiteRegEx" : "Satyapanich et al\\.",
      "year" : 2020
    }, {
      "title" : "Jointly extracting event triggers and arguments by dependency-bridge rnn and tensor-based argument interaction",
      "author" : [ "Lei Sha", "Feng Qian", "Baobao Chang", "Zhifang Sui." ],
      "venue" : "Proceedings of the Association for the Advancement of Artificial Intelligence",
      "citeRegEx" : "Sha et al\\.,? 2018",
      "shortCiteRegEx" : "Sha et al\\.",
      "year" : 2018
    }, {
      "title" : "Image enhanced event detection in news articles",
      "author" : [ "Meihan Tong", "Shuai Wang", "Yixin Cao", "Bin Xu", "Juanzi Li", "Lei Hou", "Tat-Seng Chua." ],
      "venue" : "Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI).",
      "citeRegEx" : "Tong et al\\.,? 2020a",
      "shortCiteRegEx" : "Tong et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving event detection via open-domain trigger knowledge",
      "author" : [ "Meihan Tong", "Bin Xu", "Shuai Wang", "Yixin Cao", "Lei Hou", "Juanzi Li", "Jun Xie." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Tong et al\\.,? 2020b",
      "shortCiteRegEx" : "Tong et al\\.",
      "year" : 2020
    }, {
      "title" : "Ace 2005 multilingual training corpus",
      "author" : [ "Christopher Walker", "Stephanie Strassel", "Julie Medero", "Kazuaki Maeda." ],
      "venue" : "Technical report, Linguistic Data Consortium.",
      "citeRegEx" : "Walker et al\\.,? 2006",
      "shortCiteRegEx" : "Walker et al\\.",
      "year" : 2006
    }, {
      "title" : "Adversarial training for weakly supervised event detection",
      "author" : [ "Xiaozhi Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun", "Peng Li." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Event detection with multi-order graph convolution and aggregated attention",
      "author" : [ "Haoran Yan", "Xiaolong Jin", "Xiangbin Meng", "Jiafeng Guo", "Xueqi Cheng." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Yan et al\\.,? 2019",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2019
    }, {
      "title" : "Joint extraction of events and entities within a document context",
      "author" : [ "Bishan Yang", "Tom M. Mitchell." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "citeRegEx" : "Yang and Mitchell.,? 2016",
      "shortCiteRegEx" : "Yang and Mitchell.",
      "year" : 2016
    }, {
      "title" : "Exploring pre-trained language models for event extraction and generation",
      "author" : [ "Sen Yang", "Dawei Feng", "Linbo Qiao", "Zhigang Kan", "Dongsheng Li." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Generative data augmentation for commonsense reasoning",
      "author" : [ "Yiben Yang", "Chaitanya Malaviya", "Jared Fernandez", "Swabha Swayamdipta", "Ronan Le Bras", "Ji-Ping Wang", "Chandra Bhagavatula", "Yejin Choi", "Doug Downey." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Yang et al\\.,? 2020",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    }, {
      "title" : "Open-schema event profiling for massive news corpora",
      "author" : [ "Quan Yuan", "Xiang Ren", "Wenqi He", "Chao Zhang", "Xinhe Geng", "Lifu Huang", "Heng Ji", "Chin-Yew Lin", "Jiawei Han." ],
      "venue" : "Proceedings of the Conference on Information and Knowledge Management",
      "citeRegEx" : "Yuan et al\\.,? 2018",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2018
    }, {
      "title" : "Scale up event extraction learning via automatic training data generation",
      "author" : [ "Ying Zeng", "Yansong Feng", "Rong Ma", "Zheng Wang", "Rui Yan", "Chongde Shi", "Dongyan Zhao." ],
      "venue" : "Proceedings of the Association for the Advancement of Artificial Intelligence",
      "citeRegEx" : "Zeng et al\\.,? 2017",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2017
    }, {
      "title" : "On data augmentation for extreme multi-label classification",
      "author" : [ "Danqing Zhang", "Tao Li", "Haiyang Zhang", "Bing Yin." ],
      "venue" : "arXiv preprint arXiv:2009.10778.",
      "citeRegEx" : "Zhang et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Extracting entities and events as a single task using a transition-based neural model",
      "author" : [ "Junchi Zhang", "Yanxia Qin", "Yue Zhang", "Mengchi Liu", "Donghong Ji." ],
      "venue" : "Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI).",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "A question answering-based framework for one-step event argument extraction",
      "author" : [ "Yunyan Zhang", "Guangluan Xu", "Yang Wang", "Daoyu Lin", "Feng Li", "Chenglong Wu", "Jingyuan Zhang", "Tinglei Huang." ],
      "venue" : "IEEE Access, vol 8, 65420-65431.",
      "citeRegEx" : "Zhang et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Several methods have been introduced for ED, extending from feature-based models (Ahn, 2006; Liao and Grishman, 2010a; Miwa et al., 2014) to advanced deep learning methods (Nguyen and Grishman, 2015; Chen et al.",
      "startOffset" : 81,
      "endOffset" : 137
    }, {
      "referenceID" : 20,
      "context" : "Several methods have been introduced for ED, extending from feature-based models (Ahn, 2006; Liao and Grishman, 2010a; Miwa et al., 2014) to advanced deep learning methods (Nguyen and Grishman, 2015; Chen et al.",
      "startOffset" : 81,
      "endOffset" : 137
    }, {
      "referenceID" : 29,
      "context" : "Several methods have been introduced for ED, extending from feature-based models (Ahn, 2006; Liao and Grishman, 2010a; Miwa et al., 2014) to advanced deep learning methods (Nguyen and Grishman, 2015; Chen et al.",
      "startOffset" : 81,
      "endOffset" : 137
    }, {
      "referenceID" : 10,
      "context" : "Recently, there have been some efforts to enlarge training data for ED models by exploiting unsupervised (Huang et al., 2016; Yuan et al., 2018) or distantly-supervised (Keith et al.",
      "startOffset" : 105,
      "endOffset" : 144
    }, {
      "referenceID" : 54,
      "context" : "Recently, there have been some efforts to enlarge training data for ED models by exploiting unsupervised (Huang et al., 2016; Yuan et al., 2018) or distantly-supervised (Keith et al.",
      "startOffset" : 105,
      "endOffset" : 144
    }, {
      "referenceID" : 43,
      "context" : "existing pre-trained language model GPT-2 (Radford et al., 2019) to automatically generate training data for models.",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 1,
      "context" : "to directly remove noisy generated examples before actual usage for model training via some heuristic rules (Anaby-Tavor et al., 2020; Yang et al., 2020).",
      "startOffset" : 108,
      "endOffset" : 153
    }, {
      "referenceID" : 53,
      "context" : "to directly remove noisy generated examples before actual usage for model training via some heuristic rules (Anaby-Tavor et al., 2020; Yang et al., 2020).",
      "startOffset" : 108,
      "endOffset" : 153
    }, {
      "referenceID" : 35,
      "context" : "word-level classification problem as in prior work (Nguyen and Grishman, 2015; Ngo et al., 2020).",
      "startOffset" : 51,
      "endOffset" : 96
    }, {
      "referenceID" : 30,
      "context" : "word-level classification problem as in prior work (Nguyen and Grishman, 2015; Ngo et al., 2020).",
      "startOffset" : 51,
      "endOffset" : 96
    }, {
      "referenceID" : 49,
      "context" : "Base Model: Following the prior work (Wang et al., 2019), our base model consists of the",
      "startOffset" : 37,
      "endOffset" : 56
    }, {
      "referenceID" : 42,
      "context" : "It is worth mentioning that this problem is intractable so we solve its entropy-based approximation using the Sinkhorn algorithm (Peyre and Cuturi, 2019).",
      "startOffset" : 129,
      "endOffset" : 153
    }, {
      "referenceID" : 48,
      "context" : "To evaluate the effectiveness of the proposed model, called the GPT-based data augmentation model for ED with OT (GPTEDOT), we conduct experiments on the following ED datasets: ACE 2005 (Walker et al., 2006): This dataset annotates 599 documents for 33 event types that cover different text domains(e.",
      "startOffset" : 186,
      "endOffset" : 207
    }, {
      "referenceID" : 18,
      "context" : "We use the same preprocessing script and data split as prior works (Lai et al., 2020c; Tong et al., 2020b) to achieve fair comparisons.",
      "startOffset" : 67,
      "endOffset" : 106
    }, {
      "referenceID" : 47,
      "context" : "We use the same preprocessing script and data split as prior works (Lai et al., 2020c; Tong et al., 2020b) to achieve fair comparisons.",
      "startOffset" : 67,
      "endOffset" : 106
    }, {
      "referenceID" : 18,
      "context" : "For this dataset, we compare our model with prior state-of-the-art models reported in the recent works (Lai et al., 2020c; Tong et al., 2020b), including BERT-based models such as DMBERT, AD-DMBERT (Wang et al.",
      "startOffset" : 103,
      "endOffset" : 142
    }, {
      "referenceID" : 47,
      "context" : "For this dataset, we compare our model with prior state-of-the-art models reported in the recent works (Lai et al., 2020c; Tong et al., 2020b), including BERT-based models such as DMBERT, AD-DMBERT (Wang et al.",
      "startOffset" : 103,
      "endOffset" : 142
    }, {
      "referenceID" : 49,
      "context" : ", 2020b), including BERT-based models such as DMBERT, AD-DMBERT (Wang et al., 2019), DRMM, EKD (Tong et al.",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 47,
      "context" : ", 2019), DRMM, EKD (Tong et al., 2020b), and GatedGCN (Lai et al.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 52,
      "context" : ", 2019), BERT-ED (Yang et al., 2019), as the baselines for CySecED.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : "RAMS (Ebner et al., 2020): This dataset annotates 9,124 event triggers for 38 event types.",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 49,
      "context" : ", AD-DMBERT (Wang et al., 2019) with semi-supervised and adversarial Model P R F1",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 46,
      "context" : "learning, DRMM (Tong et al., 2020a) with imageenhanced models, and EKD (Tong et al.",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 47,
      "context" : ", 2020a) with imageenhanced models, and EKD (Tong et al., 2020b) with external open-domain event triggers, the better",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 10,
      "context" : "to address this issue via unsupervised (Huang et al., 2016; Yuan et al., 2018), semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012; Ferguson et al.",
      "startOffset" : 39,
      "endOffset" : 78
    }, {
      "referenceID" : 54,
      "context" : "to address this issue via unsupervised (Huang et al., 2016; Yuan et al., 2018), semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012; Ferguson et al.",
      "startOffset" : 39,
      "endOffset" : 78
    }, {
      "referenceID" : 20,
      "context" : ", 2018), semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012; Ferguson et al., 2018), distantly supervised (Keith et al.",
      "startOffset" : 25,
      "endOffset" : 98
    }, {
      "referenceID" : 12,
      "context" : ", 2018), semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012; Ferguson et al., 2018), distantly supervised (Keith et al.",
      "startOffset" : 25,
      "endOffset" : 98
    }, {
      "referenceID" : 7,
      "context" : ", 2018), semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012; Ferguson et al., 2018), distantly supervised (Keith et al.",
      "startOffset" : 25,
      "endOffset" : 98
    } ],
    "year" : 2021,
    "abstractText" : "Event Detection (ED) aims to recognize mentions of events (i.e., event triggers) and their types in text. Recently, several ED datasets in various domains have been proposed. However, the major limitation of these resources is the lack of enough training data for individual event types which hinders the efficient training of data-hungry deep learning models. To overcome this issue, we propose to exploit the powerful pre-trained language model GPT-2 to generate training samples for ED. To prevent the noises inevitable in automatically generated data from hampering training process, we propose to exploit a teacher-student architecture in which the teacher is supposed to learn anchor knowledge from the original data. The student is then trained on combination of the original and GPT-generated data while being led by the anchor knowledge from the teacher. Optimal transport is introduced to facilitate the anchor knowledge-based guidance between the two networks. We evaluate the proposed model on multiple ED benchmark datasets, gaining consistent improvement and establishing state-of-the-art results for ED.",
    "creator" : "LaTeX with hyperref"
  }
}