{
  "name" : "2021.acl-long.286.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Best of Both Worlds: Making High Accuracy Non-incremental Transformer-based Disfluency Detection Incremental",
    "authors" : [ "Morteza Rohanian" ],
    "emails" : [ "@qmul.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3693–3703\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3693"
    }, {
      "heading" : "1 Introduction",
      "text" : "Conversational systems provide a significant addition to the present approaches in mental health care delivery. Interactions with these conversational agents have been shown to contain observable indicators of cognitive states, such as the rate of filled pauses and different temporal and turnrelated features (Gratch et al., 2014). Alzheimer’s Disease (AD) patients, for example, have trouble\nperforming tasks that leverage semantic information; they have difficulties with verbal fluency and object recognition. AD patients speak more slowly with long pauses and spend extra time looking for the correct word, which leads to speech disfluency (López-de Ipiña et al., 2013; Nasreen et al., 2021). Disfluency markers can be key features for identifying certain cognitive disorders for application in conversational agents (Rohanian et al., 2020).\nSuch conversational systems are primarily used for content processing, which is then analyzed offline. There is much work on detecting disfluencies for offline analysis of transcripts. However, given that these disfluency detection models do not work for live systems and depend on rich transcription data, including pre-segmentation of dialogue acts, to facilitate more cost-effective analysis of other data, we need systems capable of performing directly and incrementally off the speech signal, or at least from the results of automatic speech recognition (ASR) as they arrive in the system.\nAs it receives word-by-word data, an incremental model must operate with minimum latency and do so without changing its initial assumptions and delivering its best decisions as early as possible following the principles outlined in (Hough and Purver, 2014). Here we design and evaluating models that work with online, incremental speech recognition output to detect disfluencies with varying levels of granularity.\nThe best neural language encoders currently used in computational linguistics consider word sequences as a whole, and their implementations have been unsuitable for live incremental processing. Transformers (Vaswani et al., 2017), for instance, operate on representations that do not naturally have an organizing principle of linear word order. We analyze how these models work under incremental frameworks, where it is essential to present partial output relying on partial input pro-\nvided up to a certain time step that may occur in interactive healthcare systems. We explore whether we can adjust such models to function incrementally and how useful they are in terms of overall accuracy and incremental metrics.\nTo further enhance the models’ incremental performance, we use two general strategies to adjust the training regime and the real-time procedure: incremental training (‘chunk-based’ training and add-M training) and incremental decoding (constant latency and prophecies). We employ three prominent decoding methods to predict the rightward context of the word currently being processed: beam search, top-k sampling, and top-p sampling. We also measure our models’ incremental performance to set the trade-off between incremental performance and final performance."
    }, {
      "heading" : "2 Related Work",
      "text" : "Although considerable work has been done on detecting disfluencies, much of this work uses transcripts as texts rather than live speech inputs, with the goal of ‘cleaning’ the disfluent content for post-processing purposes. They are almost exclusively conducted on pre-segmented utterances of the Switchboard corpus of telephone conversations (Godfrey et al., 1992). Several disfluency detection efforts involve sentence-based parsing and language models (Johnson and Charniak, 2004; Zwarts et al., 2010). Sequence labeling models with start-inside-outside (BIO) style tags have been used in recent neural sequence approaches to disfluency detection based on bi-directional Long Short Term Memory (BiLSTM) networks and Transformers, in which the sequences are available in full (Zayats et al., 2016; Lou and Johnson, 2020; Wang et al., 2020).\nSuch offline methods are insufficient if we intend to infer meaning from repairs and edit words for disfluency detection in real-time, which is beneficial in a healthcare domain dialogue system that seeks to get a consistent and clear understanding of user statements and the user’s cognitive state.\nMethods based on strictly incremental operation have been rare. Hough and Purver (2014) used a line of classifiers and language model features in a strong incremental operating system without looking ahead. Incremental dependency parsing combined with the removal of disfluency was also studied (Rasooli and Tetreault, 2015). Some studies have used recurrent neural networks for live dis-\nfluency identification. Using a basic Elman Recurrent Neural Network (RNN), Hough and Schlangen (2015) investigated incremental processing, with an objective coupling detection accuracy with low latency.\nLanguage models have been used as an additional task for the identification of disfluencies, relying on the intuition that disfluencies can be detected by divergences from clean language models, with Johnson and Charniak (2004)’s noisy channel model beginning this effort. Shalyminov et al. (2018) made language modelling an auxiliary task to disfluency detection in a deep multi-task learning (MTL) set-up, gaining accuracy over a vanilla RNN tagger. POS tags have also been used as an input for detecting disfluencies, showing slight increases in disfluency detection over using word values alone (Purver et al., 2018).\nWhile the work above operates only on transcripts pre-segmented into utterances, recent research has been performed on combining disfluency detection with utterance segmentation. This was done in a joint tagset of disfluency, and utterance segmentation tags by (Hough and Schlangen, 2017), showing an improvement over the performance of the individual tasks, and (Rohanian and Hough, 2020) show an improvement in both tasks when framed as a multi-task learning (MTL) set-up with a Long Short-term Memory network (LSTM), also simultaneously doing POS-tagging and language modelling.\nThe recent live incremental systems fall short of the same accuracies achievable on pre-segmented transcripts, so there is a natural interest in using the best non-incremental sequence models and adapting them for incrementality. Madureira and Schlangen (2020) take up this effort in several other sequence tagging and classification tasks, showing how bidirectional encoders and Transformers can be modified to work incrementally. To reduce the impact of the partiality of the input, the models predict future content and wait for more rightward context. Dalvi et al. (2018) also use truncated inputs during the training phase of live machine translation to address the partial input sentence decoding problem Bidirectional encoders face. Here, we seek to add to this growing effort to investigate the trade-off of incremental performance against the final output quality of deep neural network-based language processing, applied to incremental disfluency detection."
    }, {
      "heading" : "3 Disfluency Detection",
      "text" : "Disfluencies are generally assumed to have a reparandum-interregnum-repair structure in their fullest form as speech repairs (Shriberg, 1994; Meteer et al., 1995). A reparandum is a stretch of speech later corrected by the speaker; the corrected expression is a repair, the beginning of which is referred to as repair onset. An interregnum word is a filler or a reference expression between the repair and reparandum, usually an interruption and hesitation step when the speaker expresses a repair, giving the structure as in (1).\nJohn [ likes︸ ︷︷ ︸ reparandum + { uh }︸ ︷︷ ︸ interregnum loves ]︸ ︷︷ ︸ repair Mary\n(1)\nIn the absence of reparandum and repair, the disfluency is reduced to an isolated edit term. A marked, lexicalised edit term such as a filled pause (“uh” or “um”) or more phrasal terms such as “I mean” and “you know” may occur. The identification of these elements and their structure is then the task of disfluency detection.\nThe task of detecting incremental disfluencies adds to the difficulty of doing this in real-time, word-by-word, from left to right. Disfluency recognition is then treated as the same problem that a human processor faces with a disfluent expression: only when an interregnum is detected, or maybe even when a repair is initiated, does it become clear that the earlier content is now to be regarded as ‘to be repaired,’ i.e., to be classified as a reparandum. Therefore, the task cannot be defined as a simple sequence labeling task in which the tags for the reparandum, interregnum, and repair phases are assigned left-to-right over words as seen in the above example; in this case, it will require the assumption that “likes” would be repaired, at a time when there is no data to make it available.\nWe use a tag set that encodes the start of the reparandum only at a time when it can be inferred, primarily when the repair starts – the disfluency detection task is to tag words as in the top line of tags in Fig. 1 as either fluent (f ) an edit term (e),\na repair onset word (rpS−N for the reparandum starting N words back) and a repair end word of the type repeat (rpnRep), substitution (rpnSub) or delete (rpnDel)."
    }, {
      "heading" : "4 Model",
      "text" : "To incrementalise a Transformer-based model for word-by-word disfluency detection, we devise a model built on top of a pre-trained BERT architecture (Devlin et al., 2019) with a Conditional Random Field (CRF) output architecture to tag sequences with tags such as those in the top line of Fig. 1. We use a BERT-based encoder and try different strategies to incrementalise the system’s operation and output, using language models to predict future word sequences as described in Section 5 while maintaining BERT’s non-incremental quality.\nUtterance segmentation Our models are designed to work not only with pre-segmented data but also on raw transcripts and ASR results, where utterance segmentation is required to leverage the use of sentence-based linguistic knowledge in BERT. Utterance segmentation has a clear interdependence with and influence on the detection of disfluency as disfluent restarts and repairs may be incorrectly predicted at fluent utterance boundaries without segmentation. In this paper, rather than performing utterance segmentation in tandem with disfluency detection, we perform it on words as they arrive in the system as a live segmentation task before sending the current prefix of the utterance to the disfluency detection system. We use the word-by-word segmentation system from (Rohanian and Hough, 2020) where four output tags define ranges of transcribed words or word hypotheses using a BIES tag scheme (Beginning, Inside, End, and Single) to allow for the prediction of an utterance ending. The tagset allows information to be captured from the context of the word to decide whether this word continues a current utterance (the - prefix) or starts anew (the . prefix), and also allows live prediction of whether the next word will continue the current utterance (the - suffix) or\nwhether the current word finishes the utterance (the . suffix). An example of the scheme is shown in the second line of Fig. 1.\nCRF We use a CRF output architecture to predict a tag for every token. Although this model generates predictions for the whole sequence, the labels are outputted individually. There are important dependencies between adjacent labels in disfluency detection, and explicit modeling of these relationships can help. The addition of the CRF enables the model to test for the most optimal path across all available label sequences."
    }, {
      "heading" : "4.1 Input Features",
      "text" : "In addition to the word values, we also experiment with two other inputs:\nPart-of-speech tags POS tags may enhance the identification of disfluencies on various settings. POS tagging helps detect disfluency structure as the parallelism between the reparandum and repair in substitutions, as shown in the repeated IN NNP sequences in Fig. 1.\nWord timings We also experiment with the duration from the ending of the previous word to the ending of the current word as it enters the system, either from ground truth word transcriptions or from ASR results."
    }, {
      "heading" : "5 Strategies for Incrementalising BERT",
      "text" : "Here we describe the different strategies we used to modify the training and live decoding methods of non-incremental models to detect speech disfluencies word-by-word incrementally. The general principle is to leverage high accuracy full sequence classification using BERT but deploying it on sequences, including future predictions for words up to the hypothesised end of the current utterance."
    }, {
      "heading" : "5.1 Modifying the Training Procedure",
      "text" : "Training is performed on full sentences/utterances, but the decoder produces outputs based on partial input data at the test time. This disparity between training and decoding can potentially affect our models’ performance. Based on (Dalvi et al., 2018), we present two methods to address this issue: chunk-based training and add-M training.\nChunk-based training In chunk-based training, we change the training scheme by removing the ends of each sentence in the training set and simply break each training sentence into chunks of N tokens. Here we use 2 and 3 for N .\nAdd-M training We begin with the first N words in training sentences in add-M training. The next training instances are then generated by N +M,N +2M,N +3M... words before the end of the sentence is reached. In our experiments, we found setting N=1 and M=1 worked best."
    }, {
      "heading" : "5.2 Modifying the Decoding Procedure",
      "text" : "Constant latency The technique of constant latency requires allowing certain ‘future’ words to be seen before a label to previous words is given. It is a form of look-ahead based on Baumann et al. (2011), in which before making the first decision with respect to previous time steps, the processor is required to wait for some correct context. We explore the one- or two-word contexts of our input. This suggests that the model generates the first label for word t after the word t+ 1 is seen or the model observes words t + 1 and t + 2 before tagging word t. This has an inherent limit on the latency achievable, and we use this as a baseline incremental decoding system.\nProphecy-based decoding For our other decoding strategies, we use a ‘prophecy’-based approach to predicting future word sequences, following the task of open-ended language generation, which, given an input text passage as context, is to produce text that constitutes a cohesive continuation (Holtzman et al., 2019). Inspired by (Madureira and Schlangen, 2020), using the GPT-2 language model (Radford et al., 2019), we first give each word as a left context and create a continuation until the end of an utterance to create a hypothetical complete context that satisfies the requirements of the models’ non-incremental structure.\nFormally, with m tokens x1...xm as our context, the task is to create the next n continuation tokens to achieve the completed sequence x1...xm+n. It is assumed that the models compute P (x1:m+n) using a standard left-to-right decomposition of the text probability as in (2). This process is used to build the utterance continuation token-by-token using a specific decoding technique.\nP (x1:m+n) = m+n∏ i=1 P (xi|x1...xi−1) (2)\nThree of the most common decoding methods are used in this paper: Beam search, Top-k sampling, and Top-p sampling. Example word sequence prophecies from these decoding methods\nare shown in Fig. 2. The right-most block shows the prediction of the continuation of the word sequences as each new word in the sequence “John likes uh loves Mary” is fed into the language model.\nBeam search Assuming that the model gives a greater likelihood to better quality text, we are looking for a sequence with the highest probability. During the search, a group of stacks is used to hold hypotheses. Beam size N is used to manage the search space by expanding the top N hypotheses in the existing stack. We used beam size 10 for all the models.\nTop-k sampling We define sampling as randomly choosing the next word based on its conditional probability distribution as in (3).\nxi ∼ P (x|x1:i−1) (3)\nIn the Top-k sampling, the most probable next k words are extracted and the probability mass is redistributed between only the following k words\n(Fan et al., 2018). Given a distribution P (x|x1:i−1), we extract its top-k vocabulary V (k) ⊂ V as the set of size k which maximizes ∑ x∈V (k) P (x|x1:i−1). After an initial investigation, we set k to 50 in all experiments.\nTop-p sampling Rather than selecting only the most probable K words, in Top-p sampling, we select the smallest possible range of words with their total likelihood exceeds the probability p (Holtzman et al., 2019). The probability mass is then redistributed between this set of words. With this method, the size of the word set will dynamically adjust based on the probability distribution of the next word. With the distribution P (x|x1:i−1), we consider its top-p sequence, with vocabulary V (p) ⊂ V as the smallest set with P (x|x1:i−1) ≥ p. We set p = 0.95."
    }, {
      "heading" : "6 Experimental Set-up",
      "text" : "We train on transcripts and test on both transcripts and ASR hypotheses. All models in testing have strictly word-by-word left to right input. In addition to using the latest word hypothesis as input, we train and evaluate the presented models with two kinds of additional inputs: time elapsed from the end of the previous word (hypothesis) to the current one and the POS tag of the current word. Results on the development set were used to find the best model to be evaluated on the test set.\nWe used the data from (Hough and Schlangen, 2017) for ASR hypotheses – this was generated by a free trial version of IBM’s Watson SpeechTo-Text service for incremental ASR. The service offers good quality ASR on noisy data-on our selected held-out data on Switchboard, and the average WER is 26.5%. The Watson service, crucially for our task, does not filter out hesitation markers or disfluencies (Baumann et al., 2017). The service delivers results incrementally, so silence-based endpointing is not used. It also outputs word timings, which are close enough to the source timings to use as features in the live version of our system.\nThe word embedding for LSTM was initialised with 50-dimensional embedding trained on Google News (Mikolov et al., 2013). The model has been implemented using Tensorflow 2.1. We train all models for a maximum of 50 epochs; otherwise, stop training if there is no improvement on the best score on the validation set after 7 epochs.\nA large version of the pre-trained BERT is used with 340M parameters (24-layer blocks, 16 self-\nattention heads, and 1024 hidden-size) for the model. In our analysis, when fine-tuning BERT, we followed the hyper-parameters of (Devlin et al., 2019). Since the datasets we use are tokenized, and each token has a matching tag, we adopt the directions provided by (Devlin et al., 2019) to deal with the sub-tokenization of BERT: to determine its label, the scores of the first sub-token are used, and further sub-token scores are discarded.\nData We use standard Switchboard training data (all conversation numbers starting sw2*,sw3 * in the Penn Treebank III release: 100k utterances, 650k words) and use standard held-out data (PTB III files sw4[5-9] *: 6.4k utterances, 49k words) as our validation set. We test on the standard test data (PTB III files 4[0-1] *) with partial words and punctuation stripped away from all files. We only choose a subset of the held-out and test data for the ASR results in assessment, whereby both channels achieve below 40 percent WER to ensure good separation- this left us with 18 dialogues in validation data and 17 dialogues for test data."
    }, {
      "heading" : "6.1 Evaluation Criteria",
      "text" : "We calculate F1 accuracy for repair onset detection FrpS and for edit term words Fe, which includes interregna and Frm for reparandum detection. Performing the task live, on hypotheses of speech recognition that may not be quite equivalent to the annotated gold-standard transcription involves the use of time-based local accuracy metrics in a time window (i.e., within this time frame, has a disfluency been detected, even if not on the\nidentical words?)-we, therefore, measure the F1 score over 10-second windows of each speaker’s channel.\nFor incremental performance, we measure latency and output stability over time. We use the first time to detection (FTD) metric of (Zwarts et al., 2010) for latency: the average latency (in number of words) before the first detection of a gold standard repair onset or edit term word. For stability, we evaluate the edit overhead (EO) of output labels (Baumann et al., 2011), the proportion of the unnecessary edits (insertions and deletions) required to achieve the final labels produced by the model, with perfect performance being 0%."
    }, {
      "heading" : "6.2 Competitor Baselines",
      "text" : "We compare our incrementalised BERT model against a number of existing baselines, largely from existing incremental disfluency detection systems trained and tested on the same data:\nSTIR (HP’14/HS’15/PHH’18): Hough and Purver (2014)’s STrongly Incremental Repair detection (STIR) non-deep model using n-gram language model features in a pipeline of Random Forest classifiers. The reparandum is detected by a backward search, showing robustness for longer lengths of repair compared to deep sequence tagging models (Purver et al., 2018). A state-ofthe-art incremental model on pre-segmented transcripts.\nRNN (HS’15): (Hough and Schlangen, 2015)’s RNN-based model, the first deep learning-based\nincremental disfluency detection model using the same tagset as in our model. Results from Purver et al. (2018) are used, which reproduced the model with some degradation in the results.\nLSTM: An LSTM version of Hough and Schlangen (2015) on pre-segmented transcripts\nLSTM joint tagset (HS’17) Hough and Schlangen (2017)’s model, which simultaneously predicts utterance segmentation using a joint tag set of utterance segmentation tags and disfluency tags, the latter of which is the same as our own. This is the only other work to use word timing information and to be testable on ASR results.\nLSTM-MTL (SEL’18) Shalyminov et al. (2018)’s multi-task learning model, which tags according to our tag set but simultaneously does language modelling by predicting the probability of the current word given the history. Also adds ground-truth POS tags to input.\nLSTM-MTL (RH’20): Rohanian and Hough (2020)’s multi-task learning model, which simultaneously predicts utterance segmentation, POS tags and language model probabilities, exhibiting state-of-the-art results for a strictly incremental deep model. The model is used as described by the authors and also here with the addition of timing information and gold standard POS information (as opposed to simultaneously predicted POS tags). It is also applied to ASR results as it is a suitable model to do so. This same model provides the automatic live utterance segmentation in our own model."
    }, {
      "heading" : "7 Results",
      "text" : "The results in terms of the final output of our best performing incremental BERT system in the three testing regimes versus its competitors is shown in\nTable 1.1 We found our best model was the add-M trained model, and the best decoding strategy was using top-p sampling for predicting future words.\nDisfluency detection on transcripts For repair detection, our system’s best FrpS score for detecting repair onsets on pre-segmented transcripts at 0.853 beats state-of-the-art incremental systems. This performance degrades using automatic segmentation to 0.802, a state-of-the-art result for this setting. Its Frm accuracy of 0.757 on reparandum words on pre-segmented transcripts is only beaten by HP’14/PHH’18 model using word and POS input, making it a state-of-the-art strictly incremental deep model. This performance degrades to 0.678 on raw transcripts but is a state-of-the-art result for this setting. In terms of edit term detection, stateof-the-art detection results of 0.960 and 0.944 are achieved on the pre-segmented and unsegmented settings, improving over the existing benchmarks of HP’14 and RH’20. These results suggest we have achieved the aim of a strictly incremental model achieving high final accuracies.\nDisfluency detection on ASR results Using the ASR results from HS’17 for comparison, a significant improvement can be seen over the previously reported results on FrpS and Fe per 10-second window, improving from 0.557 to 0.605 and from 0.727 to 0.809 respectively. Given the previously reported best system gave strong correlations in terms of real repair rates, this is encouraging that our system could be very useful in a live setting."
    }, {
      "heading" : "7.1 Incremental Performance",
      "text" : "The purpose of this paper was to adapt a highperforming, non-incremental model for incremental operation. As can be seen in Table 2 and in Fig. 3, while our BERT model with top-p sample utterance prediction outperforms the multi-task\n1Experiments are reproducible from https://github. com/mortezaro/tr-disfluency\nmodel and vanilla LSTM model in terms of final output accuracy, its incremental output stability is slightly below its competitors, with the best edit overhead of 63% unnecessary edits versus 25% (LSTM joint tagset (HS’17)) and 42% (LSTMMTL (RH’20)) on ASR results, meaning the output is slightly, though not severely, more jittery.\nOf the prophecy-based approaches, we found the top-p sampling method gave the most stable results (EO=61% with chunk training, EO=60% with add-M training) and beam search gave the least stable. As shown in Fig. 3, while the constant latency approaches offer large advantages in EO over prophecy-based models on transcripts, that advantage disappears on ASR results, where the prophecy models generally outperform them. As can be seen in Table 2, there is a slight improvement in stability across all systems using the add-M training regime for final output and incremental performance.\nIn terms of latency, results are even more encouraging, with the best FTD for rpS of 0.31 words (versus 0.03 and 0.07) on transcripts, which shows a relatively short latency of detecting the repair for the first time– this suggests a responsive, sensitive system."
    }, {
      "heading" : "7.2 Error Analysis",
      "text" : "We conduct an error analysis in terms of performance on different repair types and in terms of repairs with different lengths. Table 3 shows the performance in terms of FrpS score on detecting repairs of the three different types: verbatim repeats, substitutions, and deletes (restarts). Our BERT model performs best, either jointly or uniquely, across all three types, with a gain of 0.06 over its nearest competitors for substitutions and deletes. Through large-scale training, the enhanced linguistic knowledge equips it to recognize the syntactic\nand lexical parallelism in more complex repairs while retaining high accuracy on repeats. Table 4 shows the degradation in performance in detecting repairs of different lengths. With Add-M training, the BERT model degrades less and performs (joint) best on all lengths and nested disfluencies. While the performance on length five repairs is considerably better than the other deep models, the 0.187 accuracy on length six repairs is what gives it a slight disadvantage compared to the HP’14 explicit backtracking system (reported as high as 0.500 in PHH’18), which likely accounts for the lower Frm score despite the superior FrpS score of our system."
    }, {
      "heading" : "8 Discussion and Conclusion",
      "text" : "Our incremental GPT-2 and BERT-driven system performs well at detecting repair disfluencies on pre-segmented and unsegmented transcripts, achieving state-of-the-art results for a strictly incremental repair onset detection. Our system is competitive at reparadnum word detection and achieves state-of-the-art results in edit term detection. The results on ASR transcripts are also state-of-the-art.\nThe high sequence-final performance comes at the expense of marginally increased jitter in the word-by-word output, but with sensitive and fast repair detection, on average first detecting the repair under a third of a second after the end of the repair onset word. These results suggest it is beginning to enjoy the best of both worlds in leveraging the right-ward context which BERT uses for its high performance, while the continuation predictions from the GPT-2 model are good enough to allow good incremental performance before the true right-ward context is available.\nThe linguistic knowledge in the BERT model allows it to recognize parallelism in reparandum and repair phases and the absence thereof to increase performance on detecting substitution and delete repairs. This improvement to existing deep\ndisfluency detection models, and, with appropriate use of open-ended language generation techniques with a GPT-2 language model, its good incremental performance, is consistent with a growing body of work (Heeman and Allen, 1999; Johnson and Charniak, 2004; Zwarts et al., 2010; Hough and Purver, 2014; Shalyminov et al., 2018; Rohanian and Hough, 2020), showing good language modelling can lead to good disfluency detection, as they are inherently part of the same process.\nOur system still fails to detect longer repairs compared to an explicit backtracking mechanism like (Hough and Purver, 2014). While the vanishing gradient problem is partly overcome here, the strictly left-to-right constraint on decoding puts memory limitations on any repair detection system. In future, we will explore efficient ways to navigate this space whilst not filtering out rarer repair forms.\nThe results on ASR results show our disfluency detection system is ready for use in a live setting with a good degree of accuracy, and work is currently underway to use it to help detect a variety of different cognitive conditions, including Alzheimer’s Disease, in a live diagnostic system."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the anonymous ACL-IJCNLP reviewers for their helpful comments and Matthew Purver for his continuous support and supervision on the wider project."
    } ],
    "references" : [ {
      "title" : "Evaluation and optimisation of incremental processors",
      "author" : [ "Timo Baumann", "Okko Buß", "David Schlangen." ],
      "venue" : "Dialogue & Discourse, 2(1):113–141.",
      "citeRegEx" : "Baumann et al\\.,? 2011",
      "shortCiteRegEx" : "Baumann et al\\.",
      "year" : 2011
    }, {
      "title" : "Recognising conversational speech: What an incremental asr should do for a dialogue system and how to get there",
      "author" : [ "Timo Baumann", "Casey Kennington", "Julian Hough", "David Schlangen." ],
      "venue" : "Dialogues with social robots, pages 421–432. Springer.",
      "citeRegEx" : "Baumann et al\\.,? 2017",
      "shortCiteRegEx" : "Baumann et al\\.",
      "year" : 2017
    }, {
      "title" : "Incremental decoding and training methods for simultaneous translation in neural machine translation",
      "author" : [ "Fahim Dalvi", "Nadir Durrani", "Hassan Sajjad", "Stephan Vogel." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the",
      "citeRegEx" : "Dalvi et al\\.,? 2018",
      "shortCiteRegEx" : "Dalvi et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Hierarchical neural story generation",
      "author" : [ "Angela Fan", "Mike Lewis", "Yann Dauphin." ],
      "venue" : "arXiv preprint arXiv:1805.04833.",
      "citeRegEx" : "Fan et al\\.,? 2018",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2018
    }, {
      "title" : "Switchboard: Telephone speech corpus for research and development",
      "author" : [ "John J Godfrey", "Edward C Holliman", "Jane McDaniel." ],
      "venue" : "Acoustics, Speech, and Signal Processing, IEEE International Conference on, volume 1, pages 517–520. IEEE",
      "citeRegEx" : "Godfrey et al\\.,? 1992",
      "shortCiteRegEx" : "Godfrey et al\\.",
      "year" : 1992
    }, {
      "title" : "The distress analysis interview corpus of human and computer interviews",
      "author" : [ "Jonathan Gratch", "Ron Artstein", "Gale M Lucas", "Giota Stratou", "Stefan Scherer", "Angela Nazarian", "Rachel Wood", "Jill Boberg", "David DeVault", "Stacy Marsella" ],
      "venue" : null,
      "citeRegEx" : "Gratch et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gratch et al\\.",
      "year" : 2014
    }, {
      "title" : "Speech repains, intonational phrases, and discourse markers: modeling speakers’ utterances in spoken dialogue",
      "author" : [ "Peter A Heeman", "James Allen." ],
      "venue" : "Computational Linguistics, 25(4):527–572.",
      "citeRegEx" : "Heeman and Allen.,? 1999",
      "shortCiteRegEx" : "Heeman and Allen.",
      "year" : 1999
    }, {
      "title" : "The curious case of neural text degeneration",
      "author" : [ "Ari Holtzman", "Jan Buys", "Li Du", "Maxwell Forbes", "Yejin Choi." ],
      "venue" : "arXiv preprint arXiv:1904.09751.",
      "citeRegEx" : "Holtzman et al\\.,? 2019",
      "shortCiteRegEx" : "Holtzman et al\\.",
      "year" : 2019
    }, {
      "title" : "Strongly incremental repair detection",
      "author" : [ "Julian Hough", "Matthew Purver." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 78–89.",
      "citeRegEx" : "Hough and Purver.,? 2014",
      "shortCiteRegEx" : "Hough and Purver.",
      "year" : 2014
    }, {
      "title" : "Recurrent neural networks for incremental disfluency detection",
      "author" : [ "Julian Hough", "David Schlangen." ],
      "venue" : "Sixteenth Annual Conference of the International Speech Communication Association.",
      "citeRegEx" : "Hough and Schlangen.,? 2015",
      "shortCiteRegEx" : "Hough and Schlangen.",
      "year" : 2015
    }, {
      "title" : "Joint, incremental disfluency detection and utterance segmentation from speech",
      "author" : [ "Julian Hough", "David Schlangen." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers,",
      "citeRegEx" : "Hough and Schlangen.,? 2017",
      "shortCiteRegEx" : "Hough and Schlangen.",
      "year" : 2017
    }, {
      "title" : "On the selection of noninvasive methods based on speech analysis oriented to automatic alzheimer disease diagnosis",
      "author" : [ "Nora Barroso", "Miriam Ecay-Torres", "Pablo MartinezLage" ],
      "venue" : null,
      "citeRegEx" : "Barroso et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Barroso et al\\.",
      "year" : 2013
    }, {
      "title" : "A TAGbased noisy-channel model of speech repairs",
      "author" : [ "Mark Johnson", "Eugene Charniak." ],
      "venue" : "ACL, pages 33–39.",
      "citeRegEx" : "Johnson and Charniak.,? 2004",
      "shortCiteRegEx" : "Johnson and Charniak.",
      "year" : 2004
    }, {
      "title" : "Improving disfluency detection by self-training a selfattentive model",
      "author" : [ "Paria Jamshid Lou", "Mark Johnson." ],
      "venue" : "arXiv preprint arXiv:2004.05323.",
      "citeRegEx" : "Lou and Johnson.,? 2020",
      "shortCiteRegEx" : "Lou and Johnson.",
      "year" : 2020
    }, {
      "title" : "Incremental processing in the age of non-incremental encoders: An empirical assessment of bidirectional models for incremental NLU",
      "author" : [ "Brielen Madureira", "David Schlangen." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Madureira and Schlangen.,? 2020",
      "shortCiteRegEx" : "Madureira and Schlangen.",
      "year" : 2020
    }, {
      "title" : "Disfluency annotation stylebook for the switchboard corpus",
      "author" : [ "M. Meteer", "A. Taylor", "R. MacIntyre", "R. Iyer." ],
      "venue" : "ms. Technical report, Department of Computer and Information Science, University of Pennsylvania.",
      "citeRegEx" : "Meteer et al\\.,? 1995",
      "shortCiteRegEx" : "Meteer et al\\.",
      "year" : 1995
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in neural information processing systems, pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Alzheimer’s dementia recognition from spontaneous speech using disfluency and interactional features",
      "author" : [ "Shamila Nasreen", "Morteza Rohanian", "Matthew Purver", "Julian Hough." ],
      "venue" : "Frontiers in Computer Science, 3:49.",
      "citeRegEx" : "Nasreen et al\\.,? 2021",
      "shortCiteRegEx" : "Nasreen et al\\.",
      "year" : 2021
    }, {
      "title" : "Computational models of miscommunication phenomena",
      "author" : [ "Matthew Purver", "Julian Hough", "Christine Howes." ],
      "venue" : "Topics in cognitive science, 10(2):425– 451.",
      "citeRegEx" : "Purver et al\\.,? 2018",
      "shortCiteRegEx" : "Purver et al\\.",
      "year" : 2018
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Yara parser: A fast and accurate dependency parser",
      "author" : [ "Mohammad Sadegh Rasooli", "Joel R. Tetreault." ],
      "venue" : "Computing Research Repository, arXiv:1503.06733. Version 2.",
      "citeRegEx" : "Rasooli and Tetreault.,? 2015",
      "shortCiteRegEx" : "Rasooli and Tetreault.",
      "year" : 2015
    }, {
      "title" : "Reframing incremental deep language models for dialogue processing with multi-task learning",
      "author" : [ "Morteza Rohanian", "Julian Hough." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 497–507,",
      "citeRegEx" : "Rohanian and Hough.,? 2020",
      "shortCiteRegEx" : "Rohanian and Hough.",
      "year" : 2020
    }, {
      "title" : "Multi-modal fusion with gating using audio",
      "author" : [ "Morteza Rohanian", "Julian Hough", "Matthew Purver" ],
      "venue" : null,
      "citeRegEx" : "Rohanian et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Rohanian et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-task learning for domain-general spoken disfluency detection in dialogue systems",
      "author" : [ "Igor Shalyminov", "Arash Eshghi", "Oliver Lemon." ],
      "venue" : "Proceedings of the 22nd SemDial Workshop on the Semantics and Pragmatics of Dialogue (AixDial), Aix-",
      "citeRegEx" : "Shalyminov et al\\.,? 2018",
      "shortCiteRegEx" : "Shalyminov et al\\.",
      "year" : 2018
    }, {
      "title" : "Preliminaries to a Theory of Speech Disfluencies",
      "author" : [ "Elizabeth Shriberg." ],
      "venue" : "Ph.D. thesis, University of California, Berkeley.",
      "citeRegEx" : "Shriberg.,? 1994",
      "shortCiteRegEx" : "Shriberg.",
      "year" : 1994
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "arXiv preprint arXiv:1706.03762.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Multi-task self-supervised learning for disfluency detection",
      "author" : [ "Shaolei Wang", "Wangxiang Che", "Qi Liu", "Pengda Qin", "Ting Liu", "William Yang Wang." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9193–9200.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Disfluency detection using a bidirectional lstm",
      "author" : [ "Vicky Zayats", "Mari Ostendorf", "Hannaneh Hajishirzi." ],
      "venue" : "arXiv preprint arXiv:1604.03209.",
      "citeRegEx" : "Zayats et al\\.,? 2016",
      "shortCiteRegEx" : "Zayats et al\\.",
      "year" : 2016
    }, {
      "title" : "Detecting speech repairs incrementally using a noisy channel approach",
      "author" : [ "Simon Zwarts", "Mark Johnson", "Robert Dale." ],
      "venue" : "Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1371–1378.",
      "citeRegEx" : "Zwarts et al\\.,? 2010",
      "shortCiteRegEx" : "Zwarts et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Interactions with these conversational agents have been shown to contain observable indicators of cognitive states, such as the rate of filled pauses and different temporal and turnrelated features (Gratch et al., 2014).",
      "startOffset" : 198,
      "endOffset" : 219
    }, {
      "referenceID" : 23,
      "context" : "Disfluency markers can be key features for identifying certain cognitive disorders for application in conversational agents (Rohanian et al., 2020).",
      "startOffset" : 124,
      "endOffset" : 147
    }, {
      "referenceID" : 9,
      "context" : "delivering its best decisions as early as possible following the principles outlined in (Hough and Purver, 2014).",
      "startOffset" : 88,
      "endOffset" : 112
    }, {
      "referenceID" : 26,
      "context" : "Transformers (Vaswani et al., 2017), for instance, operate on representations that do not naturally have an organizing principle of linear word order.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 5,
      "context" : "They are almost exclusively conducted on pre-segmented utterances of the Switchboard corpus of telephone conversations (Godfrey et al., 1992).",
      "startOffset" : 119,
      "endOffset" : 141
    }, {
      "referenceID" : 28,
      "context" : "Term Memory (BiLSTM) networks and Transformers, in which the sequences are available in full (Zayats et al., 2016; Lou and Johnson, 2020; Wang et al., 2020).",
      "startOffset" : 93,
      "endOffset" : 156
    }, {
      "referenceID" : 14,
      "context" : "Term Memory (BiLSTM) networks and Transformers, in which the sequences are available in full (Zayats et al., 2016; Lou and Johnson, 2020; Wang et al., 2020).",
      "startOffset" : 93,
      "endOffset" : 156
    }, {
      "referenceID" : 27,
      "context" : "Term Memory (BiLSTM) networks and Transformers, in which the sequences are available in full (Zayats et al., 2016; Lou and Johnson, 2020; Wang et al., 2020).",
      "startOffset" : 93,
      "endOffset" : 156
    }, {
      "referenceID" : 21,
      "context" : "Incremental dependency parsing combined with the removal of disfluency was also studied (Rasooli and Tetreault, 2015).",
      "startOffset" : 88,
      "endOffset" : 117
    }, {
      "referenceID" : 11,
      "context" : "ance segmentation tags by (Hough and Schlangen, 2017), showing an improvement over the performance of the individual tasks, and (Rohanian and Hough, 2020) show an improvement in both tasks when framed as a multi-task learning (MTL) set-up",
      "startOffset" : 26,
      "endOffset" : 53
    }, {
      "referenceID" : 22,
      "context" : "ance segmentation tags by (Hough and Schlangen, 2017), showing an improvement over the performance of the individual tasks, and (Rohanian and Hough, 2020) show an improvement in both tasks when framed as a multi-task learning (MTL) set-up",
      "startOffset" : 128,
      "endOffset" : 154
    }, {
      "referenceID" : 25,
      "context" : "Disfluencies are generally assumed to have a reparandum-interregnum-repair structure in their fullest form as speech repairs (Shriberg, 1994; Meteer et al., 1995).",
      "startOffset" : 125,
      "endOffset" : 162
    }, {
      "referenceID" : 16,
      "context" : "Disfluencies are generally assumed to have a reparandum-interregnum-repair structure in their fullest form as speech repairs (Shriberg, 1994; Meteer et al., 1995).",
      "startOffset" : 125,
      "endOffset" : 162
    }, {
      "referenceID" : 3,
      "context" : "tecture (Devlin et al., 2019) with a Conditional Random Field (CRF) output architecture to tag sequences with tags such as those in the top line of Fig.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 22,
      "context" : "We use the word-by-word segmentation system from (Rohanian and Hough, 2020) where four output tags define ranges of transcribed words or word hypotheses using a BIES tag scheme (Beginning, Inside, End, and Single) to allow for the prediction of an utterance ending.",
      "startOffset" : 49,
      "endOffset" : 75
    }, {
      "referenceID" : 2,
      "context" : "Based on (Dalvi et al., 2018), we present two methods to address this issue: chunk-based training and add-M training.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 8,
      "context" : "given an input text passage as context, is to produce text that constitutes a cohesive continuation (Holtzman et al., 2019).",
      "startOffset" : 100,
      "endOffset" : 123
    }, {
      "referenceID" : 15,
      "context" : "Inspired by (Madureira and Schlangen, 2020), using the GPT-2 language model (Radford et al.",
      "startOffset" : 12,
      "endOffset" : 43
    }, {
      "referenceID" : 20,
      "context" : "Inspired by (Madureira and Schlangen, 2020), using the GPT-2 language model (Radford et al., 2019), we first give each",
      "startOffset" : 76,
      "endOffset" : 98
    }, {
      "referenceID" : 4,
      "context" : "In the Top-k sampling, the most probable next k words are extracted and the probability mass is redistributed between only the following k words (Fan et al., 2018).",
      "startOffset" : 145,
      "endOffset" : 163
    }, {
      "referenceID" : 8,
      "context" : "Top-p sampling Rather than selecting only the most probable K words, in Top-p sampling, we select the smallest possible range of words with their total likelihood exceeds the probability p (Holtzman et al., 2019).",
      "startOffset" : 189,
      "endOffset" : 212
    }, {
      "referenceID" : 11,
      "context" : "We used the data from (Hough and Schlangen, 2017) for ASR hypotheses – this was generated",
      "startOffset" : 22,
      "endOffset" : 49
    }, {
      "referenceID" : 1,
      "context" : "for our task, does not filter out hesitation markers or disfluencies (Baumann et al., 2017).",
      "startOffset" : 69,
      "endOffset" : 91
    }, {
      "referenceID" : 17,
      "context" : "The word embedding for LSTM was initialised with 50-dimensional embedding trained on Google News (Mikolov et al., 2013).",
      "startOffset" : 97,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "In our analysis, when fine-tuning BERT, we followed the hyper-parameters of (Devlin et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 97
    }, {
      "referenceID" : 3,
      "context" : "Since the datasets we use are tokenized, and each token has a matching tag, we adopt the directions provided by (Devlin et al., 2019) to deal",
      "startOffset" : 112,
      "endOffset" : 133
    }, {
      "referenceID" : 29,
      "context" : "We use the first time to detection (FTD) metric of (Zwarts et al., 2010) for latency: the average latency (in number of words) before the first detection of a gold standard repair onset or edit term word.",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 0,
      "context" : "(Baumann et al., 2011), the proportion of the unnecessary edits (insertions and deletions) required to achieve the final labels produced by the model, with perfect performance being 0%.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 19,
      "context" : "The reparandum is detected by a backward search, showing robustness for longer lengths of repair compared to deep sequence tagging models (Purver et al., 2018).",
      "startOffset" : 138,
      "endOffset" : 159
    }, {
      "referenceID" : 10,
      "context" : "RNN (HS’15): (Hough and Schlangen, 2015)’s RNN-based model, the first deep learning-based",
      "startOffset" : 13,
      "endOffset" : 40
    }, {
      "referenceID" : 9,
      "context" : "compared to an explicit backtracking mechanism like (Hough and Purver, 2014).",
      "startOffset" : 52,
      "endOffset" : 76
    } ],
    "year" : 2021,
    "abstractText" : "While Transformer-based text classifiers pretrained on large volumes of text have yielded significant improvements on a wide range of computational linguistics tasks, their implementations have been unsuitable for live incremental processing thus far, operating only on the level of complete sentence inputs. We address the challenge of introducing methods for word-by-word left-to-right incremental processing to Transformers such as BERT, models without an intrinsic sense of linear order. We modify the training method and live decoding of non-incremental models to detect speech disfluencies with minimum latency and without pre-segmentation of dialogue acts. We experiment with several decoding methods to predict the rightward context of the word currently being processed using a GPT-2 language model and apply a BERT-based disfluency detector to sequences, including predicted words. We show our method of incrementalising Transformers maintains most of their high non-incremental performance while operating strictly incrementally. We also evaluate our models’ incremental performance to establish the trade-off between incremental performance and final performance, using different prediction strategies. We apply our system to incremental speech recognition results as they arrive into a live system and achieve state-of-the-art results in this setting.",
    "creator" : "LaTeX with hyperref"
  }
}