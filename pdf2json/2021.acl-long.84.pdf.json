{
  "name" : "2021.acl-long.84.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "The Art of Abstention: Selective Prediction and Error Regularization for Natural Language Processing",
    "authors" : [ "Ji Xin", "Raphael Tang", "Yaoliang Yu", "Jimmy Lin", "David R. Cheriton" ],
    "emails" : [ "ji.xin@uwaterloo.ca", "r33tang@uwaterloo.ca", "yaoliang.yu@uwaterloo.ca", "jimmylin@uwaterloo.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1040–1051\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1040"
    }, {
      "heading" : "1 Introduction",
      "text" : "Recent advances in deep learning models have pushed the frontier of natural language processing (NLP). Pre-trained language models based on the transformer architecture (Vaswani et al., 2017) have improved the state-of-the-art results on many NLP applications. Naturally, these models are deployed in various real-world applications. However, one may wonder whether they are always reliable, as pointed out by Guo et al. (2017) that modern neural networks, while having better accuracy, tend to be overconfident compared to simple networks from 20 years ago.\nIn this paper, we study the problem of selective prediction (Geifman and El-Yaniv, 2017) in NLP. Under the setting of selective prediction, a model is allowed to abstain from making predictions on uncertain examples (Figure 1) and thereby reduce\nthe error rate. This is a practical setting in a lot of realistic scenarios, such as making entailment judgments for breaking news articles in search engines (Carlebach et al., 2020) and making critical predictions in medical and legal documents (Zhang et al., 2019). In these cases, it is totally acceptable, if not desirable, for the models to admit their uncertainty and call for help from humans or better (but more costly) models.\nUnder the selective prediction setting, we construct a selective classifier by pairing a standard classifier with a confidence estimator. The confidence estimator measures how confident the model is for a certain example, and instructs the classifier to abstain on uncertain ones. Naturally, a good confidence estimator should have higher confidence for correctly classified examples than incorrect ones. We consider two choices of confidence estimators, softmax response (SR; Hendrycks and Gimpel, 2017), and Monte-Carlo dropout (MC-dropout; Gal and Ghahramani, 2016). SR interprets the output of the final softmax layer as a probability distribution and the highest probability as confidence. MC-dropout repeats the inference process multiple times, each time with a different dropout mask, and treats the negative variance of maximum probability as confidence. Confidence estimation is critical to selective prediction, and therefore studying this problem also helps relevant tasks such as active\nlearning (Cohn et al., 1995; Shen et al., 2018) and early exiting (Schwartz et al., 2020; Xin et al., 2020; Zhou et al., 2020; Xin et al., 2021).\nIn this paper, we compare selective prediction performance of different NLP models and confidence estimators. We also propose a simple trick, error regularization, which can be applied to any of these models and confidence estimators, and improve their selective prediction performance. We further study the application of selective prediction on a variety of interesting applications, such as classification with no valid labels (no-answer problem) and using classifier cascades for accuracy– efficiency trade-offs. Experiments show that recent powerful NLP models such as BERT (Devlin et al., 2019) and ALBERT (Lan et al., 2020) improve not only accuracy but also selective prediction performance; they also demonstrate the effectiveness of the proposed error regularization by producing better confidence estimators which reduce the area under the risk–coverage curve by 10%."
    }, {
      "heading" : "2 Related Work",
      "text" : "Selective prediction has been studied by the machine learning community for a long time (Chow, 1957; El-Yaniv and Wiener, 2010). More recently, Geifman and El-Yaniv (2017, 2019) study selective prediction for modern deep learning models, though with a focus on computer vision tasks.\nSelective prediction is closely related to confidence estimation, as well as out-of-domain (OOD) detection (Schölkopf et al., 2000; Liang et al., 2018) and prediction error detection (Hendrycks and Gimpel, 2017), albeit more remotely. There have been many different methods for confidence estimation. Bayesian methods such as Markov Chain Monte Carlo (Geyer, 1992) and Variational Inference (Hinton and Van Camp, 1993; Graves, 2011) assume a prior distribution over model parameters and obtain confidence estimates through the posterior. Ensemble-based methods (Gal and Ghahramani, 2016; Lakshminarayanan et al., 2017; Geifman et al., 2019) estimate confidence based on statistics of the ensemble model’s output. These methods, however, are computationally practical for small models only. Current large-scale pre-trained NLP models, such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), are too expensive to run multiple times of inference, and therefore require lightweight confidence estimation.\nPreviously, selective prediction and confidence\nestimation have been studied in limited NLP scenarios. Dong et al. (2018) train a separate confidence scoring model to explicitly estimate confidence in semantic parsing. Kamath et al. (2020) introduce selective prediction for OOD question answering, where abstention is allowed for OOD and difficult questions. However, selective prediction for broader NLP applications has yet to be explored, and we hope to draw the attention of the NLP community to this problem.\nThere are two notable related topics, confidence calibration and unanswerable questions, but the difference between them and selective prediction is still nontrivial. Calibration (Guo et al., 2017; Jiang et al., 2018; Kumar et al., 2018; Wang et al., 2020; Desai and Durrett, 2020) focuses on adjusting the overall confidence level of a model, while selective prediction is based on relative confidence among the examples. For example, the most widely used calibration technique, temperature scaling (Platt, 1999), globally increases or decreases the model’s confidence on all examples, but the ranking of all examples’ confidence is unchanged. Unanswerable questions are considered in previous datasets, e.g., SQuAD2.0 (Rajpurkar et al., 2018). The unanswerable questions are impossible to answer even for humans, while abstention in selective prediction is due to model uncertainty rather than modelagnostic data uncertainty."
    }, {
      "heading" : "3 Background",
      "text" : "We introduce relevant concepts about selective prediction and confidence estimators, using multiclass classification as an example."
    }, {
      "heading" : "3.1 Selective Prediction",
      "text" : "Given a feature space X and a set of labels Y , a standard classifier f is a function f : X → Y . A selective classifier is another function h : X → Y ∪ {⊥}, where ⊥ is a special label indicating the abstention of prediction. Normally, the selective classifier is composed of a pair of functions h = (f, g), where f is a standard classifier and g is the selective function g : X → {0, 1}. Given an input x ∈ X , the output of the selective classifier is as follows:\nh(x) = { f(x), if g(x) = 1, ⊥, if g(x) = 0,\n(1)\nand we can see that the output of g controls prediction or abstention. In most cases, g consists of a\nconfidence estimator g̃ : X → R, and a confidence threshold θ:\ng(x) = 1[g̃(x) > θ]. (2)\ng̃(x) indicates how confident the classifier f is on the example x, and θ controls the overall prediction versus abstention level.\nA selective classifier makes trade-offs between coverage and risk. Given a labeled dataset S = {(xi, yi)}ni=1 ⊂ X × Y and an error function L to calculate each example’s error li = L(f(xi), yi), the coverage and the selective risk of a classifier h = (f, g) on S are, respectively,\nγ(h) = 1 |S| ∑\n(xi,yi)∈S\ng(xi), (3)\nr(h) = ∑ (xi,yi)∈S g(xi)li∑ (xi,yi)∈S g(xi) . (4)\nThe selective classifier aims to minimize the selective risk at a given coverage.\nThe performance of a selective classifier h = (f, g) can be evaluated by the risk–coverage curve (RCC; El-Yaniv and Wiener, 2010), which is drawn by varying the confidence threshold θ (see Figure 2 for an example). Quantitatively, the area under curve (AUC) of RCC measures the effectiveness of a selective classifier.1\nIn order to minimize the AUC of RCC, the selective classifier should, intuitively, output g(x) = 1 for correctly classified examples and g(x) = 0 for incorrect ones. Therefore, an ideal g̃ has the following property: ∀(xi, yi), (xj , yj) ∈ S, g̃(xi) ≤ g̃(xj) iff li ≥ lj . We propose the following metric, reversed pair proportion (RPP), to evaluate how far the confidence estimator g̃ is to ideal, given the labeled dataset S of size n:\nRPP =\nn∑ 1≤i,j≤n 1[g̃(xi) < g̃(xj), li < lj ]\nn2 . (5)\nRPP measures the proportion of example pairs with a reversed confidence–error relationship, and the n2 in the denominator is used to normalize the value. An ideal confidence estimator has an RPP value of 0."
    }, {
      "heading" : "3.2 Confidence Estimators",
      "text" : "In most cases for multi-class classification, the last layer of the classifier is a softmax activation, which\n1AUC in this paper always corresponds to RCCs.\noutputs a probability distribution P (y) over the set of labels Y , where y ∈ Y is a label. In this case, the classifier can be written as\nf(x) = ŷ = arg max y∈Y P (y), (6)\nwhere ŷ is the label with highest probability. Perhaps the most straightforward and popular choice for the confidence estimator is softmax response (Hendrycks and Gimpel, 2017):\ng̃SR(x) = P (ŷ) = max y∈Y P (y). (7)\nAlternatively, we can use the difference between probabilities of the top two classes for confidence estimation. We refer to this method as PD (probability difference).\nGal and Ghahramani (2016) argue that “softmax outputs are often erroneously interpreted as model confidence”, and propose to use MC-dropout as the confidence estimator. In MC-dropout, P (ŷ) is computed for a total of R times, using a different dropout mask at each time, producing P1(ŷ), P2(ŷ), · · · , PR(ŷ). The variance of them is used to estimate the confidence:\ng̃MC(x) = −Var[P1(ŷ), · · · , PR(ŷ)]. (8)\nWe use the negative sign here because a larger variance indicates a greater uncertainty, i.e., a lower confidence (Geifman and El-Yaniv, 2017; Kamath et al., 2020). By using different dropout masks, MC-dropout is equivalent to using an ensemble for confidence estimation, but does not require actually training and storing multiple models. Nevertheless, compared to SR, the inference cost of MC-dropout is multiplied by R, which can be a problem when model inference is expensive."
    }, {
      "heading" : "4 Error Regularization",
      "text" : ""
    }, {
      "heading" : "4.1 Regularizers",
      "text" : "SR and MC-dropout are often used directly out of the box as the confidence estimator. We propose a simple regularization trick that can be easily applied at training (or fine-tuning for pre-trained models) time and can improve the effectiveness of the induced confidence estimators.\nConsidering that a good confidence estimator should minimize RPP defined in Equation 5, we\nadd the following regularizer to the original training loss function:\nLtotal = n∑\ni=1\nH(f(xi), yi) + λLreg, (9)\nLreg = ∑\n1≤i,j≤n ∆i,j 1[ei > ej ], (10)\n∆i,j = max{0, g̃SR(xi)− g̃SR(xj)}2. (11)\nHere, H(·, ·) is the task-specific loss function such as cross entropy (H is not the same with the error function L), λ is the hyperparameter for regularization, g̃SR is the maximum softmax probability defined in Equation 7, and ei is the error of example i at the current iteration—details to calculate it will be explained in the next paragraph. We use SR confidence here because it is easily accessible at training time, while MC-dropout confidence is not. The intuition of this regularizer is as follows: if the model’s error on example i is larger than its error on example j (i.e., example i is considered more “difficult” for the model), then the confidence on example i should not be greater than the confidence on example j.\nIn practice, at each iteration of training (finetuning), we can obtain the error ei in one of the two following ways.\n• Current iteration error We simply use the error function L to calculate the error of the example at the current iteration, and use it as ei. In the case of multi-class classification, L is often chosen as the 0–1 error.\n• History record error Since we intend to use ei to quantify how difficult an example\nis, we draw inspiration from forgettable examples (Toneva et al., 2019). We calculate example error with L throughout the training process, and use the error averaged from the beginning to the current iteration as ei. In this case, ei takes value from [0, 1]."
    }, {
      "heading" : "4.2 Practical Approximations",
      "text" : "In practice, it is computationally prohibitive to either strictly compute Lreg from Equation 10 for all example pairs, or to calculate history record error after every iteration. We therefore make the following two approximations.\nFor Lreg from Equation 10, we only consider examples from the mini-batch of the current iteration. For current iteration error, where ei takes value from {0, 1}, we consider all pairs where ei = 1 and ej = 0. For history record error, where ei takes value from [0, 1], we sort all examples in the mini-batch by their errors, and divide the minibatch into 20% of examples with high error values and 80% of examples with low error values;2 then we consider all pairs where example i is from the former 20% and j from the latter 80%.\nFor calculating history record error, we compute and record the error values for the entire training set 10 times per epoch (once after each 10% iterations). At each training iteration, we use the average of error values recorded so far as ei."
    }, {
      "heading" : "5 Experiments",
      "text" : "We conduct experiments of selective prediction on NLP tasks. Since the formulation of selective prediction is model agnostic, we choose the\n2We choose this 20–80 division to mimic the current iteration error case, where roughly 20% of training examples have an error of 1 and 80% have an error of 0.\nfollowing representative models: (1) BERT-base and BERT-large (Devlin et al., 2019), the dominant transformer-based models of recent years; (2) ALBERT-base (Lan et al., 2020), a variant of BERT featuring parameter sharing and memory efficiency; (3) Long Short-Term Memory (LSTM; Hochreiter and Schmidhuber, 1997), the popular pre-transformer model that is lightweight and fast.\nIn this section, we compare the performance of selective prediction of these models, demonstrate the effectiveness of the proposed error regularization, and show the application of selective prediction in two interesting scenarios—the no-answer problem and the classifier cascades."
    }, {
      "heading" : "5.1 Experiment Setups",
      "text" : "We conduct experiments mainly on three datasets: MRPC (Dolan and Brockett, 2005), QNLI (Wang et al., 2018), and MNLI (Williams et al., 2018). In Section 5.4, we will need an additional non-binary dataset SST-5 (Socher et al., 2013). Statistics of these datasets can be found in Table 2. Following the setting of the GLUE benchmark (Wang et al., 2018), we use the training set for training/finetuning and the development set for evaluation (the\ntest set’s labels are not publicly available); MNLI’s development set has two parts, matched and mismatched (m/mm). These datasets include semantic equivalence judgments, entailment classification, and sentiment analysis, which are important application scenarios for selective prediction as discussed in Section 1.\nThe implementation is based on PyTorch (Paszke et al., 2019) and the Huggingface Transformers Library (Wolf et al., 2020). Training/fine-tuning and inference are done on a single NVIDIA Tesla V100 GPU. Since we are evaluating the selective prediction performance of different models instead of pursuing state-of-the-art results, we do not extensively tune hyperparameters; instead, most experiment settings such as hidden sizes, learning rates, and batch sizes are kept unchanged from the Huggingface Library. Further setup details can be found in Appendix A."
    }, {
      "heading" : "5.2 Comparing Different Models",
      "text" : "We compare selective prediction performance of different models in Table 1. For each model, we report the performance given by the two confidence estimators, softmax response (SR) and MC-dropout (MC); the results of using PD for confidence estimation are very similar to those of SR, and we report them in Appendix B due to space limitations. The accuracy and the F1 score3 measure the effectiveness of the classifier f , RPP measures the reliability of the confidence estimator g̃, and AUC is a comprehensive metric for both the classifier and the confidence estimator. The choice of confidence estimator does not affect the model’s accuracy. We also provide risk–coverage curves (RCCs) of different models and confidence estima-\n3We henceforth refer to both accuracy and F1 scores simply as accuracy for the sake of conciseness.\ntors in Figure 2. MC in the table and the figure uses a dropout rate of 0.01 and repetitive runs R = 10.\nWe first notice that models with overall higher accuracy also have better selective prediction performance (lower AUC and RPP). For example, compared with LSTM, BERT-base has higher accuracy and lower AUC/RPP on all datasets, and the same applies to the comparison between BERT-base and BERT-large. Since the classifier’s effectiveness does not directly affect RPP, the consistency of RPP’s and accuracy’s improvement indicates that sophisticated models simultaneously improve both model accuracy and confidence estimation. This is in contrast to the discovery by Guo et al. (2017) that sophisticated neural networks, despite having better accuracy, are more easily overconfident and worse calibrated than simple ones.\nWe also notice that MC-dropout performs consistently worse than softmax response, shown by\nboth AUC and RPP. This shows that for NLP tasks and models, model confidence estimated by MCdropout fails to align well with real example difficulty. We further study and visualize in Figure 3 the effect of different dropout rates and different numbers of repetitive runs R on MC-dropout’s selective prediction performance. We can see that (1) a dropout rate of 0.01 is a favorable choice: larger dropout rates lead to worse performance while smaller ones do not improve it; (2) MC-dropout needs at least 20 repetitions to obtain results comparable to SR, which is extremely expensive. Although MC-dropout has a sound theoretical foundation, its practical application to NLP tasks needs further improvements."
    }, {
      "heading" : "5.3 Effect of Error Regularization",
      "text" : "In this part, we show that our simple regularization trick improves selective prediction performance. In\nTable 3, we report the accuracy, AUC, and RPP for each model, paired with three different regularizers: no regularization (none), current error regularizer (curr.), and history error regularizer (hist.), as described in Section 4.\nWe first see that applying error regularization (either current or history) does not harm model accuracy. There are minor fluctuations, but generally speaking, error regularization has no negative effect on the models’ effectiveness.\nWe can also see that error regularization improves models’ selective prediction performance, reducing AUC and RPP. As we mention in the previous section, AUC is a comprehensive metric for both the classifier f and the confidence estimator g̃. We therefore focus on this metric in this section, and we bold the lowest AUC in Table 3. We see that error regularization consistently achieve the lowest AUC values, and on average, the best scores are approximately 10% lower than the scores without regularization. This shows that error regularization produces confidence estimators that give better confidence rankings.\nThe two regularization methods, current error and history error, are similar in quality, with neither outperforming the other across all models and datasets. Therefore, we can conclude only that the error regularization trick improves selective prediction, but the best specific method varies. We leave this exploration for future work."
    }, {
      "heading" : "5.4 The No-Answer Problem",
      "text" : "In this section, we conduct experiments to see how selective classifiers perform on datasets that either allow abstention or, equivalently, provide the no-answer label. This no-answer problem occurs\nwhenever a trained classifier encounters an example whose label is unseen in training, which is common in practice. For example, in the setting of ultrafine entity typing with more than 10,000 labels (Choi et al., 2018), it is unsurprising to encounter examples with unseen types. Ideally, in this case, the classifier should choose the no-answer label. This setting is important yet often neglected, and there exist few classification datasets with the no-answer label. We therefore build our own datasets, binarized MNLI and SST-5 (bMNLI and bSST-5), to evaluate different models in this setting (Table 2).\nThe MNLI dataset is for sentence entailment classification. Given a pair of sentences, the goal is to predict the relationship between them, among three labels: entailment, contradiction, and neutral. The SST-5 dataset is for fine-grained sentence sentiment classification. Given a sentence, the goal is to predict the sentiment of it, among five labels: strongly positive, mildly positive, strongly negative, mildly negative, and neutral. To convert the original MNLI and SST-5 datasets into our binarized versions bMNLI and bSST-5, we modify the following: for SST-5, we merge strongly and mildly positive/negative into one positive/negative class; for MNLI, we simply regard entailment as positive and contradictory as negative. We then remove all neutral instances from the training set but keep those in the development and test sets. This way, neutral instances in the development and test sets should be classified as no-answer by the model. A good model is expected to assign neutral examples in the development and test sets with low confidence scores, thereby predicting the no-answer label for them.\nWe report results for these two datasets with\nthe no-answer label in Table 4. Accuracy (Acc), AUC, and RPP have the same meaning from the previous sections. We also consider a new metric specifically for the no-answer setting, augmented accuracy (Acc*), which is calculated as follows: (1) we make a number of attempts by searching a threshold α from 0.7 to 1.0 in increments of 0.01; (2) for each attempt, we regard all examples with predicted confidence lower than α as neutral, and then calculate the accuracy; (3) among all attempts, we take the highest accuracy as Acc*. Choosing the optimal α requires knowing the ground-truth answers in advance and is not practical in reality.4 Instead, Acc* indicates how well a model recognizes examples whose label is likely unseen in the training set.\nWe first see that Acc* is consistently higher than Acc in all cases. This is unsurprising, but it demonstrates that unseen samples indeed have lower confidence and shows that introducing the abstention option is beneficial in the no-answer scenario. Also, we observe that error regularization improves the models’ selective prediction performance, producing lower AUC/RPP and higher Acc* in most cases. This further demonstrates the effectiveness of the simple error regularization trick.\nSecondly, we can see that the improvement of Acc* over Acc is larger in bMNLI than in bSST-5. The reason is that in bMNLI, neutral examples constitute about a third of the entire development set, while in bSST-5 they constitute only a fifth. The\n4Alternatively, one may use a validation set to choose the optimal α. In our experiments, however, we use the development set for evaluation, since the labels of the test set itself are not publicly available. Holding out a part of the training set for validation is left for future exploration.\nimprovement is positively correlated with the proportion of neutral examples, since they are assigned lower confidence scores and provide the potential for abstention-based improvements."
    }, {
      "heading" : "5.5 Classifier Cascades",
      "text" : "In this section, we show how confidence estimation and abstention can be used for accuracy–efficiency trade-offs. We use classifier cascades: we first use a less accurate classifier for prediction, abstain on examples with low confidence, then send them to more accurate but more costly classifiers. Here we choose LSTM and BERT-base to constitute the cascade, but one can also choose other models and more levels of classifiers.\nWe first use an LSTM for all examples’ inference, and then send “difficult” ones to BERT-base. Since the computational cost of LSTM is negligible5 compared to BERT-base, the key to efficiency here is correctly picking the “difficult” examples.\nIn Figure 4, we show the results of accuracy/F1 score versus average FLOPs6 per inference example. Each curve represents a method to choose difficult examples: The blue curves are obtained by randomly selecting examples, as a simple baseline. The orange and green curves are obtained by using SR of LSTM as the indicator of example difficulty; the orange curves represent the LSTM trained with no regularization while the green curves are with history error regularization. Different points on the curves are chosen by varying the proportion of examples sent to the more accurate model, BERT-\n5BERT-base’s cost is ∼ 105 times larger than LSTM here. 6We use the torchprofile toolkit to measure multiply– accumulate operations (MACs), and then double the number to obtain floating point operations (FLOPs).\nbase. A curve with a larger area under it indicates a better accuracy–efficiency trade-off.\nWe can see that the blue curves are basically linear interpolations between the LSTM (the lowerleft dot) and BERT-base (the upper-right dot), and this is expected for random selection. Orange and green curves are concave, indicating that using SR for confidence estimation is, unsurprisingly, more effective than random selection. Between these two, the green curves (history error regularization) have larger areas under themselves than orange ones (no regularization), i.e., green curves have better accuracy given the same FLOPs. This demonstrates the effectiveness of error regularization for better confidence estimation."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we introduce the problem of selective prediction for NLP. We provide theoretical background and evaluation metrics for the problem, and also propose a simple error regularization method that improves selective prediction performance for NLP models. We conduct experiments to compare different models under the selective prediction setting, demonstrate the effectiveness of the proposed regularization trick, and study two scenarios where selective prediction and the error regularization method can be helpful.\nWe summarize interesting experimental observations as follows:\n1. Recent sophisticated NLP models not only improve accuracy over simple models, but also provide better selective prediction results (better confidence estimation).\n2. MC-dropout, despite having a solid theoretical foundation, has difficulties matching the effectiveness of simple SR in practice.\n3. The simple error regularization helps models lower their AUC and RPP, i.e., models trained with it produce better confidence estimators.\n4. Selective prediction can be applied to scenarios where estimating example difficulties is necessary. In these cases, our proposed error regularization trick can also be helpful, such as providing better accuracy–efficiency trade-offs.\nFuture Work (1) Despite the effectiveness of the proposed error regularization trick, we are not certain on the best way for computing the error\n(current or history); it is important to unify them into one method that consistently does well. (2) We have only covered a selection of NLP tasks, and there are still other unexplored categories: tokenlevel classification such as named entity recognition and question answering, sequence generation such as summarization and translation, and so on; it would be interesting to extend selective prediction to these problems. (3) There exists another setting for selective prediction where abstention induces a fixed cost (Bartlett and Wegkamp, 2008) and the goal is to minimize the overall cost instead of AUC; it would also be interesting to investigate this setting for NLP applications."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank anonymous reviewers for their constructive suggestions. This research is supported in part by the Canada First Research Excellence Fund and the Natural Sciences and Engineering Research Council (NSERC) of Canada."
    }, {
      "heading" : "A Detailed Experiment Settings",
      "text" : "The LSTM is randomly initialized without pretraining. For models that require pre-trained, we use the following ones provided by the Huggingface Transformer Library (Wolf et al., 2020).\n• BERT-BASE-UNCASED\n• BERT-LARGE-UNCASED\n• ALBERT-BASE-V2\nAll these models are trained/fine-tuned for 3 epochs without early-stopping or checkpoint selection. Learning rate is 2× 10−5. A batch size of 32 is used for training/fine-tuning. The maximum input sequence length is 128. Choices for the regularization hyperparameter λ from Equation 9 are shown in Table 6.\nThe numbers of parameters for the two models BERT and ALBERT can be found in the paper by Lan et al. (2020).\nThe LSTM used in the paper is a two-layer bidirectional LSTM, with a hidden size of 200. On top of it there is a max-pooling layer and a fullyconnected layer."
    }, {
      "heading" : "B PD Confidence Estimator",
      "text" : "Probability difference (PD), the difference between probabilities of the top two classes, can also be used as confidence estimation. Among the four datasets used in the paper, MRPC and QNLI are\nbinary classification, and therefore PD’s results are identical to softmax response (SR). SST-5 and MNLI have more than two classes, and therefore PD’s results are different from SR’s. We show them in Table 5.\nWe can see that the results of PD are very similar to those of SR. Of course, MNLI and SST-5 have only three/five labels respectively, and for datasets with far more labels, PD will possibly show its difference from SR."
    } ],
    "references" : [ {
      "title" : "Classification with a reject option using a hinge loss",
      "author" : [ "Peter L. Bartlett", "Marten H. Wegkamp." ],
      "venue" : "Journal of Machine Learning Research, 9(59).",
      "citeRegEx" : "Bartlett and Wegkamp.,? 2008",
      "shortCiteRegEx" : "Bartlett and Wegkamp.",
      "year" : 2008
    }, {
      "title" : "News aggregation with diverse viewpoint identification using neural embeddings and semantic understanding models",
      "author" : [ "Mark Carlebach", "Ria Cheruvu", "Brandon Walker", "Cesar Ilharco Magalhaes", "Sylvain Jaume." ],
      "venue" : "Proceedings of the 7th Workshop on Ar-",
      "citeRegEx" : "Carlebach et al\\.,? 2020",
      "shortCiteRegEx" : "Carlebach et al\\.",
      "year" : 2020
    }, {
      "title" : "Ultra-fine entity typing",
      "author" : [ "Eunsol Choi", "Omer Levy", "Yejin Choi", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 87–96, Melbourne, Australia. Associa-",
      "citeRegEx" : "Choi et al\\.,? 2018",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2018
    }, {
      "title" : "An optimum character recognition system using decision functions",
      "author" : [ "Chi-Keung Chow." ],
      "venue" : "IRE Transactions on Electronic Computers, (4).",
      "citeRegEx" : "Chow.,? 1957",
      "shortCiteRegEx" : "Chow.",
      "year" : 1957
    }, {
      "title" : "Active learning with statistical models",
      "author" : [ "David Cohn", "Zoubin Ghahramani", "Michael Jordan." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 7. MIT Press.",
      "citeRegEx" : "Cohn et al\\.,? 1995",
      "shortCiteRegEx" : "Cohn et al\\.",
      "year" : 1995
    }, {
      "title" : "Calibration of pre-trained transformers",
      "author" : [ "Shrey Desai", "Greg Durrett." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 295–302, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Desai and Durrett.,? 2020",
      "shortCiteRegEx" : "Desai and Durrett.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatically constructing a corpus of sentential paraphrases",
      "author" : [ "William B. Dolan", "Chris Brockett." ],
      "venue" : "Proceedings of the Third International Workshop on Paraphrasing (IWP2005).",
      "citeRegEx" : "Dolan and Brockett.,? 2005",
      "shortCiteRegEx" : "Dolan and Brockett.",
      "year" : 2005
    }, {
      "title" : "Confidence modeling for neural semantic parsing",
      "author" : [ "Li Dong", "Chris Quirk", "Mirella Lapata." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 743–753, Melbourne, Australia. As-",
      "citeRegEx" : "Dong et al\\.,? 2018",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2018
    }, {
      "title" : "On the foundations of noise-free selective classification",
      "author" : [ "Ran El-Yaniv", "Yair Wiener." ],
      "venue" : "Journal of Machine Learning Research, 11(53).",
      "citeRegEx" : "El.Yaniv and Wiener.,? 2010",
      "shortCiteRegEx" : "El.Yaniv and Wiener.",
      "year" : 2010
    }, {
      "title" : "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
      "author" : [ "Yarin Gal", "Zoubin Ghahramani." ],
      "venue" : "Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning",
      "citeRegEx" : "Gal and Ghahramani.,? 2016",
      "shortCiteRegEx" : "Gal and Ghahramani.",
      "year" : 2016
    }, {
      "title" : "Selective classification for deep neural networks",
      "author" : [ "Yonatan Geifman", "Ran El-Yaniv." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
      "citeRegEx" : "Geifman and El.Yaniv.,? 2017",
      "shortCiteRegEx" : "Geifman and El.Yaniv.",
      "year" : 2017
    }, {
      "title" : "SelectiveNet: A deep neural network with an integrated reject option",
      "author" : [ "Yonatan Geifman", "Ran El-Yaniv." ],
      "venue" : "Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Re-",
      "citeRegEx" : "Geifman and El.Yaniv.,? 2019",
      "shortCiteRegEx" : "Geifman and El.Yaniv.",
      "year" : 2019
    }, {
      "title" : "Bias-reduced uncertainty estimation for deep neural classifiers",
      "author" : [ "Yonatan Geifman", "Guy Uziel", "Ran El-Yaniv." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Geifman et al\\.,? 2019",
      "shortCiteRegEx" : "Geifman et al\\.",
      "year" : 2019
    }, {
      "title" : "Practical markov chain monte carlo",
      "author" : [ "Charles J. Geyer." ],
      "venue" : "Statistical science, pages 473–483.",
      "citeRegEx" : "Geyer.,? 1992",
      "shortCiteRegEx" : "Geyer.",
      "year" : 1992
    }, {
      "title" : "Practical variational inference for neural networks",
      "author" : [ "Alex Graves." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 24. Curran Associates, Inc.",
      "citeRegEx" : "Graves.,? 2011",
      "shortCiteRegEx" : "Graves.",
      "year" : 2011
    }, {
      "title" : "On calibration of modern neural networks",
      "author" : [ "Chuan Guo", "Geoff Pleiss", "Yu Sun", "Kilian Q. Weinberger." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages",
      "citeRegEx" : "Guo et al\\.,? 2017",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2017
    }, {
      "title" : "A baseline for detecting misclassified and out-of-distribution examples in neural networks",
      "author" : [ "Dan Hendrycks", "Kevin Gimpel." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Hendrycks and Gimpel.,? 2017",
      "shortCiteRegEx" : "Hendrycks and Gimpel.",
      "year" : 2017
    }, {
      "title" : "Keeping the neural networks simple by minimizing the description length of the weights",
      "author" : [ "Geoffrey E. Hinton", "Drew Van Camp." ],
      "venue" : "Proceedings of the Sixth Annual Conference on Computational learning theory, pages 5–13.",
      "citeRegEx" : "Hinton and Camp.,? 1993",
      "shortCiteRegEx" : "Hinton and Camp.",
      "year" : 1993
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "To trust or not to trust a classifier",
      "author" : [ "Heinrich Jiang", "Been Kim", "Melody Guan", "Maya Gupta." ],
      "venue" : "S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Jiang et al\\.,? 2018",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2018
    }, {
      "title" : "Selective question answering under domain shift",
      "author" : [ "Amita Kamath", "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5684– 5696, Online. Association for Computational Lin-",
      "citeRegEx" : "Kamath et al\\.,? 2020",
      "shortCiteRegEx" : "Kamath et al\\.",
      "year" : 2020
    }, {
      "title" : "Trainable calibration measures for neural networks from kernel mean embeddings",
      "author" : [ "Aviral Kumar", "Sunita Sarawagi", "Ujjwal Jain." ],
      "venue" : "Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine",
      "citeRegEx" : "Kumar et al\\.,? 2018",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2018
    }, {
      "title" : "Simple and scalable predictive uncertainty estimation using deep ensembles",
      "author" : [ "Balaji Lakshminarayanan", "Alexander Pritzel", "Charles Blundell." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30, pages 6402–6413. Curran Asso-",
      "citeRegEx" : "Lakshminarayanan et al\\.,? 2017",
      "shortCiteRegEx" : "Lakshminarayanan et al\\.",
      "year" : 2017
    }, {
      "title" : "ALBERT: A lite BERT for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "Enhancing the reliability of out-of-distribution image detection in neural networks",
      "author" : [ "Shiyu Liang", "Yixuan Li", "R. Srikant." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Liang et al\\.,? 2018",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2018
    }, {
      "title" : "RoBERTa: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "PyTorch: An imperative style, high-performance deep learning library",
      "author" : [ "Junjie Bai", "Soumith Chintala." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
      "citeRegEx" : "Bai and Chintala.,? 2019",
      "shortCiteRegEx" : "Bai and Chintala.",
      "year" : 2019
    }, {
      "title" : "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods",
      "author" : [ "John C. Platt." ],
      "venue" : "Advances in Large Margin Classifiers.",
      "citeRegEx" : "Platt.,? 1999",
      "shortCiteRegEx" : "Platt.",
      "year" : 1999
    }, {
      "title" : "Know what you don’t know: Unanswerable questions for SQuAD",
      "author" : [ "Pranav Rajpurkar", "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784–",
      "citeRegEx" : "Rajpurkar et al\\.,? 2018",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2018
    }, {
      "title" : "Support vector method for novelty detection",
      "author" : [ "Bernhard Schölkopf", "Robert C. Williamson", "Alex Smola", "John Shawe-Taylor", "John Platt." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 12. MIT Press.",
      "citeRegEx" : "Schölkopf et al\\.,? 2000",
      "shortCiteRegEx" : "Schölkopf et al\\.",
      "year" : 2000
    }, {
      "title" : "The right tool for the job: Matching model and instance complexities",
      "author" : [ "Roy Schwartz", "Gabriel Stanovsky", "Swabha Swayamdipta", "Jesse Dodge", "Noah A. Smith." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Schwartz et al\\.,? 2020",
      "shortCiteRegEx" : "Schwartz et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep active learning for named entity recognition",
      "author" : [ "Yanyao Shen", "Hyokun Yun", "Zachary C. Lipton", "Yakov Kronrod", "Animashree Anandkumar." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Shen et al\\.,? 2018",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 Conference on",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "An empirical study of example forgetting during deep neural network learning",
      "author" : [ "Mariya Toneva", "Alessandro Sordoni", "Remi Tachet des Combes", "Adam Trischler", "Yoshua Bengio", "Geoffrey J. Gordon." ],
      "venue" : "International Conference on Learning Represen-",
      "citeRegEx" : "Toneva et al\\.,? 2019",
      "shortCiteRegEx" : "Toneva et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop Black-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "On the inference calibration of neural machine translation",
      "author" : [ "Shuo Wang", "Zhaopeng Tu", "Shuming Shi", "Yang Liu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3070–3079, Online. Association for",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Williams et al\\.,? 2018",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "BERxiT: Early exiting for BERT with better fine-tuning and extension to regression",
      "author" : [ "Ji Xin", "Raphael Tang", "Yaoliang Yu", "Jimmy Lin." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Xin et al\\.,? 2021",
      "shortCiteRegEx" : "Xin et al\\.",
      "year" : 2021
    }, {
      "title" : "DeeBERT: Dynamic early exiting for accelerating BERT inference",
      "author" : [ "Ji Xin", "Raphael Tang", "Jaejun Lee", "Yaoliang Yu", "Jimmy Lin." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2246–2251, On-",
      "citeRegEx" : "Xin et al\\.,? 2020",
      "shortCiteRegEx" : "Xin et al\\.",
      "year" : 2020
    }, {
      "title" : "Mitigating uncertainty in document classification",
      "author" : [ "Xuchao Zhang", "Fanglan Chen", "Chang-Tien Lu", "Naren Ramakrishnan." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT loses patience: Fast and robust inference with early exit",
      "author" : [ "Wangchunshu Zhou", "Canwen Xu", "Tao Ge", "Julian McAuley", "Ke Xu", "Furu Wei." ],
      "venue" : "H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 35,
      "context" : "Pre-trained language models based on the transformer architecture (Vaswani et al., 2017) have improved the state-of-the-art results on many NLP applications.",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "In this paper, we study the problem of selective prediction (Geifman and El-Yaniv, 2017) in NLP.",
      "startOffset" : 60,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : "of realistic scenarios, such as making entailment judgments for breaking news articles in search engines (Carlebach et al., 2020) and making critical predictions in medical and legal documents (Zhang et al.",
      "startOffset" : 105,
      "endOffset" : 129
    }, {
      "referenceID" : 42,
      "context" : ", 2020) and making critical predictions in medical and legal documents (Zhang et al., 2019).",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 17,
      "context" : "We consider two choices of confidence estimators, softmax response (SR; Hendrycks and Gimpel, 2017), and Monte-Carlo dropout (MC-dropout; Gal and Ghahramani, 2016).",
      "startOffset" : 67,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : "We consider two choices of confidence estimators, softmax response (SR; Hendrycks and Gimpel, 2017), and Monte-Carlo dropout (MC-dropout; Gal and Ghahramani, 2016).",
      "startOffset" : 125,
      "endOffset" : 163
    }, {
      "referenceID" : 4,
      "context" : "1041 learning (Cohn et al., 1995; Shen et al., 2018) and early exiting (Schwartz et al.",
      "startOffset" : 14,
      "endOffset" : 52
    }, {
      "referenceID" : 32,
      "context" : "1041 learning (Cohn et al., 1995; Shen et al., 2018) and early exiting (Schwartz et al.",
      "startOffset" : 14,
      "endOffset" : 52
    }, {
      "referenceID" : 6,
      "context" : "Experiments show that recent powerful NLP models such as BERT (Devlin et al., 2019) and ALBERT (Lan et al.",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 24,
      "context" : ", 2019) and ALBERT (Lan et al., 2020) improve not only accuracy but also selective prediction performance; they also demonstrate the effectiveness of the proposed error regularization by producing better confidence estimators which reduce the area under the risk–coverage curve by 10%.",
      "startOffset" : 19,
      "endOffset" : 37
    }, {
      "referenceID" : 3,
      "context" : "Selective prediction has been studied by the machine learning community for a long time (Chow, 1957; El-Yaniv and Wiener, 2010).",
      "startOffset" : 88,
      "endOffset" : 127
    }, {
      "referenceID" : 9,
      "context" : "Selective prediction has been studied by the machine learning community for a long time (Chow, 1957; El-Yaniv and Wiener, 2010).",
      "startOffset" : 88,
      "endOffset" : 127
    }, {
      "referenceID" : 30,
      "context" : "Selective prediction is closely related to confidence estimation, as well as out-of-domain (OOD) detection (Schölkopf et al., 2000; Liang et al., 2018) and prediction error detection (Hendrycks and Gimpel, 2017), albeit more remotely.",
      "startOffset" : 107,
      "endOffset" : 151
    }, {
      "referenceID" : 25,
      "context" : "Selective prediction is closely related to confidence estimation, as well as out-of-domain (OOD) detection (Schölkopf et al., 2000; Liang et al., 2018) and prediction error detection (Hendrycks and Gimpel, 2017), albeit more remotely.",
      "startOffset" : 107,
      "endOffset" : 151
    }, {
      "referenceID" : 17,
      "context" : ", 2018) and prediction error detection (Hendrycks and Gimpel, 2017), albeit more remotely.",
      "startOffset" : 39,
      "endOffset" : 67
    }, {
      "referenceID" : 14,
      "context" : "Bayesian methods such as Markov Chain Monte Carlo (Geyer, 1992) and Variational Inference (Hinton and Van Camp, 1993; Graves, 2011) assume a prior distribution over model parameters and obtain confidence estimates through the posterior.",
      "startOffset" : 50,
      "endOffset" : 63
    }, {
      "referenceID" : 15,
      "context" : "Bayesian methods such as Markov Chain Monte Carlo (Geyer, 1992) and Variational Inference (Hinton and Van Camp, 1993; Graves, 2011) assume a prior distribution over model parameters and obtain confidence estimates through the posterior.",
      "startOffset" : 90,
      "endOffset" : 131
    }, {
      "referenceID" : 10,
      "context" : "Ensemble-based methods (Gal and Ghahramani, 2016; Lakshminarayanan et al., 2017; Geifman et al., 2019) estimate confidence based on statistics of the ensemble model’s output.",
      "startOffset" : 23,
      "endOffset" : 102
    }, {
      "referenceID" : 23,
      "context" : "Ensemble-based methods (Gal and Ghahramani, 2016; Lakshminarayanan et al., 2017; Geifman et al., 2019) estimate confidence based on statistics of the ensemble model’s output.",
      "startOffset" : 23,
      "endOffset" : 102
    }, {
      "referenceID" : 13,
      "context" : "Ensemble-based methods (Gal and Ghahramani, 2016; Lakshminarayanan et al., 2017; Geifman et al., 2019) estimate confidence based on statistics of the ensemble model’s output.",
      "startOffset" : 23,
      "endOffset" : 102
    }, {
      "referenceID" : 6,
      "context" : "Current large-scale pre-trained NLP models, such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al.",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 26,
      "context" : ", 2019) and RoBERTa (Liu et al., 2019), are too expensive to run multiple times of inference, and therefore require lightweight confidence estimation.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 16,
      "context" : "Calibration (Guo et al., 2017; Jiang et al., 2018; Kumar et al., 2018; Wang et al., 2020; Desai and Durrett, 2020) focuses on adjusting the overall confidence level of a model, while selective prediction is based on relative confidence among the examples.",
      "startOffset" : 12,
      "endOffset" : 114
    }, {
      "referenceID" : 20,
      "context" : "Calibration (Guo et al., 2017; Jiang et al., 2018; Kumar et al., 2018; Wang et al., 2020; Desai and Durrett, 2020) focuses on adjusting the overall confidence level of a model, while selective prediction is based on relative confidence among the examples.",
      "startOffset" : 12,
      "endOffset" : 114
    }, {
      "referenceID" : 22,
      "context" : "Calibration (Guo et al., 2017; Jiang et al., 2018; Kumar et al., 2018; Wang et al., 2020; Desai and Durrett, 2020) focuses on adjusting the overall confidence level of a model, while selective prediction is based on relative confidence among the examples.",
      "startOffset" : 12,
      "endOffset" : 114
    }, {
      "referenceID" : 37,
      "context" : "Calibration (Guo et al., 2017; Jiang et al., 2018; Kumar et al., 2018; Wang et al., 2020; Desai and Durrett, 2020) focuses on adjusting the overall confidence level of a model, while selective prediction is based on relative confidence among the examples.",
      "startOffset" : 12,
      "endOffset" : 114
    }, {
      "referenceID" : 5,
      "context" : "Calibration (Guo et al., 2017; Jiang et al., 2018; Kumar et al., 2018; Wang et al., 2020; Desai and Durrett, 2020) focuses on adjusting the overall confidence level of a model, while selective prediction is based on relative confidence among the examples.",
      "startOffset" : 12,
      "endOffset" : 114
    }, {
      "referenceID" : 28,
      "context" : "For example, the most widely used calibration technique, temperature scaling (Platt, 1999), globally increases or decreases the model’s confidence on all examples, but the ranking of all examples’ confidence is unchanged.",
      "startOffset" : 77,
      "endOffset" : 90
    }, {
      "referenceID" : 9,
      "context" : "The performance of a selective classifier h = (f, g) can be evaluated by the risk–coverage curve (RCC; El-Yaniv and Wiener, 2010), which is",
      "startOffset" : 97,
      "endOffset" : 129
    }, {
      "referenceID" : 17,
      "context" : "Perhaps the most straightforward and popular choice for the confidence estimator is softmax response (Hendrycks and Gimpel, 2017):",
      "startOffset" : 101,
      "endOffset" : 129
    }, {
      "referenceID" : 34,
      "context" : "• History record error Since we intend to use ei to quantify how difficult an example is, we draw inspiration from forgettable examples (Toneva et al., 2019).",
      "startOffset" : 136,
      "endOffset" : 157
    }, {
      "referenceID" : 6,
      "context" : "following representative models: (1) BERT-base and BERT-large (Devlin et al., 2019), the dominant transformer-based models of recent years; (2) ALBERT-base (Lan et al.",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 24,
      "context" : ", 2019), the dominant transformer-based models of recent years; (2) ALBERT-base (Lan et al., 2020), a variant of BERT featuring parameter sharing and memory efficiency; (3) Long Short-Term Memory (LSTM; Hochreiter and Schmidhuber, 1997), the popular pre-transformer model that is lightweight and fast.",
      "startOffset" : 80,
      "endOffset" : 98
    }, {
      "referenceID" : 19,
      "context" : ", 2020), a variant of BERT featuring parameter sharing and memory efficiency; (3) Long Short-Term Memory (LSTM; Hochreiter and Schmidhuber, 1997), the popular pre-transformer model that is lightweight and fast.",
      "startOffset" : 105,
      "endOffset" : 145
    }, {
      "referenceID" : 7,
      "context" : "We conduct experiments mainly on three datasets: MRPC (Dolan and Brockett, 2005), QNLI (Wang et al.",
      "startOffset" : 54,
      "endOffset" : 80
    }, {
      "referenceID" : 36,
      "context" : "We conduct experiments mainly on three datasets: MRPC (Dolan and Brockett, 2005), QNLI (Wang et al., 2018), and MNLI (Williams et al.",
      "startOffset" : 87,
      "endOffset" : 106
    }, {
      "referenceID" : 33,
      "context" : "4, we will need an additional non-binary dataset SST-5 (Socher et al., 2013).",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 36,
      "context" : "Following the setting of the GLUE benchmark (Wang et al., 2018), we use the training set for training/finetuning and the development set for evaluation (the test set’s labels are not publicly available); MNLI’s development set has two parts, matched and mismatched (m/mm).",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 2,
      "context" : "For example, in the setting of ultrafine entity typing with more than 10,000 labels (Choi et al., 2018), it is unsurprising to encounter examples with unseen types.",
      "startOffset" : 84,
      "endOffset" : 103
    }, {
      "referenceID" : 0,
      "context" : "(3) There exists another setting for selective prediction where abstention induces a fixed cost (Bartlett and Wegkamp, 2008) and the goal is to minimize the overall cost instead of AUC; it would also be interesting to investigate this setting for NLP applications.",
      "startOffset" : 96,
      "endOffset" : 124
    } ],
    "year" : 2021,
    "abstractText" : "In selective prediction, a classifier is allowed to abstain from making predictions on lowconfidence examples. Though this setting is interesting and important, selective prediction has rarely been examined in natural language processing (NLP) tasks. To fill this void in the literature, we study in this paper selective prediction for NLP, comparing different models and confidence estimators. We further propose a simple error regularization trick that improves confidence estimation without substantially increasing the computation budget. We show that recent pre-trained transformer models simultaneously improve both model accuracy and confidence estimation effectiveness. We also find that our proposed regularization improves confidence estimation and can be applied to other relevant scenarios, such as using classifier cascades for accuracy– efficiency trade-offs. Source code for this paper can be found at https://github.com/ castorini/transformers-selective.",
    "creator" : "LaTeX with hyperref"
  }
}