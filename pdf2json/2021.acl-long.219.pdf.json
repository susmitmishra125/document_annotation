{
  "name" : "2021.acl-long.219.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Neural Transition-based Joint Model for Disease Named Entity Recognition and Normalization",
    "authors" : [ "Zongcheng Ji", "Tian Xia", "Mei Han", "Jing Xiao" ],
    "emails" : [ "hanmei613}@gmail.com", "2xiaojing039@pingan.com.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2819–2827\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2819"
    }, {
      "heading" : "1 Introduction",
      "text" : "Disease is one of the fundamental entities in biomedical research, thus it is one of the most searched topics in the biomedical literature (Dogan et al., 2009) and the internet (Brownstein et al., 2009). Automatically identifying diseases mentioned in a text (e.g., a PubMed article or a health webpage) and then normalizing these identified mentions to their mapping concepts in a standardized disease vocabulary (e.g., with primary\nname, synonyms and definition, etc.) offers a tremendous opportunity for many downstream applications, such as mining chemical-disease relations from the literature (Wei et al., 2015), and providing much more relevant resources based on the search queries (Dogan et al., 2014), etc. Examples of such disease vocabularies includes MeSH (http://www.nlm.nih.gov/mesh/) and OMIM (http://www.ncbi.nlm.nih.gov/omim).\nPrevious studies (Leaman and Lu, 2016; Lou et al., 2017; Zhao et al., 2019) show the effectiveness of the joint methods for the end-to-end disease recognition and normalization (aka linking) task to alleviated the error propagation problem of the traditional pipelined solutions (Strubell et al., 2017; Leaman et al., 2013; Xu et al., 2016, 2017). Although TaggerOne (Leaman and Lu, 2016) and the discrete transition-based joint model (Lou et al., 2017) successfully alleviate the error propagation problem, they heavily rely on hand-craft feature engineering. Recently, Zhao et al. (Zhao et al., 2019) proposes a neural joint model based on the multi-task learning framework (i.e., MTLfeedback) which significantly outperforms previous discrete joint solutions. MTL-feedback jointly shares the representations of the two sub-tasks (i.e., joint learning with shared representations of the input), however, their method suffers from the boundary inconsistency problem due to the separate decoding procedures (i.e., separate search in two different search spaces). Moreover, it ignores the rich information (e.g., the text surface form) of each candidate concept in the vocabulary, which is quite essential for entity normalization.\nIn this work, we propose a novel neural transition-based joint model named NeuJoRN for disease named entity recognition and normalization, to alleviate these two issues of the multi-task learning based solution (Zhao et al., 2019). We transform the end-to-end disease recognition and\nnormalization task as an action sequence prediction task. More specifically, we introduce four types of actions (i.e., OUT, SHIFT, REDUCE, SEGMENT) for the recognition purpose and one type of action (i.e., LINKING) for the normalization purpose. Our joint model not only jointly learns the model with shared representations, but also jointly searches the output by state transitions in one search space. Moreover, we introduce attention mechanisms to take advantage of text surface form of each candidate concept for better linking action prediction.\nWe summarize our contributions as follows.\n• We propose a novel neural transition-based joint model, NeuJoRN, for disease named entity recognition and normalization, which not only jointly learns the model with shared representations, but also jointly searches the output by state transitions in one search space.\n• We introduce attention mechanisms to take advantage of text surface form of each candidate concept for normalization performance.\n• We evaluate our proposed model on two public datasets, namely the NCBI and BC5CDR datasets. Extensive experiments show the effectiveness of the proposed model."
    }, {
      "heading" : "2 Task Definition",
      "text" : "We define the end-to-end disease recognition and normalization task as follows. Given a sentence x from a document d (e.g., a PubMed abstract) and a controlled vocabulary KB (e.g., MeSH and OMIM) which consists of a set of disease concepts, the task of end-to-end disease recognition and normalization is to identify all disease mentions M = {m1,m2, ...,m|M |} mentioned in x and to link each of the identified disease mention mi with its mapping concept ci in KB, mi → ci. If there is no mapping concept in KB for mi, then mi → NIL, where NIL denotes that mi is unlinkable."
    }, {
      "heading" : "3 Neural Transition-based Joint Model",
      "text" : "We first introduce the transition system used in the model, and then introduce the neural transitionbased joint model for this task."
    }, {
      "heading" : "3.1 Transition System",
      "text" : "We propose a novel transition system, inspired by the arc-eager transition-based shift-reduce\nparser (Watanabe and Sumita, 2015; Lample et al., 2016), which constructs the output of each given sentence x and controlled vocabulary KB through state transitions with a sequence of actions A.\nWe define a state as a tuple (σ, β,O), which consists of the following three structures:\n• stack (σ): the stack is used to store tokens being processed.\n• buffer (β): the buffer is used to store tokens to be processed.\n• output (O): the output is used to store the recognized and normalize mentions.\nWe define a start state with the stack σ and the output O being both empty, and the buffer β containing all the tokens of a given sentence x. Similarly, we define an end state with the stack σ and buffer β being both empty, and the output O saving the recognized and normalized entity mention. The transition system begins with a start state and ends with an end state. The state transitions are accomplished by a set of transition actions A, which consume the tokens in β and build the output O step by step.\nAs shown in Table 1, we define 5 types of transition actions for state transitions, and their logics are summarized as follows:\n• OUT pops the first token β0 from the buffer, which indicates that this token does not belong to any entity mention.\n• SHIFT moves the first token β0 from the buffer to the stack, which indicates that this token is part of an entity mention.\n• REDUCE pops the top two tokens (or spans) σ0 and σ1 from the stack and concatenates them as a new span, which is then pushed back to the stack.\n• SEGMENT-t pops the top token (or span) σ0 from the stack and creates a new entity mention σt0 with entity type t, which is then added to the output.\n• LINKING-c links the previous recognized but unnormalized mention σt0 in the output with its mapping concept with id c and updates the mention with σt,c0 .\nTable 2 shows an example of state transitions for the recognition and normalization of disease mentions given a sentence “Most colon cancers arise from mutations” and a controlled vocabulary MeSH. State 0 is the start state where φ denotes that the stack σ and output O are initially empty, and the buffer β is initialized with all the tokens of the given sentence. State 9 is the end state where φ denotes that the stack σ and buffer β are finally empty, and colon cancersdisease,D003110 in the output O denote that the mention “colon cancers” is a disease mention and is normalized to the concept with id D003110 in MeSH. More specifically, state 5 creates a new disease mention colon cancersdisease and add it to the output. State 6 links the previous recognized but unnormalized disease mention in the output with its mapping concept with id D003110 in MeSH."
    }, {
      "heading" : "3.2 Action Sequence Prediction",
      "text" : "Based on the introduced transition system, the endto-end disease recognition and normalization task becomes a new sequence to sequence task, i.e., the action sequence prediction task. The input is\na sequence of words xn1 = (w1, w2, ..., wn) and a controlled vocabulary KB, and the output is a sequence of actions Am1 = (a1, a2, ..., am). The goal of the task is to find the most probable output action sequence A∗ given the input word sequence xn1 and KB, that is\nA∗ = arg max A p(Am1 |xn1 ,KB) (1)\nFormally, at each step t, the model predicts the next action based on the current state St and the action history At−11 . Thus, the task is models as\n(A∗, S∗) = argmaxA,S ∏ t p(at, St+1|At−11 , St)\n(2) where at is the generated action at step t, and St+1 is the new state according to at.\nLet rt denote the representation for computing the probability of the action at at step t, thus\np(at|rt) = exp(wᵀatrt + bat)\nΣa′∈A(St)exp(w ᵀ a′rt + ba′)\n(3)\nwhere wa and ba denote the learnable parameter vector and bias term, respectively, and A(St) denotes the next possible valid actions that may be taken given the current state St.\nFinally, the overall optimization function of the action sequence prediction task can be written as\n(A∗, S∗) = argmaxA,S ∏ t p(at, St+1|At−11 , St)\n= argmaxA,S ∏ t p(at|rt)\n(4)"
    }, {
      "heading" : "3.3 Dense Representations",
      "text" : "We now introduce neural networks to learn the dense representations of an input sentence x and each state in the whole transition process to predict the next action.\nInput Representation We represent each word xi in a sentence x by concatenating its character-level word representation, non-contextual word representation, and contextual word representation:\nxi = [v char i ; v w i ; ELMoi] (5)\nwhere vchari denotes its character-level word representation learned by using a CNN network (Ma and Hovy, 2016), vwi denotes its non-contextual word representation initialized with Glove (Pennington et al., 2014) embeddings, which is pre-trained on 6 billion words from Wikipedia and web text, and ELMoi denotes its contextual word representation initialized with ELMo (Peters et al., 2018). We can also explore the contextual word representation from BERT (Devlin et al., 2018) by averaging the embeddings of the subwords of each word. We leave it to the future work.\nWe then run a BiLSTM (Graves et al., 2013) to derive the contextual representation of each word in the sentence x.\nState Representation At each step t in the transition process, let’s consider the representation of the current state St = (σt, βt, At), where σt = (..., σ1, σ0), βt = (β0, β1, ...) and At = (at−1, at−2, ...).\nThe buffer βt is represented with BiLSTM (Graves et al., 2013) to represent the words in the buffer:\nbt = BiLSTM([β0, β1, ...]) (6)\nThe stack σt and the actions At are represented with StackLSTM (Dyer et al., 2015):\nst = StackLSTM([..., σ1, σ0])\nat = StackLSTM([at−1, at−2, ...]) (7)\nWe classify all the actions defined in Table 1 into two categories corresponding to two different purposes, i.e., the recognition and normalization purposes. OUT, SHIFT, REDUCE, SEGMENT-t are used for the recognition purpose, and LINKING-c is used for the normalization purpose. As shown in Figure 1(a) and 1(b), we define two different\nstate representations for predicting the actions in different purposes.\nSpecifically, for predicting the actions in the recognition purpose, we represent the state as\nrNERt = ReLU(W [s 1 t ; s 0 t ; b 0 t ; a −1 t ] + d) (8)\nwhere ReLU is an activation function, W and d denote the learnable parameter matrix and bias term, respectively, and\n• s0t and s 1 t denote the first and second represen-\ntations of the stack σ.\n• b0t denotes the first representation of the buffer β.\n• a−1t denotes the last representation of the action history A.\nFor predicting the actions in the normalization purpose, we represent the state as\nrNORMt = ReLU(W [l ′ m; r ′ m;m ′; c′; c; a−1t ] + d) (9)\nwhere ReLU is an activation function, W and d denote the learnable parameter matrix and bias term, respectively, and\n• l′m and r ′ m denotes the left-side and right-side\ncontext representations by (i) first applying attention with the concept representation c to highlight the relevant parts in mentions’ local context, and (ii) then applying max-pooling operation to aggregate the reweighted representations of all the context words.\n• m′ and c′ are the representations of the mention and candidate concept by applying CoAttention mechanism (Tay et al., 2018; Jia et al., 2020).\n• c denotes the candidate concept representation by (i) first run a BiLSTM (Graves et al., 2013) to derive the contextual representation of each word in the candidate concept, and (ii) then applying max-pooling operation to aggregate the representations of all concept words.\n• a−1t denotes the last representation of the action history A."
    }, {
      "heading" : "3.4 Search and Training",
      "text" : "Decoding is the key step in both training and test, which is to search for the best output structure (i.e., action sequence) under the current model parameters. In this work, we use two different search strategies with different optimizations.\nGreedy Search For efficient decoding, a widelyused greedy search algorithm (Wang et al., 2017) can be adopted to minimize the negative loglikelihood of the local action classifier in Equation (3, 8, 9).\nBeam Search The main drawback of greedy search is error propagation (Wang et al., 2017). An incorrect action will fail the following actions, leading to an incorrect output sequence. One solution to alleviate this problem is to apply beam search. In this work, we use the Beam-Search Optimization (BSO) method with LaSO update (Wiseman and Rush, 2016) to train our beam-search model, where the max-margin loss is adopted."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We use two public available datasets in this study, namely NCBI - the NCBI disease corpus (Dogan et al., 2014) and BC5CDR - the BioCreative V CDR task corpus (Li et al., 2016b). NCBI dataset contains 792 PubMed abstracts, which was split into 692 abstracts for training and development, and 100 abstracts for testing. A disorder mention in each PubMed abstract was manually annotated with its mapping concept identifier in the MEDIC\nlexicon. BC5CDR dataset contains 1,500 PubMed abstracts, which was equally split into three parts for training, development and test, respectively. A disease mention in each abstract is manually annotated with the concept identifier to which it refers to a controlled vocabulary. In this study, we use the July 6, 2012 version of MEDIC, which contains 7,827 MeSH identifiers and 4,004 OMIM identifiers, grouped into 9,664 disease concepts. Table3 show the overall statistics of the two datasets.\nTo facilitate the generation of candidate linking actions, we perform some preprocessing steps of each candidate mention and each concept in KB with the following strategies: (i) Spelling Correction - for each candidate mention in the datasets, we replace all the misspelled words using a spelling check list as in previous work (D’Souza and Ng, 2015; Li et al., 2017). (ii) Abbreviation Resolution - we use Ab3p (Sohn et al., 2008) toolkit to detect and replace the abbreviations with their long forms within each document and also expand all possible abbreviated disease mentions using a dictionary collected from Wikipedia as in previous work (D’Souza and Ng, 2015; Li et al., 2017). (iii) Numeric Synonyms Resolutions - we replace all the numerical words in the mentions and concepts to their corresponding Arabic numerals as in previous work (D’Souza and Ng, 2015; Li et al., 2017).\nWe generate candidate linking actions (i.e., candidate concepts) for each mention with the commonly used information retrieval based method, which includes the following two steps. We first index all the concept names and training mentions with their concept ids. Then, the widely-used BM25 model provided by Lucene is employed to retrieve the top 10 candidate concepts {ci}10i=1 for each mention m."
    }, {
      "heading" : "4.2 Evaluation Metrics and Settings",
      "text" : "Following previous work (Leaman and Lu, 2016; Lou et al., 2017; Zhao et al., 2019), we utilize the evaluation kit1 for evaluating the model performances. We report F1 score for the recognition task at the mention level, and F1 score for the normalization task at the abstract level.\nWe use the AdamW optimizer (Loshchilov and Hutter, 2019) for parameter optimization. Most of the model hyper-parameters are listed in Table 4. Since increasing the beam size will increase the decoding time, we only report results with beam size 1, 2, and 4."
    }, {
      "heading" : "4.3 Results and Discussion",
      "text" : ""
    }, {
      "heading" : "4.3.1 Main results",
      "text" : "Table 5 shows the overall comparisons of different models for the end-to-end disease named entity recognition and normalization task. The first part shows the performance of different pipelined methods for the task. DNorm (Leaman et al., 2013) is a traditional method, which needs feature engineering. IDCNN (Strubell et al., 2017) is a neural model based on BiLSTM-CRF, which requires few effort of feature engineering. The second part\n1http://www.biocreative.org/tasks/biocreative-v/track-3cdr\nshows the performance of different joint models for the task. TaggerOne (Leaman et al., 2013) is a joint solution based on semi-CRF. Transition-based Model (Lou et al., 2017) is a joint solution based on discrete transition-based method. Both of these two models rely heavily on feature engineering. MTL-feedback (Zhao et al., 2019) is neural joint solution based on multi-task learning. NeuJoRN is our neural transition-based joint model for the whole task.\nFrom the comparisons, we find that (1) IDCNN does not perform well enough although it relies few efforts of feature engineering. (2) All the joint models significantly outperform the pipelined methods. (3) The deep-learning based joint models significantly outperform the traditional machine learning based methods. (4) Our proposed NeuJoRN outperforms MTL-feedback by at least 0.57% and 0.59% on the recognition and normalization tasks, respectively."
    }, {
      "heading" : "4.3.2 Effectiveness of different search strategies",
      "text" : "Table 6 shows the comparisons of different search strategies of our proposed NeuJoRN. From the results, we find that (1) The methods based on beam search strategies outperforms the greedy search strategy, which indicates that the beam search solutions could alleviate the error propagation problem of the greedy search solution. (2) The model with beam size 4 achieves the best performance. The larger the beam size, the better the performance, however the lower the decoding speed. (3) Our greedy search based solution doesn’t outperform the MLT-feedback method."
    }, {
      "heading" : "4.3.3 Effectiveness of attention mechanisms",
      "text" : "Table 7 shows the effectiveness of the proposed attention mechanisms. When we remove the attention mechanism for representing the left-side and right-side local context, the performance dropped a little bit. However, when we remove the CoAttention mechanism, which is used for directly modeling the matching between the mention and candidate concept, the performance dropped significantly. This group of comparisons indicates that importance of the matching between the mention and candidate concept for the entity normalization task."
    }, {
      "heading" : "5 Related Work",
      "text" : "Disease Named Entity Recognition DNER has been widely studied in the literature. Most previous studies (Leaman et al., 2013; Xu et al., 2015, 2016) transform this task as a sequence labeling task, and conditional random fields (CRF) based methods are widely adopted to achieve good performance. However, these methods heavily rely on hand-craft feature engineering. Recently, neural models such as BiLSTM-CRF based methods (Strubell et al., 2017; Wang et al., 2019) and BERT-based methods (Kim et al., 2019) have achieved state-of-theart performance.\nDisease Named Entity Normalization DNEN has also been widely studied in the literature. Most studies assume that the entity mentions are predetected by a separate DNER model, and focus on developing methods to improve the normaliation accuracy (Lou et al., 2017), resulting in developing rule-based methods (D’Souza and Ng, 2015), machine learning-based methods (Leaman et al., 2013; Xu et al., 2017), and recent deep learning-based methods (Li et al., 2017; Ji et al., 2020; Wang et al., 2020; Vashishth et al., 2021; Chen et al., 2021). However, the pipeline architecture which performs DNER and DNEN separately suffers from the error propagation problem. In this work, we propose a neural joint model to alleviate this issue.\nJoint DNER and DNEN Several studies (Leaman and Lu, 2016; Lou et al., 2017; Zhao et al., 2019) show the effectiveness of the joint methods to alleviated the error propagation problem. Although\nTaggerOne (Leaman and Lu, 2016) and the discrete transition-based joint model (Lou et al., 2017) successfully alleviated the error propagation problem, they heavily rely on hand-craft feature engineering. Recently, Zhao et al. (Zhao et al., 2019) propose a neural joint model based on the multi-task learning framework (i.e., MTL-feedback) which significantly outperforms previous discrete joint solutions. However, their method suffers from the boundary inconsistency problem due to the separate decoding procedures (i.e., separate search in two different search spaces). Moreover, it ignores the rich information (e.g., the text surface form) of each candidate concept in the vocabulary, which is quite essential for entity normalization. In this work, we propose a neural joint model to alleviate these two issues.\nTransition-based Models Transition-based models are widely used in parsing and translation (Watanabe and Sumita, 2015; Wang et al., 2018; Meng and Zhang, 2019). Recently, these models are successfully applied to information extraction tasks, such as joint POS tagging and dependency parsing (Yang et al., 2018), joint entity and relation extraction (Li and Ji, 2014; Li et al., 2016a; Ji et al., 2021). Several studies propose discrete transition-based joint model for entity recognition and normalization(Qian et al., 2015; Ji et al., 2016; Lou et al., 2017). In this work, we propose a neural transition-based joint model for disease named entity recognition and normalization."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this work, we proposed a novel neural transitionbased joint model for disease named entity recognition and normalization. Experimental results conducted on two public available datasets show the effectiveness of the proposed method. In the future, we will apply this joint model to more different types of datasets, such as the clinical notes, drug labels, and tweets, etc."
    } ],
    "references" : [ {
      "title" : "Digital disease detection—harnessing the Web for public health surveillance",
      "author" : [ "John S Brownstein", "Clark C Freifeld", "Lawrence C Madoff." ],
      "venue" : "New England Journal of Medicine, 360(21):2153–2157.",
      "citeRegEx" : "Brownstein et al\\.,? 2009",
      "shortCiteRegEx" : "Brownstein et al\\.",
      "year" : 2009
    }, {
      "title" : "A Lightweight Neural Model for Biomedical Entity Linking",
      "author" : [ "Lihu Chen", "Gaël Varoquaux", "Fabian M Suchanek." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Chen et al\\.,? 2021",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "NCBI disease corpus: a resource for disease name recognition and concept normalization",
      "author" : [ "Rezarta Islamaj Dogan", "Robert Leaman", "Zhiyong Lu." ],
      "venue" : "JBI, 47:1–10.",
      "citeRegEx" : "Dogan et al\\.,? 2014",
      "shortCiteRegEx" : "Dogan et al\\.",
      "year" : 2014
    }, {
      "title" : "Understanding PubMed® user search behavior through log analysis",
      "author" : [ "Rezarta Islamaj Dogan", "G Craig Murray", "Aurélie Névéol", "Zhiyong Lu." ],
      "venue" : "Database, 2009:bap018.",
      "citeRegEx" : "Dogan et al\\.,? 2009",
      "shortCiteRegEx" : "Dogan et al\\.",
      "year" : 2009
    }, {
      "title" : "Sieve-Based Entity Linking for the Biomedical Domain",
      "author" : [ "Jennifer D’Souza", "Vincent Ng" ],
      "venue" : "In ACL,",
      "citeRegEx" : "D.Souza and Ng.,? \\Q2015\\E",
      "shortCiteRegEx" : "D.Souza and Ng.",
      "year" : 2015
    }, {
      "title" : "TransitionBased Dependency Parsing with Stack Long ShortTerm Memory",
      "author" : [ "Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith." ],
      "venue" : "ACL-IJCNLP, pages 334–343, Beijing, China. Association for Computational Lin-",
      "citeRegEx" : "Dyer et al\\.,? 2015",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2015
    }, {
      "title" : "Speech recognition with deep recurrent neural networks",
      "author" : [ "Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton." ],
      "venue" : "IEEE ICASSP, pages 6645– 6649.",
      "citeRegEx" : "Graves et al\\.,? 2013",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2013
    }, {
      "title" : "A Discrete Joint Model for Entity and Relation Extraction from Clinical Notes",
      "author" : [ "Zongcheng Ji", "Omid Ghiasvand", "Stephen Wu", "Hua Xu." ],
      "venue" : "AMIA 2021 Informatics Summit.",
      "citeRegEx" : "Ji et al\\.,? 2021",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2021
    }, {
      "title" : "Joint Recognition and Linking of FineGrained Locations from Tweets",
      "author" : [ "Zongcheng Ji", "Aixin Sun", "Gao Cong", "Jialong Han." ],
      "venue" : "WWW, pages 1271–1281.",
      "citeRegEx" : "Ji et al\\.,? 2016",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2016
    }, {
      "title" : "BERTbased Ranking for Biomedical Entity Normalization",
      "author" : [ "Zongcheng Ji", "Qiang Wei", "Hua Xu." ],
      "venue" : "AMIA 2020 Informatics Summit, pages 269–277.",
      "citeRegEx" : "Ji et al\\.,? 2020",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2020
    }, {
      "title" : "CoGCN: Combining co-attention with graph convolutional network for entity linking with knowledge graphs",
      "author" : [ "Ningning Jia", "Xiang Cheng", "Sen Su", "Liyuan Ding." ],
      "venue" : "Expert Systems, page e12606.",
      "citeRegEx" : "Jia et al\\.,? 2020",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2020
    }, {
      "title" : "A Neural Named Entity Recognition and Multi-Type Normalization Tool for Biomedical Text Mining",
      "author" : [ "Donghyeon Kim", "Jinhyuk Lee", "Chan Ho So", "Hwisang Jeon", "Minbyul Jeong", "Yonghwa Choi", "Wonjin Yoon", "Mujeen Sung", "Jaewoo Kang." ],
      "venue" : "IEEE Ac-",
      "citeRegEx" : "Kim et al\\.,? 2019",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural Architectures for Named Entity Recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer." ],
      "venue" : "NAACL, pages 260–270, San Diego, California. Association for Computational Linguistics.",
      "citeRegEx" : "Lample et al\\.,? 2016",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "DNorm: disease name normalization with pairwise learning to rank",
      "author" : [ "Robert Leaman", "Rezarta Islamaj Dogan", "Zhiyong Lu." ],
      "venue" : "Bioinformatics, 29:2909– 2917.",
      "citeRegEx" : "Leaman et al\\.,? 2013",
      "shortCiteRegEx" : "Leaman et al\\.",
      "year" : 2013
    }, {
      "title" : "TaggerOne: joint named entity recognition and normalization with semi-Markov Models",
      "author" : [ "Robert Leaman", "Zhiyong Lu." ],
      "venue" : "Bioinformatics, 32(18):2839–2846.",
      "citeRegEx" : "Leaman and Lu.,? 2016",
      "shortCiteRegEx" : "Leaman and Lu.",
      "year" : 2016
    }, {
      "title" : "Joint models for extracting adverse drug events from biomedical text",
      "author" : [ "Fei Li", "Yue Zhang", "Meishan Zhang", "Donghong Ji." ],
      "venue" : "IJCAI, pages 2838– 2844.",
      "citeRegEx" : "Li et al\\.,? 2016a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "CNN-based ranking for biomedical entity normalization",
      "author" : [ "Haodi Li", "Qingcai Chen", "Buzhou Tang", "Xiaolong Wang", "Hua Xu", "Baohua Wang", "Dong Huang." ],
      "venue" : "BMC Bioinformatics, 18(11):385.",
      "citeRegEx" : "Li et al\\.,? 2017",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "BioCreative V CDR task corpus",
      "author" : [ "Jiao Li", "Yueping Sun", "Robin J Johnson", "Daniela Sciaky", "Chih-Hsuan Wei", "Robert Leaman", "Allan Peter Davis", "Carolyn J Mattingly", "Thomas C Wiegers", "Zhiyong Lu" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Incremental Joint Extraction of Entity Mentions and Relations",
      "author" : [ "Qi Li", "Heng Ji." ],
      "venue" : "ACL, pages 402–412.",
      "citeRegEx" : "Li and Ji.,? 2014",
      "shortCiteRegEx" : "Li and Ji.",
      "year" : 2014
    }, {
      "title" : "Decoupled Weight Decay Regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "ICLR (Poster). OpenReview.net.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "A Transition-based Joint Model for Disease Named Entity Recognition and Normalization",
      "author" : [ "Yinxia Lou", "Yue Zhang", "Tao Qian", "Fei Li", "Shufeng Xiong", "Donghong Ji." ],
      "venue" : "Bioinformatics.",
      "citeRegEx" : "Lou et al\\.,? 2017",
      "shortCiteRegEx" : "Lou et al\\.",
      "year" : 2017
    }, {
      "title" : "End-to-end Sequence Labeling via Bi-directional LSTM-CNNsCRF",
      "author" : [ "Xuezhe Ma", "Eduard Hovy." ],
      "venue" : "ACL, pages 1064–1074, Berlin, Germany. Association for Computational Linguistics.",
      "citeRegEx" : "Ma and Hovy.,? 2016",
      "shortCiteRegEx" : "Ma and Hovy.",
      "year" : 2016
    }, {
      "title" : "DTMT: A Novel Deep Transition Architecture for Neural Machine Translation",
      "author" : [ "Fandong Meng", "Jinchao Zhang." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Meng and Zhang.,? 2019",
      "shortCiteRegEx" : "Meng and Zhang.",
      "year" : 2019
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "EMNLP, pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep Contextualized Word Representations",
      "author" : [ "Matthew E Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "NAACL-HLT, pages 2227–2237.",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "A Transition-based Model for Joint Segmentation, POS-tagging and Normalization",
      "author" : [ "Tao Qian", "Yue Zhang", "Meishan Zhang", "Yafeng Ren", "Dong-Hong Ji." ],
      "venue" : "EMNLP, pages 1837–1846.",
      "citeRegEx" : "Qian et al\\.,? 2015",
      "shortCiteRegEx" : "Qian et al\\.",
      "year" : 2015
    }, {
      "title" : "Abbreviation definition identification based on automatic precision estimates",
      "author" : [ "Sunghwan Sohn", "Donald C Comeau", "Won Kim", "W John Wilbur." ],
      "venue" : "BMC Bioinformatics, 9.",
      "citeRegEx" : "Sohn et al\\.,? 2008",
      "shortCiteRegEx" : "Sohn et al\\.",
      "year" : 2008
    }, {
      "title" : "Fast and Accurate Entity Recognition with Iterated Dilated Convolutions",
      "author" : [ "Emma Strubell", "Patrick Verga", "David Belanger", "Andrew McCallum." ],
      "venue" : "EMNLP, pages 2670–2680. Association for Computational Linguistics.",
      "citeRegEx" : "Strubell et al\\.,? 2017",
      "shortCiteRegEx" : "Strubell et al\\.",
      "year" : 2017
    }, {
      "title" : "Hermitian Co-Attention Networks for Text Matching in Asymmetrical Domains",
      "author" : [ "Yi Tay", "Anh Tuan Luu", "Siu Cheung Hui." ],
      "venue" : "IJCAI, pages 4425– 4431. ijcai.org.",
      "citeRegEx" : "Tay et al\\.,? 2018",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving Broad-Coverage Medical Entity Linking with Semantic Type Prediction and Large-Scale Datasets",
      "author" : [ "Shikhar Vashishth", "Denis Newman-Griffis", "Rishabh Joshi", "Ritam Dutt", "Carolyn Rose." ],
      "venue" : "arXiv preprint arXiv:2005.00460.",
      "citeRegEx" : "Vashishth et al\\.,? 2021",
      "shortCiteRegEx" : "Vashishth et al\\.",
      "year" : 2021
    }, {
      "title" : "A study of entity-linking methods for normalizing Chinese diagnosis and procedure terms to ICD codes",
      "author" : [ "Qiong Wang", "Zongcheng Ji", "Jingqi Wang", "Stephen Wu", "Weiyan Lin", "Wenzhen Li", "Li Ke", "Guohong Xiao", "Qing Jiang", "Hua Xu", "Others." ],
      "venue" : "JBI,",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Transition-Based Disfluency Detection using LSTMs",
      "author" : [ "Shaolei Wang", "Wanxiang Che", "Yue Zhang", "Meishan Zhang", "Ting Liu." ],
      "venue" : "EMNLP, pages 2785–2794, Copenhagen, Denmark. Association for Computational Linguistics.",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Cross-type biomedical named entity recognition with deep multi-task learning",
      "author" : [ "Xuan Wang", "Yu Zhang", "Xiang Ren", "Yuhao Zhang", "Marinka Zitnik", "Jingbo Shang", "Curtis Langlotz", "Jiawei Han." ],
      "venue" : "Bioinformatics, 35(10):1745–1752.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "A Neural Transition-Based Approach for Semantic Dependency Graph Parsing",
      "author" : [ "Yuxuan Wang", "Wanxiang Che", "Jiang Guo", "Ting Liu." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Transitionbased Neural Constituent Parsing",
      "author" : [ "Taro Watanabe", "Eiichiro Sumita." ],
      "venue" : "ACL, pages 1169–1179, Beijing, China. Association for Computational Linguistics.",
      "citeRegEx" : "Watanabe and Sumita.,? 2015",
      "shortCiteRegEx" : "Watanabe and Sumita.",
      "year" : 2015
    }, {
      "title" : "Overview of the BioCreative V chemical disease relation (CDR) task",
      "author" : [ "Chih-Hsuan Wei", "Yifan Peng", "Robert Leaman", "Allan Peter Davis", "Carolyn J Mattingly", "Jiao Li", "Thomas C Wiegers", "Zhiyong Lu." ],
      "venue" : "Proceedings of the fifth",
      "citeRegEx" : "Wei et al\\.,? 2015",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence-to-Sequence Learning as Beam-Search Optimization",
      "author" : [ "Sam Wiseman", "Alexander M Rush." ],
      "venue" : "EMNLP, pages 1296–1306, Austin, Texas. Association for Computational Linguistics.",
      "citeRegEx" : "Wiseman and Rush.,? 2016",
      "shortCiteRegEx" : "Wiseman and Rush.",
      "year" : 2016
    }, {
      "title" : "UTH CCB System for Adverse Drug Reaction Extraction from Drug Labels at TAC-ADR 2017",
      "author" : [ "Jun Xu", "Hee-Jin Lee", "Zongcheng Ji", "Jingqi Wang", "Qiang Wei", "Hua Xu." ],
      "venue" : "Proceedings of Text Analysis Conference.",
      "citeRegEx" : "Xu et al\\.,? 2017",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2017
    }, {
      "title" : "CD-REST: a system for extracting chemical-induced disease relation in literature",
      "author" : [ "Jun Xu", "Yonghui Wu", "Yaoyun Zhang", "Jingqi Wang", "Hee-Jin Lee", "Hua Xu." ],
      "venue" : "Database.",
      "citeRegEx" : "Xu et al\\.,? 2016",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    }, {
      "title" : "UTHCCB: The Participation of the SemEval 2015 Challenge - Task 14",
      "author" : [ "Jun Xu", "Yaoyun Zhang", "Jingqi Wang", "Yonghui Wu", "Min Jiang", "Ergin Soysal", "Hua Xu." ],
      "venue" : "SemEval, pages 311–314.",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Joint POS Tagging and Dependence Parsing With TransitionBased Neural Networks",
      "author" : [ "Liner Yang", "Meishan Zhang", "Yang Liu", "Maosong Sun", "Nan Yu", "Guohong Fu." ],
      "venue" : "TASLP, 26(8):1352–1358.",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "A Neural Multi-Task Learning Framework to Jointly Model Medical Named Entity Recognition and Normalization",
      "author" : [ "Sendong Zhao", "Ting Liu", "Sicheng Zhao", "Fei Wang." ],
      "venue" : "AAAI, pages 817–824.",
      "citeRegEx" : "Zhao et al\\.,? 2019",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Disease is one of the fundamental entities in biomedical research, thus it is one of the most searched topics in the biomedical literature (Dogan et al., 2009) and the internet (Brownstein et al.",
      "startOffset" : 139,
      "endOffset" : 159
    }, {
      "referenceID" : 36,
      "context" : ") offers a tremendous opportunity for many downstream applications, such as mining chemical-disease relations from the literature (Wei et al., 2015), and providing much more relevant resources based on the search queries (Dogan et al.",
      "startOffset" : 130,
      "endOffset" : 148
    }, {
      "referenceID" : 3,
      "context" : ", 2015), and providing much more relevant resources based on the search queries (Dogan et al., 2014), etc.",
      "startOffset" : 80,
      "endOffset" : 100
    }, {
      "referenceID" : 15,
      "context" : "Previous studies (Leaman and Lu, 2016; Lou et al., 2017; Zhao et al., 2019) show the effectiveness of the joint methods for the end-to-end disease recognition and normalization (aka linking) task to alleviated the error propagation problem of the traditional pipelined solutions (Strubell et al.",
      "startOffset" : 17,
      "endOffset" : 75
    }, {
      "referenceID" : 21,
      "context" : "Previous studies (Leaman and Lu, 2016; Lou et al., 2017; Zhao et al., 2019) show the effectiveness of the joint methods for the end-to-end disease recognition and normalization (aka linking) task to alleviated the error propagation problem of the traditional pipelined solutions (Strubell et al.",
      "startOffset" : 17,
      "endOffset" : 75
    }, {
      "referenceID" : 42,
      "context" : "Previous studies (Leaman and Lu, 2016; Lou et al., 2017; Zhao et al., 2019) show the effectiveness of the joint methods for the end-to-end disease recognition and normalization (aka linking) task to alleviated the error propagation problem of the traditional pipelined solutions (Strubell et al.",
      "startOffset" : 17,
      "endOffset" : 75
    }, {
      "referenceID" : 28,
      "context" : ", 2019) show the effectiveness of the joint methods for the end-to-end disease recognition and normalization (aka linking) task to alleviated the error propagation problem of the traditional pipelined solutions (Strubell et al., 2017; Leaman et al., 2013; Xu et al., 2016, 2017).",
      "startOffset" : 211,
      "endOffset" : 278
    }, {
      "referenceID" : 14,
      "context" : ", 2019) show the effectiveness of the joint methods for the end-to-end disease recognition and normalization (aka linking) task to alleviated the error propagation problem of the traditional pipelined solutions (Strubell et al., 2017; Leaman et al., 2013; Xu et al., 2016, 2017).",
      "startOffset" : 211,
      "endOffset" : 278
    }, {
      "referenceID" : 15,
      "context" : "Although TaggerOne (Leaman and Lu, 2016) and the discrete transition-based joint model (Lou et al.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 21,
      "context" : "Although TaggerOne (Leaman and Lu, 2016) and the discrete transition-based joint model (Lou et al., 2017) successfully alleviate the error propagation problem, they heavily rely on hand-craft feature engineering.",
      "startOffset" : 87,
      "endOffset" : 105
    }, {
      "referenceID" : 42,
      "context" : "(Zhao et al., 2019) proposes a neural joint model based on the multi-task learning framework (i.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 42,
      "context" : "In this work, we propose a novel neural transition-based joint model named NeuJoRN for disease named entity recognition and normalization, to alleviate these two issues of the multi-task learning based solution (Zhao et al., 2019).",
      "startOffset" : 211,
      "endOffset" : 230
    }, {
      "referenceID" : 35,
      "context" : "parser (Watanabe and Sumita, 2015; Lample et al., 2016), which constructs the output of each given sentence x and controlled vocabulary KB through state transitions with a sequence of actions A.",
      "startOffset" : 7,
      "endOffset" : 55
    }, {
      "referenceID" : 13,
      "context" : "parser (Watanabe and Sumita, 2015; Lample et al., 2016), which constructs the output of each given sentence x and controlled vocabulary KB through state transitions with a sequence of actions A.",
      "startOffset" : 7,
      "endOffset" : 55
    }, {
      "referenceID" : 22,
      "context" : "where vchar i denotes its character-level word representation learned by using a CNN network (Ma and Hovy, 2016), vw i denotes its non-contextual word representation initialized with Glove (Pennington et al.",
      "startOffset" : 93,
      "endOffset" : 112
    }, {
      "referenceID" : 24,
      "context" : "where vchar i denotes its character-level word representation learned by using a CNN network (Ma and Hovy, 2016), vw i denotes its non-contextual word representation initialized with Glove (Pennington et al., 2014) embeddings, which is pre-trained on 6 billion words from Wikipedia and web text, and ELMoi denotes its contextual word representation initialized with ELMo (Peters et al.",
      "startOffset" : 189,
      "endOffset" : 214
    }, {
      "referenceID" : 25,
      "context" : ", 2014) embeddings, which is pre-trained on 6 billion words from Wikipedia and web text, and ELMoi denotes its contextual word representation initialized with ELMo (Peters et al., 2018).",
      "startOffset" : 164,
      "endOffset" : 185
    }, {
      "referenceID" : 2,
      "context" : "We can also explore the contextual word representation from BERT (Devlin et al., 2018) by averaging the embeddings of the subwords of each word.",
      "startOffset" : 65,
      "endOffset" : 86
    }, {
      "referenceID" : 7,
      "context" : "We then run a BiLSTM (Graves et al., 2013) to derive the contextual representation of each word in the sentence x.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 7,
      "context" : "The buffer βt is represented with BiLSTM (Graves et al., 2013) to represent the words in the buffer:",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 6,
      "context" : "The stack σt and the actions At are represented with StackLSTM (Dyer et al., 2015):",
      "startOffset" : 63,
      "endOffset" : 82
    }, {
      "referenceID" : 29,
      "context" : "• m′ and c′ are the representations of the mention and candidate concept by applying CoAttention mechanism (Tay et al., 2018; Jia et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 143
    }, {
      "referenceID" : 11,
      "context" : "• m′ and c′ are the representations of the mention and candidate concept by applying CoAttention mechanism (Tay et al., 2018; Jia et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 143
    }, {
      "referenceID" : 7,
      "context" : "• c denotes the candidate concept representation by (i) first run a BiLSTM (Graves et al., 2013) to derive the contextual representation of each word in the candidate concept, and (ii) then applying max-pooling operation to aggregate the representations of all concept words.",
      "startOffset" : 75,
      "endOffset" : 96
    }, {
      "referenceID" : 32,
      "context" : "Greedy Search For efficient decoding, a widelyused greedy search algorithm (Wang et al., 2017) can be adopted to minimize the negative loglikelihood of the local action classifier in Equation (3, 8, 9).",
      "startOffset" : 75,
      "endOffset" : 94
    }, {
      "referenceID" : 32,
      "context" : "Beam Search The main drawback of greedy search is error propagation (Wang et al., 2017).",
      "startOffset" : 68,
      "endOffset" : 87
    }, {
      "referenceID" : 37,
      "context" : "In this work, we use the Beam-Search Optimization (BSO) method with LaSO update (Wiseman and Rush, 2016) to train our beam-search model, where the max-margin loss is adopted.",
      "startOffset" : 80,
      "endOffset" : 104
    }, {
      "referenceID" : 3,
      "context" : "We use two public available datasets in this study, namely NCBI - the NCBI disease corpus (Dogan et al., 2014) and BC5CDR - the BioCreative V CDR task corpus (Li et al.",
      "startOffset" : 90,
      "endOffset" : 110
    }, {
      "referenceID" : 5,
      "context" : "To facilitate the generation of candidate linking actions, we perform some preprocessing steps of each candidate mention and each concept in KB with the following strategies: (i) Spelling Correction - for each candidate mention in the datasets, we replace all the misspelled words using a spelling check list as in previous work (D’Souza and Ng, 2015; Li et al., 2017).",
      "startOffset" : 329,
      "endOffset" : 368
    }, {
      "referenceID" : 17,
      "context" : "To facilitate the generation of candidate linking actions, we perform some preprocessing steps of each candidate mention and each concept in KB with the following strategies: (i) Spelling Correction - for each candidate mention in the datasets, we replace all the misspelled words using a spelling check list as in previous work (D’Souza and Ng, 2015; Li et al., 2017).",
      "startOffset" : 329,
      "endOffset" : 368
    }, {
      "referenceID" : 27,
      "context" : "(ii) Abbreviation Resolution - we use Ab3p (Sohn et al., 2008) toolkit to detect and replace the abbreviations with their long forms within each document and also expand all possible abbreviated disease mentions using a dictionary collected from Wikipedia as in previous work (D’Souza and Ng, 2015; Li et al.",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 5,
      "context" : ", 2008) toolkit to detect and replace the abbreviations with their long forms within each document and also expand all possible abbreviated disease mentions using a dictionary collected from Wikipedia as in previous work (D’Souza and Ng, 2015; Li et al., 2017).",
      "startOffset" : 221,
      "endOffset" : 260
    }, {
      "referenceID" : 17,
      "context" : ", 2008) toolkit to detect and replace the abbreviations with their long forms within each document and also expand all possible abbreviated disease mentions using a dictionary collected from Wikipedia as in previous work (D’Souza and Ng, 2015; Li et al., 2017).",
      "startOffset" : 221,
      "endOffset" : 260
    }, {
      "referenceID" : 5,
      "context" : "(iii) Numeric Synonyms Resolutions - we replace all the numerical words in the mentions and concepts to their corresponding Arabic numerals as in previous work (D’Souza and Ng, 2015; Li et al., 2017).",
      "startOffset" : 160,
      "endOffset" : 199
    }, {
      "referenceID" : 17,
      "context" : "(iii) Numeric Synonyms Resolutions - we replace all the numerical words in the mentions and concepts to their corresponding Arabic numerals as in previous work (D’Souza and Ng, 2015; Li et al., 2017).",
      "startOffset" : 160,
      "endOffset" : 199
    }, {
      "referenceID" : 15,
      "context" : "Following previous work (Leaman and Lu, 2016; Lou et al., 2017; Zhao et al., 2019), we utilize the evaluation kit1 for evaluating the model performances.",
      "startOffset" : 24,
      "endOffset" : 82
    }, {
      "referenceID" : 21,
      "context" : "Following previous work (Leaman and Lu, 2016; Lou et al., 2017; Zhao et al., 2019), we utilize the evaluation kit1 for evaluating the model performances.",
      "startOffset" : 24,
      "endOffset" : 82
    }, {
      "referenceID" : 42,
      "context" : "Following previous work (Leaman and Lu, 2016; Lou et al., 2017; Zhao et al., 2019), we utilize the evaluation kit1 for evaluating the model performances.",
      "startOffset" : 24,
      "endOffset" : 82
    }, {
      "referenceID" : 20,
      "context" : "We use the AdamW optimizer (Loshchilov and Hutter, 2019) for parameter optimization.",
      "startOffset" : 27,
      "endOffset" : 56
    }, {
      "referenceID" : 14,
      "context" : "DNorm (Leaman et al., 2013) is a traditional method, which needs feature engineering.",
      "startOffset" : 6,
      "endOffset" : 27
    }, {
      "referenceID" : 28,
      "context" : "IDCNN (Strubell et al., 2017) is a neural model based on BiLSTM-CRF, which requires few effort of feature engineering.",
      "startOffset" : 6,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : "TaggerOne (Leaman et al., 2013) is a joint solution based on semi-CRF.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 21,
      "context" : "Transition-based Model (Lou et al., 2017) is a joint solution based on discrete transition-based method.",
      "startOffset" : 23,
      "endOffset" : 41
    }, {
      "referenceID" : 42,
      "context" : "MTL-feedback (Zhao et al., 2019) is neural joint solution based on multi-task learning.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 14,
      "context" : "Most previous studies (Leaman et al., 2013; Xu et al., 2015, 2016) transform this task as a sequence labeling task, and conditional random fields (CRF) based methods are widely adopted to achieve good performance.",
      "startOffset" : 22,
      "endOffset" : 66
    }, {
      "referenceID" : 28,
      "context" : "Recently, neural models such as BiLSTM-CRF based methods (Strubell et al., 2017; Wang et al., 2019) and BERT-based methods (Kim et al.",
      "startOffset" : 57,
      "endOffset" : 99
    }, {
      "referenceID" : 33,
      "context" : "Recently, neural models such as BiLSTM-CRF based methods (Strubell et al., 2017; Wang et al., 2019) and BERT-based methods (Kim et al.",
      "startOffset" : 57,
      "endOffset" : 99
    }, {
      "referenceID" : 12,
      "context" : ", 2019) and BERT-based methods (Kim et al., 2019) have achieved state-of-theart performance.",
      "startOffset" : 31,
      "endOffset" : 49
    }, {
      "referenceID" : 21,
      "context" : "Most studies assume that the entity mentions are predetected by a separate DNER model, and focus on developing methods to improve the normaliation accuracy (Lou et al., 2017), resulting in developing rule-based methods (D’Souza and Ng, 2015), machine learning-based methods (Leaman et al.",
      "startOffset" : 156,
      "endOffset" : 174
    }, {
      "referenceID" : 5,
      "context" : ", 2017), resulting in developing rule-based methods (D’Souza and Ng, 2015), machine learning-based methods (Leaman et al.",
      "startOffset" : 52,
      "endOffset" : 74
    }, {
      "referenceID" : 14,
      "context" : ", 2017), resulting in developing rule-based methods (D’Souza and Ng, 2015), machine learning-based methods (Leaman et al., 2013; Xu et al., 2017), and recent deep learning-based methods (Li et al.",
      "startOffset" : 107,
      "endOffset" : 145
    }, {
      "referenceID" : 38,
      "context" : ", 2017), resulting in developing rule-based methods (D’Souza and Ng, 2015), machine learning-based methods (Leaman et al., 2013; Xu et al., 2017), and recent deep learning-based methods (Li et al.",
      "startOffset" : 107,
      "endOffset" : 145
    }, {
      "referenceID" : 17,
      "context" : ", 2017), and recent deep learning-based methods (Li et al., 2017; Ji et al., 2020; Wang et al., 2020; Vashishth et al., 2021; Chen et al., 2021).",
      "startOffset" : 48,
      "endOffset" : 144
    }, {
      "referenceID" : 10,
      "context" : ", 2017), and recent deep learning-based methods (Li et al., 2017; Ji et al., 2020; Wang et al., 2020; Vashishth et al., 2021; Chen et al., 2021).",
      "startOffset" : 48,
      "endOffset" : 144
    }, {
      "referenceID" : 31,
      "context" : ", 2017), and recent deep learning-based methods (Li et al., 2017; Ji et al., 2020; Wang et al., 2020; Vashishth et al., 2021; Chen et al., 2021).",
      "startOffset" : 48,
      "endOffset" : 144
    }, {
      "referenceID" : 30,
      "context" : ", 2017), and recent deep learning-based methods (Li et al., 2017; Ji et al., 2020; Wang et al., 2020; Vashishth et al., 2021; Chen et al., 2021).",
      "startOffset" : 48,
      "endOffset" : 144
    }, {
      "referenceID" : 1,
      "context" : ", 2017), and recent deep learning-based methods (Li et al., 2017; Ji et al., 2020; Wang et al., 2020; Vashishth et al., 2021; Chen et al., 2021).",
      "startOffset" : 48,
      "endOffset" : 144
    }, {
      "referenceID" : 15,
      "context" : "Joint DNER and DNEN Several studies (Leaman and Lu, 2016; Lou et al., 2017; Zhao et al., 2019) show the effectiveness of the joint methods to alleviated the error propagation problem.",
      "startOffset" : 36,
      "endOffset" : 94
    }, {
      "referenceID" : 21,
      "context" : "Joint DNER and DNEN Several studies (Leaman and Lu, 2016; Lou et al., 2017; Zhao et al., 2019) show the effectiveness of the joint methods to alleviated the error propagation problem.",
      "startOffset" : 36,
      "endOffset" : 94
    }, {
      "referenceID" : 42,
      "context" : "Joint DNER and DNEN Several studies (Leaman and Lu, 2016; Lou et al., 2017; Zhao et al., 2019) show the effectiveness of the joint methods to alleviated the error propagation problem.",
      "startOffset" : 36,
      "endOffset" : 94
    }, {
      "referenceID" : 15,
      "context" : "Although TaggerOne (Leaman and Lu, 2016) and the discrete transition-based joint model (Lou et al.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 21,
      "context" : "Although TaggerOne (Leaman and Lu, 2016) and the discrete transition-based joint model (Lou et al., 2017) successfully alleviated the error propagation problem, they heavily rely on hand-craft feature engineering.",
      "startOffset" : 87,
      "endOffset" : 105
    }, {
      "referenceID" : 42,
      "context" : "(Zhao et al., 2019) propose a neural joint model based on the multi-task learning framework (i.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 35,
      "context" : "Transition-based Models Transition-based models are widely used in parsing and translation (Watanabe and Sumita, 2015; Wang et al., 2018; Meng and Zhang, 2019).",
      "startOffset" : 91,
      "endOffset" : 159
    }, {
      "referenceID" : 34,
      "context" : "Transition-based Models Transition-based models are widely used in parsing and translation (Watanabe and Sumita, 2015; Wang et al., 2018; Meng and Zhang, 2019).",
      "startOffset" : 91,
      "endOffset" : 159
    }, {
      "referenceID" : 23,
      "context" : "Transition-based Models Transition-based models are widely used in parsing and translation (Watanabe and Sumita, 2015; Wang et al., 2018; Meng and Zhang, 2019).",
      "startOffset" : 91,
      "endOffset" : 159
    }, {
      "referenceID" : 41,
      "context" : "Recently, these models are successfully applied to information extraction tasks, such as joint POS tagging and dependency parsing (Yang et al., 2018), joint entity and relation extraction (Li and Ji, 2014; Li et al.",
      "startOffset" : 130,
      "endOffset" : 149
    }, {
      "referenceID" : 19,
      "context" : ", 2018), joint entity and relation extraction (Li and Ji, 2014; Li et al., 2016a; Ji et al., 2021).",
      "startOffset" : 46,
      "endOffset" : 98
    }, {
      "referenceID" : 16,
      "context" : ", 2018), joint entity and relation extraction (Li and Ji, 2014; Li et al., 2016a; Ji et al., 2021).",
      "startOffset" : 46,
      "endOffset" : 98
    }, {
      "referenceID" : 8,
      "context" : ", 2018), joint entity and relation extraction (Li and Ji, 2014; Li et al., 2016a; Ji et al., 2021).",
      "startOffset" : 46,
      "endOffset" : 98
    }, {
      "referenceID" : 26,
      "context" : "Several studies propose discrete transition-based joint model for entity recognition and normalization(Qian et al., 2015; Ji et al., 2016; Lou et al., 2017).",
      "startOffset" : 102,
      "endOffset" : 156
    }, {
      "referenceID" : 9,
      "context" : "Several studies propose discrete transition-based joint model for entity recognition and normalization(Qian et al., 2015; Ji et al., 2016; Lou et al., 2017).",
      "startOffset" : 102,
      "endOffset" : 156
    }, {
      "referenceID" : 21,
      "context" : "Several studies propose discrete transition-based joint model for entity recognition and normalization(Qian et al., 2015; Ji et al., 2016; Lou et al., 2017).",
      "startOffset" : 102,
      "endOffset" : 156
    } ],
    "year" : 2021,
    "abstractText" : "Disease is one of the fundamental entities in biomedical research. Recognizing such entities from biomedical text and then normalizing them to a standardized disease vocabulary offer a tremendous opportunity for many downstream applications. Previous studies have demonstrated that joint modeling of the two sub-tasks has superior performance than the pipelined counterpart. Although the neural joint model based on multi-task learning framework has achieved state-of-the-art performance, it suffers from the boundary inconsistency problem due to the separate decoding procedures. Moreover, it ignores the rich information (e.g., the text surface form) of each candidate concept in the vocabulary, which is quite essential for entity normalization. In this work, we propose a neural transition-based joint model to alleviate these two issues. We transform the end-to-end disease recognition and normalization task as an action sequence prediction task, which not only jointly learns the model with shared representations of the input, but also jointly searches the output by state transitions in one search space. Moreover, we introduce attention mechanisms to take advantage of the text surface form of each candidate concept for better normalization performance. Experimental results conducted on two publicly available datasets show the effectiveness of the proposed method.",
    "creator" : "LaTeX with hyperref"
  }
}