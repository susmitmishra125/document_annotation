{
  "name" : "2021.acl-long.485.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "An End-to-End Progressive Multi-Task Learning Framework for Medical Named Entity Recognition and Normalization",
    "authors" : [ "Baohang Zhou", "Xiangrui Cai", "Ying Zhang", "Xiaojie Yuan" ],
    "emails" : [ "zhoubh@mail.nankai.edu.cn,", "yingzhang@nankai.edu.cn", "caixr@nankai.edu.cn", "yuanxj@nankai.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6214–6224\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6214"
    }, {
      "heading" : "1 Introduction",
      "text" : "To dig into the large amount of electronic medical records, there has been an increasing interest in applying information extraction to them. These techniques can generate tremendous benefit for corresponding research and applications, such as med-\n∗Corresponding author.\nPipeline Framework Parallel Multi-task Framework\nical knowledge graph (Wu et al., 2019) and QA systems (Lamurias and Couto, 2019). Among the medical text mining tasks, medical named entity recognition and normalization are the most fundamental tasks.\nNamed entity recognition tries to find the boundaries of mentions from the medical texts. And named entity normalization maps mentions extracted from the medical text to standard identifiers, such as MeSH and OMIM (Zhao et al., 2019). The initial pipeline implementations for medical NER and NEN have a main limitation: error extractions from NER cascade into NEN which result in normalization errors. Besides, the mutual use between recognition and normalization is not utilized in the pipeline models. To alleviate the limitations and achieve a higher performance, some researchers focused on jointly modeling these two tasks. Leaman and Lu (2016) proposed a joint scoring function for medical NER and NEN. Lou et al. (2017) casted the output construction process of the two tasks\nas a state transition process to perform medical named entity recognition and normalization. To capture the semantic features of two tasks, Zhao et al. (2019) proposed a multi-task learning framework with an explicit feedback strategy for medical NER and NEN.\nAs shown in Figure 1, there are two common frameworks: pipeline and parallel multi-task framework. The former one is formulated to maximize the posterior probabilities p(yNER |x) and p(yNEN |m, e) where x is the medical text, m is the medical mentions extracted by a recognition model, e is the standard entity, yNER and yNEN are the labels. The latter one tries to maximize the posterior probabilities p(yNER, yNEN |x) (Zhao et al., 2019). Both of these are struggled with the bottleneck that is named entity recognition. In the above frameworks, the NER module is trained to memorize the medial mentions in the training set. However, the medical mentions are various and there is a gap between the training and test set. It is natural that the unseen mentions in training set are hard to recognize during the testing phase. Therefore, the conventional frameworks do not gain more ideal generalization ability.\nTo overcome the disadvantage mentioned above, we reconsidered the process of medical named entity recognition and normalization. The ultimate goal is to map the extracted medical mentions to the standard entity base. Therefore, the target standard entity base can be regarded as a dictionary. The initial process of NEN and NER can be reconsidered as detecting whether the medical text contains the candidate standard entity and finding the mentions should be replaced. Based on this idea, we propose an end-to-end progressive multi-task learning framework for medical named entity recognition and normalization (E2EMERN1). Compared with ordinary multi-task learning, progressive multi-task learning focuses on the aggregation logic of tasks’ specific features (Hong et al., 2020). A difficult target is divided into a few tasks that are interconnected through the combination of features. To take full advantage of the data attributes, we propose the framework including three tasks with progressive difficulty extended from the conventional NER and NEN tasks. The low-level task is the traditional NER which tries to extract all entities in the medical text. The mid-level task is defined to iden-\n1When ready, the code will be published at https:// github.com/zhoubaohang/E2EMERN\ntify whether there exist medical mentions in the text that should be mapped to the candidate standard entity. The high-level task combines the first two level tasks, and targets to extract the mentions which should be mapped to the candidate standard entity.\nUnlike the existing frameworks, E2EMERN exploits the progressive tasks to learn the fine-grained representations. The mid-level and high-level tasks facilitate the framework learning the corresponding features between the medical mentions and standard entities. The low-level task can gain the supervised signals from the higher level tasks to extract medical mentions corresponded to standard entities in the knowledge bases more exactly. Our contributions in this manuscript can be summarized as follows:\n1. We reconsider the process of the NER and NEN tasks, and firstly propose to exploit the three tasks with progressive difficulty to train the end-to-end medical named entity recognition and normalization framework.\n2. The experimental results on two medical benchmarks demonstrate that our framework outperforms the existing medical named entity recognition and normalization models. And we conducted detailed analysis on the framework to represent its superiority."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Medical Named Entity Recognition and Normalization",
      "text" : "Medical named entity recognition and normalization are two basic tasks for the medical text mining. The conventional pipeline frameworks contains the NER model and NEN one separately (Vázquez et al., 2008; Leaman and Lu, 2014; Sahu and Anand, 2016; Zhou et al., 2020). NER models extract medical mentions in texts and then NEN models map these mentions to standard entity identifiers. To reduce the error propagation in the pipeline frameworks, some researchers proposed to model NER and NEN jointly. Leaman et al. (2015) combined two traditional machine learning models as an ensemble NER and NEN model. And to learn the joint probability distribution of the NER and NEN tasks, a semi-markov based model was proposed by Leaman and Lu (2016). However, traditional methods depend on the human-based feature engineering. With the development of the deep\nlearning, recurrent neural networks (RNN) have replaced human effort and been utilized to extract features of raw texts. Zhao et al. (2019) designed an RNN-based network architecture with feedback strategy to model the two tasks jointly. Recently, the pre-trained models, such as BERT (Devlin et al., 2019), BioBERT (Lee et al., 2020), make impressive progress in the natural language processing (NLP) area. Xiong et al. (2020) used BERT as the base module and proposed a machine reading comprehension framework to solve the NER and NEN problems jointly."
    }, {
      "heading" : "2.2 Sequence Labeling",
      "text" : "Named entity recognition can be regarded as a sequence labeling problem. Sequence labeling was explored extensively as a basic task in NLP. Probabilistic graphical models, such as: hidden markov model (Xiao et al., 2005) and conditional random fields (CRF) (Lafferty et al., 2001) are the typical methods to solve the problem. With deep learning modules gradually replacing manual feature engineering, long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) network stacked with CRF (Xu et al., 2008) has been a benchmark model for sequence labeling (Lample et al., 2016). Some researchers utilized multi-task learning to model relevant NLP tasks and gained better performances on these tasks including sequence labeling (Aguilar et al., 2017; Cao et al., 2018). Besides, the attributes of the data themselves are used to design the multi-task learning model. Considering whether sentences contain entities, Wang et al. (2019) proposed the multitask learning model to predict whether input data have entities and then extract corresponding entities. Kruengkrai et al. (2020) exploited sentencelevel labels and token-level labels to propose a joint model supporting multi-class classification."
    }, {
      "heading" : "2.3 Short Text Matching",
      "text" : "Named entity normalization is formulated as a short text matching problem. The information retrieval method, such as: BM25 (Robertson et al., 1994), is a universal model to solve this problem. With the development of neural language model, text semantic is exploited to model the similarity between two short texts. The distributed representations of texts, such as: Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), are utilized to calculate the similarity distance between two texts. Some medical named entity normalization models\nare based on this method (Leaman and Lu, 2014; Zhou et al., 2020). Considering local texts are more important than global ones, some researchers utilized convolution neural networks (CNN) to extract local features and exploited interactive attention mechanism to match the semantic similarity of two texts (Yin et al., 2016; Chen et al., 2018)."
    }, {
      "heading" : "3 Methodology",
      "text" : "We introduce the notations about NER and NEN before getting into the details of the framework. For NER task, we denote {(Xi, yi)}N s\ni=1 as a training set with N s samples, where Xi is the medical text and yi is the NER label. Given a sentence withNw words, the medical text can be formulated as X = {x1, x2, . . . , xNw} and the NER label is y = {y1, y2, . . . , yNw}. To solve the NER task, we try to maximize the posterior probability p(y |X). According to the NER label, we can extract the medical mentions {mi}N m\ni=1 from the medical text, where Nm is the number of the mentions. For NEN task, we need to map each mention m to a standard entity e in the entity base B = {ei}N e\ni=1. We formulate the object of NEN task as a posterior probability p(e |m,B), and e is the standard entity which the mention m should be mapped to."
    }, {
      "heading" : "3.1 Progressive Tasks",
      "text" : "With the help of NER and NEN, we can map medical mentions in the raw texts to the corresponding standard entities. Traditional pipeline implementations for the two tasks are composed of the individual NER and NEN models. The simple partitioning of the two models leads to the error propagation between them. Considering the correlation between the two tasks, Zhao et al. (2019) proposed the parallel task framework to improve the performance of the model. However, the intuitive feedback strategy for the output layers of two tasks is not beneficial to modeling the fine-grained features between two tasks. The above implementations lack thinking about the learning process. The process of human learning often goes from easy to difficult (Xu et al., 2020). Especially for the correlated tasks, humans can dig into the hidden knowledge and extract them from the easy tasks for completing the hard ones. Based on this idea, we reconsider the process of conventional NER and NEN tasks, and propose three correlated tasks with progressive difficulty. As shown in Figure 2, we take a medical text from the real dataset NCBI (Dogan et al., 2014)\nas an example to describe the tasks. The medical text is “Familial Mediterranean fever is a recessive disorder” and its corresponding NER label is “B-Disease I-Disease I-Disease O O B-Disease IDisease”. Among the tokens, medical mentions “Familial Mediterranean fever” and “recessive disorder” are mapped to the standard entity identifiers “D010505” and “D030342” respectively.\nLow-level task is defined to memorize all medical mentions seen in training set. Given the medical text mentioned above, this task needs to predict the NER label and extract the mentions “Familial Mediterranean fever” and “recessive disorder”. Similar to the process of human learning vocabulary, the low-level task forces the framework to learn the medical mentions indiscriminantly. However, the final target is to map mentions to standard entities. We should continue to bridge the gap between medical mentions in raw texts and standard entities in the database.\nMid-level task targets to determine whether medical texts implicit the query standard entities. With the above medical text and the standard entity “D010505” as input, this task should inference the text contains this entity. Through this task, the framework establishes the coarse-grained relationship between the mentions with contexts and the query standard entities. However, the mentions are incomplete correspondence to the query standard entities. Because there is more than one mention in the raw text which should be extracted and mapped\nto the corresponding standard entities. We need to specify which mention in the text should be mapped to the input standard entity.\nHigh-level task is proposed to extract the mentions which should be mapped to the query standard entity. After acquiring the above medical text and the standard entity “D030342”, this task should extract the mention “recessive disorder”. If the input text contains no mention which should be mapped to the query entity, the output of this task is empty. The effect of this task is the same as that of NEN, but it is harder than NEN. To accomplish the high-level task, we need to build on the first two tasks. The low-level task provides the representations of the medical mentions with contexts which is beneficial to locating them in raw texts. The midlevel task forces the model to learn the correlated features between mentions with standard entities. With the help of two pre-tasks, the high-level task can be accomplished in an effective way."
    }, {
      "heading" : "3.2 Implementation Details",
      "text" : "We build on the progressive tasks to implement the framework E2EMERN as shown in Figure 2. Considering the logic of feature aggregation and the strategies for training different tasks, we need to give detailed explanations by the level of tasks.\nFor a given sentence X = {x1, x2, . . . , xNw}, we need to map it to the dense vector representations. With the impressive performances of pretrained models, we utilize BERT (Devlin et al.,\n2019) as feature extractors to acquire the distributed representations of sentences. The BERT architecture is composed by the transformer networks and its weights are trained with large number of corpus. The feature extraction process is simplified as BERT(X) = {h1,h2, . . . ,hNw}, where h ∈ R1024×1. The low-level task is defined as the same as NER, and we utilize the NER labels as the target. The sentence features {hi}N w\ni=1 are fed into the softmax layer, and we can compute the prediction probabilities of low-level task as: ŷi = softmax(Wlhi + bl) where Wl and bl are trainable parameters. For training, we utilize the cross-entropy loss as the objective function. The loss function of low-level task is defined as follows:\nLlow = − Nw∑ i=1 yi log ŷi. (1)\nThe sample for the mid-level task is defined as a tuple (X, e, ym). If the text X contain the mentions which should be mapped to the entity e, ym is assigned 1 otherwise 0. To bridge the gap between the mentions and standard entities in the mid-level task, we need also to extract the features of standard entities. The standard entity e is described with the specific name and some medical contents. We feed the name (or contents) of the entity into the BERT and perform the average pooling on the output of BERT. The feature vector of i-th standard entity in the database is defined as hei . Considering the words of mentions in raw texts are more correlated to the standard entity, we adopt the attention mechanism (Zhou et al., 2016) to focus on the local words of sentences. The attention weighted average feature can be calculated as: ha = ∑Nw i=1 αixi. And the attention score α is defined as: αi = exp (s(xi,he))∑Nw i=1 exp (s(xi,h e)) where s(xi,he) = Wa[xi;he] + ba. Wa and ba are trainable weights in the attention module. After acquiring the entity-attention feature ha and standard entity feature he, we can calculate the prediction probabilities ŷm = σ(Wm[he;ha] + bm) where σ is the sigmoid function. The loss function for the mid-level task is formulated as the cross-entropy:\nLmid = −(ym log ŷm + (1− ym) log(1− ŷm)). (2)\nWe define the tuple (X, e, yh) as the sample for the high-level task where yh = {yhi }N w\ni=1. Given that the medical text X is “Familial Mediterranean fever is a recessive disorder.” and standard en-\ntity e is “D030342”, the label sequence yh should be “O O O O O B-Disease I-Disease”. To take advantage of the pre-tasks, we propose the gate mechanism to aggregate the different features for solving this task. The sentence feature {hi}N w\ni=1\nimplicit the medical mentions while the entity attention feature ha contains clearer locations of the corresponding mentions. Therefore, we propose the gate mechanism to focus on the fine-grained feature dimensions. The formulation of the gate mechanism is G(H,Ha) = σ(Wg[H;Ha] + bg) where H = {hi}N w i=1 and H a = [ha; . . . ;ha] ∈ R1024×Nw . Considering the semantic difference between the mentions and corresponding standard entities, we exploit the gate mechanism to fuse the standard entity feature with the sentence feature. The fusion sentence feature is formulated as: Hf = H (1−G(H,Ha))+He G(H,Ha) where is the element-wise production, Hf = {hfi }N w i=1 and He = [he; . . . ;he] ∈ R1024×Nw . We feed the fusion feature into the softmax layer to predict the probabilities ŷhi = softmax(Whh f i + bh). As the same as the low-level task, we utilize the crossentropy loss function as follows:\nLhigh = − Nw∑ i=1 yhi log ŷ h i . (3)"
    }, {
      "heading" : "3.3 Training Process",
      "text" : "For the framework, we denote the training sample as (X, y, e, ym, yh). According to the definitions of the three tasks, we can generate the task labels corresponding to the input sentence. The example is shown in Figure 3. Given the medical text X, the label y for the low-level task is the same as the original NER label. We use the standard entities which the mentions {mi}N m\ni=1 should be mapped to as the input entity e respectively. The high-level task label yh is based on y, and it only keeps the original labels of y which are correlated to the input e. Besides, we adopt the negative sampling strategy\nto select the standard entity which is not related to the input sentence X as the input entity e.\nTo tackle the three level tasks at once, we introduce two hyper-parameters to sum Eqn. 1, Eqn. 2 and Eqn. 3. The overall loss function for the framework is defined as follows:\nL = Llow + λ · Lmid + µ · Lhigh (4)\nwhere λ and µ are hyper-parameters for balancing different task losses. After generating samples, we feed them into the model and then calculate the loss according to Eqn. 4. Following the backpropagation method, we update the weights of the networks with the acquired loss. After every epoch of training, we re-sample the training samples for better generalization of the model."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets and Experiment Settings",
      "text" : "We compare our framework with the existing methods on two medical benchmark datasets. Table 1 presents the detailed statistical information of the two datasets. There are 798 public medical abstracts in the NCBI dataset (Dogan et al., 2014). Each medical mention in the text is annotated with MeSH/OMIM identifiers. BC5CDR dataset (Li et al., 2016) contains 1500 public medical abstracts which are also annotated with MeSH identifiers. We split each abstract into sentence samples with an average of 40 words according to the ends of sentences. The padding char is used for filling the unequal length samples to the fixed length.\nDuring the training process, we first train the model on the training set and test it on the development set for searching the best hyper-parameters. Then, we fix the best hyper-parameters and train the model on the set composed of the training and development sets. Before the model is trained to the searched maximum number of epochs, we take the F1 score as the reported result when the loss gets the lowest. In our experiments, we set the hyper-parameters λ, µ and learning rate to 0.125, 0.1 and 1e-5 respectively. To train the model, we use the ADAM (Kingma and Ba, 2015) algorithm to update the weights. And all experiments are accelerated by the two NVIDIA GTX 2080Ti devices."
    }, {
      "heading" : "4.2 Compared Methods",
      "text" : "To represent the effectiveness of our framework, we adopt the competitive models as the compared\nmethods including traditional machine learning methods and impressive deep learning models.\nDnorm (Leaman et al., 2013) is the pipeline model for medical NER and NEN. It utilizes the TF-IDF feature to learn the bilinear mapping matrix for the normalization task. LeadMine (Lowe et al., 2015) considers Wikipedia as dictionary features for normalizing the medical mentions. TaggerOne (Leaman and Lu, 2016) is the semiMarkov based model for jointly modeling medical NER and NEN. Transition-based model (Lou et al., 2017) consists of the state transformation function for the output of NER and NEN.\nTo reduce human feature engineering, researchers focus on the deep learning for modeling NER and NEN. IDCNN (Strubell et al., 2017) was proposed with an improved CNN module for NER. MCNN (Zhao et al., 2017) was composed of the multiple-label CNN modules for better performances on NER. CollaboNet (Yoon et al., 2019) exploited the multi-source datasets for training the multi-task model and gained better results on all benchmark datasets. MTL-MERN (Zhao et al., 2019) consists of the NER and NEN parallel framework and utilizes the feedback strategy to improve the performances on two tasks.\nWith the impressive performance of pre-trained models, BioBERT (Lee et al., 2020) is built on the BERT (Devlin et al., 2019) and trained with a large medical corpus. And it achieves state-ofthe-art results on medical NER datasets. Therefore, we use the BioBERT as the feature extractor and compare it with our framework."
    }, {
      "heading" : "4.3 Experimental Results",
      "text" : "We compare E2EMERN with the baseline methods on the named entity recognition and normalization. The detailed experiment results on NCBI and BC5CDR are shown in Table 2. The first\nfour in the table is the traditional machine learning methods. Among them, the joint models, such as TaggerOne and Transition-based Model, outperform the pipeline ones including Dnorm and LeadMine. When deep learning was introduced into the pipeline frameworks, IDCNN can make a progress over conventional methods, such as Dnorm. Compared with MCNN, CollaboNet utilizes the multisource dataset as input and performs multi-task learning to improve the performances on NER task. MTL-MERN takes full advantage of multi-task learning and deep semantic representations and outperforms the above methods. By virtue of the dynamic language features, BioBERT can better model the language semantics and outperform the above NER models.\nCompared with baseline methods, E2EMERN can always achieve the best results on NER and NEN. The NER results of E2EMERN increase by 1% ∼ 2% over BioBERT. Because our framework takes full advantage of the correlation between NER and NEN. Unlike the simple strategy of MTL-MERN, E2EMERN consists of three progressive tasks that are well-designed for modeling the fine-grained features between medical mentions in raw texts and standard entities. The standard entity information of NEN is introduced into the NER module by the mechanisms in our framework. With the help of the dynamic language features and progressive multi-task learning, the framework\ncan extract the medical mentions more exactly and map them to standard entities. And the semantic correlation between medical mentions and standard entities is built on the three progressive tasks from low to high. The rich semantics captured by the progressive tasks are beneficial to NER and NEN."
    }, {
      "heading" : "4.4 Further Discussion",
      "text" : "To dig into the framework, we conduct the detailed analysis for presenting it in different aspects. The ablation study is conducted to present the effectiveness of the mechanisms proposed in the framework. Besides the supervised learning, our framework exploits the standard entity information in the NER task and is potential in a zero-shot scenario compared with BioBERT. We conduct the case study to analyze the prediction results and visualize the attention mechanism to prove its effectiveness."
    }, {
      "heading" : "4.4.1 Ablation Study",
      "text" : "As shown in Table 2, we conduct the ablation study to present the effectiveness of the progressive tasks and different mechanisms. When free from completing the mid- or high-level tasks, E2EMERN gains worse results on NER and NEN. The progressive tasks improves the ability of the framework to learn the multi-grained features between original texts and standard entities. Besides, we replace the gate and attention mechanisms with the simple feature concatenation strategy as compared methods. When removed the attention mech-\nanism, E2EMERN achieves worse results on two tasks. It proves that the supervised signals from mid-level task are beneficial to the low-task. And the entity-attention feature generated by the mechanism contributes to the high-level task. E2EMERN without the gate mechanism gains the worse results on NEN. Because the mechanism aggregates the features from lower level tasks which provides the multi-grained information between mentions and standard entities. The ablation study proves the importance of the two mechanisms to E2EMERN."
    }, {
      "heading" : "4.4.2 Results on Unseen Samples",
      "text" : "We conduct the statistic analysis on the test set of NCBI and BC5CDR. As shown in Figure 4, there are about 40% ∼ 50% samples contain the words or medial mentions which do not appear in the training set. Therefore, we need to evaluate the generalization ability of models on the unseen samples. We compare E2EMERN with BioBERT on the unseen samples in the test set. To a certain extent, our framework can outperform the existing stateof-the-art NER model. Compared with BioBERT, E2EMERN introduces the standard entity base into the framework. The fine-grained location information of medical mentions from the high-level task is propagated to the low-level task. With the help of standard entity information and progressive multi-task learning, E2EMERN can gain the better generalization ability on unseen samples."
    }, {
      "heading" : "4.4.3 Case Study",
      "text" : "We present the case study results in Table 3. Compared with BioBERT, our framework can extract the medical mentions which BioBERT can not extract. We draw the label results of E2EMERN with\nthe heat map. As the color deepens, the importance of the token in the sentence increases. The visualization results prove that the attention mechanism in E2EMERN focuses on the tokens which make of medical mentions. Although “Text2” and “Text4” are unseen samples, E2EMERN can also extract the mentions in them. The token “convulsions” is paid more attention than “seizures” in “Text3”. But convulsion is the symptom of seizures. With the help of medical correlation between them, E2EMERN can extract the token “seizures” as medical mention. To some extent, the effectiveness of E2EMERN can be proved by the case study."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we reconsider the process of NER and NEN and propose the end-to-end progressive multitask learning framework for medical named entity recognition and normalization. Compared with existing methods, the framework consists of three tasks with progressive difficulty which contributes to modeling the fine-grained features between medical mentions in raw texts and standard entities. Furthermore, the detailed analysis of E2EMERN proves its effectiveness. Considering the medical area is various, we will try to adapt the framework to the cross domain problem."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank three anonymous reviewers for their insightful comments. This research is supported by the Chinese Scientific and Technical Innovation Project 2030 (2018AAA0102100), NSFCGeneral Technology Joint Fund for Basic Research (No. U1936206), NSFC-Xinjiang Joint Fund (No. U1903128), National Natural Science Foundation of China (No. 62002178, No. 62077031), and Natural Science Foundation of Tianjin, China (No. 20JCQNJC01730)."
    } ],
    "references" : [ {
      "title" : "A multi-task approach for named entity recognition in social media data",
      "author" : [ "Gustavo Aguilar", "Suraj Maharjan", "Adrian Pastor LópezMonroy", "Thamar Solorio." ],
      "venue" : "Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 148–153, Copenhagen,",
      "citeRegEx" : "Aguilar et al\\.,? 2017",
      "shortCiteRegEx" : "Aguilar et al\\.",
      "year" : 2017
    }, {
      "title" : "Adversarial transfer learning for Chinese named entity recognition with selfattention mechanism",
      "author" : [ "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao", "Shengping Liu." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Cao et al\\.,? 2018",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2018
    }, {
      "title" : "MIX: multi-channel information crossing for text matching",
      "author" : [ "Haolan Chen", "Fred X. Han", "Di Niu", "Dong Liu", "Kunfeng Lai", "Chenglin Wu", "Yu Xu." ],
      "venue" : "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "NCBI disease corpus: A resource for disease name recognition and concept normalization",
      "author" : [ "Rezarta Islamaj Dogan", "Robert Leaman", "Zhiyong Lu." ],
      "venue" : "Journal of Biomedical Informatics, 47:1–10.",
      "citeRegEx" : "Dogan et al\\.,? 2014",
      "shortCiteRegEx" : "Dogan et al\\.",
      "year" : 2014
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Comput., 9(8):1735– 1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Mmcl-net: Spinal disease diagnosis in global mode using progressive multi-task joint learning",
      "author" : [ "Yanfei Hong", "Benzheng Wei", "Zhongyi Han", "Xiang Li", "Yuanjie Zheng", "Shuo Li." ],
      "venue" : "Neurocomputing, 399:307–316.",
      "citeRegEx" : "Hong et al\\.,? 2020",
      "shortCiteRegEx" : "Hong et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd ICLR.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Improving low-resource named entity recognition using joint sentence and token labeling",
      "author" : [ "Canasai Kruengkrai", "Thien Hai Nguyen", "Sharifah Mahani Aljunied", "Lidong Bing." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Kruengkrai et al\\.,? 2020",
      "shortCiteRegEx" : "Kruengkrai et al\\.",
      "year" : 2020
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira." ],
      "venue" : "Proceedings of the Eighteenth International Conference on Machine Learning, pages",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Lample et al\\.,? 2016",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "Lasigebiotm at MEDIQA 2019: Biomedical question answering using bidirectional transformers and named entity recognition",
      "author" : [ "Andre Lamurias", "Francisco M. Couto." ],
      "venue" : "Proceedings of the 18th BioNLP Workshop on ACL, pages 523–527.",
      "citeRegEx" : "Lamurias and Couto.,? 2019",
      "shortCiteRegEx" : "Lamurias and Couto.",
      "year" : 2019
    }, {
      "title" : "Dnorm: disease name normalization with pairwise learning to rank",
      "author" : [ "Robert Leaman", "Rezarta Islamaj Dogan", "Zhiyong Lu." ],
      "venue" : "Bioinform., 29(22):2909– 2917.",
      "citeRegEx" : "Leaman et al\\.,? 2013",
      "shortCiteRegEx" : "Leaman et al\\.",
      "year" : 2013
    }, {
      "title" : "Disease named entity recognition and normalization with dnorm",
      "author" : [ "Robert Leaman", "Zhiyong Lu." ],
      "venue" : "Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics, page 587.",
      "citeRegEx" : "Leaman and Lu.,? 2014",
      "shortCiteRegEx" : "Leaman and Lu.",
      "year" : 2014
    }, {
      "title" : "Taggerone: joint named entity recognition and normalization with semi-markov models",
      "author" : [ "Robert Leaman", "Zhiyong Lu." ],
      "venue" : "Bioinform., 32(18):2839–2846.",
      "citeRegEx" : "Leaman and Lu.,? 2016",
      "shortCiteRegEx" : "Leaman and Lu.",
      "year" : 2016
    }, {
      "title" : "tmchem: a high performance approach for chemical named entity recognition and normalization",
      "author" : [ "Robert Leaman", "Chih-Hsuan Wei", "Zhiyong Lu." ],
      "venue" : "J. Cheminformatics, 7(S-1):S3.",
      "citeRegEx" : "Leaman et al\\.,? 2015",
      "shortCiteRegEx" : "Leaman et al\\.",
      "year" : 2015
    }, {
      "title" : "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
      "author" : [ "Jinhyuk Lee", "Wonjin Yoon", "Sungdong Kim", "Donghyeon Kim", "Sunkyu Kim", "Chan Ho So", "Jaewoo Kang." ],
      "venue" : "Bioinform., 36(4):1234–",
      "citeRegEx" : "Lee et al\\.,? 2020",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "Biocreative V CDR task corpus: a resource for chemical disease",
      "author" : [ "Jiao Li", "Yueping Sun", "Robin J. Johnson", "Daniela Sciaky", "Chih-Hsuan Wei", "Robert Leaman", "Allan Peter Davis", "Carolyn J. Mattingly", "Thomas C. Wiegers", "Zhiyong Lu" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "A transition-based joint model for disease named entity recognition and normalization",
      "author" : [ "Yinxia Lou", "Yue Zhang", "Tao Qian", "Fei Li", "Shufeng Xiong", "Donghong Ji." ],
      "venue" : "Bioinform., 33(15):2363–2371.",
      "citeRegEx" : "Lou et al\\.,? 2017",
      "shortCiteRegEx" : "Lou et al\\.",
      "year" : 2017
    }, {
      "title" : "Leadmine: Disease identification and concept mapping using wikipedia",
      "author" : [ "Daniel M Lowe", "Noel M O’Boyle", "Roger A Sayle" ],
      "venue" : "In Proceedings of the Fifth BioCreative Challenge Evaluation Workshop,",
      "citeRegEx" : "Lowe et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lowe et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomás Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean." ],
      "venue" : "27th Annual Conference on Neural Information Processing Systems., pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha,",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Okapi at TREC-3",
      "author" : [ "Stephen E. Robertson", "Steve Walker", "Susan Jones", "Micheline Hancock-Beaulieu", "Mike Gatford." ],
      "venue" : "Proceedings of The Third Text REtrieval Conference, TREC 1994, Gaithersburg, Maryland, USA, November 2-4, 1994,",
      "citeRegEx" : "Robertson et al\\.,? 1994",
      "shortCiteRegEx" : "Robertson et al\\.",
      "year" : 1994
    }, {
      "title" : "Recurrent neural network models for disease name recognition using domain invariant features",
      "author" : [ "Sunil Sahu", "Ashish Anand." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
      "citeRegEx" : "Sahu and Anand.,? 2016",
      "shortCiteRegEx" : "Sahu and Anand.",
      "year" : 2016
    }, {
      "title" : "Fast and accurate entity recognition with iterated dilated convolutions",
      "author" : [ "Emma Strubell", "Patrick Verga", "David Belanger", "Andrew McCallum." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Strubell et al\\.,? 2017",
      "shortCiteRegEx" : "Strubell et al\\.",
      "year" : 2017
    }, {
      "title" : "Named entity recognition and normalization: A domain-specific language approach",
      "author" : [ "Miguel Vázquez", "Monica Chagoyen", "Alberto D. Pascual-Montano." ],
      "venue" : "2nd International Workshop on Practical Applications of Computational Biology and Bioin-",
      "citeRegEx" : "Vázquez et al\\.,? 2008",
      "shortCiteRegEx" : "Vázquez et al\\.",
      "year" : 2008
    }, {
      "title" : "SC-NER: A sequence-to-sequence model with sentence classification for named entity recognition",
      "author" : [ "Yu Wang", "Yun Li", "Ziye Zhu", "Bin Xia", "Zheng Liu." ],
      "venue" : "Advances in Knowledge Discovery and Data Mining - 23rd Pacific-Asia Conference, pages",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Relation-aware entity alignment for heterogeneous knowledge graphs",
      "author" : [ "Yuting Wu", "Xiao Liu", "Yansong Feng", "Zheng Wang", "Rui Yan", "Dongyan Zhao." ],
      "venue" : "Proceedings of the 28th IJCAI, pages 5278–5284.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Principles of non-stationary hidden markov model and its applications to sequence labeling task",
      "author" : [ "JingHui Xiao", "Bingquan Liu", "Xiaolong Wang." ],
      "venue" : "Natural Language Processing - IJCNLP 2005, Second International Joint Conference, pages 827–",
      "citeRegEx" : "Xiao et al\\.,? 2005",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2005
    }, {
      "title" : "A joint model for medical named entity recognition and normalization",
      "author" : [ "Ying Xiong", "Yuanhang Huang", "Qingcai Chen", "Xiaolong Wang", "Yuan Nic", "Buzhou Tang." ],
      "venue" : "Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2020) co-",
      "citeRegEx" : "Xiong et al\\.,? 2020",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2020
    }, {
      "title" : "Curriculum learning for natural language understanding",
      "author" : [ "Benfeng Xu", "Licheng Zhang", "Zhendong Mao", "Quan Wang", "Hongtao Xie", "Yongdong Zhang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Crf-based hybrid model for word segmentation, NER and even POS tagging",
      "author" : [ "Zhiting Xu", "Xian Qian", "Yuejie Zhang", "Yaqian Zhou." ],
      "venue" : "Third International Joint Conference on Natural Language Processing, pages 167–170.",
      "citeRegEx" : "Xu et al\\.,? 2008",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2008
    }, {
      "title" : "ABCNN: Attention-based convolutional neural network for modeling sentence pairs",
      "author" : [ "Wenpeng Yin", "Hinrich Schütze", "Bing Xiang", "Bowen Zhou." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:259–272.",
      "citeRegEx" : "Yin et al\\.,? 2016",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2016
    }, {
      "title" : "Collabonet: collaboration of deep neural networks for biomedical named entity recognition",
      "author" : [ "Wonjin Yoon", "Chan Ho So", "Jinhyuk Lee", "Jaewoo Kang." ],
      "venue" : "BMC Bioinform., 20-S(10):55–65.",
      "citeRegEx" : "Yoon et al\\.,? 2019",
      "shortCiteRegEx" : "Yoon et al\\.",
      "year" : 2019
    }, {
      "title" : "A neural multi-task learning framework to jointly model medical named entity recognition and normalization",
      "author" : [ "Sendong Zhao", "Ting Liu", "Sicheng Zhao", "Fei Wang." ],
      "venue" : "Proceedings of the 33th AAAI, pages 817–824.",
      "citeRegEx" : "Zhao et al\\.,? 2019",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2019
    }, {
      "title" : "Disease named entity recognition from biomedical literature using a novel convolutional neural network",
      "author" : [ "Zhehuan Zhao", "Zhihao Yang", "Ling Luo", "Lei Wang", "Yin Zhang", "Hongfei Lin", "Jian Wang." ],
      "venue" : "BMC Medical Genomics, 10.",
      "citeRegEx" : "Zhao et al\\.,? 2017",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2017
    }, {
      "title" : "Knowledgeenhanced biomedical named entity recognition and normalization: application to proteins and genes",
      "author" : [ "Huiwei Zhou", "Shixian Ning", "Zhe Liu", "Chengkun Lang", "Zhuang Liu", "Bizun Lei." ],
      "venue" : "Bioinform., 21(1):35.",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention-based bidirectional long short-term memory networks for relation classification",
      "author" : [ "Peng Zhou", "Wei Shi", "Jun Tian", "Zhenyu Qi", "Bingchen Li", "Hongwei Hao", "Bo Xu." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Zhou et al\\.,? 2016",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "ical knowledge graph (Wu et al., 2019) and QA systems (Lamurias and Couto, 2019).",
      "startOffset" : 21,
      "endOffset" : 38
    }, {
      "referenceID" : 34,
      "context" : "And named entity normalization maps mentions extracted from the medical text to standard identifiers, such as MeSH and OMIM (Zhao et al., 2019).",
      "startOffset" : 124,
      "endOffset" : 143
    }, {
      "referenceID" : 34,
      "context" : "The latter one tries to maximize the posterior probabilities p(yNER, yNEN |x) (Zhao et al., 2019).",
      "startOffset" : 78,
      "endOffset" : 97
    }, {
      "referenceID" : 6,
      "context" : "Compared with ordinary multi-task learning, progressive multi-task learning focuses on the aggregation logic of tasks’ specific features (Hong et al., 2020).",
      "startOffset" : 137,
      "endOffset" : 156
    }, {
      "referenceID" : 25,
      "context" : "The conventional pipeline frameworks contains the NER model and NEN one separately (Vázquez et al., 2008; Leaman and Lu, 2014; Sahu and Anand, 2016; Zhou et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 167
    }, {
      "referenceID" : 13,
      "context" : "The conventional pipeline frameworks contains the NER model and NEN one separately (Vázquez et al., 2008; Leaman and Lu, 2014; Sahu and Anand, 2016; Zhou et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 167
    }, {
      "referenceID" : 23,
      "context" : "The conventional pipeline frameworks contains the NER model and NEN one separately (Vázquez et al., 2008; Leaman and Lu, 2014; Sahu and Anand, 2016; Zhou et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 167
    }, {
      "referenceID" : 36,
      "context" : "The conventional pipeline frameworks contains the NER model and NEN one separately (Vázquez et al., 2008; Leaman and Lu, 2014; Sahu and Anand, 2016; Zhou et al., 2020).",
      "startOffset" : 83,
      "endOffset" : 167
    }, {
      "referenceID" : 3,
      "context" : "Recently, the pre-trained models, such as BERT (Devlin et al., 2019), BioBERT (Lee et al.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 16,
      "context" : ", 2019), BioBERT (Lee et al., 2020), make impressive progress in the natural language processing (NLP) area.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 28,
      "context" : "markov model (Xiao et al., 2005) and conditional random fields (CRF) (Lafferty et al.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 9,
      "context" : ", 2005) and conditional random fields (CRF) (Lafferty et al., 2001) are the typical methods to solve the problem.",
      "startOffset" : 44,
      "endOffset" : 67
    }, {
      "referenceID" : 5,
      "context" : "(LSTM) (Hochreiter and Schmidhuber, 1997) network stacked with CRF (Xu et al.",
      "startOffset" : 7,
      "endOffset" : 41
    }, {
      "referenceID" : 31,
      "context" : "(LSTM) (Hochreiter and Schmidhuber, 1997) network stacked with CRF (Xu et al., 2008) has been a benchmark model for sequence labeling (Lample et al.",
      "startOffset" : 67,
      "endOffset" : 84
    }, {
      "referenceID" : 10,
      "context" : ", 2008) has been a benchmark model for sequence labeling (Lample et al., 2016).",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "better performances on these tasks including sequence labeling (Aguilar et al., 2017; Cao et al., 2018).",
      "startOffset" : 63,
      "endOffset" : 103
    }, {
      "referenceID" : 1,
      "context" : "better performances on these tasks including sequence labeling (Aguilar et al., 2017; Cao et al., 2018).",
      "startOffset" : 63,
      "endOffset" : 103
    }, {
      "referenceID" : 22,
      "context" : "The information retrieval method, such as: BM25 (Robertson et al., 1994), is a universal model to solve this problem.",
      "startOffset" : 48,
      "endOffset" : 72
    }, {
      "referenceID" : 20,
      "context" : "The distributed representations of texts, such as: Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al.",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 21,
      "context" : ", 2013) and GloVe (Pennington et al., 2014), are utilized to calculate the similarity distance between two texts.",
      "startOffset" : 18,
      "endOffset" : 43
    }, {
      "referenceID" : 13,
      "context" : "Some medical named entity normalization models are based on this method (Leaman and Lu, 2014; Zhou et al., 2020).",
      "startOffset" : 72,
      "endOffset" : 112
    }, {
      "referenceID" : 36,
      "context" : "Some medical named entity normalization models are based on this method (Leaman and Lu, 2014; Zhou et al., 2020).",
      "startOffset" : 72,
      "endOffset" : 112
    }, {
      "referenceID" : 32,
      "context" : "Considering local texts are more important than global ones, some researchers utilized convolution neural networks (CNN) to extract local features and exploited interactive attention mechanism to match the semantic similarity of two texts (Yin et al., 2016; Chen et al., 2018).",
      "startOffset" : 239,
      "endOffset" : 276
    }, {
      "referenceID" : 2,
      "context" : "Considering local texts are more important than global ones, some researchers utilized convolution neural networks (CNN) to extract local features and exploited interactive attention mechanism to match the semantic similarity of two texts (Yin et al., 2016; Chen et al., 2018).",
      "startOffset" : 239,
      "endOffset" : 276
    }, {
      "referenceID" : 30,
      "context" : "The process of human learning often goes from easy to difficult (Xu et al., 2020).",
      "startOffset" : 64,
      "endOffset" : 81
    }, {
      "referenceID" : 4,
      "context" : "As shown in Figure 2, we take a medical text from the real dataset NCBI (Dogan et al., 2014)",
      "startOffset" : 72,
      "endOffset" : 92
    }, {
      "referenceID" : 37,
      "context" : "entity, we adopt the attention mechanism (Zhou et al., 2016) to focus on the local words of sentences.",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 4,
      "context" : "There are 798 public medical abstracts in the NCBI dataset (Dogan et al., 2014).",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 17,
      "context" : "BC5CDR dataset (Li et al., 2016) contains 1500 public medical abstracts which are also annotated with MeSH identifiers.",
      "startOffset" : 15,
      "endOffset" : 32
    }, {
      "referenceID" : 7,
      "context" : "To train the model, we use the ADAM (Kingma and Ba, 2015) algorithm to update the weights.",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 12,
      "context" : "Dnorm (Leaman et al., 2013) is the pipeline model for medical NER and NEN.",
      "startOffset" : 6,
      "endOffset" : 27
    }, {
      "referenceID" : 14,
      "context" : "TaggerOne (Leaman and Lu, 2016) is the semiMarkov based model for jointly modeling medical NER and NEN.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 24,
      "context" : "IDCNN (Strubell et al., 2017) was proposed with an improved CNN module for NER.",
      "startOffset" : 6,
      "endOffset" : 29
    }, {
      "referenceID" : 35,
      "context" : "MCNN (Zhao et al., 2017) was composed of the multiple-label CNN modules for better performances on NER.",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 33,
      "context" : "CollaboNet (Yoon et al., 2019) exploited the multi-source datasets for training the multi-task model and gained better results on all benchmark datasets.",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 34,
      "context" : "MTL-MERN (Zhao et al., 2019) consists of the NER and NEN parallel framework and utilizes the feedback strategy to improve the performances on two tasks.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 16,
      "context" : "With the impressive performance of pre-trained models, BioBERT (Lee et al., 2020) is built on the BERT (Devlin et al.",
      "startOffset" : 63,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : ", 2020) is built on the BERT (Devlin et al., 2019) and trained with a large medical corpus.",
      "startOffset" : 29,
      "endOffset" : 50
    } ],
    "year" : 2021,
    "abstractText" : "Medical named entity recognition (NER) and normalization (NEN) are fundamental for constructing knowledge graphs and building QA systems. Existing implementations for medical NER and NEN are suffered from the error propagation between the two tasks. The mispredicted mentions from NER will directly influence the results of NEN. Therefore, the NER module is the bottleneck of the whole system. Besides, the learnable features for both tasks are beneficial to improving the model performance. To avoid the disadvantages of existing models and exploit the generalized representation across the two tasks, we design an end-to-end progressive multi-task learning model for jointly modeling medical NER and NEN in an effective way. There are three level tasks with progressive difficulty in the framework. The progressive tasks can reduce the error propagation with the incremental task settings which implies the lower level tasks gain the supervised signals other than errors from the higher level tasks to improve their performances. Besides, the context features are exploited to enrich the semantic information of entity mentions extracted by NER. The performance of NEN profits from the enhanced entity mention features. The standard entities from knowledge bases are introduced into the NER module for extracting corresponding entity mentions correctly. The empirical results on two publicly available medical literature datasets demonstrate the superiority of our method over nine typical methods.",
    "creator" : "LaTeX with hyperref"
  }
}