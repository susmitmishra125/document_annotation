{
  "name" : "2021.acl-long.221.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation",
    "authors" : [ "Wenxiang Jiao", "Xing Wang", "Zhaopeng Tu", "Shuming Shi", "Michael R. Lyu", "Irwin King" ],
    "emails" : [ "wxjiao@cse.cuhk.edu.hk", "lyu@cse.cuhk.edu.hk", "king@cse.cuhk.edu.hk", "brightxwang@tencent.com", "zptu@tencent.com", "shumingshi@tencent.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2840–2850\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2840"
    }, {
      "heading" : "1 Introduction",
      "text" : "Leveraging large-scale unlabeled data has become an effective approach for improving the performance of natural language processing (NLP) models (Devlin et al., 2019; Brown et al., 2020; Jiao et al., 2020a). As for neural machine translation\n∗Work was mainly done when Wenxiang Jiao was interning at Tencent AI Lab.\n1The source code is available at https://github. com/wxjiao/UncSamp\n(NMT), compared to the parallel data, the monolingual data is available in large quantities for many languages. Several approaches on boosting the NMT performance with the monolingual data have been proposed, e.g., data augmentation (Sennrich et al., 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al., 2016; Zhang et al., 2018; Cai et al., 2021), pre-training (Siddhant et al., 2020; Liu et al., 2020). Among them, data augmentation with the synthetic parallel data (Sennrich et al., 2016a; Edunov et al., 2018) is the most widely used approach due to its simple and effective implementation. It has been a de-facto standard in developing the large-scale NMT systems (Hassan et al., 2018; Ng et al., 2019; Wu et al., 2020; Huang et al., 2021).\nSelf-training (Zhang and Zong, 2016) is one of the most commonly used approaches for data augmentation. Generally, self-training is performed in three steps: (1) randomly sample a subset from the large-scale monolingual data; (2) use a “teacher” NMT model to translate the subset data into the target language to construct the synthetic parallel data; (3) combine the synthetic and authentic parallel data to train a “student” NMT model. Recent studies have shown that synthetic data manipulation (Edunov et al., 2018; Caswell et al., 2019) and training strategy optimization (Wu et al., 2019b; Wang et al., 2019) in the last two steps can boost the self-training performance significantly. However, how to efficiently and effectively sample the subset from the large-scale monolingual data in the first step has not been well studied.\nIntuitively, self-training simplifies the complexity of generated target sentences (Kim and Rush, 2016; Zhou et al., 2019; Jiao et al., 2020b), and easy patterns in monolingual sentences with deterministic translations may not provide additional gains over the self-training “teacher” model (Shrivastava et al., 2016). Related work on computer\nvision also reveals that easy patterns in unlabeled data with the deterministic prediction may not provide additional gains (Mukherjee and Awadallah, 2020). In this work, we investigate and identify the uncertain monolingual sentences which implicitly hold difficult patterns and exploit them to boost the self-training performance. Specifically, we measure the uncertainty of the monolingual sentences by using a bilingual dictionary extracted from the authentic parallel data (§2.1). Experimental results show that NMT models benefit more from the monolingual sentences with higher uncertainty, except on those with excessively high uncertainty (§2.3). By conducting the linguistic property analysis, we find that extremely uncertain sentences contain relatively poor translation outputs, which may hinder the training of NMT models (§2.4).\nInspired by the above finding, we propose an uncertainty-based sampling strategy for selftraining, in which monolingual sentences with higher uncertainty would be selected with higher probability (§3.1). Large-scale experiments on WMT English⇒German and English⇒Chinese datasets show that self-training with the proposed uncertainty-based sampling strategy significantly outperforms that with random sampling (§3.3). Extensive analyses on the generated outputs confirm our claim by showing that our approach improves the translation of uncertain sentences and the prediction of low-frequency target words (§3.4).\nContributions. Our main contributions are:\n• We demonstrate the necessity of distinguishing monolingual sentences for self-training.\n• We propose an uncertainty-based sampling strategy for self-training, which selects more complementary sentences for the authentic parallel data.\n• We show that NMT models benefit more from uncertain monolingual sentences in selftraining, which improves the translation quality of uncertain sentences and the prediction accuracy of low-frequency words."
    }, {
      "heading" : "2 Observing Monolingual Uncertainty",
      "text" : "In this section, we aimed to understand the effect of uncertain monolingual data on self-training. We first introduced the metric for identifying uncertain monolingual sentences, then the experimental setup and at last our preliminary results.\nNotations. Let X and Y denote the source and target languages, and let X and Y represent the sentence domains of corresponding languages. Let B = {(xi,yi)}Ni=1 denote the authentic parallel data, where xi ∈ X , yi ∈ Y and N is the number of sentence pairs. LetMx = {xj}Mxj=1 denote the collection of monolingual sentences in the source language, where xj ∈ X and Mx is the size of the set. Our objective is to obtain a translation model f : X 7→ Y , that can translate sentences from language X to language Y ."
    }, {
      "heading" : "2.1 Identification of Uncertain Data",
      "text" : "Data Complexity. According to Zhou et al. (2019), the complexity of a parallel corpus can be measured by adding up the translation uncertainty of all source sentences. Formally, the translation uncertainty of a source sentence x with its translation candidates can be operationalized as conditional entropy:\nH(Y|X = x) = − ∑ y∈Y p(y|x) log p(y|x) (1)\n≈ Tx∑ t=1 H(y|x = xt), (2)\nwhere Tx denotes the length of the source sentence, x and y represent a word in the source and target vocabularies, respectively. Generally, a high H(Y|X = x) denotes that a source sentence x would have more possible translation candidates.\nEquation (2) estimates the translation uncertainty of a source sentence with all possible translation candidates in the parallel corpus. It can not be directly applied to the sentences in monolingual data due to the lack of corresponding translation candidates. One potential solution to the problem is utilizing a trained model to generate multiple translation candidates. However, generation may lead to bias estimation due to the generation diversity issue (Li et al., 2016; Shu et al., 2019). More importantly, generation is extremely time-consuming for large-scale monolingual data.\nMonolingual Uncertainty. To address the problem, we modified Equation (2) to reflect the uncertainty of monolingual sentences. We estimate the target word distribution conditioned on each source word based on the authentic parallel corpus, and then use the distribution to measure the translation uncertainty of the monolingual example. Specifically, we measure the uncertainty of monolingual sentences based on the bilingual dictionary.\n2021-01-23 Uncertainty vs. Performance\nB LE\nU\n35\n36\n37\nU nc\ner ta\nin ty\n1.5\n2.5\n3.5\nData bins\n1 2 3 4 5\nUncertainty BLEU\nAll (36.5) B LE U\n32\n34\n36\n38\nAdditional Monolingual Data\n0M 8M 16M 24M 32M 40M\nFigure 1: Performance of self-training with increased size of monolingual data. The BLEU score is averaged on WMT En⇒De newstest2019 and newstest2020.\nFor a given monolingual sentence xj ∈Mx, its uncertainty U is calculated as:\nU(xj |Ab) = 1\nTx Tx∑ t=1 H(y|Ab, x = xt), (3)\nwhich is normalized by Tx to avoid the length bias. A higher value of U indicates a higher translation uncertainty of the monolingual sentence.\nIn Equation 3, the word level entropy H(y|Ab, x = xt) captures the translation modalities of each source word by using the bilingual dictionary Ab. The bilingual dictionary records all the possible target words for each source word, as well as translation probabilities. It can be built from the word alignments by external alignment toolkits on the authentic parallel corpus. For example, given a source word x with all three word translations y1, y2 and y3 and the translation probabilities of p(y1|x), p(y2|x) and p(y3|x), respectively, the word level entropy can be calculated as follows:\nH(y|Ab, xi) = − ∑\nyj∈Ab(xi)\np(yj |xi) log p(yj |xi).\n(4)"
    }, {
      "heading" : "2.2 Experimental Setup",
      "text" : "Data. We conducted experiments on two large-scale benchmark translation datasets, i.e., WMT English⇒German (En⇒De) and WMT English⇒Chinese (En⇒Zh). The authentic parallel data for the two tasks consists of about 36.8M and 22.1M sentence pairs, respectively. The monolingual data we used is from newscrawl released by WMT2020. We combined the\nnewscrawl data from year 2011 to 2019 for the English monolingual corpus, consisting of about 200M sentences. We randomly sampled 40M monolingual data for En⇒De and 20M for En⇒Zh unless otherwise stated. We adopted newstest2018 as the validation set and used newstest2019/2020 as the test sets. For each language pair, we applied Byte Pair Encoding (BPE, Sennrich et al., 2016b) with 32K merge operations.\nModel. We chose the state-of-the-art TRANSFORMER (Vaswani et al., 2017) network as our model, which consists of an encoder of 6 layers and a decoder of 6 layers. We adopted the open-source toolkit Fairseq (Ott et al., 2019) to implement the model. We used the TRANSFORMER-BASE model for preliminary experiments (§2.3) and the constrained scenario (§3.2) for efficiency. For the unconstrained scenario (§3.3), we adopted the TRANSFORMER-BIG model. Results on these models with different capacities can also reflect the robustness of our approach. For the TRANSFORMERBASE model, we trained it for 150K steps with 32K (4096 × 8) tokens per batch. For the TRANSFORMER-BIG model, we trained it for 30K steps with 460K (3600 × 128) tokens per batch with the cosine learning rate schedule (Wu et al., 2019a). We used 16 Nvidia V100 GPUs to conduct the experiments and selected the final model by the best perplexity on the validation set.\nEvaluation. We evaluated the models by BLEU score (Papineni et al., 2002) computed by SacreBLEU (Post, 2018)2. For the En⇒Zh task, we added the option --tok zh to SacreBLEU. We measured the statistical significance of improvement with paired bootstrap resampling (Koehn, 2004) using compare-mt3 (Neubig et al., 2019)."
    }, {
      "heading" : "2.3 Effect of Uncertain Data",
      "text" : "First of all, we investigated the effect of monolingual data uncertainty on the self-training performance in NMT. We conducted the preliminary experiments on the WMT En⇒De dataset with the TRANSFORMER-BASE model. We sampled 8M bilingual sentence pairs from the authentic parallel data and randomly sampled 40M monolingual sentences for the self-training. To ensure the quality of synthetic parallel data, we trained\n2BLEU+case.mixed+lang.[Task]+numrefs.1 +smooth.exp++test.wmt[Year]+tok.[Tok]+ver sion.1.4.14, Task=en-de/en-zh, Year=19/20, Tok=13a/zh\n3https://github.com/neulab/compare-mt\n2021-01-23 Uncertainty vs. Performance\nB LE\nU\n35\n36\n37\nU nc\ner ta\nin ty\n1.5\n2.5\n3.5\nData bins 1 2 3 4 5\nUncertainty BLEU\nAll (36.5)\nB LE\nU\n32\n34\n36\n38\nAdditional Monolingual Data 0M 8M 16M 24M 32M 40M\nUncertainCertain\nFigure 2: Relationship between uncertainty of monolingual data and the corresponding NMT performance. The BLEU score is averaged on WMT En⇒De newstest2019 and newstest2020.\na TRANSFORMER-BIG model for translating the source monolingual data to the target language. We generated translations using beam search with beam width 5, and followed Edunov et al. (2018)4 to filter the generated sentence pairs (See Appendix A.1).\nSelf-training v.s. Data Size. We took a look at the performance of standard self-training and its relationship with data size. Figure 1 showed the results. Obviously, self-training with 8M synthetic data can already improve the NMT performance by a significant margin (36.2 averaged BLEU score on WMT En⇒De newstest2019 and newstest2020). Increasing the size of added monolingual data does not bring much more benefit. With all the 40M monolingual sentences, the final performance achieves only 36.5 BLEU points. It indicates that adding more monolingual data only is not a promising way to improve self-training, and more sophisticated approaches for exploiting the monolingual data are desired.\nSelf-training v.s. Uncertainty. In this experiment, we first adopted fast-align5 to establish word alignments between source and target words in the authentic parallel corpus and used the alignments to build the bilingual dictionary Ab. Then we used the bilingual dictionary to compute the data uncertainty expressed in Equation (3) for the sentences in the monolingual data set. After that, we ranked all the 40M monolingual sentences and grouped\n4https://github.com/pytorch/fairseq/ tree/master/examples/backtranslation\n5https://github.com/clab/fast_align\nthem into 5 equally-sized bins (i.e., 8M sentences per bin) according to their uncertainty scores. At last, we performed self-training with each bin of monolingual data.\nWe reported the translation performance in Figure 2. As seen, there is a trend of performance improvement with the increase of monolingual data uncertainty (e.g., bins 1 to 4) until the last bin. The last bin consists of sentences with excessively high uncertainty, which may contain erroneous synthetic target sentences. Training on these sentences forces the models to over-fit on these incorrect synthetic data, resulting in the confirmation bias issue (Arazo et al., 2020). These results corroborate with prior studies (Chang et al., 2017; Mukherjee and Awadallah, 2020) such that learning on certain examples brings little gain while on the excessively uncertain examples may also hurt the model training."
    }, {
      "heading" : "2.4 Linguistic Properties of Uncertain Data",
      "text" : "We further analyzed the differences between the monolingual sentences with varied uncertainty to gain a deeper understanding of the uncertain data. Specifically, we performed linguistic analysis on the five data bins in terms of three properties: 1) sentence length that counts the tokens in the sentence, 2) word rarity (Platanios et al., 2019) that measures the frequency of words in a sentence with a higher value indicating a more rare sentence, and 3) translation coverage (Khadivi and Ney, 2005) that measures the ratio of source words being aligned with any target words. The first two reflect the properties of monolingual sentences while the last one reflects the quality of synthetic sentence pairs. We also presented the results of the synthetic target sentences for reference. Details of the linguistic properties are in Appendix A.2.\nThe results are reported in Figure 3. For the length property, we find that monolingual sentences with higher uncertainty are usually longer except for those with excessively high uncertainty (e.g., bin 5). The monolingual sentences in the last data bin noticeably contain more rare words than other bins in Figure 3(b), and the rare words in the sentences pose a great challenge in the NMT training process (Gu et al., 2020). In Figure 3(c), the overall coverage in bin 5 is the lowest among the self-training bins. In contrast, bin 1 with the lowest uncertainty has the highest coverage. These observations suggest that monolingual sentences in bin 1 indeed contain the easiest patterns while\n2021-01-23 Uncertainty vs. Properties\n0\n25\n50\nData bins\n1 2 3 4 5\nSource Target\n7.5\n8.0\n8.5\nData bins\n1 2 3 4 5\nSource Target\n0.85\n0.90\n0.95\n1.00\nData bins\n1 2 3 4 5\nSource Target\nUncertainCertain UncertainCertain UncertainCertain\n(a) Sentence Length\n2021-01-23 Uncertainty vs. Properties\n0\n25\n50\nData bins\n1 2 3 4 5\nSource Target\n7.5\n8.0\n8.5\nData bins\n1 2 3 4 5\nSource Target\n0.85\n0.90\n0.95\n1.00\nData bins\n1 2 3 4 5\nSource Target\nUncertainCertain UncertainCertain UncertainCertain\n(b) Word Rarity\n2021-01-23 Uncertain y vs. Properties\n0\n25\n50\nData bins\n1 2 3 4 5\nSource Target\n7.5\n8.0\n8.5\nData bins\n1 2 3 4 5\nSource Target\n0.85\n0.90\n0.95\n1.00\nData bins\n1 2 3 4 5\nSource Target\nUncertainCertain UncertainCertain UncertainCertain\n(c) Coverage\nFigure 3: Comparison of monolingual sentences with varied uncertainty in terms of three properties, including sentence length, word rarity, and coverage.\n2021-01-23 Uncertainty Distribution\n2021-01-23 Uncertainty Distribution\nmonolingual sentences in bin 5 are the most difficult ones, which may explain their relatively weak performance in Figure 2."
    }, {
      "heading" : "3 Exploiting Monolingual Uncertainty",
      "text" : "By analyzing the effect of monolingual data uncertainty on self-training in Section 2, we understood that monolingual sentences with relatively high uncertainty are more informative while also with high quality, which motivates us to emphasize the training on these sentences. In this section, we introduced the uncertainty-based sampling strategy for self-training and the overall framework."
    }, {
      "heading" : "3.1 Uncertainty-based Sampling Strategy",
      "text" : "With the aforementioned measure of monolingual data uncertainty in Section 2.1, we propose the uncertainty-based sampling strategy for selftraining, which prefers to sample monolingual sen-\ntences with relatively high uncertainty. To ensure the data diversity and avoid the risk of being dominated by the excessively uncertain sentences, we sample monolingual sentences according to the uncertainty distribution with the highest uncertainty penalized. Specifically, given a budget of Ns sentences to sample, we set two hyperparameters to control the sampling probability as follows:\np =\n[ α ·U(xj |Ab) ]β∑ xj∈Mx [α ·U(xj |Ab)] β , (5)\nα = { 1, U(xj |Ab) ≤ Umax, max( 2Umax\nU(xj |Ab) − 1, 0), else,\n(6)\nwhere α is used to penalize excessively high uncertainty over a maximum uncertainty threshold Umax (See Figure 4(a)), the power rate β is used to adjust the distribution such that a larger β gives more probability mass to the sentences with high uncertainty (See Figure 4(b)).\nThe maximum uncertainty threshold Umax is assigned to the uncertainty value such that R% of sentences in the authentic parallel corpus have monolingual data uncertainty below than it. R is assumed to be as high as 80 to 100. Because for monolingual data with uncertainty higher than this threshold, they may not be translated correctly by the “teacher” model as there are inadequate such sentences in the authentic parallel data for the model to learn. As a result, monolingual sentences with uncertainty higher than Umax should be penalized in terms of the sampling probability.\nOverall Framework. Figure 5 presents the framework of our uncertainty-based sampling for\n2021-01-23 ST\nself-training, which includes four steps: 1) train a “teacher” NMT model and an alignment model on the authentic parallel data simultaneously; 2) extract the bilingual dictionary from the alignment model and perform uncertainty-based sampling for monolingual sentences; 3) use the “teacher” NMT model to translate the sampled monolingual sentences to construct the synthetic parallel data; 4) train a “student” NMT model on the combination of synthetic and authentic parallel data."
    }, {
      "heading" : "3.2 Constrained Scenario",
      "text" : "We first validated the proposed sampling approach in a constrained scenario, where we followed the experimental configuration in Section 2.3 with the TRANSFORMER-BASE model, the 8M bitext, and the 40M monolingual data. It allows the efficient evaluation of our approach with varied combinations of hyper-parameters and also the comparison with related methods. Specifically, we performed our approach by sampling 8M sentences from the 40M monolingual data and then combining the corresponding 8M synthetic data with the 8M bitext to train the TRANSFORMER-BASE model.\nTable 1 reported the impact of β and R on the BLEU score. As shown, sampling with high uncertainty sentences and penalizing those with excessively high uncertainty improves translation performance from 36.6 to 36.9. In these experiments, the uncertainty threshold Umax for penalizing are 2.90 and 2.74, which are determined by the 90% and 80% (R=90 and 80 in Table 1) most certain sentences in the authentic parallel data, respectively.\nObviously, the proposed uncertainty-based sampling strategy achieves the best performance with R at 90 and β at 2. In the following experiments, we use R = 90 and β = 2 as the default setting for our sampling strategy if not otherwise stated.\nEffect of Sampling. Some researchers may doubt that the final translation quality is affected by the quality of the teacher model. Therefore, translations of high-uncertainty sentences should contain many errors, and it is better to add the results of oracle translations to discuss the sampling effect and the quality of pseudo-sentences separately. To dispel the doubt, we still used the aforementioned 8M bitext as the bilingual data, and used the rest of WMT19 En-De data (28.8M) as the held-out data (with oracle translations) for sampling. The results are listed in Table 2.\nClearly, our uncertainty-based sampling strategy (UNCSAMP) outperforms the random sampling strategy (RANDSAMP) when manual translations are used (Rows 2 vs. 3), demonstrating the effectiveness of our sampling strategy based on the un-"
    }, {
      "heading" : "1 BITEXT 36.9 27.7 32.3",
      "text" : ""
    }, {
      "heading" : "2 + RANDSAMP ORA 37.4 28.0 32.7",
      "text" : ""
    }, {
      "heading" : "3 + UNCSAMP ORA 37.8 28.2 33.0",
      "text" : "certainty. Another interesting finding is that using the pseudo-sentences outperforms using the manual translations (Rows 4 vs. 2, 5 vs. 3). One possible reason is that the TRANSFORMER-BIG model to construct the pseudo-sentences was trained on the whole WMT19 En-De data that contains the heldout data, which serves as self-training to decently improve the supervised baseline (He et al., 2019).\nComparison with Related Work. We compared our sampling approach with two related works, i.e., difficult word by frequency (DWF, Fadaee and Monz, 2018) and source language model (SRCLM, Lewis, 2010). The former one was proposed for monolingual data selection for back-translation, in which sentences with lowfrequency words were selected to boost the performance of back-translation. The latter one was proposed for in-domain data selection for in-domain language models. Details of the implementation of related work are in Appendix A.3.\nTable 3 listed the results. For DWF, it brings no improvement over RANDSAMP, indicating that the\ntechnique developed for back-translation may not work for self-training. As for SRCLM, it achieves a marginal improvement over RANDSAMP. The proposed UNCSAMP approach outperforms the baseline RANDSAMP by +0.7 BLEU point, which demonstrates the effectiveness of our approach. In addition to our UNCSAMP approach, we also utilized another N-gram language model at the target side to further filter out the synthetic data with potentially erroneous target sentences. By filtering out 20% sentences from the sampled 8M sentences, our UNCSAMP approach achieves a further improvement up to +0.9 BLEU point."
    }, {
      "heading" : "3.3 Unconstrained Scenario",
      "text" : "We extended our sampling approach to the unconstrained scenario, where the scale of data and the capacity of NMT models for self-training are increased significantly. We conducted experiments on the high-resource En⇒De and En⇒Zh translation tasks with all the authentic parallel data, including 36.8M sentence pairs for En⇒De and 22.1M for En⇒Zh, respectively. For monolingual data,\nwe considered all the 200M English newscrawl monolingual data to perform sampling. We trained the TRANSFORMER-BIG model for experiments.\nTable 4 listed the main results of large-scale self-training on high-resource language pairs. As shown, our TRANSFORMER-BIG models trained on the authentic parallel data achieve the performance competitive with or even better than the submissions to WMT competitions. Based on such strong baselines, self-training with RANDSAMP improves the performance by +2.0 and +0.9 BLEU points on En⇒De and En⇒Zh tasks respectively, demonstrating the effectiveness of the large-scale selftraining for NMT models. With our uncertaintybased sampling strategy UNCSAMP, self-training achieves further significant improvement by +1.1 and +0.6 BLEU points over the random sampling strategy, which demonstrates the effectiveness of exploiting uncertain monolingual sentences."
    }, {
      "heading" : "3.4 Analysis",
      "text" : "In this section, we conducted analyses to understand how the proposed uncertainty-based sampling approach improved the translation performance. Concretely, we analyzed the translation outputs of WMT En⇒De newstest2019 from the TRANSFORMER-BIG model in Table 4.\nUncertain Sentences. As we propose to enhance high uncertainty sentences in self-training, one remaining question is whether our UNCSAMP approach improves the translation quality of high uncertainty sentences. Specifically, we ranked the source sentences in the newstest2019 by the monolingual uncertainty, and divided them into three equally sized groups, namely Low, Medium and High uncertainty.\nThe translation performance on these three groups is reported in Table 5. The first observation is that sentences with high uncertainty are with relatively low BLEU scores (i.e., 31.0), indicating the higher difficulty for NMT models to correctly decode the source sentences with higher uncertainty. Our UNCSAMP approach improves the translation performance on all sentences, especially on the sentences with high uncertainty (+10.9%), which confirms our motivation of emphasizing the learning on uncertain sentences for self-training.\nLow-Frequency Words. Partially motivated by Fadaee and Monz (2018), we hypothesized that the addition of monolingual data in self-training\nhas the potential to improve the prediction of lowfrequency words at the target side for the NMT models. Therefore, we investigated whether our approach has a further boost to the performance on the prediction of low-frequency words. We calculated the word accuracy of the translation outputs with respect to the reference in newstest2019 by compare-mt. Following Wang et al. (2020), we divided words into three categories based on their frequency, including High: the most 3,000 frequent words; Medium: the most 3,001-12,000 frequent words; Low: the other words.\nTable 6 listed the results of word accuracy on these three groups evaluated by F-measure. First, we observe that low-frequency words in BITEXT are more difficult to predict than medium- and high-frequency words (i.e., 52.3 v.s. 65.2 and 70.3), which is consistent with Fadaee and Monz (2018). Second, adding monolingual data by selftraining improves the prediction performance of low-frequency words. Our UNCSAMP approach outperforms RANDSAMP significantly on the lowfrequency words. These results suggest that emphasizing the learning on uncertain monolingual sentences also brings additional benefits for the learning of low-frequency words at the target side."
    }, {
      "heading" : "4 Related Work",
      "text" : "Synthetic Parallel Data. Data augmentation by synthetic parallel data has been the most simple and effective way to utilize monolingual data for NMT,\nwhich can be achieved by self-training (He et al., 2019) and back-translation (Sennrich et al., 2016a). While back-translation has dominated the NMT area for years (Fadaee and Monz, 2018; Edunov et al., 2018; Caswell et al., 2019), recent works on translationese (Marie et al., 2020; Graham et al., 2019) suggest that NMT models trained with backtranslation may lead to distortions in automatic and human evaluation. To address the problem, starting from WMT2019 (Barrault et al., 2019), the test sets only include naturally occurring text at the sourceside, which is a more realistic scenario for practical translation usage. In this new testing setup, the forward-translation (Zhang and Zong, 2016), i.e., self-training in NMT, becomes a more promising method as it also introduces naturally occurring text at the source-side. Therefore, we focus on the data sampling strategy in the self-training scenario, which is different from these prior studies.\nData Uncertainty in NMT. Data uncertainty in NMT has been investigated in the last few years. Ott et al. (2018) analyzed the NMT models with data uncertainty by observing the effectiveness of data uncertainty on the model fitting and beam search. Wang et al. (2019) and Zhou et al. (2020) computed the data uncertainty on the backtranslation data and the authentic parallel data and proposed uncertainty-aware training strategies to improve the model performance, respectively. Wei et al. (2020) proposed the uncertainty-aware semantic augmentation method to bridge the discrepancy of the data distribution between the training and the inference phases. In this work, we propose to explore monolingual data uncertainty to perform data sampling for the self-training in NMT."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this work, we demonstrate the necessity of distinguishing monolingual sentences for self-training in NMT, and propose an uncertainty-based sampling strategy to sample monolingual data. By sampling monolingual data with relatively high uncertainty, our method outperforms random sampling significantly on the large-scale WMT English⇒German and English⇒Chinese datasets. Further analyses demonstrate that our uncertainty-based sampling approach does improve the translation quality of high uncertainty sentences and also benefits the prediction of low-frequency words at the target side. The proposed technology has been applied to\nTranSmart6 (Huang et al., 2021), an interactive machine translation system in Tencent, to improve the performance of its core translation engine. Future work includes the investigation on the confirmation bias issue of self-training and the effect of decoding strategies on self-training sampling."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is partially supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (CUHK 2410021, Research Impact Fund (RIF), R5034-18; CUHK 14210717, General Research Fund), and Tencent AI Lab RhinoBird Focused Research Program (GF202036). We sincerely thank the anonymous reviewers for their insightful suggestions on various aspects of this work."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Synthetic Data When performing self-training, we constructed the synthetic data by translating the monolingual sentences via beam search with beam width 5, and followed Edunov et al. (2018)7 to remove sentences longer than 250 words as well as sentencepairs with a source/target length ratio exceeding\n7https://github.com/pytorch/fairseq/ tree/master/examples/backtranslation\n1.5. The “teacher” NMT model for self-training is the TRANSFORMER-BIG model to ensure the quality of synthetic data.\nA.2 Linguistic Properties Word Rarity. Word rarity measures the frequency of words in a sentence with a higher value indicating a more rare sentence (Platanios et al., 2019). The word rarity of a sentence is calculated as follows:\nWR(x) = − 1 Tx Tx∑ t=1 log p(xt), (7)\nwhere p(xt) denotes the normalized frequency of word xt in the authentic parallel data, and Tx is the sentence length.\nCoverage. Coverage measures the ratio of source words being aligned by any target words (Tu et al., 2016). Firstly, we trained an alignment model on the authentic parallel data by fast-align8. Then we used the alignment model to force-align the monolingual sentences and the synthetic target sentences. Next, we calculated the coverage of each source sentence, and report the averaged coverage of each data bin. The lower coverage of monolingual sentences in bin 5 indicates that they are not aligned as well as the other bins.\nA.3 Comparison with Related Work We compared our sampling approach with two related works, i.e., difficult word by frequency (DWF, Fadaee and Monz, 2018) and source language model (SRCLM, Lewis, 2010). The former one was proposed for monolingual data selection for back-translation, in which sentences with low-frequency words were selected to boost the performance of back-translation. The latter one was proposed for in-domain data selection for indomain language models.\nFor DWF, we ranked the monolingual data by word rarity (Platanios et al., 2019) of sentences and also selected the top 80M monolingual data for self-training. For SRCLM, we trained an N-gram language model (Heafield, 2011)9 on the source sentences in the bitext and measured the distance between each monolingual sentence to the bitext source sentences by cross-entropy. Similarly, we selected 8M monolingual data with the lowest crossentropy for self-training.\n8https://github.com/clab/fast_align 9https://kheafield.com/code/kenlm/"
    } ],
    "references" : [ {
      "title" : "Pseudolabeling and confirmation bias in deep semisupervised learning",
      "author" : [ "Eric Arazo", "Diego Ortego", "Paul Albert", "Noel E O’Connor", "Kevin McGuinness" ],
      "venue" : "In IJCNN",
      "citeRegEx" : "Arazo et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Arazo et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are few-shot learners",
      "author" : [ "Tom B Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell" ],
      "venue" : "NeurIPS",
      "citeRegEx" : "Brown et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural machine translation with monolingual translation memory",
      "author" : [ "Deng Cai", "Yan Wang", "Huayang Li", "Wai Lam", "Lemao Liu." ],
      "venue" : "ACL.",
      "citeRegEx" : "Cai et al\\.,? 2021",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2021
    }, {
      "title" : "Tagged back-translation",
      "author" : [ "Isaac Caswell", "Ciprian Chelba", "David Grangier." ],
      "venue" : "WMT.",
      "citeRegEx" : "Caswell et al\\.,? 2019",
      "shortCiteRegEx" : "Caswell et al\\.",
      "year" : 2019
    }, {
      "title" : "Active bias: Training more accurate neural networks by emphasizing high variance samples",
      "author" : [ "Haw-Shiuan Chang", "Erik G Learned-Miller", "Andrew McCallum." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Chang et al\\.,? 2017",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2017
    }, {
      "title" : "Semisupervised learning for neural machine translation",
      "author" : [ "Yong Cheng", "Wei Xu", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu." ],
      "venue" : "ACL.",
      "citeRegEx" : "Cheng et al\\.,? 2016",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2016
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL. 6https://transmart.qq.com/index",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Understanding back-translation at scale",
      "author" : [ "Sergey Edunov", "Myle Ott", "Michael Auli", "David Grangier." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Edunov et al\\.,? 2018",
      "shortCiteRegEx" : "Edunov et al\\.",
      "year" : 2018
    }, {
      "title" : "Backtranslation sampling by targeting difficult words in neural machine translation",
      "author" : [ "Marzieh Fadaee", "Christof Monz." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Fadaee and Monz.,? 2018",
      "shortCiteRegEx" : "Fadaee and Monz.",
      "year" : 2018
    }, {
      "title" : "Translationese in machine translation evaluation",
      "author" : [ "Yvette Graham", "Barry Haddow", "Philipp Koehn." ],
      "venue" : "arXiv.",
      "citeRegEx" : "Graham et al\\.,? 2019",
      "shortCiteRegEx" : "Graham et al\\.",
      "year" : 2019
    }, {
      "title" : "Token-level adaptive training for neural machine translation",
      "author" : [ "Shuhao Gu", "Jinchao Zhang", "Fandong Meng", "Yang Feng", "Wanying Xie", "Jie Zhou", "Dong Yu." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Gu et al\\.,? 2020",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2020
    }, {
      "title" : "Achieving human parity on automatic chinese to english news",
      "author" : [ "Hany Hassan", "Anthony Aue", "Chang Chen", "Vishal Chowdhary", "Jonathan Clark", "Christian Federmann", "Xuedong Huang", "Marcin Junczys-Dowmunt", "William Lewis", "Mu Li" ],
      "venue" : null,
      "citeRegEx" : "Hassan et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Hassan et al\\.",
      "year" : 2018
    }, {
      "title" : "Revisiting self-training for neural sequence generation",
      "author" : [ "Junxian He", "Jiatao Gu", "Jiajun Shen", "Marc’Aurelio Ranzato" ],
      "venue" : "In ICLR",
      "citeRegEx" : "He et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2019
    }, {
      "title" : "Kenlm: Faster and smaller language model queries",
      "author" : [ "Kenneth Heafield." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Heafield.,? 2011",
      "shortCiteRegEx" : "Heafield.",
      "year" : 2011
    }, {
      "title" : "Transmart: a practical interactive machine translation system",
      "author" : [ "Guoping Huang", "Lemao Liu", "Xing Wang", "Longyue Wang", "Huayang Li", "Zhaopeng Tu", "Chengyan Huang", "Shuming Shi." ],
      "venue" : "arXiv.",
      "citeRegEx" : "Huang et al\\.,? 2021",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2021
    }, {
      "title" : "Exploiting unsupervised data for emotion recognition in conversations",
      "author" : [ "Wenxiang Jiao", "Michael Lyu", "Irwin King." ],
      "venue" : "EMNLP: Findings.",
      "citeRegEx" : "Jiao et al\\.,? 2020a",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 2020
    }, {
      "title" : "Data rejuvenation: Exploiting inactive training examples for neural machine translation",
      "author" : [ "Wenxiang Jiao", "Xing Wang", "Shilin He", "Irwin King", "Michael R Lyu", "Zhaopeng Tu." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Jiao et al\\.,? 2020b",
      "shortCiteRegEx" : "Jiao et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatic filtering of bilingual corpora for statistical machine translation",
      "author" : [ "Shahram Khadivi", "Hermann Ney." ],
      "venue" : "NLDB.",
      "citeRegEx" : "Khadivi and Ney.,? 2005",
      "shortCiteRegEx" : "Khadivi and Ney.",
      "year" : 2005
    }, {
      "title" : "Sequencelevel knowledge distillation",
      "author" : [ "Yoon Kim", "Alexander M Rush." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Kim and Rush.,? 2016",
      "shortCiteRegEx" : "Kim and Rush.",
      "year" : 2016
    }, {
      "title" : "Statistical Significance Tests for Machine Translation Evaluation",
      "author" : [ "Philipp Koehn." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Koehn.,? 2004",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2004
    }, {
      "title" : "Intelligent selection of language model training data",
      "author" : [ "Robert C Moore William Lewis." ],
      "venue" : "ACL.",
      "citeRegEx" : "Lewis.,? 2010",
      "shortCiteRegEx" : "Lewis.",
      "year" : 2010
    }, {
      "title" : "A simple, fast diverse decoding algorithm for neural generation",
      "author" : [ "Jiwei Li", "Will Monroe", "Dan Jurafsky." ],
      "venue" : "arXiv.",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Multilingual denoising pre-training for neural machine translation",
      "author" : [ "Yinhan Liu", "Jiatao Gu", "Naman Goyal", "Xian Li", "Sergey Edunov", "Marjan Ghazvininejad", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "TACL.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Tagged back-translation revisited: Why does it really work",
      "author" : [ "Benjamin Marie", "Raphael Rubino", "Atsushi Fujita" ],
      "venue" : null,
      "citeRegEx" : "Marie et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Marie et al\\.",
      "year" : 2020
    }, {
      "title" : "Uncertainty-aware self-training for few-shot text classification",
      "author" : [ "Subhabrata Mukherjee", "Ahmed Awadallah." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Mukherjee and Awadallah.,? 2020",
      "shortCiteRegEx" : "Mukherjee and Awadallah.",
      "year" : 2020
    }, {
      "title" : "compare-mt: A tool for holistic comparison of language generation systems",
      "author" : [ "Graham Neubig", "Zi-Yi Dou", "Junjie Hu", "Paul Michel", "Danish Pruthi", "Xinyi Wang." ],
      "venue" : "NAACL (Demonstrations).",
      "citeRegEx" : "Neubig et al\\.,? 2019",
      "shortCiteRegEx" : "Neubig et al\\.",
      "year" : 2019
    }, {
      "title" : "Facebook fair’s wmt19 news translation task submission",
      "author" : [ "Nathan Ng", "Kyra Yee", "Alexei Baevski", "Myle Ott", "Michael Auli", "Sergey Edunov." ],
      "venue" : "WMT.",
      "citeRegEx" : "Ng et al\\.,? 2019",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2019
    }, {
      "title" : "Analyzing uncertainty in neural machine translation",
      "author" : [ "Myle Ott", "Michael Auli", "David Grangier", "Marc’Aurelio Ranzato" ],
      "venue" : null,
      "citeRegEx" : "Ott et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2018
    }, {
      "title" : "Fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "NAACL (Demonstrations).",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "ACL.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Competence-based curriculum learning for neural machine translation",
      "author" : [ "Emmanouil Antonios Platanios", "Otilia Stretcu", "Graham Neubig", "Barnabas Poczos", "Tom Mitchell." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Platanios et al\\.,? 2019",
      "shortCiteRegEx" : "Platanios et al\\.",
      "year" : 2019
    }, {
      "title" : "A call for clarity in reporting BLEU scores",
      "author" : [ "Matt Post." ],
      "venue" : "WMT 2018.",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "Improving neural machine translation models with monolingual data",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "ACL.",
      "citeRegEx" : "Sennrich et al\\.,? 2016a",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "ACL.",
      "citeRegEx" : "Sennrich et al\\.,? 2016b",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Oppo’s machine translation systems for wmt20",
      "author" : [ "Tingxun Shi", "Shiyu Zhao", "Xiaopu Li", "Xiaoxue Wang", "Qian Zhang", "Di Ai", "Dawei Dang", "Xue Zhengshan", "Jie Hao." ],
      "venue" : "WMT.",
      "citeRegEx" : "Shi et al\\.,? 2020",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2020
    }, {
      "title" : "Training region-based object detectors with online hard example mining",
      "author" : [ "Abhinav Shrivastava", "Abhinav Gupta", "Ross Girshick." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Shrivastava et al\\.,? 2016",
      "shortCiteRegEx" : "Shrivastava et al\\.",
      "year" : 2016
    }, {
      "title" : "Generating diverse translations with sentence codes",
      "author" : [ "Raphael Shu", "Hideki Nakayama", "Kyunghyun Cho." ],
      "venue" : "ACL.",
      "citeRegEx" : "Shu et al\\.,? 2019",
      "shortCiteRegEx" : "Shu et al\\.",
      "year" : 2019
    }, {
      "title" : "Leveraging monolingual data with self-supervision for multilingual neural machine translation",
      "author" : [ "Aditya Siddhant", "Ankur Bapna", "Yuan Cao", "Orhan Firat", "Mia Xu Chen", "Sneha Kudugunta", "Naveen Arivazhagan", "Yonghui Wu." ],
      "venue" : "ACL.",
      "citeRegEx" : "Siddhant et al\\.,? 2020",
      "shortCiteRegEx" : "Siddhant et al\\.",
      "year" : 2020
    }, {
      "title" : "Modeling coverage for neural machine translation",
      "author" : [ "Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li." ],
      "venue" : "ACL.",
      "citeRegEx" : "Tu et al\\.,? 2016",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2016
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Improving back-translation with uncertainty-based confidence estimation",
      "author" : [ "Shuo Wang", "Yang Liu", "Chao Wang", "Huanbo Luan", "Maosong Sun." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "On the Inference Calibration of Neural Machine Translation",
      "author" : [ "Shuo Wang", "Zhaopeng Tu", "Shuming Shi", "Yang Liu." ],
      "venue" : "ACL.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Uncertaintyaware semantic augmentation for neural machine translation",
      "author" : [ "Xiangpeng Wei", "Heng Yu", "Yue Hu", "Rongxiang Weng", "Luxi Xing", "Weihua Luo." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Wei et al\\.,? 2020",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2020
    }, {
      "title" : "Pay less attention with lightweight and dynamic convolutions",
      "author" : [ "Felix Wu", "Angela Fan", "Alexei Baevski", "Yann N Dauphin", "Michael Auli." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Wu et al\\.,? 2019a",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploiting monolingual data at scale for neural machine translation",
      "author" : [ "Lijun Wu", "Yiren Wang", "Yingce Xia", "QIN Tao", "Jianhuang Lai", "Tie-Yan Liu." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Wu et al\\.,? 2019b",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Tencent neural machine translation systems for the wmt20 news translation task",
      "author" : [ "Shuangzhi Wu", "Xing Wang", "Longyue Wang", "Fangxu Liu", "Jun Xie", "Zhaopeng Tu", "Shuming Shi", "Mu Li." ],
      "venue" : "WMT.",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploiting source-side monolingual data in neural machine translation",
      "author" : [ "Jiajun Zhang", "Chengqing Zong." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Zhang and Zong.,? 2016",
      "shortCiteRegEx" : "Zhang and Zong.",
      "year" : 2016
    }, {
      "title" : "Joint training for neural machine translation models with monolingual data",
      "author" : [ "Zhirui Zhang", "Shujie Liu", "Mu Li", "Ming Zhou", "Enhong Chen." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Understanding knowledge distillation in nonautoregressive machine translation",
      "author" : [ "Chunting Zhou", "Graham Neubig", "Jiatao Gu." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Zhou et al\\.,? 2019",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2019
    }, {
      "title" : "Uncertainty-aware curriculum learning for neural machine translation",
      "author" : [ "Yikai Zhou", "Baosong Yang", "Derek F Wong", "Yu Wan", "Lidia S Chao." ],
      "venue" : "ACL.",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    }, {
      "title" : "2018)7 to remove sentences longer than 250 words as well as sentencepairs with a source/target length ratio exceeding 7https://github.com/pytorch/fairseq/ tree/master/examples/backtranslation",
      "author" : [ "followed Edunov" ],
      "venue" : null,
      "citeRegEx" : "Edunov,? \\Q2018\\E",
      "shortCiteRegEx" : "Edunov",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Leveraging large-scale unlabeled data has become an effective approach for improving the performance of natural language processing (NLP) models (Devlin et al., 2019; Brown et al., 2020; Jiao et al., 2020a).",
      "startOffset" : 145,
      "endOffset" : 206
    }, {
      "referenceID" : 1,
      "context" : "Leveraging large-scale unlabeled data has become an effective approach for improving the performance of natural language processing (NLP) models (Devlin et al., 2019; Brown et al., 2020; Jiao et al., 2020a).",
      "startOffset" : 145,
      "endOffset" : 206
    }, {
      "referenceID" : 15,
      "context" : "Leveraging large-scale unlabeled data has become an effective approach for improving the performance of natural language processing (NLP) models (Devlin et al., 2019; Brown et al., 2020; Jiao et al., 2020a).",
      "startOffset" : 145,
      "endOffset" : 206
    }, {
      "referenceID" : 32,
      "context" : ", data augmentation (Sennrich et al., 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al.",
      "startOffset" : 20,
      "endOffset" : 66
    }, {
      "referenceID" : 46,
      "context" : ", data augmentation (Sennrich et al., 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al.",
      "startOffset" : 20,
      "endOffset" : 66
    }, {
      "referenceID" : 5,
      "context" : ", 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al., 2016; Zhang et al., 2018; Cai et al., 2021), pre-training (Siddhant et al.",
      "startOffset" : 56,
      "endOffset" : 114
    }, {
      "referenceID" : 47,
      "context" : ", 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al., 2016; Zhang et al., 2018; Cai et al., 2021), pre-training (Siddhant et al.",
      "startOffset" : 56,
      "endOffset" : 114
    }, {
      "referenceID" : 2,
      "context" : ", 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al., 2016; Zhang et al., 2018; Cai et al., 2021), pre-training (Siddhant et al.",
      "startOffset" : 56,
      "endOffset" : 114
    }, {
      "referenceID" : 32,
      "context" : "Among them, data augmentation with the synthetic parallel data (Sennrich et al., 2016a; Edunov et al., 2018) is the most widely used approach due to its simple and effective implementation.",
      "startOffset" : 63,
      "endOffset" : 108
    }, {
      "referenceID" : 7,
      "context" : "Among them, data augmentation with the synthetic parallel data (Sennrich et al., 2016a; Edunov et al., 2018) is the most widely used approach due to its simple and effective implementation.",
      "startOffset" : 63,
      "endOffset" : 108
    }, {
      "referenceID" : 11,
      "context" : "It has been a de-facto standard in developing the large-scale NMT systems (Hassan et al., 2018; Ng et al., 2019; Wu et al., 2020; Huang et al., 2021).",
      "startOffset" : 74,
      "endOffset" : 149
    }, {
      "referenceID" : 26,
      "context" : "It has been a de-facto standard in developing the large-scale NMT systems (Hassan et al., 2018; Ng et al., 2019; Wu et al., 2020; Huang et al., 2021).",
      "startOffset" : 74,
      "endOffset" : 149
    }, {
      "referenceID" : 45,
      "context" : "It has been a de-facto standard in developing the large-scale NMT systems (Hassan et al., 2018; Ng et al., 2019; Wu et al., 2020; Huang et al., 2021).",
      "startOffset" : 74,
      "endOffset" : 149
    }, {
      "referenceID" : 14,
      "context" : "It has been a de-facto standard in developing the large-scale NMT systems (Hassan et al., 2018; Ng et al., 2019; Wu et al., 2020; Huang et al., 2021).",
      "startOffset" : 74,
      "endOffset" : 149
    }, {
      "referenceID" : 46,
      "context" : "Self-training (Zhang and Zong, 2016) is one of the most commonly used approaches for data augmentation.",
      "startOffset" : 14,
      "endOffset" : 36
    }, {
      "referenceID" : 7,
      "context" : "Recent studies have shown that synthetic data manipulation (Edunov et al., 2018; Caswell et al., 2019) and training strategy optimization (Wu et al.",
      "startOffset" : 59,
      "endOffset" : 102
    }, {
      "referenceID" : 3,
      "context" : "Recent studies have shown that synthetic data manipulation (Edunov et al., 2018; Caswell et al., 2019) and training strategy optimization (Wu et al.",
      "startOffset" : 59,
      "endOffset" : 102
    }, {
      "referenceID" : 44,
      "context" : ", 2019) and training strategy optimization (Wu et al., 2019b; Wang et al., 2019) in the last two steps can boost the self-training performance significantly.",
      "startOffset" : 43,
      "endOffset" : 80
    }, {
      "referenceID" : 40,
      "context" : ", 2019) and training strategy optimization (Wu et al., 2019b; Wang et al., 2019) in the last two steps can boost the self-training performance significantly.",
      "startOffset" : 43,
      "endOffset" : 80
    }, {
      "referenceID" : 18,
      "context" : "Intuitively, self-training simplifies the complexity of generated target sentences (Kim and Rush, 2016; Zhou et al., 2019; Jiao et al., 2020b), and easy patterns in monolingual sentences with deterministic translations may not provide additional gains over the self-training “teacher” model (Shrivastava et al.",
      "startOffset" : 83,
      "endOffset" : 142
    }, {
      "referenceID" : 48,
      "context" : "Intuitively, self-training simplifies the complexity of generated target sentences (Kim and Rush, 2016; Zhou et al., 2019; Jiao et al., 2020b), and easy patterns in monolingual sentences with deterministic translations may not provide additional gains over the self-training “teacher” model (Shrivastava et al.",
      "startOffset" : 83,
      "endOffset" : 142
    }, {
      "referenceID" : 16,
      "context" : "Intuitively, self-training simplifies the complexity of generated target sentences (Kim and Rush, 2016; Zhou et al., 2019; Jiao et al., 2020b), and easy patterns in monolingual sentences with deterministic translations may not provide additional gains over the self-training “teacher” model (Shrivastava et al.",
      "startOffset" : 83,
      "endOffset" : 142
    }, {
      "referenceID" : 35,
      "context" : ", 2020b), and easy patterns in monolingual sentences with deterministic translations may not provide additional gains over the self-training “teacher” model (Shrivastava et al., 2016).",
      "startOffset" : 157,
      "endOffset" : 183
    }, {
      "referenceID" : 24,
      "context" : "2841 vision also reveals that easy patterns in unlabeled data with the deterministic prediction may not provide additional gains (Mukherjee and Awadallah, 2020).",
      "startOffset" : 129,
      "endOffset" : 160
    }, {
      "referenceID" : 21,
      "context" : "However, generation may lead to bias estimation due to the generation diversity issue (Li et al., 2016; Shu et al., 2019).",
      "startOffset" : 86,
      "endOffset" : 121
    }, {
      "referenceID" : 36,
      "context" : "However, generation may lead to bias estimation due to the generation diversity issue (Li et al., 2016; Shu et al., 2019).",
      "startOffset" : 86,
      "endOffset" : 121
    }, {
      "referenceID" : 39,
      "context" : "We chose the state-of-the-art TRANSFORMER (Vaswani et al., 2017) network as our model, which consists of an encoder of 6 layers and a decoder of 6 layers.",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 28,
      "context" : "We adopted the open-source toolkit Fairseq (Ott et al., 2019) to implement the model.",
      "startOffset" : 43,
      "endOffset" : 61
    }, {
      "referenceID" : 43,
      "context" : "For the TRANSFORMER-BIG model, we trained it for 30K steps with 460K (3600 × 128) tokens per batch with the cosine learning rate schedule (Wu et al., 2019a).",
      "startOffset" : 138,
      "endOffset" : 156
    }, {
      "referenceID" : 29,
      "context" : "We evaluated the models by BLEU score (Papineni et al., 2002) computed by SacreBLEU (Post, 2018)2.",
      "startOffset" : 38,
      "endOffset" : 61
    }, {
      "referenceID" : 19,
      "context" : "We measured the statistical significance of improvement with paired bootstrap resampling (Koehn, 2004) using compare-mt3 (Neubig et al.",
      "startOffset" : 89,
      "endOffset" : 102
    }, {
      "referenceID" : 25,
      "context" : "We measured the statistical significance of improvement with paired bootstrap resampling (Koehn, 2004) using compare-mt3 (Neubig et al., 2019).",
      "startOffset" : 121,
      "endOffset" : 142
    }, {
      "referenceID" : 0,
      "context" : "Training on these sentences forces the models to over-fit on these incorrect synthetic data, resulting in the confirmation bias issue (Arazo et al., 2020).",
      "startOffset" : 134,
      "endOffset" : 154
    }, {
      "referenceID" : 4,
      "context" : "These results corroborate with prior studies (Chang et al., 2017; Mukherjee and Awadallah, 2020) such that learning on certain examples brings little gain while on the excessively uncertain examples may also hurt the model training.",
      "startOffset" : 45,
      "endOffset" : 96
    }, {
      "referenceID" : 24,
      "context" : "These results corroborate with prior studies (Chang et al., 2017; Mukherjee and Awadallah, 2020) such that learning on certain examples brings little gain while on the excessively uncertain examples may also hurt the model training.",
      "startOffset" : 45,
      "endOffset" : 96
    }, {
      "referenceID" : 30,
      "context" : "Specifically, we performed linguistic analysis on the five data bins in terms of three properties: 1) sentence length that counts the tokens in the sentence, 2) word rarity (Platanios et al., 2019) that measures the frequency of words in a sentence with a higher value indicating a more rare sentence, and 3) translation coverage (Khadivi and Ney, 2005) that measures the ratio of source words being aligned with any target words.",
      "startOffset" : 173,
      "endOffset" : 197
    }, {
      "referenceID" : 17,
      "context" : ", 2019) that measures the frequency of words in a sentence with a higher value indicating a more rare sentence, and 3) translation coverage (Khadivi and Ney, 2005) that measures the ratio of source words being aligned with any target words.",
      "startOffset" : 140,
      "endOffset" : 163
    }, {
      "referenceID" : 10,
      "context" : "The monolingual sentences in the last data bin noticeably contain more rare words than other bins in Figure 3(b), and the rare words in the sentences pose a great challenge in the NMT training process (Gu et al., 2020).",
      "startOffset" : 201,
      "endOffset" : 218
    }, {
      "referenceID" : 27,
      "context" : "We adopt the TRANSFORMER-BIG with large batch training (Ott et al., 2018) to achieve the strong performance.",
      "startOffset" : 55,
      "endOffset" : 73
    }, {
      "referenceID" : 12,
      "context" : "One possible reason is that the TRANSFORMER-BIG model to construct the pseudo-sentences was trained on the whole WMT19 En-De data that contains the heldout data, which serves as self-training to decently improve the supervised baseline (He et al., 2019).",
      "startOffset" : 236,
      "endOffset" : 253
    }, {
      "referenceID" : 12,
      "context" : "2848 which can be achieved by self-training (He et al., 2019) and back-translation (Sennrich et al.",
      "startOffset" : 44,
      "endOffset" : 61
    }, {
      "referenceID" : 8,
      "context" : "While back-translation has dominated the NMT area for years (Fadaee and Monz, 2018; Edunov et al., 2018; Caswell et al., 2019), recent works on translationese (Marie et al.",
      "startOffset" : 60,
      "endOffset" : 126
    }, {
      "referenceID" : 7,
      "context" : "While back-translation has dominated the NMT area for years (Fadaee and Monz, 2018; Edunov et al., 2018; Caswell et al., 2019), recent works on translationese (Marie et al.",
      "startOffset" : 60,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : "While back-translation has dominated the NMT area for years (Fadaee and Monz, 2018; Edunov et al., 2018; Caswell et al., 2019), recent works on translationese (Marie et al.",
      "startOffset" : 60,
      "endOffset" : 126
    }, {
      "referenceID" : 23,
      "context" : ", 2019), recent works on translationese (Marie et al., 2020; Graham et al., 2019) suggest that NMT models trained with backtranslation may lead to distortions in automatic and human evaluation.",
      "startOffset" : 40,
      "endOffset" : 81
    }, {
      "referenceID" : 9,
      "context" : ", 2019), recent works on translationese (Marie et al., 2020; Graham et al., 2019) suggest that NMT models trained with backtranslation may lead to distortions in automatic and human evaluation.",
      "startOffset" : 40,
      "endOffset" : 81
    }, {
      "referenceID" : 46,
      "context" : "In this new testing setup, the forward-translation (Zhang and Zong, 2016), i.",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 14,
      "context" : "The proposed technology has been applied to TranSmart6 (Huang et al., 2021), an interactive machine translation system in Tencent, to improve the performance of its core translation engine.",
      "startOffset" : 55,
      "endOffset" : 75
    } ],
    "year" : 2021,
    "abstractText" : "Self-training has proven effective for improving NMT performance by augmenting model training with synthetic parallel data. The common practice is to construct synthetic data based on a randomly sampled subset of large-scale monolingual data, which we empirically show is sub-optimal. In this work, we propose to improve the sampling procedure by selecting the most informative monolingual sentences to complement the parallel data. To this end, we compute the uncertainty of monolingual sentences using the bilingual dictionary extracted from the parallel data. Intuitively, monolingual sentences with lower uncertainty generally correspond to easy-to-translate patterns which may not provide additional gains. Accordingly, we design an uncertainty-based sampling strategy to efficiently exploit the monolingual data for self-training, in which monolingual sentences with higher uncertainty would be sampled with higher probability. Experimental results on large-scale WMT English⇒German and English⇒Chinese datasets demonstrate the effectiveness of the proposed approach. Extensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side.1",
    "creator" : "LaTeX with hyperref"
  }
}