{
  "name" : "2021.acl-long.276.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "LearnDA: Learnable Knowledge-Guided Data Augmentation for Event Causality Identification",
    "authors" : [ "Xinyu Zuo", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao", "Weihua Peng", "Yuguang Chen" ],
    "emails" : [ "xinyu.zuo@nlpr.ia.ac.cn", "pengfei.cao@nlpr.ia.ac.cn", "yubo.chen@nlpr.ia.ac.cn", "kliu@nlpr.ia.ac.cn", "jzhao@nlpr.ia.ac.cn", "pengweihua@baidu.com", "chenyuguang@baidu.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3558–3571\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3558"
    }, {
      "heading" : "1 Introduction",
      "text" : "Event causality identification (ECI) aims to identify causal relations between events in texts, which can provide crucial clues for NLP tasks, such as logical reasoning and question answering (Girju, 2003; Oh et al., 2013, 2017). This task is usually modeled as a classification problem, i.e. determining whether there is a causal relation between two events in a sentence. For example in Figure 1, an ECI system should identify two causal relations in two sentences: (1) attack cause−→ killed in S1; (2) statement cause−→ protests in S2.\nMost existing methods for ECI heavily rely on annotated training data (Mirza and Tonelli, 2016;\nRiaz and Girju, 2014b; Hashimoto et al., 2014; Hu and Walker, 2017; Gao et al., 2019). However, existing datasets are relatively small, which impede the training of the high-performance event causality reasoning model. According to our statistics, the largest widely used dataset EventStoryLine Corpus (Caselli and Vossen, 2017) only contains 258 documents, 4316 sentences, and 1770 causal event pairs. Therefore, data lacking is an essential problem that urgently needs to be addressed for ECI.\nUp to now, data augmentation is one of the most effective methods to solve the data lacking problem. However, most of the NLP-related augmentation methods are a task-independent framework that produces new data at one time (Zhang et al., 2015; Guo et al., 2019; Xie et al., 2019b). In these frameworks, data augmentation and target task are modeled independently. This often leads to a lack of task-related characteristics in the generated data, such as taskrelated linguistic expression and knowledge. For example, easy data augmentation (EDA) (Wei and Zou, 2019) is the most representative method that relies on lexical substitution, deletion, swapping, and insertion to produce new data. However, solely relying on such word operations often generates new data that dissatisfies task-related qualities. As shown in Figure 1, S3 is produced by EDA, it lacks a linguistic expression that expresses the causal semantics between kill and attack. Therefore, how to\ninteractively model data augmentation and target task to generate new data with task-related characteristics is a challenging problem on ECI.\nSpecific to ECI, we argue that an ideal taskrelated generated causal sentence needs to possess two characteristics as follows. (1) The two events in the causal sentence need to have a causal relation. We call such property as Causality. For example, there is usually a causal relation between an attack event and a kill event, while nearly no causal relation between an attack event and a born event. (2) The linguistic expressions of the causal sentence need to be well-formed to express the causal semantic of events. We call such property as Well-formedness, which consists of a) canonical sentence grammar, b) event-related entities with semantic roles (e.g. the attack was carried out by a police in S1), and c) cohesive words that express complete causal semantics (e.g. in a and other words except for events and entities in S1).\nTo this end, we propose a learnable data augmentation framework for ECI, dubbed as Learnable Knowledge-Guided Data Augmentation (LearnDA). This framework regards sentence-torelation mapping (the target task, ECI) and relationto-sentence mapping (the augmentation task, sentence generation) as dual tasks and models the mutual relation between them via dual learning. Specifically, LearnDA can use the duality to generate task-related new sentences learning from identification and makes it more accurate to understand the causal semantic learning from generation. On the one hand, LearnDA is knowledge guided. It introduces diverse causal event pairs from KBs to initialize the dual generation which could ensure the causality of generated causal sentences. For example, the knowledge of judgment cause−→ demonstration from KBs can be used to construct a novel causal sentence, which is also helpful to understand the causal semantic of statement cause−→ protests. On the other hand, LearnDA is learnable. It employs a constrained generative architecture to generate well-formed linguistic expressions via iteratively learning in the dual interaction, which expresses the causal semantic between given events. Methodologically, it gradually fills the remaining missing cohesive words of the complete sentences under the constraint of given events and related entities.\nIn experiments, we evaluate our model on two benchmarks. We first concern the standard evaluation and show that our model achieves the state-of-\nthe-art performance on ECI. Then we estimate the main components of LearnDA. Finally, our learnable augmentation framework demonstrates definite advantages over other augmentation methods in generating task-related data for ECI.\nIn summary, the contributions as follows:\n• We propose a new learnable data augmentation framework to solve the data lacking problem of ECI. Our framework can leverage the duality between identification and generation via dual learning which can learn to generate task-related sentences for ECI.\n• Our framework is knowledge guided and learnable. Specifically, we introduce causal event pairs from KBs to initialize the dual generation, which could ensure the causality of generated causal sentences. We also employ a constrained generative architecture to gradually generate well-formed causal linguistic expressions of generated causal sentences via iteratively learning in the dual interaction.\n• Experimental results on two benchmarks show that our model achieves the best performance on ECI. Moreover, it also shows definite advantages over previous data augmentation methods."
    }, {
      "heading" : "2 Related Work",
      "text" : "To date, many researches attempt to identify the causality with linguistic patterns or statistical features. For example, some methods rely on syntactic and lexical features (Riaz and Girju, 2013, 2014b). Some focus on explicit causal textual patterns (Hashimoto et al., 2014; Riaz and Girju, 2014a, 2010; Do et al., 2011; Hidey and McKeown, 2016). And some others pay attention on statistical causal association and cues (Beamer and Girju, 2009; Hu et al., 2017; Hu and Walker, 2017).\nRecently, more attention is paid to the causality between events. Mirza and Tonelli (2014) annotated Causal-TimeBank of event-causal relations based on the TempEval-3 corpus. Mirza et al. (2014), Mirza and Tonelli (2016) extracted eventcausal relation with a rule-based multi-sieve approach and improved the performance incorporating with event temporal relation. Mostafazadeh et al. (2016) annotated both temporal and causal relations in 320 short stories. Caselli and Vossen (2017) annotated the EventStoryLine Corpus for\nevent causality identification. Dunietz et al. (2017) presented BECauSE 2.0, a new version of the BECauSE corpus (Dunietz et al., 2015) of causal relation and other seven relations. Gao et al. (2019) modeled document-level structures to identify causality. Liu et al. (2020) identified event causality with the mention masking generalization.\nUnlike computer vision, the augmentation of text data in NLP is pretty rare (Chaudhary, 2020). Zuo et al. (2020) solved the data lacking problem of ECI with the distantly supervised labeled training data. However, including the distant supervision, most of the existing data augmentation methods for NLP tasks are task-independent frameworks (Related work of data augmentation and dual learning are detailed in Appendix B). Inspired by some generative methods which try to generate additional training data while preserving the class label (AnabyTavor et al., 2019; Yang et al., 2019; Papanikolaou and Pierleoni, 2020), we introduce a new learnable framework for augmenting task-related training data for ECI via dual learning enhanced with external knowledge."
    }, {
      "heading" : "3 Methodology",
      "text" : "As shown in Figure 2, LearnDA jointly models a knowledge guided sentence generator (input: event pair and its causal/non-causal relation, output: causal/non-causal sentence) and an event causality identifier (input: event pair and its sentence, output: causal/non-causal relation) with dual learning. LearnDA iteratively optimizes identifier and generator to generate task-related training data, and then utilize new data to further train the identifier. Therefore, we first present the main idea of dual learning, which is the architecture of learnable dual augmentation, including the states, actions, policies, and"
    }, {
      "heading" : "Causal-Generator",
      "text" : "rewards. Then, we briefly introduce the knowledge guided sentence generator, especially the processes of knowledge guiding and constrained sentence generation. Finally, we describe the event causality identifier and training processes of LearnDA."
    }, {
      "heading" : "3.1 Architecture of Learnable Dual Augmentation",
      "text" : "The architecture of learnable dual augmentation is shown in Figure 3. Specifically, I denotes the event causality identifier, and G denotes the sentence generator which consists of two independent generators. They produce causal and non-causal sentences on the relation c of input event pair ep.\nGenerally, G generates a sentence s′ which expresses the causal or non-causal relation c of the input event pair ep. Then it receives the reward R that consists of a semantic alignment reward Rs from itself and a causality reward Rc from I (primal cycle). Similarly, I identifies the causal or non-causal relation c′ of the input event pair ep with its sentence s. Then it receives the reward R consists of a causality reward Rc from itself and a semantic alignment reward Rs from G (dual cycle).\nI and G are optimized interactively with dual reinforcement learning. Specifically, for G, an action is the generation from relation to sentence, a state is denoted by the representation of input event pair and its relation, a policy is defined by the parameters of generator. For I, an action is the identification from sentence to relation, a state is denoted by the representation of input event pair and its\nsentence, a policy is defined by the parameters of identifier. Inspired by Shen and Feng (2020), we utilize a probability distribution over actions given states to represent the policys, i.e., the probability distribution of the generation of G and identification of I. As aforementioned, we introduce two rewards, causality (Rc) and semantic alignment (Rs) rewards, which encourage G to generate taskrelated sentences with the feedback from identifier, while further optimize I with the feedback from generator. Definitions are as following:\nCausality Reward (Rc) If the relation of input event pair can be clearly expressed by the generated sentence, it will be easier to be understood by identifier. Therefore, we use the causal relation classification accuracy as the causality reward to evaluate the causality of generated sentences, while tune and optimize the identifier itself:\nRc(ep, s) = { p(c′|s; θI) Correct classification −p(c′|s; θI) Otherwise, (1) where θI is the parameter of I, p(c′|s; θI) denotes the probability of relation classification, s denotes the input sentence and c′ is the classified relation.\nSemantic Alignment Reward (Rs) We hope that the semantic of the generated sentence can be consistent with the relation of the input event pair. Additionally, if the relation of the input event pair can be more accurately classified, the semantic of the new generated sentence can be considered more consistent with it. Therefore, we measure the semantic alignment by means of the probability of constructing a sentence with similar semantic to the input relation, and the reward is:\nRs(ep, c) = p(s ′|c; θG) =\n1 |Ts| ∑ t∈Ts p(t|c; θG), (2)\nwhere θG is the parameter of G, c is the input relation, t is one of the generated tokens Ts of the generated sentence s′, and p(t|c; θG) is the generated probability of t. Specifically, there are two independent G with different θG. In detail, θcG is employed to generated causal sentence when the input c is causal relation, and non-causal sentence is generated via θncG when c is non-causal relation."
    }, {
      "heading" : "3.2 Knowledge Guided Sentence Generator",
      "text" : "As shown in Figure 4, knowledge guided sentence generator (KSG) first introduces diverse causal and non-causal event pairs from KBs for causality. Then, given an event pair and its causal or non-causal relation, it employs a constrained gen-\nerative architecture to generate new well-formed causal/non-causal sentences that contain them.\nKnowledge Guiding KSG introduces event pairs that are probabilistic causal or non-causal from multiple knowledge bases in two ways. (1) Lexical knowledge expanding: expanding annotated event pairs via external dictionaries, such as WordNet (Miller, 1995) and VerbNet (Schuler, 2005). (2) Connective knowledge introducing: introducing event pairs from external event-annotated documents (KBP corpus) assisted with FrameNet (Baker et al., 1998) and Penn Discourse Treebank (PDTB2) (Group et al., 2008). As shown in Table 1, we illustrate how to extract event pairs from multiple knowledge bases. Then, inspired by Bordes et al. (2013), we filter the extracted event pairs by converting them into triples <ei, causal/noncausal, ej> and calculating the causal-distance by maximizing L in a causal representation space:\nL = ∑\n(ei,ej)∈T ∑ (e′i,e ′ j)∈T ′ [λ+ d(e′i, e ′ j)− d(ei, ej)]+, (3)\nwhere T and T ′ are the causal and non-causal triples set respectively, and e is the representation of event. After that, the higher probability of causal relation, the shorter distance between two events, and we sort event pairs in ascending order by their distances. Finally, we keep the top and bottom α% sorted event pairs to obtain the causal and noncausal event pairs sets for generation.\nConstrained Sentence Generator Given an event pair, constrained sentence generator produces a well-formed sentence that expresses its causal or non-causal relation in three stages: (1) assigning event-related entities ensures the logic of the semantic roles of events, (2) completing sentences ensures the completeness of causal or non-causal\nsemantic expression, (3) filtering sentences ensures the quality and diversity of generated sentences.\nAssigning Event-related Entities. Event related entities play different semantic roles of events in sentences, which is an important part of eventsemantic expression. Hence, as shown in Figure 4, given an event pair, we firstly assign logical entities for input events to guarantee the logic of semantic roles in the new sentences, such as gang is a logical entity as the body of the event onrush. Logically, entities of the same type play the same semantic roles in similar events. Moreover, as shown in Table 1, there is a corresponding original sentence for each extracted event pair. Therefore, in new sentence, we assign the most similar entity in the same type from candidate set2 for each entity in the original sentence. For example, we assign gang for onrush in new sentence which is similar with the police related to attack in the original sentence. Specifically, we put the candidate entities in the same position in the original sentence to obtain their BERT embeddings. Then we select entities via the cosine similarity between their embeddings: E(ent) = 1|ent| ∑ w∈ent E(w), where ent is the entity and E(w) is the BERT embedding of ent. Completing Sentences. A well-formed sentence requires a complete linguistic expression to express the causal or non-causal semantics. Therefore, we complete sentences by filling the cohesive words between given events and assigned entities with masked BERT (Devlin et al., 2019). All words except events and entities are regarded as cohesive words. Specifically, we insert a certain number of the special token [MASK] between events and\n2We collect entities from annotated data and KBP corpus.\nentities, and then predict the [MASK]3 tokens as new words. As shown in Figure 4, we fill cohesive tokens via two independent generators to express causal and non-causal semantic according to the relation of given events. For example, in a guiding a causal semantic filled by the causal generator.\nFiltering Sentences. Inspired by Yang et al. (2019), we design a filter to select new sentences that are balanced between high quality and high diversity with two key factors: 1) Perplexity (PPL): we take the average probability of the filled cohesive words in the new sentence s′ as its perplexity: PPL(s′) = 1|T (s′)| ∑ t∈T (s′) P (t), where T is the set of filled cohesive words. 2) Distance (DIS): we calculate the cosine similarity between generated sentence s′ and annotated data Dm as its distance: DIS(s′, Dm) = 1|Dm| ∑ s∈Dm E(s′)·E(s) E(s′)×E(s) , where Dm is m random selected annotated sentences and E is the BERT sentence representation of the [CLS] token. A new sentence should have both appropriate high PPL which indicates the quality of generation, and appropriate high DIS which indicates the difference from the original sentences. Therefore, we select the top β% of the newly generated sentences according to Score for the further training of identifier as following: Score(s′) = µPPL(s′) + (1− µ)DIS(s′, Dm)), where the µ is an hyper-parameter."
    }, {
      "heading" : "3.3 Training of LearnDA for ECI",
      "text" : "We briefly describe the training processes of LearnDA for ECI, including the pre-training of generator and identifier, the dual reinforcement training, and the further training of identifier.\n3The inserted [MASK] is 1.2 times the number of words between events and entities in the original sentence.\nAlgorithm 1 Dual Reinforcement Training of G I. Require: A set of knowledge guided event pairs {(ep,s,c)} A pre-trained generator G and identifier I Repeat: Early stop on the development set according to I. 1: Loop: PRIMAL CYCLE 2: for event pair (epi, si, ci) in batch do 3: Generator generates the sentence s′i of epi; 4: Identifier re-predicts the causality c∗i of epi; 5: Computing the reward as: 6: Rsprimal = λRs(epi, ci)+(1−λ)Rc(epi, s′i). 7: Computing the stochastic gradient of θG : 8: ∇G+ = Rsprimal · ∇θGLG(epi, ci). 9: end for 10: Model batch updates: θG ← θG + η · ∇G 11: end Loop: 12: 13: Loop: DUAL CYCLE 14: for event pair (epi, si, ci) in batch do 15: Identifier predicts the causality c′i of epi; 16: Generator re-generates the sentence s∗i of epi; 17: Computing the reward as: 18: Rsdual = γRc(epi, si) + (1− γ)Rs(epi, c′i). 19: Computing the stochastic gradient of θI : 20: ∇I+ = Rsdual · ∇θILI(epi, si). 21: end for 22: Model batch updates: θI ← θI + η · ∇I 23: end Loop:\nEvent Causality Identifier First of all, we formulate event causality identification as a sentencelevel binary classification problem. Specifically, we design a classifier based on BERT (Devlin et al., 2019) to build our identifier. The input of the identifier is the event pair ep and its sentence s. Next, we take the stitching of manually designed features (same lexical, causal potential, and syntactic features as Gao et al. (2019)) and two event representations as the input of top MLP classifier. Finally, the output is a binary vector to predict the causal/noncausal relation of the input event pair ep.\nPre-training We pre-train the identifier and generator on labeled data before dual reinforcement training. On the one hand, we train identifier via the cross-entropy objective function of the relation classification. On the other hand, for generators, we keep the events and entities in the input sentences, replace the remaining tokens with a special token [MASK], and then train it via the cross-entropy objective function to re-predict the masked tokens. Specifically, causal generator and non-causal generator are pre-trained on causal and non-causal labeled sentences respectively.\nDual Reinforcement Training As shown in Algorithm 1, we interactively optimize the generator and identifier by dual reinforcement learning. Specifically, we maximize the following objective\nfunctions:\nLG(ep, c) =\n{ p(s′|c; θG) = 1|Ts| ∑ t∈Ts p(t|c; θG)\np(s′|c; θNG) = 1|Ts| ∑ t∈Ts p(t|c; θNG),\n(4)\nLI(ep, s) = p(c ′|s; θI), (5)\nwhere θG and θNG is the parameters of causal and non-causal sentence generators respectively, Ts is the masked tokens. Finally, after dual data augmentation, we utilize generated sentences to further train the dual-trained identifier via the crossentropy objective function of relation classification."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Setup",
      "text" : "Dataset and Evaluation Metrics Our experiments are conducted on two main benchmark datasets, including: EventStoryLine v0.9 (ESC) (Caselli and Vossen, 2017) described above; and (2) Causal-TimeBank (Causal-TB) (Mirza and Tonelli, 2014) which contains 184 documents, 6813 events, and 318 causal event pairs. Same as previous methods, we use the last two topics of ESC as the development set for two datasets. For evaluation, we adopt Precision (P), Recall (R), and F1-score (F1) as evaluation metrics. We conduct 5-fold and 10-fold cross-validation on ESC and Causal-TB respectively, same as previous methods to ensure comparability. All the results are the average of three independent experiments.\nParameters Settings In implementations, both the identifier and generators are implemented on BERT-Base architecture4, which has 12-layers, 768-hiddens, and 12-heads. We set the learning rate of generator pre-training, identifier pretraining/further training, and dual reinforcement training as 1e-5, 1e-5, and 1e-7 respectively. We set the ratio of the augmented data used for training to the labeled data, α, β, µ, λ and γ as 1:2, 30%, 50%, 0.2, 0.5 and 0.5 respectively tuned on the development set. And we apply early stop and SGD gradient strategy to optimize all models. We also adopt a negative sampling rate of 0.5 for training the identifier, owing to the sparseness of positive examples. (See Appendix D for more details.)\nCompared Methods Same as previous state-ofthe-art work. For ESC, we prefer 1) LSTM (Cheng and Miyao, 2017), a dependency path based\n4https://github.com/google-research/ bert\nsequential model that models the context between events to identify causality; 2) Seq (Choubey and Huang, 2017), a sequence model explores complex human designed features for ECI; 3) LR+ and ILP (Gao et al., 2019), document-level models adopt document structures for ECI. For Causal-TB, we prefer 1) RB, a rule-based system; 2) DD, a data driven machine learning based system; 3) VR-C, a verb rule based model with data filtering and gold causal signals enhancement. These models are designed by Mirza and Tonelli (2014); Mirza (2014) for ECI.\nOwing to our methods are constructed on BERT, we build BERT-based methods: 1) BERT, a BERTbased baseline, our basic proposed event causality identifier. 2) MM (Liu et al., 2020), the BERTbased SOTA method with mention masking generalization. 3) MM+Aug, the further re-trained MM with our dual augmented data. 4) KnowDis (Zuo et al., 2020) improved the performance of ECI with the distantly labeled training data. We compare with it to illustrate the quality of our generated ECI-related training data. 5) MM+ConceptAug, to make a fair comparison, we introduce causalrelated events from ConceptNet that employed by MM, and generate new sentences via KonwDis and LearnDA to further re-train MM (see Appendix C for details). Finally, we use LearnDAFull indicates our full model, which is the dual-trained identifier further trained via dual augmented data."
    }, {
      "heading" : "4.2 Our Method vs. State-of-the-art Methods",
      "text" : "Table 2 shows the results of ECI on EventStoryLine and Causal-TimeBank. From the results:\n1) Our LearnDAFull outperforms all baselines and achieves the best performance (52.6%/51.9% on F1 value), outperforming the no-bert (ILP/VRC) and bert (MM/KnowDis) state-of-the-art methods by a margin of 7.9%/8.7% and 2.5%/2.1% respectively, which justifies its effectiveness. Moreover, BERT-based methods demonstrate high recall value, which is benefited from more training data and their event-related guided knowledge.\n2) Comparing KnowDis with LearnDAFull, we note that training data generated by LearnDA is more helpful to ECI than distant supervision with external knowledge (+2.9%/+2.1%). This shows that LearnDA can generate more ECI-related data.\n3) Comparing MM+ConceptNet with MM, with the same knowledge base, our dual augmented data can further improve the performance\n(+0.8%/+2.8%), which illustrates that LearnDA can make more effective use of external knowledge by generating task-related training data.\n4) Comparing MM+Aug with MM, we note that training with our dual augmented data can improve the performance by 1.4%/3.9%, even though MM is designed on BERT-Large (LearnDA is constructed on BERT-Base) and also introduces external knowledge. This indicates that the augmented data generated by our LearnDA can effectively alleviate the problem of data lacking on the ECI."
    }, {
      "heading" : "4.3 Effect of Learnable Dual Augmentation",
      "text" : "We analyze the effect of the learnable dual augmentation for event causality identification. 1) For identifier. Comparing LearnDADual with BERT in Table 3, we note that the performance of the proposed identifier is improved (+2.6%) after the dual training only with the same labeled data. This indicates that the identifier can learn more informative expressions of causal semantic from generation with dual learning. 2) For generator. Comparing BERTDualAug with BERTAug in Table 3, we note that the dual augmented data is high quality and more helpful to ECI (+2.6%). This indicates generator can generate more ECI task-related data learned from identifier with dual learning.\nFigure 5 illustrates the learnability of our LearnDA. Specifically, as the number of training rounds of dual learning increases, the generated data gradually learns task-related information, fur-\nther improving the performance accordingly."
    }, {
      "heading" : "4.4 Effect of Knowledge Guiding",
      "text" : "Table 3 also illustrates the effect of knowledge guiding on ECI depending on different knowledge bases. 1) Comparing LearnDAFull with LearnDADualAug−w/o.KB , we note that the augmented data guided by external knowledge can further improve the performance of ECI. 2) Specifically, lexical expanding and connective introducing (Sec 3.2) can both make the representation of causal relation more generalized, further making it easier for the identifier to understand the causality. 3) Moreover, the expanding is more effective than the introducing, because the former brings a wider range of effective knowledge, thus the guidance of\ncausal-related knowledge is better."
    }, {
      "heading" : "4.5 Our Augmentation vs. Other NLP Augmentations",
      "text" : "In this section, we conduct a comparison between our augmentation framework and other NLPrelated augmentation methods to further illustrate the effectiveness of LearnDA.\nEffectiveness of Our Augmentation We train our identifier with augmented data produced by different NLP-related augmentation methods. As shown in Table 4, the augmented data generated by our LearnDA is more efficient for ECI, which is consistent with the previous analysis. The LearnDA can generate well-formed task-related new sentences that contain more event causal knowledge. Specifically, 1) text surface transformation brings a slight change to the labeled data, thus it has relatively little impact on ECI; 2) Back translation introduces limited new causal expressions by translation, thus it slightly increases the recall value on ECI; 3) EDA can introduce new expressions via substitution, but the augmented data is not canonical and cannot accurately express the causality, therefore, its impact on ECI is also limited."
    }, {
      "heading" : "Quantitative Evaluation of Task-relevance",
      "text" : "We select five Ph.D. students majoring in NLP to manual score the 100 randomly selected augmented sentences given their corresponding original sentences as reference (Cohen’s kappa = 0.85). Furthermore, we calculate the BLEU (Papineni et al., 2002) value to further evaluate the\ndiversity. As aforementioned, the task-relevance of new sentences on ECI is manifested in causality and well-formedness, while the diversity indicates the degree of generalization. As shown in Table 5, we note the sentences generated by LearnDA are equipped with the above three properties that are close to the labeled sentences. Specifically, the sentences produced by EDA has a certain degree of causality and diversity due to the lexical substitution assisted by external knowledge. However, they cannot well express the causality due to the grammatical irregularities. Correspondingly, new sentences generated via back translation are very similar to the original sentences, while the diversity is poor."
    }, {
      "heading" : "4.6 Case Study",
      "text" : "We conduct a case study to further investigate the effectiveness of our LearnDA. Figure 6 illustrates the modification process of dual learning. For example as a), given two causal events, the generator is expected to generate a causal sentence. However, the generator without dual learning produces a noncausal sentence. Fortunately, with dual learning, the identifier judges the generated sentence as a non-causal one and guides the generator to produce a causal sentence with the feedback. Similarly, as shown in b), given a causal sentence, the identifier is expected to output a causal relation, but no dual-trained one cannot do. Correspondingly, the generator constructs feedback of low confidence to guide the identifier to output a causal relation."
    }, {
      "heading" : "5 Conclusion",
      "text" : "This paper proposes a new learnable knowledgeguided data augmentation framework (LearnDA) to solve the data lacking problem on ECI. Our framework can leverage the duality between generation and identification via dual learning to gener-\nate task-related sentences for ECI. Moreover, our framework is knowledge guided and learnable. Our method achieves state-of-the-art performance on EventStoryLine and Causal-TimeBank datasets."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank anonymous reviewers for their insightful comments and suggestions. This work is supported by the National Key Research and Development Program of China (No.2018YFB1005100), the National Natural Science Foundation of China (No.U1936207, 61806201). This work is also supported by Beijing Academy of Artificial Intelligence (BAAI2019QN0301) and the joint project with Beijing Baidu Netcom Science Technology Co., Ltd."
    }, {
      "heading" : "A Supplementary Experiment Results",
      "text" : ""
    }, {
      "heading" : "A.1 Statistics of Dual Augmented Data",
      "text" : "As shown in Table 6, our dual augmented data is significantly more quantitative than the labeled data. Specifically, the causal event pairs are increased by 3.1 times, the causal sentences are increased by 5.9 times and the average number of causal sentences corresponding to each causal event pair is also increased."
    }, {
      "heading" : "A.2 Effectiveness of Different Quantities of Augmented Training Data",
      "text" : "We change the quantity of dual augmented data for training to explore the influence of augmentation ratio on ECI. As shown in Table 7, when the ratio is 1:2, the effective knowledge brought by dual augmented data is maximized. And as the ratio increasing, the dual augmented data will bring noises, which obstructs the model to identify event causality and may change the data distribution from original data (Xie et al., 2019a). This suggests that too much augmented data is not better and that there is a trade-off between introducing knowledge and reducing noise."
    }, {
      "heading" : "A.3 Effectiveness of Extracting Event Pairs with Different Filtering Ratios",
      "text" : "Table 8 tries to show the effectiveness of extracting event pairs with different filtering ratios on ECI. With the ratio of retained event pairs increasing,\nthe augmented data hurts ECI’s performance. This proves the effectiveness of filtering, which further improves the causality of the generated sentences.\nA.4 Effectiveness of Generated sentences with Different Filtering Ratios\nTable 9 tries to show the effectiveness of generated sentences with different filtering ratios. With the ratio of retained generated sentences increasing, the contribution of filtered generated sentences for ECI decreases gradually. This proves the effectiveness of filtering, which can balance the overall quality of the sentences against diversity."
    }, {
      "heading" : "B Supplementary Related Work",
      "text" : ""
    }, {
      "heading" : "B.1 Dual Learning",
      "text" : "For many Natural Language Processing (NLP) tasks, there exist many primal and dual tasks, such as open information narration (OIN) and open information extraction (OIE) (Sun et al., 2018), natural language understanding (NLU) and natural language generation (NLG) (Su et al., 2019, 2020), semantic parsing and natural language generation (Ye et al., 2019; Cao et al., 2019, 2020), link prediction and entailment graph induction (Cao et al., 2019), query-to-response and response-to-query generation (Shen and Feng, 2020) and so on. The duality between the primal task and the dual task is considered as a constraint that both problems must share the same joint probability mutually. Recently, inspired by Xia et al. (2017) who implemented the duality in a neural-based dual learning system, the above primal-dual tasks are implemented in two different ways: 1) providing additional labeled samples via bootstrapping, and 2) adding rewards at\nthe training stage for each agent. We observe that the event causality identification and the sentence generation are dual to each other. Therefore, we apply a dual learning framework in the second way to optimize identification and generation interactively for generating ECI-related data."
    }, {
      "heading" : "B.2 Data Augmentation for NLP",
      "text" : "The scarcity of annotated data is a thorny problem in machine learning. Unlike computer vision, the augmentation of text data in NLP is pretty rare. Existing text data augmentation methods for NLP tasks are almost task-independent frameworks and can be roughly summarized into the following categories (Chaudhary, 2020): (1) Lexical substitution tries to substitute words without changing the meaning (Zhang et al., 2015; Wei and Zou, 2019; Wang and Yang, 2015; Xie et al., 2019b); (2) Back translation tries to paraphrase a text while retraining the meaning (Xie et al., 2019b); (3) Text surface transformation tries to match transformations using regex (Coulombe, 2018); (4) Random noise injection tries to inject noise in the text to make the model more robust (Wei and Zou, 2019); (5) Generative method tries to generate additional training data while preserving the class label (Anaby-Tavor et al., 2019; Yang et al., 2019); (6) Distantly supervision and self-supervision try to introduce new training data from unlabeled text (Chen et al., 2017; Ruiter et al., 2019). As aforementioned, these frameworks cannot directly produce new suitable task-related examples for ECI. However, (1), (3), and (4) cannot guarantee the causality and wellformedness of new examples for ECI. Additionally, (2) and (5) are not easy to directly use external knowledge bases to generalize the event-related causal commonsense. Furthermore, (6) needs to design proprietary processing methods to generate ECI task-related training data. Zuo et al. (2020) solved the data lacking problem of ECI with the distantly supervised labeled training data. However, including the distant supervision, most of the existing text data augmentation methods for NLP tasks are task-independent frameworks. Therefore, we introduce a new learnable framework for augmenting task-related training data for ECI via dual learning enhanced with external knowledge."
    }, {
      "heading" : "C Generation with ConceptNet",
      "text" : "To make a fair comparison, we introduce causalrelated events from ConceptNet based on causal-\nrelated concepts, and obtain the causal sentence via the method in KonwDis (Zuo et al., 2020) to further re-train MM (Liu et al., 2020). Specifically, firstly, we obtain triples based on cause-related semantic relations from ConceptNet, such as Causes, HasSubevent, HasFirstSubevent, HasLastSubevent, MotivatedByGoal, and CausesDesire relations. Secondly, we assemble any two events from obtained causal triples to generate causal event pairs set and filter them via the filter of KonwDis. Next, we employ filtered causal event pairs to collect preliminary noisy labeled sentences from external documents via the DistantAnnotator of KonwDis. Then, we use the CommonFilter of KnowDis assisted with causal commonsense knowledge to pick out labeled sentences that express causal semantics between events. Finally, the refined causal sentences are input into LearnDA to generated ECIrelated dual augmented training data and further train the MM to obtain MM+ConceptAug."
    }, {
      "heading" : "D Main Experimental Environments and Other Parameters Settings",
      "text" : ""
    }, {
      "heading" : "D.1 Experimental Environments",
      "text" : "We deploy all models on a server with 250GB of memory and 4 TITAN Xp GPUs. Specifically, the configuration environment of the server is ubuntu 16.04, and our framework mainly depends on python 3.6.0 and PyTorch 1.0."
    }, {
      "heading" : "D.2 Other Parameters Settings",
      "text" : "All the final hyper-parameters for evaluation are averaged after 3 independent tunings on the development set. Moreover, the whole dual learning framework which includes event causality identifier and knowledge guided sentence generator takes approximately 5 minutes per epoch when training. According to the early stop strategy, the training rounds for different folds are different, and it takes about 20-30 rounds."
    } ],
    "references" : [ {
      "title" : "Not enough data? deep learning to the rescue! ArXiv, abs/1911.03118",
      "author" : [ "Ateret Anaby-Tavor", "Boaz Carmeli", "Esther Goldbraich", "Amir Kantor", "George Kour", "Segev Shlomov", "Naama Tepper", "Naama Zwerdling" ],
      "venue" : null,
      "citeRegEx" : "Anaby.Tavor et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Anaby.Tavor et al\\.",
      "year" : 2019
    }, {
      "title" : "The Berkeley FrameNet project",
      "author" : [ "Collin F. Baker", "Charles J. Fillmore", "John B. Lowe." ],
      "venue" : "36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1, pages 86–90,",
      "citeRegEx" : "Baker et al\\.,? 1998",
      "shortCiteRegEx" : "Baker et al\\.",
      "year" : 1998
    }, {
      "title" : "Using a bigram event model to predict causal potential",
      "author" : [ "Brandon Beamer", "Roxana Girju." ],
      "venue" : "International Conference on Intelligent Text Processing and Computational Linguistics, pages 430–441. Springer.",
      "citeRegEx" : "Beamer and Girju.,? 2009",
      "shortCiteRegEx" : "Beamer and Girju.",
      "year" : 2009
    }, {
      "title" : "Translating embeddings for modeling multirelational data",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Alberto GarciaDuran", "Jason Weston", "Oksana Yakhnenko." ],
      "venue" : "Advances in neural information processing systems, pages 2787–2795.",
      "citeRegEx" : "Bordes et al\\.,? 2013",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2013
    }, {
      "title" : "Semantic parsing with dual learning",
      "author" : [ "Ruisheng Cao", "Su Zhu", "Chen Liu", "Jieyu Li", "Kai Yu." ],
      "venue" : "pages 51–64.",
      "citeRegEx" : "Cao et al\\.,? 2019",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised dual paraphrasing for two-stage semantic parsing",
      "author" : [ "Ruisheng Cao", "Su Zhu", "Chenyu Yang", "Chen Liu", "Rao Ma", "Yanbin Zhao", "Lu Chen", "Kai Yu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Cao et al\\.,? 2020",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2020
    }, {
      "title" : "The event StoryLine corpus: A new benchmark for causal and temporal relation extraction",
      "author" : [ "Tommaso Caselli", "Piek Vossen." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Caselli and Vossen.,? 2017",
      "shortCiteRegEx" : "Caselli and Vossen.",
      "year" : 2017
    }, {
      "title" : "A visual survey of data augmentation in nlp",
      "author" : [ "Amit Chaudhary" ],
      "venue" : null,
      "citeRegEx" : "Chaudhary.,? \\Q2020\\E",
      "shortCiteRegEx" : "Chaudhary.",
      "year" : 2020
    }, {
      "title" : "Automatically labeled data generation for large scale event extraction",
      "author" : [ "Yubo Chen", "Shulin Liu", "Xiang Zhang", "Kang Liu", "Jun Zhao." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Classifying temporal relations by bidirectional LSTM over dependency paths",
      "author" : [ "Fei Cheng", "Yusuke Miyao." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1–6, Van-",
      "citeRegEx" : "Cheng and Miyao.,? 2017",
      "shortCiteRegEx" : "Cheng and Miyao.",
      "year" : 2017
    }, {
      "title" : "A sequential model for classifying temporal relations between intra-sentence events",
      "author" : [ "Prafulla Kumar Choubey", "Ruihong Huang." ],
      "venue" : "pages 1796–1802.",
      "citeRegEx" : "Choubey and Huang.,? 2017",
      "shortCiteRegEx" : "Choubey and Huang.",
      "year" : 2017
    }, {
      "title" : "Text data augmentation made simple by leveraging nlp cloud apis",
      "author" : [ "Claude Coulombe." ],
      "venue" : "ArXiv, abs/1812.04718.",
      "citeRegEx" : "Coulombe.,? 2018",
      "shortCiteRegEx" : "Coulombe.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Minimally supervised event causality identification",
      "author" : [ "Quang Do", "Yee Seng Chan", "Dan Roth." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 294–303, Edinburgh, Scotland, UK. Association for",
      "citeRegEx" : "Do et al\\.,? 2011",
      "shortCiteRegEx" : "Do et al\\.",
      "year" : 2011
    }, {
      "title" : "Annotating causal language using corpus lexicography of constructions",
      "author" : [ "Jesse Dunietz", "Lori Levin", "Jaime Carbonell." ],
      "venue" : "Proceedings of The 9th Linguistic Annotation Workshop, pages 188–196, Denver, Colorado, USA. Association for Computational",
      "citeRegEx" : "Dunietz et al\\.,? 2015",
      "shortCiteRegEx" : "Dunietz et al\\.",
      "year" : 2015
    }, {
      "title" : "The BECauSE corpus 2.0: Annotating causality and overlapping relations",
      "author" : [ "Jesse Dunietz", "Lori Levin", "Jaime Carbonell" ],
      "venue" : "In Proceedings of the 11th Linguistic Annotation Workshop,",
      "citeRegEx" : "Dunietz et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Dunietz et al\\.",
      "year" : 2017
    }, {
      "title" : "Modeling document-level causal structures for event causal relation identification",
      "author" : [ "Lei Gao", "Prafulla Kumar Choubey", "Ruihong Huang." ],
      "venue" : "Proceedings of the 2019 Conference of the North",
      "citeRegEx" : "Gao et al\\.,? 2019",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatic detection of causal relations for question answering",
      "author" : [ "Roxana Girju." ],
      "venue" : "Proceedings of the ACL 2003 Workshop on Multilingual Summarization and Question Answering, pages 76–83, Sapporo, Japan. Association for Computational Linguis-",
      "citeRegEx" : "Girju.,? 2003",
      "shortCiteRegEx" : "Girju.",
      "year" : 2003
    }, {
      "title" : "Augmenting data with mixup for sentence classification: An empirical study",
      "author" : [ "Hongyu Guo", "Yongyi Mao", "Richong Zhang." ],
      "venue" : "ArXiv, abs/1905.08941.",
      "citeRegEx" : "Guo et al\\.,? 2019",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2019
    }, {
      "title" : "Toward future scenario generation: Extracting event causality exploiting semantic relation, context, and association features",
      "author" : [ "Chikara Hashimoto", "Kentaro Torisawa", "Julien Kloetzer", "Motoki Sano", "István Varga", "Jong-Hoon Oh", "Yutaka Kidawara." ],
      "venue" : "Pro-",
      "citeRegEx" : "Hashimoto et al\\.,? 2014",
      "shortCiteRegEx" : "Hashimoto et al\\.",
      "year" : 2014
    }, {
      "title" : "Identifying causal relations using parallel Wikipedia articles",
      "author" : [ "Christopher Hidey", "Kathy McKeown." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1424–1433, Berlin, Ger-",
      "citeRegEx" : "Hidey and McKeown.,? 2016",
      "shortCiteRegEx" : "Hidey and McKeown.",
      "year" : 2016
    }, {
      "title" : "Inference of fine-grained event causality from blogs and films",
      "author" : [ "Zhichao Hu", "Elahe Rahimtoroghi", "Marilyn Walker." ],
      "venue" : "pages 52–58.",
      "citeRegEx" : "Hu et al\\.,? 2017",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2017
    }, {
      "title" : "Inferring narrative causality between event pairs in films",
      "author" : [ "Zhichao Hu", "Marilyn Walker." ],
      "venue" : "pages 342–351.",
      "citeRegEx" : "Hu and Walker.,? 2017",
      "shortCiteRegEx" : "Hu and Walker.",
      "year" : 2017
    }, {
      "title" : "Knowledge enhanced event causality identification with mention masking generalizations",
      "author" : [ "Jian Liu", "Yubo Chen", "Jun Zhao." ],
      "venue" : "IJCAI-20, pages 3608– 3614. International Joint Conferences on Artificial Intelligence Organization. Main track.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Wordnet: a lexical database for english",
      "author" : [ "George A Miller." ],
      "venue" : "Communications of the ACM, 38(11):39–",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "Extracting temporal and causal relations between events",
      "author" : [ "Paramita Mirza." ],
      "venue" : "pages 10–17.",
      "citeRegEx" : "Mirza.,? 2014",
      "shortCiteRegEx" : "Mirza.",
      "year" : 2014
    }, {
      "title" : "Annotating causality in the TempEval-3 corpus",
      "author" : [ "Paramita Mirza", "Rachele Sprugnoli", "Sara Tonelli", "Manuela Speranza." ],
      "venue" : "Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language (CAtoCL), pages",
      "citeRegEx" : "Mirza et al\\.,? 2014",
      "shortCiteRegEx" : "Mirza et al\\.",
      "year" : 2014
    }, {
      "title" : "An analysis of causality between events and its relation to temporal information",
      "author" : [ "Paramita Mirza", "Sara Tonelli." ],
      "venue" : "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2097–",
      "citeRegEx" : "Mirza and Tonelli.,? 2014",
      "shortCiteRegEx" : "Mirza and Tonelli.",
      "year" : 2014
    }, {
      "title" : "CATENA: CAusal and TEmporal relation extraction from NAtural language texts",
      "author" : [ "Paramita Mirza", "Sara Tonelli." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 64–75,",
      "citeRegEx" : "Mirza and Tonelli.,? 2016",
      "shortCiteRegEx" : "Mirza and Tonelli.",
      "year" : 2016
    }, {
      "title" : "The COLING 2016 Organizing Committee",
      "author" : [ "Osaka", "Japan" ],
      "venue" : null,
      "citeRegEx" : "Osaka and Japan.,? \\Q2016\\E",
      "shortCiteRegEx" : "Osaka and Japan.",
      "year" : 2016
    }, {
      "title" : "CaTeRS: Causal and temporal relation scheme for semantic annotation of event structures",
      "author" : [ "Nasrin Mostafazadeh", "Alyson Grealish", "Nathanael Chambers", "James Allen", "Lucy Vanderwende." ],
      "venue" : "Proceedings of the Fourth Workshop on Events,",
      "citeRegEx" : "Mostafazadeh et al\\.,? 2016",
      "shortCiteRegEx" : "Mostafazadeh et al\\.",
      "year" : 2016
    }, {
      "title" : "Why-question answering using intra- and inter-sentential causal relations",
      "author" : [ "Jong-Hoon Oh", "Kentaro Torisawa", "Chikara Hashimoto", "Motoki Sano", "Stijn De Saeger", "Kiyonori Ohtake." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Com-",
      "citeRegEx" : "Oh et al\\.,? 2013",
      "shortCiteRegEx" : "Oh et al\\.",
      "year" : 2013
    }, {
      "title" : "Multi-column convolutional neural networks with causality-attention for why-question answering",
      "author" : [ "Jong-Hoon Oh", "Kentaro Torisawa", "Canasai Kruengkrai", "Ryu Iida", "Julien Kloetzer." ],
      "venue" : "Proceedings of the Tenth ACM International Confer-",
      "citeRegEx" : "Oh et al\\.,? 2017",
      "shortCiteRegEx" : "Oh et al\\.",
      "year" : 2017
    }, {
      "title" : "Dare: Data augmented relation extraction with gpt-2",
      "author" : [ "Yannis Papanikolaou", "A. Pierleoni." ],
      "venue" : "ArXiv, abs/2004.13845.",
      "citeRegEx" : "Papanikolaou and Pierleoni.,? 2020",
      "shortCiteRegEx" : "Papanikolaou and Pierleoni.",
      "year" : 2020
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Another look at causality: Discovering scenario-specific contingency relationships with no supervision",
      "author" : [ "Mehwish Riaz", "Roxana Girju." ],
      "venue" : "2010 IEEE Fourth International Conference on Semantic Computing, pages 361–368. IEEE.",
      "citeRegEx" : "Riaz and Girju.,? 2010",
      "shortCiteRegEx" : "Riaz and Girju.",
      "year" : 2010
    }, {
      "title" : "Toward a better understanding of causality between verbal events: Extraction and analysis of the causal power of verbverb associations",
      "author" : [ "Mehwish Riaz", "Roxana Girju." ],
      "venue" : "Proceedings of the SIGDIAL 2013 Conference, pages 21–30, Metz, France. Asso-",
      "citeRegEx" : "Riaz and Girju.,? 2013",
      "shortCiteRegEx" : "Riaz and Girju.",
      "year" : 2013
    }, {
      "title" : "In-depth exploitation of noun and verb semantics to identify causation in verb-noun pairs",
      "author" : [ "Mehwish Riaz", "Roxana Girju." ],
      "venue" : "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 161–",
      "citeRegEx" : "Riaz and Girju.,? 2014a",
      "shortCiteRegEx" : "Riaz and Girju.",
      "year" : 2014
    }, {
      "title" : "Recognizing causality in verb-noun pairs via noun and verb semantics",
      "author" : [ "Mehwish Riaz", "Roxana Girju." ],
      "venue" : "Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language (CAtoCL), pages 48–57, Gothenburg,",
      "citeRegEx" : "Riaz and Girju.,? 2014b",
      "shortCiteRegEx" : "Riaz and Girju.",
      "year" : 2014
    }, {
      "title" : "Self-supervised neural machine translation",
      "author" : [ "Dana Ruiter", "Cristina España-Bonet", "Josef van Genabith." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1828–1834, Florence, Italy. Association for",
      "citeRegEx" : "Ruiter et al\\.,? 2019",
      "shortCiteRegEx" : "Ruiter et al\\.",
      "year" : 2019
    }, {
      "title" : "Verbnet: A broadcoverage, comprehensive verb lexicon",
      "author" : [ "Karin Kipper Schuler" ],
      "venue" : null,
      "citeRegEx" : "Schuler.,? \\Q2005\\E",
      "shortCiteRegEx" : "Schuler.",
      "year" : 2005
    }, {
      "title" : "CDL: Curriculum dual learning for emotion-controllable response generation",
      "author" : [ "Lei Shen", "Yang Feng." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 556–566, Online. Association for Com-",
      "citeRegEx" : "Shen and Feng.,? 2020",
      "shortCiteRegEx" : "Shen and Feng.",
      "year" : 2020
    }, {
      "title" : "Dual supervised learning for natural language understanding and generation",
      "author" : [ "Shang-Yu Su", "Chao-Wei Huang", "Yun-Nung Chen." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5472–5477, Florence,",
      "citeRegEx" : "Su et al\\.,? 2019",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards unsupervised language understanding and generation by joint dual learning",
      "author" : [ "Shang-Yu Su", "Chao-Wei Huang", "Yun-Nung Chen." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 671–680,",
      "citeRegEx" : "Su et al\\.,? 2020",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2020
    }, {
      "title" : "Logician and orator: Learning from the duality between language and knowledge in open domain",
      "author" : [ "Mingming Sun", "Xu Li", "Ping Li." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium.",
      "citeRegEx" : "Sun et al\\.,? 2018",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2018
    }, {
      "title" : "That’s so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using #petpeeve tweets",
      "author" : [ "William Yang Wang", "Diyi Yang." ],
      "venue" : "Proceedings of the 2015 Conference on",
      "citeRegEx" : "Wang and Yang.,? 2015",
      "shortCiteRegEx" : "Wang and Yang.",
      "year" : 2015
    }, {
      "title" : "EDA: Easy data augmentation techniques for boosting performance on text classification tasks",
      "author" : [ "Jason Wei", "Kai Zou." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Wei and Zou.,? 2019",
      "shortCiteRegEx" : "Wei and Zou.",
      "year" : 2019
    }, {
      "title" : "Dual supervised learning",
      "author" : [ "Yingce Xia", "Tao Qin", "Wei Chen", "Jiang Bian", "Nenghai Yu", "Tie-Yan Liu." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 3789– 3798. JMLR. org.",
      "citeRegEx" : "Xia et al\\.,? 2017",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2017
    }, {
      "title" : "Adversarial examples improve image recognition",
      "author" : [ "Cihang Xie", "Mingxing Tan", "Boqing Gong", "Jiang Wang", "Alan L. Yuille", "Quoc V. Le." ],
      "venue" : "ArXiv, abs/1911.09665.",
      "citeRegEx" : "Xie et al\\.,? 2019a",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised data augmentation for consistency training",
      "author" : [ "Qizhe Xie", "Zihang Dai", "Eduard H. Hovy", "Minh-Thang Luong", "Quoc V. Le." ],
      "venue" : "arXiv: Learning.",
      "citeRegEx" : "Xie et al\\.,? 2019b",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring pre-trained language models for event extraction and generation",
      "author" : [ "Sen Yang", "Dawei Feng", "Linbo Qiao", "Zhigang Kan", "Dongsheng Li." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Jointly learning semantic parser and natural language generator via dual information maximization",
      "author" : [ "Hai Ye", "Wenjie Li", "Lu Wang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2090–2101, Flo-",
      "citeRegEx" : "Ye et al\\.,? 2019",
      "shortCiteRegEx" : "Ye et al\\.",
      "year" : 2019
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Jake Zhao", "Yann LeCun." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "KnowDis: Knowledge enhanced data augmentation for event causality detection via distant supervision",
      "author" : [ "Xinyu Zuo", "Yubo Chen", "Kang Liu", "Jun Zhao." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 1544–1550,",
      "citeRegEx" : "Zuo et al\\.,? 2020",
      "shortCiteRegEx" : "Zuo et al\\.",
      "year" : 2020
    }, {
      "title" : "2019b); (2) Back translation tries to paraphrase a text while retraining the meaning (Xie et al., 2019b); (3) Text surface transformation tries to match transformations",
      "author" : [ "Wei", "Zou", "Wang", "Yang", "Xie" ],
      "venue" : null,
      "citeRegEx" : "Wei et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "Event causality identification (ECI) aims to identify causal relations between events in texts, which can provide crucial clues for NLP tasks, such as logical reasoning and question answering (Girju, 2003; Oh et al., 2013, 2017).",
      "startOffset" : 192,
      "endOffset" : 228
    }, {
      "referenceID" : 6,
      "context" : "According to our statistics, the largest widely used dataset EventStoryLine Corpus (Caselli and Vossen, 2017) only contains 258 documents, 4316 sentences, and 1770 causal event pairs.",
      "startOffset" : 83,
      "endOffset" : 109
    }, {
      "referenceID" : 52,
      "context" : "However, most of the NLP-related augmentation methods are a task-independent framework that produces new data at one time (Zhang et al., 2015; Guo et al., 2019; Xie et al., 2019b).",
      "startOffset" : 122,
      "endOffset" : 179
    }, {
      "referenceID" : 18,
      "context" : "However, most of the NLP-related augmentation methods are a task-independent framework that produces new data at one time (Zhang et al., 2015; Guo et al., 2019; Xie et al., 2019b).",
      "startOffset" : 122,
      "endOffset" : 179
    }, {
      "referenceID" : 49,
      "context" : "However, most of the NLP-related augmentation methods are a task-independent framework that produces new data at one time (Zhang et al., 2015; Guo et al., 2019; Xie et al., 2019b).",
      "startOffset" : 122,
      "endOffset" : 179
    }, {
      "referenceID" : 46,
      "context" : "For example, easy data augmentation (EDA) (Wei and Zou, 2019) is the most representative method that relies on lexical substitution, deletion, swapping, and insertion to produce new data.",
      "startOffset" : 42,
      "endOffset" : 61
    }, {
      "referenceID" : 19,
      "context" : "Some focus on explicit causal textual patterns (Hashimoto et al., 2014; Riaz and Girju, 2014a, 2010; Do et al., 2011; Hidey and McKeown, 2016).",
      "startOffset" : 47,
      "endOffset" : 142
    }, {
      "referenceID" : 13,
      "context" : "Some focus on explicit causal textual patterns (Hashimoto et al., 2014; Riaz and Girju, 2014a, 2010; Do et al., 2011; Hidey and McKeown, 2016).",
      "startOffset" : 47,
      "endOffset" : 142
    }, {
      "referenceID" : 20,
      "context" : "Some focus on explicit causal textual patterns (Hashimoto et al., 2014; Riaz and Girju, 2014a, 2010; Do et al., 2011; Hidey and McKeown, 2016).",
      "startOffset" : 47,
      "endOffset" : 142
    }, {
      "referenceID" : 2,
      "context" : "And some others pay attention on statistical causal association and cues (Beamer and Girju, 2009; Hu et al., 2017; Hu and Walker, 2017).",
      "startOffset" : 73,
      "endOffset" : 135
    }, {
      "referenceID" : 21,
      "context" : "And some others pay attention on statistical causal association and cues (Beamer and Girju, 2009; Hu et al., 2017; Hu and Walker, 2017).",
      "startOffset" : 73,
      "endOffset" : 135
    }, {
      "referenceID" : 22,
      "context" : "And some others pay attention on statistical causal association and cues (Beamer and Girju, 2009; Hu et al., 2017; Hu and Walker, 2017).",
      "startOffset" : 73,
      "endOffset" : 135
    }, {
      "referenceID" : 14,
      "context" : "CauSE corpus (Dunietz et al., 2015) of causal relation and other seven relations.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 7,
      "context" : "Unlike computer vision, the augmentation of text data in NLP is pretty rare (Chaudhary, 2020).",
      "startOffset" : 76,
      "endOffset" : 93
    }, {
      "referenceID" : 50,
      "context" : "tive methods which try to generate additional training data while preserving the class label (AnabyTavor et al., 2019; Yang et al., 2019; Papanikolaou and Pierleoni, 2020), we introduce a new learnable framework for augmenting task-related training data for ECI via dual learning enhanced with external knowledge.",
      "startOffset" : 93,
      "endOffset" : 171
    }, {
      "referenceID" : 33,
      "context" : "tive methods which try to generate additional training data while preserving the class label (AnabyTavor et al., 2019; Yang et al., 2019; Papanikolaou and Pierleoni, 2020), we introduce a new learnable framework for augmenting task-related training data for ECI via dual learning enhanced with external knowledge.",
      "startOffset" : 93,
      "endOffset" : 171
    }, {
      "referenceID" : 24,
      "context" : "tated event pairs via external dictionaries, such as WordNet (Miller, 1995) and VerbNet (Schuler, 2005).",
      "startOffset" : 61,
      "endOffset" : 75
    }, {
      "referenceID" : 40,
      "context" : "tated event pairs via external dictionaries, such as WordNet (Miller, 1995) and VerbNet (Schuler, 2005).",
      "startOffset" : 88,
      "endOffset" : 103
    }, {
      "referenceID" : 1,
      "context" : "(2) Connective knowledge introducing: introducing event pairs from external event-annotated documents (KBP corpus) assisted with FrameNet (Baker et al., 1998) and Penn Discourse Treebank",
      "startOffset" : 138,
      "endOffset" : 158
    }, {
      "referenceID" : 12,
      "context" : "Therefore, we complete sentences by filling the cohesive words between given events and assigned entities with masked BERT (Devlin et al., 2019).",
      "startOffset" : 123,
      "endOffset" : 144
    }, {
      "referenceID" : 12,
      "context" : "Specifically, we design a classifier based on BERT (Devlin et al., 2019) to build our identifier.",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 6,
      "context" : "9 (ESC) (Caselli and Vossen, 2017) described above; and (2) Causal-TimeBank (Causal-TB) (Mirza and Tonelli, 2014) which contains 184 documents, 6813",
      "startOffset" : 8,
      "endOffset" : 34
    }, {
      "referenceID" : 27,
      "context" : "9 (ESC) (Caselli and Vossen, 2017) described above; and (2) Causal-TimeBank (Causal-TB) (Mirza and Tonelli, 2014) which contains 184 documents, 6813",
      "startOffset" : 88,
      "endOffset" : 113
    }, {
      "referenceID" : 9,
      "context" : "For ESC, we prefer 1) LSTM (Cheng and Miyao, 2017), a dependency path based",
      "startOffset" : 27,
      "endOffset" : 50
    }, {
      "referenceID" : 10,
      "context" : "3564 sequential model that models the context between events to identify causality; 2) Seq (Choubey and Huang, 2017), a sequence model explores complex human designed features for ECI; 3) LR+ and ILP (Gao et al.",
      "startOffset" : 91,
      "endOffset" : 116
    }, {
      "referenceID" : 16,
      "context" : "3564 sequential model that models the context between events to identify causality; 2) Seq (Choubey and Huang, 2017), a sequence model explores complex human designed features for ECI; 3) LR+ and ILP (Gao et al., 2019), document-level models adopt document structures for ECI.",
      "startOffset" : 200,
      "endOffset" : 218
    }, {
      "referenceID" : 23,
      "context" : "2) MM (Liu et al., 2020), the BERTbased SOTA method with mention masking generalization.",
      "startOffset" : 6,
      "endOffset" : 24
    }, {
      "referenceID" : 53,
      "context" : "4) KnowDis (Zuo et al., 2020) improved the performance of ECI",
      "startOffset" : 11,
      "endOffset" : 29
    }, {
      "referenceID" : 9,
      "context" : "3) Comparing MM+ConceptNet with MM, with the same knowledge base, our dual augmented data can further improve the performance Methods P R F1 ESC LSTM (Cheng and Miyao, 2017) 34.",
      "startOffset" : 150,
      "endOffset" : 173
    }, {
      "referenceID" : 34,
      "context" : "Furthermore, we calculate the BLEU (Papineni et al., 2002) value to further evaluate the",
      "startOffset" : 35,
      "endOffset" : 58
    } ],
    "year" : 2021,
    "abstractText" : "Modern models for event causality identification (ECI) are mainly based on supervised learning, which are prone to the data lacking problem. Unfortunately, the existing NLPrelated augmentation methods cannot directly produce available data required for this task. To solve the data lacking problem, we introduce a new approach to augment training data for event causality identification, by iteratively generating new examples and classifying event causality in a dual learning framework. On the one hand, our approach is knowledge guided, which can leverage existing knowledge bases to generate well-formed new sentences. On the other hand, our approach employs a dual mechanism, which is a learnable augmentation framework, and can interactively adjust the generation process to generate task-related sentences. Experimental results on two benchmarks EventStoryLine and Causal-TimeBank show that 1) our method can augment suitable task-related training data for ECI; 2) our method outperforms previous methods on EventStoryLine and Causal-TimeBank (+2.5 and +2.1 points on F1 value respectively).",
    "creator" : "LaTeX with hyperref"
  }
}