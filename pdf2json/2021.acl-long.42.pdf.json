{
  "name" : "2021.acl-long.42.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning",
    "authors" : [ "Haiyang Xu", "Ming Yan", "Chenliang Li", "Bin Bi", "Songfang Huang", "Wenming Xiao", "Fei Huang" ],
    "emails" : [ "b.bi}@alibaba-inc.com", "f.huang}@alibaba-inc.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 503–513\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n503\nIn this paper, we propose the first end-to-end vision-language pre-trained model for both V+L understanding and generation, namely E2E-VLP, where we build a unified Transformer framework to jointly learn visual representation, and semantic alignments between image and text. We incorporate the tasks of object detection and image captioning into pretraining with a unified Transformer encoderdecoder architecture for enhancing visual learning. An extensive set of experiments have been conducted on well-established visionlanguage downstream tasks to demonstrate the effectiveness of this novel VLP paradigm."
    }, {
      "heading" : "1 Introduction",
      "text" : "Self-supervised pre-training has achieved great success in a wide range of natural language understanding (Devlin et al., 2018; Liu et al., 2019; Wang et al., 2019; Lan et al., 2019) and generation tasks (Song et al., 2019; Lewis et al., 2019; Bi et al., 2020). Recent studies (Li et al., 2019; Lu et al., 2019; Chen et al., 2019; Tan and Bansal, 2019; Li et al., 2020b; Yu et al., 2020) have also witnessed the progress of self-supervised pretraining on vision-and-language tasks, which learns\n∗corresponding author\ngeneral cross-modal representations from massive image-text pairs, and fine-tunes vision-language pre-training (VLP) models on task-specific data achieving state-of-the-art results on various downstream V+L tasks.\nMost existing mainstream VLP models adopt a two-step training method, which firstly extracts semantic visual features using a pre-trained object detection model, and then combines the derived object-centric representation of the image and text embedding as the input of Transformer (Vaswani et al., 2017) for cross-modal pre-training. Despite the superior performance brought by the large-scale image-text pairs, the two-stage solution suffers from the following weaknesses: 1) the object detection model in the first step is trained on specific visual dataset such as Visual Genome dataset (Krishna et al., 2017), and the visual representation is not optimized towards a more generic cross-modal understanding in the second step. It may suffer from an error propagation problem when the object detection model fails to recognize certain important information. 2) extracting region features with an object detection model is so time-consuming that most state-of-the-art models are directly trained and evaluated on cached visual features. This practice not only imposes unnecessary constraints on model designs, but also confronts the run-time inference inefficiency in the prediction phase.\nRecently, several studies such as (Jiang et al., 2020) have begun to revisit the grid features for cross-modal understanding and found the grid features can also work surprisingly well, while making the model design and training process much simpler. One pioneering work Pixel-BERT (Huang et al., 2020) explores to pre-train with grid features in an end-to-end fashion directly from pixels. It removes all the fine-grained visual pre-training tasks, which proves to be important for V+L pre-training. (Zhang et al., 2021) also demonstrates that visual\nfeatures provided by the object detection model matter significantly in VLP models.\nTo address the limitations, we propose a new endto-end paradigm for pixel-level vision-language pre-training, namely E2E-VLP, by enhancing with fine-grained visual learning. During pre-training, E2E-VLP jointly learns the visual region features and the cross-modal representation in a unified Transformer encoder-decoder architecture directly from image pixels. In addition to the typical pre-training tasks of Masked Language Modeling and Image-Text Matching, we enhance the visionlanguage pre-training with fine-grained visual semantic learning. Specifically, two end-to-end pretraining tasks are further incorporated: 1) Object Detection: inspired from DETR (Carion et al., 2020), we view the object detection as a direct set prediction problem. The cross-modal Transformer encoder and image encoder are joint learnt to fuse the cross-modal data from pixels, while the decoder is used to capture fine-grained visual information via bipartite matching between predicted and ground-truth objects; 2) Image-Text Generation: to better understand the semantics within the image, we also use the paired text to guide the learning of image features. We use the encoder network to represent the image and a left-to-right decoder to generate the caption text. The standard auto-regressive language model objective is used to maximize the data probability. These two tasks can help learn high-quality visual representations (Zhang et al., 2021; Desai and Johnson, 2020). Detection task can learn object-level visual semantics, while the image caption task can capture textaligned visual semantics. These two kinds of visual semantics matter significantly in VLP cross-modal fusion. During fine-tuning, E2E-VLP can be flexibly applied to vision-language understanding tasks with the encoder module, and vision-language generation tasks with the encoder-decoder module.\nWe evaluate E2E-VLP on a variety of representative vision-language tasks, including visual question answering, natural language visual reasoning, cross-modal retrieval and image captioning. With the new end-to-end pre-training paradigm, we can obtain surprising good performance across different V+L tasks and greatly decrease the online inference time with the new one-stage solution.\nWe make the following major contributions in this paper:\n• We propose the first end-to-end vision-language\npre-trained model for both V+L understanding and generation, namely E2E-VLP, which can achieve comparable or superior performance with faster online inference speedup. • E2E-VLP is the first model that incorporates\nfine-grained visual pre-training in an encoderdecoder architecture, which paves a new way for designing advanced vision and language pretraining tasks. • We enhance cross-modal feature fusion by vi-\nsual learning of object detection and image caption, which has empirically shown to be effective for vision-language pre-training."
    }, {
      "heading" : "2 Related Work",
      "text" : "Self-supervised pre-training has substantially advanced the performance across a variety of natural language understanding (Devlin et al., 2018; Liu et al., 2019; Wang et al., 2019; Lan et al., 2019) and text generation tasks (Song et al., 2019; Lewis et al., 2019; Bi et al., 2020). Inspired by language model pre-training, several researchers propose Visionlanguage pre-training(VLP) models on large-scale image-text pairs, which has proved effective for a wide range of vision-language (VL) tasks, such as VQA (Antol et al., 2015), NLVR (Young et al., 2014), Cross-modal Retrieval (Suhr et al., 2018).\nThe current VLP models mainly take two-step training pipeline, which consists of extracting semantic visual features by object detector and training the cross-modal pre-training model to align text and visual features. In this kind of method, there are mainly two broad directions to conduct visionlanguage pre-training. The first line uses a singlestream transformer architecture (Vaswani et al., 2017) to model both image and text representations in a unified semantic space such as VLBERT (Su et al., 2019), UNITER (Chen et al., 2019) and OSCAR (Li et al., 2020b). In contrast, the other line adopts a two-stream Transformer architecture that first encodes the image and text modalities separately, and then fuses the cross-modal representations with another Transformer network, such as LXMERT (Tan and Bansal, 2019) and ERNIEViL (Yu et al., 2020). Besides, SemVLP (Li et al., 2021) is pre-trained iteratively with two prevalent fashions. These methods are directly trained and evaluated on cached visual features, which imposes unnecessary constraints on model designs and makes it hard to enable an end-to-end visionlanguage pre-training. Furthermore, Pixel-BERT\n(Huang et al., 2020) represents the first and only work to pre-train with grid features in an end-toend fashion. However, due to the characteristics of learnt grid features, the end-to-end pre-training is conducted without object-level visual tasks, which is important in aligning the semantics between cross-modal representations.\nIn this paper, we focus on enhancing the end-toend vision-language pre-training with more finegrained visual semantic learning. The object detection task and image caption task are incorporated into the pre-training stage for further improving the fine-grained visual-language understanding and generation abilities."
    }, {
      "heading" : "3 E2E-VLP Pre-training",
      "text" : ""
    }, {
      "heading" : "3.1 Model Architecture",
      "text" : "The architecture of E2E-VLP is shown in Figure 1. Inspired by the recent breakthrough of using Transformer on computer vision tasks such as DETR (Carion et al., 2020) and ViT Transformer (Dosovitskiy et al., 2020), we propose to use a Transformer encoder-decoder framework (Vaswani et al., 2017) for cross-modal learning, and a simple CNN backbone module is used as the image encoder for extracting visual representations from pixels so as to allow for more flexible network design. We jointly train the whole framework in an end-to-end fashion, so as to learn the\ngeneric visual representations and high-level crossmodal alignment simultaneously. Different V+L pre-training tasks are designed to further enhance the cross-modal understanding and generation abilities. Next, we describe each component of this model in detail."
    }, {
      "heading" : "3.1.1 Input Representations",
      "text" : "The input to E2E-VLP is an image and its related text (e.g. caption text). We first introduce the way to represent the text sequence and raw image pixels as input to the Transformer.\nSentence Embeddings Each sentence is first split into a sequence of sub-words {w1, ..., wm} by WordPiece tokenizer. Then, similar to BERT (Devlin et al., 2018), each token wi is assigned three kinds of embeddings: token, segment and position embeddings. The three embeddings are summed and layer-normalized to represent input sentence representations as a sequence of embedding vectors Eemb = {eCLS , e1, ..., em, eSEP }, where [CLS] and [SEP ] are special tokens in BERT.\nImage Representations For image feature representation, the most existing VLP models follow Bottom-Up and Top-Down Attention (Anderson et al., 2018) to extract region features by Faster RCNN (Ren et al., 2015) trained on Visual Genome dataset. The detector extracts region features by first detecting regions under pre-defined categories,\nand then uses the features before the final classifier as the output. These methods are limited to the task-specific visual representation of the specific object detector, which may hinder the generic cross-modal understanding.\nTo improve the generalization of the image representation, we learn from pixels to represent an image instead of using bounding boxes. The pixel features are learned by a CNN visual backbone such as ResNet (He et al., 2016). Starting from the initial image vimg ∈ R3×H0×W0 (with 3 color channels), a conventional CNN backbone generates a lowerresolution activation map fimg ∈ RC×H×W using the typical values as in DETR (Carion et al., 2020): C = 2048 and H = H032 ,W = w0 32 . Then, we take a 1× 1 convolution to reduce the channel dimension of the high-level activation map f from C to a smaller dimension d, creating a new feature map zimg ∈ Rd×H×W . The encoder expects a sequence as input, hence we collapse the spatial dimensions of zimg into one dimension, resulting in a HW ×d feature map Zimg. Since the transformer architecture is permutation-invariant, we supplement the feature maps with fixed positional encodings (Parmar et al., 2018) that are added to the input of each attention layer. Finally, the sequential image representation Zimg = {o1, ..., oHW } can be seen as a HW length of d-dimensional vector."
    }, {
      "heading" : "3.1.2 Cross-modal Encoder Pre-training",
      "text" : "Given the embeddings of the tokens for the sentence {ei}mi=1 and the sequential image representations {oj}nj=1, we adopt the Transformer encoder to learn cross-modal attention between image grid features and language tokens. The encoder is a stacked model with L standard blocks, where the l-th block consists of a multi-head self-attention module and a feed forward network (FFN). To allow a fine-grained feature-level semantic fusion, we directly concatenate the derived image features and text embeddings to construct the input sequence, which is formulated as: {eCLS , e1, ..., em, eSEP , o1, ..., oHW }.\nThe CNN backbone for visual representation learning and the Transformer for cross-modal semantic fusion is combined into a single model, which is end-to-end trainable. In this way, the learnt visual feature representation can be more suitable for the pre-training tasks of generic crossmodal understanding. To facilitate cross-modal understanding, we follow (Tan and Bansal, 2019; Chen et al., 2019; Huang et al., 2020) and conduct\ntwo popular pre-training tasks in encoder side, including Masked Language Modeling (MLM) and Image-Text Matching (ITM).\nMasked Language Modeling The task setup is basically the same as in BERT (Devlin et al., 2018), we randomly mask 15% tokens in the text and the model is asked to predict these masked words with the output text and visual representations. Different from MLM task in BERT that only relies on the surrounding text of textual modality for prediction, the masked words will be predicted with the help of image feature map from visual modality so as to resolve ambiguity.\nImage-Text Matching We randomly sample 50% mismatched image-text pairs and 50% matched pairs, and train an classifier to predict whether an image and a sentence match each other on the representation of token [CLS] in the last encoder layer hLCLS ."
    }, {
      "heading" : "3.1.3 Visual-enhanced Decoder",
      "text" : "Due to that the CNN feature map has no objectlevel semantics, it is difficult to directly align the cross-modal semantics between CNN feature map and the language embeddings. Therefore, we further add a Transformer decoder to help capture the fine-grained semantics of the visual features, where two specific pre-training tasks of object detection and image-caption generation are incorporated.\nThe decoder adopts the standard architecture of the transformer with multi-headed self-attention followed by cross-attention and a feed forward network (FFN). Both tasks share the same attention parameters of decoder, while using different linear head for the two tasks. The object detection task focuses more on understanding the fine-grained object information within image, while image captioning task helps guide the learning of visual features regarding the textual semantics.\nEnhanced by Object Detection Following the one-stage detection model DETR (Carion et al., 2020), we define object detection task as the direct set prediction problem, and use a set-based global loss that forces unique predictions via bipartite matching with the Transformer encoder-decoder architecture.\nLet us denote by y the ground truth set of objects and ŷ = {ŷi}Ni=1. The set-based loss of bipartite matching is to search for a permutation of N ele-\nments σ ∈ LN with the lowest cost:\nσ̂ = argmin σ∈ϕN N∑ i Lmatch(yi, ŷσ(i)) (1)\nwhere Lmatch(yi, ŷσ(i)) is a pair-wise matching cost between ground truth yi and a prediction with index σ(i).\nThe Hungarian algorithm (Stewart et al., 2016) is used to efficiently compute the optimal assignment. Different from the original DETR for single-modal learning, our cross-modal pre-training with object detection differs in two aspects.\nIn encoder side, we combine both the visual representation and language embedding as input and reuse the Transformer encoder for cross-modal fusion. In decoder side, we take the learned positional embeddings as the input to multiple L Transformer decoder layers, and detects theN objects in parallel at each decoder layer. In addition to the tasks of box coordinate regression and class category prediction, we also incorporate an object attribute prediction task for Visual Genome Dataset so as to enhance the learning of fine-grained semantics. The model is trained with a negative log-likelihood loss for attribute, class prediction and a box regression loss defined as follows:\nLv(y, ŷ) = N∑ i=1 [−logp̂σ̂(i)(ai)− logp̂σ̂(i)(ci) +\n+ Lbox(bi, b̂σ̂(i)(i))]\nwhere p̂σ̂(i)(ai), p̂σ̂(i)(ci) is the attribute and class probability, Lbox(bi, b̂σ̂(i)(i)) is a normalized bounding boxes regression loss as in (Carion et al., 2020).\nEnhanced by Image Captioning To guide the learning of visual features in regards to the textual semantics, we use semantically dense captions to learn vision representations with sequence-tosequence (Seq2Seq) image-to-text generation task. The decoder is pre-trained to auto-regressively generate the target text based on the contextual representations from the image encoder. The pretraining loss for the decoder is defined as:\nLdec = − ∑\n(x,y)∈(X ,Y)\nlog n∏ t=1 P (yt|y<t, x) (2)\nwhere X represents the sequence of vision context, Y represents the set of text to be generated and n is the length of tokens in output text y."
    }, {
      "heading" : "3.2 Joint Training",
      "text" : "We pre-train E2E-VLP with all the encoder and decoder pre-training tasks (i.e., Masked Language Modeling, Image-Text Matching, Object Detection, Image-to-Text Generation) jointly by minimizing the four loss functions as:\nL = Lmlm + Litm + Lv + Ldec (3)"
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Pre-training Dataset",
      "text" : "We pre-train our E2E-VLP on two in-domain image-text datasets: MS-COCO (Lin et al., 2014) and Visual Genome (Krishna et al., 2017). We utilize the object detection and image caption annotations in MS-COCO, and object detection, region description annotations in Visual Genome. The total amount of the dataset is 6.01M image-andsentence pairs on 180K distinct images."
    }, {
      "heading" : "4.2 Implementation Details",
      "text" : "The maximum sequence length for the sentence is set as 40. We use scale augmentation, and resize the input images so that the shortest side is at least 480 and at most 800 pixels while the longest is at most 1333 (Carion et al., 2020). For the model architecture, we pre-train E2E-VLP with 6 and 12 layers of Transformer encoder respectively, while the decoder is fixed as 6 layers. Each layer block has 256 hidden units and 12 self-attention heads, the intermediate layer size is 1,024. The visual backbone is selected as ResNet with different sizes (He et al., 2016) from torchvision with frozen batchnorm layers. We pre-train E2E-VLP model with a total batch size of 32 for 200 epoches on 8 V100 GPUs. We use the AdamW optimizor (Loshchilov and Hutter, 2018) for both the Transformer and ResNet. The initial learning rate is set as 10−4 for Transformer and 10−5 for ResNet. The weight decay is set as 10−4."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Downstream Tasks",
      "text" : "We compare E2E-VLP model against other competitive VLP models of the comparable model size on the following downstream V+L tasks. • VQA v2.0 (Antol et al., 2015): The VQA\ntask requires the model to answer natural language questions given an image. We conduct experiments on the widely-used VQA v2.0\ndataset (Antol et al., 2015), which contains 204K images and 1.1M questions about these images. Following (Anderson et al., 2018), we treat VQA as a multi-label classification task by picking an answer from a shared set consisting of 3,129 answers. To fine-tune VQA task, we use a binary cross-entropy loss to train a multilabel classifier, we train with a batch size of 32 for 12 epochs. We set an initial learning rate of 1e-4 which decays by 0.1 at the end of epoch 6 and epoch 9. • NLVR2 (Suhr et al., 2018): NLVR2 (Suhr et al.,\n2018) is a challenging task for visual reasoning. The goal is to determine whether a natural language statement is true about a pair of images. It consists of 86K/7K data for training/development. Since each data example in NLVR2 has two natural images img0, img1 and one language statement s, we concatenate the given sentence and each image to build two sequences, and then train a binary classifier based on the concatenation of the two outputs. We\nfine-tune NLVR model with a batch size of 32 for 12 epochs, and set an initial learning rate of 1e-4 which decays by 0.1 at the end of epoch 6 and epoch 9. • Image Caption: A visual generation task that\nrequires the model to generate the content of an image. To fine-tune Image Caption task, we use the seq2seq loss with label smoothing(Szegedy et al., 2016). During inference, we use beam search (i.e., beam size=4), and set α = 0.9 for the length penalty (Wu et al., 2016). We set initial learning rate of 1e-4 which decays by 0.1 at the end of epoch 6 and epoch 9. We report our results on the COCO image captioning dataset (Chen et al., 2015). • Image-Text Retrieval: The image-text re-\ntrieval task consists of two sub-tasks: image retrieval and text retrieval, depending on which modality is used as the retrieval target. We conduct experiments on Flickr30K dataset (Young et al., 2014), which contains 31,000 images col-\nlected from Flickr website and each image has 5 captions. We follow the same split in (Lee et al., 2018) for training and evaluation. During finetuning, we follow the method in UNITER (Chen et al., 2019) and formulate it as a ranking problem. We use the hidden state of hLCLS to compute the similarity scores for the sampled positive and negative pairs, and maximize the margin between them through circle loss (Sun et al., 2020) as ERNIE-ViL (Yu et al., 2020). We finetune our model with a batch size of 64 and a learning rate of 5e-5 for 4 epochs."
    }, {
      "heading" : "5.2 Baseline Methods",
      "text" : "We compare our E2E-VLP model with all the three prevalent VLP architectures: i.e., singlestream and two-stream architectures of two-step pipeline framework and end-to-end one-step solution. Single-stream architecture uses a unified Transformer to encode the vision-language inputs, including the state-of-the-art methods such as OSCAR(Li et al., 2020b), UNITER(Chen et al., 2019), Unicoder-VL (Li et al., 2020a), VLBERT (Su et al., 2019) and VLP (Zhou et al., 2020). Image and text are separately encoded firstly and then fused together in two-stream architecture, including the state-of-the-art methods such as ERNIE-VIL(Yu et al., 2020), LXMERT (Tan and Bansal, 2019), ViLBERT (Lu et al., 2019, 2020). These two architectures both adopt the region-based visual features, where a object detector is first used to obtain the object-level feature representations. We also compare with the only end-to-end solution PixelBERT (Huang et al., 2020). PixelBERT adopts a random pixel sampling strategy to conduct the cross-modal pre-training, while it has no visual semantic understanding tasks for pre-training which is very important in V+L tasks."
    }, {
      "heading" : "5.3 Main Results",
      "text" : "The results on the downstream V+L tasks are shown in Table 1. It can be observed that: 1) with less parameters and only in-domain pre-training data (MS-COCO and Visual Genome), E2E-VLP can consistently achieve comparable performance against two-step region feature-based methods such as OSCAR and ERNIE-VIL. It shows the effectiveness of our end-to-end grid feature-based method, which can offer new perspectives to address the cross-modal pre-training and conduct fusion at a more fine-grained level. It has the potential of removing the complex procedure of region feature ex-\ntraction, and facilitate deeper interaction between visual feature and text data in an end-to-end fashion. 2) Our E2E-VLP method can significantly improve upon the end-to-end method PixelBERT, which demonstrates the advantages of our method for enhancing the fine-grained visual learning with object detection and image captioning,"
    }, {
      "heading" : "5.4 Importance of Visual Learning",
      "text" : "To further investigate the importance of each component in our method, we conduct ablation studies to assess the impact of different visual learning tasks on the VQA and NLVR2 development set. Table 3 shows the result. We can see that: 1) all the three visual pre-training tasks contribute to the final performance gain, and removing each of them can decrease the performance on both tasks. The object detection and attribute prediction tasks can help capture fine-grained object-level semantics within the image, which is consistent with the previous two-step solutions that using region features from the detection can help improve the performance for cross-modal understanding. The image-to-text generation task can help guide the learning of visual features in regards to the textual semantics, which has the same conclusion as VirTex (Desai and Johnson, 2020). 2) Among the different visual pre-training tasks, the Object Detection and Attribute Prediction tasks are more important than the Image-to-Text Generation task, this may be due to the fact that the typical cross-modal downstream tasks such as VQA and NLVR2 focus more on the fine-grained semantics of the objects within image."
    }, {
      "heading" : "5.5 Inference Efficiency",
      "text" : "One of the biggest advantages of end-to-end VLP method is the inference efficiency with one single stage. Therefore, we further examine the online inference efficiency of E2E-VLP, compared with the two-step region-based models (UNITER and LXMERT) and the existing end-to-end VLP model (PixelBERT). We examine the average inference\ntime (per query) of different models on the VQA dataset. The result is shown in Table 4. We can see that: 1) the end-to-end methods can be much more efficient in online inference (2-3 times speedup) than the two-step model. We further analyze the inference time of different components of two-step models and find that among the total cost of 500ms per image-text pair, about 80% of the total time is used to extract region-based features using Faster R-CNN (Ren et al., 2015). It takes much time for region selection and this will happen twice when extracting the final regions, and it contains many complicated post-processing procedures. 2) Our E2E-VLP model can achieve comparable results on both the VQA and NLVR2 datasets by saving about 3.5 times running time. Besides, we can also use a smaller image size to further improving the inference speed. Compared with PixelBERT, E2E-VLP can also obtain some speed-ups due to the reason that the Transformer hidden size of E2E-VLP is only 256, which makes E2E-VLP more light-weight and flexible. Our end-to-end solution can significantly improve the performance upon PixelBERT, because there are no visual pretraining tasks for PixelBERT and we enhance the pre-training of E2E-VLP with both the fine-grained Object Detection and Image Captioning tasks."
    }, {
      "heading" : "5.6 Architecture Selection",
      "text" : "Since our whole framework contains both the visual backbone and Transformer network as a whole, we further study the importance of different model\narchitectures by changing the number of Transformer encoder layers and the different ResNet visual backbone layers. We expect to further examine whether the visual backbone or Transformer network is more important for the cross-modal understanding and fusion. From Table 5, we can see that both adding more Transformer encoder layers and using more complicated visual backbones can contribute to the final performance gain, which proves the importance of both modules for cross-modal understanding. Learning better visual features and conducting more deeply interacted visual-language fusion are both important for V+L tasks. Besides, we can see that using a more strong visual backbone (such as ResNet 152) can give more benefit to the final performance than just increasing the number of Transformer encoder layers from 6 to 12. This may be due to the fact that visual semantic understanding is rather important in V+L tasks and that is also why we design more fine-grained visual pre-training tasks for further enhancing the learning of E2E-VLP."
    }, {
      "heading" : "5.7 Impact of Input Image Size",
      "text" : "As mentioned in Section 3.1.1, the sequence length of the visual features is determined by the image size HW . Therefore, the final sequence length of the input to the transformer also largely depends on the image size, which can in turn influence the inference speed of our whole framework. We further analyze the impact of input image size to the efficiency and effectiveness of E2E-VLP. The results of E2E-VLP with different image sizes as input are shown in Table 6. From the results, we can see that E2E-VLP benefits from larger images as input, and for larger images, the sequence length of the visual representation is longer and more information is embedded in the visual representation. The cross-modal Transformer is capable of learning more fine-grained vision-language fusion for better performance. Moreover, down-sampling the image to a smaller size can significantly improve the inference speed of E2E-VLP model, while the model accuracy only decreases a little. For example, when changing the input size from (800, 1333) to (448, 448), the inference can be about 5 times faster while the performance only decreases about 2%-3%."
    }, {
      "heading" : "5.8 Object Detection with Paired Text",
      "text" : "Finally, we expect to further examine whether the cross-modal fusion is stable and E2E-VLP capture\nfine-grained semantics by visual learning. Therefore, we encode both the image content and caption text with E2E-VLP, and directly fine-tune it on MSCOCO object detection benchmark dataset with the decoder as in DETR(Carion et al., 2020). Table 7 shows the detection result. We can see that our E2E-VLP model can also support the Object Detection task based on text-image pairs and perform surprising well compared with the original DETR model. This phenomenon may also demonstrate that E2E-VLP well captures the fine-grained semantics within image and can appropriately fuse the multi-modal information for conducting visualonly task."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we propose a new end-to-end paradigm for pixel-level vision-language pretraining, to jointly learn visual representation, and semantic alignments between image and text. Different from the previous methods using the region features in a two-stage pipeline, we propose to use the more flexible and efficient image grid features for vision-language pre-training. We further incorporate the tasks of object detection and image captioning into pre-training with a unified Transformer encoder-decoder architecture for enhancing visual learning. The experiments on well-established vision-language downstream tasks demonstrate the effectiveness and efficiency of our E2E-VLP model. We hope that this study can potentially offer new perspectives and guide for endto-end vision-language pre-training.\nIn the future, we will explore more deeply interacted ways for image-text fusion from a bottom\nlayer, and incorporate more advanced vision and language pre-training tasks for further improving the performance."
    } ],
    "references" : [ {
      "title" : "Bottom-up and top-down attention for image captioning and visual question answering",
      "author" : [ "Peter Anderson", "Xiaodong He", "Chris Buehler", "Damien Teney", "Mark Johnson", "Stephen Gould", "Lei Zhang." ],
      "venue" : "Proceedings of the IEEE conference on computer vi-",
      "citeRegEx" : "Anderson et al\\.,? 2018",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2018
    }, {
      "title" : "Vqa: Visual question answering",
      "author" : [ "Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "Proceedings of the IEEE international conference on computer vision, pages 2425–2433.",
      "citeRegEx" : "Antol et al\\.,? 2015",
      "shortCiteRegEx" : "Antol et al\\.",
      "year" : 2015
    }, {
      "title" : "Palm: Pre-training an autoencoding&autoregressive language model for context-conditioned generation",
      "author" : [ "Bin Bi", "Chenliang Li", "Chen Wu", "Ming Yan", "Wei Wang", "Songfang Huang", "Fei Huang", "Luo Si." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical",
      "citeRegEx" : "Bi et al\\.,? 2020",
      "shortCiteRegEx" : "Bi et al\\.",
      "year" : 2020
    }, {
      "title" : "End-to-end object detection with transformers",
      "author" : [ "Nicolas Carion", "Francisco Massa", "Gabriel Synnaeve", "Nicolas Usunier", "Alexander Kirillov", "Sergey Zagoruyko." ],
      "venue" : "European Conference on Computer Vision, pages 213–229. Springer.",
      "citeRegEx" : "Carion et al\\.,? 2020",
      "shortCiteRegEx" : "Carion et al\\.",
      "year" : 2020
    }, {
      "title" : "Microsoft coco captions: Data collection and evaluation server",
      "author" : [ "Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Dollár", "C Lawrence Zitnick." ],
      "venue" : "arXiv preprint arXiv:1504.00325.",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Uniter: Universal image-text representation learning",
      "author" : [ "Yen-Chun Chen", "Linjie Li", "Licheng Yu", "Ahmed El Kholy", "Faisal Ahmed", "Zhe Gan", "Yu Cheng", "Jingjing Liu" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Virtex: Learning visual representations from textual annotations",
      "author" : [ "Karan Desai", "Justin Johnson." ],
      "venue" : "arXiv preprint arXiv:2006.06666.",
      "citeRegEx" : "Desai and Johnson.,? 2020",
      "shortCiteRegEx" : "Desai and Johnson.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "arXiv preprint arXiv:1810.04805.",
      "citeRegEx" : "Devlin et al\\.,? 2018",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "An image is worth 16x16 words: Transformers",
      "author" : [ "Alexey Dosovitskiy", "Lucas Beyer", "Alexander Kolesnikov", "Dirk Weissenborn", "Xiaohua Zhai", "Thomas Unterthiner", "Mostafa Dehghani", "Matthias Minderer", "Georg Heigold", "Sylvain Gelly" ],
      "venue" : null,
      "citeRegEx" : "Dosovitskiy et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Dosovitskiy et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "Proceedings of the IEEE conference on",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Pixel-bert: Aligning image pixels with text by deep multi-modal transformers",
      "author" : [ "Zhicheng Huang", "Zhaoyang Zeng", "Bei Liu", "Dongmei Fu", "Jianlong Fu." ],
      "venue" : "arXiv preprint arXiv:2004.00849.",
      "citeRegEx" : "Huang et al\\.,? 2020",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "In defense of grid features for visual question answering",
      "author" : [ "Huaizu Jiang", "Ishan Misra", "Marcus Rohrbach", "Erik Learned-Miller", "Xinlei Chen." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10267–",
      "citeRegEx" : "Jiang et al\\.,? 2020",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "author" : [ "Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A Shamma" ],
      "venue" : null,
      "citeRegEx" : "Krishna et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Krishna et al\\.",
      "year" : 2017
    }, {
      "title" : "Albert: A lite bert for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Lan et al\\.,? 2019",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2019
    }, {
      "title" : "Stacked cross attention for image-text matching",
      "author" : [ "Kuang-Huei Lee", "Xi Chen", "Gang Hua", "Houdong Hu", "Xiaodong He." ],
      "venue" : "Proceedings of the European Conference on Computer Vision (ECCV), pages 201–216.",
      "citeRegEx" : "Lee et al\\.,? 2018",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2018
    }, {
      "title" : "Bart: Denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Ves Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2019
    }, {
      "title" : "Semvlp: Vision-language pre-training by aligning semantics at multiple levels",
      "author" : [ "Chenliang Li", "Ming Yan", "Haiyang Xu", "Fuli Luo", "Wei Wang", "Bin Bi", "Songfang Huang." ],
      "venue" : "arXiv preprint arXiv:2103.07829.",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Unicoder-vl: A universal encoder for vision and language by cross-modal pretraining",
      "author" : [ "Gen Li", "Nan Duan", "Yuejian Fang", "Ming Gong", "Daxin Jiang." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11336–",
      "citeRegEx" : "Li et al\\.,? 2020a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Visualbert: A simple and performant baseline for vision and language",
      "author" : [ "Liunian Harold Li", "Mark Yatskar", "Da Yin", "Cho-Jui Hsieh", "Kai-Wei Chang." ],
      "venue" : "arXiv preprint arXiv:1908.03557.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Oscar: Object-semantics aligned pre-training for vision-language",
      "author" : [ "Xiujun Li", "Xi Yin", "Chunyuan Li", "Xiaowei Hu", "Pengchuan Zhang", "Lei Zhang", "Lijuan Wang", "Houdong Hu", "Li Dong", "Furu Wei" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C Lawrence Zitnick." ],
      "venue" : "European conference on computer vision, pages 740–755. Springer.",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2018",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2018
    }, {
      "title" : "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "author" : [ "Jiasen Lu", "Dhruv Batra", "Devi Parikh", "Stefan Lee." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 13–23.",
      "citeRegEx" : "Lu et al\\.,? 2019",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2019
    }, {
      "title" : "12-in-1: Multi-task vision and language representation learning",
      "author" : [ "Jiasen Lu", "Vedanuj Goswami", "Marcus Rohrbach", "Devi Parikh", "Stefan Lee." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10437–",
      "citeRegEx" : "Lu et al\\.,? 2020",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2020
    }, {
      "title" : "Image transformer",
      "author" : [ "Niki Parmar", "Ashish Vaswani", "Jakob Uszkoreit", "Lukasz Kaiser", "Noam Shazeer", "Alexander Ku", "Dustin Tran." ],
      "venue" : "International Conference on Machine Learning, pages 4055–4064. PMLR.",
      "citeRegEx" : "Parmar et al\\.,? 2018",
      "shortCiteRegEx" : "Parmar et al\\.",
      "year" : 2018
    }, {
      "title" : "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "author" : [ "Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun." ],
      "venue" : "Advances in neural information processing systems, pages 91–99.",
      "citeRegEx" : "Ren et al\\.,? 2015",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "Mass: Masked sequence to sequence pre-training for language generation",
      "author" : [ "Kaitao Song", "Xu Tan", "Tao Qin", "Jianfeng Lu", "TieYan Liu." ],
      "venue" : "International Conference on Machine Learning, pages 5926–5936. PMLR.",
      "citeRegEx" : "Song et al\\.,? 2019",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2019
    }, {
      "title" : "End-to-end people detection in crowded scenes",
      "author" : [ "Russell Stewart", "Mykhaylo Andriluka", "Andrew Y Ng." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2325–2333.",
      "citeRegEx" : "Stewart et al\\.,? 2016",
      "shortCiteRegEx" : "Stewart et al\\.",
      "year" : 2016
    }, {
      "title" : "Vl-bert: Pretraining of generic visual-linguistic representations",
      "author" : [ "Weijie Su", "Xizhou Zhu", "Yue Cao", "Bin Li", "Lewei Lu", "Furu Wei", "Jifeng Dai." ],
      "venue" : "arXiv preprint arXiv:1908.08530.",
      "citeRegEx" : "Su et al\\.,? 2019",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2019
    }, {
      "title" : "A corpus for reasoning about natural language grounded in photographs",
      "author" : [ "Alane Suhr", "Stephanie Zhou", "Ally Zhang", "Iris Zhang", "Huajun Bai", "Yoav Artzi." ],
      "venue" : "arXiv preprint arXiv:1811.00491.",
      "citeRegEx" : "Suhr et al\\.,? 2018",
      "shortCiteRegEx" : "Suhr et al\\.",
      "year" : 2018
    }, {
      "title" : "Circle loss: A unified perspective",
      "author" : [ "Yifan Sun", "Changmao Cheng", "Yuhan Zhang", "Chi Zhang", "Liang Zheng", "Zhongdao Wang", "Yichen Wei" ],
      "venue" : null,
      "citeRegEx" : "Sun et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jon Shlens", "Zbigniew Wojna." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818–2826.",
      "citeRegEx" : "Szegedy et al\\.,? 2016",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2016
    }, {
      "title" : "Lxmert: Learning cross-modality encoder representations from transformers",
      "author" : [ "Hao Tan", "Mohit Bansal." ],
      "venue" : "arXiv preprint arXiv:1908.07490.",
      "citeRegEx" : "Tan and Bansal.,? 2019",
      "shortCiteRegEx" : "Tan and Bansal.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Structbert: Incorporating language structures into pretraining for deep language understanding",
      "author" : [ "Wei Wang", "Bin Bi", "Ming Yan", "Chen Wu", "Zuyi Bao", "Jiangnan Xia", "Liwei Peng", "Luo Si." ],
      "venue" : "arXiv preprint arXiv:1908.04577.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
      "author" : [ "Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 2:67–78.",
      "citeRegEx" : "Young et al\\.,? 2014",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 2014
    }, {
      "title" : "Ernievil: Knowledge enhanced vision-language representations through scene graph",
      "author" : [ "Fei Yu", "Jiji Tang", "Weichong Yin", "Yu Sun", "Hao Tian", "Hua Wu", "Haifeng Wang." ],
      "venue" : "arXiv preprint arXiv:2006.16934.",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Vinvl: Making visual representations matter in vision-language models",
      "author" : [ "Pengchuan Zhang", "Xiujun Li", "Xiaowei Hu", "Jianwei Yang", "Lei Zhang", "Lijuan Wang", "Yejin Choi", "Jianfeng Gao." ],
      "venue" : "arXiv preprint arXiv:2101.00529.",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Unified vision-language pre-training for image captioning and vqa",
      "author" : [ "Luowei Zhou", "Hamid Palangi", "Lei Zhang", "Houdong Hu", "Jason Corso", "Jianfeng Gao." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 13041–",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Self-supervised pre-training has achieved great success in a wide range of natural language understanding (Devlin et al., 2018; Liu et al., 2019; Wang et al., 2019; Lan et al., 2019) and generation tasks (Song et al.",
      "startOffset" : 106,
      "endOffset" : 182
    }, {
      "referenceID" : 21,
      "context" : "Self-supervised pre-training has achieved great success in a wide range of natural language understanding (Devlin et al., 2018; Liu et al., 2019; Wang et al., 2019; Lan et al., 2019) and generation tasks (Song et al.",
      "startOffset" : 106,
      "endOffset" : 182
    }, {
      "referenceID" : 35,
      "context" : "Self-supervised pre-training has achieved great success in a wide range of natural language understanding (Devlin et al., 2018; Liu et al., 2019; Wang et al., 2019; Lan et al., 2019) and generation tasks (Song et al.",
      "startOffset" : 106,
      "endOffset" : 182
    }, {
      "referenceID" : 13,
      "context" : "Self-supervised pre-training has achieved great success in a wide range of natural language understanding (Devlin et al., 2018; Liu et al., 2019; Wang et al., 2019; Lan et al., 2019) and generation tasks (Song et al.",
      "startOffset" : 106,
      "endOffset" : 182
    }, {
      "referenceID" : 18,
      "context" : "Recent studies (Li et al., 2019; Lu et al., 2019; Chen et al., 2019; Tan and Bansal, 2019; Li et al., 2020b; Yu et al., 2020) have also witnessed the progress of self-supervised pretraining on vision-and-language tasks, which learns",
      "startOffset" : 15,
      "endOffset" : 125
    }, {
      "referenceID" : 23,
      "context" : "Recent studies (Li et al., 2019; Lu et al., 2019; Chen et al., 2019; Tan and Bansal, 2019; Li et al., 2020b; Yu et al., 2020) have also witnessed the progress of self-supervised pretraining on vision-and-language tasks, which learns",
      "startOffset" : 15,
      "endOffset" : 125
    }, {
      "referenceID" : 5,
      "context" : "Recent studies (Li et al., 2019; Lu et al., 2019; Chen et al., 2019; Tan and Bansal, 2019; Li et al., 2020b; Yu et al., 2020) have also witnessed the progress of self-supervised pretraining on vision-and-language tasks, which learns",
      "startOffset" : 15,
      "endOffset" : 125
    }, {
      "referenceID" : 33,
      "context" : "Recent studies (Li et al., 2019; Lu et al., 2019; Chen et al., 2019; Tan and Bansal, 2019; Li et al., 2020b; Yu et al., 2020) have also witnessed the progress of self-supervised pretraining on vision-and-language tasks, which learns",
      "startOffset" : 15,
      "endOffset" : 125
    }, {
      "referenceID" : 37,
      "context" : "Recent studies (Li et al., 2019; Lu et al., 2019; Chen et al., 2019; Tan and Bansal, 2019; Li et al., 2020b; Yu et al., 2020) have also witnessed the progress of self-supervised pretraining on vision-and-language tasks, which learns",
      "startOffset" : 15,
      "endOffset" : 125
    }, {
      "referenceID" : 12,
      "context" : "visual dataset such as Visual Genome dataset (Krishna et al., 2017), and the visual representation is not optimized towards a more generic cross-modal understanding in the second step.",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 11,
      "context" : "Recently, several studies such as (Jiang et al., 2020) have begun to revisit the grid features for cross-modal understanding and found the grid features can also work surprisingly well, while making the model design and training process much simpler.",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 10,
      "context" : "One pioneering work Pixel-BERT (Huang et al., 2020) explores to pre-train with grid features in an end-to-end fashion directly from pixels.",
      "startOffset" : 31,
      "endOffset" : 51
    }, {
      "referenceID" : 3,
      "context" : "Specifically, two end-to-end pretraining tasks are further incorporated: 1) Object Detection: inspired from DETR (Carion et al., 2020), we view the object detection as a direct set prediction problem.",
      "startOffset" : 113,
      "endOffset" : 134
    }, {
      "referenceID" : 38,
      "context" : "These two tasks can help learn high-quality visual representations (Zhang et al., 2021; Desai and Johnson, 2020).",
      "startOffset" : 67,
      "endOffset" : 112
    }, {
      "referenceID" : 6,
      "context" : "These two tasks can help learn high-quality visual representations (Zhang et al., 2021; Desai and Johnson, 2020).",
      "startOffset" : 67,
      "endOffset" : 112
    }, {
      "referenceID" : 27,
      "context" : ", 2019) and text generation tasks (Song et al., 2019; Lewis et al., 2019; Bi et al., 2020).",
      "startOffset" : 34,
      "endOffset" : 90
    }, {
      "referenceID" : 15,
      "context" : ", 2019) and text generation tasks (Song et al., 2019; Lewis et al., 2019; Bi et al., 2020).",
      "startOffset" : 34,
      "endOffset" : 90
    }, {
      "referenceID" : 2,
      "context" : ", 2019) and text generation tasks (Song et al., 2019; Lewis et al., 2019; Bi et al., 2020).",
      "startOffset" : 34,
      "endOffset" : 90
    }, {
      "referenceID" : 1,
      "context" : "image-text pairs, which has proved effective for a wide range of vision-language (VL) tasks, such as VQA (Antol et al., 2015), NLVR (Young et al.",
      "startOffset" : 105,
      "endOffset" : 125
    }, {
      "referenceID" : 36,
      "context" : ", 2015), NLVR (Young et al., 2014), Cross-modal Retrieval (Suhr et al.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 34,
      "context" : "The first line uses a singlestream transformer architecture (Vaswani et al., 2017) to model both image and text representations in a unified semantic space such as VLBERT (Su et al.",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 29,
      "context" : ", 2017) to model both image and text representations in a unified semantic space such as VLBERT (Su et al., 2019), UNITER (Chen et al.",
      "startOffset" : 96,
      "endOffset" : 113
    }, {
      "referenceID" : 5,
      "context" : ", 2019), UNITER (Chen et al., 2019) and OSCAR (Li et al.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 33,
      "context" : "line adopts a two-stream Transformer architecture that first encodes the image and text modalities separately, and then fuses the cross-modal representations with another Transformer network, such as LXMERT (Tan and Bansal, 2019) and ERNIEViL (Yu et al.",
      "startOffset" : 207,
      "endOffset" : 229
    }, {
      "referenceID" : 37,
      "context" : "line adopts a two-stream Transformer architecture that first encodes the image and text modalities separately, and then fuses the cross-modal representations with another Transformer network, such as LXMERT (Tan and Bansal, 2019) and ERNIEViL (Yu et al., 2020).",
      "startOffset" : 243,
      "endOffset" : 260
    }, {
      "referenceID" : 16,
      "context" : "Besides, SemVLP (Li et al., 2021) is pre-trained iteratively with two prevalent fashions.",
      "startOffset" : 16,
      "endOffset" : 33
    }, {
      "referenceID" : 10,
      "context" : "(Huang et al., 2020) represents the first and only work to pre-train with grid features in an end-toend fashion.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 3,
      "context" : "Inspired by the recent breakthrough of using Transformer on computer vision tasks such as DETR (Carion et al., 2020) and ViT Transformer (Dosovitskiy et al.",
      "startOffset" : 95,
      "endOffset" : 116
    }, {
      "referenceID" : 8,
      "context" : ", 2020) and ViT Transformer (Dosovitskiy et al., 2020), we propose to use a Transformer encoder-decoder framework (Vaswani et al.",
      "startOffset" : 28,
      "endOffset" : 54
    }, {
      "referenceID" : 34,
      "context" : ", 2020), we propose to use a Transformer encoder-decoder framework (Vaswani et al., 2017) for cross-modal learning, and a simple CNN backbone module is used as the image encoder for extracting visual representations from pixels so as to allow for more flexible network design.",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 7,
      "context" : "Then, similar to BERT (Devlin et al., 2018), each token wi is assigned three kinds of embeddings: token, segment and position embeddings.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : "Image Representations For image feature representation, the most existing VLP models follow Bottom-Up and Top-Down Attention (Anderson et al., 2018) to extract region features by Faster RCNN (Ren et al.",
      "startOffset" : 125,
      "endOffset" : 148
    }, {
      "referenceID" : 26,
      "context" : ", 2018) to extract region features by Faster RCNN (Ren et al., 2015) trained on Visual Genome dataset.",
      "startOffset" : 50,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : "The pixel features are learned by a CNN visual backbone such as ResNet (He et al., 2016).",
      "startOffset" : 71,
      "endOffset" : 88
    }, {
      "referenceID" : 3,
      "context" : "Starting from the initial image vimg ∈ R3×H0×W0 (with 3 color channels), a conventional CNN backbone generates a lowerresolution activation map fimg ∈ RC×H×W using the typical values as in DETR (Carion et al., 2020): C = 2048 and H = H0 32 ,W = w0 32 .",
      "startOffset" : 194,
      "endOffset" : 215
    }, {
      "referenceID" : 25,
      "context" : "feature maps with fixed positional encodings (Parmar et al., 2018) that are added to the input of each attention layer.",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 33,
      "context" : "To facilitate cross-modal understanding, we follow (Tan and Bansal, 2019; Chen et al., 2019; Huang et al., 2020) and conduct two popular pre-training tasks in encoder side, in-",
      "startOffset" : 51,
      "endOffset" : 112
    }, {
      "referenceID" : 5,
      "context" : "To facilitate cross-modal understanding, we follow (Tan and Bansal, 2019; Chen et al., 2019; Huang et al., 2020) and conduct two popular pre-training tasks in encoder side, in-",
      "startOffset" : 51,
      "endOffset" : 112
    }, {
      "referenceID" : 10,
      "context" : "To facilitate cross-modal understanding, we follow (Tan and Bansal, 2019; Chen et al., 2019; Huang et al., 2020) and conduct two popular pre-training tasks in encoder side, in-",
      "startOffset" : 51,
      "endOffset" : 112
    }, {
      "referenceID" : 7,
      "context" : "Masked Language Modeling The task setup is basically the same as in BERT (Devlin et al., 2018), we randomly mask 15% tokens in the text and the model is asked to predict these masked words with the output text and visual representations.",
      "startOffset" : 73,
      "endOffset" : 94
    }, {
      "referenceID" : 3,
      "context" : "Enhanced by Object Detection Following the one-stage detection model DETR (Carion et al., 2020), we define object detection task as the direct set prediction problem, and use a set-based global loss that forces unique predictions via bipartite matching with the Transformer encoder-decoder architecture.",
      "startOffset" : 74,
      "endOffset" : 95
    }, {
      "referenceID" : 28,
      "context" : "The Hungarian algorithm (Stewart et al., 2016) is used to efficiently compute the optimal assignment.",
      "startOffset" : 24,
      "endOffset" : 46
    }, {
      "referenceID" : 20,
      "context" : "We pre-train our E2E-VLP on two in-domain image-text datasets: MS-COCO (Lin et al., 2014) and Visual Genome (Krishna et al.",
      "startOffset" : 71,
      "endOffset" : 89
    }, {
      "referenceID" : 3,
      "context" : "the input images so that the shortest side is at least 480 and at most 800 pixels while the longest is at most 1333 (Carion et al., 2020).",
      "startOffset" : 116,
      "endOffset" : 137
    }, {
      "referenceID" : 9,
      "context" : "The visual backbone is selected as ResNet with different sizes (He et al., 2016) from torchvision with frozen batchnorm layers.",
      "startOffset" : 63,
      "endOffset" : 80
    }, {
      "referenceID" : 22,
      "context" : "We use the AdamW optimizor (Loshchilov and Hutter, 2018) for both the Transformer and ResNet.",
      "startOffset" : 27,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : "0 (Antol et al., 2015): The VQA task requires the model to answer natural language questions given an image.",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 1,
      "context" : "dataset (Antol et al., 2015), which contains 204K images and 1.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "Following (Anderson et al., 2018), we treat VQA as a multi-label classification task by",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 30,
      "context" : ", 2018): NLVR2 (Suhr et al., 2018) is a challenging task for visual reasoning.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 32,
      "context" : "the seq2seq loss with label smoothing(Szegedy et al., 2016).",
      "startOffset" : 37,
      "endOffset" : 59
    }, {
      "referenceID" : 4,
      "context" : "We report our results on the COCO image captioning dataset (Chen et al., 2015).",
      "startOffset" : 59,
      "endOffset" : 78
    }, {
      "referenceID" : 36,
      "context" : "We conduct experiments on Flickr30K dataset (Young et al., 2014), which contains 31,000 images col-",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 14,
      "context" : "We follow the same split in (Lee et al., 2018) for training and evaluation.",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 5,
      "context" : "During finetuning, we follow the method in UNITER (Chen et al., 2019) and formulate it as a ranking problem.",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 31,
      "context" : "We use the hidden state of hCLS to compute the similarity scores for the sampled positive and negative pairs, and maximize the margin between them through circle loss (Sun et al., 2020) as ERNIE-ViL (Yu et al.",
      "startOffset" : 167,
      "endOffset" : 185
    }, {
      "referenceID" : 5,
      "context" : ", 2020b), UNITER(Chen et al., 2019), Unicoder-VL (Li et al.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 17,
      "context" : ", 2019), Unicoder-VL (Li et al., 2020a), VLBERT (Su et al.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 37,
      "context" : "Image and text are separately encoded firstly and then fused together in two-stream architecture, including the state-of-the-art methods such as ERNIE-VIL(Yu et al., 2020), LXMERT (Tan and Bansal, 2019),",
      "startOffset" : 154,
      "endOffset" : 171
    }, {
      "referenceID" : 26,
      "context" : "models and find that among the total cost of 500ms per image-text pair, about 80% of the total time is used to extract region-based features using Faster R-CNN (Ren et al., 2015).",
      "startOffset" : 160,
      "endOffset" : 178
    }, {
      "referenceID" : 3,
      "context" : "fore, we encode both the image content and caption text with E2E-VLP, and directly fine-tune it on MSCOCO object detection benchmark dataset with the decoder as in DETR(Carion et al., 2020).",
      "startOffset" : 168,
      "endOffset" : 189
    } ],
    "year" : 2021,
    "abstractText" : "Vision-language pre-training (VLP) on largescale image-text pairs has achieved huge success for the cross-modal downstream tasks. The most existing pre-training methods mainly adopt a two-step training procedure, which firstly employs a pre-trained object detector to extract region-based visual features, then concatenates the image representation and text embedding as the input of Transformer to train. However, these methods face problems of using task-specific visual representation of the specific object detector for generic crossmodal understanding, and the computation inefficiency of two-stage pipeline. In this paper, we propose the first end-to-end vision-language pre-trained model for both V+L understanding and generation, namely E2E-VLP, where we build a unified Transformer framework to jointly learn visual representation, and semantic alignments between image and text. We incorporate the tasks of object detection and image captioning into pretraining with a unified Transformer encoderdecoder architecture for enhancing visual learning. An extensive set of experiments have been conducted on well-established visionlanguage downstream tasks to demonstrate the effectiveness of this novel VLP paradigm.",
    "creator" : "LaTeX with hyperref"
  }
}