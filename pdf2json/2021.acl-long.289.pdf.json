{
  "name" : "2021.acl-long.289.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Structural Guidance for Transformer Language Models",
    "authors" : [ "Peng Qian", "Tahira Naseem", "Roger Levy", "Ramón Fernandez Astudillo" ],
    "emails" : [ "pqian@mit.edu", "tnaseem@us.ibm.com", "rplevy@mit.edu", "ramon.astudillo@ibm.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Pre-trained Transformer architectures have led to huge progress in building more human-like language processing systems (Radford et al.; Devlin et al., 2019; Brown et al., 2020, among others). These models achieve impressive perplexity results on language modelling datasets, perform well on grammatical judgments (Warstadt et al., 2020), and provide useful linguistic representations that benefit a wide range of downstream tasks. Probing analyses also suggest that these models learn to implicitly encode syntactic information (Hewitt and\nManning, 2019; Clark et al., 2019) that may support better linguistic generalization than recurrent neural network architectures (RNNs).\nHowever, the Transformer architecture (Vaswani et al., 2017) is an interesting subject of study beyond its success in transfer-learning settings. Transformer models lack the inductive biases of RNNs. Rather than maintaining vector-valued state and updating it in a recurrent manner, auto-regressive Transformer models encode all past decisions simultaneously at each inference step, thanks to a self-attention mechanism. The only notion of sequence order is also given by position embeddings summed to content embeddings in both input and auto-regressive signals.\nPrevious works have shown the advantage of structural supervision in RNNs in learning to maintain syntactic states and non-local dependencies (Kuncoro et al., 2018; Wilcox et al., 2019; Futrell et al., 2019). It remains an open question whether Transformer language models can similarly benefit from generative structural supervision, and what form of structural supervision would more effectively induce human-like syntactic generalization.\nThis work hypothesizes that the Transformer language model may benefit from explicit generative structural supervision to systematically generalize syntactic knowledge. Here we explore two major classes of structural guidance for Transformer language models based on joint modeling of language and constituency parses. The “generative parsing as language modeling” approach builds a Transformer-parameterized model to learn to predict actions that incrementally build constituency trees along with terminal words, following prior work on RNNs (Dyer et al., 2016; Choe and Charniak, 2016). The “structural scaffolding” approach follows the general idea of regularizing hidden representation through multi-task learning objective, with prior success in various NLP tasks (Zhang\nS NP\nThe birds\nVP\nsang ADVP\n〈BOS〉 NT(S) NT(NP) The birds REDUCE NT(VP) sang NT(ADVP) · · ·\nw0 w1 w2 w3\ny0:1 y1:2 y2:3 y3:4\nw1 w2 w3\nw0 w1 w2\n(a) Vanilla language model\nNT(S) NT(NP) The birds REDUCE\n〈BOS〉 NT(S) NT(NP) The birds\n(b) Parsing as Language Modelling\nw1 w2 w3\nw0 w1 w2\ny0:1 y1:2 y2:3\nw1 w2 w3\nw0 w1 w2\ny0:1 y1:2〈PAD〉\n(c) Language models with Structural Scaffold\nFigure 1: Top: Illustration of a partial constituency tree and corresponding transitions. Bottom: unidirectional transformer language model (a) without explicit structural supervision, (b) for modelling generative action parsing sequence, and (c) with structural scaffold for predicting the local incremental parsing state.\nand Weiss, 2016; Søgaard and Goldberg, 2016; Swayamdipta et al., 2018).\nWe test these two approaches on two subsets of the BLLIP dataset (Charniak et al., 2000) and evaluate models’ syntactic generalization performances on SG Test Suites (Hu et al., 2020) and a sampled subset of the BLiMP Benchmark (Warstadt et al., 2020). We show evidence that generative structural supervision indeed induces more robust and human-like linguistic generalization in Transformer language models and explore the different trade-offs involved in the presented methods."
    }, {
      "heading" : "2 Models",
      "text" : "Here we explore joint modelling of structures and words parametrized with Transformers by considering both a sentence W and its constituency parse Y and modeling the joint distribution P (W,Y )."
    }, {
      "heading" : "2.1 Generative Parsing as Language Modeling",
      "text" : "A language model can be described formally as a probability distribution over strings of a language w1, · · · , wT , usually left-to-right factored.\np(W ) = p(w1, · · · , wT ) = T∏ t=1 p(wt | w<t) (1)\nThere are many possible approaches that can combine both language modeling and syntax modeling tasks. As long as both tasks share some of the parameters they can be considered a case of multi-task learning (Caruana, 1997). Of interest\nhere is the model proposed in Recurrent Neural Network Grammars (RNNGs; Dyer et al., 2016) and parsing as language model (LSTM-LM; Choe and Charniak, 2016). Both approaches model the joint distribution of words W and constituency tree components Y as\np(Y,W ) = p(a1, · · · , aR) = R∏ t=1 p(at | a<t) (2)\nwhere at are transitions of a state machine that generates both the sentence and the tree. These transitions are similar to the well-established transition sets used for transition-based parsing (Earley, 1970) but adapted to generate both text and parse simultaneously. For the reminder of this work, we will consider each at to be integer valued and indexing a dictionary of transitions. A transition a can be a word w or a transition action that generates a component of the constituency tree y. The actions include non-terminal symbols that open and label a new constituent with the label x, indicated as NT(x), or a REDUCE action closing the closest open constituent. An example of a partial parse tree and transitions can be found at the top of Figure 1.\nRNNG and LSTM-LM parametrize the same factorization in Equation 2 in different ways. RNNG utilizes stack-LSTMs, which allow it to dynamically create representations for partial tree components by composition. The LSTM-LM, however, uses a flat parametrization treating the transitions as a sequence in a conventional language model learnt with an LSTM (Hochreiter and Schmidhuber, 1997). It should also be noted that the LSTM-\nLM is designed as a parser, while RNNG is also used as a language model. In order to derive a language model from a joint model, it is is necessary to marginalize over all possible parse trees\np(W ) = ∑\nY ∈Y(W )\np(Y,W ) (3)\nwhich is an intractable problem since there is an exponentially large number of possible trees. The original RNNG work (Dyer et al., 2016) proposes an approximate solution based on importance sampling. In this work we use the word-synchronous beam search approximation introduced in Stern et al. (2017).\nThe marginalized likelihood language model in Equation 3 is desirable because it makes no statistical independence assumption between language and syntax and shares all parameters across both tasks, with the exception of action specific embeddings. Particularly relevant for this work is the fact that both word and non-word transitions are predicted as language model output indiscriminately and are available at each prediction step through its history a<t.\nIn this work we propose to parametrize Eq 2 with a Transformer language model (Vaswani et al., 2017). This is equivalent to the flat parametrization of the LSTM-LM but using a Transformer language model instead. Unlike LSTM-LM, which is a parsing model, we derive from it a language model by marginalization as in the RNNG. A Transformer language model can be succinctly described as a neural network of vertically stacked layers where the m-th layer is given by\nhm<t = FF m O ·  Am1 (h m−1 <t ) Am2 (h m−1 <t ) · · ·\nAmN (h m−1 <t )\n  . (4)\nHere hm−1<t ∈ RH×t is the output of the previous decoder layer for all previous predictions of the model at time step t and H is the size of the hidden vector. The input to the first layer i.e. h0<t are the embeddings of all previous transitions a<t concatenated with a start symbol. Each embedding is the sum of both a content embedding, dictionary vector that is being indexed, and a position embedding that encodes the absolute or relative position of each action in the sequence. FFm() is a feed-forward layer, Am1 () · · ·AMN () are multiple self-attention heads and O ∈ RH×H\nis a matrix multiplication performed on the concatenated output of the attention heads. Both the feed-forward and the projection of N attention heads through O are wrapped around with residual, dropout and layer normalization operations that are here removed for clarity.\nEach attention head comprises a simple inner product attention mechanism\nAmn (h m−1 <t ) = V m n · hm−1<t · softmax ( (Kmn · hm−1<t )T ·Qmn · h m−1 <t +M ) (5)\nwhere V mn ,K m n , Q m n ∈ RH/N×H are value, key and query projection matrices respectively and the softmax operation is normalized over columns to sum to one. The matrixM∈ {−∞, 0}t×t is used to prevent the model from attending to future states during training, enabling efficient parallelization. It is displayed here due to its relevance for the next section.\nSimilarly to other models, to derive a distribution over all possible transitions, including words, nonterminal symbols and the REDUCE operation, we can use a softmax together with an inner product\np(at | a<t) = softmax(EW∪Y · hm<t)at (6)\nwhere EW∪Y are the embeddings for the joint vocabulary of words, non-terminals and REDUCE transitions. Henceforth, we refer to this model as Parsing as Language Model, or PLM for short.\nUnlike LSTMs or the RNNG, the Transformer has direct access to all past decisions through selfattention and relies on position embeddings to encode word order. Thus, in principle, there is no structural bias for the model to favor past decisions that are close in time to inform current prediction. On one hand, this potential ability to use long distance information can enable a less local, more human like processing of language, but on the other hand, it can also result in an additional learning burden, especially if there is not sufficient learning data available. Also worth noting for the experiments proposed here is that the total number of parameters of a typical Transformer greatly exceeds that of an LSTM or a RNNG model."
    }, {
      "heading" : "2.2 Incorporating RNNG-like characteristics",
      "text" : "As previously mentioned, unlike any of the other models, the RNNG is able to create partial tree representations by composition using stack-LSTMs.\nThis changes the RNNG model structure dynamically as a function of the partial parse, a very desirable property to derive syntax-aware representations. Moreover, the fact that Recurrent Neural Networks such as LSTMs summarize all information about previous time steps on two hidden vectors, creates a bottleneck that forces the model to focus on the local state. This is a situation where a syntax-aware representation can provide additional value by enabling the local state to better encompass past structures. We conjecture that a similarly constrained local state might benefit Transformer models in learning linguistic regularities, especially in a limited training data scenario.\nIn an attempt to capture a similar effect in the Transformer, we explore here the idea of masking some attention heads to reflect the parser state as in the stack-Transformer (Astudillo et al., 2020). In the stack-Transformer, two attention heads are specialized to attend only to the contents of buffer and stack respectively for dependency and semantic parsing tasks. Here we choose to specialize two heads as well for each layer in Equation 4, as depicted in Fig. 2. One attention head attends to the contents of the last open constituent whereas another head attends all other past decisions not involving that constituent. The rest of the heads are left free as in the original Transformer architecture. To constrain the attention heads, we only need to alter the maskM in Equation 5 to depend on head index n and past actionsMn(a<t), which results in a negligible computation overhead.\nThis hard masking makes the model structure change dynamically depending on the partial parse and it forces some heads to focus on the local syn-\ntactic state. Nevertheless, unlike the RNNG, it does not create new representations of partial parses that can be composed in a recurrent manner at each time step, and some attention heads can still operate unrestricted. We hypothesize that structure-aware attention mechanism may still help the model achieve better generalization. The symbolic representation induces a strong inductive bias to how the model should use the structure that it generates on the fly. We henceforth refer to this model PLM-mask."
    }, {
      "heading" : "2.3 Scaffolding by Learning to Predict Local Parse States",
      "text" : "Given the strong coupling between the tasks, the marginal likelihood Transformer language model of the previous section can be expected to be strongly influenced by the additional syntax prediction task. This comes however at a big cost. First, sequences combine both words and non-terminal and reduce transitions, yielding longer sentences than those of a normal language model R > T . Furthermore the approximated marginalization is computationally intensive and also introduces an approximation error.\nOne well-established regime that allows joint modeling of tasks at a low complexity is that of the syntactic scaffold (Zhang and Weiss, 2016; Søgaard and Goldberg, 2016; Swayamdipta et al., 2018). Scaffolding adds an additional structure prediction task at one of the layers of the model as a separate layer and only during training. This is a minimally intrusive change since it just branches some hidden vector of the network and computes an additional loss. It also has no influence on test runtime and avoids expensive steps such as marginalization.\nHowever, applying the idea of syntactic scaffolding to our present scenario poses one difficulty. If we use a standard language model predicting words w and predict the non-word symbols y separately, we face the problem that the two sequences have different lengths. To overcome this in a straightforward way, we predict the n-gram of non-word actions yt:t+n(t) corresponding to the partial parse synchronous with step t when we predict word wt. We use a secondary softmax layer for this action n-gram prediction.\np(yt:t+n | y<t) = softmax(EY ∗ · hm<t)yt:t+n (7)\nHere EY ∗\nis the vocabulary of all transition ngrams excluding words found in the train corpus plus a blank symbol. Note that since Scaffolding\noperates only at train time, we do not need to worry about generalization of these n-grams to test time.\nThe models are thus trained to minimize the loss function − log p(Y,W ) where\np(Y,W ) = ∏T\nt=1 p(wt | w<t) + ∏T\nt=1 p(yt:t+n(t) | w<t) (8)\nThe scaffold can be set so that the synchronous non-word action n-grams yt:t+n(t) are predicted either before (Figure 1c, left) or after (Figure 1c, right) producing wt. We considered both variants in our experiments to empirically assess their impact on performance. We refer to this model as Transformer Language Model with Syntactic Scaffold, or ScLM in short, and its two versions ScLM-past and ScLM-next, for past and next ngram prediction."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Model Training",
      "text" : "All models, including the baseline vanilla language models (LM in short), the syntactic scaffold models, and the generative parsing models, are based on the same architecture of GPT-2 small (Radford et al.) (117M parameters, 12 layers, H = 768) and use the same BPE tokenizer, but with randomly initialized weights. We believe this would give us a fair comparison to pretrained GPT-2 as well, in order to evaluate whether structural guidance helps improve sample efficiency. We implemented all the proposed models using Huggingface’s Transformer package (Wolf et al., 2020)1.\nAs our goal here is to study whether structural guidance helps models learn robust humanlike generalization of syntactic knowledge, we train our model on the BLLIP dataset (Charniak et al., 2000), an English newswire style corpus used in Hu et al. (2020). This makes the results here more comparable to the results reported in previous work, especially with RNNGs. We train the proposed models and the baseline vanilla Transformer language models on BLLIP-MD, a 14 million-token corpus, and BLLIP-LG, a 46 million-token corpus, both of which are auto-parsed using a state-of-theart constituency parser (Kitaev and Klein, 2018). We used the parsed sentences to generate oracle parsing action sequence for PLM and PLM-mask. We collected a list of word-synchronous parsing\n1Code available at https://github.com/IBM/ transformers-struct-guidance\naction sequences from the train and development oracle of BLLIP-LG and use it to parametrize the action n-gram vocabulary of ScLMs trained on both BLLIP-MD and BLLIP-LG. There are 3756 action n-gram types from the corpora, including one padding token and one blank token.\nAll models were trained with learning rate 10−5, AdamW optimizer, and minibatch of size 5. We trained the models with multiple seeds within the capacity of our resources, in order to accommodate potential variance. In total, there are three seeds of LM, four of ScLM-past, four of ScLM-next, three of PLM, and three of PLM-mask for BLLIP-MD, and the same number of seeds of each model type for BLLIP-LG. Models were trained until convergence, as suggested by the loss of the development set during training."
    }, {
      "heading" : "3.2 Targeted Syntactic Evaluation",
      "text" : "To assess whether a trained model systematically generalizes its syntactic knowledge, we employ targeted syntactic evaluation paradigm (Marvin and Linzen, 2018). Specifically, we measure models’ performance on two held-out test datasets, a collection of syntactic generalization test suites from Hu et al. (2020) and BLiMP Benchmark from Warstadt et al. (2020). These two datasets cover a wide range of English syntactic phenomena.\nTests from Hu et al. (2020), which we refer as SG Test Suites, consist of hand-designed test suites for evaluating fine-grained syntactic generalization in incremental processing of a linguistic input. The general method is to compare models’ surprisals p(continuation|prefix) of grammatical and ungrammatical continuations given certain sentence prefixes. We report the accuracy averaged across SG test suites. BLiMP Benchmark features minimal pairs of a grammatical sentence W and an ungrammatical counterpart W ∗. To evaluate a model on these minimal pairs, one simply compares the likelihood of W and W ∗ assigned by the model.\nAs is implied by the evaluation methods, we need to marginalize out the structure variables for PLM or PLM-mask models in order to estimate the surprisal of a continuation, given a sentence prefix or the likelihood of a complete sentence. We follow similar setup as in Futrell et al. (2019); Wilcox et al. (2019) applying word-synchronous beam search (Stern et al., 2017) to find a list Yk of k incremental parses given a sentence prefix w<t.\nWe then sum the joint probability p(w<t, y<t) over the list of incremental parses given by the model to approximate the likelihood of p(w<t). We set the parse beam size to 100, word-synchronous beam size k as 10, and fast track size of 5. Since the search process can be computationally intensive, the large number of items in BLiMP benchmark poses a computational challenge. We therefore select the first 10% out of the 1000 items in each of the 67 tests of BLiMP Benchmark. We report the accuracy over the 100 items and refer to this down-sized BLiMP Benchmark as BLiMP-10%.\nWe compare models’ performance on the SG Test Suites and BLiMP-10% in Figure 3. Each bar shows a model’s performance averaged across multiple seeds on a given benchmark, with each dot plotting the accuracy of a specific seed. Overall, syntactic generalization performance improves as the training data size increases from BLLIP-MD (14 million tokens) to BLLIP-LG (42 million tokens). Models with structural guidance achieve higher accuracy than the vanilla Transformer language model trained on the same set of raw text data without explicit structural information. We\nalso include the results for the RNNGs taken from Hu et al. (2020). RNNG lags behind all Transformer models by a large margin in average scores. We also notice that among different forms of structural guidance, generative parsing as language modeling is the most effective in improving syntactic generalization performance against the baseline transformer language models. We didn’t observe consistent benefits of adding dynamic masking mechanism to PLM. While scaffolding approach slightly improves vanilla Transformer language models, it still falls behind the best performance of the model trained with generative parsing. We hypothesize that our scaffold did not fully exploit the compositional structure in the local parses by modelling each action n-gram as a distinct type, while the generative parsing models only predict actions in a relatively small set of non-terminal action space, which might make it easier for PLM and PLM-mask to learn compositional generalization. We leave it for future work to design new scaffolds that can take advantage of the combinatorial nature of syntactic structure.\nFor completeness, we also ran the pre-trained GPT-2 model on the syntactic suites. This yielded a score of 0.808 on the SG Test Suites and 0.827 on BLiMP-10% for the small version of pre-trained GPT-2. Among models trained on BLLIP-LG, the average accuracy score on the SG Test Suites is 0.723 for PLMs, 0.748 for PLM-masks, and 0.665 for LMs. Similar trend is observed on BLiMP-10% as well, where among models trained on BLLIPLG the average accuracy is 0.751 for PLMs, 0.753 for PLM-masks, and 0.708 for LMs. The proposed PLM method is able to close the gap between GPT-2 small and the same model trained with BLLIP-LG by about half, while the improvement for BLiMP is more modest but still significative. It remains an open question whether scaling syntactic supervision to a larger dataset than BLLIP-LG would bring the generalization performance of PLM models closer to that of the pretrained GPT-2 model."
    }, {
      "heading" : "3.2.1 Relationship between Perplexity and Syntactic Generalization Performance",
      "text" : "We compare perplexity on the BLLIP held-out test set against syntactic generalization performance in Figure 4. Perplexities of PLM and PLM-mask models are computed setting the parse tree equal to the gold parse in Equation 3 to approximate the likelihood. Note that, unlike Hu et al. (2020), all\nour models use the same BPE vocabulary and word tokenization from GPT-2. The only exception are the additional parsing actions in the vocabulary y.\nFrom Figure 4, both perplexity and syntactic generalization performance improve with dataset size. However, for both training dataset sizes, we see that structural guidance can improve syntactic generalization. PLM models consistently perform better than vanilla models. While all models achieve very similar perplexity results after being trained on a specific dataset, their syntactic generalization performances differ dramatically."
    }, {
      "heading" : "3.2.2 Effect of Structural Guidance on Learning Specific Syntactic Structures",
      "text" : "In addition to comparing model’s aggregated performances, we also compare their generalization performances in the clustered subsets of tests in SG Test Suites and BLiMP-10%. These subsets consist of several related tests that target specific type of syntactic phenomenon, such as NPI licensing, subject-verb agreement, filler-gap dependencies, etc. We also include the results for the RNNGs taken from Hu et al. (2020).\nResults in Figure 5 show converging evidence that structural guidance in the form of generative\nparsing can robustly improve learning of subjectverb agreement and NPI licensing, and helps the model to better capture incremental processing phenomenon such as garden-path effects, but seems to slightly hurt the performance on gross syntactic state. While overall the RNNG shows a poor performance this is mostly due to its very low scores for licensing suites. Excluding these suites only the RNNG shows a performance close to the PLM model, even outperforming it clearly for the gross syntactic state suites. In this category and binding PLM variants seem inferior to all other models."
    }, {
      "heading" : "4 Related Work",
      "text" : "Multitask learning (Caruana, 1997) has been applied to a variety of NLP tasks with traditional modeling approaches (Miller et al., 2000; Sutton and McCallum, 2005; Sutton et al., 2007) as well as more recent neural models (Collobert et al., 2011; Li et al., 2020a). A recurring theme has been the use of structure in the form of syntactic trees to benefit other NLP tasks. Among the early works exploring this direction, Punyakanok et al. (2008) showed that syntactic parses can benefit Semantic Role Labeling (SRL). Poon and Domingos (2009) extended this idea to induce first-order logic representation in a unsupervised fashion, by clustering the dependency structures. In both cases syntax forms part of a pipeline and is not strictly supervision for the end task.\nThis trend continued with the rise of neural models. Collobert et al. (2011) improved deep convolution neural network for syntactic chunking models with additional POS supervision. Zhang and Weiss (2016); Søgaard and Goldberg (2016) observe the benefits of POS supervision at different depths of a neural network model with impact on dependency parsing, tagging and CCG super tagging performance. He et al. (2019) perform a syntax-based pruning of semantic roles, showing benefits in a multilingual setting. More recently, Sachan et al. (2020) incorporate a syntactic graph recurrent neural network into BERT models for better semantic role labeling. However, their method shows little or no benefit of syntax modeling for Named Entity Recognition and relation linking task. Neural machine translation (Chen et al., 2018) and text generation (Li et al., 2020a) have also been shown to benefit from syntactic modeling. In a recent work, Li et al. (2020b) use syntactic modeling in BERT based transformers to achieve performance gains\n0.05 0.15 0.25 0.35 0.45 0.55 0.65 A cc ur ac\ny Licensing (10 suites)\n0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75\nLong-Distance Dependencies (8 suites)\n0.3 0.4 0.5 0.6 0.7 0.8\nAgreement (3 suites)\nModel Performance on Specific Clusters of SG Test Suites\nRNNG LM ScLM-past ScLM-next PLM PLM-mask\non several text classification benchmarks. Other works have found that structural supervision in the form of intermediate fine-tuning (e.g., on CCG super tagging) is not helpful or even harmful (Pruksachatkun et al., 2020; Warstadt et al., 2019).\nThe focus of our work is on gauging the impact of joint modeling on syntactic generalization performance. In this direction, the work of Swayamdipta et al. (2018) is close to the scaffolding version of our model. They predict multiple labels, extracted from syntactic information, as auxiliary task and show positive effects on shallow semantic parsing and co-reference resolution. We use however a single feature, constituency parsing n-gram, which is closer to prior work relying on Part-of-Speech information. In addition, we explore impact of using preceding structure as feature vs postceding structure, which as shown plays a role in the learning process.\nIn terms of modeling objective and syntactic rep-\nresentations, our method is closest to the works of Choe and Charniak (2016); Dyer et al. (2016) that jointly model syntax and language. A more recent work from Peng et al. (2019) uses Rational Neural Networks language model that can derive binary unlabeled constituents from attention weights and can supervise the attention to attain a structural inductive bias. The proposed models show lower language modeling perplexity compared to their structure agnostic counterparts. We also extend here the idea of syntax-aware language modeling to transformer-based language models.\nFinally, our approach relates to the other works that propose ways of incorporating structural information into Transformer-based models. This includes the use of dependency or tree structure for constraining self-attention patterns (Strubell et al., 2018; Wang et al., 2019; Zhang et al., 2020), guiding cross-attention (Chen et al., 2018; Astudillo et al., 2020), modelling syntactic distance (Du et al.,\n2020), using syntactic information to guide the computation flow in the model (Shen et al., 2021), or through knowledge distillation (Kuncoro et al., 2020). Our structured masking in parsing as language modeling approach is close in spirit to the methods that modify attention mechanism according to syntactic connections (Astudillo et al., 2020); This work, however, primarily aims to study the impact of structural guidance on syntactic generalization. Therefore, we resort to simpler methods of incorporating structure to minimize the impact of modeling intricacies."
    }, {
      "heading" : "5 Conclusion",
      "text" : "Our work explores two forms of syntactic supervision as structural guidance for Transformer language models. Experiments suggest that generative parsing approach can effectively improve systematic generalization of learned syntactic knowledge in small training data regime, while a naive syntactic scaffold approach does not improve the baseline to the same extent despite reduced computation cost at inference time. Future work may explore alternative structural guidance strategies that combine the best of both approaches."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank the anonymous reviewers for their helpful comments. This work was supported by the MIT-IBM Watson AI Lab."
    } ],
    "references" : [ {
      "title" : "Transition-based parsing with stacktransformers",
      "author" : [ "Ramón Fernandez Astudillo", "Miguel Ballesteros", "Tahira Naseem", "Austin Blodgett", "Radu Florian." ],
      "venue" : "page 1001–1007.",
      "citeRegEx" : "Astudillo et al\\.,? 2020",
      "shortCiteRegEx" : "Astudillo et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are few-shot learners. arXiv preprint arXiv:2005.14165",
      "author" : [ "Tom B Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell" ],
      "venue" : null,
      "citeRegEx" : "Brown et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 2020
    }, {
      "title" : "Multitask learning",
      "author" : [ "Rich Caruana." ],
      "venue" : "Machine learning, 28(1):41–75.",
      "citeRegEx" : "Caruana.,? 1997",
      "shortCiteRegEx" : "Caruana.",
      "year" : 1997
    }, {
      "title" : "Bllip 1987-89 wsj corpus release 1",
      "author" : [ "Eugene Charniak", "Don Blaheta", "Niyu Ge", "Keith Hall", "John Hale", "Mark Johnson." ],
      "venue" : "Linguistic Data Consortium, Philadelphia, 36.",
      "citeRegEx" : "Charniak et al\\.,? 2000",
      "shortCiteRegEx" : "Charniak et al\\.",
      "year" : 2000
    }, {
      "title" : "Syntax-directed attention for neural machine translation",
      "author" : [ "Kehai Chen", "Rui Wang", "Masao Utiyama", "Eiichiro Sumita", "Tiejun Zhao." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Parsing as language modeling",
      "author" : [ "Do Kook Choe", "Eugene Charniak." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2331–2336, Austin, Texas. Association for Computational Linguistics.",
      "citeRegEx" : "Choe and Charniak.,? 2016",
      "shortCiteRegEx" : "Choe and Charniak.",
      "year" : 2016
    }, {
      "title" : "What does BERT look at? an analysis of BERT’s attention",
      "author" : [ "Kevin Clark", "Urvashi Khandelwal", "Omer Levy", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for",
      "citeRegEx" : "Clark et al\\.,? 2019",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa." ],
      "venue" : "Journal of machine learning research, 12(ARTICLE):2493–2537.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploiting syntactic structure for better language modeling: A syntactic distance approach",
      "author" : [ "Wenyu Du", "Zhouhan Lin", "Yikang Shen", "Timothy J. O’Donnell", "Yoshua Bengio", "Yue Zhang" ],
      "venue" : "In Proceedings of the 58th Annual Meeting of the Asso-",
      "citeRegEx" : "Du et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2020
    }, {
      "title" : "Recurrent neural network grammars",
      "author" : [ "Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Dyer et al\\.,? 2016",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2016
    }, {
      "title" : "An efficient context-free parsing algorithm",
      "author" : [ "Jay Earley." ],
      "venue" : "Communications of the ACM, 13(2):94– 102.",
      "citeRegEx" : "Earley.,? 1970",
      "shortCiteRegEx" : "Earley.",
      "year" : 1970
    }, {
      "title" : "Neural language models as psycholinguistic subjects: Representations of syntactic state",
      "author" : [ "Richard Futrell", "Ethan Wilcox", "Takashi Morita", "Peng Qian", "Miguel Ballesteros", "Roger Levy." ],
      "venue" : "Proceedings of the 2019 Conference of the North American",
      "citeRegEx" : "Futrell et al\\.,? 2019",
      "shortCiteRegEx" : "Futrell et al\\.",
      "year" : 2019
    }, {
      "title" : "Syntaxaware multilingual semantic role labeling",
      "author" : [ "Shexia He", "Zuchao Li", "Hai Zhao." ],
      "venue" : "arXiv preprint arXiv:1909.00310.",
      "citeRegEx" : "He et al\\.,? 2019",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2019
    }, {
      "title" : "A structural probe for finding syntax in word representations",
      "author" : [ "John Hewitt", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Hewitt and Manning.,? 2019",
      "shortCiteRegEx" : "Hewitt and Manning.",
      "year" : 2019
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "A systematic assessment of syntactic generalization in neural language models",
      "author" : [ "Jennifer Hu", "Jon Gauthier", "Peng Qian", "Ethan Wilcox", "Roger Levy." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "Constituency parsing with a self-attentive encoder",
      "author" : [ "Nikita Kitaev", "Dan Klein." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Melbourne, Australia. Association for Com-",
      "citeRegEx" : "Kitaev and Klein.,? 2018",
      "shortCiteRegEx" : "Kitaev and Klein.",
      "year" : 2018
    }, {
      "title" : "LSTMs can learn syntax-sensitive dependencies well, but modeling structure makes them better",
      "author" : [ "Adhiguna Kuncoro", "Chris Dyer", "John Hale", "Dani Yogatama", "Stephen Clark", "Phil Blunsom." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the As-",
      "citeRegEx" : "Kuncoro et al\\.,? 2018",
      "shortCiteRegEx" : "Kuncoro et al\\.",
      "year" : 2018
    }, {
      "title" : "Syntactic structure distillation pretraining for bidirectional encoders",
      "author" : [ "Adhiguna Kuncoro", "Lingpeng Kong", "Daniel Fried", "Dani Yogatama", "Laura Rimell", "Chris Dyer", "Phil Blunsom." ],
      "venue" : "Transactions of the Association for Computational Linguistics,",
      "citeRegEx" : "Kuncoro et al\\.,? 2020",
      "shortCiteRegEx" : "Kuncoro et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformer-based neural text generation with syntactic guidance",
      "author" : [ "Yinghao Li", "Rui Feng", "Isaac Rehg", "Chao Zhang." ],
      "venue" : "arXiv preprint arXiv:2010.01737.",
      "citeRegEx" : "Li et al\\.,? 2020a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving bert with syntax-aware local attention",
      "author" : [ "Zhongli Li", "Qingyu Zhou", "Chao Li", "Ke Xu", "Yunbo Cao." ],
      "venue" : "arXiv preprint arXiv:2012.15150.",
      "citeRegEx" : "Li et al\\.,? 2020b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Targeted syntactic evaluation of language models",
      "author" : [ "Rebecca Marvin", "Tal Linzen." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1192–1202, Brussels, Belgium. Association for Computational",
      "citeRegEx" : "Marvin and Linzen.,? 2018",
      "shortCiteRegEx" : "Marvin and Linzen.",
      "year" : 2018
    }, {
      "title" : "A novel use of statistical parsing to extract information from text",
      "author" : [ "Scott Miller", "Heidi Fox", "Lance Ramshaw", "Ralph Weischedel." ],
      "venue" : "1st Meeting of the North American Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Miller et al\\.,? 2000",
      "shortCiteRegEx" : "Miller et al\\.",
      "year" : 2000
    }, {
      "title" : "PaLM: A hybrid parser and language model",
      "author" : [ "Hao Peng", "Roy Schwartz", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Peng et al\\.,? 2019",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised semantic parsing",
      "author" : [ "Hoifung Poon", "Pedro Domingos." ],
      "venue" : "Proceedings of the 2009 conference on empirical methods in natural language processing, pages 1–10.",
      "citeRegEx" : "Poon and Domingos.,? 2009",
      "shortCiteRegEx" : "Poon and Domingos.",
      "year" : 2009
    }, {
      "title" : "Intermediate-task transfer learning with pretrained models for natural language",
      "author" : [ "Yada Pruksachatkun", "Jason Phang", "Haokun Liu", "Phu Mon Htut", "Xiaoyi Zhang", "Richard Yuanzhe Pang", "Clara Vania", "Katharina Kann", "Samuel R Bowman" ],
      "venue" : null,
      "citeRegEx" : "Pruksachatkun et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Pruksachatkun et al\\.",
      "year" : 2020
    }, {
      "title" : "The importance of syntactic parsing and inference in semantic role labeling",
      "author" : [ "Vasin Punyakanok", "Dan Roth", "Wen-tau Yih." ],
      "venue" : "Computational Linguistics, 34(2):257–287.",
      "citeRegEx" : "Punyakanok et al\\.,? 2008",
      "shortCiteRegEx" : "Punyakanok et al\\.",
      "year" : 2008
    }, {
      "title" : "Do syntax trees help pretrained transformers extract information? arXiv preprint arXiv:2008.09084",
      "author" : [ "Devendra Singh Sachan", "Yuhao Zhang", "Peng Qi", "William Hamilton" ],
      "venue" : null,
      "citeRegEx" : "Sachan et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Sachan et al\\.",
      "year" : 2020
    }, {
      "title" : "Explicitly modeling syntax in language models with incremental parsing and a dynamic oracle",
      "author" : [ "Yikang Shen", "Shawn Tan", "Alessandro Sordoni", "Siva Reddy", "Aaron Courville." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of",
      "citeRegEx" : "Shen et al\\.,? 2021",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2021
    }, {
      "title" : "Deep multitask learning with low level tasks supervised at lower layers",
      "author" : [ "Anders Søgaard", "Yoav Goldberg." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 231–235, Berlin,",
      "citeRegEx" : "Søgaard and Goldberg.,? 2016",
      "shortCiteRegEx" : "Søgaard and Goldberg.",
      "year" : 2016
    }, {
      "title" : "Effective inference for generative neural parsing",
      "author" : [ "Mitchell Stern", "Daniel Fried", "Dan Klein." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1695–1700, Copenhagen, Denmark. Association for",
      "citeRegEx" : "Stern et al\\.,? 2017",
      "shortCiteRegEx" : "Stern et al\\.",
      "year" : 2017
    }, {
      "title" : "Linguistically-informed self-attention for semantic role labeling",
      "author" : [ "Emma Strubell", "Patrick Verga", "Daniel Andor", "David Weiss", "Andrew McCallum." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Strubell et al\\.,? 2018",
      "shortCiteRegEx" : "Strubell et al\\.",
      "year" : 2018
    }, {
      "title" : "Joint parsing and semantic role labeling",
      "author" : [ "Charles Sutton", "Andrew McCallum." ],
      "venue" : "Technical report, MASSACHUSETTS UNIV AMHERST DEPT OF COMPUTER SCIENCE.",
      "citeRegEx" : "Sutton and McCallum.,? 2005",
      "shortCiteRegEx" : "Sutton and McCallum.",
      "year" : 2005
    }, {
      "title" : "Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data",
      "author" : [ "Charles Sutton", "Andrew McCallum", "Khashayar Rohanimanesh." ],
      "venue" : "Journal of Machine Learning Research, 8(3).",
      "citeRegEx" : "Sutton et al\\.,? 2007",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2007
    }, {
      "title" : "Syntactic scaffolds for semantic structures",
      "author" : [ "Swabha Swayamdipta", "Sam Thomson", "Kenton Lee", "Luke Zettlemoyer", "Chris Dyer", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Swayamdipta et al\\.,? 2018",
      "shortCiteRegEx" : "Swayamdipta et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30, pages 5998–6008. Cur-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Tree transformer: Integrating tree structures into self-attention",
      "author" : [ "Yaushian Wang", "Hung-Yi Lee", "Yun-Nung Chen." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Investigating bert’s knowledge of language: Five analysis methods with npis",
      "author" : [ "Alex Warstadt", "Yu Cao", "Ioana Grosu", "Wei Peng", "Hagen Blix", "Yining Nie", "Anna Alsop", "Shikha Bordia", "Haokun Liu", "Alicia Parrish" ],
      "venue" : null,
      "citeRegEx" : "Warstadt et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2019
    }, {
      "title" : "BLiMP: The benchmark of linguistic minimal pairs for English",
      "author" : [ "Alex Warstadt", "Alicia Parrish", "Haokun Liu", "Anhad Mohananey", "Wei Peng", "Sheng-Fu Wang", "Samuel R. Bowman." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:377–392.",
      "citeRegEx" : "Warstadt et al\\.,? 2020",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2020
    }, {
      "title" : "Structural supervision improves learning of non-local grammatical dependencies",
      "author" : [ "Ethan Wilcox", "Peng Qian", "Richard Futrell", "Miguel Ballesteros", "Roger Levy." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Wilcox et al\\.,? 2019",
      "shortCiteRegEx" : "Wilcox et al\\.",
      "year" : 2019
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Clara Ma", "Yacine Jernite", "Julien Plu", "Canwen Xu", "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Em-",
      "citeRegEx" : "Ma et al\\.,? 2020",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2020
    }, {
      "title" : "Stackpropagation: Improved representation learning for syntax",
      "author" : [ "Yuan Zhang", "David Weiss." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1557–1566, Berlin,",
      "citeRegEx" : "Zhang and Weiss.,? 2016",
      "shortCiteRegEx" : "Zhang and Weiss.",
      "year" : 2016
    }, {
      "title" : "Sg-net: Syntax guided transformer for language representation",
      "author" : [ "Zhuosheng Zhang", "Yuwei Wu", "Junru Zhou", "Sufeng Duan", "Hai Zhao", "Rui Wang." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 39,
      "context" : "These models achieve impressive perplexity results on language modelling datasets, perform well on grammatical judgments (Warstadt et al., 2020), and provide useful linguistic representations that benefit a wide range of downstream tasks.",
      "startOffset" : 121,
      "endOffset" : 144
    }, {
      "referenceID" : 14,
      "context" : "Probing analyses also suggest that these models learn to implicitly encode syntactic information (Hewitt and Manning, 2019; Clark et al., 2019) that may support better linguistic generalization than recurrent neural network architectures (RNNs).",
      "startOffset" : 97,
      "endOffset" : 143
    }, {
      "referenceID" : 6,
      "context" : "Probing analyses also suggest that these models learn to implicitly encode syntactic information (Hewitt and Manning, 2019; Clark et al., 2019) that may support better linguistic generalization than recurrent neural network architectures (RNNs).",
      "startOffset" : 97,
      "endOffset" : 143
    }, {
      "referenceID" : 36,
      "context" : "However, the Transformer architecture (Vaswani et al., 2017) is an interesting subject of study beyond its success in transfer-learning settings.",
      "startOffset" : 38,
      "endOffset" : 60
    }, {
      "referenceID" : 10,
      "context" : "The “generative parsing as language modeling” approach builds a Transformer-parameterized model to learn to predict actions that incrementally build constituency trees along with terminal words, following prior work on RNNs (Dyer et al., 2016; Choe and Charniak, 2016).",
      "startOffset" : 224,
      "endOffset" : 268
    }, {
      "referenceID" : 5,
      "context" : "The “generative parsing as language modeling” approach builds a Transformer-parameterized model to learn to predict actions that incrementally build constituency trees along with terminal words, following prior work on RNNs (Dyer et al., 2016; Choe and Charniak, 2016).",
      "startOffset" : 224,
      "endOffset" : 268
    }, {
      "referenceID" : 3,
      "context" : "We test these two approaches on two subsets of the BLLIP dataset (Charniak et al., 2000) and evaluate models’ syntactic generalization performances on SG Test Suites (Hu et al.",
      "startOffset" : 65,
      "endOffset" : 88
    }, {
      "referenceID" : 16,
      "context" : ", 2000) and evaluate models’ syntactic generalization performances on SG Test Suites (Hu et al., 2020) and a sampled subset of the BLiMP Benchmark (Warstadt et al.",
      "startOffset" : 85,
      "endOffset" : 102
    }, {
      "referenceID" : 39,
      "context" : ", 2020) and a sampled subset of the BLiMP Benchmark (Warstadt et al., 2020).",
      "startOffset" : 52,
      "endOffset" : 75
    }, {
      "referenceID" : 2,
      "context" : "As long as both tasks share some of the parameters they can be considered a case of multi-task learning (Caruana, 1997).",
      "startOffset" : 104,
      "endOffset" : 119
    }, {
      "referenceID" : 10,
      "context" : "Of interest here is the model proposed in Recurrent Neural Network Grammars (RNNGs; Dyer et al., 2016) and parsing as language model (LSTM-LM; Choe",
      "startOffset" : 76,
      "endOffset" : 102
    }, {
      "referenceID" : 11,
      "context" : "These transitions are similar to the well-established transition sets used for transition-based parsing (Earley, 1970) but adapted to generate both text and parse simultaneously.",
      "startOffset" : 104,
      "endOffset" : 118
    }, {
      "referenceID" : 15,
      "context" : "The LSTM-LM, however, uses a flat parametrization treating the transitions as a sequence in a conventional language model learnt with an LSTM (Hochreiter and Schmidhuber, 1997).",
      "startOffset" : 142,
      "endOffset" : 176
    }, {
      "referenceID" : 10,
      "context" : "The original RNNG work (Dyer et al., 2016) proposes an approximate solution based on importance sampling.",
      "startOffset" : 23,
      "endOffset" : 42
    }, {
      "referenceID" : 36,
      "context" : "In this work we propose to parametrize Eq 2 with a Transformer language model (Vaswani et al., 2017).",
      "startOffset" : 78,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "In an attempt to capture a similar effect in the Transformer, we explore here the idea of masking some attention heads to reflect the parser state as in the stack-Transformer (Astudillo et al., 2020).",
      "startOffset" : 175,
      "endOffset" : 199
    }, {
      "referenceID" : 3,
      "context" : "As our goal here is to study whether structural guidance helps models learn robust humanlike generalization of syntactic knowledge, we train our model on the BLLIP dataset (Charniak et al., 2000), an English newswire style corpus used in Hu et al.",
      "startOffset" : 172,
      "endOffset" : 195
    }, {
      "referenceID" : 17,
      "context" : "We train the proposed models and the baseline vanilla Transformer language models on BLLIP-MD, a 14 million-token corpus, and BLLIP-LG, a 46 million-token corpus, both of which are auto-parsed using a state-of-theart constituency parser (Kitaev and Klein, 2018).",
      "startOffset" : 237,
      "endOffset" : 261
    }, {
      "referenceID" : 22,
      "context" : "To assess whether a trained model systematically generalizes its syntactic knowledge, we employ targeted syntactic evaluation paradigm (Marvin and Linzen, 2018).",
      "startOffset" : 135,
      "endOffset" : 160
    }, {
      "referenceID" : 31,
      "context" : "(2019) applying word-synchronous beam search (Stern et al., 2017) to find a list Yk of k incremental parses given a sentence prefix w<t.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 23,
      "context" : "plied to a variety of NLP tasks with traditional modeling approaches (Miller et al., 2000; Sutton and McCallum, 2005; Sutton et al., 2007) as well as more recent neural models (Collobert et al.",
      "startOffset" : 69,
      "endOffset" : 138
    }, {
      "referenceID" : 33,
      "context" : "plied to a variety of NLP tasks with traditional modeling approaches (Miller et al., 2000; Sutton and McCallum, 2005; Sutton et al., 2007) as well as more recent neural models (Collobert et al.",
      "startOffset" : 69,
      "endOffset" : 138
    }, {
      "referenceID" : 34,
      "context" : "plied to a variety of NLP tasks with traditional modeling approaches (Miller et al., 2000; Sutton and McCallum, 2005; Sutton et al., 2007) as well as more recent neural models (Collobert et al.",
      "startOffset" : 69,
      "endOffset" : 138
    }, {
      "referenceID" : 7,
      "context" : ", 2007) as well as more recent neural models (Collobert et al., 2011; Li et al., 2020a).",
      "startOffset" : 45,
      "endOffset" : 87
    }, {
      "referenceID" : 20,
      "context" : ", 2007) as well as more recent neural models (Collobert et al., 2011; Li et al., 2020a).",
      "startOffset" : 45,
      "endOffset" : 87
    }, {
      "referenceID" : 4,
      "context" : "Neural machine translation (Chen et al., 2018) and text generation (Li et al.",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 20,
      "context" : ", 2018) and text generation (Li et al., 2020a) have also been shown to benefit from syntactic modeling.",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 26,
      "context" : ", on CCG super tagging) is not helpful or even harmful (Pruksachatkun et al., 2020; Warstadt et al., 2019).",
      "startOffset" : 55,
      "endOffset" : 106
    }, {
      "referenceID" : 38,
      "context" : ", on CCG super tagging) is not helpful or even harmful (Pruksachatkun et al., 2020; Warstadt et al., 2019).",
      "startOffset" : 55,
      "endOffset" : 106
    }, {
      "referenceID" : 32,
      "context" : "This includes the use of dependency or tree structure for constraining self-attention patterns (Strubell et al., 2018; Wang et al., 2019; Zhang et al., 2020), guiding cross-attention (Chen et al.",
      "startOffset" : 95,
      "endOffset" : 157
    }, {
      "referenceID" : 37,
      "context" : "This includes the use of dependency or tree structure for constraining self-attention patterns (Strubell et al., 2018; Wang et al., 2019; Zhang et al., 2020), guiding cross-attention (Chen et al.",
      "startOffset" : 95,
      "endOffset" : 157
    }, {
      "referenceID" : 43,
      "context" : "This includes the use of dependency or tree structure for constraining self-attention patterns (Strubell et al., 2018; Wang et al., 2019; Zhang et al., 2020), guiding cross-attention (Chen et al.",
      "startOffset" : 95,
      "endOffset" : 157
    }, {
      "referenceID" : 4,
      "context" : ", 2020), guiding cross-attention (Chen et al., 2018; Astudillo et al., 2020), modelling syntactic distance (Du et al.",
      "startOffset" : 33,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : ", 2020), guiding cross-attention (Chen et al., 2018; Astudillo et al., 2020), modelling syntactic distance (Du et al.",
      "startOffset" : 33,
      "endOffset" : 76
    }, {
      "referenceID" : 29,
      "context" : "2020), using syntactic information to guide the computation flow in the model (Shen et al., 2021), or through knowledge distillation (Kuncoro et al.",
      "startOffset" : 78,
      "endOffset" : 97
    }, {
      "referenceID" : 19,
      "context" : ", 2021), or through knowledge distillation (Kuncoro et al., 2020).",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "Our structured masking in parsing as language modeling approach is close in spirit to the methods that modify attention mechanism according to syntactic connections (Astudillo et al., 2020); This work, however, primarily aims to study the impact of structural guidance on syntactic generalization.",
      "startOffset" : 165,
      "endOffset" : 189
    } ],
    "year" : 2021,
    "abstractText" : "Transformer-based language models pretrained on large amounts of text data have proven remarkably successful in learning generic transferable linguistic representations. Here we study whether structural guidance leads to more human-like systematic linguistic generalization in Transformer language models without resorting to pre-training on very large amounts of data. We explore two general ideas. The “Generative Parsing” idea jointly models the incremental parse and word sequence as part of the same sequence modeling task. The “Structural Scaffold” idea guides the language model’s representation via additional structure loss that separately predicts the incremental constituency parse. We train the proposed models along with a vanilla Transformer language model baseline on a 14 million-token and a 46 million-token subset of the BLLIP dataset, and evaluate models’ syntactic generalization performances on SG Test Suites and sized BLiMP. Experiment results across two benchmarks suggest converging evidence that generative structural supervisions can induce more robust and humanlike linguistic generalization in Transformer language models without the need for data intensive pre-training.",
    "creator" : "LaTeX with hyperref"
  }
}