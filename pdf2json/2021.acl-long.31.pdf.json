{
  "name" : "2021.acl-long.31.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Deep Differential Amplifier for Extractive Summarization",
    "authors" : [ "Ruipeng Jia", "Yanan Cao", "Fang Fang", "Yuchen Zhou", "Zheng Fang", "Yanbing Liu", "Shi Wang" ],
    "emails" : [ "liuyanbing}@iie.ac.cn", "wangshi@ict.ac.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 366–376\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n366"
    }, {
      "heading" : "1 Introduction",
      "text" : "Single-document extractive summarization forms summary by copying and concatenating the most important spans (usually sentences) in a document. Sentence-level summarization is a very challenging task, because it arguably requires an in-depth understanding of the source document sentences, and current automatic solutions are still far from human performance. Recent approaches frame the task as a sequence labeling problem, taking advantage of the success of neural network architectures.\n∗Corresponding authors: Fang Fang and Shi Wang\nHowever, there are still two inherent obstacles for sentence-level extractive summarization:\n1) It should be detrimental to keep tangential information (West et al., 2019). The intuitive limitation of those approaches is that they always prefer to model and retain all informative content from the source document. This goes against the fundamental goal of summarization, which crucially needs to forget all but the “pivotal” information. Recently, the Information Bottleneck principle (Tishby et al., 2000; West et al., 2019) is introduced to incorporate a tradeoff between information selection and pruning. Length penalty and the topic loss (Baziotis et al., 2019) are used in the autoencoding system to augment the reconstruction loss. However, these methods require external variables or augmentative terms, without enhancing the representation of pivotal information.\n2) Imbalanced classes inherently result in models that have poor predictive performance, specifically for the minority class. The distribution of examples across the known classes can vary from a slight bias to a severe imbalance, where there is one example in the minority class for dozens of examples in the majority class. For instance, according to the statistics on the popular summarization dataset, only 7.33% sentences of\nCNN/DM (Hermann et al., 2015) are labeled as “1” and others are “0”, indicating whether this sentence should be selected as summary or not. Conversely, most machine learning algorithms for classification predictive models are designed and demonstrated on problems that assume an equal distribution of classes. This means that a naive application of a model may only focus on learning the characteristics of the abundant observations, neglecting the examples from the minority class. Furthermore, as shown in Figure 1, the ROUGE score gradually declines along with the number of sentences accumulating, since the valuable summary sentences is generally a tiny minority (with the quantity of 1-4), while more and more majority sentences will swamp the minority ones. Unfortunately, the imbalance in summarization is inherent, which can’t be addressed by common data augmentation (He and Ma, 2013; Asai and Hajishirzi, 2020; Min et al., 2020; Zoph et al., 2019; Xie et al., 2020), for there is a rare influence on the 0/1 distribution by adding or deleting the entire document.\nThese two obstacles are interrelated and interact with each other. Highlighting the pivotal information will strengthen the unique semantic and weaken the common informative content. Additionally, a more balanced distribution would make minority class more attractive. If we can’t resolve the category imbalance problem in extractive summarization by data augmentation, how to make the minority class more attractive? Inspired by the differential amplifier of analog electronics1, we propose a heuristic model, DifferSum, as shorthand for Differential Amplifier for Extractive Summarization to enhance the representation of the summary sentences. Specifically, we calculate and amplify the semantic difference between each sentence and other sentences, by the subtraction operation. The original differential amplifier consists of two terms and the second term is used to avoid making the final output zero. In our model, we use the residual unit instead of the second term to make the architecture deeper. We further design a more appropriate objective function to avoid biasing the data, by making the loss of a minority much greater than the majority. DifferSum shows superiority over other extractive methods in two aspects: 1) enhancing the representation of the pivotal information and 2) compensating the minority class and penalizing the majority ones.\n1https://en.wikipedia.org/wiki/Differential amplifier\nExperimental results validate the effectiveness of DifferSum. The human evaluation also shows that our model is better in relevance compared with others. Our contributions in this work are concluded as follows:\n• We propose a novel conceptualization of extractive summarization as rebalance problem.\n• We introduce a heuristic approach, calculating and amplifying the semantic representation of pivotal information by integrating both the differential amplifier and residual learning.\n• Our proposed framework has achieved superior performance compared with strong baselines."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Extractive Summarization",
      "text" : "Recent research work on extractive summarization spans a large range of approaches. These works usually instantiate their encoder-decoder architecture by choosing RNN (Nallapati et al., 2017; Zhou et al., 2018), Transformer (Wang et al., 2019; Zhong et al., 2019b; Liu and Lapata, 2019; Zhang et al., 2019b) or GNN (Wang et al., 2020; Jia et al., 2020b) as encoder, autoregressive (Jadhav and Rajan, 2018; Liu and Lapata, 2019) or RL-based (Narayan et al., 2018; Arumae and Liu, 2018; Luo et al., 2019) decoders. For two-stage summarization, Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. (2019), Xu and Durrett (2019) and Mendes et al. (2019) focus on the extract-then-compress learning paradigm, which will first train an extractor for content selection. Zhong et al. (2020) introduces extract-thenmatch framework, which employs BERTSUMEXT (Liu and Lapata, 2019) as first-stage to prune unnecessary information. However, these above extractive approaches prefer to model all source informative context and they pay little attention to the imbalance problem."
    }, {
      "heading" : "2.2 Deep Residual Learning",
      "text" : "The original deep residual learning is introduced in image recognition (He et al., 2016a) for the notorious degradation problem. Then, residual is introduced to the natural language process by Transformer (Vaswani et al., 2017). Essentially, we cannot determine the depth of the network very well\nwhen building a deep network. There will be optimal layers in the network, and outside the optimal layer is the redundant layer. We expect the redundant layer to correspond to the input and output, namely identity mapping (He et al., 2016a,b; Veit et al., 2016; Balduzzi et al., 2018). Resnet (He et al., 2016a) addresses the degradation problem by introducing a deep residual learning framework. If an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers (Huang and Wang, 2017). In this paper, the residual unit serves as the second item of the differential amplifier to keep our architecture deep enough and capture pivotal information."
    }, {
      "heading" : "3 Methodology",
      "text" : ""
    }, {
      "heading" : "3.1 Problem Definition",
      "text" : "We model the sentence extraction task as a sequence tagging problem (Kedzie et al., 2018). Given a documentD consisting of a sequence ofM sentences [s1, s2, ..., sM ] and a sentence si consisting of a sequence of N words [wi1, wi2, ..., wiN ]. We denote by hi and hij the embedding of sentences and words in a continuous space. The extractive summarizer aims to produce a summary S by selecting m sentences from D (where m ≤M ). For each sentence si ∈ D, there is ground-truth yi ∈ {0, 1} and we will predict a label ŷi ∈ {0, 1}, where 1 means that si should be included in the summary. We assign a score p(ŷi|si, D, θ) to quantify si’s relevance to the summary, where θ is the parameters of neural network model. Finally, we assemble a summary S by selecting m sentences,\naccording to the probability of p(1|si, D, θ)."
    }, {
      "heading" : "3.2 Sentence Encoder",
      "text" : "The sentence encoder in extractive summarization models is usually a recurrent neural network with Long-Short Term Memory (Hochreiter and Schmidhuber, 1997) or Gated Recurrent Units (Cho et al., 2014). In this paper, our sentence encoder builds on the BERT architecture (Devlin et al., 2019), a recently proposed highly efficient model which is based on the deep bidirectional Transformer (Vaswani et al., 2017) and has achieved state-ofthe-art performance in many NLP tasks. The Transformer aims at reducing the fundamental constraint of sequential computation which underlies most architecture (Liu et al., 2019). It eliminates recurrence in favor of applying a self-attention mechanism which directly models relationships between all words in a sentence.\nOur extractive model is composed of a sentencelevel Transformer (TS) and a document-level Transformer (TD) (Liu et al., 2019). For each sentence si in the input document, TS is applied to obtain a contextual representation for each word:\n[u11, u12, ..., uMN ] = TS([w11, w12, ..., wMN ]) (1)\nAnd the representation of a sentence is acquired by applying weighted-pooling:\naij = W0u T ij\nsi = 1\nN N∑ j=1 aijuij (2)\nDocument-level transformer TD takes si as input and yields a contextual representation for each sentence:\n[v1, v2, ..., vM ] = TD([s1, s2, ..., sM ]) (3)"
    }, {
      "heading" : "3.3 Deep Differential Amplifier",
      "text" : "In the Transformer model sketched above, intersentence relations are modeled by multi-head attention based on softmax functions, which only capture shallow structural information (Liu et al., 2019).\nA differential amplifier is a type of electronic amplifier that amplifies the difference between two input voltages but suppresses any voltage common\nto the two inputs. The output of an ideal differential amplifier is given by:\nVout = Ad(V + in − V − in ) (4)\nwhere V +in and V − in are the input voltage; Ad is the differential-mode gain. In practice, the gain should not be quite equal for the two inputs, V +in and V − in . For instance, even if V +in and V − in are equal, the output Vout should not be zero. So, modern differential amplifiers are usually implemented with a more realistic expression, which includes a second term:\nVout = Ad(V + in − V − in ) +Ac\nV +in + V − in\n2 (5)\nwhere Ac is called the common-mode gain of the amplifier.\nInspired by the differential amplifier above, we calculate and amplify the semantic difference between each sentence and other sentences by the subtraction operation of the sentence representations [v1, v2, ..., vM ]. Particularly, for sentence si, V +in and V − in are calculated as follows:\nV +in = vi\nV −in =\n∑ j∈{1,2,...,M}\\{i} vj\nM − 1 (6)\nThe original differential amplifier consists of two terms and the second one avoids making the final output zero. While for the deep neural network: 1) inputs of the differential amplifier are vector instances in the high dimensional space, which is practically impossible for the zero output, compared with scalar; 2) the second term of the differential amplifier is not suitable for the deep iterative architecture, since it is exposed to the degradation problem.\nNotably, residual learning is introduced in deep learning as shortcut connections to skip one or more layers, which is naturally an alternative to the second item of the differential amplifier. The advantages of this method are: 1) the residual architecture will highlight the pivotal information as well as reserving the original sentence representation; 2) it is easier to optimize the residual mapping than to optimize the original (He et al., 2016a). Hence, the residual unit is employed as the second item, along with an iterative refinement algorithm to enhance the final representation of sentences."
    }, {
      "heading" : "3.4 Residual Representation for Sentence",
      "text" : "The differential amplifier in our architecture consists of a few stacked layers to iteratively refine the pivotal representation. Let us considerH(x) as an underlying mapping to be fit, with x denoting the inputs to the first of these layers. Since multiple nonlinear layers can asymptotically approximate complicated functions (He et al., 2016a; Montúfar et al., 2014), the differential amplifier mapping H(x) is recast into a residual mapping F(x) and an identity mapping x:\nH(x) = F(x) + x (7)\nObviously, residual learning is just a variant of the differential amplifier:\nH(x) := Vout F(x) := Ad(V +in − V − in )\n(8)\nwhere the output voltage Vout thus becomes the original mapping H(x) and the first item of amplifier Ad(V +in − V − in ) equals to residual mapping F(x), In our model, the second item of the differential amplifier is replaced by the identity mapping x, which is the shortcut connection and the output is added to the outputs of F(x). Furthermore, 1) the identity shortcut connections advance the architecture without extra parameter; 2) the identity shortcut doesn’t add the computational complexity (He et al., 2016a);\nThus, for sentence respresentation vi, the deep differential amplifier is:\nH(vi) = Ad(vi− ∑ j∈{1,2,...,M}\\{i} vj\nM − 1 )+vi (9)"
    }, {
      "heading" : "3.5 Iterative Structure Refinement",
      "text" : "The differential amplifier and residual unit specialize in modeling the pivotal information, while deeper neural networks with more parameters are able to infer semantic more accurately. So, an iterative refinement algorithm is introduced to enhance the final representation of pivotal information. For sentence vi, the fundamental iterative unit is:\nH(vi) = F(vi) + vi vi = H(vi)\n(10)\nwhere we iteratively refine the representation vi for K times; and thanks to the built-in residual mechanism, most shorter paths are needed during training, as longer paths do not contribute any gradient.\nAlong with the supervision, each iteration will pay more attention to the key semantic difference F(vi) of sentences with label 1, while trying to zero other F(vj). Conversely, previous extractive approaches without differential amplifier can only classify those sentences by compensating or penalizing vi / vj , which is more difficult to model.\nFollowing previous work (Nallapati et al., 2017; Liu et al., 2019), we use a sigmoid function after a linear transformation to calculate the probability ri of selecting si as a summary sentence:\nri = sigmoid(W1v T i ) (11)"
    }, {
      "heading" : "3.6 Weighted Objective Function",
      "text" : "To rebalance the bias of minority 1-class and majority 0-class, we have built a deep differential amplifier to amplify and capture the unique information for summary sentences. Besides, another heuristic method is to make our model pay more attention to 1-class: a weighted cross-entropy function.\nParticularly, we further design a more appropriate objective function to avoid biasing the data, by making the loss of a minority much greater than the majority. The weight we employed is to rebalance the observations for each class, so the sum of observations for each class are equal. Finally, we define the model’s loss function as the summation of the losses of all iterations:\nL = K∑ k=1\n{ 1\nM M∑ i=1 [∑ sj∈D I(sj /∈ S)∑ sj∈D I(sj ∈ S) y log(rki )\n+(1− y) log(1− rki ) ]} (12)\nwhere I(·) is an indicator function and K is the number of iterations."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "As shown in Table 1, we employ two datasets widely-used with multiple sentences summary: CNN and Dailymail (CNN/DM) (Hermann et al., 2015) and New York Times (NYT) (Sandhaus, 2008).\nCNN/DM We used the standard split (Hermann et al., 2015) for training, validation, and test (90,266/1,220/1,093 for CNN and 196,96/12.148/10,397 for Daily Mail), with splitting sentences by Stanford CoreNLP (Manning et al., 2014) toolkit and pre-processing the dataset following (See et al., 2017) and (Zhong et al., 2020). This dataset contains news articles and several associated abstractive highlights. We use the unanonymized version as in previous summarization work and each document is truncated to 800 BPE tokens.\nNYT Following previous work (Zhang et al., 2019b; Xu and Durrett, 2019), we use 137,778, 17,222 and 17,223 samples for training, validation, and test, respectively. We also followed their filtering procedure, documents with summaries less than 50 words were removed from the dataset. Sentences were split with the Stanford CoreNLP toolkit (Manning et al., 2014). Input documents were truncated to 800 BPE tokens too."
    }, {
      "heading" : "4.2 Parameters",
      "text" : "Our code is based on Pytorch (Paszke et al., 2019) and the pre-trained model employed in DifferSum is ‘albert-xxlarge-v2’, which is based on the huggingface/transformers2. We train DifferSum two days for 100,000 steps on 2GPUs(Nvidia Tesla V100, 32GB) with gradient accumulation every two steps. Adam with β1 = 0.9, β2 = 0.999 is used as optimizer. Learning rate schedule follows the strategy with warming-up on first 10,000 steps. We have tried the iteration steps of 2/4/6/8 for iterative refinement, and K = 4 is the best choice based on the validation set. We select the top-3 checkpoints based on the evaluation loss on the validation set, and report the averaged results on the test set.\nFollowing Jia et al. (2020a) and Jia et al. (2021), we employ the greedy algorithm for the sentencelevel soft labels, which falls under the umbrella\n2https://github.com/huggingface/transformers\nof subset selection. Besides, we employ the Trigram Blocking strategy for decoding, which is a simple but powerful version of Maximal Marginal Relevance (Carbonell and Goldstein, 1998). Specifically, when predicting summaries for a new document, we first use the model to obtain the probability score p(1|si, D, θ) for each sentence, and then we rank sentences by their scores and discard those which have trigram overlappings with their predecessors."
    }, {
      "heading" : "4.3 Metric",
      "text" : "ROUGE (Lin, 2004) is the standard metric for evaluating the quality of summaries. We report the ROUGE-1, ROUGE-2, and ROUGE-L of DifferSum by ROUGE-1.5.5.pl, which calculates the overlap lexical units of extracted sentences and ground-truth."
    }, {
      "heading" : "5 Results and Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Results on CNN/DM",
      "text" : "Table 2 shows the results on CNN/DailyMail. All of these scores are in accordance with original papers. Following Nallapati et al. (2017); Liu and Lapata (2019), we compare extractive summariza-\ntion models against abstractive models, and it is certainly that the abstractive paradigm is still on the frontier of summarization. The first part of extractive approaches is the Lead-3 baseline and Oracle upper bound, while the second part includes other extractive summarization models. We present our models finally at the bottom. It is obvious that our DifferSum outperforms all extractive baseline models. Compared with large version BERTSUMEXT, our DifferSum achieves 0.85/1.02/0.93 improvements on R-1, R-2, and R-L, which indicates the pivotal information captured by the differential amplifier is more powerful than the other structures. Compared with early approaches, especially for BERTSUMEXT, we observe that BERT outperforms all previous non-BERT-based summarization systems, and Trigram-Blocking leads to a great improvement on all ROUGE metrics. MATCHSUM is a comparable competitor to our DifferSum, which formulates the extractive summarization task as a two-step problem and extract-thenmatch summary based on a well-trained BERTSUMEXT. Therefore, we only train a large version DifferSum for a fair comparison."
    }, {
      "heading" : "5.2 Results on NYT",
      "text" : "Results on NYT are summarized in Table 3. Note that we use limited-length ROUGE recall as Durrett et al. (2016), where the selected sentences are truncated to the length of the human-written summaries. The parts of Table 3 is similar to Table 2. The first four lines are abstractive models, and the next two lines are our golden baselines for extrac-\ntive summarization. The third part reports the performance of other extractive works and our model respectively. Again, we observe that our differential amplifier modeling performs better than both LSTM and BERT. Meanwhile, we find that extractive approaches show superiority over abstractive models, and the ROUGE scores are higher than CNN/DailyMail."
    }, {
      "heading" : "5.3 Ablation Studies",
      "text" : "We propose several strategies to improve the performance of extractive summarization, including differential amplifier (vs. normal residual network), pre-trained ALBERT(vs. BERT), and iterative refinement (vs. None). To investigate the influence of these factors, we conduct experiments and list the results in Table 4. Significantly, 1) differential amplifier is more critical than ALBERT, for the reason that the pivotal information is essential and difficult for ALBERT to model; 2) iterative refinement mechanism enlarges the advantage of the differential amplifier, demonstrating the superiority of deep architecture."
    }, {
      "heading" : "5.4 Human Evaluation for Summarization",
      "text" : "It is not enough to only rely on the ROUGE evaluation for a summarization system, although the ROUGE correlates well with human judgments (Owczarzak et al., 2012). Therefore, we design an experiment based on a ranking method to evaluate the performance of DifferSum by humans. Following Cheng and Lapata (2016), Narayan et al. (2018) and Zhang et al. (2019b), firstly, we randomly select 40 samples from CNN/DM test set. Then the human participants are presented with one original document and a list of corresponding summaries produced by different model systems. Participants are requested to rank these summaries (ties allowed) by taking informativeness (Can the summary capture the important information from the document) and fluency (Is the summary grammatical) into account. Each document is annotated by three different participants separately.\nThe input article and ground truth summaries are\nalso shown to the human participants in addition to the three model summaries (SummaRuNNer, BERTSUMEXT, and DifferSum). From the results shown in Table 5, it is obvious that DifferSum is better in relevance compared with others."
    }, {
      "heading" : "5.5 Trigram Blocking Strategy",
      "text" : "Trigram Blocking leads to a great improvement on all ROUGE metrics for many extractive approaches (Liu and Lapata, 2019; Wang et al., 2020). It is has become a fundamental module in extractive summarization. In this paper, DifferSum extracts summary sentences with the Trigram-Blocking algorithm, but whether there is a great improvement along with it, like in SummaRuNNer or BERTSUMEXT?\nIt has been explained by Nallapati et al. (2017); Liu and Lapata (2019), that picking all sentences by comparing the predicted probability with a threshold may not be an optimal strategy since the training data is very imbalanced in terms of summarymembership of sentences. Therefore, the TrigramBlocking algorithm is introduced to select top-k sentences and reduce the redundancy.\nCoincidentally, our DifferSum is designed to 1) rebalance the distribution of majority and minority and 2) filter the tangential and redundant information. Thus, the Trigram-Blocking algorithm may be useless for our DifferSum.\nTable 6 further summarizes the performance gain of Trigram-Blocking strategy. It is obvious that this strategy is essential for BERTSUMEXT or SummaRuNNer, achieving more than 2.68 / 0.98 improvements on R-1 separately, for that there is no enough redundancy modeling for both of them. While on the other hand, the efficiency of the Trigram-Blocking strategy is weak for DifferSum."
    }, {
      "heading" : "5.6 Documents with a Different Number of Sentences",
      "text" : "In this paper, we emphasize the inherent imbalance problem of the majority 0-class and the minority 1-class. In fact, in CNN/DailyMail dataset, there are plenty of documents with a different num-\nber of sentences, ranging from 3-sentences to 100- sentences. While the number of summary sentences, labeled with 1, is from 1-sentences to 5- sentences, and the average number of sentences labeled 1 in CNN/DailyMail is only 7.33%. What is worse is that the distribution of the number of sentences for documents is a uniform distribution, thus we could not avoid the imbalance by cleaning the data.\nIn this paper, we design another experiment to analysis the harmful effect of imbalance classes. We train the BERTSUMEXT (12-layers) from scratch on CNN/DailyMail, and evaluate the model on the test set to check the tendency of ROUGE scores, along with the number of sentences accumulating. The result is shown in the line chart of Figure 1 and Figure 3a, and obviously we only pay attention to the document in which the number of sentences less than 55. Specifically, each document is truncated to 2000 BPE tokens to involve more sentences, but this can not cover those whole documents with more than 55-sentences. Therefore, we choose to calculate the ROUGE scores for documents with sentences from 3 to 55.\nFor comparison, we train our DifferSum (12- layers) from scratch, and each document is truncated to 2000 BPE tokens too. The tendency of our DifferSum is as Figure 3b. Compared with the tendency of BERTSUMEXT, there is no obvious ROUGE decrease, demonstrating that our approach has strengthened the representation of pivotal and\nrebalanced the disproportionate ratio of summary sentences and other sentences.\nNote that more truncated BPE tokens will increase the final average ROUGE slightly, for it may lose some summary sentences when truncating too many tokens. Unfortunately, our 24-layers DifferSum can only be trained with 800 BPE tokens for the limitation of GPU source."
    }, {
      "heading" : "5.7 Map Words Representation into Sentence Representation",
      "text" : "A key issue motivating the sentence-level Transformer (TS) and the document-level Transformer (TD) is that the features for words after the TS might be at different scales or magnitudes. This can be due to some words having very sharp or very distributed attention weights when summing over the features of the other words.\nIn this paper, we apply two ways to map the words representation into its sentence representation: weighted-pooling at Equation 2 and picking [CLS] token as sentence (Liu and Lapata, 2019).\nTable 7 shows that [CLS] is not enough to convey enough informative information of words for both our DifferSum and BERTSUMEXT. Especially, DifferSum is more sensitive to the word features since our differential amplifier may amplify the semantic features effectively."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we introduce a heuristic model, DifferSum, 1) to calculate and amplifier the pivotal information and 2) to rebalance the distribution of minority 1-class and majority 0-class. Besides, we employ another weighted cross-entropy function to compensate for the imbalance. Experimental results show that our method significantly outperforms previous models. In the future, we would like to generalize DifferSum to other fields."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This research is supported by the National Key Research and Development Program of China\n(NO.2017YFC0820700) and National Natural Science Foundation of China (No.61902394). We thank all authors for their contributions and all anonymous reviewers for their constructive comments."
    } ],
    "references" : [ {
      "title" : "Reinforced extractive summarization with question-focused rewards",
      "author" : [ "Kristjan Arumae", "Fei Liu." ],
      "venue" : "ACL, pages 105–111.",
      "citeRegEx" : "Arumae and Liu.,? 2018",
      "shortCiteRegEx" : "Arumae and Liu.",
      "year" : 2018
    }, {
      "title" : "Logicguided data augmentation and regularization for consistent question answering",
      "author" : [ "Akari Asai", "Hannaneh Hajishirzi." ],
      "venue" : "ACL, pages 5642– 5650.",
      "citeRegEx" : "Asai and Hajishirzi.,? 2020",
      "shortCiteRegEx" : "Asai and Hajishirzi.",
      "year" : 2020
    }, {
      "title" : "Summary level training of sentence rewriting for abstractive summarization",
      "author" : [ "Sanghwan Bae", "Taeuk Kim", "Jihoon Kim", "Sang goo Lee." ],
      "venue" : "arXiv preprint arXiv:1909.08752.",
      "citeRegEx" : "Bae et al\\.,? 2019",
      "shortCiteRegEx" : "Bae et al\\.",
      "year" : 2019
    }, {
      "title" : "The shattered gradients problem: If resnets are the answer, then what is the question",
      "author" : [ "David Balduzzi", "Marcus Frean", "Lennox Leary", "JP Lewis", "Kurt Wan-Duo Ma", "Brian McWilliams" ],
      "venue" : null,
      "citeRegEx" : "Balduzzi et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Balduzzi et al\\.",
      "year" : 2018
    }, {
      "title" : "Seq3: Differentiable sequence-to-sequence-to-sequence autoencoder for unsupervised abstractive sentence compression",
      "author" : [ "Christos Baziotis", "Ion Androutsopoulos", "Ioannis Konstas", "Alexandros Potamianos." ],
      "venue" : "NAACL-HLT, pages 673–681.",
      "citeRegEx" : "Baziotis et al\\.,? 2019",
      "shortCiteRegEx" : "Baziotis et al\\.",
      "year" : 2019
    }, {
      "title" : "Aredsum: Adaptive redundancy-aware iterative sentence ranking for extractive document summarization",
      "author" : [ "Keping Bi", "Rahul Jha", "W. Bruce Croft", "Asli Celikyilmaz" ],
      "venue" : null,
      "citeRegEx" : "Bi et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Bi et al\\.",
      "year" : 2020
    }, {
      "title" : "The use of mmr, diversity-based reranking for reordering documents and producing summaries",
      "author" : [ "Jaime Carbonell", "Jade Goldstein." ],
      "venue" : "SIGIR, pages 209–210.",
      "citeRegEx" : "Carbonell and Goldstein.,? 1998",
      "shortCiteRegEx" : "Carbonell and Goldstein.",
      "year" : 1998
    }, {
      "title" : "Fast abstractive summarization with reinforce-selected sentence rewriting",
      "author" : [ "Yen-Chun Chen", "Mohit Bansal." ],
      "venue" : "ACL, pages 675–686.",
      "citeRegEx" : "Chen and Bansal.,? 2018",
      "shortCiteRegEx" : "Chen and Bansal.",
      "year" : 2018
    }, {
      "title" : "Neural summarization by extracting sentences and words",
      "author" : [ "Jianpeng Cheng", "Mirella Lapata." ],
      "venue" : "ACL.",
      "citeRegEx" : "Cheng and Lapata.,? 2016",
      "shortCiteRegEx" : "Cheng and Lapata.",
      "year" : 2016
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "EMNLP, pages",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL-HLT, pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning-based single-document summarization with compression and anaphoricity constraints",
      "author" : [ "Greg Durrett", "Taylor Berg-Kirkpatrick", "Dan Klein." ],
      "venue" : "arXiv preprint arXiv:1603.08887.",
      "citeRegEx" : "Durrett et al\\.,? 2016",
      "shortCiteRegEx" : "Durrett et al\\.",
      "year" : 2016
    }, {
      "title" : "Imbalanced learning: foundations, algorithms, and applications",
      "author" : [ "Haibo He", "Yunqian Ma." ],
      "venue" : "John Wiley & Sons.",
      "citeRegEx" : "He and Ma.,? 2013",
      "shortCiteRegEx" : "He and Ma.",
      "year" : 2013
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "CVPR, pages 770–778.",
      "citeRegEx" : "He et al\\.,? 2016a",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Identity mappings in deep residual networks",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "ECCV, pages 630–645.",
      "citeRegEx" : "He et al\\.,? 2016b",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "NIPS, pages 1693–1701.",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, pages 1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Deep residual learning for weakly-supervised relation extraction",
      "author" : [ "Yi Yao Huang", "William Yang Wang." ],
      "venue" : "EMNLP, pages 1803–1807.",
      "citeRegEx" : "Huang and Wang.,? 2017",
      "shortCiteRegEx" : "Huang and Wang.",
      "year" : 2017
    }, {
      "title" : "Extractive summarization with swap-net: Sentences and words from alternating pointer networks",
      "author" : [ "Aishwarya Jadhav", "Vaibhav Rajan." ],
      "venue" : "ACL, pages 142–151.",
      "citeRegEx" : "Jadhav and Rajan.,? 2018",
      "shortCiteRegEx" : "Jadhav and Rajan.",
      "year" : 2018
    }, {
      "title" : "Flexible nonautoregressive extractive summarization with threshold: How to extract a non-fixed number of summary sentences",
      "author" : [ "Ruipeng Jia", "Yanan Cao", "Haichao Shi", "Fang Fang", "Cong Cao", "Shi Wang." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Jia et al\\.,? 2021",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2021
    }, {
      "title" : "Distilsum: Distilling the knowledge for extractive summarization",
      "author" : [ "Ruipeng Jia", "Yanan Cao", "Haichao Shi", "Fang Fang", "Yanbing Liu", "Jianlong Tan." ],
      "venue" : "CIKM, pages 2069–2072.",
      "citeRegEx" : "Jia et al\\.,? 2020a",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural extractive summarization with hierarchical attentive heterogeneous graph network",
      "author" : [ "Ruipeng Jia", "Yanan Cao", "Hengzhu Tang", "Fang Fang", "Cong Cao", "Shi Wang." ],
      "venue" : "EMNLP, pages 3622– 3631.",
      "citeRegEx" : "Jia et al\\.,? 2020b",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2020
    }, {
      "title" : "Content selection in deep learning models of summarization",
      "author" : [ "Chris Kedzie", "Kathleen McKeown", "Hal Daumé III." ],
      "venue" : "EMNLP, pages 1818–1828.",
      "citeRegEx" : "Kedzie et al\\.,? 2018",
      "shortCiteRegEx" : "Kedzie et al\\.",
      "year" : 2018
    }, {
      "title" : "Scoring sentence singletons and pairs for abstractive summarization",
      "author" : [ "Logan Lebanoff", "Kaiqiang Song", "Franck Dernoncourt", "Doo Soon Kim", "Seokhwan Kim", "Walter Chang", "Fei Liu." ],
      "venue" : "ACL, pages 2175– 2189.",
      "citeRegEx" : "Lebanoff et al\\.,? 2019",
      "shortCiteRegEx" : "Lebanoff et al\\.",
      "year" : 2019
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text summarization branches out, pages 74–81.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Text summarization with pretrained encoders",
      "author" : [ "Yang Liu", "Mirella Lapata." ],
      "venue" : "EMNLP, pages 3728–3738.",
      "citeRegEx" : "Liu and Lapata.,? 2019",
      "shortCiteRegEx" : "Liu and Lapata.",
      "year" : 2019
    }, {
      "title" : "Single document summarization as tree induction",
      "author" : [ "Yang Liu", "Ivan Titov", "Mirella Lapata." ],
      "venue" : "NAACL-HLT, pages 1745–1755.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Reading like HER: Human reading inspired extractive summarization",
      "author" : [ "Ling Luo", "Xiang Ao", "Yan Song", "Feiyang Pan", "Min Yang", "Qing He." ],
      "venue" : "EMNLP, pages 3033–3043.",
      "citeRegEx" : "Luo et al\\.,? 2019",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2019
    }, {
      "title" : "The stanford corenlp natural language processing toolkit",
      "author" : [ "Christopher Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven Bethard", "David McClosky." ],
      "venue" : "ACL, pages 55–60.",
      "citeRegEx" : "Manning et al\\.,? 2014",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "Jointly extracting and compressing documents with summary state representations",
      "author" : [ "Afonso Mendes", "Shashi Narayan", "Sebastião Miranda", "Zita Marinho", "André FT Martins", "Shay B Cohen." ],
      "venue" : "NAACL-HLT, pages 3955–3966.",
      "citeRegEx" : "Mendes et al\\.,? 2019",
      "shortCiteRegEx" : "Mendes et al\\.",
      "year" : 2019
    }, {
      "title" : "Syntactic data augmentation increases robustness to inference heuristics",
      "author" : [ "Junghyun Min", "R. Thomas McCoy", "Dipanjan Das", "Emily Pitler", "Tal Linzen." ],
      "venue" : "ACL, pages 2339–2352.",
      "citeRegEx" : "Min et al\\.,? 2020",
      "shortCiteRegEx" : "Min et al\\.",
      "year" : 2020
    }, {
      "title" : "On the number of linear regions of deep neural networks",
      "author" : [ "Guido Montúfar", "Razvan Pascanu", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "NIPS, pages 2924–2932.",
      "citeRegEx" : "Montúfar et al\\.,? 2014",
      "shortCiteRegEx" : "Montúfar et al\\.",
      "year" : 2014
    }, {
      "title" : "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents",
      "author" : [ "Ramesh Nallapati", "Feifei Zhai", "Bowen Zhou." ],
      "venue" : "AAAI, pages 3075–3081.",
      "citeRegEx" : "Nallapati et al\\.,? 2017",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2017
    }, {
      "title" : "Ranking sentences for extractive summarization with reinforcement learning",
      "author" : [ "Shashi Narayan", "Shay B Cohen", "Mirella Lapata." ],
      "venue" : "NAACL-HLT, pages 1747–1759.",
      "citeRegEx" : "Narayan et al\\.,? 2018",
      "shortCiteRegEx" : "Narayan et al\\.",
      "year" : 2018
    }, {
      "title" : "Stepwise extractive summarization and planning with structured transformers",
      "author" : [ "Shashi Narayan", "Joshua Maynez", "Jakub Adamek", "Daniele Pighin", "Blaž Bratanič", "Ryan McDonald." ],
      "venue" : "EMNLP, pages 4143–4159.",
      "citeRegEx" : "Narayan et al\\.,? 2020",
      "shortCiteRegEx" : "Narayan et al\\.",
      "year" : 2020
    }, {
      "title" : "An assessment of the accuracy of automatic evaluation in summarization",
      "author" : [ "Karolina Owczarzak", "John M Conroy", "Hoa Trang Dang", "Ani Nenkova." ],
      "venue" : "Proceedings of Workshop on Evaluation Metrics and System Comparison for Automatic Summarization,",
      "citeRegEx" : "Owczarzak et al\\.,? 2012",
      "shortCiteRegEx" : "Owczarzak et al\\.",
      "year" : 2012
    }, {
      "title" : "Pytorch: An imperative style, high-performance deep learning library",
      "author" : [ "Sasank Chilamkurthy", "Benoit Steiner", "Lu Fang", "Junjie Bai", "Soumith Chintala." ],
      "venue" : "NIPS, pages 8024–8035.",
      "citeRegEx" : "Chilamkurthy et al\\.,? 2019",
      "shortCiteRegEx" : "Chilamkurthy et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "J. Mach. Learn. Res., pages 140:1–140:67.",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Alexander M. Rush", "Sumit Chopra", "Jason Weston." ],
      "venue" : "EMNLP, pages 379–389.",
      "citeRegEx" : "Rush et al\\.,? 2015",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2015
    }, {
      "title" : "The new york times annotated corpus",
      "author" : [ "Evan Sandhaus." ],
      "venue" : "Linguistic Data Consortium, Philadelphia.",
      "citeRegEx" : "Sandhaus.,? 2008",
      "shortCiteRegEx" : "Sandhaus.",
      "year" : 2008
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J. Liu", "Christopher D. Manning." ],
      "venue" : "ACL, pages 1073–1083.",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "The information bottleneck method",
      "author" : [ "Naftali Tishby", "Fernando C Pereira", "William Bialek." ],
      "venue" : "arXiv preprint physics/0004057.",
      "citeRegEx" : "Tishby et al\\.,? 2000",
      "shortCiteRegEx" : "Tishby et al\\.",
      "year" : 2000
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "NIPS, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Residual networks behave like ensembles of relatively shallow networks",
      "author" : [ "Andreas Veit", "Michael Wilber", "Serge Belongie." ],
      "venue" : "NIPS, pages 550– 558.",
      "citeRegEx" : "Veit et al\\.,? 2016",
      "shortCiteRegEx" : "Veit et al\\.",
      "year" : 2016
    }, {
      "title" : "Heterogeneous graph neural networks for extractive document summarization",
      "author" : [ "Danqing Wang", "Pengfei Liu", "Yining Zheng", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "ACL, pages 6209–6219.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring domain shift in extractive text summarization",
      "author" : [ "Danqing Wang", "Pengfei Liu", "Ming Zhong", "Jie Fu", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "arXiv preprint arXiv:1908.11664.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Bottlesum: Unsupervised and self-supervised sentence summarization using the information bottleneck principle",
      "author" : [ "Peter West", "Ari Holtzman", "Jan Buys", "Yejin Choi." ],
      "venue" : "EMNLP, pages 3750–3759.",
      "citeRegEx" : "West et al\\.,? 2019",
      "shortCiteRegEx" : "West et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised data augmentation for consistency training",
      "author" : [ "Qizhe Xie", "Zihang Dai", "Eduard Hovy", "Minh-Thang Luong", "Quoc V. Le." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Xie et al\\.,? 2020",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural extractive text summarization with syntactic compression",
      "author" : [ "Jiacheng Xu", "Greg Durrett." ],
      "venue" : "EMNLP, pages 3290–3301.",
      "citeRegEx" : "Xu and Durrett.,? 2019",
      "shortCiteRegEx" : "Xu and Durrett.",
      "year" : 2019
    }, {
      "title" : "Discourse-aware neural extractive text summarization",
      "author" : [ "Jiacheng Xu", "Zhe Gan", "Yu Cheng", "Jingjing Liu." ],
      "venue" : "ACL, pages 5021–5031.",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training",
      "author" : [ "Yu Yan", "Weizhen Qi", "Yeyun Gong", "Dayiheng Liu", "Nan Duan", "Jiusheng Chen", "Ruofei Zhang", "Ming Zhou." ],
      "venue" : "arXiv preprint arXiv:2001.04063, pages 2401–2410.",
      "citeRegEx" : "Yan et al\\.,? 2020",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2020
    }, {
      "title" : "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
      "author" : [ "Jingqing Zhang", "Yao Zhao", "Mohammad Saleh", "Peter J Liu." ],
      "venue" : "arXiv preprint arXiv:1912.08777, pages 11328– 11339.",
      "citeRegEx" : "Zhang et al\\.,? 2019a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Hibert: Document level pre-training of hierarchical bidirectional transformers for document summarization",
      "author" : [ "Xingxing Zhang", "Furu Wei", "Ming Zhou." ],
      "venue" : "ACL, pages 5059–5069.",
      "citeRegEx" : "Zhang et al\\.,? 2019b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Extractive summarization as text matching",
      "author" : [ "Ming Zhong", "Pengfei Liu", "Yiran Chen", "Danqing Wang", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "ACL, pages 6197–6208.",
      "citeRegEx" : "Zhong et al\\.,? 2020",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2020
    }, {
      "title" : "Searching for effective neural extractive summarization: What works and what’s next",
      "author" : [ "Ming Zhong", "Pengfei Liu", "Danqing Wang", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "ACL, pages 1049–1058.",
      "citeRegEx" : "Zhong et al\\.,? 2019a",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2019
    }, {
      "title" : "A closer look at data bias in neural extractive summarization models",
      "author" : [ "Ming Zhong", "Danqing Wang", "Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "arXiv preprint arXiv:1909.13705.",
      "citeRegEx" : "Zhong et al\\.,? 2019b",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural document summarization by jointly learning to score and select sentences",
      "author" : [ "Qingyu Zhou", "Nan Yang", "Furu Wei", "Shaohan Huang", "Ming Zhou", "Tiejun Zhao." ],
      "venue" : "ACL, pages 654–663.",
      "citeRegEx" : "Zhou et al\\.,? 2018",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning data augmentation strategies for object detection",
      "author" : [ "Barret Zoph", "Ekin D. Cubuk", "Golnaz Ghiasi", "Tsung-Yi Lin", "Jonathon Shlens", "Quoc V. Le." ],
      "venue" : "ECCV, pages 566–583.",
      "citeRegEx" : "Zoph et al\\.,? 2019",
      "shortCiteRegEx" : "Zoph et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 46,
      "context" : "1) It should be detrimental to keep tangential information (West et al., 2019).",
      "startOffset" : 59,
      "endOffset" : 78
    }, {
      "referenceID" : 41,
      "context" : "Recently, the Information Bottleneck principle (Tishby et al., 2000; West et al., 2019) is introduced to incorporate a tradeoff between information selection and pruning.",
      "startOffset" : 47,
      "endOffset" : 87
    }, {
      "referenceID" : 46,
      "context" : "Recently, the Information Bottleneck principle (Tishby et al., 2000; West et al., 2019) is introduced to incorporate a tradeoff between information selection and pruning.",
      "startOffset" : 47,
      "endOffset" : 87
    }, {
      "referenceID" : 4,
      "context" : "Length penalty and the topic loss (Baziotis et al., 2019) are used in the autoencoding system to augment the reconstruction loss.",
      "startOffset" : 34,
      "endOffset" : 57
    }, {
      "referenceID" : 15,
      "context" : "367 CNN/DM (Hermann et al., 2015) are labeled as “1” and others are “0”, indicating whether this sentence should be selected as summary or not.",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 12,
      "context" : "Unfortunately, the imbalance in summarization is inherent, which can’t be addressed by common data augmentation (He and Ma, 2013; Asai and Hajishirzi, 2020; Min et al., 2020; Zoph et al., 2019; Xie et al., 2020), for there is a rare influence on the 0/1 distribution by adding or deleting the entire document.",
      "startOffset" : 112,
      "endOffset" : 211
    }, {
      "referenceID" : 1,
      "context" : "Unfortunately, the imbalance in summarization is inherent, which can’t be addressed by common data augmentation (He and Ma, 2013; Asai and Hajishirzi, 2020; Min et al., 2020; Zoph et al., 2019; Xie et al., 2020), for there is a rare influence on the 0/1 distribution by adding or deleting the entire document.",
      "startOffset" : 112,
      "endOffset" : 211
    }, {
      "referenceID" : 30,
      "context" : "Unfortunately, the imbalance in summarization is inherent, which can’t be addressed by common data augmentation (He and Ma, 2013; Asai and Hajishirzi, 2020; Min et al., 2020; Zoph et al., 2019; Xie et al., 2020), for there is a rare influence on the 0/1 distribution by adding or deleting the entire document.",
      "startOffset" : 112,
      "endOffset" : 211
    }, {
      "referenceID" : 57,
      "context" : "Unfortunately, the imbalance in summarization is inherent, which can’t be addressed by common data augmentation (He and Ma, 2013; Asai and Hajishirzi, 2020; Min et al., 2020; Zoph et al., 2019; Xie et al., 2020), for there is a rare influence on the 0/1 distribution by adding or deleting the entire document.",
      "startOffset" : 112,
      "endOffset" : 211
    }, {
      "referenceID" : 47,
      "context" : "Unfortunately, the imbalance in summarization is inherent, which can’t be addressed by common data augmentation (He and Ma, 2013; Asai and Hajishirzi, 2020; Min et al., 2020; Zoph et al., 2019; Xie et al., 2020), for there is a rare influence on the 0/1 distribution by adding or deleting the entire document.",
      "startOffset" : 112,
      "endOffset" : 211
    }, {
      "referenceID" : 32,
      "context" : "These works usually instantiate their encoder-decoder architecture by choosing RNN (Nallapati et al., 2017; Zhou et al., 2018), Transformer (Wang et al.",
      "startOffset" : 83,
      "endOffset" : 126
    }, {
      "referenceID" : 56,
      "context" : "These works usually instantiate their encoder-decoder architecture by choosing RNN (Nallapati et al., 2017; Zhou et al., 2018), Transformer (Wang et al.",
      "startOffset" : 83,
      "endOffset" : 126
    }, {
      "referenceID" : 45,
      "context" : ", 2018), Transformer (Wang et al., 2019; Zhong et al., 2019b; Liu and Lapata, 2019; Zhang et al., 2019b) or GNN (Wang et al.",
      "startOffset" : 21,
      "endOffset" : 104
    }, {
      "referenceID" : 55,
      "context" : ", 2018), Transformer (Wang et al., 2019; Zhong et al., 2019b; Liu and Lapata, 2019; Zhang et al., 2019b) or GNN (Wang et al.",
      "startOffset" : 21,
      "endOffset" : 104
    }, {
      "referenceID" : 25,
      "context" : ", 2018), Transformer (Wang et al., 2019; Zhong et al., 2019b; Liu and Lapata, 2019; Zhang et al., 2019b) or GNN (Wang et al.",
      "startOffset" : 21,
      "endOffset" : 104
    }, {
      "referenceID" : 52,
      "context" : ", 2018), Transformer (Wang et al., 2019; Zhong et al., 2019b; Liu and Lapata, 2019; Zhang et al., 2019b) or GNN (Wang et al.",
      "startOffset" : 21,
      "endOffset" : 104
    }, {
      "referenceID" : 44,
      "context" : ", 2019b) or GNN (Wang et al., 2020; Jia et al., 2020b) as encoder, autoregressive (Jadhav and Rajan, 2018; Liu and Lapata, 2019) or RL-based (Narayan et al.",
      "startOffset" : 16,
      "endOffset" : 54
    }, {
      "referenceID" : 21,
      "context" : ", 2019b) or GNN (Wang et al., 2020; Jia et al., 2020b) as encoder, autoregressive (Jadhav and Rajan, 2018; Liu and Lapata, 2019) or RL-based (Narayan et al.",
      "startOffset" : 16,
      "endOffset" : 54
    }, {
      "referenceID" : 18,
      "context" : ", 2020b) as encoder, autoregressive (Jadhav and Rajan, 2018; Liu and Lapata, 2019) or RL-based (Narayan et al.",
      "startOffset" : 36,
      "endOffset" : 82
    }, {
      "referenceID" : 25,
      "context" : ", 2020b) as encoder, autoregressive (Jadhav and Rajan, 2018; Liu and Lapata, 2019) or RL-based (Narayan et al.",
      "startOffset" : 36,
      "endOffset" : 82
    }, {
      "referenceID" : 33,
      "context" : ", 2020b) as encoder, autoregressive (Jadhav and Rajan, 2018; Liu and Lapata, 2019) or RL-based (Narayan et al., 2018; Arumae and Liu, 2018; Luo et al., 2019) decoders.",
      "startOffset" : 95,
      "endOffset" : 157
    }, {
      "referenceID" : 0,
      "context" : ", 2020b) as encoder, autoregressive (Jadhav and Rajan, 2018; Liu and Lapata, 2019) or RL-based (Narayan et al., 2018; Arumae and Liu, 2018; Luo et al., 2019) decoders.",
      "startOffset" : 95,
      "endOffset" : 157
    }, {
      "referenceID" : 27,
      "context" : ", 2020b) as encoder, autoregressive (Jadhav and Rajan, 2018; Liu and Lapata, 2019) or RL-based (Narayan et al., 2018; Arumae and Liu, 2018; Luo et al., 2019) decoders.",
      "startOffset" : 95,
      "endOffset" : 157
    }, {
      "referenceID" : 25,
      "context" : "(2020) introduces extract-thenmatch framework, which employs BERTSUMEXT (Liu and Lapata, 2019) as first-stage to prune unnecessary information.",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 13,
      "context" : "The original deep residual learning is introduced in image recognition (He et al., 2016a) for the notorious degradation problem.",
      "startOffset" : 71,
      "endOffset" : 89
    }, {
      "referenceID" : 42,
      "context" : "Then, residual is introduced to the natural language process by Transformer (Vaswani et al., 2017).",
      "startOffset" : 76,
      "endOffset" : 98
    }, {
      "referenceID" : 43,
      "context" : "We expect the redundant layer to correspond to the input and output, namely identity mapping (He et al., 2016a,b; Veit et al., 2016; Balduzzi et al., 2018).",
      "startOffset" : 93,
      "endOffset" : 155
    }, {
      "referenceID" : 3,
      "context" : "We expect the redundant layer to correspond to the input and output, namely identity mapping (He et al., 2016a,b; Veit et al., 2016; Balduzzi et al., 2018).",
      "startOffset" : 93,
      "endOffset" : 155
    }, {
      "referenceID" : 13,
      "context" : "Resnet (He et al., 2016a) addresses the degradation problem by introducing a deep residual learning framework.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 17,
      "context" : "If an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers (Huang and Wang, 2017).",
      "startOffset" : 148,
      "endOffset" : 170
    }, {
      "referenceID" : 22,
      "context" : "We model the sentence extraction task as a sequence tagging problem (Kedzie et al., 2018).",
      "startOffset" : 68,
      "endOffset" : 89
    }, {
      "referenceID" : 16,
      "context" : "The sentence encoder in extractive summarization models is usually a recurrent neural network with Long-Short Term Memory (Hochreiter and Schmidhuber, 1997) or Gated Recurrent Units (Cho et al.",
      "startOffset" : 122,
      "endOffset" : 156
    }, {
      "referenceID" : 9,
      "context" : "The sentence encoder in extractive summarization models is usually a recurrent neural network with Long-Short Term Memory (Hochreiter and Schmidhuber, 1997) or Gated Recurrent Units (Cho et al., 2014).",
      "startOffset" : 182,
      "endOffset" : 200
    }, {
      "referenceID" : 10,
      "context" : "In this paper, our sentence encoder builds on the BERT architecture (Devlin et al., 2019), a recently proposed highly efficient model which is based on the deep bidirectional Transformer (Vaswani et al.",
      "startOffset" : 68,
      "endOffset" : 89
    }, {
      "referenceID" : 42,
      "context" : ", 2019), a recently proposed highly efficient model which is based on the deep bidirectional Transformer (Vaswani et al., 2017) and has achieved state-ofthe-art performance in many NLP tasks.",
      "startOffset" : 105,
      "endOffset" : 127
    }, {
      "referenceID" : 26,
      "context" : "The Transformer aims at reducing the fundamental constraint of sequential computation which underlies most architecture (Liu et al., 2019).",
      "startOffset" : 120,
      "endOffset" : 138
    }, {
      "referenceID" : 26,
      "context" : "Our extractive model is composed of a sentencelevel Transformer (TS) and a document-level Transformer (TD) (Liu et al., 2019).",
      "startOffset" : 107,
      "endOffset" : 125
    }, {
      "referenceID" : 26,
      "context" : "In the Transformer model sketched above, intersentence relations are modeled by multi-head attention based on softmax functions, which only capture shallow structural information (Liu et al., 2019).",
      "startOffset" : 179,
      "endOffset" : 197
    }, {
      "referenceID" : 13,
      "context" : "The advantages of this method are: 1) the residual architecture will highlight the pivotal information as well as reserving the original sentence representation; 2) it is easier to optimize the residual mapping than to optimize the original (He et al., 2016a).",
      "startOffset" : 241,
      "endOffset" : 259
    }, {
      "referenceID" : 13,
      "context" : "Since multiple nonlinear layers can asymptotically approximate complicated functions (He et al., 2016a; Montúfar et al., 2014), the differential amplifier mapping H(x) is recast into a residual mapping F(x) and an identity mapping x:",
      "startOffset" : 85,
      "endOffset" : 126
    }, {
      "referenceID" : 31,
      "context" : "Since multiple nonlinear layers can asymptotically approximate complicated functions (He et al., 2016a; Montúfar et al., 2014), the differential amplifier mapping H(x) is recast into a residual mapping F(x) and an identity mapping x:",
      "startOffset" : 85,
      "endOffset" : 126
    }, {
      "referenceID" : 13,
      "context" : "Furthermore, 1) the identity shortcut connections advance the architecture without extra parameter; 2) the identity shortcut doesn’t add the computational complexity (He et al., 2016a);",
      "startOffset" : 166,
      "endOffset" : 184
    }, {
      "referenceID" : 32,
      "context" : "Following previous work (Nallapati et al., 2017; Liu et al., 2019), we use a sigmoid function after a linear transformation to calculate the probability ri of selecting si as a summary sentence:",
      "startOffset" : 24,
      "endOffset" : 66
    }, {
      "referenceID" : 26,
      "context" : "Following previous work (Nallapati et al., 2017; Liu et al., 2019), we use a sigmoid function after a linear transformation to calculate the probability ri of selecting si as a summary sentence:",
      "startOffset" : 24,
      "endOffset" : 66
    }, {
      "referenceID" : 15,
      "context" : "As shown in Table 1, we employ two datasets widely-used with multiple sentences summary: CNN and Dailymail (CNN/DM) (Hermann et al., 2015) and New York Times (NYT) (Sandhaus, 2008).",
      "startOffset" : 116,
      "endOffset" : 138
    }, {
      "referenceID" : 15,
      "context" : "CNN/DM We used the standard split (Hermann et al., 2015) for training, validation, and test (90,266/1,220/1,093 for CNN and 196,96/12.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 28,
      "context" : "148/10,397 for Daily Mail), with splitting sentences by Stanford CoreNLP (Manning et al., 2014) toolkit and pre-processing the dataset following (See et al.",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 40,
      "context" : ", 2014) toolkit and pre-processing the dataset following (See et al., 2017) and (Zhong et al.",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 52,
      "context" : "NYT Following previous work (Zhang et al., 2019b; Xu and Durrett, 2019), we use 137,778, 17,222 and 17,223 samples for training, validation, and test, respectively.",
      "startOffset" : 28,
      "endOffset" : 71
    }, {
      "referenceID" : 48,
      "context" : "NYT Following previous work (Zhang et al., 2019b; Xu and Durrett, 2019), we use 137,778, 17,222 and 17,223 samples for training, validation, and test, respectively.",
      "startOffset" : 28,
      "endOffset" : 71
    }, {
      "referenceID" : 28,
      "context" : "Sentences were split with the Stanford CoreNLP toolkit (Manning et al., 2014).",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 6,
      "context" : "Besides, we employ the Trigram Blocking strategy for decoding, which is a simple but powerful version of Maximal Marginal Relevance (Carbonell and Goldstein, 1998).",
      "startOffset" : 132,
      "endOffset" : 163
    }, {
      "referenceID" : 24,
      "context" : "ROUGE (Lin, 2004) is the standard metric for evaluating the quality of summaries.",
      "startOffset" : 6,
      "endOffset" : 17
    }, {
      "referenceID" : 35,
      "context" : "It is not enough to only rely on the ROUGE evaluation for a summarization system, although the ROUGE correlates well with human judgments (Owczarzak et al., 2012).",
      "startOffset" : 138,
      "endOffset" : 162
    }, {
      "referenceID" : 25,
      "context" : "Trigram Blocking leads to a great improvement on all ROUGE metrics for many extractive approaches (Liu and Lapata, 2019; Wang et al., 2020).",
      "startOffset" : 98,
      "endOffset" : 139
    }, {
      "referenceID" : 44,
      "context" : "Trigram Blocking leads to a great improvement on all ROUGE metrics for many extractive approaches (Liu and Lapata, 2019; Wang et al., 2020).",
      "startOffset" : 98,
      "endOffset" : 139
    }, {
      "referenceID" : 25,
      "context" : "In this paper, we apply two ways to map the words representation into its sentence representation: weighted-pooling at Equation 2 and picking [CLS] token as sentence (Liu and Lapata, 2019).",
      "startOffset" : 166,
      "endOffset" : 188
    } ],
    "year" : 2021,
    "abstractText" : "For sentence-level extractive summarization, there is a disproportionate ratio of selected and unselected sentences, leading to flatting the summary features when optimizing the classification. The imbalanced sentence classification in extractive summarization is inherent, which can’t be addressed by data sampling or data augmentation algorithms easily. In order to address this problem, we innovatively consider the single-document extractive summarization as a rebalance problem and present a deep differential amplifier framework to enhance the features of summary sentences. Specifically, we calculate and amplify the semantic difference between each sentence and other sentences, and apply the residual unit to deepen the differential amplifier architecture. Furthermore, the corresponding objective loss of the minority class is boosted by a weighted cross-entropy. In this way, our model pays more attention to the pivotal information of one sentence, that is different from previous approaches which model all informative context in the source document. Experimental results on two benchmark datasets show that our summarizer performs competitively against state-of-the-art methods. Our source code will be available on Github.",
    "creator" : "LaTeX with hyperref"
  }
}