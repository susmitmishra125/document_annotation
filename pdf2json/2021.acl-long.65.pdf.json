{
  "name" : "2021.acl-long.65.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Do Context-Aware Translation Models Pay the Right Attention?",
    "authors" : [ "Kayo Yin", "Patrick Fernandes", "Danish Pruthi", "Aditi Chaudhary", "André F. T. Martins", "Graham Neubig" ],
    "emails" : [ "gneubig}@cs.cmu.edu", "andre.t.martins@tecnico.ulisboa.pt" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 788–801\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n788"
    }, {
      "heading" : "1 Introduction",
      "text" : "There is a growing consensus in machine translation research that it is necessary to move beyond sentence-level translation and incorporate document-level context (Guillou et al., 2018; Läubli et al., 2018; Toral et al., 2018). While various methods to incorporate context in neural machine translation (NMT) have been proposed (Tiedemann and Scherrer (2017); Miculicich et al. (2018); Maruf and Haffari (2018), inter alia), it is unclear whether models rely on the “right” context that is actually sufficient to disambiguate difficult translations. Even when additional context\n1Our SCAT data and code for experiments are available at https://github.com/neulab/contextual-mt.\nHuman\nEn Look after her a lot. Okay. Any questions? Have wegot her report? Yes, it’s in the infirmary already Fr Dorlotez-la. D’accord. Vous avez des questions ? Ondispose de son rapport. Oui, il est à l’infirmerie.\nContext-aware baseline\nEn Look after her a lot. Okay. Any questions? Have we got her report? Yes, it’s in the infirmary already. Fr Dorlotez-la. D’accord. Vous avez des questions ? Ondispose de son rapport ? Oui, elle est déjà à l’infirmerie.\n3 words with the highest attention are highlighted.\nis provided, models often perform poorly on evaluation of relatively simple discourse phenomena (Müller et al., 2018; Bawden et al., 2018; Voita et al., 2019b,a; Lopes et al., 2020) and rely on spurious word co-occurences during translation of polysemous words (Emelin et al., 2020). Some evidence suggests that models attend to uninformative tokens (Voita et al., 2018) and do not use contextual information adequately (Kim et al., 2019).\nTo understand plausibly why current NMT models are unable to fully leverage the disambiguating context they are provided, and how we can develop models that use context more effectively, we pose the following research questions: (i) In context aware translation, what context is intrinsically useful to disambiguate hard translation phenomena such as ambiguous pronouns or word senses?; (ii) Are context-aware MT models paying attention to the relevant context or not?; and (iii) If not, can we\nencourage them to do so? To answer the first question, we collect annotations of context that human translators found useful in choosing between ambiguous translation options (§3). Specifically, we ask 20 professional translators to choose the correct French translation between two contrastive translations of an ambiguous word, given an English source sentence and the previous source- and target-side sentences. The translators additionally highlight the words they found the most useful to make their decision, giving an idea of the context useful in making these decisions. We collect 14K such annotations and release SCAT (“Supporting Context for Ambiguous Translations”), the first dataset of human rationales for resolving ambiguity in document-level translation. Analysis reveals that inter-sentential target context is important for pronoun translation, whereas intra-sentential source context is often sufficient for word sense disambiguation.\nTo answer the second question, we quantify the similarity of the attention distribution of contextaware models and the human annotations in SCAT (§4). We measure alignment between the baseline context-aware model’s attention and human rationales across various model attention heads and layers. We observe a relatively high alignment between self attention scores from the top encoder layers and the source-side supporting context marked by translators, however, the model’s attention is poorly aligned with target-side supporting context.\nFor the third question, we explore a method to regularize attention towards human-annotated disambiguating context (§5). We find that attention regularization is an effective technique to encourage models to pay more attention to words humans find useful to resolve ambiguity in translations. Our models with regularized attention outperform previous context-aware baselines, improving translation quality by 0.54 BLEU, and yielding a relative improvement of 14.7% in contrastive evaluation. An example of translations from a baseline and our model, along with the supporting rationale by a professional translator is illustrated in Table 1."
    }, {
      "heading" : "2 Document-Level Translation",
      "text" : "Neural Machine Translation. Current NMT models employ encoder-decoder architectures (Bahdanau et al., 2015; Vaswani et al., 2017). First, the encoder maps a source sequence x = (x1, x2, ..., xS) to a continuous representation z =\n(z1, z2, ..., zS). Then, given z, the decoder generates the corresponding target sequence y = (y1, y2, ..., yT ), one token at a time. Sentence-level NMT models take one source sentence and generate one target sentence at a time. These models perform reasonably well, but given that they only have intra-sentential context, they fail to handle some phenomena that require inter-sentential context to accurately translate. Well-known examples of these phenomena include gender-marked anaphoric pronouns (Guillou et al., 2018) and maintenance of lexical coherence (Läubli et al., 2018).\nDocument-Level Translation. Document-level translation models learn to maximize the probability of a target document Y given the source document X: Pθ(Y |X) = ∏J j=1 Pθ(y\nj |xj ,Cj), where yj and xj are the j-th target and source sentences, and Cj is the collection of contextual sentences for the j-th sentence pair. There are many methods for incorporating context (§6), but even simple concatenation (Tiedemann and Scherrer, 2017), which prepends the previous source or target sentences to the current sentence separated by a 〈BRK〉 tag, achieves comparable performance to more sophisticated approaches, especially in highresource scenarios (Lopes et al., 2020).\nEvaluation. BLEU (Papineni et al., 2002) is most widely used to evaluate MT, but it can be poorly correlated with human evaluation (CallisonBurch et al., 2006; Reiter, 2018). Recently, a number of neural evaluation methods, such as COMET (Rei et al., 2020), have shown better correlation with human judgement. Nevertheless, common automatic metrics have limited ability to evaluate discourse in MT (Hardmeier, 2012). As a remedy to this, researchers often use contrastive test sets for a targeted discourse phenomenon (Müller et al., 2018), such as pronoun anaphora resolution and word sense disambiguation, to verify if the model ranks the correct translation of an ambiguous sentence higher than the incorrect translation."
    }, {
      "heading" : "3 What Context Do Human Translators Pay Attention to?",
      "text" : "We first conduct a user study to collect supporting context that translators use in disambiguation, and analyze characteristics of the supporting words."
    }, {
      "heading" : "3.1 Recruitment and Annotation Setup",
      "text" : "We recruited 20 freelance English-French translators on Upwork.2 The translators are native speakers of at least one of the two languages and have a job success rate of over 90%. Each translator is given 400 examples with an English source sentence and two possible French translations, and one out of 5 possible context levels: no context (0+0), only the previous source sentence as context (1+0), only the previous target sentence (0+1), the previous source sentence and target sentence (1+1), and the 5 previous source and target sentences (5+5). We vary the context level in each example to measure how human translation quality changes.\nTranslators provide annotations using the interface shown in Figure 1. They are first asked to select the correct translation out of the two contrastive translations, and then highlight word(s) they found useful to arrive at their answer. In cases where multiple words are sufficient to disambiguate, translators were asked to mark only the most salient words rather than all of them. Further, translators also reported their confidence in their answers, choosing from “not at all”, “somewhat”, and “very”."
    }, {
      "heading" : "3.2 Tasks and Data Quality",
      "text" : "We perform this study for two tasks: pronoun anaphora resolution (PAR), where the translators are tasked with choosing the correct French gendered pronoun associated to a neutral English pronoun, and word sense disambiguation (WSD), where the translators pick the correct translation of a polysemous word. PAR, and WSD to a lesser extent, have been commonly studied to evaluate context-aware NMT models (Voita et al., 2018; Lopes et al., 2020; Müller et al., 2018; Huo et al., 2020; Nagata and Morishita, 2020).\n2https://www.upwork.com\nPronoun Anaphora Resolution. We annotate examples from the contrastive test set by Lopes et al. (2020). This set includes 14K examples from the OpenSubtitles2018 dataset (Lison et al., 2018) with occurrences of the English pronouns “it” and “they” that correspond to the French translations “il” or “elle” and “ils” or “elles”, with 3.5K examples for each French pronoun type. Through our annotation effort, we obtain 14K examples of supporting context for pronoun anaphora resolution in ambiguous translations selected by professional human translators. Statistics on this dataset, SCAT: Supporting Context for Ambiguous Translations, are provided in Appendix A.\nWord Sense Disambiguation. There are no existing contrastive datasets for WSD with a context window larger than 1 sentence, therefore, we automatically generate contrastive examples with context window of 5 sentences from OpenSubtitles2018 by identifying polysemous English words and possible French translations. We describe our methodology in Appendix B.\nQuality. For quality control, we asked 8 internal speakers of English and French, with native or bilingual proficiency in both languages, to carefully annotate the same 100 examples given to all professional translators. We compared both the answer accuracies and the selected words for each hired translator against this control set and discarded submissions that either had several incorrect answers while the internal bilinguals were able to choose the correct answer on the same example, or that highlighted contextual words that the internal annotators did not select and that had little relevance to the ambiguous word. Furthermore, among the 400 examples given to each annotator, the first hundred are identical, allowing us to measure the inter-annotator agreement for both answer and supporting context selection.\nFirst, for answer selection on PAR, we find 91.0% overall agreement, with Fleiss’ freemarginal Kappa κ = 0.82. For WSD, we find 85.9% overall agreement with κ = 0.72. This indicates a substantial inter-annotator agreement for the selected answer. In addition, we measure the inter-annotator agreement for the selected words by calculating the F1 between the word selections for each pair of annotators given identical context settings. For PAR, we obtain an average F1 of 0.52 across all possible pairs, and a standard deviation of\n0.12. For WSD, we find an average F1 of 0.46 and a standard deviation of 0.12. There is a high agreement between annotators for the selected words as well."
    }, {
      "heading" : "3.3 Answer Accuracy and Confidence",
      "text" : "Table 2 shows the accuracy of answers and the percentage of answers being reported as not at all confident for each of the 5 different context levels. For PAR, there is a large increase in accuracy and confidence when just one previous sentence in either language is provided as context compared to no context at all. Target-side context also seems more useful than source: only target-side context gives higher answer accuracy than only source-side context, while the accuracy does not increase significantly by having both previous sentences.\nFor WSD, we do not observe significant differences in answer accuracy and confidence between the different context levels (Figure 2).The high answer accuracy with 0+0 context and the low rate of zero-confidence answers across all settings suggest that the necessary disambiguating information is often present in the intra-sentential context. Alternatively, this may be partially due to characteristics of the automatically generated dataset itself: we found that some examples are misaligned so the previous sentences given as context do not actually correspond to the context of the current sentences, and therefore do not add useful information. We also observe that translators tend to report a high confidence and high agreement in incorrect answers as well. This can be explained by the tendency to select the masculine pronoun in PAR (Figure 3) or the prevailing word sense in WSD.\nTo properly translate an anaphoric pronoun, the translator must identify its antecedent and deter-\nmine its gender, so we hypothesize that the antecedent is of high importance for disambiguation. In our study, 72.4% of the examples shown to annotators contain the antecedent in the context or current sentences. We calculate how answer accuracy and confidence vary between examples that do or do not contain the pronoun antecedent. We find that the presence of the antecedent in the context leads to larger variations in answer accuracy than the level of context given, demonstrating the importance of antecedents for resolution."
    }, {
      "heading" : "3.4 Analysis of the Highlighted Words",
      "text" : "Next, we examine the words that were selected as rationales from several angles.\nDistance. Figure 4 shows, for each context level, the number of highlighted words at a given distance (in sentences) from the ambiguous word. For PAR, when no previous sentences are provided, there are as many selected words from the source as the target context. With inter-sentential context, experts selected more supporting context from the target side. One possible reason is that the source and target sentences on their own are equally descriptive to perform PAR, but one may look for the coreference chain of the anaphoric pronoun in the target context to determine its gender, whereas the same coreference chain in the source context would not necessarily contain gender information. Moreover, the antecedent in the target side is more reliable\nthan the source antecedent, since the antecedent can have multiple possible translations with different genders. For WSD, we find that inter-sentential context is seldom highlighted, which reinforces our previous claim that most supporting context for WSD can be found in the current sentences.\nPart-of-Speech and Dependency. We use spaCy (Honnibal and Montani, 2017) to predict part-of-speech (POS) tags of selected words and syntactic dependencies between selected words and the ambiguous word. In Table 3a, we find that nominals are the most useful for PAR, which suggests that human translators look for other referents of the ambiguous pronoun to determine its gender. This is reinforced by Table 3b, where the antecedent of the pronoun is selected the most often.\nFor WSD, proper nouns and pronouns are not as important as nouns, probably because they do not carry as much semantic load that indicates the sense of the ambiguous word. Determiners, verbs and adpositions are relatively important since they offer clues on the syntactic dependencies of the ambiguous word on other words as well as its role in the sentence, and modifiers provide additional\ninformation about the ambiguous word. The main difference between PAR and WSD is that for PAR, the key supporting information is gender. The source side does not contain explicit information about the gender of the ambiguous pronoun whereas the target side may contain other gendered pronouns and determiners referring to the ambiguous pronoun. For WSD however, the key supporting information is word sense. While the source and target sides contain around the same amount of semantic information, humans may prefer to attend to source sentences that express how the ambiguous word is used in the sentence."
    }, {
      "heading" : "4 Do Models Pay the Right Attention?",
      "text" : "Next, we study NMT models and quantify the degree to which the model’s attention is aligned with the supporting context from professional translators."
    }, {
      "heading" : "4.1 Model",
      "text" : "We incorporate the 5 previous source and target sentences as context to the base Transformer (Vaswani et al., 2017) by prepending the previous sentences to the current sentence, separated by a 〈BRK〉 tag, as proposed by Tiedemann and Scherrer (2017)."
    }, {
      "heading" : "4.2 Similarity Metrics",
      "text" : "To calculate similarity between model attention and highlighted context, we first construct a human attention vector αhuman, where 1 corresponds to tokens marked by the human annotators, and 0 otherwise. We compare this vector against the model’s attention for the ambiguous pronoun for a given layer and head, αmodel, across three metrics:\nDot Product. The dot product αhuman · αmodel measures the total attention mass the model assigns to words highlighted by humans.\nKL Divergence. We compute the KL divergence between the model attention and the normalized human attention vector KL(αhuman-norm||αmodel(θ)), where the normalized distribution αhuman-norm is uniform over all tokens selected by humans and a very small constant elsewhere such that the sum of values in αhuman-norm is equal to 1.\nProbes Needed. We adapt the “probes needed” metric by Zhong et al. (2019) to measure the number of tokens we need to probe, based on the model attention, to find a token highlighted by humans. This corresponds to the ranking of the first highlighted token after sorting all tokens by descending model attention. The intuition is that the more attention the model assigns to supporting context, the fewer probes are needed to find a supporting token."
    }, {
      "heading" : "4.3 Results",
      "text" : "We compute the similarity between the model attention distribution for the ambiguous pronoun and the supporting context from 1,000 SCAT samples. In Table 5, for each attention type we report the best score across layers and attention heads. We also report the alignment score between a uniform\ndistribution and supporting context for comparison. We find that although there is a reasonably high alignment between encoder self attention and SCAT, decoder attentions have very low alignment with SCAT."
    }, {
      "heading" : "5 Making Models Pay the Right Attention",
      "text" : ""
    }, {
      "heading" : "5.1 Attention Regularization",
      "text" : "We hypothesize that by encouraging models to increase attention on words that humans use to resolve ambiguity, translation quality may improve. We apply attention regularization to guide model attention to increase alignment with the supporting context from SCAT. To do so, we append the translation loss with an attention regularization loss between the normalized human attention vector αhuman-norm and the model attention vector for the corresponding ambiguous pronoun αmodel:\nR(θ) = −λKL(αhuman-norm||αmodel(θ))\nwhere λ is a scalar weight parameter for the loss. During training, we randomly sample batches from SCAT with p = 0.2. We train with the standard MT objective on the full dataset, and on examples from SCAT, we additionally compute the attention regularization loss."
    }, {
      "heading" : "5.2 Data",
      "text" : "For document translation, we use the English and French data from OpenSubtitles2018 (Lison et al., 2018), which we clean then split into 16M training, 10,036 development, and 9,740 testing samples. For attention regularization, we retain examples from SCAT where 5+5 context was given to the annotator. We use 11,471 examples for training and 1,000 for testing."
    }, {
      "heading" : "5.3 Models",
      "text" : "We first train a baseline model, where the 5 previous source and target sentences serve as context and are incorporated via concatenation. This baseline model is trained without attention regularization.\nWe explore two models with attention regularization: (1) attnreg-rand, where we jointly train on the MT objective and regularize attention on a randomly initialized model; (2) attnreg-pre, where we first pre-train the model solely on the MT objective, then we jointly train on the MT objective and regularize attention. We describe the full setup in Appendix C."
    }, {
      "heading" : "5.4 Evaluation",
      "text" : "As described in Section 2, we evaluate translation outputs with BLEU and COMET. In addition, to evaluate the direct translation quality of specific phenomena, we translate the 4,015 examples from Lopes et al. (2020) containing ambiguous pronouns that were not used for attention regularization, and we compute the mean word f-measure of translations of the ambiguous pronouns and other words, with respect to reference texts.\nWe also perform contrastive evaluation on the same subset of Lopes et al. (2020) with a context window of 5 sentences (Big-PAR) and the contrastive test sets by Bawden et al. (2018), which include 200 examples on anaphoric pronoun translation and 200 examples on lexical consistency/word sense disambiguation. The latter test sets were crafted manually, have a context window of 1 sentence, and either the previous source or target sentence is necessary to disambiguate.\nContext-aware models often suffer from error propagation when using previously decoded output tokens as the target context (Li et al., 2020a). Therefore, during inference, we experiment with both using the gold target context (Gold) as well as using previous output tokens (Non-Gold)."
    }, {
      "heading" : "5.5 Overall Performance",
      "text" : "Before delving into the main results, we note that we explored regularizing different attention vectors in the model (Appendix C.3) and obtain the best BLEU and COMET scores for attnreg-rand when regularizing the self-attention of the top encoder layer, cross-attention of the top decoder layer and self-attention of the bottom decoder layer. For attnreg-pre, regularizing self-attention in the top decoder layer gives the best scores. Thus, we use these as the default regularization methods below.\nMoving on to the main results in Table 6, we observe that attnreg-rand improves on all metrics, which demonstrates that attention regularization is an effective method to improve translation quality. Although attnreg-pre does not improve gen-\neral translation scores significantly, it yields considerable gains in word f-measure on ambiguous pronouns and achieves some improvement over the baseline on contrastive evaluation on Big-PAR and PAR. Attention regularization with supporting context for PAR seems to especially improve models on similar tasks. The disparity between BLEU/COMET scores and targeted evaluations such as word f-measure and contrastive evaluation further suggests that general MT metrics are somewhat insensitive to improvements on specific discourse phenomena. For both models with attention regularization, there are no significant gains in WSD. As discussed in §3.4, WSD and PAR require different types of supporting context, so it is natural that regularizing attention using supporting context extracted from only one task does not always lead to improvement on the other."
    }, {
      "heading" : "5.6 Analysis",
      "text" : "We now investigate how models trained with attention regularization handle context differently compared to the baseline model.\nHow does attention regularization influence alignment with human rationales? We revisit the similarity metrics from §4.2 to measure alignment with SCAT. In Table 5, the dot product alignment over attention in the decoder increases with attention regularization, suggesting that attention regularization guides different parts of the model to pay attention to useful context. Interestingly, although only the encoder self-attention was explicitly regularized for attnreg-pre, the model seems to also have learned better alignment for attention in the decoder. Moreover, attnreg-pre generally has better alignment than attnreg-rand, suggesting that models respond more to attention regularization once it has been trained to perform translation.\nWhich attention is the most useful? For each of attnreg-rand and attnreg-pre, we perform attention regularization on either the encoder selfattention, decoder cross-attention or decoder selfattention only. In Table 7, encoder self-attention seems to contribute the most to both translation performance and contrastive evaluation. Although attnreg-rand models achieve higher BLEU and COMET scores, attnreg-pre obtain higher scores on metrics targeted to pronoun translation. Attention regularization seems to have limited effect on WSD performance, the scores vary little between attention types.\nHow much do models rely on supporting context? We compare model performance on contrastive evaluation on SCAT when it is given full context, and when we mask either the supporting context, random context words with p = 0.1, the source context, the target context, or all of the context. In Table 8, we find that baseline varies little when the supporting context is masked, which again suggests that context-aware baselines do not use the relevant context, although they do observe a drop in contrastive performance when the source and all context are masked. Models with attention regularization, especially attnreg-pre observe a large drop in contrastive performance when supporting context is masked, which indicates that they learned to rely more on supporting context. Furthermore, for attnreg-pre, the score after masking supporting context is significantly lower than when masking all context, which may indicate that having irrelevant context can have an adverse ef-\nfect. Another interesting finding is that both baseline and attnreg-rand seem to rely more on the source context than the target context, in contrast to human translators. This result corroborates prior results where models have better alignment with supporting context on attention that attends to the source (encoder self-attention and decoder crossattention), and regularizing these attention vectors contributes more to translation quality than regularizing the decoder self-attention."
    }, {
      "heading" : "6 Related Work",
      "text" : ""
    }, {
      "heading" : "6.1 Context-Aware Machine Translation",
      "text" : "Most current context-aware NMT approaches enhance NMT by including source- and/or targetside surrounding sentences as context to the model. Tiedemann and Scherrer (2017) concatenate the previous sentences to the input; Jean et al. (2017); Bawden et al. (2018); Zhang et al. (2018) use an additional encoder to extract contextual features; Wang et al. (2017) use a hierarchical RNN to encode the global context from all previous sentences; Maruf and Haffari (2018); Tu et al. (2018) use cache-based memories to encode context; Miculicich et al. (2018); Maruf et al. (2019) use hierarchical attention networks; Chen et al. (2020) add document-level discourse structure information to the input. While Maruf et al. (2019); Voita et al. (2018) also find higher attention mass attributed to relevant tokens in selected examples, our work is the first to guide model attention in context-aware NMT using human supervision and analyze its attention distribution in a quantitative manner.\nHowever, recent studies suggest that current context-aware NMT models often do not use context meaningfully. Kim et al. (2019) claim that improvements by context-aware models are mostly from regularization by reserving parameters for context inputs, and Li et al. (2020b) show that replacing the context in multi-encoder models with random signals leads to similar accuracy as using the actual context. Our work addresses the above\ndisparities by collecting human supporting context to regularize model attention heads during training."
    }, {
      "heading" : "6.2 Attention Mechanisms",
      "text" : "Though attention is usually learned in an unsupervised manner, recent work supervises attention with word alignments (Mi et al., 2016; Liu et al., 2016), event arguments and trigger words (Liu et al., 2017; Zhao et al., 2018), syntactic dependencies (Strubell et al., 2018) or word lexicons (Zou et al., 2018). Our work is closely related to a large body of work that supervises attention using human rationales for text classification (Barrett et al., 2018; Bao et al., 2018; Zhong et al., 2019; Choi et al., 2020; Pruthi et al., 2020). Our work, however, is the first to collect human evidence for document translation and use it to regularize the attention of NMT models."
    }, {
      "heading" : "7 Implications and Future Work",
      "text" : "In this work, we collected a corpus of supporting context for translating ambiguous words. We examined how baseline context-aware translation models use context, and demonstrated how context annotations can improve context-aware translation accuracy. While we obtain promising results for context-aware translation by testing one method for attention regularization, our publicly available SCAT dataset could enable future research on alternative attention regularizers. Moreover, our analyses demonstrate that humans rely on different types of context for PAR and WSD in English-French translation, similar user studies can be conducted to better understand the usage of context in other ambiguous discourse phenomena, such as ellipsis, or other language pairs. We also find that regularizing attention using SCAT for PAR especially improves anaphoric pronoun translation, suggesting that supervising attention using supporting context from different tasks may help models resolve other types of ambiguities.\nOne caveat regarding our method for collecting supporting context from humans is the difference between translation, translating text from the input, and disambiguation, choosing between translation candidates. During translation, humans might pay more attention to the source sentences to understand the source material, but during disambiguation, we have shown that human translators rely more often on the target sentences. One reason why the model benefits more from increased attention on source may be because the model is trained and\nevaluated to perform translation, not disambiguation. A future step would be to explore alternative methods for extracting supporting context, such as eye-tracking during translation (O’Brien, 2009)."
    }, {
      "heading" : "8 Acknowledgements",
      "text" : "We would like to thank Emma Landry, Guillaume Didier, Wilson Jallet, Baptiste Moreau-Pernet, Pierre Gianferrara, and Duy-Anh Alexandre for helping with a preliminary English-French translation study. We would also like to thank Nikolai Vogler for the original interface for data annotation, and the anonymous reviewers for their helpful feedback. This work was supported by the European Research Council (ERC StG DeepSPIN 758969), by the P2020 programs MAIA and Unbabel4EU (LISBOA-01-0247-FEDER-045909 and LISBOA-01-0247-FEDER-042671), and by the Fundação para a Ciência e Tecnologia through contract UIDB/50008/2020."
    }, {
      "heading" : "B Generating Data for Word Sense Disambiguation",
      "text" : "To automatically generate contrastive examples of WSD, we identify English words that have multiple French translations. To do so, we first extract word alignments from OpenSubtitles2018 using AWESOME-align (Dou and Neubig, 2021) and obtain:\nAm = {〈xi, yj〉 : xi ∈ xm, yj ∈ ym},\nwhere for each word pair 〈xi, yj〉, xi and yj are semantically similar to each other in context.\nFor each pair 〈xi, yj〉 ∈ Am, we compute the number of times the lemmatized source word type (vx = lemma(xi)) along with its POS tag (tx = tag(xi)) is aligned to the lemmatized target word type (vy = lemma(yj)): c(vx, tx, vy). Then, we extract tuples of source types with its POS tags 〈vx, tx〉 that have at least two target words that have been aligned at least 50 times (|{vy|c(vx, tx, vy) ≥ 50}| ≥ 2). Finally, we filter out the source tuples which have an entropy\nH(vx, tx) less than a pre-selected threshold z. This entropy is computed using the conditional probability of a target translation given the source word type and its POS tag as follows:\np := p(vy|vx, tx) = c(vx, tx, vy)\nc(vx, tx)\nH(vx, tx) = ∑\nvy∈trans(vx,tx)\n−p loge p\nwhere trans(vx, tx) is the set of target translations for the source tuple 〈vx, tx〉 and p(vy|vx, tx) is the conditional probability of a given target translation vy for the source word type vx and its POS tag tx\nOut of the 394 extracted word groups, we manually validate and retain 201 groups and then classify them into 64 synonymous and 137 nonsynonymous word groups (Table 10). We create contrastive translations by extracting sentence pairs containing an ambiguous word pair, and replacing the translation of the polysemous English word by a different French word in the same group. For word groups with synonymous French words, we only retain examples where the French word appears within the previous 5 sentences to enforce lexical consistency, as otherwise the different French words may be interchangeable."
    }, {
      "heading" : "C Experimental Setup",
      "text" : "C.1 Data preprocessing\nWe use the English and French data from the publicly available OpenSubtitles2018 dataset (Lison et al., 2018). We first clean the data by selecting sentence pairs with a relative time overlap between source and target language subtitle frames of at least 0.9 to reduce noise. Each data is then encoded with byte-pair encoding (Sennrich et al., 2016) using SentencePiece (Kudo and Richardson, 2018), with source and target vocabularies of 32k tokens.\nC.2 Training configuration\nWe follow the Transformer base (Vaswani et al., 2017) configuration in all our experiments, with N = 6 encoder and decoder layers, h = 8 attention heads, hidden size dmodel = 512 and feedforward size dff = 2048. We use the learning rate schedule and regularization described in Vaswani et al. (2017). We train using the Adam optimizer (Kingma and Ba, 2015) with β1 = 0.9, β2 = 0.98.\nC.3 Attention regularization setups For both attnreg-rand and attnreg-pre, we experiment performing regularization on different model attentions at different layers. For the attention regularization loss, In all experiments, we compute the attention regularization loss on the first attention head with λ = 10, and we divide the loss by the length of the input. We give the results for all setups in Table 11.\nC.4 Evaluation To calculate BLEU scores we use SacreBLEU BLEU+case.mixed+numrefs.1+smooth.exp+tok.13 a+version.1.4.14 (Post, 2018) and for COMET we use wmt-large-da-estimator-1719 3. We test for statistical significance with p < 0.05 using bootstrap sampling on a single run (Koehn, 2004).\n3https://github.com/Unbabel/COMET"
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Deriving machine attention from human rationales",
      "author" : [ "Yujia Bao", "Shiyu Chang", "Mo Yu", "Regina Barzilay." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1903–1913, Brussels, Belgium. Association",
      "citeRegEx" : "Bao et al\\.,? 2018",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2018
    }, {
      "title" : "Sequence classification with human attention",
      "author" : [ "Maria Barrett", "Joachim Bingel", "Nora Hollenstein", "Marek Rei", "Anders Søgaard." ],
      "venue" : "Proceedings of the 22nd Conference on Computational Natural Language Learning, pages 302–312, Brussels, Bel-",
      "citeRegEx" : "Barrett et al\\.,? 2018",
      "shortCiteRegEx" : "Barrett et al\\.",
      "year" : 2018
    }, {
      "title" : "Evaluating discourse phenomena in neural machine translation",
      "author" : [ "Rachel Bawden", "Rico Sennrich", "Alexandra Birch", "Barry Haddow." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Bawden et al\\.,? 2018",
      "shortCiteRegEx" : "Bawden et al\\.",
      "year" : 2018
    }, {
      "title" : "Re-evaluating the role of Bleu in machine translation research",
      "author" : [ "Chris Callison-Burch", "Miles Osborne", "Philipp Koehn." ],
      "venue" : "11th Conference of the European Chapter of the Association for Computational Linguistics, Trento, Italy. Association for",
      "citeRegEx" : "Callison.Burch et al\\.,? 2006",
      "shortCiteRegEx" : "Callison.Burch et al\\.",
      "year" : 2006
    }, {
      "title" : "Modeling discourse structure for document-level neural machine translation",
      "author" : [ "Junxuan Chen", "Xiang Li", "Jiarui Zhang", "Chulun Zhou", "Jianwei Cui", "Bin Wang", "Jinsong Su." ],
      "venue" : "Proceedings of the First Workshop on Automatic Simultaneous Translation,",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Less is more: Attention supervision with counterfactuals for text classification",
      "author" : [ "Seungtaek Choi", "Haeju Park", "Jinyoung Yeo", "Seungwon Hwang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Choi et al\\.,? 2020",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2020
    }, {
      "title" : "Word alignment by fine-tuning embeddings on parallel corpora",
      "author" : [ "Zi-Yi Dou", "Graham Neubig." ],
      "venue" : "Conference of the European Chapter of the Association for Computational Linguistics (EACL).",
      "citeRegEx" : "Dou and Neubig.,? 2021",
      "shortCiteRegEx" : "Dou and Neubig.",
      "year" : 2021
    }, {
      "title" : "Detecting word sense disambiguation biases in machine translation for model-agnostic adversarial attacks",
      "author" : [ "Denis Emelin", "Ivan Titov", "Rico Sennrich." ],
      "venue" : "arXiv preprint arXiv:2011.01846.",
      "citeRegEx" : "Emelin et al\\.,? 2020",
      "shortCiteRegEx" : "Emelin et al\\.",
      "year" : 2020
    }, {
      "title" : "A pronoun test suite evaluation of the English– German MT systems at WMT 2018",
      "author" : [ "Liane Guillou", "Christian Hardmeier", "Ekaterina Lapshinova-Koltunski", "Sharid Loáiciga." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation:",
      "citeRegEx" : "Guillou et al\\.,? 2018",
      "shortCiteRegEx" : "Guillou et al\\.",
      "year" : 2018
    }, {
      "title" : "Discourse in statistical machine translation",
      "author" : [ "Christian Hardmeier." ],
      "venue" : "a survey and a case study. Discours. Revue de linguistique, psycholinguistique et informatique. A journal of linguistics, psycholinguistics and computational linguistics.",
      "citeRegEx" : "Hardmeier.,? 2012",
      "shortCiteRegEx" : "Hardmeier.",
      "year" : 2012
    }, {
      "title" : "spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing",
      "author" : [ "Matthew Honnibal", "Ines Montani." ],
      "venue" : "To appear.",
      "citeRegEx" : "Honnibal and Montani.,? 2017",
      "shortCiteRegEx" : "Honnibal and Montani.",
      "year" : 2017
    }, {
      "title" : "Diving deep into context-aware neural machine translation",
      "author" : [ "Jingjing Huo", "Christian Herold", "Yingbo Gao", "Leonard Dahlmann", "Shahram Khadivi", "Hermann Ney." ],
      "venue" : "Proceedings of the Fifth Conference on Machine Translation, pages 604–616, On-",
      "citeRegEx" : "Huo et al\\.,? 2020",
      "shortCiteRegEx" : "Huo et al\\.",
      "year" : 2020
    }, {
      "title" : "Does neural machine translation benefit from larger context? arXiv preprint arXiv:1704.05135",
      "author" : [ "Sebastien Jean", "Stanislas Lauly", "Orhan Firat", "Kyunghyun Cho" ],
      "venue" : null,
      "citeRegEx" : "Jean et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Jean et al\\.",
      "year" : 2017
    }, {
      "title" : "When and why is document-level context useful in neural machine translation",
      "author" : [ "Yunsu Kim", "Duc Thanh Tran", "Hermann Ney" ],
      "venue" : "In Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT",
      "citeRegEx" : "Kim et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Statistical significance tests for machine translation evaluation",
      "author" : [ "Philipp Koehn." ],
      "venue" : "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388– 395, Barcelona, Spain. Association for Computa-",
      "citeRegEx" : "Koehn.,? 2004",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2004
    }, {
      "title" : "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "author" : [ "Taku Kudo", "John Richardson." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System",
      "citeRegEx" : "Kudo and Richardson.,? 2018",
      "shortCiteRegEx" : "Kudo and Richardson.",
      "year" : 2018
    }, {
      "title" : "Has machine translation achieved human parity? a case for document-level evaluation",
      "author" : [ "Samuel Läubli", "Rico Sennrich", "Martin Volk." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4791–4796,",
      "citeRegEx" : "Läubli et al\\.,? 2018",
      "shortCiteRegEx" : "Läubli et al\\.",
      "year" : 2018
    }, {
      "title" : "Does multi-encoder help? a case study on contextaware neural machine translation",
      "author" : [ "Bei Li", "Hui Liu", "Ziyang Wang", "Yufan Jiang", "Tong Xiao", "Jingbo Zhu", "Tongran Liu", "Changliang Li." ],
      "venue" : "arXiv preprint arXiv:2005.03393.",
      "citeRegEx" : "Li et al\\.,? 2020a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Does multi-encoder help? a case study on contextaware neural machine translation",
      "author" : [ "Bei Li", "Hui Liu", "Ziyang Wang", "Yufan Jiang", "Tong Xiao", "Jingbo Zhu", "Tongran Liu", "Changliang Li." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for",
      "citeRegEx" : "Li et al\\.,? 2020b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "OpenSubtitles2018: Statistical rescoring of sentence alignments in large, noisy parallel corpora",
      "author" : [ "Pierre Lison", "Jörg Tiedemann", "Milen Kouylekov." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC",
      "citeRegEx" : "Lison et al\\.,? 2018",
      "shortCiteRegEx" : "Lison et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural machine translation with supervised attention",
      "author" : [ "Lemao Liu", "Masao Utiyama", "Andrew Finch", "Eiichiro Sumita." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages",
      "citeRegEx" : "Liu et al\\.,? 2016",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Exploiting argument information to improve event detection via supervised attention mechanisms",
      "author" : [ "Shulin Liu", "Yubo Chen", "Kang Liu", "Jun Zhao." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Liu et al\\.,? 2017",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2017
    }, {
      "title" : "Document-level neural MT: A systematic comparison",
      "author" : [ "António Lopes", "M. Amin Farajian", "Rachel Bawden", "Michael Zhang", "André F.T. Martins." ],
      "venue" : "Proceedings of the 22nd Annual Conference of the European Association for Machine Transla-",
      "citeRegEx" : "Lopes et al\\.,? 2020",
      "shortCiteRegEx" : "Lopes et al\\.",
      "year" : 2020
    }, {
      "title" : "Document context neural machine translation with memory networks",
      "author" : [ "Sameen Maruf", "Gholamreza Haffari." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1275–",
      "citeRegEx" : "Maruf and Haffari.,? 2018",
      "shortCiteRegEx" : "Maruf and Haffari.",
      "year" : 2018
    }, {
      "title" : "Selective attention for context-aware neural machine translation",
      "author" : [ "Sameen Maruf", "André F.T. Martins", "Gholamreza Haffari." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Maruf et al\\.,? 2019",
      "shortCiteRegEx" : "Maruf et al\\.",
      "year" : 2019
    }, {
      "title" : "Supervised attentions for neural machine translation",
      "author" : [ "Haitao Mi", "Zhiguo Wang", "Abe Ittycheriah." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2283–2288, Austin, Texas. Association for Compu-",
      "citeRegEx" : "Mi et al\\.,? 2016",
      "shortCiteRegEx" : "Mi et al\\.",
      "year" : 2016
    }, {
      "title" : "Document-level neural machine translation with hierarchical attention networks",
      "author" : [ "Lesly Miculicich", "Dhananjay Ram", "Nikolaos Pappas", "James Henderson." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Miculicich et al\\.,? 2018",
      "shortCiteRegEx" : "Miculicich et al\\.",
      "year" : 2018
    }, {
      "title" : "A large-scale test set for the evaluation of context-aware pronoun translation in neural machine translation",
      "author" : [ "Mathias Müller", "Annette Rios", "Elena Voita", "Rico Sennrich." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Pa-",
      "citeRegEx" : "Müller et al\\.,? 2018",
      "shortCiteRegEx" : "Müller et al\\.",
      "year" : 2018
    }, {
      "title" : "A test set for discourse translation from Japanese to English",
      "author" : [ "Masaaki Nagata", "Makoto Morishita." ],
      "venue" : "Proceedings of the 12th Language Resources and Evaluation Conference, pages 3704– 3709, Marseille, France. European Language Re-",
      "citeRegEx" : "Nagata and Morishita.,? 2020",
      "shortCiteRegEx" : "Nagata and Morishita.",
      "year" : 2020
    }, {
      "title" : "Eye tracking in translation process research: methodological challenges and solutions. Methodology, technology and innovation in translation process research, 38:251–266",
      "author" : [ "Sharon O’Brien" ],
      "venue" : null,
      "citeRegEx" : "O.Brien.,? \\Q2009\\E",
      "shortCiteRegEx" : "O.Brien.",
      "year" : 2009
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "A call for clarity in reporting BLEU scores",
      "author" : [ "Matt Post." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186– 191, Brussels, Belgium. Association for Computational Linguistics.",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "Evaluating explanations: How much do explanations from the teacher aid students",
      "author" : [ "Danish Pruthi", "Bhuwan Dhingra", "Livio Baldini Soares", "Michael Collins", "Zachary C. Lipton", "Graham Neubig", "William W. Cohen" ],
      "venue" : null,
      "citeRegEx" : "Pruthi et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Pruthi et al\\.",
      "year" : 2020
    }, {
      "title" : "COMET: A neural framework for MT evaluation",
      "author" : [ "Ricardo Rei", "Craig Stewart", "Ana C Farinha", "Alon Lavie." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685–2702, Online. Associa-",
      "citeRegEx" : "Rei et al\\.,? 2020",
      "shortCiteRegEx" : "Rei et al\\.",
      "year" : 2020
    }, {
      "title" : "A structured review of the validity of BLEU",
      "author" : [ "Ehud Reiter." ],
      "venue" : "Computational Linguistics, 44(3):393– 401.",
      "citeRegEx" : "Reiter.,? 2018",
      "shortCiteRegEx" : "Reiter.",
      "year" : 2018
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Linguistically-informed self-attention for semantic role labeling",
      "author" : [ "Emma Strubell", "Patrick Verga", "Daniel Andor", "David Weiss", "Andrew McCallum." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Strubell et al\\.,? 2018",
      "shortCiteRegEx" : "Strubell et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural machine translation with extended context",
      "author" : [ "Jörg Tiedemann", "Yves Scherrer." ],
      "venue" : "Proceedings of the Third Workshop on Discourse in Machine Translation, pages 82–92, Copenhagen, Denmark. Association for Computational Linguistics.",
      "citeRegEx" : "Tiedemann and Scherrer.,? 2017",
      "shortCiteRegEx" : "Tiedemann and Scherrer.",
      "year" : 2017
    }, {
      "title" : "Attaining the unattainable? reassessing claims of human parity in neural machine translation",
      "author" : [ "Antonio Toral", "Sheila Castilho", "Ke Hu", "Andy Way." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 113–123, Brus-",
      "citeRegEx" : "Toral et al\\.,? 2018",
      "shortCiteRegEx" : "Toral et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning to remember translation history with a continuous cache",
      "author" : [ "Zhaopeng Tu", "Y. Liu", "Shuming Shi", "T. Zhang." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:407–420.",
      "citeRegEx" : "Tu et al\\.,? 2018",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30, pages 5998–6008. Cur-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Context-aware monolingual repair for neural machine translation",
      "author" : [ "Elena Voita", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-",
      "citeRegEx" : "Voita et al\\.,? 2019a",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    }, {
      "title" : "When a good translation is wrong in context: Context-aware machine translation improves on deixis, ellipsis, and lexical cohesion",
      "author" : [ "Elena Voita", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for",
      "citeRegEx" : "Voita et al\\.,? 2019b",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    }, {
      "title" : "Context-aware neural machine translation learns anaphora resolution",
      "author" : [ "Elena Voita", "Pavel Serdyukov", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Voita et al\\.,? 2018",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploiting cross-sentence context for neural machine translation",
      "author" : [ "Longyue Wang", "Zhaopeng Tu", "Andy Way", "Qun Liu." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2826–2831, Copenhagen,",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Improving the transformer translation model with document-level context",
      "author" : [ "Jiacheng Zhang", "Huanbo Luan", "Maosong Sun", "Feifei Zhai", "Jingfang Xu", "Min Zhang", "Yang Liu." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Document embedding enhanced event detection with hierarchical and supervised attention",
      "author" : [ "Yue Zhao", "Xiaolong Jin", "Yuanzhuo Wang", "Xueqi Cheng." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Zhao et al\\.,? 2018",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2018
    }, {
      "title" : "Fine-grained sentiment analysis with faithful attention",
      "author" : [ "Ruiqi Zhong", "Steven Shao", "Kathleen R. McKeown." ],
      "venue" : "CoRR, abs/1908.06870.",
      "citeRegEx" : "Zhong et al\\.,? 2019",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2019
    }, {
      "title" : "A lexicon-based supervised attention model for neural sentiment analysis",
      "author" : [ "Yicheng Zou", "Tao Gui", "Qi Zhang", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 868–877, Santa Fe, New Mexico,",
      "citeRegEx" : "Zou et al\\.,? 2018",
      "shortCiteRegEx" : "Zou et al\\.",
      "year" : 2018
    }, {
      "title" : "We train using the Adam optimizer (Kingma and Ba, 2015) with β1",
      "author" : [ "Vaswani" ],
      "venue" : "= 0.9,",
      "citeRegEx" : "Vaswani,? \\Q2017\\E",
      "shortCiteRegEx" : "Vaswani",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "There is a growing consensus in machine translation research that it is necessary to move beyond sentence-level translation and incorporate document-level context (Guillou et al., 2018; Läubli et al., 2018; Toral et al., 2018).",
      "startOffset" : 163,
      "endOffset" : 226
    }, {
      "referenceID" : 18,
      "context" : "There is a growing consensus in machine translation research that it is necessary to move beyond sentence-level translation and incorporate document-level context (Guillou et al., 2018; Läubli et al., 2018; Toral et al., 2018).",
      "startOffset" : 163,
      "endOffset" : 226
    }, {
      "referenceID" : 40,
      "context" : "There is a growing consensus in machine translation research that it is necessary to move beyond sentence-level translation and incorporate document-level context (Guillou et al., 2018; Läubli et al., 2018; Toral et al., 2018).",
      "startOffset" : 163,
      "endOffset" : 226
    }, {
      "referenceID" : 29,
      "context" : "is provided, models often perform poorly on evaluation of relatively simple discourse phenomena (Müller et al., 2018; Bawden et al., 2018; Voita et al., 2019b,a; Lopes et al., 2020) and rely on spurious word co-occurences during translation of polysemous words (Emelin et al.",
      "startOffset" : 96,
      "endOffset" : 181
    }, {
      "referenceID" : 3,
      "context" : "is provided, models often perform poorly on evaluation of relatively simple discourse phenomena (Müller et al., 2018; Bawden et al., 2018; Voita et al., 2019b,a; Lopes et al., 2020) and rely on spurious word co-occurences during translation of polysemous words (Emelin et al.",
      "startOffset" : 96,
      "endOffset" : 181
    }, {
      "referenceID" : 24,
      "context" : "is provided, models often perform poorly on evaluation of relatively simple discourse phenomena (Müller et al., 2018; Bawden et al., 2018; Voita et al., 2019b,a; Lopes et al., 2020) and rely on spurious word co-occurences during translation of polysemous words (Emelin et al.",
      "startOffset" : 96,
      "endOffset" : 181
    }, {
      "referenceID" : 8,
      "context" : ", 2020) and rely on spurious word co-occurences during translation of polysemous words (Emelin et al., 2020).",
      "startOffset" : 87,
      "endOffset" : 108
    }, {
      "referenceID" : 45,
      "context" : "Some evidence suggests that models attend to uninformative tokens (Voita et al., 2018) and do not use contextual information adequately (Kim et al.",
      "startOffset" : 66,
      "endOffset" : 86
    }, {
      "referenceID" : 14,
      "context" : ", 2018) and do not use contextual information adequately (Kim et al., 2019).",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : "Current NMT models employ encoder-decoder architectures (Bahdanau et al., 2015; Vaswani et al., 2017).",
      "startOffset" : 56,
      "endOffset" : 101
    }, {
      "referenceID" : 42,
      "context" : "Current NMT models employ encoder-decoder architectures (Bahdanau et al., 2015; Vaswani et al., 2017).",
      "startOffset" : 56,
      "endOffset" : 101
    }, {
      "referenceID" : 9,
      "context" : "Well-known examples of these phenomena include gender-marked anaphoric pronouns (Guillou et al., 2018) and maintenance of lexical coherence (Läubli et al.",
      "startOffset" : 80,
      "endOffset" : 102
    }, {
      "referenceID" : 18,
      "context" : ", 2018) and maintenance of lexical coherence (Läubli et al., 2018).",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 39,
      "context" : "methods for incorporating context (§6), but even simple concatenation (Tiedemann and Scherrer, 2017), which prepends the previous source or target sentences to the current sentence separated by a 〈BRK〉 tag, achieves comparable performance to",
      "startOffset" : 70,
      "endOffset" : 100
    }, {
      "referenceID" : 24,
      "context" : "more sophisticated approaches, especially in highresource scenarios (Lopes et al., 2020).",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 32,
      "context" : "BLEU (Papineni et al., 2002) is most widely used to evaluate MT, but it can be poorly correlated with human evaluation (CallisonBurch et al.",
      "startOffset" : 5,
      "endOffset" : 28
    }, {
      "referenceID" : 36,
      "context" : ", 2002) is most widely used to evaluate MT, but it can be poorly correlated with human evaluation (CallisonBurch et al., 2006; Reiter, 2018).",
      "startOffset" : 98,
      "endOffset" : 140
    }, {
      "referenceID" : 35,
      "context" : "Recently, a number of neural evaluation methods, such as COMET (Rei et al., 2020), have shown better correlation with human judgement.",
      "startOffset" : 63,
      "endOffset" : 81
    }, {
      "referenceID" : 10,
      "context" : "Nevertheless, common automatic metrics have limited ability to evaluate discourse in MT (Hardmeier, 2012).",
      "startOffset" : 88,
      "endOffset" : 105
    }, {
      "referenceID" : 29,
      "context" : "As a remedy to this, researchers often use contrastive test sets for a targeted discourse phenomenon (Müller et al., 2018), such as pronoun anaphora resolution and word sense disambiguation, to verify if the model ranks the correct translation of an ambiguous sentence higher than the incorrect translation.",
      "startOffset" : 101,
      "endOffset" : 122
    }, {
      "referenceID" : 45,
      "context" : "PAR, and WSD to a lesser extent, have been commonly studied to evaluate context-aware NMT models (Voita et al., 2018; Lopes et al., 2020; Müller et al., 2018; Huo et al., 2020; Nagata and Morishita, 2020).",
      "startOffset" : 97,
      "endOffset" : 204
    }, {
      "referenceID" : 24,
      "context" : "PAR, and WSD to a lesser extent, have been commonly studied to evaluate context-aware NMT models (Voita et al., 2018; Lopes et al., 2020; Müller et al., 2018; Huo et al., 2020; Nagata and Morishita, 2020).",
      "startOffset" : 97,
      "endOffset" : 204
    }, {
      "referenceID" : 29,
      "context" : "PAR, and WSD to a lesser extent, have been commonly studied to evaluate context-aware NMT models (Voita et al., 2018; Lopes et al., 2020; Müller et al., 2018; Huo et al., 2020; Nagata and Morishita, 2020).",
      "startOffset" : 97,
      "endOffset" : 204
    }, {
      "referenceID" : 12,
      "context" : "PAR, and WSD to a lesser extent, have been commonly studied to evaluate context-aware NMT models (Voita et al., 2018; Lopes et al., 2020; Müller et al., 2018; Huo et al., 2020; Nagata and Morishita, 2020).",
      "startOffset" : 97,
      "endOffset" : 204
    }, {
      "referenceID" : 30,
      "context" : "PAR, and WSD to a lesser extent, have been commonly studied to evaluate context-aware NMT models (Voita et al., 2018; Lopes et al., 2020; Müller et al., 2018; Huo et al., 2020; Nagata and Morishita, 2020).",
      "startOffset" : 97,
      "endOffset" : 204
    }, {
      "referenceID" : 21,
      "context" : "This set includes 14K examples from the OpenSubtitles2018 dataset (Lison et al., 2018) with occurrences of the English pronouns “it” and “they” that correspond to the French translations “il” or “elle” and “ils” or “elles”, with 3.",
      "startOffset" : 66,
      "endOffset" : 86
    }, {
      "referenceID" : 11,
      "context" : "We use spaCy (Honnibal and Montani, 2017) to predict part-of-speech (POS) tags of selected words and syntactic dependencies between selected words and the ambiguous word.",
      "startOffset" : 13,
      "endOffset" : 41
    }, {
      "referenceID" : 42,
      "context" : "We incorporate the 5 previous source and target sentences as context to the base Transformer (Vaswani et al., 2017) by prepending the previous sentences to the current sentence, separated by a 〈BRK〉 tag, as proposed by Tiedemann and Scherrer (2017).",
      "startOffset" : 93,
      "endOffset" : 115
    }, {
      "referenceID" : 21,
      "context" : "For document translation, we use the English and French data from OpenSubtitles2018 (Lison et al., 2018), which we clean then split into 16M training, 10,036 development, and 9,740 testing samples.",
      "startOffset" : 84,
      "endOffset" : 104
    }, {
      "referenceID" : 19,
      "context" : "Context-aware models often suffer from error propagation when using previously decoded output tokens as the target context (Li et al., 2020a).",
      "startOffset" : 123,
      "endOffset" : 141
    }, {
      "referenceID" : 27,
      "context" : "Though attention is usually learned in an unsupervised manner, recent work supervises attention with word alignments (Mi et al., 2016; Liu et al., 2016), event arguments and trigger words (Liu et al.",
      "startOffset" : 117,
      "endOffset" : 152
    }, {
      "referenceID" : 22,
      "context" : "Though attention is usually learned in an unsupervised manner, recent work supervises attention with word alignments (Mi et al., 2016; Liu et al., 2016), event arguments and trigger words (Liu et al.",
      "startOffset" : 117,
      "endOffset" : 152
    }, {
      "referenceID" : 23,
      "context" : ", 2016), event arguments and trigger words (Liu et al., 2017; Zhao et al., 2018), syntactic dependencies (Strubell",
      "startOffset" : 43,
      "endOffset" : 80
    }, {
      "referenceID" : 48,
      "context" : ", 2016), event arguments and trigger words (Liu et al., 2017; Zhao et al., 2018), syntactic dependencies (Strubell",
      "startOffset" : 43,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : "Our work is closely related to a large body of work that supervises attention using human rationales for text classification (Barrett et al., 2018; Bao et al., 2018; Zhong et al., 2019; Choi et al., 2020; Pruthi et al., 2020).",
      "startOffset" : 125,
      "endOffset" : 225
    }, {
      "referenceID" : 1,
      "context" : "Our work is closely related to a large body of work that supervises attention using human rationales for text classification (Barrett et al., 2018; Bao et al., 2018; Zhong et al., 2019; Choi et al., 2020; Pruthi et al., 2020).",
      "startOffset" : 125,
      "endOffset" : 225
    }, {
      "referenceID" : 49,
      "context" : "Our work is closely related to a large body of work that supervises attention using human rationales for text classification (Barrett et al., 2018; Bao et al., 2018; Zhong et al., 2019; Choi et al., 2020; Pruthi et al., 2020).",
      "startOffset" : 125,
      "endOffset" : 225
    }, {
      "referenceID" : 6,
      "context" : "Our work is closely related to a large body of work that supervises attention using human rationales for text classification (Barrett et al., 2018; Bao et al., 2018; Zhong et al., 2019; Choi et al., 2020; Pruthi et al., 2020).",
      "startOffset" : 125,
      "endOffset" : 225
    }, {
      "referenceID" : 34,
      "context" : "Our work is closely related to a large body of work that supervises attention using human rationales for text classification (Barrett et al., 2018; Bao et al., 2018; Zhong et al., 2019; Choi et al., 2020; Pruthi et al., 2020).",
      "startOffset" : 125,
      "endOffset" : 225
    }, {
      "referenceID" : 31,
      "context" : "A future step would be to explore alternative methods for extracting supporting context, such as eye-tracking during translation (O’Brien, 2009).",
      "startOffset" : 129,
      "endOffset" : 144
    } ],
    "year" : 2021,
    "abstractText" : "Context-aware machine translation models are designed to leverage contextual information, but often fail to do so. As a result, they inaccurately disambiguate pronouns and polysemous words that require context for resolution. In this paper, we ask several questions: What contexts do human translators use to resolve ambiguous words? Are models paying large amounts of attention to the same context? What if we explicitly train them to do so? To answer these questions, we introduce SCAT (Supporting Context for Ambiguous Translations), a new English-French dataset comprising supporting context words for 14K translations that professional translators found useful for pronoun disambiguation. Using SCAT, we perform an in-depth analysis of the context used to disambiguate, examining positional and lexical characteristics of the supporting words. Furthermore, we measure the degree of alignment between the model’s attention scores and the supporting context from SCAT, and apply a guided attention strategy to encourage agreement between the two.1",
    "creator" : "LaTeX with hyperref"
  }
}