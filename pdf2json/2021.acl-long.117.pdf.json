{
  "name" : "2021.acl-long.117.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Language Model as an Annotator: Exploring DialoGPT for Dialogue Summarization",
    "authors" : [ "Xiachong Feng", "Xiaocheng Feng", "Libo Qin", "Bing Qin", "Ting Liu" ],
    "emails" : [ "xiachongfeng@ir.hit.edu.cn", "xcfeng@ir.hit.edu.cn", "lbqin@ir.hit.edu.cn", "bqin@ir.hit.edu.cn", "tliu@ir.hit.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1479–1491\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1479"
    }, {
      "heading" : "1 Introduction",
      "text" : "Dialogue summarization aims to generate a succinct summary while retaining essential information of the dialogue (Gurevych and Strube, 2004; Chen and Yang, 2020). Theoretically, Peyrard (2019) point out that a good summary is intuitively related to three aspects, including Informativeness, Redundancy and Relevance.\nTo this end, previous works have taken the above three aspects into account by incorporating auxiliary annotations into the dialogue. To improve informativeness, some works annotated linguistically specific words (e.g., nouns and verbs), domain terminologies and topic words in the dialogue (Riedhammer et al., 2008; Koay et al., 2020; Zhao et al., 2020). To reduce redundancy, some works\n∗Corresponding author. 1Our codes are available at: https://github.com/\nxcfcode/PLM_annotator\nused sentence similarity-based methods to annotate redundant utterances. (Zechner, 2002; Murray et al., 2005). To improve relevance, some works annotated topics for the dialogue (Li et al., 2019; Liu et al., 2019; Chen and Yang, 2020). However, these annotations are usually obtained via open-domain toolkits, which are not suitable for dialogues, or require manual annotations, which are labor-consuming.\nTo alleviate the above problem, we explore the pre-trained language model as an unsupervised annotator to automatically provide annotations for the dialogue. Recently, some works have investigated the use of pre-trained language models in an unsupervised manner. For example, Sainz and Rigau (2021) exploited pre-trained models for assigning domain labels to WordNet synsets. The successful recipe is that a model is obtained extensive knowledge via pre-training on a huge volume of data. When it comes to the dialogue domain, DialoGPT (Zhang et al., 2020b) is a SOTA conversational response generation model, which is pre-trained on the massive dialogue data. Therefore, we draw support from DialoGPT and present our DialoGPT annotator, which can perform three dialogue annotation tasks, including keywords extraction, redundancy detection and topic segmentation, to measure informativeness, redundancy and relevance of the input dialogue, respectively.\nKeywords Extraction aims to automatically identify important words in the dialogue (shown in Figure 1(a)). Our DialoGPT annotator extracts unpredictable words as keywords. We assume that keywords contain high information, which are difficult to be predicted considering both background knowledge encoded in the DialoGPT and contextual information of dialogue context. Redundancy Detection aims to detect redundant utterances that have no core contribution to the overall meaning of the dialogue (shown in Figure 1(b)). Our DialoGPT\nannotator detects utterances that are useless for dialogue context representation as redundant. We assume that if adding a new utterance does not change the dialogue context representation, then this utterance has no effect on predicting the response, so it is redundant. Topic Segmentation aims to divide a dialogue into topically coherent segments (shown in Figure 1(c)). Our DialoGPT annotator inserts a topic segmentation point before one utterance if it is unpredictable. We assume that if an utterance is difficult to be inferred from the dialogue context based on DialoGPT, this utterance may belong to a new topic.\nWe use our DialoGPT annotator to annotate the SAMSum (Gliwa et al., 2019) and AMI (Carletta et al., 2005) datasets. Each annotation is converted into a specific identifier and we insert them into the dialogue text. Then, we employ pre-traind BART (Lewis et al., 2020) and non pre-trained PGN (See et al., 2017) as our summarizers. Extensive experimental results show that our method can obtain consistent and remarkable improvements over strong baselines on both datasets and achieves new stateof-the-art performance on the SAMSum dataset."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "In this section, we will describe the task definition as well as the background of DialoGPT."
    }, {
      "heading" : "2.1 Task Definition",
      "text" : "Given an input dialogue D, a dialogue summarizer aims to produce a condensed summary S, where D consists of |D| utterances [u1, u2, ...u|D|] and S consists of |S| words [s1, s2, ...s|S|]. Each utterance ui is compose of a sequence of words\n[ui,1, ui,2, ...ui,|ui|,EOSi], where i ∈ [1 : |D|] and EOSi indicates the end of the utterance. Besides, each utterance ui associates with a speaker pi. Thus, this task can be formalized as producing the summary S given the dialogue sequence: D = [p1, u1,1, ...,EOS1, ..., p|D|, u|D|,1, ...,EOS|D|]"
    }, {
      "heading" : "2.2 DialoGPT",
      "text" : "DialoGPT (Zhang et al., 2020b) is a neural conversational response generation model, which inherits from GPT-2 (Radford et al., 2019) and is trained on 147M conversation-like exchanges extracted from Reddit comment chains. There are 3 different sizes of the model with total parameters of 117M, 345M and 762M respectively. It achieves state-of-the-art results over various dialogue generation benchmarks. Given the dialogue context ui−1 = [ui−1,1, ..., ui−1,|ui−1|,EOSi−1], DialoGPT aims to produce the response ui = [ui,1, ..., ui,|ui|,EOSi], which can be formalized as the conditional probability of P (ui|ui−1). It first takes the context word sequence of no more than 1024 tokens and outputs the representation of the sequence hi = (h i−1,1, ...,h i−1,|ui−1|,h i−1,EOSi−1), where h i−1,EOSi−1 can be viewed as the representation of dialogue context ui−1. Then, DialoGPT starts decoding the response by attending to the context token representations and partially decoded response tokens until reaching EOS. The loss function is the negative log-likelihood of the response word sequence LDialoGPT = − ∑|ui|\nt=1 log p (ui,t|ui,1 . . . ui,t−1, ui−1). It’s worth noting that DialoGPT tokenizes texts with the same byte-pair encoding as GPT-2, thus either context or response tokens are tokenized into subwords."
    }, {
      "heading" : "3 Method",
      "text" : "In this section, we will first introduce our DialoGPT annotator. The workflow consists of three steps (1) dialogue preprocessing; (2) DialoGPT forward passing; (3) annotation. The overall framework is shown in Figure 2. Then, we will describe our dialogue summarizer, including BART and PGN."
    }, {
      "heading" : "3.1 Dialogue Preprocessing",
      "text" : "Dialogue preprocessing aims to transform the original dialogue D = [p1, u1,1, ...,EOS1, ..., p|D|, u|D|,1, ...,EOS|D|] into the format that DialoGPT can process.\nSpecifically, we transform it into two formats. The first one is context-response pairs (shown in Figure 2(a)). Given a dialogue D, two adjacent utterances (ui−1, ui) are combined into a contextresponse pair, where i ∈ [2 : |D|] . The second one is dialogue sequence (shown in Figure 2(b)). All the utterances in the dialogue D are serialized into a sequence [u1,1, ...,EOS1, ..., u|D|,1, ...,EOS|D|], with EOS separates each utterance.\nNote that either for context-response pairs or the dialogue sequence, we do not take speaker information p into consideration. The reason is that DialoGPT is trained on a huge volume of conversational data without speaker information. Even so, Zhang et al. (2020b) proved that DialoGPT can simulate real-world dialogues in various scenes and has already learned diverse response generation patterns between the same speakers or different speakers according to the given context."
    }, {
      "heading" : "3.2 DialoGPT Forward Passing",
      "text" : "DialoGPT forward passing has two purposes. (1) For each context-response pair, we aim to get the word-level and utterance-level predicted losses for the response (shown in Figure 2(c)). (2) For the dialogue sequence, we aim to get the representations for each EOS (shown in Figure 2(d)).\nFor the first purpose, given one context-response pair (ui−1, ui), we input the context words ui−1 = [ui−1,1, ui−1,2, ..., ui−1,|ui−1|,EOSi−1] into the DialoGPT and start to decode the response. At each decode step t, we calculate the negative loglikelihood between the predicted distribution and the golden target from the given response.\nlossi,t = − log p (ui,t|ui,<t, ui−1)\nlossi = 1\n|ui|+ 1 |ui|+1∑ t=1 lossi,t (1)\nwhere lossi,t and lossi are the predicted losses for each word and each utterance respectively2.\nFor the second purpose, after the single forward pass of DialoGPT over the dialogue sequence, we can get representations H for each token on the top of the DialoGPT. Afterward, we extract all representations for each EOS.\nhEOS1 ,hEOS2 , ...,hEOS|D| = H (EOS) (2)\nwhere each hEOSi can be viewed as the representation for the dialogue context [u1, ..., ui].\n2Note that DialoGPT uses BPE to tokenize texts, thus, losses are calculated at the sub-word level. We recover the word-level predicted loss by averaging the losses of multiple sub-words. Besides, since the first utterance u1 can only be served as the context, so we do not compute loss for u1."
    }, {
      "heading" : "3.3 Annotation",
      "text" : ""
    }, {
      "heading" : "3.3.1 Keywords Extraction: DialoGPTKE",
      "text" : "Motivation Considering both background knowledge encoded in the DialoGPT and contextual information of the dialogue context, if one word in the golden response is difficult to be inferred from DialoGPT, we assume that it contains high information and can be viewed as a keyword.\nGiven a dialogue D, we have loss lossi,j for each word ui,j , where i ∈ [2 : |D|]. We extract rKE percent of words with the highest loss as keywords, where rKE is a hyper-parameter3. Moreover, the names of all speakers P mentioned in the dialogue are also added into the keywords set. Finally, we append a specific tag #KEY# and the keywords to the end of the original dialogue D. The new dialogue with keywords annotation is DKE = [p1, u1,1, ...,︸ ︷︷ ︸\nD\n#KEY#,P,Key1,Key2, ...︸ ︷︷ ︸ keywords ].4\n3We use a heuristic rule to predetermine the possible value of rKE by calculating the average of length of summaries (remove stopwords) divided by the length of dialogues in the train set. We search the best rKE based on the calculated score.\n4In experiments, we find that the predicted loss for the first word of each utterance is extremely high, probably due to the first word in the response is the most uncertain and hard to be predicted. Thus, we ignore the first word of each utterance."
    }, {
      "heading" : "3.3.2 Redundancy Detection: DialoGPTRD",
      "text" : "Motivation DialoGPT inherits a decoder architecture, where one token attends to all previous tokens to aggregate information. Thus, given the representation hEOSi for each EOSi, it can be viewed as the representation for the dialogue context [u1, u2, ..., ui]. Adding a new utterance ui+1, if the new context representation hEOSi+1 is similar to the previous hEOSi , we assume that the new utterance ui+1 brings little information and has small effects on predicting the response, thus ui+1 becomes a redundant utterance.\nWe start with the last two dialogue context representations hEOS|D|−1 and hEOS|D| , and calculate the cosine similarity between them. If the similarity score exceeds the threshold tRD, the utterance u|D| is detected as redundant. tRD is a hyper-parameter. If the similarity score doesn’t exceed the threshold tRD, we move forward one step to calculate the similarity between hEOS|D|−2 and hEOS|D|−1 , and repeat the process until reaching hEOS1 . An example is shown in Figure 3.\nWe insert a specific tag [RD] before each redundant utterance. For example, if utterance u1 is redundant, the new dialogue with redundant utterances annotation is DRD = [p1,[RD], u1,1, ...,EOS1, ..., p|D|, ...,EOS|D|]."
    }, {
      "heading" : "3.3.3 Topic Segmentation: DialoGPTTS",
      "text" : "Motivation DialoGPT is skilled in generating the context-consistent response. Therefore, if the response is difficult to be predicted given the context based on DialoGPT, we assume the response may belong to another topic and there is a topic segmentation between the context and response.\nGiven a dialogue D, we have loss lossi for each utterance ui, where i ∈ [2 : |D|]. We select rTS percent of utterances with the highest loss as topic segmentation points. rTS is a hyper-parameter5. Before each selected utterance, we insert a specific tag [TS]. For example, if there is a segmentation point between utterance u1 and utterance u2, the new dialogue with topic annotation is DTS = [p1, u1,1, ...,EOS1,[TS], p2, u2,1, ...,EOS2, ...].\n5We use a heuristic rule to predetermine the possible value of rTS by calculating the average of the number of summary sentences divided by the number of dialogue utterances in the train set. This is based on the observation that each sentence in golden summary tends to correspond to one topic of the dialogue. We search the best rTS based on the calculated score."
    }, {
      "heading" : "3.4 Summarizer",
      "text" : "We employ two kinds of summarizer, one is BART (Lewis et al., 2020), which is a Transformer-based model and pre-trained on a huge volume of data. The other one is PGN (See et al., 2017), which is a LSTM-based model. Both models inherit a typical sequence-to-sequence framework, which first encodes the source dialogue D to distributed representations and then generates the target summary S with the decoder.\nBART BART adopts the Transformer (Vaswani et al., 2017) as the backbone architecture. It first map the source dialogue into distributed representations, based on which a decoder generates the target sequence:\nXN = ENCODER(X 0) N:= n=1\nFFN ( ATT(X n−1) ) YM = DECODER(Y 0,XN )\nM := m=1\nFFN ( ATT ( ATT(Ym−1),XN )) (3)\nwhere N := n=1\ndenotes N identical encoding layers, M := m=1 denotes M identical decoding layers, X 0 denotes the sum of the word embeddings X emb and position embeddings X pos of D, Y 0 denotes that of the shifted right S, FFN(·) denotes a positionwise feed-forward network, and ATT(·) denotes a multi-head attention. Residual connection (He et al., 2016) and layer normalization (Ba et al., 2016) are used in each sub-layer, which are suppressed in Equation 3 for clarity. Finally, the output representation YM of the decoder is projected into the vocabulary space and the decoder outputs the highest probability token.\nPGN PGN is a hybrid model of the typical Seq2Seq Attention model (Nallapati et al., 2016) and Pointer-Network (Vinyals et al., 2015). The input dialogue is fed into the LSTM encoder token by token, producing the encoder hidden states. The decoder receives word embedding of the previous word and generates a distribution to decide the target token, retaining decoder hidden states. PGN not only allows to generate from the fixed vocabulary, but also allows to copy from the input tokens.\nTraining Objective Model parameters θ are trained to maximize the conditional likelihood of\nthe outputs in a parallel training corpus (D,S):\nargmax θ ∑ (D,S)∈(D,S) log p(S |D; θ). (4)"
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We experiment on 2 datasets (statistics in Table 1): SAMSum (Gliwa et al., 2019) is a humangenerated dialogue summary dataset, which contains dialogues in various scenes of the real-life. AMI (Carletta et al., 2005) is a meeting summary dataset. Each meeting contains four participants and is about a remote control design project."
    }, {
      "heading" : "4.2 Implementation Details",
      "text" : "DialoGPT We initialize DialoGPT with DialoGPTlarge6. For SAMSum, we set keywords extraction ratio rKE to 15, similarity threshold tRD to 0.99 and topic segmentation ratio rTS to 15. For AMI, rKE is 4, tRD is 0.95 and rTS is 5 7. BART We initialize BART with bart.large8 . For fine-tuning on SAMSum, the learning rate is set to 3e-05, the dropout rate is 0.1, the warmup is set to 400. At the test process, beam size is 5, minimum decoded length is 5 and maximum length is 100. PGN The word embedding size is set to 300 and initialized with the pre-trained GloVe vector. The dimension of encoder and pointer decoder is set to 200. The dropout is set to 0.5. The learning rate is 0.001. At the test process, beam size is 10, minimum decoded length is 280 and maximum length is 4509.\n6https://huggingface.co/transformers 7We show more hyper-parameter search results for SAM-\nSum and AMI datasets in the supplementary file. 8https://github.com/pytorch/fairseq 9https://github.com/OpenNMT/OpenNMT-py"
    }, {
      "heading" : "Extractive",
      "text" : ""
    }, {
      "heading" : "Abstractive",
      "text" : ""
    }, {
      "heading" : "Ours",
      "text" : ""
    }, {
      "heading" : "4.3 Baselines and Metrics",
      "text" : "For SAMSum, LONGEST-3 views the first three utterances as the summary. TextRank (Mihalcea and Tarau, 2004) is a traditional graph-based method. Transformer (Vaswani et al., 2017) is a seq2seq method based on full self-attention operations. D-HGN (Feng et al., 2020a) incorporates commonsense knowledge to help understand dialogues. TGDGA (Zhao et al., 2020) uses topic words and models graph structures for dialogues. DialoGPT (Zhang et al., 2020b) means that finetuning DialoGPT on the SAMSum. MV-BART (Chen and Yang, 2020) is a BART-based method that incorporates topic and stage information.\nFor AMI, SummaRunner (Nallapati et al., 2017) is an extractive method based on hierarchical RNN network. UNS (Shang et al., 2018) is a fully unsupervised and graph-based method. TopicSeg (Li et al., 2019) incorporates topics to model the meeting. HMNet (Zhu et al., 2020) is a transformer-based method that incorporates POS and entity information and is pre-trained on news summarization dataset.\nWe adopt ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2020a) for evaluating our models."
    }, {
      "heading" : "Extractive",
      "text" : ""
    }, {
      "heading" : "Abstractive",
      "text" : ""
    }, {
      "heading" : "Ours",
      "text" : ""
    }, {
      "heading" : "4.4 Automatic Evaluation",
      "text" : "The results on SAMSum and AMI are shown in Table 2 and 3 respectively. We can see that using our annotated datasets DKE, DRD and DTS, both BART and PGN can obtain improvements. Furthermore, our BART(DALL) achieves SOTA performance.\nFor SAMSum, it’s worth noting that BART(DKE) performs better compared with BART(DRD) and BART(DTS). We attribute this to the fact that keywords can retain essential information for shorter dialogues. For AMI, PGN(DRD) contributes the most, which shows the importance of detecting redundancy in verbose meeting transcripts. Although HMNet and TopicSeg achieve better scores, HMNet needs news summarization dataset to pre-train the model and TopicSeg designs complex attention mechanism to incorporate topic information.\nIn terms of new embedding-based metric BERTScore (shown in Table 4), our method BART(DALL) and PGN(DALL) can consistently outperform the baseline models10.\n10Evaluation details are shown in the supplementary file."
    }, {
      "heading" : "4.5 Human Evaluation",
      "text" : "We conduct a human evaluation of the dialogue summary to assess its informativeness, conciseness and coverage. Informativeness measures how well the summary includes key information. Conciseness measures how well the summary discards the redundant information. Coverage measures how well the summary covers each part of the dialogue.\nWe randomly sample 100 dialogues (SAMSum) and 10 meetings (AMI) with corresponding generated summaries to conduct the evaluation. In order to reduce variance caused by humans, we have 4 human evaluators and they were asked to rate each summary on a scale of 1 to 5 (higher is better) for each metric. The results are shown in Table 5.\nWe can see that our method can achieve higher scores in all three metrics. Especially, combined with DRD, our model can get the best score in conciseness. Besides, combined with DTS, our model can perform better in coverage. However, HMNet gets the best score in informativeness and coverage. We argue this is because HMNet forces a minimum summary length of 400. Due to this, it scores the worst in conciseness. For the AMI, we also find there is still a gap between the scores of generated summaries and the scores of golden summaries, indicating that the AMI is more difficult."
    }, {
      "heading" : "Rule-Based Methods",
      "text" : ""
    }, {
      "heading" : "Traditional Methods",
      "text" : ""
    }, {
      "heading" : "Pre-trained Language Model-Based Methods",
      "text" : ""
    }, {
      "heading" : "Ours",
      "text" : ""
    }, {
      "heading" : "4.6 Analysis",
      "text" : "Effect of DialoGPTKE. To verify the effectiveness of our DialoGPTKE method, we fine-tune BART on SAMSum, which is annotated by various keywords extraction methods. The results are shown in Table 6. We can see that our method achieves higher scores. The results also show that entities play an important role in the summary generation. Besides, combined with DialoGPT embeddings, KeyBERT can get better results.\nTo give a quantitative evaluation, we view reference summary words as golden keywords and calculate the precision, recall and F1 scores for extracted keywords. The results are shown in Table 7. Directly using entities as keywords can get the best precision score. However, both TextRank and Entities perform poorly in recall. Our method gets the best score in terms of F1 and its advantage is mainly reflected in recall score, which shows our method can extract more diverse keywords."
    }, {
      "heading" : "SAMSum",
      "text" : ""
    }, {
      "heading" : "AMI",
      "text" : "Effect of DialoGPTRD. To verify the effectiveness of our DialoGPTRD method, we compare it with a Rule-based method (Dinarelli et al., 2009), which annotates utterances without noun, verb and adjective as redundant. The results are shown in Table 8. We can see that our method performs better. Especially, our method shows more advantages for long and verbose meeting transcripts in the AMI.\nEffect of DialoGPTTS. To verify the effectiveness of our DialoGPTTS method, we compare it with the C99 algorithm (Choi, 2000), which is a sentence similarity-based segmentation method. Chen and Yang (2020) enhance it with BERT (Devlin et al., 2019) embeddings. We further combine the algorithm with DialoGPT embeddings. The results are shown in Table 9. We can see that our method can get comparable results with the strong baseline C99(w/ DialoGPT emb). For AMI, combined with golden topic annotation, PGN can achieve the best result, which shows modeling topics is an essential task for dialogue summarization."
    }, {
      "heading" : "4.7 Case Study",
      "text" : "Figure 4 shows summaries generated by different models for an example dialogue in the SAMSum dataset. We can see that BART (Lewis et al., 2020) tends to generate long and redundant summaries. By incorporating topic and stage information, MVBART (Chen and Yang, 2020) can generate summaries that cover main topics of the dialogue. However, it still suffers from redundancy problem. Our BART(DALL) can get higher ROUGE scores while generating better summaries. The generated summary can include extracted keywords and correspond to each topic of the dialogue. We also find that even some redundant utterances have already been detected, our model still generate the summary contains some redundant information. We"
    }, {
      "heading" : "AMI",
      "text" : "attribute this to the fact that the small dataset leads to insufficient training of the model."
    }, {
      "heading" : "5 Related Work",
      "text" : "Dialogue Summarization Current works mainly incorporate auxiliary information to help better modeling dialogues. Some works used various types of keywords to identify the core part of the dialogue, including entities (Zhu et al., 2020), domain terminologies (Koay et al., 2020) and topic words (Zhao et al., 2020). Some works aimed to reduce redundancy, Zechner (2002); Murray et al. (2005) used sentence-level similarity-based methods. Some works incorporate topics as a coarsegrained dialogue structure (Li et al., 2019; Liu et al., 2019; Chen and Yang, 2020). Other works also explored dialogue act (Goo and Chen, 2018), dialogue discourse (Feng et al., 2020b) and commonsense knowledge (Feng et al., 2020a). In this paper, we combine three types of auxiliary information to help better modeling dialogues, including keywords, redundant utterances and topics. Pre-trained Language Models Pre-trained models such as BERT (Devlin et al., 2019) and GPT-3 (Brown et al., 2020) have advanced various NLP tasks. On one hand, some works utilized the knowledge contained in pre-trained models by finetuning on supervised data of downstream tasks (Qin et al., 2019; Liu and Lapata, 2019; Qin et al., 2020). On the other hand, some works examined the knowledge in an unsupervised manner (Jiang et al., 2020; Xiao et al., 2020; Lin et al., 2020). Ku-\nmar et al. (2020) explored pre-trained models for conditional data augmentation. Wang et al. (2020) used the knowledge in pre-trained models to construct knowledge graphs. In this paper, we belong to the second paradigm and propose our DialoGPT annotator that can perform three annotation tasks in an unsupervised manner."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We investigate to use DialoGPT as unsupervised annotators for dialogue summarization, including keywords extraction, redundancy detection and topic segmentation. We conduct our DialoGPT annotator on two datasets, SAMSum and AMI. Experimental results show that our method consistently obtains improvements upon pre-traind summarizer (BART) and non pre-trained summarizer (PGN) on both datasets. Besides, combining all three annotations, our summarizer can achieve new state-of-the-art performance on the SAMSum dataset."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is supported by the National Key R&D Program of China via grant 2018YFB1005103 and National Natural Science Foundation of China (NSFC) via grant 61906053 and 61976073. We thank all the anonymous reviewers for their insightful comments. We also thank Lifu Huang and Xinwei Geng for helpful discussion."
    }, {
      "heading" : "A Evaluation Details",
      "text" : "For ROUGE (Lin, 2004), we employ Py-rouge11 package to evaluate our models following Gliwa et al. (2019). For BERTScore (Zhang et al., 2020a), we use the official implementation12 to evaluate our models. The detailed command line for BERTScore is bert-score -r golden.txt -c gen.txt --lang en."
    }, {
      "heading" : "B Ablation Studies for Annotations",
      "text" : "To further verify the effectiveness of our method, we conduct ablation studies for each annotation. The results are shown in Table 10 and Table 11. We can find that: (1) For both datasets, training summarizers based on datasets with two of three annotations can obtain improvements. (2) For both datasets, training summarizers based on\n11https://pypi.org/project/py-rouge/ 12https://github.com/Tiiiger/bert score"
    }, {
      "heading" : "Ours",
      "text" : ""
    }, {
      "heading" : "Ours",
      "text" : "datasets with two of three annotations can surpass corresponding summarizers that are trained based on datasets with one type of annotation (e.g., BART(DKE+RD) is better than BART(DKE) and BART(DRD)). (3) Compared with summarizers that are trained on DRD+TS and DKE+RD, summarizers that are trained on DKE+TS get relatively small improvements on both datasets. Nevertheless, it indicates that DialoGPTKE and DialoGPTTS still have non-overlapping parts. (4) Combining all three annotations, both summarizers can achieve the best results in all ROUGE scores."
    }, {
      "heading" : "C Hyper-parameter Search Results",
      "text" : "Tables 12 to 17 show the hyper-parameter search results. Finally, for SAMSum (Gliwa et al., 2019), we set keywords extraction ratio rKE to 15, similarity threshold tRD to 0.99 and topic segmentation ratio rTS to 15. for AMI (Carletta et al., 2005), rKE is 4, tRD is 0.95 and rTS is 5."
    } ],
    "references" : [ {
      "title" : "Layer normalization",
      "author" : [ "Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey E Hinton." ],
      "venue" : "arXiv.",
      "citeRegEx" : "Ba et al\\.,? 2016",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2016
    }, {
      "title" : "Language models are few-shot learners",
      "author" : [ "Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "Ilya Sutskever", "Dario Amodei." ],
      "venue" : "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Informa-",
      "citeRegEx" : "Chess et al\\.,? 2020",
      "shortCiteRegEx" : "Chess et al\\.",
      "year" : 2020
    }, {
      "title" : "The ami meeting corpus: A pre-announcement",
      "author" : [ "Jean Carletta", "Simone Ashby", "Sebastien Bourban", "Mike Flynn", "Mael Guillemot", "Thomas Hain", "Jaroslav Kadlec", "Vasilis Karaiskos", "Wessel Kraaij", "Melissa Kronenthal" ],
      "venue" : null,
      "citeRegEx" : "Carletta et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Carletta et al\\.",
      "year" : 2005
    }, {
      "title" : "Multi-view sequenceto-sequence models with conversational structure for abstractive dialogue summarization",
      "author" : [ "Jiaao Chen", "Diyi Yang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Chen and Yang.,? 2020",
      "shortCiteRegEx" : "Chen and Yang.",
      "year" : 2020
    }, {
      "title" : "Advances in domain independent linear text segmentation",
      "author" : [ "Freddy Y.Y. Choi." ],
      "venue" : "1st Meeting of the North American Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Choi.,? 2000",
      "shortCiteRegEx" : "Choi.",
      "year" : 2000
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Annotating spoken dialogs: from speech segments to dialog acts and frame semantics",
      "author" : [ "Marco Dinarelli", "Silvia Quarteroni", "Sara Tonelli", "Alessandro Moschitti", "Giuseppe Riccardi." ],
      "venue" : "Proceedings of SRSL 2009, the 2nd Workshop on Semantic Rep-",
      "citeRegEx" : "Dinarelli et al\\.,? 2009",
      "shortCiteRegEx" : "Dinarelli et al\\.",
      "year" : 2009
    }, {
      "title" : "Incorporating commonsense knowledge into abstractive dialogue summarization via heterogeneous graph networks",
      "author" : [ "Xiachong Feng", "X. Feng", "B. Qin", "T. Liu." ],
      "venue" : "ArXiv, abs/2010.10044.",
      "citeRegEx" : "Feng et al\\.,? 2020a",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2020
    }, {
      "title" : "2020b. Dialogue discourse-aware graph model and data augmentation for meeting summarization",
      "author" : [ "Xiachong Feng", "Xiaocheng Feng", "Bing Qin", "Xinwei Geng" ],
      "venue" : null,
      "citeRegEx" : "Feng et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2020
    }, {
      "title" : "SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization",
      "author" : [ "Bogdan Gliwa", "Iwona Mochol", "Maciej Biesek", "Aleksander Wawer." ],
      "venue" : "Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 70–79,",
      "citeRegEx" : "Gliwa et al\\.,? 2019",
      "shortCiteRegEx" : "Gliwa et al\\.",
      "year" : 2019
    }, {
      "title" : "Abstractive dialogue summarization with sentence-gated modeling optimized by dialogue acts",
      "author" : [ "Chih-Wen Goo", "Yun-Nung Chen." ],
      "venue" : "2018 IEEE Spoken Language Technology Workshop (SLT), pages 735–742.",
      "citeRegEx" : "Goo and Chen.,? 2018",
      "shortCiteRegEx" : "Goo and Chen.",
      "year" : 2018
    }, {
      "title" : "Keybert: Minimal keyword extraction with bert",
      "author" : [ "Maarten Grootendorst" ],
      "venue" : null,
      "citeRegEx" : "Grootendorst.,? \\Q2020\\E",
      "shortCiteRegEx" : "Grootendorst.",
      "year" : 2020
    }, {
      "title" : "Semantic similarity applied to spoken dialogue summarization",
      "author" : [ "Iryna Gurevych", "Michael Strube." ],
      "venue" : "COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics, pages 764–770, Geneva, Switzerland. COLING.",
      "citeRegEx" : "Gurevych and Strube.,? 2004",
      "shortCiteRegEx" : "Gurevych and Strube.",
      "year" : 2004
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770–778.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423–438",
      "author" : [ "Zhengbao Jiang", "Frank F. Xu", "Jun Araki", "Graham Neubig" ],
      "venue" : null,
      "citeRegEx" : "Jiang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "How domain terminology affects meeting summarization performance",
      "author" : [ "Jia Jin Koay", "Alexander Roustai", "Xiaojin Dai", "Dillon Burns", "Alec Kerrigan", "Fei Liu." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics,",
      "citeRegEx" : "Koay et al\\.,? 2020",
      "shortCiteRegEx" : "Koay et al\\.",
      "year" : 2020
    }, {
      "title" : "Data augmentation using pre-trained transformer models",
      "author" : [ "Varun Kumar", "Ashutosh Choudhary", "Eunah Cho." ],
      "venue" : "Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems, pages 18–26, Suzhou, China. Association for Com-",
      "citeRegEx" : "Kumar et al\\.,? 2020",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2020
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Keep meeting summaries on topic: Abstractive multi-modal meeting summarization",
      "author" : [ "Manling Li", "Lingyu Zhang", "Heng Ji", "Richard J. Radke." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models",
      "author" : [ "Bill Yuchen Lin", "Seyeon Lee", "Rahul Khanna", "Xiang Ren." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Lin et al\\.,? 2020",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Text summarization with pretrained encoders",
      "author" : [ "Yang Liu", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
      "citeRegEx" : "Liu and Lapata.,? 2019",
      "shortCiteRegEx" : "Liu and Lapata.",
      "year" : 2019
    }, {
      "title" : "Topic-aware pointergenerator networks for summarizing spoken conversations",
      "author" : [ "Zhengyuan Liu", "A. Ng", "Sheldon Lee Shao Guang", "AiTi Aw", "Nancy F. Chen." ],
      "venue" : "2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 814–",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "TextRank: Bringing order into text",
      "author" : [ "Rada Mihalcea", "Paul Tarau." ],
      "venue" : "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 404–411, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Mihalcea and Tarau.,? 2004",
      "shortCiteRegEx" : "Mihalcea and Tarau.",
      "year" : 2004
    }, {
      "title" : "Extractive summarization of meeting recordings",
      "author" : [ "Gabriel Murray", "S. Renals", "J. Carletta." ],
      "venue" : "INTERSPEECH.",
      "citeRegEx" : "Murray et al\\.,? 2005",
      "shortCiteRegEx" : "Murray et al\\.",
      "year" : 2005
    }, {
      "title" : "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents",
      "author" : [ "Ramesh Nallapati", "Feifei Zhai", "Bowen Zhou." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 31.",
      "citeRegEx" : "Nallapati et al\\.,? 2017",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2017
    }, {
      "title" : "Abstractive text summarization using sequence-to-sequence rnns and beyond",
      "author" : [ "Ramesh Nallapati", "Bowen Zhou", "C.D. Santos", "Çaglar Gülçehre", "Bing Xiang." ],
      "venue" : "CoNLL.",
      "citeRegEx" : "Nallapati et al\\.,? 2016",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2016
    }, {
      "title" : "Don’t give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization",
      "author" : [ "Shashi Narayan", "Shay B. Cohen", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Narayan et al\\.,? 2018",
      "shortCiteRegEx" : "Narayan et al\\.",
      "year" : 2018
    }, {
      "title" : "A simple theoretical model of importance for summarization",
      "author" : [ "Maxime Peyrard." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1059–1073, Florence, Italy. Association for Computational Linguistics.",
      "citeRegEx" : "Peyrard.,? 2019",
      "shortCiteRegEx" : "Peyrard.",
      "year" : 2019
    }, {
      "title" : "Stanza: A python natural language processing toolkit for many human languages",
      "author" : [ "Peng Qi", "Yuhao Zhang", "Yuhui Zhang", "Jason Bolton", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Qi et al\\.,? 2020",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2020
    }, {
      "title" : "A stack-propagation framework with token-level intent detection for spoken language understanding",
      "author" : [ "Libo Qin", "Wanxiang Che", "Yangming Li", "Haoyang Wen", "T. Liu." ],
      "venue" : "EMNLP/IJCNLP.",
      "citeRegEx" : "Qin et al\\.,? 2019",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2019
    }, {
      "title" : "Co-gat: A co-interactive graph attention network for joint dialog act recognition and sentiment classification",
      "author" : [ "Libo Qin", "Zhouyang Li", "Wanxiang Che", "Minheng Ni", "Ting Liu." ],
      "venue" : "ArXiv, abs/2012.13260.",
      "citeRegEx" : "Qin et al\\.,? 2020",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "A keyphrase based approach to interactive meeting summarization",
      "author" : [ "K. Riedhammer", "B. Favre", "Dilek Z. Hakkani-Tür." ],
      "venue" : "2008 IEEE Spoken Language Technology Workshop, pages 153–156.",
      "citeRegEx" : "Riedhammer et al\\.,? 2008",
      "shortCiteRegEx" : "Riedhammer et al\\.",
      "year" : 2008
    }, {
      "title" : "Ask2transformers: Zero-shot domain labelling with pre-trained language models",
      "author" : [ "Oscar Sainz", "German Rigau" ],
      "venue" : null,
      "citeRegEx" : "Sainz and Rigau.,? \\Q2021\\E",
      "shortCiteRegEx" : "Sainz and Rigau.",
      "year" : 2021
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J. Liu", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073–",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "Unsupervised abstractive meeting summarization with multisentence compression and budgeted submodular",
      "author" : [ "Guokan Shang", "Wensi Ding", "Zekun Zhang", "Antoine Tixier", "Polykarpos Meladianos", "Michalis Vazirgiannis", "Jean-Pierre Lorré" ],
      "venue" : null,
      "citeRegEx" : "Shang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Shang et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Pointer networks",
      "author" : [ "Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly." ],
      "venue" : "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec,",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Language models are open knowledge graphs",
      "author" : [ "C. Wang", "Xiao Liu", "D. Song." ],
      "venue" : "ArXiv, abs/2010.11967.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Modeling content importance for summarization with pre-trained language models",
      "author" : [ "Liqiang Xiao", "Lu Wang", "Hao He", "Yaohui Jin." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods",
      "citeRegEx" : "Xiao et al\\.,? 2020",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatic summarization of open-domain multiparty dialogues in diverse genres",
      "author" : [ "Klaus Zechner." ],
      "venue" : "Computational Linguistics, 28(4):447–485.",
      "citeRegEx" : "Zechner.,? 2002",
      "shortCiteRegEx" : "Zechner.",
      "year" : 2002
    }, {
      "title" : "Bertscore: Evaluating text generation with bert",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "DIALOGPT : Largescale generative pre-training for conversational response generation",
      "author" : [ "Yizhe Zhang", "Siqi Sun", "Michel Galley", "Yen-Chun Chen", "Chris Brockett", "Xiang Gao", "Jianfeng Gao", "Jingjing Liu", "Bill Dolan." ],
      "venue" : "Proceedings of the 58th An-",
      "citeRegEx" : "Zhang et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving abstractive dialogue summarization with graph structures and topic words",
      "author" : [ "Lulu Zhao", "Weiran Xu", "Jun Guo." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 437–449, Barcelona, Spain (Online). In-",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    }, {
      "title" : "A hierarchical network for abstractive meeting summarization with cross-domain pretraining",
      "author" : [ "Chenguang Zhu", "Ruochen Xu", "Michael Zeng", "Xuedong Huang." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 194–",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 43,
      "context" : "In this paper, we show how DialoGPT (Zhang et al., 2020b), a pre-trained model for conversational response generation, can be developed as an unsupervised dialogue annotator, which takes advantage of dialogue background knowledge encoded in DialoGPT.",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 12,
      "context" : "Dialogue summarization aims to generate a succinct summary while retaining essential information of the dialogue (Gurevych and Strube, 2004; Chen and Yang, 2020).",
      "startOffset" : 113,
      "endOffset" : 161
    }, {
      "referenceID" : 3,
      "context" : "Dialogue summarization aims to generate a succinct summary while retaining essential information of the dialogue (Gurevych and Strube, 2004; Chen and Yang, 2020).",
      "startOffset" : 113,
      "endOffset" : 161
    }, {
      "referenceID" : 33,
      "context" : ", nouns and verbs), domain terminologies and topic words in the dialogue (Riedhammer et al., 2008; Koay et al., 2020; Zhao et al., 2020).",
      "startOffset" : 73,
      "endOffset" : 136
    }, {
      "referenceID" : 15,
      "context" : ", nouns and verbs), domain terminologies and topic words in the dialogue (Riedhammer et al., 2008; Koay et al., 2020; Zhao et al., 2020).",
      "startOffset" : 73,
      "endOffset" : 136
    }, {
      "referenceID" : 44,
      "context" : ", nouns and verbs), domain terminologies and topic words in the dialogue (Riedhammer et al., 2008; Koay et al., 2020; Zhao et al., 2020).",
      "startOffset" : 73,
      "endOffset" : 136
    }, {
      "referenceID" : 18,
      "context" : "To improve relevance, some works annotated topics for the dialogue (Li et al., 2019; Liu et al., 2019; Chen and Yang, 2020).",
      "startOffset" : 67,
      "endOffset" : 123
    }, {
      "referenceID" : 22,
      "context" : "To improve relevance, some works annotated topics for the dialogue (Li et al., 2019; Liu et al., 2019; Chen and Yang, 2020).",
      "startOffset" : 67,
      "endOffset" : 123
    }, {
      "referenceID" : 3,
      "context" : "To improve relevance, some works annotated topics for the dialogue (Li et al., 2019; Liu et al., 2019; Chen and Yang, 2020).",
      "startOffset" : 67,
      "endOffset" : 123
    }, {
      "referenceID" : 43,
      "context" : "When it comes to the dialogue domain, DialoGPT (Zhang et al., 2020b) is a SOTA conversational response generation model, which is pre-trained on the massive dialogue data.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : "Figure 1: Example dialogue from SAMSum (Gliwa et al., 2019) with the human annotated summary.",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 9,
      "context" : "We use our DialoGPT annotator to annotate the SAMSum (Gliwa et al., 2019) and AMI (Carletta et al.",
      "startOffset" : 53,
      "endOffset" : 73
    }, {
      "referenceID" : 17,
      "context" : "Then, we employ pre-traind BART (Lewis et al., 2020) and non pre-trained PGN (See et al.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 35,
      "context" : ", 2020) and non pre-trained PGN (See et al., 2017) as our summarizers.",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 43,
      "context" : "DialoGPT (Zhang et al., 2020b) is a neural conversational response generation model, which in-",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 32,
      "context" : "herits from GPT-2 (Radford et al., 2019) and is trained on 147M conversation-like exchanges extracted from Reddit comment chains.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 17,
      "context" : "We employ two kinds of summarizer, one is BART (Lewis et al., 2020), which is a Transformer-based model and pre-trained on a huge volume of data.",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 35,
      "context" : "The other one is PGN (See et al., 2017), which is a LSTM-based model.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 13,
      "context" : "Residual connection (He et al., 2016) and layer normalization (Ba et al.",
      "startOffset" : 20,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : ", 2016) and layer normalization (Ba et al., 2016) are used in each sub-layer, which are suppressed in Equation 3 for clarity.",
      "startOffset" : 32,
      "endOffset" : 49
    }, {
      "referenceID" : 26,
      "context" : "PGN PGN is a hybrid model of the typical Seq2Seq Attention model (Nallapati et al., 2016) and Pointer-Network (Vinyals et al.",
      "startOffset" : 65,
      "endOffset" : 89
    }, {
      "referenceID" : 9,
      "context" : "We experiment on 2 datasets (statistics in Table 1): SAMSum (Gliwa et al., 2019) is a humangenerated dialogue summary dataset, which contains dialogues in various scenes of the real-life.",
      "startOffset" : 60,
      "endOffset" : 80
    }, {
      "referenceID" : 23,
      "context" : "TextRank (Mihalcea and Tarau, 2004) is a traditional graph-based method.",
      "startOffset" : 9,
      "endOffset" : 35
    }, {
      "referenceID" : 37,
      "context" : "Transformer (Vaswani et al., 2017) is a seq2seq method based on full self-attention operations.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 7,
      "context" : "D-HGN (Feng et al., 2020a) incorporates commonsense knowledge to help understand dialogues.",
      "startOffset" : 6,
      "endOffset" : 26
    }, {
      "referenceID" : 44,
      "context" : "TGDGA (Zhao et al., 2020) uses topic words and models graph structures for dialogues.",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 43,
      "context" : "DialoGPT (Zhang et al., 2020b) means that finetuning DialoGPT on the SAMSum.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 3,
      "context" : "MV-BART (Chen and Yang, 2020) is a BART-based method that incorporates topic and stage information.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 25,
      "context" : "For AMI, SummaRunner (Nallapati et al., 2017) is an extractive method based on hierarchical RNN network.",
      "startOffset" : 21,
      "endOffset" : 45
    }, {
      "referenceID" : 36,
      "context" : "UNS (Shang et al., 2018) is a fully unsupervised and graph-based method.",
      "startOffset" : 4,
      "endOffset" : 24
    }, {
      "referenceID" : 18,
      "context" : "TopicSeg (Li et al., 2019) incorporates topics to model the meeting.",
      "startOffset" : 9,
      "endOffset" : 26
    }, {
      "referenceID" : 45,
      "context" : "HMNet (Zhu et al., 2020) is a transformer-based method that incorporates POS and entity information and is pre-trained on news summarization dataset.",
      "startOffset" : 6,
      "endOffset" : 24
    }, {
      "referenceID" : 20,
      "context" : "We adopt ROUGE (Lin, 2004) and BERTScore (Zhang et al.",
      "startOffset" : 15,
      "endOffset" : 26
    }, {
      "referenceID" : 42,
      "context" : "We adopt ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2020a) for evaluating our models.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 27,
      "context" : "Topic words are obtained by a pre-trained LDA model (Narayan et al., 2018).",
      "startOffset" : 52,
      "endOffset" : 74
    }, {
      "referenceID" : 11,
      "context" : "KeyBERT (Grootendorst, 2020) leverages pre-trained language model embeddings to create keywords.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 6,
      "context" : "To verify the effectiveness of our DialoGPTRD method, we compare it with a Rule-based method (Dinarelli et al., 2009), which annotates utterances without noun, verb and adjective as redundant.",
      "startOffset" : 93,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : "To verify the effectiveness of our DialoGPTTS method, we compare it with the C99 algorithm (Choi, 2000), which is a sentence similarity-based segmentation method.",
      "startOffset" : 91,
      "endOffset" : 103
    }, {
      "referenceID" : 5,
      "context" : "Chen and Yang (2020) enhance it with BERT (Devlin et al., 2019) embeddings.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 17,
      "context" : "We can see that BART (Lewis et al., 2020) tends to generate long and redundant summaries.",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 3,
      "context" : "By incorporating topic and stage information, MVBART (Chen and Yang, 2020) can generate summaries that cover main topics of the dialogue.",
      "startOffset" : 53,
      "endOffset" : 74
    }, {
      "referenceID" : 4,
      "context" : "C99 (Choi, 2000) segments dialogues based on intersentence similarities.",
      "startOffset" : 4,
      "endOffset" : 16
    }, {
      "referenceID" : 45,
      "context" : "Some works used various types of keywords to identify the core part of the dialogue, including entities (Zhu et al., 2020), domain terminologies (Koay et al.",
      "startOffset" : 104,
      "endOffset" : 122
    }, {
      "referenceID" : 15,
      "context" : ", 2020), domain terminologies (Koay et al., 2020) and topic words (Zhao et al.",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 18,
      "context" : "Some works incorporate topics as a coarsegrained dialogue structure (Li et al., 2019; Liu et al., 2019; Chen and Yang, 2020).",
      "startOffset" : 68,
      "endOffset" : 124
    }, {
      "referenceID" : 22,
      "context" : "Some works incorporate topics as a coarsegrained dialogue structure (Li et al., 2019; Liu et al., 2019; Chen and Yang, 2020).",
      "startOffset" : 68,
      "endOffset" : 124
    }, {
      "referenceID" : 3,
      "context" : "Some works incorporate topics as a coarsegrained dialogue structure (Li et al., 2019; Liu et al., 2019; Chen and Yang, 2020).",
      "startOffset" : 68,
      "endOffset" : 124
    }, {
      "referenceID" : 10,
      "context" : "Other works also explored dialogue act (Goo and Chen, 2018), dialogue discourse (Feng et al.",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : ", 2020b) and commonsense knowledge (Feng et al., 2020a).",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 5,
      "context" : "Pre-trained Language Models Pre-trained models such as BERT (Devlin et al., 2019) and GPT-3 (Brown et al.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 30,
      "context" : "On one hand, some works utilized the knowledge contained in pre-trained models by finetuning on supervised data of downstream tasks (Qin et al., 2019; Liu and Lapata, 2019; Qin et al., 2020).",
      "startOffset" : 132,
      "endOffset" : 190
    }, {
      "referenceID" : 21,
      "context" : "On one hand, some works utilized the knowledge contained in pre-trained models by finetuning on supervised data of downstream tasks (Qin et al., 2019; Liu and Lapata, 2019; Qin et al., 2020).",
      "startOffset" : 132,
      "endOffset" : 190
    }, {
      "referenceID" : 31,
      "context" : "On one hand, some works utilized the knowledge contained in pre-trained models by finetuning on supervised data of downstream tasks (Qin et al., 2019; Liu and Lapata, 2019; Qin et al., 2020).",
      "startOffset" : 132,
      "endOffset" : 190
    }, {
      "referenceID" : 14,
      "context" : "On the other hand, some works examined the knowledge in an unsupervised manner (Jiang et al., 2020; Xiao et al., 2020; Lin et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 136
    }, {
      "referenceID" : 40,
      "context" : "On the other hand, some works examined the knowledge in an unsupervised manner (Jiang et al., 2020; Xiao et al., 2020; Lin et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 136
    }, {
      "referenceID" : 19,
      "context" : "On the other hand, some works examined the knowledge in an unsupervised manner (Jiang et al., 2020; Xiao et al., 2020; Lin et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 136
    } ],
    "year" : 2021,
    "abstractText" : "Current dialogue summarization systems usually encode the text with a number of general semantic features (e.g., keywords and topics) to gain more powerful dialogue modeling capabilities. However, these features are obtained via open-domain toolkits that are dialogagnostic or heavily relied on human annotations. In this paper, we show how DialoGPT (Zhang et al., 2020b), a pre-trained model for conversational response generation, can be developed as an unsupervised dialogue annotator, which takes advantage of dialogue background knowledge encoded in DialoGPT. We apply DialoGPT to label three types of features on two dialogue summarization datasets, SAMSum and AMI, and employ pre-trained and non pre-trained models as our summarizers. Experimental results show that our proposed method can obtain remarkable improvements on both datasets and achieves new state-of-theart performance on the SAMSum dataset1.",
    "creator" : "LaTeX with hyperref"
  }
}