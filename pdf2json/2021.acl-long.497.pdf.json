{
  "name" : "2021.acl-long.497.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Neural Transition-based Model for Argumentation Mining",
    "authors" : [ "Jianzhu Bao", "Chuang Fan", "Jipeng Wu", "Yixue Dang", "Jiachen Du", "Ruifeng Xu" ],
    "emails" : [ "fanchuanghit}@gmail.com", "wujipeng@stu.hit.edu.cn,", "dangyixue@cmschina.com.cn", "jacobvan199165@gmail.com,", "xuruifeng@hit.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6354–6364\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6354"
    }, {
      "heading" : "1 Introduction",
      "text" : "Argumentation mining (AM) aims to identify the argumentation structures in text, which has received widespread attention in recent years (Lawrence and Reed, 2019). It has been shown beneficial in a broad range of fields, such as information retrieval (Carstens and Toni, 2015; Stab et al., 2018), automated essay scoring (Wachsmuth et al., 2016; Ke et al., 2018), and legal decision support (Palau and Moens, 2009; Walker et al., 2018). Given a piece of paragraph-level argumentative text, an AM system first detects argument components (ACs), which are segments of text with argumentative meaning, and then extracts the argumentative relations (ARs) between ACs to obtain an argumentation graph, where the nodes and edges represent ACs and ARs,\n∗Equal Contribution †Corresponding Author\nrespectively. An example of AM is shown in Figure 1, where the text is segmented into five ACs, and there are four ARs. In this instance, the types of AC2 and AC3 are Fact (non-experiential objective proposition) and Value (proposition containing value judgments), respectively. In addition, there is an AR from AC2 to AC3, i.e., “The check either bounced or it did not.” is the reason of “There’s not a lot of grey area here.”, for the latter is a value judgment based on the fact of the former.\nGenerally, AM involves several subtasks, including 1) Argument component segmentation (ACS), which separates argumentative text from non-argumentative text; 2) Argument component type classification (ACTC), which determines the types of ACs (e.g., Policy, Fact, Value, etc.); 3) Argumentative relation identification (ARI), which identifies ARs between ACs; 4) Argumentative relation type classification (ARTC), which determines\nthe types of ARs (e.g., Reason and Evidence). Most previous works assume that subtask 1) ACS has been completed, that is, ACs have been segmented, and focus on other subtasks (Potash et al., 2017; Kuribayashi et al., 2019; Chakrabarty et al., 2019). In this paper, we also make such an assumption, and perform ACTC and ARI on this basis.\nAmong all the subtasks of AM, ARI is the most challenging because it requires understanding complex semantic interactions between ACs. Most previous works exhaustively enumerate all possible pairs of ACs (i.e., all ACs are matched to each other by Cartesian products) to determine the ARs between them (Kuribayashi et al., 2019; Morio et al., 2020). However, these approaches are of low efficiency and can cause class imbalance, since the majority of AC pairs have no relation. Besides, due to different annotation schemes, there are mainly two kinds of structures of argumentation graphs, tree (Stab and Gurevych, 2014; Peldszus, 2014) and non-tree (Park and Cardie, 2018). Briefly, in tree structures, each AC has at most one outgoing AR, but there is no such restriction in non-tree structures (Figure 1). However, studies on these two kinds of structures are usually conducted separately. To date, there is no universal method that can address both tree and non-tree structured argumentation without any corpus-specific constraints.\nTowards these issues, we present a neural transition-based model for AM, which can classify the types of ACs and identify ARs simultaneously. Our model predicts a sequence of actions to incrementally construct a directed argumentation graph, often with O(n) parsing complexity. This allows our model to avoid inefficient enumeration operations and reduce the number of potential AC pairs that need evaluating, thus alleviating the class imbalance problem and achieving speedup. Also, our transition-based model does not introduce any corpus-specific structural constraints, and thus can handle both tree and non-tree structured argumentation, yielding promising generalization ability. Furthermore, we enhance our transition-based model with pre-trained BERT (Devlin et al., 2019), and use LSTM (Hochreiter and Schmidhuber, 1997) to represent the parser state of our model.\nExtensive experiments on two public datasets with different structures show that our transitionbased model outperforms previous methods, and achieves state-of-the-art results. Further analysis reveals that our model is of low parsing complexity\nand has a strong structure adaptive ability. To the best of our knowledge, we are the first to investigate transition-based methods for AM."
    }, {
      "heading" : "2 Related Work",
      "text" : "In computational AM, there are mainly two types of approaches to model argumentation structures, that is, tree and non-tree."
    }, {
      "heading" : "2.1 Tree Structured AM",
      "text" : "Most previous works assume that the argumentation graphs can be viewed as tree or forest structures, which makes the problem computationally easier because many tree-based structural constraints can be applied.\nUnder the theory of Van Eemeren et al. (2004), Palau and Moens (2009) modeled argumentation in the legal text as tree structures and used handcrafted context-free grammar to identify these structures. Presented by Stab and Gurevych (2014, 2017), the tree structured Persuasive Essay (PE) dataset has been utilized in a number of studies in AM. Following this dataset, Persing and Ng (2016) and Stab and Gurevych (2017) leveraged the Integer Linear Programming (ILP) framework to jointly predict ARs and AC types, in which several structural constraints are defined to ensure the tree structures. The arg-microtext (MT) dataset, created by Peldszus (2014), is another tree structured dataset. Studies on this dataset usually apply decoding mechanisms based on tree structures, such as Minimum Spanning Trees (MST) (Peldszus and Stede, 2015) and ILP (Afantenos et al., 2018).\nRegarding neural network-based methods, Eger et al. (2017) studied AM as a dependency parsing and a sequence labeling problem with multiple neural networks. Potash et al. (2017) introduced the sequence-to-sequence based Pointer Networks (Vinyals et al., 2015) to AM, and used the output of encoder and decoder to identify AC types and the presence of ARs, respectively. Kuribayashi et al. (2019) proposed an argumentation structure parsing model based on span representation, which used ELMo (Peters et al., 2018) to obtain representations for ACs."
    }, {
      "heading" : "2.2 Non-tree Structured AM",
      "text" : "Those studies described in Section 2.1 are all based upon the assumption that the argumentation forms tree structures. However, this assumption is somewhat idealistic since argumentation structures in\nreal-life scenarios may not be such well-formed. Hence, some studies have focused on non-tree structured AM, and these studies typically use the Consumer Debt Collection Practices (CDCP) (Park and Cardie, 2018) dataset. Regarding this dataset, Niculae et al. (2017) presented a structured learning approach based on factor graphs, which can also handle the tree structured PE dataset. However, the factor graph needs to be specifically designed according to the types of argumentation structures. Galassi et al. (2018) adopted residual networks for AM on the CDCP dataset. Recently, Morio et al. (2020) proposed a model devoted to non-tree structured AM, with a task-specific parameterization module to encode ACs and a biaffine attention module to capture ARs.\nTo the best of our knowledge, until now there is no universal method that can address both tree and non-tree structured argumentation without any corpus-specific design. Thus, in this work, we fill this gap by proposing a neural transition-based model that can identify both tree and non-tree argumentation structures without introducing any prior structural assumptions."
    }, {
      "heading" : "2.3 Transition-based Methods",
      "text" : "Transition-based methods are commonly used in dependency parsing (Chen and Manning, 2014; Gómez-Rodrı́guez et al., 2018), and has also been successfully applied to other NLP tasks with promising performance, such as discourse parsing (Yu et al., 2018), information extraction (Zhang et al., 2019), word segmentation (Zhang et al., 2016) and mention recognition (Wang et al., 2018)."
    }, {
      "heading" : "3 Task Definition",
      "text" : "Following previous works (Potash et al., 2017; Kuribayashi et al., 2019), we assume subtask 1) ACS has been completed, i.e., the spans of ACs are given. Then, we aim at jointly classifying AC types (ACTC) and determining the presence of ARs (ARI). The reason why we do not jointly conduct AR type classification (ARTC) is that performing ARTC along with ACTC and ARI jointly will hurt the overall performance. More details on this issue will be discussed in Section 6.4.\nFormally, we assume a piece of argumentation related paragraph P = (w1, w2, . . . , wm) consisting of m tokens and a set X = (x1, x2, . . . , xn) consisting of n AC spans are given. Each AC span xi is a tuple containing the beginning token index\nbi and the ending token index ei of this AC, i.e., xi = (bi, ei). The goal is to classify the types of ACs and identify the ARs, and finally obtain a directed argumentation graph with ACs and ARs representing nodes and edges, respectively."
    }, {
      "heading" : "4 Our Approach",
      "text" : "We present a neural transition-based model for AM, which can jointly learn ACTC and ARI. Our model generates a sequence of actions in terms of the parser state to incrementally build an argumentation graph. We utilize BERT and LSTM to represent our parser state, which contains a stack σ to store processed ACs, a buffer β to store unprocessed ACs, a delay set D to record ACs that need to be removed subsequently, and an action list α to record historical actions. Then, the learning problem is framed as: given the parser state of current step t: (σt,βt, Dt,αt), predict an action to determine the parser state of the next step, and simultaneously identify ARs according to the predicted action. Figure 2 shows the architecture of our model. In the following, we first introduce our transition system, then describe the parser state representation."
    }, {
      "heading" : "4.1 Transition System",
      "text" : "Our transition system contains six types of actions. Different actions will change the state in different ways, which are also summarized in Table 1: • SHIFT (SH): When βt is not empty and σ1 is\nnot in Dt, pop β0 from βt and move it to the top of σt. • DELETE-DELAY (DEd). When βt is not empty and σ1 is inDt, remove σ1 from σt andDt, and keep βt unchanged. • DELETE (DE). When βt is empty, remove σ1 from σt and keep βt and Dt unchanged. • RIGHT-ARC (RA). When βt is empty, remove σ0 from σt and assign an AR from σ0 to σ1. • RIGHT-ARC-DELAY (RAd). When βt is not empty, pop β0 from βt and move it to the top of σt. Then assign an AR from σ0 to σ1 and add σ0 into Dt for delayed deletion. This strategy can help extract more ARs related to σ0. • LEFT-ARC (LA). Remove σ1 from σt and assign an AR from σ1 to σ0. Table 2 illustrates the golden transition sequence\nof the text in Figure 1. This example text contains five ACs and four ARs. At the initial state, all ACs are in buffer. Then, a series of actions change the parser state according to Table 1, and extract ARs simultaneously. This procedure stops when meeting the terminal state, that is, buffer is empty and stack only contains one element."
    }, {
      "heading" : "4.2 State Representation",
      "text" : "We employ BERT to obtain the representation of each AC and use LSTM to encode the long-term dependencies of stack, buffer and action list.\nRepresentation of ACs. We feed the input paragraph P = (w1, w2, . . . , wm) into BERT to get the contextual representation matrix H ∈ Rm×db , where db is the vector dimension of the last layer of BERT. In this way, paragraph P can be represented as H = (h1,h2, . . . ,hm), where hi is the contextual representation of the i-th token of P .\nThen, we use the AC spans set X = (x1, x2, . . . , xn) to produce a contextual representation of each AC from H by mean pooling over the representations of words in each AC span. Specifically, for the i-th AC with span xi = (bi, ei), the contextual representation of this AC could be obtained by:\nui = 1\nei − bi + 1 ei∑ j=bi hj (1)\nwhere ui ∈ Rdb . In addition, following previous works (Potash et al., 2017; Kuribayashi et al., 2019), we also combine some extra features with ui to represent ACs, including the bag-of-words (BoW) vector, position and paragraph type embedding of each AC1. We denote these features of the i-th AC as φi. Then, the i-th AC is represented by\n1Details of these features are described in Appendix A.\nthe concatenation of ui and φi:\nci = [ui;φi] (2)\nHence, the ACs in paragraph P can be represented as C = (c1, c2, . . . , cn).\nRepresentation of Parser State. Our transitionbased model utilizes the parser state to predict a sequence of actions. At each step t, we denote our parser state as (σt,βt, Dt,αt). σt and βt are stack and buffer, which store the representations of processed and unprocessed ACs, respectively. Dt is the delay set that records ACs that need to be removed from stack subsequently. αt is the action list that stores the actions generated so far. At the beginning, all ACs are in the buffer, i.e., the initial parser state is ([ ], [c1, c2, . . . , cn],∅, [ ]). Then, a series of predicted actions will iteratively change the parser state.\nSpecifically, at step t, we have σt = (σ0,σ1, . . .), βt = (β0,β1, . . .), where σi and βi indicate the representations of ACs in the stack and the buffer at the current state. In addition, we also have αt = (. . . ,αt−2,αt−1) where αi denotes the distributed representation of the i-th action obtained by a looking-up table Ea. In order to capture the context information in the stack σt, we feed it into a bidirectional LSTM:\nSt = [s0, s1, . . .]\n= BiLSTMs([σ0,σ1, . . .]) (3)\nwhere St ∈ R|σt|×2dl is the output of LSTM from both directions, |σt| is the size of stack, and dl is the hidden size of LSTM. Similarly, we can obtain the contextual representation of βt by:\nBt = [b0,b1, . . .]\n= BiLSTMb([β0,β1, . . .]) (4)\nwhere Bt ∈ R|βt|×2dl , |βt| is the size of buffer. Besides, in order to incorporate the historical action information into our model, we apply a unidirectional LSTM to process the action list:\nAt = [. . . ,at−2,at−1]\n= LSTMa([. . . ,αt−2,αt−1]) (5)\nwhere At ∈ R|αt|×dl , |αt| is the size of action list. Furthermore, since the relative distance between the pair (σ0,σ1) is a strong feature for determining their relations, we represent it as an embedding ed\nthrough another looking-up table Ed. Thus, the parser state representation rt can be obtained by:\nrt = [s0; s1;b0;at−1; ed] (6)\nwhere s0 and s1 denote the first and second elements of St, b0 is the first element of the Bt, and at−1 indicates the last action representation of At."
    }, {
      "heading" : "4.3 Action Prediction",
      "text" : "To predict the current action at step t, we first apply a multi-layer perceptron (MLP) with ReLU activation to squeeze the state representation rt to a lower-dimensional vector zt, and then compute the action probability by a softmax output layer:\nzt = MLPa(r t) (7)\np(αt|zt) = exp(W>α z t + bα)∑ α′∈A(S) exp(W > α′z t + bα′) (8)\nwhere Wα denotes a learnable parameter matrix, bα is the bias term, αt is the predicted action for step t. A(S) represents the set of valid candidate actions that may be taken according to the preconditions. For efficient decoding, we greedily take the candidate action with the highest probability. With the predicted action sequence, we could identify ARs according to Table 1. Note that, the univocal supervision over actions for one input paragraph is built based on the gold labels of ARs."
    }, {
      "heading" : "4.4 Training",
      "text" : "We jointly train an AC type classifier over the AC representations: p(yi|C) = softmax(MLPc(ci)), where yi is the predicted type for the i-th AC. Finally, combining this task with action prediction, the training objective of our model can be obtained:\nJ (θ) = ∑ t logp(αt|zt) + ∑ i logp(yi|C)\n+ λ\n2 ||θ||2\n(9)\nwhere λ is the coefficient of L2-norm regularization, and θ denotes all the parameters in this model."
    }, {
      "heading" : "5 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "5.1 Dataset",
      "text" : "We conduct experiments on two datasets: Persuasive Essays (PE) (Stab and Gurevych, 2017) and Consumer Debt Collection Practices (CDCP) (Niculae et al., 2017; Park and Cardie, 2018).\nThe PE dataset contains 402 essays (1,833 paragraphs), in which 80 essays (369 paragraphs) are held out for testing. There are three types of ACs in this dataset: Major-Claim, Claim, and Premise. Also, each AC in PE dataset has at most one outgoing AR. That is, the argumentation graph of one paragraph can be either directed trees or forests. We extend each AC by including its argumentative marker in the same manner as Kuribayashi et al. (2019).\nThe CDCP dataset consists of 731 paragraphs, and 150 of them are reserved for testing. It provides five types of ACs (propositions): Reference, Fact, Testimony, Value, and Policy. Unlike PE dataset, each AC in CDCP dataset can have two or more outgoing ARs, thus forming non-tree structures."
    }, {
      "heading" : "5.2 Implementation Details",
      "text" : "For PE dataset, we randomly choose 10% of the training set as the validation set, which is consistent with the work of Kuribayashi et al. (2019). For CDCP dataset, we randomly choose 15% of the training set for validation. Following Potash et al. (2017), for ACTC, we employ F1 score for each AC type and their macro averaged score to measure the performance. Similarly, for ARI, we present F1 scores for the presence/absence of links between ACs and their macro averaged score. All experiments are performed 5 times with different random seeds, and the scores are averaged.\nWe finetune uncased BERTBase 2 in our model. AdamW optimizer (Loshchilov and Hutter, 2019) is adopted for parameter optimization, and the initial learning rates for the BERT layer and other layers are set to 1e-5 and 1e-3, respectively. All LSTMs are 1 layer with the hidden size of 256, and the hidden size of MLP is 512. Besides, the dropout rate (Srivastava et al., 2014) is set to 0.5, and the batch size is set to 32. All parameters of our model are unfixed and can be learned during training. We train the model 50 epochs with early stopping strategy, and choose model parameters with the best performance (average of macro F1 scores of ACTC and ARI) on the validation set. Our model is implemented in PyTorch (Paszke et al., 2019) on a NVIDIA Tesla V100 GPU."
    }, {
      "heading" : "5.3 Baselines",
      "text" : "In order to evaluate our proposed BERT-Trans model, we compare it with several baselines.\n2https://github.com/huggingface/ transformers\nFor PE dataset, the following baselines are compared: Joint-ILP (Stab and Gurevych, 2017) jointly optimizes AC types and ARs by Integer Linear Programming (ILP). St-SVM-full is structured SVM with full factor graph, which performs best on PE dataset in the work of Niculae et al. (2017). Joint-PN (Potash et al., 2017) applies Pointer Network with attention mechanism to AM, which can jointly address both ACTC and ARI. Span-LSTM (Kuribayashi et al., 2019) employs LSTM-minus-based span representation with pretrained ELMo embedding for AM, which is the current state-of-the-art method on PE dataset.\nFor CDCP dataset, we compare our model with the following baselines: Deep-Res-LG (Galassi et al., 2018) applies residual network model with link-guided training procedure, to perform ACTC and ARI. St-SVM-strict is structured SVM with strict factor graph, which performs best on CDCP dataset in the work of (Niculae et al., 2017). TSP-PLBA (Morio et al., 2020) uses task-specific parameterization to encode ACs and biaffine attention to capture ARs with ELMo based features, which is the current state-of-the-art method on CDCP dataset.\nFurthermore, in order to show the effectiveness of our proposed transition system, we implemented two additional baselines: Span-LSTM-Trans incorporates the span representation method used in Span-LSTM and our transition system on PE dataset. For a fair comparison, features and ELMo used to represent ACs are consistent with that of Span-LSTM. ELMo-Trans replaces BERT in our proposed model with ELMo on CDCP dataset for a fair comparison with TSP-PLBA."
    }, {
      "heading" : "6 Results and Analysis",
      "text" : ""
    }, {
      "heading" : "6.1 Main Results",
      "text" : "The overall performance of our proposed model and the baselines are shown in Table 3 and Table 4. Our model achieves the best performance on both datasets. On PE dataset, our model outperforms the current sota model Span-LSTM by at least 1.1% and 1.4% in macro F1 score over ACTC and ARI, respectively. On CDCP dataset, compared with TSP-PLBA, our model obtains at least 3.6% higher\nmacro F1 score over ACTC, and achieves about 3.3% higher relation F1 over ARI.\nWe also show the results where our BERT-based AC representation is replaced by the ELMo-based method, that is, Span-LSTM-Trans on PE dataset and ELMo-Trans on CDCP dataset. We found that, without employing pre-trained BERT, SpanLSTM-Trans and ELMo-Trans still outperform Span-LSTM and TSP-PLBA over ARI, respectively, which demonstrates the effectiveness of our proposed transition system. It can also be observed that our BERT-based AC representation method can further improve the model performance.\nSome of the baselines improve overall performance by imposing structural constraints when predicting or decoding. For example, Joint-PN only predicts one outgoing AR for each AC to partially enforce the predicted argumentation graphs as tree structures. Similarly, to ensure tree structures, Span-LSTM applies MST algorithm based on the probabilities calculated by the model. However, these two methods can only deal with tree structured argumentation. The method proposed by Nic-\nulae et al. (2017), which is based on factor graph, can handle both tree and no-tree structured argumentative text (St-SVM-full and St-SVM-strict), but the factor graph need to be specifically designed for datasets of different structures. Differently, our proposed model can handle datasets of both tree and non-tree structures without introducing any corpus-specific structural constraints and also outperforms all the structured baselines."
    }, {
      "heading" : "6.2 Ablation Study",
      "text" : "We conduct ablation experiments on the PE dataset to further investigate the impacts of each component in BERT-Trans. The results are shown in Table 5. It can be observed that applying LSTM to encode buffer, stack, and action list contributes about 2.0% macro F1 score of ARI, showing the necessity of capturing non-local dependencies in parser state. Also, incorporating buffer into parser state can improve the macro F1 score of ARI by about 1.8%, for buffer can provide crucial information about subsequent ACs to be processed. Besides, the macro F1 score of ARI drops heavily without action list (-1.6%), indicating that the historical action information has a significant impact on predicting the next action. Without the distance information between the top two ACs of the stack, the macro F1 score of ARI decreases by 0.7%. The model components described above mainly affect ARI by modifying the parsing procedure, but have little impact on ACTC. However, BoW feature has a significant influence on both two tasks, and removing it causes 2.5% and 1.9% decreases in macro F1 score of ACTC and ARI, respectively."
    }, {
      "heading" : "6.3 Parsing Complexity",
      "text" : "Most previous models parse argumentation graphs by exhaustively enumerating all possible pairs of ACs, that is, all ACs are connected by Cartesian products, which lead to O(n2) parsing complexity. Differently, our transition-based model can incrementally parse an argumentation graph by predicting a sequence of actions, often with linear parsing complexity. Concretely, given a paragraph with n ACs, our system can parse it with O(n) actions.\nParsing complexity of our transition system can be determined by the number of actions performed with respect to the number of ACs in a paragraph. Specifically, we measure the length of the action sequence predicted by our model for every paragraph from the test sets of PE dataset and CDCP dataset and depict the relation between them and the number of ACs. As shown in Figure 3, the number of predicted actions is linearly related to the number of ACs in both two datasets, proving that our system can construct an argumentation graph with O(n) complexity. In addition, we also compared our model with the current state-of-the-art model on PE dataset, i.e., Span-LSTM, in terms of training time, and our model is around two times faster."
    }, {
      "heading" : "6.4 Joint Learning Analysis",
      "text" : "Following Kuribayashi et al. (2019), we also try to add the task of AR type classification (ARTC) to our model for joint learning on PE dataset. However, as shown in Table 6, jointly learning ARTC together with ACTC and ARI degrades the overall performance, while learning ARTC separately actually yields better performance. Such an observation is consistent with the joint learning results\nof Span-LSTM in Kuribayashi et al. (2019). The reason may be that the class labels are usually very unbalanced for ARTC (around 1:10 in PE dataset and 1:25 in CDCP dataset), such that the high uncertainty can seriously affect the overall learning. Thus, we mainly focus on joint learning of ACTC and ARI. We also argue that learning ARTC individually is better than jointly learning it with other subtasks. Besides, our model outperforms Span-LSTM over ACTC and ARI even when joint learning all three subtasks."
    }, {
      "heading" : "6.5 Structure Adaptive",
      "text" : "To validate the structure adaptive ability of our model on both tree and non-tree structures, we analyze the structure type of the predicted argumentation graphs on the test set of both PE and CDCP datasets in Figure 4. It can be seen that for non-tree structured CDCP dataset, even though there are few non-tree structured paragraphs in the test set of CDCP (only 16%), our model is still able to identify 29.2% of them. This is an acceptable performance considering the poor results of ARI on the CDCP dataset due to the complex non-tree structures. For tree structured PE dataset, our model predicts all the paragraphs as tree structures, showing a strong structure adaptive ability. In contrast, most previous models like Joint-PN and Span-LSTM can only predict tree structures."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we propose a neural transition-based model for argumentation mining, which can incrementally construct an argumentation graph by predicting a sequence of actions. Our proposed model can handle both tree and non-tree structures, and often with linear parsing complexity. The experimental results on two public datasets demonstrate the effectiveness of our model. One potential drawback of our model is the greedy decoding for action prediction. For future work, we plan to optimize the decoding process by using methods like beam search to further boost the performance."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was partially supported by National Natural Science Foundation of China (61632011, 61876053, 62006062), Guangdong Province Covid-19 Pandemic Control Research Funding (2020KZDZX1224), Shenzhen Foundational Research Funding (JCYJ20180507183527919, JCYJ20180507183608379), and the Joint Lab of China Merchants Securities and HITSZ."
    }, {
      "heading" : "A Extra Features",
      "text" : "Following the work of Potash et al. (2017) and Kuribayashi et al. (2019), we further incorporate some extra features to represent ACs, including:\n• the bag-of-words (BoW) vector: one-hot vector, which is later fed into a MLP layer.\n• position embedding of each AC: The position of an AC in the paragraph, which is represented as an embedding vector through a looking-up table Ep.\n• paragraph type embedding of each AC: The type (intro, body, conclusion) of the paragraph in which the AC is present, which is also represented as an embedding vector through another looking-up table Et."
    } ],
    "references" : [ {
      "title" : "Comparing decoding mechanisms for parsing argumentative structures",
      "author" : [ "Stergos D. Afantenos", "Andreas Peldszus", "Manfred Stede." ],
      "venue" : "Argument Comput., 9(3):177–192.",
      "citeRegEx" : "Afantenos et al\\.,? 2018",
      "shortCiteRegEx" : "Afantenos et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards relation based argumentation mining",
      "author" : [ "Lucas Carstens", "Francesca Toni." ],
      "venue" : "Proceedings of the 2nd Workshop on Argumentation Mining, ArgMining@HLT-NAACL 2015, June 4, 2015, Denver, Colorado, USA, pages 29–34. The Association",
      "citeRegEx" : "Carstens and Toni.,? 2015",
      "shortCiteRegEx" : "Carstens and Toni.",
      "year" : 2015
    }, {
      "title" : "AMPERSAND: argument mining for persuasive online discussions",
      "author" : [ "Tuhin Chakrabarty", "Christopher Hidey", "Smaranda Muresan", "Kathy McKeown", "Alyssa Hwang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Chakrabarty et al\\.,? 2019",
      "shortCiteRegEx" : "Chakrabarty et al\\.",
      "year" : 2019
    }, {
      "title" : "A fast and accurate dependency parser using neural networks",
      "author" : [ "Danqi Chen", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A",
      "citeRegEx" : "Chen and Manning.,? 2014",
      "shortCiteRegEx" : "Chen and Manning.",
      "year" : 2014
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural end-to-end learning for computational argumentation mining",
      "author" : [ "Steffen Eger", "Johannes Daxenberger", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancou-",
      "citeRegEx" : "Eger et al\\.,? 2017",
      "shortCiteRegEx" : "Eger et al\\.",
      "year" : 2017
    }, {
      "title" : "Argumentative link prediction using residual networks and multi-objective learning",
      "author" : [ "Andrea Galassi", "Marco Lippi", "Paolo Torroni." ],
      "venue" : "Proceedings of the 5th Workshop on Argument Mining, ArgMining@EMNLP 2018, Brussels, Belgium, November",
      "citeRegEx" : "Galassi et al\\.,? 2018",
      "shortCiteRegEx" : "Galassi et al\\.",
      "year" : 2018
    }, {
      "title" : "Global transition-based non-projective dependency parsing",
      "author" : [ "Carlos Gómez-Rodrı́guez", "Tianze Shi", "Lillian Lee" ],
      "venue" : "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Gómez.Rodrı́guez et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Gómez.Rodrı́guez et al\\.",
      "year" : 2018
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Comput., 9(8):1735– 1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Learning to give feedback: Modeling attributes affecting argument persuasiveness in student essays",
      "author" : [ "Zixuan Ke", "Winston Carlile", "Nishant Gurrapadi", "Vincent Ng." ],
      "venue" : "Proceedings of the TwentySeventh International Joint Conference on Artificial",
      "citeRegEx" : "Ke et al\\.,? 2018",
      "shortCiteRegEx" : "Ke et al\\.",
      "year" : 2018
    }, {
      "title" : "An empirical study of span representations in argumentation structure parsing",
      "author" : [ "Tatsuki Kuribayashi", "Hiroki Ouchi", "Naoya Inoue", "Paul Reisert", "Toshinori Miyoshi", "Jun Suzuki", "Kentaro Inui." ],
      "venue" : "Proceedings of the 57th Conference of the Association",
      "citeRegEx" : "Kuribayashi et al\\.,? 2019",
      "shortCiteRegEx" : "Kuribayashi et al\\.",
      "year" : 2019
    }, {
      "title" : "Argument mining: A survey",
      "author" : [ "John Lawrence", "Chris Reed." ],
      "venue" : "Comput. Linguistics, 45(4):765–818.",
      "citeRegEx" : "Lawrence and Reed.,? 2019",
      "shortCiteRegEx" : "Lawrence and Reed.",
      "year" : 2019
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Towards better non-tree argument mining: Proposition-level biaffine parsing with task-specific parameterization",
      "author" : [ "Gaku Morio", "Hiroaki Ozaki", "Terufumi Morishita", "Yuta Koreeda", "Kohsuke Yanai." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the As-",
      "citeRegEx" : "Morio et al\\.,? 2020",
      "shortCiteRegEx" : "Morio et al\\.",
      "year" : 2020
    }, {
      "title" : "Argument mining with structured svms and rnns",
      "author" : [ "Vlad Niculae", "Joonsuk Park", "Claire Cardie." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1:",
      "citeRegEx" : "Niculae et al\\.,? 2017",
      "shortCiteRegEx" : "Niculae et al\\.",
      "year" : 2017
    }, {
      "title" : "Argumentation mining: the detection, classification and structure of arguments in text",
      "author" : [ "Raquel Mochales Palau", "Marie-Francine Moens." ],
      "venue" : "The 12th International Conference on Artificial Intelligence and Law, Proceedings of the Conference, June",
      "citeRegEx" : "Palau and Moens.,? 2009",
      "shortCiteRegEx" : "Palau and Moens.",
      "year" : 2009
    }, {
      "title" : "A corpus of erulemaking user comments for measuring evaluability of arguments",
      "author" : [ "Joonsuk Park", "Claire Cardie." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018, Miyazaki, Japan, May",
      "citeRegEx" : "Park and Cardie.,? 2018",
      "shortCiteRegEx" : "Park and Cardie.",
      "year" : 2018
    }, {
      "title" : "Pytorch: An imperative style, high-performance deep learning library",
      "author" : [ "jani", "Sasank Chilamkurthy", "Benoit Steiner", "Lu Fang", "Junjie Bai", "Soumith Chintala" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "jani et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "jani et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards segment-based recognition of argumentation structure in short texts",
      "author" : [ "Andreas Peldszus." ],
      "venue" : "Proceedings of the First Workshop on Argument Mining, hosted by the 52nd Annual Meeting of the Association for Computational Linguistics, ArgMin-",
      "citeRegEx" : "Peldszus.,? 2014",
      "shortCiteRegEx" : "Peldszus.",
      "year" : 2014
    }, {
      "title" : "Joint prediction in mst-style discourse parsing for argumentation mining",
      "author" : [ "Andreas Peldszus", "Manfred Stede." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-",
      "citeRegEx" : "Peldszus and Stede.,? 2015",
      "shortCiteRegEx" : "Peldszus and Stede.",
      "year" : 2015
    }, {
      "title" : "End-to-end argumentation mining in student essays",
      "author" : [ "Isaac Persing", "Vincent Ng." ],
      "venue" : "NAACL HLT",
      "citeRegEx" : "Persing and Ng.,? 2016",
      "shortCiteRegEx" : "Persing and Ng.",
      "year" : 2016
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Here’s my point: Joint pointer architecture for argument mining",
      "author" : [ "Peter Potash", "Alexey Romanov", "Anna Rumshisky." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark,",
      "citeRegEx" : "Potash et al\\.,? 2017",
      "shortCiteRegEx" : "Potash et al\\.",
      "year" : 2017
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "J. Mach. Learn. Res., 15(1):1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Annotating argument components and relations in persuasive essays",
      "author" : [ "Christian Stab", "Iryna Gurevych." ],
      "venue" : "COLING 2014, 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, August 23-29,",
      "citeRegEx" : "Stab and Gurevych.,? 2014",
      "shortCiteRegEx" : "Stab and Gurevych.",
      "year" : 2014
    }, {
      "title" : "Parsing argumentation structures in persuasive essays",
      "author" : [ "Christian Stab", "Iryna Gurevych." ],
      "venue" : "Comput. Linguistics, 43(3):619–659.",
      "citeRegEx" : "Stab and Gurevych.,? 2017",
      "shortCiteRegEx" : "Stab and Gurevych.",
      "year" : 2017
    }, {
      "title" : "Cross-topic argument mining from heterogeneous sources",
      "author" : [ "Christian Stab", "Tristan Miller", "Benjamin Schiller", "Pranav Rai", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels,",
      "citeRegEx" : "Stab et al\\.,? 2018",
      "shortCiteRegEx" : "Stab et al\\.",
      "year" : 2018
    }, {
      "title" : "A systematic theory of argumentation: The pragma-dialectical approach",
      "author" : [ "Frans Van Eemeren", "Rob Grootendorst", "Frans H van Eemeren." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Eemeren et al\\.,? 2004",
      "shortCiteRegEx" : "Eemeren et al\\.",
      "year" : 2004
    }, {
      "title" : "Pointer networks",
      "author" : [ "Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly." ],
      "venue" : "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec,",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Using argument mining to assess the argumentation quality of essays",
      "author" : [ "Henning Wachsmuth", "Khalid Al Khatib", "Benno Stein." ],
      "venue" : "COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Techni-",
      "citeRegEx" : "Wachsmuth et al\\.,? 2016",
      "shortCiteRegEx" : "Wachsmuth et al\\.",
      "year" : 2016
    }, {
      "title" : "Evidence types, credibility factors, and patterns or soft rules for weighing conflicting evidence: Argument mining in the context of legal rules governing evidence assessment",
      "author" : [ "Vern R. Walker", "Dina Foerster", "Julia Monica Ponce", "Matthew Rosen" ],
      "venue" : null,
      "citeRegEx" : "Walker et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Walker et al\\.",
      "year" : 2018
    }, {
      "title" : "A neural transition-based model for nested mention recognition",
      "author" : [ "Bailin Wang", "Wei Lu", "Yu Wang", "Hongxia Jin." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4,",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Transition-based neural RST parsing with implicit syntax features",
      "author" : [ "Nan Yu", "Meishan Zhang", "Guohong Fu." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, COLING 2018, Santa Fe, New Mexico, USA, August",
      "citeRegEx" : "Yu et al\\.,? 2018",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2018
    }, {
      "title" : "Extracting entities and events as a single task using a transition-based neural model",
      "author" : [ "Junchi Zhang", "Yanxia Qin", "Yue Zhang", "Mengchi Liu", "Donghong Ji." ],
      "venue" : "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJ-",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Transition-based neural word segmentation",
      "author" : [ "Meishan Zhang", "Yue Zhang", "Guohong Fu." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long",
      "citeRegEx" : "Zhang et al\\.,? 2016",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Argumentation mining (AM) aims to identify the argumentation structures in text, which has received widespread attention in recent years (Lawrence and Reed, 2019).",
      "startOffset" : 137,
      "endOffset" : 162
    }, {
      "referenceID" : 1,
      "context" : "It has been shown beneficial in a broad range of fields, such as information retrieval (Carstens and Toni, 2015; Stab et al., 2018), automated essay scoring (Wachsmuth et al.",
      "startOffset" : 87,
      "endOffset" : 131
    }, {
      "referenceID" : 26,
      "context" : "It has been shown beneficial in a broad range of fields, such as information retrieval (Carstens and Toni, 2015; Stab et al., 2018), automated essay scoring (Wachsmuth et al.",
      "startOffset" : 87,
      "endOffset" : 131
    }, {
      "referenceID" : 29,
      "context" : ", 2018), automated essay scoring (Wachsmuth et al., 2016; Ke et al., 2018), and legal decision support (Palau and Moens, 2009; Walker et al.",
      "startOffset" : 33,
      "endOffset" : 74
    }, {
      "referenceID" : 9,
      "context" : ", 2018), automated essay scoring (Wachsmuth et al., 2016; Ke et al., 2018), and legal decision support (Palau and Moens, 2009; Walker et al.",
      "startOffset" : 33,
      "endOffset" : 74
    }, {
      "referenceID" : 15,
      "context" : ", 2018), and legal decision support (Palau and Moens, 2009; Walker et al., 2018).",
      "startOffset" : 36,
      "endOffset" : 80
    }, {
      "referenceID" : 30,
      "context" : ", 2018), and legal decision support (Palau and Moens, 2009; Walker et al., 2018).",
      "startOffset" : 36,
      "endOffset" : 80
    }, {
      "referenceID" : 16,
      "context" : "Figure 1: An example of argumentation mining from the CDCP dataset (Park and Cardie, 2018).",
      "startOffset" : 67,
      "endOffset" : 90
    }, {
      "referenceID" : 22,
      "context" : "Most previous works assume that subtask 1) ACS has been completed, that is, ACs have been segmented, and focus on other subtasks (Potash et al., 2017; Kuribayashi et al., 2019; Chakrabarty et al., 2019).",
      "startOffset" : 129,
      "endOffset" : 202
    }, {
      "referenceID" : 10,
      "context" : "Most previous works assume that subtask 1) ACS has been completed, that is, ACs have been segmented, and focus on other subtasks (Potash et al., 2017; Kuribayashi et al., 2019; Chakrabarty et al., 2019).",
      "startOffset" : 129,
      "endOffset" : 202
    }, {
      "referenceID" : 2,
      "context" : "Most previous works assume that subtask 1) ACS has been completed, that is, ACs have been segmented, and focus on other subtasks (Potash et al., 2017; Kuribayashi et al., 2019; Chakrabarty et al., 2019).",
      "startOffset" : 129,
      "endOffset" : 202
    }, {
      "referenceID" : 10,
      "context" : ", all ACs are matched to each other by Cartesian products) to determine the ARs between them (Kuribayashi et al., 2019; Morio et al., 2020).",
      "startOffset" : 93,
      "endOffset" : 139
    }, {
      "referenceID" : 13,
      "context" : ", all ACs are matched to each other by Cartesian products) to determine the ARs between them (Kuribayashi et al., 2019; Morio et al., 2020).",
      "startOffset" : 93,
      "endOffset" : 139
    }, {
      "referenceID" : 24,
      "context" : "Besides, due to different annotation schemes, there are mainly two kinds of structures of argumentation graphs, tree (Stab and Gurevych, 2014; Peldszus, 2014) and non-tree (Park and Cardie, 2018).",
      "startOffset" : 117,
      "endOffset" : 158
    }, {
      "referenceID" : 18,
      "context" : "Besides, due to different annotation schemes, there are mainly two kinds of structures of argumentation graphs, tree (Stab and Gurevych, 2014; Peldszus, 2014) and non-tree (Park and Cardie, 2018).",
      "startOffset" : 117,
      "endOffset" : 158
    }, {
      "referenceID" : 16,
      "context" : "Besides, due to different annotation schemes, there are mainly two kinds of structures of argumentation graphs, tree (Stab and Gurevych, 2014; Peldszus, 2014) and non-tree (Park and Cardie, 2018).",
      "startOffset" : 172,
      "endOffset" : 195
    }, {
      "referenceID" : 4,
      "context" : "Furthermore, we enhance our transition-based model with pre-trained BERT (Devlin et al., 2019), and use LSTM (Hochreiter and Schmidhuber, 1997) to represent the parser state of our model.",
      "startOffset" : 73,
      "endOffset" : 94
    }, {
      "referenceID" : 8,
      "context" : ", 2019), and use LSTM (Hochreiter and Schmidhuber, 1997) to represent the parser state of our model.",
      "startOffset" : 22,
      "endOffset" : 56
    }, {
      "referenceID" : 19,
      "context" : "Studies on this dataset usually apply decoding mechanisms based on tree structures, such as Minimum Spanning Trees (MST) (Peldszus and Stede, 2015) and ILP (Afantenos et al.",
      "startOffset" : 121,
      "endOffset" : 147
    }, {
      "referenceID" : 0,
      "context" : "Studies on this dataset usually apply decoding mechanisms based on tree structures, such as Minimum Spanning Trees (MST) (Peldszus and Stede, 2015) and ILP (Afantenos et al., 2018).",
      "startOffset" : 156,
      "endOffset" : 180
    }, {
      "referenceID" : 28,
      "context" : "(2017) introduced the sequence-to-sequence based Pointer Networks (Vinyals et al., 2015) to AM, and used the output of encoder and decoder to identify AC types and the presence of ARs, respectively.",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 21,
      "context" : "(2019) proposed an argumentation structure parsing model based on span representation, which used ELMo (Peters et al., 2018) to obtain representations for ACs.",
      "startOffset" : 103,
      "endOffset" : 124
    }, {
      "referenceID" : 16,
      "context" : "Hence, some studies have focused on non-tree structured AM, and these studies typically use the Consumer Debt Collection Practices (CDCP) (Park and Cardie, 2018) dataset.",
      "startOffset" : 138,
      "endOffset" : 161
    }, {
      "referenceID" : 3,
      "context" : "Transition-based methods are commonly used in dependency parsing (Chen and Manning, 2014; Gómez-Rodrı́guez et al., 2018), and has also been successfully applied to other NLP tasks with promising performance, such as discourse parsing (Yu et al.",
      "startOffset" : 65,
      "endOffset" : 120
    }, {
      "referenceID" : 7,
      "context" : "Transition-based methods are commonly used in dependency parsing (Chen and Manning, 2014; Gómez-Rodrı́guez et al., 2018), and has also been successfully applied to other NLP tasks with promising performance, such as discourse parsing (Yu et al.",
      "startOffset" : 65,
      "endOffset" : 120
    }, {
      "referenceID" : 32,
      "context" : ", 2018), and has also been successfully applied to other NLP tasks with promising performance, such as discourse parsing (Yu et al., 2018), information extraction (Zhang et al.",
      "startOffset" : 121,
      "endOffset" : 138
    }, {
      "referenceID" : 33,
      "context" : ", 2018), information extraction (Zhang et al., 2019), word segmentation (Zhang et al.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 34,
      "context" : ", 2019), word segmentation (Zhang et al., 2016) and mention recognition (Wang et al.",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 22,
      "context" : "Following previous works (Potash et al., 2017; Kuribayashi et al., 2019), we assume subtask 1) ACS has been completed, i.",
      "startOffset" : 25,
      "endOffset" : 72
    }, {
      "referenceID" : 10,
      "context" : "Following previous works (Potash et al., 2017; Kuribayashi et al., 2019), we assume subtask 1) ACS has been completed, i.",
      "startOffset" : 25,
      "endOffset" : 72
    }, {
      "referenceID" : 22,
      "context" : "In addition, following previous works (Potash et al., 2017; Kuribayashi et al., 2019), we also combine some extra features with ui to represent ACs, including the bag-of-words (BoW) vector, position and paragraph type embedding of each AC1.",
      "startOffset" : 38,
      "endOffset" : 85
    }, {
      "referenceID" : 10,
      "context" : "In addition, following previous works (Potash et al., 2017; Kuribayashi et al., 2019), we also combine some extra features with ui to represent ACs, including the bag-of-words (BoW) vector, position and paragraph type embedding of each AC1.",
      "startOffset" : 38,
      "endOffset" : 85
    }, {
      "referenceID" : 25,
      "context" : "We conduct experiments on two datasets: Persuasive Essays (PE) (Stab and Gurevych, 2017) and Consumer Debt Collection Practices (CDCP) (Niculae et al.",
      "startOffset" : 63,
      "endOffset" : 88
    }, {
      "referenceID" : 14,
      "context" : "We conduct experiments on two datasets: Persuasive Essays (PE) (Stab and Gurevych, 2017) and Consumer Debt Collection Practices (CDCP) (Niculae et al., 2017; Park and Cardie, 2018).",
      "startOffset" : 135,
      "endOffset" : 180
    }, {
      "referenceID" : 16,
      "context" : "We conduct experiments on two datasets: Persuasive Essays (PE) (Stab and Gurevych, 2017) and Consumer Debt Collection Practices (CDCP) (Niculae et al., 2017; Park and Cardie, 2018).",
      "startOffset" : 135,
      "endOffset" : 180
    }, {
      "referenceID" : 12,
      "context" : "AdamW optimizer (Loshchilov and Hutter, 2019) is adopted for parameter optimization, and the initial learning rates for the BERT layer and other layers are set to 1e-5 and 1e-3, respectively.",
      "startOffset" : 16,
      "endOffset" : 45
    }, {
      "referenceID" : 23,
      "context" : "Besides, the dropout rate (Srivastava et al., 2014) is set to 0.",
      "startOffset" : 26,
      "endOffset" : 51
    }, {
      "referenceID" : 25,
      "context" : "Joint-ILP (Stab and Gurevych, 2017) jointly optimizes AC types and ARs by Integer Linear Programming (ILP).",
      "startOffset" : 10,
      "endOffset" : 35
    }, {
      "referenceID" : 22,
      "context" : "Joint-PN (Potash et al., 2017) applies Pointer Network with attention mechanism to AM, which can jointly address both ACTC and ARI.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 10,
      "context" : "Span-LSTM (Kuribayashi et al., 2019) employs LSTM-minus-based span representation with pretrained ELMo embedding for AM, which is the current state-of-the-art method on PE dataset.",
      "startOffset" : 10,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : "Deep-Res-LG (Galassi et al., 2018) applies residual network model with link-guided training procedure, to perform ACTC and ARI.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 14,
      "context" : "St-SVM-strict is structured SVM with strict factor graph, which performs best on CDCP dataset in the work of (Niculae et al., 2017).",
      "startOffset" : 109,
      "endOffset" : 131
    }, {
      "referenceID" : 13,
      "context" : "TSP-PLBA (Morio et al., 2020) uses task-specific parameterization to encode ACs and biaffine attention to capture ARs with ELMo based features, which is the current state-of-the-art method on CDCP dataset.",
      "startOffset" : 9,
      "endOffset" : 29
    } ],
    "year" : 2021,
    "abstractText" : "The goal of argumentation mining is to automatically extract argumentation structures from argumentative texts. Most existing methods determine argumentative relations by exhaustively enumerating all possible pairs of argument components, which suffer from low efficiency and class imbalance. Moreover, due to the complex nature of argumentation, there is, so far, no universal method that can address both tree and non-tree structured argumentation. Towards these issues, we propose a neural transition-based model for argumentation mining, which incrementally builds an argumentation graph by generating a sequence of actions, avoiding inefficient enumeration operations. Furthermore, our model can handle both tree and non-tree structured argumentation without introducing any structural constraints. Experimental results show that our model achieves the best performance on two public datasets of different structures.",
    "creator" : "LaTeX with hyperref"
  }
}