{
  "name" : "2021.acl-long.388.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Concept-Based Label Embedding via Dynamic Routing for Hierarchical Text Classification",
    "authors" : [ "Xuepeng Wang", "Li Zhao", "Bing Liu", "Tao Chen", "Feng Zhang", "Di Wang" ],
    "emails" : [ "woodswang@tencent.com", "lilythzhao@tencent.com", "andybliu@tencent.com", "vitochen@tencent.com", "jayzhang@tencent.com", "diwang@tencent.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5010–5019\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5010"
    }, {
      "heading" : "1 Introduction",
      "text" : "Text classification is a classical Natural Language Processing (NLP) task. In the real world, the text classification is usually cast as a hierarchical text classification (HTC) problem, such as patent collection (Tikk et al., 2005), web content collection (Dumais and Chen, 2000) and medical record coding (Cao et al., 2020). In these scenarios, the HTC task aims to categorize a textual description within a set of labels that are organized in a structured class hierarchy (Silla and Freitas, 2011). Lots of researchers devote their effort to investigate this challenging problem. They have proposed various HTC solutions, which are usually categorized into flat (Aly et al., 2019), local (Xu and Geng, 2019), global (Qiu et al., 2011) and combined approaches (Wehrmann et al., 2018).\nIn most of the previous HTC work, researchers mainly focus on modeling the text, the labels are\nsimply represented as one-hot vectors (Zhu and Bain, 2017; Wehrmann et al., 2018). Actually, the one-hot vectors act as IDs without any semantic information. How to describe a class is also worthy of discussion. There is some work that embeds labels into a vector space which contains more semantic information. Compared with one-hot representations, label embeddings have advantages in capturing domain-specific information and importing external knowledge. In the field of text classification (includes the HTC task), researchers propose several forms of label embeddings to encode different kinds of information, such as 1) anchor points (Du et al., 2019), 2) compatibility between labels and words (Wang et al., 2018; Huang et al., 2019; Tang et al., 2015), 3) taxonomic hierarchy (Cao et al., 2020; Zhou et al., 2020) and 4) external knowledge (Rivas Rojas et al., 2020).\nAlthough the external knowledge has been proven effective for HTC, it comes from a dictionary or knowledge base that humans constructed for entity definition, and it doesn’t focus on the class explanations of a certain HTC task. In this sense, external knowledge is a type of domainindependent information. The taxonomic hierarchy encoding can capture the structural information of classes, which is a sort of domain-specific information for HTC. However, actually it only models the hypernym-hyponym relations in the class hierarchy. The process is implicit and difficult to\nbe interpreted. Besides the structural connections between classes, we find that the information of concept shared between adjacent levels of classes is ignored by previous work. For instance, there is a parent node named “Sports” in a concrete class hierarchy (Qiu et al., 2011). Its subclasses “Surfing” and “Swimming” are “water” related sports. The subclasses “Basketball” and “Football” are “ball” related sports. The “water” and “ball” are a type of abstract concept included in the parent class “Sports” and can be shared by the subclasses. As shown in Figure 1, we have a similar observation in WOS (Kowsari et al., 2017), which is a widely used public dataset (details in our experiments). The concept “design” of the parent class “Computer Science” is shared by the child classes “Soft engineering” and “Algorithm design”. The concept “distributed” is shared by “Network security” and “Distributed computing”. The concept information can help to group the classes and measure the correlation intensity between parent and child classes. Compared with the information of node connections in the class hierarchy, the concept is more semantic and fine-grained, but rarely investigated. Although Qiu et al. (2011) have noticed the concept in HTC, they define the concept in a latent way and the process of represent learning is also implicit. Additionally, few of previous work investigates how to extract the concepts or model the sharing interactions among class nodes.\nTo further exploit the information of concept for HTC, we propose a novel concept-based label embedding method which can explicitly represent the concepts and model the sharing mechanism among classes. More specifically, we first construct a hierarchical attention-based framework which is proved to be effective by Wehrmann et al. (2018) and Huang et al. (2019). There is one conceptbased classifier for each level. The prior level classification result (i.e. predicted soft label embedding) is fed into the next level. A label embedding attention mechanism is utilized to measure the compatibility between texts and classes. Then we design a concept sharing module in our model. It firstly extracts the concepts explicitly in the corpus and represents them in the form of embeddings. Inspired by the CapsNet (Sabour et al., 2017), we employ the dynamic routing mechanism. The iterative routing helps to share the information from the lower level to the higher level with the agreement in CapsNet. Taking into account the characters\nof HTC, we modify the dynamic routing mechanism for modeling the concepts sharing interactions among classes. In detail, we calculate the agreement between concepts and classes. An external knowledge source is taken as an initial reference of the child classes. Different from the full connections in CapsNet, we build routing only between the class and its own child classes to utilize the structured class hierarchy of HTC. Then the routing coefficients are iteratively refined by measuring the agreement between the parent class concepts embeddings and the child class embeddings. In this way, the module models the concept sharing process and outputs a novel label representation which is constructed by the concepts of parent classes. Finally, our hierarchical network adopts such label embeddings to represent the input document with an attention mechanism and makes a classification.\nIn summary, our major contributions include:\n• This paper investigates the concept in HTC problem, which is a type of domain-specific information ignored by previous work. We summarize several kinds of existing label embeddings and propose a novel label representation: concept-based label embedding.\n• We propose a hierarchical network to extract the concepts and model the sharing process via a modified dynamic routing algorithm. To our best knowledge, this is the first work that explores the concepts of the HTC problem in an explicit and interpretable way.\n• The experimental results on two widely used datasets empirically demonstrate the effective performance of the proposed model.\n• We complement the public datasets WOS (Kowsari et al., 2017) and DBpedia (Sinha et al., 2018) by exacting the hierarchy concept and annotating the classes with the definitions from Wikipedia. We release these complementary resources and the code of the proposed model for further use by the community1."
    }, {
      "heading" : "2 Model",
      "text" : "In this section, we detailedly introduce our model CLED (Figure 2). It is designed for hierarchical text classification with Concept-based Label\n1https://github.com/wxpkanon/ CLEDforHTC.git\nEmbeddings via a modified Dynamic routing mechanism. Firstly, we construct a hierarchical attentionbased framework. Then a concept sharing module is designed for extracting concepts and modeling the sharing mechanism among classes. The module learns a novel label representation with concepts. Finally, the model takes the concept-based label embeddings to categorize a textual description."
    }, {
      "heading" : "2.1 Hierarchical Attention-based Framework",
      "text" : "In recent years, the hierarchical neural network has been proven effective for the HTC task by much work (Sinha et al., 2018; Wehrmann et al., 2018; Huang et al., 2019). We adopt it as the framework of our model.\nText Encoder We first map each document d = (w1, w2, ..., w|d|) into a low dimensional word embedding space and denote it as X = (x1,x2, ...,x|d|). A CNN layer is used for extracting n-gram features. Then a bidirectional GRU layer extracts contextual features and represents the document as S = (s1, s2, ..., s|d|).\nLabel Embedding Attention To measure the compatibility between labels and texts, we adopt the label embedding attention mechanism. Given a structured class hierarchy, we denote the label embeddings of the i-th level as C = (c1, c2, ..., c|li|), where |li| is the number of classes in the i-th level. Then we calculate the cosine similarity\nmatrix G ∈ R|d|×|li| between words and labels via gkj = (s>k cj)/(‖sk‖ ‖cj‖) for the i-th level. Inspired by Wang et al. (2018) and Wang et al. (2019), we adopt convolutional filters F to measure the correlations rp between the p-th phrase of length 2k + 1 and the classes at i-th level, rp = ReLU(F ⊗Gp−k:p+k + b), where b ∈ R|li|. We denote the largest correlation value of the pth phrase with regard to the labels of i-th level as tp = max-pooling(rp). Then we get the labelto-text attention score α ∈ R|d| by normalizing t ∈ R|d| with the SoftMax function. Finally, the document representation datt can be obtained by averaging the word embeddings, weighted by labelto-text attention score: datt = ∑|d| k αksk."
    }, {
      "heading" : "2.2 Concept Sharing Module (CSM)",
      "text" : "Most of researchers focus on measuring the correlations of classes by modeling the structured class hierarchy. In fact, they only get the information of graphic connections. By contrast, the concepts are more semantic, fine-grained and interpretable, but have been ignored. To further exploit the concepts, we design a concept module to explicitly model the mechanism of sharing concepts among classes and measure the intensity of interactions.\nConcepts Encoder Given the corpus of class c, we extract the keywords from the documents and take top-n ranked keywords as the concepts of class\nAlgorithm 1 Pseudo Code of Concepts Sharing via Dynamic Routing Input: all the classes c and their concepts e in level l; all the classes in level (l + 1) Output: cCLj : the concept-based label embedding of the class in level (l + 1);\n1: for each concept i of a class c in level l and each of its child class j in level (l + 1): bij ← 0; 2: for r iterations do 3: for each concept i of class c in level l: βi ← softmax(bi); .softmax computes Eq. 1 4: for each child class j of class c in level (l + 1): vj ← ∑ i βijei; 5: for each child class j of class c in level (l + 1): cCLj ← squash(vj) .squash computes Eq. 4 6: for each concept i of class c in level l and each of its child class j in level (l + 1): bij ← bij+ei·cCLj 7: end for 8: return cCLj\nc. In the WOS dataset, every document is already annotated with several keywords. So we rank the keywords by term frequency within each class. For the DBpedia dataset, there is no annotated keyword available. We carry out the Chi-square (χ2) statistical test, which has been widely accepted as a statistical hypothesis test to evaluate the dependency between words and classes (Barnard, 1992; Palomino et al., 2009; Kuang and Davison, 2017). The words are ranked by the χ2 values. Having extracted concepts for each class, we represent them with word embeddings.\nTo further encode the concepts, we exploit two different ways and make a comparison in experiments. A simple and efficient way is to feed the concept embeddings into the sharing networks directly. Alternatively, we try the k-means clustering algorithm (Hartigan and Wong, 1979) in consideration of the similarity between concepts, then get the embeddings of cluster centers. The outputs (word embeddings or cluster centers) of concepts encoder are denoted as Ec = (e1, e2, ..., en) for class c.\nConcepts Sharing via Dynamic Routing For the HTC task, we find that there are concepts of parent classes shared by their child classes. The semantically related classes share some concepts in common. The concepts describe a class in different views. We adopt the dynamic routing mechanism in the CapsNet (Sabour et al., 2017), which is effective for sharing the information from lower levels to higher levels. Considering the characters of HTC, we modify it to explicitly model the interactions among classes and quantitatively measure the intensity.\nTo utilize the taxonomic hierarchy, we build routing only between the class and its own child classes, which is different from the full connections in CapsNet. We take the coupling coefficients between\nconcepts of a parent class and all its child classes as the intensities of the sharing interactions. The intensity (coupling coefficient) βij sums to 1 and is determined by a “routing softmax”. The logit bij is the log prior probability that concept i of a parent class should be shared to its child class j in level ln.\nβij = exp(bij)∑|ln| k exp(bik)\n(1)\nThe logit bij is iteratively refined by adding with the agreement.\nbij ← bij + ei · cCLj (2)\nThe agreement is the scalar product between the concept embedding ei and the concept-based label embedding (CL) of the child class cCLj . The vj is the intermediate label embedding of the child class, which is generated by weighting over all the concepts of its parent class.\nvj = ∑ i βijei (3)\nAs Sabour et al. (2017) do in the CapsNet, we also use a non-linear “squashing” function which is effective in our experiments.\ncCLj = ‖vj‖2 1 + ‖vj‖2 vj ‖vj‖\n(4)\nFinally, we get the concept-based label embedding for class cj by modeling the sharing mechanism. The new generated label embedding cCLj is constructed with several concepts ei in different views and affected in different intensities βij . Compared with randomly initializing cCLj , an external knowledge source is taken as an initial reference which is more effective in experiments. The procedures are illustrated in Algorithm 1."
    }, {
      "heading" : "2.3 Classification",
      "text" : "We build a classifier for each class level. Let ŷli denote the predictions of the classes in i-th level.\nŷli = softmax(Wom+ bo) (5)\nm = ReLU(Wm[d EK att;d CL att;d PRE att ] + bm) (6)\nwhere Wo, bo,Wm, bm are learnable parameters and [; ] is the vector concatenating operator. The dEKatt and d CL att are document representations weighted respectively by the label-to-text attention scores via external knowledge (EK) initialized label embeddings and concepts-based label embeddings (CL). To utilize the predictions in the (i-1)-th level, we feed the document represent dPREatt into the i-th level classifier. dPREatt is weighted by the attention scores of the predicted soft label embedding cP. dPREatt = ∑|d| k αksk, where αk = (s>k c P)/(‖sk‖\n∥∥cP∥∥), cP = ∑|li−1|j ŷli−1j cEKj and cEKj is the label embedding represented by averaging word embeddings of class definition in external knowledge (EK encoder in Figure 2). We calculate the loss of classifier in i-th level as follows:\nLli = 1 N N∑ n=1 CE(ylin , ŷ li n ) (7)\nwhere ylin is the one-hot vector of ground truth label in the i-th level for document n and CE(·, ·) is the cross entropy between two probability vectors. We optimize the model parameters by minimize the overall loss function:\nL = H∑ i=1 Lli (8)\nwhere H is the total number of levels in the structured class hierarchy."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Datasets",
      "text" : "We evaluate our model on two widely used hierarchical text classification datasets: Web of Science (WOS; Kowsari et al. (2017)) and DBpedia (Sinha et al., 2018). The former includes published papers available from the Web of Science (Reuters, 2012). The latter is curated by Sinha et al. (2018) from DBpedia2. The general information of datasets\n2https://wiki.dbpedia.org/\nis shown in Table 1. We complement these two datasets by extracting the hierarchy concepts and annotating the classes with the definitions from Wikipedia3."
    }, {
      "heading" : "3.2 Metrics and Parameter Settings",
      "text" : "As the state-of-the-art methods do, we take the accuracy of each level and the overall accuracy as metrics. Hyper-parameters are tuned on a validation set by grid search. We take Stanford’s publicly available GloVe 300-dimensional embeddings trained on 42 billion tokens from Common Crawl (Pennington et al., 2014) as initialization for word embeddings. The number of filters in CNN is 128 and the region size is {2, 3}. The number of hidden units in bi-GRU is 150. We set the maximum length of token inputs as 512. The rate of dropout is 0.5. The number of routing iterations is 3. We compare two different inputs of the sharing networks: 1) top 30 ranked concepts of each parent class as inputs; 2) 40 cluster centers generated by the k-means clustering algorithm on 1k concepts for each parent class. We train the parameters by the Adam Optimizer (Kingma and Ba, 2014) with an initial learning rate of 1e-3 and a batch size of 128."
    }, {
      "heading" : "3.3 Baselines",
      "text" : "HDLTex Kowsari et al. (2017) prove that the hierarchical deep learning networks outperform the conventional approaches (Naı̈ve Bayes or SVM).\nHNATC Sinha et al. (2018) propose a Hierarchical Neural Attention-based Text Classifier. They build one classifier for each level and concatenate the predicted category embedding at (i-1)-th level with each of the encoder’s outputs to calculate attention scores for i-th level.\n3https://www.wikipedia.org/\nHARNN Huang et al. (2019) propose a model called Hierarchical Attention-based Recurrent Neural Network with one classifier for each class level. They focus on modeling the dependencies among class levels and the text-label compatibility.\nA-PNC-B Rivas Rojas et al. (2020) define the HTC as a sequence-to-sequence problem and propose a synthetic task of bottom-up-classification. They represent classes with external dictionaries. Their best combined strategy is Auxiliary task + Parent Node Conditioning (PNC) + Beam search.\nHiAGM Zhou et al. (2020) propose a hierarchyaware global model. They employ Tree-LSTM and hierarchy-GCN as the hierarchy encoder. Text feature Propagation (TP) and Label Attention (LA) are utilized for measuring the label-word compatibility. There are four HiAGM variants: TP-LSTM, TP-GCN, LA-LSTM, and LA-GCN."
    }, {
      "heading" : "3.4 Compared with State-of-the-art Methods",
      "text" : "To illustrate the practical significance of our proposed model, we make comparisons with several competitive state-of-the-art methods. The results of experiments conducted on the public datasets are shown in Table 2. Most of the state-of-the-art methods referred to in Section 3.3 adopt a hierarchical attention-based network as their models’ framework. Within their models, the hierarchical framework is effective in utilizing the classification results of the previous levels for the next levels. The label embedding attention mechanism helps to import external knowledge sources and the taxonomic hierarchy. On both of the two datasets,\nthe state-of-the-art methods obtain competitive performance. With a similar framework, our model focuses on the concept-based label embedding and outperforms the other methods on both level and overall accuracy. The results indicate the effectiveness of the concepts among classes which have been ignored by previous work. The concept-based label embedding models related classes by the sharing mechanism with common concepts (visualizations in Section 3.6). The ablation comparisons are shown in Section 3.5.\nThe experimental results of the two variants of our model are also shown in Table 2. Compared with directly feeding the concepts into the sharing networks (CLED), the variant CLEDcluster performs slightly better. It indicates that cluster centers generated by the k-means algorithm are more informative and effective."
    }, {
      "heading" : "3.5 Ablation Experiments",
      "text" : "To investigate the effectiveness of different parts in our model, we carry out ablation studies. The experiment results are shown in Table 3.\nEffectiveness of Concept-based Label Embedding By comparing the results of CLED and the model without the learnt concept-based label embedding (w/o CL), we further confirm that the concepts shared among classes help to improve the performance.\nEffectiveness of Dynamic Routing We remove the dynamic routing networks from the model CLED. Because there is no dynamic routing to share the concepts from the parent classes to their\nchild classes, it is an intuitive way to represent the label embeddings by averaging the word embeddings of the child classes’ concepts. Specifically, there are top-30 ranked concepts for each parent class to share with their child classes. So for the model without dynamic routing (w/o DR), we represent the child class label embedding with the top-30 ranked concepts of each child class. Although the concepts of child classes are more finegrained and informative than the concepts of parent classes, the model CLED with the dynamic routing networks to share the concepts among classes performs better. It indicates that modeling the sharing mechanism and learning to represent the child classes with common concepts are more effective.\nEffectiveness of External Knowledge We take an external knowledge source as the initial reference of child classes in the concepts sharing module. When we remove the reference (w/o reference in CSM), the results are slightly worse on accuracy. It demonstrates that the external knowledge makes an efficient reference for the concept sharing.\nSimilar to the state-of-the-art methods, the external knowledge is also used individually as the representation of each class in our model. It helps to measure the compatibility between labels and texts via the attention mechanism. When we fully remove the external knowledge and initialize the label embeddings randomly (w/o EK), the performances are slightly worse than that with external knowledge (CLED). It indicates the effectiveness of external knowledge. Besides, the experiment which removes the predicted soft label embedding (w/o PRE) proves that, it is effective to utilize the predictions of previous level."
    }, {
      "heading" : "3.6 Visualizations of Concepts Sharing",
      "text" : "In this paper, we explicitly investigate the concept sharing process. A concept sharing module is designed to model the mechanism of sharing concepts\namong classes and measure the intensity of interactions. The heat map of the learnt dynamic routing scores between the concepts of class “Computer Science” and its child classes is illustrated in Figure 3. The color changes from white to blue while the score increases. The score indicates the intensity between the concept and class in the sharing process. In Figure 3, we find that the concept “design” is shared by the classes “Soft engineering” and “Algorithm design”. The concept “distributed” is shared by the classes “Network security” and “Distributed computing”. The concept is shared by related classes.\nWe use t-SNE (Van der Maaten and Hinton, 2008) to visualize the concept embeddings of class “Computer Science” and the concept-based label embeddings of its child classes on a 2D map in Figure 4. The label embedding (red triangle) is constructed with the embeddings of concepts (blue dot). As shown, the class “Software engineering” is surrounded by the concepts “optimization” and “design”. “Network security” is surrounded by “cloud”, “machine” and “security”. The class is described by several concepts in different views.\nThe visualizations in Figure 3 and 4 indicate that we successfully model the concept sharing mechanism in a semantic and explicit way."
    }, {
      "heading" : "4 Related Work",
      "text" : "Hierarchical text classification with label embeddings Recently, researchers try to adopt the label embeddings in the hierarchical text classification task. Huang et al. (2019) propose hierarchical attention-based recurrent neural network (HARNN) by adopting label embeddings. Mao et al. (2019) propose to learn a label assignment policy via deep reinforcement learning with label embeddings. Peng et al. (2019) propose hierarchical taxonomy-aware and attentional graph RCNNs with label embeddings. Rivas Rojas et al. (2020)\ndefine the HTC task as a sequence-to-sequence problem. Their label embedding is defined by external knowledge. For modeling label dependencies, Zhou et al. (2020) formulate the hierarchy as a directed graph and introduce hierarchy-aware structure encoders. Cao et al. (2020) and Chen et al. (2020a) exploit the hyperbolic representation for labels by encoding the taxonomic hierarchy.\nHierarchical text classification besides label embeddings According to the motivation of this work, we separate previous work with label embeddings from the HTC task and present it in the above paragraph. Besides, existing work is usually categorized into flat, local and global approaches (Silla and Freitas, 2011). The flat classification approach completely ignores the class hierarchy and only predicts classes at the leaf nodes (Aly et al., 2019). The local classification approaches could be grouped as a local classifier per node (LCN), a local classifier per parent node (LCPN) and a local classifier per level (LCL). The LCN approach train one binary classifier for each node of the hierarchy (Fagni and Sebastiani, 2007). Banerjee et al. (2019) apply transfer learning in LCN by fine-tuning the parent classifier for the child class. For the LCPN, a multiclass classifier for each parent node is trained to distinguish between its child nodes (Wu et al., 2005; Dumais and Chen, 2000). Xu and Geng (2019) investigate the correlation among labels by the label\ndistribution as an LCPN approach. The LCL approach consists of training one multi-class classifier for each class level (Kowsari et al., 2017; Shimura et al., 2018). Zhu and Bain (2017) introduce a BCNN model which outputs predictions corresponding to the hierarchical structure. Chen et al. (2020b) propose a multi-level learning to rank model with multi-level hinge loss margins. The global approach learns a global classification model about the whole class hierarchy (Cai and Hofmann, 2004; Gopal and Yang, 2013; Wing and Baldridge, 2014; Karn et al., 2017). Qiu et al. (2011) exploit the latent nodes in the taxonomic hierarchy with a global approach. For the need for a large amount of training data, a weakly-supervised global HTC method is proposed by Meng et al. (2019). Meta-learning is adopted by Wu et al. (2019) for HTC in a global way. In addition, there is some work combined with both local and global approach (Wehrmann et al., 2018). A local flat tree classifier is introduced by Peng et al. (2018) which utilizes the graph-CNN."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we investigate the concept which is a kind of domain-specific and fine-grained information for the hierarchical text classification. We propose a novel concept-based label embedding model. Compared with several competitive stateof-the-art methods, the experimental results on two widely used datasets prove the effectiveness of our proposed model. The visualization of the concepts and the learnt concept-based label embeddings re-\nveal the high interpretability of our model."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We sincerely thank Bingning Wang 4 for helpful discussions, and all reviewers and ACs for their insightful comments, time and efforts."
    } ],
    "references" : [ {
      "title" : "Hierarchical multi-label classification of text with capsule networks",
      "author" : [ "Rami Aly", "Steffen Remus", "Chris Biemann." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 323–",
      "citeRegEx" : "Aly et al\\.,? 2019",
      "shortCiteRegEx" : "Aly et al\\.",
      "year" : 2019
    }, {
      "title" : "Hierarchical transfer learning for multi-label text classification",
      "author" : [ "Siddhartha Banerjee", "Cem Akkaya", "Francisco PerezSorrosal", "Kostas Tsioutsiouliklis." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Banerjee et al\\.,? 2019",
      "shortCiteRegEx" : "Banerjee et al\\.",
      "year" : 2019
    }, {
      "title" : "Introduction to pearson (1900) on the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling",
      "author" : [ "GA Barnard." ],
      "venue" : "Break-",
      "citeRegEx" : "Barnard.,? 1992",
      "shortCiteRegEx" : "Barnard.",
      "year" : 1992
    }, {
      "title" : "Hierarchical document categorization with support vector machines",
      "author" : [ "Lijuan Cai", "Thomas Hofmann." ],
      "venue" : "Proceedings of the thirteenth ACM international conference on Information and knowledge management, pages 78–87.",
      "citeRegEx" : "Cai and Hofmann.,? 2004",
      "shortCiteRegEx" : "Cai and Hofmann.",
      "year" : 2004
    }, {
      "title" : "Hypercore: Hyperbolic and co-graph representation for automatic icd coding",
      "author" : [ "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao", "Shengping Liu", "Weifeng Chong." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Cao et al\\.,? 2020",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2020
    }, {
      "title" : "Hyperbolic interaction model for hierarchical multi-label classification",
      "author" : [ "Boli Chen", "Xin Huang", "Lin Xiao", "Zixin Cai", "Liping Jing." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7496–7503.",
      "citeRegEx" : "Chen et al\\.,? 2020a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Hierarchical entity typing via multi-level learning to rank",
      "author" : [ "Tongfei Chen", "Yunmo Chen", "Benjamin Van Durme." ],
      "venue" : "arXiv preprint arXiv:2004.02286.",
      "citeRegEx" : "Chen et al\\.,? 2020b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Explicit interaction model towards text classification",
      "author" : [ "Cunxiao Du", "Zhaozheng Chen", "Fuli Feng", "Lei Zhu", "Tian Gan", "Liqiang Nie." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6359–6366.",
      "citeRegEx" : "Du et al\\.,? 2019",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2019
    }, {
      "title" : "Hierarchical classification of web content",
      "author" : [ "Susan Dumais", "Hao Chen." ],
      "venue" : "Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 256–263.",
      "citeRegEx" : "Dumais and Chen.,? 2000",
      "shortCiteRegEx" : "Dumais and Chen.",
      "year" : 2000
    }, {
      "title" : "On the selection of negative examples for hierarchical text categorization",
      "author" : [ "Tiziano Fagni", "Fabrizio Sebastiani." ],
      "venue" : "Proceedings of the 3rd Language & Technology Conference (LTC’07), pages 24–28. Citeseer.",
      "citeRegEx" : "Fagni and Sebastiani.,? 2007",
      "shortCiteRegEx" : "Fagni and Sebastiani.",
      "year" : 2007
    }, {
      "title" : "Recursive regularization for large-scale classification with hierarchical and graphical dependencies",
      "author" : [ "Siddharth Gopal", "Yiming Yang." ],
      "venue" : "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "Gopal and Yang.,? 2013",
      "shortCiteRegEx" : "Gopal and Yang.",
      "year" : 2013
    }, {
      "title" : "Algorithm as 136: A k-means clustering algorithm",
      "author" : [ "John A Hartigan", "Manchek A Wong." ],
      "venue" : "Journal of the royal statistical society. series c (applied statistics), 28(1):100–108.",
      "citeRegEx" : "Hartigan and Wong.,? 1979",
      "shortCiteRegEx" : "Hartigan and Wong.",
      "year" : 1979
    }, {
      "title" : "Hierarchical multi-label text classification: An attention-based recurrent network approach",
      "author" : [ "Wei Huang", "Enhong Chen", "Qi Liu", "Yuying Chen", "Zai Huang", "Yang Liu", "Zhou Zhao", "Dan Zhang", "Shijin Wang." ],
      "venue" : "Proceedings of the 28th ACM Interna-",
      "citeRegEx" : "Huang et al\\.,? 2019",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "End-to-end trainable attentive decoder for hierarchical entity classification",
      "author" : [ "Sanjeev Karn", "Ulli Waltinger", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2,",
      "citeRegEx" : "Karn et al\\.,? 2017",
      "shortCiteRegEx" : "Karn et al\\.",
      "year" : 2017
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Hdltex: Hierarchical deep learning for text classification",
      "author" : [ "Kamran Kowsari", "Donald E Brown", "Mojtaba Heidarysafa", "Kiana Jafari Meimandi", "Matthew S Gerber", "Laura E Barnes." ],
      "venue" : "2017 16th IEEE international conference on machine learning",
      "citeRegEx" : "Kowsari et al\\.,? 2017",
      "shortCiteRegEx" : "Kowsari et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning word embeddings with chi-square weights for healthcare tweet classification",
      "author" : [ "Sicong Kuang", "Brian D Davison." ],
      "venue" : "Applied Sciences, 7(8):846.",
      "citeRegEx" : "Kuang and Davison.,? 2017",
      "shortCiteRegEx" : "Kuang and Davison.",
      "year" : 2017
    }, {
      "title" : "Visualizing data using t-sne",
      "author" : [ "Laurens Van der Maaten", "Geoffrey Hinton." ],
      "venue" : "Journal of machine learning research, 9(11).",
      "citeRegEx" : "Maaten and Hinton.,? 2008",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    }, {
      "title" : "Hierarchical text classification with reinforced label assignment",
      "author" : [ "Yuning Mao", "Jingjing Tian", "Jiawei Han", "Xiang Ren." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Mao et al\\.,? 2019",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2019
    }, {
      "title" : "Weakly-supervised hierarchical text classification",
      "author" : [ "Yu Meng", "Jiaming Shen", "Chao Zhang", "Jiawei Han." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6826–6833.",
      "citeRegEx" : "Meng et al\\.,? 2019",
      "shortCiteRegEx" : "Meng et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatic extraction of keywords for a multimedia search engine using the chi-square test",
      "author" : [ "Marco A Palomino", "Michael P Oakes", "Tom Wuytack." ],
      "venue" : "Proceedings of the 9th Dutch–Belgian information retrieval workshop (DIR 2009), pages 3–10.",
      "citeRegEx" : "Palomino et al\\.,? 2009",
      "shortCiteRegEx" : "Palomino et al\\.",
      "year" : 2009
    }, {
      "title" : "Large-scale hierarchical text classification with recursively regularized deep graph-cnn",
      "author" : [ "Hao Peng", "Jianxin Li", "Yu He", "Yaopeng Liu", "Mengjiao Bao", "Lihong Wang", "Yangqiu Song", "Qiang Yang." ],
      "venue" : "Proceedings of the 2018 World Wide Web Conference,",
      "citeRegEx" : "Peng et al\\.,? 2018",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2018
    }, {
      "title" : "Hierarchical taxonomy-aware and attentional graph capsule rcnns for large-scale multi-label text classification",
      "author" : [ "Hao Peng", "Jianxin Li", "Senzhang Wang", "Lihong Wang", "Qiran Gong", "Renyu Yang", "Bo Li", "Philip Yu", "Lifang He." ],
      "venue" : "IEEE Transactions on",
      "citeRegEx" : "Peng et al\\.,? 2019",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2019
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Hierarchical text classification with latent concepts",
      "author" : [ "Xipeng Qiu", "Xuan-Jing Huang", "Zhao Liu", "Jinlong Zhou." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages",
      "citeRegEx" : "Qiu et al\\.,? 2011",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2011
    }, {
      "title" : "Efficient strategies for hierarchical text classification: External knowledge and auxiliary tasks",
      "author" : [ "Kervy Rivas Rojas", "Gina Bustamante", "Arturo Oncevay", "Marco Antonio Sobrevilla Cabezudo." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Asso-",
      "citeRegEx" : "Rojas et al\\.,? 2020",
      "shortCiteRegEx" : "Rojas et al\\.",
      "year" : 2020
    }, {
      "title" : "Dynamic routing between capsules",
      "author" : [ "Sara Sabour", "Nicholas Frosst", "Geoffrey E Hinton." ],
      "venue" : "Advances in neural information processing systems, pages 3856–3866.",
      "citeRegEx" : "Sabour et al\\.,? 2017",
      "shortCiteRegEx" : "Sabour et al\\.",
      "year" : 2017
    }, {
      "title" : "Hft-cnn: Learning hierarchical category structure for multi-label short text categorization",
      "author" : [ "Kazuya Shimura", "Jiyi Li", "Fumiyo Fukumoto." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 811–816.",
      "citeRegEx" : "Shimura et al\\.,? 2018",
      "shortCiteRegEx" : "Shimura et al\\.",
      "year" : 2018
    }, {
      "title" : "A survey of hierarchical classification across different application domains",
      "author" : [ "Carlos N Silla", "Alex A Freitas." ],
      "venue" : "Data Mining and Knowledge Discovery, 22(1-2):31–72.",
      "citeRegEx" : "Silla and Freitas.,? 2011",
      "shortCiteRegEx" : "Silla and Freitas.",
      "year" : 2011
    }, {
      "title" : "A hierarchical neural attentionbased text classifier",
      "author" : [ "Koustuv Sinha", "Yue Dong", "Jackie Chi Kit Cheung", "Derek Ruths." ],
      "venue" : "Proceedings of the 2018",
      "citeRegEx" : "Sinha et al\\.,? 2018",
      "shortCiteRegEx" : "Sinha et al\\.",
      "year" : 2018
    }, {
      "title" : "Pte: Predictive text embedding through large-scale heterogeneous text networks",
      "author" : [ "Jian Tang", "Meng Qu", "Qiaozhu Mei." ],
      "venue" : "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1165–1174.",
      "citeRegEx" : "Tang et al\\.,? 2015",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2015
    }, {
      "title" : "Experiment with a hierarchical text categorization method on wipo patent collections",
      "author" : [ "Domonkos Tikk", "György Biró", "Jae Dong Yang." ],
      "venue" : "Applied Research in Uncertainty Modeling and Analysis, pages 283–302. Springer.",
      "citeRegEx" : "Tikk et al\\.,? 2005",
      "shortCiteRegEx" : "Tikk et al\\.",
      "year" : 2005
    }, {
      "title" : "Document gated reader for open-domain question answering",
      "author" : [ "Bingning Wang", "Ting Yao", "Qi Zhang", "Jingfang Xu", "Zhixing Tian", "Kang Liu", "Jun Zhao." ],
      "venue" : "Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Joint embedding of words and labels for text classification",
      "author" : [ "Guoyin Wang", "Chunyuan Li", "Wenlin Wang", "Yizhe Zhang", "Dinghan Shen", "Xinyuan Zhang", "Ricardo Henao", "Lawrence Carin." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Associa-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Hierarchical multi-label classification networks",
      "author" : [ "Jonatas Wehrmann", "Ricardo Cerri", "Rodrigo Barros." ],
      "venue" : "International Conference on Machine Learning, pages 5075–5084.",
      "citeRegEx" : "Wehrmann et al\\.,? 2018",
      "shortCiteRegEx" : "Wehrmann et al\\.",
      "year" : 2018
    }, {
      "title" : "Hierarchical discriminative classification for text-based geolocation",
      "author" : [ "Benjamin Wing", "Jason Baldridge." ],
      "venue" : "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 336–348.",
      "citeRegEx" : "Wing and Baldridge.,? 2014",
      "shortCiteRegEx" : "Wing and Baldridge.",
      "year" : 2014
    }, {
      "title" : "Learning classifiers using hierarchically structured class taxonomies",
      "author" : [ "Feihong Wu", "Jun Zhang", "Vasant Honavar." ],
      "venue" : "International symposium on abstraction, reformulation, and approximation, pages 313–320. Springer.",
      "citeRegEx" : "Wu et al\\.,? 2005",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2005
    }, {
      "title" : "Learning to learn and predict: A metalearning approach for multi-label classification",
      "author" : [ "Jiawei Wu", "Wenhan Xiong", "William Yang Wang." ],
      "venue" : "arXiv preprint arXiv:1909.04176.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Hierarchical classification based on label distribution learning",
      "author" : [ "Changdong Xu", "Xin Geng." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 5533–5540.",
      "citeRegEx" : "Xu and Geng.,? 2019",
      "shortCiteRegEx" : "Xu and Geng.",
      "year" : 2019
    }, {
      "title" : "Hierarchy-aware global model for hierarchical text classification",
      "author" : [ "Jie Zhou", "Chunping Ma", "Dingkun Long", "Guangwei Xu", "Ning Ding", "Haoyu Zhang", "Pengjun Xie", "Gongshen Liu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    }, {
      "title" : "B-cnn: branch convolutional neural network for hierarchical classification",
      "author" : [ "Xinqi Zhu", "Michael Bain." ],
      "venue" : "arXiv preprint arXiv:1709.09890.",
      "citeRegEx" : "Zhu and Bain.,? 2017",
      "shortCiteRegEx" : "Zhu and Bain.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 31,
      "context" : "In the real world, the text classification is usually cast as a hierarchical text classification (HTC) problem, such as patent collection (Tikk et al., 2005), web content collection (Dumais and Chen, 2000) and medical record cod-",
      "startOffset" : 138,
      "endOffset" : 157
    }, {
      "referenceID" : 8,
      "context" : ", 2005), web content collection (Dumais and Chen, 2000) and medical record cod-",
      "startOffset" : 32,
      "endOffset" : 55
    }, {
      "referenceID" : 28,
      "context" : "In these scenarios, the HTC task aims to categorize a textual description within a set of labels that are organized in a structured class hierarchy (Silla and Freitas, 2011).",
      "startOffset" : 148,
      "endOffset" : 173
    }, {
      "referenceID" : 0,
      "context" : "They have proposed various HTC solutions, which are usually categorized into flat (Aly et al., 2019), local (Xu and Geng, 2019), global (Qiu et al.",
      "startOffset" : 82,
      "endOffset" : 100
    }, {
      "referenceID" : 38,
      "context" : ", 2019), local (Xu and Geng, 2019), global (Qiu et al.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 24,
      "context" : ", 2019), local (Xu and Geng, 2019), global (Qiu et al., 2011) and combined approaches (Wehrmann et al.",
      "startOffset" : 43,
      "endOffset" : 61
    }, {
      "referenceID" : 40,
      "context" : "simply represented as one-hot vectors (Zhu and Bain, 2017; Wehrmann et al., 2018).",
      "startOffset" : 38,
      "endOffset" : 81
    }, {
      "referenceID" : 34,
      "context" : "simply represented as one-hot vectors (Zhu and Bain, 2017; Wehrmann et al., 2018).",
      "startOffset" : 38,
      "endOffset" : 81
    }, {
      "referenceID" : 7,
      "context" : "In the field of text classification (includes the HTC task), researchers propose several forms of label embeddings to encode different kinds of information, such as 1) anchor points (Du et al., 2019), 2) compatibility between labels and words (Wang et al.",
      "startOffset" : 182,
      "endOffset" : 199
    }, {
      "referenceID" : 33,
      "context" : ", 2019), 2) compatibility between labels and words (Wang et al., 2018; Huang et al., 2019; Tang et al., 2015), 3) taxonomic hierarchy (Cao et al.",
      "startOffset" : 51,
      "endOffset" : 109
    }, {
      "referenceID" : 12,
      "context" : ", 2019), 2) compatibility between labels and words (Wang et al., 2018; Huang et al., 2019; Tang et al., 2015), 3) taxonomic hierarchy (Cao et al.",
      "startOffset" : 51,
      "endOffset" : 109
    }, {
      "referenceID" : 30,
      "context" : ", 2019), 2) compatibility between labels and words (Wang et al., 2018; Huang et al., 2019; Tang et al., 2015), 3) taxonomic hierarchy (Cao et al.",
      "startOffset" : 51,
      "endOffset" : 109
    }, {
      "referenceID" : 4,
      "context" : ", 2015), 3) taxonomic hierarchy (Cao et al., 2020; Zhou et al., 2020) and 4) external knowledge (Rivas Rojas et al.",
      "startOffset" : 32,
      "endOffset" : 69
    }, {
      "referenceID" : 39,
      "context" : ", 2015), 3) taxonomic hierarchy (Cao et al., 2020; Zhou et al., 2020) and 4) external knowledge (Rivas Rojas et al.",
      "startOffset" : 32,
      "endOffset" : 69
    }, {
      "referenceID" : 24,
      "context" : "For instance, there is a parent node named “Sports” in a concrete class hierarchy (Qiu et al., 2011).",
      "startOffset" : 82,
      "endOffset" : 100
    }, {
      "referenceID" : 15,
      "context" : "As shown in Figure 1, we have a similar observation in WOS (Kowsari et al., 2017), which is a widely used public dataset (details in our experiments).",
      "startOffset" : 59,
      "endOffset" : 81
    }, {
      "referenceID" : 26,
      "context" : "Inspired by the CapsNet (Sabour et al., 2017), we employ the dynamic routing mechanism.",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 15,
      "context" : "• We complement the public datasets WOS (Kowsari et al., 2017) and DBpedia (Sinha et al.",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 29,
      "context" : ", 2017) and DBpedia (Sinha et al., 2018) by exacting the hierarchy concept and annotating the classes with the definitions from Wikipedia.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 29,
      "context" : "In recent years, the hierarchical neural network has been proven effective for the HTC task by much work (Sinha et al., 2018; Wehrmann et al., 2018; Huang et al., 2019).",
      "startOffset" : 105,
      "endOffset" : 168
    }, {
      "referenceID" : 34,
      "context" : "In recent years, the hierarchical neural network has been proven effective for the HTC task by much work (Sinha et al., 2018; Wehrmann et al., 2018; Huang et al., 2019).",
      "startOffset" : 105,
      "endOffset" : 168
    }, {
      "referenceID" : 12,
      "context" : "In recent years, the hierarchical neural network has been proven effective for the HTC task by much work (Sinha et al., 2018; Wehrmann et al., 2018; Huang et al., 2019).",
      "startOffset" : 105,
      "endOffset" : 168
    }, {
      "referenceID" : 2,
      "context" : "We carry out the Chi-square (χ2) statistical test, which has been widely accepted as a statistical hypothesis test to evaluate the dependency between words and classes (Barnard, 1992; Palomino et al., 2009; Kuang and Davison, 2017).",
      "startOffset" : 168,
      "endOffset" : 231
    }, {
      "referenceID" : 20,
      "context" : "We carry out the Chi-square (χ2) statistical test, which has been widely accepted as a statistical hypothesis test to evaluate the dependency between words and classes (Barnard, 1992; Palomino et al., 2009; Kuang and Davison, 2017).",
      "startOffset" : 168,
      "endOffset" : 231
    }, {
      "referenceID" : 16,
      "context" : "We carry out the Chi-square (χ2) statistical test, which has been widely accepted as a statistical hypothesis test to evaluate the dependency between words and classes (Barnard, 1992; Palomino et al., 2009; Kuang and Davison, 2017).",
      "startOffset" : 168,
      "endOffset" : 231
    }, {
      "referenceID" : 11,
      "context" : "Alternatively, we try the k-means clustering algorithm (Hartigan and Wong, 1979) in considera-",
      "startOffset" : 55,
      "endOffset" : 80
    }, {
      "referenceID" : 26,
      "context" : "We adopt the dynamic routing mechanism in the CapsNet (Sabour et al., 2017), which is effective for sharing the information from lower levels to higher levels.",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 23,
      "context" : "We take Stanford’s publicly available GloVe 300-dimensional embeddings trained on 42 billion tokens from Common Crawl (Pennington et al., 2014) as initialization for word embeddings.",
      "startOffset" : 118,
      "endOffset" : 143
    }, {
      "referenceID" : 14,
      "context" : "We train the parameters by the Adam Optimizer (Kingma and Ba, 2014) with",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 28,
      "context" : "Besides, existing work is usually categorized into flat, local and global approaches (Silla and Freitas, 2011).",
      "startOffset" : 85,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : "The flat classification approach completely ignores the class hierarchy and only predicts classes at the leaf nodes (Aly et al., 2019).",
      "startOffset" : 116,
      "endOffset" : 134
    }, {
      "referenceID" : 9,
      "context" : "The LCN approach train one binary classifier for each node of the hierarchy (Fagni and Sebastiani, 2007).",
      "startOffset" : 76,
      "endOffset" : 104
    }, {
      "referenceID" : 36,
      "context" : "For the LCPN, a multiclass classifier for each parent node is trained to distinguish between its child nodes (Wu et al., 2005; Dumais and Chen, 2000).",
      "startOffset" : 109,
      "endOffset" : 149
    }, {
      "referenceID" : 8,
      "context" : "For the LCPN, a multiclass classifier for each parent node is trained to distinguish between its child nodes (Wu et al., 2005; Dumais and Chen, 2000).",
      "startOffset" : 109,
      "endOffset" : 149
    }, {
      "referenceID" : 15,
      "context" : "The LCL approach consists of training one multi-class classifier for each class level (Kowsari et al., 2017; Shimura et al., 2018).",
      "startOffset" : 86,
      "endOffset" : 130
    }, {
      "referenceID" : 27,
      "context" : "The LCL approach consists of training one multi-class classifier for each class level (Kowsari et al., 2017; Shimura et al., 2018).",
      "startOffset" : 86,
      "endOffset" : 130
    } ],
    "year" : 2021,
    "abstractText" : "Hierarchical Text Classification (HTC) is a challenging task that categorizes a textual description within a taxonomic hierarchy. Most of the existing methods focus on modeling the text. Recently, researchers attempt to model the class representations with some resources (e.g., external dictionaries). However, the concept shared among classes which is a kind of domain-specific and fine-grained information has been ignored in previous work. In this paper, we propose a novel concept-based label embedding method that can explicitly represent the concept and model the sharing mechanism among classes for the hierarchical text classification. Experimental results on two widely used datasets prove that the proposed model outperforms several state-of-theart methods. We release our complementary resources (concepts and definitions of classes) for these two datasets to benefit the research on HTC.",
    "creator" : "LaTeX with hyperref"
  }
}