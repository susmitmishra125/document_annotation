{
  "name" : "2021.acl-long.200.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Beyond Sentence-Level End-to-End Speech Translation: Context Helps",
    "authors" : [ "Biao Zhang", "Ivan Titov", "Barry Haddow", "Rico Sennrich" ],
    "emails" : [ "B.Zhang@ed.ac.uk,", "ititov@inf.ed.ac.uk,", "bhaddow@inf.ed.ac.uk,", "sennrich@cl.uzh.ch" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2566–2578\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2566"
    }, {
      "heading" : "1 Introduction",
      "text" : "Document-level context often offers extra informative clues that could improve the understanding of individual sentences. Such clues have been proven effective for textual machine translation (MT), particularly in handling translation errors specific to discourse phenomena, such as inaccurate coreference of pronouns (Guillou, 2016) and mistranslation of ambiguous words (Rios et al., 2017). Besides, ensuring consistency in translation is virtually impossible without document-level context as well (Voita et al., 2019). Analogous to MT, speech translation (ST) also suffers from these translation issues, and super-sentential context could in fact be more valuable to ST because 1) homophones\n1Source code is available at https://github.com/ bzhangGo/zero.\nand acoustic noise bring additional ambiguity to ST, and 2) a common use case in ST is simultaneous translation, where the system has to output translations of sentence fragments, and may have to predict future input to account for word order differences between the source and target language (Grissom II et al., 2014). Both for ambiguity from the acoustic signal, and operating on small sentence fragments, we hypothesize that access to extra context2 will be beneficial.\nAlthough recent studies on ST have achieved promising results with end-to-end (E2E) models (Anastasopoulos and Chiang, 2018; Di Gangi et al., 2019; Zhang et al., 2020a; Wang et al., 2020; Dong et al., 2020), nevertheless, they mainly focus on sentence-level translation. One practical challenge when scaling up sentence-level E2E ST to the document-level is the encoding of very long audio segments, which can easily hit the computational bottleneck, especially with Transformers (Vaswani et al., 2017). So far, the research question of whether and how contextual information benefits E2E ST has received little attention.\nIn this paper, we answer this question through extensive experiments by exploring a concatenation-\n2By default, we use context to denote both source- and target-side information from previous sentences.\nbased context-aware ST model. Figure 1 illustrates our model, where neighboring source (target) sequences are chained together into one sequence for joint translation. This paradigm only requires data-level manipulation, thus allowing us to reuse any existing sentence-level E2E ST models. Despite its simplicity, this approach successfully leverages contextual information to improve textual MT (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020), and here we adapt it to ST. As for the computational bottleneck, we shorten the speech encoding sequence via adaptive feature selection (Zhang et al., 2020b,a, AFS), which only retains a small subset of encodings (∼16%) for each audio segment.\nWe investigate several decoding methods, including chunk-based decoding and sliding-window based decoding. We also study an extension of the latter with the constraint of target prefix, where the prefix denotes the translation of previous context speeches. We find that using these methods sometimes results in misaligned translations, particularly when using the constraint. This issue manifests itself in mismatching sentence boundaries and producing over- and/or under-translation, which greatly hurts sentence-based evaluation metrics. To avoid such misalignments, we introduce inmodel ensemble decoding (IMED) to regularize the document-level translation with its sentence-level counterpart. Note that we use the same contextaware ST model here for both types of translation – that’s why we call it in-model ensemble.\nWe adopt Transformer (Vaswani et al., 2017) for experiments with the MuST-C dataset (Di Gangi et al., 2019). We study the impact of context on translation in different settings. Our results demonstrate the effectiveness of contextual modeling. Our main findings are summarized below:\n• Incorporating context improves overall translation quality (+0.18-2.61 BLEU) and benefits pronoun translation across different language pairs, resonating with previous findings in textual MT (Miculicich et al., 2018; Huo et al., 2020). In addition, context also improves the translation of homophones.\n• ST models with contexts suffer less from (artificial) audio segmentation errors.\n• Contextual modeling improves translation quality and reduces latency and flicker for simultaneous translation under re-translation strategy (Arivazhagan et al., 2020a)."
    }, {
      "heading" : "2 Related Work",
      "text" : "Our work is inspired by pioneer studies on contextaware textual MT. Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang, 2013). With the rapid development of neural MT and also available document-level textual datasets, research in this direction gained great popularity. Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; Läubli et al., 2018; Guillou et al., 2018; Voita et al., 2019; Kim et al., 2019; Cai and Xiong, 2020). We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future work.\nContext-aware ST extends the sentence-level ST towards streaming ST which allows models to access unlimited previous audio inputs. Instead of improving contextual modeling, many studies on streaming ST aim at developing better sentence/word segmentation policies to avoid segmentation errors that greatly hurt translation (Matusov et al., 2007; Rangarajan Sridhar et al., 2013; IranzoSánchez et al., 2020; Zhang and Zhang, 2020; Arivazhagan et al., 2020b). Very recently, Ma et al. (2020b) proposed a memory augmented Transformer encoder for streaming ST, where the previous audio features are summarized into a growing continuous memory to improve the model’s context awareness. Despite its success, this method ignores the target-side context, which turns out to have significant positive impact on ST in our experiments.\nOur study still relies on oracle sentence segmentation of the audio. The most related work to ours is (Gaido et al., 2020), which also investigated contextualized translation and showed that contextaware ST is less sensitive to audio segmentation errors. While they exclusively focus on the robustness to segmentation errors, our study investigates the benefits of context-aware E2E ST more broadly."
    }, {
      "heading" : "3 Context-aware ST via Concatenation",
      "text" : "We extend the sentence-level ST with documentlevel context, by modeling up to C previous source/target segments/sentences for translation. Formally, given a pre-segmented audio (source document) A = ( a1, . . . ,aN ) as well as its paired\ntarget document Y = ( y1, . . . ,yN ) , the model is trained to maximize the following likelihood:\nlog p (Y|A) = N∑ n=1 log p ( yn|xn, Cny , Cnx ) , (1)\nwhere xn = AFS (an), i.e. the speech encodings extracted via AFS (Zhang et al., 2020a). an and yn denote the n-th audio segment and target sentence, respectively. N is the number of segments/sentences in the document. Cnx and Cny stand for the source and target context, respectively, i.e. {xn−i}Ci=1 and {yn−i}Ci=1.\nAdaptive Feature Selection Audio segment is often converted into frame-based features for neural modeling. Different from text, each segment might contain hundreds or even thousands of such features, making contextual modeling computationally difficult. Zhang et al. (2020a) found that most speech encodings emitted by a Transformer-based audio encoder carry little information for translation, and their deletion even improves translation quality. We follow Zhang et al. (2020a) and perform AFS to only extract those informative encodings (∼16%) optimized via sentence-level speech recognition with L0DROP (Zhang et al., 2020b). This greatly shortens the speech encoding sequence, thus enabling broader context exploration.\nConcatenation-based Contextual Modeling We adopt the concatenation method to incorporate\nthe previous context (Cnx /Cny ) (Tiedemann and Scherrer, 2017; Bawden et al., 2018) as shown in Figure 1. After obtaining the AFS-based encodings (xn) for each audio segment, we concatenate those encodings of neighboring segments to form the source input. The same is applied to the target-side sentences, except for a separator symbol “<s>” inserted in-between sentences to distinguish sentence boundaries.3 Such modeling enables us to use arbitrary encoder-decoder models for contextaware ST, such as the Transformer (Vaswani et al., 2017) used in this paper. Despite no dedicated hierarchical modeling (Miculicich et al., 2018), this paradigm still allows for intra- and inter-sentence attention during encoding and decoding, which explicitly utilizes context for translation and has been proven successful (Lopes et al., 2020)."
    }, {
      "heading" : "4 Inference",
      "text" : "Concatenation-based contextual modeling allows for different inference strategies with possible trade-offs between simplicity/efficiency and accuracy. We investigate the following inference strategies (see Figure 2):\nChunk-based Decoding (CBD) CBD splits all audio segments in one document into nonoverlapping chunks, with each chunk concatenating C + 1 segments, as shown in Figure 2a. CBD directly translates each chunk, and then recovers sentence-level translation via the separator symbol “<s>”. CBD is the most efficient inference strategy, only encoding/decoding each sentence once, but it might suffer from misaligned translation,\n3Note that we did not add similar boundary information to audio segments, because AFS implicitly captures these signals through independent segment encoding.\nproducing more or fewer sentences than the input segments. We simply drop the extra generated sentences and replace the missing ones with “<unk>” when computing sentence-based evaluation metrics. Also, CBD introduces an independence assumption between chunks.\nSliding Window-based Decoding (SWBD) SWBD avoids such inter-chunk independence by sequentially translating each audio segment (xn), together with its corresponding previous source context (Cnx ). We distinguish two variants of SWBD. The first variant, SWBD, translates the concatenated segments and regards the last generated sentence as the translation of the current segment while discarding all other generations (Figure 2b). Note that this might introduce inconsistencies between the output produced at a time step, and the one used as target context in future time steps. By contrast, the second variant, SWBD-Cons, leverages the previously generated (up to C) sentences as a decoding constraint, based on which the model only needs to generate one sentence (Figure 2c).\nIn-Model Ensemble Decoding (IMED) We observe that SWBD still suffers from misaligned translation, where the translation of the current segment might contain information from previous segments. We introduce IMED to alleviate this issue as shown in Figure 2d. IMED extends SWBD-Cons by interpolating the document-level prediction (pd) with the sentence-level prediction (ps) as follows:\nλpsθ (y n t |yn<t,xn) + (1− λ)pdθ (ynt |C) , (2)\nwhere C = {Cnx , Cny ,xn,yn<t}, λ is a hyperparameter, ynt denotes the t-th target word in sentence yn, and both predictions are based on the same model θ. Intuitively, the sentence-level translation acts as a regularizer, avoiding the over- or undertranslation. Note IMED with λ = 0 corresponds to SWBD-Cons."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Setup",
      "text" : "We use the MuST-C dataset (Di Gangi et al., 2019) for experiments, which was collected from English TED talks and covers translations from English to 8 different languages, including German (De), Spanish (Es), French (Fr), Italian (It), Dutch (Nl), Portuguese (Pt), Romanian (Ro) and Russian (Ru). MuST-C offers a standard training, development\nand test set split for each language pair, with each dataset consisting of English audio, English transcriptions and their translations. Each training set contains transcribed speeches of ∼452 hours with ∼252K utterances on average. We report results on tst-COMMON, whose size ranges from 2502 (Es) to 2641 (De) utterances. We perform our major study on MuST-C En-De.\nTo construct acoustic features, for each audio segment, we extract 40-channel log-Mel filterbanks using overlapping windows of 25 ms and step size of 10 ms. We enrich these features with their first and second-order derivatives, followed by mean subtraction and variance normalization. Following Zhang et al. (2020a), we perform nonoverlapping feature stacking to combine the features of three consecutive frames. All the texts are tokenized and truecased (Koehn et al., 2007), with out-of-vocabulary words handled by BPE segmentation (Sennrich et al., 2016), using 16K merging operations.\nModel Settings and Evaluation Our contextaware ST follows Transformer base (Vaswani et al., 2017): 6 layers, 8 attention heads, and hidden/feedforward size 512/2048. We use Adam (β1 = 0.9, β2 = 0.98) (Kingma and Ba, 2015) for parameter updates with label smoothing of 0.1. We use the same learning rate schedule as Vaswani et al. (2017) and set the warmup step to 4K. We apply dropout to attention weights and residual connections with a rate of 0.2 and 0.5, respectively. By default, we setC = 2 and λ = 0.5. Following (Zhang et al., 2020a), we apply AFS( = −0.1, β = 2/3) to both temporal and feature dimensions for feature selection, which prunes out ∼84% speech encodings. We initialize our context-aware ST with the sentence-level Baseline, i.e. ST+AFS, and then finetune the model for 20K steps based on the concatenation method with a batch size of around 40K subwords.4 We adopt beam search for decoding, with a beam size of 4 and length penalty of 0.6. We average the last 5 checkpoints for evaluation.\nWe measure general translation quality with tokenized case-sensitive BLEU (Papineni et al., 2002) and also report the detokenized one via sacreBLEU (Post, 2018)5 for cross-paper comparison. We calculate BLEU based on sentences unless oth-\n4Our experiments show that such initialization eases the learning of long inputs and improves the convergence of context-aware ST.\n5signature: BLEU+c.mixed+#.1+s.exp+tok.13a+v.1.3.6\nerwise specified. We use APT (Miculicich Werlen and Popescu-Belis, 2017), the accuracy of pronoun translation, as an approximate proxy for documentlevel evaluation. Word alignment required by APT is automatically extracted via fast align (Dyer et al., 2013) with the strategy “grow-diag-final-and”."
    }, {
      "heading" : "5.2 Results on MuST-C En-De",
      "text" : "Does context improve translation? Yes, but the decoding method matters for context-aware ST. Table 1 summarizes the results. Our model with IMED outperforms Baseline by +0.48 BLEU (significant at p < 0.05)6 and +1.79 APT (1→5), clearly showing the benefits from contextual modeling. Although SWBD-Cons yields worse sentencebased BLEU (-0.27, 1→4), it still beats Baseline in document-based BLEU (+0.58) and pronoun translation (+0.17 APT). The reason behind this inferior BLEU partially lies in misaligned translation (see Table 8 in Appendix for example). We observe that SWBD-Cons sometimes segments its output in a way that is misaligned to the reference segmentation. This also hurts CBD, where CBD produces mismatched sentences for around 1.8% cases. This is only a problem if we rely on the sentence-level alignment for BLEU, but not when we measure document-based BLEU (in brackets), where translations in one document are concatenated into a sequence for BLEU calculation. Overall, SWBD\n6We perform significance test using bootstrap-hypothesisdifference-significance.pl in moses (Koehn et al., 2007).\nand IMED are more stable and perform the best, and SWBD surpasses Baseline by 2.06 APT (1→3). We will proceed with using IMED and SWBD for more reliable results with APT and later analysis.\nSince we finetune our model based on the pretrained Baseline, directly comparing with Baseline might be unfair. To offset its influence, we continue to train Baseline for the same 20K steps, following the settings in Section 5.1. Results show that this extra training (1→6) slightly deteriorates BLEU (-0.36) and only explains part of the improvement in APT (+0.81). Therefore, the gain brought by SWBD and IMED does not come from longer training. However, we do observe that initializing from the sentence-level Baseline benefits context-aware ST, compared to directly training context-aware ST from the AFS model (13→3, 14→4).\nApart from faster convergence and higher quality, another benefit of this finetuning is that the trained context-aware ST still carries the ability to translate individual sentences. Table 1 shows that using context-aware ST for sentence-level translation (1→7) yields similar BLEU to Baseline (+0.04) but surprisingly much better pronoun translation (+1.19), although it still underperforms SWBD and IMED. The fact that we can perform sentence-level ST using the same context-aware ST model indicates that it can be useful for ensembling, as confirmed by the effectiveness of IMED.\nUpon closer inspection, we find that contextaware ST prefers to produce longer translations than Baseline. To control for the effects of output length on BLEU differences, we experiment with larger length penalty (lp: 0.6→1.0) to beam search. Results in Table 1 show that biasing the decoding greatly improves sentence-level ST (1→8), achieving performance on par with context-aware ST (when lp is 0.6) in terms of BLEU with similar translation lengths but still falling short of pronoun translation (-0.94 APT, 8→3). In addition, we observe that context-aware ST also benefits from decoding with larger length penalty, beating all sentence-level ST models (3→9, 5→10). Particularly, SWBD with lp of 1.0 delivers the best BLEU of 22.97 and APT of 63.51 (3→9). Note we adopt lp of 0.6 for the following experiments.\nDoes target-side context matter for contextaware ST? Yes, it matters a lot. By default, we utilize both source- and target-side context for contextual modeling. Removing the target-side part (also at training), as shown in Table 1 (11, 12), sub-\nstantially weakens translation quality, even leading to worse performance than Baseline. Apart from offering direct target-side translation clues, we argue that the target-side context also enforces the context-aware ST to utilize the source-side context for translation, thus benefiting its training. This observation echoes with several previous studies on textual translation (Bawden et al., 2018; Huo et al., 2020; Lopes et al., 2020).\nDoes the model learn to utilize context? Yes. We answer this question by studying the impact of incorrect context on our model. We replace the correct source context with some random audio segments from the same document, and randomly select the target context from previous translations during decoding. Intuitively, the performance of our model should be intact if it ignores the context. Note that we trained our model with correct contexts but test it with random contexts here.\nResults in Table 2 show that the randomized context, either source- or target-side, hurts the performance of our model in both BLEU and APT, similar to the findings in (Voita et al., 2018), and the translation of pronouns suffers more (> -1.6 APT). Compared to SWBD, the incorrect context has more negative impact on IMED, resulting in worse performance than Baseline (Table 1), although IMED also uses sentence-level translation. We ascribe this to the target prefix constraint in IMED which makes translation errors at early decoding much easier to propagate. We observe that the incorrect target context acts similarly to its source counterpart under IMED, albeit its selection scope is much smaller (only limited to the translated segments), and combining both contexts leads to a slight but consistent performance degradation. These results demonstrate that our model indeed learns to use contextual information for translation.\nHow much context sentences should we use? Although adding extra context provides more information, it makes learning harder: neural models often struggle with long sequences. Figure 3 shows the impact of context size on translation. We find that our models do not benefit from context size beyond 2 previous segments. Figure 3 also shows that the overall trend of the impact of C on BLEU and APT is similar for different decoding methods. Increasing C to 1 delivers the best APT, while context-aware ST achieves its best BLEU at C = 2. We use C = 2 for the following experiments.\nImpact of λ on IMED. IMED heavily relies on the hyperparameter λ (Eq. 2) to control its preference between sentence-level and document-level decoding. Figure 4 shows its impact on translation\nquality, which clearly reveals a trade-off. The performance of IMED (BLEU and APT) reaches its peak at λ = 0.4, and decreases when λ becomes either smaller or larger. The optimal value of λ for IMED might vary greatly across different language pairs. It also shows some difference across evaluation sets (see Figure 7 in Appendix). In the following experiments, we will apply equal weighting (λ = 0.5), a common choice for model ensembles and not substantially worse than the optimum on this dataset.\nImpact of context on homophone translation. Homophones (words that sound the same but hold different meanings, such as “I” vs. “eye” and “would” vs. “wood”) and other acoustically similar words increase the learning difficulty of ST models compared to textual MT. To allow for automatic quantitative evaluation, we extract words from the MuST-C test set transcriptions which share the same phonemes with Montreal Forced Aligner (McAuliffe et al., 2017). We collect all homophones and evaluate their translation accuracy (ACChp) in the same way as APT.\nTable 3 shows that context-aware ST outperforms Baseline by > 0.73 ACChp, where SWBD performs slightly better than IMED. After removing the document-level decoding, IMED (λ = 1.0) performance drops greatly, even underperforming Baseline. While we see some improvements to homophone translations, they are in the same relative range as general improvements from context. Anecdotal examples from manual inspection (see Table 7 in Appendix) indicate that context may at times help disambiguate acoustically similar forms, but that (near-)homophones still remain a salient source of translation errors.\nContext improves the robustness of ST models to audio segmentation errors. In MuST-C, the audio is already well-segmented, with each segment corresponding to a short transcript. Nevertheless, natural audio, streaming speeches in particular, has no such segment boundaries, and how to parti-\ntion audio itself is an active research area (Rangarajan Sridhar et al., 2013; Zhang and Zhang, 2020). Since ST models are often trained with gold segments, they inevitably suffer from segmentation errors at inference when the gold ones are unavailable.\nThe bottleneck mainly comes from the incompleteness of each segment, which, we argue, contextual information could alleviate. We simulate segmentation errors by randomly re-segmenting the audio in MuST-C En-De test set based on the given segment number. Especially, given an audio with N gold segments, we randomly re-segment it into N disjoint pieces, where each piece usually has different boundaries against its gold counterpart.7 We evaluate different ST models with document-based BLEU.\nTable 4 summarizes the results. Segmentation noise deteriorates translation quality for all ST models to a large degree (> -6 BLEU). Compared to sentence-level ST, context-aware ST is less sensitive to those errors. In particular, our model with IMED yields a document-based BLEU of 22.03, substantially outperforming Baseline (by 1.63 BLEU). Our results also confirm the findings of Gaido et al. (2020).\nContext benefits simultaneous translation. Simultaneous translation requires that we start decoding before receiving the whole audio input to minimize latency; operating on such short units increases ambiguity, and the model may be forced to predict future input to account for word order differences, which we hypothesize is easier with access to super-sentential context. We focus on segment-\n7Note we intentionally keep the same segment number, N , in the simulated noisy segmentation, because this offers us a fair setup to analyze the impact of segmentation errors on the final translation when compared to the gold segmentation. This avoids the potential influence resulting from mismatched segment number. We leave the study of the model’s robustness to genuine segmentation noises to future work.\nlevel E2E simultaneous translation, and adopt the re-translation method (Niehues et al., 2016; Arivazhagan et al., 2020b,a) where we translate the source input segment from scratch after every 1 second. For training, we finetune each model for extra 20K steps with a 1:1 mix of full-segment and prefix pairs, following Arivazhagan et al. (2020a). We construct the prefix pairs by uniformly selecting an audio prefix length and then proportionally deciding the target prefix length based on the sentence length. Note that the context inputs in our model are still full segments/sentences. We adopt tokenized BLEU, differentiable average lagging (DAL), and normalized erasure (NE) to evaluate the translation quality, latency and stability, respectively, following Arivazhagan et al. (2020a). Note DAL and NE are measured based on words.\nResults in Table 6 show that context-aware ST improves translation quality (> +0.84 BLEU) and reduces translation latency (> -0.06 DAL) regardless of the decoding method. It also enhances translation stability when the target prefix constraint is applied (> -0.08 NE, SWBD-Cons & IMED). SWBD performs worse in NE, because it allows changes in the translation of context which increases instability. Overall, context provides extra information to the translation model, before the\nE2E ST models see the whole input, which benefits simultaneous translation.\nFigure 5 further illustrates how context impacts simultaneous translation. With the increase of sentence-level decoding (λ → 1.0), IMED produces higher DAL and NE, i.e. worse quality. We ascribe the reduction of latency and stability in our model to the inclusion of contextual information."
    }, {
      "heading" : "5.3 Results on Other Language Pairs",
      "text" : "Table 5 summarizes the results for all 8 translation pairs covered by MuST-C. Overall, our model obtains improvements over most metrics and language pairs, despite their different language characteristics. Out of 8 languages, our model performs relatively worse on Es and It with smaller BLEU gains and even negative results in ACChp. By contrast, our model yields the largest improvement on Ro. In particular, our model with IMED achieves a detokenized BLEU of 23.6 on En-Ro, surpassing the state-of-the-art result 22.2 (Zhao et al., 2020) reported so far."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "Our experiments confirm the effectiveness of context-aware modeling for end-to-end speech translation. With concatenation-based contextual modeling and appropriate decoding method, we observe positive impact of context on translation. Context-aware ST improves general translation quality in BLEU, and also helps pronoun and homophone translation. ST models become less sensitive to (artificial) audio segmentation errors with context. In addition, context also improves simultaneous translation by reducing latency and erasure. We observe overall positive results over different languages and evaluation metrics on the MuST-C corpus.\nIn the future, we will investigate more dedicated neural architectures to handle long-form speech input. While we relied on a dataset with sentence segmentation in this work, we are interested in removing the reliance on segmentation at inference time to implement the full-fledged streaming translation scenario."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank the reviewers for their insightful comments. This project has received funding from the European Union’s Horizon 2020 Research and Innovation Programme under Grant Agreements 825460 (ELITR). Rico Sennrich acknowledges support of the Swiss National Science Foundation (MUTAMUR; no. 176727)."
    }, {
      "heading" : "B Case Study on Homophone Translation",
      "text" : "C Examples for Misaligned Translation"
    } ],
    "references" : [ {
      "title" : "Tied multitask learning for neural speech translation",
      "author" : [ "Antonios Anastasopoulos", "David Chiang." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Anastasopoulos and Chiang.,? 2018",
      "shortCiteRegEx" : "Anastasopoulos and Chiang.",
      "year" : 2018
    }, {
      "title" : "Re-translation versus streaming for simultaneous translation",
      "author" : [ "Naveen Arivazhagan", "Colin Cherry", "Wolfgang Macherey", "George Foster." ],
      "venue" : "Proceedings of the 17th International Conference on Spoken Language Translation, pages 220–227,",
      "citeRegEx" : "Arivazhagan et al\\.,? 2020a",
      "shortCiteRegEx" : "Arivazhagan et al\\.",
      "year" : 2020
    }, {
      "title" : "2020b. Re-translation strategies for long form, simultaneous, spoken language translation",
      "author" : [ "Naveen Arivazhagan", "Colin Cherry", "Isabelle Te", "Wolfgang Macherey", "Pallavi Baljekar", "George Foster" ],
      "venue" : null,
      "citeRegEx" : "Arivazhagan et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Arivazhagan et al\\.",
      "year" : 2020
    }, {
      "title" : "Evaluating discourse phenomena in neural machine translation",
      "author" : [ "Rachel Bawden", "Rico Sennrich", "Alexandra Birch", "Barry Haddow." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Bawden et al\\.,? 2018",
      "shortCiteRegEx" : "Bawden et al\\.",
      "year" : 2018
    }, {
      "title" : "A test suite for evaluating discourse phenomena in document-level neural machine translation",
      "author" : [ "Xinyi Cai", "Deyi Xiong." ],
      "venue" : "Proceedings of the Second International Workshop of Discourse Processing, pages 13–17, Suzhou, China. Association for",
      "citeRegEx" : "Cai and Xiong.,? 2020",
      "shortCiteRegEx" : "Cai and Xiong.",
      "year" : 2020
    }, {
      "title" : "Modeling discourse structure for document-level neural machine translation",
      "author" : [ "Junxuan Chen", "Xiang Li", "Jiarui Zhang", "Chulun Zhou", "Jianwei Cui", "Bin Wang", "Jinsong Su." ],
      "venue" : "Proceedings of the First Workshop on Automatic Simultaneous Translation,",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "MuST-C: a Multilingual Speech Translation Corpus",
      "author" : [ "Mattia A. Di Gangi", "Roldano Cattoni", "Luisa Bentivogli", "Matteo Negri", "Marco Turchi." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Gangi et al\\.,? 2019",
      "shortCiteRegEx" : "Gangi et al\\.",
      "year" : 2019
    }, {
      "title" : "Adapting Transformer to End-to-End Spoken Language Translation",
      "author" : [ "Mattia A. Di Gangi", "Matteo Negri", "Marco Turchi." ],
      "venue" : "Proc. Interspeech 2019, pages 1133–1137.",
      "citeRegEx" : "Gangi et al\\.,? 2019",
      "shortCiteRegEx" : "Gangi et al\\.",
      "year" : 2019
    }, {
      "title" : "Sdst: Successive decoding for speech-to-text translation",
      "author" : [ "Qianqian Dong", "Mingxuan Wang", "Hao Zhou", "Shuang Xu", "Bo Xu", "Lei Li." ],
      "venue" : "arXiv preprint arXiv:2009.09737.",
      "citeRegEx" : "Dong et al\\.,? 2020",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2020
    }, {
      "title" : "A simple, fast, and effective reparameterization of IBM model 2",
      "author" : [ "Chris Dyer", "Victor Chahuneau", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Dyer et al\\.,? 2013",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2013
    }, {
      "title" : "Contextualized Translation of Automatically Segmented Speech",
      "author" : [ "Marco Gaido", "Mattia A. Di Gangi", "Matteo Negri", "Mauro Cettolo", "Marco Turchi." ],
      "venue" : "Proc. Interspeech 2020, pages 1471– 1475.",
      "citeRegEx" : "Gaido et al\\.,? 2020",
      "shortCiteRegEx" : "Gaido et al\\.",
      "year" : 2020
    }, {
      "title" : "A pronoun test suite evaluation of the English– German MT systems at WMT 2018",
      "author" : [ "Liane Guillou", "Christian Hardmeier", "Ekaterina Lapshinova-Koltunski", "Sharid Loáiciga." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation:",
      "citeRegEx" : "Guillou et al\\.,? 2018",
      "shortCiteRegEx" : "Guillou et al\\.",
      "year" : 2018
    }, {
      "title" : "Incorporating pronoun function into statistical machine translation",
      "author" : [ "Liane Kirsten Guillou." ],
      "venue" : "Ph.D. thesis, University of Edinburgh.",
      "citeRegEx" : "Guillou.,? 2016",
      "shortCiteRegEx" : "Guillou.",
      "year" : 2016
    }, {
      "title" : "Document-wide decoding for phrasebased statistical machine translation",
      "author" : [ "Christian Hardmeier", "Joakim Nivre", "Jörg Tiedemann." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Com-",
      "citeRegEx" : "Hardmeier et al\\.,? 2012",
      "shortCiteRegEx" : "Hardmeier et al\\.",
      "year" : 2012
    }, {
      "title" : "Diving deep into context-aware neural machine translation",
      "author" : [ "Jingjing Huo", "Christian Herold", "Yingbo Gao", "Leonard Dahlmann", "Shahram Khadivi", "Hermann Ney." ],
      "venue" : "Proceedings of the Fifth Conference on Machine Translation, pages 604–616, On-",
      "citeRegEx" : "Huo et al\\.,? 2020",
      "shortCiteRegEx" : "Huo et al\\.",
      "year" : 2020
    }, {
      "title" : "Direct segmentation models for streaming speech translation",
      "author" : [ "Javier Iranzo-Sánchez", "Adrià Giménez Pastor", "Joan Albert Silvestre-Cerdà", "Pau Baquero-Arnal", "Jorge Civera Saiz", "Alfons Juan." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical",
      "citeRegEx" : "Iranzo.Sánchez et al\\.,? 2020",
      "shortCiteRegEx" : "Iranzo.Sánchez et al\\.",
      "year" : 2020
    }, {
      "title" : "Dynamic context selection for document-level neural machine translation via reinforcement learning",
      "author" : [ "Xiaomian Kang", "Yang Zhao", "Jiajun Zhang", "Chengqing Zong." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Kang et al\\.,? 2020",
      "shortCiteRegEx" : "Kang et al\\.",
      "year" : 2020
    }, {
      "title" : "When and why is document-level context useful in neural machine translation",
      "author" : [ "Yunsu Kim", "Duc Thanh Tran", "Hermann Ney" ],
      "venue" : "In Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT",
      "citeRegEx" : "Kim et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Moses: Open source toolkit for statistical machine translation",
      "author" : [ "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ondřej Bojar", "Alexandra Constantin", "Evan Herbst." ],
      "venue" : "Proceedings of the 45th Annual Meeting of the As-",
      "citeRegEx" : "Cowan et al\\.,? 2007",
      "shortCiteRegEx" : "Cowan et al\\.",
      "year" : 2007
    }, {
      "title" : "Modeling coherence for neural machine translation with dynamic and topic caches",
      "author" : [ "Shaohui Kuang", "Deyi Xiong", "Weihua Luo", "Guodong Zhou." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages",
      "citeRegEx" : "Kuang et al\\.,? 2018",
      "shortCiteRegEx" : "Kuang et al\\.",
      "year" : 2018
    }, {
      "title" : "Has machine translation achieved human parity? a case for document-level evaluation",
      "author" : [ "Samuel Läubli", "Rico Sennrich", "Martin Volk." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4791–4796,",
      "citeRegEx" : "Läubli et al\\.,? 2018",
      "shortCiteRegEx" : "Läubli et al\\.",
      "year" : 2018
    }, {
      "title" : "Document-level neural MT: A systematic comparison",
      "author" : [ "António Lopes", "M. Amin Farajian", "Rachel Bawden", "Michael Zhang", "André F.T. Martins." ],
      "venue" : "Proceedings of the 22nd Annual Conference of the European Association for Machine Transla-",
      "citeRegEx" : "Lopes et al\\.,? 2020",
      "shortCiteRegEx" : "Lopes et al\\.",
      "year" : 2020
    }, {
      "title" : "A simple and effective unified encoder for document-level machine translation",
      "author" : [ "Shuming Ma", "Dongdong Zhang", "Ming Zhou." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3505–3511, On-",
      "citeRegEx" : "Ma et al\\.,? 2020a",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2020
    }, {
      "title" : "Streaming simultaneous speech translation with augmented memory transformer",
      "author" : [ "Xutai Ma", "Yongqiang Wang", "Mohammad Javad Dousti", "Philipp Koehn", "Juan Pino." ],
      "venue" : "arXiv preprint arXiv:2011.00033.",
      "citeRegEx" : "Ma et al\\.,? 2020b",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving speech translation with automatic boundary prediction",
      "author" : [ "Evgeny Matusov", "Dustin Hillard", "Mathew MagimaiDoss", "Dilek Hakkani-Tür", "Mari Ostendorf", "Hermann Ney." ],
      "venue" : "Eighth Annual Conference of the International Speech Communica-",
      "citeRegEx" : "Matusov et al\\.,? 2007",
      "shortCiteRegEx" : "Matusov et al\\.",
      "year" : 2007
    }, {
      "title" : "Montreal forced aligner: Trainable textspeech alignment using kaldi",
      "author" : [ "Michael McAuliffe", "Michaela Socolof", "Sarah Mihuc", "Michael Wagner", "Morgan Sonderegger." ],
      "venue" : "Interspeech, volume 2017, pages 498–502.",
      "citeRegEx" : "McAuliffe et al\\.,? 2017",
      "shortCiteRegEx" : "McAuliffe et al\\.",
      "year" : 2017
    }, {
      "title" : "Document-level neural machine translation with hierarchical attention networks",
      "author" : [ "Lesly Miculicich", "Dhananjay Ram", "Nikolaos Pappas", "James Henderson." ],
      "venue" : "Proceedings of the 2018 Conference",
      "citeRegEx" : "Miculicich et al\\.,? 2018",
      "shortCiteRegEx" : "Miculicich et al\\.",
      "year" : 2018
    }, {
      "title" : "Validation of an automatic metric for the accuracy of pronoun translation (APT)",
      "author" : [ "Lesly Miculicich Werlen", "Andrei Popescu-Belis." ],
      "venue" : "Proceedings of the Third Workshop on Discourse in Machine Translation, pages 17–25, Copenhagen, Denmark.",
      "citeRegEx" : "Werlen and Popescu.Belis.,? 2017",
      "shortCiteRegEx" : "Werlen and Popescu.Belis.",
      "year" : 2017
    }, {
      "title" : "Dynamic transcription for low-latency speech translation",
      "author" : [ "Jan Niehues", "Thai Son Nguyen", "Eunah Cho", "Thanh-Le Ha", "Kevin Kilgour", "Markus Müller", "Matthias Sperber", "Sebastian Stüker", "Alex Waibel." ],
      "venue" : "Interspeech 2016, pages 2513–2517.",
      "citeRegEx" : "Niehues et al\\.,? 2016",
      "shortCiteRegEx" : "Niehues et al\\.",
      "year" : 2016
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "A call for clarity in reporting BLEU scores",
      "author" : [ "Matt Post." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186– 191, Belgium, Brussels. Association for Computational Linguistics.",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "Segmentation strategies for streaming speech translation",
      "author" : [ "Vivek Kumar Rangarajan Sridhar", "John Chen", "Srinivas Bangalore", "Andrej Ljolje", "Rathinavelu Chengalvarayan." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the",
      "citeRegEx" : "Sridhar et al\\.,? 2013",
      "shortCiteRegEx" : "Sridhar et al\\.",
      "year" : 2013
    }, {
      "title" : "Improving word sense disambiguation in neural machine translation with sense embeddings",
      "author" : [ "Annette Rios", "Laura Mascarell", "Rico Sennrich." ],
      "venue" : "Proceedings of the Second Conference on Machine Translation, pages 11–19, Copenhagen, Denmark.",
      "citeRegEx" : "Rios et al\\.,? 2017",
      "shortCiteRegEx" : "Rios et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation with extended context",
      "author" : [ "Jörg Tiedemann", "Yves Scherrer." ],
      "venue" : "Proceedings of the Third Workshop on Discourse in Machine Translation, pages 82–92, Copenhagen, Denmark. Association for Computational Linguistics.",
      "citeRegEx" : "Tiedemann and Scherrer.,? 2017",
      "shortCiteRegEx" : "Tiedemann and Scherrer.",
      "year" : 2017
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "I. Guyon, U. V. Luxburg, S. Bengio,",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "When a good translation is wrong in context: Context-aware machine translation improves on deixis, ellipsis, and lexical cohesion",
      "author" : [ "Elena Voita", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for",
      "citeRegEx" : "Voita et al\\.,? 2019",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    }, {
      "title" : "Context-aware neural machine translation learns anaphora resolution",
      "author" : [ "Elena Voita", "Pavel Serdyukov", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Voita et al\\.,? 2018",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2018
    }, {
      "title" : "Curriculum pre-training for end-to-end speech translation",
      "author" : [ "Chengyi Wang", "Yu Wu", "Shujie Liu", "Ming Zhou", "Zhenglu Yang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3728–3738, Online. As-",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "A topic-based coherence model for statistical machine translation",
      "author" : [ "Deyi Xiong", "Min Zhang." ],
      "venue" : "Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence, AAAI’13, page 977–983. AAAI Press.",
      "citeRegEx" : "Xiong and Zhang.,? 2013",
      "shortCiteRegEx" : "Xiong and Zhang.",
      "year" : 2013
    }, {
      "title" : "Adaptive feature selection for end-toend speech translation",
      "author" : [ "Biao Zhang", "Ivan Titov", "Barry Haddow", "Rico Sennrich." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2533–2544, Online. Association for Compu-",
      "citeRegEx" : "Zhang et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "On sparsifying encoder outputs in sequence-tosequence models",
      "author" : [ "Biao Zhang", "Ivan Titov", "Rico Sennrich." ],
      "venue" : "arXiv preprint arXiv:2004.11854.",
      "citeRegEx" : "Zhang et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving the transformer translation model with document-level context",
      "author" : [ "Jiacheng Zhang", "Huanbo Luan", "Maosong Sun", "Feifei Zhai", "Jingfang Xu", "Min Zhang", "Yang Liu." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Long-short term masking transformer: A simple but effective baseline for document-level neural machine translation",
      "author" : [ "Pei Zhang", "Boxing Chen", "Niyu Ge", "Kai Fan." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Zhang et al\\.,? 2020c",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Dynamic sentence boundary detection for simultaneous translation",
      "author" : [ "Ruiqing Zhang", "Chuanqiang Zhang." ],
      "venue" : "Proceedings of the First Workshop",
      "citeRegEx" : "Zhang and Zhang.,? 2020",
      "shortCiteRegEx" : "Zhang and Zhang.",
      "year" : 2020
    }, {
      "title" : "Neurst: Neural speech translation toolkit",
      "author" : [ "Chengqi Zhao", "Mingxuan Wang", "Lei Li." ],
      "venue" : "arXiv preprint arXiv:2012.10018.",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards making the most of context in neural machine translation",
      "author" : [ "Zaixiang Zheng", "Xiang Yue", "Shujian Huang", "Jiajun Chen", "Alexandra Birch." ],
      "venue" : "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-",
      "citeRegEx" : "Zheng et al\\.,? 2020",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Such clues have been proven effective for textual machine translation (MT), particularly in handling translation errors specific to discourse phenomena, such as inaccurate coreference of pronouns (Guillou, 2016) and mistranslation of ambiguous words (Rios et al.",
      "startOffset" : 196,
      "endOffset" : 211
    }, {
      "referenceID" : 33,
      "context" : "Such clues have been proven effective for textual machine translation (MT), particularly in handling translation errors specific to discourse phenomena, such as inaccurate coreference of pronouns (Guillou, 2016) and mistranslation of ambiguous words (Rios et al., 2017).",
      "startOffset" : 250,
      "endOffset" : 269
    }, {
      "referenceID" : 37,
      "context" : "Besides, ensuring consistency in translation is virtually impossible without document-level context as well (Voita et al., 2019).",
      "startOffset" : 108,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : "Although recent studies on ST have achieved promising results with end-to-end (E2E) models (Anastasopoulos and Chiang, 2018; Di Gangi et al., 2019; Zhang et al., 2020a; Wang et al., 2020; Dong et al., 2020), nevertheless, they mainly focus on sentence-level translation.",
      "startOffset" : 91,
      "endOffset" : 206
    }, {
      "referenceID" : 41,
      "context" : "Although recent studies on ST have achieved promising results with end-to-end (E2E) models (Anastasopoulos and Chiang, 2018; Di Gangi et al., 2019; Zhang et al., 2020a; Wang et al., 2020; Dong et al., 2020), nevertheless, they mainly focus on sentence-level translation.",
      "startOffset" : 91,
      "endOffset" : 206
    }, {
      "referenceID" : 39,
      "context" : "Although recent studies on ST have achieved promising results with end-to-end (E2E) models (Anastasopoulos and Chiang, 2018; Di Gangi et al., 2019; Zhang et al., 2020a; Wang et al., 2020; Dong et al., 2020), nevertheless, they mainly focus on sentence-level translation.",
      "startOffset" : 91,
      "endOffset" : 206
    }, {
      "referenceID" : 8,
      "context" : "Although recent studies on ST have achieved promising results with end-to-end (E2E) models (Anastasopoulos and Chiang, 2018; Di Gangi et al., 2019; Zhang et al., 2020a; Wang et al., 2020; Dong et al., 2020), nevertheless, they mainly focus on sentence-level translation.",
      "startOffset" : 91,
      "endOffset" : 206
    }, {
      "referenceID" : 36,
      "context" : "One practical challenge when scaling up sentence-level E2E ST to the document-level is the encoding of very long audio segments, which can easily hit the computational bottleneck, especially with Transformers (Vaswani et al., 2017).",
      "startOffset" : 209,
      "endOffset" : 231
    }, {
      "referenceID" : 35,
      "context" : "Despite its simplicity, this approach successfully leverages contextual information to improve textual MT (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020), and here we adapt it to ST.",
      "startOffset" : 106,
      "endOffset" : 177
    }, {
      "referenceID" : 3,
      "context" : "Despite its simplicity, this approach successfully leverages contextual information to improve textual MT (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020), and here we adapt it to ST.",
      "startOffset" : 106,
      "endOffset" : 177
    }, {
      "referenceID" : 22,
      "context" : "Despite its simplicity, this approach successfully leverages contextual information to improve textual MT (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020), and here we adapt it to ST.",
      "startOffset" : 106,
      "endOffset" : 177
    }, {
      "referenceID" : 36,
      "context" : "We adopt Transformer (Vaswani et al., 2017) for experiments with the MuST-C dataset (Di Gangi et al.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 27,
      "context" : "61 BLEU) and benefits pronoun translation across different language pairs, resonating with previous findings in textual MT (Miculicich et al., 2018; Huo et al., 2020).",
      "startOffset" : 123,
      "endOffset" : 166
    }, {
      "referenceID" : 14,
      "context" : "61 BLEU) and benefits pronoun translation across different language pairs, resonating with previous findings in textual MT (Miculicich et al., 2018; Huo et al., 2020).",
      "startOffset" : 123,
      "endOffset" : 166
    }, {
      "referenceID" : 1,
      "context" : "• Contextual modeling improves translation quality and reduces latency and flicker for simultaneous translation under re-translation strategy (Arivazhagan et al., 2020a).",
      "startOffset" : 142,
      "endOffset" : 169
    }, {
      "referenceID" : 13,
      "context" : "Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang, 2013).",
      "startOffset" : 134,
      "endOffset" : 181
    }, {
      "referenceID" : 40,
      "context" : "Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang, 2013).",
      "startOffset" : 134,
      "endOffset" : 181
    }, {
      "referenceID" : 35,
      "context" : "Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al.",
      "startOffset" : 89,
      "endOffset" : 267
    }, {
      "referenceID" : 20,
      "context" : "Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al.",
      "startOffset" : 89,
      "endOffset" : 267
    }, {
      "referenceID" : 27,
      "context" : "Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al.",
      "startOffset" : 89,
      "endOffset" : 267
    }, {
      "referenceID" : 16,
      "context" : "Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al.",
      "startOffset" : 89,
      "endOffset" : 267
    }, {
      "referenceID" : 5,
      "context" : "Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al.",
      "startOffset" : 89,
      "endOffset" : 267
    }, {
      "referenceID" : 23,
      "context" : "Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al.",
      "startOffset" : 89,
      "endOffset" : 267
    }, {
      "referenceID" : 47,
      "context" : "Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al.",
      "startOffset" : 89,
      "endOffset" : 267
    }, {
      "referenceID" : 35,
      "context" : "We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST.",
      "startOffset" : 81,
      "endOffset" : 152
    }, {
      "referenceID" : 3,
      "context" : "We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST.",
      "startOffset" : 81,
      "endOffset" : 152
    }, {
      "referenceID" : 22,
      "context" : "We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST.",
      "startOffset" : 81,
      "endOffset" : 152
    }, {
      "referenceID" : 25,
      "context" : "Instead of improving contextual modeling, many studies on streaming ST aim at developing better sentence/word segmentation policies to avoid segmentation errors that greatly hurt translation (Matusov et al., 2007; Rangarajan Sridhar et al., 2013; IranzoSánchez et al., 2020; Zhang and Zhang, 2020; Arivazhagan et al., 2020b).",
      "startOffset" : 191,
      "endOffset" : 324
    }, {
      "referenceID" : 45,
      "context" : "Instead of improving contextual modeling, many studies on streaming ST aim at developing better sentence/word segmentation policies to avoid segmentation errors that greatly hurt translation (Matusov et al., 2007; Rangarajan Sridhar et al., 2013; IranzoSánchez et al., 2020; Zhang and Zhang, 2020; Arivazhagan et al., 2020b).",
      "startOffset" : 191,
      "endOffset" : 324
    }, {
      "referenceID" : 10,
      "context" : "The most related work to ours is (Gaido et al., 2020), which also investigated contextualized translation and showed that contextaware ST is less sensitive to audio segmentation errors.",
      "startOffset" : 33,
      "endOffset" : 53
    }, {
      "referenceID" : 41,
      "context" : "the speech encodings extracted via AFS (Zhang et al., 2020a).",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 42,
      "context" : "(2020a) and perform AFS to only extract those informative encodings (∼16%) optimized via sentence-level speech recognition with L0DROP (Zhang et al., 2020b).",
      "startOffset" : 135,
      "endOffset" : 156
    }, {
      "referenceID" : 35,
      "context" : "Concatenation-based Contextual Modeling We adopt the concatenation method to incorporate the previous context (Cn x /Cn y ) (Tiedemann and Scherrer, 2017; Bawden et al., 2018) as shown in Figure 1.",
      "startOffset" : 124,
      "endOffset" : 175
    }, {
      "referenceID" : 3,
      "context" : "Concatenation-based Contextual Modeling We adopt the concatenation method to incorporate the previous context (Cn x /Cn y ) (Tiedemann and Scherrer, 2017; Bawden et al., 2018) as shown in Figure 1.",
      "startOffset" : 124,
      "endOffset" : 175
    }, {
      "referenceID" : 36,
      "context" : "use arbitrary encoder-decoder models for contextaware ST, such as the Transformer (Vaswani et al., 2017) used in this paper.",
      "startOffset" : 82,
      "endOffset" : 104
    }, {
      "referenceID" : 27,
      "context" : "Despite no dedicated hierarchical modeling (Miculicich et al., 2018), this paradigm still allows for intra- and inter-sentence",
      "startOffset" : 43,
      "endOffset" : 68
    }, {
      "referenceID" : 22,
      "context" : "attention during encoding and decoding, which explicitly utilizes context for translation and has been proven successful (Lopes et al., 2020).",
      "startOffset" : 121,
      "endOffset" : 141
    }, {
      "referenceID" : 34,
      "context" : "out-of-vocabulary words handled by BPE segmentation (Sennrich et al., 2016), using 16K merging operations.",
      "startOffset" : 52,
      "endOffset" : 75
    }, {
      "referenceID" : 36,
      "context" : "Model Settings and Evaluation Our contextaware ST follows Transformer base (Vaswani et al., 2017): 6 layers, 8 attention heads, and hidden/feedforward size 512/2048.",
      "startOffset" : 75,
      "endOffset" : 97
    }, {
      "referenceID" : 18,
      "context" : "98) (Kingma and Ba, 2015) for parameter updates with label smoothing of 0.",
      "startOffset" : 4,
      "endOffset" : 25
    }, {
      "referenceID" : 30,
      "context" : "We measure general translation quality with tokenized case-sensitive BLEU (Papineni et al., 2002) and also report the detokenized one via sacreBLEU (Post, 2018)5 for cross-paper comparison.",
      "startOffset" : 74,
      "endOffset" : 97
    }, {
      "referenceID" : 31,
      "context" : ", 2002) and also report the detokenized one via sacreBLEU (Post, 2018)5 for cross-paper comparison.",
      "startOffset" : 58,
      "endOffset" : 70
    }, {
      "referenceID" : 9,
      "context" : "Word alignment required by APT is automatically extracted via fast align (Dyer et al., 2013) with the strategy “grow-diag-final-and”.",
      "startOffset" : 73,
      "endOffset" : 92
    }, {
      "referenceID" : 38,
      "context" : "Results in Table 2 show that the randomized context, either source- or target-side, hurts the performance of our model in both BLEU and APT, similar to the findings in (Voita et al., 2018), and the translation of pronouns suffers more (> -1.",
      "startOffset" : 168,
      "endOffset" : 188
    }, {
      "referenceID" : 26,
      "context" : "To allow for automatic quantitative evaluation, we extract words from the MuST-C test set transcriptions which share the same phonemes with Montreal Forced Aligner (McAuliffe et al., 2017).",
      "startOffset" : 164,
      "endOffset" : 188
    }, {
      "referenceID" : 45,
      "context" : "tion audio itself is an active research area (Rangarajan Sridhar et al., 2013; Zhang and Zhang, 2020).",
      "startOffset" : 45,
      "endOffset" : 101
    } ],
    "year" : 2021,
    "abstractText" : "Document-level contextual information has shown benefits to text-based machine translation, but whether and how context helps endto-end (E2E) speech translation (ST) is still under-studied. We fill this gap through extensive experiments using a simple concatenationbased context-aware ST model, paired with adaptive feature selection on speech encodings for computational efficiency. We investigate several decoding approaches, and introduce inmodel ensemble decoding which jointly performs documentand sentence-level translation using the same model. Our results on the MuST-C benchmark with Transformer demonstrate the effectiveness of context to E2E ST. Compared to sentence-level ST, context-aware ST obtains better translation quality (+0.182.61 BLEU), improves pronoun and homophone translation, shows better robustness to (artificial) audio segmentation errors, and reduces latency and flicker to deliver higher quality for simultaneous translation.1",
    "creator" : "LaTeX with hyperref"
  }
}