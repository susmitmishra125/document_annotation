{
  "name" : "2021.acl-long.350.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Syntax-augmented Multilingual BERT for Cross-lingual Transfer",
    "authors" : [ "Wasi Uddin Ahmad", "Haoran Li", "Kai-Wei Chang", "Yashar Mehdad" ],
    "emails" : [ "wasiahmad@cs.ucla.edu,", "kwchang@cs.ucla.edu,", "aimeeli@fb.com", "mehdad@fb.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4538–4554\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4538"
    }, {
      "heading" : "1 Introduction",
      "text" : "Cross-lingual transfer reduces the requirement of labeled data to perform natural language processing (NLP) in a target language, and thus has the ability to avail NLP applications in low-resource languages. However, transferring across languages is challenging because of linguistic differences at levels of morphology, syntax, and semantics. For example, word order difference is one of the crucial factors that impact cross-lingual transfer (Ahmad et al., 2019). The two sentences in English and Hindi, as shown in Figure 1 have the same\n∗Work done during internship at Facebook AI.\nmeaning but a different word order (while English has an SVO (Subject-Verb-Object) order, Hindi follows SOV). However, the sentences have a similar dependency structure, and the constituent words have similar part-of-speech tags. Presumably, language syntax can help to bridge the typological differences across languages.\nIn recent years, we have seen a colossal effort to pre-train Transformer encoder (Vaswani et al., 2017) on large-scale unlabeled text data in one or many languages. Multilingual encoders, such as mBERT (Devlin et al., 2019) or XLM-R (Conneau et al., 2020) map text sequences into a shared multilingual space by jointly pre-training in many languages. This allows us to transfer the multilingual encoders across languages and have found effective for many NLP applications, including text classification (Bowman et al., 2015; Conneau\net al., 2018), question answering (Rajpurkar et al., 2016; Lewis et al., 2020), named entity recognition (Pires et al., 2019; Wu and Dredze, 2019), and more. Since the introduction of mBERT, several works (Wu and Dredze, 2019; Pires et al., 2019; K et al., 2020) attempted to reason their success in cross-lingual transfer. In particular, Wu and Dredze (2019) showed that mBERT captures language syntax that makes it effective for cross-lingual transfer. A few recent works (Hewitt and Manning, 2019; Jawahar et al., 2019; Chi et al., 2020) suggest that BERT learns compositional features; mimicking a tree-like structure that agrees with the Universal Dependencies taxonomy.\nHowever, fine-tuning for the downstream task in a source language may not require mBERT to retain structural features or learn to encode syntax. We argue that encouraging mBERT to learn the correlation between syntax structure and target labels can benefit cross-lingual transfer. To support our argument, we show an example of question answering (QA) in Figure 2. In the example, mBERT predicts incorrect answers given the Spanish language context that can be corrected by exploiting syntactic clues. Utilizing syntax structure can also benefit generalized cross-lingual transfer (Lewis et al., 2020) where the input text sequences belong to different languages. For example, answering an English question based on a Spanish passage or predicting text similarity given the two sentences as shown in Figure 1. In such a setting, syntactic clues may help to align sentences.\nIn this work, we propose to augment mBERT with universal language syntax while fine-tuning on downstream tasks. We use a graph attention\nnetwork (GAT) (Veličković et al., 2018) to learn structured representations of the input sequences that are incorporated into the self-attention mechanism. We adopt an auxiliary objective to train GAT such that it embeds the dependency structure of the input sequence accurately. We perform an evaluation on zero-shot cross-lingual transfer for text classification, question answering, named entity recognition, and task-oriented semantic parsing. Experiment results show that augmenting mBERT with syntax improves cross-lingual transfer, such as in PAWS-X and MLQA, by 1.4 and 1.6 points on average across all the target languages. Syntaxaugmented mBERT achieves remarkable gain in the generalized cross-lingual transfer; in PAWS-X and MLQA, performance is boosted by 3.9 and 3.1 points on average across all language pairs. Furthermore, we discuss challenges and limitations in modeling universal language syntax. We release the code to help future works.1"
    }, {
      "heading" : "2 Syntax-augmented Multilingual BERT",
      "text" : "Multilingual BERT (mBERT) (Devlin et al., 2019) enables cross-lingual learning as it embeds text sequences into a shared multilingual space. mBERT is fine-tuned on downstream tasks, e.g., text classification using monolingual data and then directly employed to perform on the target languages. This refers to zero-shot cross-lingual transfer. Our main idea is to augment mBERT with language syntax for zero-shot cross-lingual transfer. We employ graph attention network (GAT) (Veličković et al., 2018) to learn syntax representations and fuse them into the self-attention mechanism of mBERT.\n1https://github.com/wasiahmad/Syntax-MBERT\nIn this section, we first briefly review the transformer encoder that bases mBERT (§ 2.1), and then describe the graph attention network (GAT) that learns syntax representations from dependency structure of text sequences (§ 2.2). Finally, we describe how language syntax is explicitly incorporated into the transformer encoder (§ 2.3)."
    }, {
      "heading" : "2.1 Transformer Encoder",
      "text" : "Transformer encoder (Vaswani et al., 2017) is composed of an embedding layer and stacked encoder layers. Each encoder layer consists of two sublayers, a multi-head attention layer followed by a fully connected feed-forward layer. We detail the process of encoding an input token sequence (w1, . . . , wn) into a sequence of vector representations H = [h1, . . . , hn] as follows.\nEmbedding Layer is parameterized by two embedding matrices — the token embedding matrix We ∈ RU×dmodel and the position embedding matrix Wp ∈ RU×dmodel (where U is the vocabulary size and dmodel is the encoder output dimension). An input text sequence enters into the model as two sequences: the token sequence (w1, . . . , wn) and the corresponding absolute position sequence (p1, . . . , pn). The output of the embedding layer is a sequence of vectors {xi}ni=1 where xi = wiWe+ piWp. The vectors are packed into matrix H0 = [x1, . . . , xn] ∈ Rn×dmodel and fed to an L-layer encoder.\nMulti-head Attention allows to jointly attend to information from different representation subspaces, known as attention heads. Multi-head attention layer composed of h attention heads with the same parameterization structure. At each attention head, the output from the previous layer H l−1 is first linearly projected into queries, keys, and values as follows.\nQ = H l−1WQl ,K = H l−1WKl , V = H l−1W Vl ,\nwhere the parameters WQl ,W K l ∈ Rdmodel×dk and W Vl ∈ Rdmodel×dv are unique per attention head. Then scaled dot-product attention is performed to compute the output vectors {oi}ni=1 ∈ Rn×dv .\nAttention(Q,K, V,M, dk)\n= softmax ( QKT +M√\ndk\n) V,\n(1)\nwhere M ∈ Rn×n is the masking matrix that determines whether a pair of input positions can attend\neach other. In classic multi-head attention, M is a zero matrix (all positions can attend each other).\nThe output vectors from all the attention heads are concatenated and projected into dmodel dimension using the parameter matrixWo ∈ Rhdv×dmodel . Finally the vectors are passed through a feedforward network to output H l ∈ Rn×dmodel ."
    }, {
      "heading" : "2.2 Graph Attention Network",
      "text" : "We embed the syntax structure of the input token sequences using their universal dependency parse. A dependency parse is a directed graph where the nodes represent words, and the edges represent dependencies (the dependency relation between the head and dependent words). We use a graph attention network (GAT) (Veličković et al., 2018) to embed the dependency tree structure of the input sequence. We illustrate GAT in Figure 3.\nGiven the input sequence, the words (wi) and their part-of-speech tags (posi) are embedded into vectors using two parameter matrices: the token embedding matrix We and the part-of-tag embedding Wpos. The input sequence is then encoded into an input matrix G0 = [g1, . . . , gn], where gi = wiWe + posiWpos ∈ Rdmodel . Note that token embedding matrix We is shared between GAT and the Transformer encoder. Then G0 is fed into an LG-layer GAT where each layer generates word representations by attending their adjacent words.\nGAT uses the multi-head attention mechanism and perform a dependency-aware self-attention as\nO = Attention(T , T , V,M, dg) (2) namely setting the query and key matrices to be the same T ∈ Rn×dg respectively and the mask M by\nMij = { 0, Dij ≤ δ −∞, otherwise\n(3)\nwhere D is the distance matrix and Dij indicates the shortest path distance between word i and j in the dependency graph structure.\nTypically in GAT, δ is set to 1; allowing attention between adjacent words only. However, in our study, we find setting δ to [2, 4] helpful for the downstream tasks. Finally, the vector representations from all the attention heads (as in Eq. (2)) are concatenated to form the output representations Gl ∈ Rn×kdg , where k is the number of attention heads employed. The goal of the GAT encoder is to encode the dependency structure into vector representations. Therefore, we design GAT to be light-weight; consisting of much less parameters in comparison to Transformer encoder. Note that, GAT does not employ positional representations and only consists of multi-head attention; there is no feed-forward sublayer and residual connections.\nDependency Tree over Wordpieces and Special Symbols mBERT tokenizes the input sequence into subword units, also known as wordpieces. Therefore, we modify the dependency structure of linguistic tokens to accommodate wordpieces. We introduce additional dependencies between the first subword (head) and the rest of the subwords (dependents) of a linguistic token. More specifically, we introduce new edges from the head subword to the dependent subwords. The inputs to mBERT use special symbols: [CLS] and [SEP]. We add an edge from the [CLS] token to the root of the dependency tree and the [SEP] tokens."
    }, {
      "heading" : "2.3 Syntax-augmented Transformer Encoder",
      "text" : "We want the Transformer encoder to consider syntax structure while performing the self-attention between input sequence elements. We use the syntax representations produced by GAT (outputs from the last layer, denoting as G) to bias the self-attention. O = Attention(Q+ GGQl ,K + GG K l , V,M, dk),\nwhere GQl , G K l ∈ R dkdg×dk are new parameters that learn representations to bias the self-attention.\nWe consider the addition terms (GGQl ,GG K l ) as syntax-bias that provide syntactic clues to guide the self-attention. The high-level intuition behind the syntax bias is to attend tokens with a specific part-of-speech tag sequence or dependencies.2\nSyntax-heads mBERT employs h (=12) attention heads and the syntax representations can be infused into one or more of these heads, and we refer them as syntax-heads. In our experiments, we observed that instilling structural information into many attention heads degenerates the performance. For the downstream tasks, we consider one or two syntax-heads that gives the best performance.3\nSyntax-layers refers to the encoder layers that are infused by syntax representations from GAT. mBERT has a 12-layer encoder and our study finds considering all of the layers as syntax-layers beneficial for cross-lingual transfer."
    }, {
      "heading" : "2.4 Fine-tuning",
      "text" : "We jointly fine-tune mBERT and GAT on downstream tasks in the source language (English in this work) following the standard procedure. However, the task-specific training may not guide GAT to encode the tree structure. Therefore, we adopt an auxiliary objective that supervises GAT to learn representations which can be used to decode the tree structure. More specifically, we use GAT’s output representations G = [g1, . . . , gn] to predict the tree distance between all pairs of words (gi, gj) and the tree depth ||gi|| of each word wi in the input sequence. Following Hewitt and Manning (2019), we apply a linear transformation θ1 ∈ Rm×kdg to compute squared distances as follows.\ndθ1(gi, gj) 2 = (θ1(gi − gj))T (θ1(gi − gj)).\nThe parameter matrix θ1 is learnt by minimizing:\nmin θ1 ∑ s 1 n2 ∑ i,j |dist(wi, wj)2 − dθ(gi, gj)2|,\nwhere s denotes all the text sequences in the training corpus. Similarly, we train another parameter matrix θ2 to compute squared vector norms, dθ2(gi) = (θ2gi) T (θ2gi) that characterize the tree\n2In example shown in Figure 2, token dependencies: [en: root → has → has → members → 315], and [es: root → formada → hay → senadores → 315] or corresponding part-ofspeech tag sequence [VERB → VERB → NOUN → NUM]) may help mBERT to predict the correct answer.\n3This aligns with the findings of Hewitt and Manning (2019) as they showed 64 or 128 dimension of the contextual representations are sufficient to capture the syntax structure.\ndepth of the words. We train GAT’s parameters and θ1, θ2 by minimizing the loss: L = Ltask + α(Ldist + Ldepth), where α is weight for the tree structure prediction loss.\nPre-training GAT Unlike mBERT’s parameters, GAT’s parameters are trained from scratch during task-specific fine-tuning. For low-resource tasks, GAT may not learn to encode the syntax structure accurately. Therefore, we utilize the universal dependency parses (Nivre et al., 2019) to pre-train GAT on the source and target languages. Note that, the pre-training objective for GAT is to predict the tree distances and depths as described above."
    }, {
      "heading" : "3 Experiment Setup",
      "text" : "To study syntax-augmented mBERT’s performance in a broader context, we perform an evaluation on four NLP applications: text classification, named entity recognition, question answering, and taskoriented semantic parsing. Our evaluation focuses on assessing the usefulness of utilizing universal syntax in the zero-shot cross-lingual transfer."
    }, {
      "heading" : "3.1 Evaluation Tasks",
      "text" : "Text Classification We conduct experiments on two widely used cross-lingual text classification tasks: (i) natural language inference and (ii) paraphrase detection. We use the XNLI (Conneau et al., 2018) and PAWS-X (Yang et al., 2019) datasets for the tasks, respectively. In both tasks, a pair of sentences is given as input to mBERT. We combine the dependency tree structure of the two sentences by adding two edges from the [CLS] token to the roots of the dependency trees.\nNamed Entity Recognition is a structure prediction task that requires to identify the named entities mentioned in the input sentence. We use the\nWikiann dataset (Pan et al., 2017) and a subset of two tasks from CoNLL-2002 (Tjong Kim Sang, 2002) and CoNLL-2003 NER (Tjong Kim Sang and De Meulder, 2003). We collect the CoNLL datasets from XGLUE (Liang et al., 2020). In both datasets, there are 4 types of named entities: Person, Location, Organization, and Miscellaneous.4\nQuestion Answering We evaluate on two crosslingual question answering benchmarks, MLQA (Lewis et al., 2020), and XQuAD (Artetxe et al., 2020). We use the SQuAD dataset (Rajpurkar et al., 2016) for training and validation. In the QA task, the inputs are a question and a context passage that consists of many sentences. We formulate QA as a multi-sentence reading comprehension task; jointly train the models to predict the answer sentence and extract the answer span from it. We concatenate the question and each sentence from the context passage and use the [CLS] token representation to score the candidate sentences. We adopt the confidence method from Clark and Gardner (2018) and pick the highest-scored sentence to extract the answer span during inference. We provide more details of the QA models in Appendix.\nTask-oriented Semantic Parsing The fourth evaluation task is cross-lingual task-oriented semantic parsing. In this task, the input is a user utterance and the goal is to predict the intent of the utterance and fill the corresponding slots. We conduct experiments on two recently proposed benchmarks: (i) mTOP (Li et al., 2021) and (ii) mATIS++ (Xu et al., 2020). We jointly train the BERT models as suggested in Chen et al. (2019).\nWe summarize the evaluation task benchmark datasets and evaluation metrics in Table 1.\n4Miscellaneous entity type covers named entities that do not belong to the other three types\nModel en ar bg de el es fr hi ru tr ur vi zh ko ja nl pt AVG Classification - XNLI (Conneau et al., 2018) [1] 80.8 64.3 68.0 70.0 65.3 73.5 73.4 58.9 67.8 60.9 57.2 69.3 67.8 - - - - 67.5 mBERT 81.8 63.8 68.0 70.7 65.4 73.8 72.4 59.3 68.4 60.7 56.7 68.6 67.8 - - - - 67.5 + Syn. 81.6 65.4 69.3 70.7 66.5 74.1 73.2 60.5 68.8 62.4 58.7 69.9 69.3 - - - - 68.5\nClassification - PAWS-X (Yang et al., 2019) [1] 94.0 - - 85.7 - 87.4 87.0 - - - - - 77.0 69.6 73.0 - - 82.0 mBERT 93.9 - - 85.7 - 88.4 87.6 - - - - - 78.0 73.6 73.1 - - 82.9 + Syn. 94.0 - - 85.9 - 89.1 88.2 - - - - - 80.7 76.3 75.8 - - 84.3\nNER - Wikiann (Pan et al., 2017) [1] 85.2 41.1 77.0 78.0 72.5 77.4 79.6 65.0 64.0 71.8 36.9 71.8 - 59.6 - 81.8 80.8 69.5 mBERT 83.6 38.8 77.0 76.0 70.4 74.7 78.9 63.4 63.5 70.9 37.7 73.5 - 59.3 - 81.9 78.7 68.5 + Syn. 84.4 40.0 77.0 77.0 71.5 76.1 79.3 64.2 63.8 71.4 37.3 72.7 - 59.3 - 81.9 79.0 69.0\nNER - CoNLL (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) [2] 90.6 - - 69.2 - 75.4 - - - - - - - - - 77.9 - 78.2 mBERT 90.7 - - 68.3 - 74.5 - - - - - - - - - 77.6 - 77.8 + Syn. 90.6 - - 69.1 - 73.6 - - - - - - - - - 78.5 - 78.0 QA - MLQA (Lewis et al., 2020) [3] 77.7 45.7 - 57.9 - 64.3 - 43.8 - - - 57.1 57.5 - - - - 57.7 mBERT 80.5 47.2 - 59.0 - 63.9 - 47.5 - - - 56.5 56.6 - - - - 58.7 + Syn. 80.4 48.9 - 60.8 - 65.9 - 46.7 - - - 59.3 60.1 - - - - 60.3\nQA - XQuAD (Artetxe et al., 2020) [1] 83.5 61.5 - 70.6 62.6 75.5 - 59.2 71.3 55.4 - 69.5 58.0 - - - - 66.7 mBERT 84.2 54.8 - 68.9 60.2 71.1 - 55.7 68.6 48.9 - 64.0 57.2 - - - - 63.4 + Syn. 84.0 55.5 - 71.4 61.3 72.8 - 54.6 68.4 49.8 - 67.6 56.1 - - - - 64.2\nSemantic Parsing - mTOP (Li et al., 2021) mBERT 81.0 - - 28.1 - 40.2 38.8 9.8 - - - - - - - - - 39.6 + Syn. 81.3 - - 30.0 - 43.0 41.2 11.5 - - - - - - - - - 41.4\nSemantic Parsing - mATIS++ (Xu et al., 2020) mBERT 86.0 - - 38.1 - 43.7 36.9 16.2 - 1.3 - - 7.8 - 28.2 - 38.2 32.9 + Syn. 86.2 - - 40.1 - 44.5 38.9 18.7 - 1.5 - - 8.0 - 27.3 - 37.3 33.6\nTable 2: Cross-lingual transfer results for all the evaluation tasks (on test set) across 17 languages. We report F1 score for the question answering (QA) datasets (for other datasets, see Table 1). We train and evaluate mBERT on the same pre-processed datasets and considers its performance as the baseline (denoted by “mBERT” rows in the table) for syntax-augmented mBERT (denoted by “+ Syn.” rows in the table). Bold-faced values indicate that the syntax-augmented mBERT is statistically significantly better (by paired bootstrap test, p < 0.05) than the baseline. We include results from published works ([1]: Hu et al. (2020), [2]: Liang et al. (2020), and [3]: Lewis et al. (2020)) as a reference. Except for the QA datasets, all our results are averaged over three different seeds."
    }, {
      "heading" : "3.2 Implementation Details",
      "text" : "We collect the universal part-of-speech tags and the dependency parse of sentences by pre-processing the datasets using UDPipe.5 We fine-tune mBERT on the pre-processed datasets and consider it as the baseline for our proposed syntax-augmented mBERT. We extend the XTREME framework (Hu et al., 2020) that is developed based on transformers API (Wolf et al., 2020). We use the same hyper-parameter setting for mBERT models, as suggested in XTREME. For the graph at-\n5https://ufal.mff.cuni.cz/udpipe/2\ntention network (GAT), we set LG = 4, k = 4, and dg = 64 (resulting in ∼0.5 million parameters). We tune δ6 (shown in Eq. (3)) and α (weight of the tree structure prediction loss) in the range [1, 2, 4, 8] and [0.5− 1.0], respectively. We detail the hyper-parameters in the Appendix.\n6We observed that the value of δ depends on the downstream task and the source language. For example, a larger δ value is beneficial for tasks taking a pair of text sequences as inputs, while a smaller δ value results in better performances for tasks taking single text input. Experiments on PAWS-X using each target language as the source language indicate that δ should be set to a larger value for source language with longer text sequences (e.g., Arabic) and vice versa."
    }, {
      "heading" : "4 Experiment Results",
      "text" : "We aim to address the following questions. 1. Does augmenting mBERT with syntax im-\nprove (generalized) cross-lingual transfer? 2. Does incorporating syntax benefit specific lan-\nguages or language families? 3. Which NLP tasks or types of tasks get more\nbenefits from utilizing syntax?"
    }, {
      "heading" : "4.1 Cross-lingual Transfer",
      "text" : "Experiment results to compare mBERT and syntaxaugmented mBERT are presented in Table 2. Overall, the incorporation of language syntax in mBERT improves cross-lingual transfer for the downstream tasks, in many languages by a significant margin (p < 0.05, t-test). The average performances across all languages on XNLI, PAWS-X, MLQA, and mTOP benchmarks improve significantly (by at least 1 point). On the other benchmarks: Wikiann, CoNLL, XQuAD, and mATIS++, the average performance improvements are 0.5, 0.2, 0.8, and 0.7 points, respectively. Note that the performance gains in the source language (English) for all the datasets except Wikiann is ≤ 0.3. This indicates that cross-lingual transfer gains are not due to improving the downstream tasks, but instead, language syntax helps to transfer across languages."
    }, {
      "heading" : "4.2 Generalized Cross-lingual Transfer",
      "text" : "In the generalized cross-lingual transfer setting (Lewis et al., 2020), the input text sequences for the downstream tasks (e.g., text classification, QA) may come from different languages. As shown in Figure 2, given the context passage in English, a multilingual QA model should answer the question written in Spanish. Due to the parallel nature of\nthe existing benchmark datasets: XNLI, PAWS-X, MLQA, and XQuAD, we evaluate mBERT and its’ syntax-augmented variant on the generalized crosslingual transfer setting. The results for PAWS-X and MLQA are presented in Table 3 (results for the other datasets are provided in Appendix).\nIn both text classification and QA benchmarks, we observe significant improvements for most language pairs. In the PAWS-X text classification task, language pairs with different typologies (e.g., en-ja, en-zh) have the most gains. When Chinese (zh) or Japanese (ja) is in the language pairs, the performance is boosted by at least 4.5%. The dataset characteristics explain this; the task requires modeling structure, context, and word order information. On the other hand, in the XNLI task, the performance gain pattern is scattered, and this is perhaps syntax plays a less significant role in the XNLI task. The largest improvements result when the languages of the premise and hypothesis sentences belong to {Bulgarian, Chinese} and {French, Arabic}.\nIn both QA datasets, syntax-augmented mBERT boosts performance when the question and context languages are typologically different except the Hindi language. Surprisingly, we observe a large performance gain when questions in Spanish and German are answered based on the English context. Based on our manual analysis on MLQA, we suspect that although questions in Spanish and German are translated from English questions (by human), the context passages are from Wikipedia that often are not exact translation of the corresponding English passage. Take the context passages in Figure 2 as an example. We anticipate that syntactic clues help a QA model in identifying the correct answer span when there are more than one semantically equivalent and plausible answer choices."
    }, {
      "heading" : "4.3 Analysis & Discussion",
      "text" : "We discuss and analyze our findings on the following points based on the empirical results.\nImpact on Languages We study if fine-tuning syntax-augmented mBERT on English (source language) impacts specific target languages or families of languages. We show the performance gains on the target languages grouped by their families in four downstream tasks in Figure 4. There is no observable trend in the overall performance improvements across tasks. However, the XNLI curve weakly indicates that when target languages are typologically different from the source language, there is an increase in the transfer performance (comparing left half to the right half of the curve).\nImpact of Pre-training GAT Before fine-tuning syntax-augmented mBERT, we pre-train GAT on the 17 target languages (discussed in § 2.4). In our experiments, we observe such pre-training boosts semantic parsing performance, while there is a little gain on the classification and QA tasks. We also observe that pre-training GAT diminishes the gain of fine-tuning with the auxiliary objective (predicting the tree structure). We hypothesize that pre-training or fine-tuning GAT using auxiliary objective helps when there is limited training data. For example, semantic parsing benchmarks have a small number of training examples, while XNLI has many. As a result, the improvement due to pre-training or fine-tuning GAT in the semantic parsing tasks is significant, and in the XNLI task, it is marginal.\nDiscussion To foster research in this direction, we discuss additional experiment findings.\n• A natural question is, instead of using GAT, why we do not modify attention heads in mBERT to embed the dependency structure (as shown in Eq. 3). We observed a consistent performance drop\nacross all the tasks if we intervene in self-attention (blocking pair-wise attention). We anticipate fusing GAT encoded syntax representations helps as it adds bias to the self-attention. For future works, we suggest exploring ways of adding structure bias, e.g., scaling attention weights based on dependency structure (Bugliarello and Okazaki, 2020).\n• Among the evaluation datasets, Wikiann consists of sentence fragments, and the semantic parsing benchmarks consist of user utterances that are typically short in length. Sorting and analyzing the performance improvements based on sequence lengths suggests that the utilization of dependency structure has limited scope for shorter text sequences. However, part-of-speech tags help to identify span boundaries improving the slot filling tasks."
    }, {
      "heading" : "4.4 Limitations and Challenges",
      "text" : "In this work, we assume we have access to an offthe-shelf universal parser, e.g., UDPipe (Straka and Straková, 2017) or Stanza (Qi et al., 2020) to collect part-of-speech tags and the dependency structure of the input sequences. Relying on such a parser has a limitation that it may not support all the languages available in benchmark datasets, e.g., we do not consider Thai and Swahili languages in the benchmark datasets.\nThere are a couple of challenges in utilizing the universal parsers. First, universal parsers tokenize the input sequence into words and provide partof-speech tags and dependencies for them. The tokenized words may not be a part of the input.7 As a result, tasks requiring extracting text spans (e.g., QA) need additional mapping from input tokens to words. Second, the parser’s output word sequence is tokenized into wordpieces that often results in\n7For example, in the German sentence “Wir gehen zum kino” (we are going to the cinema), the token “zum” is decomposed into words “zu” and “dem”.\ninconsistent wordpieces resulting in degenerated performance in the downstream tasks.8"
    }, {
      "heading" : "5 Related Work",
      "text" : "Encoding Syntax for Language Transfer Universal language syntax, e.g., part-of-speech (POS) tags, dependency parse structure, and relations are shown to be helpful for cross-lingual transfer (Kozhevnikov and Titov, 2013; Pražák and Konopı́k, 2017; Wu et al., 2017; Subburathinam et al., 2019; Liu et al., 2019; Zhang et al., 2019; Xie et al., 2020; Ahmad et al., 2021). Many of these prior works utilized graph neural networks (GNN) to encode the dependency graph structure of the input sequences. In this work, we utilize graph attention networks (GAT) (Veličković et al., 2018), a variant of GNN that employs the multihead attention mechanism.\nSyntax-aware Multi-head Attention A large body of prior works investigated the advantages of incorporating language syntax to enhance the self-attention mechanism (Vaswani et al., 2017). Existing techniques can be broadly divided into two types. The first type of approach relies on an external parser (or human annotation) to get a sentence’s dependency structure during inference. This type of approaches embed the dependency structure into contextual representations (Wu et al., 2017; Chen et al., 2017; Wang et al., 2019a,b; Zhang et al., 2019, 2020; Bugliarello and Okazaki, 2020; Sachan et al., 2021; Ahmad et al., 2021). Our proposed method falls under this category; however, unlike prior works, our study investigates if fusing the universal dependency structure into the self-attention of existing multilingual encoders help cross-lingual transfer. Graph attention networks (GATs) that use multi-head attention has also been adopted for NLP tasks (Huang and Carley, 2019) also fall into this category. The second category of approaches does not require the syntax structure of the input text during inference. These approaches are trained to predict the dependency parse via supervised learning (Strubell et al., 2018; Deguchi et al., 2019)."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work, we propose incorporating universal language syntax into multilingual BERT (mBERT)\n8This happen for languages, such as Arabic as parsers normalize the input that lead to inconsistent characters between input text and the output tokenized text.\nby infusing structured representations into its multihead attention mechanism. We employ a modified graph attention network to encode the syntax structure of the input sequences. The results endorse the effectiveness of our proposed approach in the cross-lingual transfer. We discuss limitations and challenges to drive future works."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Yuqing Tang for his insightful comments on our paper and anonymous reviewers for their helpful feedback. We also thank UCLA-NLP group for helpful discussions and comments.\nBroader Impact\nIn today’s world, the number of speakers for some languages is in billions, while it is only a few thousands for many languages. As a result, a few languages offer large-scale annotated resources, while for many languages, there are limited or no labeled data. Due to this disparity, natural language processing (NLP) is extremely challenging in the lowresourced languages. In recent years, cross-lingual transfer learning has achieved significant improvements, enabling us to avail NLP applications to a wide range of languages that people use across the world. However, one of the challenges in crosslingual transfer is to learn the linguistic similarity and differences between languages and their correlation with the target NLP applications. Modern transferable models are pre-trained on unlabeled humongous corpora such that they can learn language syntax and semantic and encode them into universal representations. Such pre-trained models can benefit from explicit incorporation of universal language syntax during fine-tuning for different downstream applications. This work presents a thorough study to analyze the pros and cons of utilizing Universal Dependencies (UD) framework that consists of grammar annotations across many human languages. Our work can broadly impact the development of cross-lingual transfer solutions and making them accessible to people across the globe. In this work, we discuss the limitations and challenges in utilizing universal parsers to benefit the pre-trained models. Among the negative aspects of our work is the lack of explanation that why some languages get more benefits over others due to universal syntax knowledge incorporation."
    }, {
      "heading" : "A Model Implementations",
      "text" : "We follow the standard way to model text classification, named entity recognition, and task-oriented semantic parsing using mBERT. However, since our proposed model uses the input sentences’ dependency structure, we frame question answering (QA) as multi-sentence reading comprehension. The input context is split into a list of sentences and train the mBERT model to predict the answer sentence and extract the answer span from the selected sentence following Clark and Gardner (2018). We concatenate the question and each sentence from the context passage and use the [CLS] token representation to score the candidate sentences. We adopt the shared-normalization approach from the “confidence method” as suggested in Clark and Gardner (2018) and pick the highest-scored sentence to extract the answer span during inference. Our approach of utilizing syntax can be extended to apply to passages directly. To combine all the sentences’ dependency structure in the passage, we can add edges from the [CLS] token to the roots of all the sentences’ dependency tree. However, would that approach work in practice requires empirical study, and we leave this as future work."
    }, {
      "heading" : "B Hyper-prameter Details",
      "text" : "We present the hyper-parameter details in Table 4."
    }, {
      "heading" : "C Additional Experiment Results",
      "text" : "Cross-lingual Transfer We provide the exact match (EM) and F1 accuracy of the MLQA dataset in Table 5. Intent classification accuracy, slot F1, and exact match (EM) accuracy for the taskoriented semantic parsing is reported in Table zeroshot cross-lingual transfer results for the evaluation tasks in Table 6. We highlight the cross-lingual transfer gap for mBERT and syntax-augmented mBERT on the evaluation tasks in Table 7.\nGeneralized Cross-lingual Transfer In generalized cross-lingual transfer, we assume the task inputs are a pair of text that belong to two different languages, e.g., answering Spanish question based on an English context (Lewis et al., 2020). We present the generalized cross-lingual transfer performance of syntax-augmented mBERT on XNLI, MLQA, and XQuAD in Table 8, 9, 10, and 11, respectively. The performance differences between\nsyntax-augmented mBERT and mBERT on the generalized cross-lingual transfer on XNLI and XQuAD is presented in Table 12 and 13.\nDifferent Source Languages In our study, we primarily use English as the source language as training examples used in all the benchmarks are in English. However, authors of many of these benchmarks released translated-train examples in the target languages. This allows us to train mBERT and syntax-augmented mBERT in different languages (as source) and examine how it impacts cross-lingual transfer. We perform experiments on PAWS-X task and present the results in Figure 5. We observe the largest transfer performance improvements when English and German are used as the source language. The improvements are relatively smaller when Japanese, Korean, and Chinese languages are used as the source language. We suspect that the dependency parser may not accurately parse translated sentences, and as a result, we do not see an explainable trend in the improvements."
    } ],
    "references" : [ {
      "title" : "On difficulties of cross-lingual transfer with order differences: A case study on dependency parsing",
      "author" : [ "Wasi Ahmad", "Zhisong Zhang", "Xuezhe Ma", "Eduard Hovy", "Kai-Wei Chang", "Nanyun Peng." ],
      "venue" : "Proceedings of the 2019 Conference of the North Amer-",
      "citeRegEx" : "Ahmad et al\\.,? 2019",
      "shortCiteRegEx" : "Ahmad et al\\.",
      "year" : 2019
    }, {
      "title" : "GATE: graph attention transformer encoder for cross-lingual relation and event extraction",
      "author" : [ "Wasi Uddin Ahmad", "Nanyun Peng", "Kai-Wei Chang." ],
      "venue" : "Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Ahmad et al\\.,? 2021",
      "shortCiteRegEx" : "Ahmad et al\\.",
      "year" : 2021
    }, {
      "title" : "On the cross-lingual transferability of monolingual representations",
      "author" : [ "Mikel Artetxe", "Sebastian Ruder", "Dani Yogatama." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623–4637, Online. Asso-",
      "citeRegEx" : "Artetxe et al\\.,? 2020",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2020
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Enhancing machine translation with dependency-aware self-attention",
      "author" : [ "Emanuele Bugliarello", "Naoaki Okazaki." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1618–1627, Online. Association for",
      "citeRegEx" : "Bugliarello and Okazaki.,? 2020",
      "shortCiteRegEx" : "Bugliarello and Okazaki.",
      "year" : 2020
    }, {
      "title" : "Improved neural machine translation with a syntax-aware encoder and decoder",
      "author" : [ "Huadong Chen", "Shujian Huang", "David Chiang", "Jiajun Chen." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Bert for joint intent classification and slot filling",
      "author" : [ "Qian Chen", "Zhu Zhuo", "Wen Wang." ],
      "venue" : "arXiv preprint arXiv:1902.10909.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Finding universal grammatical relations in multilingual BERT",
      "author" : [ "Ethan A. Chi", "John Hewitt", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5564–5577, Online. As-",
      "citeRegEx" : "Chi et al\\.,? 2020",
      "shortCiteRegEx" : "Chi et al\\.",
      "year" : 2020
    }, {
      "title" : "Simple and effective multi-paragraph reading comprehension",
      "author" : [ "Christopher Clark", "Matt Gardner." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 845–855, Melbourne,",
      "citeRegEx" : "Clark and Gardner.,? 2018",
      "shortCiteRegEx" : "Clark and Gardner.",
      "year" : 2018
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "XNLI: Evaluating cross-lingual sentence representations",
      "author" : [ "Alexis Conneau", "Ruty Rinott", "Guillaume Lample", "Adina Williams", "Samuel Bowman", "Holger Schwenk", "Veselin Stoyanov." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods",
      "citeRegEx" : "Conneau et al\\.,? 2018",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "Dependency-based self-attention for transformer NMT",
      "author" : [ "Hiroyuki Deguchi", "Akihiro Tamura", "Takashi Ninomiya." ],
      "venue" : "Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019), pages 239–",
      "citeRegEx" : "Deguchi et al\\.,? 2019",
      "shortCiteRegEx" : "Deguchi et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "A structural probe for finding syntax in word representations",
      "author" : [ "John Hewitt", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Hewitt and Manning.,? 2019",
      "shortCiteRegEx" : "Hewitt and Manning.",
      "year" : 2019
    }, {
      "title" : "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalization",
      "author" : [ "Junjie Hu", "Sebastian Ruder", "Aditya Siddhant", "Graham Neubig", "Orhan Firat", "Melvin Johnson." ],
      "venue" : "Proceedings of the 37th Interna-",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "Syntaxaware aspect level sentiment classification with graph attention networks",
      "author" : [ "Binxuan Huang", "Kathleen Carley." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Huang and Carley.,? 2019",
      "shortCiteRegEx" : "Huang and Carley.",
      "year" : 2019
    }, {
      "title" : "What does BERT learn about the structure of language",
      "author" : [ "Ganesh Jawahar", "Benoı̂t Sagot", "Djamé Seddah" ],
      "venue" : "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Jawahar et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Jawahar et al\\.",
      "year" : 2019
    }, {
      "title" : "Cross-lingual ability of multilingual bert: An empirical study",
      "author" : [ "Karthikeyan K", "Zihan Wang", "Stephen Mayhew", "Dan Roth." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "K et al\\.,? 2020",
      "shortCiteRegEx" : "K et al\\.",
      "year" : 2020
    }, {
      "title" : "Crosslingual transfer of semantic role labeling models",
      "author" : [ "Mikhail Kozhevnikov", "Ivan Titov." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1190–1200, Sofia, Bulgaria.",
      "citeRegEx" : "Kozhevnikov and Titov.,? 2013",
      "shortCiteRegEx" : "Kozhevnikov and Titov.",
      "year" : 2013
    }, {
      "title" : "MLQA: Evaluating cross-lingual extractive question answering",
      "author" : [ "Patrick Lewis", "Barlas Oguz", "Ruty Rinott", "Sebastian Riedel", "Holger Schwenk." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7315–",
      "citeRegEx" : "Lewis et al\\.,? 2020",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "MTOP: A comprehensive multilingual task-oriented semantic parsing benchmark",
      "author" : [ "Haoran Li", "Abhinav Arora", "Shuohui Chen", "Anchit Gupta", "Sonal Gupta", "Yashar Mehdad." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "XGLUE: A new benchmark datasetfor cross-lingual pre-training, understanding and generation",
      "author" : [ "Wu", "Shuguang Liu", "Fan Yang", "Daniel Campos", "Rangan Majumder", "Ming Zhou." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural cross-lingual event detection with minimal parallel resources",
      "author" : [ "Jian Liu", "Yubo Chen", "Kang Liu", "Jun Zhao." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Universal dependencies 2.4",
      "author" : [ "Wong", "Alina Wróblewska", "Mary Yako", "Naoki Yamazaki", "Chunxiao Yan", "Koichi Yasuoka", "Marat M. Yavrumyan", "Zhuoran Yu", "Zdeněk Žabokrtský", "Amir Zeldes", "Daniel Zeman", "Manying Zhang", "Hanzhi Zhu" ],
      "venue" : null,
      "citeRegEx" : "Wong et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wong et al\\.",
      "year" : 2019
    }, {
      "title" : "Crosslingual name tagging and linking for 282 languages",
      "author" : [ "Xiaoman Pan", "Boliang Zhang", "Jonathan May", "Joel Nothman", "Kevin Knight", "Heng Ji." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Pan et al\\.,? 2017",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2017
    }, {
      "title" : "How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996– 5001, Florence, Italy",
      "author" : [ "Telmo Pires", "Eva Schlinger", "Dan Garrette." ],
      "venue" : "Association for Computa-",
      "citeRegEx" : "Pires et al\\.,? 2019",
      "shortCiteRegEx" : "Pires et al\\.",
      "year" : 2019
    }, {
      "title" : "Crosslingual SRL based upon Universal Dependencies",
      "author" : [ "Ondřej Pražák", "Miloslav Konopı́k" ],
      "venue" : "In Proceedings of the International Conference Recent Advances in Natural Language Processing,",
      "citeRegEx" : "Pražák and Konopı́k.,? \\Q2017\\E",
      "shortCiteRegEx" : "Pražák and Konopı́k.",
      "year" : 2017
    }, {
      "title" : "Stanza: A python natural language processing toolkit for many human languages",
      "author" : [ "Peng Qi", "Yuhao Zhang", "Yuhui Zhang", "Jason Bolton", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Qi et al\\.,? 2020",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2020
    }, {
      "title" : "SQuAD: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Do syntax trees help pre-trained transformers extract information",
      "author" : [ "Devendra Sachan", "Yuhao Zhang", "Peng Qi", "William L. Hamilton" ],
      "venue" : "In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Sachan et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Sachan et al\\.",
      "year" : 2021
    }, {
      "title" : "Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with UDPipe",
      "author" : [ "Milan Straka", "Jana Straková" ],
      "venue" : "In Proceedings of the CoNLL",
      "citeRegEx" : "Straka and Straková.,? \\Q2017\\E",
      "shortCiteRegEx" : "Straka and Straková.",
      "year" : 2017
    }, {
      "title" : "Linguistically-informed self-attention for semantic role labeling",
      "author" : [ "Emma Strubell", "Patrick Verga", "Daniel Andor", "David Weiss", "Andrew McCallum." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Strubell et al\\.,? 2018",
      "shortCiteRegEx" : "Strubell et al\\.",
      "year" : 2018
    }, {
      "title" : "Cross-lingual structure transfer for relation and event extraction",
      "author" : [ "Ananya Subburathinam", "Di Lu", "Heng Ji", "Jonathan May", "Shih-Fu Chang", "Avirup Sil", "Clare Voss." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Subburathinam et al\\.,? 2019",
      "shortCiteRegEx" : "Subburathinam et al\\.",
      "year" : 2019
    }, {
      "title" : "Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang." ],
      "venue" : "COLING-02: The 6th Conference on Natural Language Learning 2002 (CoNLL-2002).",
      "citeRegEx" : "Sang.,? 2002",
      "shortCiteRegEx" : "Sang.",
      "year" : 2002
    }, {
      "title" : "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "In",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30, pages 5998–6008. Curran Asso-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Graph Attention Networks",
      "author" : [ "Petar Veličković", "Guillem Cucurull", "Arantxa Casanova", "Adriana Romero", "Pietro Liò", "Yoshua Bengio." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Veličković et al\\.,? 2018",
      "shortCiteRegEx" : "Veličković et al\\.",
      "year" : 2018
    }, {
      "title" : "Source dependency-aware transformer with supervised self-attention",
      "author" : [ "Chengyi Wang", "Shuangzhi Wu", "Shujie Liu." ],
      "venue" : "arXiv preprint arXiv:1909.02273.",
      "citeRegEx" : "Wang et al\\.,? 2019a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Self-attention with structural position representations",
      "author" : [ "Xing Wang", "Zhaopeng Tu", "Longyue Wang", "Shuming Shi." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Wang et al\\.,? 2019b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT",
      "author" : [ "Shijie Wu", "Mark Dredze." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Wu and Dredze.,? 2019",
      "shortCiteRegEx" : "Wu and Dredze.",
      "year" : 2019
    }, {
      "title" : "Improved neural machine translation with source syntax",
      "author" : [ "Shuangzhi Wu", "Ming Zhou", "Dongdong Zhang." ],
      "venue" : "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17, pages 4179–4185.",
      "citeRegEx" : "Wu et al\\.,? 2017",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2017
    }, {
      "title" : "A contextual alignment enhanced cross graph attention network for cross-lingual entity alignment",
      "author" : [ "Zhiwen Xie", "Runjie Zhu", "Kunsong Zhao", "Jin Liu", "Guangyou Zhou", "Jimmy Xiangji Huang." ],
      "venue" : "Proceedings of the 28th International Conference",
      "citeRegEx" : "Xie et al\\.,? 2020",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2020
    }, {
      "title" : "End-to-end slot alignment and recognition for crosslingual NLU",
      "author" : [ "Weijia Xu", "Batool Haider", "Saab Mansour." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5052–5063, Online. As-",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "PAWS-X: A cross-lingual adversarial dataset for paraphrase identification",
      "author" : [ "Yinfei Yang", "Yuan Zhang", "Chris Tar", "Jason Baldridge." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Syntaxenhanced self-attention-based semantic role labeling",
      "author" : [ "Yue Zhang", "Rui Wang", "Luo Si." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "SG-Net: Syntax-guided machine reading comprehension",
      "author" : [ "Zhuosheng Zhang", "Yuwei Wu", "Junru Zhou", "Sufeng Duan", "Hai Zhao", "Rui Wang." ],
      "venue" : "Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Previous works have shown that pretrained multilingual encoders, such as mBERT (Devlin et al., 2019), capture language syntax, helping cross-lingual transfer.",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 10,
      "context" : "Figure 1: Two parallel sentences in English and Hindi from XNLI (Conneau et al., 2018) dataset.",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 35,
      "context" : "In recent years, we have seen a colossal effort to pre-train Transformer encoder (Vaswani et al., 2017) on large-scale unlabeled text data in one or many languages.",
      "startOffset" : 81,
      "endOffset" : 103
    }, {
      "referenceID" : 12,
      "context" : "Multilingual encoders, such as mBERT (Devlin et al., 2019) or XLM-R (Conneau et al.",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 9,
      "context" : ", 2019) or XLM-R (Conneau et al., 2020) map text sequences into a shared multilingual space by jointly pre-training in many languages.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 19,
      "context" : "Figure 2: A parallel QA example in English (en) and Spanish (es) from MLQA (Lewis et al., 2020) with predictions from mBERT and our proposed syntax-augmented mBERT.",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 28,
      "context" : ", 2018), question answering (Rajpurkar et al., 2016; Lewis et al., 2020), named entity recogni-",
      "startOffset" : 28,
      "endOffset" : 72
    }, {
      "referenceID" : 19,
      "context" : ", 2018), question answering (Rajpurkar et al., 2016; Lewis et al., 2020), named entity recogni-",
      "startOffset" : 28,
      "endOffset" : 72
    }, {
      "referenceID" : 40,
      "context" : "Since the introduction of mBERT, several works (Wu and Dredze, 2019; Pires et al., 2019; K et al., 2020) attempted to reason their success in cross-lingual transfer.",
      "startOffset" : 47,
      "endOffset" : 104
    }, {
      "referenceID" : 25,
      "context" : "Since the introduction of mBERT, several works (Wu and Dredze, 2019; Pires et al., 2019; K et al., 2020) attempted to reason their success in cross-lingual transfer.",
      "startOffset" : 47,
      "endOffset" : 104
    }, {
      "referenceID" : 17,
      "context" : "Since the introduction of mBERT, several works (Wu and Dredze, 2019; Pires et al., 2019; K et al., 2020) attempted to reason their success in cross-lingual transfer.",
      "startOffset" : 47,
      "endOffset" : 104
    }, {
      "referenceID" : 13,
      "context" : "A few recent works (Hewitt and Manning, 2019; Jawahar et al., 2019; Chi et al., 2020) suggest that BERT learns compositional features; mimicking",
      "startOffset" : 19,
      "endOffset" : 85
    }, {
      "referenceID" : 16,
      "context" : "A few recent works (Hewitt and Manning, 2019; Jawahar et al., 2019; Chi et al., 2020) suggest that BERT learns compositional features; mimicking",
      "startOffset" : 19,
      "endOffset" : 85
    }, {
      "referenceID" : 7,
      "context" : "A few recent works (Hewitt and Manning, 2019; Jawahar et al., 2019; Chi et al., 2020) suggest that BERT learns compositional features; mimicking",
      "startOffset" : 19,
      "endOffset" : 85
    }, {
      "referenceID" : 19,
      "context" : "Utilizing syntax structure can also benefit generalized cross-lingual transfer (Lewis et al., 2020) where the input text sequences belong to different languages.",
      "startOffset" : 79,
      "endOffset" : 99
    }, {
      "referenceID" : 36,
      "context" : "We use a graph attention network (GAT) (Veličković et al., 2018) to learn structured representations of the input sequences",
      "startOffset" : 39,
      "endOffset" : 64
    }, {
      "referenceID" : 36,
      "context" : "We employ graph attention network (GAT) (Veličković et al., 2018) to learn syntax representations and fuse them into the self-attention mechanism of mBERT.",
      "startOffset" : 40,
      "endOffset" : 65
    }, {
      "referenceID" : 35,
      "context" : "Transformer encoder (Vaswani et al., 2017) is composed of an embedding layer and stacked encoder layers.",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 36,
      "context" : "We use a graph attention network (GAT) (Veličković et al., 2018) to embed the dependency tree structure of the input sequence.",
      "startOffset" : 39,
      "endOffset" : 64
    }, {
      "referenceID" : 10,
      "context" : "We use the XNLI (Conneau et al., 2018) and PAWS-X (Yang et al.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 44,
      "context" : ", 2018) and PAWS-X (Yang et al., 2019) datasets for the tasks, respectively.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 24,
      "context" : "We use the Wikiann dataset (Pan et al., 2017) and a subset of two tasks from CoNLL-2002 (Tjong Kim Sang, 2002) and CoNLL-2003 NER (Tjong Kim Sang and De Meulder, 2003).",
      "startOffset" : 27,
      "endOffset" : 45
    }, {
      "referenceID" : 19,
      "context" : "lingual question answering benchmarks, MLQA (Lewis et al., 2020), and XQuAD (Artetxe et al.",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 28,
      "context" : "We use the SQuAD dataset (Rajpurkar et al., 2016) for training and validation.",
      "startOffset" : 25,
      "endOffset" : 49
    }, {
      "referenceID" : 20,
      "context" : "We conduct experiments on two recently proposed benchmarks: (i) mTOP (Li et al., 2021) and (ii) mATIS++ (Xu et al.",
      "startOffset" : 69,
      "endOffset" : 86
    }, {
      "referenceID" : 14,
      "context" : "We extend the XTREME framework (Hu et al., 2020) that is developed based on transformers API (Wolf et al.",
      "startOffset" : 31,
      "endOffset" : 48
    }, {
      "referenceID" : 19,
      "context" : "In the generalized cross-lingual transfer setting (Lewis et al., 2020), the input text sequences for the downstream tasks (e.",
      "startOffset" : 50,
      "endOffset" : 70
    }, {
      "referenceID" : 4,
      "context" : ", scaling attention weights based on dependency structure (Bugliarello and Okazaki, 2020).",
      "startOffset" : 58,
      "endOffset" : 89
    }, {
      "referenceID" : 27,
      "context" : ", UDPipe (Straka and Straková, 2017) or Stanza (Qi et al., 2020) to collect part-of-speech tags and the dependency structure of the input sequences.",
      "startOffset" : 47,
      "endOffset" : 64
    }, {
      "referenceID" : 18,
      "context" : ", part-of-speech (POS) tags, dependency parse structure, and relations are shown to be helpful for cross-lingual transfer (Kozhevnikov and Titov, 2013; Pražák and Konopı́k, 2017; Wu et al., 2017; Subburathinam et al., 2019; Liu et al., 2019; Zhang et al., 2019; Xie et al., 2020; Ahmad et al., 2021).",
      "startOffset" : 122,
      "endOffset" : 299
    }, {
      "referenceID" : 26,
      "context" : ", part-of-speech (POS) tags, dependency parse structure, and relations are shown to be helpful for cross-lingual transfer (Kozhevnikov and Titov, 2013; Pražák and Konopı́k, 2017; Wu et al., 2017; Subburathinam et al., 2019; Liu et al., 2019; Zhang et al., 2019; Xie et al., 2020; Ahmad et al., 2021).",
      "startOffset" : 122,
      "endOffset" : 299
    }, {
      "referenceID" : 41,
      "context" : ", part-of-speech (POS) tags, dependency parse structure, and relations are shown to be helpful for cross-lingual transfer (Kozhevnikov and Titov, 2013; Pražák and Konopı́k, 2017; Wu et al., 2017; Subburathinam et al., 2019; Liu et al., 2019; Zhang et al., 2019; Xie et al., 2020; Ahmad et al., 2021).",
      "startOffset" : 122,
      "endOffset" : 299
    }, {
      "referenceID" : 32,
      "context" : ", part-of-speech (POS) tags, dependency parse structure, and relations are shown to be helpful for cross-lingual transfer (Kozhevnikov and Titov, 2013; Pražák and Konopı́k, 2017; Wu et al., 2017; Subburathinam et al., 2019; Liu et al., 2019; Zhang et al., 2019; Xie et al., 2020; Ahmad et al., 2021).",
      "startOffset" : 122,
      "endOffset" : 299
    }, {
      "referenceID" : 22,
      "context" : ", part-of-speech (POS) tags, dependency parse structure, and relations are shown to be helpful for cross-lingual transfer (Kozhevnikov and Titov, 2013; Pražák and Konopı́k, 2017; Wu et al., 2017; Subburathinam et al., 2019; Liu et al., 2019; Zhang et al., 2019; Xie et al., 2020; Ahmad et al., 2021).",
      "startOffset" : 122,
      "endOffset" : 299
    }, {
      "referenceID" : 45,
      "context" : ", part-of-speech (POS) tags, dependency parse structure, and relations are shown to be helpful for cross-lingual transfer (Kozhevnikov and Titov, 2013; Pražák and Konopı́k, 2017; Wu et al., 2017; Subburathinam et al., 2019; Liu et al., 2019; Zhang et al., 2019; Xie et al., 2020; Ahmad et al., 2021).",
      "startOffset" : 122,
      "endOffset" : 299
    }, {
      "referenceID" : 42,
      "context" : ", part-of-speech (POS) tags, dependency parse structure, and relations are shown to be helpful for cross-lingual transfer (Kozhevnikov and Titov, 2013; Pražák and Konopı́k, 2017; Wu et al., 2017; Subburathinam et al., 2019; Liu et al., 2019; Zhang et al., 2019; Xie et al., 2020; Ahmad et al., 2021).",
      "startOffset" : 122,
      "endOffset" : 299
    }, {
      "referenceID" : 1,
      "context" : ", part-of-speech (POS) tags, dependency parse structure, and relations are shown to be helpful for cross-lingual transfer (Kozhevnikov and Titov, 2013; Pražák and Konopı́k, 2017; Wu et al., 2017; Subburathinam et al., 2019; Liu et al., 2019; Zhang et al., 2019; Xie et al., 2020; Ahmad et al., 2021).",
      "startOffset" : 122,
      "endOffset" : 299
    }, {
      "referenceID" : 35,
      "context" : "body of prior works investigated the advantages of incorporating language syntax to enhance the self-attention mechanism (Vaswani et al., 2017).",
      "startOffset" : 121,
      "endOffset" : 143
    }, {
      "referenceID" : 15,
      "context" : "Graph attention networks (GATs) that use multi-head attention has also been adopted for NLP tasks (Huang and Carley, 2019) also fall into this category.",
      "startOffset" : 98,
      "endOffset" : 122
    }, {
      "referenceID" : 31,
      "context" : "These approaches are trained to predict the dependency parse via supervised learning (Strubell et al., 2018; Deguchi et al., 2019).",
      "startOffset" : 85,
      "endOffset" : 130
    }, {
      "referenceID" : 11,
      "context" : "These approaches are trained to predict the dependency parse via supervised learning (Strubell et al., 2018; Deguchi et al., 2019).",
      "startOffset" : 85,
      "endOffset" : 130
    } ],
    "year" : 2021,
    "abstractText" : "In recent years, we have seen a colossal effort in pre-training multilingual text encoders using large-scale corpora in many languages to facilitate cross-lingual transfer learning. However, due to typological differences across languages, the cross-lingual transfer is challenging. Nevertheless, language syntax, e.g., syntactic dependencies, can bridge the typological gap. Previous works have shown that pretrained multilingual encoders, such as mBERT (Devlin et al., 2019), capture language syntax, helping cross-lingual transfer. This work shows that explicitly providing language syntax and training mBERT using an auxiliary objective to encode the universal dependency tree structure helps cross-lingual transfer. We perform rigorous experiments on four NLP tasks, including text classification, question answering, named entity recognition, and taskoriented semantic parsing. The experiment results show that syntax-augmented mBERT improves cross-lingual transfer on popular benchmarks, such as PAWS-X and MLQA, by 1.4 and 1.6 points on average across all languages. In the generalized transfer setting, the performance boosted significantly, with 3.9 and 3.1 points on average in PAWS-X and MLQA.",
    "creator" : "LaTeX with hyperref"
  }
}