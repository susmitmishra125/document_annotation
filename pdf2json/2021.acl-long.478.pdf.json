{
  "name" : "2021.acl-long.478.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learn to Resolve Conversational Dependency: A Consistency Training Framework for Conversational Question Answering",
    "authors" : [ "Gangwoo Kim", "Hyunjae Kim", "Jungsoo Park", "Jaewoo Kang" ],
    "emails" : [ "kim@korea.ac.kr", "hyunjae-kim@korea.ac.kr", "park@korea.ac.kr", "kangj@korea.ac.kr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6130–6141\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6130"
    }, {
      "heading" : "1 Introduction",
      "text" : "Conversational question answering (CQA) involves modeling the information-seeking process of humans in a dialogue. Unlike single-turn question answering (QA) tasks (Rajpurkar et al., 2016; Kwiatkowski et al., 2019), CQA is a multi-turn QA task, where questions in a dialogue are contextdependent;2 hence they need to be understood with the conversation history (Choi et al., 2018; Reddy et al., 2019). As illustrated in Figure 1, to answer\n† Corresponding author 1Our models and code are available at:\nhttps://github.com/dmis-lab/excord 2While the term “context” usually refers to the evidence document from which the answer is extracted, in CQA, it refers to conversational context.\nthe current question “Was he close with anyone else?,” a model should resolve the conversational dependency, such as anaphora and ellipsis, based on the conversation history.\nA line of research in CQA proposes the end-toend approach, where a single QA model jointly encodes the evidence document, the current question, and the whole conversation history (Huang et al., 2018; Yeh and Chen, 2019; Qu et al., 2019a). In this approach, models are required to automatically learn to resolve conversational dependencies. However, existing models have limitations to do so without explicit guidance on how to resolve these dependencies. In the example presented in Figure\n1, models are trained without explicit signals that “he” refers to “Leonardo da Vinci,” and “anyone else” can be more elaborated with “other than his pupils, Salai and Melzi.”\nAnother line of research proposes a pipeline approach that decomposes the CQA task into question rewriting (QR) and QA, to reduce the complexity of the task (Vakulenko et al., 2020). Based on the conversation history, QR models first generate selfcontained questions by rewriting the original questions, such that the self-contained questions can be understood without the conversation history. For instance, the current question q3 is reformulated as the self-contained question q̃3 by a QR model in Figure 1. After rewriting the question, QA models are asked to answer the self-contained questions rather than the original questions. In this approach, QA models are trained to answer relatively simple questions whose dependencies have been resolved by QR models. Thus, this limits reasoning abilities of QA models for the CQA task, and causes QA models to rely on QR models.\nIn this paper, we emphasize that QA models can be enhanced by using both types of questions with explicit guidance on how to resolve the conversational dependency. Accordingly, we propose EXCORD (Explicit guidance on how to Resolve Conversational Dependency), a novel training framework for the CQA task. In this framework, we first generate self-contained questions using QR models. We then pair the self-contained questions with the original questions, and jointly encode them to train QA models with consistency regularization (Laine and Aila, 2016; Xie et al., 2019). Specifically, when original questions are given, we encourage QA models to yield similar answers to those when self-contained questions are given. This training strategy helps QA models to better understand the conversational context, while circumventing the limitations of previous approaches.\nTo demonstrate the effectiveness of EXCORD, we conduct extensive experiments on the three CQA benchmarks. In the experiments, our framework significantly outperforms the existing approaches by up to 1.2 F1 on QuAC (Choi et al., 2018) and by 5.2 F1 on CANARD (Elgohary et al., 2019). In addition, we find that our framework is also effective on a dataset CoQA (Reddy et al., 2019) that does not have the self-contained questions generated by human annotators. This indicates that the proposed framework can be adopted\non various CQA datasets in future work. We summarize the contributions of this work as follows:\n• We identify the limitations of previous approaches and propose a unified framework to address these. Our novel framework improves QA models by incorporating QR models, while reducing the reliance on them.\n• Our framework encourages QA models to learn how to resolve the conversational dependency via consistency regularization. To the best of our knowledge, our work is the first to apply the consistency training framework to the CQA task.\n• We demonstrate the effectiveness of our framework on three CQA benchmarks. Our framework is model-agnostic and systematically improves the performance of QA models."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 Task Formulation",
      "text" : "In CQA, a single instance is a dialogue, which consists of an evidence document d, a list of questions q = [q1, ..., qT ], and a list of answers for the questions a = [a1, ..., aT ], where T represents the number of turns in the dialogue. For the t-th turn, the question qt and the conversation history Ht = [(q1, a1), ..., (qt−1, at−1)] are given, and a model should extract the answer from the evidence document as:\nât = arg max at\nP(at|d, qt,Ht) (1)\nwhere P(·) represents a likelihood function over all the spans in the evidence document, and ât is the predicted answer. Unlike single-turn QA, since the current question qt is dependent on the conversation history Ht, it is important to effectively encode the conversation history and resolve the conversational dependency in CQA."
    }, {
      "heading" : "2.2 End-to-end Approach",
      "text" : "A naive approach in solving CQA is to train a model in an end-to-end manner (Figure 2a). Since standard QA models generally are ineffective in the CQA task, most studies attempt to develop a QA model structure or mechanism for encoding the conversation history effectively (Huang et al., 2018; Yeh and Chen, 2019; Qu et al., 2019a,b). Although\nthese efforts improved performance on the CQA benchmarks, existing models remain limited in understanding conversational context. In this paper, we emphasize that QA models can be further improved with explicit guidance using self-contained questions effectively."
    }, {
      "heading" : "2.3 Pipeline Approach",
      "text" : "Recent studies decompose the task into two subtasks to reduce the complexity of the CQA task. The first sub-task, question rewriting, involves generating self-contained questions by reformulating the original questions. Neural-net-based QR models are commonly used to obtain selfcontained questions (Lin et al., 2020; Vakulenko et al., 2020). The QR models are trained on the CANARD dataset (Elgohary et al., 2019), which consists of 40K pairs of original QuAC questions and their self-contained versions that are generated by human annotators.\nAfter generating the self-contained questions, the next sub-task, question answering, is carried out. Since it is assumed that the dependencies in the questions have already been resolved by QR models, existing works usually use standard QA models (not specialized to CQA); however conversational QA models can also be used (the dotted line in Figure 2b). We formulate the process of\npredicting the answer in the pipeline approach as:\nP(at|d, qt,Ht) ≈ Prewr(q̃t|qt,Ht) · Pread(at|d, q̃t)\n(2)\nwhere Prewr(·) and Pread(·) are the likelihood functions of QR and QA models, respectively. q̃t is a self-contained question rewritten by the QR model.\nThe main limitation of the pipeline approach is that QA models are never trained on the original questions, which limits their abilities to understand the conversational context. Moreover, this approach makes QA models dependent on QR models; hence QA models suffer from the error propagation from QR models. 3 On the other hand, our framework enhances QA models’ reasoning abilities for CQA by jointly utilizing original and self-contained questions. In addition, QA models in our framework do not rely on QR models at inference time and thus do not suffer from error propagation."
    }, {
      "heading" : "3 EXCORD: Explicit Guidance on Resolving Conversational Dependency",
      "text" : "We introduce a unified framework that jointly encodes the original and self-contained questions as\n3We present an example of the error propagation in Section 5.2.\nillustrated in Figure 2c. Our framework consists of two stages: (1) generating self-contained questions using a QR model (§3.1) and (2) training a QA model with the original and self-contained questions via consistency regularization (§3.2)."
    }, {
      "heading" : "3.1 Question Rewriting",
      "text" : "Similar to the pipeline approach, we utilize a QR model to obtain self-contained questions. We use the obtained questions for explicit guidance in the next stage. As shown in Equation 2, the QR task is to generate a self-contained question given an original question and a conversation history. Following Lin et al. (2020), we adopt a T5-based sequence generator (Raffel et al., 2020) as our QR model, which achieves comparable performance with that of humans in QR.4 For training and evaluating the QR model, we use the CANARD dataset following previous works on QR (Lin et al., 2020; Vakulenko et al., 2020). During inference, we utilize the top-k random sampling decoding based on beam search with the adjustment of the softmax temperature (Fan et al., 2018; Xie et al., 2019)."
    }, {
      "heading" : "3.2 Consistency Regularization",
      "text" : "Our goal is to enhance the QA model’s ability to understand conversational context. Accordingly, we use consistency regularization (Laine and Aila, 2016; Xie et al., 2019), which enforces a model to make consistent predictions in response to transformations to the inputs. We encourage the model’s predicted answers from the original questions to be similar to those from the self-contained questions (§3.1). Our consistency loss is defined as:\nLconst = KL(Preadθ (at|d, qt,Ht)||Preadθ̄ (at|d, q̃t, H̃t)) (3) where KL(·) represents the Kullback–Leibler divergence function between two probability distributions. θ is the model’s parameters, and θ̄ depicts a fixed copy of θ.\nWith the consistency loss, QA models are regularized to make consistent predictions, regardless of whether the given question is self-contained or not. In order to output an answer distribution that is closer to Pread\nθ̄ (at|d, q̃t, H̃t), QA models should\ntreat original questions as if they were rewritten into self-contained questions by referring to the\n4On CANARD, our QR model achieved comparable performance with the human performance in preliminary experiments.\nconversation history. Through this process, our consistency regularization method serves as explicit guidance that encourages QA models to resolve the conversational dependency. In our framework, Preadθ (at|·) is the answer span distribution over all evidence document tokens. In contrast to Asai and Hajishirzi (2020), by using all probability values in the answer distributions, the signals of selfcontained questions can be effectively propagated to the QA model. In addition to using all probability values, we also sharpened the target distribution Pread\nθ̄ (at|d, q̃t, H̃t) by adjusting the tempera-\nture (Xie et al., 2019) to strengthen the QA model’s training signal.\nFinally, we calculate the final loss as:\nL = Lorig + λ1Lself + λ2Lcons (4)\nwhere λ1 and λ2 are hyperparameters. Lorig and Lself are calculated by the negative log-likelihood between the predicted answers and gold standards given the original and self-contained questions, respectively.\nComparison with previous works Consistency training has mainly been studied as a method for regularizing model predictions to be invariant to small noises that are injected into the input samples (Sajjadi et al., 2016; Laine and Aila, 2016; Miyato et al., 2016; Xie et al., 2019). The intuition behind consistency training is to push noisy inputs closer towards their original versions. Therefore, only the original parameters (i.e., θ) are updated, while the copied model parameters (i.e., θ̄) are fixed.\nIn contrast to the original concept of consistency training, our goal is to go in the opposite direction and update the original parameters. Thus, we fix the parameters θ̄ with self-contained questions, and soley update θ for each training step as shown in Equation 3."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we describe our experimental setup and compare our framework to baseline approaches (i.e., the end-to-end and pipeline approaches)."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "QuAC QuAC (Choi et al., 2018) comprises 100k QA pairs in information-seeking dialogues, where a student asks questions based on a topic with background information provided, and a teacher provides the answers in the form of text spans in\nWikipedia documents. Since the test set is only available in the QuAC challenge, we evaluate models on the development set.5 For validation, we use a subset of the original training set of QuAC, which consists of questions that correspond to the self-contained questions in CANARD’s development set. The remaining data is used for training.\nCANARD CANARD (Elgohary et al., 2019) consists of 31K, 3K, and 5K QA pairs for training, development, and test sets, respectively. The questions in CANARD are generated by rewriting a subset of the original questions in QuAC. We use the training and development sets for training and validating QR models, and the test set for evaluating QA models.\nCoQA CoQA (Reddy et al., 2019) consists of 127K QA pairs and evidence documents in seven domains. In terms of the question distribution, CoQA significantly differs from QuAC (see §5.3). We use CoQA to test the transferability of EXCORD, where a QR model trained on CANARD generates the self-contained questions in a zeroshot manner. Subsequently, we train a QA model by using the original and synthetic questions. Similar to QuAC, the test set of CoQA is soley available in the CoQA challenge. 6 Therefore, we randomly sample 5% of the QA dialogues in the training set and adopt them as our development set."
    }, {
      "heading" : "4.2 Metrics",
      "text" : "Following Choi et al. (2018), we use the F1, HEQQ, and HEQ-D for QuAC and CANARD. HEQ-Q measures whether a model finds more accurate answers than humans (or the same answers) in a given question. HEQ-D measures the same thing, but in a given dialog instead of a question. For CoQA, we report the F1 scores for each domain (children’s story, literature from Project Gutenberg, middle and high school English exams, news articles from CNN, Wikipedia) and the overall F1 score, as suggested by Reddy et al. (2019)."
    }, {
      "heading" : "4.3 QA models",
      "text" : "Note that the baseline approaches and our framework do not limit the structure of QA models. For a fair comparison of the baseline approaches and EXCORD, we test the same QA models in all approaches. The selected QA models are commonly used and have been proven to be effective in CQA.\n5https://quac.ai/ 6https://stanfordnlp.github.io/coqa/\nBERT BERT (Devlin et al., 2019) is a contextualized word representation model that is pretrained on large corpora. BERT also works well on CQA datasets, although it is not designed for CQA. It receives the evidence document, current question, and conversation history of the previous turn as input.\nBERT+HAE BERT+HAE is a BERT-based QA model with a CQA-specific module. Following Qu et al. (2019a), we add the history answer embedding (HAE) to BERT’s word embeddings. HAE encodes the information of the answer spans from the previous questions.\nRoBERTa RoBERTa (Liu et al., 2019) improves BERT by using pretraining techniques to obtain the robustly optimized weights on larger corpora. In our experiments, we found that RoBERTa performs well in CQA, achieving comparable performance with the previous SOTA model, HAM (Qu et al., 2019b), on QuAC. Thus, we adopt RoBERTa as our main baseline model owing to its simplicity and effectiveness. It receives the same input as BERT, otherwise specified."
    }, {
      "heading" : "4.4 Implementation Details",
      "text" : "The CANARD training set provides 31,527 selfcontained questions from the original QuAC questions. Therefore, we can obtain 31,527 pairs of original and self-contained questions without question rewriting. For the rest of the original questions, we automatically generate self-contained questions by using our QR model. Finally, we obtain 83,568 question pairs and use them in our consistency training. We denote the original questions, selfcontained questions generated by humans, and selfcontained questions generated by a QR model as Q, Q̃human, and Q̃syn, respectively. Additional implementation details are described in Appendix B"
    }, {
      "heading" : "4.5 Results",
      "text" : "Table 1 presents the performance comparison of the baseline approaches to our framework on QuAC and CANARD. Compared to the end-to-end approach, EXCORD consistently improves the performance of QA models on both datasets. Also, these improvements are significant: EXCORD improves the performance of the RoBERTa by absolutely 1.2 and 2.3 F1 scores and BERT by 1.2 and 5.2 F1 scores on QuAC and CANARD, respectively. From these results, we conclude that the consistency training with original and self-contained\nquestions enhances ability of QA models to understand the conversational context.\nOn QuAC, the pipeline approach underperforms the end-to-end approach in all baseline models. This indicates that training a QA model soley with self-contained questions is ineffective when human rewrites are not given at the inference phase. On the other hand, EXCORD improves QA models by using both types of questions. As presented in Table 1, our framework significantly outperforms the baseline approaches on QuAC.\nOn CANARD, the pipeline approach is significantly more effective than the end-to-end approach. Since QA models are trained with self-contained questions in the pipeline approach, they perform well on CANARD questions. Nevertheless, EXCORD still outperforms the pipeline approach in most cases. Compared to the pipeline approach, our framework improves the performance of RoBERTa by absolutely 1.2 F1 score."
    }, {
      "heading" : "5 Analysis and Discussion",
      "text" : "We elaborate on analyses regarding component ablation and transferability. We also describe a case study carried out to highlight such differences between our and baseline approaches."
    }, {
      "heading" : "5.1 Ablation Study",
      "text" : "In this section, we comprehensively explore the factors contributing to this improvement in detail: (1) using self-contained questions that are rewritten by humans (Q̃human) as additional data, (2) using self-contained questions that are synthetically generated by the QR model (Q̃syn), and (3) training a QA model with our consistency framework. In Table 2, we present the performance gaps when each component is removed from our framework. We use RoBERTa on QuAC in this experiment.\nWe first explore the effects of Q̃human and Q̃syn. As shown in Table 2, excluding Q̃human degrades the performance of RoBERTa in our framework. Although automatically generated, Q̃syn contributes to the performance improvement. Therefore, both types of self-contained questions are useful in our framework.\nTo investigate the effect of our framework, we simply augment Q̃human and Q̃syn to Qorig, which is called Question Augment (question data augmentation). We find that Question Augment slightly improves the performance of RoBERTa on CANARD, whereas it degrades the performance on QuAC. This shows that simply augmenting the questions is ineffective and does not guarantee improvement. On the other hand, our consistency training approach significantly improves performance, showing that EXCORD is a more optimal way to utilizing self-contained questions."
    }, {
      "heading" : "5.2 Case Study",
      "text" : "We analyze several cases that the baseline approaches answered incorrectly, but our framework answered correctly. We also explore how our framework improves the reasoning ability of QA models, compared to the baseline approaches. These cases\nError case # 1 Title : Montgomery Clift Section Title : Film career Document d : · · · His second movie was The Search . Clift was unhappy with the quality of the script, and edited it himself. The movie was awarded a screenwriting Academy Award for the credited writers. · · ·\nare obtained from the development set of QuAC.\nThe first case in Table 3 shows the predictions of the two RoBERTa models trained in the end-to-end approach and our framework, respectively. Note that “the film” in the current question does not refer to “The Search” (red box) in the document d, but “Red River” (blue box) in a1. When trained in the end-to-end approach, the model failed to comprehend the conversational context and misunderstood what “the film” refers to, resulting in an incorrect prediction. On the other hand, when trained in EXCORD, the model predicted the correct answer because it enhances the ability to resolve conversational dependency.\nIn the second case, we compare the pipeline approach to EXCORD. In this case, the QR model misunderstood “my” in the current question as a pronoun and replaced it with the band’s name, “Train’s.” Consequently, the QA model received the erroneous self-contained question, resulting in an incorrect prediction. On the other hand, the QA model trained in our framework predicted the\ncorrect answer based on the original question q6."
    }, {
      "heading" : "5.3 Transferability",
      "text" : "We train a QR model to rewrite QuAC questions into CANARD questions. Then, self-contained questions can be generated for the samples that do not have human rewrites. This results in the improvement of QA models’ performance on QuAC and CANARD (§4.5). However, it is questionable whether the QR model can successfully rewrite questions when the original questions significantly differ from those in QuAC. To answer this, we test our framework on another CQA dataset, CoQA. We first analyze how the question distributions of QuAC and CoQA differ. We found that question types in QuAC and CoQA are significantly different, such that QR models could suffer from the gap of question distributions between two datasets. (See details in Appendix A).\nTo test the transferability of EXCORD, we compare the end-to-end approach to our framework on the CoQA dataset. Using a QR model trained on\nCANARD, we generate the self-contained questions for CoQA and train QA models with our framework. As presented in Table 4, our framework performs well on CoQA. The improvement in BERT is 0.5 based on the overall F1, and the performance of RoBERTa is also improved by an overall F1 of 0.6. Improvements are also consistent in most of the documents’ domains. Therefore, we conclude that our framework can be simply extended to other datasets and improve QA performance even when question distributions are significantly different. We plan to improve the transferability of our framework by fine-tuning QR models on target datasets in future work."
    }, {
      "heading" : "6 Related Work",
      "text" : "Conversational Question Answering Recently, several works introduced CQA datasets such as QUAC (Choi et al., 2018) and COQA (Reddy et al., 2019). We classified proposed methods to solve the datasets into two approaches: (1) end-to-end and (2) pipeline. Most works based on the endto-end approach focused on developing a model structure (Zhu et al., 2018; Ohsugi et al., 2019; Qu et al., 2019a,b) or training strategy such as multitask with rationale tagging (Ju et al., 2019) that are specialized in the CQA task or datasets. Several works demonstrated the effectiveness of the flow mechanism in CQA (Huang et al., 2018; Chen et al., 2019; Yeh and Chen, 2019).\nWith the advent of a dataset consisting of selfcontained questions rewritten by human annotators (Elgohary et al., 2019), the pipeline approach has drawn attention as a promising method for CQA in recent days (Vakulenko et al., 2020). The approach is particularly useful for the open-domain CQA or passage-retrieval (PR) tasks (Dalton et al., 2019; Ren et al., 2020; Anantha et al., 2020; Qu et al., 2020) since self-contained questions can be\nfed into existing non-conversational search engines such as BM25. Note that our framework can be used jointly with the pipeline approach in the opendomain setting because our framework can improve QA models’ ability to find the answers from the retrieved documents. We will test our framework in the open-domain setting in future work.\nQuestion Rewriting QR has been studied for augmenting training data (Buck et al., 2018; Sun et al., 2018; Zhu et al., 2019; Liu et al., 2020) or clarifying ambiguous questions (Min et al., 2020). In CQA, QR can be viewed as a task of simplifying difficult questions that include anaphora and ellipsis in a conversation. Elgohary et al. (2019) first proposed the question rewriting task as a sub-task of CQA and the CANARD dataset for the task, which consists of pairs of original and self-contained questions that are generated by human annotators. Vakulenko et al. (2020) used a coreference-based model (Lee et al., 2018) and GPT-2 (Radford et al., 2019) as QR models and tested the models in the QR and PR tasks. Lin et al. (2020) conducted the QR task using T5 (Raffel et al., 2020) and achieved on performance comparable to humans on CANARD. Following Lin et al. (2020), we use T5 in our experiments to generate high-quality questions for enhancing QA models.\nConsistency Training Consistency regularization (Laine and Aila, 2016; Sajjadi et al., 2016) has been mainly explored in the context of semisupervised learning (SSL) (Chapelle et al., 2009; Oliver et al., 2018), which has been adopted in the textual domain as well (Miyato et al., 2016; Clark et al., 2018; Xie et al., 2020). However, the consistency training framework is also applicable when only the labeled samples are available (Miyato et al., 2018; Jiang et al., 2019; Asai and Hajishirzi, 2020). The consistency regularization requires adding noise to the sample, which can be either discrete (Xie et al., 2020; Asai and Hajishirzi, 2020) or continuous (Miyato et al., 2016; Jiang et al., 2019). Existing works regularize the predictions of the perturbed samples to be equivalent to be that of the originals’. On the other hand, our method encourages the models’ predictions for the original asnwers to be similar to those from the rewritten questions, i.e., synthetic ones."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We propose a consistency training framework for conversational question answering, which enhances QA models’ abilities to understand conversational context. Our framework leverages both the original and self-contained questions for explicit guidance on how to resolve conversational dependency. In our experiments, we demonstrate that our framework significantly improves the QA model’s performance on QuAC and CANARD, compared to the existing approaches. In addition, we verified that our framework can be extended to CoQA. In future work, the transferability of our framework can be further improved by fine-tuning the QR model on target datasets. Furthermore, future work would include applying our framework to the open-domain setting."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Sean S. Yi, Miyoung Ko, and Jinhyuk Lee for providing valuable comments and feedback. This research was supported by the MSIT (Ministry of Science and ICT), Korea, under the ICT Creative Consilience program (IITP-2021-2020-001819) supervised by the IITP (Institute for Information communications Technology Planning Evaluation). This research was also supported by National Research Foundation of Korea (NRF2020R1A2C3010638)."
    }, {
      "heading" : "A Comparison of Questions in QuAC and CoQA",
      "text" : "Before testing the transferability of EXCORD (§5.3), we compare the question distribution of QuAC to that of CoQA. The types of questions are significantly different due to the difference in task setups. When questions were generated in QuAC, evidence documents were soley provided to answerers, but not to questioners. This setup prevented questioners from referring to the evidence documents, which encouraged the questioners to ask natural and information-seeking questions. By contrast, when creating CoQA, questioners and answerers shared the same evidence documents.\nExamples of QuAC and CoQA are presented in Table 5 and the categorization of question types in Table 6. The results are as follows: (1) QuAC has more non-factoid questions. Approximately half of QuAC questions are non-factoid, whereas more than 60% of questions in CoQA can be answered with either entities or noun phrases. (2) “Anything else?” questions are more frequently observed in QuAC. When questioners cannot find what to ask, they use “Anything else?” questions to seek new topics and continue the conversation. In CoQA,\nquestioners rarely used the “Anything else?” question (2.8%) since they did not need to seek new topics. This type of question is observed in Table 5 (q2 in the left side). (3) CoQA has few unanswerable questions. Since questioners and answerers share the evidence documents when creating CoQA, only 1.3% of unanswerable questions are asked. However, approximately 20% of questions in QuAC are unanswerable."
    }, {
      "heading" : "B Hyperparameters",
      "text" : "Our implementation is based on PyTorch.7 We implemented BERT using the Transformers library.8 We implemented the T5-based QR model using the Transformers library and adopted the same QR model in the pipeline approach and EXCORD. We use a single 24GB GPU (RTX TITAN) for the experiments.\nWe measured the F1 scores on the development set for each 4k training step, and adopted the bestperforming models. We trained QA models based on the AdamW optimizer with a learning rate of 3e-5. We use the maximum input sequence length as 512 and the maximum answer length as 30. We set the maximum query length to 128 for all approaches since self-contained questions are usually longer than original questions. We use a batch size 12 for BERT and RoBERTa in all baseline approaches. For EXCORD, we set the coefficient λ1 for QA loss for rewritten questions to 0.5. Also we search the coefficient λ2 for consistency loss within the range of [0.7, 0.5] and the softmax temperature within the range of [1.0, 0.9] (Xie et al., 2019).\n7https://pytorch.org/ 8https://github.com/huggingface"
    } ],
    "references" : [ {
      "title" : "Open-domain question answering goes conversational via question rewriting",
      "author" : [ "Raviteja Anantha", "Svitlana Vakulenko", "Zhucheng Tu", "Shayne Longpre", "Stephen Pulman", "Srinivas Chappidi." ],
      "venue" : "arXiv preprint arXiv:2010.04898.",
      "citeRegEx" : "Anantha et al\\.,? 2020",
      "shortCiteRegEx" : "Anantha et al\\.",
      "year" : 2020
    }, {
      "title" : "Logicguided data augmentation and regularization for consistent question answering",
      "author" : [ "Akari Asai", "Hannaneh Hajishirzi." ],
      "venue" : "arXiv preprint arXiv:2004.10157.",
      "citeRegEx" : "Asai and Hajishirzi.,? 2020",
      "shortCiteRegEx" : "Asai and Hajishirzi.",
      "year" : 2020
    }, {
      "title" : "Ask the right questions: Active question reformulation with reinforcement learning",
      "author" : [ "Christian Buck", "Jannis Bulian", "Massimiliano Ciaramita", "Wojciech Gajewski", "Andrea Gesmundo", "Neil Houlsby", "Wei Wang." ],
      "venue" : "International Conference on",
      "citeRegEx" : "Buck et al\\.,? 2018",
      "shortCiteRegEx" : "Buck et al\\.",
      "year" : 2018
    }, {
      "title" : "Semi-supervised learning (chapelle, o",
      "author" : [ "Olivier Chapelle", "Bernhard Scholkopf", "Alexander Zien." ],
      "venue" : "et al., eds.; 2006)[book reviews]. IEEE Transactions on Neural Networks, 20(3):542–542.",
      "citeRegEx" : "Chapelle et al\\.,? 2009",
      "shortCiteRegEx" : "Chapelle et al\\.",
      "year" : 2009
    }, {
      "title" : "Graphflow: Exploiting conversation flow with graph neural networks for conversational machine comprehension",
      "author" : [ "Yu Chen", "Lingfei Wu", "Mohammed J Zaki." ],
      "venue" : "arXiv preprint arXiv:1908.00059.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Quac: Question answering in context",
      "author" : [ "Eunsol Choi", "He He", "Mohit Iyyer", "Mark Yatskar", "Wentau Yih", "Yejin Choi", "Percy Liang", "Luke Zettlemoyer." ],
      "venue" : "arXiv preprint arXiv:1808.07036.",
      "citeRegEx" : "Choi et al\\.,? 2018",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2018
    }, {
      "title" : "Semi-supervised sequence modeling with cross-view training",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Christopher D Manning", "Quoc V Le." ],
      "venue" : "arXiv preprint arXiv:1809.08370.",
      "citeRegEx" : "Clark et al\\.,? 2018",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2018
    }, {
      "title" : "Cast 2019: The conversational assistance track overview",
      "author" : [ "Jeffrey Dalton", "Chenyan Xiong", "Jamie Callan." ],
      "venue" : "Proceedings of the Twenty-Eighth Text REtrieval Conference, TREC, pages 13–15.",
      "citeRegEx" : "Dalton et al\\.,? 2019",
      "shortCiteRegEx" : "Dalton et al\\.",
      "year" : 2019
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Can you unpack that? learning to rewrite questions-in-context",
      "author" : [ "Ahmed Elgohary", "Denis Peskov", "Jordan BoydGraber." ],
      "venue" : "Can You Unpack That? Learning to Rewrite Questions-in-Context.",
      "citeRegEx" : "Elgohary et al\\.,? 2019",
      "shortCiteRegEx" : "Elgohary et al\\.",
      "year" : 2019
    }, {
      "title" : "Hierarchical neural story generation",
      "author" : [ "Angela Fan", "Mike Lewis", "Yann Dauphin." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889–898.",
      "citeRegEx" : "Fan et al\\.,? 2018",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2018
    }, {
      "title" : "Flowqa: Grasping flow in history for conversational machine comprehension",
      "author" : [ "Hsin-Yuan Huang", "Eunsol Choi", "Wen-tau Yih." ],
      "venue" : "arXiv preprint arXiv:1810.06683.",
      "citeRegEx" : "Huang et al\\.,? 2018",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2018
    }, {
      "title" : "Smart: Robust and efficient fine-tuning for pretrained natural language models through principled regularized optimization",
      "author" : [ "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Tuo Zhao." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Jiang et al\\.,? 2019",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2019
    }, {
      "title" : "Technical report on conversational question answering",
      "author" : [ "Ying Ju", "Fubang Zhao", "Shijie Chen", "Bowen Zheng", "Xuefeng Yang", "Yunfeng Liu." ],
      "venue" : "arXiv preprint arXiv:1909.10772.",
      "citeRegEx" : "Ju et al\\.,? 2019",
      "shortCiteRegEx" : "Ju et al\\.",
      "year" : 2019
    }, {
      "title" : "Natural questions: a benchmark for question answering research",
      "author" : [ "Tom Kwiatkowski", "Jennimaria Palomaki", "Olivia Redfield", "Michael Collins", "Ankur Parikh", "Chris Alberti", "Danielle Epstein", "Illia Polosukhin", "Jacob Devlin", "Kenton Lee" ],
      "venue" : null,
      "citeRegEx" : "Kwiatkowski et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Kwiatkowski et al\\.",
      "year" : 2019
    }, {
      "title" : "Temporal ensembling for semi-supervised learning",
      "author" : [ "Samuli Laine", "Timo Aila." ],
      "venue" : "arXiv preprint arXiv:1610.02242.",
      "citeRegEx" : "Laine and Aila.,? 2016",
      "shortCiteRegEx" : "Laine and Aila.",
      "year" : 2016
    }, {
      "title" : "Higher-order coreference resolution with coarse-tofine inference",
      "author" : [ "Kenton Lee", "Luheng He", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Lee et al\\.,? 2018",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2018
    }, {
      "title" : "Conversational question reformulation via sequence-to-sequence architectures and pretrained language models",
      "author" : [ "Sheng-Chieh Lin", "Jheng-Hong Yang", "Rodrigo Nogueira", "Ming-Feng Tsai", "Chuan-Ju Wang", "Jimmy Lin." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Lin et al\\.,? 2020",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "Tell me how to ask again: Question data augmentation with controllable rewriting in continuous space",
      "author" : [ "Dayiheng Liu", "Yeyun Gong", "Jie Fu", "Yu Yan", "Jiusheng Chen", "Jiancheng Lv", "Nan Duan", "Ming Zhou." ],
      "venue" : "Proceedings of the 2020 Conference on",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Ambigqa: Answering ambiguous open-domain questions",
      "author" : [ "Sewon Min", "Julian Michael", "Hannaneh Hajishirzi", "Luke Zettlemoyer." ],
      "venue" : "arXiv preprint arXiv:2004.10645.",
      "citeRegEx" : "Min et al\\.,? 2020",
      "shortCiteRegEx" : "Min et al\\.",
      "year" : 2020
    }, {
      "title" : "Adversarial training methods for semi-supervised text classification",
      "author" : [ "Takeru Miyato", "Andrew M Dai", "Ian Goodfellow." ],
      "venue" : "arXiv preprint arXiv:1605.07725.",
      "citeRegEx" : "Miyato et al\\.,? 2016",
      "shortCiteRegEx" : "Miyato et al\\.",
      "year" : 2016
    }, {
      "title" : "Virtual adversarial training: a regularization method for supervised and semisupervised learning",
      "author" : [ "Takeru Miyato", "Shin-ichi Maeda", "Masanori Koyama", "Shin Ishii." ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence, 41(8):1979–",
      "citeRegEx" : "Miyato et al\\.,? 2018",
      "shortCiteRegEx" : "Miyato et al\\.",
      "year" : 2018
    }, {
      "title" : "A simple but effective method to incorporate multi-turn context with bert for conversational machine comprehension",
      "author" : [ "Yasuhito Ohsugi", "Itsumi Saito", "Kyosuke Nishida", "Hisako Asano", "Junji Tomita." ],
      "venue" : "arXiv preprint arXiv:1905.12848.",
      "citeRegEx" : "Ohsugi et al\\.,? 2019",
      "shortCiteRegEx" : "Ohsugi et al\\.",
      "year" : 2019
    }, {
      "title" : "Realistic evaluation of deep semi-supervised learning algorithms",
      "author" : [ "Avital Oliver", "Augustus Odena", "Colin A Raffel", "Ekin Dogus Cubuk", "Ian Goodfellow." ],
      "venue" : "Advances in neural information processing systems, 31:3235–3246.",
      "citeRegEx" : "Oliver et al\\.,? 2018",
      "shortCiteRegEx" : "Oliver et al\\.",
      "year" : 2018
    }, {
      "title" : "Open-retrieval conversational question answering",
      "author" : [ "Chen Qu", "Liu Yang", "Cen Chen", "Minghui Qiu", "W Bruce Croft", "Mohit Iyyer." ],
      "venue" : "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval,",
      "citeRegEx" : "Qu et al\\.,? 2020",
      "shortCiteRegEx" : "Qu et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert with history answer embedding for conversational question answering",
      "author" : [ "Chen Qu", "Liu Yang", "Minghui Qiu", "W Bruce Croft", "Yongfeng Zhang", "Mohit Iyyer." ],
      "venue" : "Proceedings of the 42nd International ACM SIGIR Conference on Research and",
      "citeRegEx" : "Qu et al\\.,? 2019a",
      "shortCiteRegEx" : "Qu et al\\.",
      "year" : 2019
    }, {
      "title" : "Attentive history selection for conversational question answering",
      "author" : [ "Chen Qu", "Liu Yang", "Minghui Qiu", "Yongfeng Zhang", "Cen Chen", "W Bruce Croft", "Mohit Iyyer." ],
      "venue" : "Proceedings of the 28th ACM International Conference on Information and Knowl-",
      "citeRegEx" : "Qu et al\\.,? 2019b",
      "shortCiteRegEx" : "Qu et al\\.",
      "year" : 2019
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Squad: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "arXiv preprint arXiv:1606.05250.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Coqa: A conversational question answering challenge",
      "author" : [ "Siva Reddy", "Danqi Chen", "Christopher D Manning." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:249–266.",
      "citeRegEx" : "Reddy et al\\.,? 2019",
      "shortCiteRegEx" : "Reddy et al\\.",
      "year" : 2019
    }, {
      "title" : "Conversations with search engines",
      "author" : [ "Pengjie Ren", "Zhumin Chen", "Zhaochun Ren", "Evangelos Kanoulas", "Christof Monz", "Maarten de Rijke." ],
      "venue" : "arXiv preprint arXiv:2004.14162.",
      "citeRegEx" : "Ren et al\\.,? 2020",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2020
    }, {
      "title" : "Regularization with stochastic transformations and perturbations for deep semi-supervised learning",
      "author" : [ "Mehdi Sajjadi", "Mehran Javanmardi", "Tolga Tasdizen." ],
      "venue" : "Advances in neural information processing systems, pages 1163–1171.",
      "citeRegEx" : "Sajjadi et al\\.,? 2016",
      "shortCiteRegEx" : "Sajjadi et al\\.",
      "year" : 2016
    }, {
      "title" : "Improving machine reading comprehension with general reading strategies",
      "author" : [ "Kai Sun", "Dian Yu", "Dong Yu", "Claire Cardie." ],
      "venue" : "arXiv preprint arXiv:1810.13441.",
      "citeRegEx" : "Sun et al\\.,? 2018",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2018
    }, {
      "title" : "Question rewriting for conversational question answering",
      "author" : [ "Svitlana Vakulenko", "Shayne Longpre", "Zhucheng Tu", "Raviteja Anantha." ],
      "venue" : "arXiv preprint arXiv:2004.14652.",
      "citeRegEx" : "Vakulenko et al\\.,? 2020",
      "shortCiteRegEx" : "Vakulenko et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised data augmentation for consistency training",
      "author" : [ "Qizhe Xie", "Zihang Dai", "Eduard Hovy", "Minh-Thang Luong", "Quoc V Le." ],
      "venue" : "arXiv preprint arXiv:1904.12848.",
      "citeRegEx" : "Xie et al\\.,? 2019",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised data augmentation for consistency training",
      "author" : [ "Qizhe Xie", "Zihang Dai", "Eduard Hovy", "Thang Luong", "Quoc Le." ],
      "venue" : "Advances in Neural Information Processing Systems, 33.",
      "citeRegEx" : "Xie et al\\.,? 2020",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2020
    }, {
      "title" : "Flowdelta: Modeling flow information gain in reasoning for conversational machine comprehension",
      "author" : [ "Yi-Ting Yeh", "Yun-Nung Chen." ],
      "venue" : "arXiv preprint arXiv:1908.05117.",
      "citeRegEx" : "Yeh and Chen.,? 2019",
      "shortCiteRegEx" : "Yeh and Chen.",
      "year" : 2019
    }, {
      "title" : "Sdnet: Contextualized attention-based deep network for conversational question answering",
      "author" : [ "Chenguang Zhu", "Michael Zeng", "Xuedong Huang." ],
      "venue" : "arXiv preprint arXiv:1812.03593.",
      "citeRegEx" : "Zhu et al\\.,? 2018",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning to ask unanswerable questions for machine reading comprehension",
      "author" : [ "Haichao Zhu", "Li Dong", "Furu Wei", "Wenhui Wang", "Bing Qin", "Ting Liu." ],
      "venue" : "arXiv preprint arXiv:1906.06045.",
      "citeRegEx" : "Zhu et al\\.,? 2019",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "2 F1 on CANARD (Elgohary et al., 2019), while addressing the limitations of the existing approaches.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 30,
      "context" : "Unlike single-turn question answering (QA) tasks (Rajpurkar et al., 2016; Kwiatkowski et al., 2019), CQA is a multi-turn QA task, where questions in a dialogue are contextdependent;2 hence they need to be understood with the conversation history (Choi et al.",
      "startOffset" : 49,
      "endOffset" : 99
    }, {
      "referenceID" : 14,
      "context" : "Unlike single-turn question answering (QA) tasks (Rajpurkar et al., 2016; Kwiatkowski et al., 2019), CQA is a multi-turn QA task, where questions in a dialogue are contextdependent;2 hence they need to be understood with the conversation history (Choi et al.",
      "startOffset" : 49,
      "endOffset" : 99
    }, {
      "referenceID" : 5,
      "context" : ", 2019), CQA is a multi-turn QA task, where questions in a dialogue are contextdependent;2 hence they need to be understood with the conversation history (Choi et al., 2018; Reddy et al., 2019).",
      "startOffset" : 154,
      "endOffset" : 193
    }, {
      "referenceID" : 31,
      "context" : ", 2019), CQA is a multi-turn QA task, where questions in a dialogue are contextdependent;2 hence they need to be understood with the conversation history (Choi et al., 2018; Reddy et al., 2019).",
      "startOffset" : 154,
      "endOffset" : 193
    }, {
      "referenceID" : 5,
      "context" : "Figure 1: An example of the QuAC dataset (Choi et al., 2018).",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 11,
      "context" : "A line of research in CQA proposes the end-toend approach, where a single QA model jointly encodes the evidence document, the current question, and the whole conversation history (Huang et al., 2018; Yeh and Chen, 2019; Qu et al., 2019a).",
      "startOffset" : 179,
      "endOffset" : 237
    }, {
      "referenceID" : 38,
      "context" : "A line of research in CQA proposes the end-toend approach, where a single QA model jointly encodes the evidence document, the current question, and the whole conversation history (Huang et al., 2018; Yeh and Chen, 2019; Qu et al., 2019a).",
      "startOffset" : 179,
      "endOffset" : 237
    }, {
      "referenceID" : 26,
      "context" : "A line of research in CQA proposes the end-toend approach, where a single QA model jointly encodes the evidence document, the current question, and the whole conversation history (Huang et al., 2018; Yeh and Chen, 2019; Qu et al., 2019a).",
      "startOffset" : 179,
      "endOffset" : 237
    }, {
      "referenceID" : 35,
      "context" : "Another line of research proposes a pipeline approach that decomposes the CQA task into question rewriting (QR) and QA, to reduce the complexity of the task (Vakulenko et al., 2020).",
      "startOffset" : 157,
      "endOffset" : 181
    }, {
      "referenceID" : 31,
      "context" : "In addition, we find that our framework is also effective on a dataset CoQA (Reddy et al., 2019) that does not have the self-contained questions generated by human annotators.",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 17,
      "context" : "Neural-net-based QR models are commonly used to obtain selfcontained questions (Lin et al., 2020; Vakulenko et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 121
    }, {
      "referenceID" : 35,
      "context" : "Neural-net-based QR models are commonly used to obtain selfcontained questions (Lin et al., 2020; Vakulenko et al., 2020).",
      "startOffset" : 79,
      "endOffset" : 121
    }, {
      "referenceID" : 9,
      "context" : "The QR models are trained on the CANARD dataset (Elgohary et al., 2019), which consists of 40K pairs of original QuAC questions and their self-contained versions that are generated by human annotators.",
      "startOffset" : 48,
      "endOffset" : 71
    }, {
      "referenceID" : 29,
      "context" : "(2020), we adopt a T5-based sequence generator (Raffel et al., 2020) as our QR model, which achieves comparable performance with that of humans in QR.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 17,
      "context" : "4 For training and evaluating the QR model, we use the CANARD dataset following previous works on QR (Lin et al., 2020; Vakulenko et al., 2020).",
      "startOffset" : 101,
      "endOffset" : 143
    }, {
      "referenceID" : 35,
      "context" : "4 For training and evaluating the QR model, we use the CANARD dataset following previous works on QR (Lin et al., 2020; Vakulenko et al., 2020).",
      "startOffset" : 101,
      "endOffset" : 143
    }, {
      "referenceID" : 10,
      "context" : "with the adjustment of the softmax temperature (Fan et al., 2018; Xie et al., 2019).",
      "startOffset" : 47,
      "endOffset" : 83
    }, {
      "referenceID" : 36,
      "context" : "with the adjustment of the softmax temperature (Fan et al., 2018; Xie et al., 2019).",
      "startOffset" : 47,
      "endOffset" : 83
    }, {
      "referenceID" : 15,
      "context" : "Accordingly, we use consistency regularization (Laine and Aila, 2016; Xie et al., 2019), which enforces a model to make consistent predictions in response to transformations to the inputs.",
      "startOffset" : 47,
      "endOffset" : 87
    }, {
      "referenceID" : 36,
      "context" : "Accordingly, we use consistency regularization (Laine and Aila, 2016; Xie et al., 2019), which enforces a model to make consistent predictions in response to transformations to the inputs.",
      "startOffset" : 47,
      "endOffset" : 87
    }, {
      "referenceID" : 36,
      "context" : "In addition to using all probability values, we also sharpened the target distribution Pread θ̄ (at|d, q̃t, H̃t) by adjusting the temperature (Xie et al., 2019) to strengthen the QA model’s training signal.",
      "startOffset" : 142,
      "endOffset" : 160
    }, {
      "referenceID" : 5,
      "context" : "QuAC QuAC (Choi et al., 2018) comprises 100k QA pairs in information-seeking dialogues, where a student asks questions based on a topic with background information provided, and a teacher provides the answers in the form of text spans in",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 9,
      "context" : "CANARD CANARD (Elgohary et al., 2019) consists of 31K, 3K, and 5K QA pairs for training, development, and test sets, respectively.",
      "startOffset" : 14,
      "endOffset" : 37
    }, {
      "referenceID" : 31,
      "context" : "CoQA CoQA (Reddy et al., 2019) consists of 127K QA pairs and evidence documents in seven domains.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 13,
      "context" : ", 2019a,b) or training strategy such as multitask with rationale tagging (Ju et al., 2019) that are specialized in the CQA task or datasets.",
      "startOffset" : 73,
      "endOffset" : 90
    }, {
      "referenceID" : 11,
      "context" : "Several works demonstrated the effectiveness of the flow mechanism in CQA (Huang et al., 2018; Chen et al., 2019; Yeh and Chen, 2019).",
      "startOffset" : 74,
      "endOffset" : 133
    }, {
      "referenceID" : 4,
      "context" : "Several works demonstrated the effectiveness of the flow mechanism in CQA (Huang et al., 2018; Chen et al., 2019; Yeh and Chen, 2019).",
      "startOffset" : 74,
      "endOffset" : 133
    }, {
      "referenceID" : 38,
      "context" : "Several works demonstrated the effectiveness of the flow mechanism in CQA (Huang et al., 2018; Chen et al., 2019; Yeh and Chen, 2019).",
      "startOffset" : 74,
      "endOffset" : 133
    }, {
      "referenceID" : 9,
      "context" : "With the advent of a dataset consisting of selfcontained questions rewritten by human annotators (Elgohary et al., 2019), the pipeline approach has drawn attention as a promising method for CQA in recent days (Vakulenko et al.",
      "startOffset" : 97,
      "endOffset" : 120
    }, {
      "referenceID" : 35,
      "context" : ", 2019), the pipeline approach has drawn attention as a promising method for CQA in recent days (Vakulenko et al., 2020).",
      "startOffset" : 96,
      "endOffset" : 120
    }, {
      "referenceID" : 7,
      "context" : "The approach is particularly useful for the open-domain CQA or passage-retrieval (PR) tasks (Dalton et al., 2019; Ren et al., 2020; Anantha et al., 2020; Qu et al., 2020) since self-contained questions can be fed into existing non-conversational search engines",
      "startOffset" : 92,
      "endOffset" : 170
    }, {
      "referenceID" : 32,
      "context" : "The approach is particularly useful for the open-domain CQA or passage-retrieval (PR) tasks (Dalton et al., 2019; Ren et al., 2020; Anantha et al., 2020; Qu et al., 2020) since self-contained questions can be fed into existing non-conversational search engines",
      "startOffset" : 92,
      "endOffset" : 170
    }, {
      "referenceID" : 0,
      "context" : "The approach is particularly useful for the open-domain CQA or passage-retrieval (PR) tasks (Dalton et al., 2019; Ren et al., 2020; Anantha et al., 2020; Qu et al., 2020) since self-contained questions can be fed into existing non-conversational search engines",
      "startOffset" : 92,
      "endOffset" : 170
    }, {
      "referenceID" : 25,
      "context" : "The approach is particularly useful for the open-domain CQA or passage-retrieval (PR) tasks (Dalton et al., 2019; Ren et al., 2020; Anantha et al., 2020; Qu et al., 2020) since self-contained questions can be fed into existing non-conversational search engines",
      "startOffset" : 92,
      "endOffset" : 170
    }, {
      "referenceID" : 2,
      "context" : "Question Rewriting QR has been studied for augmenting training data (Buck et al., 2018; Sun et al., 2018; Zhu et al., 2019; Liu et al., 2020) or clarifying ambiguous questions (Min et al.",
      "startOffset" : 68,
      "endOffset" : 141
    }, {
      "referenceID" : 34,
      "context" : "Question Rewriting QR has been studied for augmenting training data (Buck et al., 2018; Sun et al., 2018; Zhu et al., 2019; Liu et al., 2020) or clarifying ambiguous questions (Min et al.",
      "startOffset" : 68,
      "endOffset" : 141
    }, {
      "referenceID" : 40,
      "context" : "Question Rewriting QR has been studied for augmenting training data (Buck et al., 2018; Sun et al., 2018; Zhu et al., 2019; Liu et al., 2020) or clarifying ambiguous questions (Min et al.",
      "startOffset" : 68,
      "endOffset" : 141
    }, {
      "referenceID" : 18,
      "context" : "Question Rewriting QR has been studied for augmenting training data (Buck et al., 2018; Sun et al., 2018; Zhu et al., 2019; Liu et al., 2020) or clarifying ambiguous questions (Min et al.",
      "startOffset" : 68,
      "endOffset" : 141
    }, {
      "referenceID" : 20,
      "context" : ", 2020) or clarifying ambiguous questions (Min et al., 2020).",
      "startOffset" : 42,
      "endOffset" : 60
    }, {
      "referenceID" : 16,
      "context" : "(2020) used a coreference-based model (Lee et al., 2018) and GPT-2 (Radford et al.",
      "startOffset" : 38,
      "endOffset" : 56
    }, {
      "referenceID" : 28,
      "context" : ", 2018) and GPT-2 (Radford et al., 2019) as QR models and tested the models in the QR and PR tasks.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 15,
      "context" : "Consistency Training Consistency regularization (Laine and Aila, 2016; Sajjadi et al., 2016) has been mainly explored in the context of semisupervised learning (SSL) (Chapelle et al.",
      "startOffset" : 48,
      "endOffset" : 92
    }, {
      "referenceID" : 33,
      "context" : "Consistency Training Consistency regularization (Laine and Aila, 2016; Sajjadi et al., 2016) has been mainly explored in the context of semisupervised learning (SSL) (Chapelle et al.",
      "startOffset" : 48,
      "endOffset" : 92
    }, {
      "referenceID" : 3,
      "context" : ", 2016) has been mainly explored in the context of semisupervised learning (SSL) (Chapelle et al., 2009; Oliver et al., 2018), which has been adopted in the textual domain as well (Miyato et al.",
      "startOffset" : 81,
      "endOffset" : 125
    }, {
      "referenceID" : 24,
      "context" : ", 2016) has been mainly explored in the context of semisupervised learning (SSL) (Chapelle et al., 2009; Oliver et al., 2018), which has been adopted in the textual domain as well (Miyato et al.",
      "startOffset" : 81,
      "endOffset" : 125
    }, {
      "referenceID" : 21,
      "context" : ", 2018), which has been adopted in the textual domain as well (Miyato et al., 2016; Clark et al., 2018; Xie et al., 2020).",
      "startOffset" : 62,
      "endOffset" : 121
    }, {
      "referenceID" : 6,
      "context" : ", 2018), which has been adopted in the textual domain as well (Miyato et al., 2016; Clark et al., 2018; Xie et al., 2020).",
      "startOffset" : 62,
      "endOffset" : 121
    }, {
      "referenceID" : 37,
      "context" : ", 2018), which has been adopted in the textual domain as well (Miyato et al., 2016; Clark et al., 2018; Xie et al., 2020).",
      "startOffset" : 62,
      "endOffset" : 121
    }, {
      "referenceID" : 22,
      "context" : "cable when only the labeled samples are available (Miyato et al., 2018; Jiang et al., 2019; Asai and Hajishirzi, 2020).",
      "startOffset" : 50,
      "endOffset" : 118
    }, {
      "referenceID" : 12,
      "context" : "cable when only the labeled samples are available (Miyato et al., 2018; Jiang et al., 2019; Asai and Hajishirzi, 2020).",
      "startOffset" : 50,
      "endOffset" : 118
    }, {
      "referenceID" : 1,
      "context" : "cable when only the labeled samples are available (Miyato et al., 2018; Jiang et al., 2019; Asai and Hajishirzi, 2020).",
      "startOffset" : 50,
      "endOffset" : 118
    }, {
      "referenceID" : 37,
      "context" : "The consistency regularization requires adding noise to the sample, which can be either discrete (Xie et al., 2020; Asai and Hajishirzi, 2020) or continuous (Miyato et al.",
      "startOffset" : 97,
      "endOffset" : 142
    }, {
      "referenceID" : 1,
      "context" : "The consistency regularization requires adding noise to the sample, which can be either discrete (Xie et al., 2020; Asai and Hajishirzi, 2020) or continuous (Miyato et al.",
      "startOffset" : 97,
      "endOffset" : 142
    }, {
      "referenceID" : 21,
      "context" : ", 2020; Asai and Hajishirzi, 2020) or continuous (Miyato et al., 2016; Jiang et al., 2019).",
      "startOffset" : 49,
      "endOffset" : 90
    }, {
      "referenceID" : 12,
      "context" : ", 2020; Asai and Hajishirzi, 2020) or continuous (Miyato et al., 2016; Jiang et al., 2019).",
      "startOffset" : 49,
      "endOffset" : 90
    } ],
    "year" : 2021,
    "abstractText" : "One of the main challenges in conversational question answering (CQA) is to resolve the conversational dependency, such as anaphora and ellipsis. However, existing approaches do not explicitly train QA models on how to resolve the dependency, and thus these models are limited in understanding human dialogues. In this paper, we propose a novel framework, EXCORD (Explicit guidance on how to resolve Conversational Dependency) to enhance the abilities of QA models in comprehending conversational context. EXCORD first generates self-contained questions that can be understood without the conversation history, then trains a QA model with the pairs of original and self-contained questions using a consistency-based regularizer. In our experiments, we demonstrate that EXCORD significantly improves the QA models’ performance by up to 1.2 F1 on QuAC (Choi et al., 2018), and 5.2 F1 on CANARD (Elgohary et al., 2019), while addressing the limitations of the existing approaches.1",
    "creator" : "LaTeX with hyperref"
  }
}