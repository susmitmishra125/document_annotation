{
  "name" : "2021.acl-long.412.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "CTFN: Hierarchical Learning for Multimodal Sentiment Analysis Using Coupled-Translation Fusion Network",
    "authors" : [ "Jiajia Tang", "Kang Li", "Xuanyu Jin", "Andrzej Cichocki", "Qibin Zhao", "Wanzeng Kong" ],
    "emails" : [ "Jxuanyu599}@163.com", "kongwanzeng}@hdu.edu.cn", "qibin.zhao}@riken.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5301–5311\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5301"
    }, {
      "heading" : "1 Introduction",
      "text" : "Sentiment analysis has witnessed many significant advances in the artificial intelligence community, in which text (Yadollahi et al., 2017), visual (Kahou et al., 2016), and acoustic (Luo et al., 2019) modalities are primarily employed to the related research\n∗Equal contribution †Corresponding author: Wanzeng Kong\nrespectively, allowing to exploit the human emotional characteristic and intention effectively (Deng et al., 2018). Intuitively, due to the consistency and complementarity among different sources, the joint representation attend to reason about multimodal messages, which are capable of boosting the performance of the specific task (Pan et al., 2016; Gebru et al., 2017; Al Hanai et al., 2018).\nMultimodal fusion procedure is to incorporate multiple knowledge for predicting a precise and proper outcome (Baltrušaitis et al., 2018). Historically, the existing fusion has been done generally by leveraging the model-agnostic process, considering the early fusion, late fusion, and hybrid fusion technique (Poria et al., 2017a). Among those, early fusion focussed on the concatenation of the unimodal presentation (D’mello and Kory, 2015). On the contrast, late fusion performs the integration at the decision level, by voting among all the model results (Shutova et al., 2016). As to the hybrid fusion, the output comes from the combination of the early fusion and unimodal prediction (Lan et al., 2014). Nevertheless, multimodal sentiment sequences often consists of unaligned properties, and the traditional fusion manners are failed to take the heterogeneity and misalignment into account carefully, which raises a question on investigating the more sophisticated models and estimating emotional information. (Tsai et al., 2020; Niu et al., 2017).\nRecently, Transformer-based multimodal fusion framework has been developed to address the above issues with the help of multi-head attention mechanism (Rahman et al., 2020; Le et al., 2019; Tsai et al., 2019). By introducing the standard Transformer network (Vaswani et al., 2017) as the basis, Tsai et al. (Tsai et al., 2019) captured the integrations directly from unaligned multimodal streams in an end-to-end fashion, latently adapted streams from one modality to another with the cross-modal\nattention module, regardless of the need for alignment. Furthermore, Wang et al. (Wang et al., 2020) proposed a parallel Transformer unit, allowing to explore the correlation between multimodal knowledge effectively. However, the decoder component of standard Transformer is employed to improve the translation performance, which may lead to some redundancy. Moreover, the explicit interaction among cross-modality translations were not considered. Essentially, compared to our CTFN, their architecture require access to all modalities as inputs for exploring multimodal interplay with the sequential fusion strategy, thus are rather sensitive in the case of multiple missing modalities.\nIn this paper, CTFN is proposed to model bidirectional interplay based on coupled learning, ensuring the robustness in respect to missing modalities. Specifically, the cyclic consistency constraint is proposed to improve the translation performance, allowing us directly to discard decoder and only embrace encoder of Transformer. This could contribute to a much lighter model. Thanks to the couple learning, CTFN is able to conduct bi-direction cross-modality intercorrelation parallelly. Take CTFN as a basis, a hierarchical architecture is established to exploit modality-guidance translation. Then, the convolution fusion block is presented to further explore the explicit correlation among the above translations. Importantly, based on the parallel fusion strategy, our CTFN model still provides flexibility and robustness when considering only one input modality.\nFor evaluation, CTFN was verified on two multimodal sentiment benchmarks, CMU-MOSI (Zadeh\net al., 2016) and MELD (Poria et al., 2019). The experiments demonstrate that CTFN could achieve the state-of-the-art or even better performance compared to the baseline models. We also provide several extended ablation studies, to investigate intrinsic properties of the proposed model."
    }, {
      "heading" : "2 Related Work",
      "text" : "The off-the-shelf multimodal sentiment fusion architecture comprises two leading groups: translation-based and non-translation based model.\nNon-translation based: Recently, RNN-based models, considering GRU and LSTM, have received significant advances in exploiting the context-aware information across the data (Yang et al., 2016; Agarwal et al., 2019). bc − LSTM (Poria et al., 2017b) and GME − LSTM (Chung et al., 2014) presented a LSTM-based model to retrieve contextual information, where the unimodal features are concatenated into a unit one as the input information. Similarly, MELD − base (Poria et al., 2019) leveraged the concatenation of audio and textual features on the input layer, and employed GRU to model sentimental context. In contrast, CHFusion (Majumder et al., 2018) employed the RNN-based hierarchical structure to draw fine-grained local correlations among the modalities, and the empirical evidence illustrates superior advances compared to the simple concatenation of unimodal presentation. On the basis of RNN, MMMU−BA (Ghosal et al., 2018) further employed multimodal attention block to absorb the contribution of all the neighboring utterances, which demonstrates that the attention mechanism\ncan utilize the neighborhood contribution for integrating the contextual information. However, all these methods are suitable for the low-level presentation within the single modality with a nontranslation manner, which may be easily sensitive to the noisy terms and missing information in the sources.\nTranslation-based model: Inspired by the recent success of sequence to sequence (Seq2Seq) models (Lin et al., 2019; ?) in machine translation, (Pham et al., 2019) and (Pham et al., 2018) presented multimodal fusion model via the essential insight that translates from a source modality to a target modality, which is able to capture much more robust associations across multiple modalities. MCTN model incorporated a cyclic translation module to retrieve the robust joint representation between modalities in a sequential manner, e.g., the language information firstly associated with the visual modality, and latently translated into the acoustic modality. Compared with the MCTN , Seq2Seq2Sent introduced a hierarchical fusion model using the Seq2Seq methods. For the first layer, the joint representation of a modality pair is treated as an input sequence for the next Seq2Seq layer in an attempt to decode the third modality. Inspired by the success of the Transformer-based model, Tsai et al. introduced a directional crossmodality attention module to extend the standard Transformer network. Follow the basic idea of Tsai et al., Wang et al. provided a novel multimodal fusion cell which is comprised of two standard Transformers, embracing the association with a modality pair during the forward and backward translation implicitly. However, all existing models adopt sequential multimodal fusion architecture,\nwhich requires all modalities as input, therefore they can be sensitive to the case of multiple missing modalities. Moreover, the explicit interactions among cross-modality translations were not considered."
    }, {
      "heading" : "3 Methodology",
      "text" : "In this section, we firstly present CTFN (Figure 2), which is capable of exploring bi-direction crossmodality translation via couple learning. On the basis of CTFN, a hierarchical architecture is established to exploit multiple bi-direction translations, leading to double multimodal fusing embeddings (Figure 4). Then, the convolutional fusion block (Figure 3) is applied to further highlight explicit correlation among cross-modality translations."
    }, {
      "heading" : "3.1 Preliminaries",
      "text" : "The two benchmarks consist of three modalities, audio, video and textual modality. Specifically, the above utterance-level modalities are denoted as Xa ∈ RTa×da , Xv ∈ RTv×dv and Xt ∈ RTt×dt , respectively. The number of utterances is presented as Ti(i ∈ {a, v, t}), and di(i ∈ {a, v, t}) stands for the dimension of the unimodality features."
    }, {
      "heading" : "3.2 Coupled-Translation Fusion Network",
      "text" : "For simplicity, we consider two unimodality presentation Xa and Xv explored from audio (A) and video (V), respectively. In the primal process of CTFN, we focus on learning a directional translator TranA→V (Xa,Xv) for translating the modality audio to video. Then, the dual process aims to learn an inverse directional translator TranV→A(Xv,Xa), allowing for the translation from modality video to audio. Inspired by the suc-\ncess of Transformer in Natural Language Processing, the encoder of Transformer is introduced to our model as the translation block, which is an efficient and adaptive manner for retrieving the long-range interplay along the temporal domain. Importantly, the cyclic consistency constraint is presented to improve the translation performance. And due to the couple learning, CTFN is able to combine primal and dual process into a coupled structure, ensuring the robustness in respect to missing modalities.\nFor the primal task, Xa ∈ RTa×da is firstly delivered to a densely connected layer for receiving a linear transformation Xa ∈ RTa×La , where La is the output dimension of the linear layer. And the corresponding query matrix, key matrix and value matrix are denoted as Qa = XaWQa ∈ RTa×La , Ka = XaWKa ∈ RTa×La , Va = XaWVa ∈ R Ta×La , where WQa ∈ RLa×La , WKa ∈ R La×La and WVa ∈ RLa×La are weight matrixes. The translation from modality A to V is performed as Xv , = TranA→V (Xa,Xv) ∈ RTa×Lv , where Xv , refers to the fake Xv, and √ La is the scale coefficient. Note that the input Xa is directly delivered to the translation process, while the input Xv is used to analyze the difference between real data Xv and fake output Xv ,. Subsequently, Xv , is passed through the TranV→A, leading to the reconstruct output Xa, = TranV→A(Xv ,, Xa), and the Xa is only used to calculate the diversity between the real and reconstruct data. Xv , = TranA→V (Xa,Xv)\n= softmax( QaKa\nT\n√ La )Va\n= softmax( XaWQaWKa\nTXa T\n√ La\n)XaWVa . (1)\nAnalogously, in the dual process, Xv ∈ R Tv×Lv is captured based on the input Xv ∈\nR Tv×dv , Xa, = TranV→A(Xv,Xa) ∈ R Ta×La , and reconstructed representation Xv , = TranA→V (Xa,,Xv) ∈ RTv×Lv . Essentially, TranA→V and TranV→A are implemented by several sequential encoder layers. During the translation period, we hypothesize that intermediate encoder layer contains the cross-modality fusion information and effectively balance the contribution of two modalities. Hence, the output of the middle encoder layer TranA→V [L/2] and TranV→A[L/2] stand for the multimodal fusion knowledge, where L refers to the number of layers, and when L is odd number, then L = L+ 1.\nAs for the model reward, the primal process has an immediate reward rp = ‖Xa − TranV→A(Xv ,)‖F , and the dual step related reward is rd = ‖Xv − TranA→V (Xa,)‖F , indicating the similarity between the real data and the reconstructed output of the translator. For simplicity, a linear transformation module is adopted to combine the primal and dual step reward into a total model reward, e.g., rall = αrp + (1 − α)rd, where α is employed to balance the contribution between dual and primal block. Additionally, the loss functions utilized in the coupled-translation multimodal fusion block are defined as follows:\nlA→V (Xa,Xv) =‖TranA→V (Xa,Xv)−Xv‖F+ ‖TranA→V (Xa,,Xv)−Xv‖F lV →A(Xv,Xa) =‖TranV →A(Xv,Xa)−Xa‖F+ ‖TranV →A(Xv ,,Xa)−Xa‖F\nlA↔V = αlA→V (Xa,Xv) + (1− α)lV →A(Xv,Xa), (2)\nwhere lA→V (Xa,Xv) and lV→A(Xv,Xa) refer to the training loss of the primal and dual translator respectively, and lA↔V stands for the loss of bi-directional translator unit. Essentially, when the training process of all coupled-translation blocks\nare finished, our model only needs one input modality at predicting time, without the help of target modalities.\nIndeed, lA↔V indicates the cycle-consistency constraint in our couple learning model. The cycleconsistency is well-known, which refers to combination of forward and backward cycle-consistency. However, our goal is to solve missing modality problem in multi-modal learning, which cannot be achieved by applying cycle-consistency straightforward. This is because that introducing this strict cycle-consistency to CTFN fail to effectively associate primal task with dual task of the couple learning model. To solve this problem, we relaxed constraint of original cycle-consistency by using a parameter ‘α’ to balance the contribution of forward and backward cycle-consistency, leading to a much more flexible cycle-consistency. Thanks to the great flexibility of new proposed cycle-consistency, we could adaptively and adequately associate primal with dual task, resulting in much more balanced consistency among modalities."
    }, {
      "heading" : "3.3 Multimodal convolutional fusion block",
      "text" : "Based on CTFN, each modality is treated as the source moment for (M − 1) times, which means that each modality holds (M − 1) directional translations, {Tranmodality source→modality m}Mm=1, where M refers to the total number of modalities. For instance, given modality audio, we can retrieve the following two modality-guidance translations:\n[Trana→vL/2, video,] = Trana→v(audio, video)\n[Trana→tL/2, text,] = Trana→t(audio, text). (3)\nNote that audio plays a key role in different crossmodality translations, and provides the strong guid-\nance for capturing various cross-modality interplay. For blending the contribution of source modality (audio) effectively, a convolution fusion block is incorporated to explore explicit and local correlation among modality-guidance translations.\nInitially, the two cross-modality intermediate correlations Tranaudio→vedioL/2 and Tranaudio→textL/2 are concatenated along the temporal domain into a unit representation, where the size of time sequence is equal (Ta = Tv = Tt), thus the concatenation is of size Ta × (Lv + Lt):\nZconcat = Trana→vL/2 ⊕ Trana→tL/2. (4) Subsequently, the temporal convolution is employed to further retrieve explicit interactions among cross-modality translations. Specifically, we adopt a 1D temporal convolutional layer to exploit the local patten in a light manner:\nẐconcat = Conv1D(Zconcat,Kconcat) ∈ RTa×Ld , (5)\nwhere Kconcat is the size of the convolutional kernel, and Ld is the length of the cross-modality integration dimension. The temporal kernel is used to perform the convolutional operation along the feature dimension, allowing to further exploit local interplay among cross-modality translations. That is to say, the local interplay fully exploits the contribution from modality-guidance translations."
    }, {
      "heading" : "3.4 Hierarchical Architecture",
      "text" : "On the basis of CTFN and convolutional multimodal fusion network, a hierarchical architecture was proposed for exploiting multiple bidirection translations, leading to double multimodal fusing embeddings. For instance, given M modalities, our model could achieve double\nC2M embeddings. As illustrated in Figure 4, the proposed architecture consists of three CTFNs TranA↔V , TranA↔T and TranV↔T . Considering the contribution of the guidance (source) modality, the modality-guidance translations are denoted as TranT←A→V = [Tran L/2 A→V , T ran L/2 A→T ], TranT←V→A = [Tran L/2 V→T , T ran L/2 V→A], and TranA←T→V = [Tran L/2 T→A, T ran L/2 T→V ], respectively. Similarly, when taking the contribution of target modalities into account, corresponding modality-guidance translations are illustrated as TranT→A←V = [Tran L/2 V→A, T ran L/2 T→A], TranT→V←A = [Tran L/2 T→V , T ran L/2 A→V ], and TranA→T←V = [Tran L/2 A→T , T ran L/2 V→T ], respectively. Subsequently, the convolutional fusion layer is used to further exploit explicit local interplay among modality-guidance translations associated with the same source/target modality, which can fully leverage the contribution of source/target modality.\nEssentially, as demonstrated in Figure 4, our model has “12+1” loss constraints in total, which includes 3 CTFNs, each one has 4 training loss (primal & dual translator training loss), and 1 classification loss. However, we do not need to balance these targets together, which is achieved by our training strategy that 3 CTFNs are trained individually. For each CTFN, one hyper-parameter ‘α’ is introduced to balance the loss of primal translator and dual translator, and this hyper-parameter is shared among 3 CTFNs. Hence, 3 CTFNs only need 1 hyper-parameter to balance the training loss, which is easy to be tuned. The classification loss\nis used for training the classifier on the 3 CTFNs’s outputs."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental setups",
      "text" : "Datasets. CMU-MOSI consists of 2199 opinion video clips from online sharing websites (e.g., YouTube). Each utterance of the video clip is annotated with a specific sentimental label of positive or negative in the range scale of [−3,+3]. The corresponding training, validation, and testing size refer to division set (1284, 229, 686). Additionally, the same speaker will not appear in both training and testing sets, allowing to exploit speakerindependent joint representations. MELD dataset contains 13000 utterances from the famous TVseries Friends. Each utterance is annotated with emotion and sentiment labels, considering 7 classes of emotion tag (anger, disgust, fear, joy, neutral, sadness, and surprise) and 3 sentimental tendency levels (positive, neutral, and negative). Hence, the original dataset can be denoted as MELD (Sentiment) and MELD (Emotion) with respect to the data annotation, we only verified our model on the MELD (Sentiment). Note that CMU-MOSI and MELD are the public and widely-used datasets which have been aligned and segmented already.\nFeatures. For CMU-MOSI dataset, we adopt the same preprocess manner mentioned in MFN (Zadeh et al., 2018) to extract the low-level representation of multimodal data, and synchronized at the utterance level that in consistent with text modality. For MELD benchmark, we follow\nthe related work of MELD, in which the 300- dimensional GloVe (Pennington et al., 2014) text vectors are fed into a 1D-CNN (Chen et al., 2017) layer to extract textual representation, and audiobased descriptors are explored with the popular toolkit openSMILE (Eyben et al., 2010), while visual features were not taken into account for the sentiment analysis.\nComparisons. We introduced the translationbased and non-translation based models to this work as the baselines. Translation-based: Multimodal Cyclic Translation Network (MCTN), Sequence to Sequence for Sentiment (Seq2Seq2Sent), Multimodal Sentiment Analysis with Transformer (TransModality). And non-translation based: bidirectional contextual LSTM (bc-LSTM), Gated Embedding LSTM (GME-LSTM), Multimodal EmotionLines Dataset baseline model (MELDbase), Hierarchical Fusion with Context Modeling (CHFusion), Multi-Modal Multi-Utterance - Bi-Modal Attention (MMMU-BA)."
    }, {
      "heading" : "4.2 Experiment results and analysis",
      "text" : "Performance comparison with state-of-the-art models. Firstly, we analyzed the performance between state-of-the-art baselines and our proposed model. The bottom rows in Table 1 indicate the effectiveness and superiority of our model. Particularly, on CMU-MOSI dataset, CTFN exceeded the previous best TransModality on (video, audio) by a margin of 4.51. Additionally, on MELD (Sentiment) dataset, the empirical improvement of CTFN was 0.78. It is interesting to note that the improvement of (video, audio) is more significant than (text, video) and (text, audio). This implies that coupled-translation structure is capable of decreasing the risk of interference between video and audio efficiently, and further leverage the explicit con-\nsistency between auxiliary features. As for (text, audio, video), CTFN exceeds the previous best TransModality with an improvement of 0.06, leading to a comparable performance. Indeed, for the same tri-modality fusion task, TransModality needs 4 encoders and 4 decoders, while CTFN only requires 6 encoders. It should be emphasized that the cyclic consistency mechanism could contribute to a much lighter model, as well as the more effective bi-directional translation. In addition, compared to the bi-modality setting, the tri-modality case achieved the improvement of 0.61, indicating the benefits brought by hierarchical architecture and convolution fusion.\nEffect of CTFN with missing modalities. Existing translation-based manners focus only on the join representation between modalities, and ignore the potential occurrence of the missing modalities. Therefore, we analyzed how does missing modality may affect the final performance of CTFN and the sequential translation-based model SeqSeq2Sent. Note that SeqSeq2Sent only employs LSTM to analyze uni-modality rather than the translationbased method. Specifically, we take the hierarchical architecture combined with three CTFNs as\nthe testing model. From the Table 2, we observe that compared to the setting (text, audio, video), the text-based settings {(audio, video, text), (audio, video, text), (audio,video, text)} seem to reach the comparable result with only a relatively small performance drop. On the contrast, when text was missing, the model has a relatively large performance drop, which implies that language modality contains much more discriminative sentimental message than audio and video, leading to the significantly better performance. Essentially, the performance of (audio,video, text) demonstrates that hierarchical CTFN is able to maintain robustness and consistency when considering only a single input modality. In other words, the cyclic consistency mechanism allows CTFN to fully exploit the crossmodality interplay, thus hierarchical CTFN could transmit the single modality to various pre-trained CTFNs for retrieving multimodal fusion message.\nEffect of the translation direction. In this paper, we propose a coupled-translation block, which aims to embrace fusion messages from the bidirectional translation process. Hence, we are interested to investigate the impact of translation direction. Figure 6 depicts the performance of various translations, considering (audio, text), (audio, video), and (text, video) translation. For the (audio, text) instance, the translation text→audio achieves better performance than audio→text . Similarly, the translation text→video surpasses the result of video→text. However, the performance of audio→video and video→audio seems to be quite similar. The superiority of text→video and text→audio may demonstrate that text modal-\nity possesses much more sentimental information. Moreover, the prospects of text modality allow text to be the strong backbone of the translation.\nEffect of the translator layer. As each translator is comprised of several sequential encoder layers. In this part, we assume that the output representation of a specific layer may affect the performance of the proposed model. For simplicity, we perform the related task on CMU-MOSI with the setting of (a, v, t), as well as the (t, a) on MELD (Sentiment). Initially, we retrieve the embedding from the specific layer, where the layer ranges from 1 to L (L is the total number of the layer). In Figure 7, it is interesting to note that the model reaches the peak value at layer 5 on CMU-MOSI, which means that the output of the fifth layer embraces the most discriminative fusion message. In comparison, on MELD (Sentiment), the model achieves the best performance at layer 1, which may imply that the simple translator associated with only one layer is able to capture the joint representation for the simple case (text, audio). In conclusion, the lower encoder layer may involve low-level characteristics of interplay, while the higher encoder layer may embrace the explicit messages. Additionally, the output of the specific layer of the encoder lies on the corresponding task and dataset. We tried also (text, audio) on MOSI, and CTFN maximizes the performance at layer 3. Compared to (text, audio, video), (text, audio) is the relatively simple case, thus the lower encoder layer may is sufficient to demonstrate the interaction between text and audio.\nEffect of concatenation strategy of translation. In our work, those translations associated\nwith the same guidance (source) modality are concatenated along the feature domain. As each modality serves as the source and target modality in turn, we are interested to analyze the impact of the distinct concatenation strategies, e.g., concatenate the translations via the same source or target modality. As shown in Figure 8, it is obvious to find that audio-based target concatenation [(T→A) ⊕ (V→A)] performs significantly better than [(A→T)⊕(A→V)] with a large margin. Analogously, video-based target concatenation [(T→V)⊕(A→V)] works better than [(V→A)⊕(V→T)]. The above performance may indicate that joint presentation is able to achieve the significantly improved benefits with the help of guidance modality text. In conclusion, when text modality serves as the guidance modality, which may effectively leverage the contribution from audio and video, and further boost the task performance in a robust and consistent way."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we present a novel hierarchical multimodal fusion architecture using coupled-translation fusion network (CTFN). Initially, CTFN is utilized for exploiting bi-directional interplay via couple learning, ensuring the robustness in respect to missing modalities. Specifically, the cyclic mechanism directly discards the decoder and only embraces the encoder of Transformer, which could\ncontribute to a much lighter model. Due to the couple learning, CTFN is able to conduct bi-direction cross-modality intercorrelation parallelly. Based on CTFN, a hierarchical architecture is further established to exploit multiple bi-direction translations, leading to double multimodal fusing embeddings compared with traditional translation methods. Additionally, a multimodal convolutional fusion block is employed to further explore the complementarity and consistency between crossmodality translations. Essentially, the parallel fusion strategy allows the model maintains robustness and flexibility when considering only one input modality. CTFN was verified on two public multimodal sentiment benchmarks, the experiments demonstrate the effectiveness and flexibility of CTFN, and CTFN achieves state-of-theart or comparable performance on CMU-MOSI and MELD (Sentiment). For future work, we like to evaluate CTFN on more multimodal fusion tasks. The source code can be obtained from https://github.com/deepsuperviser/CTFN."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We sincerely thank the anonymous reviewers for their insightful comments and valuable suggestions. This work was supported by National Key R&D Program of China for Intergovernmental International Science and Technology Innovation Cooperation Project (2017YFE0116800), National Natural Science Foundation of China (U20B2074, U1909202), Science and Technology Program of Zhejiang Province (2018C04012), Key Laboratory of Brain Machine Collaborative Intelligence of Zhejiang Province (2020E10010), JSPS KAKENHI (Grant No. 20H04249), and supported by the Ministry of Education and Science of the Russian Federation (Grant 14.756.31.0001)."
    } ],
    "references" : [ {
      "title" : "Multimodal sentiment analysis via rnn variants",
      "author" : [ "Ayush Agarwal", "Ashima Yadav", "Dinesh Kumar Vishwakarma." ],
      "venue" : "2019 IEEE International Conference on Big Data, Cloud Computing, Data Science & Engineering (BCD), pages 19–23.",
      "citeRegEx" : "Agarwal et al\\.,? 2019",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2019
    }, {
      "title" : "Detecting depression with audio/text sequence modeling of interviews",
      "author" : [ "Tuka Al Hanai", "Mohammad M Ghassemi", "James R Glass." ],
      "venue" : "Interspeech, pages 1716–1720.",
      "citeRegEx" : "Hanai et al\\.,? 2018",
      "shortCiteRegEx" : "Hanai et al\\.",
      "year" : 2018
    }, {
      "title" : "Multimodal machine",
      "author" : [ "Tadas Baltrušaitis", "Chaitanya Ahuja", "LouisPhilippe Morency" ],
      "venue" : null,
      "citeRegEx" : "Baltrušaitis et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Baltrušaitis et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving sentiment analysis via sentence type classification using bilstm-crf and cnn",
      "author" : [ "Tao Chen", "Ruifeng Xu", "Yulan He", "Xuan Wang." ],
      "venue" : "Expert Systems with Applications, 72:221–230.",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "author" : [ "Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1412.3555.",
      "citeRegEx" : "Chung et al\\.,? 2014",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2014
    }, {
      "title" : "Multimodal utterance-level affect analysis using visual, audio and text features",
      "author" : [ "Didan Deng", "Yuqian Zhou", "Jimin Pi", "Bertram E Shi." ],
      "venue" : "arXiv preprint arXiv:1805.00625.",
      "citeRegEx" : "Deng et al\\.,? 2018",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2018
    }, {
      "title" : "A review and meta-analysis of multimodal affect detection systems",
      "author" : [ "Sidney K D’mello", "Jacqueline Kory" ],
      "venue" : "ACM Computing Surveys (CSUR),",
      "citeRegEx" : "D.mello and Kory.,? \\Q2015\\E",
      "shortCiteRegEx" : "D.mello and Kory.",
      "year" : 2015
    }, {
      "title" : "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "author" : [ "Florian Eyben", "Martin Wöllmer", "Björn Schuller." ],
      "venue" : "Proceedings of the 18th ACM International Conference on Multimedia, pages 1459–1462.",
      "citeRegEx" : "Eyben et al\\.,? 2010",
      "shortCiteRegEx" : "Eyben et al\\.",
      "year" : 2010
    }, {
      "title" : "Audio-visual speaker diarization based on spatiotemporal bayesian fusion",
      "author" : [ "Israel D Gebru", "Sileye Ba", "Xiaofei Li", "Radu Horaud." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(5):1086–1099.",
      "citeRegEx" : "Gebru et al\\.,? 2017",
      "shortCiteRegEx" : "Gebru et al\\.",
      "year" : 2017
    }, {
      "title" : "Contextual inter-modal attention for multi-modal sentiment analysis",
      "author" : [ "Deepanway Ghosal", "Md Shad Akhtar", "Dushyant Chauhan", "Soujanya Poria", "Asif Ekbal", "Pushpak Bhattacharyya." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods",
      "citeRegEx" : "Ghosal et al\\.,? 2018",
      "shortCiteRegEx" : "Ghosal et al\\.",
      "year" : 2018
    }, {
      "title" : "Emonets: Multimodal deep learning",
      "author" : [ "Samira Ebrahimi Kahou", "Xavier Bouthillier", "Pascal Lamblin", "Caglar Gulcehre", "Vincent Michalski", "Kishore Konda", "Sébastien Jean", "Pierre Froumenty", "Yann Dauphin", "Nicolas Boulanger-Lewandowski" ],
      "venue" : null,
      "citeRegEx" : "Kahou et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kahou et al\\.",
      "year" : 2016
    }, {
      "title" : "Multimedia classification and event detection using double fusion",
      "author" : [ "Zhen-Zhong Lan", "Lei Bao", "Shoou-I Yu", "Wei Liu", "Alexander G Hauptmann." ],
      "venue" : "Multimedia tools and applications, 71(1):333–347.",
      "citeRegEx" : "Lan et al\\.,? 2014",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2014
    }, {
      "title" : "Multimodal transformer networks for end-toend video-grounded dialogue systems",
      "author" : [ "Hung Le", "Doyen Sahoo", "Nancy Chen", "Steven Hoi." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5612–5623,",
      "citeRegEx" : "Le et al\\.,? 2019",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2019
    }, {
      "title" : "Dual-modality seq2seq network for audiovisual event localization",
      "author" : [ "Yan-Bo Lin", "Yu-Jhe Li", "Yu-Chiang Frank Wang." ],
      "venue" : "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2019, Brighton, United Kingdom,",
      "citeRegEx" : "Lin et al\\.,? 2019",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "Audio sentiment analysis by heterogeneous signal features learned from utterance-based parallel neural network",
      "author" : [ "Ziqian Luo", "Hua Xu", "Feiyang Chen." ],
      "venue" : "AffCon@ AAAI.",
      "citeRegEx" : "Luo et al\\.,? 2019",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2019
    }, {
      "title" : "Multimodal sentiment analysis using hierarchical fusion with context modeling",
      "author" : [ "Navonil Majumder", "Devamanyu Hazarika", "Alexander Gelbukh", "Erik Cambria", "Soujanya Poria." ],
      "venue" : "Knowledge-Based Systems, 161:124–133.",
      "citeRegEx" : "Majumder et al\\.,? 2018",
      "shortCiteRegEx" : "Majumder et al\\.",
      "year" : 2018
    }, {
      "title" : "Hierarchical multimodal LSTM for dense visual-semantic embedding",
      "author" : [ "Zhenxing Niu", "Mo Zhou", "Le Wang", "Xinbo Gao", "Gang Hua." ],
      "venue" : "IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages",
      "citeRegEx" : "Niu et al\\.,? 2017",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2017
    }, {
      "title" : "Jointly modeling embedding and translation to bridge video and language",
      "author" : [ "Yingwei Pan", "Tao Mei", "Ting Yao", "Houqiang Li", "Yong Rui." ],
      "venue" : "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June",
      "citeRegEx" : "Pan et al\\.,? 2016",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2016
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha,",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "author" : [ "Hai Pham", "Paul Pu Liang", "Thomas Manzini", "LouisPhilippe Morency", "Barnabás Póczos." ],
      "venue" : "The Thirty-Third AAAI Conference on Artificial",
      "citeRegEx" : "Pham et al\\.,? 2019",
      "shortCiteRegEx" : "Pham et al\\.",
      "year" : 2019
    }, {
      "title" : "Seq2Seq2Sentiment: Multimodal sequence to sequence models for sentiment analysis",
      "author" : [ "Hai Pham", "Thomas Manzini", "Paul Pu Liang", "Barnabás Poczós." ],
      "venue" : "Proceedings of Grand Challenge and Workshop on Human Multimodal Language",
      "citeRegEx" : "Pham et al\\.,? 2018",
      "shortCiteRegEx" : "Pham et al\\.",
      "year" : 2018
    }, {
      "title" : "A review of affective computing: From unimodal analysis to multimodal fusion",
      "author" : [ "Soujanya Poria", "Erik Cambria", "Rajiv Bajpai", "Amir Hussain." ],
      "venue" : "Information Fusion, 37:98–125.",
      "citeRegEx" : "Poria et al\\.,? 2017a",
      "shortCiteRegEx" : "Poria et al\\.",
      "year" : 2017
    }, {
      "title" : "Context-dependent sentiment analysis in user-generated videos",
      "author" : [ "Soujanya Poria", "Erik Cambria", "Devamanyu Hazarika", "Navonil Majumder", "Amir Zadeh", "Louis-Philippe Morency." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Poria et al\\.,? 2017b",
      "shortCiteRegEx" : "Poria et al\\.",
      "year" : 2017
    }, {
      "title" : "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "author" : [ "Soujanya Poria", "Devamanyu Hazarika", "Navonil Majumder", "Gautam Naik", "Erik Cambria", "Rada Mihalcea." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the As-",
      "citeRegEx" : "Poria et al\\.,? 2019",
      "shortCiteRegEx" : "Poria et al\\.",
      "year" : 2019
    }, {
      "title" : "Integrating multimodal information in large pretrained transformers",
      "author" : [ "Wasifur Rahman", "Md Kamrul Hasan", "Sangwu Lee", "AmirAli Bagher Zadeh", "Chengfeng Mao", "LouisPhilippe Morency", "Ehsan Hoque." ],
      "venue" : "Proceedings of the 58th Annual",
      "citeRegEx" : "Rahman et al\\.,? 2020",
      "shortCiteRegEx" : "Rahman et al\\.",
      "year" : 2020
    }, {
      "title" : "Black holes and white rabbits: Metaphor identification with visual features",
      "author" : [ "Ekaterina Shutova", "Douwe Kiela", "Jean Maillard." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Shutova et al\\.,? 2016",
      "shortCiteRegEx" : "Shutova et al\\.",
      "year" : 2016
    }, {
      "title" : "Multimodal transformer for unaligned multimodal language sequences",
      "author" : [ "Yao-Hung Hubert Tsai", "Shaojie Bai", "Paul Pu Liang", "J. Zico Kolter", "Louis-Philippe Morency", "Ruslan Salakhutdinov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the",
      "citeRegEx" : "Tsai et al\\.,? 2019",
      "shortCiteRegEx" : "Tsai et al\\.",
      "year" : 2019
    }, {
      "title" : "Interpretable multimodal routing for human multimodal language",
      "author" : [ "Yao-Hung Hubert Tsai", "Martin Q Ma", "Muqiao Yang", "Ruslan Salakhutdinov", "Louis-Philippe Morency." ],
      "venue" : "arXiv preprint arXiv:2004.14198.",
      "citeRegEx" : "Tsai et al\\.,? 2020",
      "shortCiteRegEx" : "Tsai et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Transmodality: An end2end fusion method with transformer for multimodal sentiment analysis",
      "author" : [ "Zilong Wang", "Zhaohong Wan", "Xiaojun Wan." ],
      "venue" : "WWW ’20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020, pages 2514–2520. ACM /",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Current state of text sentiment analysis from opinion to emotion mining",
      "author" : [ "Ali Yadollahi", "Ameneh Gholipour Shahraki", "Osmar R Zaiane." ],
      "venue" : "ACM Computing Surveys (CSUR), 50(2):1–33.",
      "citeRegEx" : "Yadollahi et al\\.,? 2017",
      "shortCiteRegEx" : "Yadollahi et al\\.",
      "year" : 2017
    }, {
      "title" : "Multilayer and multimodal fusion of deep neural networks for video classification",
      "author" : [ "Xiaodong Yang", "Pavlo Molchanov", "Jan Kautz." ],
      "venue" : "Proceedings of the 2016 ACM Conference on Multimedia Conference, MM 2016, Amsterdam, The Nether-",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Memory fusion network for multiview sequential learning",
      "author" : [ "Amir Zadeh", "Paul Pu Liang", "Navonil Mazumder", "Soujanya Poria", "Erik Cambria", "Louis-Philippe Morency." ],
      "venue" : "Proceedings of the Thirty-Second AAAI Conference on Artificial Intel-",
      "citeRegEx" : "Zadeh et al\\.,? 2018",
      "shortCiteRegEx" : "Zadeh et al\\.",
      "year" : 2018
    }, {
      "title" : "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "author" : [ "Amir Zadeh", "Rowan Zellers", "Eli Pincus", "LouisPhilippe Morency." ],
      "venue" : "arXiv preprint arXiv:1606.06259.",
      "citeRegEx" : "Zadeh et al\\.,? 2016",
      "shortCiteRegEx" : "Zadeh et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 30,
      "context" : "Sentiment analysis has witnessed many significant advances in the artificial intelligence community, in which text (Yadollahi et al., 2017), visual (Kahou et al.",
      "startOffset" : 115,
      "endOffset" : 139
    }, {
      "referenceID" : 10,
      "context" : ", 2017), visual (Kahou et al., 2016), and acoustic (Luo et al.",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 14,
      "context" : ", 2016), and acoustic (Luo et al., 2019) modalities are primarily employed to the related research",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 5,
      "context" : "∗Equal contribution †Corresponding author: Wanzeng Kong respectively, allowing to exploit the human emotional characteristic and intention effectively (Deng et al., 2018).",
      "startOffset" : 151,
      "endOffset" : 170
    }, {
      "referenceID" : 17,
      "context" : "messages, which are capable of boosting the performance of the specific task (Pan et al., 2016; Gebru et al., 2017; Al Hanai et al., 2018).",
      "startOffset" : 77,
      "endOffset" : 138
    }, {
      "referenceID" : 8,
      "context" : "messages, which are capable of boosting the performance of the specific task (Pan et al., 2016; Gebru et al., 2017; Al Hanai et al., 2018).",
      "startOffset" : 77,
      "endOffset" : 138
    }, {
      "referenceID" : 2,
      "context" : "Multimodal fusion procedure is to incorporate multiple knowledge for predicting a precise and proper outcome (Baltrušaitis et al., 2018).",
      "startOffset" : 109,
      "endOffset" : 136
    }, {
      "referenceID" : 21,
      "context" : "Historically, the existing fusion has been done generally by leveraging the model-agnostic process, considering the early fusion, late fusion, and hybrid fusion technique (Poria et al., 2017a).",
      "startOffset" : 171,
      "endOffset" : 192
    }, {
      "referenceID" : 6,
      "context" : "Among those, early fusion focussed on the concatenation of the unimodal presentation (D’mello and Kory, 2015).",
      "startOffset" : 85,
      "endOffset" : 109
    }, {
      "referenceID" : 25,
      "context" : "On the contrast, late fusion performs the integration at the decision level, by voting among all the model results (Shutova et al., 2016).",
      "startOffset" : 115,
      "endOffset" : 137
    }, {
      "referenceID" : 11,
      "context" : "As to the hybrid fusion, the output comes from the combination of the early fusion and unimodal prediction (Lan et al., 2014).",
      "startOffset" : 107,
      "endOffset" : 125
    }, {
      "referenceID" : 24,
      "context" : "Recently, Transformer-based multimodal fusion framework has been developed to address the above issues with the help of multi-head attention mechanism (Rahman et al., 2020; Le et al., 2019; Tsai et al., 2019).",
      "startOffset" : 151,
      "endOffset" : 208
    }, {
      "referenceID" : 12,
      "context" : "Recently, Transformer-based multimodal fusion framework has been developed to address the above issues with the help of multi-head attention mechanism (Rahman et al., 2020; Le et al., 2019; Tsai et al., 2019).",
      "startOffset" : 151,
      "endOffset" : 208
    }, {
      "referenceID" : 26,
      "context" : "Recently, Transformer-based multimodal fusion framework has been developed to address the above issues with the help of multi-head attention mechanism (Rahman et al., 2020; Le et al., 2019; Tsai et al., 2019).",
      "startOffset" : 151,
      "endOffset" : 208
    }, {
      "referenceID" : 28,
      "context" : "By introducing the standard Transformer network (Vaswani et al., 2017) as the basis, Tsai et al.",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 26,
      "context" : "(Tsai et al., 2019) captured the integrations directly from unaligned multimodal streams in an end-to-end fashion, latently adapted streams from one modality to another with the cross-modal",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 29,
      "context" : "(Wang et al., 2020) proposed a parallel Transformer unit, allowing to explore the correlation between multimodal knowledge effectively.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 33,
      "context" : "For evaluation, CTFN was verified on two multimodal sentiment benchmarks, CMU-MOSI (Zadeh et al., 2016) and MELD (Poria et al.",
      "startOffset" : 83,
      "endOffset" : 103
    }, {
      "referenceID" : 31,
      "context" : "Non-translation based: Recently, RNN-based models, considering GRU and LSTM, have received significant advances in exploiting the context-aware information across the data (Yang et al., 2016; Agarwal et al., 2019).",
      "startOffset" : 172,
      "endOffset" : 213
    }, {
      "referenceID" : 0,
      "context" : "Non-translation based: Recently, RNN-based models, considering GRU and LSTM, have received significant advances in exploiting the context-aware information across the data (Yang et al., 2016; Agarwal et al., 2019).",
      "startOffset" : 172,
      "endOffset" : 213
    }, {
      "referenceID" : 22,
      "context" : "bc − LSTM (Poria et al., 2017b) and GME − LSTM (Chung et al.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 4,
      "context" : ", 2017b) and GME − LSTM (Chung et al., 2014) presented a LSTM-based model to retrieve contextual information, where the unimodal features are concatenated into a unit one as the input information.",
      "startOffset" : 24,
      "endOffset" : 44
    }, {
      "referenceID" : 23,
      "context" : "Similarly, MELD − base (Poria et al., 2019) leveraged the concatenation of audio and textual features on the input layer, and employed GRU to model sentimental context.",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 15,
      "context" : "In contrast, CHFusion (Majumder et al., 2018) employed the RNN-based hierarchical structure to",
      "startOffset" : 22,
      "endOffset" : 45
    }, {
      "referenceID" : 9,
      "context" : "On the basis of RNN, MMMU−BA (Ghosal et al., 2018) further employed multimodal attention block to absorb",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 19,
      "context" : ", 2019; ?) in machine translation, (Pham et al., 2019) and (Pham et al.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 20,
      "context" : ", 2019) and (Pham et al., 2018) presented multimodal fusion model via the essential insight that translates from a source modality to a target modality, which is able to capture much more robust associations across multiple modalities.",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 32,
      "context" : "For CMU-MOSI dataset, we adopt the same preprocess manner mentioned in MFN (Zadeh et al., 2018) to extract the low-level representation of multimodal data, and synchronized",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 18,
      "context" : "dimensional GloVe (Pennington et al., 2014) text vectors are fed into a 1D-CNN (Chen et al.",
      "startOffset" : 18,
      "endOffset" : 43
    }, {
      "referenceID" : 3,
      "context" : ", 2014) text vectors are fed into a 1D-CNN (Chen et al., 2017) layer to extract textual representation, and audiobased descriptors are explored with the popular toolkit openSMILE (Eyben et al.",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 7,
      "context" : ", 2017) layer to extract textual representation, and audiobased descriptors are explored with the popular toolkit openSMILE (Eyben et al., 2010), while visual features were not taken into account for the",
      "startOffset" : 124,
      "endOffset" : 144
    } ],
    "year" : 2021,
    "abstractText" : "and the 11th International Joint Conference on Natural Language Processing, pages 5301–5311 August 1–6, 2021. ©2021 Association for Computational Linguistics 5301 CTFN: Hierarchical Learning for Multimodal Sentiment Analysis Using Coupled-Translation Fusion Network Jiajia Tang1∗, Kang Li1∗, Xuanyu Jin, Andrzej Cichocki , Qibin Zhao, Wanzeng Kong1† Key Laboratory of Brain Machine Collaborative Intelligence of Zhejiang Province, School of Computer Science and Technology, Hangzhou Dianzi University, China Skolkovo Institute of Science and Technology, Moscow, Russia Systems Research Institute, Polish Academy of Science, Warsaw, Poland Center for Advanced Intelligence Project, RIKEN {hdutangjiajia, Jxuanyu599}@163.com {likang bro, kongwanzeng}@hdu.edu.cn {a.cichocki, qibin.zhao}@riken.jp Abstract",
    "creator" : "LaTeX with hyperref"
  }
}