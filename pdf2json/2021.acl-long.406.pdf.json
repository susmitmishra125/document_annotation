{
  "name" : "2021.acl-long.406.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Word Sense Disambiguation: Towards Interactive Context Exploitation from Both Word and Sense Perspectives",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5218–5229\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5218\nLately proposed Word Sense Disambiguation (WSD) systems have approached the estimated upper bound of the task on standard evaluation benchmarks. However, these systems typically implement the disambiguation of words in a document almost independently, underutilizing sense and word dependency in context. In this paper, we convert the nearly isolated decisions into interrelated ones by exposing senses in context when learning sense embeddings in a similaritybased Sense Aware Context Exploitation (SACE) architecture. Meanwhile, we enhance the context embedding learning with selected sentences from the same document, rather than utilizing only the sentence where each ambiguous word appears. Experiments on both English and multilingual WSD datasets have shown the effectiveness of our approach, surpassing previous state-of-the-art by large margins (3.7% and 1.2% respectively), especially on few-shot (14.3%) and zero-shot (35.9%) scenarios."
    }, {
      "heading" : "1 Introduction",
      "text" : "Word Sense Disambiguation (WSD) is the task of determining a word’s sense given its context. Recently, contextualized representation learning (Devlin et al., 2019; Liu et al., 2019) have accelerated the advancement of WSD, raising the performance on a standard evaluation framework (Raganato et al., 2017a) from slightly higher than 70% (Raganato et al., 2017b; Luo et al., 2018; Kumar et al., 2019) to about 80% (Vial et al., 2019; Blevins and Zettlemoyer, 2020; Bevilacqua and\n* corresponding author\nNavigli, 2020). This is an estimated upper bound of the task, which is from the inter-annotator agreement: the percentage of words that are annotated with the same meaning by two or more annotators (Navigli, 2009). There is a clear trend that supervised systems tend to incorporate sense knowledge into their architecture, ranging from sense definition, usage examples to sense relation.\nHowever, the disambiguation of words in a document is almost independent of each other, especially from the perspective of senses in context. The connection of each word’s disambiguation is limited to the utilization of a sentence (Loureiro and Jorge, 2019; Huang et al., 2019; Hadiwinoto et al., 2019; Scarlini et al., 2020a) or a small window of text (Bevilacqua and Navigli, 2020) because of computation cost or model restriction. More severely, the interaction of senses in context is barely explored. Similar to word cooccurrence, the appearance of one sense can sometimes dominate the choice of another sense in the same sentence (Agirre et al., 2014; Maru et al., 2019).\nIn this paper, we introduce SACE, a similaritybased WSD approach. Precisely, we transform the previously almost isolated disambiguation of words in a document into interrelated ones to maximize the contribution of context from both word and sense perspectives. We summarize our contributions as follows:\n1. We devise an interactive sense embedding learning technique that takes into account senses in context via a selective attention layer in a neural architecture. It connects senses via their appearance in a piece of text rather than using manually constructed sense relations, being less costly. 2. We introduce a method to better exploit the\ncontext sentences of an ambiguous word in the neural architecture by selecting important sentences from the same document according to sentence relatedness. 3. With experiments on corresponding datasets, the proposed architecture is proved to have an overwhelming advantage of few-shot and zero-shot WSD learning ability compared with other strong baselines. 4. We show that the proposed architecture is portable to multilingual scenarios when trained merely on an English dataset with a multilingual pre-trained model, achieving new state-of-the-art on most tested benchmarks and the combined one."
    }, {
      "heading" : "2 Related Work",
      "text" : "There are mainly two alternatives for solving WSD, namely knowledge-based and supervised approaches. While the former mainly relies on a sense inventory for disambiguation, the latter is dependent on sense-annotated corpora to train a sense classifier, either for each word or the whole vocabulary. However, many recently proposed systems combine the above two strategies, injecting sense knowledge into their supervised models while somehow inadequately modeling the provided context in a document from both word and sense perspectives."
    }, {
      "heading" : "2.1 Supervised Method",
      "text" : "Early supervised approaches model the relational pattern between an ambiguous word’s local features and its gold sense from sense-annotated data. IMS (Zhong and Ng, 2010) was one of the most prevalent systems that trained a sense classifier for each lemma in training data. In comparison, Raganato et al. (2017b) unified the disambiguation of words into a single sequence labeling architecture, relieving the efficiency issue. Many following systems improved this architecture by incorporating sense knowledge.\nFor unseen lemmas, these systems require most frequent sense (MFS) fallback (select the most frequent candidate sense in the training data). To tackle this problem, LMMS (Loureiro and Jorge 2019) implements the disambiguation in a similarity-based manner. It learns a sense embedding for each labeled sense in SemCor (Miller et al., 1994) and maps them to full coverage of WordNet (Miller, 1995) senses using\nsense relations. BERT (Devlin et al., 2019) is used as a feature-extraction module for both gloss and context encoding. Further, BEM (Blevins and Zettlemoyer, 2020) utilizes two encoders for the above approach in a fine-tuning manner. Although the model is more effective even without exploiting sense knowledge other than glosses, it takes around 2.5 days for training.\nThe employment of sense relations in previous supervised systems is mostly limited to explicitly defined sense relations including hypernymy and hyponymy relation, severely neglecting how senses in context contribute to the selection of a word’s sense."
    }, {
      "heading" : "2.2 Context Exploitation",
      "text" : "For supervised WSD approaches, it is typical to use a small fraction of the whole context to carry out disambiguation, such as a sentence, or a sliding window of text. In contrast, knowledge-based WSD approaches tend to more sufficiently exploit a word’s context, ranging from a sentence (Lesk, 1986; Wang and Wang, 2020), a few sentences (Agirre et al., 2018, Wang et al., 2020) to even the whole document (Chaplot and Salakhutdinov, 2018). Some studies draw in out-of-dataset context (Ponzetto and Navigli, 2010; Scarlini et al., 2020a) for disambiguation, including Wikipedia documents. Therefore, it is worth exploring whether the disambiguation of words within the same document can benefit from each other in a supervised system.\nThe utilization of senses in context is far less investigated compared with words in context. UKB (Agirre et al., 2014, a knowledge-based system) is one of the related systems that model sense relations in context. It first connects senses in context via WordNet sense relations and operates personalized PageRank on the constructed sense graph to decide sense importance. For each word, the most important potential sense is considered as the correct sense. SyntagNet (Maru et al., 2019) improves the idea by introducing manually disambiguated sense pairs in context during sense graph construction. Although the system was able to challenge supervised systems at the time, it relied on human labor to obtain sense pairs in context. There was no attempt on integrating the utilization of senses in context into a supervised architecture."
    }, {
      "heading" : "3 Preliminary",
      "text" : "WSD is to select the correct sense \uD835̃\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57 of a word \uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57 given its context. \uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57 is the \uD835\uDC57\uD835\uDC57\uD835\uDC61\uD835\uDC61ℎ word in the \uD835\uDC56\uD835\uDC56\uD835\uDC61\uD835\uDC61ℎ sentence \uD835\uDC46\uD835\uDC46\uD835\uDC56\uD835\uDC56 = {\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC561,\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC562, … ,\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57 , … ,\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC56\uD835\uDC56} of a document \uD835\uDC37\uD835\uDC37 = {\uD835\uDC46\uD835\uDC461, \uD835\uDC46\uD835\uDC462, … , \uD835\uDC46\uD835\uDC46\uD835\uDC56\uD835\uDC56 , … , \uD835\uDC46\uD835\uDC46\uD835\uDC5A\uD835\uDC5A}. The candidate senses \uD835\uDC60\uD835\uDC60�\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57� = {\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC571, \uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC572, … , \uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57, … , \uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57} are from a sense inventory such as WordNet. Here, \uD835\uDC56\uD835\uDC56, \uD835\uDC57\uD835\uDC57, and \uD835\uDC57\uD835\uDC57 denote the index of sentence, word, and sense respectively.\nIn a similarity-based WSD approach, the disambiguation of a word is determined by the similarity between its context representation \uD835\uDC63\uD835\uDC63\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57 and each candidate sense representation \uD835\uDC63\uD835\uDC63\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57 . In many cases, both representations are vectors and the similarity is measured by their dot product after normalization. Then, the sense with the highest similarity is selected as the correct sense.\nTypically, a word’s context representation is learned using the sentence \uD835\uDC46\uD835\uDC46\uD835\uDC56\uD835\uDC56 where the word appears (Loureiro and Jorge, 2019; Scarlini et al., 2020a; Scarlini et al., 2020b). The representation of a candidate sense is obtained using its gloss/definition \uD835\uDC3A\uD835\uDC3A\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57 defined in WordNet (Blevins and Zettlemoyer, 2020). A common approach of encoding these two sequences in recent research is to utilize pre-trained models such as BERT, RoBERTa (Liu et al., 2019), and so on, taking the sum of the outputs of the last four layers as encoded features (Loureiro and Jorge, 2019; Scarlini et al., 2020a), as in (1) and (2). Before feeding \uD835\uDC46\uD835\uDC46\uD835\uDC56\uD835\uDC56 and \uD835\uDC3A\uD835\uDC3A\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57 to the models, a special token [CLS]/[SEP] is added to the beginning/end of the sequence, modifying them into \uD835\uDC46\uD835\uDC46\uD835̅\uDC56\uD835\uDC56 and \uD835̅\uDC3A\uD835\uDC3A\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57 , respectively.\n\uD835\uDC63\uD835\uDC63\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57 = ∑ \uD835\uDC35\uD835\uDC35\uD835\uDC35\uD835\uDC35\uD835\uDC35\uD835\uDC35\uD835\uDC35\uD835\uDC35\uD835\uDC67\uD835\uDC67 \uD835\uDC57\uD835\uDC57(−4,−1) \uD835\uDC67\uD835\uDC67 (\uD835\uDC46\uD835\uDC46\uD835̅\uDC56\uD835\uDC56) (1)\nFor each \uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57’s context representation, a normal choice is to utilize the model’s output at the position of the word (\uD835\uDC57\uD835\uDC57), using \uD835\uDC46\uD835\uDC46\uD835̅\uDC56\uD835\uDC56 as input, shown in equation (1). If the word is tokenized into several pieces, their mean is taken. In contrast, for each sense representation, when it is fine-tuning a pretrained model, the sense embedding is the output at the position of [CLS] (Blevins and Zettlemoyer, 2020), with the modified gloss as input, as in (2).\n\uD835\uDC63\uD835\uDC63\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57 [\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36] = ∑ \uD835\uDC35\uD835\uDC35\uD835\uDC35\uD835\uDC35\uD835\uDC35\uD835\uDC35\uD835\uDC35\uD835\uDC35\uD835\uDC67\uD835\uDC67 [\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36](−4,−1) \uD835\uDC67\uD835\uDC67 (\uD835̅\uDC3A\uD835\uDC3A\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57) (2)\nTo utilize the supervision from a training corpus, a cross-entropy loss is implemented against the similarity distribution of candidate senses (the SoftMax product without index \uD835\uDC57\uD835\uDC57 in (3)) and the one-hot ground-truth distribution, shown in equation (4). \uD835\uDC49\uD835\uDC49\uD835\uDC60\uD835\uDC60�\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57� ∈ ℝ\n�\uD835\uDC60\uD835\uDC60�\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57��×ℎ is a matrix of concatenated sense embeddings arranged in rows. ℎ is the dimension of the pre-trained model’s hidden states (768 or 1024 of BERT). \uD835\uDC66\uD835\uDC66\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57 is equal to 1 when \uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57 (the \uD835\uDC57\uD835\uDC57\uD835\uDC61\uD835\uDC61ℎ sense of \uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57) is the correct sense, otherwise 0, representing each element in the ground-truth one-hot vector. For prediction, the model selects the sense with the largest dot product for each word.\n\uD835\uDC60\uD835\uDC60\uD835\uDC56\uD835\uDC56\uD835\uDC60\uD835\uDC60 �\uD835\uDC63\uD835\uDC63\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57 ∙ \uD835\uDC63\uD835\uDC63\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57 [\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36]� = \uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC60\uD835\uDC60\uD835\uDC60\uD835\uDC60\uD835\uDC60\uD835\uDC60\uD835\uDC60\uD835\uDC60\uD835\uDC60\uD835\uDC60(\uD835\uDC49\uD835\uDC49\uD835\uDC60\uD835\uDC60�\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57�\uD835\uDC63\uD835\uDC63\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57) \uD835\uDC57\uD835\uDC57 (3)\nℒ�\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57 , \uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57� = −∑ \uD835\uDC66\uD835\uDC66\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC59\uD835\uDC59\uD835\uDC57\uD835\uDC57\uD835\uDC59\uD835\uDC59(\uD835\uDC60\uD835\uDC60\uD835\uDC56\uD835\uDC56\uD835\uDC60\uD835\uDC60(\uD835\uDC63\uD835\uDC63\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57 ∙ \uD835\uDC63\uD835\uDC63\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57 [\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36]))�\uD835\uDC60\uD835\uDC60(\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57)�\uD835\uDC57\uD835\uDC57=1 (4)\nIn the above approach (from BEM, Blevins and Zettlemoyer, 2020), the embedding learning process of different senses is independent of each other, relying merely on sense gloss. Besides, the\ninteraction between different words’ disambiguation is limited to the utilization of a sentence, leading to inadequate exploitation of the words in context. Therefore, we transform the above almost isolated decisions into interrelated ones by learning the sense and context embeddings interactively."
    }, {
      "heading" : "4 SACE: Sense Aware Context Exploitation in Supervised WSD",
      "text" : ""
    }, {
      "heading" : "4.1 Sense-level Context (SlC)",
      "text" : "The interactive sense embedding learning mainly involves a selective attention layer upon the original sense embeddings from the pre-trained model. The goal of this interaction is to assist the learning of one sense’s embedding to be aware of the others in the same context. It is supported by the fact that many sense pairs are more commonly used than the others.\nIn practice, each of the ambiguous words in the document has several candidate senses, which poses questions about which senses should be attended in the selective attention layer. To address this problem, we make use of the iterative characteristic of the model training. In other words, the system’s predicted senses of each word within a particular context from the former iteration are attended. For the first iteration, the first sense of each word in context is attended. In such a strategy, the senses of monosemous words (has a single sense) can be exploited at all iterations.\nFor convenient demonstration, we use the embedding of predicted senses \uD835̂\uDC60\uD835\uDC60\uD835\uDC5D\uD835\uDC5D of the context words in \uD835\uDC46\uD835\uDC46\uD835\uDC56\uD835\uDC56 to enhance that of each sense \uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57 of word \uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57. We note that, \uD835\uDC46\uD835\uDC46\uD835\uDC56\uD835\uDC56 can be a larger context. In equation (5), \uD835\uDC5B\uD835\uDC5B is the number of words in \uD835\uDC46\uD835\uDC46\uD835\uDC56\uD835\uDC56. In (6), \uD835\uDC4A\uD835\uDC4A ∈ ℝℎ×ℎ is a learnable weight matrix.\n\uD835̅\uDC63\uD835\uDC63\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57 = \uD835\uDC63\uD835\uDC63\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57 [\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36] + ∑ \uD835\uDEFC\uD835\uDEFC(\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57, \uD835̂\uDC60\uD835\uDC60\uD835\uDC5D\uD835\uDC5D)\uD835\uDC56\uD835\uDC56\uD835\uDC5D\uD835\uDC5D=1(\uD835\uDC5D\uD835\uDC5D≠\uD835\uDC57\uD835\uDC57) \uD835\uDC63\uD835\uDC63\uD835̂\uDC60\uD835\uDC60\uD835\uDC5D\uD835\uDC5D [\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36] (5)\n\uD835\uDEFC\uD835\uDEFC(\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57, \uD835̂\uDC60\uD835\uDC60\uD835\uDC5D\uD835\uDC5D) = \uD835\uDC4A\uD835\uDC4A\uD835\uDC63\uD835\uDC63\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57 [\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36] ∙ \uD835\uDC4A\uD835\uDC4A\uD835\uDC63\uD835\uDC63\uD835̂\uDC60\uD835\uDC60\uD835\uDC5D\uD835\uDC5D [\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36] (6)\nThe attention score in (6) only takes into consideration the representation at [CLS] position (sentence level representation) for each gloss, neglecting the relatedness between each gloss word of two senses. To tackle this, we devise a combined attention score by considering both [CLS] and gloss word relevance, in equation (7). \uD835\uDC59\uD835\uDC59 is a predefined gloss length of all senses for normalization. \uD835\uDC63\uD835\uDC63\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57 \uD835\uDC4E\uD835\uDC4E ∈ ℝℎ×1 is obtained with\nequation (2) by changing the output position to \uD835\uDC60\uD835\uDC60. If the length (e.g., \uD835\uDC59\uD835\uDC59) of a sense gloss is smaller than \uD835\uDC59\uD835\uDC59, \uD835\uDC63\uD835\uDC63\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57 \uD835\uDC4E\uD835\uDC4E is a zero vector where \uD835\uDC60\uD835\uDC60 is larger than \uD835\uDC59\uD835\uDC59.\n\uD835\uDEFC\uD835\uDEFC�\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57, \uD835̂\uDC60\uD835\uDC60\uD835\uDC5D\uD835\uDC5D� = \uD835\uDC4A\uD835\uDC4A\uD835\uDC63\uD835\uDC63\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57 [\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36] ∙ \uD835\uDC4A\uD835\uDC4A\uD835\uDC63\uD835\uDC63\uD835̂\uDC60\uD835\uDC60\uD835\uDC5D\uD835\uDC5D [\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36] +\n1 \uD835\uDC54\uD835\uDC542 ∑ ∑ (\uD835\uDC4A\uD835\uDC4A\uD835\uDC63\uD835\uDC63\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57 \uD835\uDC4E\uD835\uDC4E ∙ \uD835\uDC4A\uD835\uDC4A\uD835\uDC63\uD835\uDC63\uD835̂\uDC60\uD835\uDC60\uD835\uDC5D\uD835\uDC5D \uD835\uDC4F\uD835\uDC4F )\uD835\uDC54\uD835\uDC54\uD835\uDC4F\uD835\uDC4F=1 \uD835\uDC54\uD835\uDC54 \uD835\uDC4E\uD835\uDC4E=1 (7)"
    }, {
      "heading" : "4.2 Word-level Context (WlC)",
      "text" : "In many previous supervised systems, the disambiguation of one word in a sentence is isolated from the words in the other sentences of the same document. We convert the isolated disambiguation into interactive ones by utilizing several highly related sentences within the same document for context embedding learning.\nFor each sentence \uD835\uDC46\uD835\uDC46\uD835\uDC56\uD835\uDC56 , we select its related sentences under two criteria, with one being the distance to \uD835\uDC46\uD835\uDC46\uD835\uDC56\uD835\uDC56 , and the other being the semantic relatedness to \uD835\uDC46\uD835\uDC46\uD835\uDC56\uD835\uDC56. The first criterion can be regarded as local features and the second one is aimed at injecting global features while maintaining a low noise level.\nFrom the perspective of local features, directly surrounding sentences within a window are used as related sentences. For global features, we score context sentences and utilize the top related sentences for context embedding learning. Precisely, in a document \uD835\uDC37\uD835\uDC37 , we regard each sentence as a document \uD835\uDC51\uD835\uDC51 and calculate the TFIDF score of each word in the vocabulary \uD835\uDC63\uD835\uDC63 of \uD835\uDC37\uD835\uDC37 for all sentences. The intuition behind modeling sentences with TF-IDF is that we find the average length of SemCor sentences is 22, which is reasonably long. This represents the original document as a matrix \uD835\uDC49\uD835\uDC49\uD835\uDC37\uD835\uDC37 ∈ ℝ\uD835\uDC5A\uD835\uDC5A×|\uD835\uDC63\uD835\uDC63| , where each row and column indicate sentence and word dimension respectively. For instance, \uD835\uDC49\uD835\uDC49\uD835\uDC37\uD835\uDC37(\uD835\uDC46\uD835\uDC46\uD835\uDC56\uD835\uDC56 ,\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57) is the TF-IDF score of \uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57 in \uD835\uDC46\uD835\uDC46\uD835\uDC56\uD835\uDC56 . The score of \uD835\uDC46\uD835\uDC46\uD835\uDC57\uD835\uDC57 concerning \uD835\uDC46\uD835\uDC46\uD835\uDC56\uD835\uDC56 is shown as follows:\n\uD835\uDC60\uD835\uDC60\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC60\uD835\uDC60\uD835\uDC60\uD835\uDC60\uD835\uDC36\uD835\uDC36\uD835\uDC56\uD835\uDC56�\uD835\uDC46\uD835\uDC46\uD835\uDC57\uD835\uDC57� = \uD835\uDC49\uD835\uDC49\uD835\uDC37\uD835\uDC37(\uD835\uDC46\uD835\uDC46\uD835\uDC56\uD835\uDC56) ∙ \uD835\uDC49\uD835\uDC49\uD835\uDC37\uD835\uDC37(\uD835\uDC46\uD835\uDC46\uD835\uDC57\uD835\uDC57) (8)\nAfter scoring all context sentences for each sentence \uD835\uDC46\uD835\uDC46\uD835\uDC56\uD835\uDC56, we concatenate related sentences with \uD835\uDC46\uD835\uDC46\uD835\uDC56\uD835\uDC56 and utilize them as an input to BERT for context embedding learning. As an example, {\uD835\uDC46\uD835\uDC46\uD835\uDC56\uD835\uDC56−1, \uD835\uDC46\uD835\uDC46\uD835\uDC56\uD835\uDC56+1} are related sentences from local features, and if {\uD835\uDC46\uD835\uDC46\uD835\uDC56\uD835\uDC56−12, \uD835\uDC46\uD835\uDC46\uD835\uDC56\uD835\uDC56+7} are top-scored sentences from global features, we use \uD835\uDC36\uD835\uDC36\uD835\uDC56\uD835\uDC56 = {\uD835\uDC46\uD835\uDC46� \uD835\uDC56\uD835\uDC56−12, \uD835\uDC46\uD835\uDC46\uD835̅\uDC56\uD835\uDC56−1, \uD835\uDC46\uD835\uDC46\uD835̅\uDC56\uD835\uDC56 , \uD835\uDC46\uD835\uDC46\uD835̅\uDC56\uD835\uDC56+1, \uD835\uDC46\uD835\uDC46\uD835̅\uDC56\uD835\uDC56+7} as an input to equation (1) and retrieve the enhanced context embedding \uD835̅\uDC63\uD835\uDC63\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57 of each word\n\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57 in \uD835\uDC46\uD835\uDC46\uD835\uDC56\uD835\uDC56. In such a way, different \uD835\uDC36\uD835\uDC36\uD835\uDC56\uD835\uDC56 is retrieved for each sentence in the document. We note that, when the total sequence length is longer than 512, we remove the furthest sentences away from \uD835\uDC46\uD835\uDC46\uD835̅\uDC56\uD835\uDC56 . For instance, \uD835\uDC46\uD835\uDC46\uD835̅\uDC56\uD835\uDC56−12 , \uD835\uDC46\uD835\uDC46\uD835̅\uDC56\uD835\uDC56+7 and so on in the above example will be removed in order.\nFinally, \uD835\uDC63\uD835\uDC63\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57 and \uD835\uDC63\uD835\uDC63\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57 [\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36\uD835\uDC36] in equation (4) are replaced with \uD835̅\uDC63\uD835\uDC63\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57 and \uD835̅\uDC63\uD835\uDC63\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57 respectively to calculate the loss, with which to update the weights of the pre-trained model and the selective attention layer."
    }, {
      "heading" : "4.3 Try-again Mechanism (TaM)",
      "text" : "In a previous similarity-based WSD approach, Wang and Wang (2020) proposed a Try-again Mechanism (TaM) that takes into account not only the similarity of \uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57 ’s context embedding to the sense embedding of \uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57 , but also to the sense embedding of \uD835\uDC60\uD835\uDC60\uD835\uDC5F\uD835\uDC5F ∈ \uD835\uDC60\uD835\uDC60\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC4E\uD835\uDC4E\uD835\uDC61\uD835\uDC61\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F during evaluation. Here, \uD835\uDC60\uD835\uDC60\uD835\uDC5F\uD835\uDC5F and \uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57 are connected by either WordNet relations or the super-sense relation (i.e., senses that belong to the same super-sense category in WordNet). This mechanism in (9) manages to boost the performance of its knowledge-based system by a relatively large margin.\n\uD835\uDC60\uD835\uDC60\uD835\uDC56\uD835\uDC56\uD835\uDC60\uD835\uDC60�\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57 , \uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57� = \uD835\uDC63\uD835\uDC63\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57 ∙ \uD835\uDC63\uD835\uDC63\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57 + max\uD835\uDC60\uD835\uDC60\uD835\uDC5F\uD835\uDC5F∈\uD835\uDC60\uD835\uDC60\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F (\uD835\uDC63\uD835\uDC63\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57 ∙ \uD835\uDC63\uD835\uDC63\uD835\uDC60\uD835\uDC60\uD835\uDC5F\uD835\uDC5F) (9)\nIn this subsection, we reconstruct TaM so that it becomes effective in our model. This process helps the disambiguation of words to be even more interactive since it considers an increased number of senses by utilizing sense relation knowledge.\nIn our implementation, we replace the above relations with only those derived from Coarse Sense Inventory (CSI, Lacerra et al., 2020). Similar to the utilization of super-sense categories, we connect senses that belong to the same label in CSI as related senses. Also, we change the direct sum of the above two similarities into a weighted sum using a hyperparameter \uD835\uDEFD\uD835\uDEFD.\n\uD835\uDC60\uD835\uDC60\uD835\uDC56\uD835\uDC56\uD835\uDC60\uD835\uDC60�\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57 , \uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57� = (1 − \uD835\uDEFD\uD835\uDEFD) ∗ \uD835̅\uDC63\uD835\uDC63\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57 ∙ \uD835̅\uDC63\uD835\uDC63\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57 + \uD835\uDEFD\uD835\uDEFD ∗ max\n\uD835\uDC60\uD835\uDC60\uD835\uDC5F\uD835\uDC5F∈\uD835\uDC60\uD835\uDC60\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F (\uD835̅\uDC63\uD835\uDC63\uD835\uDC64\uD835\uDC64\uD835\uDC56\uD835\uDC56\uD835\uDC57\uD835\uDC57 ∙ \uD835̅\uDC63\uD835\uDC63\uD835\uDC60\uD835\uDC60\uD835\uDC5F\uD835\uDC5F) (10)\nIn addition, our approach only learns a sense embedding for the candidate senses whose lemma is annotated in training data. Therefore, in TaM, we save sense embeddings from training for each\n† http://lcl.uniroma1.it/wsdeval/home\nepoch and use them to implement TaM during evaluation. It is worth mentioning that for senses that do not have a sense embedding in \uD835\uDC60\uD835\uDC60\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC4E\uD835\uDC4E\uD835\uDC61\uD835\uDC61\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F\uD835\uDC5F, we neglect their calculation in equation (10)."
    }, {
      "heading" : "5 Experiment Settings",
      "text" : ""
    }, {
      "heading" : "5.1 Datasets",
      "text" : "To validate the effectiveness of our approach, we use SemCor and an evaluation framework† to train and evaluate our model, SACEbase, respectively. The evaluation framework contains 5 English allwords WSD benchmarks. We report the experimental results on each dataset including SensEval-2 (SE2, Palmer et al., 2001), SensEval3 (SE3, Snyder and Palmer, 2004), SemEval-2007 Task-17 (SE07, Pradhan et al., 2007), SemEval2013 (SE13, Navigli et al., 2013) and SemEval2015 (SE15, Moro and Navigli, 2015). Also, the results from Part-Of-Speech (POS) perspectives on their combined dataset (ALL) are reported. Following previous works, we train large models, SACElarge on SemCor and SACElarge+ on SemCor, WordNet Gloss Tagged (WNGT), and WordNet examples (WNE) for fair comparisons. Here, WNE is regarded as an extra sense gloss and is concatenated after the original sense gloss for sense embedding learning, which is similar to the implementation in SREF (Wang and Wang, 2020).\nFor few-shot WSD, we partition ALL according to the gold label of each annotation into ALLWN_1st and ALLWN_others. Besides, according to whether senses and lemmas of ALL instances appear in SemCor, we extract two subsets, ALLZSS and ALLZSL, to evaluate the zero-shot learning ability of our model.\nFor cross-lingual datasets, we use the WordNet version of the latest evaluation framework‡ which contains test datasets for Spanish, Italian, French, and German. These datasets are preprocessed data from SemEval-2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015). The former only disambiguates nouns while the latter covers words in four POS (noun-N, verb-V, adjective-A, adverb-R).\nWe note that the performance in each table is reported with F1 in percentage."
    }, {
      "heading" : "5.2 Model Design",
      "text" : "‡ https://github.com/SapienzaNLP/mwsd-datasets\nOur base and large model utilize RoBERTabase and RoBERTalarge respectively, which perform relatively better than BERT models. For crosslingual evaluation, we fine-tune XLM-RoBERTabase (SACEmul, Conneau et al., 2020) with the same training data as SACElarge+, following the setting in EWISER. In each system, two encoders are adopted, with one being a context encoder and the other being a sense gloss encoder. This is identical to the setting in BEM. We note that a major difference is that the pre-trained model adopted in the above papers is BERT.\nThe hyperparameters of our model are selected using SE07. They include the number of surrounding sentences (2) on both sides of \uD835\uDC46\uD835\uDC46\uD835\uDC56\uD835\uDC56, the number of top related sentences (2) of \uD835\uDC46\uD835\uDC46\uD835\uDC56\uD835\uDC56 and \uD835\uDEFD\uD835\uDEFD (0.1) in TaM. The learning rate for SACEbase, SACElarge, SACElarge+, and SACEmul is 1e-5, 1e-6, 1e-6, and 5e-6 respectively.\nTo accelerate the model training, we organize the sentences in a document into batches according to the total number of candidate senses (400 for SACEbase and SACEmul, 150 for SACElarge and SACElarge+), i.e., if the total number of candidate senses exceeds 400 or 150 when adding a sentence, then the sentence belongs to the next batch. For each batch, the gloss and context encoders are only called once. The context and gloss length is normalized to the maximal sequence length within each batch to reduce unnecessary padding and computation. Also, apex is employed for mixedprecision computing. More details are shown in Appendix A."
    }, {
      "heading" : "5.3 Baselines",
      "text" : "We compare the proposed model with previous supervised state-of-the-art from different perspectives. These systems include Sense Vocabulary Compression (SVC, Vial et al., 2019), EWISE (Kumar et al., 2019), LMMS (Loureiro and Jorge, 2019), GLU (Hadiwinoto et al., 2019), GlossBERT (Huang et al., 2019), EWISER (Bevilacqua and Navigli, 2020), BEM (Blevins and Zettlemoyer, 2020), ARES (Scarlini et al., 2020b) and SREF (Wang and Wang, 2020). BEM is our direct baseline, which utilizes two encoders to learn context and sense embedding separately and achieves state-of-the-art with only SemCor.\nFor cross-lingual evaluation, we compare our results with those reported in SyntagNet, EWISER, ARES, MuLaN (Barba et al., 2020). These systems are all recently proposed systems with state-of-the-\nart performance."
    }, {
      "heading" : "6 Results",
      "text" : ""
    }, {
      "heading" : "6.1 Ablation Analysis",
      "text" : "In this subsection, we demonstrate how each component of our model benefits WSD performance. In table 1, the system’s performance on ALL has illustrated that enhancing the interaction between different words’ disambiguation in the same document (WlC) can raise the system’s performance by the largest margin, 1.5 F1. This promotion is slightly larger than that (1.2 F1) provided by the interactive sense embedding learning (SlC). The gloss word attention in SlC is also proved effective, which helps increase the system’s performance by 0.5 F1, similar to the contribution of TaM, 0.6 F1. Most importantly, when all components are removed, the performance on ALL decreases to 78.4 F1. We note that the baseline here is different from BEM since we remove unnecessary padding and utilize RoBERTa. This has dramatically accelerated the training process from 3.5 hours to 0.5 hour per epoch while achieved similar performance. We also note that the experimental results reported in this paper are obtained using the same random seed as BEM. With different random seeds, the performance gap on ALL between SACEbase and its baseline (-w/o all) ranges from 1.7 F1 to 2.7 F1."
    }, {
      "heading" : "6.2 All-words WSD",
      "text" : "Table 2 demonstrates how our systems and lately proposed baselines perform on different partitions of ALL. When it is trained on SemCor, SACEbase has already outperformed all its competitors by at least 1.9 F1, on ALL. This is obtained without utilizing prior sense relation knowledge. It is the first system that surpasses the estimated upper bound (80 F1) of the task using only SemCor.\nExcept GlossBERT and BEM, the other systems adopt BERTlarge as their pre-trained model. When\nwe use RoBERTalarge, SACElarge can further reach 81.9 F1 on ALL, surpassing the previous state-ofthe-art by 2.9 (3.7% of 79.0) F1. This is a large margin given that BEM and EWISER are strong baselines. When extra training data and WNE are employed, a similar margin, 2.8 F1, is attained on ALL.\nOur systems also obtain state-of-the-art performance on each dataset, with the margin ranging from 0.2 to 2.9 F1 for SACEbase and 1.8 to 3.0 F1 for SACElarge, in the first category. As for SACElarge+, the margin above the previous best system for each dataset is even larger, varying from 1.7 to 5.5 F1. It is noteworthy that SACEbase outperforms SACElarge by 0.9 F1 on SE15 and they obtain similar performance on SE13. These two datasets are less ambiguous since each lemma has fewer candidate senses on average. This illustrates the competitive disambiguation capability of SACEbase on easier instances. We also note that the development set in two categories is different, with the first being SE07 and the second being SE15. This is because we follow most systems’ setting in the first category and follow EWISER’s setting in the second category for better comparison.\nFor the performance on different POS, our systems set new lines for all of them in ALL. The largest advancement comes from the higher disambiguation ability of verbs, making our system the first to reach the line of 70 F1. The systems also obtain unprecedented performance on noun disambiguation, surpassing the previous best system by 1.5, 2.4, and 2.4 for SACEbase, SACElarge, and SACElarge+ respectively. SACElarge+ is the only system that exceeds 85 F1 on noun disambiguation."
    }, {
      "heading" : "6.3 Rare and Unseen Sense Disambiguation",
      "text" : "Rare Sense Disambiguation Table 3 reports different systems’ performance on ALLWN_1st and ALLWN_others, which has 4278 and 2525 annotations respectively. Compared with previous wellperforming systems including LMMS and SREF, our systems achieve much better performance on both datasets, with the major contribution coming from WordNet 1st sense disambiguation. On the contrary, SACE and BEM obtain similar performance on ALLWN_1st while SACE can disambiguate rare senses with higher accuracy. This shows a better few-shot learning ability of SACE in comparison to BEM because the ALLWN_others dataset only contains the words whose correct sense appears infrequently in SemCor.\nHere, sense disambiguation is defined as whether a system can select the sense as the correct sense, which is viewed from a sense perspective. In comparison, word or lemma disambiguation is to determine the correct sense of a word or lemma, which is viewed from a word perspective.\nUnseen Sense Disambiguation In the second column of table 4, different system’s performance on ALLZSS (691 polysemous instances) is provided. This dataset only contains polysemous words whose gold label is not in SemCor, which evaluate the zero-shot sense disambiguation ability of different systems. It is shown that lately proposed systems have an overwhelming advantage of zeroshot sense disambiguation over ordinary baselines including WordNet S1 and BERT-base, with the margin ranging from about 12 F1 to about 42 F1. Specifically, although BEM outperforms its\nbaselines by around 25 F1, our base and large system still beat BEM by almost 12 and 18 F1 respectively.\nIn the third column, we follow previous works and show how different systems perform on ALLZSS* (1139 instances including monosemous ones). The aforementioned gaps become narrower since each system can correctly disambiguate monosemous instances.\nUnseen Lemma Disambiguation In the last two columns of table 4, the systems’ performance on zero-shot lemmas is presented. The difference between these two datasets is whether monosemous lemmas are included. We believe it is more reasonable to focus on ALLZSL (222 polysemous instances) since monosemous lemmas do not require disambiguation and thus the statistics on ALLZSL* cannot fully reveal the systems’ zero-shot disambiguation ability of words.\nSimilarly, it shows that lately proposed systems tend to outperform the baselines by large margins, varying from 19 to almost 36 F1. Among them, BEM performs the worst on this dataset, 2.2 F1 lower than a similar system, GlossBERT. In contrast, after incorporating both word and sense level context, our system obtains an unprecedented performance on this dataset, being the first system to reach the line of 90 F1 and beating BEM by almost 16 F1. Also, different from SREF and ARES, our systems do not rely on WordNet or SyntagNet sense relation knowledge."
    }, {
      "heading" : "6.4 Cross-lingual All-words WSD",
      "text" : "We utilize two multilingual datasets (including French-FR, German-DE, Italian-IT, and SpanishES subsets) to evaluate the multilingual transferability of our method. Table 5 presents the performance of some lately proposed systems and ours. For our system, the baseline is trained with the same training data as SACElarge+ using XLMRoBERTa-base, while removing all the proposed\ncomponents including SlC, WlC, and TaM. For the systems under comparison, all but UKB+Syn utilizes English training data. Also, EWISER and MuLaN further employ SemCor and WNGT as their training data, being the same as SACEmul.\nIt shows that SACEmul has obtained a new stateof-the-art on both the combined dataset and most individual datasets, surpassing its direct baseline by 2.4 F1. In detail, the largest margin, about 5.5 F1 on its Spanish and Italian subset, above the previous best system is acquired on SE15, which covers instances in all POS. This has revealed the overwhelming advantage of SACEmul on disambiguating instances of other POS. In contrast, SACEmul performs 6.5 F1 lower than MuLaN on the Spanish subset of SE13, which only covers noun instances. In a word, SACEmul is more compatible with real cross-lingual scenarios since it has a strong disambiguation ability of words in different POS."
    }, {
      "heading" : "6.5 Analysis",
      "text" : "Error Analysis By comparing the disambiguation results of SACEbase and its baseline (all factors removed), it is revealed that both systems have correctly disambiguated 5346 instances in ALL while 525 and 339 instances are only correctly disambiguated by SACEbase and its baseline respectively. In other words, SACEbase has falsely\npredicted 339 examples that are correctly predicted by its baseline. This indicates the proposed methods might have injected excessive noise for the disambiguation of these instances. Therefore, selective exploitation of context for different instances might be beneficial.\nThe bottom half of table 6 shows an example (country) that SACEbase falsely predicted. It is shown that the WlC does not manage to retrieve valuable information for disambiguating the word while injecting some irrelevant context.\nCase Study Table 6 gives an example of top related sentences (#47 and #19) of a particular sentence (#10) under disambiguation. Here, church is falsely predicted when WlC is disabled. It shows that WlC has detected similar sentences in the same document and incorporated valuable context for context embedding learning.\nTable 7 provides some examples regarding synsets that are connected by the selective attention layer, indicating its ability of detecting some syntagmatic sense relations and senses of close meaning. The connection is established by using the largest attention score \uD835\uDEFC\uD835\uDEFC�\uD835\uDC60\uD835\uDC60\uD835\uDC57\uD835\uDC57\uD835\uDC57\uD835\uDC57 , \uD835̂\uDC60\uD835\uDC60\uD835\uDC5D\uD835\uDC5D� in a batch after filtering self-connection."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we propose an interactive context\nexploitation method from both word and sense perspectives in a supervised similarity-based WSD architecture. Experiments on English and crosslingual all-words WSD datasets verify the effectiveness of our approach, surpassing previous state-of-the-art by large margins. It also shows that the proposed method has an overwhelming advantage of learning few-shot and zero-shot WSD ability. For future work, we intend to utilize reinforcement learning to enhance current interactive WSD by customizing the context exploitation for different instances. The source code is available at: https://github.com/lwmlyy/SACE."
    }, {
      "heading" : "8 Ethics Impact Statement",
      "text" : "This paper does not involve the presentation of a new dataset, an NLP application and the utilization of demographic or identity characteristics in formation. For compute time/power, the proposed system requires less GPU amount (1 versus 2 GPUs) and time (10 versus about 70 hours) for training compared with its direct baseline (Blevins and Zettlemoyer, 2020)."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the anonymous reviewers and Jianzhang Zhang for their insightful comments. This work was supported by the National Natural Science Foundation of China (under Project No. 61375053) and the graduate innovation fund of Shanghai University of Finance and Economics (under Project No. CXJJ-2019-395)."
    }, {
      "heading" : "A Experimental Setting",
      "text" : "Computing Infrastructure We use Pytorch deep learning infrastructure along with Transformers and Apex to implement our model. Other required packages can be found in readme.md file in the source code.\nRuntime The average training time for SACEbase, SACElarge, SACElarge+ and SACEmul is 10 hours, 20 hours, 59 hours and 17 hours, respectively.\nParameters The parameters include those from the pre-trained models such as RoBERTa-base, RoBERTa-large and XLM-RoBERTa-base, and those from the selective attention layer (6 heads * 768/1024 * 768/1024).\nEvaluation Metrics We use F1-measure to report the evaluation results. For systems that can provide sense predictions for each lemma, F1measure is equal to accuracy, which is the number of instances that are correctly predicted by the model. See Navigli, 2009 for details.\nHyperparameter Search The bounds for each hyperparameter are listed in table 1, with configurations for best performing models underlined. We use the F1-measure on SE07 to select the values. All the details are shown in the source code. For those that have two underlined numbers, they are the best setting for base and large models."
    }, {
      "heading" : "B Experimental Results",
      "text" : "In figure 1, we show how SACEbase and SACElarge perform on SE07 at each epoch during training. It is shown that both systems reach their optimal performance on SE07 at early epoch, 3rd or 4th epoch. This indicates if we utilize the method of early stopping during training, its time efficiency can further be enlarged."
    } ],
    "references" : [ {
      "title" : "Random walks for knowledge-based word sense disambiguation",
      "author" : [ "Eneko Agirre", "Oier López de Lacalle", "Aitor Soroa." ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "Agirre et al\\.,? 2014",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2014
    }, {
      "title" : "The risk of sub-optimal use of Open Source NLP Software: UKB is inadvert-ently state-of-theart in knowledge-based WSD",
      "author" : [ "Eneko Agirre", "Oier López de Lacalle", "Aitor Soroa." ],
      "venue" : "Proceedings of Workshop for NLP Open Source Software, pages 29-",
      "citeRegEx" : "Agirre et al\\.,? 2018",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2018
    }, {
      "title" : "MuLaN: multilingual label propagation for word sense disambiguation",
      "author" : [ "Edoardo Barba", "Luigi Procopio", "Niccolò Campolungo", "Tommaso Pasini", "Roberto Navigli." ],
      "venue" : "IJCAI-2020, pages 38373844.",
      "citeRegEx" : "Barba et al\\.,? 2020",
      "shortCiteRegEx" : "Barba et al\\.",
      "year" : 2020
    }, {
      "title" : "An enhanced Lesk word sense disambiguation algorithm through a distributional semantic model",
      "author" : [ "Pierpaolo Basile", "Annalina Caputo", "Giovanni Semeraro." ],
      "venue" : "COLING 2014, pages 15911600, Dublin, Ireland.",
      "citeRegEx" : "Basile et al\\.,? 2014",
      "shortCiteRegEx" : "Basile et al\\.",
      "year" : 2014
    }, {
      "title" : "Breaking Through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information",
      "author" : [ "Michele Bevilacqua", "Roberto Navigli." ],
      "venue" : "ACL 2020, pages 2854-2864. Association for",
      "citeRegEx" : "Bevilacqua and Navigli.,? 2020",
      "shortCiteRegEx" : "Bevilacqua and Navigli.",
      "year" : 2020
    }, {
      "title" : "Moving Down the Long Tail of Word Sense Disambiguation with Gloss Informed Bi-encoders",
      "author" : [ "Terra Blevins", "Luke Zettlemoyer." ],
      "venue" : "ACL 2020, pages 1006-1017. Association for Computational Linguistics.",
      "citeRegEx" : "Blevins and Zettlemoyer.,? 2020",
      "shortCiteRegEx" : "Blevins and Zettlemoyer.",
      "year" : 2020
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Knowledge-based word sense disambiguation using topic models",
      "author" : [ "Devendra Singh Chaplot", "Ruslan Salakhutdinov." ],
      "venue" : "AAAI 2018.",
      "citeRegEx" : "Chaplot and Salakhutdinov.,? 2018",
      "shortCiteRegEx" : "Chaplot and Salakhutdinov.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL 2019, pages 4171-4186, Minneapolis, Minnesota.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Improved Word Sense Disambiguation Using Pre-Trained Contextualized Word Representations",
      "author" : [ "Christian Hadiwinoto", "Hwee Tou Ng", "Wee Chung Gan" ],
      "venue" : "EMNLP-IJCNLP",
      "citeRegEx" : "Hadiwinoto et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Hadiwinoto et al\\.",
      "year" : 2019
    }, {
      "title" : "Zero-shot Word Sense Disambiguation using Sense Definition Embeddings",
      "author" : [ "Sawan Kumar", "Sharmistha Jat", "Karan Saxena", "Partha Talukdar." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Kumar et al\\.,? 2019",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2019
    }, {
      "title" : "CSI: a coarse sense inventory for 85%word sense disambiguation",
      "author" : [ "Caterina Lacerra", "Michele Bevilacqua", "Tommaso Pasini", "Roberto Navigli." ],
      "venue" : "Proceedings of The Thirty-Fourth AAAI Conference on Artificial Intelligence, Pages 8123-8130.",
      "citeRegEx" : "Lacerra et al\\.,? 2020",
      "shortCiteRegEx" : "Lacerra et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone",
      "author" : [ "Michael Lesk." ],
      "venue" : "SIGDOC ’86, pages 24-26, New York, NY, USA. ACM.",
      "citeRegEx" : "Lesk.,? 1986",
      "shortCiteRegEx" : "Lesk.",
      "year" : 1986
    }, {
      "title" : "RoBERTa: a robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Language modelling makes sense: Propagating representations through WordNet for full coverage word sense disambiguation",
      "author" : [ "Daniel Loureiro", "Alípio Mário Jorge." ],
      "venue" : "ACL 2019, pages 5682-5691, Florence, Italy.",
      "citeRegEx" : "Loureiro and Jorge.,? 2019",
      "shortCiteRegEx" : "Loureiro and Jorge.",
      "year" : 2019
    }, {
      "title" : "Incorporating glosses into neural word sense disambiguation",
      "author" : [ "Fuli Luo", "Tianyu Liu", "Qiaolin Xia", "Baobao Chang", "Zhifang Sui." ],
      "venue" : "ACL 2018, pages 2473-2482, Melbourne, Australia. Association for Computational Linguistics.",
      "citeRegEx" : "Luo et al\\.,? 2018",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2018
    }, {
      "title" : "SyntagNet: challenging supervised word sense disambiguation with lexicalsemantic combinations",
      "author" : [ "Marco Maru", "Federico Scozzafava", "Federico Martelli", "Roberto Navigli." ],
      "venue" : "Proc. Of EMNLP, pages 3525-3531. Association for Computational",
      "citeRegEx" : "Maru et al\\.,? 2019",
      "shortCiteRegEx" : "Maru et al\\.",
      "year" : 2019
    }, {
      "title" : "Using a semantic concordance for sense identification",
      "author" : [ "George A. Miller", "Martin Chodorow", "Shari Landes", "Claudia Leacock", "Robert G. Thomas." ],
      "venue" : "HUMAN LANGUAGE TECHNOLOGY: Proceedings of a Workshop held",
      "citeRegEx" : "Miller et al\\.,? 1994",
      "shortCiteRegEx" : "Miller et al\\.",
      "year" : 1994
    }, {
      "title" : "WordNet: A lexical database for English",
      "author" : [ "George A. Miller." ],
      "venue" : "Communications of the ACM, 41(2): 39-41.",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "SemEval2015 task 13: Multilingual all-words sense disambiguation and entity linking",
      "author" : [ "Andrea Moro", "Roberto Navigli." ],
      "venue" : "SemEval 2015,",
      "citeRegEx" : "Moro and Navigli.,? 2015",
      "shortCiteRegEx" : "Moro and Navigli.",
      "year" : 2015
    }, {
      "title" : "SemEval-2013 task 12: Multilingual word sense disambiguation",
      "author" : [ "Roberto Navigli", "David Jurgens", "Daniele Vannella." ],
      "venue" : "SemEval 2013 *SEM, pages 222-231, Atlanta, Georgia, USA.",
      "citeRegEx" : "Navigli et al\\.,? 2013",
      "shortCiteRegEx" : "Navigli et al\\.",
      "year" : 2013
    }, {
      "title" : "Semeval-2007 task 07: Coarse grained English all-words task",
      "author" : [ "Roberto Navigli", "Kenneth C. Litkowski", "Orin Hargraves." ],
      "venue" : "Proceedings of the 4th International Workshop on Semantic Evaluations, SemEval ’07, pages 30–35, Prague,",
      "citeRegEx" : "Navigli et al\\.,? 2007",
      "shortCiteRegEx" : "Navigli et al\\.",
      "year" : 2007
    }, {
      "title" : "Word sense disambiguation: A survey",
      "author" : [ "Roberto Navigli." ],
      "venue" : "ACM Computing Surveys, 41(2):10:1-10:69.",
      "citeRegEx" : "Navigli.,? 2009",
      "shortCiteRegEx" : "Navigli.",
      "year" : 2009
    }, {
      "title" : "English tasks: All-words and verb lexical sample",
      "author" : [ "Martha Palmer", "Christiane Fellbaum", "Scott Cotton", "Lauren Delfs", "Hoa Trang Dang." ],
      "venue" : "Proceedings of SENSEVAL-2, pages 21-24, Toulouse, France.",
      "citeRegEx" : "Palmer et al\\.,? 2001",
      "shortCiteRegEx" : "Palmer et al\\.",
      "year" : 2001
    }, {
      "title" : "Knowledge-rich Word Sense Disambiguation Rivaling Supervised Systems",
      "author" : [ "Simone Paolo Ponzetto", "Roberto Navigli." ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1522-1531.",
      "citeRegEx" : "Ponzetto and Navigli.,? 2010",
      "shortCiteRegEx" : "Ponzetto and Navigli.",
      "year" : 2010
    }, {
      "title" : "Word sense disambiguation: A unified evaluation framework and empirical comparison",
      "author" : [ "Alessandro Raganato", "Jose Camacho-Collados", "Roberto Navigli." ],
      "venue" : "EACL 2017, pages 99110, Valencia, Spain.",
      "citeRegEx" : "Raganato et al\\.,? 2017a",
      "shortCiteRegEx" : "Raganato et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural sequence learning models for word sense disambiguation",
      "author" : [ "Alessandro Raganato", "Claudio Delli Bovi", "Roberto Navigli." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1156-1167,",
      "citeRegEx" : "Raganato et al\\.,? 2017b",
      "shortCiteRegEx" : "Raganato et al\\.",
      "year" : 2017
    }, {
      "title" : "SENSEMBERT: Context-enhanced sense embeddings for multilingual word sense disambiguation",
      "author" : [ "Bianca Scarlini", "Tommaso Pasini", "Roberto Navigli." ],
      "venue" : "AAAI 2020.",
      "citeRegEx" : "Scarlini et al\\.,? 2020a",
      "shortCiteRegEx" : "Scarlini et al\\.",
      "year" : 2020
    }, {
      "title" : "With more contexts comes better performance: Contextualized sense embeddings for all-round word sense disambiguation",
      "author" : [ "Bianca Scarlini", "Tommaso Pasini", "Roberto Navigli." ],
      "venue" : "the 2020 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Scarlini et al\\.,? 2020b",
      "shortCiteRegEx" : "Scarlini et al\\.",
      "year" : 2020
    }, {
      "title" : "The English all-words task",
      "author" : [ "Benjamin Snyder", "Martha Palmer." ],
      "venue" : "Senseval-3, pages 41-43, Barcelona, Spain.",
      "citeRegEx" : "Snyder and Palmer.,? 2004",
      "shortCiteRegEx" : "Snyder and Palmer.",
      "year" : 2004
    }, {
      "title" : "A synset relationenhanced framework with a try-again mechanism for word sense disambiguation",
      "author" : [ "Ming Wang", "Yinglin Wang." ],
      "venue" : "the 2020 Conference on Empirical Methods in Natural Language Processing. Association for",
      "citeRegEx" : "Wang and Wang.,? 2020",
      "shortCiteRegEx" : "Wang and Wang.",
      "year" : 2020
    }, {
      "title" : "Word Sense Disambiguation: A Comprehensive Knowledge Exploitation Framework",
      "author" : [ "Yinglin Wang", "Ming Wang", "Hamido Fujita." ],
      "venue" : "KnowledgeBased Systems, 10530.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "It makes sense: A wide-coverage word sense disambiguation system for free text",
      "author" : [ "Zhi Zhong", "Hwee Tou Ng." ],
      "venue" : "ACL 2010 System Demonstrations, pages 78-83, Uppsala, Sweden.",
      "citeRegEx" : "Zhong and Ng.,? 2010",
      "shortCiteRegEx" : "Zhong and Ng.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Recently, contextualized representation learning (Devlin et al., 2019; Liu et al., 2019) have accelerated the advancement of WSD, raising the performance on a standard evaluation framework (Raganato et al.",
      "startOffset" : 49,
      "endOffset" : 88
    }, {
      "referenceID" : 13,
      "context" : "Recently, contextualized representation learning (Devlin et al., 2019; Liu et al., 2019) have accelerated the advancement of WSD, raising the performance on a standard evaluation framework (Raganato et al.",
      "startOffset" : 49,
      "endOffset" : 88
    }, {
      "referenceID" : 25,
      "context" : ", 2019) have accelerated the advancement of WSD, raising the performance on a standard evaluation framework (Raganato et al., 2017a) from slightly higher than 70% (Raganato et al.",
      "startOffset" : 108,
      "endOffset" : 132
    }, {
      "referenceID" : 26,
      "context" : ", 2017a) from slightly higher than 70% (Raganato et al., 2017b; Luo et al., 2018; Kumar et al., 2019) to about 80% (Vial et al.",
      "startOffset" : 39,
      "endOffset" : 101
    }, {
      "referenceID" : 15,
      "context" : ", 2017a) from slightly higher than 70% (Raganato et al., 2017b; Luo et al., 2018; Kumar et al., 2019) to about 80% (Vial et al.",
      "startOffset" : 39,
      "endOffset" : 101
    }, {
      "referenceID" : 10,
      "context" : ", 2017a) from slightly higher than 70% (Raganato et al., 2017b; Luo et al., 2018; Kumar et al., 2019) to about 80% (Vial et al.",
      "startOffset" : 39,
      "endOffset" : 101
    }, {
      "referenceID" : 22,
      "context" : "This is an estimated upper bound of the task, which is from the inter-annotator agreement: the percentage of words that are annotated with the same meaning by two or more annotators (Navigli, 2009).",
      "startOffset" : 182,
      "endOffset" : 197
    }, {
      "referenceID" : 14,
      "context" : "The connection of each word’s disambiguation is limited to the utilization of a sentence (Loureiro and Jorge, 2019; Huang et al., 2019; Hadiwinoto et al., 2019; Scarlini et al., 2020a) or a small window of text (Bevilacqua and Navigli, 2020) because of computation cost or model restriction.",
      "startOffset" : 89,
      "endOffset" : 184
    }, {
      "referenceID" : 9,
      "context" : "The connection of each word’s disambiguation is limited to the utilization of a sentence (Loureiro and Jorge, 2019; Huang et al., 2019; Hadiwinoto et al., 2019; Scarlini et al., 2020a) or a small window of text (Bevilacqua and Navigli, 2020) because of computation cost or model restriction.",
      "startOffset" : 89,
      "endOffset" : 184
    }, {
      "referenceID" : 27,
      "context" : "The connection of each word’s disambiguation is limited to the utilization of a sentence (Loureiro and Jorge, 2019; Huang et al., 2019; Hadiwinoto et al., 2019; Scarlini et al., 2020a) or a small window of text (Bevilacqua and Navigli, 2020) because of computation cost or model restriction.",
      "startOffset" : 89,
      "endOffset" : 184
    }, {
      "referenceID" : 4,
      "context" : ", 2020a) or a small window of text (Bevilacqua and Navigli, 2020) because of computation cost or model restriction.",
      "startOffset" : 35,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "Similar to word cooccurrence, the appearance of one sense can sometimes dominate the choice of another sense in the same sentence (Agirre et al., 2014; Maru et al., 2019).",
      "startOffset" : 130,
      "endOffset" : 170
    }, {
      "referenceID" : 16,
      "context" : "Similar to word cooccurrence, the appearance of one sense can sometimes dominate the choice of another sense in the same sentence (Agirre et al., 2014; Maru et al., 2019).",
      "startOffset" : 130,
      "endOffset" : 170
    }, {
      "referenceID" : 32,
      "context" : "IMS (Zhong and Ng, 2010) was one of the most prevalent systems that trained a sense classifier for each lemma in training data.",
      "startOffset" : 4,
      "endOffset" : 24
    }, {
      "referenceID" : 17,
      "context" : "It learns a sense embedding for each labeled sense in SemCor (Miller et al., 1994) and maps them to full coverage of WordNet (Miller, 1995) senses using sense relations.",
      "startOffset" : 61,
      "endOffset" : 82
    }, {
      "referenceID" : 18,
      "context" : ", 1994) and maps them to full coverage of WordNet (Miller, 1995) senses using sense relations.",
      "startOffset" : 50,
      "endOffset" : 64
    }, {
      "referenceID" : 8,
      "context" : "BERT (Devlin et al., 2019) is used as a feature-extraction module for both gloss and context encoding.",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 5,
      "context" : "Further, BEM (Blevins and Zettlemoyer, 2020) utilizes two encoders for the above approach in a fine-tuning manner.",
      "startOffset" : 13,
      "endOffset" : 44
    }, {
      "referenceID" : 12,
      "context" : "In contrast, knowledge-based WSD approaches tend to more sufficiently exploit a word’s context, ranging from a sentence (Lesk, 1986; Wang and Wang, 2020), a few sentences (Agirre et al.",
      "startOffset" : 120,
      "endOffset" : 153
    }, {
      "referenceID" : 30,
      "context" : "In contrast, knowledge-based WSD approaches tend to more sufficiently exploit a word’s context, ranging from a sentence (Lesk, 1986; Wang and Wang, 2020), a few sentences (Agirre et al.",
      "startOffset" : 120,
      "endOffset" : 153
    }, {
      "referenceID" : 7,
      "context" : ", 2020) to even the whole document (Chaplot and Salakhutdinov, 2018).",
      "startOffset" : 35,
      "endOffset" : 68
    }, {
      "referenceID" : 24,
      "context" : "Some studies draw in out-of-dataset context (Ponzetto and Navigli, 2010; Scarlini et al., 2020a) for disambiguation, including Wikipedia documents.",
      "startOffset" : 44,
      "endOffset" : 96
    }, {
      "referenceID" : 27,
      "context" : "Some studies draw in out-of-dataset context (Ponzetto and Navigli, 2010; Scarlini et al., 2020a) for disambiguation, including Wikipedia documents.",
      "startOffset" : 44,
      "endOffset" : 96
    }, {
      "referenceID" : 16,
      "context" : "SyntagNet (Maru et al., 2019) improves the idea by introducing manually disambiguated sense pairs in context during sense graph construction.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : "learned using the sentence SSii where the word appears (Loureiro and Jorge, 2019; Scarlini et al., 2020a; Scarlini et al., 2020b).",
      "startOffset" : 55,
      "endOffset" : 129
    }, {
      "referenceID" : 27,
      "context" : "learned using the sentence SSii where the word appears (Loureiro and Jorge, 2019; Scarlini et al., 2020a; Scarlini et al., 2020b).",
      "startOffset" : 55,
      "endOffset" : 129
    }, {
      "referenceID" : 28,
      "context" : "learned using the sentence SSii where the word appears (Loureiro and Jorge, 2019; Scarlini et al., 2020a; Scarlini et al., 2020b).",
      "startOffset" : 55,
      "endOffset" : 129
    }, {
      "referenceID" : 5,
      "context" : "gloss/definition GGssjjjj defined in WordNet (Blevins and Zettlemoyer, 2020).",
      "startOffset" : 45,
      "endOffset" : 76
    }, {
      "referenceID" : 13,
      "context" : "A common approach of encoding these two sequences in recent research is to utilize pre-trained models such as BERT, RoBERTa (Liu et al., 2019), and so on, taking the",
      "startOffset" : 124,
      "endOffset" : 142
    }, {
      "referenceID" : 5,
      "context" : "In contrast, for each sense representation, when it is fine-tuning a pretrained model, the sense embedding is the output at the position of [CLS] (Blevins and Zettlemoyer, 2020), with the modified gloss as input, as in (2).",
      "startOffset" : 146,
      "endOffset" : 177
    }, {
      "referenceID" : 30,
      "context" : "Here, WNE is regarded as an extra sense gloss and is concatenated after the original sense gloss for sense embedding learning, which is similar to the implementation in SREF (Wang and Wang, 2020).",
      "startOffset" : 174,
      "endOffset" : 195
    }, {
      "referenceID" : 20,
      "context" : "from SemEval-2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015).",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 10,
      "context" : "EWISE (Kumar et al., 2019), LMMS (Loureiro and Jorge, 2019), GLU (Hadiwinoto et al.",
      "startOffset" : 6,
      "endOffset" : 26
    }, {
      "referenceID" : 14,
      "context" : ", 2019), LMMS (Loureiro and Jorge, 2019), GLU (Hadiwinoto et al.",
      "startOffset" : 14,
      "endOffset" : 40
    }, {
      "referenceID" : 9,
      "context" : ", 2019), LMMS (Loureiro and Jorge, 2019), GLU (Hadiwinoto et al., 2019), GlossBERT (Huang et al.",
      "startOffset" : 46,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : ", 2019), EWISER (Bevilacqua and Navigli, 2020), BEM (Blevins and Zettlemoyer, 2020), ARES (Scarlini et al.",
      "startOffset" : 16,
      "endOffset" : 46
    }, {
      "referenceID" : 5,
      "context" : ", 2019), EWISER (Bevilacqua and Navigli, 2020), BEM (Blevins and Zettlemoyer, 2020), ARES (Scarlini et al.",
      "startOffset" : 52,
      "endOffset" : 83
    }, {
      "referenceID" : 28,
      "context" : ", 2019), EWISER (Bevilacqua and Navigli, 2020), BEM (Blevins and Zettlemoyer, 2020), ARES (Scarlini et al., 2020b) and SREF (Wang and Wang, 2020).",
      "startOffset" : 90,
      "endOffset" : 114
    }, {
      "referenceID" : 2,
      "context" : "For cross-lingual evaluation, we compare our results with those reported in SyntagNet, EWISER, ARES, MuLaN (Barba et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 5,
      "context" : "For compute time/power, the proposed system requires less GPU amount (1 versus 2 GPUs) and time (10 versus about 70 hours) for training compared with its direct baseline (Blevins and Zettlemoyer, 2020).",
      "startOffset" : 170,
      "endOffset" : 201
    } ],
    "year" : 2021,
    "abstractText" : "Lately proposed Word Sense Disambiguation (WSD) systems have approached the estimated upper bound of the task on standard evaluation benchmarks. However, these systems typically implement the disambiguation of words in a document almost independently, underutilizing sense and word dependency in context. In this paper, we convert the nearly isolated decisions into interrelated ones by exposing senses in context when learning sense embeddings in a similaritybased Sense Aware Context Exploitation (SACE) architecture. Meanwhile, we enhance the context embedding learning with selected sentences from the same document, rather than utilizing only the sentence where each ambiguous word appears. Experiments on both English and multilingual WSD datasets have shown the effectiveness of our approach, surpassing previous state-of-the-art by large margins (3.7% and 1.2% respectively), especially on few-shot (14.3%) and zero-shot (35.9%) scenarios.",
    "creator" : "Acrobat PDFMaker 19 Word 版"
  }
}