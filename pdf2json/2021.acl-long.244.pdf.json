{
  "name" : "2021.acl-long.244.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Evaluating morphological typology in zero-shot cross-lingual transfer",
    "authors" : [ "Antonio Martı́nez-Garcı́a", "Toni Badia", "Jeremy Barnes" ],
    "emails" : [ "@gmail.com", "tbadia@upf.edu", "jeremycb@ifi.uio.no" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3136–3153\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3136"
    }, {
      "heading" : "1 Introduction",
      "text" : "Cross-lingual transfer uses available annotated resources in a source language to learn a model that will transfer to a target language. Earlier work used machine translation (Mihalcea et al., 2007), parallel data (Padó and Lapata, 2009), or delexicalized models (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) to bridge the gap between languages. However, recent improvements (Devlin et al., 2019) have reduced the need for parallel data, instead relying on multi-lingual language models, trained on the concatenation of monolingual corpora. Fine-tuning these multilingual language models on a task in a source language can lead to strong performance when applied directly to the targetlanguage task (zero-shot transfer).\nThis progress has uncovered gaps in performance, as transfer is generally easier between similar languages, and some language families consistently perform worse (Artetxe et al., 2020; Conneau et al., 2020a). So far, however, the analysis of these differences has only been anecdotal, rather than centered as a research question of its own merit. For these cases, linguistic typology has important implications, as it gives us ways to quantify the similarity of languages along certain variables, such as shared morphological or syntactic features (Bender, 2013). While previous work has studied the effects of morphological typology on language modeling (Gerz et al., 2018; Cotterell et al., 2018; Mielke et al., 2019), this effect on cross-lingual transfer has not been looked at in detail.\nIn this paper we attempt to answer (RQ1) to what degree morphological typology affects the performance of state-of-the-art cross-lingual models, (RQ2) whether morphological typology has a stronger effect than other variables, e.g., the amount of data for pretraining the LM or domain mismatches between source and target, (RQ3) whether there is a different effect on a low-level structural task (POS tagging) vs. a semantic task (sentiment analysis).\nTo answer these questions we experiment with two state-of-the-art cross-lingual models: multilingual BERT and XLM RoBERTa. We fine-tune the models for part-of-speech tagging and sentiment analysis on 19 languages from four morphologically diverse typologies. Our results show that POS tagging is more sensitive to morphological typology than sentiment analysis and that the models perform much better on fusional languages, such as German, than on the other typologies. We release the code and data1 in order to reproduce the experiments and facilitate future work in this area.\n1Code and data available at https://github.com/ jerbarnes/typology_of_crosslingual."
    }, {
      "heading" : "2 Related Work",
      "text" : "Cross-lingual transfer has become ubiquitous in recent years, including cross-lingual POS tagging (Täckström et al., 2013; Huck et al., 2019) and cross-lingual sentiment analysis (Mihalcea et al., 2007; Balahur and Turchi, 2014; Barnes and Klinger, 2019). While earlier research focused on annotation projection (Yarowsky et al., 2001; Banea et al., 2008) or cross-lingual embeddings (Kim et al., 2017; Artetxe et al., 2017; Barnes et al., 2018b), multi-lingual pretraining currently leads to state-of-the-art results (Devlin et al., 2019; Lample and Conneau, 2019). These approaches rely on training transformer-based language models (Vaswani et al., 2017) on unlabeled data from multiple languages, while using careful data selection methods to avoid the over-representation of larger languages.\nAlthough these approaches have led to large improvements on many cross-lingual tasks, it is clear that the success of zero-shot cross-lingual transfer depends on the typological similarity of the source and target language (Conneau et al., 2020b; Libovický et al., 2020). Pires et al. (2019) find POS performance correlates with word order features taken from the World Atlas of Language Structures (WALS) database (Dryer and Haspelmath, 2013). Similarly, morphologically complex languages tend to achieve poorer performance (Artetxe et al., 2020; Conneau et al., 2020a).\nSimilar to this work, Lauscher et al. (2020) perform zero-shot and few-shot transfer on 20 languages and 5 tasks. However, the choice of languages does not allow one to answer what is the effect of morphological typology.\nThe effect of morphological typology on NLP tasks is well known (Ponti et al., 2019), with several dedicated workshop series (Nicolai et al., 2020; Zampieri et al., 2018). More recently, attention has turned to larger scale analyses of morphological typology effects on language modeling (Gerz et al., 2018; Cotterell et al., 2018; Mielke et al., 2019).\nIn contrast to these previous works, we are interested in how morphological typology affects crosslingual transfer for two supervised tasks, namely part of speech (POS) tagging and sentiment analysis. We choose these two tasks as 1) they both have data available in typologically diverse languages, and 2) represent a lower-level structural and higher-level semantic task, respectively. Our experimental setup reduces some of the complexity\nof comparing test results across languages, as we compare relative differences, instead of absolute differences. At the same time, it is necessary to take into account several other variables, i.e., presence of the language in pretraining, the amount of training data, the effect of byte-pair tokenization, the length of train and test examples, and any domain mismatches across languages.\nAlthough it is a simplification of the variation in morphological features (Plank, 1999), languages have traditionally been grouped into four morphological categories, i.e., isolating, fusional, introflexive, and agglutinative.2 These categories describe a language’s tendency to group concepts together into a single word or disperse them into separate words. Pure isolating languages have maximally one morpheme per word. In agglutinative languages, morphemes tend to be neatly segmentable and carry a single feature, whereas in fusional languages, a single morpheme often carries multiple grammatic, syntactic, and semantic features. Finally, in introflexive languages root words are based on consonant stems, where vowels introduced around and between them lead to syntactic and semantic changes (see Plank (1999); Bickel and Nichols (2005); Gerz et al. (2018) for a more in-depth discussion)."
    }, {
      "heading" : "3 Data",
      "text" : "We select five languages from each category except introflexive (four), shown in Table 1. A short example sentence in a fusional (Norwegian ○ no ), isolating (Indonesian é in ), agglutinative\n(Basque eu ), and introflexive (Maltese mt ) language with glosses and translation in English is shown in Example 1.\n(1) ○ no\né in\neu\nmt\nBuss-en\nbus-DEF.ART\nBus\nbus\nAutobus-a\nbus-DEF.ART\nIx-xarabank\nDEF.ART-bus\nkom\ncome:PERF\nitu\nthat\nberandu\nlate\nwaslet\ncome:PERF\nsen-t\nlate-ADV\ndatang\ncome\netorri\ncome:PCP\ntard\nlate\nterlambat\nlate\nzen\nPRT.3S\n‘The bus came late.’\n2We use the following color combinations to denote é isolating , agglutinative , introflexive , and\n○ fusional languages.\nType Language Part-of-Speech Sentiment Analysis train dev test train dev test"
    }, {
      "heading" : "3.1 Part-of-speech",
      "text" : "We obtain the data for the part-of-speech tagging task from the Universal Dependencies project (Zeman et al., 2020), which currently gathers data annotated with universal POS tags for more than 90 languages, although there are differences in size and domain. For Algerian we use the annotations from Seddah et al. (2020). We found no training sets available for Thai and Cantonese, hence we use them for testing only. For more details on these datasets, see Table 5 in the Appendix."
    }, {
      "heading" : "3.2 Sentiment Analysis",
      "text" : "For sentiment analysis, however, there is no centralized repository of similar data. Therefore, we collect data from a number of sources and process\n3Including https://github.com/ dimitrakatseli/review_sentiment_analysis\n4https://github.com/ljw9609/ SentimentAnalysis\n5https://github.com/e9t/nsmc 6https://github.com/Darkmap/japanese_\nsentiment 7Including https://github.com/ozturkaslii/ analyze-turkish-sentiment\nthem to create binary (positive, negative) sentencelevel sentiment datasets. For convenience, we list the origin of each dataset in Table 2 and their full characteristics in Table 6 in the Appendix."
    }, {
      "heading" : "4 Methods",
      "text" : "We fine-tune both multilingual BERT (mBERT) (Xu et al., 2019) and XLM RoBERTa (XLM-R) (Conneau et al., 2020a) models on the available training data in each language, using a shared set of hyperparameters selected from recommended values according to the characteristics of our data. We set the learning rate to 2e-5, maximum sequence length of 256, batch size of 8 or 168, and perform early stopping once the validation score has not improved in the last epochs, saving the model that performs best on the dev set. We then test each model on all languages, giving us a matrix of test scores, where the diagonal is in-language, and all others are cross-lingual. We use accuracy as our metric for POS and macro F1 for sentiment, as the latter often contains unbalanced classes, and define\n8Depending on the size of the training set, model architecture and available GPU memory.\na baseline as the result of predicting the majority class."
    }, {
      "heading" : "5 Results",
      "text" : "Once our scores matrix is built, we average9 the score of each fine-tuned model, which we refer to as language-to-language cross-lingual scores, over the other languages in each morphological group, thus obtaining each model’s average cross-lingual performance per target group (language-to-group cross-lingual scores). Next, we average again for each source language group. This yields the average cross-lingual performance values per training and testing language groups (group-to-group crosslingual scores), which we report in Table 3.\nIn the part-of-speech task, the best group-togroup cross-lingual performance always corresponds to models fine-tuned in a language of\n9Note that, throughout this paper, when we average across morphological groups, we do so with a weighted average so that all groups are equally represented regardless of how many languages they include.\nthe same morphological group, regardless of the model’s architecture. Fusional models, in particular, obtain a remarkably higher score when tested on other fusional languages (over 80%). On the other hand, the group-to-group cross-lingual scores where the target language is introflexive are considerably lower than the rest (always below 50%).\nIn contrast, both model architectures show different patterns in the sentiment analysis task. For the XLM-R models, the best group-to-group crosslingual scores are all achieved by those trained on a fusional language, while for the mBERT it is mainly models trained on an isolating language that achieve the best scores. In any case, all scores are within a similar range of values. In fact, the main difference in this task seems to be due to XLM-R’s considerably higher scores.\nIn order to capture the cross-lingual phenomenon more accurately, we introduce transfer loss, a relative metric defined in Equation 1:\nTLx→y = Sx→x − Sx→y (1)\nwhere TLx→y is the transfer loss experienced by a model fine-tuned in language x when transferring to language y (language-to-language transfer loss) and Sx→y is the score10 achieved when testing a model fine-tuned in language x on language y. Thus, it is a measure of the performance lost in the zero-shot transfer process: the better the transfer between both languages, the lower it will be.\nWe also define its averaged variants:\nTLx→A = Sx→x − 1\nNA ∑ i∈A i 6=x Sx→i (2)\nTLA→B = 1\nNA ∑ i∈A TLi→B (3)\nwhere TLx→A denotes the average transfer loss from language x to languages belonging to morphological type A (language-to-group transfer loss), TLA→B refers to the average transfer loss experienced by languages from morphological group A to languages from group B (group-to-group transfer loss) and NA is the number of languages (other than x) included in the experiment that belong to group A. Table 4 shows the resulting group-togroup transfer loss values for each task.\n10The score metric will depend on the task: accuracy in POS and macro F1 in sentiment analysis.\nModels fine-tuned in all groups except agglutinative experience the lowest performance drop when transferring to fusional languages in the part-ofspeech task, whereas in the sentiment analysis task there is no clear pattern. It is also worth noting that the XLM-R models tend to transfer better compared to mBERT, only slightly in part-of-speech tagging but more drastically in sentiment analysis. Additionally, the cases of worst transfer happen when the target language is introflexive (especially for XLM-R).\nNext, to address RQ1 more directly, we compare two different types of transfer: intra-group transfer, where both the fine-tuning and target languages belong to the same morphological group, and inter-group transfer, where the two differ in morphological type. We calculate an average for both types of transfer and for each training group, model architecture and task. We present the resulting values in Figure 1.\nGenerally, transfer to another morphological type implies a higher cost in terms of performance, except for the introflexive models. This difference in transfer loss appears to be similar for all groups in the sentiment task, yet it varies considerably in the part-of-speech task. More specifically, there are two extremes in this latter case: fusional models suffer large performance drops when switching morphological groups, whereas isolating models experience similar transfer losses in both conditions.\nFinally, we average again to obtain a single trans-\nfer loss value for each task and model, and use it to establish a comparison in Figure 2. Here we observe that: (1) the difference in transfer loss between an intra-group and inter-group transfer is higher on the part-of-speech task, (2) transfer is also generally worse on this task11, (3) XLMR models perform better cross-lingual transfers in general (especially on the sentiment analysis task), and (4) the difference between intra-group and inter-group transfer is similar on both model architectures."
    }, {
      "heading" : "6 Analysis",
      "text" : "In this section, we run several statistical tests to verify our conclusion to RQ1 and detail several points of analysis that relate to RQ2 and RQ3. Namely, to what degree do other variables contribute to effects on cross-lingual transfer."
    }, {
      "heading" : "6.1 Testing the effect of transfer type",
      "text" : "We run a set of statistical tests to validate the observations made from Figure 2 in Section 5. In the part-of-speech tagging task, an analysis of variance (ANOVA) reveals there is a statistically significant, although weak, difference in transfer loss between the intra- and inter-group conditions, for both model architectures (η2 ≈ 0.06, p < 0.01 in both cases). In contrast, a Kruskal-Wallis analysis of variance12 finds no significant difference\n11Strictly speaking, we use different metrics for both tasks, which are not necessarily comparable.\n12The normality condition for ANOVA is not met.\nbetween the two types of transfer in the sentiment analysis task, in neither mBERT or XLM-R models (p > 0.01 in both cases). We also test for differences in transfer loss between model architectures and find a significant difference in the sentiment analysis task (Kruskal-Wallis, p < 0.01),\nbut not in the part-of speech tagging task (ANOVA, p > 0.01). This is all consistent with our previous observations."
    }, {
      "heading" : "6.2 Linear regression model for transfer loss",
      "text" : "Additionally, we model language-to-language transfer loss with a linear regression model, using transfer type, as well as other variables, as possible predictors. This allows us to (a) test whether the intra-/inter-group difference retains its statistical significance in the presence of other variables and (b) evaluate its effect in comparison to other predictors.\nFirst, we select a set of variables that might be relevant in cross-lingual transfer, and remove those that are highly correlated with the rest to avoid multicollinearity in the model (see Table 7 in the Appendix for the final list of selected variables). We standardize all of the remaining features so that their units are comparable and, consequently, so are their regression coefficients.\nAgain, we find transfer type (intra-/inter-group) to be a significant predictor in both regression models for part-of-speech tagging (p < 0.01), but not in sentiment analysis. In the former case, it has the second strongest effect with a standardized coefficient of 8.613, the first being presence of the target language in pretraining with a coefficient of -25.9. In other words, transferring to a language on which the model has not been pretrained implies an additional performance drop of 25.9 percentage points, while transferring to another morphological group incurs an additional 8.6.\nThe remaining predictors for this task are average test example length (measured in tokens, coefficient of 4.0) and in-language score (3.3). The first is a complex variable because differences in text length can be due to their domain or to the lan-\n13Since the regression models for mBERT and XLM-R are quite similar, we report the averaged coefficients here.\nguages themselves but, in either case, its coefficient confirms our intuition that longer sequences generally make the task more difficult. The second could indicate some overfitting to the fine-tuning language, as higher in-language score entails slightly poorer transfer.\nXLM-R adds another predictor: the proportion of words that have been split into subword tokens in the test data (2.1). This variable is related to the size of the pretraining corpus for each language14: a richer pretraining vocabulary will ensure more words are considered frequent during Byte Pair Encoding and, therefore, assigned a single token, instead of being broken down into subword tokens by the tokenizer. This means that high-resource languages will have a lower word split probability and, hence, it will be slightly easier to transfer to them. However, it is worth pointing out that this bias has little effect and is only statistically significant in XLM-R.\nIn the case of sentiment analysis, relevant predictors are: presence of the fine-tuning (coefficient of -11.8 for mBERT and -18.7 for XLM-R) and target (-10.3 and -16.3) languages in pretraining, inlanguage score (6.8 and 6.5), proportion of words split into subword tokens in the training data (3.3 and 2.7) and proportion of examples labeled as positive in the test set (-2.8, XLM-R only).\nCuriously, sentiment analysis is more sensitive to variables related to the training data compared to part-of-speech tagging, whereas sequence length only affects the latter. On the other hand, language inclusion in pretraining and in-language score are useful predictors in both tasks, yet the former is far stronger in POS and the latter is more relevant in sentiment analysis. In summary, we verify that transferring to a different morphological type has a relevant effect in part-of-speech tagging but not in sentiment analysis, regardless of the model architecture."
    }, {
      "heading" : "6.3 Testing pretrained languages only",
      "text" : "Given the considerable effect pretraining seems to have on transfer loss (discussed in Section 6.2), we re-evaluate our results after removing the languages that were not present during the pretraining of either of the two model architectures (Cantonese, Algerian and Maltese) and check whether there are relevant differences with our previous results.\n14In fact, we do not include pretraining data size as a predictor because of its correlation with the variable in question.\nOf course, we observe an improvement in crosslingual scores involving either an isolating or an introflexive language, because these are the groups the excluded languages belong to. Overall, however, re-running the statistical tests does not modify our previous conclusions (see Figure 3)."
    }, {
      "heading" : "6.4 Balanced in-language scores",
      "text" : "Since in-language score is relevant in all regression models considered in 6.2 (and the value of transfer loss is relative to it), we decide to re-train all models, this time preventing them from increasing said score above a fixed threshold value (we choose the minimum in-language score achieved previously in each task and model architecture) and re-evaluate our previous conclusions.\nThe intra-/inter-group difference in transfer loss is still statistically significant in part-of-speech tagging and not in sentiment analysis. Similarly, there is still a statistically significant difference in transfer loss between both models only in the sentiment analysis task. All of this can be seen in Figure 3. The only remarkable difference is in the part-ofspeech task, where the average inter-group transfer\nloss values for all morphological groups seem to converge to the same value (see Figure 5 in the Appendix). For more information, see Figures 5 and 6, as well as Tables 8 and 9, all of which can be found in the Appendix."
    }, {
      "heading" : "6.5 Effect of training data size",
      "text" : "We also test the effect that training with considerably more data has on cross-lingual transfer. We select two languages, each with around 150,000 examples available: German for the part-of-speech tagging task and Korean for sentiment analysis. We train four models with increasingly more data and then test them on all languages.\nIn German, we notice an important decline in cross-lingual scores when increasing data size from 80,000 to 150,000 examples (see Figure 4). More specifically, in mBERT models there is an average decrease of 15.6 and 9.0 points when the crosslingual transfer is intra- and inter-group, respectively. In XLM-R, the corresponding values are 25.4 and 19.5. Hence, it appears that a phenomenon of language specialization takes place, one to which XLM-R is more susceptible and that has more important consequences in intra-group transfer. To ensure this is a language and not a domain/dataset specialization, we test these models on another\nGerman dataset (PUD) and find no decrease in performance.\nIn contrast, average Korean cross-lingual scores remain relatively constant (see Figure 4). Therefore, the language specialization phenomenon could be more characteristic of part-of-speech tagging than sentiment analysis."
    }, {
      "heading" : "6.6 Domain effects",
      "text" : "Conneau et al. (2020b) find that domain mismatch in pretraining of multilingual LMs is more problematic than domain mismatch in fine-tuning. Yet given the variety of domains present in the sentiment data, we decided to test its effect. Proxy A-distance (Glorot et al., 2011) measures the generalization error of a linear SVM trained to discriminate between two domains. We translate 1000 sentences from each dataset to English using GoogleTranslate and then compute the proxy A-distance.15 For POS tagging, there are small but insignificant negative effects of proxy A-distance on results for both models (a Pearson coefficient of -0.07, p > 0.01 and -0.07, p > 0.01 for mBERT and XLM-R, respectively). On the sentiment task, there is no significant domain effect for mBERT (-0.06, p > 0.01), while there is a small negative effect for XLM-R (-0.27, p < 0.01). This suggests that most of the transfer loss is not due to domain mismatch."
    }, {
      "heading" : "7 Discussion and Future Work",
      "text" : "In this paper, we have conducted an extensive analysis of the effects of morphological typology on cross-lingual transfer and attempted to isolate these factors from other variables. We have compared performance of two state-of-the-art zero-shot cross-lingual models on two tasks (part-of-speech tagging and sentiment analysis) for 19 languages across four morphological typologies. We have found that transfer to another morphological type generally implies a higher performance loss than transfer to another language with the same morphological typology. Additionally, part-of-speech tagging is more sensitive to morphological differences than sentiment analysis, while sentiment analysis is more sensitive to variables related to the fine-tuning data and is less predictable in general.\nWe have tested this sensitivity to morphology after balancing other influential factors, such as\n15Implementation adapted from the code available at https://github.com/rpryzant/ proxy-a-distance.\nin-language score, and, still, the intra-/inter-group difference remains. However, the effect of morphological typology, while significant, is not strong, given that most of the variability in transfer loss is due to other factors.\nWe have also confirmed that XLM-R generally transfers better than mBERT, especially on sentiment analysis. In part-of-speech tagging, we have reported considerably better transfer within fusional languages, as well as easier transfer from the other groups towards the fusional type. Moreover, we have found a case that suggests that finetuning on large training sets might lead to language specialization and, consequently, be detrimental to cross-lingual transfer.\nIt is worth noting that we do not explore whether the type of script used by the languages has an effect on cross-lingual transfer. This is hard to control in our experimental setup, as there are some scripts that are either unique to a language or only have one with enough data to represent it, making it impossible to make comparisons.\nThe recent cross-lingual suite Xtreme (Hu et al., 2020) includes a number of benchmark tasks in 40 languages. While this dataset is a useful collection of cross-lingual tasks, it is unfortunately not sufficient for our purposes. The POS data is the same as we use, while other tasks either a) do not contain a representative sample of language typologies b) use translation, introducing problems of ‘translationese’, or c) are automatically created and not manually curated Named Entity Recognition data. Our experimental setup avoids these problems by focusing on binary sentiment analysis, which is a task that has data available in many languages and does not require translation to get multilingual data.\nFinally, this work ties in with the increasing interest in typological questions in NLP (Takamura et al., 2016; Ponti et al., 2019; Bjerva et al., 2019; Nooralahzadeh et al., 2020; Bjerva and Augenstein, 2021), which often try to directly predict typological features, or use these to analyze model performance.\nIn the future, it would be interesting to train multi-lingual language models on specific language families in order to find maximal benefits from shared morphology. Finally, as typology seems to affect tasks differently, it would be interesting to explore other tasks, e.g., dependency parsing or semantic role labeling."
    }, {
      "heading" : "A Appendix",
      "text" : "Language Text Type Domain Annotation Examples Train % Dev/Test %\nIntra- and Inter-Group Transfer Losses in Part-of-Speech (Balanced In-language Scores)\nIntra- and Inter-Group Transfer Losses in Sentiment Analysis (Balanced In-language Scores)"
    } ],
    "references" : [ {
      "title" : "Arabic sentiment analysis: Lexicon-based and corpus-based",
      "author" : [ "Nawaf Abdulla", "Nizar A. Ahmed", "Mohammed Shehab", "Mahmoud Al-Ayyoub." ],
      "venue" : "pages 1–",
      "citeRegEx" : "Abdulla et al\\.,? 2013",
      "shortCiteRegEx" : "Abdulla et al\\.",
      "year" : 2013
    }, {
      "title" : "Opener: Open polarity enhanced named entity recognition",
      "author" : [ "Rodrigo Agerri", "Montse Cuadros", "Seán Gaines", "German Rigau." ],
      "venue" : "Procesamiento del Lenguaje Natural, 51(0):215–218.",
      "citeRegEx" : "Agerri et al\\.,? 2013",
      "shortCiteRegEx" : "Agerri et al\\.",
      "year" : 2013
    }, {
      "title" : "Representations and architectures in neural sentiment analysis for morphologically rich languages: A case study from modern Hebrew",
      "author" : [ "Adam Amram", "Anat Ben David", "Reut Tsarfaty." ],
      "venue" : "Proceedings of the 27th International Conference on",
      "citeRegEx" : "Amram et al\\.,? 2018",
      "shortCiteRegEx" : "Amram et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning bilingual word embeddings with (almost) no bilingual data",
      "author" : [ "Mikel Artetxe", "Gorka Labaka", "Eneko Agirre." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 451–462,",
      "citeRegEx" : "Artetxe et al\\.,? 2017",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2017
    }, {
      "title" : "On the cross-lingual transferability of monolingual representations",
      "author" : [ "Mikel Artetxe", "Sebastian Ruder", "Dani Yogatama." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623–4637, Online. Asso-",
      "citeRegEx" : "Artetxe et al\\.,? 2020",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2020
    }, {
      "title" : "Comparative experiments using supervised learning and machine translation for multilingual sentiment analysis",
      "author" : [ "Alexandra Balahur", "Marco Turchi." ],
      "venue" : "Computer Speech Language, 28(1):56 – 75.",
      "citeRegEx" : "Balahur and Turchi.,? 2014",
      "shortCiteRegEx" : "Balahur and Turchi.",
      "year" : 2014
    }, {
      "title" : "Multilingual subjectivity analysis using machine translation",
      "author" : [ "Carmen Banea", "Rada Mihalcea", "Janyce Wiebe", "Samer Hassan." ],
      "venue" : "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 127–135, Honolulu,",
      "citeRegEx" : "Banea et al\\.,? 2008",
      "shortCiteRegEx" : "Banea et al\\.",
      "year" : 2008
    }, {
      "title" : "MultiBooked: A corpus of basque and Catalan hotel reviews annotated for aspect-level sentiment classification",
      "author" : [ "Jeremy Barnes", "Toni Badia", "Patrik Lambert." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Eval-",
      "citeRegEx" : "Barnes et al\\.,? 2018a",
      "shortCiteRegEx" : "Barnes et al\\.",
      "year" : 2018
    }, {
      "title" : "Embedding projection for targeted cross-lingual sentiment: Model comparisons and a real-world study",
      "author" : [ "Jeremy Barnes", "Roman Klinger." ],
      "venue" : "Journal of Artificial Intelligence Research, 66:691–742.",
      "citeRegEx" : "Barnes and Klinger.,? 2019",
      "shortCiteRegEx" : "Barnes and Klinger.",
      "year" : 2019
    }, {
      "title" : "Bilingual sentiment embeddings",
      "author" : [ "Jeremy Barnes", "Roman Klinger", "Sabine Schulte im Walde" ],
      "venue" : null,
      "citeRegEx" : "Barnes et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Barnes et al\\.",
      "year" : 2018
    }, {
      "title" : "Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Morphology and Syntax",
      "author" : [ "Emily M. Bender." ],
      "venue" : "Morgan amp; Claypool Publishers.",
      "citeRegEx" : "Bender.,? 2013",
      "shortCiteRegEx" : "Bender.",
      "year" : 2013
    }, {
      "title" : "Inflectional morphology",
      "author" : [ "Balthasar Bickel", "Johanna Nichols." ],
      "venue" : "Timothy Shopen, editor, Language Typology and Syntactic Description. Cambridge University Press, Cambridge. 2nd edition.",
      "citeRegEx" : "Bickel and Nichols.,? 2005",
      "shortCiteRegEx" : "Bickel and Nichols.",
      "year" : 2005
    }, {
      "title" : "Does typological blinding impede cross-lingual sharing",
      "author" : [ "Johannes Bjerva", "Isabelle Augenstein" ],
      "venue" : null,
      "citeRegEx" : "Bjerva and Augenstein.,? \\Q2021\\E",
      "shortCiteRegEx" : "Bjerva and Augenstein.",
      "year" : 2021
    }, {
      "title" : "Uncovering probabilistic implications in typological knowledge bases",
      "author" : [ "Johannes Bjerva", "Yova Kementchedjhieva", "Ryan Cotterell", "Isabelle Augenstein." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Bjerva et al\\.,? 2019",
      "shortCiteRegEx" : "Bjerva et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020a",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Emerging cross-lingual structure in pretrained language models",
      "author" : [ "Alexis Conneau", "Shijie Wu", "Haoran Li", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Conneau et al\\.,? 2020b",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "A social opinion gold standard for the Malta government budget 2018",
      "author" : [ "Keith Cortis", "Brian Davis." ],
      "venue" : "Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019), pages 364–369, Hong Kong, China. Association for Computational",
      "citeRegEx" : "Cortis and Davis.,? 2019",
      "shortCiteRegEx" : "Cortis and Davis.",
      "year" : 2019
    }, {
      "title" : "Are all languages equally hard to language-model",
      "author" : [ "Ryan Cotterell", "Sebastian J. Mielke", "Jason Eisner", "Brian Roark" ],
      "venue" : "In Proceedings of the 2018 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Cotterell et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Cotterell et al\\.",
      "year" : 2018
    }, {
      "title" : "Vlsp 2016 shared task: Vietnamese analysis",
      "author" : [ "Le Anh Cuong", "Ng. T. Minh Huyen", "Ng. Viet Hung." ],
      "venue" : "VLSP 2016.",
      "citeRegEx" : "Cuong et al\\.,? 2016",
      "shortCiteRegEx" : "Cuong et al\\.",
      "year" : 2016
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Sentiment analysis on maltese using machine learning",
      "author" : [ "Alexiei Dingli", "Nicole Sant." ],
      "venue" : "Proceedings of The Tenth International Conference on Advances in Semantic Processing (SEMAPRO 2016), pages 21–25.",
      "citeRegEx" : "Dingli and Sant.,? 2016",
      "shortCiteRegEx" : "Dingli and Sant.",
      "year" : 2016
    }, {
      "title" : "On the relation between linguistic typology and (limitations of) multilingual language modeling",
      "author" : [ "Daniela Gerz", "Ivan Vulić", "Edoardo Maria Ponti", "Roi Reichart", "Anna Korhonen." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Gerz et al\\.,? 2018",
      "shortCiteRegEx" : "Gerz et al\\.",
      "year" : 2018
    }, {
      "title" : "Domain adaptation for large-scale sentiment classification: A deep learning approach",
      "author" : [ "Xavier Glorot", "Antoine Bordes", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 28th International Conference on International Conference on Machine Learning,",
      "citeRegEx" : "Glorot et al\\.,? 2011",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2011
    }, {
      "title" : "Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization",
      "author" : [ "Junjie Hu", "Sebastian Ruder", "Aditya Siddhant", "Graham Neubig", "Orhan Firat", "Melvin Johnson" ],
      "venue" : null,
      "citeRegEx" : "Hu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "Cross-lingual annotation projection is effective for neural part-of-speech tagging",
      "author" : [ "Matthias Huck", "Diana Dutka", "Alexander Fraser." ],
      "venue" : "Proceedings of the Sixth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 223–233,",
      "citeRegEx" : "Huck et al\\.,? 2019",
      "shortCiteRegEx" : "Huck et al\\.",
      "year" : 2019
    }, {
      "title" : "Sentiment analysis of greek tweets and hashtags using a sentiment lexicon",
      "author" : [ "Georgios Kalamatianos", "Dimitrios Mallis", "Symeon Symeonidis", "Avi Arampatzis." ],
      "venue" : "pages 63–68.",
      "citeRegEx" : "Kalamatianos et al\\.,? 2015",
      "shortCiteRegEx" : "Kalamatianos et al\\.",
      "year" : 2015
    }, {
      "title" : "Cross-lingual transfer learning for POS tagging without cross-lingual resources",
      "author" : [ "Joo-Kyung Kim", "Young-Bum Kim", "Ruhi Sarikaya", "Eric Fosler-Lussier." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Kim et al\\.,? 2017",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2017
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Guillaume Lample", "Alexis Conneau." ],
      "venue" : "Advances in Neural Information Processing Systems (NeurIPS).",
      "citeRegEx" : "Lample and Conneau.,? 2019",
      "shortCiteRegEx" : "Lample and Conneau.",
      "year" : 2019
    }, {
      "title" : "From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers",
      "author" : [ "Anne Lauscher", "Vinit Ravishankar", "Ivan Vulić", "Goran Glavaš." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Lauscher et al\\.,? 2020",
      "shortCiteRegEx" : "Lauscher et al\\.",
      "year" : 2020
    }, {
      "title" : "On the language neutrality of pre-trained multilingual representations",
      "author" : [ "Jindřich Libovický", "Rudolf Rosa", "Alexander Fraser." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1663–1674, Online. Association for Computa-",
      "citeRegEx" : "Libovický et al\\.,? 2020",
      "shortCiteRegEx" : "Libovický et al\\.",
      "year" : 2020
    }, {
      "title" : "Finnsentiment – a finnish social media corpus for sentiment polarity annotation",
      "author" : [ "Krister Lindén", "Tommi Jauhiainen", "Sam Hardwick" ],
      "venue" : null,
      "citeRegEx" : "Lindén et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lindén et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-source transfer of delexicalized dependency parsers",
      "author" : [ "Ryan McDonald", "Slav Petrov", "Keith Hall." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 62–72, Edinburgh, Scotland, UK. Association",
      "citeRegEx" : "McDonald et al\\.,? 2011",
      "shortCiteRegEx" : "McDonald et al\\.",
      "year" : 2011
    }, {
      "title" : "What kind of language is hard to language-model",
      "author" : [ "Sebastian J. Mielke", "Ryan Cotterell", "Kyle Gorman", "Brian Roark", "Jason Eisner" ],
      "venue" : "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Mielke et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Mielke et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning multilingual subjective language via cross-lingual projections",
      "author" : [ "Rada Mihalcea", "Carmen Banea", "Janyce Wiebe." ],
      "venue" : "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 976–983, Prague, Czech Repub-",
      "citeRegEx" : "Mihalcea et al\\.,? 2007",
      "shortCiteRegEx" : "Mihalcea et al\\.",
      "year" : 2007
    }, {
      "title" : "ASTD: Arabic sentiment tweets dataset",
      "author" : [ "Mahmoud Nabil", "Mohamed Aly", "Amir Atiya." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2515– 2519, Lisbon, Portugal. Association for Computa-",
      "citeRegEx" : "Nabil et al\\.,? 2015",
      "shortCiteRegEx" : "Nabil et al\\.",
      "year" : 2015
    }, {
      "title" : "Zero-shot cross-lingual transfer with meta learning",
      "author" : [ "Farhad Nooralahzadeh", "Giannis Bekoulis", "Johannes Bjerva", "Isabelle Augenstein." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Nooralahzadeh et al\\.,? 2020",
      "shortCiteRegEx" : "Nooralahzadeh et al\\.",
      "year" : 2020
    }, {
      "title" : "A fine-grained sentiment dataset for Norwegian",
      "author" : [ "Lilja Øvrelid", "Petter Mæhlum", "Jeremy Barnes", "Erik Velldal." ],
      "venue" : "Proceedings of the 12th Language",
      "citeRegEx" : "Øvrelid et al\\.,? 2020",
      "shortCiteRegEx" : "Øvrelid et al\\.",
      "year" : 2020
    }, {
      "title" : "Token and type constraints for cross-lingual part-of-speech tagging",
      "author" : [ "Oscar Täckström", "Dipanjan Das", "Slav Petrov", "Ryan McDonald", "Joakim Nivre." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 1:1–12.",
      "citeRegEx" : "Täckström et al\\.,? 2013",
      "shortCiteRegEx" : "Täckström et al\\.",
      "year" : 2013
    }, {
      "title" : "Discriminative analysis of linguistic features for typological study",
      "author" : [ "Hiroya Takamura", "Ryo Nagata", "Yoshifumi Kawasaki." ],
      "venue" : "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016), pages 69–76,",
      "citeRegEx" : "Takamura et al\\.,? 2016",
      "shortCiteRegEx" : "Takamura et al\\.",
      "year" : 2016
    }, {
      "title" : "The interplay between language similarity and script on a novel multi-layer Algerian dialect corpus",
      "author" : [ "Samia Touileb", "Jeremy Barnes." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics, Online. Association for",
      "citeRegEx" : "Touileb and Barnes.,? 2021",
      "shortCiteRegEx" : "Touileb and Barnes.",
      "year" : 2021
    }, {
      "title" : "Building and evaluating resources for sentiment analysis in the greek language",
      "author" : [ "Adam Tsakalidis", "Symeon Papadopoulos", "Rania Voskaki", "Kyriaki Ioannidou", "Christina Boididou", "Alexandra I Cristea", "Maria Liakata", "Yiannis Kompatsiaris" ],
      "venue" : null,
      "citeRegEx" : "Tsakalidis et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Tsakalidis et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30, pages 5998–6008. Cur-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Germeval 2017: Shared task on aspect-based sentiment in social media customer feedback",
      "author" : [ "Michael Wojatzki", "Eugen Ruppert", "Sarah Holschneider", "Torsten Zesch", "Chris Biemann." ],
      "venue" : "Proceedings of the GermEval 2017 – Shared Task on Aspect-based Sen-",
      "citeRegEx" : "Wojatzki et al\\.,? 2017",
      "shortCiteRegEx" : "Wojatzki et al\\.",
      "year" : 2017
    }, {
      "title" : "Sentiment augmented attention network for cantonese restaurant review analysis",
      "author" : [ "Rong Xiang" ],
      "venue" : null,
      "citeRegEx" : "Xiang.,? \\Q2019\\E",
      "shortCiteRegEx" : "Xiang.",
      "year" : 2019
    }, {
      "title" : "BERT post-training for review reading comprehension and aspect-based sentiment analysis",
      "author" : [ "Hu Xu", "Bing Liu", "Lei Shu", "Philip Yu." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Xu et al\\.,? 2019",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2019
    }, {
      "title" : "Inducing multilingual text analysis",
      "author" : [ "David Yarowsky", "Grace Ngai", "Richard Wicentowski" ],
      "venue" : null,
      "citeRegEx" : "Yarowsky et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Yarowsky et al\\.",
      "year" : 2001
    }, {
      "title" : "Universal dependencies 2.6",
      "author" : [ "Tak-sum Wong", "Alina Wróblewska", "Mary Yako", "Kayo Yamashita", "Naoki Yamazaki", "Chunxiao Yan", "Koichi Yasuoka", "Marat M. Yavrumyan", "Zhuoran Yu", "Zdeněk Žabokrtský", "Amir Zeldes", "Hanzhi Zhu", "Anna Zhuravleva" ],
      "venue" : null,
      "citeRegEx" : "Wong et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wong et al\\.",
      "year" : 2020
    }, {
      "title" : "Crosslanguage parser adaptation between related languages",
      "author" : [ "Daniel Zeman", "Philip Resnik." ],
      "venue" : "Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages.",
      "citeRegEx" : "Zeman and Resnik.,? 2008",
      "shortCiteRegEx" : "Zeman and Resnik.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 33,
      "context" : "Earlier work used machine translation (Mihalcea et al., 2007), parallel data (Padó and Lapata, 2009), or delexicalized models (Zeman and Resnik, 2008; McDonald et al.",
      "startOffset" : 38,
      "endOffset" : 61
    }, {
      "referenceID" : 47,
      "context" : ", 2007), parallel data (Padó and Lapata, 2009), or delexicalized models (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) to bridge the gap between languages.",
      "startOffset" : 72,
      "endOffset" : 134
    }, {
      "referenceID" : 31,
      "context" : ", 2007), parallel data (Padó and Lapata, 2009), or delexicalized models (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) to bridge the gap between languages.",
      "startOffset" : 72,
      "endOffset" : 134
    }, {
      "referenceID" : 19,
      "context" : "However, recent improvements (Devlin et al., 2019) have reduced the need for parallel data, instead relying on multi-lingual language models, trained on the concatenation of monolingual corpora.",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 4,
      "context" : "This progress has uncovered gaps in performance, as transfer is generally easier between similar languages, and some language families consistently perform worse (Artetxe et al., 2020; Conneau et al., 2020a).",
      "startOffset" : 162,
      "endOffset" : 207
    }, {
      "referenceID" : 14,
      "context" : "This progress has uncovered gaps in performance, as transfer is generally easier between similar languages, and some language families consistently perform worse (Artetxe et al., 2020; Conneau et al., 2020a).",
      "startOffset" : 162,
      "endOffset" : 207
    }, {
      "referenceID" : 10,
      "context" : "For these cases, linguistic typology has important implications, as it gives us ways to quantify the similarity of languages along certain variables, such as shared morphological or syntactic features (Bender, 2013).",
      "startOffset" : 201,
      "endOffset" : 215
    }, {
      "referenceID" : 21,
      "context" : "While previous work has studied the effects of morphological typology on language modeling (Gerz et al., 2018; Cotterell et al., 2018; Mielke et al., 2019), this effect on cross-lingual transfer has not been looked at in detail.",
      "startOffset" : 91,
      "endOffset" : 155
    }, {
      "referenceID" : 17,
      "context" : "While previous work has studied the effects of morphological typology on language modeling (Gerz et al., 2018; Cotterell et al., 2018; Mielke et al., 2019), this effect on cross-lingual transfer has not been looked at in detail.",
      "startOffset" : 91,
      "endOffset" : 155
    }, {
      "referenceID" : 32,
      "context" : "While previous work has studied the effects of morphological typology on language modeling (Gerz et al., 2018; Cotterell et al., 2018; Mielke et al., 2019), this effect on cross-lingual transfer has not been looked at in detail.",
      "startOffset" : 91,
      "endOffset" : 155
    }, {
      "referenceID" : 37,
      "context" : "Cross-lingual transfer has become ubiquitous in recent years, including cross-lingual POS tagging (Täckström et al., 2013; Huck et al., 2019) and cross-lingual sentiment analysis (Mihalcea et al.",
      "startOffset" : 98,
      "endOffset" : 141
    }, {
      "referenceID" : 24,
      "context" : "Cross-lingual transfer has become ubiquitous in recent years, including cross-lingual POS tagging (Täckström et al., 2013; Huck et al., 2019) and cross-lingual sentiment analysis (Mihalcea et al.",
      "startOffset" : 98,
      "endOffset" : 141
    }, {
      "referenceID" : 33,
      "context" : ", 2019) and cross-lingual sentiment analysis (Mihalcea et al., 2007; Balahur and Turchi, 2014; Barnes and Klinger, 2019).",
      "startOffset" : 45,
      "endOffset" : 120
    }, {
      "referenceID" : 5,
      "context" : ", 2019) and cross-lingual sentiment analysis (Mihalcea et al., 2007; Balahur and Turchi, 2014; Barnes and Klinger, 2019).",
      "startOffset" : 45,
      "endOffset" : 120
    }, {
      "referenceID" : 8,
      "context" : ", 2019) and cross-lingual sentiment analysis (Mihalcea et al., 2007; Balahur and Turchi, 2014; Barnes and Klinger, 2019).",
      "startOffset" : 45,
      "endOffset" : 120
    }, {
      "referenceID" : 45,
      "context" : "While earlier research focused on annotation projection (Yarowsky et al., 2001; Banea et al., 2008) or cross-lingual embeddings (Kim et al.",
      "startOffset" : 56,
      "endOffset" : 99
    }, {
      "referenceID" : 6,
      "context" : "While earlier research focused on annotation projection (Yarowsky et al., 2001; Banea et al., 2008) or cross-lingual embeddings (Kim et al.",
      "startOffset" : 56,
      "endOffset" : 99
    }, {
      "referenceID" : 26,
      "context" : ", 2008) or cross-lingual embeddings (Kim et al., 2017; Artetxe et al., 2017; Barnes et al., 2018b), multi-lingual pretraining currently leads to state-of-the-art results (Devlin et al.",
      "startOffset" : 36,
      "endOffset" : 98
    }, {
      "referenceID" : 3,
      "context" : ", 2008) or cross-lingual embeddings (Kim et al., 2017; Artetxe et al., 2017; Barnes et al., 2018b), multi-lingual pretraining currently leads to state-of-the-art results (Devlin et al.",
      "startOffset" : 36,
      "endOffset" : 98
    }, {
      "referenceID" : 19,
      "context" : ", 2018b), multi-lingual pretraining currently leads to state-of-the-art results (Devlin et al., 2019; Lample and Conneau, 2019).",
      "startOffset" : 80,
      "endOffset" : 127
    }, {
      "referenceID" : 27,
      "context" : ", 2018b), multi-lingual pretraining currently leads to state-of-the-art results (Devlin et al., 2019; Lample and Conneau, 2019).",
      "startOffset" : 80,
      "endOffset" : 127
    }, {
      "referenceID" : 41,
      "context" : "These approaches rely on training transformer-based language models (Vaswani et al., 2017) on unlabeled data from multiple languages, while using careful data selection methods to avoid the over-representation of larger languages.",
      "startOffset" : 68,
      "endOffset" : 90
    }, {
      "referenceID" : 15,
      "context" : "Although these approaches have led to large improvements on many cross-lingual tasks, it is clear that the success of zero-shot cross-lingual transfer depends on the typological similarity of the source and target language (Conneau et al., 2020b; Libovický et al., 2020).",
      "startOffset" : 223,
      "endOffset" : 270
    }, {
      "referenceID" : 29,
      "context" : "Although these approaches have led to large improvements on many cross-lingual tasks, it is clear that the success of zero-shot cross-lingual transfer depends on the typological similarity of the source and target language (Conneau et al., 2020b; Libovický et al., 2020).",
      "startOffset" : 223,
      "endOffset" : 270
    }, {
      "referenceID" : 4,
      "context" : "Similarly, morphologically complex languages tend to achieve poorer performance (Artetxe et al., 2020; Conneau et al., 2020a).",
      "startOffset" : 80,
      "endOffset" : 125
    }, {
      "referenceID" : 14,
      "context" : "Similarly, morphologically complex languages tend to achieve poorer performance (Artetxe et al., 2020; Conneau et al., 2020a).",
      "startOffset" : 80,
      "endOffset" : 125
    }, {
      "referenceID" : 21,
      "context" : "More recently, attention has turned to larger scale analyses of morphological typology effects on language modeling (Gerz et al., 2018; Cotterell et al., 2018; Mielke et al., 2019).",
      "startOffset" : 116,
      "endOffset" : 180
    }, {
      "referenceID" : 17,
      "context" : "More recently, attention has turned to larger scale analyses of morphological typology effects on language modeling (Gerz et al., 2018; Cotterell et al., 2018; Mielke et al., 2019).",
      "startOffset" : 116,
      "endOffset" : 180
    }, {
      "referenceID" : 32,
      "context" : "More recently, attention has turned to larger scale analyses of morphological typology effects on language modeling (Gerz et al., 2018; Cotterell et al., 2018; Mielke et al., 2019).",
      "startOffset" : 116,
      "endOffset" : 180
    }, {
      "referenceID" : 44,
      "context" : "We fine-tune both multilingual BERT (mBERT) (Xu et al., 2019) and XLM RoBERTa (XLM-R) (Conneau et al.",
      "startOffset" : 44,
      "endOffset" : 61
    }, {
      "referenceID" : 14,
      "context" : ", 2019) and XLM RoBERTa (XLM-R) (Conneau et al., 2020a) models on the available training data in each language, using a shared set of hyperparameters selected from recommended values according to the characteristics of our data.",
      "startOffset" : 32,
      "endOffset" : 55
    }, {
      "referenceID" : 22,
      "context" : "Proxy A-distance (Glorot et al., 2011) measures the generalization error of a linear SVM trained to discriminate between two domains.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 23,
      "context" : "The recent cross-lingual suite Xtreme (Hu et al., 2020) includes a number of benchmark tasks in 40 languages.",
      "startOffset" : 38,
      "endOffset" : 55
    }, {
      "referenceID" : 38,
      "context" : "Finally, this work ties in with the increasing interest in typological questions in NLP (Takamura et al., 2016; Ponti et al., 2019; Bjerva et al., 2019; Nooralahzadeh et al., 2020; Bjerva and Augenstein, 2021), which often try to directly predict typological features, or use these to analyze model performance.",
      "startOffset" : 88,
      "endOffset" : 209
    }, {
      "referenceID" : 13,
      "context" : "Finally, this work ties in with the increasing interest in typological questions in NLP (Takamura et al., 2016; Ponti et al., 2019; Bjerva et al., 2019; Nooralahzadeh et al., 2020; Bjerva and Augenstein, 2021), which often try to directly predict typological features, or use these to analyze model performance.",
      "startOffset" : 88,
      "endOffset" : 209
    }, {
      "referenceID" : 35,
      "context" : "Finally, this work ties in with the increasing interest in typological questions in NLP (Takamura et al., 2016; Ponti et al., 2019; Bjerva et al., 2019; Nooralahzadeh et al., 2020; Bjerva and Augenstein, 2021), which often try to directly predict typological features, or use these to analyze model performance.",
      "startOffset" : 88,
      "endOffset" : 209
    }, {
      "referenceID" : 12,
      "context" : "Finally, this work ties in with the increasing interest in typological questions in NLP (Takamura et al., 2016; Ponti et al., 2019; Bjerva et al., 2019; Nooralahzadeh et al., 2020; Bjerva and Augenstein, 2021), which often try to directly predict typological features, or use these to analyze model performance.",
      "startOffset" : 88,
      "endOffset" : 209
    } ],
    "year" : 2021,
    "abstractText" : "Cross-lingual transfer has improved greatly through multi-lingual language model pretraining, reducing the need for parallel data and increasing absolute performance. However, this progress has also brought to light the differences in performance across languages. Specifically, certain language families and typologies seem to consistently perform worse in these models. In this paper, we address what effects morphological typology has on zero-shot cross-lingual transfer for two tasks: Part-of-speech tagging and sentiment analysis. We perform experiments on 19 languages from four language typologies (fusional, isolating, agglutinative, and introflexive) and find that transfer to another morphological type generally implies a higher loss than transfer to another language with the same morphological typology. Furthermore, POS tagging is more sensitive to morphological typology than sentiment analysis and, on this task, models perform much better on fusional languages than on the other typologies.",
    "creator" : "LaTeX with hyperref"
  }
}