{
  "name" : "2021.acl-long.172.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation",
    "authors" : [ "Ruidan He", "Linlin Liu", "Hai Ye", "Qingyu Tan", "Bosheng Ding", "Liying Cheng", "Jia-Wei Low", "Lidong Bing", "Luo Si" ],
    "emails" : [ "ruidan.he@alibaba-inc.com", "linlin.liu@alibaba-inc.com", "qingyu.tan@alibaba-inc.com", "bosheng.ding@alibaba-inc.com", "liying.cheng@alibaba-inc.com", "jiawei.low@alibaba-inc.com", "yeh@comp.nus.edu.sg", "l.bing@alibaba-inc.com", "luo.si@alibaba-inc.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2208–2222\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2208"
    }, {
      "heading" : "1 Introduction",
      "text" : "Large scale pretrained language models (PrLMs) (Devlin et al., 2019; Liu et al., 2019; Conneau et al., 2020a; Brown et al., 2020) have achieved state-ofthe-art results on most natural language processing (NLP) tasks, where fine-tuning has become a dominant approach to utilize PrLMs. A standard finetuning process copies weights from a PrLM and tunes them on a downstream task, which requires a new set of weights for each task.\nAdapter-based tuning (Houlsby et al., 2019; Bapna and Firat, 2019) has been proposed as a\n∗ Equally Contributed † Linlin, Qingyu, Bosheng, Liying, and Jia-wei are under the Joint PhD Program between Alibaba and their corresponding universities.\nmore parameter-efficient alternative. For NLP, adapters are usually light-weight modules inserted between transformer layers (Vaswani et al., 2017). During model tuning on a downstream task, only the parameters of adapters are updated while the weights of the original PrLM are frozen. Hence, adapter-based tuning adds only a small amount of parameters for each task, allowing a high degree of parameter-sharing. Though using much less trainable parameters, adapter-based tuning has demonstrated comparable performance with full PrLM fine-tuning (Houlsby et al., 2019; Bapna and Firat, 2019; Stickland and Murray, 2019).\nExisting work mostly focuses on the parameterefficient aspect of adapters and attempt to derive useful applications from that, which is still the case in most recent works: Rücklé et al. (2020) explore methods to further improve the parameter and computation efficiency of adapters; Pfeiffer et al. (2020a) combine knowledge from multiple adapters to improve the performance on downstream tasks; Artetxe et al. (2020) and Pfeiffer et al. (2020c) leverage the modular architecture of adapters for parameter-efficient transfer to new languages or tasks, and Wang et al. (2020) utilize the same property for knowledge injection.\nBesides parameter-efficiency, the unique characteristic of adapter-based tuning, with alternating frozen and learnable layers, might be directly useful for improving model performances. However, this has not yet been discussed in the prior work. In this paper, we first empirically demonstrate that adapter-based tuning better regularizes training than fine-tuning by mitigating the issue of forgetting. We show that it yields representations with less deviation from those generated by the original PrLM. Next, to see what this property of adapters will help when adapting PrLMs, we compare the performance of fine-tuning and adapter-based tuning on a wide range of datasets\nand NLP tasks. Extensive experiments and analysis are conducted in different settings, including low-resource and high-resource, monolingual and cross-lingual.\nOur main findings can be summarized as follows:\n• For monolingual adaptation, adapter-based tuning yields better results in low-resource settings, especially when the task is more domain-specific. With increasing training samples, the performance gain over finetuning is less significant (§3).\n• Adapter-based tuning tends to outperform fine-tuning on zero-shot cross-lingual tasks under different amounts of training data (§4).\n• Adapter-based tuning demonstrates higher stability and better generalization ability. It is less sensitive to learning rates compared to fine-tuning (§5)."
    }, {
      "heading" : "2 Adapter Better Regularizes Tuning",
      "text" : ""
    }, {
      "heading" : "2.1 Adapter-based Tuning",
      "text" : "When adapting a pretrained language model (PrLM), adapter-based tuning inserts light-weight neural networks (adapters) between the transformer layers of the PrLM, and only updates the parameters of the adapters on a downstream task, but keeps the ones of the PrLM frozen. Unlike fine-tuning which introduces an entire new model for every task, one great advantage of adapter-based tuning is generating a compact\nmodel with only a few trainable parameters added per task.\nHoulsby et al. (2019) have extensively studied the choices of adapter architectures and where they should be inserted into PrLMs. They find that a stack of down- and up-scale neural networks works well which only introduces a small amount of extra parameters to the network. This design inspires most of the following work (Pfeiffer et al., 2020a,c; Bapna and Firat, 2019). As shown in Figure 1, the adapter maps an input hidden vector h from dimension d to dimension m where m < d, and then re-maps it to dimension d. We refer m as the hidden size of the adapter. A skip-connection is employed inside the adapter network such that if the parameters of the projection layers are near zeros, the adapter module approximates an identity function. Formally, given the input hidden vector h, the output vector h′ is calculated as:\nh′ = f2(tanh f1(h)) + h (1)\nin which f1(·) and f2(·) are the down- and upprojection layers. At each transformer layer, two adapters are inserted right after the self-attention and the feed-forward layers respectively. During adapter tuning, only the parameters of the adapters, the normalization layers, and the final classification layer are updated. We use the above described adapter configuration in all of our experiments, since it is adopted in most prior work with few modifications."
    }, {
      "heading" : "2.2 Representation Similarity",
      "text" : "Fine-tuning large-scale PrLMs on downstream tasks can suffer from overfitting and bad generalization issues (Dodge et al., 2020; Phang et al., 2018). Recently, Lee et al. (2020) propose Mixout to regularize the fine-tuning of PrLMs. They show that Mixout avoids catastrophic forgetting and stabilizes the fine-tuning process by encouraging the weights of the updated model to stay close to the initial weights. Since adapter-based tuning does not update the weights of PrLMs at all, we suspect that it has a similar effect of alleviating the issue of catastrophic forgetting. Since the weights of the PrLM are the same before and after adapter-based tuning, to verify this, we use Representational Similarity Analysis (RSA) (Laakso and Cottrell, 2000) to assess the similarity of tuned representations to those without tuning at each transformer layer.\nRSA has been widely used to analyze the similarity between two neural network outputs (Abnar et al., 2019; Chrupała and Alishahi, 2019; Merchant et al., 2020), which works by creating two comparable sets of representations by inputting a same set of n samples to the two models. For each set of representations, a n× n pairwise similarity1 matrix is calculated. The final RSA similarity score between the two representation space is computed as the Pearson correlation between the flattened upper triangulars of the two similarity matrices. We use a subset of GLUE tasks (Wang et al., 2018) for our analysis. Given a task, we first perform adapter-based tuning and fine-tuning to adapt a BERT-base model (Morg) to the target task, which yields models Madapt and Mft respectively (See Appendix A.2 for training details). Then we pass sentences (or sentence-pairs depend on the task) from the development set to Morg, Madapt, and Mft respectively. We extract representations at each layer from the three models and select the corresponding representations of 5k randomly sampled tokens2 (n = 5000) for evaluation. Note that the same set of tokens is used for all models. Finally, we compare the representations obtained from Madapt or Mft to those from Morg using RSA.\nFigure 2 plots the results on STS-2, results of other tasks demonstrate a similar trend and can be found in Appendix A.3. For both fine-tuning and adapter-based tuning, we observe that the repre-\n1Cosine similarity is used 2We skip [PAD], [CLS], [SEP] for token selection.\nsentation change generally arises in the top layers of the network, which is consistent with previous findings that higher layers are more task relevant (Howard and Ruder, 2018). It can be clearly observed that compared to fine-tuning, adapterbased tuning yields representations with less deviation from those of BERT-base at each layer, which verifies our claim that adapter-based tuning can better regularize the tuning process by mitigating the forgetting problem. Apparently, this property of adapter tuning comes from that it freezes all the parameters of PrLMs. And because of the skipconnection in the adapter, the hidden representation out of the adapter can mimic the input representation, in this way, some of the original knowledge of PrLMs (before injecting adapters) can be preserved.\nSince we find that adapter-based tuning better regularizes the learning process, the next question is how this property will help to improve the performance when adapting PrLMs to downstream tasks. We conduct extensive experiments to investigate this. The remainder of this paper is organized as follows. We compare fine-tuning and adapterbased tuning on monolingual text-level adaptation tasks in §3, followed by cross-lingual adaptation in §4. Further analysis about the training stability and generalization capabilities is shown in §5."
    }, {
      "heading" : "3 Monolingual Adaptation",
      "text" : "In this section, we first experiment with eight datasets as used in Gururangan et al. (2020) including both high- and low-resource tasks (§3.1). We refer this set of tasks as Task Adaptation Evaluation (TAE). We observe that adapter-based tuning consistently outperforms fine-tuning on lowresource tasks, while they perform similarly on high-resource tasks. We further confirm the effectiveness of adapters in low-resource settings on the GLUE benchmark (Wang et al., 2018) (§3.2)."
    }, {
      "heading" : "3.1 TAE",
      "text" : "TAE consists of four domains (biomedical, computer science, news text, and AMAZON reviews) and eight classification tasks (two in each domain), whose domain diversity makes it suitable to assess the adaptation effectiveness of different approaches. Detailed data statistics are displayed in Appendix A.1. We consider tasks with fewer than 5k training examples as low-resource tasks and the others as high-resource tasks.\nExperimental Setup We perform supervised fine-tuning on RoBERTa-base as our baseline (RoBa.-ft). For adapter-based tuning, we set the hidden size m of adapters to 256 (RoBa.adapter256). We also present the results of adding task-adaptive pretraining (+TAPT) (Gururangan et al., 2020). In this setting, before fine-tuning or adapter-based tuning, the model was trained with a masked language modeling (MLM) objective on the training texts (without labels) of the task. Note that in RoBa.-adapter256+TAPT, we also use adapter-based tuning for TAPT where only the weights of adapters are updated at the TAPT stage. This is to evaluate whether adapter-based tuning can work with unsupervised learning objectives. We follow the experimental settings in Gururangan et al. (2020) for TAPT. For fine-tuning and adapter-based tuning, we train models for 20 epochs to make sure they are sufficiently trained and save the checkpoint after each training epoch. We select the checkpoint that achieves the best score on the validation set for evaluation on the test set. The batch size is set to 16 for both methods. The learning rate is set to 2e-5 for fine-tuning, and 1e-4 for adapter-based tuning. See Appendix A.2 for the hyperparameter selection process and more training details.\nResults Table 1 presents the comparison results. We report the average result over 5 runs with different random seeds. On four low-resource tasks, adapter-based tuning consistently outperforms finetuning and improves the average result by 1.9%. Adapter-based tuning alone without TAPT even outperforms fine-tuning with TAPT. Besides, adding TAPT before adapter-based tuning further improves the performance on 3 out of 4 low-resource tasks, which suggests that adapter-based tuning works with both supervised and unsupervised objectives.\nAnother finding is that when trained on highresource tasks, both methods achieve similar results. To verify the effects of training size, on high-resource tasks, we plot the performances with varying numbers of training examples in Figure 3. The trend is consistent with our existing observations – adapter-based tuning achieves better results when the training set is small while fine-tuning will gradually catch up with an increasing number of training examples."
    }, {
      "heading" : "3.2 GLUE Low-resource Adaptation",
      "text" : "To further validate that adapters tend to generalize better than fine-tuning under low-resource settings, we follow Zhang et al. (2021) to study lowresource adaptation using eight datasets from the GLUE benchmark (Wang et al., 2018) which covers four types of tasks: natural language inference (MNLI, QNLI, RTE), paraphrase detection (MRPC, QQP), sentiment classification (SST-2) and linguistic acceptability (CoLA). Appendix A.1 provides detailed data statistics and descriptions.\nExperimental Setup For each dataset, we simulate two low-resource settings by randomly sampling 1k and 5k instances from the original training\ndata as the new training sets. In each setting, we draw another 1k samples from the remaining training set as the validation set and instead use the original validation set as the test set, since the original GLUE test sets are not publicly available 3.\nWe perform fine-tuning on BERT-base (BERTft) and RoBERTa-base (RoBa.-ft) respectively as our baselines. We set the learning rate to 2e-5 and the batch size to 16 for BERT and RoBERTa finetuning experiments (See Appendix A.2 for details). For adapters, we only tune its hidden sizes in {64, 128, 256}, setting the learning rate to 1e-4 and batch size to 16 as the same used in §3.1.\nResults Table 2 presents the comparison results. For adapter-based tuning, we report two results on each task. One is obtained with the optimal hidden size which varies per dataset, and the other is obtained with the size of 64. We observe that adapter-based tuning outperforms fine-tuning most of the time under both 1k and 5k settings. In particular, the performance gain is more significant in 1k setting, where on average across all tasks, adapterbased tuning outperforms fine-tuning by 2.5% and 0.7% on BERT and RoBERTa respectively."
    }, {
      "heading" : "3.3 Discussions",
      "text" : "One consistent observation from § 3.1 and § 3.2 is that adapters tend to outperform fine-tuning on\n3Users are limited to a maximum of two submissions per day to obtain test results, which is inconvenient for a large number of runs\ntext-level classification tasks when the training set is small, but with more training samples, the benefit of adapters is less significant. In low-resource setting, fine-tuning has more severe overfitting problem, since it has much more tunable parameters compared to adapter-tuning, so adapter-tuning works better than fine-tuning. However, in highresource setting, overfitting is not a big issue and model capacity counts more. Obviously, the model capacity under fine-tuning is larger than that under adapter-tuning since fine-tuning can update much more model parameters.\nWhen comparing the improvements of adapter tuning over fine-tuning on tasks from TAE (§ 3.1) and GLUE (§ 3.2), we find that the improvement is more significant on low-resource tasks from TAE – on RoBERTa-base, the average improvement brought by adapters is 1.9% across four lowresource tasks from TAE, while the average improvement on GLUE is 0.7% and 0.4% in 1k and 5k settings respectively. As indicated in Gururangan et al. (2020), the TAE dataset is more domainspecific and has less overlap with the corpus used for RoBERTa-base pretraining, one intuitive explanation for this observation is that fine-tuning has more severe forgetting and overfitting issues in domain adaptation where the target domain is dissimilar to the source domain in pretraining, thus adapter-based tuning is more preferable in this scenario."
    }, {
      "heading" : "4 Cross-lingual Adaptation",
      "text" : "In this section, we further compare fine-tuning and adapter-based tuning in the zero-shot cross-lingual transfer setting. All experiments in this section are based on XLM-R-large (Conneau et al., 2020a), a recent SOTA multilingual PrLM covering 100 languages. We conduct evaluations on a set of multilingual tasks from XTREME (Hu et al., 2020), including Universal Dependencies v2.5 tree banks (UD-POS) (Nivre et al., 2018), Wikiann NER (Pan et al., 2017), and cross-lingual natural language inference (XNLI) (Conneau et al., 2020b). UDPOS contains 34 languages, Wikiann NER contains 40 languages, and XNLI contains 15 languages. We refer the reader to Hu et al. (2020) for additional details about the datasets.\nExperimental Setup On each task, we perform hyperparameter tuning on the English development set. For both fine-tuning and adapter-based tuning, we use batch size 32, and tune the learning rates in {1e-5, 2e-5, 3e-5, 4e-5, 5e-5}. For adapterbased tuning, we further tune the hidden sizes in {64, 128, 256} and find size 256 often performs the best. We train and select models with the English training and development sets and then evaluate the tuned models on test sets of all languages. See Appendix A.2 for hyperparameter and training details.\nResults Table 3 summarizes the results. To better compare cross-lingual transfer to different groups of languages, we present the average results of all languages (All), the target languages except English (Target), and the Non-Indo-European languages (Distant). It can be observed that adapterbased tuning significantly outperforms fine-tuning\non all three settings for each task. Specifically, adapter-based tuning outperforms the reported finetuning results (Hu et al., 2020) on Target and Distant by 2.06% and 3.71% on UD-POS, 1.08% and 0.8% on Wikiann NER, and 0.87% and 0.87% on XNLI. See Appendix A.3 for detailed results on each language.\nNote that UD-POS, Wikiann NER, and XNLI are all high-resource tasks, with 20k, 20k, and 400k training samples respectively. Unlike monolingual tasks, adapters achieve consistent performance gains even under high-resource settings on cross-lingual tasks. We suspect that the ability to mitigate forgetting is more useful in cross-lingual scenarios since the model knowledge of the target languages only comes from pretraining. Adapterbased tuning can better maintain the knowledge. We further investigate the effectiveness of adapterbased tuning on XNLI with smaller training sets. Table 4 summarizes the results when trained on 5%, 10%, and 20% of the original training sets. In all settings, adapters still demonstrate consistent improvements over fine-tuning."
    }, {
      "heading" : "5 Analysis",
      "text" : "Adapter Hidden Size The hidden size m4 is the only adapter-specific hyperparameter. As indicated in Houlsby et al. (2019), the hidden size provides a simple means to trade off performance with parameter efficiency. Table 5 shows the performance with different hidden sizes, from which we find that increasing the hidden size may not always lead to performance gains. For monolingual low-resource adaptation, TAE tasks prefer a larger hidden size, while the results on GLUE are similar across different hidden sizes. We suspect that this is due to that TAE datasets are more dissimilar to the pretraining corpus, which requires relatively more trainable parameters to learn the domain-specific knowledge. On XNLI, a larger hidden size helps improve the performance when the full data is used. However, when only 5% training data is used, increasing the hidden size does not yield consistent improvements. The results indicate that the optimal hidden size depends on both the domain and the training size of the task.\nLearning Rate Robustness We compare the two tuning methods in terms of their stability w.r.t the learning rate. Figure 4 shows the performance distributions on CoLA and MNLI under 1k and 5k settings. The learning rates are varied in {2e-5, 4e5, 6e-5, 8e-5, 1e-4}. Each box in the plot is drawn from the results of 20 runs with different random seeds. We observe that fine-tuning yields larger variances when increasing the learning rates. It often collapses with learning rates larger than 4e-5\n4The fraction of adapter parameters w.r.t. BERT-base (110M parameters) is 2%, 4%, and 6% when m is set to 64, 128, and 256. The fraction w.r.t. XLMR-large (550M parameters) is 1%, 2%, and 3%, respectively.\nwhen RoBERTa-base is used. Adapter-based tuning is more stable across a wider range of learning rates.\nOverfitting and Generalization Here, we first study the robustness of adapter-based tuning to overfitting. We use CoLA, MRPC, QNLI, and SST2 with their original training and development sets for our analysis. The CoLA and MRPC contain 8.5k and 3.7k training samples and are regarded as low-resource tasks. The QNLI and SST-2 con-\ntain 104k and 67k training samples and are used as high-resource tasks. We train the two low-resource tasks for 10k steps, and the high resource tasks for 60k steps with a batch size of 16. We use BERTbase for all experiments. Figure 5 plots the loss curves on dev sets w.r.t training steps. We observe that models with fine-tuning can easily overfit on both low- and high-resource tasks. Adapter-based tuning is more robust to overfitting. Additional results on accuracy w.r.t. training steps and a similar analysis on XNLI are in Appendix A.3.\nWe also present the mean and best dev results across all evaluation steps in Table 6, where we perform an evaluation step every 20 training steps. The mean results of adapter-based tuning consistently outperform those of fine-tuning. The differences between the mean and the best values are also smaller with adapter-based tuning. The results suggest that the performance of adapters is more stable over fine-tuning along the training process.\nTraining neural networks can be viewed as searching for a good minima in the non-convex landscape defined by the loss function. Prior work (Hochreiter and Schmidhuber, 1997; Li et al., 2018) shows that the flatness of a local minima correlates with the generalization capability. Thus, we further show the loss landscapes of the two tuning methods. Following Hao et al. (2019), we plot the loss curve by linear interpolation between θ0 and θ1 with function f(α) = L(θ0 + α · (θ1 − θ0)), where θ0 and θ1 denote the model weights before and after tuning. L(θ) is the loss function and α is a scalar parameter. In our experiments, we set the range of α to [−2, 2] and uniformly sample 20 points. Figure 6 shows the loss landscape curves on CoLA and SST based on BERT-base. It shows that the minimas of adapter-based tuning are more wide and flat, which indicates that adapter-based tuning tends to generalize better.\nCompare to Mixout The focus of this paper is to answer the question – besides being parameter-\nefficient, when would adapter-based tuning be more effective than fine-tuning for PrLM adaptation? Thus, we only use fine-tuning as our primary baseline in previous sections. Here, for the sake of curiosity, we further compare adapter-based tuning to fine-tuning regularized by mixout (Lee et al., 2020) on a subset of GLUE tasks, since mixout similarly regularizes the learning process by mitigating the forgetting issue. Specifically, it replaces all outgoing parameters from a randomly selected neuron to the corresponding parameters of the initial model without tuning, such that it reduces divergence from the initial model. Following the suggestions in the paper, we conduct experiments by replacing all dropout modules in the network with mixout and set the mixout probability to 0.9. From the results in Table 7, we find that using adapterbased tuning alone yields the best results in most cases. Applying mixout to fine-tuning improves the performance on CoLA and MRPC only. However, applying it to adapters instead tends to degrade the performance. We suspect that this is because the number of trainable parameters of adapters is very few to begin with. Hence, further replacing a large percentage of them with their initial weights may weaken the learning ability."
    }, {
      "heading" : "6 Related Work",
      "text" : "Fine-tuning pretrained large scale language models has proven its effectiveness on a wide range of NLP tasks (Devlin et al., 2019; Liu et al., 2019; Conneau et al., 2020a; Brown et al., 2020). However, fine-tuning requires a new set of weights for each task, which is parameter inefficient. Adapterbased tuning is proposed to deal with this problem (Houlsby et al., 2019). Most previous work has demonstrated that it achieves comparable performance to fine-tuning (Bapna and Firat, 2019; Pfeiffer et al., 2020b,a,c; Rücklé et al., 2020; Wang et al., 2020; Guo et al., 2020). However, existing work mostly focuses on the parameter-efficient aspect while overlooks the effectiveness.\nFine-tuning PrLMs in a low-resource setting has been studied for a while (Dodge et al., 2020; Lee et al., 2020; Phang et al., 2018; Jiang et al., 2020; Zhang et al., 2021). Previous work points out that with large-scale parameters, fine-tuning on a few samples can lead to overfitting and bad generalization, which causes the results unstable. Phang et al. (2018) find that pretraining on an intermediate task can improve fine-tuning outcomes. Jiang et al. (2020) improve the robustness of fine-tuning by controlling the model complexity and preventing aggressive updating. On the other hand, catastrophic forgetting can appear when transferring a pretrained neural networks (French, 1999; McCloskey and Cohen, 1989; Goodfellow et al., 2013), where the learned knowledge from pretraining is lost when adapting to downstream tasks. This phenomenon often appears in NLP tasks (Mou et al., 2016; Arora et al., 2019). To relieve this problem of adapting pretrained language models, Howard and Ruder (2018) gradually unfreeze the layers starting from the last layer and Sun et al. (2019) find assigning lower learning rate to the bottom layers can improve the performance. Lee et al. (2020) regularize learning by encouraging the weights of the updated model to stay close to the initial weights. Aghajanyan et al. (2021) regularize fine-tuning by introducing noise to the input which is similar to adversarial training for fine-tuning studied in Zhu et al. (2020). Mosbach et al. (2021) point out that the instability of fine-tuning lies in the optimizer and propose to revise the Adam optimizer by replacing it with a de-bias version. Chen et al. (2020) propose a mechanism to recall the knowledge from pretraining tasks."
    }, {
      "heading" : "7 Conclusion",
      "text" : "Prior work often focuses on the parameter-efficient aspect while overlooks the effectiveness of adapterbased tuning. We empirically demonstrate that adapter-based tuning can better regularize the learning process. We conduct extensive experiments to verify its effectiveness and conclude that 1) it tends to outperform fine-tuning on both low-resource and cross-lingual tasks; 2) it demonstrates higher stability under different learning rates compared to fine-tuning. We hope our study will inspire more future work on PrLM adaptation based on adapters and other methods that only tune part of the PrLM parameters."
    }, {
      "heading" : "Acknowledgements",
      "text" : "Linlin Liu would like to thank the support from Interdisciplinary Graduate School, Nanyang Technological University."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Datasets TAE Table 8 presents the data statistics of the TAE datasets we used in § 3.1.\nGLUE Table 9 presents the statistics and descriptions of GLUE tasks. In § 3.2, to investigate the effectiveness in low-resource scenarios, we simulate two low-resource settings by randomly sampling 1k and 5k examples respectively from each of the original training set as the new training sets. In each setting, we draw 1k samples from the remaining training set as our validation set and use the original validation set as held-out test set since the original GLUE test sets are not publicly available.\nFor the RSA analysis in § 2 and the analysis of overfitting and generalization in § 5, we use the original training and development sets for analysis purpose, as this better reveals the behaviors under both high- and low- resource settings.\nA.2 Experimental Details Implementation We use language model implementations from HuggingFace Transfromers library (Wolf et al., 2019). Our adapter implementation is also based on that. Following standard practice (Devlin et al., 2019), we pass the final layer [CLS] token representation to a task-specific feedforward layer for prediction on downstream tasks. Each experiment was performed on a single v100 GPU. We use the Adam optimizer (Kingma and Ba, 2015) with a linear learning rate scheduler.\nTraining Details on TAE and GLUE For both fine-tuning and adapter-based tuning, we train models for a fixed number of epochs, and select models with the best validation performances on epoch end for evaluation.\nFor fine-tuning, on TAE we follow the learning rate and batch size as suggested by Houlsby et al. (2019). On GLUE, we tune learning rates in {1e-5, 2e-5, 3e-5, 4e-5, 5e-5} and batch sizes in {16, 32} to select the best configuration across tasks.\nFor adapters, on TAE, we set the batch size the same as used in fine-tuning, and tune learning rates in {2e-5, 5e-5,1e-4, 2e-4} and adapter’s hidden size in {64, 128, 256} to select the best configuration across all tasks. On GLUE, we keep the learning rate and batch size the same as used in TAE, and tune the adapter’s hidden sizes in {64, 128, 256} for each task. We use the same hyperparameter\nsetting for all our analysis experiments with GLUE tasks as well.\nTable 10 presents the detailed hyperparameter settings for TAE and GLUE.\nTraining Details on Xtreme Tasks For UDPOS, Wikiann NER, and XNLI, we use batch size 32, and tune learning rates in {1e-5, 2e-5, 3e-5, 4e-5, 5e-5} on each task. We tune the adapter’s hidden sizes in {64, 128, 256} to select the best value across all tasks. We use the English training and development sets of each task for hyperparameter tuning. Table 11 presents the detailed settings.\nA.3 Additional Results\nRSA Figure 7 presents additional Representational Similarity Analysis (RSA) plots on three GLUE tasks as mentioned in § 2. We further conduct RSA to show the deviation of representation space before and after tuning (with English training set) on three distant languages (zh, ja, th) from the cross-lingual NER task. Figure 8 presents the results.\nAccuracy w.r.t Training Steps Figure 9 shows the change of accuracy with increasing training steps on four GLUE tasks. The results again indicate that adapter-based tuning is more robust to overfitting.\nOverfitting Analysis on XNLI We train XLMRlarge with 10% of the original English training data of XNLI, and plot the average loss and accuracy curves on development sets across all target languages except English in Figure 10. The plots demonstrate similar trends as shown in the plots of GLUE tasks (Figure 5 and Figure 9), where models with fine-tuning are easily overfitted and adapterbased tuning is more robust to overfitting.\nDetailed Cross-lingual Results Table 12 and Table 13 presents the cross-lingual POS tagging results and the cross-lingual NER results on each language respectively. Table 14 presents detailed results on XNLI when trained with full data. 15 presents detailed XNLI results when trained on 5%, 10%, and 20% of training data."
    } ],
    "references" : [ {
      "title" : "Blackbox meets blackbox: Representational similarity and stability analysis of neural language models and brains",
      "author" : [ "Samira Abnar", "Lisa Beinborn", "Rochelle Choenni", "Jelle Zuidema." ],
      "venue" : "Proceedings of the ACL-Workshop on Analyzing and Interpreting",
      "citeRegEx" : "Abnar et al\\.,? 2019",
      "shortCiteRegEx" : "Abnar et al\\.",
      "year" : 2019
    }, {
      "title" : "Better fine-tuning by reducing representational collapse",
      "author" : [ "Armen Aghajanyan", "Akshat Shrivastava", "Anchit Gupta", "Naman Goyal", "Luke Zettlemoyer", "Sonal Gupta." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Aghajanyan et al\\.,? 2021",
      "shortCiteRegEx" : "Aghajanyan et al\\.",
      "year" : 2021
    }, {
      "title" : "Does an LSTM forget more than a cnn? an empirical study of catastrophic forgetting in NLP",
      "author" : [ "Gaurav Arora", "Afshin Rahimi", "Timothy Baldwin." ],
      "venue" : "Proceedings of ALTA.",
      "citeRegEx" : "Arora et al\\.,? 2019",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2019
    }, {
      "title" : "On the cross-lingual transferability of monolingual representations",
      "author" : [ "Mikel Artetxe", "Sebastian Ruder", "Dani Yogatama." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Artetxe et al\\.,? 2020",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2020
    }, {
      "title" : "Simple, scalable adaptation for neural machine translation",
      "author" : [ "Ankur Bapna", "Orhan Firat." ],
      "venue" : "Proceedings of EMNLP-IJCNLP.",
      "citeRegEx" : "Bapna and Firat.,? 2019",
      "shortCiteRegEx" : "Bapna and Firat.",
      "year" : 2019
    }, {
      "title" : "Recall and learn: Fine-tuning deep pretrained language models with less forgetting",
      "author" : [ "Sanyuan Chen", "Yutai Hou", "Yiming Cui", "Wanxiang Che", "Ting Liu", "Xiangzhan Yu." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Correlating neural and symbolic representations of language",
      "author" : [ "Grzegorz Chrupała", "Afra Alishahi." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Chrupała and Alishahi.,? 2019",
      "shortCiteRegEx" : "Chrupała and Alishahi.",
      "year" : 2019
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020a",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Xnli: Evaluating crosslingual sentence representations",
      "author" : [ "Alexis Conneau", "Ruty Rinott", "Guillaume Lample", "Holger Schwenk", "Ves Stoyanov", "Adina Williams", "Samuel R. Bowman." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Conneau et al\\.,? 2020b",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of NAACL.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping",
      "author" : [ "Jesse Dodge", "Gabriel Ilharco", "Roy Schwartz", "Ali Farhadi", "Hannaneh Hajishirzi", "Noah A. Smith." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Dodge et al\\.,? 2020",
      "shortCiteRegEx" : "Dodge et al\\.",
      "year" : 2020
    }, {
      "title" : "Catastrophic forgetting in connectionist networks",
      "author" : [ "Robert M French." ],
      "venue" : "Trends in cognitive sciences, 3(4):128–135.",
      "citeRegEx" : "French.,? 1999",
      "shortCiteRegEx" : "French.",
      "year" : 1999
    }, {
      "title" : "An empirical investigation of catastrophic forgetting in gradientbased neural networks",
      "author" : [ "Ian J Goodfellow", "Mehdi Mirza", "Da Xiao", "Aaron Courville", "Yoshua Bengio." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Goodfellow et al\\.,? 2013",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2013
    }, {
      "title" : "Incorporating BERT into parallel sequence decoding with adapters",
      "author" : [ "Junliang Guo", "Zhirui Zhang", "Linli Xu", "Hao-Ran Wei", "Boxing Chen", "Enhong Chen." ],
      "venue" : "Proceedings of NeurIPS.",
      "citeRegEx" : "Guo et al\\.,? 2020",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2020
    }, {
      "title" : "Don’t stop pretraining: Adapt language models to domains and tasks",
      "author" : [ "Suchin Gururangan", "Ana Marasović", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A. Smith." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Gururangan et al\\.,? 2020",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2020
    }, {
      "title" : "Visualizing and understanding the effectiveness of BERT",
      "author" : [ "Yaru Hao", "Li Dong", "Furu Wei", "Ke Xu." ],
      "venue" : "Proceedings of the EMNLP-IJCNLP.",
      "citeRegEx" : "Hao et al\\.,? 2019",
      "shortCiteRegEx" : "Hao et al\\.",
      "year" : 2019
    }, {
      "title" : "Flat minima",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Comput., 9(1):1–42.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Parameter-efficient transfer learning for NLP",
      "author" : [ "Neil Houlsby", "Andrei Giurgiu", "Stanislaw Jastrzebski", "Bruna Morrone", "Quentin De Laroussilhe", "Andrea Gesmundo", "Mona Attariyan", "Sylvain Gelly." ],
      "venue" : "Proceedings of ICML.",
      "citeRegEx" : "Houlsby et al\\.,? 2019",
      "shortCiteRegEx" : "Houlsby et al\\.",
      "year" : 2019
    }, {
      "title" : "Universal language model fine-tuning for text classification",
      "author" : [ "Jeremy Howard", "Sebastian Ruder." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Howard and Ruder.,? 2018",
      "shortCiteRegEx" : "Howard and Ruder.",
      "year" : 2018
    }, {
      "title" : "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation",
      "author" : [ "Junjie Hu", "Sebastian Ruder", "Aditya Siddhant", "Graham Neubig", "Orhan Firat", "Melvin Johnson." ],
      "venue" : "Proceedings of ICML.",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "SMART: robust and efficient fine-tuning for pretrained natural language models through principled regularized optimization",
      "author" : [ "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Tuo Zhao." ],
      "venue" : "Proceedings ACL.",
      "citeRegEx" : "Jiang et al\\.,? 2020",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Content and cluster analysis: assessing representational similarity in neural systems",
      "author" : [ "Aarre Laakso", "Garrison Cottrell." ],
      "venue" : "Philosophical psychology, 13(1):47–76.",
      "citeRegEx" : "Laakso and Cottrell.,? 2000",
      "shortCiteRegEx" : "Laakso and Cottrell.",
      "year" : 2000
    }, {
      "title" : "Mixout: Effective regularization to finetune large-scale pretrained language models",
      "author" : [ "Cheolhyoung Lee", "Kyunghyun Cho", "Wanmo Kang." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Lee et al\\.,? 2020",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "Visualizing the loss landscape of neural nets",
      "author" : [ "Hao Li", "Zheng Xu", "Gavin Taylor", "Christoph Studer", "Tom Goldstein." ],
      "venue" : "Proceedings of NeurIPS.",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Catastrophic interference in connectionist networks: The sequential learning problem",
      "author" : [ "Michael McCloskey", "Neal J Cohen." ],
      "venue" : "Psychology of learning and motivation, volume 24, pages 109–165. Elsevier.",
      "citeRegEx" : "McCloskey and Cohen.,? 1989",
      "shortCiteRegEx" : "McCloskey and Cohen.",
      "year" : 1989
    }, {
      "title" : "What happens to BERT embeddings during fine-tuning",
      "author" : [ "Amil Merchant", "Elahe Rahimtoroghi", "Ellie Pavlick", "Ian Tenney" ],
      "venue" : "In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
      "citeRegEx" : "Merchant et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Merchant et al\\.",
      "year" : 2020
    }, {
      "title" : "On the stability of fine-tuning BERT: misconceptions, explanations, and strong baselines",
      "author" : [ "Marius Mosbach", "Maksym Andriushchenko", "Dietrich Klakow." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Mosbach et al\\.,? 2021",
      "shortCiteRegEx" : "Mosbach et al\\.",
      "year" : 2021
    }, {
      "title" : "How transferable are neural networks in NLP applications",
      "author" : [ "Lili Mou", "Zhao Meng", "Rui Yan", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin" ],
      "venue" : "Proceedings of EMNLP",
      "citeRegEx" : "Mou et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mou et al\\.",
      "year" : 2016
    }, {
      "title" : "Universal dependencies 2.2",
      "author" : [ "Joakim Nivre", "Rogier Blokland", "Niko Partanen", "Michael Rießler" ],
      "venue" : null,
      "citeRegEx" : "Nivre et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Nivre et al\\.",
      "year" : 2018
    }, {
      "title" : "Crosslingual name tagging and linking for 282 languages",
      "author" : [ "Xiaoman Pan", "Boliang Zhang", "Jonathan May", "Joel Nothman", "Kevin Knight", "Heng Ji." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Pan et al\\.,? 2017",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2017
    }, {
      "title" : "Adapterfusion: Non-destructive task composition for transfer learning",
      "author" : [ "Jonas Pfeiffer", "Aishwarya Kamath", "Andreas Rücklé", "Kyunghyun Cho", "Iryna Gurevych." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Pfeiffer et al\\.,? 2020a",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2020
    }, {
      "title" : "Adapterhub: A framework for adapting transformers",
      "author" : [ "Jonas Pfeiffer", "Andreas Rücklé", "Clifton Poth", "Aishwarya Kamath", "Ivan Vulic", "Sebastian Ruder", "Kyunghyun Cho", "Iryna Gurevych." ],
      "venue" : "Proceedings of EMNLP: System Demonstra-",
      "citeRegEx" : "Pfeiffer et al\\.,? 2020b",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2020
    }, {
      "title" : "Mad-x: An adapter-based framework for multi-task cross-lingual transfer",
      "author" : [ "Jonas Pfeiffer", "Ivan Vulić", "Iryna Gurevych", "Sebastian Ruder." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Pfeiffer et al\\.,? 2020c",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2020
    }, {
      "title" : "Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks",
      "author" : [ "Jason Phang", "Thibault Févry", "Samuel R. Bowman." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Phang et al\\.,? 2018",
      "shortCiteRegEx" : "Phang et al\\.",
      "year" : 2018
    }, {
      "title" : "Adapterdrop: On the efficiency of adapters in transformers",
      "author" : [ "Andreas Rücklé", "Gregor Geigle", "Max Glockner", "Tilman Beck", "Jonas Pfeiffer", "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Rücklé et al\\.,? 2020",
      "shortCiteRegEx" : "Rücklé et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT and PALs: Projected attention layers for efficient adaptation in multi-task learning",
      "author" : [ "Asa Cooper Stickland", "Iain Murray." ],
      "venue" : "Proceedings of ICML.",
      "citeRegEx" : "Stickland and Murray.,? 2019",
      "shortCiteRegEx" : "Stickland and Murray.",
      "year" : 2019
    }, {
      "title" : "How to fine-tune BERT for text classification",
      "author" : [ "Chi Sun", "Xipeng Qiu", "Yige Xu", "Xuanjing Huang" ],
      "venue" : "In Proceedings of CCL",
      "citeRegEx" : "Sun et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of NeurIPS.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop Black-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "K-adapter: Infusing knowledge into pre-trained models with adapters",
      "author" : [ "Ruize Wang", "Duyu Tang", "Nan Duan", "Zhongyu Wei", "Xuanjing Huang", "Jianshu ji", "Guihong Cao", "Daxin Jiang", "Ming Zhou." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Huggingface’s transformers: State-of-the-art natural language",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "Rémi Louf", "Morgan Funtowicz", "Jamie Brew" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "Revisiting fewsample BERT fine-tuning",
      "author" : [ "Tianyi Zhang", "Felix Wu", "Arzoo Katiyar", "Kilian Q. Weinberger", "Yoav Artzi." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Freelb: Enhanced adversarial training for natural language understanding",
      "author" : [ "Chen Zhu", "Yu Cheng", "Zhe Gan", "Siqi Sun", "Tom Goldstein", "Jingjing Liu." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Large scale pretrained language models (PrLMs) (Devlin et al., 2019; Liu et al., 2019; Conneau et al., 2020a; Brown et al., 2020) have achieved state-ofthe-art results on most natural language processing (NLP) tasks, where fine-tuning has become a dominant approach to utilize PrLMs.",
      "startOffset" : 47,
      "endOffset" : 129
    }, {
      "referenceID" : 25,
      "context" : "Large scale pretrained language models (PrLMs) (Devlin et al., 2019; Liu et al., 2019; Conneau et al., 2020a; Brown et al., 2020) have achieved state-ofthe-art results on most natural language processing (NLP) tasks, where fine-tuning has become a dominant approach to utilize PrLMs.",
      "startOffset" : 47,
      "endOffset" : 129
    }, {
      "referenceID" : 7,
      "context" : "Large scale pretrained language models (PrLMs) (Devlin et al., 2019; Liu et al., 2019; Conneau et al., 2020a; Brown et al., 2020) have achieved state-ofthe-art results on most natural language processing (NLP) tasks, where fine-tuning has become a dominant approach to utilize PrLMs.",
      "startOffset" : 47,
      "endOffset" : 129
    }, {
      "referenceID" : 17,
      "context" : "Adapter-based tuning (Houlsby et al., 2019; Bapna and Firat, 2019) has been proposed as a",
      "startOffset" : 21,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : "Adapter-based tuning (Houlsby et al., 2019; Bapna and Firat, 2019) has been proposed as a",
      "startOffset" : 21,
      "endOffset" : 66
    }, {
      "referenceID" : 39,
      "context" : "For NLP, adapters are usually light-weight modules inserted between transformer layers (Vaswani et al., 2017).",
      "startOffset" : 87,
      "endOffset" : 109
    }, {
      "referenceID" : 17,
      "context" : "Though using much less trainable parameters, adapter-based tuning has demonstrated comparable performance with full PrLM fine-tuning (Houlsby et al., 2019; Bapna and Firat, 2019; Stickland and Murray, 2019).",
      "startOffset" : 133,
      "endOffset" : 206
    }, {
      "referenceID" : 4,
      "context" : "Though using much less trainable parameters, adapter-based tuning has demonstrated comparable performance with full PrLM fine-tuning (Houlsby et al., 2019; Bapna and Firat, 2019; Stickland and Murray, 2019).",
      "startOffset" : 133,
      "endOffset" : 206
    }, {
      "referenceID" : 37,
      "context" : "Though using much less trainable parameters, adapter-based tuning has demonstrated comparable performance with full PrLM fine-tuning (Houlsby et al., 2019; Bapna and Firat, 2019; Stickland and Murray, 2019).",
      "startOffset" : 133,
      "endOffset" : 206
    }, {
      "referenceID" : 4,
      "context" : "This design inspires most of the following work (Pfeiffer et al., 2020a,c; Bapna and Firat, 2019).",
      "startOffset" : 48,
      "endOffset" : 97
    }, {
      "referenceID" : 10,
      "context" : "Fine-tuning large-scale PrLMs on downstream tasks can suffer from overfitting and bad generalization issues (Dodge et al., 2020; Phang et al., 2018).",
      "startOffset" : 108,
      "endOffset" : 148
    }, {
      "referenceID" : 35,
      "context" : "Fine-tuning large-scale PrLMs on downstream tasks can suffer from overfitting and bad generalization issues (Dodge et al., 2020; Phang et al., 2018).",
      "startOffset" : 108,
      "endOffset" : 148
    }, {
      "referenceID" : 22,
      "context" : "Since the weights of the PrLM are the same before and after adapter-based tuning, to verify this, we use Representational Similarity Analysis (RSA) (Laakso and Cottrell, 2000) to assess the similarity of tuned representations to those without tuning at each transformer layer.",
      "startOffset" : 148,
      "endOffset" : 175
    }, {
      "referenceID" : 0,
      "context" : "RSA has been widely used to analyze the similarity between two neural network outputs (Abnar et al., 2019; Chrupała and Alishahi, 2019; Merchant et al., 2020), which works by creating two comparable sets of representations by inputting a same set of n samples to the two models.",
      "startOffset" : 86,
      "endOffset" : 158
    }, {
      "referenceID" : 6,
      "context" : "RSA has been widely used to analyze the similarity between two neural network outputs (Abnar et al., 2019; Chrupała and Alishahi, 2019; Merchant et al., 2020), which works by creating two comparable sets of representations by inputting a same set of n samples to the two models.",
      "startOffset" : 86,
      "endOffset" : 158
    }, {
      "referenceID" : 27,
      "context" : "RSA has been widely used to analyze the similarity between two neural network outputs (Abnar et al., 2019; Chrupała and Alishahi, 2019; Merchant et al., 2020), which works by creating two comparable sets of representations by inputting a same set of n samples to the two models.",
      "startOffset" : 86,
      "endOffset" : 158
    }, {
      "referenceID" : 40,
      "context" : "We use a subset of GLUE tasks (Wang et al., 2018) for our analysis.",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 18,
      "context" : "sentation change generally arises in the top layers of the network, which is consistent with previous findings that higher layers are more task relevant (Howard and Ruder, 2018).",
      "startOffset" : 153,
      "endOffset" : 177
    }, {
      "referenceID" : 40,
      "context" : "We further confirm the effectiveness of adapters in low-resource settings on the GLUE benchmark (Wang et al., 2018) (§3.",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 14,
      "context" : "We also present the results of adding task-adaptive pretraining (+TAPT) (Gururangan et al., 2020).",
      "startOffset" : 72,
      "endOffset" : 97
    }, {
      "referenceID" : 40,
      "context" : "(2021) to study lowresource adaptation using eight datasets from the GLUE benchmark (Wang et al., 2018) which covers four types of tasks: natural language inference (MNLI, QNLI, RTE), paraphrase detection (MRPC, QQP), sentiment classification (SST-2) and linguistic acceptability (CoLA).",
      "startOffset" : 84,
      "endOffset" : 103
    }, {
      "referenceID" : 7,
      "context" : "All experiments in this section are based on XLM-R-large (Conneau et al., 2020a), a recent SOTA multilingual PrLM covering 100 languages.",
      "startOffset" : 57,
      "endOffset" : 80
    }, {
      "referenceID" : 19,
      "context" : "We conduct evaluations on a set of multilingual tasks from XTREME (Hu et al., 2020), including Universal Dependencies v2.",
      "startOffset" : 66,
      "endOffset" : 83
    }, {
      "referenceID" : 30,
      "context" : "5 tree banks (UD-POS) (Nivre et al., 2018), Wikiann NER (Pan et al.",
      "startOffset" : 22,
      "endOffset" : 42
    }, {
      "referenceID" : 31,
      "context" : ", 2018), Wikiann NER (Pan et al., 2017), and cross-lingual natural language inference (XNLI) (Conneau et al.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 8,
      "context" : ", 2017), and cross-lingual natural language inference (XNLI) (Conneau et al., 2020b).",
      "startOffset" : 61,
      "endOffset" : 84
    }, {
      "referenceID" : 19,
      "context" : "Specifically, adapter-based tuning outperforms the reported finetuning results (Hu et al., 2020) on Target and Distant by 2.",
      "startOffset" : 79,
      "endOffset" : 96
    }, {
      "referenceID" : 16,
      "context" : "Prior work (Hochreiter and Schmidhuber, 1997; Li et al., 2018) shows that the flatness of a local minima correlates with the generalization capability.",
      "startOffset" : 11,
      "endOffset" : 62
    }, {
      "referenceID" : 24,
      "context" : "Prior work (Hochreiter and Schmidhuber, 1997; Li et al., 2018) shows that the flatness of a local minima correlates with the generalization capability.",
      "startOffset" : 11,
      "endOffset" : 62
    }, {
      "referenceID" : 23,
      "context" : "Here, for the sake of curiosity, we further compare adapter-based tuning to fine-tuning regularized by mixout (Lee et al., 2020) on a subset of GLUE tasks, since mixout similarly regularizes the learning process by mitigating the forgetting issue.",
      "startOffset" : 110,
      "endOffset" : 128
    }, {
      "referenceID" : 9,
      "context" : "Fine-tuning pretrained large scale language models has proven its effectiveness on a wide range of NLP tasks (Devlin et al., 2019; Liu et al., 2019; Conneau et al., 2020a; Brown et al., 2020).",
      "startOffset" : 109,
      "endOffset" : 191
    }, {
      "referenceID" : 25,
      "context" : "Fine-tuning pretrained large scale language models has proven its effectiveness on a wide range of NLP tasks (Devlin et al., 2019; Liu et al., 2019; Conneau et al., 2020a; Brown et al., 2020).",
      "startOffset" : 109,
      "endOffset" : 191
    }, {
      "referenceID" : 7,
      "context" : "Fine-tuning pretrained large scale language models has proven its effectiveness on a wide range of NLP tasks (Devlin et al., 2019; Liu et al., 2019; Conneau et al., 2020a; Brown et al., 2020).",
      "startOffset" : 109,
      "endOffset" : 191
    }, {
      "referenceID" : 17,
      "context" : "Adapterbased tuning is proposed to deal with this problem (Houlsby et al., 2019).",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : "Most previous work has demonstrated that it achieves comparable performance to fine-tuning (Bapna and Firat, 2019; Pfeiffer et al., 2020b,a,c; Rücklé et al., 2020; Wang et al., 2020; Guo et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 200
    }, {
      "referenceID" : 36,
      "context" : "Most previous work has demonstrated that it achieves comparable performance to fine-tuning (Bapna and Firat, 2019; Pfeiffer et al., 2020b,a,c; Rücklé et al., 2020; Wang et al., 2020; Guo et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 200
    }, {
      "referenceID" : 41,
      "context" : "Most previous work has demonstrated that it achieves comparable performance to fine-tuning (Bapna and Firat, 2019; Pfeiffer et al., 2020b,a,c; Rücklé et al., 2020; Wang et al., 2020; Guo et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 200
    }, {
      "referenceID" : 13,
      "context" : "Most previous work has demonstrated that it achieves comparable performance to fine-tuning (Bapna and Firat, 2019; Pfeiffer et al., 2020b,a,c; Rücklé et al., 2020; Wang et al., 2020; Guo et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 200
    }, {
      "referenceID" : 10,
      "context" : "2216 Fine-tuning PrLMs in a low-resource setting has been studied for a while (Dodge et al., 2020; Lee et al., 2020; Phang et al., 2018; Jiang et al., 2020; Zhang et al., 2021).",
      "startOffset" : 78,
      "endOffset" : 176
    }, {
      "referenceID" : 23,
      "context" : "2216 Fine-tuning PrLMs in a low-resource setting has been studied for a while (Dodge et al., 2020; Lee et al., 2020; Phang et al., 2018; Jiang et al., 2020; Zhang et al., 2021).",
      "startOffset" : 78,
      "endOffset" : 176
    }, {
      "referenceID" : 35,
      "context" : "2216 Fine-tuning PrLMs in a low-resource setting has been studied for a while (Dodge et al., 2020; Lee et al., 2020; Phang et al., 2018; Jiang et al., 2020; Zhang et al., 2021).",
      "startOffset" : 78,
      "endOffset" : 176
    }, {
      "referenceID" : 20,
      "context" : "2216 Fine-tuning PrLMs in a low-resource setting has been studied for a while (Dodge et al., 2020; Lee et al., 2020; Phang et al., 2018; Jiang et al., 2020; Zhang et al., 2021).",
      "startOffset" : 78,
      "endOffset" : 176
    }, {
      "referenceID" : 43,
      "context" : "2216 Fine-tuning PrLMs in a low-resource setting has been studied for a while (Dodge et al., 2020; Lee et al., 2020; Phang et al., 2018; Jiang et al., 2020; Zhang et al., 2021).",
      "startOffset" : 78,
      "endOffset" : 176
    }, {
      "referenceID" : 11,
      "context" : "On the other hand, catastrophic forgetting can appear when transferring a pretrained neural networks (French, 1999; McCloskey and Cohen, 1989; Goodfellow et al., 2013), where the learned knowledge from pretraining is lost when adapting to downstream tasks.",
      "startOffset" : 101,
      "endOffset" : 167
    }, {
      "referenceID" : 26,
      "context" : "On the other hand, catastrophic forgetting can appear when transferring a pretrained neural networks (French, 1999; McCloskey and Cohen, 1989; Goodfellow et al., 2013), where the learned knowledge from pretraining is lost when adapting to downstream tasks.",
      "startOffset" : 101,
      "endOffset" : 167
    }, {
      "referenceID" : 12,
      "context" : "On the other hand, catastrophic forgetting can appear when transferring a pretrained neural networks (French, 1999; McCloskey and Cohen, 1989; Goodfellow et al., 2013), where the learned knowledge from pretraining is lost when adapting to downstream tasks.",
      "startOffset" : 101,
      "endOffset" : 167
    }, {
      "referenceID" : 29,
      "context" : "This phenomenon often appears in NLP tasks (Mou et al., 2016; Arora et al., 2019).",
      "startOffset" : 43,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : "This phenomenon often appears in NLP tasks (Mou et al., 2016; Arora et al., 2019).",
      "startOffset" : 43,
      "endOffset" : 81
    } ],
    "year" : 2021,
    "abstractText" : "Adapter-based tuning has recently arisen as an alternative to fine-tuning. It works by adding light-weight adapter modules to a pretrained language model (PrLM) and only updating the parameters of adapter modules when learning on a downstream task. As such, it adds only a few trainable parameters per new task, allowing a high degree of parameter sharing. Prior studies have shown that adapter-based tuning often achieves comparable results to finetuning. However, existing work only focuses on the parameter-efficient aspect of adapterbased tuning while lacking further investigation on its effectiveness. In this paper, we study the latter. We first show that adapterbased tuning better mitigates forgetting issues than fine-tuning since it yields representations with less deviation from those generated by the initial PrLM. We then empirically compare the two tuning methods on several downstream NLP tasks and settings. We demonstrate that 1) adapter-based tuning outperforms fine-tuning on low-resource and cross-lingual tasks; 2) it is more robust to overfitting and less sensitive to changes in learning rates.",
    "creator" : "LaTeX with hyperref"
  }
}