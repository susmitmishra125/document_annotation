{
  "name" : "2021.acl-long.444.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Modeling Bilingual Conversational Characteristics for Neural Chat Translation",
    "authors" : [ "Yunlong Liang", "Fandong Meng", "Yufeng Chen", "Jinan Xu", "Jie Zhou" ],
    "emails" : [ "yunlongliang@bjtu.edu.cn", "chenyf@bjtu.edu.cn", "jaxu@bjtu.edu.cn", "fandongmeng@tencent.com", "withtomzhou@tencent.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5711–5724\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5711"
    }, {
      "heading" : "1 Introduction",
      "text" : "A conversation may involve participants that speak in different languages (e.g., one speaking in English and another in Chinese). Fig. 1 shows an example, where the English role R1 and the Chinese role R2 are talking about the “boat”. The\n∗Work was done when Yunlong Liang was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China.\n† Jinan Xu is the corresponding author. 1Code and data are publicly available at: https://\ngithub.com/XL2248/CPCC\ngoal of chat translation is to translate bilingual conversational text, i.e., converting one participant’s language (e.g., English) to another’s (e.g., Chinese) and vice versa (Farajian et al., 2020). It enables multiple speakers to communicate with each other in their native languages, which has a wide application in industry-level services.\nAlthough sentence-level Neural Machine Translation (NMT) (Sutskever et al., 2014; Vaswani et al., 2017; Meng and Zhang, 2019; Hassan et al., 2018; Yan et al., 2020; Zhang et al., 2019) has achieved promising progress, it still faces challenges in accurately translating conversational text due to abandoning the dialogue history, which leads to role-irrelevant, incoherent and inconsistent translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018). Further, context-aware NMT (Tiedemann and Scherrer, 2017; Voita et al., 2018, 2019a,b; Wang et al., 2019; Maruf and Haffari, 2018; Maruf et al., 2019; Ma et al., 2020) can be directly applied to chat translation through incorporating the dialogue history but cannot obtain satisfactory results in this sce-\nnario (Moghe et al., 2020). One important reason is the lack of explicitly modeling the inherent bilingual conversational characteristics, e.g., role preference, dialogue coherence, and translation consistency, as pointed out by Farajian et al. (2020).\nFor a conversation, its dialogue history contains rich role preference information such as emotion, style, and humor, which is beneficial to rolerelevant utterance generation (Wu et al., 2020). As shown in Fig. 1, the utterances X1, X3 and X5 from role R1 always have strong emotions (i.e., joy) because of his/her preference, and preserving the same preference information across languages can help raise emotional resonance and mutual understanding (Moghe et al., 2020). Meanwhile, there exists semantic coherence in the conversation, as the solid green arrow in Fig. 1, where the utterance X5 naturally and semantically connects with the dialogue history (X1∼4) on the topic “boat”. In addition, the bilingual conversation exhibits translation consistency, where the correct lexical choice to translate the current utterance might have appeared in preceding turns. For instance, the word “sail” in X1 is translated into “jiàchuán”, and thus the word “sailing” in X3 should be mapped into “jiàchuán” rather than other words (e.g., “hángxı́ng”2) to maintain translation consistency. On the contrary, if we ignore these characteristics, translations might be role-irrelevant, incoherent, inconsistent, and detrimental to further communication like the translation produced by the “S-NMT” in Fig. 1. Although the translation is acceptable at the sentence level, it is abrupt at the bilingual conversation level.\nApparently, how to effectively exploit these bilingual conversational characteristics is one of the core issues in chat translation. And it is challenging to implicitly capture these properties by just incorporating the complex dialogue history into encoders due to lacking the relevant information guidance (Farajian et al., 2020). On the other hand, the Conditional Variational Auto-Encoder (CVAE) (Sohn et al., 2015) has shown its superiority in learning distributions of data properties, which is often utilized to model the diversity (Zhao et al., 2017), coherence (Wang and Wan, 2019) and users’ personalities (Bak and Oh, 2019), etc. In spite of its success, adapting it to chat translation is non-trivial, especially involving multiple tailored latent variables.\n2The words “jiàchuán” and “hángxı́ng” express similar meaning.\nTherefore, in this paper, we propose a model, named CPCC, to capture role preference, dialogue coherence, and translation consistency with latent variables learned by the CVAE for neural chat translation. CPCC contains three specific latent variational modules to learn the distributions of role preference, dialogue coherence, and translation consistency, respectively. Specifically, we firstly use one role-tailored latent variable, sampled from the learned distribution conditioned only on the utterances from this role, to preserve preference. Then, we utilize another latent variable, generated by the distribution conditioned on source-language dialogue history, to maintain coherence. Finally, we leverage the last latent variable, generated by the distribution conditioned on paired bilingual conversational utterances, to keep translation consistency. As a result, these tailored latent variables allow our CPCC to produce role-specific, coherent, and consistent translations, and hence make the bilingual conversation go fluently.\nWe conduct experiments on WMT20 Chat Translation dataset: BConTrasT (En⇔De3) (Farajian et al., 2020) and a self-collected dialogue corpus: BMELD (En⇔Ch). Results demonstrate that our model achieves consistent improvements in four directions in terms of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), showing its effectiveness and generalizability. Human evaluation further suggests that our model effectively alleviates the issue of role-irrelevant, incoherent and inconsistent translations compared to other methods. Our contributions are summarized as follows:\n• To the best of our knowledge, we are the first to incorporate the role preference, dialogue coherence, and translation consistency into neural chat translation. • We are the first to build a bridge between the dialogue and machine translation via conditional variational auto-encoder, which effectively models three inherent characteristics in bilingual conversation for neural chat translation. • Our approach gains consistent and significant performance over the standard context-aware baseline and remarkably outperforms some state-of-the-art context-aware NMT models. • We contribute a new bilingual dialogue corpus (BMELD, En⇔Ch) with manual translations and our codes to the research community.\n3English⇔German: En⇔De. English⇔Chinese: En⇔Ch."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 Sentence-Level NMT",
      "text" : "Given an input sentence X={xi}Mi=1 with M tokens, the model is asked to produce its translation Y={yi}Ni=1 with N tokens. The conditional distribution of the NMT is:\npθ(Y |X) = N∏ t=1 pθ(yt|X, y1:t−1),\nwhere θ are model parameters and y1:t−1 is the partial translation."
    }, {
      "heading" : "2.2 Context-Aware NMT",
      "text" : "Given a source context DX={Xi}Ji=1 and a target context DY={Yi}Ji=1 with J aligned sentence pairs (Xi, Yi), the context-aware NMT (Ma et al., 2020) is formalized as:\npθ(DY |DX) = J∏ i=1 pθ(Yi|Xi, X<i, Y<i),\nwhere X<i and Y<i are the preceding context."
    }, {
      "heading" : "2.3 Variational NMT",
      "text" : "The variational NMT model (Zhang et al., 2016) is the combination of CVAE (Sohn et al., 2015) and NMT. It introduces a random latent variable z into the NMT conditional distribution:\npθ(Y |X) = ∫ z pθ(Y |X, z) · pθ(z|X)dz. (1) Given a source sentence X , a latent variable z is firstly sampled by the prior network from the encoder, and then target sentence is generated by the decoder: Y ∼ pθ(Y |X, z), where z ∼ pθ(z|X).\nAs it is hard to marginalize Eq. 1, the CVAE training objective is a variational lower bound of the conditional log-likelihood: L(θ, φ;X,Y ) = −KL(qφ(z|X,Y )‖pθ(z|X))\n+ Eqφ(z|X,Y )[log pθ(Y |z, X)] ≤ log p(Y |X),\nwhere φ are parameters of the posterior network and KL(·) indicates Kullback–Leibler divergence between two distributions produced by prior networks and posterior networks (Sohn et al., 2015; Kingma and Welling, 2013)."
    }, {
      "heading" : "3 Chat NMT",
      "text" : "We aim to learn a model that can capture inherent characteristics in the bilingual dialogue history for producing high-quality translations, i.e., using the context for better translations (Farajian\net al., 2020). Following (Maruf et al., 2018), we define paired bilingual utterances (Xi, Yi) as a turn in Fig. 2, where we will translate the current utterance X2k+1 at the (2k+1)-th turn. Here, we denote the utterance X2k+1 as Xu and its translation Y2k+1 as Yu for simplicity, where Xu={xi}mi=1 with m tokens and Yu={yi}ni=1 with n tokens. Formally, the conditional distribution for the current utterance is\npθ(Yu|Xu, C) = n∏ t=1 pθ(yt|Xu, y1:t−1, C),\nwhere C is the bilingual dialogue history. Before we dig into the details of how to utilize C, we define three types of context in C (as shown in Fig. 2): (1) the set of previous role-specific source-language turns, denoted as CroleX ={X1, X3, X5, ..., X2k+1}4 where k ∈ [0, |T |−32 ] and T is the total number of turns; (2) the set of previous source-language turns, denoted as CX={X1, X2, X3, ..., X2k}; and (3) the set of previous target-language turns, denoted as CY={Y1, Y2, Y3, ..., Y2k}."
    }, {
      "heading" : "4 Our Methodology",
      "text" : "Fig. 3 demonstrates an overview of our model, consisting of five components: input representation, encoder, latent variational modules, decoder, and training objectives. Specifically, we aim to model both dialogue and translation simultaneously. Therefore, for the input representation (§ 4.1), we incorporate dialogue-level embeddings, i.e., role and dialogue turn embeddings, into the encoder (§ 4.2). Then, we introduce three specific latent variational modules (§ 4.3) to learn the distributions for varied inherent bilingual characteristics. Finally, we elaborate on how to incorporate the three tailored latent variables sampled from\n4CroleY ={Y2, Y4, Y6, ..., Y2k} is also role-specific utterances of the interlocutor, which is used to model the interlocutor’s consistency in the reverse translation direction. Here, we take one translation direction (i.e., En⇒Ch) as an example.\nthe distributions into the decoder (§ 4.4) and our two-stage training objectives (§ 4.5)."
    }, {
      "heading" : "4.1 Input Representation",
      "text" : "The CPCC contains three types of inputs: source input Xu, target input Yu, and context inputs {CroleX , CX , CY }. Apart from the conventional word embeddings WE and position embeddings PE (Vaswani et al., 2017), we also introduce role embeddings RE and dialogue turn embeddings TE to identify different utterances. Specifically, for Xu, we firstly project it into these embeddings. Then, we perform a sum operation to unify them into a single input for each token xi: h0i = WE(xi) +PE(xi) +RE(xi) +TE(xi), (2) where 1 ≤ i ≤ m and WE ∈ R|V |×d, RE ∈ R|R|×d and SE ∈ R|T |×d. |V |, |R|, |T |, and d denote the size of shared vocabulary, number of roles, max turns of dialogue, and hidden size, respectively. h0 ∈ Rm×d, similarly for Yu. For each of {CroleX , CX , CY }, we add ‘[cls]’ tag at the head of it and use ‘[sep]’ tag to separate its utterances (Devlin et al., 2019), and then get its embeddings via Eq. 2."
    }, {
      "heading" : "4.2 Encoder",
      "text" : "The Transformer encoder consists of Ne stacked layers and each layer includes two sub-layers:5 a multi-head self-attention (SelfAtt) sub-layer and a position-wise feed-forward network (FFN) sublayer (Vaswani et al., 2017):\ns`e = SelfAtt(h `−1 e ) + h `−1 e , h `−1 e ∈ Rm×d,\nh`e = FFN(s ` e) + s ` e, {h`e, s`e} ∈ Rm×d,\n5We omit the layer normalization for simplicity, and you may refer to (Vaswani et al., 2017) for more details.\nwhere h`e denotes the state of the `-th encoder layer and h0e denotes the initialized feature h\n0. We prepare the representations of Xu and {CroleX , CX , CY } for training prior and recognition networks. For Xu, we apply mean-pooling with mask operation over the output hNe,Xe of the Neth encoder layer, i.e., hX= 1m ∑m i=1(M X i h Ne,X e,i ), hX ∈ Rd, where MX ∈ Rm denotes the mask matrix, whose value is either 1 or 0 indicating whether the token is padded (Zhang et al., 2016). For CroleX , as shown in Fig. 3, we follow (Ma et al., 2020) and share the first encoder layer to obtain the context representation. Here, we take the hidden state of ‘[cls]’ as its representation, denoted as hctxrole ∈ Rd. Similarly, we obtain representations ofCX andCY , denoted as hctxX ∈ Rd and hctxY ∈ Rd, respectively.\nFor training recognition networks, we obtain the representation of Yu as hY= 1n ∑n i=1(M Y i h Ne,Y e,i ), hY ∈ Rd, where MY ∈ Rn, similar to MX ."
    }, {
      "heading" : "4.3 Latent Variational Modules",
      "text" : "We design three tailored latent variational modules to learn the distributions of inherent bilingual conversational characteristics, i.e., role preference, dialogue coherence, and translation consistency.\nRole Preference. To preserve the role preference when translating the role’s current utterance, we only encode the previous utterances of this role and produce a role-tailored latent variable zrole ∈ Rdz , where dz is the latent size. Inspired by (Wang and Wan, 2019), we use isotropic Gaussian distribution as the prior distribution of zrole: pθ(zrole|Xu, CroleX ) ∼ N (µrole,σ2roleI), where I\ndenotes the identity matrix and we have µrole = MLP role θ (hX ;h ctx role),\nσrole = Softplus(MLP role θ (hX ;h ctx role)),\nwhere MLP(·) and Softplus(·) are multi-layer perceptron and approximation of ReLU function, respectively. (·;·) indicates concatenation operation.\nAt training, the posterior distribution conditions on both role-specific utterances and the current translation, which contain rich role preference information. Therefore, the prior network can learn a role-tailored distribution by approaching the posterior network via KL divergence (Sohn et al., 2015): qφ(zrole|Xu, CroleX , Yu) ∼ N (µ′role,σ′2roleI) and {µ′role, σ′role} are calculated as:\nµ′role = MLP role φ (hX ;h ctx role;hY ), σ′role = Softplus(MLP role φ (hX ;h ctx role;hY )).\nDialogue Coherence. To maintain the coherence in chat translation, we encode the entire sourcelanguage utterances and then generate a latent variable zdia ∈ Rdz . Similar to zrole, we define its prior distribution as: pθ(zdia|Xu, CX) ∼ N (µdia,σ2diaI) and {µdia, σdia} are calculated as:\nµdia = MLP dia θ (hX ;h ctx X ), σdia = Softplus(MLP dia θ (hX ;h ctx X )).\nAt training, the posterior distribution conditions on both the entire source-language utterances and the translation that provide a dialoguelevel coherence clue, and is responsible for guiding the learning of the prior distribution. Specifically, we define the posterior distribution as: qφ(zdia|Xu, CX , Yu) ∼ N (µ′dia,σ′2diaI), where µ′dia and σ ′ dia are calculated as:\nµ′dia = MLP dia φ (hX ;h ctx X ;hY ), σ′dia = Softplus(MLP dia φ (hX ;h ctx X ;hY )).\nTranslation Consistency. To keep the lexical choice of translation consistent with those of previous utterances, we encode the paired sourcetarget utterances and then sample a latent variable ztra ∈ Rdz . We define its prior distribution as: pθ(ztra|Xu, CX , CY ) ∼ N (µtra,σ2traI) and {µtra, σtra} are calculated as:\nµtra = MLP tra θ (hX ;h ctx X ;h ctx Y ), σtra = Softplus(MLP tra θ (hX ;h ctx X ;h ctx Y )).\nAt training, the posterior distribution conditions on all paired bilingual dialogue utterances that contain implicit and aligned information,\nand serves as learning of the prior distribution. Specifically, we define the posterior distribution as: qφ(ztra|Xu, CX , CY , Yu) ∼ N (µ′tra,σ′2traI), where µ′tra and σ ′ tra are calculated as: µ′tra = MLP tra φ (hX ;h ctx X ;h ctx Y ;hY ), σ′tra = Softplus(MLP tra φ (hX ;h ctx X ;h ctx Y ;hY ))."
    }, {
      "heading" : "4.4 Decoder",
      "text" : "The decoder adopts a similar structure to the encoder, and each of Nd decoder layers contains an additional cross-attention sub-layer (CrossAtt):\ns`d = SelfAtt(h `−1 d ) + h `−1 d , h `−1 d ∈ R n×d, c`d = CrossAtt(s ` d,h Ne e ) + s ` d, s ` d ∈ Rn×d, h`d = FFN(c ` d) + c ` d, {c`d,h`d} ∈ Rn×d,\nwhere h`d denotes the state of the `-th decoder layer. As shown in Fig. 3, we obtain the latent variables {zrole, zdia, ztra} either from the posterior distribution predicted by recognition networks (training process as the solid grey lines) or from prior distribution predicted by prior networks (inference process as the dashed red lines). Finally, we incorporate {zrole, zdia, ztra} into the state of the top layer of the decoder with a projection layer:\not = Tanh(Wp[h Nd d,t ; zrole; zdia; ztra] + bp), ot ∈ R d, where Wp ∈ Rd×(d+3dz) and bp ∈ Rd are training parameters, hNdd,t is the hidden state at time-step t of the Nd-th decoder layer. Then, ot is fed to a linear transformation and softmax layer to predict the probability distribution of the next target token:\npt = Softmax(Woot + bo), pt ∈ R|V |, where Wo ∈ R|V |×d and bo ∈ R|V | are training parameters."
    }, {
      "heading" : "4.5 Training Objectives",
      "text" : "We apply a two-stage training strategy (Zhang et al., 2018; Ma et al., 2020). Firstly, we train our model on large-scale sentence-level NMT data to minimize the cross-entropy objective:\nL(θ;X,Y ) = − N∑ t=1 logpθ(yt|X, y1:t−1).\nSecondly, we fine-tune it on the chat translation data to maximize the following objective: J (θ, φ;Xu, CroleX , CX , CY , Yu) = −KL(qφ(zrole|Xu, CroleX , Yu)‖pθ(zrole|Xu, CroleX )) −KL(qφ(zdia|Xu, CX , Yu)‖pθ(zdia|Xu, CX)) −KL(qφ(ztra|Xu, CX , CY , Yu)‖pθ(ztra|Xu, CX , CY )) + Eqφ [logpθ(Yu|Xu, zrole, zdia, ztra)].\nWe use the reparameterization trick (Kingma and Welling, 2013) to estimate the gradients of the prior and recognition networks (Zhao et al., 2017)."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Datasets and Metrics",
      "text" : "Datasets. We apply a two-stage training strategy, i.e., firstly training on a large-scale sentence-level NMT corpus (WMT206) and then fine-tuning on chat translation corpus (BConTrasT (Farajian et al., 2020)7 and BMELD). The details (WMT20 data and results of the first stage) are shown in Appendix A.\nBConTrasT. The dataset8 is first provided by WMT 2020 Chat Translation Task (Farajian et al., 2020), which is translated from English into German and is based on the monolingual Taskmaster-1 corpus (Byrne et al., 2019). The conversations (originally in English) were first automatically translated into German and then manually postedited by Unbabel editors,9 who are native German speakers. Having the conversations in both languages allows us to simulate bilingual conversations in which one speaker, the customer, speaks in German and the other speaker, the agent, answers in English.\nBMELD. Similarly, based on the dialogue dataset in the MELD (originally in English) (Poria et al., 2019),10 we firstly crawled the corresponding Chinese translations from this11 and then manually post-edited them according to the dialogue history by native Chinese speakers, who are postgraduate students majoring in English. Finally, following (Farajian et al., 2020), we assume 50% speakers as Chinese speakers to keep data balance for Ch⇒En translations and build the bilingual MELD (BMELD). For the Chinese, we segment the sentence using Stanford CoreNLP toolkit12.\nMetrics. For fair comparison, we use the SacreBLEU13 (Post, 2018) and v0.7.25 for TER (Snover\n6http://www.statmt.org/wmt20/translation-task.html 7http://www.statmt.org/wmt20/chat-task.html 8https://github.com/Unbabel/BConTrasT 9www.unbabel.com\n10The MELD is a multimodal emotionLines dialogue dataset, each utterance of which corresponds to a video, voice, and text, and is annotated with detailed emotion and sentiment.\n11https://www.zimutiantang.com/ 12https://stanfordnlp.github.io/CoreNLP/index.html 13BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+\nversion.1.4.13\net al., 2006) (the lower the better) with the statistical significance test (Koehn, 2004). For En⇔De, we report case-sensitive score following the WMT20 chat task (Farajian et al., 2020). For Ch⇒En, we report case-insensitive score. For En⇒Ch, we report the character-level BLEU score."
    }, {
      "heading" : "5.2 Implementation Details",
      "text" : "For all experiments, we follow the TransformerBase and Transformer-Big settings illustrated in (Vaswani et al., 2017). In Transformer-Base, we use 512 as hidden size (i.e., d), 2048 as filter size and 8 heads in multi-head attention. In Transformer-Big, we use 1024 as hidden size, 4096 as filter size, and 16 heads in multi-head attention. All our Transformer models containNe = 6 encoder layers and Nd = 6 decoder layers and all models are trained using THUMT (Tan et al., 2020) framework. We conduct experiments on the validation set of En⇒De to select the hyperparameters of context length and latent dimension, which are then shared for all tasks. For the results and more details (other hyperparameters setting and average running time), please refer to Appendix B, C, and D."
    }, {
      "heading" : "5.3 Comparison Models",
      "text" : "Baseline NMT Models. Transformer (Vaswani et al., 2017): the de-facto NMT model that does not fine-tune on chat translation data. Transformer+FT: fine-tuning on the chat translation data after being pre-trained on sentence-level NMT corpus.\nContext-Aware NMT Models. DocTransformer+FT (Ma et al., 2020): a stateof-the-art document-level NMT model based on Transformer sharing the first encoder layer to incorporate the bilingual dialogue history. Dia-Transformer+FT (Maruf et al., 2018): using an additional RNN-based (Hochreiter and Schmidhuber, 1997) encoder to incorporate the mixed-language dialogue history, where we re-implement it based on Transformer and use another Transformer layer to introduce context. V-Transformer+FT (Zhang et al., 2016; McCarthy\net al., 2020): the variational NMT model based on Transformer also sharing the first encoder layer to exploit the bilingual context for fair comparison."
    }, {
      "heading" : "5.4 Main Results",
      "text" : "Overall, we separate the models into two parts in Tab. 2: the Base setting and the Big setting. In each part, we show the results of our re-implemented Transformer baselines, the context-aware NMT systems, and our approach on En⇔De and En⇔Ch.\nResults on En⇔De. Under the Base setting, CPCC substantially outperforms the baselines (e.g., “Transformer+FT”) by a large margin with 1.70↑ and 1.48↑ BLEU scores on En⇒De and De⇒En, respectively. On the TER, our CPCC achieves a significant improvement of 1.3 points in both language pairs. Under the Big setting, our CPCC also consistently boosts the performance in both direc-\ntions (i.e., 1.22↑ and 1.47↑ BLEU scores, 0.4↓ and 1.1↓ TER scores), showing its effectiveness.\nCompared against the strong context-aware NMT systems (underlined results), our CPCC significantly surpasses them (about 1.39∼1.59↑ BLEU scores and 0.6∼0.9↓ TER scores) in both language directions under both Base and Big settings, demonstrating the superiority of our model.\nResults on En⇔Ch. We also conduct experiments on our self-collected data to validate the generalizability across languages in Tab. 2.\nOur CPCC presents remarkable BLEU improvements over the “Transformer+FT” by a large margin in two directions by 2.33↑ and 0.91↑ BLEU gains under the Base setting, respectively, and by 2.03↑ and 0.83↑ BLEU gains in both directions under the Big setting. These results suggest that CPCC consistently performs well across languages.\nCompared with strong context-aware NMT systems (e.g., “V-Transformer+FT”), our approach notably surpasses them in both language directions under both Base and Big settings, which shows the generalizability and superiority of our model."
    }, {
      "heading" : "6 Analysis",
      "text" : ""
    }, {
      "heading" : "6.1 Ablation Study",
      "text" : "We conduct ablation studies to investigate how well each tailored latent variable of our model works. When removing latent variables listed in Tab. 3, we have the following findings.\n(1) All latent variables make substantial contributions to performance, proving the importance of modeling role preference, dialogue coherence, and translation consistency, which is consistent with our intuition that the properties should be beneficial to better translations (rows 1∼3 vs. row 0).\n(2) Results of rows 4∼7 show the combination effect of three latent variables, suggesting that the combination among three latent variables has a cumulative effect (rows 4∼7 vs. rows 0∼3).\n(3) Row 7 vs. row 0 shows that explicitly modeling the bilingual conversational characteristics significantly outperforms implicit modeling (i.e., just incorporating the dialogue history into encoders), which lacks the relevant information guidance."
    }, {
      "heading" : "6.2 Dialogue Coherence",
      "text" : "Following (Lapata and Barzilay, 2005; Xiong et al., 2019), we measure dialogue coherence as sentence similarity. Specifically, the representation of each sentence is the mean of the distributed vectors of its words, and the dialogue coherence between two sentences s1 and s2 is determined by the cosine similarity:\nsim(s1, s2) = cos(f(s1), f(s2)),\nf(si) = 1 |si| ∑ w∈si (w),\nwhere w is the vector for word w. We use Word2Vec14 (Mikolov et al., 2013) to learn the distributed vectors of words by training on the monolingual dialogue dataset: Taskmaster1 (Byrne et al., 2019). And we set the dimensionality of word embeddings to 100.\nTab. 4 shows the cosine similarity on the test set of De⇒En. It reveals that our model encouraged by tailor-made latent variables produces better coherence in chat translation than contrast systems."
    }, {
      "heading" : "6.3 Human Evaluation",
      "text" : "Inspired by (Bao et al., 2020; Farajian et al., 2020), we use four criteria for human evaluation: (1) Preference measures whether the translation preserves the role preference information; (2) Coherence denotes whether the translation is semantically coherent with the dialogue history; (3) Consistency measures whether the lexical choice of translation is consistent with the preceding utterances; (4) Fluency measures whether the translation is logically reasonable and grammatically correct.\n14https://code.google.com/archive/p/word2vec/\nWe firstly randomly sample 200 examples from the test set of Ch⇒En. Then, we assign each bilingual dialogue history and corresponding 6 generated translations to three human annotators without order, and ask them to evaluate whether each translation meets the criteria defined above. All annotators are postgraduate students and not involved in other parts of our experiments.\nTab. 5 shows that our CPCC effectively alleviates the problem of role-irrelevant, incoherent and inconsistent translations compared with other models (significance test (Koehn, 2004), p < 0.05), indicating the superiority of our model. The interannotator agreement is 0.527, 0.491, 0.556 and 0.485 calculated by the Fleiss’ kappa (Fleiss and Cohen, 1973), for preference, coherence, consistency and fluency, respectively, indicating “Moderate Agreement” for all four criteria. We also present some case studies in Appendix H."
    }, {
      "heading" : "7 Related Work",
      "text" : "Chat NMT. It only involves several researches due to the lack of human-annotated publicly available data (Farajian et al., 2020). Therefore, some\nexisting work (Wang et al., 2016; Maruf et al., 2018; Zhang and Zhou, 2019; Rikters et al., 2020) mainly pays attention to designing methods to automatically construct the subtitles corpus, which may contain noisy bilingual utterances. Recently, Farajian et al. (2020) organize the WMT20 chat translation task and first provide a human postedited corpus, where some teams investigate the effect of dialogue history and finally ensemble their models for higher ranks (Berard et al., 2020; Mohammed et al., 2020; Wang et al., 2020; Bao et al., 2020; Moghe et al., 2020). As a synchronizing study, Wang et al. (2021) use multitask learning to auto-correct the translation error, such as pronoun dropping, punctuation dropping, and typos. Unlike them, we focus on explicitly modeling role preference, dialogue coherence, and translation consistency with tailored latent variables to promote the translation quality.\nContext-Aware NMT. Chat NMT can be viewed as a special case of context-aware NMT, which has attracted many researchers (Gong et al., 2011; Jean et al., 2017; Wang et al., 2017b; Bawden et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Tu et al., 2018; Yang et al., 2019; Kang et al., 2020; Li et al., 2020; Ma et al., 2020) to extend the encoder or decoder for exploring the context impact on translation quality. Although these models can be directly applied to chat translation, they cannot explicitly capture the bilingual conversational characteristics and thus lead to unsatisfactory translations (Moghe et al., 2020). Different from these studies, we focus on explicitly modeling these bilingual conversational characteristics via CVAE for better translations.\nConditional Variational Auto-Encoder. CVAE has verified its superiority in many fields (Sohn et al., 2015). In NMT, Zhang et al. (2016) and Su et al. (2018) extend CVAE to capture the global/local information of source sentence for better results. McCarthy et al. (2020) focus on addressing the posterior collapse with mutual information. Besides, some studies use CVAE to model the correlations between image and text for multimodal NMT (Toyama et al., 2016; Calixto et al., 2019). Although the CVAE has been widely used in NLP tasks, its adaption and utilization to chat translation for modeling inherent bilingual conversational characteristics are non-trivial, and to the best of our knowledge,\nhas never been investigated before."
    }, {
      "heading" : "8 Conclusion and Future Work",
      "text" : "We propose to model bilingual conversational characteristics through tailored latent variables for neural chat translation. Experiments on En⇔De and En⇔Ch directions show that our model notably improves translation quality on both BLEU and TER metrics, showing its superiority and generalizability. Human evaluation further verifies that our model yields role-specific, coherent, and consistent translations by incorporating tailored latent variables into NMT. Moreover, we contribute a new bilingual dialogue data (BMELD, En⇔Ch) with manual translations to the research community. In the future, we would like to explore the effect of multimodality and emotion on chat translation, which has been well studied in dialogue field (Liang et al., 2020)."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The research work descried in this paper has been supported by the National Key R&D Program of China (2020AAA0108001) and the National Nature Science Foundation of China (No. 61976015, 61976016, 61876198 and 61370130). The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve this paper."
    }, {
      "heading" : "A Datasets",
      "text" : "WMT20. For the En⇔De, we combine six corpora including Euporal, ParaCrawl, CommonCrawl, TildeRapid, NewsCommentary, and WikiMatrix, and we combine News Commentary v15, Wiki Titles v2, UN Parallel Corpus V1.0, CCMT\nCorpus, and WikiMatrix for the En⇔Ch. We firstly filter noisy sentence pairs according to their characteristics in terms of duplication and length (whose length exceeds 80). To pre-process the raw data, we employ a series of open-source/in-house scripts, including full-/half-width conversion, unicode conversation, punctuation normalization, and tokenization (Wang et al., 2020). After filtering steps, we generate subwords via joint BPE (Sennrich et al., 2016) with 32K merge operations. Finally, we obtain 45,541,367 sentence pairs for En⇔De and 22,244,006 sentence pairs for En⇔Ch, respectively.\nWe test the model performance of the first stage on newstest2019. The results are shown in Tab. 6.\nB Implementation Details\nFor all experiments, we follow two model settings illustrated in (Vaswani et al., 2017), namely Transformer-Base and Transformer-Big. The training step is set to 200,000 and 2,000 for the first stage and the fine-tuning stage, respectively. The batch size for each GPU is set to 4096 tokens. The beam size is set to 4, and the length penalty is 0.6 among all experiments. All experiments in the first stage are conducted utilizing 8 NVIDIA Tesla V100 GPUs, while we use 2 GPUs for the second stage, i.e., fine-tuning. That gives us about 8*4096 and 2*4096 tokens per update for all experiments in the first-stage and second-stage, respectively. All models are optimized using Adam (Kingma and Ba, 2015) with β1 = 0.9 and β2 = 0.998, and learning rate is set to 1.0 for all experiments. Label smoothing is set to 0.1. We use dropout of 0.1/0.3 for Base and Big setting, respectively. To alleviate the degeneration problem of the variational framework, we apply KL annealing. The KL multiplier λ gradually increases from 0 to 1 over 10, 000 steps. |R| is set to 2 for En⇔De and 7 for En⇔Ch, respectively. |T | is set to 10. The criterion for selecting hyperparameters is the BLEU score on validation sets for both tasks. The average running time is shown in Tab. 7.\nIn the case of blind testing or online use (assumed dealing with En⇒De), since translations of target utterances (i.e., English) will not be given, an inverse De⇒En model is simultaneously trained and used to back-translate target utterances (Bao et al., 2020), similar to all tasks."
    }, {
      "heading" : "C Effect of Context Length",
      "text" : "We firstly investigate the effect of context length (i.e., the number of preceding utterances) on our approach under the Transformer Base setting. As shown in the left of Fig. 4, using three preceding source sentences as dialogue history achieves the best translation performance on the validation set (En⇒De). Using more preceding sentences does not bring any improvement and increases the computational cost. This confirms the finding of Tu et al. (2018) and Zhang et al. (2018) that longdistance context only has limited influence. Therefore, we set the number of preceding sentences to 3 in all experiments."
    }, {
      "heading" : "D Effect of Latent Dimension",
      "text" : "The right of Fig. 4 shows the effect of the latent dimension on translation quality under the Transformer Base setting. Obviously, using latent dimension 32 suffices to achieve superior performance. Increasing the dimension does not lead to any improvements. Therefore, we set the latent dimension to 32 in all experiments."
    }, {
      "heading" : "E KL Divergence",
      "text" : "Generally, KL divergence measures the amount of information encoded in a latent variable. In the extreme case where the KL divergence of latent variable z equals to zero, the model completely ignores z, i.e., it degenerates. Fig. 5 shows that the total KL divergence of our model maintains around 0.2∼0.5 indicating that the degeneration problem does not exist in our model and latent variables can play their corresponding roles."
    }, {
      "heading" : "F Case Study",
      "text" : "In this section, we show some cases in Fig. 6 and Fig. 7 to investigate the effect of different models.\nRole Preference and Dialogue Coherence. As shown in Fig. 6, we observe that the baseline models and the context-aware models except “VTransformer+FT” cannot preserve the role preference information, e.g., joy emotion, even these\n“*-Transformer+FT” models incorporate the bilingual conversational history into the encoder. The “V-Transformer+FT” model produces very slightly emotional elements (e.g., “zěnme?”) due to the latent variable over the source sentence capturing relevant preference information. Meanwhile, we find that all comparison models cannot generate a coherent translation. The reason may be that they fail to capture the conversation-level coherence clue, i.e., “boat”. By contrast, we explicitly model the two characteristics through tailored latent variables and thus obtain satisfactory results.\nTranslation Consistency. As shown in Fig. 7, we observe that all comparison models cannot maintain the translation consistency due to the lack of explicitly modeling this characteristic. Our model has the ability to overcome the issue and can keep the correct lexical choice to translate the current utterance that might have appeared in preceding turns, i.e., “jiàchuàn”.\nTo sum up, both cases show that our model yields role-specific, coherent, and consistent translations by incorporating tailored latent variables into translators, demonstrating its effectiveness and superiority."
    } ],
    "references" : [ {
      "title" : "Variational hierarchical user-based conversation model",
      "author" : [ "JinYeong Bak", "Alice Oh." ],
      "venue" : "Proceedings of EMNLP-IJCNLP, pages 1941–1950.",
      "citeRegEx" : "Bak and Oh.,? 2019",
      "shortCiteRegEx" : "Bak and Oh.",
      "year" : 2019
    }, {
      "title" : "The university of maryland’s submissions to the wmt20 chat translation task: Searching for more data to adapt discourseaware neural machine translation",
      "author" : [ "Calvin Bao", "Yow-Ting Shiue", "Chujun Song", "Jie Li", "Marine Carpuat." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Bao et al\\.,? 2020",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2020
    }, {
      "title" : "Evaluating discourse phenomena in neural machine translation",
      "author" : [ "Rachel Bawden", "Rico Sennrich", "Alexandra Birch", "Barry Haddow." ],
      "venue" : "Proceedings of NAACL, pages 1304–1313.",
      "citeRegEx" : "Bawden et al\\.,? 2018",
      "shortCiteRegEx" : "Bawden et al\\.",
      "year" : 2018
    }, {
      "title" : "Naver labs europe’s participation in the robustness, chat, and biomedical tasks at wmt 2020",
      "author" : [ "Alexandre Berard", "Ioan Calapodescu", "Vassilina Nikoulina", "Jerin Philip." ],
      "venue" : "Proceedings of WMT, pages 460–470.",
      "citeRegEx" : "Berard et al\\.,? 2020",
      "shortCiteRegEx" : "Berard et al\\.",
      "year" : 2020
    }, {
      "title" : "Taskmaster-1: Toward a realistic and diverse dialog dataset",
      "author" : [ "Duckworth", "Semih Yavuz", "Amit Dubey", "Kyu-Young Kim", "Andy Cedilnik." ],
      "venue" : "Proceedings of EMNLP-IJCNLP, pages 4516–4525.",
      "citeRegEx" : "Duckworth et al\\.,? 2019",
      "shortCiteRegEx" : "Duckworth et al\\.",
      "year" : 2019
    }, {
      "title" : "Latent variable model for multi-modal translation",
      "author" : [ "Iacer Calixto", "Miguel Rios", "Wilker Aziz." ],
      "venue" : "Proceedings of ACL, pages 6392–6405.",
      "citeRegEx" : "Calixto et al\\.,? 2019",
      "shortCiteRegEx" : "Calixto et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of NAACL-HLT, pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Findings of the WMT 2020 shared task on chat translation",
      "author" : [ "M. Amin Farajian", "António V. Lopes", "André F.T. Martins", "Sameen Maruf", "Gholamreza Haffari." ],
      "venue" : "Proceedings of WMT, pages 65–75.",
      "citeRegEx" : "Farajian et al\\.,? 2020",
      "shortCiteRegEx" : "Farajian et al\\.",
      "year" : 2020
    }, {
      "title" : "The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability",
      "author" : [ "Joseph L. Fleiss", "Jacob Cohen." ],
      "venue" : "Educational and Psychological Measurement, pages 613– 619.",
      "citeRegEx" : "Fleiss and Cohen.,? 1973",
      "shortCiteRegEx" : "Fleiss and Cohen.",
      "year" : 1973
    }, {
      "title" : "Cache-based document-level statistical machine translation",
      "author" : [ "Zhengxian Gong", "Min Zhang", "Guodong Zhou." ],
      "venue" : "Proceedings of EMNLP, pages 909–919.",
      "citeRegEx" : "Gong et al\\.,? 2011",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2011
    }, {
      "title" : "Achieving human parity on automatic chinese to english news translation",
      "author" : [ "Xu Tan", "Fei Tian", "Lijun Wu", "Shuangzhi Wu", "Yingce Xia", "Dongdong Zhang", "Zhirui Zhang", "Ming Zhou." ],
      "venue" : "arXiv preprint arXiv:1803.05567.",
      "citeRegEx" : "Tan et al\\.,? 2018",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2018
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, pages 1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Does neural machine translation benefit from larger context? arXiv preprint arXiv:1704.05135",
      "author" : [ "Sebastien Jean", "Stanislas Lauly", "Orhan Firat", "Kyunghyun Cho" ],
      "venue" : null,
      "citeRegEx" : "Jean et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Jean et al\\.",
      "year" : 2017
    }, {
      "title" : "Dynamic context selection for document-level neural machine translation via reinforcement learning",
      "author" : [ "Xiaomian Kang", "Yang Zhao", "Jiajun Zhang", "Chengqing Zong." ],
      "venue" : "Proceedings of EMNLP, pages 2242–2254.",
      "citeRegEx" : "Kang et al\\.,? 2020",
      "shortCiteRegEx" : "Kang et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Autoencoding variational bayes",
      "author" : [ "Diederik P Kingma", "Max Welling." ],
      "venue" : "arXiv preprint arXiv:1312.6114.",
      "citeRegEx" : "Kingma and Welling.,? 2013",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2013
    }, {
      "title" : "Statistical significance tests for machine translation evaluation",
      "author" : [ "Philipp Koehn." ],
      "venue" : "Proceedings of EMNLP, pages 388–395.",
      "citeRegEx" : "Koehn.,? 2004",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2004
    }, {
      "title" : "Modeling coherence for neural machine translation with dynamic and topic caches",
      "author" : [ "Shaohui Kuang", "Deyi Xiong", "Weihua Luo", "Guodong Zhou." ],
      "venue" : "Proceedings of COLING, pages 596–606.",
      "citeRegEx" : "Kuang et al\\.,? 2018",
      "shortCiteRegEx" : "Kuang et al\\.",
      "year" : 2018
    }, {
      "title" : "Automatic evaluation of text coherence: Models and representations",
      "author" : [ "Mirella Lapata", "Regina Barzilay." ],
      "venue" : "Proceedings of IJCAI, pages 1085–1090.",
      "citeRegEx" : "Lapata and Barzilay.,? 2005",
      "shortCiteRegEx" : "Lapata and Barzilay.",
      "year" : 2005
    }, {
      "title" : "Has machine translation achieved human parity? a case for document-level evaluation",
      "author" : [ "Samuel Läubli", "Rico Sennrich", "Martin Volk." ],
      "venue" : "Proceedings of EMNLP, pages 4791–4796.",
      "citeRegEx" : "Läubli et al\\.,? 2018",
      "shortCiteRegEx" : "Läubli et al\\.",
      "year" : 2018
    }, {
      "title" : "Does multi-encoder help? a case study on contextaware neural machine translation",
      "author" : [ "Bei Li", "Hui Liu", "Ziyang Wang", "Yufan Jiang", "Tong Xiao", "Jingbo Zhu", "Tongran Liu", "Changliang Li." ],
      "venue" : "Proceedings of ACL, pages 3512–3518.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Infusing multisource knowledge with heterogeneous graph neural network for emotional conversation generation",
      "author" : [ "Yunlong Liang", "Fandong Meng", "Ying Zhang", "Jinan Xu", "Yufeng Chen", "Jie Zhou" ],
      "venue" : null,
      "citeRegEx" : "Liang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2020
    }, {
      "title" : "A simple and effective unified encoder for documentlevel machine translation",
      "author" : [ "Shuming Ma", "Dongdong Zhang", "Ming Zhou." ],
      "venue" : "Proceedings of ACL, pages 3505–3511.",
      "citeRegEx" : "Ma et al\\.,? 2020",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2020
    }, {
      "title" : "Document context neural machine translation with memory networks",
      "author" : [ "Sameen Maruf", "Gholamreza Haffari." ],
      "venue" : "Proceedings of ACL, pages 1275– 1284.",
      "citeRegEx" : "Maruf and Haffari.,? 2018",
      "shortCiteRegEx" : "Maruf and Haffari.",
      "year" : 2018
    }, {
      "title" : "Contextual neural model for translating bilingual multi-speaker conversations",
      "author" : [ "Sameen Maruf", "André F.T. Martins", "Gholamreza Haffari." ],
      "venue" : "Proceedings of WMT, pages 101–112.",
      "citeRegEx" : "Maruf et al\\.,? 2018",
      "shortCiteRegEx" : "Maruf et al\\.",
      "year" : 2018
    }, {
      "title" : "Selective attention for contextaware neural machine translation",
      "author" : [ "Sameen Maruf", "André F.T. Martins", "Gholamreza Haffari." ],
      "venue" : "Proceedings of NAACL, pages 3092–3102.",
      "citeRegEx" : "Maruf et al\\.,? 2019",
      "shortCiteRegEx" : "Maruf et al\\.",
      "year" : 2019
    }, {
      "title" : "Addressing posterior collapse with mutual information for improved variational neural machine translation",
      "author" : [ "Arya D. McCarthy", "Xian Li", "Jiatao Gu", "Ning Dong." ],
      "venue" : "Proceedings of ACL, pages 8512– 8525.",
      "citeRegEx" : "McCarthy et al\\.,? 2020",
      "shortCiteRegEx" : "McCarthy et al\\.",
      "year" : 2020
    }, {
      "title" : "DTMT: A novel deep transition architecture for neural machine translation",
      "author" : [ "Fandong Meng", "Jinchao Zhang." ],
      "venue" : "Proceedings of AAAI, pages 224–231.",
      "citeRegEx" : "Meng and Zhang.,? 2019",
      "shortCiteRegEx" : "Meng and Zhang.",
      "year" : 2019
    }, {
      "title" : "Document-level neural machine translation with hierarchical attention networks",
      "author" : [ "Lesly Miculicich", "Dhananjay Ram", "Nikolaos Pappas", "James Henderson." ],
      "venue" : "Proceedings of EMNLP, pages 2947– 2954.",
      "citeRegEx" : "Miculicich et al\\.,? 2018",
      "shortCiteRegEx" : "Miculicich et al\\.",
      "year" : 2018
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Motivating personality-aware machine translation",
      "author" : [ "Shachar Mirkin", "Scott Nowson", "Caroline Brun", "Julien Perez." ],
      "venue" : "Proceedings of EMNLP, pages 1102–1108.",
      "citeRegEx" : "Mirkin et al\\.,? 2015",
      "shortCiteRegEx" : "Mirkin et al\\.",
      "year" : 2015
    }, {
      "title" : "The university of edinburgh-uppsala university’s submission to the wmt 2020 chat translation task",
      "author" : [ "Nikita Moghe", "Christian Hardmeier", "Rachel Bawden." ],
      "venue" : "Proceedings of WMT, pages 471–476.",
      "citeRegEx" : "Moghe et al\\.,? 2020",
      "shortCiteRegEx" : "Moghe et al\\.",
      "year" : 2020
    }, {
      "title" : "Just system for wmt20 chat translation task",
      "author" : [ "Roweida Mohammed", "Mahmoud Al-Ayyoub", "Malak Abdullah." ],
      "venue" : "Proceedings of WMT, pages 477– 480.",
      "citeRegEx" : "Mohammed et al\\.,? 2020",
      "shortCiteRegEx" : "Mohammed et al\\.",
      "year" : 2020
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of ACL, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "author" : [ "Soujanya Poria", "Devamanyu Hazarika", "Navonil Majumder", "Gautam Naik", "Erik Cambria", "Rada Mihalcea." ],
      "venue" : "Proceedings of ACL, pages 527–536.",
      "citeRegEx" : "Poria et al\\.,? 2019",
      "shortCiteRegEx" : "Poria et al\\.",
      "year" : 2019
    }, {
      "title" : "A call for clarity in reporting BLEU scores",
      "author" : [ "Matt Post." ],
      "venue" : "Proceedings of WMT, pages 186–191.",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "Document-aligned JapaneseEnglish conversation parallel corpus",
      "author" : [ "Matı̄ss Rikters", "Ryokan Ri", "Tong Li", "Toshiaki Nakazawa" ],
      "venue" : "In Proceedings of MT,",
      "citeRegEx" : "Rikters et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Rikters et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of ACL, pages 1715– 1725.",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "A study of translation edit rate with targeted human annotation",
      "author" : [ "Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul." ],
      "venue" : "Proceedings of AMTA.",
      "citeRegEx" : "Snover et al\\.,? 2006",
      "shortCiteRegEx" : "Snover et al\\.",
      "year" : 2006
    }, {
      "title" : "Learning structured output representation using deep conditional generative models",
      "author" : [ "Kihyuk Sohn", "Honglak Lee", "Xinchen Yan." ],
      "venue" : "Proceedings of NIPS, pages 3483–3491.",
      "citeRegEx" : "Sohn et al\\.,? 2015",
      "shortCiteRegEx" : "Sohn et al\\.",
      "year" : 2015
    }, {
      "title" : "Variational recurrent neural machine translation",
      "author" : [ "Jinsong Su", "Shan Wu", "Deyi Xiong", "Yaojie Lu", "Xianpei Han", "Biao Zhang." ],
      "venue" : "Proceedings of AAAI.",
      "citeRegEx" : "Su et al\\.,? 2018",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2018
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Proceedings of NIPS, pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "THUMT: An opensource toolkit for neural machine translation",
      "author" : [ "Zhixing Tan", "Jiacheng Zhang", "Xuancheng Huang", "Gang Chen", "Shuo Wang", "Maosong Sun", "Huanbo Luan", "Yang Liu." ],
      "venue" : "Proceedings of AMTA, pages 116–122.",
      "citeRegEx" : "Tan et al\\.,? 2020",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural machine translation with extended context",
      "author" : [ "Jörg Tiedemann", "Yves Scherrer." ],
      "venue" : "Proceedings of the DiscoMT, pages 82–92.",
      "citeRegEx" : "Tiedemann and Scherrer.,? 2017",
      "shortCiteRegEx" : "Tiedemann and Scherrer.",
      "year" : 2017
    }, {
      "title" : "Attaining the unattainable? reassessing claims of human parity in neural machine translation",
      "author" : [ "Antonio Toral", "Sheila Castilho", "Ke Hu", "Andy Way." ],
      "venue" : "Proceedings of WMT, pages 113–123.",
      "citeRegEx" : "Toral et al\\.,? 2018",
      "shortCiteRegEx" : "Toral et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural machine translation with latent semantic of image and text",
      "author" : [ "Joji Toyama", "Masanori Misono", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo." ],
      "venue" : "arXiv preprint arXiv:1611.08459.",
      "citeRegEx" : "Toyama et al\\.,? 2016",
      "shortCiteRegEx" : "Toyama et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to remember translation history with a continuous cache",
      "author" : [ "Zhaopeng Tu", "Yang Liu", "Shuming Shi", "Tong Zhang." ],
      "venue" : "TACL, pages 407–420.",
      "citeRegEx" : "Tu et al\\.,? 2018",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of NIPS, pages 5998– 6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Context-aware monolingual repair for neural machine translation",
      "author" : [ "Elena Voita", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of EMNLPIJCNLP, pages 877–886.",
      "citeRegEx" : "Voita et al\\.,? 2019a",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    }, {
      "title" : "When a good translation is wrong in context: Context-aware machine translation improves on deixis, ellipsis, and lexical cohesion",
      "author" : [ "Elena Voita", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of ACL, pages 1198–1212.",
      "citeRegEx" : "Voita et al\\.,? 2019b",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    }, {
      "title" : "Context-aware neural machine translation learns anaphora resolution",
      "author" : [ "Elena Voita", "Pavel Serdyukov", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of ACL, pages 1264–1274.",
      "citeRegEx" : "Voita et al\\.,? 2018",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2018
    }, {
      "title" : "Semanticsenhanced task-oriented dialogue translation: A case study on hotel booking",
      "author" : [ "Longyue Wang", "Jinhua Du", "Liangyou Li", "Zhaopeng Tu", "Andy Way", "Qun Liu." ],
      "venue" : "Proceedings of IJCNLP, pages 33–36.",
      "citeRegEx" : "Wang et al\\.,? 2017a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Tencent ai lab machine translation systems for wmt20 chat translation task",
      "author" : [ "Longyue Wang", "Zhaopeng Tu", "Xing Wang", "Li Ding", "Liang Ding", "Shuming Shi." ],
      "venue" : "Proceedings of WMT, pages 481–489.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "One model to learn both: Zero pronoun prediction and translation",
      "author" : [ "Longyue Wang", "Zhaopeng Tu", "Xing Wang", "Shuming Shi." ],
      "venue" : "Proceedings of EMNLP-IJCNLP, pages 921–930.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploiting cross-sentence context for neural machine translation",
      "author" : [ "Longyue Wang", "Zhaopeng Tu", "Andy Way", "Qun Liu." ],
      "venue" : "Proceedings of EMNLP, pages 2826–2831.",
      "citeRegEx" : "Wang et al\\.,? 2017b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Automatic construction of discourse corpora for dialogue translation",
      "author" : [ "Longyue Wang", "Xiaojun Zhang", "Zhaopeng Tu", "Andy Way", "Qun Liu." ],
      "venue" : "Proceedings of LREC, pages 2748–2754.",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Autocorrect in the process of translation – multi-task learning improves dialogue machine translation",
      "author" : [ "Tao Wang", "Chengqi Zhao", "Mingxuan Wang", "Lei Li", "Deyi Xiong" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "T-cvae: Transformer-based conditioned variational autoencoder for story completion",
      "author" : [ "Tianming Wang", "Xiaojun Wan." ],
      "venue" : "Proceedings of IJCAI, pages 5233–5239.",
      "citeRegEx" : "Wang and Wan.,? 2019",
      "shortCiteRegEx" : "Wang and Wan.",
      "year" : 2019
    }, {
      "title" : "Guiding variational response generator to exploit persona",
      "author" : [ "Bowen Wu", "MengYuan Li", "Zongsheng Wang", "Yifu Chen", "Derek F. Wong", "Qihang Feng", "Junhong Huang", "Baoxun Wang." ],
      "venue" : "Proceedings of ACL, pages 53–65.",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Modeling coherence for discourse neural machine translation",
      "author" : [ "Hao Xiong", "Zhongjun He", "Hua Wu", "Haifeng Wang." ],
      "venue" : "Proceedings of AAAI, pages 7338– 7345.",
      "citeRegEx" : "Xiong et al\\.,? 2019",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi-unit transformers for neural machine translation",
      "author" : [ "Jianhao Yan", "Fandong Meng", "Jie Zhou." ],
      "venue" : "Proceedings of EMNLP, pages 1047–1059, Online.",
      "citeRegEx" : "Yan et al\\.,? 2020",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2020
    }, {
      "title" : "Enhancing topic-to-essay generation with external commonsense knowledge",
      "author" : [ "Pengcheng Yang", "Lei Li", "Fuli Luo", "Tianyu Liu", "Xu Sun." ],
      "venue" : "Proceedings of ACL, pages 2002–2012.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Variational neural machine translation",
      "author" : [ "Biao Zhang", "Deyi Xiong", "Jinsong Su", "Hong Duan", "Min Zhang." ],
      "venue" : "Proceedings of EMNLP, pages 521–530.",
      "citeRegEx" : "Zhang et al\\.,? 2016",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "Improving the transformer translation model with document-level context",
      "author" : [ "Jiacheng Zhang", "Huanbo Luan", "Maosong Sun", "Feifei Zhai", "Jingfang Xu", "Min Zhang", "Yang Liu." ],
      "venue" : "Proceedings of EMNLP, pages 533–542.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Automatically annotate tv series subtitles for dialogue corpus construction",
      "author" : [ "L. Zhang", "Q. Zhou." ],
      "venue" : "APSIPA ASC, pages 1029–1035.",
      "citeRegEx" : "Zhang and Zhou.,? 2019",
      "shortCiteRegEx" : "Zhang and Zhou.",
      "year" : 2019
    }, {
      "title" : "Bridging the gap between training and inference for neural machine translation",
      "author" : [ "Wen Zhang", "Yang Feng", "Fandong Meng", "Di You", "Qun Liu." ],
      "venue" : "Proceedings of ACL, pages 4334–4343, Florence, Italy.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning discourse-level diversity for neural dialog models using conditional variational autoencoders",
      "author" : [ "Tiancheng Zhao", "Ran Zhao", "Maxine Eskenazi." ],
      "venue" : "Proceedings of ACL, pages 654–664.",
      "citeRegEx" : "Zhao et al\\.,? 2017",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 41,
      "context" : "Although sentence-level Neural Machine Translation (NMT) (Sutskever et al., 2014; Vaswani et al., 2017; Meng and Zhang, 2019; Hassan et al., 2018; Yan et al., 2020; Zhang et al., 2019) has achieved promising progress, it still faces challenges in accurately translating conversational text due to abandoning the dialogue history, which leads to role-irrelevant, incoherent and inconsistent translations (Mirkin et al.",
      "startOffset" : 57,
      "endOffset" : 184
    }, {
      "referenceID" : 47,
      "context" : "Although sentence-level Neural Machine Translation (NMT) (Sutskever et al., 2014; Vaswani et al., 2017; Meng and Zhang, 2019; Hassan et al., 2018; Yan et al., 2020; Zhang et al., 2019) has achieved promising progress, it still faces challenges in accurately translating conversational text due to abandoning the dialogue history, which leads to role-irrelevant, incoherent and inconsistent translations (Mirkin et al.",
      "startOffset" : 57,
      "endOffset" : 184
    }, {
      "referenceID" : 27,
      "context" : "Although sentence-level Neural Machine Translation (NMT) (Sutskever et al., 2014; Vaswani et al., 2017; Meng and Zhang, 2019; Hassan et al., 2018; Yan et al., 2020; Zhang et al., 2019) has achieved promising progress, it still faces challenges in accurately translating conversational text due to abandoning the dialogue history, which leads to role-irrelevant, incoherent and inconsistent translations (Mirkin et al.",
      "startOffset" : 57,
      "endOffset" : 184
    }, {
      "referenceID" : 60,
      "context" : "Although sentence-level Neural Machine Translation (NMT) (Sutskever et al., 2014; Vaswani et al., 2017; Meng and Zhang, 2019; Hassan et al., 2018; Yan et al., 2020; Zhang et al., 2019) has achieved promising progress, it still faces challenges in accurately translating conversational text due to abandoning the dialogue history, which leads to role-irrelevant, incoherent and inconsistent translations (Mirkin et al.",
      "startOffset" : 57,
      "endOffset" : 184
    }, {
      "referenceID" : 65,
      "context" : "Although sentence-level Neural Machine Translation (NMT) (Sutskever et al., 2014; Vaswani et al., 2017; Meng and Zhang, 2019; Hassan et al., 2018; Yan et al., 2020; Zhang et al., 2019) has achieved promising progress, it still faces challenges in accurately translating conversational text due to abandoning the dialogue history, which leads to role-irrelevant, incoherent and inconsistent translations (Mirkin et al.",
      "startOffset" : 57,
      "endOffset" : 184
    }, {
      "referenceID" : 30,
      "context" : ", 2019) has achieved promising progress, it still faces challenges in accurately translating conversational text due to abandoning the dialogue history, which leads to role-irrelevant, incoherent and inconsistent translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018).",
      "startOffset" : 226,
      "endOffset" : 308
    }, {
      "referenceID" : 51,
      "context" : ", 2019) has achieved promising progress, it still faces challenges in accurately translating conversational text due to abandoning the dialogue history, which leads to role-irrelevant, incoherent and inconsistent translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018).",
      "startOffset" : 226,
      "endOffset" : 308
    }, {
      "referenceID" : 19,
      "context" : ", 2019) has achieved promising progress, it still faces challenges in accurately translating conversational text due to abandoning the dialogue history, which leads to role-irrelevant, incoherent and inconsistent translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018).",
      "startOffset" : 226,
      "endOffset" : 308
    }, {
      "referenceID" : 44,
      "context" : ", 2019) has achieved promising progress, it still faces challenges in accurately translating conversational text due to abandoning the dialogue history, which leads to role-irrelevant, incoherent and inconsistent translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018).",
      "startOffset" : 226,
      "endOffset" : 308
    }, {
      "referenceID" : 43,
      "context" : "Further, context-aware NMT (Tiedemann and Scherrer, 2017; Voita et al., 2018, 2019a,b; Wang et al., 2019; Maruf and Haffari, 2018; Maruf et al., 2019; Ma et al., 2020) can be directly applied to chat translation through incorporating the dialogue history but cannot obtain satisfactory results in this sce-",
      "startOffset" : 27,
      "endOffset" : 167
    }, {
      "referenceID" : 53,
      "context" : "Further, context-aware NMT (Tiedemann and Scherrer, 2017; Voita et al., 2018, 2019a,b; Wang et al., 2019; Maruf and Haffari, 2018; Maruf et al., 2019; Ma et al., 2020) can be directly applied to chat translation through incorporating the dialogue history but cannot obtain satisfactory results in this sce-",
      "startOffset" : 27,
      "endOffset" : 167
    }, {
      "referenceID" : 23,
      "context" : "Further, context-aware NMT (Tiedemann and Scherrer, 2017; Voita et al., 2018, 2019a,b; Wang et al., 2019; Maruf and Haffari, 2018; Maruf et al., 2019; Ma et al., 2020) can be directly applied to chat translation through incorporating the dialogue history but cannot obtain satisfactory results in this sce-",
      "startOffset" : 27,
      "endOffset" : 167
    }, {
      "referenceID" : 25,
      "context" : "Further, context-aware NMT (Tiedemann and Scherrer, 2017; Voita et al., 2018, 2019a,b; Wang et al., 2019; Maruf and Haffari, 2018; Maruf et al., 2019; Ma et al., 2020) can be directly applied to chat translation through incorporating the dialogue history but cannot obtain satisfactory results in this sce-",
      "startOffset" : 27,
      "endOffset" : 167
    }, {
      "referenceID" : 22,
      "context" : "Further, context-aware NMT (Tiedemann and Scherrer, 2017; Voita et al., 2018, 2019a,b; Wang et al., 2019; Maruf and Haffari, 2018; Maruf et al., 2019; Ma et al., 2020) can be directly applied to chat translation through incorporating the dialogue history but cannot obtain satisfactory results in this sce-",
      "startOffset" : 27,
      "endOffset" : 167
    }, {
      "referenceID" : 58,
      "context" : "For a conversation, its dialogue history contains rich role preference information such as emotion, style, and humor, which is beneficial to rolerelevant utterance generation (Wu et al., 2020).",
      "startOffset" : 175,
      "endOffset" : 192
    }, {
      "referenceID" : 31,
      "context" : ", joy) because of his/her preference, and preserving the same preference information across languages can help raise emotional resonance and mutual understanding (Moghe et al., 2020).",
      "startOffset" : 162,
      "endOffset" : 182
    }, {
      "referenceID" : 7,
      "context" : "And it is challenging to implicitly capture these properties by just incorporating the complex dialogue history into encoders due to lacking the relevant information guidance (Farajian et al., 2020).",
      "startOffset" : 175,
      "endOffset" : 198
    }, {
      "referenceID" : 39,
      "context" : "On the other hand, the Conditional Variational Auto-Encoder (CVAE) (Sohn et al., 2015) has shown its superiority in learning distributions of data properties, which is often utilized to model the diversity (Zhao et al.",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 66,
      "context" : ", 2015) has shown its superiority in learning distributions of data properties, which is often utilized to model the diversity (Zhao et al., 2017), coherence (Wang and Wan, 2019) and users’ personalities (Bak and Oh, 2019), etc.",
      "startOffset" : 127,
      "endOffset" : 146
    }, {
      "referenceID" : 57,
      "context" : ", 2017), coherence (Wang and Wan, 2019) and users’ personalities (Bak and Oh, 2019), etc.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : ", 2017), coherence (Wang and Wan, 2019) and users’ personalities (Bak and Oh, 2019), etc.",
      "startOffset" : 65,
      "endOffset" : 83
    }, {
      "referenceID" : 7,
      "context" : "We conduct experiments on WMT20 Chat Translation dataset: BConTrasT (En⇔De3) (Farajian et al., 2020) and a self-collected dialogue corpus: BMELD (En⇔Ch).",
      "startOffset" : 77,
      "endOffset" : 100
    }, {
      "referenceID" : 33,
      "context" : "Results demonstrate that our model achieves consistent improvements in four directions in terms of BLEU (Papineni et al., 2002) and TER (Snover et al.",
      "startOffset" : 104,
      "endOffset" : 127
    }, {
      "referenceID" : 38,
      "context" : ", 2002) and TER (Snover et al., 2006), showing its effectiveness and generalizability.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 22,
      "context" : "Given a source context DX={Xi}i=1 and a target context DY={Yi}i=1 with J aligned sentence pairs (Xi, Yi), the context-aware NMT (Ma et al., 2020) is formalized as:",
      "startOffset" : 128,
      "endOffset" : 145
    }, {
      "referenceID" : 62,
      "context" : "The variational NMT model (Zhang et al., 2016) is the combination of CVAE (Sohn et al.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 39,
      "context" : ", 2016) is the combination of CVAE (Sohn et al., 2015) and NMT.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 39,
      "context" : "+ Eqφ(z|X,Y )[log pθ(Y |z, X)] ≤ log p(Y |X), where φ are parameters of the posterior network and KL(·) indicates Kullback–Leibler divergence between two distributions produced by prior networks and posterior networks (Sohn et al., 2015; Kingma and Welling, 2013).",
      "startOffset" : 218,
      "endOffset" : 263
    }, {
      "referenceID" : 15,
      "context" : "+ Eqφ(z|X,Y )[log pθ(Y |z, X)] ≤ log p(Y |X), where φ are parameters of the posterior network and KL(·) indicates Kullback–Leibler divergence between two distributions produced by prior networks and posterior networks (Sohn et al., 2015; Kingma and Welling, 2013).",
      "startOffset" : 218,
      "endOffset" : 263
    }, {
      "referenceID" : 24,
      "context" : "Following (Maruf et al., 2018), we define paired bilingual utterances (Xi, Yi) as a turn in Fig.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 47,
      "context" : "Apart from the conventional word embeddings WE and position embeddings PE (Vaswani et al., 2017), we also introduce role embeddings RE and dialogue turn embeddings TE to identify different utterances.",
      "startOffset" : 74,
      "endOffset" : 96
    }, {
      "referenceID" : 6,
      "context" : "For each of {Crole X , CX , CY }, we add ‘[cls]’ tag at the head of it and use ‘[sep]’ tag to separate its utterances (Devlin et al., 2019), and then get its embeddings via Eq.",
      "startOffset" : 118,
      "endOffset" : 139
    }, {
      "referenceID" : 47,
      "context" : "2 Encoder The Transformer encoder consists of Ne stacked layers and each layer includes two sub-layers:5 a multi-head self-attention (SelfAtt) sub-layer and a position-wise feed-forward network (FFN) sublayer (Vaswani et al., 2017): se = SelfAtt(h `−1 e ) + h `−1 e , h `−1 e ∈ Rm×d, he = FFN(s ` e) + s ` e, {he, se} ∈ Rm×d, (5)We omit the layer normalization for simplicity, and you may refer to (Vaswani et al.",
      "startOffset" : 209,
      "endOffset" : 231
    }, {
      "referenceID" : 47,
      "context" : ", 2017): se = SelfAtt(h `−1 e ) + h `−1 e , h `−1 e ∈ Rm×d, he = FFN(s ` e) + s ` e, {he, se} ∈ Rm×d, (5)We omit the layer normalization for simplicity, and you may refer to (Vaswani et al., 2017) for more details.",
      "startOffset" : 174,
      "endOffset" : 196
    }, {
      "referenceID" : 62,
      "context" : ", hX= 1 m ∑m i=1(M X i h Ne,X e,i ), hX ∈ Rd, where MX ∈ Rm denotes the mask matrix, whose value is either 1 or 0 indicating whether the token is padded (Zhang et al., 2016).",
      "startOffset" : 153,
      "endOffset" : 173
    }, {
      "referenceID" : 22,
      "context" : "3, we follow (Ma et al., 2020) and share the first encoder layer to obtain the context representation.",
      "startOffset" : 13,
      "endOffset" : 30
    }, {
      "referenceID" : 57,
      "context" : "Inspired by (Wang and Wan, 2019), we use isotropic Gaussian distribution as the prior distribution of zrole: pθ(zrole|Xu, Crole X ) ∼ N (μrole,σ(2) roleI), where I",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 39,
      "context" : "Therefore, the prior network can learn a role-tailored distribution by approaching the posterior network via KL divergence (Sohn et al., 2015): qφ(zrole|Xu, Crole X , Yu) ∼ N (μrole,σ roleI) and {μrole, σ′ role} are calculated as: μrole = MLP role φ (hX ;h ctx role;hY ), σ′ role = Softplus(MLP role φ (hX ;h ctx role;hY )).",
      "startOffset" : 123,
      "endOffset" : 142
    }, {
      "referenceID" : 63,
      "context" : "We apply a two-stage training strategy (Zhang et al., 2018; Ma et al., 2020).",
      "startOffset" : 39,
      "endOffset" : 76
    }, {
      "referenceID" : 22,
      "context" : "We apply a two-stage training strategy (Zhang et al., 2018; Ma et al., 2020).",
      "startOffset" : 39,
      "endOffset" : 76
    }, {
      "referenceID" : 15,
      "context" : "5716 We use the reparameterization trick (Kingma and Welling, 2013) to estimate the gradients of the prior and recognition networks (Zhao et al.",
      "startOffset" : 41,
      "endOffset" : 67
    }, {
      "referenceID" : 66,
      "context" : "5716 We use the reparameterization trick (Kingma and Welling, 2013) to estimate the gradients of the prior and recognition networks (Zhao et al., 2017).",
      "startOffset" : 132,
      "endOffset" : 151
    }, {
      "referenceID" : 7,
      "context" : ", firstly training on a large-scale sentence-level NMT corpus (WMT206) and then fine-tuning on chat translation corpus (BConTrasT (Farajian et al., 2020)7 and BMELD).",
      "startOffset" : 130,
      "endOffset" : 153
    }, {
      "referenceID" : 7,
      "context" : "The dataset8 is first provided by WMT 2020 Chat Translation Task (Farajian et al., 2020), which is translated from English into German and is based on the monolingual Taskmaster-1 corpus (Byrne et al.",
      "startOffset" : 65,
      "endOffset" : 88
    }, {
      "referenceID" : 34,
      "context" : "Similarly, based on the dialogue dataset in the MELD (originally in English) (Poria et al., 2019),10 we firstly crawled the corresponding Chinese translations from this11 and then manually post-edited them according to the dialogue history by native Chinese speakers, who are postgraduate students majoring in English.",
      "startOffset" : 77,
      "endOffset" : 97
    }, {
      "referenceID" : 7,
      "context" : "Finally, following (Farajian et al., 2020), we assume 50% speakers as Chinese speakers to keep data balance for Ch⇒En translations and build the bilingual MELD (BMELD).",
      "startOffset" : 19,
      "endOffset" : 42
    }, {
      "referenceID" : 35,
      "context" : "For fair comparison, we use the SacreBLEU13 (Post, 2018) and v0.",
      "startOffset" : 44,
      "endOffset" : 56
    }, {
      "referenceID" : 16,
      "context" : ", 2006) (the lower the better) with the statistical significance test (Koehn, 2004).",
      "startOffset" : 70,
      "endOffset" : 83
    }, {
      "referenceID" : 7,
      "context" : "For En⇔De, we report case-sensitive score following the WMT20 chat task (Farajian et al., 2020).",
      "startOffset" : 72,
      "endOffset" : 95
    }, {
      "referenceID" : 47,
      "context" : "For all experiments, we follow the TransformerBase and Transformer-Big settings illustrated in (Vaswani et al., 2017).",
      "startOffset" : 95,
      "endOffset" : 117
    }, {
      "referenceID" : 42,
      "context" : "All our Transformer models containNe = 6 encoder layers and Nd = 6 decoder layers and all models are trained using THUMT (Tan et al., 2020) framework.",
      "startOffset" : 121,
      "endOffset" : 139
    }, {
      "referenceID" : 47,
      "context" : "Transformer (Vaswani et al., 2017): the de-facto NMT model that does not fine-tune on chat translation data.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 22,
      "context" : "DocTransformer+FT (Ma et al., 2020): a stateof-the-art document-level NMT model based on Transformer sharing the first encoder layer to incorporate the bilingual dialogue history.",
      "startOffset" : 18,
      "endOffset" : 35
    }, {
      "referenceID" : 24,
      "context" : "Dia-Transformer+FT (Maruf et al., 2018): using an additional RNN-based (Hochreiter and Schmidhuber, 1997) encoder to incorporate the mixed-language dialogue history, where we re-implement it based on Transformer and use another Transformer layer to introduce context.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 11,
      "context" : ", 2018): using an additional RNN-based (Hochreiter and Schmidhuber, 1997) encoder to incorporate the mixed-language dialogue history, where we re-implement it based on Transformer and use another Transformer layer to introduce context.",
      "startOffset" : 39,
      "endOffset" : 73
    }, {
      "referenceID" : 18,
      "context" : "Following (Lapata and Barzilay, 2005; Xiong et al., 2019), we measure dialogue coherence as sentence similarity.",
      "startOffset" : 10,
      "endOffset" : 57
    }, {
      "referenceID" : 59,
      "context" : "Following (Lapata and Barzilay, 2005; Xiong et al., 2019), we measure dialogue coherence as sentence similarity.",
      "startOffset" : 10,
      "endOffset" : 57
    }, {
      "referenceID" : 29,
      "context" : "We use Word2Vec14 (Mikolov et al., 2013) to learn the distributed vectors of words by training on the monolingual dialogue dataset: Taskmaster1 (Byrne et al.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 1,
      "context" : "Inspired by (Bao et al., 2020; Farajian et al., 2020), we use four criteria for human evaluation: (1) Preference measures whether the translation preserves the role preference information; (2) Coherence denotes whether the translation is semantically coherent with the dialogue history; (3) Consistency measures whether the lexical choice of translation is consistent with the preceding utterances; (4) Fluency measures whether the translation is logically reasonable and grammatically correct.",
      "startOffset" : 12,
      "endOffset" : 53
    }, {
      "referenceID" : 7,
      "context" : "Inspired by (Bao et al., 2020; Farajian et al., 2020), we use four criteria for human evaluation: (1) Preference measures whether the translation preserves the role preference information; (2) Coherence denotes whether the translation is semantically coherent with the dialogue history; (3) Consistency measures whether the lexical choice of translation is consistent with the preceding utterances; (4) Fluency measures whether the translation is logically reasonable and grammatically correct.",
      "startOffset" : 12,
      "endOffset" : 53
    }, {
      "referenceID" : 16,
      "context" : "5 shows that our CPCC effectively alleviates the problem of role-irrelevant, incoherent and inconsistent translations compared with other models (significance test (Koehn, 2004), p < 0.",
      "startOffset" : 164,
      "endOffset" : 177
    }, {
      "referenceID" : 8,
      "context" : "485 calculated by the Fleiss’ kappa (Fleiss and Cohen, 1973), for preference, coherence, consistency and fluency, respectively, indicating “Moderate Agreement” for all four criteria.",
      "startOffset" : 36,
      "endOffset" : 60
    }, {
      "referenceID" : 7,
      "context" : "It only involves several researches due to the lack of human-annotated publicly available data (Farajian et al., 2020).",
      "startOffset" : 95,
      "endOffset" : 118
    }, {
      "referenceID" : 55,
      "context" : "5719 existing work (Wang et al., 2016; Maruf et al., 2018; Zhang and Zhou, 2019; Rikters et al., 2020) mainly pays attention to designing methods to automatically construct the subtitles corpus, which may contain noisy bilingual utterances.",
      "startOffset" : 19,
      "endOffset" : 102
    }, {
      "referenceID" : 24,
      "context" : "5719 existing work (Wang et al., 2016; Maruf et al., 2018; Zhang and Zhou, 2019; Rikters et al., 2020) mainly pays attention to designing methods to automatically construct the subtitles corpus, which may contain noisy bilingual utterances.",
      "startOffset" : 19,
      "endOffset" : 102
    }, {
      "referenceID" : 64,
      "context" : "5719 existing work (Wang et al., 2016; Maruf et al., 2018; Zhang and Zhou, 2019; Rikters et al., 2020) mainly pays attention to designing methods to automatically construct the subtitles corpus, which may contain noisy bilingual utterances.",
      "startOffset" : 19,
      "endOffset" : 102
    }, {
      "referenceID" : 36,
      "context" : "5719 existing work (Wang et al., 2016; Maruf et al., 2018; Zhang and Zhou, 2019; Rikters et al., 2020) mainly pays attention to designing methods to automatically construct the subtitles corpus, which may contain noisy bilingual utterances.",
      "startOffset" : 19,
      "endOffset" : 102
    }, {
      "referenceID" : 3,
      "context" : "(2020) organize the WMT20 chat translation task and first provide a human postedited corpus, where some teams investigate the effect of dialogue history and finally ensemble their models for higher ranks (Berard et al., 2020; Mohammed et al., 2020; Wang et al., 2020; Bao et al., 2020; Moghe et al., 2020).",
      "startOffset" : 204,
      "endOffset" : 305
    }, {
      "referenceID" : 32,
      "context" : "(2020) organize the WMT20 chat translation task and first provide a human postedited corpus, where some teams investigate the effect of dialogue history and finally ensemble their models for higher ranks (Berard et al., 2020; Mohammed et al., 2020; Wang et al., 2020; Bao et al., 2020; Moghe et al., 2020).",
      "startOffset" : 204,
      "endOffset" : 305
    }, {
      "referenceID" : 52,
      "context" : "(2020) organize the WMT20 chat translation task and first provide a human postedited corpus, where some teams investigate the effect of dialogue history and finally ensemble their models for higher ranks (Berard et al., 2020; Mohammed et al., 2020; Wang et al., 2020; Bao et al., 2020; Moghe et al., 2020).",
      "startOffset" : 204,
      "endOffset" : 305
    }, {
      "referenceID" : 1,
      "context" : "(2020) organize the WMT20 chat translation task and first provide a human postedited corpus, where some teams investigate the effect of dialogue history and finally ensemble their models for higher ranks (Berard et al., 2020; Mohammed et al., 2020; Wang et al., 2020; Bao et al., 2020; Moghe et al., 2020).",
      "startOffset" : 204,
      "endOffset" : 305
    }, {
      "referenceID" : 31,
      "context" : "(2020) organize the WMT20 chat translation task and first provide a human postedited corpus, where some teams investigate the effect of dialogue history and finally ensemble their models for higher ranks (Berard et al., 2020; Mohammed et al., 2020; Wang et al., 2020; Bao et al., 2020; Moghe et al., 2020).",
      "startOffset" : 204,
      "endOffset" : 305
    }, {
      "referenceID" : 9,
      "context" : "Chat NMT can be viewed as a special case of context-aware NMT, which has attracted many researchers (Gong et al., 2011; Jean et al., 2017; Wang et al., 2017b; Bawden et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Tu et al., 2018; Yang et al., 2019; Kang et al., 2020; Li et al., 2020; Ma et al., 2020) to extend the encoder or decoder for exploring the context impact on translation quality.",
      "startOffset" : 100,
      "endOffset" : 313
    }, {
      "referenceID" : 12,
      "context" : "Chat NMT can be viewed as a special case of context-aware NMT, which has attracted many researchers (Gong et al., 2011; Jean et al., 2017; Wang et al., 2017b; Bawden et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Tu et al., 2018; Yang et al., 2019; Kang et al., 2020; Li et al., 2020; Ma et al., 2020) to extend the encoder or decoder for exploring the context impact on translation quality.",
      "startOffset" : 100,
      "endOffset" : 313
    }, {
      "referenceID" : 54,
      "context" : "Chat NMT can be viewed as a special case of context-aware NMT, which has attracted many researchers (Gong et al., 2011; Jean et al., 2017; Wang et al., 2017b; Bawden et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Tu et al., 2018; Yang et al., 2019; Kang et al., 2020; Li et al., 2020; Ma et al., 2020) to extend the encoder or decoder for exploring the context impact on translation quality.",
      "startOffset" : 100,
      "endOffset" : 313
    }, {
      "referenceID" : 2,
      "context" : "Chat NMT can be viewed as a special case of context-aware NMT, which has attracted many researchers (Gong et al., 2011; Jean et al., 2017; Wang et al., 2017b; Bawden et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Tu et al., 2018; Yang et al., 2019; Kang et al., 2020; Li et al., 2020; Ma et al., 2020) to extend the encoder or decoder for exploring the context impact on translation quality.",
      "startOffset" : 100,
      "endOffset" : 313
    }, {
      "referenceID" : 28,
      "context" : "Chat NMT can be viewed as a special case of context-aware NMT, which has attracted many researchers (Gong et al., 2011; Jean et al., 2017; Wang et al., 2017b; Bawden et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Tu et al., 2018; Yang et al., 2019; Kang et al., 2020; Li et al., 2020; Ma et al., 2020) to extend the encoder or decoder for exploring the context impact on translation quality.",
      "startOffset" : 100,
      "endOffset" : 313
    }, {
      "referenceID" : 17,
      "context" : "Chat NMT can be viewed as a special case of context-aware NMT, which has attracted many researchers (Gong et al., 2011; Jean et al., 2017; Wang et al., 2017b; Bawden et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Tu et al., 2018; Yang et al., 2019; Kang et al., 2020; Li et al., 2020; Ma et al., 2020) to extend the encoder or decoder for exploring the context impact on translation quality.",
      "startOffset" : 100,
      "endOffset" : 313
    }, {
      "referenceID" : 46,
      "context" : "Chat NMT can be viewed as a special case of context-aware NMT, which has attracted many researchers (Gong et al., 2011; Jean et al., 2017; Wang et al., 2017b; Bawden et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Tu et al., 2018; Yang et al., 2019; Kang et al., 2020; Li et al., 2020; Ma et al., 2020) to extend the encoder or decoder for exploring the context impact on translation quality.",
      "startOffset" : 100,
      "endOffset" : 313
    }, {
      "referenceID" : 61,
      "context" : "Chat NMT can be viewed as a special case of context-aware NMT, which has attracted many researchers (Gong et al., 2011; Jean et al., 2017; Wang et al., 2017b; Bawden et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Tu et al., 2018; Yang et al., 2019; Kang et al., 2020; Li et al., 2020; Ma et al., 2020) to extend the encoder or decoder for exploring the context impact on translation quality.",
      "startOffset" : 100,
      "endOffset" : 313
    }, {
      "referenceID" : 13,
      "context" : "Chat NMT can be viewed as a special case of context-aware NMT, which has attracted many researchers (Gong et al., 2011; Jean et al., 2017; Wang et al., 2017b; Bawden et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Tu et al., 2018; Yang et al., 2019; Kang et al., 2020; Li et al., 2020; Ma et al., 2020) to extend the encoder or decoder for exploring the context impact on translation quality.",
      "startOffset" : 100,
      "endOffset" : 313
    }, {
      "referenceID" : 20,
      "context" : "Chat NMT can be viewed as a special case of context-aware NMT, which has attracted many researchers (Gong et al., 2011; Jean et al., 2017; Wang et al., 2017b; Bawden et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Tu et al., 2018; Yang et al., 2019; Kang et al., 2020; Li et al., 2020; Ma et al., 2020) to extend the encoder or decoder for exploring the context impact on translation quality.",
      "startOffset" : 100,
      "endOffset" : 313
    }, {
      "referenceID" : 22,
      "context" : "Chat NMT can be viewed as a special case of context-aware NMT, which has attracted many researchers (Gong et al., 2011; Jean et al., 2017; Wang et al., 2017b; Bawden et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Tu et al., 2018; Yang et al., 2019; Kang et al., 2020; Li et al., 2020; Ma et al., 2020) to extend the encoder or decoder for exploring the context impact on translation quality.",
      "startOffset" : 100,
      "endOffset" : 313
    }, {
      "referenceID" : 31,
      "context" : "Although these models can be directly applied to chat translation, they cannot explicitly capture the bilingual conversational characteristics and thus lead to unsatisfactory translations (Moghe et al., 2020).",
      "startOffset" : 188,
      "endOffset" : 208
    }, {
      "referenceID" : 39,
      "context" : "CVAE has verified its superiority in many fields (Sohn et al., 2015).",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 45,
      "context" : "Besides, some studies use CVAE to model the correlations between image and text for multimodal NMT (Toyama et al., 2016; Calixto et al., 2019).",
      "startOffset" : 99,
      "endOffset" : 142
    }, {
      "referenceID" : 5,
      "context" : "Besides, some studies use CVAE to model the correlations between image and text for multimodal NMT (Toyama et al., 2016; Calixto et al., 2019).",
      "startOffset" : 99,
      "endOffset" : 142
    }, {
      "referenceID" : 21,
      "context" : "In the future, we would like to explore the effect of multimodality and emotion on chat translation, which has been well studied in dialogue field (Liang et al., 2020).",
      "startOffset" : 147,
      "endOffset" : 167
    } ],
    "year" : 2021,
    "abstractText" : "Neural chat translation aims to translate bilingual conversational text, which has a broad application in international exchanges and cooperation. Despite the impressive performance of sentence-level and context-aware Neural Machine Translation (NMT), there still remain challenges to translate bilingual conversational text due to its inherent characteristics such as role preference, dialogue coherence, and translation consistency. In this paper, we aim to promote the translation quality of conversational text by modeling the above properties. Specifically, we design three latent variational modules to learn the distributions of bilingual conversational characteristics. Through sampling from these learned distributions, the latent variables, tailored for role preference, dialogue coherence, and translation consistency, are incorporated into the NMT model for better translation. We evaluate our approach on the benchmark dataset BConTrasT (English⇔German) and a self-collected bilingual dialogue corpus, named BMELD (English⇔Chinese). Extensive experiments show that our approach notably boosts the performance over strong baselines by a large margin and significantly surpasses some state-of-the-art context-aware NMT models in terms of BLEU and TER. Additionally, we make the BMELD dataset publicly available for the research community.1",
    "creator" : "LaTeX with hyperref"
  }
}