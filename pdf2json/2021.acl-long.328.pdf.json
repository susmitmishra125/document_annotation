{
  "name" : "2021.acl-long.328.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Improving Speech Translation by Understanding and Learning from the Auxiliary Text Translation Task",
    "authors" : [ "Yun Tang", "Juan Pino", "Xian Li", "Changhan Wang", "Dmitriy Genzel" ],
    "emails" : [ "yuntang@fb.com", "juancarabina@fb.com", "xianl@fb.com", "changhan@fb.com", "dgenzel@fb.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4252–4261\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4252"
    }, {
      "heading" : "1 Introduction",
      "text" : "End-to-end methods have achieved significant progress in speech to text translation (ST) and even surpassed the traditional pipeline-based methods\nin some applications (Niehues et al., 2019; Salesky and Black, 2020). However, the success of endto-end methods relies on large amounts of training data, which is quite expensive to obtain and relatively small in practice. Building ST systems from pretrained models with multitask learning (MTL) is widely used to overcome the limited training data issue (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Bahar et al., 2019; Indurthi et al., 2020; Wang et al., 2020b; Li et al., 2020). Nevertheless, little prior work has been devoted to understanding the interactions between different tasks. Standley et al. (2020) conduct an empirical study on computer vision tasks for MTL. They find many “assumptions” for MTL may not be held for specific applications. For example, “similar” tasks do not necessarily train better together.\nIn this study, we focus on training the ST model along with an auxiliary text to text machine translation (MT) task. We are interested in the task interactions with different modalities and in improving the primary ST task with the help from the auxiliary MT task. The model is initialized with pretrained modules from automatic speech recognition (ASR) and MT. Two types of analysis are conducted on the fine-tuned multitask learned models. The first focuses on the model variation by comparing fine-tuned models with pretrained models for different tasks. The second aims to measure internal representation differences due to different modalities. The analysis leads to three main findings. First, the analysis confirms that MTL tends to generate similar model representations for different input modalities and preserves more information from the pretrained MT modules. Second, we do not observe significant negative transfer effect from the MT task to the corresponding ST task. Sharing more parameters is helpful to transfer knowledge to the primary ST task. Finally, the top layers in the ST decoder are more critical to the translation\nperformance and they are also more sensitive to the modality difference. The model representations from different modalities demonstrate larger difference for the top layers in our analysis.\nInspired by these findings, we propose three techniques to enhance the performance of the primary ST task. First, we propose to maximize parameter sharing between the ST and MT tasks, i.e. the entire decoder and the top encoder layers. Those shared parameters are initialized with the corresponding MT models. Second, a cross-attentive regularization is introduced for the encoders. It minimizes the L2 distance between two reconstructed encoder output sequences and encourages the encoder outputs from different modalities to be closer to each other. Finally, an online knowledge distillation learning is introduced for MTL in order to enhance knowledge transfer from the MT to the ST task.\nOur contributions are summarized as follows:\n1. A detailed analysis is conducted on the interaction between the primary ST task and the auxiliary MT task.\n2. A parameter sharing and initialization strategy are proposed to encourage information sharing between tasks.\n3. Cross-attentive regularization and online knowledge distillation are proposed to reduce the model representation difference between different modalities and enhance the knowledge transfer from the MT task to the ST task.\n4. Our system achieves state of the art results on the MUST-C English-German (EN-DE), English-French (EN-FR) and English-Spanish (EN-ES) language pairs, with 2 or more BLEU gains over strong baselines."
    }, {
      "heading" : "2 Related Work",
      "text" : "Multitask learning aims to improve generalization by leveraging domain-specific information contained in the training signals of related tasks (Vandenhende et al., 2020). Compared with single task, MTL has many advantages, such as the potential to improve performance by sharing complementary information or acting as a regularizer. Many previous works focus on learning a good model for all tasks. Chen et al. (2018) study the gradients from different tasks and conduct task dependent gradient normalization to encourage different tasks to learn at similar speed. Maninis et al.\n(2019); Liu et al. (2019a); Pfeiffer et al. (2020) introduce task-dependent components to enhance individual task performance.\nWeiss et al. (2017) explore different multitask training strategies for ST, and they find the oneto-many strategy, in which an encoder is shared between the ST and ASR tasks, is more effective. Anastasopoulos and Chiang (2018) further extend it to a triangle structure by concatenating ASR and ST models. Bahar et al. (2019) compare different multitask strategies for the ST task, and they confirm many-to-one strategy, in which MT and ST are trained together and the decoder is shared between two tasks, is effective if extra bitext data is used. In this work, we carefully study the relation between co-trained tasks in the many-to-one strategy, and the analysis results guide us to propose three techniques to learn more from the auxiliary MT task and enhance the ST performance further.\nModel analysis Chatterji et al. (2020) propose criticality analysis to measure the importance of different modules from the trained model. Parameters\nin the selected module or layer are partially rolled back to the initial values, and the module criticality or importance is measured by the performance drop after modification. Larger performance drops indicate a more critical module. Inspired by their work, we extend it to the analysis on the jointly trained models with different pretrained modules and schemes. Raghu et al. (2017); Morcos et al. (2018) propose to employ canonical correlation to measure the similarity between different models given the same input. We extend their work to study a model with inputs from different modalities."
    }, {
      "heading" : "3 Methods",
      "text" : "The proposed ST system is co-trained with the MT task as depicted in Figure 1. The modules in the primary ST task are connected with dark gray lines and the auxiliary MT task is illustrated with light gray lines. The parameters in the blue modules are shared between the two tasks. During inference with speech input, only modules related to the ST task are used.\nThe model has two encoders, a text encoder and a speech encoder, to take text and speech input respectively. The decoder is shared between the two tasks. To encourage knowledge sharing between the two tasks, the top encoder layers are also shared. The parameters of the shared modules are initialized with a pretrained MT model. A novel crossattentive regularization is proposed to reduce the distance between encoder outputs from different input modalities. We also introduce a novel online knowledge distillation method where the output from the auxiliary MT task is used to guide the ST model training. The cross-attentive regularization and online knowledge distillation are illustrated as orange modules in Figure 1 and the details are presented in the following two subsections."
    }, {
      "heading" : "3.1 Cross-Attentive Regularization",
      "text" : "The cross-attentive regularization (CAR) is proposed to increase the similarity between the text encoder outputs and their corresponding speech encoder outputs. Hence, the performance of the more difficult ST task can be improved by learning from the relatively easier MT task. Encoder output sequences from different modalities can not be compared directly since they have different lengths. In CAR, the two reconstructed sequences are calculated from the text output sequence via self-attention or the speech output sequence via\ncross attention over the text output sequence. The two reconstructed sequences have the same length and the distance is simply measured as the L2 distance between the two sequences.\nFormally, we denote a speech to text translation training sample as a triplet o = (Xs,xt,y). Xs ∈ Rds×N , xt ∈ RM , and y ∈ RK are the speech feature input, text token input and target text output respectively. N , M and K are the corresponding sequence lengths. Assume Hs = (hs1,h s 2, · · ·,hsN ) and Ht = (ht1,h t 2, · · ·,htM ), hsn,htm ∈ Rdh are outputs from the speech encoder and text encoder respectively, where dh is the dimension of the output states. A similarity matrix S ∈ RN×M is defined as the cosine distance between the tensors in the two sequences:\nsi,j = (hsi ) ′ · htj ||hsi ||2||htj ||2\n(1)\nwhere si,j is the ith row and jth column component in S. The text encoder outputs Ht are reconstructed through the speech encoder outputs Hs and similarity matrix S as below.\nHs→t = Hs · softmax(S) (2)\nHt→t, the reconstruction of Ht from itself, can be computed similarly via self-attention. CAR is defined as the L2 distance between the two reconstruction encoder outputs:\nLCAR(θs) = 1\nM ∥∥∥Hs→t − sg[Ht→t]∥∥∥ 2\n(3)\nwhere sg[·] is the stop-gradient operator and θs are the ST model parameters. By optimizing the model with CAR, the speech encoder is encouraged to learn from more accurate text encoder and generates similar encoder outputs after reconstruction. CAR is inspired by the attention mechanism between the encoder and decoder where the decoder states are reconstructed through encoder output states via the attention mechanism."
    }, {
      "heading" : "3.2 Online Knowledge Distillation",
      "text" : "Knowledge distillation (KD) is widely used for model compression (Hinton et al., 2015; Kim and Rush, 2016) where a smaller student network is trained to mimic the original teacher network by minimizing the loss between the student and teacher outputs. The ST task is considerably more difficult than the MT task since the speech input is noisier and more ambiguous than the text input.\nThe accuracy of the MT model is usually much higher than the corresponding ST model. Knowledge distillation from a well trained MT model to a ST model has been proved to be an effective way to improve the ST performance (Liu et al., 2019b; Gaido et al., 2020). In this work, we extend knowledge distillation to the MTL framework where both ST and MT are fine-tuned simultaneously with shared parameters.\nConcretely, we assume an MTL model learns from a data set D with target vocabulary size |V |. The training criterion is to minimize negative log likelihood (NLL) for each example o = (Xs,xt,y) ∈ D from the training data:\nLNLL(θs) = − D∑ o K∑ k=1 |V |∑ v=1 δ(yk = v)\nlog p(yk = v|y<k,Xs, θs) (4)\nwhere δ(·) is the indicator function and p the distribution from the ST model (parameterized by θs).\nAssume the probability distribution for yk given text input xt and MT model θt is q(yk = v|y<k,xt, θt), the knowledge distillation loss is defined as minimizing the cross-entropy with the MT’s probability distribution LKD(θs) = − D∑ o K∑ k=1 |V |∑ v=1 q(yk = v|y<k,xt, θt)\nlog p(yk = v|y<k,Xs, θs) (5)\nThe overall loss is the combination of crossattentive regularization, knowledge distillation loss, negative log likelihood loss for both ST and MT, as follows:\nL(θs, θt) = αLNLL(θs) + (1− α)LKD(θs) +λLCAR(θs) + LNLL(θt) (6)\nwhere α and λ are predefined hyper-parameters."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : "Experiments are conducted on three MUSTC (Gangi et al., 2019a) language pairs: EN-DE, EN-ES and EN-FR. The models are developed and analyzed on the dev set and the final results are reported on the tst-COMMON set. We use WMT parallel data from different years, 2013 for Spanish, 2014 for German, and 2016 for French, as extra text training corpus for MTL. Case-sensitive detokenized BLEU is reported by SACREBLEU with default options (Post, 2018).\nWe use the “T-Md” configuration from (Wang et al., 2020a) in all experiments. The speech encoder has 12 transformer layers while the decoder is with 6 transformer layers. For the MTL model, the text encoder has 6 transformer layers. The transformer layer has an input embedding size of 512 and middle layer dimension 2048. We share parameters of all 6 text encoder transformer layers with the top 6 transformer layers in the speech encoder, hence both encoders use the same modules to generate the encoder outputs.\nThe Adam optimizer (Kingma and Ba, 2014) with a learning rate 0.002 is employed in the experiments. Label smoothing and dropout rate are both set to 0.1. We choose α = 0.8 and λ = 0.02 in Equation 6 through grid search ([0.1, 1.0] for α and [0.01, 0.05] for λ).\nInput speech is represented as 80D log melfilterbank coefficients computed every 10ms with a 25ms window. Global channel mean and variance normalization is applied. The SpecAugment (Park et al., 2019) data augmentation with the LB policy is applied in all experiments. The input text tokens are converted into their corresponding pronunciation form as phoneme sequences (Tang et al., 2021; Renduchintala et al., 2018). The grapheme to phoneme conversion is done through the “g2p en” python package (Lee and Kim, 2018). The leading phoneme in a word is appended with an extra “ ” to mark word boundaries. In total, the vocabulary size for the input phonemes is 134. The target vocabulary consists of 10k “unigram” subword units learned by SentencePiece (Kudo and Richardson, 2018) with full character coverage of all training text data.\nAll ST or jointly trained models are initialized with pretrained ASR and MT modules. The ASR model is trained on the same English speech training data from MUST-C with the “T-Md” configuration too. The pretrained MT models are trained for each language pair with the aforementioned WMT data. The MT encoder and decoder configurations are the same as the text encoder and decoder in the MTL model mentioned above.\nThe models are fine-tuned to 100 epochs using 8 V100 GPUs for approximate one day. The batch size is 10,000 frames for speech to text translation samples and 10,000 tokens for parallel text samples per GPU. The model parameters are updated every 4 batches. Speech training samples and text input samples are used to update the model alternatively.\nThe models are trained with FAIRSEQ (Ott et al., 2019; Wang et al., 2020a). The last 10 checkpoints are averaged for inference with beam size 5. 1."
    }, {
      "heading" : "5 MTL Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Model Variation",
      "text" : "We extend Chatterji et al. (2020)’s work to analyze a MTL model. We initialize models with different pretrained modules and fine-tune them for ST and MT tasks within the MTL framework. The pretrained modules come from ASR and MT tasks.\nCriticality analysis is conducted on the ST model after the MTL fine-tuning step. The parameters in the selected modules are interpolated with corresponding parameters in the pretrained modules. MUST-C EN-DE dev set is used for BLEU computation. With different interpolation ratios, we obtain different BLEU scores. The BLEU difference comes from two sources. The first one comes from the selected module itself. If the module is important and sensitive, very small perturbation could result in a nontrivial BLEU difference as (Chatterji et al., 2020). Another source of difference is that if the selected module changes significantly to adapt to the ST task, rewinding the parameters back to the initial task may lead to a substantial decrease in BLEU. We attempt to quantify the extent of the degradation from the second source, which can be indicative of the model variation from the pretrained task to the ST task. This is accomplished by comparing the BLEU differences for the same module but using different initialization and training schemes.\nTable 1 lists models initialized with different pretrained modules. “ST” designates a ST model trained with the single ST task, “JT” corresponds to a ST model trained with the primary ST task and auxiliary MT task together. “JT-S-ASR” and “JTS-MT” are another two jointly trained models but\n1The source code will be released at https://github.com/pytorch/fairseq/tree/master/examples/speech text joint to text\nwith the top encoder layers shared as described in section 4. The difference between the two models is how we initialized the shared encoder layers, either from the pretrained ASR model for “JT-SASR” or from the pretrained MT model for “JT-SMT”. ST Figure 2 shows the analysis for the “ST” model. The x-axis is the interpolation ratio and “1.0” means the pretrained parameters are used. The y-axis is the relative change in BLEU compared with the well-trained ST model. It is clear that higher layers are more critical to the performance. Around 5 BLEU decrease is observed on the top encoder layer (11) and top decoder layer (5) during the criticality tests. The following analysis will compare with Figure 2 and we can separate the aforementioned second source from the first one. JT Figure 3 presents the analysis for the “JT” model. The jointly trained model shows smaller degradation compared with “ST” for the decoder layers. This indicates that training the ST and MT tasks together helps to preserve more information from the original MT decoder and partially remedies the catastrophic forgetting (McCloskey and Cohen, 1989) during the finetuning phase. On the other hand, after rolling parameters back to the initial ASR model, the jointly trained model shows a larger degradation for the encoder layers. This means that the speech encoder in the jointly trained model has deviated far away from the speech encoder in the initial ASR task. We conclude that the shared decoder is subject to more constraints since it is optimized toward both MT and ST tasks while the speech encoder has to undergo larger changes in order to align with the text encoder, although there is no parameter sharing between two encoders. JT-S-ASR and JT-S-MT Results for models with\nthe top encoder layers shared are presented in Figure 4 and 5. In “JT-S-MT”, the top 6 shared encoder layers are initialized with the pretrained MT encoder. We illustrate their BLEU difference trajectories with dotted lines in Figure 5 (a) so they can be easily distinguished from other layers initialized from the ASR encoder.\nThe BLEU difference for the top encoder layer is down from 20.2 to 17.6 when the parameters are replaced with the ones in the pretrained ASR encoder. It is further reduced to 10.0 if the shared layers are initialized with MT encoder layers. The BLEU differences in the decoder layers are mixed. The performance of “JT-S-ASR” degrades quickly in the criticality test for the top decoder layer, while “JT-S-MT performs similarly in the test as “JT” decoder. We argue that the top layers in the finetuned ST encoder might be closer to the MT encoder than the ASR encoder. It preserves more information from the MT task by sharing more parameters between two tasks and initializing them with pretrained MT modules. This is a desirable property since we want to transfer more knowledge from the text corpus to the ST task."
    }, {
      "heading" : "5.2 Modality Variation",
      "text" : "The jointly trained model takes input from two modalities, i.e. text or speech, and we are interested in the model internal representation difference for paired inputs. Given text target y, we extract the decoder hidden state representations for the corresponding text input xt and speech input Xs. The decoder representation difference solely comes from different input modalities. The difference is quantified by the correlation coefficient over all samples evaluated between two input modalities:\nrs,t(l, d) = σst(l, d)\nσs(l, d)σt(l, d) (7)\nwhere σz(l, d), z ∈ [s, t] is the standard deviations of decoder hidden states at layer l for component d in all samples, and σst(l, d) is the corresponding covariance. The layer-wise correlation coefficient is the average of all components:\nrs,t(l) = 1\nD ∑ d rs,t(l, d) (8)\nFigure 6 depicts the correlation coefficient between speech input and text input for each decoder layer in the model “JT-S-MT”. The x-axis is the number of training epochs and the y-axis represents the correlation coefficient for each layer. There\nare two observations. First, the correlation coefficients become larger and close to “1.0” as training converges. Second, the higher the layer, the smaller the correlation coefficient. We hypothesize that the inputs to the lower layers are dominated by the decoder text embeddings, which are the same for both modalities, and the inputs to the higher layers would contain more information from the encoder outputs, which result in the decoder internal representation differences. The analysis shows a well trained MTL decoder has similar representations for paired text and speech input. However, the top decoder layers still have nontrivial representation differences due to different modalities."
    }, {
      "heading" : "6 Experimental Results",
      "text" : ""
    }, {
      "heading" : "6.1 Main Results",
      "text" : "The main ST results are presented in Table 2. The first three rows are results from the literature. “ST” and “JT” are models initialized as Table 1 and studied in section 5. The last row (“JT Proposed”) presents results from the proposed system, in which the top encoder layers and decoder are shared, and the models are optimized following Equation 6. The second column (“pars(m)”) lists the number of parameters used during inference. From Table 2, our “ST” baseline is comparable to the previously reported results except (Pino et al., 2020), who use a much larger model and additional weakly supervised speech training data. As expected, the vanilla joint training baseline (“JT”) outperforms the “ST” baseline with the help of extra bitext training data. Finally, the proposed joint training model (“JT Proposed”) achieves 2.0∼2.7 BLEU gains over the strong joint training baseline (“JT”)."
    }, {
      "heading" : "6.2 Ablation",
      "text" : "Table 3 breaks down the performance gains into individual components/changes. Sharing encoder layers improves the quality for all three language pairs\n(“JT” v.s. “JT-S-ASR”). Initializing the shared encoder layers with pretrained MT modules leads to BLEU increase for two of the three evaluated translation pairs (“JT-S-ASR” v.s. “JT-S-MT”). For EN-FR, the degradation is minimal (-0.1 BLEU). Overall, sharing top encoder layers can increase BLEU by 0.2∼0.7 (“JT-S-MT” v.s. “JT”). CAR further improves the translation by another 0.3∼0.9 BLEU. The best results are achieved by applying the shared top encoder layers, CAR and online KD together. They are about 2.9+ BLEU better than the single task based system (“ST”) and achieve 2+ BLEU increase on top of the strong vanilla joint training system(“JT”).\nFigure 7 demonstrates the model variation for the proposed system on the MUST-C EN-DE dev set. Compared with Figure 5, the decoder shows less degradation during the criticality test and it shows CAR and online KD help to preserve more information from the MT task. Figure 8 shows the corresponding correlation coefficients between paired text and speech input from the top decoder\nlayer from different model configurations. It also confirms that the proposed methods, i.e., shared top encoder layers, CAR and online KD, all reduce the modality difference substantially."
    }, {
      "heading" : "6.3 Task Dependent Components",
      "text" : "In MLT, many works (Maninis et al., 2019; Liu et al., 2019a; Zhang et al., 2020; Pfeiffer et al., 2020) employ task-dependent components to alleviate the negative transfer effect. In Table 4, we compare the “JT-S-MT” model with two variants using different task-dependent components. The first one (“JT-S-MT + Adapter”) (Bapna et al., 2019) adds an extra adapter module on the top of the speech encoder. Hence, the speech encoder outputs, which are generated from shared encoder layers, are further processed to reduce the difference between speech input and text input. The adapter module consists of a linear layer and layer normalization layer. The second variant (“JT-S-MT + Dedicated Attention”) (Blackwood et al., 2018) introduces dedicated decoder modules for different tasks. Attention layers between encoder and decoder, and the layer normalization modules are not shared between the ST and MT tasks. It gives the decoder more flexibility to handle information from different modalities.\nThe results show the extra adapter layer doesn’t bring gain while the task dependent attention module actually makes the performance worse. It indicates that the negative transfer effect is not significant in this study and adding extra task-dependent components might not be necessary."
    }, {
      "heading" : "6.4 Impact on the MT Task",
      "text" : "As shown in Table 2, training ST models with an auxiliary MT task improves the translation quality substantially. It may be interesting to examine the impact on the auxiliary task itself. We evaluate the MT model jointly trained with the ST task. Results are shown in Table 5. “ST (JT Proposed)” in the first row corresponds to the best results obtained for the ST task. The detailed experimental setup is described in Appendix A. For reference, we also\ninclude the MT evaluation results from MUSTC (Gangi et al., 2019a) in the second row. All MT models (in the last 4 rows) take phoneme sequences as input instead of SentencePiece.\n“MT” (row 3) shows the results from pretrained MT models on WMT. In the “MT (Tuned)” row, the MT models pretrained on WMT are fine-tuned on the MUST-C datasets. The large improvements clearly show a domain mismatch between WMT and MUST-C. The MT models trained with WMT data are improved after fine-tuning, and they are comparable with the ones reported in (Gangi et al., 2019a), though the input token is in pronunciation form, which is more ambiguous than the corresponding SentencePiece unit.\n“MT (JT)” and “MT (JT Proposed)” are results from the co-trained MT models in “JT” and “JT Proposed” respectively. After fine-tuning using both MuST-C (speech and text) and WMT (text only) training data, the auxiliary MT models perform better than the corresponding ST models. The proposed techniques further improve the co-trained MT models by 0.7∼1.6 BLEU. While this is a surprising result, we note that the dedicated MT models may be improved with better hyperparameter tuning. In conclusion, the results show the proposed methods are effective to unify two tasks into one model with minimal negative transfer effect."
    }, {
      "heading" : "7 Conclusions",
      "text" : "In this study, we focus on understanding the interactions between the ST and MT tasks under the MTL framework, and on boosting the performance of the primary ST model with the auxiliary MT task. Two types of analysis on model variation and modality variation, are conducted on the MTL models. The analysis demonstrates MTL helps to preserve information from the MT task and generates similar model representations for different modalities. We observe a minimal negative transfer effect between the two tasks. Sharing more parameters can further boost the information transfer from\nthe MT task to the ST model. The analysis also reveals that the model representation difference due to modality difference is nontrivial, especially for the top decoder layers, which are critical for the translation performance. Inspired by the findings, we propose three techniques to increase knowledge transfer from the MT task to the ST task. These techniques include parameter sharing and initialization strategy to improve the information sharing between tasks, CAR and online KD to encourage the ST system to learn more from the auxiliary MT task and then generate similar model representations from different modalities. Our results show that the proposed methods improve translation performance and achieve state-of–the-art results on three MUST-C language pairs."
    }, {
      "heading" : "A Appendix",
      "text" : "The detailed experimental setup for “MT” and “MT(Tuned)” in Table 5 are described as below.\nWe trained MT models for each language pair in “EN-DE”, “EN-ES”, and “EN-FR”. The training data is from WMT from different years, 2013 for “EN-ES”, 2014 for “EN-DE” and 2016 for “ENFR”. We use “transformer wmt en de” architecture from Fairseq. The models are with embedding size 512 and feed-forward layer dimension 2048. Both encoder and decoder are with 6 transformer layers. The input is phoneme sequence and output is SentencePiece sequence. The vocabularies are shared with the corresponding speech to text translation models. The models are optimized with Adam with learning rate equal to 0.001. Beside experiments in Table 5, the trained MT models are used to initialize the jointly trained models.\nWe further fine-tuned the “MT” models trained from WMT data to MUST-C data sets using source transcripts and target translation labels. No speech data is used. Similar to the “MT” models, Adam optimizer with learning rate equal to 0.001 is used. The models are fine-tuned on the corresponding MUST-C data sets for 15 epochs and the checkpoints from the last 5 epochs are averaged for evaluation."
    } ],
    "references" : [ {
      "title" : "Tied multitask learning for neural speech translation",
      "author" : [ "Antonios Anastasopoulos", "David Chiang." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Anastasopoulos and Chiang.,? 2018",
      "shortCiteRegEx" : "Anastasopoulos and Chiang.",
      "year" : 2018
    }, {
      "title" : "A comparative study on end-to-end speech to text translation",
      "author" : [ "Parnia Bahar", "Tobias Bieschke", "Hermann Ney." ],
      "venue" : "ASRU.",
      "citeRegEx" : "Bahar et al\\.,? 2019",
      "shortCiteRegEx" : "Bahar et al\\.",
      "year" : 2019
    }, {
      "title" : "Simple, scalable adaptation for neural machine translation",
      "author" : [ "Ankur Bapna", "N. Arivazhagan", "Orhan Firat." ],
      "venue" : "EMNLP/IJCNLP.",
      "citeRegEx" : "Bapna et al\\.,? 2019",
      "shortCiteRegEx" : "Bapna et al\\.",
      "year" : 2019
    }, {
      "title" : "Multilingual neural machine translation with taskspecific attention",
      "author" : [ "G. Blackwood", "Miguel Ballesteros", "T. Ward." ],
      "venue" : "COLING.",
      "citeRegEx" : "Blackwood et al\\.,? 2018",
      "shortCiteRegEx" : "Blackwood et al\\.",
      "year" : 2018
    }, {
      "title" : "The intriguing role of module criticality in the generalization of deep networks",
      "author" : [ "Niladri S. Chatterji", "Behnam Neyshabur", "H. Sedghi." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Chatterji et al\\.,? 2020",
      "shortCiteRegEx" : "Chatterji et al\\.",
      "year" : 2020
    }, {
      "title" : "Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks",
      "author" : [ "Z. Chen", "Vijay Badrinarayanan", "Chen-Yu Lee", "Andrew Rabinovich." ],
      "venue" : "ICML.",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "End-toend speech-translation with knowledge distillation: Fbk@iwslt2020",
      "author" : [ "Marco Gaido", "Mattia Antonino Di Gangi", "Matteo Negri", "Marco Turchi" ],
      "venue" : null,
      "citeRegEx" : "Gaido et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Gaido et al\\.",
      "year" : 2020
    }, {
      "title" : "MuST-C: a multilingual speech translation corpus",
      "author" : [ "Mattia Antonino Di Gangi", "Roldano Cattoni", "Luisa Bentivogli", "Matteo Negri", "Marco Turchi." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Gangi et al\\.,? 2019a",
      "shortCiteRegEx" : "Gangi et al\\.",
      "year" : 2019
    }, {
      "title" : "One-to-many multilingual end-toend speech translation",
      "author" : [ "Mattia Antonino Di Gangi", "Matteo Negri", "Marco Turchi." ],
      "venue" : "ASRU.",
      "citeRegEx" : "Gangi et al\\.,? 2019b",
      "shortCiteRegEx" : "Gangi et al\\.",
      "year" : 2019
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey E. Hinton", "Oriol Vinyals", "J. Dean." ],
      "venue" : "ArXiv, abs/1503.02531.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Espnet-st: Allin-one speech translation toolkit",
      "author" : [ "H. Inaguma", "S. Kiyono", "K. Duh", "S. Karita", "N. Soplin", "T. Hayashi", "S. Watanabe." ],
      "venue" : "ACL.",
      "citeRegEx" : "Inaguma et al\\.,? 2020",
      "shortCiteRegEx" : "Inaguma et al\\.",
      "year" : 2020
    }, {
      "title" : "Endend speech-to-text translation with modality agnostic meta-learning",
      "author" : [ "Sathish Reddy Indurthi", "HouJeung Han", "Nikhil Kumar Lakumarapu", "Beom seok Lee", "Insoo Chung", "Sang-Ha Kim", "Chanwoo Kim." ],
      "venue" : "ICASSP.",
      "citeRegEx" : "Indurthi et al\\.,? 2020",
      "shortCiteRegEx" : "Indurthi et al\\.",
      "year" : 2020
    }, {
      "title" : "Sequencelevel knowledge distillation",
      "author" : [ "Yoon Kim", "Alexander M. Rush." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Kim and Rush.,? 2016",
      "shortCiteRegEx" : "Kim and Rush.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "author" : [ "T. Kudo", "J. Richardson." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Kudo and Richardson.,? 2018",
      "shortCiteRegEx" : "Kudo and Richardson.",
      "year" : 2018
    }, {
      "title" : "Learning pronunciation from a foreign language in speech synthesis networks",
      "author" : [ "Y. Lee", "T. Kim." ],
      "venue" : "ArXiv.",
      "citeRegEx" : "Lee and Kim.,? 2018",
      "shortCiteRegEx" : "Lee and Kim.",
      "year" : 2018
    }, {
      "title" : "Multilingual speech translation with efficient finetuning of pretrained models",
      "author" : [ "Xian Li", "Changhan Wang", "Yun Tang", "Chau Tran", "Yuqing Tang", "Juan Pino", "Alexei Baevski", "Alexis Conneau", "Michael Auli." ],
      "venue" : "arXiv: Computation and Language.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "End-to-end multi-task learning with attention",
      "author" : [ "Shikun Liu", "Edward Johns", "A. Davison." ],
      "venue" : "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1871–1880.",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "End-to-end speech translation with knowledge distillation",
      "author" : [ "Yuchen Liu", "Hao Xiong", "Zhongjun He", "Jiajun Zhang", "Hua Wu", "Haifeng Wang", "Chengqing Zong." ],
      "venue" : "Interspeech.",
      "citeRegEx" : "Liu et al\\.,? 2019b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Attentive single-tasking of multiple tasks",
      "author" : [ "K. Maninis", "Ilija Radosavovic", "I. Kokkinos." ],
      "venue" : "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1851–1860.",
      "citeRegEx" : "Maninis et al\\.,? 2019",
      "shortCiteRegEx" : "Maninis et al\\.",
      "year" : 2019
    }, {
      "title" : "Catastrophic interference in connectionist networks: The sequential learning problem",
      "author" : [ "M. McCloskey", "N.J. Cohen." ],
      "venue" : "Psychology of Learning and Motivation, 24:109–165.",
      "citeRegEx" : "McCloskey and Cohen.,? 1989",
      "shortCiteRegEx" : "McCloskey and Cohen.",
      "year" : 1989
    }, {
      "title" : "Insights on representational similarity in neural networks with canonical correlation",
      "author" : [ "Ari S. Morcos", "M. Raghu", "S. Bengio." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Morcos et al\\.,? 2018",
      "shortCiteRegEx" : "Morcos et al\\.",
      "year" : 2018
    }, {
      "title" : "The IWSLT 2019 evaluation campaign",
      "author" : [ "Jan Niehues", "R. Cattoni", "Sebastian Stüker", "Matteo Negri", "Marco Turchi", "Elizabeth Salesky", "Ramon Sanabria", "Loı̈c Barrault", "Lucia Specia", "Marcello Federico" ],
      "venue" : null,
      "citeRegEx" : "Niehues et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Niehues et al\\.",
      "year" : 2019
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "S. Gross", "Nathan Ng", "David Grangier", "M. Auli." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "Specaugment: A simple data augmentation method for automatic speech recognition",
      "author" : [ "D. Park", "W. Chan", "Y. Zhang", "C. Chiu", "B. Zoph", "E. Cubuk", "Q. Le." ],
      "venue" : "Interspeech.",
      "citeRegEx" : "Park et al\\.,? 2019",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2019
    }, {
      "title" : "MAD-X: An adapter-based framework for multi-task cross-lingual transfer",
      "author" : [ "Jonas Pfeiffer", "Ivan Vulic", "Iryna Gurevych", "Sebastian Ruder." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Pfeiffer et al\\.,? 2020",
      "shortCiteRegEx" : "Pfeiffer et al\\.",
      "year" : 2020
    }, {
      "title" : "Self-training for end-to-end speech translation",
      "author" : [ "J. Pino", "Q. Xu", "X. Ma", "M. Dousti", "Y. Tang." ],
      "venue" : "Interspeech.",
      "citeRegEx" : "Pino et al\\.,? 2020",
      "shortCiteRegEx" : "Pino et al\\.",
      "year" : 2020
    }, {
      "title" : "A call for clarity in reporting BLEU scores",
      "author" : [ "Matt Post." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186– 191, Brussels, Belgium. Association for Computational Linguistics.",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability",
      "author" : [ "M. Raghu", "J. Gilmer", "J. Yosinski", "Jascha SohlDickstein." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Raghu et al\\.,? 2017",
      "shortCiteRegEx" : "Raghu et al\\.",
      "year" : 2017
    }, {
      "title" : "Multi-modal data augmentation for endto-end asr",
      "author" : [ "A. Renduchintala", "S. Ding", "M. Wiesner", "S. Watanabe." ],
      "venue" : "INTERSPEECH.",
      "citeRegEx" : "Renduchintala et al\\.,? 2018",
      "shortCiteRegEx" : "Renduchintala et al\\.",
      "year" : 2018
    }, {
      "title" : "Phone features improve speech translation",
      "author" : [ "Elizabeth Salesky", "Alan W Black." ],
      "venue" : "ACL.",
      "citeRegEx" : "Salesky and Black.,? 2020",
      "shortCiteRegEx" : "Salesky and Black.",
      "year" : 2020
    }, {
      "title" : "Which tasks should be learned together in multi-task learning",
      "author" : [ "T. Standley", "A. Zamir", "Dawn Chen", "L. Guibas", "Jitendra Malik", "S. Savarese" ],
      "venue" : null,
      "citeRegEx" : "Standley et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Standley et al\\.",
      "year" : 2020
    }, {
      "title" : "A general multi-task learning framework to leverage text data for speech to text tasks",
      "author" : [ "Yun Tang", "Juan Pino", "Changhan Wang", "Xutai Ma", "Dmitriy Genzel." ],
      "venue" : "ICASSP.",
      "citeRegEx" : "Tang et al\\.,? 2021",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2021
    }, {
      "title" : "Multi-task learning for dense prediction tasks: A survey",
      "author" : [ "Simon Vandenhende", "S. Georgoulis", "Wouter Van Gansbeke", "M. Proesmans", "Dengxin Dai", "L. Gool." ],
      "venue" : "arXiv: Computer Vision and Pattern Recognition.",
      "citeRegEx" : "Vandenhende et al\\.,? 2020",
      "shortCiteRegEx" : "Vandenhende et al\\.",
      "year" : 2020
    }, {
      "title" : "fairseq s2t: Fast speech-to-text modeling with fairseq",
      "author" : [ "C. Wang", "Y. Tang", "X. Ma", "A. Wu", "D. Okhonko", "J. Pino." ],
      "venue" : "AACL (demo).",
      "citeRegEx" : "Wang et al\\.,? 2020a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Bridging the gap between pretraining and fine-tuning for end-to-end speech translation",
      "author" : [ "Chengyi Wang", "Yu Wu", "Shujie Liu", "Zhenglu Yang", "Ming Zhou." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Wang et al\\.,? 2020b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Sequence-tosequence models can directly translate foreign speech",
      "author" : [ "Ron J. Weiss", "Jan Chorowski", "Navdeep Jaitly", "Yonghui Wu", "Zhifeng Chen." ],
      "venue" : "INTERSPEECH.",
      "citeRegEx" : "Weiss et al\\.,? 2017",
      "shortCiteRegEx" : "Weiss et al\\.",
      "year" : 2017
    }, {
      "title" : "Improving massively multilingual neural machine translation and zero-shot translation",
      "author" : [ "Biao Zhang", "Philip Williams", "Ivan Titov", "Rico Sennrich." ],
      "venue" : "ACL.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "End-to-end methods have achieved significant progress in speech to text translation (ST) and even surpassed the traditional pipeline-based methods in some applications (Niehues et al., 2019; Salesky and Black, 2020).",
      "startOffset" : 168,
      "endOffset" : 215
    }, {
      "referenceID" : 30,
      "context" : "End-to-end methods have achieved significant progress in speech to text translation (ST) and even surpassed the traditional pipeline-based methods in some applications (Niehues et al., 2019; Salesky and Black, 2020).",
      "startOffset" : 168,
      "endOffset" : 215
    }, {
      "referenceID" : 36,
      "context" : "pretrained models with multitask learning (MTL) is widely used to overcome the limited training data issue (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Bahar et al., 2019; Indurthi et al., 2020; Wang et al., 2020b; Li et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 240
    }, {
      "referenceID" : 0,
      "context" : "pretrained models with multitask learning (MTL) is widely used to overcome the limited training data issue (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Bahar et al., 2019; Indurthi et al., 2020; Wang et al., 2020b; Li et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 240
    }, {
      "referenceID" : 1,
      "context" : "pretrained models with multitask learning (MTL) is widely used to overcome the limited training data issue (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Bahar et al., 2019; Indurthi et al., 2020; Wang et al., 2020b; Li et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 240
    }, {
      "referenceID" : 11,
      "context" : "pretrained models with multitask learning (MTL) is widely used to overcome the limited training data issue (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Bahar et al., 2019; Indurthi et al., 2020; Wang et al., 2020b; Li et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 240
    }, {
      "referenceID" : 35,
      "context" : "pretrained models with multitask learning (MTL) is widely used to overcome the limited training data issue (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Bahar et al., 2019; Indurthi et al., 2020; Wang et al., 2020b; Li et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 240
    }, {
      "referenceID" : 16,
      "context" : "pretrained models with multitask learning (MTL) is widely used to overcome the limited training data issue (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Bahar et al., 2019; Indurthi et al., 2020; Wang et al., 2020b; Li et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 240
    }, {
      "referenceID" : 33,
      "context" : "Multitask learning aims to improve generalization by leveraging domain-specific information contained in the training signals of related tasks (Vandenhende et al., 2020).",
      "startOffset" : 143,
      "endOffset" : 169
    }, {
      "referenceID" : 9,
      "context" : "Knowledge distillation (KD) is widely used for model compression (Hinton et al., 2015; Kim and Rush, 2016) where a smaller student network is trained to mimic the original teacher network by minimizing the loss between the student and teacher outputs.",
      "startOffset" : 65,
      "endOffset" : 106
    }, {
      "referenceID" : 12,
      "context" : "Knowledge distillation (KD) is widely used for model compression (Hinton et al., 2015; Kim and Rush, 2016) where a smaller student network is trained to mimic the original teacher network by minimizing the loss between the student and teacher outputs.",
      "startOffset" : 65,
      "endOffset" : 106
    }, {
      "referenceID" : 18,
      "context" : "Knowledge distillation from a well trained MT model to a ST model has been proved to be an effective way to improve the ST performance (Liu et al., 2019b; Gaido et al., 2020).",
      "startOffset" : 135,
      "endOffset" : 174
    }, {
      "referenceID" : 6,
      "context" : "Knowledge distillation from a well trained MT model to a ST model has been proved to be an effective way to improve the ST performance (Liu et al., 2019b; Gaido et al., 2020).",
      "startOffset" : 135,
      "endOffset" : 174
    }, {
      "referenceID" : 7,
      "context" : "Experiments are conducted on three MUSTC (Gangi et al., 2019a) language pairs: EN-DE, EN-ES and EN-FR.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 27,
      "context" : "Case-sensitive detokenized BLEU is reported by SACREBLEU with default options (Post, 2018).",
      "startOffset" : 78,
      "endOffset" : 90
    }, {
      "referenceID" : 34,
      "context" : "We use the “T-Md” configuration from (Wang et al., 2020a) in all experiments.",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 13,
      "context" : "The Adam optimizer (Kingma and Ba, 2014) with a learning rate 0.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 24,
      "context" : "The SpecAugment (Park et al., 2019) data augmentation with the LB policy is applied in all experiments.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 15,
      "context" : "The grapheme to phoneme conversion is done through the “g2p en” python package (Lee and Kim, 2018).",
      "startOffset" : 79,
      "endOffset" : 98
    }, {
      "referenceID" : 14,
      "context" : "The target vocabulary consists of 10k “unigram” subword units learned by SentencePiece (Kudo and Richardson, 2018) with full character coverage of all training text data.",
      "startOffset" : 87,
      "endOffset" : 114
    }, {
      "referenceID" : 23,
      "context" : "The models are trained with FAIRSEQ (Ott et al., 2019; Wang et al., 2020a).",
      "startOffset" : 36,
      "endOffset" : 74
    }, {
      "referenceID" : 34,
      "context" : "The models are trained with FAIRSEQ (Ott et al., 2019; Wang et al., 2020a).",
      "startOffset" : 36,
      "endOffset" : 74
    }, {
      "referenceID" : 4,
      "context" : "If the module is important and sensitive, very small perturbation could result in a nontrivial BLEU difference as (Chatterji et al., 2020).",
      "startOffset" : 114,
      "endOffset" : 138
    }, {
      "referenceID" : 20,
      "context" : "This indicates that training the ST and MT tasks together helps to preserve more information from the original MT decoder and partially remedies the catastrophic forgetting (McCloskey and Cohen, 1989) during the finetuning phase.",
      "startOffset" : 173,
      "endOffset" : 200
    }, {
      "referenceID" : 26,
      "context" : "From Table 2, our “ST” baseline is comparable to the previously reported results except (Pino et al., 2020), who use a much larger model and additional weakly supervised speech training data.",
      "startOffset" : 88,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : "(“JT-S-MT + Adapter”) (Bapna et al., 2019) adds an extra adapter module on the top of the speech encoder.",
      "startOffset" : 22,
      "endOffset" : 42
    }, {
      "referenceID" : 3,
      "context" : "The second variant (“JT-S-MT + Dedicated Attention”) (Blackwood et al., 2018) introduces dedicated decoder modules for different tasks.",
      "startOffset" : 53,
      "endOffset" : 77
    }, {
      "referenceID" : 7,
      "context" : "include the MT evaluation results from MUSTC (Gangi et al., 2019a) in the second row.",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 7,
      "context" : "comparable with the ones reported in (Gangi et al., 2019a), though the input token is in pronunciation form, which is more ambiguous than the corresponding SentencePiece unit.",
      "startOffset" : 37,
      "endOffset" : 58
    } ],
    "year" : 2021,
    "abstractText" : "Pretraining and multitask learning are widely used to improve the speech to text translation performance. In this study, we are interested in training a speech to text translation model along with an auxiliary text to text translation task. We conduct a detailed analysis to understand the impact of the auxiliary task on the primary task within the multitask learning framework. Our analysis confirms that multitask learning tends to generate similar decoder representations from different modalities and preserve more information from the pretrained text translation modules. We observe minimal negative transfer effect between the two tasks and sharing more parameters is helpful to transfer knowledge from the text task to the speech task. The analysis also reveals that the modality representation difference at the top decoder layers is still not negligible, and those layers are critical for the translation quality. Inspired by these findings, we propose three methods to improve translation quality. First, a parameter sharing and initialization strategy is proposed to enhance information sharing between the tasks. Second, a novel attention-based regularization is proposed for the encoders and pulls the representations from different modalities closer. Third, an online knowledge distillation is proposed to enhance the knowledge transfer from the text to the speech task. Our experiments show that the proposed approach improves translation performance by more than 2 BLEU over a strong baseline and achieves state-of-theart results on the MUST-C English-German, English-French and English-Spanish language pairs.",
    "creator" : "LaTeX with hyperref"
  }
}