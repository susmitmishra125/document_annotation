{
  "name" : "2021.acl-long.531.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Neural semi-Markov CRF for Monolingual Word Alignment",
    "authors" : [ "Wuwei LanF", "Chao JiangF", "Wei Xu" ],
    "emails" : [ "lan.105@osu.edu", "wei.xu}@cc.gatech.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6815–6828\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6815"
    }, {
      "heading" : "1 Introduction",
      "text" : "Monolingual word alignment aims to align words or phrases with similar meaning in two sentences that are written in the same language. It is useful for improving the interpretability in natural language understanding tasks, including semantic textual similarity (Li and Srikumar, 2016) and question answering (Yao, 2014). Monolingual word alignment can also support the analysis of human editing operations (Figure 1) and improve model performance for text-to-text generation tasks, such as text simplification (Maddela et al., 2021) and neutralizing biased language (Pryzant et al., 2020). It has also been shown to be helpful for data augmentation and label projection\n1Our code and data will be available at: https:// github.com/chaojiang06/neural-Jacana\nFAuthors contributed equally.\n(Culkin et al., 2021) when combined with paraphrase generation.\nOne major challenge for automatic alignment is the need to handle not only alignments between words and linguistic phrases (e.g., a dozen ↔ more than 10), but also non-linguistic phrases that are semantically related given the context (e.g., tensions ↔ relations being strained in Figure 3). In this paper, we present a novel neural semiMarkov CRF alignment model, which unifies both word and phrase alignments though variablelength spans, calculates span-based semantic similarities, and takes alignment label transitions into consideration. We also create a new manually annotated benchmark, Multi-Genre Monolingual Word Alignment (MultiMWA), which consists of four datasets across different text genres and is large enough to support the training of neuralbased models (Table 1). It addresses the shortcomings of existing datasets for monolingual word alignment: MTReference (Yao, 2014) was annotated by crowd-sourcing workers and contains many obvious errors (more details in §4); iSTS (Agirre et al., 2016) and SPADE/ESPADA (Arase and Tsujii, 2018, 2020) were annotated based on chunking and parsing results, which may restrict the granularity and flexibility of the alignments.\nOur experimental results show that the proposed semi-Markov CRF model achieves state-of-the-art performance with higher precision, in comparison to the previous monolingual word alignment models (Yao et al., 2013a,b; Sultan et al., 2014), as well as another very competitive span-based neural model (Nagata et al., 2020) that had previously only applied to bilingual data. Our model exceeds 90% F1 in the in-domain evaluation and also has very good generalizability on three out-of-domain datasets. We present a detailed ablation and error analysis to better understand the performance gains. Finally, we demonstrate the utility of monolingual word alignment in two downstream applications, namely automatic text simplification and sentence pair classification."
    }, {
      "heading" : "2 Related Work",
      "text" : "Word alignment has a long history and was first proposed for statistical machine translation. The most representative ones are the IBM models(Brown et al., 1993), which are a sequence of unsupervised models with increased complexity and implemented the GIZA++ toolkit (Och and Ney, 2003). Many more works followed, such as FastAlign (Dyer et al., 2013). Dyer et al. (2011) also used a globally normalized log-linear model for discriminative word alignment. Bansal et al. (2011) proposed a hidden semi-Markov model to handle both continuous and noncontinuous phrase alignment. These statistical methods promoted the development of monolingual word alignment (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012). Yao et al. (2013a) proposed a CRF aligner following (Blunsom and Cohn, 2006), then extended it to a semiCRF model for phrase-level alignments (Yao et al., 2013b). Sultan et al. (2014) designed a simple system with heuristic rules based on word similarity and contextual evidence.\nNeural methods have been explored in the past decade primarily for bilingual word alignment. Some early attempts (Yang et al., 2013; Tamura et al., 2014) did not match the performance of GIZA++, but recent Transformer-based models started to outperform. Garg et al. (2019) proposed a multi-task framework for machine translation and word alignment, while Zenkel et al. (2020) designed an alignment layer on top of Transformer for machine translation. Both can be trained without word alignment annotations but\nrely on millions of bilingual sentence pairs. As for supervised methods, Stengel-Eskin et al. (2019) extracted representations from the Transformerbased MT system, then used convolutional neural network to incorporate neighboring words for alignment. Nagata et al. (2020) proposed a span prediction method and formulated bilingual word alignment as a SQuAD-style question answering task, then solved it by fine-tuning multilingual BERT. We adapt their method to monolingual word alignment as a new state-of-the-art baseline (§5.1). Some monolingual neural models have different settings from this work. Ouyang and McKeown (2019) introduced pointer networks for long, sentence- or clause-level alignments. Arase and Tsujii (2017, 2020) utilized constituency parsers for compositional and non-compositional phrase alignments. Culkin et al. (2021) considered span alignment for FrameNet (Baker et al., 1998) annotations and treated each span pair as independent prediction."
    }, {
      "heading" : "3 Neural Semi-CRF Alignment Model",
      "text" : "In this section, we first describe the problem formulation for monolingual word alignment, then present the architecture of our neural semi-CRF word alignment model (Figure 2)."
    }, {
      "heading" : "3.1 Problem Formulation",
      "text" : "We formulate word alignment as a sequence tagging problem following previous works (Blunsom and Cohn, 2006; Yao et al., 2013b). Given a source sentence s and a target sentence t of the same language, the span alignment a consists of a sequence of tuples (i, j), which indicates that span si in the source sentence is aligned with span tj in the target sentence. More specifically, ai = j means source span si is aligned with target span tj . We consider all spans up to a maximum length of D words. Given a source span si of d (d ≤ D) words [swbi , s w bi+1\n, ..., swbi+d−1], where bi is the beginning word index, its corresponding label ai means every word within the span si is aligned to the target span tai . That is, the word-level alignments awbi , a w bi+1\n, ..., awbi+d−1 have the same value j. We use aw to denote the label sequence of alignments between words and swbi to denote the bith word in the source sentence. There might be cases where span si is not aligned to any words in the target sentence, then ai = [NULL]. When D ≥ 2, the Markov property would no longer hold for word-\nlevel alignment labels, but for span-level labels. That is, ai depends on awbi−1, the position in the target sentence where the source span (with ending word index bi − 1) that precedes the current span si is aligned to. We therefore design a discriminative model using semi-Markov conditional random fields (Sarawagi and Cohen, 2005) to segment the source sentence and find the best span alignment, which we present below. One unique aspect of our semi-Markov CRF model is that it utilizes a varied set of labels for each sentence pair."
    }, {
      "heading" : "3.2 Our Model",
      "text" : "The conditional probability of alignment a given a sentence pair s and t is defined as follows:\np(a|s, t) = e ψ(a,s,t)∑\na′∈A e ψ(a′,s,t) (1)\nwhere the set A denotes all possible alignments between the two sentences. The potential function ψ can be decomposed into:\nψ(a, s, t) = ∑ i υ(si,tai) + τ(a w bi−1, ai)+\ncost(a,a∗) (2)\nwhere i denotes the indices of a subset of source spans that are involved in the alignment a; a∗ represents the gold alignment sequence at spanlevel. The potential function ψ consists of three elements, of which the first two compose negative log-likelihood loss: the span interaction function υ, which accounts the similarity between a source span and a target span; the Markov transition function τ , which models the transition of alignment labels between adjacent source spans; the cost is implemented with Hamming loss to encourage the predicted alignment sequence to be\nconsistent with gold labels. Function υ and τ are implemented as two neural components which we describe below.\nSpan Representation Layer. First, source and target sentences are concatenated together and encoded by the pre-trained SpanBERT (Joshi et al., 2020) model. The hidden representations in the last layer of the encoder are extracted for each WordPiece token, then averaged to form the word representations. Following previous work (Joshi et al., 2020), the span is represented by a selfattention vector computed over the representations of each word within the span, concatenated with the Transformer output states of two endpoints.\nSpan Interaction Layer. The semantic similarity score between source span si and target span tj is calculated by a 2-layer feed-forward neural network FFsim with Parametric Relu (PReLU) (He et al., 2015),2 after applying layer normalization to each span representation:\nυ(si, tj) = FFsim([hsi ;h t j ; |hsi − htj |;hsi ◦ htj ]) (3)\nwhere [; ] is concatenation and ◦ is element-wise multiplication. We use hsi and h t j to denote the representation of source span si and target span tj , respectively.\nMarkov Transition Layer. Monolingual word alignment moves along the diagonal direction in most cases. To incorporate this intuition, we propose a scoring function to model the transition between the adjacent alignment labels awbi−1 and ai. The main feature we use is the distance between the beginning index of current target span and the\n2We also compared ReLU and GeLU, and found PReLU works slightly better.\nend index of the target span that the prior source span is aligned to. The distance is binned into 1 of 13 buckets with the following boundaries [-11, -6, -4, -3, -2, -1, 0, 1, 2, 3, 5, 10], and each bucket is encoded by a 128-dim randomly initialized embedding. It is then transformed into a real-value score by a 1-layer feed forward neural network.\nTraining and Inference. During training, we minimizes the negative log-likelihood of the gold alignment a∗, and the model is trained from both directions (source to target, target to source):∑\n(s,t,a∗)\n−log p(a∗s2t|s, t)− log p(a∗t2s|t, s) (4)\nwhere a∗s2t and a ∗ t2s represent the gold alignment labels from both directions. During inference, we use the Viterbi algorithm to find the optimal alignment. There are different strategies to merge the outputs from two directions, including intersection, union, grow-diag (Koehn, 2009), bidi-avg (Nagata et al., 2020), etc. It can be seen as a hyper-parameter and decided based on the dev set. In this work, we use intersection in our semi-CRF model for all experiments."
    }, {
      "heading" : "3.3 Implementation Details",
      "text" : "We implement our model in PyTorch (Paszke et al., 2017). We use the Adam optimizer and set both the learning rate and weight decay as 1e-5. We set the maximum span size to 3 for our neural semi-CRF model, which can converge within 5 epochs. The neural semi-CRF model has ∼2 hour training time per epoch for MultiMWA-MTRef, measured on a single GeForce GTX 1080 Ti GPU."
    }, {
      "heading" : "4 A Multi-Genre Benchmark for Monolingual Word Alignment",
      "text" : "In this section, we present the manually annotated Multi-genre Monolingual Word Alignment (MultiMWA) benchmark that consists of four datasets of different text genres. As summarized in Table 1, our new benchmark is the largest to date and of higher quality compared to existing datasets. In contrast to iSTS (Agirre et al., 2016) and SPADE/ESPADA (Arase and Tsujii, 2018, 2020), our annotation does not rely on external chunking or parsing that may introduce errors or restrict the granularity and flexibility. Our benchmark contains both token alignments and a significant portion of phrase alignments as they are semantically\nequivalent as a whole. Our benchmark also contains a large portion of semantically similar but not strictly equivalent sentence pairs, which are common in text-to-text generation tasks and thus important for evaluating the monolingual word alignment models under this realistic setting.\nFor all four datasets, we closely follow the standard 6-page annotation guideline3 from (CallisonBurch et al., 2006) and further extend it to improve the phrase-level annotation consistency (more details in Appendix B.1). We describe each of the four datasets below.\nMultiMWA-MTRef. We create this dataset by annotating 3,998 sentence pairs from the MTReference (Yao, 2014), which are human references used in a machine translation task. The original labels in MTReference were annotated by crowd-sourcing workers on Amazon Mechanical Turk following the guideline from (CallisonBurch et al., 2006). In an early pilot study, we discovered that these crowd-sourced annotations are noisy and contain many obvious errors. It only gets 73.6/96.3/83.4 for Precision/Recall/F1 on a random sample of 100 sentence pairs, when compared to the labels we manually corrected.\nTo address the lack of reliable annotation, we hire two in-house annotators to correct the original labels using GoldAlign4 (Gokcen et al., 2016), an annotation tool for monolingual word alignment. Both annotators have linguistic background and extensive NLP annotation experience. We provide a three-hour training session to the the annotators, during which they are asked to align 50 sentence pairs and discuss until consensus. Following previous work, we calculate the inter-annotator agreement as 84.2 of F1 score for token-level nonidentical alignments by comparing one annotator’s annotation against the other’s. The alignments between identical words are usually easy for human annotators. After merging the the labels from both annotators, we create a new split of 2398/800/800 for train/dev/test set. To ensure the quality, an adjudicator further exams the dev and test sets and constructs the final labels.\nMultiMWA-Newsela. Newsela corpus (Xu et al., 2015b) consists of 1,932 English news articles and their simplified versions written by\n3http://www.cs.jhu.edu/˜ccb/ publications/paraphrase_guidelines.pdf\n4https://github.com/ajdagokcen/ goldalign-repo\nprofessional editors. It has been widely used in text simplification research (Xu et al., 2016; Zhang and Lapata, 2017; Zhong et al., 2020). We randomly select 500 complex-simple sentence pairs from the test set of Newsela-Auto (Jiang et al., 2020),5 which is the newest sentencealigned version of Newsela. 214 of these 500 pairs contain sentence splitting. An in-house annotator6 labels the word alignment by correcting the outputs from GIZA++ (Och and Ney, 2003).\nMultiMWA-arXiv. The arXiv7 is an openaccess platform that stores more than 1.7 million research papers with their historical versions. It has been used to study paraphrase generation (Dong et al., 2021) and statement strength (Tan and Lee, 2014). We first download the LATEX source code for 750 randomly sampled papers and their historical versions, then use OpenDetex8 package to extract plain text from them. We use the trained neural CRF sentence alignment model (Jiang et al., 2020) to align sentences between different versions of the papers and sample 200 nonidentical aligned sentence pairs for further annotation. The word alignment is annotated in a similar procedure to that of the MultiMWA-Wiki.\nMultiMWA-Wiki. Wikipedia has been widely used in text-to-text tasks, including text simpli-\n5More specifically, we sample from the exact test set used in Table 2 in Maddela et al. (2021).\n6This annotator has annotated MultiMWA-MTRef. 7https://arxiv.org/ 8https://github.com/pkubowicz/\nopendetex\nfication (Jiang et al., 2020), sentence splitting (Botha et al., 2018), and neutralizing bias language (Pryzant et al., 2020). We follow the method in (Pryzant et al., 2020) to extract parallel sentences from Wikipedia revision history dump (dated 01/01/2021) and randomly sample 4,099 sentence pairs for further annotation. We first use an earlier version of our neural semi-CRF word aligner (§3) to automatically align words for the sentence pairs, then ask two in-house annotators to correct the aligner’s outputs. The interannotator agreement is 98.1 at token-level measured by F1.9 We split the data into 2514/533/1052 sentence pairs for train/dev/test sets."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we present both in-domain and outof-domain evaluations for different word alignment models on our MultiWMA benchmark. We also provide a detailed error analysis of our neural semi-CRF model and an ablation study to analyze the importance of each component."
    }, {
      "heading" : "5.1 Baselines",
      "text" : "We introduce a novel state-of-the-art baseline by adapting the QA-based method in (Nagata et al., 2020), which has not previously applied to monolingual word alignment but only bilingual word alignment. This method treats the word alignment problem as a collection of independent predictions\n9The inter-annotator agreement is much higher compared to that of MultiMWA-MTRef, as the parallel sentences extracted from Wikipedia revision history have more overlap.\nfrom every token in the source sentence to a span in the target sentence, which is then solved by finetuning multilingual BERT (Devlin et al., 2019) similarly as for SQuAD-style question answering task. Taking the sentence pair in Figure 1 as an example, the word to be aligned is marked by ¶ in the source sentence and concatenated with the entire target sentence to form the input as “With Canadian · · · ¶conduct¶ · · · his model. Lkoyd performed · · · his model. ”. A span prediction model based on fine-tuning multilingual BERT is then expected to extract performed from the target sentence. The predictions from both directions (source to target, target to source) are symmetrized to produce the final alignment, using a probability threshold of 0.4 instead of the typical 0.5.\nWe change to use standard BERT in this model for monolingual alignment and find that the 0.4 threshold chosen by Nagata et al. (2020) is almost optimal in maximizing the F1 score on our MultiMWA-MTRef dataset. This QA-based method alone outperforms all existing models for monolingual word alignment, including: Jacana-\nToken aligner (Yao et al., 2013a), which is a CRF model using hand-crafted features and external resources; JacanaPhrase aligner (Yao et al., 2013b), which is a semi-CRF model relying on feature templates and external resources; PipelineAligner (Sultan et al., 2014), which is a pipeline system that utilizes word similarity and contextual information with heuristic algorithms. We also create a variation of our model, a Neural CRF aligner, in which all modules remain the same but the max span length is set to 1, to evaluate the benefits of span-based alignments."
    }, {
      "heading" : "5.2 Experimental Results",
      "text" : "Following the literature (Thadani et al., 2012; Yao et al., 2013a,b), we present results under both Sure and Sure + Poss settings for the MultiMWA-MTRef dataset. Sure+ Poss setting includes all the annotated alignments, and Sure only contains a subset of them which are agreed by multiple annotators. We consider Sure+Poss as the default setting for all the other three datasets.\nThe in-domain evaluation results are shown in\nTable 2. The neural models are working remarkably well in comparison to the non-neural methods, especially as measured by Exact Matches (EM). On both MTRef and Wiki datasets, our neural semi-CRF model achieves the best F1 and EM. QA-based aligner also has competitive performance with strong recall, however, its precision is lower compared to our model. It is worthy to note that our model has a modular design, and can be more easily adjusted than QA-based method to suit different datasets and downstream tasks.\nTable 3 presents the out-of-domain evaluation results. Our neural models achieve the best performance across all three datasets. This demonstrates the generalization ability of our model, which can be useful in the downstream applications."
    }, {
      "heading" : "5.3 Ablation Study",
      "text" : "Table 4 shows the ablation study for our neural semi-CRF model. F1 and EM drops by 1.3 and 4.4 points respectively after replacing SpanBERT with BERT, indicating the importance of optimized pre-trained representations. Markov transition layer contributes mainly to the alignment accuracy (EM). We have experimented with different strategies to merge the outputs from two directions: intersection yields better precision, growdiag and union bias towards recall. Leveraging the span interaction matrix generated by our model (details in §3.2), we design a simple postprocessing rule to extend the phrasal alignment to spans that are longer than 3 tokens. Adjacent target words are gradually included if they have very high semantic similarity with the same source span. This rule further improves recall and achieves the best F1 on the MultiMWA-MTRef."
    }, {
      "heading" : "5.4 Error Analysis",
      "text" : "We sample 50 sentence pairs from the dev set of MultiMWA-MTRef and analyze the errors under Sure+Poss setup.10 Figure 4 shows how the performance of different alignment models would improve, if we resolve each of the 7 types of errors. We discuss the categorization of errors and their breakdown percentages below:\nPhrase Boundary (58.6%). The phrase boundary error (see 3 in Figure 3 for an example) is the most prominent error in all models, attributing 7.6 points of F1 for JacanaPhrase, 5.7 for QA aligner, and 4.7 for neural semi-CRF aligner. For another example, instead of 3x2 alignment funds for research ↔ research funding, our model captures two 1x1 alignments, funds ↔ funding and research ↔ research. This is largely due to the fact that alignments are not limited to linguistic phrases (e.g., noun phrases, verb phrases, etc.), but rather, include non-linguistic phrases. It could also be challenging to handle longer spans, such as keep his position ↔ protect himself from being removed (more on this in Appendix B.2). Although we use SpanBERT for better phrase representation, there is still room for improvement.\nFunction Words (19.1%). Function words can be tricky to align when rewording and reordering happens, such as 2 . Adding on the complexity, same function word may appear more than once in one sentence. This type of error is common in all the models we experiment with. It attributes 4.7 points of F1 for JacanaPhrase, 1.3 for QA aligner, and 1.5 for our neural semi-CRF aligner.\nContent Words (14.2%). Similar to function words, content words (e.g., security bureau ↔ defense ministry) can also be falsely aligned or missed, but the difference between neural and nonneural model is much more significant. This error type attributes 7.7 points of F1 score for Jacana aligner, but only 1.1 and 0.8 for neural semi-CRF aligner and QA aligner, respectively.\nContext Implication (5.6%). Some words or phrases that are not strictly semantically equivalent can also be aligned if they appear in a similar context. For example, given the source sentence\n10The strict Sure only labels exclude many alignments that are critical for certain applications, such as label projection. We thus focus on the Sure+Poss labels for error analysis.\n‘Gaza international airport was put into operation the day before’ and the target sentence ‘The airport began operations one day before’, the phrase pair was put into ↔ began can be aligned. This type is related to 2.8 F1 score improvement for Jacana aligner, but only 0.4 and 0.2 for neural semiCRF and QA-based aligners, respectively.\nDebatable Labels (1.9%). Word alignment annotation can be subjective sometimes. Take phrase alignment two days of ↔ a two-day for example, it can go either way to include the function word ‘a’ in the alignment, or not.\nName Variations (0.6%). While our neural semi-CRF model is designed to handle spelling variations or name abbreviations, it fails sometimes as shown by 1 in Figure 3 as an example. Some cases can be very difficult, such as SAWS↔ the state’s supervision and control bureau of safe production, where SAWS stands for State Administration of Work Safety.\nSkip Alignment (0.0%). Non-contiguous tokens can be aligned to the same target token or phrase (e.g., owes ... to↔ is a result of), posing a challenging situation for monolingual word aligners. However, this error is rare, as only 0.6% of all alignments in MTRef dev set are discontinuous."
    }, {
      "heading" : "6 Downstream Applications",
      "text" : "In this section, we apply our monolingual word aligner to some downstream applications, including both generation and understanding tasks."
    }, {
      "heading" : "6.1 Automatic Text Simplification",
      "text" : "Text simplification aims to improve the readability of text by rewriting complex sentences with simpler language. We propose to incorporate word alignment information into the state-of-the-art EditNTS model (Dong et al., 2019) to explicitly learn the edit operations, including addition, deletion and paraphrase. The EditNTS model uses a neural programmer-interpreter architecture, which derives the ADD, KEEP and DELETE operation sequence based on the edit-distance measurements during training time. We instead construct this edit sequence based on the neural semi-CRF aligner’s outputs (trained on MTRefSure+Poss) with an additional REPLACE tag to train the EditNTS model (more details in Appendix A).\nTable 5 presents the text simplification results on two benchmark datasets, Newsela-auto and Wikipedia-auto (Jiang et al., 2020), where we improve the SARI score (Xu et al., 2016) by 0.9 and 0.6, respectively. The SARI score averages the F1/precision of n-grams inserted (add), kept (keep) and deleted (del) when compared to human references. We also calculate the BLEU score with respect to the input (s-BL), the percentage of new words (%new) added, and the percentage of system outputs being identical to the input (%eq) to show the paraphrasing capability. We manually inspect 50 sentences sampled from Newselaauto test set and find that both models (EditNTS and EditNTS+Aligner) generate the same output for 10 sentences. For the remaining 40 sentences, the original EditNTS only attempts to paraphrase 4 times (2 are good). Our modified model (EditNTS+Aligner) is more aggressive, generating 25 paraphrases (11 are good). With the help of word aligner, the modified model also produces a higher number of good deletions (20 vs. 13) and a lower number of bad deletions (6 vs. 12), which is consistent with the better keep and del scores."
    }, {
      "heading" : "6.2 Sentence Pair Modeling",
      "text" : "We can utilize our neural aligner in sentence pair classification tasks (Lan and Xu, 2018), adding conditional alignment probability p(a|s, t) as an extra feature. We concatenate it with the [CLS] representation in fine-tuned BERT and apply the softmax layer for prediction. We experiment with on different datsets for various tasks, including: natural language inference on SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), SICK (Marelli et al., 2014), and RTE (Giampiccolo et al., 2007) from the GLUE benchmark (Wang et al., 2018); semantic textual similarity on STS-B (Cer et al., 2017) and STS14 (Agirre et al., 2014); question answering on WikiQA (Yang et al., 2015) and TrecQA (Wang et al., 2007); paraphrase identification on MRPC (Dolan and Brockett, 2005), URL (Lan et al., 2017), PIT (Xu et al., 2015a), and QQP (Iyer et al., 2017).\nWe implement the fine-tuned BERTbase model using Huggingface’s library (Wolf et al., 2019). Table 6 shows performance improvement on small (2k-15k) datasets, which include SICK, STS-B, MRPC, RTE, WikiQA, and PIT, but little or no improvement on large (40k-550k) datasets, such as SNLI, MNLI, and QQP. We hypothesize that the Transformer model can potentially learn the latent word alignment through self-attentions, but not as effectively for small data size."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this work, we present the first neural semiCRF word alignment model which achieves competitive performance on both in-domain and outof-domain evaluations. We also create a manually annotated Multi-Genre Monolingual Word Alignment (MultiMWA) benchmark which is the largest and of higher quality compared to existing datasets."
    }, {
      "heading" : "Acknowledgement",
      "text" : "We thank Yang Chen, Sarthak Garg, and anonymous reviewers for their helpful comments. We also thank Sarah Flanagan, Yang Zhong, Panya Bhinder, Kenneth Kannampully for helping with data annotation. This research is supported in part by the NSF awards IIS-2055699, ODNI and IARPA via the BETTER program contract 19051600004, ARO and DARPA via the SocialSim program contract W911NF-17-C-0095, and Criteo Faculty Research Award to Wei Xu. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of NSF, ODNI, IARPA, ARO, DARPA or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein."
    }, {
      "heading" : "A EditNTS with Aligner",
      "text" : "The original EditNTS model constructs expert program with the shortest edit path from complex sentence to simple sentence, specifically, it calculates the Levenshtein distances without substitutions and recovers the edit path with three labels: ADD, KEEP and DEL. Since edit distance relies on word identity to match the sentence pair, it cannot produce lexical paraphrases (e.g. conduct ↔ performed and simulations↔ experiments in Figure 1,). The final edit sequence will mix paraphrase words (performed and experiments) and normal added words (successful) together under the same ADD label. In order to differentiate these two types of added words, we introduced special tags (REPLACE-S and REPLACE-E) to refer to lexical paraphrases specifically. During the edit label construction process, after checking the word pair identity for KEEP label, we additionally check whether they are aligned by our neural semi-CRF aligner, if so, we produce REPLACES/E tags, otherwise we do normal ADD/DEL tags. See Table 7 for a specific example. Word alignment can arbitrarily align any words in the target sentence, this can break the sequential de-\npendency of the edit labels, we therefore discard some lexical paraphrases to guarantee such propriety (conduct↔ performed in Table 7).\nIn order to show the effectiveness of our modified model, we compared two more versions of EditNTS in Table 8: EditNTS (original) + Aligner, where we directly add word alignment information to the original EditNTS model without any REPLACE tags; EditNTS (new), where we keep the REPLACE tags but don’t use any word alignments. The results show that EditNTS model with REPLACE tags can improve the performance, but it is not significant. After adding the word alignment information, we can further improve the SARI score significantly, which can demonstrate the effectiveness of our modified EditNTS with aligner."
    }, {
      "heading" : "B More Details for MultiMWA Benchmark",
      "text" : "B.1 Updated Annotation Guideline After the first round of annotation, we discovery that the definition of phrasal alignment can be ambiguous, which will hinder the development and error analysis for word alignemnt models. There-\nfore, we further extend the standard 6-page annotation guideline11 from (Callison-Burch et al., 2006) to cover three linguistics phenomena to improve the phrase-level annotation consistency.\n• “a/an/the + noun” should be aligned together with noun if both nouns are same. • noun1 should be only aligned to noun1 in the\nphrase “noun1 and noun2”. • noun should be only aligned to noun in the\n“adjective + noun” phrase.\nUtilizing the constituency parser implemented in the AllenNLP package (Gardner et al., 2018), we first write a script to implement these rules and apply them to all the training/dev/test sets of MultiMWA-MTRef. Then, we manually go through both dev and test sets to further ensure the annotation consistency.\nB.2 Statistics of Alignment Shape We also analyze the shape of alignment in each dataset, and the statistics can be found in Table 9. Statistical result showes that the dev and test of MultiMWA-MTRef contain a similar portion of phrasal alignment, and less than the training set. There even exists 1×10 alignment annotations in MultiMWA-MTRef, which are actually correct based on our manual inspection. Both MultiMWA-Newsela and MultiMWA-arXiv contain significantly larger portion of 1×1 alignment, especially the latter one contains only 3.2% of phrasal alignment.\n11http://www.cs.jhu.edu/˜ccb/ publications/paraphrase_guidelines.pdf\n#p ai\nrs 1×\n1 %\nof 1×\n1 1×\n2 1×\n3 1×\n4 1×\n5 1×\n6 1×\n7 1×\n8 1×\n9 1×\n10 2×\n2 2×\n3 2×\n4 2×\n5 2×\n6 2×\n7 2×\n8 2×\n9 2×\n10 3×\n3 3×\n4 3×\n5 3×\n6 3×\n7 3×\n8 3×\n9 4×\n4 4×\n5 4×\n6 4×\n8 4×\n9 5×\n5 5×\n7 5×\n8 6×\n6\nM ul\ntiM W\nA -M\nT R\nef Tr\nai n\n23 98\n30 37\n1 59\n.5 0%\n79 56\n34 23\n16 48\n53 0\n19 8\n11 2\n48 0\n10 10\n48 15\n36 93\n6 27\n0 20\n4 84\n16 18\n20 54\n9 61\n2 36\n0 19\n8 63\n24 27\n17 6\n18 0\n72 32\n72 25\n70 80\n72\nD ev\n80 0\n10 35\n0 65\n.6 5%\n24 34\n87 6\n40 8\n10 0\n30 14\n16 9\n10 30\n8 47\n4 14\n4 60\n48 0\n0 0\n0 13\n5 15\n6 10\n5 0\n0 0\n0 48\n40 0\n0 0\n0 0\n0 0\nTe st\n80 0\n10 32\n9 63\n.1 7%\n26 50\n10 92\n40 4\n17 5\n36 28\n24 0\n0 31\n2 51\n6 20\n0 50\n72 0\n0 0\n0 81\n14 4\n75 36\n0 0\n0 64\n40 24\n0 0\n0 0\n0 0\nM ul\ntiM W\nA -N\new se\nla 50\n0 84\n64 74\n.8 2%\n83 8\n48 3\n18 0\n80 48\n35 8\n36 0\n15 2\n27 6\n14 4\n40 24\n28 16\n18 0\n11 7\n18 0\n45 18\n63 0\n0 0\n20 0\n0 0\n0 0\n0 0\nM ul\ntiM W\nA -a\nrX iv\n20 0\n50 20\n96 .8\n0% 78\n30 8\n0 0\n0 0\n0 0\n8 12\n0 0\n0 0\n0 18\n0 0\n12 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0\nTa bl\ne 9:\nSt at\nis tic\ns of\nal ig\nnm en\nts ha\npe s\nin ea\nch da\nta se\nt. E\nac h\nnu m\nbe rr\nep re\nse nt\ns ho\nw m\nan y\nw or\nd al\nig nm\nen ts\nar e\nin cl\nud ed\nfo rp\nhr as\nal al\nig nm\nen tw\nith sp\nec ifi\nc sh\nap e.\nFo re\nxa m\npl e,\non e\n2× 3\nph ra\nsa la\nlig nm\nen tw\nill co\nnt ri\nbu te\nto si\nx w\nor d\nal ig\nnm en\nts .%\nof 1×\n1 is\nca lc\nul at\ned by\n1× 1\nov er\nth e\nsu m\nof th\ne ro\nw ."
    } ],
    "references" : [ {
      "title" : "Semeval-2014 task 10: Multilingual semantic textual similarity",
      "author" : [ "Eneko Agirre", "Carmen Banea", "Claire Cardie", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Weiwei Guo", "Rada Mihalcea", "German Rigau", "Janyce Wiebe." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Agirre et al\\.,? 2014",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2014
    }, {
      "title" : "Semeval-2016 task 2: Interpretable semantic textual similarity",
      "author" : [ "Eneko Agirre", "Aitor Gonzalez Agirre", "Inigo Lopez-Gazpio", "Montserrat Maritxalar", "German Rigau Claramunt", "Larraitz Uria." ],
      "venue" : "Proceedings of the 10th International",
      "citeRegEx" : "Agirre et al\\.,? 2016",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2016
    }, {
      "title" : "Monolingual phrase alignment on parse forests",
      "author" : [ "Yuki Arase", "Junichi Tsujii." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Arase and Tsujii.,? 2017",
      "shortCiteRegEx" : "Arase and Tsujii.",
      "year" : 2017
    }, {
      "title" : "SPADE: Evaluation dataset for monolingual phrase alignment",
      "author" : [ "Yuki Arase", "Junichi Tsujii." ],
      "venue" : "Proceedings of the International Conference on Language Resources and Evaluation (LREC).",
      "citeRegEx" : "Arase and Tsujii.,? 2018",
      "shortCiteRegEx" : "Arase and Tsujii.",
      "year" : 2018
    }, {
      "title" : "Compositional phrase alignment and beyond",
      "author" : [ "Yuki Arase", "Jun’ichi Tsujii" ],
      "venue" : "In Proceedings of Empirical Methods in Natural Language Processing (EMNLP)",
      "citeRegEx" : "Arase and Tsujii.,? \\Q2020\\E",
      "shortCiteRegEx" : "Arase and Tsujii.",
      "year" : 2020
    }, {
      "title" : "The berkeley framenet project",
      "author" : [ "Collin F Baker", "Charles J Fillmore", "John B Lowe." ],
      "venue" : "36th Annual Meeting of the Association for Computational Linguistics (ACL) and 17th International Conference on Computational Linguistics (COLING).",
      "citeRegEx" : "Baker et al\\.,? 1998",
      "shortCiteRegEx" : "Baker et al\\.",
      "year" : 1998
    }, {
      "title" : "Gappy phrasal alignment by agreement",
      "author" : [ "Mohit Bansal", "Chris Quirk", "Robert Moore." ],
      "venue" : "Proceedings of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Bansal et al\\.,? 2011",
      "shortCiteRegEx" : "Bansal et al\\.",
      "year" : 2011
    }, {
      "title" : "Discriminative word alignment with conditional random fields",
      "author" : [ "Phil Blunsom", "Trevor Cohn." ],
      "venue" : "Proceedings of the 21st International Conference on Computational Linguistics (COLING) and the 44th annual meeting of the Association for Compu-",
      "citeRegEx" : "Blunsom and Cohn.,? 2006",
      "shortCiteRegEx" : "Blunsom and Cohn.",
      "year" : 2006
    }, {
      "title" : "Learning to split and rephrase from Wikipedia edit history",
      "author" : [ "Jan A. Botha", "Manaal Faruqui", "John Alex", "Jason Baldridge", "Dipanjan Das." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Botha et al\\.,? 2018",
      "shortCiteRegEx" : "Botha et al\\.",
      "year" : 2018
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Aligning the rte 2006 corpus",
      "author" : [ "Chris Brockett." ],
      "venue" : "Technical Report MSR-TR-2007-77, Microsoft Research.",
      "citeRegEx" : "Brockett.,? 2007",
      "shortCiteRegEx" : "Brockett.",
      "year" : 2007
    }, {
      "title" : "The mathematics of statistical machine translation",
      "author" : [ "Peter F. Brown", "Stephen A. Della-Pietra", "Vincent J. Della-Pietra", "Robert L. Mercer." ],
      "venue" : "Computational Linguistics (CL).",
      "citeRegEx" : "Brown et al\\.,? 1993",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 1993
    }, {
      "title" : "Annotation guidelines for paraphrase alignment",
      "author" : [ "Chris Callison-Burch", "Trevor Cohn", "Mirella Lapata." ],
      "venue" : "Technical report.",
      "citeRegEx" : "Callison.Burch et al\\.,? 2006",
      "shortCiteRegEx" : "Callison.Burch et al\\.",
      "year" : 2006
    }, {
      "title" : "SemEval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation",
      "author" : [ "Daniel Cer", "Mona Diab", "Eneko Agirre", "Inigo LopezGazpio", "Lucia Specia." ],
      "venue" : "Proceedings of the 10th International Workshop on Semantic Eval-",
      "citeRegEx" : "Cer et al\\.,? 2017",
      "shortCiteRegEx" : "Cer et al\\.",
      "year" : 2017
    }, {
      "title" : "Iterative Paraphrastic Augmentation with Discriminative Span Alignment",
      "author" : [ "Ryan Culkin", "J. Edward Hu", "Elias Stengel-Eskin", "Guanghui Qin", "Benjamin Van Durme." ],
      "venue" : "Transactions of the Association for Computational Linguistics (TACL).",
      "citeRegEx" : "Culkin et al\\.,? 2021",
      "shortCiteRegEx" : "Culkin et al\\.",
      "year" : 2021
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatically constructing a corpus of sentential paraphrases",
      "author" : [ "William B Dolan", "Chris Brockett." ],
      "venue" : "Proceedings of the Third International Workshop on Paraphrasing (IWP).",
      "citeRegEx" : "Dolan and Brockett.,? 2005",
      "shortCiteRegEx" : "Dolan and Brockett.",
      "year" : 2005
    }, {
      "title" : "Parasci: A large scientific paraphrase dataset for longer paraphrase generation",
      "author" : [ "Qingxiu Dong", "Xiaojun Wan", "Yue Cao." ],
      "venue" : "Proceedings of the European Chapter of the Association for Computational Linguistics (EACL).",
      "citeRegEx" : "Dong et al\\.,? 2021",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2021
    }, {
      "title" : "EditNTS: An neural programmer-interpreter model for sentence simplification through explicit editing",
      "author" : [ "Yue Dong", "Zichao Li", "Mehdi Rezagholizadeh", "Jackie Chi Kit Cheung." ],
      "venue" : "Proceedings of the Association for Computational Linguis-",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "A simple, fast, and effective reparameterization of IBM model 2",
      "author" : [ "Chris Dyer", "Victor Chahuneau", "Noah A. Smith." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Dyer et al\\.,? 2013",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2013
    }, {
      "title" : "Unsupervised word alignment with arbitrary features",
      "author" : [ "Chris Dyer", "Jonathan H. Clark", "Alon Lavie", "Noah A. Smith." ],
      "venue" : "Proceedings of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Dyer et al\\.,? 2011",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2011
    }, {
      "title" : "AllenNLP: A deep semantic natural language processing platform",
      "author" : [ "Matt Gardner", "Joel Grus", "Mark Neumann", "Oyvind Tafjord", "Pradeep Dasigi", "Nelson F. Liu", "Matthew Peters", "Michael Schmitz", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of Workshop for",
      "citeRegEx" : "Gardner et al\\.,? 2018",
      "shortCiteRegEx" : "Gardner et al\\.",
      "year" : 2018
    }, {
      "title" : "Jointly learning to align and translate with transformer models",
      "author" : [ "Sarthak Garg", "Stephan Peitz", "Udhyakumar Nallasamy", "Matthias Paulik." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing and the International Joint Con-",
      "citeRegEx" : "Garg et al\\.,? 2019",
      "shortCiteRegEx" : "Garg et al\\.",
      "year" : 2019
    }, {
      "title" : "The third PASCAL recognizing textual entailment challenge",
      "author" : [ "Danilo Giampiccolo", "Bernardo Magnini", "Ido Dagan", "Bill Dolan." ],
      "venue" : "Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing.",
      "citeRegEx" : "Giampiccolo et al\\.,? 2007",
      "shortCiteRegEx" : "Giampiccolo et al\\.",
      "year" : 2007
    }, {
      "title" : "A corpus of word-aligned asked and anticipated questions in a virtual patient dialogue system",
      "author" : [ "Ajda Gokcen", "Evan Jaffe", "Johnsey Erdmann", "Michael White", "Douglas Danforth." ],
      "venue" : "Proceedings of the International Conference on Language Re-",
      "citeRegEx" : "Gokcen et al\\.,? 2016",
      "shortCiteRegEx" : "Gokcen et al\\.",
      "year" : 2016
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "Proceedings of the International Conference on Computer Vision (ICCV).",
      "citeRegEx" : "He et al\\.,? 2015",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "First Quora Dataset Release: Question Pairs",
      "author" : [ "Shankar Iyer", "Nikhil Dandekar", "Kornél Csernai." ],
      "venue" : "https://data.quora.com/First-QuoraDataset-Release-Question-Pairs.",
      "citeRegEx" : "Iyer et al\\.,? 2017",
      "shortCiteRegEx" : "Iyer et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural crf model for sentence alignment in text simplification",
      "author" : [ "Chao Jiang", "Mounica Maddela", "Wuwei Lan", "Yang Zhong", "Wei Xu." ],
      "venue" : "Proceedings of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Jiang et al\\.,? 2020",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "SpanBERT: Improving pre-training by representing and predicting spans",
      "author" : [ "Mandar Joshi", "Danqi Chen", "Yinhan Liu", "Daniel S. Weld", "Luke Zettlemoyer", "Omer Levy." ],
      "venue" : "Transactions of the Association for Computational Linguistics (TACL).",
      "citeRegEx" : "Joshi et al\\.,? 2020",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2020
    }, {
      "title" : "Statistical machine translation",
      "author" : [ "Philipp Koehn." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Koehn.,? 2009",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2009
    }, {
      "title" : "A continuously growing dataset of sentential paraphrases",
      "author" : [ "Wuwei Lan", "Siyu Qiu", "Hua He", "Wei Xu." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Lan et al\\.,? 2017",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural network models for paraphrase identification, semantic textual similarity, natural language inference, and question answering",
      "author" : [ "Wuwei Lan", "Wei Xu." ],
      "venue" : "Proceedings of International Conference on Computational Linguistics (COLING).",
      "citeRegEx" : "Lan and Xu.,? 2018",
      "shortCiteRegEx" : "Lan and Xu.",
      "year" : 2018
    }, {
      "title" : "Exploiting sentence similarities for better alignments",
      "author" : [ "Tao Li", "Vivek Srikumar." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Li and Srikumar.,? 2016",
      "shortCiteRegEx" : "Li and Srikumar.",
      "year" : 2016
    }, {
      "title" : "A phrase-based alignment model for natural language inference",
      "author" : [ "Bill MacCartney", "Michel Galley", "Christopher D Manning." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "MacCartney et al\\.,? 2008",
      "shortCiteRegEx" : "MacCartney et al\\.",
      "year" : 2008
    }, {
      "title" : "Controllable text simplification with explicit paraphrasing",
      "author" : [ "Mounica Maddela", "Fernando Alva-Manchego", "Wei Xu." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Maddela et al\\.,? 2021",
      "shortCiteRegEx" : "Maddela et al\\.",
      "year" : 2021
    }, {
      "title" : "A SICK cure for the evaluation of compositional distributional semantic models",
      "author" : [ "Marco Marelli", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli." ],
      "venue" : "Proceedings of the International Conference on",
      "citeRegEx" : "Marelli et al\\.,? 2014",
      "shortCiteRegEx" : "Marelli et al\\.",
      "year" : 2014
    }, {
      "title" : "A supervised word alignment method based on cross-language span prediction using multilingual BERT",
      "author" : [ "Masaaki Nagata", "Katsuki Chousa", "Masaaki Nishino." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Nagata et al\\.,? 2020",
      "shortCiteRegEx" : "Nagata et al\\.",
      "year" : 2020
    }, {
      "title" : "A systematic comparison of various statistical alignment models",
      "author" : [ "Franz Josef Och", "Hermann Ney." ],
      "venue" : "Computational Linguistics (CL).",
      "citeRegEx" : "Och and Ney.,? 2003",
      "shortCiteRegEx" : "Och and Ney.",
      "year" : 2003
    }, {
      "title" : "Neural network alignment for sentential paraphrases",
      "author" : [ "Jessica Ouyang", "Kathy McKeown." ],
      "venue" : "Proceedings of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Ouyang and McKeown.,? 2019",
      "shortCiteRegEx" : "Ouyang and McKeown.",
      "year" : 2019
    }, {
      "title" : "Automatic differentiation in PyTorch",
      "author" : [ "Adam Paszke", "Sam Gross", "Soumith Chintala", "Gregory Chanan", "Edward Yang", "Zachary DeVito", "Zeming Lin", "Alban Desmaison", "Luca Antiga", "Adam Lerer." ],
      "venue" : "NIPS Autodiff Workshop.",
      "citeRegEx" : "Paszke et al\\.,? 2017",
      "shortCiteRegEx" : "Paszke et al\\.",
      "year" : 2017
    }, {
      "title" : "Automatically neutralizing subjective bias in text",
      "author" : [ "Reid Pryzant", "Richard Diehl Martinez", "Nathan Dass", "Sadao Kurohashi", "Dan Jurafsky", "Diyi Yang." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI).",
      "citeRegEx" : "Pryzant et al\\.,? 2020",
      "shortCiteRegEx" : "Pryzant et al\\.",
      "year" : 2020
    }, {
      "title" : "Semimarkov conditional random fields for information extraction",
      "author" : [ "Sunita Sarawagi", "William W Cohen." ],
      "venue" : "Proceedings of Advances in Neural Information Processing Systems (NeurIPS).",
      "citeRegEx" : "Sarawagi and Cohen.,? 2005",
      "shortCiteRegEx" : "Sarawagi and Cohen.",
      "year" : 2005
    }, {
      "title" : "A discriminative neural model for cross-lingual word alignment",
      "author" : [ "Elias Stengel-Eskin", "Tzu-ray Su", "Matt Post", "Benjamin Van Durme." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing and the International Joint Con-",
      "citeRegEx" : "Stengel.Eskin et al\\.,? 2019",
      "shortCiteRegEx" : "Stengel.Eskin et al\\.",
      "year" : 2019
    }, {
      "title" : "Back to basics for monolingual alignment: Exploiting word similarity and contextual evidence",
      "author" : [ "Md Arafat Sultan", "Steven Bethard", "Tamara Sumner." ],
      "venue" : "Transactions of the Association for Computational Linguistics (TACL).",
      "citeRegEx" : "Sultan et al\\.,? 2014",
      "shortCiteRegEx" : "Sultan et al\\.",
      "year" : 2014
    }, {
      "title" : "Recurrent neural networks for word alignment model",
      "author" : [ "Akihiro Tamura", "Taro Watanabe", "Eiichiro Sumita." ],
      "venue" : "Proceedings of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Tamura et al\\.,? 2014",
      "shortCiteRegEx" : "Tamura et al\\.",
      "year" : 2014
    }, {
      "title" : "A corpus of sentence-level revisions in academic writing: A step towards understanding statement strength in communication",
      "author" : [ "Chenhao Tan", "Lillian Lee." ],
      "venue" : "Proceedings of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Tan and Lee.,? 2014",
      "shortCiteRegEx" : "Tan and Lee.",
      "year" : 2014
    }, {
      "title" : "A joint phrasal and dependency model for paraphrase alignment",
      "author" : [ "Kapil Thadani", "Scott Martin", "Michael White." ],
      "venue" : "Proceedings of International Conference on Computational Linguistics (COLING).",
      "citeRegEx" : "Thadani et al\\.,? 2012",
      "shortCiteRegEx" : "Thadani et al\\.",
      "year" : 2012
    }, {
      "title" : "Optimal and syntactically-informed decoding for monolingual phrase-based alignment",
      "author" : [ "Kapil Thadani", "Kathleen McKeown." ],
      "venue" : "Proceedings of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Thadani and McKeown.,? 2011",
      "shortCiteRegEx" : "Thadani and McKeown.",
      "year" : 2011
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop Black-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "What is the Jeopardy model? A quasisynchronous grammar for qa",
      "author" : [ "Mengqiu Wang", "Noah A Smith", "Teruko Mitamura." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
      "citeRegEx" : "Wang et al\\.,? 2007",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2007
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel Bowman." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Williams et al\\.,? 2018",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "Huggingface’s transformers: State-of-the-art natural language processing",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "Rémi Louf", "Morgan Funtowicz" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "SemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT)",
      "author" : [ "Wei Xu", "Chris Callison-Burch", "William B. Dolan." ],
      "venue" : "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval).",
      "citeRegEx" : "Xu et al\\.,? 2015a",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Problems in current text simplification research: New data can help",
      "author" : [ "Wei Xu", "Chris Callison-Burch", "Courtney Napoles." ],
      "venue" : "Transactions of the Association for Computational Linguistics (TACL).",
      "citeRegEx" : "Xu et al\\.,? 2015b",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Optimizing statistical machine translation for text simplification",
      "author" : [ "Wei Xu", "Courtney Napoles", "Ellie Pavlick", "Quanze Chen", "Chris Callison-Burch." ],
      "venue" : "Transactions of the Association for Computational Linguistics (TACL).",
      "citeRegEx" : "Xu et al\\.,? 2016",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    }, {
      "title" : "Word alignment modeling with context dependent deep neural network",
      "author" : [ "Nan Yang", "Shujie Liu", "Mu Li", "Ming Zhou", "Nenghai Yu." ],
      "venue" : "Proceedings of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Yang et al\\.,? 2013",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2013
    }, {
      "title" : "WikiQA: A challenge dataset for open-domain question answering",
      "author" : [ "Yi Yang", "Wen-tau Yih", "Christopher Meek." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Yang et al\\.,? 2015",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "Feature-driven Question Answering With Natural Language Alignment",
      "author" : [ "Xuchen Yao." ],
      "venue" : "Ph.D. thesis, Johns Hopkins University.",
      "citeRegEx" : "Yao.,? 2014",
      "shortCiteRegEx" : "Yao.",
      "year" : 2014
    }, {
      "title" : "A lightweight and high performance monolingual word aligner",
      "author" : [ "Xuchen Yao", "Benjamin Van Durme", "Chris CallisonBurch", "Peter Clark." ],
      "venue" : "Proceedings of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Yao et al\\.,? 2013a",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2013
    }, {
      "title" : "Semi-markov phrase-based monolingual alignment",
      "author" : [ "Xuchen Yao", "Benjamin Van Durme", "Chris CallisonBurch", "Peter Clark." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Yao et al\\.,? 2013b",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2013
    }, {
      "title" : "End-to-end neural word alignment outperforms giza++",
      "author" : [ "Thomas Zenkel", "Joern Wuebker", "John DeNero." ],
      "venue" : "Proceedings of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Zenkel et al\\.,? 2020",
      "shortCiteRegEx" : "Zenkel et al\\.",
      "year" : 2020
    }, {
      "title" : "Sentence simplification with deep reinforcement learning",
      "author" : [ "Xingxing Zhang", "Mirella Lapata." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Zhang and Lapata.,? 2017",
      "shortCiteRegEx" : "Zhang and Lapata.",
      "year" : 2017
    }, {
      "title" : "Discourse level factors for sentence deletion in text simplification",
      "author" : [ "Yang Zhong", "Chao Jiang", "Wei Xu", "Junyi Jessy Li." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI).",
      "citeRegEx" : "Zhong et al\\.,? 2020",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 32,
      "context" : "It is useful for improving the interpretability in natural language understanding tasks, including semantic textual similarity (Li and Srikumar, 2016) and question answering (Yao, 2014).",
      "startOffset" : 127,
      "endOffset" : 150
    }, {
      "referenceID" : 57,
      "context" : "It is useful for improving the interpretability in natural language understanding tasks, including semantic textual similarity (Li and Srikumar, 2016) and question answering (Yao, 2014).",
      "startOffset" : 174,
      "endOffset" : 185
    }, {
      "referenceID" : 34,
      "context" : "Monolingual word alignment can also support the analysis of human editing operations (Figure 1) and improve model performance for text-to-text generation tasks, such as text simplification (Maddela et al., 2021) and neutralizing biased language (Pryzant et al.",
      "startOffset" : 189,
      "endOffset" : 211
    }, {
      "referenceID" : 40,
      "context" : ", 2021) and neutralizing biased language (Pryzant et al., 2020).",
      "startOffset" : 41,
      "endOffset" : 63
    }, {
      "referenceID" : 14,
      "context" : "(Culkin et al., 2021) when combined with paraphrase generation.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 57,
      "context" : "It addresses the shortcomings of existing datasets for monolingual word alignment: MTReference (Yao, 2014) was annotated by crowd-sourcing workers and contains many obvious errors (more details in §4); iSTS (Agirre et al.",
      "startOffset" : 95,
      "endOffset" : 106
    }, {
      "referenceID" : 1,
      "context" : "It addresses the shortcomings of existing datasets for monolingual word alignment: MTReference (Yao, 2014) was annotated by crowd-sourcing workers and contains many obvious errors (more details in §4); iSTS (Agirre et al., 2016) and SPADE/ESPADA (Arase and Tsujii, 2018, 2020) were annotated based on chunking and parsing results, which may restrict the granularity and flexibility of the alignments.",
      "startOffset" : 207,
      "endOffset" : 228
    }, {
      "referenceID" : 43,
      "context" : "6816 Our experimental results show that the proposed semi-Markov CRF model achieves state-of-the-art performance with higher precision, in comparison to the previous monolingual word alignment models (Yao et al., 2013a,b; Sultan et al., 2014), as well as another very competitive span-based neural model (Nagata et al.",
      "startOffset" : 200,
      "endOffset" : 242
    }, {
      "referenceID" : 36,
      "context" : ", 2014), as well as another very competitive span-based neural model (Nagata et al., 2020) that had previously only applied to bilingual data.",
      "startOffset" : 69,
      "endOffset" : 90
    }, {
      "referenceID" : 11,
      "context" : "The most representative ones are the IBM models(Brown et al., 1993), which are a sequence of unsupervised models with increased complexity and implemented the GIZA++ toolkit (Och and Ney, 2003).",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 37,
      "context" : ", 1993), which are a sequence of unsupervised models with increased complexity and implemented the GIZA++ toolkit (Och and Ney, 2003).",
      "startOffset" : 114,
      "endOffset" : 133
    }, {
      "referenceID" : 19,
      "context" : "Many more works followed, such as FastAlign (Dyer et al., 2013).",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 33,
      "context" : "These statistical methods promoted the development of monolingual word alignment (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012).",
      "startOffset" : 81,
      "endOffset" : 155
    }, {
      "referenceID" : 47,
      "context" : "These statistical methods promoted the development of monolingual word alignment (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012).",
      "startOffset" : 81,
      "endOffset" : 155
    }, {
      "referenceID" : 46,
      "context" : "These statistical methods promoted the development of monolingual word alignment (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012).",
      "startOffset" : 81,
      "endOffset" : 155
    }, {
      "referenceID" : 7,
      "context" : "(2013a) proposed a CRF aligner following (Blunsom and Cohn, 2006), then extended it to a semiCRF model for phrase-level alignments (Yao et al.",
      "startOffset" : 41,
      "endOffset" : 65
    }, {
      "referenceID" : 59,
      "context" : "(2013a) proposed a CRF aligner following (Blunsom and Cohn, 2006), then extended it to a semiCRF model for phrase-level alignments (Yao et al., 2013b).",
      "startOffset" : 131,
      "endOffset" : 150
    }, {
      "referenceID" : 55,
      "context" : "Some early attempts (Yang et al., 2013; Tamura et al., 2014) did not match the performance of GIZA++, but recent Transformer-based models started to outperform.",
      "startOffset" : 20,
      "endOffset" : 60
    }, {
      "referenceID" : 44,
      "context" : "Some early attempts (Yang et al., 2013; Tamura et al., 2014) did not match the performance of GIZA++, but recent Transformer-based models started to outperform.",
      "startOffset" : 20,
      "endOffset" : 60
    }, {
      "referenceID" : 5,
      "context" : "(2021) considered span alignment for FrameNet (Baker et al., 1998) annotations and treated each span pair as independent prediction.",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 7,
      "context" : "We formulate word alignment as a sequence tagging problem following previous works (Blunsom and Cohn, 2006; Yao et al., 2013b).",
      "startOffset" : 83,
      "endOffset" : 126
    }, {
      "referenceID" : 59,
      "context" : "We formulate word alignment as a sequence tagging problem following previous works (Blunsom and Cohn, 2006; Yao et al., 2013b).",
      "startOffset" : 83,
      "endOffset" : 126
    }, {
      "referenceID" : 41,
      "context" : "We therefore design a discriminative model using semi-Markov conditional random fields (Sarawagi and Cohen, 2005) to segment the source sentence and find the best span alignment, which we present below.",
      "startOffset" : 87,
      "endOffset" : 113
    }, {
      "referenceID" : 28,
      "context" : "First, source and target sentences are concatenated together and encoded by the pre-trained SpanBERT (Joshi et al., 2020) model.",
      "startOffset" : 101,
      "endOffset" : 121
    }, {
      "referenceID" : 28,
      "context" : "Following previous work (Joshi et al., 2020), the span is represented by a selfattention vector computed over the representations of each word within the span, concatenated with the Transformer output states of two endpoints.",
      "startOffset" : 24,
      "endOffset" : 44
    }, {
      "referenceID" : 25,
      "context" : "The semantic similarity score between source span si and target span tj is calculated by a 2-layer feed-forward neural network FFsim with Parametric Relu (PReLU) (He et al., 2015),2 after applying layer normalization to each span representation:",
      "startOffset" : 162,
      "endOffset" : 179
    }, {
      "referenceID" : 29,
      "context" : "There are different strategies to merge the outputs from two directions, including intersection, union, grow-diag (Koehn, 2009), bidi-avg (Nagata et al.",
      "startOffset" : 114,
      "endOffset" : 127
    }, {
      "referenceID" : 36,
      "context" : "There are different strategies to merge the outputs from two directions, including intersection, union, grow-diag (Koehn, 2009), bidi-avg (Nagata et al., 2020), etc.",
      "startOffset" : 138,
      "endOffset" : 159
    }, {
      "referenceID" : 39,
      "context" : "We implement our model in PyTorch (Paszke et al., 2017).",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 1,
      "context" : "In contrast to iSTS (Agirre et al., 2016) and SPADE/ESPADA (Arase and Tsujii, 2018, 2020), our annotation does not rely on external chunking or parsing that may introduce errors or restrict the granularity and flexibility.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 57,
      "context" : "We create this dataset by annotating 3,998 sentence pairs from the MTReference (Yao, 2014), which are human references used in a machine translation task.",
      "startOffset" : 79,
      "endOffset" : 90
    }, {
      "referenceID" : 24,
      "context" : "To address the lack of reliable annotation, we hire two in-house annotators to correct the original labels using GoldAlign4 (Gokcen et al., 2016), an annotation tool for monolingual word alignment.",
      "startOffset" : 124,
      "endOffset" : 145
    }, {
      "referenceID" : 53,
      "context" : "Newsela corpus (Xu et al., 2015b) consists of 1,932 English news articles and their simplified versions written by",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 54,
      "context" : "It has been widely used in text simplification research (Xu et al., 2016; Zhang and Lapata, 2017; Zhong et al., 2020).",
      "startOffset" : 56,
      "endOffset" : 117
    }, {
      "referenceID" : 61,
      "context" : "It has been widely used in text simplification research (Xu et al., 2016; Zhang and Lapata, 2017; Zhong et al., 2020).",
      "startOffset" : 56,
      "endOffset" : 117
    }, {
      "referenceID" : 62,
      "context" : "It has been widely used in text simplification research (Xu et al., 2016; Zhang and Lapata, 2017; Zhong et al., 2020).",
      "startOffset" : 56,
      "endOffset" : 117
    }, {
      "referenceID" : 27,
      "context" : "We randomly select 500 complex-simple sentence pairs from the test set of Newsela-Auto (Jiang et al., 2020),5 which is the newest sentencealigned version of Newsela.",
      "startOffset" : 87,
      "endOffset" : 107
    }, {
      "referenceID" : 37,
      "context" : "An in-house annotator6 labels the word alignment by correcting the outputs from GIZA++ (Och and Ney, 2003).",
      "startOffset" : 87,
      "endOffset" : 106
    }, {
      "referenceID" : 17,
      "context" : "It has been used to study paraphrase generation (Dong et al., 2021) and statement strength (Tan and Lee, 2014).",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 27,
      "context" : "We use the trained neural CRF sentence alignment model (Jiang et al., 2020) to align sentences between different versions of the papers and sample 200 nonidentical aligned sentence pairs for further annotation.",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 27,
      "context" : "opendetex fication (Jiang et al., 2020), sentence splitting (Botha et al.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 8,
      "context" : ", 2020), sentence splitting (Botha et al., 2018), and neutralizing bias language (Pryzant et al.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 40,
      "context" : ", 2018), and neutralizing bias language (Pryzant et al., 2020).",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 40,
      "context" : "We follow the method in (Pryzant et al., 2020) to extract parallel sentences from Wikipedia revision history dump (dated 01/01/2021) and randomly sample 4,099 sentence pairs for further annotation.",
      "startOffset" : 24,
      "endOffset" : 46
    }, {
      "referenceID" : 36,
      "context" : "We introduce a novel state-of-the-art baseline by adapting the QA-based method in (Nagata et al., 2020), which has not previously applied to monolingual word alignment but only bilingual word alignment.",
      "startOffset" : 82,
      "endOffset" : 103
    }, {
      "referenceID" : 15,
      "context" : "from every token in the source sentence to a span in the target sentence, which is then solved by finetuning multilingual BERT (Devlin et al., 2019) similarly as for SQuAD-style question answering task.",
      "startOffset" : 127,
      "endOffset" : 148
    }, {
      "referenceID" : 58,
      "context" : "This QA-based method alone outperforms all existing models for monolingual word alignment, including: JacanaToken aligner (Yao et al., 2013a), which is a CRF model using hand-crafted features and external resources; JacanaPhrase aligner (Yao et al.",
      "startOffset" : 122,
      "endOffset" : 141
    }, {
      "referenceID" : 59,
      "context" : ", 2013a), which is a CRF model using hand-crafted features and external resources; JacanaPhrase aligner (Yao et al., 2013b), which is a semi-CRF model relying on feature templates and external resources; PipelineAligner (Sultan et al.",
      "startOffset" : 104,
      "endOffset" : 123
    }, {
      "referenceID" : 43,
      "context" : ", 2013b), which is a semi-CRF model relying on feature templates and external resources; PipelineAligner (Sultan et al., 2014), which is a pipeline system that utilizes word similarity and contextual information with heuristic algorithms.",
      "startOffset" : 105,
      "endOffset" : 126
    }, {
      "referenceID" : 18,
      "context" : "We propose to incorporate word alignment information into the state-of-the-art EditNTS model (Dong et al., 2019) to explicitly learn the edit operations, including addition, deletion and paraphrase.",
      "startOffset" : 93,
      "endOffset" : 112
    }, {
      "referenceID" : 27,
      "context" : "Table 5 presents the text simplification results on two benchmark datasets, Newsela-auto and Wikipedia-auto (Jiang et al., 2020), where we improve the SARI score (Xu et al.",
      "startOffset" : 108,
      "endOffset" : 128
    }, {
      "referenceID" : 54,
      "context" : ", 2020), where we improve the SARI score (Xu et al., 2016) by 0.",
      "startOffset" : 41,
      "endOffset" : 58
    }, {
      "referenceID" : 18,
      "context" : "By incorporating our monolingual word aligner into the EditNTS (Dong et al., 2019) model, we improve the performance measured by SARI score (the main automatic metric for simplification) and its three parts: precision for delete (del), F1 scores for add and keep operations.",
      "startOffset" : 63,
      "endOffset" : 82
    }, {
      "referenceID" : 31,
      "context" : "We can utilize our neural aligner in sentence pair classification tasks (Lan and Xu, 2018), adding conditional alignment probability p(a|s, t) as an extra feature.",
      "startOffset" : 72,
      "endOffset" : 90
    }, {
      "referenceID" : 9,
      "context" : "We experiment with on different datsets for various tasks, including: natural language inference on SNLI (Bowman et al., 2015), MNLI (Williams et al.",
      "startOffset" : 105,
      "endOffset" : 126
    }, {
      "referenceID" : 50,
      "context" : ", 2015), MNLI (Williams et al., 2018), SICK (Marelli et al.",
      "startOffset" : 14,
      "endOffset" : 37
    }, {
      "referenceID" : 35,
      "context" : ", 2018), SICK (Marelli et al., 2014), and RTE (Giampiccolo et al.",
      "startOffset" : 14,
      "endOffset" : 36
    }, {
      "referenceID" : 23,
      "context" : ", 2014), and RTE (Giampiccolo et al., 2007) from the GLUE benchmark (Wang et al.",
      "startOffset" : 17,
      "endOffset" : 43
    }, {
      "referenceID" : 48,
      "context" : ", 2007) from the GLUE benchmark (Wang et al., 2018); semantic textual similarity on STS-B (Cer et al.",
      "startOffset" : 32,
      "endOffset" : 51
    }, {
      "referenceID" : 13,
      "context" : ", 2018); semantic textual similarity on STS-B (Cer et al., 2017) and STS14 (Agirre et al.",
      "startOffset" : 46,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : ", 2017) and STS14 (Agirre et al., 2014); question answering on WikiQA (Yang et al.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 56,
      "context" : ", 2014); question answering on WikiQA (Yang et al., 2015) and TrecQA (Wang et al.",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 49,
      "context" : ", 2015) and TrecQA (Wang et al., 2007); paraphrase identification on MRPC (Dolan and Brockett, 2005), URL (Lan et al.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 16,
      "context" : ", 2007); paraphrase identification on MRPC (Dolan and Brockett, 2005), URL (Lan et al.",
      "startOffset" : 43,
      "endOffset" : 69
    }, {
      "referenceID" : 30,
      "context" : ", 2007); paraphrase identification on MRPC (Dolan and Brockett, 2005), URL (Lan et al., 2017), PIT (Xu et al.",
      "startOffset" : 75,
      "endOffset" : 93
    }, {
      "referenceID" : 52,
      "context" : ", 2017), PIT (Xu et al., 2015a), and QQP (Iyer et al.",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 51,
      "context" : "We implement the fine-tuned BERTbase model using Huggingface’s library (Wolf et al., 2019).",
      "startOffset" : 71,
      "endOffset" : 90
    } ],
    "year" : 2021,
    "abstractText" : "Monolingual word alignment is important for studying fine-grained editing operations (i.e., deletion, addition, and substitution) in textto-text generation tasks, such as paraphrase generation, text simplification, neutralizing biased language, etc. In this paper, we present a novel neural semi-Markov CRF alignment model, which unifies word and phrase alignments through variable-length spans. We also create a new benchmark with human annotations that cover four different text genres to evaluate monolingual word alignment models in more realistic settings. Experimental results show that our proposed model outperforms all previous approaches for monolingual word alignment as well as a competitive QA-based baseline, which was previously only applied to bilingual data. Our model demonstrates good generalizability to three out-of-domain datasets and shows great utility in two downstream applications: automatic text simplification and sentence pair classification tasks.1",
    "creator" : "LaTeX with hyperref"
  }
}