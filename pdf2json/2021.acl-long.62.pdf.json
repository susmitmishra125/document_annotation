{
  "name" : "2021.acl-long.62.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Compare to The Knowledge: Graph Neural Fake News Detection with External Knowledge",
    "authors" : [ "Linmei Hu", "Tianchi Yang", "Luhao Zhang", "Wanjun Zhong", "Duyu Tang", "Chuan Shi", "Nan Duan", "Ming Zhou" ],
    "emails" : [ "hulinmei@bupt.edu.cn", "yangtianchi@bupt.edu.cn", "shichuan@bupt.edu.cn", "zhangluhao@meituan.com", "zhongwj25@mail2.sysu.edu.cn", "dutang@microsoft.com", "nanduan@microsoft.com", "mingzhou@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 754–763\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n754"
    }, {
      "heading" : "1 Introduction",
      "text" : "With the rapid development of the Internet, there are increasingly huge opportunities for fake news\n∗The work was done while visiting Micorosft Research Asia.\nproduction, dissemination and consumption. Fake news are news documents that are intentionally and verifiably false, and could mislead readers (Allcott and Gentzkow, 2017). Fake news can easily misguide public opinion, cause the crisis of confidence, and disturb the social order (Vosoughi et al., 2018). It is well known that fake news exerted an influence in the past 2016 US presidential elections (Allcott and Gentzkow, 2017). Thus, it is very important to develop effective methods for early fake news detection based on the textual content of the news document.\nSome existing fake news detection methods rely heavily on various hand-crafted linguistic and semantic features for differentiating between news documents (Conroy et al., 2015; Rubin et al., 2016; Rashkin et al., 2017; Khurana and Intelligentie, 2017; Shu et al., 2020). To avoid feature engineering, deep neural models such as Bi-LSTM and convolutional neural networks (CNN) have been employed (Oshikawa et al., 2020; Wang, 2017; Rodrı́guez and Iglesias, 2019). However, they fail to consider the sentence interactions in the document. Vaibhav et al. showed that trusted news and fake news have different patterns of sentence interactions (Vaibhav et al., 2019). They modeled a news document as a fully connected sentence graph and proposed a graph attention model for fake news detection. Although these existing approaches can be effective, they fail to fully exploit external KB which could help determine whether the news is fake or trusted.\nExternal KB such as Wikipedia contains a large amount of high-quality structured subjectpredicate-object triplets and unstructured entity descriptions, which could serve as evidence for detecting fake news. As shown in Figure 4, the news\ndocument about “mammograms are not effective at detecting breast tumors” is likely to be detected as fake news with the knowledge that “ The goal of mammography is the early detection of breast cancer” in the Wikipedia entity description page 1. Pan et al. proposed to construct knowledge graphs from positive and negative news, and apply TransE to learn triplet scores for fake news detection (Pan et al., 2018). Nevertheless, the performance is largely influenced by construction of the knowledge graph. In this paper, to take full advantage of the external knowledge, we propose a novel endto-end graph neural model CompareNet which directly compares the news to the KB through entities for fake news detection. In CompareNet, we also consider using topics to enrich the news document representation for improving fake news detection, since fake news detection and topics are highly correlated (Zhang et al., 2020; Jin et al., 2016). For example, the news documents in the “health” topic are inclined towards false, while the documents belonging to the “economy” topic are biased to be trusted instead.\nParticularly, we first construct a directed heterogeneous document graph for each news document, containing sentences, topics and entities as nodes.The sentences are fully connected in bidirection. Each sentence is also connected with its top relevant topics in bi-direction. If a sentence contains an entity, one directed link is built from the sentence to the entity. The reason for building one-way links from sentences to entities is to ensure that we can learn contextual entity representations that encode the semantics of the news, while avoiding the influence of the true entity knowledge to the news representation. Based on the directed heterogeneous document graph, we develop a heterogeneous graph attention network to learn topic-enriched news representations and contextual entity representations. The learned contextual entity representations are then compared to the corresponding KB-based entity representations with a carefully designed entity comparison network, in order to capture the semantic consistency between the news content and external KB. Finally, the topic-enriched news representations and the entity comparison features are combined for fake news classification. To facilitate related researches, we release both our code and dataset to the public2.\n1https://en.wikipedia.org/wiki/Mammography 2https://github.com/ytc272098215/FakeNewsDetection\nIn summary, our main contributions include:\n1) In this paper, we propose a novel end-to-end graph neural model CompareNet which compares the news to the external knowledge through entities for fake news detection.\n2) In CompareNet, we also consider the useful topic information. We construct a directed heterogeneous document graph incorporating topics and entities. Then we develop heterogeneous graph attention networks to learn topicenriched news representations. A novel entity comparison network is designed to compare the news to the KB.\n3) Extensive experiments on two benchmark datasets demonstrate that our model significantly outperforms state-of-the-art models on fake news detection by effectively incorporating external knowledge and topic information."
    }, {
      "heading" : "2 Related Work",
      "text" : "Fake news detection has attracted much attention in recent years (Zhou and Zafarani, 2020; Oshikawa et al., 2020). A lot of works also focus on the related problem, i.e., fact checking, which aims to search evidence from external knowledge to verify the veracity of a claim (e.g., a subject-predicateobject triple) (Thorne et al., 2018; Zhou et al., 2019; Zhong et al., 2020). Generally, fake news detection usually focuses on news events while fact-checking is broader (Oshikawa et al., 2020). The approaches for fake news detection can be divided into two categories: social-based and content-based."
    }, {
      "heading" : "2.1 Social-based Fake News Detection",
      "text" : "Social context related to news documents contains rich information such as user profiles and social relationships to help detect fake news. Social based models basically include stance-based and propagation-based. Stance-based models utilize users’ opinions to infer news veracity (Jin et al., 2016; Wu et al., 2019). Tacchini et al. constructed a bipartite network of user and posts with ‘like’ stance information, and proposed a semisupervised probabilistic model to predict the likelihood of posts being hoaxes (Tacchini et al., 2017). Propagation-based approaches for fake news detection are based on the basic assumption that the credibility of a news event is highly related to the credibilities of relevant social media posts. Both\nhomogeneous (Jin et al., 2016) and heterogeneous credibility networks (Gupta et al., 2012; Shu et al., 2019; Zhang et al., 2020) have been built to model the propagation process. For instance, (Zhang et al., 2020) constructed a heterogeneous network of news articles, creators and news subjects, and proposed a deep diffusive network model for incorporating the network structure information to simultaneously detect fake news articles, creators and subjects."
    }, {
      "heading" : "2.2 Content-based Fake News Detection",
      "text" : "On the other hand, news contents contain the clues to differentiate fake and trusted news. A lot of existing works extract specific writing styles such as lexical and syntactic features (Conroy et al., 2015; Rubin et al., 2016; Khurana and Intelligentie, 2017; Rashkin et al., 2017; Shu et al., 2020; Oshikawa et al., 2020) and sensational headlines (Potthast et al., 2018; Sitaula et al., 2019) for fake news classifier. To avoid hand-crafted feature engineering, neural models have been proposed (Wang, 2017; Rodrı́guez and Iglesias, 2019). For example, Ibrain et al. applied deep neural networks, such as BiLSTM and convolutional neural networks (CNN) for fake news detection (Rodrı́guez and Iglesias, 2019). However, these works fail to consider different sentence interaction patterns between trusted and fake news documents. Vaibhav et al. proposed to model a document as a sentence graph capturing the sentence interactions and applied graph attention networks for learning document representation (Vaibhav et al., 2019). Pan et al. proposed to construct knowledge graphs from positive and negative news, and apply TransE to learn triplet scores for fake news detection (Pan et al., 2018). Nevertheless, they relied heavily on the quality of the construction of knowledge graphs. In this paper, we propose a novel graph neural model Com-\npareNet which directly compares the news to external knowledge for fake news detection. Considering that the detection of fake news is correlated with topics, we also use topics to enrich the news representation for improving fake news detection.\nSome works (Wang, 2017; Khattar et al., 2019; Wang et al., 2020) also consider incorporating multi-modal features such as images for improving fake news detection."
    }, {
      "heading" : "3 Our Proposed CompareNet",
      "text" : "In this section, we detail our proposed fake news detection model CompareNet, which directly compares the news to external knowledge for fake news detection. As shown in Figure 2, we also consider topics for enriching news representation since fake news detection is highly correlated with topics (Zhang et al., 2020). Specifically, we first construct a directed heterogeneous document graph for each news document incorporating topics and entities as shown in Figure 1. The graph well captures the interactions among sentences, topics and entities. Based on the graph, we develop a heterogeneous graph attention network to learn the topic-enriched news representation as well as the contextual entity representations that encode the semantics of the news document. To fully leverage external KB, we take the entities as the bridge between the news document and the KB. We compare the contextual entity representations with the corresponding KB-based entity representations using a carefully designed entity comparison network. Finally, the obtained entity comparison features are combined with the topic-enriched news document representation for fake news detection."
    }, {
      "heading" : "3.1 Directed Heterogeneous Document Graph",
      "text" : "For each news document d, we construct a directed heterogeneous document graph G = (V, E) incorporating topics and entities, as shown in Figure 1. There are three kinds of nodes in the graph: sentences S = {s1, s2, · · ·, sm}, topics T = {t1, t2, · · ·, tK} and entities E = {e1, e2, · · ·, en}, i.e., V = S ∪ T ∪ E. The set of edges E represent the relations among sentences, topics and entities. The details of constructing the graph are described as follows.\nWe first split the news document as a set of sentences. Sentences are bidirectionally connected with each other in the graph, capturing the interaction of each sentence with every other sentence. Since topic information is important for fake news detection (Zhang et al., 2020), we apply the unsupervised LDA (Blei et al., 2003) (the total topic number K is set as 100) to mine the latent topics T from all the sentences of all the documents in our dataset. Specifically, each sentence is taken as a pseudo-document and is assigned to the top P relevant topics with the largest probabilities. Thus, each sentence is also connected with its top P assigned topics in bi-direction, allowing the useful topic information to propagate among the sentences. Note that we can also deal with new coming news documents by inferring the topics with trained LDA. We identify the entities E in the document d and map them to Wikipedia using the entity linking\ntool TAGME3. If a sentence s contains an entity e, we build a one-way directed edge from a sentence to the entity e, in order to allow only information propagation from sentences to entities. In this way, we can avoid integrating true entity knowledge directly into news representation, which may mislead the detection of fake news."
    }, {
      "heading" : "3.2 Heterogeneous Graph Convolution",
      "text" : "Based on the above directed heterogeneous document graph G, we develop a heterogeneous graph attention network for learning the news representation as well as the contextual entity representations. It considers not only the weights of different nodes with different types (Hu et al., 2019) but also the edge directions in the heterogeneous graph.\nFormally, we have three types T = {τ1, τ2, τ3} of nodes: sentences S, topics T and entities E with different feature spaces. We apply LSTM to encode a sentence s = {w1, · · ·, wm} and get its feature vector xs ∈ RM . The entity e ∈ E is initialized with the entity representations eKB ∈ RM learned from the external KB (see Subsection 3.3.1). The topic t ∈ T is initialized with one-hot vector xt ∈ RK .\nNext, consider the graph G = (V, E) where V and E represent the set of nodes and edges respectively. Let X ∈ R|V|×M be a matrix containing the nodes with their features xv ∈ RM (each row xv is a feature vector for a node v). A and D are\n3https://sobigdata.d4science.org/group/tagme/\nthe adjacency matrix and the degree matrix, respectively. The heterogeneous convolution layer updates the (l + 1)-th layer representation of the nodes H(l+1) by aggregating the features of their neighboring nodes H(l)τ with different types τ . (Initially, H(0) = X):\nH(l+1) = σ( ∑ τ∈T Bτ ·H(l)τ ·W(l)τ ), (1)\nwhere σ(·) denotes the activation function. Nodes with different types τ have different transformation matrix W(l)τ . The transformation matrix W (l) τ considers the different feature spaces and projects them into an implicit common space. Bτ ∈ R|V|×|Vτ | is the attention matrix, whose rows represent all the nodes and columns represent their neighboring nodes with the type τ . Its element βvv′ in the v-th row and the v′-th column is computed as follows:\nβvv′ = Softmaxv′(σ(νT · ατ [hv,hv′ ])), (2)\nwhere ν is the attention vector and ατ is the typelevel attention weight. hv and hv′ are respectively the representation of the current node v and its neighboring node v′. Softmax function is applied to normalize across the neighboring nodes of node v.\nWe calculate the type-level attention weights ατ based on the current node embedding hv and the type embedding hτ = ∑ v′ Ãvv′hv′ (the weighted sum of the neighboring node embeddings hv′ with the type τ , where the weight matrix Ã = D− 1 2 (A+ I)D− 1 2 is the normalized adjacency matrix with added self-connections) as follows:\nατ = Softmaxτ (σ(µTτ · [hv,hτ ])), (3)\nwhere µτ is the attention vector for the type τ . Softmax function is applied to normalize across all the types.\nAfter L-layer heterogeneous graph convolution, we can finally get all the node (including sentences and entities) representations aggregating neighborhood semantics. We use max pooling over the representations of the sentence nodes Hs ∈ RN to obtain the final topic-enriched news document embedding Hd ∈ RN . The learned entity representations that encode the contextual semantics of the document are taken as contextual entity representations ec ∈ RN ."
    }, {
      "heading" : "3.3 Entity Comparison Network",
      "text" : "In this subsection, we detail our entity comparison network which compares the learned contextual entity embeddings ec to the corresponding KBbased entity embeddings eKB. We believe entity comparison features could improve fake news detection based on the assumption that ec learned from trusted news document can be better aligned with the corresponding eKB; while inverse for fake news."
    }, {
      "heading" : "3.3.1 KB-based Entity Representation",
      "text" : "We first illustrate how to take full advantage of both structured subject-predicate-object triplets and unstructured textual entity descriptions in the KB (i.e., Wikipedia) to learn KB-based entity representations eKB.\nStructural Embedding. A wide range of knowledge graph embedding methods can be applied to obtain structured entity embeddings. Due to the simplicity of TransE (Bordes et al., 2013), we adopted TransE to learn entity representations es ∈ RM from the triplets. Formally, given a triplet (h, r, t), TransE regards a relationship r as a translation vector r from the head entity h to the tail entity t, namely h + r = t.\nTextual Embedding. For each entity, we take the first paragraph of the corresponding Wikipedia page as its text description. Then we apply LSTM (Hochreiter and Schmidhuber, 1997) to learn entity representations ed ∈ RM that encode the entity descriptions.\nGating Integration. Since both the structural triplets and textual description provide valuable information for an entity, we integrate these information into a joint representation. Particularly, as we have the structural embedding es and textual embedding ed, we adopt a learnable gating function to integrate entity embeddings from the two sources. Formally,\neKB = ge es + (1− ge) ed, (4)\nwhere ge ∈ RM is a gating vector (w.r.t. the entity e) to trade-off information from the two sources and its elements are in [0, 1]. denotes elementwise multiplication. The gating vector ge means that each dimension of es and ed are summed by different weights. To constrain the value of each element in [0, 1], we compute the gate ge with the Sigmoid function:\nge = σ(g̃e), (5)\nwhere g̃e ∈ RM is a real-value vector and is learned in the training process.\nAfter fusing the two types of embeddings with the gating function, we obtain the final KB-based entity embeddings eKB ∈ RM which encode both structural information from the triplets and textual information from the entity descriptions in the KB."
    }, {
      "heading" : "3.3.2 Entity Comparison",
      "text" : "We then perform entity-to-entity comparison between the news document and the KB, to capture the semantic consistency between the news content and the KB. We calculate a comparison vector ai between each contextual entity representation ec ∈ RN and its corresponding KB-based entity embedding eKB ∈ RM .\nai = fcmp(ec,We · eKB) , (6)\nwhere fcmp() denotes the comparison function, and We ∈ RN×M is a transformation matrix. To measure the embedding closeness and relevance (Shen et al., 2018), we design our comparison function as:\nfcmp(x, y) = Wa[x− y, x y], (7)\nwhere Wa ∈ RN×2N is a transformation matrix and is hadamard product, i.e., element-wise product. The final output comparison feature vector C ∈ RN is obtained by the max pooling over the alignment vectors A = [a1,a2, ...,an] of all the entities E = {e1, e2, ..., en} in the news document."
    }, {
      "heading" : "3.4 Model Training",
      "text" : "After obtaining the comparison vector C ∈ RN and the final news document representation vector Hd ∈ RN , we concatenate and feed them into a Softmax layer for fake news classification. Formally,\nZ = Softmax(Wo[Hd,C] + bo), (8)\nwhere Wo and bo are the parameter matrix and vection of a linear transformation. During model training, we exploit the cross-entropy loss over the training data with the L2-norm of the parameters:\nL = − ∑\ni∈Dtrain\n∑ j=1 Yij · logZij + η ‖Θ‖2, (9)\nwhere Dtrain is the set of news documents for training, Y is the corresponding label indicator matrix, Θ is the model parameters, and η is regularization factor. For model optimization, we adopt the gradient descent algorithm."
    }, {
      "heading" : "4 Experiments",
      "text" : "We conduct extensive experiments across various settings and datasets. Following the previous work (Vaibhav et al., 2019), we use SLN: Satirical and Legitimate News Database (Rubin et al., 2016), and LUN: Labeled Unreliable News Dataset (Rashkin et al., 2017) for our experiments. Table 1 shows the statistics.\nOur baseline models include deep neural models: LSTM (Hochreiter and Schmidhuber, 1997), CNN (Kim, 2014), BERT+LSTM (Vaibhav et al., 2019) (BERT for sentence encoder and then LSTM for document encoder) and BERT (Devlin et al., 2019) (directly for document encoder). We also compare our model with graph neural models: GCN and GAT based on an undirected fully-connected sentence graph, which use attention pooling or max pooling for learning news document representation. For fair comparison with the previous work (Vaibhav et al., 2019), we use LSTM to encode sentences with randomly initialized word embeddings, which is the same as all the graph neural baselines. We run our model 5 times and report the micro-averaged (Precision = Recall = F1) and macro-averaged scores (Precision, Recall, F1) in all the settings including 2-way and 4-way classification.\n2-way classification: We use the satirical and trusted news articles from LUN-train for training, LUN-test for validation and evaluate our model on the entire SLN dataset. This is done to emulate a real-world scenario where we want to see the performance of our model on an out-of-domain dataset.\n4-way classification: We split the LUN-train into a 80:20 split to create our training and validation set. We use the LUN-test as our in-domain test set.\nExperimental Setting. In our experiments, we set the number of topics K = 100 in LDA. Each sentence is assigned to top P = 2 topics with the largest probabilities. The layer number of our heterogeneous graph convolution is set as L = 1. These parameters are chosen according to the best experimental results on validation set. The other hyper-parameters are set as the same as the baseline (Vaibhav et al., 2019) for fair comparison. Specifically, all the hidden dimensions used in our model are set as M = 100. The node embedding dimension N= 32. For GCN, GAT and CompareNet, we set the activation function as LeakyRelU with slope 0.2. For model training, we train the models for a\nmaximum of 15 epochs and use Adam optimizer with learning rate 0.001. We set L2 normalization factor η as 1e-6."
    }, {
      "heading" : "4.1 Overall Results",
      "text" : "Table 2 shows the results for the two-way classification between satirical and trusted news articles. We report only micro F1 since micro Precision=Recall=F1. As we can see, our proposed model CompareNet significantly outperforms all the state-of-the-art baselines in terms of all the metrics. Compared to the best baseline model, CompareNet improves both micro F1 and macro F1 by nearly 3%. We can also find that the graph neural network based models GCN and GAT all perform better than the deep neural models including CNN, LSTM and BERT. The reason is that the deep neural models fail to consider the interactions between sentences, which is important for fake news detection since different interaction patterns are observed in trusted and fake news documents (Vaibhav et al., 2019). Our model CompareNet further improves fake news detection by effectively exploiting the topics as well as the external KB. The topics enrich the news representation, and the external KB offers evidences for fake news detection.\nWe also present the results of four-way classification in Table 3. Consistently, all graph neural models capturing sentence interactions outperform the deep neural models. Our model CompareNet achieves the best performance in terms of all metrics. We believe that our model CompareNet benefits from the topics and external knowledge."
    }, {
      "heading" : "4.2 Ablation Study",
      "text" : "In this subsection, we conduct experiments to study the effectiveness of each module in CompareNet and the way we incorporate external knowledge. We study the average performance of 5 runs on the LUN-test set. As shown in Table 4, we test the performance of CompareNet removing structured triplets, removing the entire external knowledge, removing topics, and removing both topics and external knowledge. In the last two rows, we further\nexamine the constructed directed heterogeneous document graph and the designed entity comparison function. The variant CompareNet (undirected) does not consider the edge directions of the directed heterogeneous document graph. The variant model CompareNet (concatenation) replaces the entity comparison function as the simple concatenation operation. As we can see from Table 4, removing structural entity knowledge (i.e., w/o Structured Triplets) leads to slight performance drop. If we remove the entire external knowledge (i.e., w/o Entity Cmp), the performance decreases by around 1.3% and 1.8% on micro F1 and macro F1, respectively. Removing topics (i.e., w/o topics) will comparably impair the performance, which shows that the topic\ninformation is as important as the external knowledge. Removing both topics and external knowledge (i.e., w/o Both) will lead to substantial performance drop (4.0-5.0%). It demonstrates the importance of both topics and external knowledge. The variant model CompareNet (undirected) although incorporating both topics and external knowledge achieves lower performance than CompareNet w/o Entity Cmp and CompareNet w/o Topics. The reason could be that CompareNet (undirected) directly aggregates the true entity knowledge into the news representation in graph convolution without considering the directed edges, which misleads the classifier for differentiating fake news. This verifies the appropriateness of our constructed directed heterogeneous document graph. The last variant CompareNet (concatenation) also performs lower than CompareNet w/o Entity Cmp, further indicating that directly concatenating true entity knowledge is not a good way for incorporating entity knowledge. Its performance drops by around 2.0% compared to CompareNet. These demonstrate the effectiveness of the carefully designed entity comparison network in CompareNet."
    }, {
      "heading" : "4.3 Analysis of Top Assigned Topic Number",
      "text" : "Figure 3 shows the performance (micro and macro F1) of our model CompareNet on LUN validation set with different number of top assigned topics P to each sentence. As we can see clearly, micro F1 and macro F1 first consistently rises with the increase of P and then drops when P is larger than\n2. This may because that connecting too many lowprobability topics will introduce some noise. Thus, in our experiments, we set P = 2."
    }, {
      "heading" : "4.4 Case Study",
      "text" : "To further illustrate why our model outperforms state-of-the-art baseline GAT+Attn (Vaibhav et al., 2019), we present two real news examples from the LUN-test set. The baseline model GAT+Attn and the variant model CompareNet w/o Entity Cmp mistakenly predict these two examples as trusted news, while our model CompareNet can successfully predict both of them. As we can see from Figure 4, the content of the news document is in conflict with the entity description from Wikipedia. Specifically, the news about “FDA target and threaten the natural health community” delivers contrary meaning from the entity description that “FDA is responsible for protecting and promoting public health” 4. Similarly, the news document about “mammograms are not effective at detecting breast tumors” conveys different meaning from the entity description of “mammograms”. We believe that our model CompareNet benefits from the comparison to Wikipedia knowledge by the entity comparison network. We find there are also unsuccessful cases since an entity could be mistakenly linked to a wrong entity in the Wikipedia."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we propose a novel end-to-end graph neural model CompareNet which compares the news to the external knowledge for fake news detection. Considering that the detection of fake news is correlated with topics, in our model, we also use topics to enrich the news document representation for improving fake news detection. Particularly, we first construct a directed heterogeneous document graph for each news document capturing the interactions among sentences, topics and entities.\n4https://en.wikipedia.org/wiki/Food and Drug Administration\nBased on the graph, we develop a heterogeneous graph attention network for learning topic-enriched news representation as well as contextual entity representations that encode the semantics of the content of the news document. To capture the semantic consistency of the news content and the KB, the learned contextual entity representations are then compared to the KB-based entity representations, with a carefully designed entity comparison network. Finally, the obtained entity comparison features are combined with the news representation for an improved fake news classifier. Experiments on two benchmark datasets have demonstrated the effectiveness of the way we incorporate the external knowledge and topics.\nIn future work, we will explore a better way to combine multi-modal data (e.g., images) and external knowledge for fake news detection."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The work is supported by the National Natural Science Fundation of China (No. 61806020, U1936220, 61972047, 62076245) and the Microsoft Research Asia’s Star Track project."
    } ],
    "references" : [ {
      "title" : "Social media and fake news in the 2016 election",
      "author" : [ "Hunt Allcott", "Matthew Gentzkow." ],
      "venue" : "Journal of economic perspectives, 31(2):211–236.",
      "citeRegEx" : "Allcott and Gentzkow.,? 2017",
      "shortCiteRegEx" : "Allcott and Gentzkow.",
      "year" : 2017
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "David M Blei", "Andrew Y Ng", "Michael I Jordan." ],
      "venue" : "Journal of machine Learning research, 3(Jan):993–1022.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Translating embeddings for modeling multirelational data",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Alberto GarciaDuran", "Jason Weston", "Oksana Yakhnenko." ],
      "venue" : "Advances in Neural Information Processing Systems 26, pages 2787–2795.",
      "citeRegEx" : "Bordes et al\\.,? 2013",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2013
    }, {
      "title" : "Automatic deception detection: Methods for finding fake news",
      "author" : [ "Nadia K Conroy", "Victoria L Rubin", "Yimin Chen." ],
      "venue" : "Proceedings of the Association for Information Science and Technology, 52(1):1–4.",
      "citeRegEx" : "Conroy et al\\.,? 2015",
      "shortCiteRegEx" : "Conroy et al\\.",
      "year" : 2015
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Evaluating event credibility on twitter",
      "author" : [ "Manish Gupta", "Peixiang Zhao", "Jiawei Han." ],
      "venue" : "Proceedings of the Twelfth SIAM International Conference on Data Mining, pages 153–164.",
      "citeRegEx" : "Gupta et al\\.,? 2012",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2012
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Heterogeneous graph attention networks for semi-supervised short text classification",
      "author" : [ "Linmei Hu", "Tianchi Yang", "Chuan Shi", "Houye Ji", "Xiaoli Li." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and",
      "citeRegEx" : "Hu et al\\.,? 2019",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2019
    }, {
      "title" : "News verification by exploiting conflicting social viewpoints in microblogs",
      "author" : [ "Zhiwei Jin", "Juan Cao", "Yongdong Zhang", "Jiebo Luo." ],
      "venue" : "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, pages 2972–2978.",
      "citeRegEx" : "Jin et al\\.,? 2016",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2016
    }, {
      "title" : "Mvae: Multimodal variational autoencoder for fake news detection",
      "author" : [ "Dhruv Khattar", "Jaipal Singh Goud", "Manish Gupta", "Vasudeva Varma." ],
      "venue" : "The World Wide Web Conference, page 2915–2921.",
      "citeRegEx" : "Khattar et al\\.,? 2019",
      "shortCiteRegEx" : "Khattar et al\\.",
      "year" : 2019
    }, {
      "title" : "The linguistic features of fake news headlines and statements",
      "author" : [ "Urja Khurana", "Bachelor Opleiding Kunstmatige Intelligentie." ],
      "venue" : "Ph.D. thesis, Master’s thesis, University of Amsterdam.",
      "citeRegEx" : "Khurana and Intelligentie.,? 2017",
      "shortCiteRegEx" : "Khurana and Intelligentie.",
      "year" : 2017
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1746–1751.",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "A survey on natural language processing for fake news detection",
      "author" : [ "Ray Oshikawa", "Jing Qian", "William Yang Wang." ],
      "venue" : "ArXiv, abs/1811.00770.",
      "citeRegEx" : "Oshikawa et al\\.,? 2020",
      "shortCiteRegEx" : "Oshikawa et al\\.",
      "year" : 2020
    }, {
      "title" : "Content based fake news detection using knowledge graphs",
      "author" : [ "Jeff Z. Pan", "Siyana Pavlova", "Chenxi Li", "Ningxi Li", "Yangmei Li", "Jinshuo Liu." ],
      "venue" : "The Semantic Web - ISWC 2018 - 17th International Semantic Web Conference, volume 11136, pages 669–",
      "citeRegEx" : "Pan et al\\.,? 2018",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2018
    }, {
      "title" : "A stylometric inquiry into hyperpartisan and fake news",
      "author" : [ "Martin Potthast", "Johannes Kiesel", "Kevin Reinartz", "Janek Bevendorff", "Benno Stein." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 231–",
      "citeRegEx" : "Potthast et al\\.,? 2018",
      "shortCiteRegEx" : "Potthast et al\\.",
      "year" : 2018
    }, {
      "title" : "Truth of varying shades: Analyzing language in fake news and political fact-checking",
      "author" : [ "Hannah Rashkin", "Eunsol Choi", "Jin Yea Jang", "Svitlana Volkova", "Yejin Choi." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Rashkin et al\\.,? 2017",
      "shortCiteRegEx" : "Rashkin et al\\.",
      "year" : 2017
    }, {
      "title" : "Fake news detection using deep learning. CoRR, abs/1910.03496",
      "author" : [ "Álvaro Ibrain Rodrı́guez", "Lara Lloret Iglesias" ],
      "venue" : null,
      "citeRegEx" : "Rodrı́guez and Iglesias.,? \\Q2019\\E",
      "shortCiteRegEx" : "Rodrı́guez and Iglesias.",
      "year" : 2019
    }, {
      "title" : "Fake news or truth? using satirical cues to detect potentially misleading news",
      "author" : [ "Victoria Rubin", "Niall Conroy", "Yimin Chen", "Sarah Cornwell." ],
      "venue" : "Proceedings of the Second Workshop on Computational Approaches to Deception Detection, pages 7–17.",
      "citeRegEx" : "Rubin et al\\.,? 2016",
      "shortCiteRegEx" : "Rubin et al\\.",
      "year" : 2016
    }, {
      "title" : "Improved semantic-aware network embedding with fine-grained word alignment",
      "author" : [ "Dinghan Shen", "Xinyuan Zhang", "Ricardo Henao", "Lawrence Carin." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Shen et al\\.,? 2018",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "Fakenewsnet: A data repository with news content, social context, and spatiotemporal information for studying fake news on social media",
      "author" : [ "Kai Shu", "Deepak Mahudeswaran", "Suhang Wang", "Dongwon Lee", "Huan Liu." ],
      "venue" : "Big Data, 8(3):171–188.",
      "citeRegEx" : "Shu et al\\.,? 2020",
      "shortCiteRegEx" : "Shu et al\\.",
      "year" : 2020
    }, {
      "title" : "Beyond news contents: The role of social context for fake news detection",
      "author" : [ "Kai Shu", "Suhang Wang", "Huan Liu." ],
      "venue" : "Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, pages 312–320.",
      "citeRegEx" : "Shu et al\\.,? 2019",
      "shortCiteRegEx" : "Shu et al\\.",
      "year" : 2019
    }, {
      "title" : "Credibilitybased fake news detection",
      "author" : [ "Niraj Sitaula", "Chilukuri K. Mohan", "Jennifer Grygiel", "Xinyi Zhou", "Reza Zafarani." ],
      "venue" : "CoRR, abs/1911.00643.",
      "citeRegEx" : "Sitaula et al\\.,? 2019",
      "shortCiteRegEx" : "Sitaula et al\\.",
      "year" : 2019
    }, {
      "title" : "Some like it hoax: Automated fake news detection in social networks",
      "author" : [ "Eugenio Tacchini", "Gabriele Ballarin", "Marco L. Della Vedova", "Stefano Moret", "Luca de Alfaro." ],
      "venue" : "CoRR, abs/1704.07506.",
      "citeRegEx" : "Tacchini et al\\.,? 2017",
      "shortCiteRegEx" : "Tacchini et al\\.",
      "year" : 2017
    }, {
      "title" : "FEVER: a large-scale dataset for fact extraction and VERification",
      "author" : [ "James Thorne", "Andreas Vlachos", "Christos Christodoulopoulos", "Arpit Mittal." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the",
      "citeRegEx" : "Thorne et al\\.,? 2018",
      "shortCiteRegEx" : "Thorne et al\\.",
      "year" : 2018
    }, {
      "title" : "Do sentence interactions matter? leveraging sentence level representations for fake news classification",
      "author" : [ "Vaibhav Vaibhav", "Raghuram Mandyam", "Eduard Hovy." ],
      "venue" : "Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural",
      "citeRegEx" : "Vaibhav et al\\.,? 2019",
      "shortCiteRegEx" : "Vaibhav et al\\.",
      "year" : 2019
    }, {
      "title" : "The spread of true and false news online",
      "author" : [ "Soroush Vosoughi", "Deb Roy", "Sinan Aral." ],
      "venue" : "Science, 359(6380):1146–1151.",
      "citeRegEx" : "Vosoughi et al\\.,? 2018",
      "shortCiteRegEx" : "Vosoughi et al\\.",
      "year" : 2018
    }, {
      "title" : "liar, liar pants on fire”: A new benchmark dataset for fake news detection",
      "author" : [ "William Yang Wang." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 422–426.",
      "citeRegEx" : "Wang.,? 2017",
      "shortCiteRegEx" : "Wang.",
      "year" : 2017
    }, {
      "title" : "Fake news detection via knowledge-driven multimodal graph convolutional networks",
      "author" : [ "Youze Wang", "Shengsheng Qian", "Jun Hu", "Quan Fang", "Changsheng Xu." ],
      "venue" : "Proceedings of the 2020 International Conference on Multimedia Retrieval, pages 540–",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Different absorption from the same sharing: Sifted multi-task learning for fake news detection",
      "author" : [ "Lianwei Wu", "Yuan Rao", "Haolin Jin", "Ambreen Nazir", "Ling Sun." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Fakedetector: Effective fake news detection with deep diffusive neural network",
      "author" : [ "Jiawei Zhang", "Bowen Dong", "Philip S. Yu." ],
      "venue" : "In Proceedings of the 36th IEEE International Conference on Data Engineering, pages 1826–1829.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Reasoning over semantic-level graph for fact checking",
      "author" : [ "Wanjun Zhong", "Jingjing Xu", "Duyu Tang", "Zenan Xu", "Nan Duan", "Ming Zhou", "Jiahai Wang", "Jian Yin." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Zhong et al\\.,? 2020",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2020
    }, {
      "title" : "GEAR: Graph-based evidence aggregating and reasoning for fact verification",
      "author" : [ "Jie Zhou", "Xu Han", "Cheng Yang", "Zhiyuan Liu", "Lifeng Wang", "Changcheng Li", "Maosong Sun." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Zhou et al\\.,? 2019",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2019
    }, {
      "title" : "A survey of fake news: Fundamental theories, detection methods, and opportunities",
      "author" : [ "Xinyi Zhou", "Reza Zafarani." ],
      "venue" : "ACM Computing Surveys (CSUR), 53(5):1–40.",
      "citeRegEx" : "Zhou and Zafarani.,? 2020",
      "shortCiteRegEx" : "Zhou and Zafarani.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Fake news are news documents that are intentionally and verifiably false, and could mislead readers (Allcott and Gentzkow, 2017).",
      "startOffset" : 100,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : "It is well known that fake news exerted an influence in the past 2016 US presidential elections (Allcott and Gentzkow, 2017).",
      "startOffset" : 96,
      "endOffset" : 124
    }, {
      "referenceID" : 12,
      "context" : "To avoid feature engineering, deep neural models such as Bi-LSTM and convolutional neural networks (CNN) have been employed (Oshikawa et al., 2020; Wang, 2017; Rodrı́guez and Iglesias, 2019).",
      "startOffset" : 124,
      "endOffset" : 190
    }, {
      "referenceID" : 26,
      "context" : "To avoid feature engineering, deep neural models such as Bi-LSTM and convolutional neural networks (CNN) have been employed (Oshikawa et al., 2020; Wang, 2017; Rodrı́guez and Iglesias, 2019).",
      "startOffset" : 124,
      "endOffset" : 190
    }, {
      "referenceID" : 16,
      "context" : "To avoid feature engineering, deep neural models such as Bi-LSTM and convolutional neural networks (CNN) have been employed (Oshikawa et al., 2020; Wang, 2017; Rodrı́guez and Iglesias, 2019).",
      "startOffset" : 124,
      "endOffset" : 190
    }, {
      "referenceID" : 24,
      "context" : "showed that trusted news and fake news have different patterns of sentence interactions (Vaibhav et al., 2019).",
      "startOffset" : 88,
      "endOffset" : 110
    }, {
      "referenceID" : 13,
      "context" : "proposed to construct knowledge graphs from positive and negative news, and apply TransE to learn triplet scores for fake news detection (Pan et al., 2018).",
      "startOffset" : 137,
      "endOffset" : 155
    }, {
      "referenceID" : 29,
      "context" : "since fake news detection and topics are highly correlated (Zhang et al., 2020; Jin et al., 2016).",
      "startOffset" : 59,
      "endOffset" : 97
    }, {
      "referenceID" : 8,
      "context" : "since fake news detection and topics are highly correlated (Zhang et al., 2020; Jin et al., 2016).",
      "startOffset" : 59,
      "endOffset" : 97
    }, {
      "referenceID" : 32,
      "context" : "Fake news detection has attracted much attention in recent years (Zhou and Zafarani, 2020; Oshikawa et al., 2020).",
      "startOffset" : 65,
      "endOffset" : 113
    }, {
      "referenceID" : 12,
      "context" : "Fake news detection has attracted much attention in recent years (Zhou and Zafarani, 2020; Oshikawa et al., 2020).",
      "startOffset" : 65,
      "endOffset" : 113
    }, {
      "referenceID" : 23,
      "context" : ", a subject-predicateobject triple) (Thorne et al., 2018; Zhou et al., 2019; Zhong et al., 2020).",
      "startOffset" : 36,
      "endOffset" : 96
    }, {
      "referenceID" : 31,
      "context" : ", a subject-predicateobject triple) (Thorne et al., 2018; Zhou et al., 2019; Zhong et al., 2020).",
      "startOffset" : 36,
      "endOffset" : 96
    }, {
      "referenceID" : 30,
      "context" : ", a subject-predicateobject triple) (Thorne et al., 2018; Zhou et al., 2019; Zhong et al., 2020).",
      "startOffset" : 36,
      "endOffset" : 96
    }, {
      "referenceID" : 8,
      "context" : "Stance-based models utilize users’ opinions to infer news veracity (Jin et al., 2016; Wu et al., 2019).",
      "startOffset" : 67,
      "endOffset" : 102
    }, {
      "referenceID" : 28,
      "context" : "Stance-based models utilize users’ opinions to infer news veracity (Jin et al., 2016; Wu et al., 2019).",
      "startOffset" : 67,
      "endOffset" : 102
    }, {
      "referenceID" : 22,
      "context" : "constructed a bipartite network of user and posts with ‘like’ stance information, and proposed a semisupervised probabilistic model to predict the likelihood of posts being hoaxes (Tacchini et al., 2017).",
      "startOffset" : 180,
      "endOffset" : 203
    }, {
      "referenceID" : 8,
      "context" : "homogeneous (Jin et al., 2016) and heterogeneous credibility networks (Gupta et al.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 5,
      "context" : ", 2016) and heterogeneous credibility networks (Gupta et al., 2012; Shu et al., 2019; Zhang et al., 2020) have been built to model the propagation process.",
      "startOffset" : 47,
      "endOffset" : 105
    }, {
      "referenceID" : 20,
      "context" : ", 2016) and heterogeneous credibility networks (Gupta et al., 2012; Shu et al., 2019; Zhang et al., 2020) have been built to model the propagation process.",
      "startOffset" : 47,
      "endOffset" : 105
    }, {
      "referenceID" : 29,
      "context" : ", 2016) and heterogeneous credibility networks (Gupta et al., 2012; Shu et al., 2019; Zhang et al., 2020) have been built to model the propagation process.",
      "startOffset" : 47,
      "endOffset" : 105
    }, {
      "referenceID" : 14,
      "context" : ", 2020) and sensational headlines (Potthast et al., 2018; Sitaula et al., 2019) for fake news classifier.",
      "startOffset" : 34,
      "endOffset" : 79
    }, {
      "referenceID" : 21,
      "context" : ", 2020) and sensational headlines (Potthast et al., 2018; Sitaula et al., 2019) for fake news classifier.",
      "startOffset" : 34,
      "endOffset" : 79
    }, {
      "referenceID" : 26,
      "context" : "To avoid hand-crafted feature engineering, neural models have been proposed (Wang, 2017; Rodrı́guez and Iglesias, 2019).",
      "startOffset" : 76,
      "endOffset" : 119
    }, {
      "referenceID" : 16,
      "context" : "To avoid hand-crafted feature engineering, neural models have been proposed (Wang, 2017; Rodrı́guez and Iglesias, 2019).",
      "startOffset" : 76,
      "endOffset" : 119
    }, {
      "referenceID" : 16,
      "context" : "applied deep neural networks, such as BiLSTM and convolutional neural networks (CNN) for fake news detection (Rodrı́guez and Iglesias, 2019).",
      "startOffset" : 109,
      "endOffset" : 140
    }, {
      "referenceID" : 24,
      "context" : "proposed to model a document as a sentence graph capturing the sentence interactions and applied graph attention networks for learning document representation (Vaibhav et al., 2019).",
      "startOffset" : 159,
      "endOffset" : 181
    }, {
      "referenceID" : 13,
      "context" : "proposed to construct knowledge graphs from positive and negative news, and apply TransE to learn triplet scores for fake news detection (Pan et al., 2018).",
      "startOffset" : 137,
      "endOffset" : 155
    }, {
      "referenceID" : 29,
      "context" : "As shown in Figure 2, we also consider topics for enriching news representation since fake news detection is highly correlated with topics (Zhang et al., 2020).",
      "startOffset" : 139,
      "endOffset" : 159
    }, {
      "referenceID" : 29,
      "context" : "Since topic information is important for fake news detection (Zhang et al., 2020), we apply the unsupervised LDA (Blei et al.",
      "startOffset" : 61,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : ", 2020), we apply the unsupervised LDA (Blei et al., 2003) (the total topic",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 7,
      "context" : "It considers not only the weights of different nodes with different types (Hu et al., 2019) but also the edge directions in the heterogeneous graph.",
      "startOffset" : 74,
      "endOffset" : 91
    }, {
      "referenceID" : 6,
      "context" : "Then we apply LSTM (Hochreiter and Schmidhuber, 1997) to learn entity representations ed ∈ RM that encode the entity descriptions.",
      "startOffset" : 19,
      "endOffset" : 53
    }, {
      "referenceID" : 24,
      "context" : "Following the previous work (Vaibhav et al., 2019), we use SLN: Satirical and Legitimate News Database (Rubin et al.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 17,
      "context" : ", 2019), we use SLN: Satirical and Legitimate News Database (Rubin et al., 2016), and LUN: Labeled Unreliable News Dataset (Rashkin et al.",
      "startOffset" : 60,
      "endOffset" : 80
    }, {
      "referenceID" : 15,
      "context" : ", 2016), and LUN: Labeled Unreliable News Dataset (Rashkin et al., 2017) for our experiments.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 6,
      "context" : "Our baseline models include deep neural models: LSTM (Hochreiter and Schmidhuber, 1997), CNN (Kim, 2014), BERT+LSTM (Vaibhav et al.",
      "startOffset" : 53,
      "endOffset" : 87
    }, {
      "referenceID" : 11,
      "context" : "Our baseline models include deep neural models: LSTM (Hochreiter and Schmidhuber, 1997), CNN (Kim, 2014), BERT+LSTM (Vaibhav et al.",
      "startOffset" : 93,
      "endOffset" : 104
    }, {
      "referenceID" : 24,
      "context" : "Our baseline models include deep neural models: LSTM (Hochreiter and Schmidhuber, 1997), CNN (Kim, 2014), BERT+LSTM (Vaibhav et al., 2019) (BERT for sentence encoder and then LSTM for document encoder) and BERT (Devlin et al.",
      "startOffset" : 116,
      "endOffset" : 138
    }, {
      "referenceID" : 4,
      "context" : ", 2019) (BERT for sentence encoder and then LSTM for document encoder) and BERT (Devlin et al., 2019) (directly for document encoder).",
      "startOffset" : 80,
      "endOffset" : 101
    }, {
      "referenceID" : 24,
      "context" : "For fair comparison with the previous work (Vaibhav et al., 2019), we use LSTM to encode sentences with randomly initialized word embeddings, which is the same as all the graph neural baselines.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 24,
      "context" : "The other hyper-parameters are set as the same as the baseline (Vaibhav et al., 2019) for fair comparison.",
      "startOffset" : 63,
      "endOffset" : 85
    }, {
      "referenceID" : 24,
      "context" : "patterns are observed in trusted and fake news documents (Vaibhav et al., 2019).",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 24,
      "context" : "state-of-the-art baseline GAT+Attn (Vaibhav et al., 2019), we present two real news examples from the LUN-test set.",
      "startOffset" : 35,
      "endOffset" : 57
    } ],
    "year" : 2021,
    "abstractText" : "Nowadays, fake news detection, which aims to verify whether a news document is trusted or fake, has become urgent and important. Most existing methods rely heavily on linguistic and semantic features from the news content, and fail to effectively exploit external knowledge which could help determine whether the news document is trusted. In this paper, we propose a novel end-to-end graph neural model called CompareNet, which compares the news to the knowledge base (KB) through entities for fake news detection. Considering that fake news detection is correlated with topics, we also incorporate topics to enrich the news representation. Specifically, we first construct a directed heterogeneous document graph for each news incorporating topics and entities. Based on the graph, we develop a heterogeneous graph attention network for learning the topic-enriched news representation as well as the contextual entity representations that encode the semantics of the news content. The contextual entity representations are then compared to the corresponding KB-based entity representations through a carefully designed entity comparison network, to capture the consistency between the news content and KB. Finally, the topic-enriched news representation combining the entity comparison features are fed into a fake news classifier. Experimental results on two benchmark datasets demonstrate that CompareNet significantly outperforms state-of-the-art methods.",
    "creator" : "LaTeX with hyperref"
  }
}