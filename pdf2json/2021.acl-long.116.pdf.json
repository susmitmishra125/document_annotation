{
  "name" : "2021.acl-long.116.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "BACO: A Background Knowledge- and Content-Based Framework for Citing Sentence Generation",
    "authors" : [ "Yubin Ge", "Ly Dinh", "Xiaofeng Liu", "Jinsong Su", "Ziyao Lu", "Ante Wang", "Jana Diesner" ],
    "emails" : [ "jdiesner}@illinois.edu", "jssu@xmu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1466–1478\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1466"
    }, {
      "heading" : "1 Introduction",
      "text" : "A citation systematically, strategically, and critically synthesizes content from a cited paper in the context of a citing paper (Smith, 1981). A paper’s text that refers to prior work, which we herein refer to as citing sentences, forms the conceptual basis for a research question or problem; identifies issues, contradictions, or gaps with state of the art solutions; and prepares readers to understand the contributions of a citing paper, e.g., in terms of theory, methods, or findings (Elkiss et al., 2008). Writing meaningful and concise citing sentences that capture the gist of cited papers and identify connections between citing and cited papers is not trivial (White, 2004). Learning how to write up information about related work with appropriate and\nmeaningful citations is particularly challenging for new scholars (Mansourizadeh and Ahmad, 2011).\nTo assist scholars with note taking on prior work when working on a new research problem, this paper focuses on the task of citing sentence generation, which entails identifying salient information from cited papers and capturing connections between cited and citing papers. With this work, we hope to reduce scientific information overload for researchers by providing examples of concise citing sentences that address information from cited papers in the context of a new research problem and related write up. While this task cannot and is not meant to replace the scholarly tasks of finding, reading, and synthesizing prior work, the proposed computational solution is intended to support especially new researchers in practicing the process of writing effective and focused reflections on prior work given a new context or problem.\nA number of recent papers have focused on the task of citing sentence generation (Hu and Wan, 2014; Saggion et al., 2020; Xing et al., 2020), which is defined as generating a short text that describes a cited paper B in the context of a citing paper A, and the sentences before and after the citing sentences in paper A are considered as context. However, previous work has mainly utilized limited information from citing and cited papers to solve this task. We acknowledge that any such solution, including ours, is a simplification of the intricate process of how scholars write citing sentences.\nGiven this motivation, we explore two sets of information to generate citing sentences, namely background knowledge in the form of citation networks, and content from both citing and cited papers, as shown in Figure 1. Using citation networks was inspired by the fact that scholars have analyzed such networks to identify the main themes and research developments in domain areas such as information sciences (Hou et al., 2018), busi-\nness modeling (Li et al., 2017), and pharmaceutical research (Chen and Guan, 2011).\nWe use the content of citing and cited papers as a second set of features to capture two more in-depth content features: (1) What to cite - while the overall content of a cited paper needs to be understood by the authors of the citing paper, not all content is relevant for writing citing sentences. Therefore, we follow the example of estimating salient sentences (Yasunaga et al., 2019) and use the predicted salience to filter crucial information that should be integrated into the resulting citing sentence; (2) Why to cite - we define “citation function” as an approximation of an author’s reason for citing a paper (Teufel et al., 2006). A number of previous research on citation functions has used citing sentences and their context for classification (Zhao et al., 2019; Cohan et al., 2019). Our paper involves citation functions into citing sentence generation so that the generated citing sentences can be coherent given their context, and can still contain the motivation for a specific citation.\nIn this paper, we propose a BAckground knowledge- and COntent-based framework, named BACO. Specifically, we encode a citation network based on citation relations among papers to obtain\nbackground knowledge, and the given citing and cited papers to provide content information. We extend a standard pointer-generator (See et al., 2017) to copy words from cited and citing papers, and determine what to cite by estimating sentence salience in the cited paper. The various pieces of captured information are then combined as the context for the decoder. Furthermore, we extend our framework to include why to cite by jointly training the generation with citation function classification and facilitate the acquisition of the content information.\nAs for the dataset, we extended the ACL Anthology Network corpus (AAN) (Radev et al., 2013) with extracted citing sentences by using RegEx. We then hand-annotated the citation functions on a subset of the dataset, and trained a citation function labeling model based on SciBERT (Beltagy et al., 2019). The resulting labeling model was then used to automatically label the rest data to build a large-scale dataset.\nWe summarize our contributions as follows: • We propose a BAckground knowledge- and COntent-based framework, named BACO, for citing sentence generation. •We manually annotated a subset of citing sentences with citation functions to train a SciBERTbased model to automatically label the rest data for citing sentence generation. • Based on the results from experiments, we show that BACO outperforms comparative baselines by at least 2.57 points on ROUGE-2."
    }, {
      "heading" : "2 Related Work",
      "text" : "Several studies on citing sentence generation have used keyword-based summarization methods (Hoang and Kan, 2010; Chen and Zhuge, 2016, 2019). To that end, they built keyword-based trees to extract sentences from cited papers as related work write-ups. These studies have two limitations: First, since related work sections are not simply (chronological) summaries of cited papers, synthesizing prior work in this manner is insufficient. Second, extractive summarization uses verbatim content from cited papers, which implies intellectual property issues (e.g., copyright violations) as well as ethical problems, such as a lack of intellectual engagement with prior work. Alternatively, abstractive summarization approaches, such as methods based on linear programming (Hu and Wan, 2014) and neural seq2seq methods (Wang et al., 2018), have also been explored. These approaches\nmainly focus on utilizing papers’ content information, specifically on the text of cited papers directly. A recent paper that went beyond summarizing the content of cited papers (Xing et al., 2020) used a multi-source, pointer-generator network with a cross attention mechanism to calculate the attention distribution between the citing sentences’ context and the cited paper’s abstract.\nOur paper is based on the premise that citation network analysis can provide background knowledge that facilitates the understanding of papers in a field. Prior analyses of citation networks have been used to reveal the cognitive structure and interconnectedness of scientific (sub-)fields (Moore et al., 2005; Bruner et al., 2010), and to understand and detect trends in academic fields (You et al., 2017; Asatani et al., 2018). Network analysis has also been applied to citation networks to identify influential papers and key concepts (Huang et al., 2018), and to scope out research areas.\nWhile previous studies have shown that using text from citing papers is useful to generate citing sentences, the benefit of other content-based features of a citation (e.g., reasons for citing) is insufficiently understood (Xing et al., 2020). Extant literature on citation context analysis (Moravcsik and Murugesan, 1975; Lipetz, 1965), which focused on the connections between the citing and cited papers with respect to purposes and reasons for citations, has found that citation function (Ding et al., 2014; White, 2004) is an important indicator of why a paper chose to cite specific paper(s). Based on a content analysis of 750 citing sentences from 60 papers published in two prominent physics journals, Lipetz (1965) identified 11 citation functions, such as questioned, affirmed, or refuted cited paper’s premises. Similarly, Moravcsik and Murugesan (1975) qualitatively coded the citation context of 30 articles on high energy physics, finding 10 citation functions grouped into 5 pairs: conceptual-operational, organic-perfunctory, evolutionary-juxtapositional, confirmative-negational, valuable-redundant.\nCitation context analysis has also been used to study the valence of citing papers towards cited papers (Athar, 2011; Abu-Jbara et al., 2013) by classifying citation context as positive, negative, or neutral. In this paper, we adopt Abu-Jbara et al. (2013)’s definition of a positive citation as a citation that explicitly states the strength(s) of a cited paper, or a situation where the citing paper’s work\nis guided by the cited paper. In contrast to that, a negative citation is one that explicitly states the weakness(es) of a cited paper. A neutral citation is one that objectively summarizes the cited paper without an additional evaluation. In addition to these three categories, we also consider mixed citation contexts (Cullars, 1990), which are citations that contain both positive and negative evaluations of a cited papers, or where the evaluation is unclear. Given that our paper is a first attempt to integrate citation functions into citing sentence generation, we opted to start with a straightforward valence category schema before exploring more complex schemas in future work."
    }, {
      "heading" : "3 Dataset and Annotation",
      "text" : "We first extended the AAN1 (Radev et al., 2013) with the extracted citing sentences using RegEx. We followed the process in (Xing et al., 2020) to label 1,200 randomly sampled citing sentences with their citation functions. The mark-up was done by 6 coders who were provided with definitions of positive, negative, neutral, and mixed citation functions, and ample examples for each valence category. Our codebook including definitions and examples of citation functions is shown in Table 1. After the annotation, we randomly split the dataset into 800 instances for training and the remaining 400 for testing. We then used the 800 human-annotated instances to train a citation function labeling model with 10-fold cross validation. The labeling task was treated as a multi-class classification problem.\nOur labeling model was built upon SciBERT (Beltagy et al., 2019), a pre-trained language model based on BERT (Devlin et al., 2019) but trained on a large corpus of scientific text. We added a multilayer perceptron (MLP) to SciBERT, and finetuned the whole model on our dataset. As for the input, we concatenated each citing sentence with its context in the citing paper, and inserted a special tag [CLS] at the beginning and another special tag [SEP] to separate them. The final hidden state that corresponded to [CLS] was used as the aggregate sequence representation. This state was fed into the MLP, followed by the softmax function for predicting the citation function of the citing sentence. We report details of test results and dataset statistics in the Appendix, Section A.1.\n1http://aan.how/download/"
    }, {
      "heading" : "4 Methodology",
      "text" : "Our proposed framework includes an encoder and a generator, as shown in Figure 2. The encoder takes the citation network and the citing and cited papers as input, and encodes them to provide background knowledge and content information, respectively. The generator contains a decoder that can copy words from citing and cited paper while retaining the ability to produce novel words, and a salience estimator that identifies key information from the cited paper. We then trained the framework with citation function classification to enable the recognition of why a paper was cited."
    }, {
      "heading" : "4.1 Encoder",
      "text" : "Our encoder (the yellow shaded area in Figure 2) consists of two parts, a graph encoder that was trained to provide background knowledge based on the citation network, and a hierarchical RNN-based encoder that encodes the content information of the citing and cited papers."
    }, {
      "heading" : "4.1.1 Graph Encoder",
      "text" : "We designed a citation network pre-training method for providing the background knowledge. In detail, we first constructed a citation network as a directed graph G = (V, E). V is a set of nodes/papers2 and E is a set of directed edges. Each edge links a citing paper (source) to a cited paper (target). To utilize G in our task, we employed a graph attention network (GAT) (Veličković et al., 2018) as our graph encoder, which leverages masked self-attentional\n2We use node and paper interchangeably\nlayers to compute the hidden representation of each node. This GAT has been shown to be effective on multiple citation network benchmarks. We input a set of node pairs {(vp, vq)} into it for training of the link prediction task. We pre-trained our graph encoder network using negative sampling to learn the node representations hnp for each paper p, which contains structural information of the citation network and can provide background knowledge for the downstream task."
    }, {
      "heading" : "4.1.2 Hierarchical RNN-based Encoder",
      "text" : "Given the word sequence {cwi} of the citing sentence’s context and the word sequence {awj} of the cited paper’s abstract, we input the embedding of word tokens (e.g., e(wt)) into a hierarchical RNNbased encoder that includes a word-level Bi-LSTM and a sentence-level Bi-LSTM. The output wordlevel representation of the citing sentence’s context is denoted as {hcwi }, and the cited paper’s abstract is encoded similarly as its word-level representation {hawj }. Meanwhile, their sentence-level representations are represented as {hcsm} and {hasn }."
    }, {
      "heading" : "4.2 Generator",
      "text" : "Our generator (the green shaded area in Figure 2) is an extension of the standard pointer generator (See et al., 2017). It integrates both background knowledge and content information as context for text generation. The generator contains a decoder and an additional salience estimator that predicts the salience of sentences in the cited paper’s abstract for refining the corresponding attention."
    }, {
      "heading" : "4.2.1 Decoder",
      "text" : "The decoder is a unidirectional LSTM conditioned on all encoded hidden states. The attention distribution is calculated as in (Bahdanau et al., 2015).\nSince we considered both the citing sentence’s context and the cited paper’s abstract on the source side, we applied the attention mechanism to {hcwi } and {hawj } separately to obtain two attention vectors actxt , a abs t , and their corresponding context vectors cctxt , c abs t at the step t. We then aggregated input context c∗t from the citing sentence’s context, the cited paper’s abstract, and background knowledge by applying a dynamic fusion operation based on modality attention as described in (Moon et al., 2018b,a), which selectively attenuated or amplified each modality based on their importance to the task:\n[attctx; attabs; attnet] = σ(Wm[cctxt ; c abs t ; c net t ] + bm),\n(1)\nãttm = exp(attm)∑\nm′∈{abs,ctx,net} exp(attm′) , (2)\nc∗t = ∑\nm∈{abs,ctx,net}\nãttmcmt , (3)\nwhere cnett = [h n p ;h n q ] represents the learned background knowledge for papers p and q, and is kept constant during all decoding steps t, and [attctx; attabs; attnet] is the attention vector.\nTo enable our model to copy words from both the citing sentence’s context and the cited paper’s\nabstract, we calculated the generation probability and copy probabilities as follows:\n[pgen, pcopy1, pcopy2] = softmax(Wctxcctxt +Wabsc abs t +Wnetc net t +Wdecst\n+Wembe(wt−1) + bptr), (4)\nwhere pgen is the probability of generating words, pcopy1 is the probability of copying words from the citing sentence’s context, pcopy2 is the probability of copying words from the cited paper’s abstract, st represents the hidden state of the decoder at step t, and e(wt−1) indicates the input word embedding. Meanwhile, the context vector c∗t , which can be seen as an enhanced representation of source-side information, was concatenated with the decoder state st to produce the vocabulary distribution Pvocab:\nPvocab = softmax(V′(V[st; c∗t ] + b) + b ′). (5)\nFinally, for each text, we defined an extended vocabulary as the union of the vocabulary and all words appearing in the source text, and calculated the probability distribution over the extended vocabulary to predict words w:\nP (w) =pgenPvocab(w) + pcopy1 ∑\ni:cwi=w\nactxt,i\n+ pcopy2 ∑\ni:awi=w\naabst,i . (6)"
    }, {
      "heading" : "4.2.2 Salience Estimation",
      "text" : "The estimation of the salience of each sentence that occurs in a cited paper’s abstract was used to identify what information needed to be concentrated for the generation. We assumed a sentence’s salience to depend on the citing paper such that the same sentences from one cited paper can have different salience in the context of different citing papers. Hence, we represented this salience as a conditional probability P (si|Dsrc), which can be interpreted as the probability of picking sentence si from a cited paper’s abstract given the citing paper Dsrc.\nWe first obtained the document representation dsrc of a citing paper as the average of all its abstract’s sentence representations. Then, for calculating salience, which is defined as P (si|Dsrc), we designed an attention mechanism that assigns a weight αi to each sentence si in a cited paper’s abstract Dtgt. This weight is expected to be large if the semantics of si are similar to dsrc. Formally, we have:\nαi = v T tanh(Wdocdsrc +Wsenth as i + bsal),\n(7)\nα̃i = αi∑\nsk∈Dtgt αk , (8)\nwhere hasi is the i th sentence representation in the cited paper’s abstract, v,Wdoc,Wsent and bsal are learnable parameters, and α̃i is the salience score of the sentence si.\nWe then used the estimated salience of sentences in the cited paper’s abstract to update the wordlevel attention of the cited paper’s abstract {hawj } so that the decoder can focus on these important sentences during text generation. Considering that the estimated salience α̃i is a sentence weight, we determined each token in a sentence to share the same value of α̃i. Accordingly, the new attention aabst of the cited paper’s abstract became a abs t = α̃ia abs t . After normalizing a abs t , the context vector cabst was updated accordingly."
    }, {
      "heading" : "4.3 Model Training",
      "text" : "During model training, the objective of our framework covers three parts: generation loss, salience estimation loss, and citation function classification."
    }, {
      "heading" : "4.3.1 Generation Loss",
      "text" : "The generation loss was based on the prediction of words from the decoder. We minimized the\nnegative log-likelihood of all target words w∗t and used them as the objective function of generation:\nLgen = − ∑ t logP (w∗t ). (9)"
    }, {
      "heading" : "4.3.2 Salience Estimation Loss",
      "text" : "To include extra supervision into the salience estimation, we adopted a ROUGE-based approximation (Yasunaga et al., 2017) as the target. We assume citing sentences to depend heavily on salient sentences from the cited papers’ abstracts. Based on this premise, we calculated the ROUGE scores between the citing sentence and sentences in the corresponding cited paper’s abstract to obtain an approximation of the salience distribution as the ground-truth. If a sentence shared a high ROUGE score with the citing sentence, this sentence would be considered as a salient sentence because the citing sentence was likely to be generated based on this sentence, while a low ROUGE score implied that this sentence may be ignored during the generation process due to its low salience. Kullback–Leibler divergence was used as our loss function for enforcing the output salience distribution to be close to the normalized ROUGE score distribution of sentences in the cited paper’s abstract:\nLsal = DKL(R‖α̃), (10)\nRi = βr(si)∑\nsk∈Dtgt βr(sk) , (11)\nwhere α̃,R ∈ Rm, Ri refers to the scalar indexed i in R (1 ≤ i ≤ m), and r(si) is the average of ROUGE-1 and ROUGE-2 F1 scores between the sentence si in the cited paper’s abstract and the citing sentence. We also introduced a hyper-parameter β as a constant rescaling factor to sharpen the distribution."
    }, {
      "heading" : "4.3.3 Citation Function Classification",
      "text" : "We added a supplementary component to enable the citation function classification to be trained with the generator, aiming to make the generation conscious of why to cite. Following a prior general pipeline of citation function classification (Cohan et al., 2019; Zhao et al., 2019), we first concatenated the last hidden state sT of the decoder, which we considered as a representation of the generated citing sentence, with the document representation dctx of the citing sentence’s context. Here, dctx was calculated as the average of its sentence representations. We then fed the concatenated representation into an\nMLP followed by the softmax function to predict the probability of the citation function ŷfunc for the generated citing sentence. Cross-entropy loss was set as the objective function for training the classifier with the ground truth label yfunc, which is a one-hot vector:\nLfunc = − 1\nN N∑ i=1 K∑ j=1 yifunc(j) log ŷ i func(j), (12)\nwhere N refers to the size of training data and K is the number of different citation functions.\nFinally, all aforementioned losses were combined as the training objective of the whole framework:\nJ (θ) = Lgen + λSLsal + λFLfunc, (13)\nwhere λS and λF are the hyper-parameters to balance these losses."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Metrics and Baselines",
      "text" : "Following previous work, we report ROUGE-1 (unigram), ROUGE-2 (bigram), and ROUGE-L (longest common subsequence) scores to evaluate the generated citing sentences (Lin, 2004). Implementation details are shown in the Appendix, Section A.2. We also report ROUGE F1 score on our dataset. Finally, we compare our model to competitive baselines: • PTGEN (See et al., 2017): is the original pointer-generator network. • EXT-Oracle (Xing et al., 2020): selects the best possible sentence from the abstract of a cited paper that gives the highest ROUGE w.r.t. the ground truth. This method can be seen as an upper bound of extractive methods. • PTGEN-Cross (Xing et al., 2020): enhances the original pointer-generator network with a cross attention mechanism applied to the citing sentence’s context and the cited paper’s abstract.\nAdditionally, we report results from using several extractive methods that have been used for summarization tasks3, including: • LexRank (Erkan and Radev, 2004): is an unsupervised graph-based method for computing relative importance of extractive summarization. • TextRank (Mihalcea and Tarau, 2004): is an unsupervised algorithm where sentence importance 3We apply extractive methods on the cited paper’s abstract to extract one sentence as the citing sentence.\nscores are computed based on eigenvector centrality within weighted-graphs."
    }, {
      "heading" : "5.2 Experimental Results",
      "text" : "As the results in Table 2 show, our proposed framework (BACO) outperformed all of the considered baselines. BACO achieved scores of 32.54 (ROUGE-1), 9.71 (ROUGE-2), and 24.90 (ROUGE-L). We also observed that the extractive methods performed comparatively poorly and notably worse than the abstractive methods. All abstractive methods did better than EXT-Oracle; a result different from performance on other summarization tasks, such as news document summarization. We think that this deviation from prior performance outcomes is because citing sentence in the domain of scholarly papers contain new expressions when referring to cited papers, which requires high-level summarizing or paraphrasing of cited papers instead of copying sentences verbatim from cited papers. Our results suggest that extractive methods may not be suitable for our task.\nAmong the extractive methods we tested, we observed EXT-Oracle to be superior to others, which aligns with our expectation of EXT-Oracle to serve as an upper bound of extractive methods. For abstractive methods, our framework achieved about 2.57 points improvement on ROUGE-2 F1 score compared to PTGEN-Cross. We assume two reasons for this improvement: First, BACO uses richer text features, e.g., what to cite (sentence salience estimation) and why to cite (citation function classification), that provide useful information for this task. Second, we included structural information from the citation network, which might offer supplemental background knowledge about a field that is not explicitly covered by the given cited and citing papers."
    }, {
      "heading" : "5.3 Ablation Study",
      "text" : "We performed an ablation study to investigate the efficacy of the three main components in our framework: (1) we removed the node features (papers) that are output from the graph encoder to test the effectiveness of background knowledge; (2) we removed the predicted salience of sentences in the abstracts of cited papers to assess the effectiveness of one part of content (what to cite); and (3) we removed the training of citation function classification and only trained the generator to test the effectiveness of the other part of content (why to cite). As the removal of node features of papers re-\nduces the input to the dynamic fusion operation for the context vector (Equation 1), we changed Equation 2 to a sigmoid function so that the calculated attention becomes a vector of size 2 when combining the context vectors of the citing sentence’s context and the cited paper’s abstract.\nTable 3 presents the results of the ablation study. We observed the ROUGE-2 F1 score to drop by 0.81 after the removal of the nodes (papers) feature. This indicates that considering background knowledge in a structured representation is useful for citing sentence generation. The ROUGE-2 F1 score dropped by 2.20 after disregarding salience of sentences in the cited paper. This implies that sentence-level salience estimation is beneficial, and it can be used to identify important sentences during the decoding phase so that the decoder can pay higher attention to those sentences. This process\nmight also align with how scholars write citing sentences: they focus on specific parts or elements of cited papers, e.g., methods or results, and do not consider all parts equally when writing citing sentences. Lastly, the ROUGE-2 F1 score dropped by 1.04 after the removal of citation function classification; indicating that this feature is also helpful to the text generation task. We conclude that for a citing sentence generation, considering and training a model on background knowledge, sentence salience, and citation function improves the performance."
    }, {
      "heading" : "5.4 Case study",
      "text" : "We present an illustrative example generated by our re-implementation of PTGEN-Cross versus by BACO, and compare both to ground truth (see Appendix, Section A.3). The output from BACO showed a higher overlap with the ground truth, specifically because it included background that is not explicitly covered in the cited paper. Furthermore, our output contained the correct citation function (“... have been shown to be effective”), which was present in the ground truth, but missing in PTGEN-Cross’s output."
    }, {
      "heading" : "5.5 Human Evaluation",
      "text" : "We sampled 50 instances from the generated texts. Three graduate students who are fluent in English and familiar with NLP were asked to rate citing sentences produced by BACO and the re-implemented PTGEN-Cross with respect to four aspects on a 1 (very poor) to 5 (excellent) point scale: fluency (whether a citing sentence is fluent), relevance (whether a citing sentence is relevant to the cited paper’s abstract), coherence (whether a citing sentence is coherent within its context), and overall quality. Every instance was scored by the three judges, and we averaged their scores (Table 4). Our results showed that citing sentences generated by BACO score were generally better than output by PTGEN-Cross (e.g., Relevance score: BACO=3.07; PTGEN-Cross=2.64). This finding provided further evidence for the effectiveness of including the features we used for this task."
    }, {
      "heading" : "6 Conclusions and Future Work",
      "text" : "We have brought together multiple pieces of information from and about cited and citing papers to improve citing sentence generation. We integrated them into BACO, a BAckground knowledge- and\nCOntent-based framework for citing sentence generation, which learns and uses information that relate to (1) background knowledge; and (2) content. Extensive experimental results suggest that our framework outperforms competitive baseline models.\nThis work is limited in several ways. We only demonstrated the utility of our model within the standard RNN-based seq2seq framework. Secondly, our citation functions scheme only contained valence-based items. Finally, while this method is intended to support scholars in practicing strategic note taking on prior work with respect to a new literature review or research project, we did not evaluate the usefulness or effectiveness of this training option for researchers.\nIn future work, we plan to investigate the adaptation of our framework into more powerful models such as Transformer (Vaswani et al., 2017). We also hope to extend our citation functions scheme beyond valence of the citing sentences to more fine-grained categories, such as those outlined in Moravcsik and Murugesan (1975) and Lipetz (1965).\nImpact Statement\nThis work is intended to support scholars in doing research, not to replace or automate any scholarly responsibilities. Finding, reading, understanding, reviewing, reflecting upon, and properly citing literature are key components of the research process and require deep intellectual engagement, which remains a human task. The presented approach is meant to help scholars to see examples for how to strategically synthesize scientific papers relevant to a certain topic or research problem, thereby helping them to cope with information overload (or “research deluge”) and honing their scholarly writing skills. Additional professional responsibilities also still apply, such as not violating intellectual property/ copyright issues.\nWe believe that this work does not present foreseeable negative societal consequence. While not intended, our method may be misused for the automated generation of parts of literature reviews. We strongly discourage this misuse as it violates basic assumptions about scholarly diligence, responsibilities, and expectations. We advocate for our method to be used as a scientific writing training tool."
    }, {
      "heading" : "A Appendices",
      "text" : "A.1 Experiments for Citation Function Labeling Model\nTo test our citation function labeling model, we applied 10-fold cross-validation to our training dataset with 800 citing sentences. We then tested our trained model on the test data with 400 sentences, which we refer to as the external test set.\nWe set the hidden size of the MLP in our labeling model to 256, and adopted a dropout with a rate of 0.2. For the optimizer, an Adam (Kingma and Ba, 2015) with a learning rate of 2e-3 was used. The batch size was set to 8. We used F1 score for evaluating labeling accuracy. Since there was imbalance among the distributions of labels, we choose the micro-F1 score specifically. The results are shown in Table 5. After training, we used the trained model to label the rest of the data (84,376 instances) for further training the citing sentence generation model. The final dataset contains 85,576 instances. Following (Xing et al., 2020), we used the above-mentioned 400 citing sentences as the test set, and combined the 800 citing sentences with the rest of the model-labelled instances as our training set. The average length of citing sentences in the training and test data is 28.72 words and 26.45 words, respectively.\nA.2 Implementation Details We used pre-trained Glove vectors (Pennington et al., 2014) to initialize word embeddings with the vector dimension 300 and followed Veličković et al. (2018) to initialize the node features for the graph encoder as a bag-of-words representation of the paper’s abstract. The hidden state size of LSTM was set to 256 for the encoder, and 512 for the decoder. An AdaGrad (Duchi et al., 2011) optimizer was used with a learning rate of 0.15 and an initial accumulator value of 0.1. We picked 64 as the batch size for training. For the rescaling factor β in Equation 11, we chose 40 based on the results reported in Yasunaga et al. (2017). We also used ROUGE scores (Lin, 2004) for quantitative evaluation, and reported the F1 scores of ROUGE-1, ROUGE-2 and ROUGE-L for comparing BACO to alternative models.\nA.3 Case Study We present an example generated by our reimplementation of the baseline PTGEN-Cross, and our framework in Table 6. Note that we used\n#REFR to mark the citation of the cited paper. The reference signs to other papers are masked as #OTHEREFR. The #CITE in context indicates the position where the citing sentence should be inserted. The output of our framework has a higher overlap with the ground truth than the output from PTGEN-Cross. Please note that our framework was able to infer that the mentioned methods in the generated citing sentence are “supervised”, and we believe that this knowledge was gained from the citation network where other neighboring cited papers explicitly mentioned “supervised methods”. Also, the generated citing sentence from our framework showed a positive citation function (... have been shown to be effective) as the ground truth, while PTGEN-Cross’s output expressed the wrong citation function (neutral). We think the underlying reason for this difference in outputs may be that our joint training of citing sentence generation and citation function classification, which forced our framework to recognize the corresponding citation function during the generation and further improved the performance."
    } ],
    "references" : [ {
      "title" : "Purpose and polarity of citation: Towards nlpbased bibliometrics",
      "author" : [ "Amjad Abu-Jbara", "Jefferson Ezra", "Dragomir Radev." ],
      "venue" : "Proceedings of 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Abu.Jbara et al\\.,? 2013",
      "shortCiteRegEx" : "Abu.Jbara et al\\.",
      "year" : 2013
    }, {
      "title" : "Detecting trends in academic research from a citation network using network representation learning",
      "author" : [ "Kimitaka Asatani", "Junichiro Mori", "Masanao Ochi", "Ichiro Sakata." ],
      "venue" : "PloS one, 13(5):e0197260.",
      "citeRegEx" : "Asatani et al\\.,? 2018",
      "shortCiteRegEx" : "Asatani et al\\.",
      "year" : 2018
    }, {
      "title" : "Sentiment analysis of citations using sentence structure-based features",
      "author" : [ "Awais Athar." ],
      "venue" : "Proceedings of the ACL 2011 Student Session.",
      "citeRegEx" : "Athar.,? 2011",
      "shortCiteRegEx" : "Athar.",
      "year" : 2011
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyung Hyun Cho", "Yoshua Bengio." ],
      "venue" : "3rd International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Scibert: A pretrained language model for scientific text",
      "author" : [ "Iz Beltagy", "Kyle Lo", "Arman Cohan." ],
      "venue" : "Proceedings of 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language",
      "citeRegEx" : "Beltagy et al\\.,? 2019",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2019
    }, {
      "title" : "An appraisal of athlete development models through citation network analysis",
      "author" : [ "Mark W Bruner", "Karl Erickson", "Brian Wilson", "Jean Côté." ],
      "venue" : "Psychology of sport and exercise, 11(2):133–139.",
      "citeRegEx" : "Bruner et al\\.,? 2010",
      "shortCiteRegEx" : "Bruner et al\\.",
      "year" : 2010
    }, {
      "title" : "Summarization of related work through citations",
      "author" : [ "Jingqiang Chen", "Hai Zhuge." ],
      "venue" : "12th International Conference on Semantics, Knowledge and Grids (SKG). IEEE.",
      "citeRegEx" : "Chen and Zhuge.,? 2016",
      "shortCiteRegEx" : "Chen and Zhuge.",
      "year" : 2016
    }, {
      "title" : "Automatic generation of related work through summarizing citations",
      "author" : [ "Jingqiang Chen", "Hai Zhuge." ],
      "venue" : "Concurrency and Computation: Practice and Experience, 31(3):e4261.",
      "citeRegEx" : "Chen and Zhuge.,? 2019",
      "shortCiteRegEx" : "Chen and Zhuge.",
      "year" : 2019
    }, {
      "title" : "A bibliometric investigation of research performance in emerging nanobiopharmaceuticals",
      "author" : [ "Kaihua Chen", "Jiancheng Guan." ],
      "venue" : "Journal of Informetrics, 5(2):233–247.",
      "citeRegEx" : "Chen and Guan.,? 2011",
      "shortCiteRegEx" : "Chen and Guan.",
      "year" : 2011
    }, {
      "title" : "Structural scaffolds for citation intent classification in scientific publications",
      "author" : [ "Arman Cohan", "Waleed Ammar", "Madeleine van Zuylen", "Field Cady." ],
      "venue" : "Proceedings of 2019 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Cohan et al\\.,? 2019",
      "shortCiteRegEx" : "Cohan et al\\.",
      "year" : 2019
    }, {
      "title" : "Citation characteristics of italian and spanish literary monographs",
      "author" : [ "John Cullars." ],
      "venue" : "The Library Quarterly, 60(4):337–356.",
      "citeRegEx" : "Cullars.,? 1990",
      "shortCiteRegEx" : "Cullars.",
      "year" : 1990
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of 2019 Conference of the North",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Content-based citation analysis: The next generation of citation analysis",
      "author" : [ "Ying Ding", "Guo Zhang", "Tamy Chambers", "Min Song", "Xiaolong Wang", "Chengxiang Zhai." ],
      "venue" : "Journal of the Association for Information Science and Technology, 65(9):1820–",
      "citeRegEx" : "Ding et al\\.,? 2014",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2014
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer." ],
      "venue" : "Journal of machine learning research, 12(Jul):2121–2159.",
      "citeRegEx" : "Duchi et al\\.,? 2011",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Blind men and elephants: What do citation summaries tell us about a research article",
      "author" : [ "Aaron Elkiss", "Siwei Shen", "Anthony Fader", "Güneş Erkan", "David States", "Dragomir Radev" ],
      "venue" : "Journal of the American Society",
      "citeRegEx" : "Elkiss et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Elkiss et al\\.",
      "year" : 2008
    }, {
      "title" : "Lexrank: Graph-based lexical centrality as salience in text summarization",
      "author" : [ "Günes Erkan", "Dragomir R Radev." ],
      "venue" : "Journal of Artificial Intelligence Research, 22:457–479.",
      "citeRegEx" : "Erkan and Radev.,? 2004",
      "shortCiteRegEx" : "Erkan and Radev.",
      "year" : 2004
    }, {
      "title" : "Towards automated related work summarization",
      "author" : [ "Cong Duy Vu Hoang", "Min-Yen Kan." ],
      "venue" : "Proceedings of the 23rd International Conference on Computational Linguistics (COLING).",
      "citeRegEx" : "Hoang and Kan.,? 2010",
      "shortCiteRegEx" : "Hoang and Kan.",
      "year" : 2010
    }, {
      "title" : "Emerging trends and new developments in information science: A document co-citation analysis (2009–2016)",
      "author" : [ "Jianhua Hou", "Xiucai Yang", "Chaomei Chen." ],
      "venue" : "Scientometrics, 115(2):869–892.",
      "citeRegEx" : "Hou et al\\.,? 2018",
      "shortCiteRegEx" : "Hou et al\\.",
      "year" : 2018
    }, {
      "title" : "Automatic generation of related work sections in scientific papers: an optimization approach",
      "author" : [ "Yue Hu", "Xiaojun Wan." ],
      "venue" : "Proceedings of 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Hu and Wan.,? 2014",
      "shortCiteRegEx" : "Hu and Wan.",
      "year" : 2014
    }, {
      "title" : "Topicsensitive influential paper discovery in citation network",
      "author" : [ "Xin Huang", "Chang-an Chen", "Changhuan Peng", "Xudong Wu", "Luoyi Fu", "Xinbing Wang." ],
      "venue" : "Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer.",
      "citeRegEx" : "Huang et al\\.,? 2018",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2018
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Exploring evolution and emerging trends in business model study: a co-citation analysis",
      "author" : [ "Xuerong Li", "Han Qiao", "Shouyang Wang." ],
      "venue" : "Scientometrics, 111(2):869–887.",
      "citeRegEx" : "Li et al\\.,? 2017",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Improvement of the selectivity of citation indexes to science literature through inclusion of citation relationship indicators",
      "author" : [ "Ben-Ami Lipetz." ],
      "venue" : "American Documentation, 16(2):81–90.",
      "citeRegEx" : "Lipetz.,? 1965",
      "shortCiteRegEx" : "Lipetz.",
      "year" : 1965
    }, {
      "title" : "Citation practices among non-native expert and novice scientific writers",
      "author" : [ "Kobra Mansourizadeh", "Ummul K. Ahmad." ],
      "venue" : "Journal of English for Academic Purposes, 10(3):152–161.",
      "citeRegEx" : "Mansourizadeh and Ahmad.,? 2011",
      "shortCiteRegEx" : "Mansourizadeh and Ahmad.",
      "year" : 2011
    }, {
      "title" : "Textrank: Bringing order into text",
      "author" : [ "Rada Mihalcea", "Paul Tarau." ],
      "venue" : "Proceedings of 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Mihalcea and Tarau.,? 2004",
      "shortCiteRegEx" : "Mihalcea and Tarau.",
      "year" : 2004
    }, {
      "title" : "Multimodal named entity disambiguation for noisy social media posts",
      "author" : [ "Seungwhan Moon", "Leonardo Neves", "Vitor Carvalho." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Moon et al\\.,? 2018a",
      "shortCiteRegEx" : "Moon et al\\.",
      "year" : 2018
    }, {
      "title" : "Multimodal named entity recognition for short social media posts",
      "author" : [ "Seungwhan Moon", "Leonardo Neves", "Vitor Carvalho." ],
      "venue" : "Proceedings of 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Moon et al\\.,? 2018b",
      "shortCiteRegEx" : "Moon et al\\.",
      "year" : 2018
    }, {
      "title" : "The privileging of communitarian ideas: citation practices and the translation of social capital into public health research",
      "author" : [ "Spencer Moore", "Alan Shiell", "Penelope Hawe", "Valerie A Haines." ],
      "venue" : "American Journal of Public Health, 95(8):1330–1337.",
      "citeRegEx" : "Moore et al\\.,? 2005",
      "shortCiteRegEx" : "Moore et al\\.",
      "year" : 2005
    }, {
      "title" : "Some results on the function and quality of citations",
      "author" : [ "Michael J Moravcsik", "Poovanalingam Murugesan." ],
      "venue" : "Social Studies of Science, 5(1):86–92.",
      "citeRegEx" : "Moravcsik and Murugesan.,? 1975",
      "shortCiteRegEx" : "Moravcsik and Murugesan.",
      "year" : 1975
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2014 conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "The acl anthology network corpus",
      "author" : [ "Dragomir R Radev", "Pradeep Muthukrishnan", "Vahed Qazvinian", "Amjad Abu-Jbara." ],
      "venue" : "Language Resources and Evaluation, 47(4):919–944.",
      "citeRegEx" : "Radev et al\\.,? 2013",
      "shortCiteRegEx" : "Radev et al\\.",
      "year" : 2013
    }, {
      "title" : "Automatic related work section generation: experiments in scientific document abstracting",
      "author" : [ "Horacio Saggion", "Alexander Shvets", "Àlex Bravo" ],
      "venue" : null,
      "citeRegEx" : "Saggion et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Saggion et al\\.",
      "year" : 2020
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J Liu", "Christopher D Manning." ],
      "venue" : "Proceedings of 55th Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "Citation analysis",
      "author" : [ "Linda C Smith." ],
      "venue" : "Library Trends, 30(3):83–106.",
      "citeRegEx" : "Smith.,? 1981",
      "shortCiteRegEx" : "Smith.",
      "year" : 1981
    }, {
      "title" : "Automatic classification of citation function",
      "author" : [ "Simone Teufel", "Advaith Siddharthan", "Dan Tidhar." ],
      "venue" : "Proceedings of 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Teufel et al\\.,? 2006",
      "shortCiteRegEx" : "Teufel et al\\.",
      "year" : 2006
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of 31st International Conference on Neural Information Processing Systems",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Graph attention networks",
      "author" : [ "Petar Veličković", "Guillem Cucurull", "Arantxa Casanova", "Adriana Romero", "Pietro Liò", "Yoshua Bengio." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Veličković et al\\.,? 2018",
      "shortCiteRegEx" : "Veličković et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural related work summarization with a joint context-driven attention mechanism",
      "author" : [ "Yongzhen Wang", "Xiaozhong Liu", "Zheng Gao." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Citation analysis and discourse analysis revisited",
      "author" : [ "Howard D White." ],
      "venue" : "Applied linguistics, 25(1):89–116.",
      "citeRegEx" : "White.,? 2004",
      "shortCiteRegEx" : "White.",
      "year" : 2004
    }, {
      "title" : "Automatic generation of citation texts in scholarly papers: A pilot study",
      "author" : [ "Xinyu Xing", "Xiaosheng Fan", "Xiaojun Wan." ],
      "venue" : "Proceedings of 58th Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Xing et al\\.,? 2020",
      "shortCiteRegEx" : "Xing et al\\.",
      "year" : 2020
    }, {
      "title" : "Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks",
      "author" : [ "Michihiro Yasunaga", "Jungo Kasai", "Rui Zhang", "Alexander R Fabbri", "Irene Li", "Dan Friedman", "Dragomir R Radev." ],
      "venue" : "In",
      "citeRegEx" : "Yasunaga et al\\.,? 2019",
      "shortCiteRegEx" : "Yasunaga et al\\.",
      "year" : 2019
    }, {
      "title" : "Graph-based neural multi-document summarization",
      "author" : [ "Michihiro Yasunaga", "Rui Zhang", "Kshitijh Meelu", "Ayush Pareek", "Krishnan Srinivasan", "Dragomir Radev." ],
      "venue" : "Proceedings of 21st Conference on Computational Natural Language Learning",
      "citeRegEx" : "Yasunaga et al\\.,? 2017",
      "shortCiteRegEx" : "Yasunaga et al\\.",
      "year" : 2017
    }, {
      "title" : "Development trend forecasting for coherent light generator technology based on patent citation network analysis",
      "author" : [ "Hanlin You", "Mengjun Li", "Keith W Hipel", "Jiang Jiang", "Bingfeng Ge", "Hante Duan." ],
      "venue" : "Scientometrics, 111(1):297–315.",
      "citeRegEx" : "You et al\\.,? 2017",
      "shortCiteRegEx" : "You et al\\.",
      "year" : 2017
    }, {
      "title" : "A context-based framework for modeling the role and function of on-line resource citations in scientific literature",
      "author" : [ "He Zhao", "Zhunchen Luo", "Chong Feng", "Anqing Zheng", "Xiaopeng Liu." ],
      "venue" : "Proceedings of 2019 Conference on Empirical Methods in",
      "citeRegEx" : "Zhao et al\\.,? 2019",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 34,
      "context" : "A citation systematically, strategically, and critically synthesizes content from a cited paper in the context of a citing paper (Smith, 1981).",
      "startOffset" : 129,
      "endOffset" : 142
    }, {
      "referenceID" : 14,
      "context" : ", in terms of theory, methods, or findings (Elkiss et al., 2008).",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 39,
      "context" : "Writing meaningful and concise citing sentences that capture the gist of cited papers and identify connections between citing and cited papers is not trivial (White, 2004).",
      "startOffset" : 158,
      "endOffset" : 171
    }, {
      "referenceID" : 24,
      "context" : "Learning how to write up information about related work with appropriate and meaningful citations is particularly challenging for new scholars (Mansourizadeh and Ahmad, 2011).",
      "startOffset" : 143,
      "endOffset" : 174
    }, {
      "referenceID" : 18,
      "context" : "A number of recent papers have focused on the task of citing sentence generation (Hu and Wan, 2014; Saggion et al., 2020; Xing et al., 2020), which is defined as generating a short text that describes a cited paper B in the context of a citing",
      "startOffset" : 81,
      "endOffset" : 140
    }, {
      "referenceID" : 32,
      "context" : "A number of recent papers have focused on the task of citing sentence generation (Hu and Wan, 2014; Saggion et al., 2020; Xing et al., 2020), which is defined as generating a short text that describes a cited paper B in the context of a citing",
      "startOffset" : 81,
      "endOffset" : 140
    }, {
      "referenceID" : 40,
      "context" : "A number of recent papers have focused on the task of citing sentence generation (Hu and Wan, 2014; Saggion et al., 2020; Xing et al., 2020), which is defined as generating a short text that describes a cited paper B in the context of a citing",
      "startOffset" : 81,
      "endOffset" : 140
    }, {
      "referenceID" : 17,
      "context" : "Using citation networks was inspired by the fact that scholars have analyzed such networks to identify the main themes and research developments in domain areas such as information sciences (Hou et al., 2018), busi-",
      "startOffset" : 190,
      "endOffset" : 208
    }, {
      "referenceID" : 31,
      "context" : "1467 Figure 1: An example from our dataset (source: ACL Anthology Network corpus (Radev et al., 2013).",
      "startOffset" : 81,
      "endOffset" : 101
    }, {
      "referenceID" : 21,
      "context" : "ness modeling (Li et al., 2017), and pharmaceutical research (Chen and Guan, 2011).",
      "startOffset" : 14,
      "endOffset" : 31
    }, {
      "referenceID" : 8,
      "context" : ", 2017), and pharmaceutical research (Chen and Guan, 2011).",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 41,
      "context" : "Therefore, we follow the example of estimating salient sentences (Yasunaga et al., 2019) and use the predicted salience to filter crucial information that should be integrated into the resulting citing sentence; (2) Why to cite - we define “citation function” as an approximation of an author’s reason for",
      "startOffset" : 65,
      "endOffset" : 88
    }, {
      "referenceID" : 44,
      "context" : "A number of previous research on citation functions has used citing sentences and their context for classification (Zhao et al., 2019; Cohan et al., 2019).",
      "startOffset" : 115,
      "endOffset" : 154
    }, {
      "referenceID" : 9,
      "context" : "A number of previous research on citation functions has used citing sentences and their context for classification (Zhao et al., 2019; Cohan et al., 2019).",
      "startOffset" : 115,
      "endOffset" : 154
    }, {
      "referenceID" : 33,
      "context" : "We extend a standard pointer-generator (See et al., 2017) to copy words from cited and citing papers, and determine what to cite by estimating sentence salience in the cited paper.",
      "startOffset" : 39,
      "endOffset" : 57
    }, {
      "referenceID" : 31,
      "context" : "As for the dataset, we extended the ACL Anthology Network corpus (AAN) (Radev et al., 2013) with extracted citing sentences by using RegEx.",
      "startOffset" : 71,
      "endOffset" : 91
    }, {
      "referenceID" : 4,
      "context" : "a subset of the dataset, and trained a citation function labeling model based on SciBERT (Beltagy et al., 2019).",
      "startOffset" : 89,
      "endOffset" : 111
    }, {
      "referenceID" : 16,
      "context" : "Several studies on citing sentence generation have used keyword-based summarization methods (Hoang and Kan, 2010; Chen and Zhuge, 2016, 2019).",
      "startOffset" : 92,
      "endOffset" : 141
    }, {
      "referenceID" : 18,
      "context" : "Alternatively, abstractive summarization approaches, such as methods based on linear programming (Hu and Wan, 2014) and neural seq2seq methods (Wang et al.",
      "startOffset" : 97,
      "endOffset" : 115
    }, {
      "referenceID" : 38,
      "context" : "Alternatively, abstractive summarization approaches, such as methods based on linear programming (Hu and Wan, 2014) and neural seq2seq methods (Wang et al., 2018), have also been explored.",
      "startOffset" : 143,
      "endOffset" : 162
    }, {
      "referenceID" : 40,
      "context" : "A recent paper that went beyond summarizing the content of cited papers (Xing et al., 2020) used a multi-source, pointer-generator network with a cross attention mechanism to calculate the attention distribution between the citing sentences’ context and the cited paper’s abstract.",
      "startOffset" : 72,
      "endOffset" : 91
    }, {
      "referenceID" : 28,
      "context" : "Prior analyses of citation networks have been used to reveal the cognitive structure and interconnectedness of scientific (sub-)fields (Moore et al., 2005; Bruner et al., 2010), and to understand and detect trends in academic fields (You et al.",
      "startOffset" : 135,
      "endOffset" : 176
    }, {
      "referenceID" : 5,
      "context" : "Prior analyses of citation networks have been used to reveal the cognitive structure and interconnectedness of scientific (sub-)fields (Moore et al., 2005; Bruner et al., 2010), and to understand and detect trends in academic fields (You et al.",
      "startOffset" : 135,
      "endOffset" : 176
    }, {
      "referenceID" : 43,
      "context" : ", 2010), and to understand and detect trends in academic fields (You et al., 2017; Asatani et al., 2018).",
      "startOffset" : 64,
      "endOffset" : 104
    }, {
      "referenceID" : 1,
      "context" : ", 2010), and to understand and detect trends in academic fields (You et al., 2017; Asatani et al., 2018).",
      "startOffset" : 64,
      "endOffset" : 104
    }, {
      "referenceID" : 19,
      "context" : "also been applied to citation networks to identify influential papers and key concepts (Huang et al., 2018), and to scope out research areas.",
      "startOffset" : 87,
      "endOffset" : 107
    }, {
      "referenceID" : 40,
      "context" : ", reasons for citing) is insufficiently understood (Xing et al., 2020).",
      "startOffset" : 51,
      "endOffset" : 70
    }, {
      "referenceID" : 29,
      "context" : "Extant literature on citation context analysis (Moravcsik and Murugesan, 1975; Lipetz, 1965), which focused on the connections between the citing and",
      "startOffset" : 47,
      "endOffset" : 92
    }, {
      "referenceID" : 23,
      "context" : "Extant literature on citation context analysis (Moravcsik and Murugesan, 1975; Lipetz, 1965), which focused on the connections between the citing and",
      "startOffset" : 47,
      "endOffset" : 92
    }, {
      "referenceID" : 12,
      "context" : "cited papers with respect to purposes and reasons for citations, has found that citation function (Ding et al., 2014; White, 2004) is an important indicator of why a paper chose to cite specific paper(s).",
      "startOffset" : 98,
      "endOffset" : 130
    }, {
      "referenceID" : 39,
      "context" : "cited papers with respect to purposes and reasons for citations, has found that citation function (Ding et al., 2014; White, 2004) is an important indicator of why a paper chose to cite specific paper(s).",
      "startOffset" : 98,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "Citation context analysis has also been used to study the valence of citing papers towards cited papers (Athar, 2011; Abu-Jbara et al., 2013) by classifying citation context as positive, negative, or neutral.",
      "startOffset" : 104,
      "endOffset" : 141
    }, {
      "referenceID" : 0,
      "context" : "Citation context analysis has also been used to study the valence of citing papers towards cited papers (Athar, 2011; Abu-Jbara et al., 2013) by classifying citation context as positive, negative, or neutral.",
      "startOffset" : 104,
      "endOffset" : 141
    }, {
      "referenceID" : 10,
      "context" : "In addition to these three categories, we also consider mixed citation contexts (Cullars, 1990), which are citations that contain both positive and negative evaluations of a cited papers, or where the evaluation is unclear.",
      "startOffset" : 80,
      "endOffset" : 95
    }, {
      "referenceID" : 31,
      "context" : "We first extended the AAN1 (Radev et al., 2013) with the extracted citing sentences using RegEx.",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 40,
      "context" : "We followed the process in (Xing et al., 2020) to label 1,200 randomly sampled citing sentences with their citation functions.",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 4,
      "context" : "Our labeling model was built upon SciBERT (Beltagy et al., 2019), a pre-trained language model based on BERT (Devlin et al.",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 11,
      "context" : ", 2019), a pre-trained language model based on BERT (Devlin et al., 2019) but trained",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 37,
      "context" : "To utilize G in our task, we employed a graph attention network (GAT) (Veličković et al., 2018) as our graph encoder, which leverages masked self-attentional",
      "startOffset" : 70,
      "endOffset" : 95
    }, {
      "referenceID" : 33,
      "context" : "Our generator (the green shaded area in Figure 2) is an extension of the standard pointer generator (See et al., 2017).",
      "startOffset" : 100,
      "endOffset" : 118
    }, {
      "referenceID" : 42,
      "context" : "To include extra supervision into the salience estimation, we adopted a ROUGE-based approximation (Yasunaga et al., 2017) as the target.",
      "startOffset" : 98,
      "endOffset" : 121
    }, {
      "referenceID" : 9,
      "context" : "Following a prior general pipeline of citation function classification (Cohan et al., 2019; Zhao et al., 2019), we first concatenated the last hidden state sT of the decoder, which we considered as a representation of the generated citing sentence, with the document representation dctx of the citing sentence’s context.",
      "startOffset" : 71,
      "endOffset" : 110
    }, {
      "referenceID" : 44,
      "context" : "Following a prior general pipeline of citation function classification (Cohan et al., 2019; Zhao et al., 2019), we first concatenated the last hidden state sT of the decoder, which we considered as a representation of the generated citing sentence, with the document representation dctx of the citing sentence’s context.",
      "startOffset" : 71,
      "endOffset" : 110
    }, {
      "referenceID" : 22,
      "context" : "Following previous work, we report ROUGE-1 (unigram), ROUGE-2 (bigram), and ROUGE-L (longest common subsequence) scores to evaluate the generated citing sentences (Lin, 2004).",
      "startOffset" : 163,
      "endOffset" : 174
    }, {
      "referenceID" : 33,
      "context" : "Finally, we compare our model to competitive baselines: • PTGEN (See et al., 2017): is the original pointer-generator network.",
      "startOffset" : 64,
      "endOffset" : 82
    }, {
      "referenceID" : 40,
      "context" : "• EXT-Oracle (Xing et al., 2020): selects the best possible sentence from the abstract of a cited paper that gives the highest ROUGE w.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 40,
      "context" : "• PTGEN-Cross (Xing et al., 2020): enhances the original pointer-generator network with a cross attention mechanism applied to the citing sentence’s context and the cited paper’s abstract.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 15,
      "context" : "Additionally, we report results from using several extractive methods that have been used for summarization tasks3, including: • LexRank (Erkan and Radev, 2004): is an unsupervised graph-based method for computing relative importance of extractive summarization.",
      "startOffset" : 137,
      "endOffset" : 160
    }, {
      "referenceID" : 25,
      "context" : "• TextRank (Mihalcea and Tarau, 2004): is an unsupervised algorithm where sentence importance",
      "startOffset" : 11,
      "endOffset" : 37
    }, {
      "referenceID" : 36,
      "context" : "In future work, we plan to investigate the adaptation of our framework into more powerful models such as Transformer (Vaswani et al., 2017).",
      "startOffset" : 117,
      "endOffset" : 139
    } ],
    "year" : 2021,
    "abstractText" : "In this paper, we focus on the problem of citing sentence generation, which entails generating a short text to capture the salient information in a cited paper and the connection between the citing and cited paper. We present BACO, a BAckground knowledgeand COntent-based framework for citing sentence generation, which considers two types of information: (1) background knowledge by leveraging structural information from a citation network; and (2) content, which represents in-depth information about what to cite and why to cite. First, a citation network is encoded to provide background knowledge. Second, we apply salience estimation to identify what to cite by estimating the importance of sentences in the cited paper. During the decoding stage, both types of information are combined to facilitate the text generation. We then conduct joint training of the generator and citation function classification to make the model aware of why to cite. Our experimental results show that our framework outperforms comparative baselines.",
    "creator" : "LaTeX with hyperref"
  }
}