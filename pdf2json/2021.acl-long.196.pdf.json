{
  "name" : "2021.acl-long.196.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "KACE: Generating Knowledge-Aware Contrastive Explanations for Natural Language Inference",
    "authors" : [ "Qianglong Chen", "Feng Ji", "Xiangji Zeng", "Feng-Lin Li", "Ji Zhang", "Haiqing Chen", "Yin Zhang" ],
    "emails" : [ "chenqianglong@zju.edu.cn", "zengxiangji@zju.edu.cn", "zhangyin98@zju.edu.cn", "fenglin.lfl@alibaba-inc.com", "zj122146@alibaba-inc.com", "haiqing.chenhq@alibaba-inc.com", "neilji@tencent.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2516–2527\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2516"
    }, {
      "heading" : "1 Introduction",
      "text" : "In recent years, pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have been widely adopted in many tasks of natural language processing (Talmor et al., 2019; Choi et al., 2018; Bowman et al., 2015). However, due to\n∗ Work is done during internship at Alibaba Group. † The work is mainly conducted while being at Alibaba\nGroup. ‡ Corresponding Author: Yin Zhang\nthe lack of textual explanations, most downstream models become more complicated and difficult to understand. End users, especially those working in critical domains such as healthcare or online education, become more skeptical and reluctant to adopt or trust them, although these models have been proved to improve the decision-making performance. Therefore, providing faithful textual explanations has become a promising way to overcome the black-box property of neural networks, which has attracted the attention of academia and industrial communities.\nRecently, the majority of existing methods (Xu et al., 2020; Cheng et al., 2020; Karimi et al., 2020; Ramamurthy et al., 2020; Atanasova et al., 2020; Kumar and Talukdar, 2020) in natural language processing try to explain the predictions of neural models in a model-intrinsic or model-agnostic (also known as post-hoc) way. While post-hoc models (Chen et al., 2020b; Karimi et al., 2020; Kumar and Talukdar, 2020) provide explanations after making predictions without affecting the overall accuracy, most of them neglect the rationales in inputs and provide textual explanations just in the form of “WHY A”. However, we argue that contrastive explanations in the form of “WHY A NOT B” could provide more informative and important clues that are easier to understand and persuade end-users. Moreover, we believe that contrastive explanations could benefit downstream tasks (e.g., NLI), since such kind of explanations contain more helpful information (e.g. relations between rationales) that can be used to improve model performance.\nTo further enhance the explainability and performance of NLI, we propose a novel textual contrastive explanation generation framework in this paper, which is post-hoc and considers rationales, counterfactual examples, and external knowledge. Specifically, we first identify rationales (i.e., key phrases) from a premise-hypothesis (P-H) pair with\nlabel A, and then use them as the key perturbations for transforming and generating candidate counterfactual examples. Then we further select one most qualified counterfactual example for any other label B. Note that the acquisition of a qualified counterfactual example of class B is essential to generate a meaningful explanation for “WHY NOT B”, otherwise the resultant contrastive explanation will be groundless or useless. After that, we take the selected examples along with the original P-H pair and related external knowledge as input, and finally employ a knowledge-aware pre-trained language model to generate contrastive explanation, which will specify why the prediction label is A rather than B, and clarify the confusions for end-users. Moreover, we train an NLI model enhanced with contrastive explanations and achieve the new stateof-art performance on SNLI.\nThe contributions of this paper are as follows:\n• We introduce a novel knowledge-aware contrastive explanation generation framework (KACE) for natural language inference tasks.\n• We consider the rationales in inputs and regard them as important perturbations for generating counterfactual examples rather than just discarding them like previous post-hoc work (Hendricks et al., 2018; Cheng et al., 2020).\n• We integrate external knowledge with generative pre-trained language model rather than only taking original inputs (Kumar and Taluk-\ndar, 2020; Rajani et al., 2019) for contrastive explanation generation.\n• Experimental results show that knowledgeaware contrastive explanations are able to clarify the difference between predicted class and the others, which help to clarify the confusion of end-users and further improve model performance than “WHY A” explanations1."
    }, {
      "heading" : "2 Task Definition and Overall Workflow",
      "text" : "Here, we define the task of contrastive explanation generation for NLI. Given a trained neural network model f with input x and predicted class A, the problem of generating contrastive explanations (CE) to an input x is to specify why x belongs to category/class A rather than B, defined as:\nr = Rationales(x,A) (1)\nx′ = Reversal(x,B, r) (2) CE = Generator(x′, x, A) (3)\nIn Equation 1, we first identify a set of rationales in given inputs, as described in Section 3.1, and in Equation 2 we generate counterfactual examples with reversal mechanism as presented in Section 3.2. In Equation 3, we take the selected counterfactual example along with original example and external knowledge as input, and employ a knowledge-aware generator to produce contrastive explanation as detailed in Section 3.3.\n1Our code will be released as soon as possible at https://github.com/AI4NLP/KACE"
    }, {
      "heading" : "3 Approach",
      "text" : ""
    }, {
      "heading" : "3.1 Rationale Identification",
      "text" : "Considering that rationales are important features of an instance, it is essential to regard rationales as key perturbations for counterfactual example generation. In this paper, we formulate rationale identification as a token-level sequence labelling task where 1 indicates a rationale token and 0 indicates a background token.\nBeing similar with (Thorne et al., 2019), we first construct the input sequence for a premise p and a hypothesis h as Sp=〈s〉Label 〈s〉Premise 〈s〉 and Sh=〈s〉Hypothesis 〈s〉, where 〈s〉 is a special token that separates the components. Let y represent the relation between Sp and Sh where y ∈ {entailment, contradiction, neutral}. For each instance, we need to identify a subset r of zero or more tokens as rationales from both premise and hypothesis sentences. Both premise and hypothesis are encoded with RoBERTa (Liu et al., 2019), yielding hidden representation Hp=[· · · , hpj , · · · ] and Hh=[· · · , hhi , · · · ] respectively.\nAs rationalizer is proposed by (Zhao and Vydiswaran, 2021), we follow this work for rationale identification using cross attention to embed the hypothesis (premise) into premise (hypothesis), which is defined as:\naij = exp((hhi ) TTanh(W T1 h p j ))∑Lp\nm=0 exp((h h i ) TTanh(W T1 h p m))\n(4)\nĥhi = [h h i , Pooling(H p), ∑ k aijh p j ] (5)\nwhere aij denotes the attention score of jth token in premise to the ith token in the hypothesis, Lp denotes the length of the premise sentence and W1 is a trainable parameter matrix. The representation of ith token in the hypothesis, denoted as ĥhi , is created by concatenating its original state representation, max-pooling representation over hp, and the corresponding sum of attention representation from hp. At last, we use a softmax layer with a linear transformation to model the probability of the ith token in Sh being a rationale token."
    }, {
      "heading" : "3.2 Counterfactual Example Generation",
      "text" : "As we have introduced above, counterfactual examples of other classes are of key importance to generate contrastive explanations. In this part, we describe how to generate counterfactual examples.\nGiven a trained neural network model f , the problem of generating counterfactual example for an instance x is to find a set of examples c1, c2, ..., ck that lead to a desired prediction y′. The counterfactual examples are explainable and contrastive when they appropriately consider proximity, diversity and validity.\nHere, we define a three-part loss function to select qualified counterfactual example:\nL = Lvalid + λ1Ldist + λ2Ldiv (6)\nwhere λ1 and λ2 are hyperparameters for balancing Ldist and Ldiv. For generating counterfactual example, the validity term, which ensures the generated counterfactual examples have desired prediction target, is defined as:\nLvalid = k∑ i=1 loss(f(ci), y ′) (7)\nMeanwhile, the generated examples should be proximal to the original instance as described in (Cheng et al., 2020), which means only a small change needs to be made. We do not expect a big change that transforms a large portion of the original, in which way there will be no difference with merely presenting an example of counter classes and the corresponding explanation will be uninformative or useless. That is, we expect that resultant examples are able to preserve the main content of input while changing domain-related parts.\nLdist = k∑\ni=1\ndist(ci, x) (8)\nIn this paper, we choose a weighted Heterogeneous Manhattan-Overlay Metric (Wilson and Martinez, 1997) to calculate the distance as follows:\ndist(c, x) = ∑ t dt(c t, xt) (9)\nwhere t indicates a rationale. To achieve diversity, we want generated examples to be different from each other. Specifically,\nwe calculate the pairwise distance of a set of counterfactual examples and minimize:\nLdiv = − 1\nk k∑ i=1 k∑ j=i dist(ci, cj) (10)\nAfter defining the loss function, we use a reversal mechanism to produce counterfactual examples. In the reversal mechanism, we use hypernym and hyponym of tokens in WordNet2 for perturbation.\nFor example, as shown in Figure 2, the original premise and hypothesis are “a woman and a young child are making sculptures out of clay” and “a man and a woman painting on canvas”, and the label is “contradiction”. We find from WordNet the hypernyms of “making sculptures out of clay” and “painting on canvas” as “doing art” and “making something” respectively. We replace them with their hypernyms to obtain counterfactual examples, and use the model f trained on the original P-H training dataset to predict the resultant examples (Equation 7), and keep those belong to neutral or entailment. After the validity justification, we perform further selection by following Equation 8 and Equation 10, and choose the samples with the smallest loss for neutral and entailment for latter contrastive explanation generation."
    }, {
      "heading" : "3.3 Contrastive Explanation Generation",
      "text" : "After obtaining qualified counterfactual examples, some work (Cheng et al., 2020; Wachter et al.,\n2https://wordnet.princeton.edu/\n2017; Verma et al., 2020) provides them as counterfactual explanation directly. However, since counterfactual examples do not provide explanations explicitly, it could be difficult for users to understand. Hence, in this part, we focus on generating contrastive explanation via knowledge-aware generative language model, which explain “WHY A NOT B” rather than merely “WHY A”.\nWhile traditional approach generate explanation with SHAP3 or LIME4, recent work has exploited to use pre-trained generative language models (Radford et al., 2019; Lewis et al., 2020; Raffel et al., 2020). In this paper, we use knowledgeaware pre-trained language model to generate contrastive explanation.\nKnowledge Extraction Given selected counterfactual examples and identified rationales, we extract relevant knowledge to enhance the generative language model. We acquire structured knowledge and rationale definitions from ConceptNet5 and dictionary source6 separately. For ConceptNet, we extract knowledge with Breadth-First-Search (BFS) algorithm as described in (Ji et al., 2020). For dictionary, we extract the definition of rationales by following (Chen et al., 2020a). After extraction, we concatenate these knowledge for training knowledge-aware explanation generator.\nKnowledge-Aware Explanation Generator For contrastive explanation generation, we divide the “WHY A NOT B” problem into two simple question: 1) why the label of the input belong to A, 2) why the label of the input not belong to B.\nIn previous study, (Kumar and Talukdar, 2020) proposed a label-specific explanation generator, which fine-tuned GPT2 independently for each label. However, the generator can only produce explanations for “WHY A”. For the other part of contrastive explanation, we collect some contrastive explanations annotated by human and use them to fine-tune a “WHY NOT B” generator.\nTaking a premise-hypothesis pair x along with the qualified counterfactual example x′ and extracted knowledge KE as input, which is in the form of 〈s〉Label 〈s〉x 〈s〉x′ 〈s〉KE 〈s〉, our finetuned language model generates explanations that support the corresponding label in a “WHY A NOT\n3https://github.com/slundberg/shap 4https://github.com/marcotcr/lime 5https://github.com/commonsense/conceptnet5/ 6https://dictionary.cambridge.org/\nB” way. With these explanations, end-users can observe and understand the difference between original input and counterfactual example explicitly."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : ""
    }, {
      "heading" : "4.1.1 Natural Language Inference",
      "text" : "SNLI & e-SNLI The SNLI dataset (Bowman et al., 2015) is a balanced collection of P-H annotated pairs with labels from {entailment, neutral, contradiction}, which consists of about 550K, 10K and 10K examples for train, development, and test set, respectively 7. (Camburu et al., 2018) extend the SNLI dataset to e-SNLI 8 with natural language explanations of the ground truth labels. Annotators were asked to highlight words in the premise and hypothesis pairs which could explain the labels and write a natural language explanation using the highlighted words. In this paper, we use the highlighted words for rationale identification and use the natural language explanation to fine-tune the language model based “WHY A” generator.\nIMDB The IMDB dataset (Maas et al., 2011) is a movie reviews dataset for sentiment classification. It contains 25,000 training data and 25,000 test data with movie reviews labeled as positive or negative. In this paper, we use IMDB as a out-of-domain dataset to evaluate if counterfactual examples can improve the robustness of our model."
    }, {
      "heading" : "4.2 Evaluation",
      "text" : "We are committed to generate contrastive explanations which can distinguish the predicted label and others at semantic level, hence, BLEU (Papineni et al., 2002) score is not a proper way to measure the quality of explanations. That is, it can be better confirmed by manual evaluation. In this work, we use manual evaluation and case study for contrastive explanations quality evaluation. Meanwhile, we use accuracy to measure the effectiveness of generated contrastive explanations on improving model performance in terms of data augmentation (organized in the form of 〈s〉CE 〈s〉Premise 〈s〉Hypothesis 〈s〉).\n7https://nlp.stanford.edu/projects/snli/snli 1.0.zip 8https://github.com/OanaMariaCamburu/e-SNLI"
    }, {
      "heading" : "4.3 Baselines",
      "text" : ""
    }, {
      "heading" : "4.3.1 Pre-trained Language Model",
      "text" : "RoBERTa & BERT For sequence labelling during rationale identification, we use RoBERTa-large and BERT-large, which have 24 layers, 16 attention heads and a hidden size of 1024 (355M parameters for RoBERTa-large, 340M parameters for BERTlarge). For downstream classifications tasks, a classification layer is added over the hidden state of the first [CLS] token at the last layer.\nGPT-2 For natural language explanation generation, we use the GPT-2 architecture (Radford et al., 2019). In particular, we use the GPT2-medium model that has 24 layers, 16 attention heads and a hidden size of 1024 (345M parameters). We finetuned GPT-2 model with label-specific examples that are integrated with contrastive examples and external knowledge from ConceptNet."
    }, {
      "heading" : "4.3.2 NLI Baselines",
      "text" : "ESIM & SemBERT & CA-MTL ESIM (Chen et al., 2017) proposes a enhanced sequential infer-\nence model that considers recursive architectures in both local inference modeling and inference composition, and incorporates syntactic parsing information. (Zhang et al., 2020) incorporate explicit contextual semantics from pre-trained semantic role labeling and introduce an improved language representation model, Semantics-aware BERT (SemBERT), which is capable of explicitly absorbing contextual semantics with a BERT backbone. CA-MTL (Pilault et al., 2021) is a novel transformer based architecture that consists of a new conditional attention mechanism as well as a set of task conditioned modules that facilitate weight sharing, and achieves the new state-of-art performance on SNLI."
    }, {
      "heading" : "4.4 NLI with Explanation Baselines",
      "text" : "ETPA (Camburu et al., 2018) propose ExplainThen-Predict-Attention (ETPA) that generates an explanation and then predicts the label with only the generated explanation.\nNILE:post-hoc (Kumar and Talukdar, 2020) propose natural language inference over labelspecific explanations (NILE). A premise and hypothesis pair is input to label-specific a candidate explanation generator that generates natural language explanations supporting the corresponding label. The generated explanations are then fed into an explanation processor, which predicts labels using evidence presented in these explanations.\nLIREx-base (Zhao and Vydiswaran, 2021) propose LIREx-base that incorporates both a rationale enabled explanation generator and an instance selector to select only relevant, plausible natural language explanations (NLEs) to augment NLI models and evaluate on the standardized SNLI."
    }, {
      "heading" : "4.5 Experiment Setting",
      "text" : "For rationale identification, we use RoBERTa-base to extract hidden representations and set the learning rate to 2e-5, dropout to 0.02, batch size to 8 and number of epochs to 10. Meanwhile, we use AdamW (Loshchilov and Hutter, 2018) as the optimizer and adopt cross-entropy loss as the loss function. In the counterfactual example generation part, we build a hypernym and hyponym table, and use hypernym and hyponym of tokens in WordNet for perturbation. In the contrastive explanation generation part, we use GPT-2 as the generative language model for training “WHY A” generator and “WHY NOT B” Generator. For generator, we set the learning rate to 5e-5, adam epsilon to 1e-8, length for generation to 100."
    }, {
      "heading" : "4.6 Results And Analysis",
      "text" : "Explanation Generation for SNLI In Table 1, we present the inputs of our model, the results of our approach that include token-level explanation (rationales), counterfactual example and generated contrastive explanation, compared with manually annotated explanation and generated “WHY A” explanations by NILE:post-hoc and LIREx-base.\nCompared with “WHY A” explanations that are simple and lack essential information, the contrastive explanation contains more information such as “making sculptures out of clay is a type of art” and “making sculptures is different from painting on canvas”. As shown in Table 1, we provide not only the contrastive explanation but also the identified rationales and reversed counterfactual example for reference.\nTo quantitatively assess contrastive explanations, we compared our method with LIREx-base and NILE:post-hoc in terms of explanation quality through human evaluation on 100 SNLI test samples. The explanation quality refers to whether an explanation provides enough essential information for a predicted label. As shown in Table 2, contrastive explanations produced by our method have a better quality by obtaining over 2.0% and 9.0% than LIREx-base and NILE:post-hoc .\nExplanation Enhanced NLI In Table 3, we report the experimental results of our method and other baselines include BERT, SemBERT (Zhang et al., 2020), CA-MTL (Pilault et al., 2021), NILE:post-hoc (Kumar and Talukdar, 2020) and LIREx-base (Zhao and Vydiswaran, 2021) on SNLI. With contrastive explanations, we are able to improve the performance of both BERT-large and RoBERTa-large. Compared with NILE:posthoc (Kumar and Talukdar, 2020), the same scale\nBERT-large model with contrastive explanations brings a gain of 0.4% on test, which indicates the knowledge-aware contrastive generator are better than the generator of NILE. Compared with LIREx-base that uses RoBERTa-large (Zhao and Vydiswaran, 2021), the BERT-large model and RoBERTa-large with contrastive explanations bring a gain of 0.3% and 1.0% separately, which suggests contrastive explanations are better than rationale enabled explanation. In general, contrastive explanations can achieve new state-of-art performance and get it closer to human annotation (a gain of 1.1% on BERT-Large). We believe that contrastive explanations contain more helpful information (e.g., relations between rationales, differences between original and counterfactual examples) that can be used to improve model performance.\nAblation Study We perform ablation studies with BERT-large on the SNLI dataset to evaluate the impacts of different components employed in our method, and report the results in Table 4. We isolated rationales, counterfactual examples and external knowledge, separately. The model without rationales means we generate contrastive explanations with counterfactual examples generated through randomly replacing tokens and extracted external knowledge. The model without counterfactual examples means we extracted knowledge with given rationales and generate contrastive explanation with them. The model without external knowledge means we generate contrastive explanation only with rationales and counterfactual examples. The model without contrastive explanation actually is the BERT-large baseline in SNLI. We can observe that each component is helpful. Especially, if we remove external knowledge and contrastive explanations, we can see a clear decrease of 0.6% and 0.8%, respectively. It indicates that external knowledge and contrastive explanation generation are the most essential components, while rationales and counterfactual examples affect the performance less. On one hand, the ablation study results show, external knowledge and rationales affect more than counterfactual examples on explanation generation. On the other hand, the results suggest that each component contributes positively, and indicate the importance of knowledge aware contrastive explanations, as we highlighted in the title.\nOut of Domain Counterfactual Example In this part, we use the generated counterfactual ex-\namples of IMDB for out of domain evaluation. As shown in Table 5, we train BERT-base on two different training sets: the original training set TRAINO, and the union of original training examples and generated counterfactual examples TRAINO∪C , and evaluate it with two separated dev sets: the original dev set DEVO and the generated counterfactual example dev set DEVC . Experimental results shown that BERT-base model enhanced with counterfactual examples achieves 88.5% and 95.1%, bringing a gain of 11.0% on DEVC while a slight decrease of 1.7% on DEVO. It indicates that counterfactual examples can help to improve the robustness of model for more diversified data distribution.\nWith IMDB evaluation, we demonstrate that counterfactual examples can not only help to generate contrastive explanation, but also contribute to data augmentation. In the experiments on SNLI, we evaluated the effectiveness of counterfactual example in contrastive explanation generation. In IMDB experiments, we further verify the effectiveness of counter-factual examples for data augmentation with only rationales identification and heuristic reversal mechanism."
    }, {
      "heading" : "5 Related Work",
      "text" : ""
    }, {
      "heading" : "5.1 Counterfactual Example Generation",
      "text" : "Counterfactual example aims to find a minimal change in data that “flips” the model’s prediction and is used for explanation. (Wachter et al., 2017) first propose the concept of unconditional counterfactual explanations and a framework to generate counterfactual explanations. (Hendricks et al.,\n2018) first consider the evidence that is discriminative for one class but not present in another class, and learn a model to generate counterfactual explanations for why a model predicts class A instead of B. In this paper, we focus on counterfactual example generation providing contrastive example for natural language inference."
    }, {
      "heading" : "5.2 Post-hoc Explanation Generation",
      "text" : "For post-hoc explainable NLP system, we can divide explanations into three types: feature-based, example-based and concept-based.\nFor feature-based explanation, (Ribeiro et al., 2016) propose LIME and (Guidotti et al., 2018) extend LIME by fitting a decision tree classifier to approximate the non-linear model. However, there is no guarantee that they are faithful to the original model. For example-based explanation, (Kim et al., 2016) select both prototypes and criticisms from the original data points. (Wachter et al., 2017) propose counterfactual explanations providing alternative perturbations. For concept-based explanation, (Ghorbani et al., 2019) explains model decisions through concepts that are more understandable to human than individual features or characters. In this paper, we integrate counterfactual example and concepts for contrastive explanation generation."
    }, {
      "heading" : "5.3 Natural Language Inference",
      "text" : "For natural language inference, (Bowman et al., 2015) propose SNLI which contains samples of premise and hypothesis pairs with human annotations. In order to provide interpretable and robust explanations for model decisions, (Camburu et al., 2018) extend the SNLI dataset with natural language explanations of the ground truth labels, named e-SNLI. For explanation generation in NLI, (Kumar and Talukdar, 2020) propose NILE, which utilizes label-specific generators to produce labels along with explanation. However, (Zhao and Vydiswaran, 2021) find NILE do not take into account the variability inherent in human explanation, and propose LIREx which incorporates a rationale enabled explanation generator. In this paper, we consider generating contrastive explanations in NLI."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we focus on knowledge-aware contrastive explanation generation for NLI. We generate counterfactual examples by changing identified rationales of given instances. Afterwards,\nwe extract concepts knowledge from ConceptNet and dictionary to train knowledge-aware explanation generators. We show that contrastive explanations that specify why a model makes prediction A rather than B can provide more faithful information than other “WHY A” explanations. Moreover, contrastive explanations can be used for data augmentation to improve the performance and robustness of existing model. The exploration of contrastive explanation in other NLP tasks (i.e. question answering) and better evaluation metrics for explanation will be performed in the future."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the anonymous reviewers for their helpful comments on this paper. This work is supported by National Key R&D Program of China (No. 2018AAA0101900), the NSFC projects (No. 62072399, No. U19B2042, No. 61402403), Chinese Knowledge Center for Engineering Sciences and Technology, MoE Engineering Research Center of Digital Library, Alibaba Research Intern Program of Alibaba Group, Alibaba-Zhejiang University Joint Institute of Frontier Technologies, and the Fundamental Research Funds for the Central Universities."
    }, {
      "heading" : "A Appendices",
      "text" : "Reported Experimental Results Here, we report some other experimental results for reproduction. We use 2 RTX-6000 GPUs for generator training. For each epoch, it takes 3 hours to fine-tune the contrastive generator. As we set 4 epochs for each “WHY A” generator and “WHY NOT B” generator, it takes 12 hours for each approach. There are 355M parameters in RoBERTa-large, 340M parameters in BERT-large and 345M parameters in GPT2-medium. And our code is based on Pytorch.\nThe Difference between Counterfactual Example and Contrastive Explanation In this paper, we generate contrastive explanations with qualified counterfactual examples. As counterfactual examples provide example-based explanations, the contrastive explanations provide concept-based explanations and explain “WHY A NOT B”. Meanwhile, for end-user, contrastive explanations are easier to understand than counterfactual example, which can integrate external knowledge from knowledge bases.\nCommon Replaced Words Here, we show some common replaced words in reversal mechanism.\nFor entailment to neutral, the top 10 removed words are “man, wearing, white, blue,black, shirt, one, young, people, woman”, the top 10 inserted words are “people, there, playing, man, person, wearing, outside, two, old, near”. For entailment to contradiction, the top 10 removed words are “man, wearing, white, blue,black, two, shirt, one, young,people”, the top 10 inserted words are “people, man, woman, playing,no, inside, person, two, wearing, women”.\nFor contradiction to neutral, the top 10 removed words are “wearing, blue, black, man,white, two, red, sitting, young, standing”, the top 10 inserted words are “people, playing, man, woman, two, wearing, near, tall, men, old”. For contradiction to entailment, the top 10 removed words are “wearing, blue, black, man,white, two, red, shirt, young, one”, the top 10 inserted words are “people, there, man, two, wearing,playing, people, men, woman, outside”.\nFor neutral to entailment, the top 10 removed words are “white, wearing, shirt, black,blue, man, two, standing,young, red”, the top 10 inserted words are “playing, wearing, man, two, there, woman, people, men, near, person”. For neutral\nto contradiction, the top 10 removed words are “white, man, wearing, shirt,black, blue, two, standing,woman, red”, the top 10 inserted words are “woman, man, there, playing,two, wearing, one, men, girl,no”.\nThe Demand For Contrastive Explanation A “contrastive explanation” explains not only why some event A occurred, but why A occurred as opposed to some alternative event B. Some philosophers argue that agents could only be morally responsible for their choices if those choices have contrastive explanations, since they would otherwise be “luck infested”. Moreover, if the answer predicted by a well-trained model is A but confusing with B, it is natural for end-users to ask “why the answer is A rather than B”. A similar scenario is possible to occur when a child is going to recognize characters or learn other language skills. Therefore, contrastive explanation generation is essential in critical domains."
    } ],
    "references" : [ {
      "title" : "Generating fact checking explanations",
      "author" : [ "Pepa Atanasova", "Jakob Grue Simonsen", "Christina Lioma", "Isabelle Augenstein." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7352–7364, Online. As-",
      "citeRegEx" : "Atanasova et al\\.,? 2020",
      "shortCiteRegEx" : "Atanasova et al\\.",
      "year" : 2020
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "e-snli: Natural language inference with natural language explanations",
      "author" : [ "Oana-Maria Camburu", "Tim Rocktäschel", "Thomas Lukasiewicz", "Phil Blunsom." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc.",
      "citeRegEx" : "Camburu et al\\.,? 2018",
      "shortCiteRegEx" : "Camburu et al\\.",
      "year" : 2018
    }, {
      "title" : "Enhanced LSTM for natural language inference",
      "author" : [ "Qian Chen", "Xiaodan Zhu", "Zhen-Hua Ling", "Si Wei", "Hui Jiang", "Diana Inkpen." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Improving commonsense question answering by graph-based iterative retrieval over multiple knowledge sources",
      "author" : [ "Qianglong Chen", "Feng Ji", "Haiqing Chen", "Yin Zhang." ],
      "venue" : "Proceedings of the 28th International Conference on Compu-",
      "citeRegEx" : "Chen et al\\.,? 2020a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Towards explainable conversational recommendation",
      "author" : [ "Zhongxia Chen", "Xiting Wang", "Xing Xie", "Mehul Parsana", "Akshay Soni", "Xiang Ao", "Enhong Chen." ],
      "venue" : "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intel-",
      "citeRegEx" : "Chen et al\\.,? 2020b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Dece: Decision explorer with counterfactual explanations for machine learning models",
      "author" : [ "Furui Cheng", "Yao Ming", "Huamin Qu." ],
      "venue" : "IEEE Transactions on Visualization and Computer Graphics.",
      "citeRegEx" : "Cheng et al\\.,? 2020",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2020
    }, {
      "title" : "QuAC: Question answering in context",
      "author" : [ "Eunsol Choi", "He He", "Mohit Iyyer", "Mark Yatskar", "Wentau Yih", "Yejin Choi", "Percy Liang", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Choi et al\\.,? 2018",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards automatic concept-based explanations",
      "author" : [ "Amirata Ghorbani", "James Wexler", "James Y Zou", "Been Kim." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 9273–9282.",
      "citeRegEx" : "Ghorbani et al\\.,? 2019",
      "shortCiteRegEx" : "Ghorbani et al\\.",
      "year" : 2019
    }, {
      "title" : "Local rule-based explanations of black box decision systems",
      "author" : [ "Riccardo Guidotti", "Anna Monreale", "Salvatore Ruggieri", "Dino Pedreschi", "Franco Turini", "Fosca Giannotti." ],
      "venue" : "arXiv preprint arXiv:1805.10820.",
      "citeRegEx" : "Guidotti et al\\.,? 2018",
      "shortCiteRegEx" : "Guidotti et al\\.",
      "year" : 2018
    }, {
      "title" : "Generating counterfactual explanations with natural language",
      "author" : [ "Lisa Anne Hendricks", "Ronghang Hu", "Trevor Darrell", "Zeynep Akata." ],
      "venue" : "ICML Workshop on Human Interpretability in Machine Learning, pages 95–98.",
      "citeRegEx" : "Hendricks et al\\.,? 2018",
      "shortCiteRegEx" : "Hendricks et al\\.",
      "year" : 2018
    }, {
      "title" : "Language generation with multi-hop reasoning on commonsense knowledge graph",
      "author" : [ "Haozhe Ji", "Pei Ke", "Shaohan Huang", "Furu Wei", "Xiaoyan Zhu", "Minlie Huang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Ji et al\\.,? 2020",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2020
    }, {
      "title" : "Model-agnostic counterfactual explanations for consequential decisions",
      "author" : [ "Amir-Hossein Karimi", "Gilles Barthe", "Borja Balle", "Isabel Valera." ],
      "venue" : "International Conference on Artificial Intelligence and Statistics, pages 895–905.",
      "citeRegEx" : "Karimi et al\\.,? 2020",
      "shortCiteRegEx" : "Karimi et al\\.",
      "year" : 2020
    }, {
      "title" : "Examples are not enough, learn to criticize! criticism for interpretability",
      "author" : [ "Been Kim", "Rajiv Khanna", "Oluwasanmi O Koyejo." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc.",
      "citeRegEx" : "Kim et al\\.,? 2016",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2016
    }, {
      "title" : "NILE : Natural language inference with faithful natural language explanations",
      "author" : [ "Sawan Kumar", "Partha Talukdar." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8730–8742, Online. Association for",
      "citeRegEx" : "Kumar and Talukdar.,? 2020",
      "shortCiteRegEx" : "Kumar and Talukdar.",
      "year" : 2020
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Fixing weight decay regularization in adam",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2018",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2018
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Maas et al\\.,? 2011",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Conditionally adaptive multi-task learning: Improving transfer learning in {nlp} using fewer parameters & less data",
      "author" : [ "Jonathan Pilault", "Amine El hattami", "Christopher Pal." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Pilault et al\\.,? 2021",
      "shortCiteRegEx" : "Pilault et al\\.",
      "year" : 2021
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Re-",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Explain yourself! leveraging language models for commonsense reasoning",
      "author" : [ "Nazneen Fatema Rajani", "Bryan McCann", "Caiming Xiong", "Richard Socher." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Rajani et al\\.,? 2019",
      "shortCiteRegEx" : "Rajani et al\\.",
      "year" : 2019
    }, {
      "title" : "Model agnostic multilevel explanations",
      "author" : [ "Karthikeyan Natesan Ramamurthy", "Bhanukiran Vinzamuri", "Yunfeng Zhang", "Amit Dhurandhar." ],
      "venue" : "arXiv preprint arXiv:2003.06005.",
      "citeRegEx" : "Ramamurthy et al\\.,? 2020",
      "shortCiteRegEx" : "Ramamurthy et al\\.",
      "year" : 2020
    }, {
      "title" : " why should i trust you?” explaining the predictions of any classifier",
      "author" : [ "Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin." ],
      "venue" : "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining,",
      "citeRegEx" : "Ribeiro et al\\.,? 2016",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2016
    }, {
      "title" : "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
      "author" : [ "Alon Talmor", "Jonathan Herzig", "Nicholas Lourie", "Jonathan Berant." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Talmor et al\\.,? 2019",
      "shortCiteRegEx" : "Talmor et al\\.",
      "year" : 2019
    }, {
      "title" : "Generating token-level explanations for natural language inference",
      "author" : [ "James Thorne", "Andreas Vlachos", "Christos Christodoulopoulos", "Arpit Mittal." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the",
      "citeRegEx" : "Thorne et al\\.,? 2019",
      "shortCiteRegEx" : "Thorne et al\\.",
      "year" : 2019
    }, {
      "title" : "Counterfactual explanations for machine learning: A review",
      "author" : [ "Sahil Verma", "John Dickerson", "Keegan Hines." ],
      "venue" : "arXiv preprint arXiv:2010.10596.",
      "citeRegEx" : "Verma et al\\.,? 2020",
      "shortCiteRegEx" : "Verma et al\\.",
      "year" : 2020
    }, {
      "title" : "Counterfactual explanations without opening the black box: Automated decisions and the gdpr",
      "author" : [ "Sandra Wachter", "Brent Mittelstadt", "Chris Russell." ],
      "venue" : "Harv. JL & Tech., 31:841.",
      "citeRegEx" : "Wachter et al\\.,? 2017",
      "shortCiteRegEx" : "Wachter et al\\.",
      "year" : 2017
    }, {
      "title" : "Improved heterogeneous distance functions",
      "author" : [ "D Randall Wilson", "Tony R Martinez." ],
      "venue" : "Journal of artificial intelligence research, 6:1–34.",
      "citeRegEx" : "Wilson and Martinez.,? 1997",
      "shortCiteRegEx" : "Wilson and Martinez.",
      "year" : 1997
    }, {
      "title" : "Learning post-hoc causal explanations for recommendation",
      "author" : [ "Shuyuan Xu", "Yunqi Li", "Shuchang Liu", "Zuohui Fu", "Yongfeng Zhang." ],
      "venue" : "arXiv preprint arXiv:2006.16977.",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32. Curran",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Semantics-aware bert for language understanding",
      "author" : [ "Zhuosheng Zhang", "Yuwei Wu", "Hai Zhao", "Zuchao Li", "Shuailiang Zhang", "Xi Zhou", "Xiang Zhou." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 05, pages 9628–9635.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Lirex: Augmenting language inference with relevant explanation",
      "author" : [ "Xinyan Zhao", "VG Vydiswaran." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Zhao and Vydiswaran.,? 2021",
      "shortCiteRegEx" : "Zhao and Vydiswaran.",
      "year" : 2021
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "In recent years, pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have been widely adopted in many tasks of natural language processing (Talmor et al.",
      "startOffset" : 45,
      "endOffset" : 103
    }, {
      "referenceID" : 17,
      "context" : "In recent years, pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have been widely adopted in many tasks of natural language processing (Talmor et al.",
      "startOffset" : 45,
      "endOffset" : 103
    }, {
      "referenceID" : 33,
      "context" : "In recent years, pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have been widely adopted in many tasks of natural language processing (Talmor et al.",
      "startOffset" : 45,
      "endOffset" : 103
    }, {
      "referenceID" : 27,
      "context" : ", 2019) have been widely adopted in many tasks of natural language processing (Talmor et al., 2019; Choi et al., 2018; Bowman et al., 2015).",
      "startOffset" : 78,
      "endOffset" : 139
    }, {
      "referenceID" : 7,
      "context" : ", 2019) have been widely adopted in many tasks of natural language processing (Talmor et al., 2019; Choi et al., 2018; Bowman et al., 2015).",
      "startOffset" : 78,
      "endOffset" : 139
    }, {
      "referenceID" : 1,
      "context" : ", 2019) have been widely adopted in many tasks of natural language processing (Talmor et al., 2019; Choi et al., 2018; Bowman et al., 2015).",
      "startOffset" : 78,
      "endOffset" : 139
    }, {
      "referenceID" : 5,
      "context" : "models (Chen et al., 2020b; Karimi et al., 2020; Kumar and Talukdar, 2020) provide explanations after making predictions without affecting the overall accuracy, most of them neglect the rationales in inputs and provide textual explanations just in the form of “WHY A”.",
      "startOffset" : 7,
      "endOffset" : 74
    }, {
      "referenceID" : 13,
      "context" : "models (Chen et al., 2020b; Karimi et al., 2020; Kumar and Talukdar, 2020) provide explanations after making predictions without affecting the overall accuracy, most of them neglect the rationales in inputs and provide textual explanations just in the form of “WHY A”.",
      "startOffset" : 7,
      "endOffset" : 74
    }, {
      "referenceID" : 15,
      "context" : "models (Chen et al., 2020b; Karimi et al., 2020; Kumar and Talukdar, 2020) provide explanations after making predictions without affecting the overall accuracy, most of them neglect the rationales in inputs and provide textual explanations just in the form of “WHY A”.",
      "startOffset" : 7,
      "endOffset" : 74
    }, {
      "referenceID" : 11,
      "context" : "erating counterfactual examples rather than just discarding them like previous post-hoc work (Hendricks et al., 2018; Cheng et al., 2020).",
      "startOffset" : 93,
      "endOffset" : 137
    }, {
      "referenceID" : 6,
      "context" : "erating counterfactual examples rather than just discarding them like previous post-hoc work (Hendricks et al., 2018; Cheng et al., 2020).",
      "startOffset" : 93,
      "endOffset" : 137
    }, {
      "referenceID" : 15,
      "context" : "• We integrate external knowledge with generative pre-trained language model rather than only taking original inputs (Kumar and Talukdar, 2020; Rajani et al., 2019) for contrastive explanation generation.",
      "startOffset" : 117,
      "endOffset" : 164
    }, {
      "referenceID" : 24,
      "context" : "• We integrate external knowledge with generative pre-trained language model rather than only taking original inputs (Kumar and Talukdar, 2020; Rajani et al., 2019) for contrastive explanation generation.",
      "startOffset" : 117,
      "endOffset" : 164
    }, {
      "referenceID" : 28,
      "context" : "Being similar with (Thorne et al., 2019), we first construct the input sequence for a premise p and a hypothesis h as Sp=〈s〉Label 〈s〉Premise 〈s〉 and Sh=〈s〉Hypothesis 〈s〉, where 〈s〉 is a special token that separates the components.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 17,
      "context" : "Both premise and hypothesis are encoded with RoBERTa (Liu et al., 2019), yielding hidden representation H=[· · · , hpj , · · · ] and H=[· · · , hi , · · · ] respectively.",
      "startOffset" : 53,
      "endOffset" : 71
    }, {
      "referenceID" : 35,
      "context" : "As rationalizer is proposed by (Zhao and Vydiswaran, 2021), we follow this work for rationale identification using cross attention to embed the hypothesis (premise) into premise (hypothesis), which is defined as:",
      "startOffset" : 31,
      "endOffset" : 58
    }, {
      "referenceID" : 6,
      "context" : "proximal to the original instance as described in (Cheng et al., 2020), which means only a small change needs to be made.",
      "startOffset" : 50,
      "endOffset" : 70
    }, {
      "referenceID" : 31,
      "context" : "In this paper, we choose a weighted Heterogeneous Manhattan-Overlay Metric (Wilson and Martinez, 1997) to calculate the distance as follows:",
      "startOffset" : 75,
      "endOffset" : 102
    }, {
      "referenceID" : 22,
      "context" : "While traditional approach generate explanation with SHAP3 or LIME4, recent work has exploited to use pre-trained generative language models (Radford et al., 2019; Lewis et al., 2020; Raffel et al., 2020).",
      "startOffset" : 141,
      "endOffset" : 204
    }, {
      "referenceID" : 16,
      "context" : "While traditional approach generate explanation with SHAP3 or LIME4, recent work has exploited to use pre-trained generative language models (Radford et al., 2019; Lewis et al., 2020; Raffel et al., 2020).",
      "startOffset" : 141,
      "endOffset" : 204
    }, {
      "referenceID" : 23,
      "context" : "While traditional approach generate explanation with SHAP3 or LIME4, recent work has exploited to use pre-trained generative language models (Radford et al., 2019; Lewis et al., 2020; Raffel et al., 2020).",
      "startOffset" : 141,
      "endOffset" : 204
    }, {
      "referenceID" : 12,
      "context" : "For ConceptNet, we extract knowledge with Breadth-First-Search (BFS) algorithm as described in (Ji et al., 2020).",
      "startOffset" : 95,
      "endOffset" : 112
    }, {
      "referenceID" : 15,
      "context" : "In previous study, (Kumar and Talukdar, 2020) proposed a label-specific explanation generator, which fine-tuned GPT2 independently for each label.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "SNLI & e-SNLI The SNLI dataset (Bowman et al., 2015) is a balanced collection of P-H annotated pairs with labels from {entailment, neutral, contradiction}, which consists of about 550K, 10K",
      "startOffset" : 31,
      "endOffset" : 52
    }, {
      "referenceID" : 2,
      "context" : "(Camburu et al., 2018) extend the SNLI dataset to e-SNLI 8 with natural language explanations of the ground truth labels.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 19,
      "context" : "IMDB The IMDB dataset (Maas et al., 2011) is a movie reviews dataset for sentiment classification.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 20,
      "context" : "We are committed to generate contrastive explanations which can distinguish the predicted label and others at semantic level, hence, BLEU (Papineni et al., 2002) score is not a proper way to measure the quality of explanations.",
      "startOffset" : 138,
      "endOffset" : 161
    }, {
      "referenceID" : 22,
      "context" : "GPT-2 For natural language explanation generation, we use the GPT-2 architecture (Radford et al., 2019).",
      "startOffset" : 81,
      "endOffset" : 103
    }, {
      "referenceID" : 3,
      "context" : "ESIM & SemBERT & CA-MTL ESIM (Chen et al., 2017) proposes a enhanced sequential inference model that considers recursive architectures in both local inference modeling and inference composition, and incorporates syntactic parsing",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 34,
      "context" : "(Zhang et al., 2020) incorporate explicit contextual semantics from pre-trained semantic role labeling and introduce an improved language representation model, Semantics-aware BERT (SemBERT), which is capable of explicitly absorbing contextual semantics with a BERT backbone.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 21,
      "context" : "CA-MTL (Pilault et al., 2021) is a novel transformer based architecture that consists of a new conditional attention mechanism as well as a set of task conditioned modules that facilitate",
      "startOffset" : 7,
      "endOffset" : 29
    }, {
      "referenceID" : 2,
      "context" : "ETPA (Camburu et al., 2018) propose ExplainThen-Predict-Attention (ETPA) that generates an explanation and then predicts the label with only the generated explanation.",
      "startOffset" : 5,
      "endOffset" : 27
    }, {
      "referenceID" : 15,
      "context" : "2522 NILE:post-hoc (Kumar and Talukdar, 2020) propose natural language inference over labelspecific explanations (NILE).",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 35,
      "context" : "LIREx-base (Zhao and Vydiswaran, 2021) propose LIREx-base that incorporates both a rationale enabled explanation generator and an instance selector to select only relevant, plausible natural language explanations (NLEs) to augment NLI models and evaluate on the standardized SNLI.",
      "startOffset" : 11,
      "endOffset" : 38
    }, {
      "referenceID" : 18,
      "context" : "Meanwhile, we use AdamW (Loshchilov and Hutter, 2018) as the optimizer and adopt cross-entropy loss as the loss",
      "startOffset" : 24,
      "endOffset" : 53
    }, {
      "referenceID" : 34,
      "context" : "Explanation Enhanced NLI In Table 3, we report the experimental results of our method and other baselines include BERT, SemBERT (Zhang et al., 2020), CA-MTL (Pilault et al.",
      "startOffset" : 128,
      "endOffset" : 148
    }, {
      "referenceID" : 21,
      "context" : ", 2020), CA-MTL (Pilault et al., 2021), NILE:post-hoc (Kumar and Talukdar, 2020) and LIREx-base (Zhao and Vydiswaran, 2021) on SNLI.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 15,
      "context" : ", 2021), NILE:post-hoc (Kumar and Talukdar, 2020) and LIREx-base (Zhao and Vydiswaran, 2021) on SNLI.",
      "startOffset" : 23,
      "endOffset" : 49
    }, {
      "referenceID" : 35,
      "context" : ", 2021), NILE:post-hoc (Kumar and Talukdar, 2020) and LIREx-base (Zhao and Vydiswaran, 2021) on SNLI.",
      "startOffset" : 65,
      "endOffset" : 92
    }, {
      "referenceID" : 15,
      "context" : "Compared with NILE:posthoc (Kumar and Talukdar, 2020), the same scale",
      "startOffset" : 27,
      "endOffset" : 53
    }, {
      "referenceID" : 35,
      "context" : "Compared with LIREx-base that uses RoBERTa-large (Zhao and Vydiswaran, 2021), the BERT-large model and RoBERTa-large with contrastive explanations bring a gain of 0.",
      "startOffset" : 49,
      "endOffset" : 76
    }, {
      "referenceID" : 30,
      "context" : "(Wachter et al., 2017) first propose the concept of unconditional counterfactual explanations and a framework to generate counterfactual explanations.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 26,
      "context" : "For feature-based explanation, (Ribeiro et al., 2016) propose LIME and (Guidotti et al.",
      "startOffset" : 31,
      "endOffset" : 53
    }, {
      "referenceID" : 10,
      "context" : ", 2016) propose LIME and (Guidotti et al., 2018) extend LIME by fitting a decision tree classifier to approximate the non-linear model.",
      "startOffset" : 25,
      "endOffset" : 48
    }, {
      "referenceID" : 30,
      "context" : "(Wachter et al., 2017) propose counterfactual explanations providing alternative perturbations.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 9,
      "context" : "For concept-based explanation, (Ghorbani et al., 2019) explains model decisions through concepts that are more understandable to",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 1,
      "context" : "For natural language inference, (Bowman et al., 2015) propose SNLI which contains samples of",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : "In order to provide interpretable and robust explanations for model decisions, (Camburu et al., 2018) extend the SNLI dataset with natural language explanations of the ground truth labels, named e-SNLI.",
      "startOffset" : 79,
      "endOffset" : 101
    }, {
      "referenceID" : 15,
      "context" : "For explanation generation in NLI, (Kumar and Talukdar, 2020) propose NILE, which utilizes label-specific generators to produce labels along with explanation.",
      "startOffset" : 35,
      "endOffset" : 61
    }, {
      "referenceID" : 35,
      "context" : "However, (Zhao and Vydiswaran, 2021) find NILE do not take into account the variability inherent in human explanation, and propose LIREx which incorporates a rationale enabled explanation generator.",
      "startOffset" : 9,
      "endOffset" : 36
    } ],
    "year" : 2021,
    "abstractText" : "In order to better understand the reason behind model behaviors (i.e., making predictions), most recent work has exploited generative models to provide complementary explanations. However, existing approaches in natural language processing (NLP) mainly focus on “WHY A” rather than contrastive “WHY A NOT B”, which is shown to be able to better distinguish confusing candidates and improve model performance in other research fields. In this paper, we focus on generating Contrastive Explanations with counterfactual examples in NLI and propose a novel Knowledge-Aware generation framework (KACE). Specifically, we first identify rationales (i.e., key phrases) from input sentences, and use them as key perturbations for generating counterfactual examples. After obtaining qualified counterfactual examples, we take them along with original examples and external knowledge as input, and employ a knowledge-aware generative pre-trained language model to generate contrastive explanations. Experimental results show that contrastive explanations are beneficial to clarify the difference between predicted answer and other answer options. Moreover, we train an BERT-large based NLI model enhanced with contrastive explanations and achieve an accuracy of 91.9% on SNLI, gaining an improvement of 5.7% against ETPA (“Explain-Then-Predict-Attention”) and 0.6% against NILE (“WHY A”).",
    "creator" : "LaTeX with hyperref"
  }
}