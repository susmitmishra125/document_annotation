{
  "name" : "2021.acl-long.160.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Modeling Fine-Grained Entity Types with Box Embeddings",
    "authors" : [ "Yasumasa Onoe", "Michael Boratko", "Andrew McCallum", "Greg Durrett" ],
    "emails" : [ "yasumasa@cs.utexas.edu", "gdurrett@cs.utexas.edu", "mboratko@cs.umass.edu", "mccallum@cs.umass.edu", "mccallum@google.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2051–2064\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2051"
    }, {
      "heading" : "1 Introduction",
      "text" : "The development of named entity recognition and entity typing has been characterized by a growth in the size and complexity of type sets: from 4 (Tjong Kim Sang and De Meulder, 2003) to 17 (Hovy et al., 2006) to hundreds (Weischedel and Brunstein, 2005; Ling and Weld, 2012) or thousands (Choi et al., 2018). These types follow some kind\n1The code is available at https://github.com/ yasumasaonoe/Box4Types.\nof hierarchical structure (Weischedel and Brunstein, 2005; Ling and Weld, 2012; Gillick et al., 2014; Murty et al., 2018), so effective models for these tasks frequently engage with this hierarchy explicitly. Prior systems incorporate this structure via hierarchical losses (Murty et al., 2018; Xu and Barbosa, 2018; Chen et al., 2020) or by embedding types into a high-dimensional Euclidean or hyperbolic space (Yogatama et al., 2015; López and Strube, 2020). However, the former approach requires prior knowledge of the type hierarchy, which is unsuitable for a recent class of large type sets where the hierarchy is not explicit (Choi et al., 2018; Onoe and Durrett, 2020a). The latter approaches, while leveraging the inductive bias of hyperbolic space to represent trees, lack a probabilistic interpretation of the embedding and do not naturally capture all of the complex type relationships beyond strict containment.\nIn this paper, we describe an approach that represents entity types with box embeddings in a highdimensional space (Vilnis et al., 2018). We build an entity typing model that jointly embeds each entity mention and entity types into the same box space to determine the relation between them. Volumes of boxes correspond to probabilities and taking intersections of boxes corresponds to computing joint distributions, which allows us to model mentiontype relations (what types does this mention exhibit?) and type-type relations (what is the type hierarchy?). Concretely, we can compute the conditional probability of a type given the entity mention with straightforward volume calculations, allowing us to construct a probabilistic type classification model.\nCompared to embedding types as points in Euclidean space (Ren et al., 2016a), the box space is expressive and suitable for representing entity types due to its geometric properties. Boxes can nest, overlap, or be completely disjoint to capture\nsubtype, correlation, or disjunction relations, properties which are not explicitly manifested in Euclidean space. The nature of the box computation also allows these complex relations to be represented in a lower-dimensional space than needed by vector-based models.\nIn our experiments, we focus on comparing our box-based model against a vector-based baseline. We evaluate on four entity typing benchmarks: Ultra-fine Entity Typing (Choi et al., 2018), OntoNotes (Gillick et al., 2014), BBN (Weischedel and Brunstein, 2005), and FIGER (Ling and Weld, 2012). To understand the behavior of box embeddings, we further analyze the model outputs in terms of consistency (predicting coherent supertypes and subtypes together), robustness (sensitivity against label noise), and calibration (i.e., model confidence). Lastly, we compare entity representations obtained by the box-based and vector-based models. Our box-based model outperforms the vector-based model on two benchmarks, Ultra-fine Entity Typing and OntoNotes, achieving state-ofthe-art-performance. In our other experiments, the box-based model also performs better at predicting supertypes and subtypes consistently and being robust against label noise, indicating that our approach is capable of capturing the latent hierarchical structure in entity types."
    }, {
      "heading" : "2 Motivation",
      "text" : "When predicting class labels like entity types that exhibit a hierarchical structure, we naturally want our model’s output layer to be sensitive to this structure. Previous work (Ren et al., 2016a; Shimaoka et al., 2017; Choi et al., 2018; Onoe and Durrett, 2019, inter alia) has fundamentally treated types as vectors, as shown in the left half of Figure 1. As is standard in multiclass or multi-label classification, the output layer of these models typically involves taking a dot product between a mention embedding\nand each possible type. A type could be more general and predicted on more examples by having higher norm,2 but it is hard for these representations to capture that a coarse type like Person will have many mutually orthogonal subtypes.\nBy contrast, box embeddings naturally represent these kinds of hierarchies as shown in the right half of Figure 1. A box that is completely contained in another box is a strict subtype of that box: any entity exhibiting the inner type will exhibit the outer one as well. Overlapping boxes like Politician and Author represent types that are not related in the type hierarchy but which are not mutually exclusive. The geometric structure of boxes enables complex interactions with only a moderate number of dimensions (Dasgupta et al., 2020). Vilnis et al. (2018) also define a probability measure over the box space, endowing it with probabilistic semantics. If the boxes are restricted to a unit hypercube, for example, the volumes of type boxes represent priors on types and intersections capture joint probabilities, which can then be used to derive conditional probabilities.\nCritically, box embeddings have previously been trained explicitly to reproduce a given hierarchy such as WordNet. A central question of this work is whether box embeddings can be extended to model the hierarchies and type relationships that are implicit in entity typing data: we do not assume access to explicit knowledge of a hierarchy during training. While some datasets such as OntoNotes have orderly ontologies, recent work on entity typing has often focused on noisy type sets from crowdworkers (Choi et al., 2018) or derived from Wikipedia (Onoe and Durrett, 2020a). We show that box embeddings can learn these structures organically; in fact, they are not restricted to only tree structures, but enable a natural Venndiagram style of representation for concepts, as\n2We do not actually observe this in our vector-based model.\nwith Politician and Author in Figure 1."
    }, {
      "heading" : "3 Type Modeling with Boxes",
      "text" : ""
    }, {
      "heading" : "3.1 Background: Box Embeddings",
      "text" : "Our box embeddings represent entity types as n-dimensional hyperrectangles. A box x is characterized by two points (xm, xM ), where xm, xM ∈ Rd are the minimum and the maximum corners of the box x and xm,i ≤ xM,i for each coordinate i ∈ {1, ..., d}. The volume of the box x is computed as Vol(x) = ∏︁ i(xM,i − xm,i). If we normalize the volume of the box space to be 1, we can interpret the volume of each box as the marginal probability of a mention exhibiting the given entity type. Furthermore, the intersection volume between two boxes, x and y, is defined as Vol(x ∩ y) =∏︁\nimax (min(xM,i, yM,i)−max(xm,i, ym,i), 0) and can be seen as the joint probability of entity types x and y. Thus, we can obtain the conditional probability P (y | x) = Vol(x∩y)Vol(x) .\nSoft boxes Computing conditional probabilities based on hard intersection poses some practical difficulties in the context of machine learning: sparse gradients caused by disjoint or completely contained boxes prevent gradient-based optimization methods from working effectively. To ensure that gradients always flow for disjoint boxes, Li et al. (2019) relax the hard edges of the boxes using Gaussian convolution. We follow the more recent approach of Dasgupta et al. (2020), who further improve training of box embeddings using max and min Gumbel distributions (i.e., Gumbel boxes) to represent the min and max coordinates of a box."
    }, {
      "heading" : "3.2 Box-based Multi-label Type Classifier",
      "text" : "Let s denote a sequence of context words and m denote an entity mention span in s. Given the input tuple (m, s), the output of the entity typing\nmodel is an arbitrary number of predicted types {t0, t1, ...} ∈ T , where tk is an entity type belonging to a type inventory T . Because we do not assume an explicit type hierarchy, we treat entity typing as a multi-label classification problem, or |T | independent binary classification problems for each mention.\nSection 3.3 will describe how to use a BERTbased model to predict a mention and context box3 x from (m, s). For now, we assume x is given and we are computing the probability of that mention exhibiting the kth entity type, with type box yk. Each type tk ∈ T has a dedicated box yk, which is parameterized by a center vector cky ∈ Rd and an offset vector oky ∈ Rd. The minimum and maximum corners of a box yk are computed as ykm = σ(c k y − softplus(oky)) and ykM = σ(c k y + softplus(o k y)) respectively, so that parameters c ∈ Rd and o ∈ Rd yield a valid box with nonzero volume.\nThe conditional probability of the type tk given the mention and context (m, s) is calculated as\npθ(t k | m, s) = Vol(z k)\nVol(x) = Vol(x ∩ yk) Vol(x) ,\nwhere zk is the intersection between x and yk ((2) and (3) in Figure 2). Our final type predictions are based on thresholding these probabilities; i.e., predict the type if p > 0.5.\nAs mentioned in Section 3.1, we use the Gumbel box approach of Dasgupta et al. (2020), in which the box coordinates are interpreted as the location parameter of a Gumbel max (resp. min) distribution with variance β. In this approach, the intersection\n3We could represent mentions as points instead of boxes; however, representing them as boxes enables the size of a mention box to naturally reflect epistemic uncertainty about a mention’s types given limited information.\nbox coordinates become\nzkm = β ln\n(︃ e xm β + e ykm β )︃ ,\nzkM = −β ln\n(︄ e −xM β + e − y k M β )︄ .\nFollowing Dasgupta et al. (2020), we approximate the expected volume of a Gumbel box using a softplus function:\nVol(x) ≈ ∏︂ i softplus (︃ xM,i − xm,i β − 2γ )︃ ,\nwhere i is an index of each coordinate and γ ≈ 0.5772 is the Euler–Mascheroni constant,4 and softplus(x) = 1t log(1 + exp(xt)), with t as an inverse temperature value."
    }, {
      "heading" : "3.3 Mention and Context Encoder",
      "text" : "We format the context words s and the mention span m as x = [CLS] m [SEP] s [SEP] and chunk into WordPiece tokens (Wu et al., 2016). Using pre-trained BERT5 (Devlin et al., 2019), we encode the whole sequence into a single vector by taking the hidden vector at the [CLS] token. A highway layer (Srivastava et al., 2015) projects down the hidden vector h[CLS] ∈ Rℓ to the R2d space, where ℓ is the hidden dimension of the encoder (BERT), and d is the dimension of the box space. This highway layer transforms representations in a vector space to the box space without impeding the gradient flow. We further split the hidden vector h̄ ∈ R2d into two vectors: the center point of the box cx ∈ Rd and the offset from the maximum and minimum corners ox ∈ Rd. The minimum and maximum corners of the mention and context box are computed as xm = σ(cx − SOFTPLUS(ox)) and xM = σ(cx + SOFTPLUS(ox)), where σ is an element-wise sigmoid function, and SOFTPLUS is an element-wise softplus function as defined in Section 3.2 ((1) in Figure 2). The output of the softplus is guaranteed to be positive, guaranteeing that the boxes have volume greater than zero."
    }, {
      "heading" : "3.4 Learning",
      "text" : "The goal of training is to find a set of parameters θ that minimizes the sum of binary cross-entropy losses over all types over all examples in our train-\n4From Dasgupta et al. (2020), the Euler-Mascheroni constant appears due to the interpretation of xm,i, xM,i as the location parameters of Gumbel distributions.\n5We use BERT-large uncased (whole word masking) in our experiments.\ning dataset D: L = − ∑︂ (m,s,t)∈D ∑︂ k tkgold · log pθ(tk | m, s)\n+ (1− tkgold) · log(1− pθ(tk | m, s)), where tkgold ∈ {0, 1} is the gold label for the type tk. We optimize this objective using gradient-based optimization algorithms such as Adam (Kingma and Ba, 2015).6"
    }, {
      "heading" : "4 Experimental Setup",
      "text" : "Our focus here is to shed light on the difference between type hierarchies learned by the box-based model and the vector-based model. To this end, we first evaluate those two models on standard entity typing datasets. Then, we test models’ consistency, robustness, and calibration, and evaluate the predicted types as entity representations on a downstream task (coreference resolution). See Appendix A for hyperparameters."
    }, {
      "heading" : "4.1 Baseline",
      "text" : "Our chief comparison is between box-based and vector-based modeling of entity types. As our main baseline for all experiments, we use a vector-based version of our entity typing model. We use the same mention and context encoder followed by a highway layer, but this baseline has vector-based type embeddings (i.e., a |T | × d′ matrix), and type predictions are given by a dot product between the type embeddings and the mention and context representation followed by element-wise logistic regression. This model is identical to that of Onoe and Durrett (2020b) except for the additional highway layer."
    }, {
      "heading" : "4.2 Evaluation and Datasets",
      "text" : "Entity Typing We evaluate our approach on the Ultra-Fine Entity Typing (UFET) dataset (Choi et al., 2018) with the standard splits (2k for each of train, dev, and test). In addition to the manually annotated training examples, we use the denoised distantly annotated training examples from Onoe and Durrett (2019).7 This dataset contains 10,331 entity types, and each type is marked as one of the three classes: coarse, fine, and ultra-fine. Note\n6With large type sets, most types are highly skewed towards the negative class (>99% negative for many finegrained types). While past work such as Choi et al. (2018) has used modified training objectives to handle this class imbalance, we did not find any modification to be necessary.\n7This consists of 727k training examples derived from the distantly labeled UFET data.\nthat this classification does not provide explicit hierarchies in the types, and all classes are treated equally during training.\nAdditionally, we test our box-based model on three other entity typing benchmarks that have relatively simpler entity type inventories with known hierarchies, namely OntoNotes (Gillick et al., 2014), BBN (Weischedel and Brunstein, 2005) , and FIGER (Ling and Weld, 2012). See Appendix B for more details on these datasets.\nConsistency A model that captures hierarchical structure should be aware of the relationships between supertypes and subtypes. When a model predicts a subtype, we want it to predict the corresponding supertype together, even when this is not explicitly enforced as a constraint or consistently demonstrated in the data, such as in the UFET dataset. That is, when a model predicts artist, person should also be predicted. To check this ability, we analyze the model predictions on the UFET dev set. We select 30 subtypes from the UFET type inventory and annotate corresponding supertypes for them in cases where these relationships are clear, based on their cooccurrence in the UFET training set and human intuition. Based on the 30 pairs, we compute accuracy of predicting supertypes and subtypes together. Table 10 in Appendix C lists the 30 pairs.\nRobustness Entity typing datasets with very large ontologies like UFET are noisy; does our box-based model’s notion of hierarchy do a better job of handling intrinsic noise in a dataset? To test this in a controlled fashion, we synthetically create noisy labels by randomly dropping the gold labels with probability 13 .\n8 We derive two noisy training sets from the UFET training set: 1) adding noise to the coarse types and 2) adding noise to fine & ultra-fine types. We train on these noised datasets and evaluate on the standard UFET dev set.\nCalibration Desai and Durrett (2020) study calibration of pre-trained Transformers such as BERT and RoBERTa (Liu et al., 2019) on natural language inference, paraphrase detection, and commonsense reasoning. In a similar manner, we investigate if our box-based entity typing model is calibrated: do the probabilities assigned to types by the model match the empirical likelihoods of those types? Since models may naturally have different scales\n8If this causes the gold type set to be empty, we retain the original gold type(s); however, this case is rare.\nfor their logits depending on how long they are trained, we post-hoc calibrate each of our models using temperature scaling (Guo et al., 2017) and a shift parameter. We report the total error (e.g., the sum of the errors between the mean confidence and the empirical accuracy) on the UFET dev set and the OntoNotes dev set.\nEntity Representations We are interested in the usefulness of the trained entity typing models in a downstream task. Following Onoe and Durrett (2020b), we evaluate entity representation given by the box-based and vector-based models on the Coreference Arc Prediction (CAP) task (Chen et al., 2019) derived from PreCo (Chen et al., 2018). This task is a binary classification problem, requiring to judge if two mention spans (either in one sentence or two sentences) are the same entity or not. As in Onoe and Durrett (2020b), we obtain type predictions (a vector of probabilities associated with types) for each span and use it as an entity representation. The final prediction of coreference for a pair of mentions is given by the cosine similarity between the entity type probability vectors with a threshold 0.5. The original data split provides 8k examples for each of the training, dev, and test sets. We report accuracy on the CAP test set."
    }, {
      "heading" : "5 Results and Discussion",
      "text" : ""
    }, {
      "heading" : "5.1 Entity Typing",
      "text" : "Here we report entity typing performance on UltraFine Entity Typing (UFET), OntoNotes, FIGER, and BBN. For each dataset, we select the best model from 5 runs with different random seeds based on the development performance.\nUFET Table 1 shows the macro-precision, recall, and F1 scores on the UFET test set. Our boxbased model outperforms the vector-based model and state-of-the-art systems in terms of macro-\nF1.9 Compared to the vector-based model, the boxbased model improves primarily in macro-recall compared to macro-precision. Choi et al. (2018) is a LSTM-based model using GloVe (Pennington et al., 2014). On top of this model, Xiong et al. (2019) add a graph convolution layer to model type dependencies. Onoe and Durrett (2019) use ELMo (Peters et al., 2018) and apply denoising to fix label inconsistency in the distantly annotated data.\nNote that past work on this dataset has used BERT-base (Onoe and Durrett, 2019). Work on other datasets has used ELMo and observed that BERT-based models have surprisingly underperformed (Lin and Ji, 2019). Some of the gain from our vector-based model can be attributed to our use of BERT-Large; however, our box model still achieves stronger performance than the corresponding vector-based version which uses the same pretrained model.\nTable 2 breaks down the performance into the coarse, fine, and ultra-fine classes. Our box-based model consistently outperforms the vector-based model in macro-recall and F1 across the three classes. The largest gap in macro-recall is in the fine class, leading to the largest gap in macro-F1 within the three classes.\nWe also list the numbers from prior work in Table 2. HY XLarge (López and Strube, 2020), a hyperbolic model designed to learn hierarchical structure in entity types, exceeds the performance of the models with similar sizes such as Choi et al. (2018) and Xiong et al. (2019) especially in macrorecall. In the ultra-fine class, both our box-based model and HY XLarge achieve higher macro-F1 compared to their vector-based counterparts.\nOne possible reason for the higher recall of our\n9We omit the test number of López and Strube (2020), since they report results broken down into coarse, fine, and ultra-fine types instead of an aggregated F1 value. However, based on the development results, their approach substantially underperforms the past work of Onoe and Durrett (2019) regardless.\nmodel is a stronger ability to model dependencies between types. Instead of failing to predict a highly correlated type, the model may be more likely to predict a complete, coherent set of types.\nOther datasets Table 3 compares macro-F1 and micro-F1 on the OntoNotes, BBN, and FIGER test sets.10 On OntoNotes, our box-based model achieves better performance than the vector-based model. Zhang et al. (2018) use document-level information, Chen et al. (2020) apply a hierarchical ranking loss that assumes prior knowledge of type hierarchies, and Lin and Ji (2019) propose an ELMo-based model with an attention layer over mention spans and train their model on the augmented data from Choi et al. (2018). Among the models trained only on the original OntoNotes training set, the box-based model achieves the highest macro-F1 and micro-F1.\nThe state-of-the-art system on BBN, the system of Chen et al. (2020) in the “undefined” setting, uses explicit knowledge of the type hierarchy. This is particularly relevant on the BBN dataset, where the training data is noisy and features training points with obviously conflicting labels like person and organization, which appear systematically in the data. To simulate constraints like the ones they use, we use three simple rules to modify our models’ prediction: (1) dropping person if organization exists, (2) dropping location if gpe exists, and (3) replacing facility by fac, since both versions of this tag appear in the training set but only fac in the dev and test set. Our box-based model and the vectorbased model perform similarly and both achieve results comparable with recent systems.\nOn FIGER, our box-based model shows lower performance compared to the vector-based model, though both are approaching comparable results\n10Note that our hyperparameters are optimized for macro F1 on OntoNotes.\nwith state-of-the-art systems. We notice that some of the test examples have inconsistent labels (e.g., /organization/sports team is present, but its supertype /organization is missing), penalizing models that predict the supertype correctly. In addition, FIGER, like BBN, has systematic shifts between training and test distributions. We hypothesize that our model’s hyperparameters (tuned on OntoNotes only) are suboptimal. The high dev performance shown in Table 4 implies that our model optimized on held-out training examples may not capture these specific shifts as well as other models whose inductive biases are better suited to this unusually mislabeled data."
    }, {
      "heading" : "5.2 Consistency",
      "text" : "One factor we can investigate is whether our model is able to predict type relations in a sensible, consistent fashion independent of the ground truth for a particular example. For this evaluation, we investigate our model’s predictions on the UFET dev set. We count the number of occurrences for each subtype in 30 supertype/subtype pairs (see Table 10 in Appendix C). Then, for each subtype, we count how many times its corresponding supertype is also predicted. Although these supertype-subtype relations are not strictly defined in the training data, we believe they should nevertheless be exhibited by models’ predictions. Accuracy is given by the ratio between those counts, indicating how often the supertype was correctly picked up.\nTable 5 lists the total and per-supertype accuracy on the supertype/subtype pairs. We report the number of subtypes grouped by their supertypes to show their frequency (the “Count” column in Table 5). Our box-based model achieves better accuracy compared to the vector-based model on all supertypes. The gaps are particularly large on place and organization. Note that some\nof the UFET training examples have inconsistent labels (e.g., a subtype team can be a supertype organization or group), and this ambiguity potentially confuses a model during training. Even in those tricky cases, the box-based model shows reasonable performance. The geometry of the box space itself gives some evidence as to why this consistency would arise (see Section 5.6 for visualization of box edges)."
    }, {
      "heading" : "5.3 Robustness",
      "text" : "Table 6 analyzes models’ sensitivity to the label noise. We list the UFET dev performance by models trained on the noised UFET training set. When the coarse types are noised (i.e., omitting some supertypes), the vector-based model loses 4.8 points of macro-F1 while our box-based model only loses 1.5 points. A similar trend can be seen when the fine and ultra-fine types are noised (i.e., omitting some subtypes). In both cases, the vector-based model shows lower recall compared to the same model trained on the clean data, while our boxbased model is more robust. We also note that the vector-based model tends to overfit to the training data quickly. We hypothesize that the use of boxes works as a form of regularization, since moving boxes may be harder than moving points in a space, thus being less impacted by noisy labels."
    }, {
      "heading" : "5.4 Calibration",
      "text" : "Following Nguyen and O’Connor (2015), we split model confidence (output probability) for each typing decision of each example into 10 bins (e.g., 0- 0.1, 0.1-0.2 etc.). For each bin, we compute mean confidence and empirical accuracy. We show the total calibration error (lower is better) as well as the scaling and shifting constants in Table 7. As the results on UFET and OntoNotes show, both boxbased and vector-based entity typing models can be\nreasonably well calibrated after applying temperature scaling and shifting. However, the box-based model achieves slightly lower total error."
    }, {
      "heading" : "5.5 Entity Representation for Coreference",
      "text" : "This experiment evaluates if model outputs are immediately useful in a downstream task. For this task, we use the box-based and vector-based entity typing models trained on the UFET training set (i.e., we do not train models on the CAP training set). Table 8 shows the test accuracy on the CAP data. Our box-based model achieves slightly higher accuracy than the vector-based model, indicating that “out-of-the-box” entity representations obtained by the box-based model contains more useful features for the CAP task.11"
    }, {
      "heading" : "5.6 Box Edges",
      "text" : "To analyze how semantically related type boxes are located relative to one another in the box space, we plot the edges of the person and actor boxes along the 109 dimensions one by one. Figure 3 shows how those two boxes overlap each other in the high-dimensional box space. The upper plot\n11Our results are not directly comparable to those of Onoe and Durrett (2020b); we train on the training set of UFET dataset, and they train on examples from the train, dev, and test sets.\nin Figure 3 compares the person box and the actor box learned on the UFET data. We can see that the edges of person contain the edges of actor in many dimensions but not all, meaning that the person box overlaps with the actor box but doesn’t contain it perfectly as we might expect.\nHowever, we can additionally investigate whether the actor box is effectively contained in the person for parts of the space actually used by the mention boxes. The lower plot in Figure 3 compares the person box and the minimum bounding box of the intersections between the actor and the mention and context boxes obtained using the UFET dev examples where the actor type is predicted. This minimum bounding box approximates the effective region within the actor box. Now the edges of actor are contained in the edges of person in the most of dimensions, indicating that the person box almost contains this “effective” actor box."
    }, {
      "heading" : "6 Related Work",
      "text" : "Embeddings Embedding concepts/words into a high-dimensional vector space (Hinton, 1986) has a long history and has been an essential part of neural networks for language (Bengio et al., 2003; Collobert et al., 2011). There is similarly a long history of rethinking the semantics of these embedding spaces, such as treating words as regions using sparse count-based vectors (Erk, 2009a,b) or dense distributed vectors (Vilnis and McCallum, 2015). Order embeddings (Vendrov et al., 2016) or their probabilistic version (POE) (Lai and Hockenmaier, 2017) are one technique suited for hierarchical modeling. However, OE can only handle binary entailment decisions, and POE cannot model negative correlations between types, a critical limitation in its use as a probabilistic model; these shortcomings directly led to the development of box embeddings. Hyperbolic embeddings (Nickel and Kiela,\n2017; López and Strube, 2020) can also model hierarchical relationships as can hyperbolic entailment cones (Ganea et al., 2018); however, these approaches lack a probabilistic interpretation.\nRecent work on knowledge base completion (Abboud et al., 2020) and reasoning over knowledge graphs (Ren et al., 2020) embeds relations or queries using box embeddings, but entities are still represented as vectors. In contrast, our model embed both entity mentions and types as boxes.\nEntity typing Entity typing and named entity recognition (Tjong Kim Sang and De Meulder, 2003) are old problems in NLP. Recent work has focused chiefly on predicted fine-grained entity types (Ling and Weld, 2012; Gillick et al., 2014; Choi et al., 2018), as these convey significantly more information for downstream tasks. As a result, there is a challenge of scaling to large type inventories, which has inspired work on type embeddings (Ren et al., 2016a,b).\nEntity typing information has been used across a range of NLP tasks, including models for entity linking and coreference (Durrett and Klein, 2014). Typing has been shown to be useful for crossdomain entity linking specifically (Gupta et al., 2017; Onoe and Durrett, 2020a). It has also recently been applied to coreference resolution (Onoe and Durrett, 2020b; Khosla and Rose, 2020) and text generation (Dong et al., 2020), suggesting that it can be a useful intermediate layer even in pretrained neural models."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we investigated a box-based model for fine-grained entity typing. By representing entity types in a box embedding space and projecting entity mentions into the same space, we can naturally capture the hierarchy of and correlations between entity types. Our experiments showed several benefits of box embeddings over the equivalent vector-based model, including typing performance, calibration, and robustness to noise."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Thanks to the members of the UT TAUR lab, Pengxiang Cheng, and Eunsol Choi for helpful discussion; Tongfei Chen and Ying Lin for providing the details of experiments. This work was also partially supported by NSF Grant IIS-1814522, NSF Grant SHF-1762299, and based on research in part supported by the Air Force Research Laboratory (AFRL), DARPA, for the KAIROS program under agreement number FA8750-19-2-1003, as well as University of Southern California subcontract no. 123875727 under Office of Naval Research prime contract no. N660011924032. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of AFRL, DARPA, or the U.S. Government."
    }, {
      "heading" : "Appendix A: Hyperparameter Search",
      "text" : "We use Bayesian hyperparameter tuning and the Hyperband stopping criteria (Li et al., 2017) implemented in the Weights & Biases software (Biewald, 2020). We use Adam (Kingma and Ba, 2015) for all experiments. We perform hyperparameter search on OntoNotes due to its fast convergence. This finds a lower dimension for the box-based model compared to the vector-based model (109-d × 2 vs 307-d), resulting fewer parameters in the boxbased model. When we train the box-based model on the UFET dataset, we sample 1,000 negatives (i.e., wrong types) to speed up convergence; this is not effective in the vector-based model, so we do not do this there.\nWe use the same hyperparameters for the other three datasets. We train all models using NVIDIA V100 GPU with batch size 128. We implement our models using HuggingFace’s Transformers library (Wolf et al., 2020).\nTable 9 shows hyperparameters of the box-based and vector-based models as well as their ranges to search. For Adam, we use β1 = 0.9 and β2 = 0.999 for training."
    }, {
      "heading" : "Appendix B: Entity Typing Benchmarks",
      "text" : "OntoNotes (Gillick et al., 2014) has 89 types with a 3-level hierarchy (e.g., /location/geography/mountain). We use the same splits (250k train / 2k dev / 9k test) provided by (Shimaoka et al., 2017). FIGER (Ling and Weld, 2012), derived from Wikipedia, uses 113 types with a 2-level hierarchy (e.g., /person/musician). We use the same splits (2M train / 10k dev / 563 test) as (Shimaoka et al., 2017). BBN (Weischedel and Brunstein, 2005) is based on the one million word Penn Treebank\ncorpus from Wall Street Journal articles. We use the same splits (84k train / 2k dev / 14k test) as Ren et al. (2016b); Chen et al. (2020)."
    }, {
      "heading" : "Appendix C: Supertype/subtype pairs",
      "text" : "Table 10 shows the supertype/subtype pairs we manually annotated for our consistency test."
    }, {
      "heading" : "Appendix D: Box Edges",
      "text" : "Similar to Figure 3, we plot the semantically unrelated type boxes food and building in Figure 4. These boxes are largely misaligned as expected, and the minimum bounding box of the intersections between the building and the mention and context boxes is also off from the food box."
    }, {
      "heading" : "Appendix E: Reliability Plot",
      "text" : "Figure 5 visualizes the alignment between confidence and empirical accuracy on the UFET and OntoNotes dev sets."
    } ],
    "references" : [ {
      "title" : "BoxE: A Box Embedding Model for Knowledge Base Completion",
      "author" : [ "Ralph Abboud", "İsmail İlkan Ceylan", "Thomas Lukasiewicz", "Tommaso Salvatori." ],
      "venue" : "Proceedings of Advances in Neural Information Processing Systems (NeurIPS).",
      "citeRegEx" : "Abboud et al\\.,? 2020",
      "shortCiteRegEx" : "Abboud et al\\.",
      "year" : 2020
    }, {
      "title" : "A Neural Probabilistic Language Model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Janvin." ],
      "venue" : "3:1137–1155.",
      "citeRegEx" : "Bengio et al\\.,? 2003",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Experiment Tracking with Weights and Biases",
      "author" : [ "Lukas Biewald." ],
      "venue" : "Software available from wandb.com.",
      "citeRegEx" : "Biewald.,? 2020",
      "shortCiteRegEx" : "Biewald.",
      "year" : 2020
    }, {
      "title" : "PreCo: A large-scale dataset in preschool vocabulary for coreference resolution",
      "author" : [ "Hong Chen", "Zhenhua Fan", "Hao Lu", "Alan Yuille", "Shu Rong." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "EntEval: A holistic evaluation benchmark for entity representations",
      "author" : [ "Mingda Chen", "Zewei Chu", "Yang Chen", "Karl Stratos", "Kevin Gimpel." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Hierarchical Entity Typing via Multi-level Learning to Rank",
      "author" : [ "Tongfei Chen", "Yunmo Chen", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Ultra-Fine Entity Typing",
      "author" : [ "Eunsol Choi", "Omer Levy", "Yejin Choi", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Choi et al\\.,? 2018",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2018
    }, {
      "title" : "Natural Language Processing (Almost) from Scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa." ],
      "venue" : "Journal of Machine Learning Research, 12:2493–2537.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Improving Local Identifiability in Probabilistic Box Embeddings",
      "author" : [ "Shib Sankar Dasgupta", "Michael Boratko", "Dongxu Zhang", "Luke Vilnis", "Xiang Lorraine Li", "Andrew McCallum." ],
      "venue" : "Proceedings of Advances in Neural Information Processing Systems",
      "citeRegEx" : "Dasgupta et al\\.,? 2020",
      "shortCiteRegEx" : "Dasgupta et al\\.",
      "year" : 2020
    }, {
      "title" : "Calibration of Pre-trained Transformers",
      "author" : [ "Shrey Desai", "Greg Durrett." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Desai and Durrett.,? 2020",
      "shortCiteRegEx" : "Desai and Durrett.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Injecting Entity Types into Entity-Guided Text Generation",
      "author" : [ "Xiangyu Dong", "Wenhao Yu", "Chenguang Zhu", "Meng Jiang." ],
      "venue" : "ArXiv, abs/2009.13401.",
      "citeRegEx" : "Dong et al\\.,? 2020",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2020
    }, {
      "title" : "A Joint Model for Entity Analysis: Coreference, Typing, and Linking",
      "author" : [ "Greg Durrett", "Dan Klein." ],
      "venue" : "Transactions of the Association for Computational Linguistics (TACL), 2:477–490.",
      "citeRegEx" : "Durrett and Klein.,? 2014",
      "shortCiteRegEx" : "Durrett and Klein.",
      "year" : 2014
    }, {
      "title" : "Representing words as regions in vector space",
      "author" : [ "Katrin Erk." ],
      "venue" : "Proceedings of the Conference on Computational Natural Language Learning (CoNLL).",
      "citeRegEx" : "Erk.,? 2009a",
      "shortCiteRegEx" : "Erk.",
      "year" : 2009
    }, {
      "title" : "Supporting inferences in semantic space: representing words as regions",
      "author" : [ "Katrin Erk." ],
      "venue" : "Proceedings of the International Conference on Computational Semantics (IWCS).",
      "citeRegEx" : "Erk.,? 2009b",
      "shortCiteRegEx" : "Erk.",
      "year" : 2009
    }, {
      "title" : "Hyperbolic Entailment Cones for Learning Hierarchical Embeddings",
      "author" : [ "Octavian-Eugen Ganea", "Gary Bécigneul", "Thomas Hofmann." ],
      "venue" : "Proceedings of the International Conference on Machine Learning (ICML).",
      "citeRegEx" : "Ganea et al\\.,? 2018",
      "shortCiteRegEx" : "Ganea et al\\.",
      "year" : 2018
    }, {
      "title" : "ContextDependent Fine-Grained Entity Type Tagging",
      "author" : [ "Dan Gillick", "Nevena Lazic", "Kuzman Ganchev", "Jesse Kirchner", "David Huynh." ],
      "venue" : "CoRR, abs/1412.1820.",
      "citeRegEx" : "Gillick et al\\.,? 2014",
      "shortCiteRegEx" : "Gillick et al\\.",
      "year" : 2014
    }, {
      "title" : "On Calibration of Modern Neural Networks",
      "author" : [ "Chuan Guo", "Geoff Pleiss", "Yu Sun", "Kilian Q. Weinberger." ],
      "venue" : "Proceedings of the International Conference on Machine Learning (ICML).",
      "citeRegEx" : "Guo et al\\.,? 2017",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2017
    }, {
      "title" : "Entity Linking via Joint Encoding of Types, Descriptions, and Context",
      "author" : [ "Nitish Gupta", "Sameer Singh", "Dan Roth." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Gupta et al\\.,? 2017",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning distributed representations of concepts",
      "author" : [ "Geoffrey E Hinton." ],
      "venue" : "Proceedings of the eighth annual conference of the cognitive science society.",
      "citeRegEx" : "Hinton.,? 1986",
      "shortCiteRegEx" : "Hinton.",
      "year" : 1986
    }, {
      "title" : "OntoNotes: The 90% Solution",
      "author" : [ "Eduard Hovy", "Mitchell Marcus", "Martha Palmer", "Lance Ramshaw", "Ralph Weischedel." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Hovy et al\\.,? 2006",
      "shortCiteRegEx" : "Hovy et al\\.",
      "year" : 2006
    }, {
      "title" : "Using Type Information to Improve Entity Coreference Resolution",
      "author" : [ "Sopan Khosla", "Carolyn Rose." ],
      "venue" : "Proceedings of the First Workshop on Computational Approaches to Discourse.",
      "citeRegEx" : "Khosla and Rose.,? 2020",
      "shortCiteRegEx" : "Khosla and Rose.",
      "year" : 2020
    }, {
      "title" : "Adam: A Method for Stochastic Optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Learning to Predict Denotational Probabilities For Modeling Entailment",
      "author" : [ "Alice Lai", "Julia Hockenmaier." ],
      "venue" : "Proceedings of the Conference of the",
      "citeRegEx" : "Lai and Hockenmaier.,? 2017",
      "shortCiteRegEx" : "Lai and Hockenmaier.",
      "year" : 2017
    }, {
      "title" : "Hyperband: Bandit-Based Configuration Evaluation for Hyperparameter Optimization",
      "author" : [ "Lisha Li", "Kevin G. Jamieson", "Giulia DeSalvo", "Afshin Rostamizadeh", "Ameet Talwalkar." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Li et al\\.,? 2017",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "Smoothing the Geometry of Probabilistic box Embeddings",
      "author" : [ "Xiang Li", "Luke Vilnis", "Dongxu Zhang", "Michael Boratko", "Andrew McCallum" ],
      "venue" : "In International Conference on Learning Representations (ICLR)",
      "citeRegEx" : "Li et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "An Attentive FineGrained Entity Typing Model with Latent Type Representation",
      "author" : [ "Ying Lin", "Heng Ji." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Lin and Ji.,? 2019",
      "shortCiteRegEx" : "Lin and Ji.",
      "year" : 2019
    }, {
      "title" : "Fine-Grained Entity Recognition",
      "author" : [ "Xiao Ling", "Daniel S. Weld." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Ling and Weld.,? 2012",
      "shortCiteRegEx" : "Ling and Weld.",
      "year" : 2012
    }, {
      "title" : "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "ArXiv, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "A Fully Hyperbolic Neural Model for Hierarchical Multi-Class Classification",
      "author" : [ "Federico López", "Michael Strube." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP.",
      "citeRegEx" : "López and Strube.,? 2020",
      "shortCiteRegEx" : "López and Strube.",
      "year" : 2020
    }, {
      "title" : "Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking",
      "author" : [ "Shikhar Murty", "Patrick Verga", "Luke Vilnis", "Irena Radovanovic", "Andrew McCallum." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Murty et al\\.,? 2018",
      "shortCiteRegEx" : "Murty et al\\.",
      "year" : 2018
    }, {
      "title" : "Posterior Calibration and Exploratory Analysis for Natural Language Processing Models",
      "author" : [ "Khanh Nguyen", "Brendan O’Connor" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "citeRegEx" : "Nguyen and O.Connor.,? \\Q2015\\E",
      "shortCiteRegEx" : "Nguyen and O.Connor.",
      "year" : 2015
    }, {
      "title" : "Poincaré Embeddings for Learning Hierarchical Representations",
      "author" : [ "Maximilian Nickel", "Douwe Kiela." ],
      "venue" : "Proceedings of Advances in Neural Information Processing Systems (NeurIPS).",
      "citeRegEx" : "Nickel and Kiela.,? 2017",
      "shortCiteRegEx" : "Nickel and Kiela.",
      "year" : 2017
    }, {
      "title" : "Learning to Denoise Distantly-Labeled Data for Entity Typing",
      "author" : [ "Yasumasa Onoe", "Greg Durrett." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "citeRegEx" : "Onoe and Durrett.,? 2019",
      "shortCiteRegEx" : "Onoe and Durrett.",
      "year" : 2019
    }, {
      "title" : "FineGrained Entity Typing for Domain Independent Entity Linking",
      "author" : [ "Yasumasa Onoe", "Greg Durrett." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Onoe and Durrett.,? 2020a",
      "shortCiteRegEx" : "Onoe and Durrett.",
      "year" : 2020
    }, {
      "title" : "Interpretable Entity Representations through Large-Scale Typing",
      "author" : [ "Yasumasa Onoe", "Greg Durrett." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP.",
      "citeRegEx" : "Onoe and Durrett.,? 2020b",
      "shortCiteRegEx" : "Onoe and Durrett.",
      "year" : 2020
    }, {
      "title" : "GloVe: Global Vectors for Word Representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep Contextualized Word Representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Query2box: Reasoning over Knowledge Graphs in Vector Space using Box Embeddings",
      "author" : [ "Hongyu Ren", "Weihua Hu", "Jure Leskovec." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Ren et al\\.,? 2020",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2020
    }, {
      "title" : "AFET: Automatic FineGrained Entity Typing by Hierarchical Partial-Label Embedding",
      "author" : [ "Xiang Ren", "Wenqi He", "Meng Qu", "Lifu Huang", "Heng Ji", "Jiawei Han." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Ren et al\\.,? 2016a",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2016
    }, {
      "title" : "Label Noise Reduction in Entity Typing by Heterogeneous Partial-Label Embedding",
      "author" : [ "Xiang Ren", "Wenqi He", "Meng Qu", "Clare R. Voss", "Heng Ji", "Jiawei Han." ],
      "venue" : "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery",
      "citeRegEx" : "Ren et al\\.,? 2016b",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural Architectures for Fine-grained Entity Type Classification",
      "author" : [ "Sonse Shimaoka", "Pontus Stenetorp", "Kentaro Inui", "Sebastian Riedel." ],
      "venue" : "Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics",
      "citeRegEx" : "Shimaoka et al\\.,? 2017",
      "shortCiteRegEx" : "Shimaoka et al\\.",
      "year" : 2017
    }, {
      "title" : "Highway Networks",
      "author" : [ "Rupesh Kumar Srivastava", "Klaus Greff", "Jürgen Schmidhuber." ],
      "venue" : "ArXiv, abs/1505.00387.",
      "citeRegEx" : "Srivastava et al\\.,? 2015",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2015
    }, {
      "title" : "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Order-embeddings of Images and Languagen",
      "author" : [ "Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Vendrov et al\\.,? 2016",
      "shortCiteRegEx" : "Vendrov et al\\.",
      "year" : 2016
    }, {
      "title" : "Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures",
      "author" : [ "Luke Vilnis", "Xiang Li", "Shikhar Murty", "Andrew McCallum." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Vilnis et al\\.,? 2018",
      "shortCiteRegEx" : "Vilnis et al\\.",
      "year" : 2018
    }, {
      "title" : "Word Representations via Gaussian Embedding",
      "author" : [ "Luke Vilnis", "Andrew McCallum." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Vilnis and McCallum.,? 2015",
      "shortCiteRegEx" : "Vilnis and McCallum.",
      "year" : 2015
    }, {
      "title" : "BBN pronoun coreference and entity type corpus",
      "author" : [ "Ralph Weischedel", "Ada Brunstein." ],
      "venue" : "Linguistic Data Consortium.",
      "citeRegEx" : "Weischedel and Brunstein.,? 2005",
      "shortCiteRegEx" : "Weischedel and Brunstein.",
      "year" : 2005
    }, {
      "title" : "Transformers: State-of-the-Art Natural Language Processing",
      "author" : [ "wen Xu", "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Xu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Imposing Label-Relational Inductive Bias for Extremely Fine-Grained Entity Typing",
      "author" : [ "Wenhan Xiong", "Jiawei Wu", "Deren Lei", "Mo Yu", "Shiyu Chang", "Xiaoxiao Guo", "William Yang Wang." ],
      "venue" : "Proceedings of the Conference of the North American Chap-",
      "citeRegEx" : "Xiong et al\\.,? 2019",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural FineGrained Entity Type Classification with HierarchyAware Loss",
      "author" : [ "Peng Xu", "Denilson Barbosa." ],
      "venue" : "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Xu and Barbosa.,? 2018",
      "shortCiteRegEx" : "Xu and Barbosa.",
      "year" : 2018
    }, {
      "title" : "Embedding methods for fine grained entity type classification",
      "author" : [ "Dani Yogatama", "Daniel Gillick", "Nevena Lazic." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Yogatama et al\\.,? 2015",
      "shortCiteRegEx" : "Yogatama et al\\.",
      "year" : 2015
    }, {
      "title" : "Fine-grained Entity Typing through Increased Discourse Context and Adaptive Classification Thresholds",
      "author" : [ "Sheng Zhang", "Kevin Duh", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the Seventh Joint",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "The development of named entity recognition and entity typing has been characterized by a growth in the size and complexity of type sets: from 4 (Tjong Kim Sang and De Meulder, 2003) to 17 (Hovy et al., 2006) to hundreds (Weischedel and Brunstein, 2005; Ling and Weld, 2012) or thousands (Choi et al.",
      "startOffset" : 189,
      "endOffset" : 208
    }, {
      "referenceID" : 47,
      "context" : ", 2006) to hundreds (Weischedel and Brunstein, 2005; Ling and Weld, 2012) or thousands (Choi et al.",
      "startOffset" : 20,
      "endOffset" : 73
    }, {
      "referenceID" : 27,
      "context" : ", 2006) to hundreds (Weischedel and Brunstein, 2005; Ling and Weld, 2012) or thousands (Choi et al.",
      "startOffset" : 20,
      "endOffset" : 73
    }, {
      "referenceID" : 6,
      "context" : ", 2006) to hundreds (Weischedel and Brunstein, 2005; Ling and Weld, 2012) or thousands (Choi et al., 2018).",
      "startOffset" : 87,
      "endOffset" : 106
    }, {
      "referenceID" : 47,
      "context" : "of hierarchical structure (Weischedel and Brunstein, 2005; Ling and Weld, 2012; Gillick et al., 2014; Murty et al., 2018), so effective models for these tasks frequently engage with this hierarchy explicitly.",
      "startOffset" : 26,
      "endOffset" : 121
    }, {
      "referenceID" : 27,
      "context" : "of hierarchical structure (Weischedel and Brunstein, 2005; Ling and Weld, 2012; Gillick et al., 2014; Murty et al., 2018), so effective models for these tasks frequently engage with this hierarchy explicitly.",
      "startOffset" : 26,
      "endOffset" : 121
    }, {
      "referenceID" : 16,
      "context" : "of hierarchical structure (Weischedel and Brunstein, 2005; Ling and Weld, 2012; Gillick et al., 2014; Murty et al., 2018), so effective models for these tasks frequently engage with this hierarchy explicitly.",
      "startOffset" : 26,
      "endOffset" : 121
    }, {
      "referenceID" : 30,
      "context" : "of hierarchical structure (Weischedel and Brunstein, 2005; Ling and Weld, 2012; Gillick et al., 2014; Murty et al., 2018), so effective models for these tasks frequently engage with this hierarchy explicitly.",
      "startOffset" : 26,
      "endOffset" : 121
    }, {
      "referenceID" : 30,
      "context" : "Prior systems incorporate this structure via hierarchical losses (Murty et al., 2018; Xu and Barbosa, 2018; Chen et al., 2020) or by embedding types into a high-dimensional Euclidean or hyperbolic space (Yogatama et al.",
      "startOffset" : 65,
      "endOffset" : 126
    }, {
      "referenceID" : 50,
      "context" : "Prior systems incorporate this structure via hierarchical losses (Murty et al., 2018; Xu and Barbosa, 2018; Chen et al., 2020) or by embedding types into a high-dimensional Euclidean or hyperbolic space (Yogatama et al.",
      "startOffset" : 65,
      "endOffset" : 126
    }, {
      "referenceID" : 5,
      "context" : "Prior systems incorporate this structure via hierarchical losses (Murty et al., 2018; Xu and Barbosa, 2018; Chen et al., 2020) or by embedding types into a high-dimensional Euclidean or hyperbolic space (Yogatama et al.",
      "startOffset" : 65,
      "endOffset" : 126
    }, {
      "referenceID" : 51,
      "context" : ", 2020) or by embedding types into a high-dimensional Euclidean or hyperbolic space (Yogatama et al., 2015; López and Strube, 2020).",
      "startOffset" : 84,
      "endOffset" : 131
    }, {
      "referenceID" : 29,
      "context" : ", 2020) or by embedding types into a high-dimensional Euclidean or hyperbolic space (Yogatama et al., 2015; López and Strube, 2020).",
      "startOffset" : 84,
      "endOffset" : 131
    }, {
      "referenceID" : 6,
      "context" : "However, the former approach requires prior knowledge of the type hierarchy, which is unsuitable for a recent class of large type sets where the hierarchy is not explicit (Choi et al., 2018; Onoe and Durrett, 2020a).",
      "startOffset" : 171,
      "endOffset" : 215
    }, {
      "referenceID" : 34,
      "context" : "However, the former approach requires prior knowledge of the type hierarchy, which is unsuitable for a recent class of large type sets where the hierarchy is not explicit (Choi et al., 2018; Onoe and Durrett, 2020a).",
      "startOffset" : 171,
      "endOffset" : 215
    }, {
      "referenceID" : 45,
      "context" : "In this paper, we describe an approach that represents entity types with box embeddings in a highdimensional space (Vilnis et al., 2018).",
      "startOffset" : 115,
      "endOffset" : 136
    }, {
      "referenceID" : 39,
      "context" : "Compared to embedding types as points in Euclidean space (Ren et al., 2016a), the box space is expressive and suitable for representing entity types due to its geometric properties.",
      "startOffset" : 57,
      "endOffset" : 76
    }, {
      "referenceID" : 6,
      "context" : "We evaluate on four entity typing benchmarks: Ultra-fine Entity Typing (Choi et al., 2018), OntoNotes (Gillick et al.",
      "startOffset" : 71,
      "endOffset" : 90
    }, {
      "referenceID" : 16,
      "context" : ", 2018), OntoNotes (Gillick et al., 2014), BBN (Weischedel and Brunstein, 2005), and FIGER (Ling and Weld, 2012).",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 47,
      "context" : ", 2014), BBN (Weischedel and Brunstein, 2005), and FIGER (Ling and Weld, 2012).",
      "startOffset" : 13,
      "endOffset" : 45
    }, {
      "referenceID" : 27,
      "context" : ", 2014), BBN (Weischedel and Brunstein, 2005), and FIGER (Ling and Weld, 2012).",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 8,
      "context" : "The geometric structure of boxes enables complex interactions with only a moderate number of dimensions (Dasgupta et al., 2020).",
      "startOffset" : 104,
      "endOffset" : 127
    }, {
      "referenceID" : 6,
      "context" : "While some datasets such as OntoNotes have orderly ontologies, recent work on entity typing has often focused on noisy type sets from crowdworkers (Choi et al., 2018) or derived from Wikipedia (Onoe and Durrett, 2020a).",
      "startOffset" : 147,
      "endOffset" : 166
    }, {
      "referenceID" : 34,
      "context" : ", 2018) or derived from Wikipedia (Onoe and Durrett, 2020a).",
      "startOffset" : 34,
      "endOffset" : 59
    }, {
      "referenceID" : 10,
      "context" : "Using pre-trained BERT5 (Devlin et al., 2019), we encode the whole sequence into a single vector by taking the hidden vector at the [CLS] token.",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 42,
      "context" : "A highway layer (Srivastava et al., 2015) projects down the hidden vector h[CLS] ∈ Rl to the R2d space, where l is the hidden dimension of the encoder (BERT), and d is the dimension of the box space.",
      "startOffset" : 16,
      "endOffset" : 41
    }, {
      "referenceID" : 22,
      "context" : "We optimize this objective using gradient-based optimization algorithms such as Adam (Kingma and Ba, 2015).",
      "startOffset" : 85,
      "endOffset" : 106
    }, {
      "referenceID" : 6,
      "context" : "Entity Typing We evaluate our approach on the Ultra-Fine Entity Typing (UFET) dataset (Choi et al., 2018) with the standard splits (2k for each of train, dev, and test).",
      "startOffset" : 86,
      "endOffset" : 105
    }, {
      "referenceID" : 16,
      "context" : "Additionally, we test our box-based model on three other entity typing benchmarks that have relatively simpler entity type inventories with known hierarchies, namely OntoNotes (Gillick et al., 2014), BBN (Weischedel and Brunstein, 2005) , and FIGER (Ling and Weld, 2012).",
      "startOffset" : 176,
      "endOffset" : 198
    }, {
      "referenceID" : 47,
      "context" : ", 2014), BBN (Weischedel and Brunstein, 2005) , and FIGER (Ling and Weld, 2012).",
      "startOffset" : 13,
      "endOffset" : 45
    }, {
      "referenceID" : 27,
      "context" : ", 2014), BBN (Weischedel and Brunstein, 2005) , and FIGER (Ling and Weld, 2012).",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 28,
      "context" : "Calibration Desai and Durrett (2020) study calibration of pre-trained Transformers such as BERT and RoBERTa (Liu et al., 2019) on natural language inference, paraphrase detection, and commonsense reasoning.",
      "startOffset" : 108,
      "endOffset" : 126
    }, {
      "referenceID" : 17,
      "context" : "for their logits depending on how long they are trained, we post-hoc calibrate each of our models using temperature scaling (Guo et al., 2017) and a shift parameter.",
      "startOffset" : 124,
      "endOffset" : 142
    }, {
      "referenceID" : 4,
      "context" : "Following Onoe and Durrett (2020b), we evaluate entity representation given by the box-based and vector-based models on the Coreference Arc Prediction (CAP) task (Chen et al., 2019) derived from PreCo (Chen et al.",
      "startOffset" : 162,
      "endOffset" : 181
    }, {
      "referenceID" : 36,
      "context" : "(2018) is a LSTM-based model using GloVe (Pennington et al., 2014).",
      "startOffset" : 41,
      "endOffset" : 66
    }, {
      "referenceID" : 37,
      "context" : "Onoe and Durrett (2019) use ELMo (Peters et al., 2018) and apply denoising to fix label inconsistency in the distantly annotated data.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 33,
      "context" : "Note that past work on this dataset has used BERT-base (Onoe and Durrett, 2019).",
      "startOffset" : 55,
      "endOffset" : 79
    }, {
      "referenceID" : 26,
      "context" : "Work on other datasets has used ELMo and observed that BERT-based models have surprisingly underperformed (Lin and Ji, 2019).",
      "startOffset" : 106,
      "endOffset" : 124
    }, {
      "referenceID" : 29,
      "context" : "HY XLarge (López and Strube, 2020), a hyperbolic model designed to learn hierarchical structure in entity types, exceeds the performance of the models with similar sizes such as Choi et al.",
      "startOffset" : 10,
      "endOffset" : 34
    }, {
      "referenceID" : 4,
      "context" : "Table 8: Accuracy on the CAP test set (Chen et al., 2019).",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 19,
      "context" : "Embeddings Embedding concepts/words into a high-dimensional vector space (Hinton, 1986) has a long history and has been an essential part of neural networks for language (Bengio et al.",
      "startOffset" : 73,
      "endOffset" : 87
    }, {
      "referenceID" : 1,
      "context" : "Embeddings Embedding concepts/words into a high-dimensional vector space (Hinton, 1986) has a long history and has been an essential part of neural networks for language (Bengio et al., 2003; Collobert et al., 2011).",
      "startOffset" : 170,
      "endOffset" : 215
    }, {
      "referenceID" : 7,
      "context" : "Embeddings Embedding concepts/words into a high-dimensional vector space (Hinton, 1986) has a long history and has been an essential part of neural networks for language (Bengio et al., 2003; Collobert et al., 2011).",
      "startOffset" : 170,
      "endOffset" : 215
    }, {
      "referenceID" : 46,
      "context" : "There is similarly a long history of rethinking the semantics of these embedding spaces, such as treating words as regions using sparse count-based vectors (Erk, 2009a,b) or dense distributed vectors (Vilnis and McCallum, 2015).",
      "startOffset" : 200,
      "endOffset" : 227
    }, {
      "referenceID" : 44,
      "context" : "Order embeddings (Vendrov et al., 2016) or their probabilistic version (POE) (Lai and Hockenmaier, 2017) are one technique suited for hierarchical modeling.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 23,
      "context" : ", 2016) or their probabilistic version (POE) (Lai and Hockenmaier, 2017) are one technique suited for hierarchical modeling.",
      "startOffset" : 45,
      "endOffset" : 72
    }, {
      "referenceID" : 15,
      "context" : "2017; López and Strube, 2020) can also model hierarchical relationships as can hyperbolic entailment cones (Ganea et al., 2018); however, these approaches lack a probabilistic interpretation.",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 0,
      "context" : "Recent work on knowledge base completion (Abboud et al., 2020) and reasoning over knowledge graphs (Ren et al.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 38,
      "context" : ", 2020) and reasoning over knowledge graphs (Ren et al., 2020) embeds relations or queries using box embeddings, but entities are still represented as vectors.",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 27,
      "context" : "Recent work has focused chiefly on predicted fine-grained entity types (Ling and Weld, 2012; Gillick et al., 2014; Choi et al., 2018), as these convey significantly more information for downstream tasks.",
      "startOffset" : 71,
      "endOffset" : 133
    }, {
      "referenceID" : 16,
      "context" : "Recent work has focused chiefly on predicted fine-grained entity types (Ling and Weld, 2012; Gillick et al., 2014; Choi et al., 2018), as these convey significantly more information for downstream tasks.",
      "startOffset" : 71,
      "endOffset" : 133
    }, {
      "referenceID" : 6,
      "context" : "Recent work has focused chiefly on predicted fine-grained entity types (Ling and Weld, 2012; Gillick et al., 2014; Choi et al., 2018), as these convey significantly more information for downstream tasks.",
      "startOffset" : 71,
      "endOffset" : 133
    }, {
      "referenceID" : 12,
      "context" : "Entity typing information has been used across a range of NLP tasks, including models for entity linking and coreference (Durrett and Klein, 2014).",
      "startOffset" : 121,
      "endOffset" : 146
    }, {
      "referenceID" : 18,
      "context" : "Typing has been shown to be useful for crossdomain entity linking specifically (Gupta et al., 2017; Onoe and Durrett, 2020a).",
      "startOffset" : 79,
      "endOffset" : 124
    }, {
      "referenceID" : 34,
      "context" : "Typing has been shown to be useful for crossdomain entity linking specifically (Gupta et al., 2017; Onoe and Durrett, 2020a).",
      "startOffset" : 79,
      "endOffset" : 124
    }, {
      "referenceID" : 35,
      "context" : "It has also recently been applied to coreference resolution (Onoe and Durrett, 2020b; Khosla and Rose, 2020) and text generation (Dong et al.",
      "startOffset" : 60,
      "endOffset" : 108
    }, {
      "referenceID" : 21,
      "context" : "It has also recently been applied to coreference resolution (Onoe and Durrett, 2020b; Khosla and Rose, 2020) and text generation (Dong et al.",
      "startOffset" : 60,
      "endOffset" : 108
    }, {
      "referenceID" : 11,
      "context" : "It has also recently been applied to coreference resolution (Onoe and Durrett, 2020b; Khosla and Rose, 2020) and text generation (Dong et al., 2020), suggesting that it can be a useful intermediate layer even in pretrained neural models.",
      "startOffset" : 129,
      "endOffset" : 148
    } ],
    "year" : 2021,
    "abstractText" : "Neural entity typing models typically represent fine-grained entity types as vectors in a high-dimensional space, but such spaces are not well-suited to modeling these types’ complex interdependencies. We study the ability of box embeddings, which embed concepts as d-dimensional hyperrectangles, to capture hierarchies of types even when these relationships are not defined explicitly in the ontology. Our model represents both types and entity mentions as boxes. Each mention and its context are fed into a BERT-based model to embed that mention in our box space; essentially, this model leverages typological clues present in the surface text to hypothesize a type representation for the mention. Box containment can then be used to derive both the posterior probability of a mention exhibiting a given type and the conditional probability relations between types themselves. We compare our approach with a vector-based typing model and observe state-of-the-art performance on several entity typing benchmarks. In addition to competitive typing performance, our box-based model shows better performance in prediction consistency (predicting a supertype and a subtype together) and confidence (i.e., calibration), demonstrating that the box-based model captures the latent type hierarchies better than the vector-based model does.1",
    "creator" : "LaTeX with hyperref"
  }
}