{
  "name" : "2021.acl-long.91.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation",
    "authors" : [ "Elena Voita", "Rico Sennrich", "Ivan Titov" ],
    "emails" : [ "lena-voita@hotmail.com", "sennrich@cl.uzh.ch", "ititov@inf.ed.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1126–1140\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1126"
    }, {
      "heading" : "1 Introduction",
      "text" : "With the success of neural approaches to natural language processing, analysis of NLP models has become an important and active topic of research. In NMT, approaches to analysis include probing for linguistic structure (Belinkov et al., 2017; Conneau et al., 2018), evaluating via contrastive translation pairs (Sennrich, 2017; Burlot and Yvon, 2017; Rios Gonzales et al., 2017; Tang\n1We release the code at https://github.com/ lena-voita/the-story-of-heads.\net al., 2018), inspecting model components, such as attention (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019) or neurons (Dalvi et al., 2019; Bau et al., 2019), among others.\nUnfortunately, although a lot of work on model analysis has been done, a question of how the NMT predictions are formed remains largely open. Namely, the generation of a target token is defined by two types of context, source and target, but there is no method which explicitly evaluates the relative contribution of source and target to a given prediction. The ability to measure this relative contribution is important for model understanding since previous work showed that NMT models often fail to effectively control information flow from source and target contexts. For example, adding context gates to dynamically control the influence of source and target leads to improvement for both RNN (Tu et al., 2017; Wang et al., 2018) and Transfomer (Li et al., 2020) models. A more popular example is a model’s tendency to generate hallucinations (fluent but inadequate translations); it is usually attributed to the inappropriately strong influence of target context. Several works observed that, when hallucinating, a model fails to properly use source: it produces a deficient attention matrix, where almost all the probability mass is concentrated on uninformative source tokens (EOS and punctuation) (Lee et al., 2018; Berard et al., 2019).\nWe argue that a natural way to estimate how the source and target contexts contribute to generation is to apply Layerwise Relevance Propagation (LRP) (Bach et al., 2015) to NMT models. LRP redistributes the information used for a prediction between all input elements keeping the total contribution constant. This ‘conservation principle’ makes relevance propagation unique: differently from other methods estimating influence of individual tokens (Alvarez-Melis and Jaakkola, 2017; He\net al., 2019a; Ma et al., 2018), LRP evaluates not an abstract quantity reflecting a token importance, but the proportion of each token’s influence.\nWe extend one of the LRP variants to the Transformer and conduct the first analysis of NMT models which explicitly evaluates the source and target relative contributions to the generation process. We analyze changes in these contributions when conditioning on different types of prefixes (reference, generated by a model or random translations), when varying training objective or the amount of training data, and during the training process. We show that models suffering from exposure bias are more prone to over-relying on target history (and hence to hallucinating) than the ones where the exposure bias is mitigated. When comparing models trained with different amount of data, we find that extra training data teaches a model to rely on source information more heavily and to be more confident in the choice of important tokens. When analyzing the training process, we find that changes in training are non-monotonic and form several distinct stages (e.g., stages changing direction from decreasing influence of source to increasing).\nOur key contributions are as follows:\n• we show how to use LRP to evaluate the relative contribution of source and target to NMT predictions;\n• we analyze how the contribution of source and target changes when conditioning on different types of prefixes: reference, generated by a model or random translations;\n• by looking at the contributions when conditioning on random prefixes, we observe that models suffering from exposure bias are more prone to over-relying on target history (and hence to hallucinating);\n• we find that (i) with more data, models rely on source information more and have more sharp token contributions, (ii) the training process is non-monotonic with several distinct stages."
    }, {
      "heading" : "2 Layer-wise Relevance Propagation",
      "text" : "Layer-wise relevance propagation is a framework which decomposes the prediction of a deep neural network computed over an instance, e.g. an image or sentence, into relevance scores for single input dimensions of the sample such as subpixels of an image or neurons of input token embeddings. The\noriginal LRP version was developed for computer vision models (Bach et al., 2015) and is not directly applicable to the Transformer (e.g., to the attention layers). In this section, we explain the general idea behind LRP, specify which of the existing LRP variants we use, and show how to extend LRP to the NMT Transformer model.2"
    }, {
      "heading" : "2.1 General Idea: Conservation Principle",
      "text" : "In its general form, LRP assumes that the model can be decomposed into several layers of computation. The first layer are the inputs (for example, the pixels of an image or tokens of a sentence), the last layer is the real-valued prediction output of the model f . The l-th layer is modeled as a vector x(l) = (x\n(l) i ) V (l) i=1 with dimensionality V (l). Layer-\nwise relevance propagation assumes that we have a relevance score R(l+1)i for each dimension x (l+1) i of the vector x at layer l + 1. The idea is to find a relevance score R(l)i for each dimension x (l) i of the previous layer l such that the following holds:\nf= ...= ∑ i R (l+1) i = ∑ i R (l) i = ...= ∑ i R (1) i . (1)\nThis equation represents a conservation principle, which LRP exploits to back-propagate the prediction. Intuitively, this means that the total contribution of neurons at each layer is constant."
    }, {
      "heading" : "2.2 Redistribution Rules",
      "text" : "Assume that we know the relevanceR(l+1)j of a neuron j at network layer l+1 for the prediction f(x). Then we would like to decompose this relevance into messages R(l,l+1)i←j sent from the neuron j at layer l + 1 to each of its input neurons i at layer l. For the conservation principle to hold, these messages R(l,l+1)i←j have to satisfy the constraint:\nR (l+1) j = ∑ i R (l,l+1) i←j . (2)\nThen we can define the relevance of a neuron i at layer l by summing all messages from neurons at layer (l + 1):\nR (l) i = ∑ j R (l,l+1) i←j . (3)\nEquations (2) and (3) define the propagation of relevance from layer l+1 to layer l. The only thing that is missing is specific formulas for computing the\n2Previous work applying one of the LRP variants to NMT (Ding et al., 2017; Voita et al., 2019) do not describe extensions beyond the original LRP rules (Bach et al., 2015).\nmessages R(l,l+1)i←j . Usually, the message R (l,l+1) i←j has the following structure:\nR (l,l+1) i←j = vijR (l+1) j , ∑ i vij = 1. (4)\nSeveral versions of LRP satisfying equation (4) (and, therefore, the conservation principle) have been introduced: LRP-ε, LRP-αβ and LRPγ (Bach et al., 2015; Binder et al., 2016; Montavon et al., 2019). We use LRP-αβ (Bach et al., 2015; Binder et al., 2016), which defines relevances at each step in such a way that they are positive."
    }, {
      "heading" : "Rule for relevance propagation: the αβ-rule.",
      "text" : "Let us consider the simplest case of linear layers with non-linear activation functions, namely\nzij = x (l) i wij , zj = ∑ i zij + bi, x (l+1) j = g(zj),\nwhere wij is a weight connecting the neuron x (l) i to neuron x(l+1)j , bj is a bias term, and g is a nonlinear activation function. Let\nz+j = ∑ i z+ij + b + j , z − j = ∑ i z−ij + b − j ,\nwhere + = max(0, ) and − = min(0, ). Then the αβ-rule (Bach et al., 2015; Binder et al., 2016) is given by the equation\nR (l,l+1) i←j = R (l+1) j ·\n( α · z+ij\nz+j + β ·\nz−ij z−j\n) , (5)\nwhere α+β = 1. Note that all terms in the brackets are always positive: negative signs of z−j and z − ij cancel out when evaluating the ratio. This propagation method allows to control manually the importance of positive and negative evidence by choosing different α and β. For example, α, β = 12 treats positive and negative contributions as equally important, while α = 1, β = 0 considers only positive contributions. In our experiments, both versions lead to the same observations.\nNote that (5) is directly applicable to all layers for which there exist functions gj and hij such that\nx (l+1) j = gj (∑ i hij(x (l) i ) ) . (6)\nThese layers include linear, convolutional and maxpooling operations. Additionally, pointwise monotonic activation functions gj (e.g., ReLU) are ignored by LRP (Bach et al., 2015).\nPropagating relevance through attention layers. For the structures that do not fit the form (6), the weighting vij can be obtained by performing a first order Taylor expansion of a neuron x(l+1)j (Bach et al., 2015; Binder et al., 2016).\nFor attention layers in the Transformer, we extend the approach by Binder et al. (2016). Namely, let x(l+1)j = f(x\n(l)), f(x) = f(x1, . . . , xn). Then by Taylor expansion at some point x̂ = (x̂1, . . . , x̂n), we get\nf(x̂) ≈ f(x(l)) + ∑ i←j ∂f ∂xi (x(l)) · (x̂i − x(l)i ),\nx (l+1) j =f(x (l)) ≈ f(x̂)+ ∑ i←j ∂f ∂xi (x(l))·(x(l)i −x̂i).\nElements of the sum can be assigned to incoming neurons, and the zero-order term can be redistributed equally between them. This leads to the following decomposition:\nzij = 1\nn f(x̂) +\n∂f ∂xi (x(l)) · (x(l)i − x̂i). (7)\nWe use the zero vector in place of x̂. Equation (7), along with the standard redistribution rules (5), defines relevance propagation for complex non-linear layers. In the Transformer, we apply equation (7) to the softmax operations in the attention layers; all other operations inside the attention layers are linear functions, and the rule (5) can be used."
    }, {
      "heading" : "2.3 LRP for Conditional Language Models",
      "text" : "Given a source sequence x = (x1, . . . , xS) and a target sequence y = (y1, . . . , yT ), standard autoregressive NMT models (or, in a more broad sense, conditional language models) are trained to predict words in the target sequence, word by word. Formally, at each generation step such models predict p(yt|x1:S , y1:t−1) relying on both source tokens x1:S and already generated target tokens y1:t−1. Using LRP, we evaluate relative contribution of all tokens, source and target, to the current prediction.\nPropagating through decoder and encoder. At first glance, it can be unclear how to apply a layerwise method to a not completely layered architecture (such as encoder-decoder). This, however, is rather straightforward and is done in two steps:\n1. total relevance is propagated through the decoder. Since the decoder uses representations\nfrom the final encoder layer, part of the relevance ‘leaks’ to the encoder; this happens at each decoder layer;\n2. relevance leaked to the encoder is propagated through the encoder layers.\nThe total contribution of neurons in each decoder layer is not preserved (part of the relevance leaks to the encoder), but the total contribution of all tokens – across the source and the target prefix – remains equal to the model prediction.\nWe evaluate relevance of input neurons to the top-1 logit predicted by a model. Then token relevance (or its contribution) is the sum of relevances of its neurons.\nNotation. Without loss of generality, we can assume that the total relevance for each prediction equals 1.3 Let us denote by Rt(xi) and Rt(yj) the contribution of source token xi and target token yj to the prediction at generation step t, respectively. Then source and target contributions are defined as\nRt(source) = ∑ i Rt(xi), Rt(target) = t−1∑ j=1 Rt(yj).\nNote that ∀ t Rt(source)+Rt(target)=1; R1(source) = 1, R1(target) = 0, and ∀j ≥ t Rt(yj)=0."
    }, {
      "heading" : "3 Experimental setting",
      "text" : "Model. We follow the setup of Transformer base model (Vaswani et al., 2017) with the standard training setting. More details on hyperparameters and the optimizer can be found in the appendix.\nData. We use random subsets of the WMT14 EnFr dataset of different size: 1m, 2.5m, 5m, 10m, 20m, 30m sentence pairs. In Sections 4 and 7, we report results for the model trained on the 1m subset. In Section 6, we show how the results depend on the amount of training data.\nEvaluating LRP. The αβ-LRP we use requires choosing values for α and β, α + β = 1. We tried treating positive and negative contributions as equally important (α = β = 12 ), or considering only positive contributions (α = 1, β = 0). The observed patterns in behavior were the same for these two versions. In the main text, we use α = 1; in the appendix, we provide results for α = β = 12 .\n3More formally, if we evaluate relevance for top-1 logit predicted by a model, then the total relevance is equal to the value of this logit. However, the conservation principle allows us to assume that this logit is equal to 1 and to consider relative contributions.\nReporting results. All presented results are averaged over an evaluation dataset of 1000 sentence pairs. In each evaluation dataset, all examples have the same number of tokens in the source, as well as in the target (e.g., 20 source and 23 target tokens; the exact number for each experiment is clear from the results).4"
    }, {
      "heading" : "4 Getting Acquainted",
      "text" : "In this section, we explain general patterns in model behavior and illustrate the usage of LRP by evaluating different statistics within a single model. Later, we will show how these results change when varying the amount of training data (Section 6) and during model training (Section 7)."
    }, {
      "heading" : "4.1 Changes in contributions",
      "text" : "Here we evaluate changes in the source contribution during generation, and in contributions of source tokens at different positions to entire output.\nSource −→ target(k). For each generation step t, we evaluate total contribution of source Rt(source). Note that this is equivalent to evaluating total contribution of prefix since Rt(prefix) = 1− Rt(source) (Section 2.3).\nResults are shown in Figure 1(a).5 We see that, during the generation process, the influence of source decreases (or, equivalently, the influence of the prefix increases). This is expected: with a longer prefix, the model has less uncertainty in deciding which source tokens to use, but needs to control more for fluency. There is also a large drop of source influence for the last token: apparently, to\n4Note that we have to fix the number of tokens in the source and target to get reliable comparisons. We choose sentences of length 20 and 23 because these are among the most frequent sentence lengths in the dataset. We also looked at sentences with 16, 25, 29 tokens – observed patterns were the same.\n5Since the first token is always generated solely relying on the source, we plot starting from the second token.\ngenerate the EOS token, the model relies on prefix much more than when generating other tokens.\nSource(k) −→ target. Now we want to understand if there is a tendency to use source tokens at certain positions more than tokens at the others. For each source token position k, we evaluate its total contribution to the whole target sequence. To eliminate the effect of decreasing source influence during generation, at each step t we normalize source contributions Rt(xk) over the total contribution of source at this step Rt(source). Formally, for\nthe k-th token we evaluate T∑ t=1 Rt(xk)/Rt(source). For convenience, we multiply the result by ST : this makes the average total contribution of each token equal to 1.\nFigure 1(b) shows that, on average, source tokens at earlier positions influence translations more than tokens at later ones. This may be because the alignment between English and French languages is roughly monotonic. We leave for future work investigating the changes in this behavior for language pairs with more complex alignment (e.g., English-Japanese)."
    }, {
      "heading" : "4.2 Entropy of contributions",
      "text" : "Now let us look at how ‘sharp’ contributions of source or target tokens are at different generation steps. For each step t, we evaluate entropy of (normalized) source or target contributions: {Rt(xi)/Rt(source)}Si=1 or {Rt(yj)/Rt(target)} t−1 j=1.\nEntropy of source contributions. Figure 2(a) shows that during generation, entropy increases until approximately 2/3 of the translation is generated, then decreases when generating the remaining part. Interestingly, for the last punctuation mark and the EOS token, entropy of source contributions is very high: the decision to complete the sentence\nrequires broader context.\nEntropy of target contributions. Figure 2(b) shows that entropy of target contributions is higher for longer prefixes. This means that the model does use longer contexts in a non-trivial way."
    }, {
      "heading" : "4.3 Reference, Model and Random Prefixes",
      "text" : "Let us now look at how model behavior changes when feeding different types of prefixes: prefixes of reference translations, translations generated by the model, and random sentences in the target language.6 As in previous experiments, we evaluate relevance for top-1 logit predicted by the model.\nReference vs model prefixes. When feeding model-generated prefixes, the model uses source more (Figure 3(a)) and has more focused source contributions (lower entropy in Figure 3(b)) than when generating the reference. This may be because model-generated translations are ‘easier’ than references. For example, beam search translations contain fewer rare tokens (Burlot and Yvon, 2018; Ott et al., 2018), are simpler syntactically (Burlot and Yvon, 2018) and, according to the fuzzy reordering score (Talbot et al., 2011), model translations have significantly less reordering compared to the real parallel sentences (Zhou et al., 2020). As we see from our experiments, these simpler model-generated prefixes allow for the model\n6Random prefixes come from the same evaluation set, but with shuffled target sentences.\nto rely on the source more and to be more confident when choosing relevant source tokens.\nReference vs random prefixes. Results for random sentence prefixes are given in Figures 3c, 3d. The reaction to random prefixes helps us study the self-recovery ability of NMT models. Previous work has found that models can fall into a hallucination mode where “the decoder ignores context from the encoder and samples from its language mode” (Koehn and Knowles, 2017; Lee et al., 2018). In contrast, He et al. (2019b) found that a language model is able to recover from artificially distorted history input and generate reasonable samples.\nOur results show evidence for both. At the beginning of the generation process, the model tends to rely more on the source context when given a random prefix compared to the reference prefix, indicating a self-recovery mode. However, when the prefix becomes longer, the model choice shifts towards ignoring the source and relying more on the target: Figure 3c shows a large drop of source influence for later positions. Figure 3d also shows that with a random prefix, the entropy of source contributions is high and is roughly constant."
    }, {
      "heading" : "5 Exposure Bias and Source Contributions",
      "text" : "The results in the previous section agree with some observations made in previous work studying selfrecovery and hallucinations. In this section, we illustrate more explicitly how our methodology can be used to shed light on the effects of exposure bias and training objectives.\nWang and Sennrich (2020) empirically link the hallucination mode to exposure bias (Ranzato et al., 2016), i.e. the mismatch between the gold history seen at training time, and the (potentially erroneous) model-generated prefixes at test time. The authors hypothesize that exposure bias leads to an over-reliance on target history, and show that Minimum Risk Training (MRT), which does not suffer from exposure bias, reduces hallucinations. However, they did not directly measure this overreliance on target history. Our method is able to directly test whether there is indeed an over-reliance on the target history with MLE-trained models, and more robust inclusion of source context with MRT. We also consider a simpler heuristic, word dropout, which we hypothesize to have a similar effect.\nMinimum Risk Training (Shen et al., 2016) is a sentence-level objective that inherently avoids exposure bias. It minimises the expected loss (‘risk’) with respect to the posterior distribution:\nR(θ) = ∑ (x,y) ∑ ỹ∈Y(x) P (ỹ|x, θ)∆(ỹ, y),\nwhere Y(x) is a set of candidate translations for x, ∆(ỹ, y) is the discrepancy between the model prediction ỹ and the gold translation y (e.g., a negative smoothed sentence-level BLEU). More details on the method can be found in Shen et al. (2016) or Edunov et al. (2018); training details for our models are in the appendix.\nWord Dropout is a simple data augmentation technique. During training, it replaces some of the tokens with a special token (e.g., UNK) or a random token (in our experiments, we replace 10% of the tokens with random). When used on the target side, it may serve as the simplest way to alleviate exposure bias: it exposes a model to something other than gold prefixes. This is not true when used on the source side, but for analysis, we consider both variants."
    }, {
      "heading" : "5.1 Experiments",
      "text" : "We consider two types of prefixes: modelgenerated and random. Random prefixes are our main interest here. We feed prefixes that are fluent but unrelated to the source and look whether a model is likely to fall into a language modeling regime, i.e., to what extent it ignores the source. For model-generated prefixes, we do not expect to see large differences in contributions: this mode is ‘easy’ for the model and the source contributions are high (see Section 4.3). The results are shown in Figures 4 and 5.\nModel-generated prefixes. MRT causes more prominent changes in contributions (Figure 4). We\nsee the largest difference in the beginning and the end of the generation process, which may be expected when comparing models trained with tokenlevel and sequence-level objectives. The direction of change, i.e. decreasing influence of source, is rather unexpected; we leave a detailed investigation of this behavior to future work. For word dropout, changes in the amount of contributions are less noticeable; we see, however, that target-side word dropout makes the model more confident in the choice of relevant source tokens (Figure 4b).\nRandom prefixes. We see that, among all models, the MRT model has the highest influence of source (Figure 5a) and the most focused source contributions (Figure 5b). This agrees with our expectations: by construction, MRT removes exposure bias completely. Therefore, it is confused by random prefixes less than other models. Additionally, this also links to Wang and Sennrich (2020) who showed that MRT reduces hallucinations. When using word dropout, both its variants also increase the influence of source, but to a much lesser extent (Figure 5a). As expected, since targetside word dropout slightly reduces exposure bias (in contrast to source-side word dropout), it leads to a larger increase of source influence.\nExperiments in this section highlight that the methodology we propose can be applied to study exposure bias, robustness, and hallucinations, both in machine translation and more broadly for other language generation tasks. In this work, however, we want to illustrate more broadly the potential of this approach. In the following, we will compare models trained with varying amounts of data and will look into the training process."
    }, {
      "heading" : "6 Data Amount",
      "text" : "In this section, we show how the results from Section 4 change when increasing the amount of train-\ning data. The observed patterns are the same when evaluating on datasets with reference translations or the ones generated by the corresponding model (in each case, all sentences in the evaluation dataset have the same length). In the main text, we show figures for references.\nMore data =⇒ higher source contribution. Figure 6(a) shows the source contribution at each generation step. We can see that, generally, models trained with more data rely on source more heavily. Surprisingly, this increase is not spread evenly across positions: at approximately 80% of the target length, models trained with more data use source more, but at the last positions, they switch to more actively using the prefix.\nMore data =⇒ more focused contributions. Figure 6(b) shows that at each generation step, entropy of source contributions decreases with more data. This means that with more training data, the model becomes more confident in the choice of important tokens. In the appendix, we show that this is also the case for target contributions."
    }, {
      "heading" : "7 Training Stages",
      "text" : "Now we turn to analyzing the training process of an NMT model. Specifically, we look at the changes in how the predictions are formed: changes in the amount of source/target contributions and in the entropy of these contributions. Our findings are summarized in Figure 7. In the following, we explain them in more detail. In Section 7.1, we draw connections between our training stages (shown in Figure 7) and the ones found in previous work focused on validating the lottery ticket hypothesis.\nContributions converge early. First, we evaluate how fast the contributions converge, i.e., how quickly a model understands which tokens are the most important for prediction. For\nthis, at each generation step t we evaluate the KL divergence in token influence distributions (Rt(x1), . . . , Rt(xS), Rt(y1), . . . , Rt(yt−1)) from the final converged model to the model in training. Figure 8(a) shows that contributions converge early. After approximately 12k batches, the model is very close to its final state in the choice of tokens to rely on for a prediction.\nChanges in training are not monotonic. Figures 8(b-d) show how the amount of source contribution and the entropy of source and target contributions change in training. We see that all three figures have the same distinct stages (shown with vertical lines). First, source influence decreases, and both source and target contributions become more focused. In this stage, most of the change happens (Figure 8(a)). In the second stage, the model also undergoes substantial change, but all processes change their direction: source influence increases and the model learns to rely on broader context (entropy is increasing). Finally, in the third stage, the direction of changes remains the same, but very little is going on – the model slowly converges.\nThese three stages correspond to the first three stages shown in Figure 7; at this point, the model trained on 1m sentence pairs converges. With more data (e.g., 20m sentence pairs), we further observed the next stage (the last one in Figure 7), where the entropy of both source and target contributions is decreasing again. However, this last stage is much\nslower than the third, and the final state does not differ much from the end of the third stage.\nEarly positions change more. Figures 9(a-b) show how source contributions and their entropy changes for each target position. We see that earlier positions are the ones that change most actively: at these positions, we see the largest decrease at the first stage and the largest following increase at the subsequent stages. If we look at how accuracy for each position changes in training (Figure 10), we see that at the end of the first stage, early tokens have the highest accuracy.7 This is not surprising: one could expect early positions to train faster because they are observed more frequently in training. Previously such intuition motivated the usage of sentence length as one of the criteria for curriculum learning (e.g., Kocmi and Bojar (2017))."
    }, {
      "heading" : "7.1 Relation to Previous Work",
      "text" : "Interestingly, our stages in Figure 7 agree with the ones found by Frankle et al. (2020) for ResNet-20 trained on CIFAR-10 when investigating, among other things, the lottery ticket hypothesis (Frankle and Carbin, 2019). Their stages were defined based on the changes in gradient magnitude, in the weight space, in the performance, and in the effectiveness of rewinding in search of the ‘winning’ subnetwork (for more details on the lottery ticket hypothesis\n7Accuracy is the proportion of cases where the correct token is the most probable choice.\nand the rewinding, see the work by Frankle et al. (2019)). Comparing the stages by Frankle et al. (2020) with ours, we see that (1) their relative sizes in the corresponding timelines match well, (2) the rewinding starts to be effective at the third stage; for our model, this is when the contributions have almost converged. In future work, it would be interesting to further investigate this relation."
    }, {
      "heading" : "8 Additional Related Work",
      "text" : "To estimate the influence of source to an NMT prediction, Ma et al. (2018) trained an NMT model with an auxiliary second decoder where the encoder context vector was masked. Then the source influence was measured as the KL divergence between predictions of the two decoders. However, the ability of an auxiliary decoder to generate similar distribution is not equivalent to the main model\nnot using source. More recently, as a measure of individual token importance, He et al. (2019a) used Integrated Gradients (Sundararajan et al., 2017).\nIn machine translation, LRP was previously used for visualization (Ding et al., 2017) and to find the most important attention heads in the Transformer’s encoder (Voita et al., 2019). Similar to our work, Voita et al. (2019) evaluated LRP on average over a dataset (and not for a single prediction) to extract patterns in model behaviour. Both works used the more popular ε-LRP, while for our analysis, the αβLRP was more suitable (Section 2). For language modeling, Calvillo and Crocker (2018) use LRP to evaluate relevance of neurons in RNNs for a small synthetic setting."
    }, {
      "heading" : "9 Conclusions",
      "text" : "We show how to use LRP to evaluate the relative contributions of source and target to NMT predictions. We illustrate the potential of this approach by analyzing changes in these contributions when conditioning on different types of prefixes (references, model predictions or random translations), when varying training objectives or the amount of training data, and during the training process. Some of our findings are: (1) models trained with more data rely on source information more and have more sharp token contributions; (2) the training process is non-monotonic with several distinct stages. These stages agree with the ones found in previous work focused on validating the lottery ticket hypothesis, which suggests future investigation of this connection. Additionally, we show that models suffering from exposure bias are more prone to over-relying on target history (and hence to hallucinating) than the ones where the exposure bias is mitigated. In future work, our methodology can be used to measure the effects of different and novel training regimes on the balance of source and target contributions."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank the anonymous reviewers for their comments. The work is partially supported by the European Research Council (Titov, ERC StG BroadSem 678254), Dutch NWO (Titov, VIDI 639.022.518) and EU Horizon 2020 (GoURMET, no. 825299). Lena is supported by the Facebook PhD Fellowship. Rico Sennrich acknowledges support of the Swiss National Science Foundation (MUTAMUR; no. 176727)."
    }, {
      "heading" : "A Experimental setup",
      "text" : ""
    }, {
      "heading" : "A.1 Data preprocessing",
      "text" : "We use random subsets of the WMT14 EnFr dataset: http://www.statmt.org/wmt14/ translation-task.html. Sentences were encoded using byte-pair encoding (Sennrich et al., 2016), with source and target vocabularies of about 32000 tokens. Translation pairs were batched together by approximate sequence length. Each training batch contained a set of translation pairs containing approximately 160008 source tokens for 1m subsample and 32000 for larger datasets."
    }, {
      "heading" : "A.2 Model parameters",
      "text" : "We follow the setup of Transformer base model (Vaswani et al., 2017). More precisely, the number of layers in the encoder and in the decoder is N = 6. We employ h = 8 parallel attention layers, or heads. The dimensionality of input and output is dmodel = 512, and the inner-layer of a feedforward networks has dimensionality dff = 2048.\nWe use regularization as described in (Vaswani et al., 2017)."
    }, {
      "heading" : "A.3 Optimizer",
      "text" : "The optimizer we use is the same as in (Vaswani et al., 2017). We use the Adam optimizer (Kingma and Ba, 2015) with β1 = 0.9, β2 = 0.98 and ε = 10−9. We vary the learning rate over the course of training, according to the formula:\nlrate = scale ·min(step_num−0.5, step_num · warmup_steps−1.5)\nWe use warmup_steps = 16000, scale = 4. We train models till convergence and average 5 latest checkpoints. Approximate number of training batches are: 57k for 1m dataset, 220k for 2.5m dataset and 600k for the rest."
    }, {
      "heading" : "B Minimum Risk Training",
      "text" : ""
    }, {
      "heading" : "B.1 Background",
      "text" : "Minimum Risk Training (MRT) minimises the expected loss (‘risk’) with respect to the posterior distribution:\nR(θ) = ∑ (x,y) ∑ ỹ∈Y(x) P (ỹ|x, θ)∆(ỹ, y),\n8This can be reached by using several of GPUs or by accumulating the gradients for several batches and then making an update.\nwhere Y(x) is a set of all possible candidate translations for x, ∆(ỹ, y) is the discrepancy between the model prediction ỹ and the gold translation y.\nSince the search space Y(x) is exponential, in practice it is common to use only a subset of the full space. Formally, instead of Y(x) we use S(x) ∈ Y(x), where S(x) is obtained by sampling several translations. The probabilities P (ỹ|x, θ) are replaced with the P̃ , which is renormalized over the subset S:\nP̃ (ỹ|x, θ, α) = P (ỹ|x, θ) α∑\ny′∈S(x) P (y′|x, θ)α\n.\nThe hyperparameter α is used to control the sharpness of the distribution."
    }, {
      "heading" : "B.2 Experimental setting",
      "text" : "To choose the setting, we mostly relied on previous work (Shen et al., 2016; Edunov et al., 2018). Model is pre-trained with the token-level objective MLE and then fine-tuned with MRT; the fine-tuning stage is approximately one epoch.\nCandidate translations. The translations are sampled using standard random sampling without temperature. Following Shen et al. (2016), we take the large number of candidates; specifically, we use 50 translations and add a reference to the subset. While Edunov et al. (2018) report that adding the reference to the set of candidates hurts quality, in preliminary experiments we found that this was not the case for our setting.\nMeasure of discrepancy. The measure of discrepancy, ∆(ỹ, y), is a negative smoothed sentencelevel BLEU.\nBatch size. On average, the number of examples (where an example is a translation pair along with all candidates) is the same as in training of the baseline models. This is achieved by accumulating gradients for several steps and making an update.\nOther parameters. Following (Wang and Sennrich, 2020), we set α = 0.005 and the learning rate to 0.00001."
    }, {
      "heading" : "C Additional results",
      "text" : ""
    }, {
      "heading" : "C.1 Data Amount",
      "text" : "When varying the amount of data, Figure 11 shows changes in the influence of source tokens at different positions to the whole output, Figure 12 – in the entropy of target contributions."
    }, {
      "heading" : "C.2 Training Stages",
      "text" : "Figure 13 shows how influence of source tokens at different positions to the whole output changes during training.\nD All results for LRP with α = β = 1 2\nHere we present all results from the main text evaluated with α = β = 12 in the redistribution rules of αβ-LRP."
    }, {
      "heading" : "D.1 Getting Acquainted",
      "text" : "Figures 14 and 15."
    }, {
      "heading" : "D.2 Data Amount",
      "text" : "Figures 16 and 17."
    }, {
      "heading" : "D.3 Training stages",
      "text" : "Figures 18, 19 and 20."
    } ],
    "references" : [ {
      "title" : "A causal framework for explaining the predictions of black-box sequence-to-sequence models",
      "author" : [ "David Alvarez-Melis", "Tommi Jaakkola." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 412–",
      "citeRegEx" : "Alvarez.Melis and Jaakkola.,? 2017",
      "shortCiteRegEx" : "Alvarez.Melis and Jaakkola.",
      "year" : 2017
    }, {
      "title" : "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
      "author" : [ "Sebastian Bach", "Alexander Binder", "Grégoire Montavon", "Frederick Klauschen", "Klaus-Robert Müller", "Wojciech Samek." ],
      "venue" : "PloS one, 10(7):e0130140.",
      "citeRegEx" : "Bach et al\\.,? 2015",
      "shortCiteRegEx" : "Bach et al\\.",
      "year" : 2015
    }, {
      "title" : "Identifying and controlling important neurons in neural machine translation",
      "author" : [ "Anthony Bau", "Yonatan Belinkov", "Hassan Sajjad", "Nadir Durrani", "Fahim Dalvi", "James Glass." ],
      "venue" : "International Conference on Learning Representations, New Orleans.",
      "citeRegEx" : "Bau et al\\.,? 2019",
      "shortCiteRegEx" : "Bau et al\\.",
      "year" : 2019
    }, {
      "title" : "What do neural machine translation models learn about morphology? In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol",
      "author" : [ "Yonatan Belinkov", "Nadir Durrani", "Fahim Dalvi", "Hassan Sajjad", "James Glass" ],
      "venue" : null,
      "citeRegEx" : "Belinkov et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Belinkov et al\\.",
      "year" : 2017
    }, {
      "title" : "Naver labs Europe’s systems for the WMT19 machine translation robustness task",
      "author" : [ "Alexandre Berard", "Ioan Calapodescu", "Claude Roux." ],
      "venue" : "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day",
      "citeRegEx" : "Berard et al\\.,? 2019",
      "shortCiteRegEx" : "Berard et al\\.",
      "year" : 2019
    }, {
      "title" : "Layer-wise relevance propagation for neural networks with local renormalization layers",
      "author" : [ "Alexander Binder", "Grégoire Montavon", "Sebastian Lapuschkin", "Klaus-Robert Müller", "Wojciech Samek." ],
      "venue" : "Lecture Notes in Computer Science, page 63–71.",
      "citeRegEx" : "Binder et al\\.,? 2016",
      "shortCiteRegEx" : "Binder et al\\.",
      "year" : 2016
    }, {
      "title" : "Evaluating the morphological competence of machine translation systems",
      "author" : [ "Franck Burlot", "François Yvon." ],
      "venue" : "Proceedings of the Second Conference on Machine Translation, pages 43–55, Copenhagen, Denmark. Association for Computational",
      "citeRegEx" : "Burlot and Yvon.,? 2017",
      "shortCiteRegEx" : "Burlot and Yvon.",
      "year" : 2017
    }, {
      "title" : "Using monolingual data in neural machine translation: a systematic study",
      "author" : [ "Franck Burlot", "François Yvon." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 144–155, Brussels, Belgium. Association for Com-",
      "citeRegEx" : "Burlot and Yvon.,? 2018",
      "shortCiteRegEx" : "Burlot and Yvon.",
      "year" : 2018
    }, {
      "title" : "Language production dynamics with recurrent neural networks",
      "author" : [ "Jesús Calvillo", "Matthew Crocker." ],
      "venue" : "Proceedings of the Eight Workshop on Cognitive Aspects of Computational Language Learning and Processing, pages 17–26, Melbourne. Association",
      "citeRegEx" : "Calvillo and Crocker.,? 2018",
      "shortCiteRegEx" : "Calvillo and Crocker.",
      "year" : 2018
    }, {
      "title" : "What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties",
      "author" : [ "Alexis Conneau", "German Kruszewski", "Guillaume Lample", "Loïc Barrault", "Marco Baroni." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the As-",
      "citeRegEx" : "Conneau et al\\.,? 2018",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "What is one grain of sand in the desert? analyzing individual neurons in deep nlp models",
      "author" : [ "Fahim Dalvi", "Nadir Durrani", "Hassan Sajjad", "Yonatan Belinkov", "Anthony Bau", "James Glass." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intel-",
      "citeRegEx" : "Dalvi et al\\.,? 2019",
      "shortCiteRegEx" : "Dalvi et al\\.",
      "year" : 2019
    }, {
      "title" : "Visualizing and understanding neural machine translation",
      "author" : [ "Yanzhuo Ding", "Yang Liu", "Huanbo Luan", "Maosong Sun." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1150–",
      "citeRegEx" : "Ding et al\\.,? 2017",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2017
    }, {
      "title" : "Classical structured prediction losses for sequence to sequence learning",
      "author" : [ "Sergey Edunov", "Myle Ott", "Michael Auli", "David Grangier", "Marc’Aurelio Ranzato" ],
      "venue" : "In Proceedings of the 2018 Conference of the North American Chapter of the Asso-",
      "citeRegEx" : "Edunov et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Edunov et al\\.",
      "year" : 2018
    }, {
      "title" : "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
      "author" : [ "Jonathan Frankle", "Michael Carbin." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Frankle and Carbin.,? 2019",
      "shortCiteRegEx" : "Frankle and Carbin.",
      "year" : 2019
    }, {
      "title" : "Stabilizing the lottery ticket hypothesis",
      "author" : [ "Jonathan Frankle", "Gintare Karolina Dziugaite", "Daniel M. Roy", "Michael Carbin" ],
      "venue" : null,
      "citeRegEx" : "Frankle et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Frankle et al\\.",
      "year" : 2019
    }, {
      "title" : "The early phase of neural network training",
      "author" : [ "Jonathan Frankle", "David J. Schwab", "Ari S. Morcos." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Frankle et al\\.,? 2020",
      "shortCiteRegEx" : "Frankle et al\\.",
      "year" : 2020
    }, {
      "title" : "What does attention in neural machine translation pay attention to",
      "author" : [ "Hamidreza Ghader", "Christof Monz" ],
      "venue" : "In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
      "citeRegEx" : "Ghader and Monz.,? \\Q2017\\E",
      "shortCiteRegEx" : "Ghader and Monz.",
      "year" : 2017
    }, {
      "title" : "Towards understanding neural machine translation with word importance",
      "author" : [ "Shilin He", "Zhaopeng Tu", "Xing Wang", "Longyue Wang", "Michael Lyu", "Shuming Shi." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "He et al\\.,? 2019a",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2019
    }, {
      "title" : "Quantifying exposure bias for neural language generation",
      "author" : [ "Tianxing He", "Jingzhao Zhang", "Zhiming Zhou", "James Glass" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "Proceedings of the International Conference on Learning Representation (ICLR 2015).",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Curriculum learning and minibatch bucketing in neural machine translation",
      "author" : [ "Tom Kocmi", "Ondřej Bojar." ],
      "venue" : "Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017, pages 379–386, Varna, Bulgaria.",
      "citeRegEx" : "Kocmi and Bojar.,? 2017",
      "shortCiteRegEx" : "Kocmi and Bojar.",
      "year" : 2017
    }, {
      "title" : "Six challenges for neural machine translation",
      "author" : [ "Philipp Koehn", "Rebecca Knowles." ],
      "venue" : "Proceedings of the First Workshop on Neural Machine Translation, pages 28–39, Vancouver. Association for Computational Linguistics.",
      "citeRegEx" : "Koehn and Knowles.,? 2017",
      "shortCiteRegEx" : "Koehn and Knowles.",
      "year" : 2017
    }, {
      "title" : "Hallucinations in neural machine translation",
      "author" : [ "Katherine Lee", "Orhan Firat", "Ashish Agarwal", "Clara Fannjiang", "David Sussillo" ],
      "venue" : null,
      "citeRegEx" : "Lee et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2018
    }, {
      "title" : "Regularized context gates on transformer for machine translation",
      "author" : [ "Xintong Li", "Lemao Liu", "Rui Wang", "Guoping Huang", "Max Meng." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Compu-",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "An analysis of source context dependency in neural machine translation",
      "author" : [ "Xutai Ma", "Ke Li", "Philipp Koehn" ],
      "venue" : null,
      "citeRegEx" : "Ma et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2018
    }, {
      "title" : "Layer-wise relevance propagation: an overview",
      "author" : [ "Grégoire Montavon", "Alexander Binder", "Sebastian Lapuschkin", "Wojciech Samek", "Klaus-Robert Müller." ],
      "venue" : "Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, pages 193–209.",
      "citeRegEx" : "Montavon et al\\.,? 2019",
      "shortCiteRegEx" : "Montavon et al\\.",
      "year" : 2019
    }, {
      "title" : "Analyzing uncertainty in neural machine translation",
      "author" : [ "Myle Ott", "Michael Auli", "David Grangier", "Marc’Aurelio Ranzato" ],
      "venue" : "In Proceedings of the 35th International Conference on Machine Learning,",
      "citeRegEx" : "Ott et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2018
    }, {
      "title" : "An analysis of encoder representations in transformerbased machine translation",
      "author" : [ "Alessandro Raganato", "Jörg Tiedemann." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages",
      "citeRegEx" : "Raganato and Tiedemann.,? 2018",
      "shortCiteRegEx" : "Raganato and Tiedemann.",
      "year" : 2018
    }, {
      "title" : "Sequence level training with recurrent neural networks",
      "author" : [ "Marc’Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba" ],
      "venue" : null,
      "citeRegEx" : "Ranzato et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ranzato et al\\.",
      "year" : 2016
    }, {
      "title" : "Improving word sense disambiguation in neural machine translation with sense embeddings",
      "author" : [ "Annette Rios Gonzales", "Laura Mascarell", "Rico Sennrich." ],
      "venue" : "Proceedings of the Second Conference on Machine Translation, pages 11–19, Copenhagen,",
      "citeRegEx" : "Gonzales et al\\.,? 2017",
      "shortCiteRegEx" : "Gonzales et al\\.",
      "year" : 2017
    }, {
      "title" : "How grammatical is characterlevel neural machine translation? assessing MT quality with contrastive translation pairs",
      "author" : [ "Rico Sennrich." ],
      "venue" : "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Sennrich.,? 2017",
      "shortCiteRegEx" : "Sennrich.",
      "year" : 2017
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Minimum risk training for neural machine translation",
      "author" : [ "Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
      "citeRegEx" : "Shen et al\\.,? 2016",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2016
    }, {
      "title" : "Axiomatic attribution for deep networks",
      "author" : [ "Mukund Sundararajan", "Ankur Taly", "Qiqi Yan." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 3319–3328, In-",
      "citeRegEx" : "Sundararajan et al\\.,? 2017",
      "shortCiteRegEx" : "Sundararajan et al\\.",
      "year" : 2017
    }, {
      "title" : "A lightweight evaluation framework for machine translation reordering",
      "author" : [ "David Talbot", "Hideto Kazawa", "Hiroshi Ichikawa", "Jason Katz-Brown", "Masakazu Seno", "Franz Och." ],
      "venue" : "Proceedings of the Sixth Workshop on Statistical Machine Translation,",
      "citeRegEx" : "Talbot et al\\.,? 2011",
      "shortCiteRegEx" : "Talbot et al\\.",
      "year" : 2011
    }, {
      "title" : "Why self-attention? a targeted evaluation of neural machine translation architectures",
      "author" : [ "Gongbo Tang", "Mathias Müller", "Annette Rios", "Rico Sennrich." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Tang et al\\.,? 2018",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2018
    }, {
      "title" : "An analysis of attention mechanisms: The case of word sense disambiguation in neural machine translation",
      "author" : [ "Gongbo Tang", "Rico Sennrich", "Joakim Nivre." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 26–",
      "citeRegEx" : "Tang et al\\.,? 2018",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2018
    }, {
      "title" : "Context gates for neural machine translation",
      "author" : [ "Zhaopeng Tu", "Yang Liu", "Zhengdong Lu", "Xiaohua Liu", "Hang Li." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:87–99.",
      "citeRegEx" : "Tu et al\\.,? 2017",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2017
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30, pages 5998–6008, Los Angeles.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Context-aware neural machine translation learns anaphora resolution",
      "author" : [ "Elena Voita", "Pavel Serdyukov", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Voita et al\\.,? 2018",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2018
    }, {
      "title" : "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned",
      "author" : [ "Elena Voita", "David Talbot", "Fedor Moiseev", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Voita et al\\.,? 2019",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    }, {
      "title" : "On exposure bias, hallucination and domain shift in neural machine translation",
      "author" : [ "Chaojun Wang", "Rico Sennrich." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3544–3552, Online. Association for",
      "citeRegEx" : "Wang and Sennrich.,? 2020",
      "shortCiteRegEx" : "Wang and Sennrich.",
      "year" : 2020
    }, {
      "title" : "Neural machine translation with decoding history enhanced attention",
      "author" : [ "Mingxuan Wang", "Jun Xie", "Zhixing Tan", "Jinsong Su", "Deyi Xiong", "Chao Bian." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 1464–",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Understanding knowledge distillation in nonautoregressive machine translation",
      "author" : [ "Chunting Zhou", "Graham Neubig", "Jiatao Gu." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    }, {
      "title" : "2016), we take the large number of candidates; specifically, we use 50 translations and add a reference to the subset",
      "author" : [ "temperature. Following Shen" ],
      "venue" : "While Edunov et al",
      "citeRegEx" : "Shen,? \\Q2018\\E",
      "shortCiteRegEx" : "Shen",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "In NMT, approaches to analysis include probing for linguistic structure (Belinkov et al., 2017; Conneau et al., 2018), evaluating via contrastive translation pairs (Sennrich, 2017; Burlot and Yvon, 2017; Rios Gonzales et al.",
      "startOffset" : 72,
      "endOffset" : 117
    }, {
      "referenceID" : 9,
      "context" : "In NMT, approaches to analysis include probing for linguistic structure (Belinkov et al., 2017; Conneau et al., 2018), evaluating via contrastive translation pairs (Sennrich, 2017; Burlot and Yvon, 2017; Rios Gonzales et al.",
      "startOffset" : 72,
      "endOffset" : 117
    }, {
      "referenceID" : 16,
      "context" : ", 2018), inspecting model components, such as attention (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019) or neurons (Dalvi et al.",
      "startOffset" : 56,
      "endOffset" : 168
    }, {
      "referenceID" : 39,
      "context" : ", 2018), inspecting model components, such as attention (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019) or neurons (Dalvi et al.",
      "startOffset" : 56,
      "endOffset" : 168
    }, {
      "referenceID" : 35,
      "context" : ", 2018), inspecting model components, such as attention (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019) or neurons (Dalvi et al.",
      "startOffset" : 56,
      "endOffset" : 168
    }, {
      "referenceID" : 27,
      "context" : ", 2018), inspecting model components, such as attention (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019) or neurons (Dalvi et al.",
      "startOffset" : 56,
      "endOffset" : 168
    }, {
      "referenceID" : 40,
      "context" : ", 2018), inspecting model components, such as attention (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019) or neurons (Dalvi et al.",
      "startOffset" : 56,
      "endOffset" : 168
    }, {
      "referenceID" : 37,
      "context" : "For example, adding context gates to dynamically control the influence of source and target leads to improvement for both RNN (Tu et al., 2017; Wang et al., 2018) and Transfomer (Li et al.",
      "startOffset" : 126,
      "endOffset" : 162
    }, {
      "referenceID" : 42,
      "context" : "For example, adding context gates to dynamically control the influence of source and target leads to improvement for both RNN (Tu et al., 2017; Wang et al., 2018) and Transfomer (Li et al.",
      "startOffset" : 126,
      "endOffset" : 162
    }, {
      "referenceID" : 22,
      "context" : "Several works observed that, when hallucinating, a model fails to properly use source: it produces a deficient attention matrix, where almost all the probability mass is concentrated on uninformative source tokens (EOS and punctuation) (Lee et al., 2018; Berard et al., 2019).",
      "startOffset" : 236,
      "endOffset" : 275
    }, {
      "referenceID" : 4,
      "context" : "Several works observed that, when hallucinating, a model fails to properly use source: it produces a deficient attention matrix, where almost all the probability mass is concentrated on uninformative source tokens (EOS and punctuation) (Lee et al., 2018; Berard et al., 2019).",
      "startOffset" : 236,
      "endOffset" : 275
    }, {
      "referenceID" : 1,
      "context" : "We argue that a natural way to estimate how the source and target contexts contribute to generation is to apply Layerwise Relevance Propagation (LRP) (Bach et al., 2015) to NMT models.",
      "startOffset" : 150,
      "endOffset" : 169
    }, {
      "referenceID" : 1,
      "context" : "The original LRP version was developed for computer vision models (Bach et al., 2015) and is not directly applicable to the Transformer (e.",
      "startOffset" : 66,
      "endOffset" : 85
    }, {
      "referenceID" : 11,
      "context" : "Previous work applying one of the LRP variants to NMT (Ding et al., 2017; Voita et al., 2019) do not describe extensions beyond the original LRP rules (Bach et al.",
      "startOffset" : 54,
      "endOffset" : 93
    }, {
      "referenceID" : 40,
      "context" : "Previous work applying one of the LRP variants to NMT (Ding et al., 2017; Voita et al., 2019) do not describe extensions beyond the original LRP rules (Bach et al.",
      "startOffset" : 54,
      "endOffset" : 93
    }, {
      "referenceID" : 1,
      "context" : ", 2019) do not describe extensions beyond the original LRP rules (Bach et al., 2015).",
      "startOffset" : 65,
      "endOffset" : 84
    }, {
      "referenceID" : 1,
      "context" : "Several versions of LRP satisfying equation (4) (and, therefore, the conservation principle) have been introduced: LRP-ε, LRP-αβ and LRPγ (Bach et al., 2015; Binder et al., 2016; Montavon et al., 2019).",
      "startOffset" : 138,
      "endOffset" : 201
    }, {
      "referenceID" : 5,
      "context" : "Several versions of LRP satisfying equation (4) (and, therefore, the conservation principle) have been introduced: LRP-ε, LRP-αβ and LRPγ (Bach et al., 2015; Binder et al., 2016; Montavon et al., 2019).",
      "startOffset" : 138,
      "endOffset" : 201
    }, {
      "referenceID" : 25,
      "context" : "Several versions of LRP satisfying equation (4) (and, therefore, the conservation principle) have been introduced: LRP-ε, LRP-αβ and LRPγ (Bach et al., 2015; Binder et al., 2016; Montavon et al., 2019).",
      "startOffset" : 138,
      "endOffset" : 201
    }, {
      "referenceID" : 1,
      "context" : "We use LRP-αβ (Bach et al., 2015; Binder et al., 2016), which defines relevances at each step in such a way that they are positive.",
      "startOffset" : 14,
      "endOffset" : 54
    }, {
      "referenceID" : 5,
      "context" : "We use LRP-αβ (Bach et al., 2015; Binder et al., 2016), which defines relevances at each step in such a way that they are positive.",
      "startOffset" : 14,
      "endOffset" : 54
    }, {
      "referenceID" : 1,
      "context" : "Then the αβ-rule (Bach et al., 2015; Binder et al., 2016) is given by the equation",
      "startOffset" : 17,
      "endOffset" : 57
    }, {
      "referenceID" : 5,
      "context" : "Then the αβ-rule (Bach et al., 2015; Binder et al., 2016) is given by the equation",
      "startOffset" : 17,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "For the structures that do not fit the form (6), the weighting vij can be obtained by performing a first order Taylor expansion of a neuron x j (Bach et al., 2015; Binder et al., 2016).",
      "startOffset" : 144,
      "endOffset" : 184
    }, {
      "referenceID" : 5,
      "context" : "For the structures that do not fit the form (6), the weighting vij can be obtained by performing a first order Taylor expansion of a neuron x j (Bach et al., 2015; Binder et al., 2016).",
      "startOffset" : 144,
      "endOffset" : 184
    }, {
      "referenceID" : 38,
      "context" : "We follow the setup of Transformer base model (Vaswani et al., 2017) with the standard training setting.",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 7,
      "context" : "For example, beam search translations contain fewer rare tokens (Burlot and Yvon, 2018; Ott et al., 2018), are simpler syntactically (Burlot and Yvon, 2018) and, according to the fuzzy reordering score (Talbot et al.",
      "startOffset" : 64,
      "endOffset" : 105
    }, {
      "referenceID" : 26,
      "context" : "For example, beam search translations contain fewer rare tokens (Burlot and Yvon, 2018; Ott et al., 2018), are simpler syntactically (Burlot and Yvon, 2018) and, according to the fuzzy reordering score (Talbot et al.",
      "startOffset" : 64,
      "endOffset" : 105
    }, {
      "referenceID" : 7,
      "context" : ", 2018), are simpler syntactically (Burlot and Yvon, 2018) and, according to the fuzzy reordering score (Talbot et al.",
      "startOffset" : 35,
      "endOffset" : 58
    }, {
      "referenceID" : 34,
      "context" : ", 2018), are simpler syntactically (Burlot and Yvon, 2018) and, according to the fuzzy reordering score (Talbot et al., 2011), model translations have significantly less reordering compared to the real parallel sentences (Zhou et al.",
      "startOffset" : 104,
      "endOffset" : 125
    }, {
      "referenceID" : 43,
      "context" : ", 2011), model translations have significantly less reordering compared to the real parallel sentences (Zhou et al., 2020).",
      "startOffset" : 103,
      "endOffset" : 122
    }, {
      "referenceID" : 21,
      "context" : "Previous work has found that models can fall into a hallucination mode where “the decoder ignores context from the encoder and samples from its language mode” (Koehn and Knowles, 2017; Lee et al., 2018).",
      "startOffset" : 159,
      "endOffset" : 202
    }, {
      "referenceID" : 22,
      "context" : "Previous work has found that models can fall into a hallucination mode where “the decoder ignores context from the encoder and samples from its language mode” (Koehn and Knowles, 2017; Lee et al., 2018).",
      "startOffset" : 159,
      "endOffset" : 202
    }, {
      "referenceID" : 28,
      "context" : "Wang and Sennrich (2020) empirically link the hallucination mode to exposure bias (Ranzato et al., 2016), i.",
      "startOffset" : 82,
      "endOffset" : 104
    }, {
      "referenceID" : 32,
      "context" : "Minimum Risk Training (Shen et al., 2016) is a sentence-level objective that inherently avoids exposure bias.",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 13,
      "context" : "(2020) for ResNet-20 trained on CIFAR-10 when investigating, among other things, the lottery ticket hypothesis (Frankle and Carbin, 2019).",
      "startOffset" : 111,
      "endOffset" : 137
    }, {
      "referenceID" : 33,
      "context" : "(2019a) used Integrated Gradients (Sundararajan et al., 2017).",
      "startOffset" : 34,
      "endOffset" : 61
    }, {
      "referenceID" : 11,
      "context" : "In machine translation, LRP was previously used for visualization (Ding et al., 2017) and to find the most important attention heads in the Transformer’s encoder (Voita et al.",
      "startOffset" : 66,
      "endOffset" : 85
    }, {
      "referenceID" : 40,
      "context" : ", 2017) and to find the most important attention heads in the Transformer’s encoder (Voita et al., 2019).",
      "startOffset" : 84,
      "endOffset" : 104
    } ],
    "year" : 2021,
    "abstractText" : "In Neural Machine Translation (and, more generally, conditional language modeling), the generation of a target token is influenced by two types of context: the source and the prefix of the target sequence. While many attempts to understand the internal workings of NMT models have been made, none of them explicitly evaluates relative source and target contributions to a generation decision. We argue that this relative contribution can be evaluated by adopting a variant of Layerwise Relevance Propagation (LRP). Its underlying ‘conservation principle’ makes relevance propagation unique: differently from other methods, it evaluates not an abstract quantity reflecting token importance, but the proportion of each token’s influence. We extend LRP to the Transformer and conduct an analysis of NMT models which explicitly evaluates the source and target relative contributions to the generation process. We analyze changes in these contributions when conditioning on different types of prefixes, when varying the training objective or the amount of training data, and during the training process. We find that models trained with more data tend to rely on source information more and to have more sharp token contributions; the training process is non-monotonic with several stages of different nature.1",
    "creator" : "LaTeX with hyperref"
  }
}