{
  "name" : "2021.acl-long.358.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Recursive Tree-Structured Self-Attention for Answer Sentence Selection",
    "authors" : [ "Khalil Mrini", "Emilia Farcas", "Ndapa Nakashole" ],
    "emails" : [ "nnakashole}@ucsd.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4651–4661\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4651"
    }, {
      "heading" : "1 Introduction",
      "text" : "Motivation. Natural language text is characterized by structure. For instance, syntactic parse trees decompose a sentence into syntactic groups, which in turn are decomposed recursively until we get to single-word spans. Therefore, syntactic parse trees have a varying number of levels that can be accurately represented by recursive model architectures.\nTree-structured LSTM networks (Tai et al., 2015) are the recursive extension of LSTM networks (Hochreiter and Schmidhuber, 1997), and allow for syntactic trees to be represented hierarchically. Tree-LSTMs and bidirectional Tree-LSTMs\n(Teng and Zhang, 2017) do not represent sequence position information, whereas the hybrid neural inference networks (Chen et al., 2017a) represent sequence position information separately from treestructured hierarchical information.\nTree-structured models have been applied to the tasks of natural language inference (Chen et al., 2017a), sentence pair similarity (Tai et al., 2015), dependency parsing (Kiperwasser and Goldberg, 2016), and text embeddings (Mrini et al., 2019). In this paper, we consider the problem of Answer Sentence Selection (AS2), where the goal is to predict for a question-sentence pair whether the sentence contains an answer to the question. Given that treestructured models have performed strongly on a task that takes a sentence pair as input – sentence pair similarity, we hypothesize that tree structures can help in AS2, another sentence pair task.\nThe most recent top-performing model architectures for Answer Sentence Selection have been based on the self-attention transformer architecture (Vaswani et al., 2017). Three of them (Lai et al., 2019; Garg et al., 2019; Tran et al., 2020)\nuse transfer learning on large AS2 datasets; another one (Laskar et al., 2020) uses direct fine-tuning on pre-trained transformer-based language encoders, whereas all three use pre-trained BERT (Devlin et al., 2019) and/or RoBERTa embeddings (Liu et al., 2019).\nContribution. We investigate whether tree structures are useful for AS2. We introduce the Tree Aggregation Transformer: a novel recursive and treestructured self-attention model for Answer Sentence Selection. We use the syntactic parse trees of questions and candidate answer sentences to model them in a tree-structured way. We then form representations for questions and candidate answers using one additional self-attention layer in a recursive, bottom-up fashion, as shown in Figure 1. We learn syntactic embeddings to represent hierarchical order and phrase-level syntactic information. We find in an ablation study that our learned syntactic embeddings improve performance.\nWithout using AS2 datasets for transfer learning, our model establishes a new state of the art for the clean versions of TrecQA and WikiQA, two widely used benchmark datasets in question answering and AS2. Our tree-structured self-attention matches or exceeds the state of the art – which is fine-tuning on RoBERTa – on 2 out of 4 Community Question Answering (CQA) datasets. We conduct experiments for 3 probing tasks to establish what information our models leverage to increase performance, and likewise what they fail to leverage when they do not exceed baselines. We find that tree-structured representations that successfully absorb the provided syntactic information consistently perform better than baselines. Our probing task results suggest that there is more work to be done for tree structures to adapt to noisy user-generated text."
    }, {
      "heading" : "2 Related Work",
      "text" : "Tree-structured Transformers. To the best of our knowledge, our method is the first to introduce tree self-attention to Answer Sentence Selection. There is a growing body of work incorporating tree structures in self-attention for a range of other NLP tasks.\nNguyen et al. (2019) introduce a transformerbased encoder-decoder that incorporates treestructured attention. The tree-structured attention is accumulated hierarchically. A token in the tree has as many representations as overall children, therefore it is first accumulated in a bottom-up fashion\n(vertically), and then horizontally to compute a token’s representation. Their model is not recursive and uses different parameters for each level. The authors evaluate their model in machine translation and text classification.\nSun et al. (2020) develop a tree-structured transformer encoder-decoder architecture for code generation. Here, the tree structure is based on the code syntax. The model uses character-level embeddings as input.\nHarer et al. (2019) introduce Tree-Transformer: a model with a tree convolution block for correction of code and grammar. Wang et al. (2019) propose a model of the same name, where the model learns syntactic parse trees in an unsupervised manner. The model uses up to 12 layers of non-recursive self-attention on top of a pre-trained BERT.\nAhmed et al. (2019) introduce Constituency and Dependency Tree Transformer models, largely inspired by the Constituency and Dependency TreeLSTM models (Tai et al., 2015) and RvNN models (Socher et al., 2011, 2012, 2013). On 4 datasets of semantic relatedness, natural language inference and paraphrase identification, their transformer models achieve performance on par with TreeLSTM models, and do not set a new state of the art. The authors use two convolution layers to form a parent representation from the corresponding children. Their model does not learn an explicit syntactic representation, and the authors do not analyze the fluctuating results.\nAnswer Sentence Selection (AS2). The recent state-of-the-art models in the AS2 task all use transfer learning from large-scale datasets, and do not incorporate syntactic information. All of them use a standard linear (or sequential) input format, where the first input sentence is the question and the second is the candidate answer.\nLai et al. (2019) introduce the Gated SelfAttention Memory Network (GSAMN). It combines gated attention (Dhingra et al., 2017; Tran et al., 2017), memory networks (Sukhbaatar et al., 2015) and self-attention (Vaswani et al., 2017) in one model. The authors use transfer learning with their Stack Exchange QA dataset.\nGarg et al. (2019) propose the TandA method: Transfer and Adapt. The method is simply finetuning directly on a pre-trained BERT or RoBERTa model. The transfer step is transfer learning: finetuning a large pre-trained BERT or RoBERTa on the ASNQ dataset: a large-scale answer sentence\nselection dataset extracted from Google’s Natural Questions (Kwiatkowski et al., 2019). The second step is to adapt the language model fine-tuned for answer sentence selection to the smaller, target benchmarks TrecQA and WikiQA.\nTran et al. (2020) build upon the work of Lai et al. (2019). They propose to use a neural Turing machine (Graves et al., 2014) as a controller for the memory network, instead of the gated attention that Lai et al. (2019) use. Like Garg et al. (2019), they use the ASNQ dataset for transfer learning.\nLaskar et al. (2020) achieve state-of-the-art results on a wide range of QA and CQA datasets by directly fine-tuning on the target datasets, without transfer learning from an external large-scale dataset. They show results for two methods: the first trains a self-attention layer while freezing pretrained language model layers, and the second directly fine-tunes on the language model."
    }, {
      "heading" : "3 Tree Aggregation Transformer for Answer Sentence Selection",
      "text" : "In the AS2 task, the input is a pair of sentences, where the first one is the question and the second is a candidate answer. This is a binary classification problem on whether or not the candidate answer sentence contains an answer to the question. We therefore design our model to form a representation of the question and a representation of the candidate answer, in a bottom-up tree aggregation fashion.\nSemantic and Syntactic Representation. We define a token embedding in our input representation as the concatenation of a semantic embedding and a syntactic embedding. The semantic embedding is a projection of the token embedding from a given pre-trained language model, whereas the syntactic embedding contains information from partof-speech tags, syntactic categories, and the level within the syntactic parse tree.\nThe syntactic embedding is the sum of three learned embeddings. The first embedding represents the token’s tag – a part-of-speech tag if the token is a word, or a syntactic category if the token is a classification or separator token. The second embedding represents the token’s level within the tree, inherited from the head of the token’s constituent span. Our recursive model allows to represent sentences with as many tree levels as the corresponding syntax tree has. The third embedding represents the position of a token within the\nconstituent span, as seen in the example in Figure 2. This position embedding puts the token within its span context, whereas the position embedding of the semantic (language model) embedding puts the token within the context of the question-sentence pair.\nMore formally, given a token t, its language model embedding xt, its position index pt, its partof-speech tag or syntactic category st, and its tree level lt, the token’s semantic embedding et and syntactic embedding nt are as follows:\net = W1 ∗ xt + b1 (1)\nnt = W2 [ Es [st] +E p [pt] +E l [lt] ] + b2 (2)\nwhere W1, W2, b1, b2 are learned, and Es, Ep and El are learned embedding layers, respectively for the part-of-speech tag or syntactic category, the position index, and the tree level.\nRecursive Self-Attention. We add 1 layer of recursive self-attention layer on top of the language model layers. The recursive self-attention layer has separate attention distributions aet and a n t for the semantic embedding et and syntactic embedding nt:\naet = softmax ( qet ∗Ke√\nde\n) (3)\nant = softmax ( qnt ∗Kn√\ndn\n) (4)\nwhere dn and de are the dimensions of the query and key vectors for the semantic and syntactic embeddings respectively, and Ke and Kn are the learned matrices of key vectors of input tokens. qet and q n t are the query vectors for the token t, such that:\nqet = W Q,e ∗ et (5)\nqnt = W Q,n ∗ nt (6)\nwhere WQ,e and WQ,n are learned. The resulting vectors oet and o n t are computed as:\noet = et +W O,e ∗ (aet ∗Ve) + bO,e (7)\nont = nt +W O,n ∗ (ant ∗Vn) + bO,n (8)\nwhere Ve and Vn are the value vectors for the input tokens, and WO,e, WO,n, bO,e, bO,n are learned. Finally, we apply separate position-wise feed-forward layers to these output vectors.\nUsually, self-attention includes residual dropout over the attention-weighted value vectors. We found in preliminary experiments that the performance on the dev set improved when we omitted dropout regularization. We omit dropout in both self-attention and position-wise feed-forward layer.\nThe recursiveness of the self-attention allows the model to re-use the same sets of parameters across each tree level, instead of training new ones as in previous work (Nguyen et al., 2019; Wang et al., 2019).\nConstituent Span Embedding. Each input sentence is represented in a tree-structured fashion using its constituency parse tree. We use a pre-trained parser, whose parameters are fixed, to produce the trees before training time.\nThe constituent span is fed to the recursive selfattention as a matrix of token vectors. This matrix includes the embeddings of the words of the constituent span, preceded by a first, start-of-sentence embedding, and followed by an end-of-sentence embedding. The start-of-sentence token is the classification token if the span is part of the question, or a separator token if the span is part of the candidate sentence. Figure 3 shows how we compose a constituent span embedding for RoBERTa models.\nThe constituent span embedding is the output embedding of the first token. The first token embedding obtains through the recursive self-attention\nan attention-weighted sum of all of the span’s token embeddings. This creates a span-specific embedding, conscious of the entire question-sentence pair input as a result of the language model layers, but focused on the tokens of a span as a result of the recursive self-attention.\nIn using only one layer of recursive selfattention, the first token embedding gets an attention-weighted sum of value vectors that contains token embeddings that did not go through a layer of self-attention, and syntactic embeddings that came directly out of the embedding layers.\nEfficient Tree Aggregation. To obtain an aggregate sentence embedding, we proceed by embedding from the deepest level of the tree (the leaves) to the root, as shown in Figure 3. The computations are done on the same two sets of self-attention parameters.\nTo reduce training time, we compute the constituent span embeddings one level at a time. For instance, in Figure 2, we compute the NP, VP and PP groups at once when computing the span embeddings at tree level 2.\nWe efficiently compute all span embeddings only once, and keep all computed span embeddings, as they will be used in the next level.\nThe sentence embedding is obtained from the first token output of the computation at the root of the tree, as shown in Figure 1.\nPrediction. Finally, we concatenate the aggregate embeddings for the question-sentence input pair. Given the question’s aggregate semantic embedding weq and aggregate syntactic embedding\nwnq , and the sentence’s aggregate semantic embedding wes and aggregate syntactic embedding w n s , we obtain the prediction values as follows:\np(s|q) = softmax ( W ∗ tanh [ weq;w n q ;w e s ;w n s ] + b ) (9) where W and b are learned. We use binary crossentropy as our loss function.\nOur model can optionally include a residual connection, by adding the classification token embedding output of the language model to the beginning of the question-sentence pair vector. This residual connection does not contain syntactic information, and the classification token embedding is not projected in this case."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We evaluate our proposed Tree Aggregation Transformer on six English-language benchmark datasets for answer sentence selection. The first two – TrecQA and WikiQA – are widely used benchmarks in Question Answering (QA). The other four – YahooCQA and SemEval 2015, 2016 and 2017 – are all from the Community Question Answering (CQA) domain. We show the statistics of these six datasets in Table 1.\nTrecQA (Wang et al., 2007) is collected from labeled sentences of the QA track of the Text REtrieval Conference (TREC). Over time, the dataset has evolved into two versions: the raw version includes all question-sentence pairs, whereas the clean version excludes questions with only nonrelevant or only relevant candidate answers.\nWikiQA (Yang et al., 2015) contains questions originally sampled from Bing query logs, and matched with candidate answer sentences from the first paragraph of relevant Wikipedia articles. Likewise, it also has a raw and a clean version. Following Lai et al. (2019); Tran et al. (2020), we evaluate our method on the clean versions of TrecQA and WikiQA.\nYahooCQA (Tay et al., 2017) is a filtered and pre-processed subset of the large-scale Yahoo! Answers Manner Questions dataset (Surdeanu et al., 2008). The latter is based on the Yahoo! Answers online forum.\nSemEval 2015 CQA (Nakov et al., 2015) is the challenge dataset of Subtask A of Task 3 of SemEval 2015. It is based on the Qatar Living online forum, and the goal is to predict the relevance\nscores of candidate answers given a question. The original subtask divides labels into three categories: definitely relevant, potentially useful, and irrelevant. Following previous work (Sha et al., 2018; Laskar et al., 2020), only definitely relevant candidate answers are marked as relevant in our binary classification setting.\nSemEval 2016 CQA (Nakov et al., 2016) corresponds as well to Subtask A of Task 3 of SemEval 2016, about question-comment similarity. It is a new dataset also based on the Qatar Living online forum. The training set includes the training, development and testing sets of the SemEval 2015 CQA, and two new training sets. The authors of the dataset have described the first one as highly reliable, and the second one as noisier.\nSemEval 2017 CQA (Nakov et al., 2017) is the latest version of the community question answering task. The training and development sets are the same as the 2016 version, but the testing set is different.\nIn Figure 2, we show an example of questionsentence pairs for a QA dataset and a CQA dataset. The aim is to illustrate the difference in style and length between formal (QA) and informal (CQA) text."
    }, {
      "heading" : "4.2 Setup",
      "text" : "The standard evaluation metrics in answer sentence selection are Mean Average Precision (MAP) and Mean Reciprocical Rank (MRR). Both metrics are widely used in Information Retrieval (IR) and are averaged per query – in this case per question. Our model produces relevance scores going from 0 (irrelevant) to 1 (relevant) for each candidate answer, and therefore produces a list of candidate answers that can be ranked by relevance. Whereas MRR scores how early a first relevant answer appears in that candidate list, MAP scores the order in which all candidate answers are listed for each question.\nTo produce parse trees, we use the NLTK partof-speech tagger (Loper and Bird, 2002) trained on the part-of-speech tagset of the English Penn Tree-\nbank (PTB) (Marcus et al., 1994), and the Englishlanguage parser of Mrini et al. (2020), which is the state of the art on the parse trees of the PTB."
    }, {
      "heading" : "4.3 Training Parameters",
      "text" : "We use 1 layer of recursive self-attention for all datasets. We use the residual connection described in §3 for TrecQA only. For all our models, we use either BERT large or RoBERTa large, so as to match our baselines. Our recursive self-attention layers have: 16 attention heads, a feed-forward dimension of 4096, and a hidden dimension of 2048. We use half of the dimensions to encode semantic information, and the rest to encode syntactic information."
    }, {
      "heading" : "4.4 Ablation Study on Syntactic Embeddings",
      "text" : "We perform an ablation study by removing the syntactic embedding part of the input representation. In this experiment, we are quantifying the added value of the learned syntactic embeddings for span position, part-of-speech tags and syntactic categories, and tree levels.\nOur results on the dev sets are in Table 3. SemEval 2016 and 2017 results are the same since both have the same dev set. Across all AS2 datasets, we notice that there is an advantage to learning syntactic embeddings, as the sum of MRR and MAP scores are higher for the variant that includes learned syntactic embeddings. The advantage is clearer for QA datasets, suggesting that formal language tends to benefit more from learned syntactic information. We use syntactic embeddings in our next experiments."
    }, {
      "heading" : "4.5 Baselines",
      "text" : "We conside five strong baselines, described in §2:\n(1) GSAMN (Lai et al., 2019): Gated SelfAttention Memory Networks. (2) TandA (Garg et al., 2019): the two-step Transfer and Adapt method. (3) Regular Self-Attention (Laskar et al., 2020): a self-attention layer fine-tuned over frozen BERT Large embeddings. (4) Direct Fine-tuning (Laskar et al., 2020): directly fine-tuning on a pre-trained language model. (5) Evidence Memory (Tran et al., 2020): the neural Turing machine as memory controller.\nBaselines 1, 2, and 5 are available only on TrecQA and/or WikiQA, whereas baselines 3 and 4 use the exact same datasets as we do."
    }, {
      "heading" : "4.6 Results and Discussion",
      "text" : "The results of our experiments with the QA datasets are in Table 4, and the results of our experiments\nwith CQA datasets are in Table 5."
    }, {
      "heading" : "4.6.1 State of the Art in QA datasets",
      "text" : "Our results in Table 4 establish a new state of the art in TrecQA and WikiQA, two widely used benchmark datasets in answer sentence selection.\nIn TrecQA, our average of MAP and MRR scores matches the one for TandA (Garg et al., 2019) in BERT, without any transfer learning on a large dataset. This shows that our model is able to leverage the tree structure to increase performance on relatively small datasets.\nFor the RoBERTa results in WikiQA, the added value between the direct fine-tuning and our recursive self-attention confirms that our model is beneficial to formally written text, such as the one found in Wikipedia.\nThe increase in performance compared to the Evidence Memory models (Tran et al., 2020) when we add our tree representation shows that our tree aggregation method brings about a consistent and robust added value for the QA datasets."
    }, {
      "heading" : "4.6.2 Limitations in CQA datasets",
      "text" : "As shown in Table 5, our Tree Aggregation Transformer is able to establish a new state of the art in SemEval 2015, and our BERT-based version exceeds other BERT-based baselines. However, our method scores below the state of the art in YahooCQA and SemEval 2016, and only manages to match the MRR – but not the MAP – of the state of the art in SemEval 2017.\nTherefore, there is a contrast in the performance of our recursive tree-structured self-attention between the QA and the CQA datasets. The difference lies in the style of the datasets, as questions and sentences can be much longer in QA datasets than in CQA datasets. On average, a training set pair in QA has 32 words for WikiQA, and 39 words in TrecQA, whereas a training set pair in CQA has 78 words for SemEval 2015, 85 words for SemEval 2016-2017, and 40 words for YahooCQA. As shown in the example, CQA pairs may also have spelling mistakes or lack coherent structure. Thus, the informal writing style and larger text length of CQA datasets may be decreasing the ability of our model to leverage tree structures. Accordingly, we see that our model achieves very competitive scores for YahooCQA, and that it has a text length that is very close to the QA datasets. The SemEval 2015 exception could be explained by the fact that the 2015 training dataset is less noisy than the 2016-\n2017 training dataset, as pointed out by the authors of the SemEval CQA datasets."
    }, {
      "heading" : "4.7 Do Tree Structures Improve Performance?",
      "text" : "We investigate how tree structures are leveraged in the Answer Sentence Selection task across the different datasets. We evaluate our tree-structured representations and compare them with the corresponding sequential representations, using three probing tasks from Conneau et al. (2018)."
    }, {
      "heading" : "4.7.1 Probing Tasks",
      "text" : "The three probing tasks are as follows: (1) Top Constituent Prediction. This task looks to predict the top constituent sequence of the question-sentence pair: the sequence of syntactic categories immediately below the S (Sentence) syntactic category. Following Conneau et al. (2018), we define this task as a 20-way classification problem, where the first 19 classes are the 19 most popular top constituent sequences, and the last category is for all the remaining top constituent sequences. (2) Tree Depth Prediction. The tree depth is the number of hops from the root node of the syntactic tree to the lowest-level leaf nodes. (3) Input Length Regression. This tasks investigates whether the embedding is aware of how many words it contains. The length of the questionsentence pair input is defined as the number of its tokens – full words and punctuation symbols.\nThe first two tasks are syntactic, and investigate whether our tree-structured representations absorbed the syntactic category information that we fed it – respectively syntactic categories and tree levels – and whether that information was already present in the sequential representations."
    }, {
      "heading" : "4.7.2 Probing Experiment Setup",
      "text" : "In our probing experiments, we consider all six datasets used both in our work and in Laskar et al. (2020). We consider the sequential representation of a question-answer pair to be the classification token embedding used for prediction in the RoBERTa-based models of Laskar et al. (2020). We take our own RoBERTa-based tree-structured models (without evidence memory), where we consider the tree-structured representation to be the classification token embedding fed to the prediction layer. The tree-structured and sequential representations have the same number of dimensions.\nThe probing model architecture is a simple MLP with a layer of the same size as the input embeddings, a ReLU activation, and a prediction layer. We train 36 probing models for each of the 36 combinations of a probing task, a dataset and a representation type. The input embeddings are frozen, so that the training does not change the weights of the pre-trained AS2 models. All experiments are trained for the same number of epochs, and use the same train/dev/splits as AS2 experiments."
    }, {
      "heading" : "4.7.3 Probing Results and Discussion",
      "text" : "Our probing experiment results are shown in Table 6. We compute the Spearman correlations of the added values of the tree-structured representations compared to the sequential representations in each probing task with the same added value in the AS2 task. We compute the added value of the tree representation in a given task by subtracting the performance of the sequential representations (Laskar et al., 2020) from the performance of the tree-structured representations (ours).\nFor the syntactic probing tasks (the first two), the tree-structured representation gets an F1 score about 3 to 4 times higher than the one obtained by the sequential representation in 4 datasets: TrecQA, WikiQA, and SemEval 2015 and 2017. These 4 datasets correspond to the ones in which our tree-\nstructured AS2 models set a new state of the art or matched the performance of the fine-tuning baseline of Laskar et al. (2020). In the other datasets, the tree-structured representation’s F1 score is just slightly higher than the sequential representation’s F1 score, if not about the same. This shows that when the tree-structured representations successfully absorb the syntactic information we fed it, there is a consistent increase in performance in the answer sentence selection task. The high correlation values for both MAP and MRR confirm that successfully absorbing syntactic information is associated with higher performance in AS2. The weakness of tree-structured representations in certain datasets may be due to the lack of generalization of syntactic parsers trained on the Penn Treebank.\nIn the input length probing experiment, we observe that the mean-squared error (MSE) of the tree-structured representations is consistently and significantly lower than the one of the sequential representations, except for YahooCQA. This shows that the recursion of our tree-structured AS2 model makes representations aware of the length of their question-sentence pair, but the correlation values show that this information does not necessarily help in the AS2 task."
    }, {
      "heading" : "5 Conclusions",
      "text" : "We introduce the Tree Aggregation Transformer: a novel, recursive and tree-structured self-attention model for AS2. Our method embeds sentences by aggregating word representations following the corresponding parse tree. We show that our model leverages tree structure and, through an ablation study, that its learned syntactic embeddings increase performance. Our method establishes a new state of the art in the TrecQA and WikiQA benchmark datasets with only one additional selfattention layer. Our tree-structured self-attention exceeds or matches the state of the art in 2 out of 4 CQA datasets, where text is informal and longer. To investigate this mixed performance, we devise 3 probing tasks to examine what our tree-structured representations learn compared to their sequential counterparts. We find that there is a strong correlation between a tree-structured model’s ability to absorb syntactic information and its ability to increase performance in the AS2 task compared to baselines. Our findings suggest that there is more work to be done for tree-structured representations to adapt to noisy user-generated text."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We gratefully acknowledge the award from NIH/NIA grant R56AG067393. This work is part of the VOLI project (Mrini et al., 2021; Johnson et al., 2020). We thank the anonymous reviewers for their feedback."
    } ],
    "references" : [ {
      "title" : "You only need attention to traverse trees",
      "author" : [ "Mahtab Ahmed", "Muhammad Rifayat Samee", "Robert E Mercer." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 316–322.",
      "citeRegEx" : "Ahmed et al\\.,? 2019",
      "shortCiteRegEx" : "Ahmed et al\\.",
      "year" : 2019
    }, {
      "title" : "A compare-aggregate model with dynamic-clip attention for answer selection",
      "author" : [ "Weijie Bian", "Si Li", "Zhao Yang", "Guang Chen", "Zhiqing Lin." ],
      "venue" : "Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM ’17,",
      "citeRegEx" : "Bian et al\\.,? 2017",
      "shortCiteRegEx" : "Bian et al\\.",
      "year" : 2017
    }, {
      "title" : "Enhanced LSTM for natural language inference",
      "author" : [ "Qian Chen", "Xiaodan Zhu", "Zhen-Hua Ling", "Si Wei", "Hui Jiang", "Diana Inkpen." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
      "citeRegEx" : "Chen et al\\.,? 2017a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Ca-rnn: using context-aligned recurrent neural networks for modeling sentence similarity",
      "author" : [ "Qin Chen", "Qinmin Hu", "Jimmy Xiangji Huang", "Liang He." ],
      "venue" : "Thirty-Second AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Chen et al\\.,? 2018a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Can: Enhancing sentence similarity modeling with collaborative and adversarial network",
      "author" : [ "Qin Chen", "Qinmin Hu", "Jimmy Xiangji Huang", "Liang He." ],
      "venue" : "The 41st International ACM SIGIR Conference on Research & Development in Information",
      "citeRegEx" : "Chen et al\\.,? 2018b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Enhancing recurrent neural networks with positional attention for question answering",
      "author" : [ "Qin Chen", "Qinmin Hu", "Jimmy Xiangji Huang", "Liang He", "Weijie An." ],
      "venue" : "Proceedings of the 40th International ACM SIGIR Conference on Research and Develop-",
      "citeRegEx" : "Chen et al\\.,? 2017b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties",
      "author" : [ "Alexis Conneau", "Germán Kruszewski", "Guillaume Lample", "Loı̈c Barrault", "Marco Baroni" ],
      "venue" : "In Proceedings of the 56th Annual Meeting of the As-",
      "citeRegEx" : "Conneau et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Gatedattention readers for text comprehension",
      "author" : [ "Bhuwan Dhingra", "Hanxiao Liu", "Zhilin Yang", "William Cohen", "Ruslan Salakhutdinov." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
      "citeRegEx" : "Dhingra et al\\.,? 2017",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 2017
    }, {
      "title" : "Tanda: Transfer and adapt pre-trained transformer models for answer sentence selection",
      "author" : [ "Siddhant Garg", "Thuy Vu", "Alessandro Moschitti." ],
      "venue" : "arXiv preprint arXiv:1911.04118.",
      "citeRegEx" : "Garg et al\\.,? 2019",
      "shortCiteRegEx" : "Garg et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural turing machines",
      "author" : [ "Alex Graves", "Greg Wayne", "Ivo Danihelka." ],
      "venue" : "arXiv preprint arXiv:1410.5401.",
      "citeRegEx" : "Graves et al\\.,? 2014",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2014
    }, {
      "title" : "Treetransformer: A transformer-based method for correction of tree-structured data",
      "author" : [ "Jacob Harer", "Chris Reale", "Peter Chin." ],
      "venue" : "arXiv preprint arXiv:1908.00449.",
      "citeRegEx" : "Harer et al\\.,? 2019",
      "shortCiteRegEx" : "Harer et al\\.",
      "year" : 2019
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Voice-based conversational",
      "author" : [ "Janet Johnson", "Khalil Mrini", "Allison Moore", "Emilia Farkas", "Ndapa Nkashole", "Michael Hogarth", "Nadir Weibel" ],
      "venue" : null,
      "citeRegEx" : "Johnson et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2020
    }, {
      "title" : "Predicting and integrating expected answer types into a simple recurrent neural network model for answer sentence selection",
      "author" : [ "Sanjay Kamath", "Brigitte Grau", "Yue Ma." ],
      "venue" : "20th International Conference on Computational Linguistics and Intelligent",
      "citeRegEx" : "Kamath et al\\.,? 2019",
      "shortCiteRegEx" : "Kamath et al\\.",
      "year" : 2019
    }, {
      "title" : "Easyfirst dependency parsing with hierarchical tree lstms",
      "author" : [ "Eliyahu Kiperwasser", "Yoav Goldberg." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:445–461.",
      "citeRegEx" : "Kiperwasser and Goldberg.,? 2016",
      "shortCiteRegEx" : "Kiperwasser and Goldberg.",
      "year" : 2016
    }, {
      "title" : "Natural questions: a benchmark for question answering research",
      "author" : [ "Uszkoreit", "Quoc Le", "Slav Petrov." ],
      "venue" : "Transactions of the Association of Computational Linguistics.",
      "citeRegEx" : "Uszkoreit et al\\.,? 2019",
      "shortCiteRegEx" : "Uszkoreit et al\\.",
      "year" : 2019
    }, {
      "title" : "A gated self-attention memory network for answer selection",
      "author" : [ "Tuan Lai", "Quan Hung Tran", "Trung Bui", "Daisuke Kihara." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Lai et al\\.,? 2019",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2019
    }, {
      "title" : "Contextualized embeddings based transformer encoder for sentence similarity modeling in answer selection task",
      "author" : [ "Md Tahmid Rahman Laskar", "Xiangji Huang", "Enamul Hoque." ],
      "venue" : "Proceedings of The 12th Language Resources and Evaluation",
      "citeRegEx" : "Laskar et al\\.,? 2020",
      "shortCiteRegEx" : "Laskar et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Nltk: The natural language toolkit",
      "author" : [ "Edward Loper", "Steven Bird." ],
      "venue" : "Proceedings of the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics - Volume 1, ETMTNLP ’02,",
      "citeRegEx" : "Loper and Bird.,? 2002",
      "shortCiteRegEx" : "Loper and Bird.",
      "year" : 2002
    }, {
      "title" : "Integrating question classification and deep learning for improved answer selection",
      "author" : [ "Harish Tayyar Madabushi", "Mark Lee", "John Barnden." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 3283–3294.",
      "citeRegEx" : "Madabushi et al\\.,? 2018",
      "shortCiteRegEx" : "Madabushi et al\\.",
      "year" : 2018
    }, {
      "title" : "Medical question understanding and answering for older adults",
      "author" : [ "Khalil Mrini", "Chen Chen", "Ndapa Nakashole", "Nadir Weibel", "Emilia Farcas." ],
      "venue" : "The 3rd Southern California (SoCal) NLP Symposium.",
      "citeRegEx" : "Mrini et al\\.,? 2021",
      "shortCiteRegEx" : "Mrini et al\\.",
      "year" : 2021
    }, {
      "title" : "Rethinking self-attention: Towards interpretability in neural parsing",
      "author" : [ "Khalil Mrini", "Franck Dernoncourt", "Quan Hung Tran", "Trung Bui", "Walter Chang", "Ndapa Nakashole." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Mrini et al\\.,? 2020",
      "shortCiteRegEx" : "Mrini et al\\.",
      "year" : 2020
    }, {
      "title" : "Structure tree-lstm: Structureaware attentional document encoders",
      "author" : [ "Khalil Mrini", "Claudiu Musat", "Michael Baeriswyl", "Martin Jaggi." ],
      "venue" : "arXiv preprint arXiv:1902.09713.",
      "citeRegEx" : "Mrini et al\\.,? 2019",
      "shortCiteRegEx" : "Mrini et al\\.",
      "year" : 2019
    }, {
      "title" : "Semeval-2017 task 3: Community question answering",
      "author" : [ "Preslav Nakov", "Doris Hoogeveen", "Lluı́s Màrquez", "Alessandro Moschitti", "Hamdy Mubarak", "Timothy Baldwin", "Karin Verspoor" ],
      "venue" : "In Proceedings of the 11th International Workshop on Semantic",
      "citeRegEx" : "Nakov et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Nakov et al\\.",
      "year" : 2017
    }, {
      "title" : "SemEval-2015 task 3: Answer selection in community question answering",
      "author" : [ "Preslav Nakov", "Lluı́s Màrquez", "Walid Magdy", "Alessandro Moschitti", "Jim Glass", "Bilal Randeree" ],
      "venue" : "In Proceedings of the 9th International Workshop on Semantic Evaluation (Se-",
      "citeRegEx" : "Nakov et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Nakov et al\\.",
      "year" : 2015
    }, {
      "title" : "Association for Computational Linguistics",
      "author" : [ "Denver", "Colorado" ],
      "venue" : "mEval",
      "citeRegEx" : "Denver and Colorado.,? \\Q2015\\E",
      "shortCiteRegEx" : "Denver and Colorado.",
      "year" : 2015
    }, {
      "title" : "Semeval2016 task 3: Community question answering",
      "author" : [ "Preslav Nakov", "Lluı́s Màrquez", "Alessandro Moschitti", "Walid Magdy", "Hamdy Mubarak", "Abed Alhakim Freihat", "Jim Glass", "Bilal Randeree" ],
      "venue" : "In Proceedings of the 10th International Workshop",
      "citeRegEx" : "Nakov et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nakov et al\\.",
      "year" : 2016
    }, {
      "title" : "Tree-structured attention with hierarchical accumulation",
      "author" : [ "Xuan-Phi Nguyen", "Shafiq Joty", "Steven Hoi", "Richard Socher." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Nguyen et al\\.,? 2019",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2019
    }, {
      "title" : "A multi-view fusion neural network for answer selection",
      "author" : [ "Lei Sha", "Xiaodong Zhang", "Feng Qian", "Baobao Chang", "Zhifang Sui." ],
      "venue" : "Thirty-Second AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Sha et al\\.,? 2018",
      "shortCiteRegEx" : "Sha et al\\.",
      "year" : 2018
    }, {
      "title" : "Semantic compositionality through recursive matrix-vector spaces",
      "author" : [ "Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng." ],
      "venue" : "Proceedings of the 2012 joint conference on empirical methods in natural language processing and com-",
      "citeRegEx" : "Socher et al\\.,? 2012",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2012
    }, {
      "title" : "Parsing natural scenes and natural language with recursive neural networks",
      "author" : [ "Richard Socher", "Cliff Chiung-Yu Lin", "Andrew Y Ng", "Christopher D Manning." ],
      "venue" : "ICML.",
      "citeRegEx" : "Socher et al\\.,? 2011",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2011
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 2013 conference on",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "End-to-end memory networks. In Advances in neural information processing systems, pages 2440–2448",
      "author" : [ "Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus" ],
      "venue" : null,
      "citeRegEx" : "Sukhbaatar et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2015
    }, {
      "title" : "Treegen: A tree-based transformer architecture for code generation",
      "author" : [ "Zeyu Sun", "Qihao Zhu", "Yingfei Xiong", "Yican Sun", "Lili Mou", "Lu Zhang." ],
      "venue" : "AAAI, pages 8984–8991.",
      "citeRegEx" : "Sun et al\\.,? 2020",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning to rank answers on large online qa collections",
      "author" : [ "Mihai Surdeanu", "Massimiliano Ciaramita", "Hugo Zaragoza." ],
      "venue" : "Proceedings of ACL-08: HLT, pages 719–727.",
      "citeRegEx" : "Surdeanu et al\\.,? 2008",
      "shortCiteRegEx" : "Surdeanu et al\\.",
      "year" : 2008
    }, {
      "title" : "Improved semantic representations from tree-structured long short-term memory networks",
      "author" : [ "Kai Sheng Tai", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Tai et al\\.,? 2015",
      "shortCiteRegEx" : "Tai et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning to rank question answer pairs with holographic dual lstm architecture",
      "author" : [ "Yi Tay", "Minh C Phan", "Luu Anh Tuan", "Siu Cheung Hui." ],
      "venue" : "Proceedings of the 40th international ACM SIGIR conference on research and development in informa-",
      "citeRegEx" : "Tay et al\\.,? 2017",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2017
    }, {
      "title" : "Hyperbolic representation learning for fast and efficient neural question answering",
      "author" : [ "Yi Tay", "Luu Anh Tuan", "Siu Cheung Hui." ],
      "venue" : "Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pages 583–591.",
      "citeRegEx" : "Tay et al\\.,? 2018",
      "shortCiteRegEx" : "Tay et al\\.",
      "year" : 2018
    }, {
      "title" : "Head-lexicalized bidirectional tree lstms",
      "author" : [ "Zhiyang Teng", "Yue Zhang." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:163–177.",
      "citeRegEx" : "Teng and Zhang.,? 2017",
      "shortCiteRegEx" : "Teng and Zhang.",
      "year" : 2017
    }, {
      "title" : "Explain by evidence: An explainable memory-based neural network for question answering",
      "author" : [ "Quan Hung Tran", "Nhan Dam", "Tuan Lai", "Franck Dernoncourt", "Trung Le", "Nham Le", "Dinh Phung." ],
      "venue" : "Proceedings of the 28th International Con-",
      "citeRegEx" : "Tran et al\\.,? 2020",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2020
    }, {
      "title" : "A generative attentional neural network model for dialogue act classification",
      "author" : [ "Quan Hung Tran", "Gholamreza Haffari", "Ingrid Zukerman." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short",
      "citeRegEx" : "Tran et al\\.,? 2017",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2017
    }, {
      "title" : "Cross-pair text representations for answer sentence selection",
      "author" : [ "Kateryna Tymoshenko", "Alessandro Moschitti." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2162–2173.",
      "citeRegEx" : "Tymoshenko and Moschitti.,? 2018",
      "shortCiteRegEx" : "Tymoshenko and Moschitti.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "What is the jeopardy model? a quasisynchronous grammar for qa",
      "author" : [ "Mengqiu Wang", "Noah A Smith", "Teruko Mitamura." ],
      "venue" : "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Nat-",
      "citeRegEx" : "Wang et al\\.,? 2007",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2007
    }, {
      "title" : "Tree transformer: Integrating tree structures into self-attention",
      "author" : [ "Yaushian Wang", "Hung-Yi Lee", "Yun-Nung Chen." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "WikiQA: A challenge dataset for open-domain question answering",
      "author" : [ "Yi Yang", "Wen-tau Yih", "Christopher Meek." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2013–2018, Lisbon, Portugal. As-",
      "citeRegEx" : "Yang et al\\.,? 2015",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 37,
      "context" : "Tree-structured LSTM networks (Tai et al., 2015) are the recursive extension of LSTM networks (Hochreiter and Schmidhuber, 1997), and allow for syntactic trees to be represented hierarchically.",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 12,
      "context" : ", 2015) are the recursive extension of LSTM networks (Hochreiter and Schmidhuber, 1997), and allow for syntactic trees to be represented hierarchically.",
      "startOffset" : 53,
      "endOffset" : 87
    }, {
      "referenceID" : 40,
      "context" : "(Teng and Zhang, 2017) do not represent sequence position information, whereas the hybrid neural inference networks (Chen et al.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 2,
      "context" : "(Teng and Zhang, 2017) do not represent sequence position information, whereas the hybrid neural inference networks (Chen et al., 2017a) represent sequence position information separately from treestructured hierarchical information.",
      "startOffset" : 116,
      "endOffset" : 136
    }, {
      "referenceID" : 2,
      "context" : "Tree-structured models have been applied to the tasks of natural language inference (Chen et al., 2017a), sentence pair similarity (Tai et al.",
      "startOffset" : 84,
      "endOffset" : 104
    }, {
      "referenceID" : 37,
      "context" : ", 2017a), sentence pair similarity (Tai et al., 2015), dependency parsing (Kiperwasser and Goldberg, 2016), and text embeddings (Mrini et al.",
      "startOffset" : 35,
      "endOffset" : 53
    }, {
      "referenceID" : 15,
      "context" : ", 2015), dependency parsing (Kiperwasser and Goldberg, 2016), and text embeddings (Mrini et al.",
      "startOffset" : 28,
      "endOffset" : 60
    }, {
      "referenceID" : 24,
      "context" : ", 2015), dependency parsing (Kiperwasser and Goldberg, 2016), and text embeddings (Mrini et al., 2019).",
      "startOffset" : 82,
      "endOffset" : 102
    }, {
      "referenceID" : 44,
      "context" : "The most recent top-performing model architectures for Answer Sentence Selection have been based on the self-attention transformer architecture (Vaswani et al., 2017).",
      "startOffset" : 144,
      "endOffset" : 166
    }, {
      "referenceID" : 18,
      "context" : "4652 use transfer learning on large AS2 datasets; another one (Laskar et al., 2020) uses direct fine-tuning on pre-trained transformer-based language encoders, whereas all three use pre-trained BERT (Devlin et al.",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 7,
      "context" : ", 2020) uses direct fine-tuning on pre-trained transformer-based language encoders, whereas all three use pre-trained BERT (Devlin et al., 2019) and/or RoBERTa embeddings (Liu et al.",
      "startOffset" : 123,
      "endOffset" : 144
    }, {
      "referenceID" : 19,
      "context" : ", 2019) and/or RoBERTa embeddings (Liu et al., 2019).",
      "startOffset" : 34,
      "endOffset" : 52
    }, {
      "referenceID" : 37,
      "context" : "(2019) introduce Constituency and Dependency Tree Transformer models, largely inspired by the Constituency and Dependency TreeLSTM models (Tai et al., 2015) and RvNN models (Socher et al.",
      "startOffset" : 138,
      "endOffset" : 156
    }, {
      "referenceID" : 8,
      "context" : "It combines gated attention (Dhingra et al., 2017; Tran et al., 2017), memory networks (Sukhbaatar et al.",
      "startOffset" : 28,
      "endOffset" : 69
    }, {
      "referenceID" : 42,
      "context" : "It combines gated attention (Dhingra et al., 2017; Tran et al., 2017), memory networks (Sukhbaatar et al.",
      "startOffset" : 28,
      "endOffset" : 69
    }, {
      "referenceID" : 34,
      "context" : ", 2017), memory networks (Sukhbaatar et al., 2015) and self-attention (Vaswani et al.",
      "startOffset" : 25,
      "endOffset" : 50
    }, {
      "referenceID" : 44,
      "context" : ", 2015) and self-attention (Vaswani et al., 2017) in one model.",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 10,
      "context" : "They propose to use a neural Turing machine (Graves et al., 2014) as a controller for the memory network, instead of the gated attention that Lai et al.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 29,
      "context" : "model to re-use the same sets of parameters across each tree level, instead of training new ones as in previous work (Nguyen et al., 2019; Wang et al., 2019).",
      "startOffset" : 117,
      "endOffset" : 157
    }, {
      "referenceID" : 46,
      "context" : "model to re-use the same sets of parameters across each tree level, instead of training new ones as in previous work (Nguyen et al., 2019; Wang et al., 2019).",
      "startOffset" : 117,
      "endOffset" : 157
    }, {
      "referenceID" : 45,
      "context" : "TrecQA (Wang et al., 2007) is collected from labeled sentences of the QA track of the Text REtrieval Conference (TREC).",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 47,
      "context" : "WikiQA (Yang et al., 2015) contains questions originally sampled from Bing query logs, and matched with candidate answer sentences from the first paragraph of relevant Wikipedia articles.",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 38,
      "context" : "YahooCQA (Tay et al., 2017) is a filtered and pre-processed subset of the large-scale Yahoo! Answers Manner Questions dataset (Surdeanu et al.",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 36,
      "context" : ", 2017) is a filtered and pre-processed subset of the large-scale Yahoo! Answers Manner Questions dataset (Surdeanu et al., 2008).",
      "startOffset" : 106,
      "endOffset" : 129
    }, {
      "referenceID" : 26,
      "context" : "SemEval 2015 CQA (Nakov et al., 2015) is the challenge dataset of Subtask A of Task 3 of SemEval 2015.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 30,
      "context" : "Following previous work (Sha et al., 2018; Laskar et al., 2020), only definitely relevant candidate answers are marked as relevant in our binary classification setting.",
      "startOffset" : 24,
      "endOffset" : 63
    }, {
      "referenceID" : 18,
      "context" : "Following previous work (Sha et al., 2018; Laskar et al., 2020), only definitely relevant candidate answers are marked as relevant in our binary classification setting.",
      "startOffset" : 24,
      "endOffset" : 63
    }, {
      "referenceID" : 28,
      "context" : "SemEval 2016 CQA (Nakov et al., 2016) corresponds as well to Subtask A of Task 3 of SemEval",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 25,
      "context" : "SemEval 2017 CQA (Nakov et al., 2017) is the latest version of the community question answering task.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 20,
      "context" : "To produce parse trees, we use the NLTK partof-speech tagger (Loper and Bird, 2002) trained on the part-of-speech tagset of the English Penn Tree-",
      "startOffset" : 61,
      "endOffset" : 83
    }, {
      "referenceID" : 17,
      "context" : "716 Models using BERT Large GSAMN (Lai et al., 2019)* 0.",
      "startOffset" : 34,
      "endOffset" : 52
    }, {
      "referenceID" : 9,
      "context" : "868 Models using RoBERTa Large TandA (Garg et al., 2019)* 0.",
      "startOffset" : 37,
      "endOffset" : 56
    }, {
      "referenceID" : 41,
      "context" : "920 Models using RoBERTa Large and Evidence Memory Evidence Memory (Tran et al., 2020)* 0.",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 17,
      "context" : "(1) GSAMN (Lai et al., 2019): Gated SelfAttention Memory Networks.",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 9,
      "context" : "(2) TandA (Garg et al., 2019): the two-step Transfer and Adapt method.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 18,
      "context" : "(3) Regular Self-Attention (Laskar et al., 2020): a self-attention layer fine-tuned over frozen BERT Large embeddings.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 18,
      "context" : "(4) Direct Fine-tuning (Laskar et al., 2020): directly fine-tuning on a pre-trained language model.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 41,
      "context" : "(5) Evidence Memory (Tran et al., 2020): the neural Turing machine as memory controller.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 9,
      "context" : "In TrecQA, our average of MAP and MRR scores matches the one for TandA (Garg et al., 2019) in BERT, without any transfer learning on a large dataset.",
      "startOffset" : 71,
      "endOffset" : 90
    }, {
      "referenceID" : 41,
      "context" : "The increase in performance compared to the Evidence Memory models (Tran et al., 2020) when we add our tree representation shows that our tree aggregation method brings about a consistent and",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 18,
      "context" : "872 - Models using BERT Large Regular Self-Attention (Laskar et al., 2020) 0.",
      "startOffset" : 53,
      "endOffset" : 74
    }, {
      "referenceID" : 18,
      "context" : "955 Models using RoBERTa Large Direct Fine-tuning (Laskar et al., 2020) 0.",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 18,
      "context" : "Table 6: Results for three probing tasks comparing sequential (Laskar et al., 2020) and tree-structured (ours) representations.",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 18,
      "context" : "We compute the added value of the tree representation in a given task by subtracting the performance of the sequential representations (Laskar et al., 2020) from the performance of the tree-structured representations (ours).",
      "startOffset" : 135,
      "endOffset" : 156
    }, {
      "referenceID" : 22,
      "context" : "This work is part of the VOLI project (Mrini et al., 2021; Johnson et al., 2020).",
      "startOffset" : 38,
      "endOffset" : 80
    }, {
      "referenceID" : 13,
      "context" : "This work is part of the VOLI project (Mrini et al., 2021; Johnson et al., 2020).",
      "startOffset" : 38,
      "endOffset" : 80
    } ],
    "year" : 2021,
    "abstractText" : "Syntactic structure is an important component of natural language text. Recent topperforming models in Answer Sentence Selection (AS2) use self-attention and transfer learning, but not syntactic structure. Tree structures have shown strong performance in tasks with sentence pair input like semantic relatedness. We investigate whether tree structures can boost performance in AS2. We introduce the Tree Aggregation Transformer: a novel recursive, tree-structured self-attention model for AS2. The recursive nature of our model is able to represent all levels of syntactic parse trees with only one additional self-attention layer. Without transfer learning, we establish a new state of the art on the popular TrecQA and WikiQA benchmark datasets. Additionally, we evaluate our method on four Community Question Answering datasets, and find that tree-structured representations have limitations with noisy user-generated text. We conduct probing experiments to evaluate how our models leverage tree structures across datasets. Our findings show that the ability of treestructured models to successfully absorb syntactic information is strongly correlated with a higher performance in AS2.",
    "creator" : "LaTeX with hyperref"
  }
}