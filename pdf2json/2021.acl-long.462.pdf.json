{
  "name" : "2021.acl-long.462.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding",
    "authors" : [ "Xin Sun", "Tao Ge", "Furu Wei", "Houfeng Wang" ],
    "emails" : [ "sunx5@pku.edu.cn;", "wanghf@pku.edu.cn;", "tage@microsoft.com", "fuwei@microsoft.com", "(tage@microsoft.com)" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5937–5947\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5937"
    }, {
      "heading" : "1 Introduction",
      "text" : "The Transformer (Vaswani et al., 2017) has become the most popular model for Grammatical Error Correction (GEC). In practice, however, the sequenceto-sequence (seq2seq) approach has been blamed recently (Chen et al., 2020; Stahlberg and Kumar,\n∗ This work was done during the author’s internship at MSR Asia. Contact person: Tao Ge (tage@microsoft.com)\n†Co-first authors with equal contributions\n2020; Omelianchuk et al., 2020) for its poor inference efficiency in modern writing assistance applications (e.g., Microsoft Office Word1, Google Docs2 and Grammarly3) where a GEC model usually performs online inference, instead of batch inference, for proactively and incrementally checking a user’s latest completed sentence to offer instantaneous feedback.\nTo better exploit the Transformer for instantaneous GEC in practice, we propose a novel approach – Shallow Aggressive Decoding (SAD) to improve the model’s online inference efficiency. The core innovation of SAD is aggressive decoding: instead of sequentially decoding only one token at each step, aggressive decoding tries to decode as many tokens as possible in parallel with the assumption that the output sequence should be almost the same with the input. As shown in Figure 1, if the output prediction at each step perfectly matches its counterpart in the input sentence, the inference will finish, meaning that the model will keep the input untouched without editing; if the output token at a step does not match its corresponding token in the input, we will discard all the predictions after the bifurcation position and re-decode them in the original autoregressive decoding manner until we find a new opportunity for aggressive decoding. In this way, we can decode the most text in parallel in the same prediction quality as autoregressive greedy decoding, but largely improve the inference efficiency.\nIn addition to aggressive decoding, SAD proposes to use a shallow decoder, instead of the conventional Transformer with balanced encoderdecoder depth, to reduce the computational cost for further accelerating inference. The experimental\n1https://www.microsoft.com/en-us/ microsoft-365/word\n2https://www.google.com/docs/about 3https://www.grammarly.com\n[BOS] I 'm wri,ng to inform some some advice on traveling and working . [PAD]\nI 'm wri,ng to give you advice advice on traveling and working . [EOS]\nThe contributions of this paper are two-fold:\nWe propose a novel aggressive decoding approach, allowing us to decode as many token as possible in parallel, which yields the same predictions as greedy decoding but with a substantial improvement of computational parallelism and online inference ef ciency. We propose to combine aggressive decoding with the Transformer with a shallow decoder. Our nal approach not only advances the stateof-the-art in English GEC benchmarks with an almost10 online inference speedup but also is easily adapted to other languages.\n2 Background: Transformer The Transformer is a seq2seq neural network architecture based on multi-head attention mechanism, which has become the most successful and widely\nused seq2seq models in various generation tasks\nsuch as machine translation, abstractive summa-\nrization as well as GEC.\nThe original Transformer follows the balanced\nencoder-decoder architecture: its encoder, consist-\ning of a stack of identical encoder layers, maps an\ninput sentencex = ( x1; : : : ; xn ) to a sequence\nof continuous representationz = ( z1; : : : ; zn ); and its decoder, which is composed of a stack of the same number of identical decoder layers as the encoder, generates an output sequenceo = (o1; : : : ; om ) givenz . In the training phase, the model learns an autoregressive scoring modelP (y j x ; ), implemented with teacher forcing:\n= arg max logP (y j x ; ) = arg max l 1X\ni =0\nlogP (yi +1 j y i ; x ; ) (1)\nwherey = ( y1; : : : ; yl ) is the ground-truth target sequence andy i = ( y0; : : : ; yi ). As ground truth is available during training, Eq (1) can be ef ciently obtained as the probabilityP (yi +1 j y i ; x ) at each step can be computed in parallel.\nDuring inference, the output sequenceo =\n(o1, . . . , om) is derived by maximizing the following equation:\no∗ = argmax o logP (o | x;Φ)\n= argmax o m−1∑ j=0 logP (oj+1 | o≤j ,x;Φ) (2)\nSince no ground truth is available in the inference phase, the model has to decode only one token at each step conditioning on the previous decoded tokens o≤j instead of decoding in parallel as in the training phase."
    }, {
      "heading" : "3 Shallow Aggressive Decoding",
      "text" : ""
    }, {
      "heading" : "3.1 Aggressive Decoding",
      "text" : "As introduced in Section 2, the Transformer decodes only one token at each step during inference. The autoregressive decoding style is the main bottleneck of inference efficiency because it largely reduces computational parallelism.\nFor GEC, fortunately, the output sequence is usually very similar to the input with only a few edits if any. This special characteristic of the task makes it unnecessary to follow the original autoregressive decoding style; instead, we propose a novel decoding approach – aggressive decoding which tries to decode as many tokens as possible during inference. The overview of aggressive decoding is shown in Figure 1, and we will discuss it in detail in the following sections."
    }, {
      "heading" : "3.1.1 Initial Aggressive Decoding",
      "text" : "The core motivation of aggressive decoding is the assumption that the output sequence o = (o1, . . . , om) should be almost the same with the input sequence x = (x1, . . . , xn) in GEC. At the initial step, instead of only decoding the first token o1 conditioning on the special [BOS] token o0, aggressive decoding decodes o1...n conditioning on the pseudo previous decoded tokens ô0...n−1 in parallel with the assumption that ô0...n−1 = x0,...,n−1. Specifically, for j ∈ {0, 1, . . . , n− 2, n− 1}, oj+1 is decoded as follows:\no∗j+1 = argmaxoj+1 logP (oj+1 |o≤j ,x;Φ)\n= argmax oj+1\nlogP (oj+1 | ô≤j ,x;Φ)\n= argmax oj+1\nlogP (oj+1 | x≤j ,x;Φ)\n(3)\nwhere ô≤j is the pseudo previous decoded tokens at step j+1, which is assumed to be the same with x≤j .\nAfter we obtain o1...n, we verify whether o1...n is actually identical to x1...n or not. If o1...n is fortunately exactly the same with x1...n, the inference will finish, meaning that the model finds no grammatical errors in the input sequence x1...n and keeps the input untouched. In more cases, however, o1...n will not be exactly the same with x1...n. In such a case, we have to stop aggressive decoding and find the first bifurcation position k so that o1...k−1 = x1...k−1 and ok 6= xk.\nSince o1...k−1 = ô1...k−1 = x1...k−1, the predictions o1...k could be accepted as they will not be different even if they are decoded through the original autoregressive greedy decoding. However, for the predictions ok+1...n, we have to discard and re-decode them because ok 6= ôk."
    }, {
      "heading" : "3.1.2 Re-decoding",
      "text" : "As ok 6= ôk = xk, we have to re-decode for oj+1 (j ≥ k) one by one following the original autoregressive decoding:\no∗j+1 = argmaxoj+1 P (oj+1 | o≤j ,x;Φ) (4)\nAfter we obtain o≤j (j > k), we try to match its suffix to the input sequence x for further aggressive decoding. If we find its suffix oj−q...j (q ≥ 0) is the unique substring of x such that oj−q...j = xi−q...i, then we can assume that oj+1... will be very likely to be the same with xi+1... because of the special characteristic of the task of GEC.\nIf we fortunately find such a suffix match, then we can switch back to aggressive decoding to decode in parallel with the assumption ôj+1... = xi+1.... Specifically, the token oj+t (t > 0) is decoded as follows:\no∗j+t = argmaxoj+t P (oj+t | o<j+t,x;Φ) (5)\nIn Eq (5), o<j+t is derived as follows:\no<j+t = CAT(o≤j , ôj+1...j+t−1)\n= CAT(o≤j ,xi+1...i+t−1) (6)\nwhere CAT(a, b) is the operation that concatenates two sequences a and b.\nOtherwise (i.e., we cannot find a suffix match at the step), we continue decoding using the original\nAlgorithm 1 Aggressive Decoding Input: Φ, x = ([BOS], x1, . . . , xn, [PAD]), o = (o0) = ([BOS]); Output: o1...j = (o1, . . . , oj);\n1: Initialize j ← 0; 2: while oj 6= [EOS] and j < MAX LEN do 3: if oj−q...j (q ≥ 0) is a unique substring of x such that ∃ ! i : oj−q...j = xi−q...i then 4: Aggressive Decode õj+1... according to Eq (5) and Eq (6); 5: Find bifurcation j + k (k > 0) such that õj+1...j+k−1 = xi+1...i+k−1 and õj+k 6= xi+k; 6: o← CAT(o, õj+1...j+k); 7: j ← j + k; 8: else 9: Decode o∗j+1 = argmaxoj+1 P (oj+1 | o≤j ,x;Φ);\n10: o← CAT(o, o∗j+1); 11: j ← j + 1; 12: end if 13: end while\nautoregressive greedy decoding approach until we find a suffix match.\nWe summarize the process of aggressive decoding in Algorithm 1. For simplifying implementation, we make minor changes in Algorithm 1: 1) we set o0 = x0 = [BOS] in Algorithm 1, which enables us to regard the initial aggressive decoding as the result of suffix match of o0 = x0; 2) we append a special token [PAD] to the end of x so that the bifurcation (in the 5th line in Algorithm 1) must exist (see the bottom example in Figure 1). Since we discard all the computations and predictions after the bifurcation for re-decoding, aggressive decoding guarantees that generation results are exactly the same as greedy decoding (i.e., beam=1). However, as aggressive decoding decodes many tokens in parallel, it largely improves the computational parallelism during inference, greatly benefiting the inference efficiency."
    }, {
      "heading" : "3.2 Shallow Decoder",
      "text" : "Even though aggressive decoding can significantly improve the computational parallelism during inference, it inevitably leads to intensive computation and even possibly introduces additional computation caused by re-decoding for the discarded predictions.\nTo reduce the computational cost for decoding, we propose to use a shallow decoder, which has proven to be an effective strategy (Kasai et al., 2020; Li et al., 2021) in neural machine translation (NMT), instead of using the Transformer with balanced encoder-decoder depth as the previous state-of-the-art Transformer models in GEC. By\ncombining aggressive decoding with the shallow decoder, we are able to further improve the inference efficiency."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Data and Model Configuration",
      "text" : "We follow recent work in English GEC to conduct experiments in the restricted training setting of BEA-2019 GEC shared task (Bryant et al., 2019): We use Lang-8 Corpus of Learner English (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011) and W&I+LOCNESS (Granger; Bryant et al., 2019) as our GEC training data. For facilitating fair comparison in the efficiency evaluation, we follow the previous studies (Omelianchuk et al., 2020; Chen et al., 2020) which conduct GEC efficiency evaluation to use CoNLL-2014 (Ng et al., 2014) dataset that contains 1,312 sentences as our main test set, and evaluate the speedup as well as MaxMatch (Dahlmeier and Ng, 2012) precision, recall and F0.5 using their official evaluation scripts4. For validation, we use CoNLL-2013 (Ng et al., 2013) that contains 1,381 sentences as our validation set. We also test our approach on NLPCC-18 Chinese GEC shared task (Zhao et al., 2018), following their training5 and evaluation setting, to verify the effectiveness of our approach in other languages. To compare with the state-of-the-art approaches in English GEC that pretrain with synthetic data,\n4https://github.com/nusnlp/m2scorer 5Following Chen et al. (2020), we sample 5,000 training\ninstances as the validation set.\nwe also synthesize 300M error-corrected sentence pairs for pretraining the English GEC model following the approaches of Grundkiewicz et al. (2019) and Zhang et al. (2019). Note that in the following evaluation sections, the models evaluated are by default trained without the synthetic data unless they are explicitly mentioned.\nWe use the most popular GEC model architecture – Transformer (big) model (Vaswani et al., 2017) as our baseline model which has a 6-layer encoder and 6-layer decoder with 1,024 hidden units. We train the English GEC model using an encoder-decoder shared vocabulary of 32K Byte Pair Encoding (Sennrich et al., 2016) tokens and train the Chinese GEC model with 8.4K Chinese characters. We include more training details in the supplementary notes. For inference, we use greedy decoding6 by default.\nAll the efficiency evaluations are conducted in the online inference setting (i.e., batch size=1) as we focus on instantaneous GEC. We perform model inference with fairseq7 implementation using Pytorch 1.5.1 with 1 Nvidia Tesla V100-PCIe of 16GB GPU memory under CUDA 10.2."
    }, {
      "heading" : "4.2 Evaluation for Aggressive Decoding",
      "text" : "We evaluate aggressive decoding in our validation set (CoNLL-13) which contains 1,381 validation examples. As shown in Table 1, aggressive decoding achieves a 7× ∼ 8× speedup over the original autoregressive beam search (beam=5), and generates exactly the same predictions as greedy decoding, as discussed in Section 3.1.2. Since greedy decoding can achieve comparable overall performance (i.e., F0.5) with beam search while it tends\n6Our implementation of greedy decoding is simplified for higher efficiency (1.3× ∼ 1.4× speedup over beam=5) than the implementation of beam=1 decoding in fairseq (around 1.1× speedup over beam=5).\n7https://github.com/pytorch/fairseq\nto make more edits resulting in higher recall but lower precision, the advantage of aggressive decoding in practical GEC applications is obvious given its strong performance and superior efficiency.\nWe further look into the efficiency improvement by aggressive decoding. Figure 2 shows the speedup distribution of the 1,381 examples in CoNLL-13 with respect to their edit ratio which is defined as the normalized (by the input length) edit distance between the input and output. It is obvious that the sentences with fewer edits tend to achieve higher speedup, which is consistent with our intuition that most tokens in such sentences can be decoded in parallel through aggressive decoding; on the other hand, for the sentences that are heavily edited, their speedup is limited because of frequent re-decoding. To give a more intuitive analysis, we also present concrete examples with various speedup in our validation set to understand how aggressive decoding improves the inference efficiency in Table 2.\nMoreover, we conduct an ablation study to in-\nvestigate whether it is necessary to constrain the maximal aggressive decoding length8, because it might become highly risky to waste large amounts of computation because of potential re-decoding for a number of steps after the bifurcation if we aggressively decode a very long sequence in parallel. Table 3 shows the online inference efficiency with different maximal aggressive decoding lengths. It appears that constraining the maximal aggressive\n8Constraining the maximal aggressive decoding length to Lmax means that the model can only aggressively decode at most Lmax tokens in parallel.\ndecoding length does not help improve the efficiency; instead, it slows down the inference if the maximal aggressive decoding length is set to a small number. We think the reason is that sentences in GEC datasets are rarely too long. For example, the average length of the sentences in CoNLL-13 is 21 and 96% of them are shorter than 40 tokens. Therefore, it is unnecessary to constrain the maximal aggressive decoding length in GEC."
    }, {
      "heading" : "4.3 Evaluation for Shallow Decoder",
      "text" : "We study the effects of changing the number of encoder and decoder layers in the Transformer-big on both the performance and the online inference efficiency. By comparing 6+6 with 3+6 and 9+6 in Table 4, we observe the performance improves as the encoder becomes deeper, demonstrating the importance of the encoder in GEC. In contrast, by comparing the 6+6 with 6+3 and 6+9, we do not see a substantial fluctuation in the performance, indicating no necessity of a deep decoder. Moreover, it is observed that a deeper encoder does not significantly slow down the inference but a shallow decoder can greatly improve the inference efficiency. This is because Transformer encoders can be parallelized efficiently on GPUs, whereas Transformer decoders are auto-regressive and hence the number of layers greatly affects decoding speed, as discussed in Section 3.2. These observations motivate us to make the encoder deeper and the decoder shallower.\nAs shown in the bottom group of Table 4, we try different combinations of the number of encoder and decoder layers given approximately the same parameterization budget as the Transformerbig. It is interesting to observe that 7+5, 8+4 and 9+3 achieve the comparable and even better performance than the Transformer-big baseline with much less computational cost. When we further increase the encoder layer and decrease the decoder layer, we see a drop in the performance of 10+2\nand 11+1 despite the improved efficiency because it becomes difficult to train the Transformer with extremely imbalanced encoder and decoder well, as indicated9 by the previous work (Kasai et al., 2020; Li et al., 2021; Gu and Kong, 2020).\nSince the 9+3 model achieves the best result with an around 2× speedup in the validation set with almost the same parameterization budget, we choose it as the model architecture to combine with aggressive decoding for final evaluation."
    }, {
      "heading" : "4.4 Results",
      "text" : "We evaluate our final approach – shallow aggressive decoding which combines aggressive decoding with the shallow decoder. Table 5 shows the performance and efficiency of our approach and recently proposed efficient GEC models that are all faster than the Transformer-big baseline in CoNLL-14 test set. Our approach (the 9+3 model with aggressive decoding) that is pretrained with synthetic data achieves 63.5 F0.5 with 10.3× speedup over the Transformer-big baseline, which outperforms the majority10 of the efficient GEC models in terms of either quality or speed. The only model that shows advantages over our 9+3 model is GECToR which is developed based on the powerful pretrained mod-\n9They show that sequence-level knowledge distillation (KD) may benefit training the extremely imbalanced Transformer in NMT. However, we do not conduct KD for fair comparison to other GEC models in previous work.\n10It is notable that PIE is not strictly comparable here because their training data is different from ours: PIE does not use the W&I+LOCNESS corpus.\nels (e.g., RoBERTa (Liu et al., 2019) and XLNet (Yang et al., 2019)) with its multi-stage training strategy. Following GECToR’s recipe, we leverage the pretrained model BART (Lewis et al., 2019) to initialize a 12+2 model which proves to work well in NMT (Li et al., 2021) despite more parameters, and apply the multi-stage fine-tuning strategy used in Stahlberg and Kumar (2020). The final single model11 with aggressive decoding achieves the state-of-the-art result – 66.4 F0.5 in the CoNLL-14 test set with a 9.6× speedup over the Transformerbig baseline.\nUnlike GECToR and PIE that are difficult to adapt to other languages despite their competitive speed because they are specially designed for English GEC with many manually designed languagespecific operations like the transformation of verb forms (e.g., VBD→VBZ) and prepositions (e.g., in→at), our approach is data-driven without depending on language-specific features, and thus can be easily adapted to other languages (e.g., Chinese). As shown in Table 6, our approach consistently performs well in Chinese GEC, showing an around 12.0× online inference speedup over the Transformer-big baseline with comparable performance."
    }, {
      "heading" : "5 Related Work",
      "text" : "The state-of-the-art of GEC has been significantly advanced owing to the tremendous success of seq2seq learning (Sutskever et al., 2014) and the Transformer (Vaswani et al., 2017). Most recent work on GEC focuses on improving the performance of the Transformer-based GEC models. However, except for the approaches that add synthetic erroneous data for pretraining (Ge et al., 2018a; Grundkiewicz et al., 2019; Zhang et al.,\n11The same model checkpoint also achieves the state-ofthe-art result – 72.9 F0.5 with a 9.3× speedup in the BEA-19 test set.\n2019; Lichtarge et al., 2019; Zhou et al., 2020; Wan et al., 2020), most methods that improve performance (Ge et al., 2018b; Kaneko et al., 2020) introduce additional computational cost and thus slow down inference despite the performance improvement.\nTo make the Transformer-based GEC model more efficient during inference for practical application scenarios, some recent studies have started exploring the approaches based on edit operations. Among them, PIE (Awasthi et al., 2019) and GECToR (Omelianchuk et al., 2020) propose to accelerate the inference by simplifying GEC from sequence generation to iterative edit operation tagging. However, as they rely on many languagedependent edit operations such as the conversion of singular nouns to plurals, it is difficult for them to adapt to other languages. LaserTagger (Malmi et al., 2019) uses the similar method but it is datadriven and language-independent by learning operations from training data. However, its performance is not so desirable as its seq2seq counterpart despite its high efficiency. The only two previous efficient approaches that are both languageindependent and good-performing are Stahlberg and Kumar (2020) which uses span-based edit operations to correct sentences to save the time for copying unchanged tokens, and Chen et al. (2020) which first identifies incorrect spans with a tagging model then only corrects these spans with a generator. However, all the approaches have to extract edit operations and even conduct token alignment in advance from the error-corrected sentence pairs for training the model. In contrast, our proposed shallow aggressive decoding tries to accelerate the model inference through parallel autoregressive decoding which is related to some previous work (Ghazvininejad et al., 2019; Stern et al., 2018) in neural machine translation (NMT), and the imbalanced encoder-decoder architecture which\nis recently explored by Kasai et al. (2020) and Li et al. (2021) for NMT. Not only is our approach language-independent, efficient and guarantees that its predictions are exactly the same with greedy decoding, but also does not need to change the way of training, making it much easier to train without so complicated data preparation as in the edit operation based approaches."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "In this paper, we propose Shallow Aggressive Decoding (SAD) to accelerate online inference efficiency of the Transformer for instantaneous GEC. Aggressive decoding can yield the same prediction quality as autoregressive greedy decoding but with much less latency. Its combination with the Transformer with a shallow decoder can achieve state-of-the-art performance with a 9× ∼ 12× online inference speedup over the Transformer-big baseline for GEC.\nBased on the preliminary study of SAD in GEC, we plan to further explore the technique for accelerating the Transformer for other sentence rewriting tasks, where the input is similar to the output, such as style transfer and text simplification. We believe SAD is promising to become a general acceleration methodology for writing intelligence models in modern writing assistant applications that require fast online inference."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank all the reviewers for their valuable comments to improve our paper. We thank Xingxing Zhang, Xun Wang and Si-Qing Chen for their insightful discussions and suggestions. The work is supported by National Natural Science Foundation of China under Grant No.62036001. The corresponding author of this paper is Houfeng Wang."
    }, {
      "heading" : "A Hyper-parameters",
      "text" : "Hyper-parameters of training the Transformer for English GEC are listed in table 7. The hyperparameters for Chinese GEC are the same with those of training from scratch."
    }, {
      "heading" : "B CPU Efficiency",
      "text" : "Table 8 shows total latency and speedup of the Transformer with different encoder-decoder depth on an Intel® Xeon® E5-2690 v4 Processor(2.60GHz) with 8 and 2 threads12, respectively. Our approach achieves a 7× ∼ 8× online inference speedup over the Transformer-big baseline on CPU.\n12We set OMP NUM THREADS to 8 or 2."
    } ],
    "references" : [ {
      "title" : "Parallel iterative edit models for local sequence transduction",
      "author" : [ "Abhijeet Awasthi", "Sunita Sarawagi", "Rasna Goyal", "Sabyasachi Ghosh", "Vihari Piratla." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Awasthi et al\\.,? 2019",
      "shortCiteRegEx" : "Awasthi et al\\.",
      "year" : 2019
    }, {
      "title" : "The bea-2019 shared task on grammatical error correction",
      "author" : [ "Christopher Bryant", "Mariano Felice", "Øistein E Andersen", "Ted Briscoe." ],
      "venue" : "Proceedings of the Fourteenth Workshop on Innovative",
      "citeRegEx" : "Bryant et al\\.,? 2019",
      "shortCiteRegEx" : "Bryant et al\\.",
      "year" : 2019
    }, {
      "title" : "Improving the efficiency of grammatical error correction with erroneous span detection and correction",
      "author" : [ "Mengyun Chen", "Tao Ge", "Xingxing Zhang", "Furu Wei", "Ming Zhou." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Better evaluation for grammatical error correction",
      "author" : [ "Daniel Dahlmeier", "Hwee Tou Ng." ],
      "venue" : "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages",
      "citeRegEx" : "Dahlmeier and Ng.,? 2012",
      "shortCiteRegEx" : "Dahlmeier and Ng.",
      "year" : 2012
    }, {
      "title" : "Building a large annotated corpus of learner english: The nus corpus of learner english",
      "author" : [ "Daniel Dahlmeier", "Hwee Tou Ng", "Siew Mei Wu." ],
      "venue" : "Proceedings of the eighth workshop on innovative use of NLP for building educational applications, pages",
      "citeRegEx" : "Dahlmeier et al\\.,? 2013",
      "shortCiteRegEx" : "Dahlmeier et al\\.",
      "year" : 2013
    }, {
      "title" : "Fluency boost learning and inference for neural grammatical error correction",
      "author" : [ "Tao Ge", "Furu Wei", "Ming Zhou." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1055–",
      "citeRegEx" : "Ge et al\\.,? 2018a",
      "shortCiteRegEx" : "Ge et al\\.",
      "year" : 2018
    }, {
      "title" : "Reaching human-level performance in automatic grammatical error correction: An empirical study",
      "author" : [ "Tao Ge", "Furu Wei", "Ming Zhou." ],
      "venue" : "arXiv preprint arXiv:1807.01270.",
      "citeRegEx" : "Ge et al\\.,? 2018b",
      "shortCiteRegEx" : "Ge et al\\.",
      "year" : 2018
    }, {
      "title" : "Mask-predict: Parallel decoding of conditional masked language models",
      "author" : [ "Marjan Ghazvininejad", "Omer Levy", "Yinhan Liu", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Ghazvininejad et al\\.,? 2019",
      "shortCiteRegEx" : "Ghazvininejad et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural grammatical error correction systems with unsupervised pre-training on synthetic data",
      "author" : [ "Roman Grundkiewicz", "Marcin Junczys-Dowmunt", "Kenneth Heafield." ],
      "venue" : "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Ed-",
      "citeRegEx" : "Grundkiewicz et al\\.,? 2019",
      "shortCiteRegEx" : "Grundkiewicz et al\\.",
      "year" : 2019
    }, {
      "title" : "Fully nonautoregressive neural machine translation: Tricks of the trade",
      "author" : [ "Jiatao Gu", "Xiang Kong." ],
      "venue" : "arXiv preprint arXiv:2012.15833.",
      "citeRegEx" : "Gu and Kong.,? 2020",
      "shortCiteRegEx" : "Gu and Kong.",
      "year" : 2020
    }, {
      "title" : "Levenshtein transformer",
      "author" : [ "Jiatao Gu", "Changhan Wang", "Junbo Zhao." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 11181–11191.",
      "citeRegEx" : "Gu et al\\.,? 2019",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2019
    }, {
      "title" : "Encoder-decoder models can benefit from pre-trained masked language models in grammatical error correction",
      "author" : [ "Masahiro Kaneko", "Masato Mita", "Shun Kiyono", "Jun Suzuki", "Kentaro Inui." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Asso-",
      "citeRegEx" : "Kaneko et al\\.,? 2020",
      "shortCiteRegEx" : "Kaneko et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep encoder, shallow decoder: Reevaluating the speed-quality tradeoff in machine translation",
      "author" : [ "Jungo Kasai", "Nikolaos Pappas", "Hao Peng", "James Cross", "Noah A Smith." ],
      "venue" : "arXiv preprint arXiv:2006.10369.",
      "citeRegEx" : "Kasai et al\\.,? 2020",
      "shortCiteRegEx" : "Kasai et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Bart: Denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Ves Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2019
    }, {
      "title" : "An efficient transformer decoder with compressed sub-layers",
      "author" : [ "Yanyang Li", "Ye Lin", "Tong Xiao", "Jingbo Zhu." ],
      "venue" : "arXiv preprint arXiv:2101.00542.",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Corpora generation for grammatical error correction",
      "author" : [ "Jared Lichtarge", "Chris Alberti", "Shankar Kumar", "Noam Shazeer", "Niki Parmar", "Simon Tong." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Lichtarge et al\\.,? 2019",
      "shortCiteRegEx" : "Lichtarge et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Encode, tag, realize: High-precision text editing",
      "author" : [ "Eric Malmi", "Sebastian Krause", "Sascha Rothe", "Daniil Mirylenka", "Aliaksei Severyn." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
      "citeRegEx" : "Malmi et al\\.,? 2019",
      "shortCiteRegEx" : "Malmi et al\\.",
      "year" : 2019
    }, {
      "title" : "Mining revision log of language learning sns for automated japanese error correction of second language learners",
      "author" : [ "Tomoya Mizumoto", "Mamoru Komachi", "Masaaki Nagata", "Yuji Matsumoto." ],
      "venue" : "Proceedings of 5th International Joint Conference on",
      "citeRegEx" : "Mizumoto et al\\.,? 2011",
      "shortCiteRegEx" : "Mizumoto et al\\.",
      "year" : 2011
    }, {
      "title" : "The conll-2014 shared task on grammatical error correction",
      "author" : [ "Hwee Tou Ng", "Siew Mei Wu", "Ted Briscoe", "Christian Hadiwinoto", "Raymond Hendy Susanto", "Christopher Bryant." ],
      "venue" : "Proceedings of the Eighteenth Conference on Computational Natural",
      "citeRegEx" : "Ng et al\\.,? 2014",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2014
    }, {
      "title" : "The CoNLL2013 shared task on grammatical error correction",
      "author" : [ "Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault." ],
      "venue" : "Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared",
      "citeRegEx" : "Ng et al\\.,? 2013",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2013
    }, {
      "title" : "Gector–grammatical error correction: Tag, not rewrite",
      "author" : [ "Kostiantyn Omelianchuk", "Vitaliy Atrasevych", "Artem Chernodub", "Oleksandr Skurzhanskyi." ],
      "venue" : "arXiv preprint arXiv:2005.12592.",
      "citeRegEx" : "Omelianchuk et al\\.,? 2020",
      "shortCiteRegEx" : "Omelianchuk et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Seq2edits: Sequence transduction using span-level edit operations",
      "author" : [ "Felix Stahlberg", "Shankar Kumar." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5147–5159.",
      "citeRegEx" : "Stahlberg and Kumar.,? 2020",
      "shortCiteRegEx" : "Stahlberg and Kumar.",
      "year" : 2020
    }, {
      "title" : "Blockwise parallel decoding for deep autoregressive models",
      "author" : [ "Mitchell Stern", "Noam Shazeer", "Jakob Uszkoreit." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Stern et al\\.,? 2018",
      "shortCiteRegEx" : "Stern et al\\.",
      "year" : 2018
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "arXiv preprint arXiv:1409.3215.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jon Shlens", "Zbigniew Wojna." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818–2826.",
      "citeRegEx" : "Szegedy et al\\.,? 2016",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2016
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Improving grammatical error correction with data augmentation by editing latent representation",
      "author" : [ "Zhaohong Wan", "Xiaojun Wan", "Wenguang Wang." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 2202–2212.",
      "citeRegEx" : "Wan et al\\.,? 2020",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2020
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Ruslan Salakhutdinov", "Quoc V Le." ],
      "venue" : "arXiv preprint arXiv:1906.08237.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "A new dataset and method for automatically grading esol texts",
      "author" : [ "Helen Yannakoudakis", "Ted Briscoe", "Ben Medlock." ],
      "venue" : "Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, pages",
      "citeRegEx" : "Yannakoudakis et al\\.,? 2011",
      "shortCiteRegEx" : "Yannakoudakis et al\\.",
      "year" : 2011
    }, {
      "title" : "Sequence-to-sequence pre-training with data augmentation for sentence rewriting",
      "author" : [ "Yi Zhang", "Tao Ge", "Furu Wei", "Ming Zhou", "Xu Sun." ],
      "venue" : "arXiv preprint arXiv:1909.06002.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Overview of the nlpcc 2018 shared task: Grammatical error correction",
      "author" : [ "Yuanyuan Zhao", "Nan Jiang", "Weiwei Sun", "Xiaojun Wan." ],
      "venue" : "CCF International Conference on Natural Language Processing and Chinese Computing, pages 439–445. Springer.",
      "citeRegEx" : "Zhao et al\\.,? 2018",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving grammatical error correction with machine translation pairs",
      "author" : [ "Wangchunshu Zhou", "Tao Ge", "Chang Mu", "Ke Xu", "Furu Wei", "Ming Zhou." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings,",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : "The Transformer (Vaswani et al., 2017) has become the most popular model for Grammatical Error Correction (GEC).",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 12,
      "context" : "To reduce the computational cost for decoding, we propose to use a shallow decoder, which has proven to be an effective strategy (Kasai et al., 2020; Li et al., 2021) in neural machine translation (NMT), instead of using the Transformer with balanced encoder-decoder depth as the previous state-of-the-art Transformer models in GEC.",
      "startOffset" : 129,
      "endOffset" : 166
    }, {
      "referenceID" : 15,
      "context" : "To reduce the computational cost for decoding, we propose to use a shallow decoder, which has proven to be an effective strategy (Kasai et al., 2020; Li et al., 2021) in neural machine translation (NMT), instead of using the Transformer with balanced encoder-decoder depth as the previous state-of-the-art Transformer models in GEC.",
      "startOffset" : 129,
      "endOffset" : 166
    }, {
      "referenceID" : 1,
      "context" : "We follow recent work in English GEC to conduct experiments in the restricted training setting of BEA-2019 GEC shared task (Bryant et al., 2019): We use Lang-8 Corpus of Learner En-",
      "startOffset" : 123,
      "endOffset" : 144
    }, {
      "referenceID" : 4,
      "context" : ", 2011), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al.",
      "startOffset" : 15,
      "endOffset" : 39
    }, {
      "referenceID" : 31,
      "context" : ", 2013), FCE (Yannakoudakis et al., 2011) and W&I+LOCNESS (Granger; Bryant et al.",
      "startOffset" : 13,
      "endOffset" : 41
    }, {
      "referenceID" : 1,
      "context" : ", 2011) and W&I+LOCNESS (Granger; Bryant et al., 2019) as our GEC training data.",
      "startOffset" : 24,
      "endOffset" : 54
    }, {
      "referenceID" : 22,
      "context" : "For facilitating fair comparison in the efficiency evaluation, we follow the previous studies (Omelianchuk et al., 2020; Chen et al., 2020) which conduct GEC efficiency evaluation to use CoNLL-2014 (Ng et al.",
      "startOffset" : 94,
      "endOffset" : 139
    }, {
      "referenceID" : 2,
      "context" : "For facilitating fair comparison in the efficiency evaluation, we follow the previous studies (Omelianchuk et al., 2020; Chen et al., 2020) which conduct GEC efficiency evaluation to use CoNLL-2014 (Ng et al.",
      "startOffset" : 94,
      "endOffset" : 139
    }, {
      "referenceID" : 20,
      "context" : ", 2020) which conduct GEC efficiency evaluation to use CoNLL-2014 (Ng et al., 2014) dataset that contains 1,312 sentences as our main test set, and evaluate the speedup as well as MaxMatch (Dahlmeier and Ng, 2012) precision, recall and F0.",
      "startOffset" : 66,
      "endOffset" : 83
    }, {
      "referenceID" : 3,
      "context" : ", 2014) dataset that contains 1,312 sentences as our main test set, and evaluate the speedup as well as MaxMatch (Dahlmeier and Ng, 2012) precision, recall and F0.",
      "startOffset" : 113,
      "endOffset" : 137
    }, {
      "referenceID" : 21,
      "context" : "For validation, we use CoNLL-2013 (Ng et al., 2013) that contains 1,381 sentences as our validation set.",
      "startOffset" : 34,
      "endOffset" : 51
    }, {
      "referenceID" : 33,
      "context" : "We also test our approach on NLPCC-18 Chinese GEC shared task (Zhao et al., 2018), following their training5 and evaluation setting, to verify the effectiveness of our approach in other languages.",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 28,
      "context" : "We use the most popular GEC model architecture – Transformer (big) model (Vaswani et al., 2017) as our baseline model which has a 6-layer encoder and 6-layer decoder with 1,024 hidden",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 23,
      "context" : "We train the English GEC model using an encoder-decoder shared vocabulary of 32K Byte Pair Encoding (Sennrich et al., 2016) tokens and train the Chinese GEC model with 8.",
      "startOffset" : 100,
      "endOffset" : 123
    }, {
      "referenceID" : 10,
      "context" : "0× Levenshtein Transformer (Gu et al., 2019) No No 53.",
      "startOffset" : 27,
      "endOffset" : 44
    }, {
      "referenceID" : 12,
      "context" : "When we further increase the encoder layer and decrease the decoder layer, we see a drop in the performance of 10+2 and 11+1 despite the improved efficiency because it becomes difficult to train the Transformer with extremely imbalanced encoder and decoder well, as indicated9 by the previous work (Kasai et al., 2020; Li et al., 2021; Gu and Kong, 2020).",
      "startOffset" : 298,
      "endOffset" : 354
    }, {
      "referenceID" : 15,
      "context" : "When we further increase the encoder layer and decrease the decoder layer, we see a drop in the performance of 10+2 and 11+1 despite the improved efficiency because it becomes difficult to train the Transformer with extremely imbalanced encoder and decoder well, as indicated9 by the previous work (Kasai et al., 2020; Li et al., 2021; Gu and Kong, 2020).",
      "startOffset" : 298,
      "endOffset" : 354
    }, {
      "referenceID" : 9,
      "context" : "When we further increase the encoder layer and decrease the decoder layer, we see a drop in the performance of 10+2 and 11+1 despite the improved efficiency because it becomes difficult to train the Transformer with extremely imbalanced encoder and decoder well, as indicated9 by the previous work (Kasai et al., 2020; Li et al., 2021; Gu and Kong, 2020).",
      "startOffset" : 298,
      "endOffset" : 354
    }, {
      "referenceID" : 30,
      "context" : ", 2019) and XLNet (Yang et al., 2019)) with its multi-stage training strategy.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 14,
      "context" : "Following GECToR’s recipe, we leverage the pretrained model BART (Lewis et al., 2019)",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 15,
      "context" : "to initialize a 12+2 model which proves to work well in NMT (Li et al., 2021) despite more parameters, and apply the multi-stage fine-tuning strategy used in Stahlberg and Kumar (2020).",
      "startOffset" : 60,
      "endOffset" : 77
    }, {
      "referenceID" : 26,
      "context" : "The state-of-the-art of GEC has been significantly advanced owing to the tremendous success of seq2seq learning (Sutskever et al., 2014) and the Transformer (Vaswani et al.",
      "startOffset" : 112,
      "endOffset" : 136
    }, {
      "referenceID" : 6,
      "context" : ", 2020), most methods that improve performance (Ge et al., 2018b; Kaneko et al., 2020) introduce additional computational cost and thus",
      "startOffset" : 47,
      "endOffset" : 86
    }, {
      "referenceID" : 11,
      "context" : ", 2020), most methods that improve performance (Ge et al., 2018b; Kaneko et al., 2020) introduce additional computational cost and thus",
      "startOffset" : 47,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "Among them, PIE (Awasthi et al., 2019) and GECToR (Omelianchuk et al.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 22,
      "context" : ", 2019) and GECToR (Omelianchuk et al., 2020) propose to accel-",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 7,
      "context" : "In contrast, our proposed shallow aggressive decoding tries to accelerate the model inference through parallel autoregressive decoding which is related to some previous work (Ghazvininejad et al., 2019; Stern et al., 2018) in neural machine translation (NMT), and the imbalanced encoder-decoder architecture which",
      "startOffset" : 174,
      "endOffset" : 222
    }, {
      "referenceID" : 25,
      "context" : "In contrast, our proposed shallow aggressive decoding tries to accelerate the model inference through parallel autoregressive decoding which is related to some previous work (Ghazvininejad et al., 2019; Stern et al., 2018) in neural machine translation (NMT), and the imbalanced encoder-decoder architecture which",
      "startOffset" : 174,
      "endOffset" : 222
    } ],
    "year" : 2021,
    "abstractText" : "In this paper, we propose Shallow Aggressive Decoding (SAD) to improve the online inference efficiency of the Transformer for instantaneous Grammatical Error Correction (GEC). SAD optimizes the online inference efficiency for GEC by two innovations: 1) it aggressively decodes as many tokens as possible in parallel instead of always decoding only one token in each step to improve computational parallelism; 2) it uses a shallow decoder instead of the conventional Transformer architecture with balanced encoder-decoder depth to reduce the computational cost during inference. Experiments in both English and Chinese GEC benchmarks show that aggressive decoding could yield the same predictions as greedy decoding but with a significant speedup for online inference. Its combination with the shallow decoder could offer an even higher online inference speedup over the powerful Transformer baseline without quality loss. Not only does our approach allow a single model to achieve the state-of-the-art results in English GEC benchmarks: 66.4 F0.5 in the CoNLL14 and 72.9 F0.5 in the BEA-19 test set with an almost 10× online inference speedup over the Transformer-big model, but also it is easily adapted to other languages. Our code is available at https://github.com/AutoTemp/ Shallow-Aggressive-Decoding.",
    "creator" : "LaTeX with hyperref"
  }
}