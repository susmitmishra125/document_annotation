{
  "name" : "2021.acl-long.299.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "TAN-NTM: Topic Attention Networks for Neural Topic Modeling",
    "authors" : [ "Madhur Panwar", "Shashank Shailabh", "Milan Aggarwal", "Balaji Krishnamurthy" ],
    "emails" : [ "mdrpanwar@gmail.com,", "shailabhshashank@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3865–3880\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3865"
    }, {
      "heading" : "1 Introduction",
      "text" : "Topic models (Steyvers and Griffiths, 2007) have been popularly used to extract abstract topics which occur commonly across documents in a corpus. Each topic is interpreted as a group of semantically coherent words that represent a common concept. In addition to gaining insights from unstructured texts, topic models have been used in several tasks\n∗equal contribution †work done during summer internship at Adobe\nof practical importance such as learning text representations for document classification (Nan et al., 2019), keyphrase extraction (Wang et al., 2019b), understanding reviews for e-commerce recommendations (Jin et al., 2018), semantic similarity detection between texts (Peinelt et al., 2020) etc.\nEarly works on topic discovery include statistical methods such as Latent Semantic Analysis (Deerwester et al., 1990), Latent Dirichlet Allocation (LDA) (Blei et al., 2003) which approximates each topic as a probability distribution over word vocabulary (known as topic-word distribution) and performs approximate inference over documenttopic and topic-word distributions through Variational Bayes. This was followed by Markov Chain Monte Carlo (MCMC) (Andrieu et al., 2003) based inference algorithm - Collapsed Gibbs sampling (Griffiths and Steyvers, 2004). These methods require an expensive iterative inference step which has to be performed for each document. This was circumvented through introduction of deep neural networks and Variational Autoencoders (VAE) (Kingma and Welling, 2013), where variational inference can be performed in single forward pass.\nNeural variational inference topic models (Miao et al., 2017; Ding et al., 2018; Srivastava and Sutton, 2017) commonly convert a document to Bagof-Words (BoW) determined on the basis of frequency count of each vocabulary token in the document. The BoW input is processed through an MLP followed by variational inference which samples a latent document-topic vector. A decoder network then reconstructs original BoW using latent document-topic vector through topic-word distribution (TWD). VAE based neural topic models can be categorised on the basis of prior enforced on latent document-topic distribution. Methods such as NVDM (Miao et al., 2016), NTM-R (Ding et al., 2018), NVDM-GSM (Miao et al., 2017) use the Gaussian prior. NVLDA and ProdLDA (Srivastava\nand Sutton, 2017) use approximation to the Dirichlet prior which enables model to capture the fact that a document stems from a sparse set of topics.\nHowever, improving document encoding in topic models in order to capture document distribution and semantics better has not been explored much. In this work, we build upon VAE based topic model and propose a novel framework TAN-NTM: Topic Attention Networks for Neural Topic Modeling which process the sequence of tokens in input document through an LSTM (Hochreiter and Schmidhuber, 1997) whose contextual outputs are attended using Topic-Word Distribution (TWD). We hypothesise that TWD (being learned by the model) can be factored in the attention mechanism (Bahdanau et al., 2014) to enable the model to attend on the tokens which convey topic related information and cues. We perform separate attention for each topic using its corresponding word probability distribution and obtain the topic-wise context vectors. The learned word embeddings and TWD are used to devise a mechanism to determine topic weights representing the proportion of each topic in the document. The topic weights are used to aggregate topic-wise context vectors. The composed context vector is then used to perform variational inference followed by the BoW decoding. We perform extensive ablations to compare TAN-NTM variants and different ways of composing the topicwise context vectors.\nFor evaluation, we compute commonly used NPMI coherence (Aletras and Stevenson, 2013) which measures the extent to which most probable words in a topic are semantically related to each other. We compare our TAN-NTM model with several state-of-the-art topic models (statistical (Blei et al., 2003; Griffiths and Steyvers, 2004), neural VAE (Srivastava and Sutton, 2017; Wu et al., 2020) and non-variational inference based neural model (Nan et al., 2019)) outperforming them on three benchmark datasets of varying scale and complexity: 20Newsgroups (20NG) (Lang, 1995), Yelp Review Polarity and AGNews (Zhang et al., 2015). We verify that our model learns better document feature representations and latent document-topic vectors by achieving a higher document classification accuracy over the baseline topic models. Further, topic models have previously been used to improve supervised keyphrase generation (Wang et al., 2019b). We show that TAN-NTM can be adapted to modify topic assisted keyphrase gener-\nation achieving SOTA performance on StackExchange and Weibo datasets. Our contributions can be summarised as:\n• We propose a document encoding framework for topic modeling which leverages the topicword distribution to perform attention effectively in a topic aware manner.\n• Our proposed model achieves better NPMI coherence (∼9-15 percentage improvement over the scores of existing best topic models) on various benchmark datasets.\n• We show that the topic guided attention results in better latent document-topic features achieving a higher document classification accuracy than the baseline topic models.\n• We show that our topic model encoder can be adapted to improve the topic guided supervised keyphrase generation achieving improved performance on this task."
    }, {
      "heading" : "2 Related Work",
      "text" : "Development of neural networks has paved path for Variational Autoencoders (VAE) (Kingma and Welling, 2013) which enables performing Variational Inference (VI) efficiently. The VAE-based topic models use a prior distribution to approximate the posterior for latent document-topic space and compute the Evidence Lower Bound (ELBO) using the reparametrization trick. Since our work is based on variational inference, we use ProdLDA and NVLDA (Srivastava and Sutton, 2017) as baselines for comparison. The Dirichlet distribution has been commonly considered as a suitable prior on the latent document-topic space since it captures the property that a document belongs to a sparse subset of topics. However, in order to enforce the Dirichlet prior, VAE methods have to resort to approximations of the Dirichlet distribution.\nSeveral works have proposed solutions to impose the Dirichlet prior effectively. Rezaee and Ferraro (2020) enforces Dirichlet prior using VI without reparametrization trick through word-level topic assignments. Some works address the sparsitysmoothness trade-off in dirichlet distribution by factoring dirichlet parameter vector as a product of two vectors (Burkhardt and Kramer, 2019). Wasserstein Autoencoders (WAE) (Tolstikhin et al., 2017) have led to the development of non-variational inference based topic model: Wasserstein-LDA (WLDA) which minimizes the wasserstein distance, a\ntype of Optimal Transport (OT) distance, by leveraging distribution matching to the Dirichlet prior. We compare our work with W-LDA as a baseline. Zhao et al. (2021) proposed an OT based topic model which directly calculates topic-word distribution without a decoder.\nAdversarial Topic Model (ATM) (Wang et al., 2019a) was proposed based on GAN (Generative Adversarial Network) (Goodfellow et al., 2014) but it cannot infer document-topic distribution. A major advantage of W-LDA over ATM is distribution matching in document-topic space. Bidirectional Adversarial Topic model (BAT) (Wang et al., 2020) employs a bilateral transformation between document-word and document-topic distribution, while Hu et al. (2020) uses CycleGAN (Zhu et al., 2017) for unsupervised transfer between documentword and document-topic distribution.\nHierarchical topic models (Viegas et al., 2020) utilize relationships among the latent topics. Supervised topic models have been explored previously where the topic model is trained through human feedback (Kumar et al., 2019) or with a task specific network simultaneously such that topic extraction is guided through task labels (Pergola et al., 2019; Wang and Yang, 2020). Card et al. (2018) leverages document metadata but without metadata their method is same as ProdLDA which is our baseline. Topic modeling on document networks has been done leveraging relational links between documents (Zhang and Lauw, 2020; Zhou et al., 2020). However our problem setting is completely different, we extract topics from documents in unsupervised way where document links/metadata/labels either don’t exist or are not used to extract the topics.\nSome very recent works use pre-trained BERT (Devlin et al., 2019) either to leverage improved text representations (Bianchi et al., 2020; Sia et al., 2020) or to augment topic model through knowledge distillation (Hoyle et al., 2020a). Zhu et al. (2020) and Dieng et al. (2020) jointly train words and topics in a shared embedding space. However, we train topic-word distribution as part of our model, embed it using word embeddings being learned and use resultant topic embeddings to perform attention over sequentially processed tokens. iDocNade (Gupta et al., 2019) is an autoregressive topic model for short texts utilizing pre-trained embeddings as distributional prior. However, it attains poorer topic coherence than ProdLDA and GNB-\nNTM as shown in Wu et al. (2020).\nSome works have attempted to use other prior distributions such as Zhang et al. (2018) uses the Weibull prior, Thibaux and Jordan (2007) uses the beta distribution. Gamma Negative BinomialNeural Topic Model (GNB-NTM) (Wu et al., 2020) is one of the recent neural variational topic models which attempt to combine VI with mixed counting models. Mixed counting models can better model hierarchically dependent and over-dispersed random variables while implicitly introducing nonnegative constraints in topic modeling. GNB-NTM uses reparameterization of Gamma distribution and Gaussian approximation of Poisson distribution. We use their model as a baseline for our work.\nTopic models have been used with sequence encoders such as LSTM in applications like user activity modeling (Zaheer et al., 2017). Dieng et al. (2016) employs an RNN to detect stop words and merges its output with document-topic vector for next word prediction. Gururangan et al. (2019) uses a VAE pre-trained through topic modeling to perform text classification. We perform document classification and compare our model’s accuracy with the accuracy of VAE based and other topic models. LTMF (Jin et al., 2018) combines text features processed through an LSTM with a topic model for review based recommendations. Fundamentally different from these, we use topic-word distribution to attend on sequentially processed tokens via novel topic guided attention for performing variational inference, learning better document-topic features and improving topic modeling.\nA key application of topic models is supervised keyphrase generation. Some of the existing neural keyphrase generation methods include SEQ-TAG (Zhang et al., 2016) based on sequence tagging, SEQ2SEQ-CORR (Chen et al., 2018) based on seq2seq model without copy mechanism and SEQ2SEQ-COPY (Meng et al., 2017) which additionally uses copy mechanism. TopicAware Keyphrase Generation (TAKG) (Wang et al., 2019b) is a seq2seq based neural keyphrase generation framework for social media language. TAKG uses a neural topic model in Miao et al. (2017) and a keyphrase generation (KG) module which is conditioned on latent document-topic vector from the topic model. We adapt our proposed topic model to TAKG to improve keyphrase generation and discuss it in detail later in the Experiments section."
    }, {
      "heading" : "3 Background",
      "text" : "LDA is a generative statistical model and assumes that each document is a distribution over a fixed number of topics (say K) and that each topic is a distribution of words over the entire vocabulary. LDA proposes an iterative process of document generation where for each document d, we draw a topic distribution θ from Dirichlet(α) distribution. For each word in d at index i, we sample a topic ti from Multinomial(θ) distribution. wi is sampled from p(wi|ti, β) distribution which is a multinomial probability conditioned on topic ti. Given the document corpus and the parameters α and β, we need the joint probability distribution of a topic mixture θ, a set of K topics t, and a set of n words w. This is given analytically by an intractable integral. The solution is to use Variational Inference wherein this problem is converted into an optimization problem for finding various parameters that minimize the KL divergence between the prior and the posterior distribution.\nThis idea is leveraged at scale by the use of Variational Autoencoders. The encoder processes BoW vector of the document xbow by an MLP (Multi Layer Perceptron) which then forks into two independently trainable layers to yield zµ & zlog σ2 . Then a re-parametrization trick is employed to sample the latent vector z from a logistic-normal distribution (resulting from an approximation of\nDirichlet distribution). This is essential since backpropagation through a sampling node is infeasible. z is then used by decoder’s single dense layer D to yield the reconstructed BoW xrec. The objective function has two terms: (a) Kullback–Leibler (KL) Divergence Term - to match the variational posterior over latent variables with the prior and (b) Reconstruction Term - categorical cross entropy loss between xbow & xrec.\nLNTM = DKL(p(z) || q(z|x))− Eq(z|x)[p(x|z)]\nOur methodology improves upon the document encoder and introduces a topic guided attention whose output is used to sample z. We use the same formulation of decoder as used in ProdLDA."
    }, {
      "heading" : "4 Methodology",
      "text" : "In this section, we describe the details of our framework where we leverage the topic-word distribution to perform topic guided attention over tokens in a document. Given a collection C with |C| documents {x1,x2, ..,x|C|}, we process each document x into BoW vector xbow ∈ R|V | and as a token sequence xseq, where V represents the vocabulary. As shown in step A in figure 1, each word wj ∈ xseq is embedded as ej ∈ RE through an embedding layer E ∈ R|V |×E (E = Embedding Dimension) initialised with GloVe (Pennington et al., 2014). The embedded sequence {ej}|x|j=1,\nwhere |x| is the number of tokens in x, is processed through a sequence encoder LSTM (Hochreiter and Schmidhuber, 1997) to obtain the corresponding hidden states hj ∈ RH and cell states sj ∈ RH (step B in figure 1):\nhj , sj = fLSTM (ej , (hj−1, sj−1))\nwhere H is LSTM’s hidden size. We construct a memory bank M = 〈h1,h2, ...,h|x|〉 which is then used to perform topic-guided attention (step C in figure 1). The output vector of the attention module is used to derive prior distribution parameters zµ & zlog σ2 (as in VAE) through two linear layers. Using the re-parameterisation trick, we sample the latent document-topic vector z, which is then given as input to BoW decoder linear layer D that outputs the reconstructed BoW xrec (step D in figure 1). Objective function is same as in VAE setting, involving a reconstruction loss term between xrec & xbow and KL divergence between the prior (laplace approximation to Dirichlet prior as in ProdLDA) and posterior. We now discuss the details of our Topic Attention Network."
    }, {
      "heading" : "4.1 TAN: Topic Attention Network",
      "text" : "We intend the model to attend on document words in a manner such that the resultant attention is distributed according to the semantics of the topics relevant to the document. We hypothesize that this can enable the model to encode better document features while capturing the underlying latent document-topic representations. The topic-word distribution Tw represents the affinity of each topic towards words in the vocabulary (which is used to interpret the semantics of each topic). Therefore, we factor Tw ∈ RK×|V | into the attention mechanism, where K denotes the number of topics. The topic-aware attention encoder and topic-word distribution influence each other during training which consequently results in convergence to better topics as discussed in detail in Experiments section.\nSpecifically, we perform attention on document sequence of tokens for each topic using the embedded representation of the topics TE ∈ RK×E :\nTE = TwE, [topic embeddings]\nTw = softmax(D), [topic-word distribution]\nwhere D ∈ RK×V is the decoder layer which is used to reconstruct xbow from the sampled latent\ndocument-topic representation z as the final step D in Figure 1. The topic embeddings are then used to determine the attention alignment matrix A ∈ R|x|×K between each topic k ∈ {1, 2, ...,K} and words in the document such that:\nAjk = exp(score((TE)k,hj))∑|x| j′=1 exp(score((TE)k,hj′)) ,\nscore((TE)k,hj) = vA >tanh(WA[(TE)k;hj ])\nwhere vA ∈ RP , WA ∈ RP×(E+H), (TE)k ∈ RE is the embedded representation of the kth topic and ; is the concatenation operation. We then determine topic-wise context vector corresponding to each topic as:\nCT = |x|∑ j=1 Aj⊗hj , [topic-wise context matrix]\nwhere ⊗ denotes outer product. Note that Aj ∈ RK (jth row of matrix A) is a K - dimensional vector and hj is aH - dimensional vector, therefore Aj ⊗ hj for each j yields a matrix of order K × H , hence CT ∈ RK×H . The final aggregated context vector c is computed as a weighted average over all rows of CT (each row representing each topic specific context vector) with document-topic proportion vector td as weights:\nc = K∑ k=1 (td)i(CT)k\nwhere, (td)k is a scalar, (CT)k ∈ RH denotes the kth row of matrix CT & td is the documenttopic distribution which signifies the topic proportions in a document. To compute it, we first normalize the document BoW vector xbow and embed it using the embedding matrix E, followed by multiplication with topic embedding TE ∈ RK×E :\nxnorm = xbow∑|V |\ni=1(xbow)i , [normalized BoW]\nxemb = x > normE, [document embedding]\ntd = softmax(TE xemb), [document-topic dist.]\nwhere xnorm ∈ R|V |, xemb ∈ RE & td ∈ RK . The context vector c is the output of our topic guided attention module which is then used for sampling the latent documents-topic vector followed by the BoW decoding as done in traditional VAE based topic models.\nWe call this framework as Weighted-TAN or WTAN where the context vector c is a weighted sum of topic-wise context vectors. We also propose another model called Top-TAN or T-TAN where we use context vector of the topic with largest proportion in td as c. It has been experimentally observed that doing so yields a model which generates more coherent topics. First, we find the index m of most probable topic in td. The context vector c is then the row corresponding to index m in matrix CT."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Datasets",
      "text" : "1. Topic Quality: We evaluate and compare quality of our proposed topic model on three benchmark datasets - 20Newsgroups (20NG)1 (Lang, 1995), AGNews (Zhang et al., 2015) and Yelp Review Polarity (YRP)2 - which are of varying complexity and scale in terms of number of documents, vocabulary size and average length of text after preprocessing3. Table 1 summarises statistics related to these datasets used for evaluating topics quality.\n2. Keyphrase Generation: Neural Topic Model (NTM) has been used to improve the task of supervised keyphrase generation (Wang et al., 2019b). To further highlight the efficacy of our proposed encoding framework in providing better document-topic vectors, we modify encoder module of NTM with our proposed TAN-NTM and compare the performance on StackExchange and Weibo Datasets4."
    }, {
      "heading" : "5.2 Implementation and Training Details",
      "text" : "Documents in AGNews are padded upto a maximum length of 50, while those in 20NG and YRP are padded upto 200 tokens. Documents with longer lengths are truncated. These values were chosen such that ∼ 80− 99% of all documents in each dataset were included without truncation. We\n1Data link for 20NG dataset 2Data link for AGNews and YRP datasets 3We provide our detailed preprocessing steps in Appendix\nA.1 and release processed data to standardise it. 4The dataset details can be found in the baseline paper\nuse batch size of 100, Adam Optimizer (Kingma and Ba, 2015) with β1 = 0.99, β2 = 0.999 and = 10−8 and train each model for 200 epochs. For all models except T-TAN, learning rate was fixed at 0.002 ([0.001, 0.003], 5)5. T-TAN converges relatively faster than other models, therefore for smooth training, we decay its learning rate every epoch using exponential staircase scheduler with initial learning rate = 0.002 and decay rate = 0.96. The number of topics K = 50, a value widely used in literature. We perform hyper-parameter tuning manually to determine the hidden dimension value of various layers: E = 200 ([100, 300], 5), H = 450 ([300, 900], 10) and P = 350 ([10, 400], 10). The weight matrices of all dense layers are Xavier initialized, while bias terms are initialized with zeros. All our proposed models and baselines are trained on a machine with 32 virtual CPUs, single NVIDIA Tesla V 100 GPU and 240 GB RAM."
    }, {
      "heading" : "5.3 Comparison with baselines",
      "text" : "We compare our TAN-NTM with various baselines in table 2 that can be enumerated as (please refer to introduction and related work for their details): 1) LDA (C.G.): Statistical method (McCallum, 2002) which performs LDA using collapsed Gibbs6 sampling. 2) ProdLDA and 3) NVLDA (Srivastava and Sutton, 2017): Neural Variational Inference methods which use approximation to Dirichlet prior7. 4) W-LDA (Nan et al., 2019) which is a non variational inference based neural model using wassestein autoencoder8. 5) NB-NTM and 6) GNB-NTM: Methods using negative binomial and gamma negative binomial distribution as priors for topic discovery9(Wu et al., 2020) respectively.\nWe could not compare with other methods whose official error-free source code is not publicly available yet. We train and evaluate the baseline methods on same data as used for our method using NPMI coherence10 (Aletras and Stevenson, 2013). It computes the semantic relatedness between topL words in a given topic through determining similarity between their word embeddings trained over the\n5V ([a, b], t) means t values from [a, b] range tried for this hyper-parameter, of which V yielded best NPMI coherence.\n6https://pypi.org/project/lda/ 7Code for ProdLDA and NVLDA 8https://github.com/awslabs/w-lda 9We thank authors for providing code and parameter info.\n10Repo used to calculate NPMI. Please refer to Appendix B for a detailed discussion on choice of evaluation metric.\ncorpus used for topic modeling and reports average over topics. For W-LDA, we refer to their original paper to select dataset specific hyper-parameter values while training the model. As can be seen in table 2, our proposed T-TAN model performs significantly better than previous topic models uniformly on all datasets achieving a better NPMI (measured on a scale of -1 to 1) by a margin of 0.028 (10.44%) on 20NG, 0.047 (14.59%) on AGNews and 0.022 (8.8%) on YRP, where percentage improvements are determined over the best baseline score. Even though W-TAN does not uniformly performs better than all baselines on all datasets, it achieves better score than all baselines on AGNews and performs comparably on remaining two datasets.\nFor a more exhaustive comparison, we also evaluate our model’s performance on 20NG dataset (which is the common dataset with GNB-NTM (Wu et al., 2020)) using the NPMI metric from GNB-NTM’s code. The NPMI coherence of our model using their criteria is 0.395 which is better than GNB-NTM’s score of 0.375 (as reported in their paper). However, we would like to highlight that GNB-NTM’s computation of NPMI metric uses relaxed window size, whereas the metric used by us (Lau et al., 2014) uses much stricter window size while determining word co-occurrence counts within a document. Lau et al. (2014) is a much more common and widely used way of computing the NPMI coherence and evaluating topic models."
    }, {
      "heading" : "5.3.1 Document Classification",
      "text" : "In addition to evaluating our framework in terms of topic coherence, we also compare it with the baselines on the downstream task of document classification. Topic models have been used as text\nfeature extractors to perform classification (Nan et al., 2019). We analyse the quality of encoded document representations and predictive capacity of latent document-topic features generated by our model and compare it with existing topic models11. We train the topic model setting number of topics to 50 and freeze its weights. The trained topic model is then used to infer latent document-topic features. We then separately train a single layer linear classifier through cross entropy loss on the training split using the document-topic vectors as input and Adam optimizer at a learning rate of 0.01.\nWe report classification accuracy on the test split of 20NG, AGNews and YRP datasets (comprising of 20, 4 and 2 classes respectively) in Table 3. The document-topic features provided by TTAN achieve best accuracy on AGNews (1.43% improvement over most performant baseline) with most significant improvement of 3.06% on 20NG which shows our model learns better document features. T-TAN performs almost the same as the best baseline on YRP. Further, to analyse the predictive performance of top topic attention based context vector, we use it instead of latent documenttopic vector to perform classification which further boosts accuracy leading to an improvement of ∼6.9% on 20NG, ∼3.1% on AGNews and ∼1.3% on YRP datasets over the baselines."
    }, {
      "heading" : "5.3.2 Running Time Analysis",
      "text" : "We compare the running time of our method with baselines in terms of average time taken (in seconds) for performing a forward pass through the\n11Our aim is to analyse document-topic features among topic models only and not to compare with other non-topic model based generic text classifiers.\nmodel, where the average is taken over 10000 passes. Our TAN-NTM (implemented in tensorflow) takes 0.087s, 0.027s and 0.093s on 20NG, AGNews and YRP datasets respectively. Since TAN-NTM processes the input documents as a sequence of tokens through an LSTM, its running time is proportional to the document lengths which vary according to the dataset. The running time for baseline methods are: ProdLDA - 0.012s (implemented in tensorflow), W-LDA - 0.003s (implemented in mxnet) and GNB-NTM - 0.003s (implemented in pytorch). For baseline methods, we have used their original code implementations. We found that the running time of baseline models is independent of the dataset. This is because they use the Bag-of-Words (BoW) representation of the documents. The sequential processing in TAN-NTM is the reason for increased running time of our models compared to the baselines. In the case of AGNews, since the documents are of lesser lengths than 20NG and YRP, the running time of our TANNTM is relatively less for AGNews. Further, the running time of other ablation variants (introduced in section 5.4) of our method on 20NG, AGNews and YRP datasets respectively are: 1) only LSTM - 0.083s, 0.033s and 0.091s ; 2) vanilla attn - 0.088s, 0.037s and 0.095s."
    }, {
      "heading" : "5.4 Ablation Studies",
      "text" : "In this section, we compare the performance of different variants of our model namely, 1) only LSTM: final hidden state is used to derive sampling parameters zµ & zlog σ2 , 2) vanilla attn: final hidden state (w/o topic-word distribution) is used as query to perform attention (Bahdanau et al., 2014) on LSTM outputs such that context vector z is used for VI, 3) W-TAN: Weighted Topic Attention Network, 4) T-TAN: Top Topic Attention Network and 5) T-TAN w/o (without) GloVe: embedding layer in T-TAN is randomly initialised.\nTable 4 compares the topic coherence scores of these different ablation methods on 20NG, AGNews and YRP. As can be seen, applying attention performs better than simple LSTM model. The weighted TAN performs better than vanilla attention model, however, T-TAN uniformly provides the best coherence scores across all the datasets compared to all other methods. This shows that performing attention corresponding to the most prominent topic in a document results in more coherent topics. Further, we perform an ablation to\nstudy the effect of using pre-trained embeddings for T-TAN where it can be seen using Glove for initialising word embeddings results in improved NPMI as compared to training T-TAN initialised with random uniform embeddings (T-TAN w/o GloVe)12."
    }, {
      "heading" : "5.5 Qualitative Analysis",
      "text" : "To verify performance of T-TAN qualitatively, we display few topics generated by ProdLDA and TTAN on AGNews in Figure 2. ProdLDA achieves best score among baselines on AGNews. Consider comparison 1 in Figure 2: ProdLDA produces four topics corresponding to space, mixing them with nuclear weapons, while T-TAN produces two separate topics for both of these concepts. In second comparison, we see that ProdLDA has problems distinguishing between closely related topics (football, olympics, cricket) and mixes them while TTAN produces three coherent topics."
    }, {
      "heading" : "5.6 TAKG: Topic Aware Keyphrase Generation",
      "text" : "We further analyse the impact of our proposed framework on another downstream task where the\n12We also trained embeddings from scratch for other variants but coherence score remained unaffected.\ntask specific model is assisted by the topic model and both can be trained in an end-to-end manner. For this, we discuss TAKG (Wang et al., 2019b) and how our proposed topic model encoder can be adapted to achieve better performance on supervised keyphrase generation from textual posts. TAKG13 comprises of two sub-modules: (1) a topic model based on NVDM-GSM (as discussed in Introduction) using BoW as input to the encoder and (2) a Seq2Seq based model for keyphrase generation. Both modules have an encoder and a decoder of their own. Keyphrase generation module uses sequence input which is processed by bidirectional GRU (Cho et al., 2014) to encode input sequence. The keyphrase generation decoder uses unidirectional GRU which attends on encoder outputs and takes the latent document-topic vector from the topic model as input in a differentiable manner. Since topic model trains slower than keyphrase generation module, the topic model is warmed up for some epochs separately and then jointly trained with keyphrase generation. Please refer to original paper (Wang et al., 2019b) for more details.\nWe adapted our proposed topic model framework by changing the architecture of encoder in the topic model of TAKG, replacing it with W-TAN and T-TAN. The change subsequently results in better latent document-topic representation depicted by better performance on keyphrase generation as shown in Table 5 where the improved topic model encoding framework results in ∼1-2% improvement in F1 and MAP (mean average precision) on StackExchange and Weibo datasets compared to TAKG. Here, even though TAKG with T-TAN performs marginally better than the baseline, TAKG with W-TAN uniformly performs much better."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this work, we propose Topic Attention Network based Neural Topic Modeling framework: TAN-\n13We use their code and data (link) to conduct experiments.\nNTM to discover topics in a document corpus by performing attention on sequentially processed tokens in a topic guided manner. Attention is performed effectively by factoring Topic-word distribution (TWD) into attention mechanism. We compare different variants of our method through ablations and conclude that processing tokens sequentially without attention or applying attention without TWD gives inferior performance. Our TAN-NTM model generates more coherent topics compared to state-of-the-art topic models on several benchmark datasets. Our model encodes better latent document-topic features as validated through better performance on document classification and supervised keyphrase generation tasks. As future work, we would like to explore our framework with other sequence encoders such as Transformers, BERT etc. for topic modeling."
    }, {
      "heading" : "A Further Implementation Details",
      "text" : "A.1 Preprocessing\nFor 20NG dataset, we used its preprocessed version downloaded from ProdLDA’s (Srivastava and Sutton, 2017) repository14, whereas AGNews and YRP datasets were downloaded from this15 link. These two datasets contain train.csv and test.csv files. The csv files of YRP contain a document body only, whereas the csv files for AGNews contain a document title as well as a document body. For uniformity, we concatenate the title and body in the csv files of AGNews and keep it as a single field. The documents from train.csv and test.csv are then read into train and test lists which are passed to PREPROCESS function of Algorithm 1 for preprocessing.\nStepwise working of Algorithm 1 is expained in the following points:\n• Before invoking the PREPROCESS function, we initialize the data sampler by a fixed seed so that preprocessing yields the same result when run multiple times.\n• For each dataset, we randomly sample tr size documents (as mentioned in Table 6) from the train list in step 2. These values of tr size are taken from Table 1 of W-LDA paper (Nan et al., 2019). Note that # Train in Table 1 represents the number of training documents after preprocessing. Of the tr size documents, some documents may be removed during preprocessing, therefore # Train may be less than tr size.\n• In steps 3 through 8, we prune the train and test documents by invoking the PRUNE DOC function from Algorithm 2. First, we remove the control characters from the documents viz. ‘\\n’, ‘\\t’, and ‘\\r’ (For YRP, we additionally remove ‘\\\\t’, ‘\\\\n’, and ‘\\\\r’). Next, we remove the numeric tokens16 from the documents, convert them to lowercase and lemmatize each of their tokens using the\n14Data link for 20NG dataset 15Data link for AGNews and YRP datasets 16Fully numeric tokens e.g. ‘1487’, ‘1947’, etc. are removed, whereas partially numeric tokens e.g. ‘G47’, ‘DE1080’, etc. are retained.\nNLTK’s (Bird et al., 2009) WordNetLemmatizer. Finally, we remove punctuations17 and tokens containing any non-ASCII character.\n• In steps 9 through 15, we construct the vocabulary vocab, which is a mapping of each token to its occurrence count among the pruned training documents tr pruned. We only count a token if it is not an English stopword18\nand its length is between 3 and 15 (inclusive).\n• Steps 16 through 19 filter the vocab by removing tokens whose total occurrence count is less than num below or whose occurrence count per training document is greater than fr abv, where the values of num below and fr abv are taken from Table 6. For YRP, we follow the W-LDA paper (Nan et al., 2019) and restrict its vocab to only contain top 20, 000 most occurring tokens.\n• Steps 20 through 24 construct the token-toindex map w2idx by mapping each token in vocab to an index starting from 1. Next, we map the padding token to index 0 (Step 25).\n• The final step in the preprocessing is to encode the train and test documents by mapping each of their tokens to corresponding indices according to w2idx. This is done by the ENCODE function of Algorithm 2 which is invoked in steps 26 and 27.\n17Any of the following 32 characters is regarded as a punctuation !”#$%&’()*+,-./:;<=>?@[\\]ˆ `{|}∼\n18Gensim’s (Řehůřek and Sojka, 2010) list of English stopwords is used.\nAlgorithm 1 Pseudocode for preprocessing AGNews and YRP datasets.\n1: function PREPROCESS(train, test) 2: train← train.sample(tr size) 3: tr pruned← [] . empty list 4: te pruned← [] . empty list\n5: for document d in train do 6: tr pruned.append(PRUNE DOC(d))\n7: for document d in test do 8: te pruned.append(PRUNE DOC(d))\n9: vocab← mapping of each token to 0 10: num doc← len(tr pruned)\n11: for document d in tr pruned do 12: for token t in d do 13: if t /∈ stopwords and 14: len(t) ∈ [3, 15] then 15: vocab[t]← vocab[t] +1 16: for token t in vocab do 17: if vocab[t] < num below or 18: vocab[t]/num doc > fr abv then 19: vocab[t].remove(t)\n20: i← 1 21: w2idx← empty map 22: for token t in vocab do 23: w2idx[t]= i 24: i← i+ 1 25: w2idx[0]← PAD\n26: trD← ENCODE(tr pruned, w2idx) 27: teD← ENCODE(te pruned, w2idx) 28: return trD, teD, w2idx\nA.2 Learning Rate Scheduler As mentioned in section 5.2, we use a learning rate scheduler while training T-TAN. The rate decay follows the following equation:\nlrate = init rate ∗ decay rate ⌊ train step decay steps ⌋\nThis is an exponential staircase function which enables decrease in learning rate every epoch during training.\nWe initialize the learning rate by init rate = 0.002 and use decay rate = 0.96. train step is a\nAlgorithm 2 Pseudocode for pruning the document and encoding it given a token-to-index mapping.\n1: function PRUNE DOC(doc) 2: doc← rm control(doc) 3: doc← rm numeric(doc) 4: doc← lowercase(doc) 5: doc← lemmatize(doc) 6: doc← rm punctuations(doc) 7: doc← rm non ASCII(doc) 8: return doc\n9: function ENCODE(doc list, w2idx) 10: encDocList← [] 11: for document d in doc list do 12: ecDoc ← [] 13: for token t in d do 14: ecDoc.append(w2idx[t]) 15: encDocList.append(ecDoc) 16: return encDocList\nglobal counter of training steps and decay steps = #train docs batch size is the number of training steps taken per epoch. Therefore, effectively, the rate remains constant for all training steps in an epoch and decreases exponentially as per the above equation once the epoch completes.\nA.3 Regularization We employ two types of regularization during training:\n• Dropout: We apply dropout (Srivastava et al., 2014) to z with the rate of Pdrop = 0.6 before it is processed by the decoder for reconstruction.\n• Batch Normalization (BN): We apply a BN (Ioffe and Szegedy, 2015) to the inputs of decoder layer and to the inputs of layers being trained for zµ & zlog σ2 , with = 0.001 and decay = 0.999."
    }, {
      "heading" : "B Evaluation Metrics",
      "text" : "Topic models have been evaluated using various metrics namely perplexity, topic coherence, topic uniqueness etc. However, due to the absence of a gold standard for the unsupervised task of topic modeling, all of that metrics have received criticism by the community. Therefore, a consensus on the best metric has not been reached so far. Perplexity has been found to be negatively correlated to\ntopic quality and human judgements (Chang et al., 2009). This work presents experimental results which show that in some cases models with higher perplexity were preferred by human subjects.\nTopic Uniqueness (Nan et al., 2019) quantifies the intersection among topic words globally. However, it also suffers from drawbacks and often penalizes a model incorrectly (Hoyle et al., 2020b). Firstly, it does not account for ranking of intersected words in the topics. Secondly, it fails to distinguish between the following two scenarios: 1) When the intersected words in one topic are all present in a second topic (signifying strong similarity i.e. these two topics are essentially identical) and, 2) When the intersected words of one topic are spread across all the other topics (signifying weak similarity i.e. the topics are diffused). The first is a problem related to uniqueness among topics while second is a problem related to word intrusion in topics. (Chang et al., 2009) conducted experiments with human subjects on two tasks: word intrusion and topic intrusion. Word intrusion measures the presence of those words (called intruder words) which disagree with the semantics of the topic. Topic intrusion measures the presence of those topics (called intruder topics) which do not represent the document corpus appropriately. These are better estimates of human judgement of topic models in comparison to perplexity and uniqueness. However, since these metrics rely on human feedback, they cannot be widely used for unsupervised evaluation. Further, topic uniqueness unfairly penalizes cases when some words are common between topics, however other uncommon words in those topics change the context as well as topic semantics as also discussed in (Hoyle et al., 2020b). According to the work of (Lau et al., 2014), measuring the normalized pointwise mutual information (NPMI) between all the word pairs in a set of topics agrees with human judgements most closely. This is called the NPMI Topic Coherence in the literature and is widely used for the evaluation of topic models. We therefore adopt this metric in our work. Since the effectiveness of a topic model actually depends on the topic representations that it extracts from the documents, we report the performance of our model on two downstream tasks: document classification and keyphrase generation (which use these topic representations) for a better and holistic evaluation and comparison."
    }, {
      "heading" : "C Qualitative Analysis",
      "text" : "C.1 Key Phrase Predictions\nWe saw the quantitative improvement in results in Table 5 when we used W-TAN as the topic model\nwith TAKG. In Table 7, we display some posts from StackExchange dataset with ground truth keyphrases and top 5 predictions by TAKG with and without W-TAN. We observe that using WTAN improves keyphrase generation qualitatively.\nThe first post in Table 7 inquires if a flight officer should inform the pilot in command (PIC) about him being armed or not. For this post, TAKG alone only predicts one ground truth keyphrase correctly and misses ‘security’ and ‘crew’. However, when TAKG is used with W-TAN, it gets all three ground truth keyphrases, two of which are its top 2 predictions as well.\nThe second post is inquiring about a possible deeper meaning of three poisons in a poem by John Keats. TAKG alone predicts two of the ground truth keyphrases correctly but assigns them larger ranks and it misses ‘john keats’. When TAKG is used with W-TAN, it gets all three ground truth keyphrases and its top 2 keyphrases are assigned the exact same rank as they have in the ground truth. This hints that using W-TAN with TAKG improves the prediction as well as ranking of the generated keyphrases compared to using TAKG alone."
    } ],
    "references" : [ {
      "title" : "Evaluating topic coherence using distributional semantics",
      "author" : [ "Nikolaos Aletras", "Mark Stevenson." ],
      "venue" : "Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013)–Long Papers, pages 13–22.",
      "citeRegEx" : "Aletras and Stevenson.,? 2013",
      "shortCiteRegEx" : "Aletras and Stevenson.",
      "year" : 2013
    }, {
      "title" : "An introduction to mcmc for machine learning",
      "author" : [ "Christophe Andrieu", "Nando De Freitas", "Arnaud Doucet", "Michael I Jordan." ],
      "venue" : "Machine learning, 50(1-2):5–43.",
      "citeRegEx" : "Andrieu et al\\.,? 2003",
      "shortCiteRegEx" : "Andrieu et al\\.",
      "year" : 2003
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Pre-training is a hot topic: Contextualized document embeddings improve topic coherence",
      "author" : [ "Federico Bianchi", "Silvia Terragni", "Dirk Hovy." ],
      "venue" : "ArXiv, abs/2004.03974.",
      "citeRegEx" : "Bianchi et al\\.,? 2020",
      "shortCiteRegEx" : "Bianchi et al\\.",
      "year" : 2020
    }, {
      "title" : "Natural Language Processing with Python",
      "author" : [ "Steven Bird", "Ewan Klein", "Edward Loper." ],
      "venue" : "O’Reilly Media.",
      "citeRegEx" : "Bird et al\\.,? 2009",
      "shortCiteRegEx" : "Bird et al\\.",
      "year" : 2009
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "David M Blei", "Andrew Y Ng", "Michael I Jordan." ],
      "venue" : "Journal of machine Learning research, 3(Jan):993–1022.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Decoupling sparsity and smoothness in the dirichlet variational autoencoder topic model",
      "author" : [ "S. Burkhardt", "S. Kramer." ],
      "venue" : "J. Mach. Learn. Res., 20:131:1– 131:27.",
      "citeRegEx" : "Burkhardt and Kramer.,? 2019",
      "shortCiteRegEx" : "Burkhardt and Kramer.",
      "year" : 2019
    }, {
      "title" : "Neural models for documents with metadata",
      "author" : [ "Dallas Card", "Chenhao Tan", "Noah A. Smith." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2031–2040, Melbourne, Aus-",
      "citeRegEx" : "Card et al\\.,? 2018",
      "shortCiteRegEx" : "Card et al\\.",
      "year" : 2018
    }, {
      "title" : "Reading tea leaves: How humans interpret topic models",
      "author" : [ "Jonathan Chang", "Sean Gerrish", "Chong Wang", "Jordan Boyd-graber", "David Blei." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 22, pages 288–296. Curran Associates, Inc.",
      "citeRegEx" : "Chang et al\\.,? 2009",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2009
    }, {
      "title" : "Keyphrase generation with correlation constraints",
      "author" : [ "Jun Chen", "Xiaoming Zhang", "Yu Wu", "Zhao Yan", "Zhoujun Li." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4057–4066, Brussels, Belgium.",
      "citeRegEx" : "Chen et al\\.,? 2018",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning phrase representations using RNN encoder–decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Indexing by latent semantic analysis",
      "author" : [ "Scott Deerwester", "Susan T. Dumais", "George W. Furnas", "Thomas K. Landauer", "Richard Harshman." ],
      "venue" : "Journal of the American Society for Information Science, 41(6):391–407.",
      "citeRegEx" : "Deerwester et al\\.,? 1990",
      "shortCiteRegEx" : "Deerwester et al\\.",
      "year" : 1990
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Topic modeling in embedding spaces",
      "author" : [ "Adji B. Dieng", "Francisco J.R. Ruiz", "David M. Blei." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:439–453.",
      "citeRegEx" : "Dieng et al\\.,? 2020",
      "shortCiteRegEx" : "Dieng et al\\.",
      "year" : 2020
    }, {
      "title" : "Topicrnn: A recurrent neural network with long-range semantic dependency",
      "author" : [ "Adji B Dieng", "Chong Wang", "Jianfeng Gao", "John Paisley." ],
      "venue" : "arXiv preprint arXiv:1611.01702.",
      "citeRegEx" : "Dieng et al\\.,? 2016",
      "shortCiteRegEx" : "Dieng et al\\.",
      "year" : 2016
    }, {
      "title" : "Coherence-aware neural topic modeling",
      "author" : [ "Ran Ding", "Ramesh Nallapati", "Bing Xiang." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 830– 836, Brussels, Belgium. Association for Computa-",
      "citeRegEx" : "Ding et al\\.,? 2018",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2018
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio." ],
      "venue" : "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger,",
      "citeRegEx" : "Goodfellow et al\\.,? 2014",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Finding scientific topics",
      "author" : [ "Thomas L Griffiths", "Mark Steyvers." ],
      "venue" : "Proceedings of the National academy of Sciences, 101(suppl 1):5228–5235.",
      "citeRegEx" : "Griffiths and Steyvers.,? 2004",
      "shortCiteRegEx" : "Griffiths and Steyvers.",
      "year" : 2004
    }, {
      "title" : "Document informed neural autoregressive topic models with distributional prior",
      "author" : [ "Pankaj Gupta", "Yatin Chaudhary", "Florian Buettner", "Hinrich Schütze." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6505–6512.",
      "citeRegEx" : "Gupta et al\\.,? 2019",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2019
    }, {
      "title" : "Variational pretraining for semisupervised text classification",
      "author" : [ "Suchin Gururangan", "T. Dang", "D. Card", "Noah A. Smith." ],
      "venue" : "ACL.",
      "citeRegEx" : "Gururangan et al\\.,? 2019",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2019
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Comput., 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Improving neural topic models using knowledge distillation",
      "author" : [ "Alexander Miserlis Hoyle", "Pranav Goel", "P. Resnik." ],
      "venue" : "ArXiv, abs/2010.02377.",
      "citeRegEx" : "Hoyle et al\\.,? 2020a",
      "shortCiteRegEx" : "Hoyle et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving Neural Topic Models using Knowledge Distillation",
      "author" : [ "Alexander Miserlis Hoyle", "Pranav Goel", "Philip Resnik." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1752–1771,",
      "citeRegEx" : "Hoyle et al\\.,? 2020b",
      "shortCiteRegEx" : "Hoyle et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural topic modeling with cycleconsistent adversarial training",
      "author" : [ "Xuemeng Hu", "Rui Wang", "Deyu Zhou", "Yuxuan Xiong." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "S. Ioffe", "Christian Szegedy." ],
      "venue" : "ArXiv, abs/1502.03167.",
      "citeRegEx" : "Ioffe and Szegedy.,? 2015",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "Combining deep learning and topic modeling for review understanding in context-aware recommendation",
      "author" : [ "Mingmin Jin", "Xin Luo", "Huiling Zhu", "Hankz Hankui Zhuo." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Asso-",
      "citeRegEx" : "Jin et al\\.,? 2018",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2018
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "CoRR, abs/1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Autoencoding variational bayes",
      "author" : [ "Diederik P Kingma", "Max Welling." ],
      "venue" : "arXiv preprint arXiv:1312.6114.",
      "citeRegEx" : "Kingma and Welling.,? 2013",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2013
    }, {
      "title" : "Why didn’t you listen to me? comparing user control of human-in-the-loop topic models",
      "author" : [ "Varun Kumar", "Alison Smith-Renner", "Leah Findlater", "K. Seppi", "Jordan L. Boyd-Graber." ],
      "venue" : "ACL.",
      "citeRegEx" : "Kumar et al\\.,? 2019",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2019
    }, {
      "title" : "Newsweeder: Learning to filter netnews",
      "author" : [ "Ken Lang." ],
      "venue" : "Machine Learning Proceedings 1995, pages 331–339. Elsevier.",
      "citeRegEx" : "Lang.,? 1995",
      "shortCiteRegEx" : "Lang.",
      "year" : 1995
    }, {
      "title" : "Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality",
      "author" : [ "Jey Han Lau", "David Newman", "Timothy Baldwin." ],
      "venue" : "Proceedings of the 14th Conference of the European Chapter of the Association for Computational",
      "citeRegEx" : "Lau et al\\.,? 2014",
      "shortCiteRegEx" : "Lau et al\\.",
      "year" : 2014
    }, {
      "title" : "Mallet: A machine learning for language toolkit",
      "author" : [ "Andrew Kachites McCallum." ],
      "venue" : "http://mallet. cs. umass. edu.",
      "citeRegEx" : "McCallum.,? 2002",
      "shortCiteRegEx" : "McCallum.",
      "year" : 2002
    }, {
      "title" : "Deep keyphrase generation",
      "author" : [ "Rui Meng", "Sanqiang Zhao", "Shuguang Han", "Daqing He", "Peter Brusilovsky", "Yu Chi." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
      "citeRegEx" : "Meng et al\\.,? 2017",
      "shortCiteRegEx" : "Meng et al\\.",
      "year" : 2017
    }, {
      "title" : "Discovering discrete latent topics with neural variational inference",
      "author" : [ "Yishu Miao", "Edward Grefenstette", "Phil Blunsom." ],
      "venue" : "volume 70 of Proceedings of Machine Learning Research, pages 2410–2419, International Convention Centre, Sydney, Australia.",
      "citeRegEx" : "Miao et al\\.,? 2017",
      "shortCiteRegEx" : "Miao et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural variational inference for text processing",
      "author" : [ "Yishu Miao", "Lei Yu", "Phil Blunsom." ],
      "venue" : "volume 48 of Proceedings of Machine Learning Research, pages 1727–1736, New York, New York, USA. PMLR.",
      "citeRegEx" : "Miao et al\\.,? 2016",
      "shortCiteRegEx" : "Miao et al\\.",
      "year" : 2016
    }, {
      "title" : "Topic modeling with Wasserstein autoencoders",
      "author" : [ "Feng Nan", "Ran Ding", "Ramesh Nallapati", "Bing Xiang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6345–6381, Florence, Italy. Association",
      "citeRegEx" : "Nan et al\\.,? 2019",
      "shortCiteRegEx" : "Nan et al\\.",
      "year" : 2019
    }, {
      "title" : "tBERT: Topic models and BERT joining forces for semantic similarity detection",
      "author" : [ "Nicole Peinelt", "Dong Nguyen", "Maria Liakata." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7047–7055, Online. As-",
      "citeRegEx" : "Peinelt et al\\.,? 2020",
      "shortCiteRegEx" : "Peinelt et al\\.",
      "year" : 2020
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Tdam: a topic-dependent attention model for sentiment analysis",
      "author" : [ "Gabriele Pergola", "Lin Gui", "Yulan He." ],
      "venue" : "Inf. Process. Manag., 56.",
      "citeRegEx" : "Pergola et al\\.,? 2019",
      "shortCiteRegEx" : "Pergola et al\\.",
      "year" : 2019
    }, {
      "title" : "Software Framework for Topic Modelling with Large Corpora",
      "author" : [ "Radim Řehůřek", "Petr Sojka." ],
      "venue" : "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45–50, Valletta, Malta. ELRA.",
      "citeRegEx" : "Řehůřek and Sojka.,? 2010",
      "shortCiteRegEx" : "Řehůřek and Sojka.",
      "year" : 2010
    }, {
      "title" : "A discrete variational recurrent topic model without the reparametrization trick",
      "author" : [ "Mehdi Rezaee", "F. Ferraro." ],
      "venue" : "ArXiv, abs/2010.12055.",
      "citeRegEx" : "Rezaee and Ferraro.,? 2020",
      "shortCiteRegEx" : "Rezaee and Ferraro.",
      "year" : 2020
    }, {
      "title" : "Tired of topic models? clusters of pretrained word embeddings make for fast and good topics too",
      "author" : [ "Suzanna Sia", "Ayush Dalmia", "Sabrina J. Mielke" ],
      "venue" : "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Sia et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Sia et al\\.",
      "year" : 2020
    }, {
      "title" : "Autoencoding variational inference for topic models",
      "author" : [ "Akash Srivastava", "Charles Sutton." ],
      "venue" : "arXiv preprint arXiv:1703.01488.",
      "citeRegEx" : "Srivastava and Sutton.,? 2017",
      "shortCiteRegEx" : "Srivastava and Sutton.",
      "year" : 2017
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey E. Hinton", "A. Krizhevsky", "Ilya Sutskever", "R. Salakhutdinov." ],
      "venue" : "J. Mach. Learn. Res., 15:1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Probabilistic topic models",
      "author" : [ "Mark Steyvers", "Tom Griffiths." ],
      "venue" : "Handbook of latent semantic analysis, 427(7):424–440.",
      "citeRegEx" : "Steyvers and Griffiths.,? 2007",
      "shortCiteRegEx" : "Steyvers and Griffiths.",
      "year" : 2007
    }, {
      "title" : "Hierarchical beta processes and the indian buffet process",
      "author" : [ "Romain Thibaux", "Michael I. Jordan." ],
      "venue" : "volume 2 of Proceedings of Machine Learning Research, pages 564–571, San Juan, Puerto Rico. PMLR.",
      "citeRegEx" : "Thibaux and Jordan.,? 2007",
      "shortCiteRegEx" : "Thibaux and Jordan.",
      "year" : 2007
    }, {
      "title" : "Wasserstein autoencoders",
      "author" : [ "Ilya Tolstikhin", "Olivier Bousquet", "Sylvain Gelly", "Bernhard Schoelkopf." ],
      "venue" : "arXiv preprint arXiv:1711.01558.",
      "citeRegEx" : "Tolstikhin et al\\.,? 2017",
      "shortCiteRegEx" : "Tolstikhin et al\\.",
      "year" : 2017
    }, {
      "title" : "CluHTM - semantic hierarchical topic modeling based on CluWords",
      "author" : [ "Felipe Viegas", "Washington Cunha", "Christian Gomes", "Antônio Pereira", "Leonardo Rocha", "Marcos Goncalves." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for",
      "citeRegEx" : "Viegas et al\\.,? 2020",
      "shortCiteRegEx" : "Viegas et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural topic modeling with bidirectional adversarial training",
      "author" : [ "Rui Wang", "Xuemeng Hu", "Deyu Zhou", "Yulan He", "Yuxuan Xiong", "Chenchen Ye", "Haiyang Xu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Atm: Adversarial-neural topic model",
      "author" : [ "Rui Wang", "Deyu Zhou", "Yulan He." ],
      "venue" : "Information Processing & Management, 56(6):102098.",
      "citeRegEx" : "Wang et al\\.,? 2019a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural topic model with attention for supervised learning",
      "author" : [ "X. Wang", "Y. Yang." ],
      "venue" : "AISTATS.",
      "citeRegEx" : "Wang and Yang.,? 2020",
      "shortCiteRegEx" : "Wang and Yang.",
      "year" : 2020
    }, {
      "title" : "Topicaware neural keyphrase generation for social media language",
      "author" : [ "Yue Wang", "Jing Li", "Hou Pong Chan", "Irwin King", "Michael R. Lyu", "Shuming Shi." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Wang et al\\.,? 2019b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural mixed counting models for dispersed topic discovery",
      "author" : [ "Jiemin Wu", "Yanghui Rao", "Zusheng Zhang", "Haoran Xie", "Qing Li", "Fu Lee Wang", "Ziye Chen." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Latent lstm allocation: Joint clustering and non-linear dynamic modeling of sequence data",
      "author" : [ "M. Zaheer", "Amr Ahmed", "Alex Smola." ],
      "venue" : "ICML.",
      "citeRegEx" : "Zaheer et al\\.,? 2017",
      "shortCiteRegEx" : "Zaheer et al\\.",
      "year" : 2017
    }, {
      "title" : "Topic modeling on document networks with adjacent-encoder",
      "author" : [ "Ce Zhang", "Hady W. Lauw." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Zhang and Lauw.,? 2020",
      "shortCiteRegEx" : "Zhang and Lauw.",
      "year" : 2020
    }, {
      "title" : "Whai: Weibull hybrid autoencoding inference for deep topic modeling",
      "author" : [ "Hao Zhang", "B. Chen", "D. Guo", "M. Zhou." ],
      "venue" : "arXiv: Machine Learning.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Keyphrase extraction using deep recurrent neural networks on twitter",
      "author" : [ "Qi Zhang", "Yang Wang", "Yeyun Gong", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 836–845,",
      "citeRegEx" : "Zhang et al\\.,? 2016",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun." ],
      "venue" : "Advances in neural information processing systems, pages 649–657.",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural topic model via optimal transport",
      "author" : [ "He Zhao", "Dinh Phung", "Viet Huynh", "Trung Le", "Wray Buntine." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhao et al\\.,? 2021",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2021
    }, {
      "title" : "Neural topic modeling by incorporating document relationship graph",
      "author" : [ "Deyu Zhou", "Xuemeng Hu", "Rui Wang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3790–3796, Online. As-",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    }, {
      "title" : "Unpaired image-to-image translation using cycle-consistent adversarial networks",
      "author" : [ "Jun-Yan Zhu", "T. Park", "Phillip Isola", "Alexei A. Efros." ],
      "venue" : "2017 IEEE International Conference on Computer Vision (ICCV), pages 2242–2251.",
      "citeRegEx" : "Zhu et al\\.,? 2017",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2017
    }, {
      "title" : "A neural generative model for joint learning topics and topic-specific word embeddings",
      "author" : [ "Li-Xing Zhu", "Yulan He", "Deyu Zhou." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:471– 485.",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 44,
      "context" : "Topic models (Steyvers and Griffiths, 2007) have been popularly used to extract abstract topics which occur commonly across documents in a corpus.",
      "startOffset" : 13,
      "endOffset" : 43
    }, {
      "referenceID" : 35,
      "context" : "∗equal contribution †work done during summer internship at Adobe of practical importance such as learning text representations for document classification (Nan et al., 2019), keyphrase extraction (Wang et al.",
      "startOffset" : 155,
      "endOffset" : 173
    }, {
      "referenceID" : 51,
      "context" : ", 2019), keyphrase extraction (Wang et al., 2019b), understanding reviews for e-commerce recommendations (Jin et al.",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 25,
      "context" : ", 2019b), understanding reviews for e-commerce recommendations (Jin et al., 2018), semantic similarity detec-",
      "startOffset" : 63,
      "endOffset" : 81
    }, {
      "referenceID" : 11,
      "context" : "Early works on topic discovery include statistical methods such as Latent Semantic Analysis (Deerwester et al., 1990), Latent Dirichlet Allocation (LDA) (Blei et al.",
      "startOffset" : 92,
      "endOffset" : 117
    }, {
      "referenceID" : 5,
      "context" : ", 1990), Latent Dirichlet Allocation (LDA) (Blei et al., 2003) which approximates each topic as a probability distribution over word",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 1,
      "context" : "This was followed by Markov Chain Monte Carlo (MCMC) (Andrieu et al., 2003) based",
      "startOffset" : 53,
      "endOffset" : 75
    }, {
      "referenceID" : 17,
      "context" : "inference algorithm - Collapsed Gibbs sampling (Griffiths and Steyvers, 2004).",
      "startOffset" : 47,
      "endOffset" : 77
    }, {
      "referenceID" : 27,
      "context" : "ral networks and Variational Autoencoders (VAE) (Kingma and Welling, 2013), where variational inference can be performed in single forward pass.",
      "startOffset" : 48,
      "endOffset" : 74
    }, {
      "referenceID" : 33,
      "context" : "Neural variational inference topic models (Miao et al., 2017; Ding et al., 2018; Srivastava and Sutton, 2017) commonly convert a document to Bagof-Words (BoW) determined on the basis of fre-",
      "startOffset" : 42,
      "endOffset" : 109
    }, {
      "referenceID" : 15,
      "context" : "Neural variational inference topic models (Miao et al., 2017; Ding et al., 2018; Srivastava and Sutton, 2017) commonly convert a document to Bagof-Words (BoW) determined on the basis of fre-",
      "startOffset" : 42,
      "endOffset" : 109
    }, {
      "referenceID" : 42,
      "context" : "Neural variational inference topic models (Miao et al., 2017; Ding et al., 2018; Srivastava and Sutton, 2017) commonly convert a document to Bagof-Words (BoW) determined on the basis of fre-",
      "startOffset" : 42,
      "endOffset" : 109
    }, {
      "referenceID" : 34,
      "context" : "Methods such as NVDM (Miao et al., 2016), NTM-R (Ding et al.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 15,
      "context" : ", 2016), NTM-R (Ding et al., 2018), NVDM-GSM (Miao et al.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 33,
      "context" : ", 2018), NVDM-GSM (Miao et al., 2017) use the Gaussian prior.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 20,
      "context" : "eling which process the sequence of tokens in input document through an LSTM (Hochreiter and Schmidhuber, 1997) whose contextual outputs are attended using Topic-Word Distribution (TWD).",
      "startOffset" : 77,
      "endOffset" : 111
    }, {
      "referenceID" : 2,
      "context" : "(Bahdanau et al., 2014) to enable the model to attend on the tokens which convey topic related information and cues.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 0,
      "context" : "For evaluation, we compute commonly used NPMI coherence (Aletras and Stevenson, 2013) which measures the extent to which most probable words in a topic are semantically related to each other.",
      "startOffset" : 56,
      "endOffset" : 85
    }, {
      "referenceID" : 5,
      "context" : "We compare our TAN-NTM model with several state-of-the-art topic models (statistical (Blei et al., 2003; Griffiths and Steyvers, 2004), neural VAE (Srivastava and Sutton, 2017; Wu et al.",
      "startOffset" : 85,
      "endOffset" : 134
    }, {
      "referenceID" : 17,
      "context" : "We compare our TAN-NTM model with several state-of-the-art topic models (statistical (Blei et al., 2003; Griffiths and Steyvers, 2004), neural VAE (Srivastava and Sutton, 2017; Wu et al.",
      "startOffset" : 85,
      "endOffset" : 134
    }, {
      "referenceID" : 42,
      "context" : ", 2003; Griffiths and Steyvers, 2004), neural VAE (Srivastava and Sutton, 2017; Wu et al., 2020) and non-variational inference based neural model",
      "startOffset" : 50,
      "endOffset" : 96
    }, {
      "referenceID" : 52,
      "context" : ", 2003; Griffiths and Steyvers, 2004), neural VAE (Srivastava and Sutton, 2017; Wu et al., 2020) and non-variational inference based neural model",
      "startOffset" : 50,
      "endOffset" : 96
    }, {
      "referenceID" : 35,
      "context" : "(Nan et al., 2019)) outperforming them on three benchmark datasets of varying scale and complexity: 20Newsgroups (20NG) (Lang, 1995), Yelp Review Polarity and AGNews (Zhang et al.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 29,
      "context" : ", 2019)) outperforming them on three benchmark datasets of varying scale and complexity: 20Newsgroups (20NG) (Lang, 1995), Yelp Review Polarity and AGNews (Zhang et al.",
      "startOffset" : 109,
      "endOffset" : 121
    }, {
      "referenceID" : 57,
      "context" : ", 2019)) outperforming them on three benchmark datasets of varying scale and complexity: 20Newsgroups (20NG) (Lang, 1995), Yelp Review Polarity and AGNews (Zhang et al., 2015).",
      "startOffset" : 155,
      "endOffset" : 175
    }, {
      "referenceID" : 51,
      "context" : "Further, topic models have previously been used to improve supervised keyphrase generation (Wang et al., 2019b).",
      "startOffset" : 91,
      "endOffset" : 111
    }, {
      "referenceID" : 27,
      "context" : "for Variational Autoencoders (VAE) (Kingma and Welling, 2013) which enables performing Variational Inference (VI) efficiently.",
      "startOffset" : 35,
      "endOffset" : 61
    }, {
      "referenceID" : 42,
      "context" : "Since our work is based on variational inference, we use ProdLDA and NVLDA (Srivastava and Sutton, 2017) as baselines for comparison.",
      "startOffset" : 75,
      "endOffset" : 104
    }, {
      "referenceID" : 6,
      "context" : "Some works address the sparsitysmoothness trade-off in dirichlet distribution by factoring dirichlet parameter vector as a product of two vectors (Burkhardt and Kramer, 2019).",
      "startOffset" : 146,
      "endOffset" : 174
    }, {
      "referenceID" : 46,
      "context" : "Wasserstein Autoencoders (WAE) (Tolstikhin et al., 2017) have led to the development of non-variational inference based topic model: Wasserstein-LDA (WLDA) which minimizes the wasserstein distance, a",
      "startOffset" : 31,
      "endOffset" : 56
    }, {
      "referenceID" : 49,
      "context" : "Adversarial Topic Model (ATM) (Wang et al., 2019a) was proposed based on GAN (Generative Adversarial Network) (Goodfellow et al.",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 16,
      "context" : ", 2019a) was proposed based on GAN (Generative Adversarial Network) (Goodfellow et al., 2014) but it cannot infer document-topic distribution.",
      "startOffset" : 68,
      "endOffset" : 93
    }, {
      "referenceID" : 48,
      "context" : "Bidirectional Adversarial Topic model (BAT) (Wang et al., 2020) employs a bilateral transformation between document-word and document-topic distribution, while Hu et al.",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 60,
      "context" : "(2020) uses CycleGAN (Zhu et al., 2017) for unsupervised transfer between document-",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 47,
      "context" : "Hierarchical topic models (Viegas et al., 2020) utilize relationships among the latent topics.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 28,
      "context" : "Supervised topic models have been explored previously where the topic model is trained through human feedback (Kumar et al., 2019) or with a",
      "startOffset" : 110,
      "endOffset" : 130
    }, {
      "referenceID" : 38,
      "context" : "task specific network simultaneously such that topic extraction is guided through task labels (Pergola et al., 2019; Wang and Yang, 2020).",
      "startOffset" : 94,
      "endOffset" : 137
    }, {
      "referenceID" : 50,
      "context" : "task specific network simultaneously such that topic extraction is guided through task labels (Pergola et al., 2019; Wang and Yang, 2020).",
      "startOffset" : 94,
      "endOffset" : 137
    }, {
      "referenceID" : 54,
      "context" : "Topic modeling on document networks has been done leveraging relational links between documents (Zhang and Lauw, 2020; Zhou et al., 2020).",
      "startOffset" : 96,
      "endOffset" : 137
    }, {
      "referenceID" : 59,
      "context" : "Topic modeling on document networks has been done leveraging relational links between documents (Zhang and Lauw, 2020; Zhou et al., 2020).",
      "startOffset" : 96,
      "endOffset" : 137
    }, {
      "referenceID" : 12,
      "context" : "Some very recent works use pre-trained BERT (Devlin et al., 2019) either to leverage improved text representations (Bianchi et al.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 21,
      "context" : "2020) or to augment topic model through knowledge distillation (Hoyle et al., 2020a).",
      "startOffset" : 63,
      "endOffset" : 84
    }, {
      "referenceID" : 18,
      "context" : "iDocNade (Gupta et al., 2019) is an autoregressive topic model for short texts utilizing pre-trained embeddings as distributional prior.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 52,
      "context" : "Gamma Negative BinomialNeural Topic Model (GNB-NTM) (Wu et al., 2020) is one of the recent neural variational topic models which attempt to combine VI with mixed counting models.",
      "startOffset" : 52,
      "endOffset" : 69
    }, {
      "referenceID" : 53,
      "context" : "Topic models have been used with sequence encoders such as LSTM in applications like user activity modeling (Zaheer et al., 2017).",
      "startOffset" : 108,
      "endOffset" : 129
    }, {
      "referenceID" : 25,
      "context" : "LTMF (Jin et al., 2018) combines text features processed through an LSTM with a topic model for review based recommendations.",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 56,
      "context" : "Some of the existing neural keyphrase generation methods include SEQ-TAG (Zhang et al., 2016) based on sequence tagging, SEQ2SEQ-CORR (Chen et al.",
      "startOffset" : 73,
      "endOffset" : 93
    }, {
      "referenceID" : 9,
      "context" : ", 2016) based on sequence tagging, SEQ2SEQ-CORR (Chen et al., 2018)",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 32,
      "context" : "based on seq2seq model without copy mechanism and SEQ2SEQ-COPY (Meng et al., 2017) which additionally uses copy mechanism.",
      "startOffset" : 63,
      "endOffset" : 82
    }, {
      "referenceID" : 51,
      "context" : "TopicAware Keyphrase Generation (TAKG) (Wang et al., 2019b) is a seq2seq based neural keyphrase generation framework for social media language.",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 20,
      "context" : "3869 where |x| is the number of tokens in x, is processed through a sequence encoder LSTM (Hochreiter and Schmidhuber, 1997) to obtain the corresponding hidden states hj ∈ RH and cell states sj ∈ RH (step B in figure 1):",
      "startOffset" : 90,
      "endOffset" : 124
    }, {
      "referenceID" : 29,
      "context" : "Topic Quality: We evaluate and compare quality of our proposed topic model on three benchmark datasets - 20Newsgroups (20NG)1 (Lang, 1995), AGNews (Zhang et al.",
      "startOffset" : 126,
      "endOffset" : 138
    }, {
      "referenceID" : 57,
      "context" : "Topic Quality: We evaluate and compare quality of our proposed topic model on three benchmark datasets - 20Newsgroups (20NG)1 (Lang, 1995), AGNews (Zhang et al., 2015) and Yelp Review Polarity (YRP)2 - which are of varying complexity and scale in terms of number of documents, vocabulary size and average length of text after preprocessing3.",
      "startOffset" : 147,
      "endOffset" : 167
    }, {
      "referenceID" : 51,
      "context" : "Keyphrase Generation: Neural Topic Model (NTM) has been used to improve the task of supervised keyphrase generation (Wang et al., 2019b).",
      "startOffset" : 116,
      "endOffset" : 136
    }, {
      "referenceID" : 26,
      "context" : "(4)The dataset details can be found in the baseline paper use batch size of 100, Adam Optimizer (Kingma and Ba, 2015) with β1 = 0.",
      "startOffset" : 96,
      "endOffset" : 117
    }, {
      "referenceID" : 31,
      "context" : "): Statistical method (McCallum, 2002) which performs LDA using collapsed Gibbs6 sampling.",
      "startOffset" : 22,
      "endOffset" : 38
    }, {
      "referenceID" : 35,
      "context" : "4) W-LDA (Nan et al., 2019) which is a non variational inference based neural model using wassestein autoencoder8.",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 52,
      "context" : "5) NB-NTM and 6) GNB-NTM: Methods using negative binomial and gamma negative binomial distribution as priors for topic discovery9(Wu et al., 2020) respectively.",
      "startOffset" : 129,
      "endOffset" : 146
    }, {
      "referenceID" : 0,
      "context" : "We train and evaluate the baseline methods on same data as used for our method using NPMI coherence10 (Aletras and Stevenson, 2013).",
      "startOffset" : 102,
      "endOffset" : 131
    }, {
      "referenceID" : 52,
      "context" : "(which is the common dataset with GNB-NTM (Wu et al., 2020)) using the NPMI metric from GNB-NTM’s code.",
      "startOffset" : 42,
      "endOffset" : 59
    }, {
      "referenceID" : 30,
      "context" : "However, we would like to highlight that GNB-NTM’s computation of NPMI metric uses relaxed window size, whereas the metric used by us (Lau et al., 2014) uses much stricter window size while determining word co-occurrence counts within a document.",
      "startOffset" : 134,
      "endOffset" : 152
    }, {
      "referenceID" : 35,
      "context" : "Topic models have been used as text feature extractors to perform classification (Nan et al., 2019).",
      "startOffset" : 81,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "In this section, we compare the performance of different variants of our model namely, 1) only LSTM: final hidden state is used to derive sampling parameters zμ & zlog σ2 , 2) vanilla attn: final hidden state (w/o topic-word distribution) is used as query to perform attention (Bahdanau et al., 2014) on LSTM outputs such that context vector z is used for VI, 3) W-TAN: Weighted Topic Attention Network, 4) T-TAN: Top Topic Attention Network and 5) T-TAN w/o (without) GloVe: em-",
      "startOffset" : 277,
      "endOffset" : 300
    }, {
      "referenceID" : 51,
      "context" : "For this, we discuss TAKG (Wang et al., 2019b) and how our proposed topic model encoder can be adapted to achieve better performance on supervised keyphrase generation from textual posts.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 10,
      "context" : "sequence input which is processed by bidirectional GRU (Cho et al., 2014) to encode input sequence.",
      "startOffset" : 55,
      "endOffset" : 73
    } ],
    "year" : 2021,
    "abstractText" : "Topic models have been widely used to learn text representations and gain insight into document corpora. To perform topic discovery, most existing neural models either take document bag-of-words (BoW) or sequence of tokens as input followed by variational inference and BoW reconstruction to learn topic-word distribution. However, leveraging topic-word distribution for learning better features during document encoding has not been explored much. To this end, we develop a framework TAN-NTM, which processes document as a sequence of tokens through a LSTM whose contextual outputs are attended in a topic-aware manner. We propose a novel attention mechanism which factors in topic-word distribution to enable the model to attend on relevant words that convey topic related cues. The output of topic attention module is then used to carry out variational inference. We perform extensive ablations and experiments resulting in ∼ 9 15 percentage improvement over score of existing SOTA topic models in NPMI coherence on several benchmark datasets 20Newsgroups, Yelp Review Polarity and AGNews. Further, we show that our method learns better latent document-topic features compared to existing topic models through improvement on two downstream tasks: document classification and topic guided keyphrase generation.",
    "creator" : "LaTeX with hyperref"
  }
}