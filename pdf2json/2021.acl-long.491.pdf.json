{
  "name" : "2021.acl-long.491.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "CLEVE: Contrastive Pre-training for Event Extraction",
    "authors" : [ "Ziqi Wang", "Xiaozhi Wang", "Xu Han", "Yankai Lin", "Lei Hou", "Zhiyuan Liu", "Peng Li", "Juanzi Li", "Jie Zhou" ],
    "emails" : [ "hanxu17}@mails.tsinghua.edu.cn", "(houlei@tsinghua.edu.cn)" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6283–6297\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6283"
    }, {
      "heading" : "1 Introduction",
      "text" : "Event extraction (EE) is a long-standing crucial information extraction task, which aims at extracting event structures from unstructured text. As illustrated in Figure 1, it contains event detection task to identify event triggers (the word “attack”) and\n∗ indicates equal contribution † Correspondence to L.Hou (houlei@tsinghua.edu.cn)\nclassify event types (Attack), as well as event argument extraction task to identify entities serving as event arguments (“today” and “Netanya”) and classify their argument roles (Time-within and Place) (Ahn, 2006). By explicitly capturing the event structure in the text, EE can benefit various downstream tasks such as information retrieval (Glavaš and Šnajder, 2014) and knowledge base population (Ji and Grishman, 2011).\nExisting EE methods mainly follow the supervised-learning paradigm to train advanced neural networks (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018) with humanannotated datasets and pre-defined event schemata. These methods work well in lots of public benchmarks such as ACE 2005 (Walker et al., 2006) and TAC KBP (Ellis et al., 2016), yet they still suffer from data scarcity and limited generalizability. Since annotating event data and defining event schemata are especially expensive and laborintensive, existing EE datasets typically only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods that can generalize to continually-emerging new event types (Huang and Ji, 2020).\nInspired by the success of recent pre-trained language models (PLMs) for NLP tasks, some pio-\nneering work (Wang et al., 2019a; Wadden et al., 2019) attempts to fine-tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the strong general language understanding ability learnt from large-scale unsupervised data, these PLM-based methods have achieved state-ofthe-art performance in various public benchmarks.\nAlthough leveraging unsupervised data with pretraining has gradually become a consensus for EE and NLP community, there still lacks a pre-training method orienting event modeling to take full advantage of rich event knowledge lying in largescale unsupervised data. The key challenge here is to find reasonable self-supervised signals (Chen et al., 2017; Wang et al., 2019a) for the diverse semantics and complex structures of events. Fortunately, previous work (Aguilar et al., 2014; Huang et al., 2016) has suggested that sentence semantic structures, such as abstract meaning representation (AMR) (Banarescu et al., 2013), contain broad and diverse semantic and structure information relating to events. As shown in Figure 1, the parsed AMR structure covers not only the annotated event (Attack) but also the event that is not defined in the ACE 2005 schema (Report).\nConsidering the fact that the AMR structures of large-scale unsupervised data can be easily obtained with automatic parsers (Wang et al., 2015), we propose CLEVE, an event-oriented contrastive pre-training framework utilizing AMR structures to build self-supervision signals. CLEVE consists of two components, including a text encoder to learn event semantics and a graph encoder to learn event structure information. Specifically, to learn effective event semantic representations, we employ a PLM as the text encoder and encourage the representations of the word pairs connected by the ARG, time, location edges in AMR structures to be closer in the semantic space than other unrelated words, since these pairs usually refer to the trigger-argument pairs of the same events (as shown in Figure 1) (Huang et al., 2016). This is done by contrastive learning with the connected word pairs as positive samples and unrelated words as negative samples. Moreover, considering event structures are also helpful in extracting events (Lai et al., 2020) and generalizing to new event schemata (Huang et al., 2018), we need to learn transferable event structure representations. Hence we further introduce a graph neural network (GNN) as the graph encoder to encode AMR\nstructures as structure representations. The graph encoder is contrastively pre-trained on the parsed AMR structures of large unsupervised corpora with AMR subgraph discrimination as the objective.\nBy fine-tuning the two pre-trained models on downstream EE datasets and jointly using the two representations, CLEVE can benefit the conventional supervised EE suffering from data scarcity. Meanwhile, the pre-trained representations can also directly help extract events and discover new event schemata without any known event schema or annotated instances, leading to better generalizability. This is a challenging unsupervised setting named “liberal event extraction” (Huang et al., 2016). Experiments on the widely-used ACE 2005 and the large MAVEN datasets indicate that CLEVE can achieve significant improvements in both settings."
    }, {
      "heading" : "2 Related Work",
      "text" : "Event Extraction. Most of the existing EE works follow the supervised learning paradigm. Traditional EE methods (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013) rely on manually-crafted features to extract events. In recent years, the neural models become mainstream, which automatically learn effective features with neural networks, including convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015), recurrent neural networks (Nguyen et al., 2016), graph convolutional networks (Nguyen and Grishman, 2018; Lai et al., 2020). With the recent successes of BERT (Devlin et al., 2019), PLMs have also been used for EE (Wang et al., 2019a,b; Yang et al., 2019; Wadden et al., 2019; Tong et al., 2020). Although achieving remarkable performance in benchmarks such as ACE 2005 (Walker et al., 2006) and similar datasets (Ellis et al., 2015, 2016; Getman et al., 2017; Wang et al., 2020), these PLM-based works solely focus on better finetuning rather than pre-training for EE. In this paper, we study pre-training to better utilize rich event knowledge in large-scale unsupervised data.\nEvent Schema Induction. Supervised EE models cannot generalize to continually-emerging new event types and argument roles. To this end, Chambers and Jurafsky (2011) explore to induce event schemata from raw text by unsupervised clustering. Following works introduce more features like coreference chains (Chambers, 2013) and entities (Nguyen et al., 2015; Sha et al., 2016). Recently, Huang and Ji (2020) move to the semi-\nsupervised setting allowing to use annotated data of known types. Following Huang et al. (2016), we evaluate the generalizability of CLEVE in the most challenging unsupervised “liberal” setting, which requires to induce event schemata and extract event instances only from raw text at the same time.\nContrastive Learning. Contrastive learning was initiated by Hadsell et al. (2006) following an intuitive motivation to learn similar representations for “neighboors” and distinct representations for “non-neighbors”, and is further widely used for selfsupervised representation learning in various domains, such as computer vision (Wu et al., 2018; Oord et al., 2018; Hjelm et al., 2019; Chen et al., 2020; He et al., 2020) and graph (Qiu et al., 2020; You et al., 2020; Zhu et al., 2020). In the context of NLP, many established representation learning works can be viewed as contrastive learning methods, such as Word2Vec (Mikolov et al., 2013), BERT (Devlin et al., 2019; Kong et al., 2020) and ELECTRA (Clark et al., 2020). Similar to this work, contrastive learning is also widely-used to help specific tasks, including question answering (Yeh and Chen, 2019), discourse modeling (Iter et al., 2020), natural language inference (Cui et al., 2020) and relation extraction (Peng et al., 2020)."
    }, {
      "heading" : "3 Methodology",
      "text" : "The overall CLEVE framework is illustrated in Figure 2. As shown in the illustration, our contrastive pre-training framework CLEVE consists of two components: event semantic pre-training and event\nstructure pre-training, of which details are introduced in Section 3.2 and Section 3.3, respectively. At the beginning of this section, we first introduce the required preprocessing in Section 3.1, including the AMR parsing and how we modify the parsed AMR structures for our pre-training."
    }, {
      "heading" : "3.1 Preprocessing",
      "text" : "CLEVE relies on AMR structures (Banarescu et al., 2013) to build broad and diverse self-supervision signals for learning event knowledge from largescale unsupervised corpora. To do this, we use automatic AMR parsers (Wang et al., 2015; Xu et al., 2020) to parse the sentences in unsupervised corpora into AMR structures. Each AMR structure is a directed acyclic graph with concepts as nodes and semantic relations as edges. Moreover, each node typically only corresponds to at most one word, and a multi-word entity will be represented as a list of nodes connected with name and op (conjunction operator) edges. Considering pretraining entity representations will naturally benefits event argument extraction, we merge these lists into single nodes representing multi-word entities (like the “CNN’s Kelly Wallace” in Figure 1) during both event semantic and structure pre-training. Formally, given a sentence s in unsupervised corpora, we obtain its AMR graph gs = (Vs,Es) after AMR parsing, where Vs is the node set after word merging and Es denotes the edge set. Es = {(u, v, r) | (u, v) ∈ Vs×Vs, r ∈ R}, where R is the set of defined semantic relation types."
    }, {
      "heading" : "3.2 Event Semantic Pre-training",
      "text" : "To model diverse event semantics in large unsupervised corpora and learn contextualized event semantic representations, we adopt a PLM as the text encoder and train it with the objective to discriminate various trigger-argument pairs."
    }, {
      "heading" : "Text Encoder",
      "text" : "Like most PLMs, we adopt a multi-layer Transformer (Vaswani et al., 2017) as the text encoder since its strong representation capacity. Given a sentence s = {w1, w2, . . . , wn} containing n tokens, we feed it into the multi-layer Transformer and use the last layer’s hidden vectors as token representations. Moreover, a node v ∈ Vs may correspond to a multi-token text span in s and we need a unified representation for the node in pre-training. As suggested by Baldini Soares et al. (2019), we insert two special markers [E1] and [/E1] at the beginning and ending of the span, respectively. Then we use the hidden vector for [E1] as the span representation xv of the node v. And we use different marker pairs for different nodes.\nAs our event semantic pre-training focuses on modeling event semantics, we start our pre-training from a well-trained general PLM to obtain general language understanding abilities. CLEVE is agnostic to the model architecture and can use any general PLM, like BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019)."
    }, {
      "heading" : "Trigger-Argument Pair Discrimination",
      "text" : "We design trigger-argument pair discrimination as our contrastive pre-training task for event semantic pre-training. The basic idea is to learn closer representations for the words in the same events than the unrelated words. We note that the words connected by ARG, time and location edges in AMR structures are quite similar to the triggerargument pairs in events (Huang et al., 2016, 2018), i.e., the key words evoking events and the entities participating events. For example, in Figure 1, “Netanya” is an argument for the “attack” event, while the disconnected “CNN’s Kelly Wallace” is not. With this observation, we can use these special word pairs as positive trigger-argument samples and train the text encoder to discriminate them from negative samples, so that the encoder can learn to model event semantics without human annotation.\nLetRp = {ARG,time,location} and Ps = {(u, v)|∃(u, v, r) ∈ Es, r ∈ Rp} denotes the set of positive trigger-argument pairs in sentence s.\nFor a specific positive pair (t, a) ∈ Ps, as shown in Figure 2, we construct its corresponding negative samples with trigger replacement and argument replacement. Specifically, in the trigger replacement, we construct mt number of negative pairs by randomly sample mt number of negative triggers t̂ ∈ Vs and combine them with the positive argument a. A negative trigger t̂ must do not have a directed ARG, time or location edge with a, i.e., @(t̂, a, r) ∈ Es, r ∈ Rp. Similarly, we construct ma more negative pairs by randomly sample ma number of negative arguments â ∈ Vs satisfying @(t, â, r) ∈ Es, r ∈ Rp. As the example in Figure 2, (“attack”, “reports”) is a valid negative sample for the positive sample (“attack”, “Netanya”), but (“attack”, “today’s”) is not valid since there is a (“attack”, “today’s”, time) edge.\nTo learn to discriminate the positive triggerargument pair from the negative pairs and so that model event semantics, we define the training objective for a positive pair (t, a) as a cross-entropy loss of classifying the positive pair correctly:\nLt,a =− x>t Wxa\n+ log ( exp ( x>t Wxa ) + mt∑ i=1 exp ( x>t̂iWxa ) +\nma∑ j=1 exp ( x>t Wxâj )) ,\n(1)\nwhere mt, ma are hyper-parameters for negative sampling, and W is a trainable matrix learning the similarity metric. We adopt the cross-entropy loss here since it is more effective than other contrastive loss forms (Oord et al., 2018; Chen et al., 2020).\nThen we obtain the overall training objective for event semantic pre-training by summing up the losses of all the positive pairs of all sentences s in the mini batch Bs:\nLsem(θ) = ∑ s∈Bs ∑ (t,a)∈Ps Lt,a, (2)\nwhere θ denotes the trainable parameters, including the text encoder and W ."
    }, {
      "heading" : "3.3 Event Structure Pre-training",
      "text" : "Previous work has shown that event-related structures are helpful in extracting new events (Lai et al., 2020) as well as discovering and generalizing to new event schemata (Huang et al., 2016, 2018; Huang and Ji, 2020). Hence we conduct event structure pre-training on a GNN as graph encoder to learn transferable event-related structure representations with recent advances in graph contrastive\npre-training (Qiu et al., 2020; You et al., 2020; Zhu et al., 2020). Specifically, we pre-train the graph encoder with AMR subgraph discrimination task."
    }, {
      "heading" : "Graph Encoder",
      "text" : "In CLEVE, we utilize a GNN to encode the AMR (sub)graph to extract the event structure information of the text. Given a graph g, the graph encoder represents it with an graph embedding g = G(g, {xv}), where G(·) is the graph encoder and {xv} denotes the initial node representations fed into the graph encoder. CLEVE is agnostic to specific model architectures of the graph encoder. Here we use a state-of-the-art GNN model, Graph Isomorphism Network (Xu et al., 2019), as our graph encoder for its strong representation ability.\nWe use the corresponding text span representations {xv} produced by our pre-trained text encoder (introduced in Section 3.2) as the initial node representations for both pre-training and inference of the graph encoder. This node initialization also implicitly aligns the semantic spaces of event semantic and structure representations in CLEVE, so that can make them cooperate better."
    }, {
      "heading" : "AMR Subgraph Discrimination",
      "text" : "To learn transferable event structure representations, we design the AMR subgraph discrimination task for event structure pre-training. The basic idea is to learn similar representations for the subgraphs sampled from the same AMR graph by discriminating them from subgraphs sampled from other AMR graphs (Qiu et al., 2020).\nGiven a batch of m AMR graphs {g1, g2, . . . , gm}, each graph corresponds to a sentence in unsupervised corpora. For the i-th graph gi, we randomly sample two subgraphs from it to get a positive pair a2i−1 and a2i. And all the subgraphs sampled from the other AMR graphs in the mini-batch serve as negative samples. Like in Figure 2, the two green (w/ “attack”) subgraphs are a positive pair while the other two subgraphs sampled from the purple (w/ “solider”) graph are negative samples. Here we use the subgraph sampling strategy introduced by Qiu et al. (2020), whose details are shown in Appendix C.\nSimilar to event semantic pre-training, we adopt the graph encoder to represent the samples ai = G (ai,xv)and define the training objective as:\nLstr(θ) = − m∑ i=1 log exp\n( a>2i−1a2i )∑2m j=1 1[j 6=2i−1] exp ( a>2i−1aj ) , (3)\nwhere 1[j 6=2i−1] ∈ {0, 1} is an indicator function evaluating to 1 iff j 6= 2i− 1 and θ is the trainable parameters of graph encoder."
    }, {
      "heading" : "4 Experiment",
      "text" : "We evaluate our methods in both the supervised setting and unsupervised “liberal” setting of EE."
    }, {
      "heading" : "4.1 Pre-training Setup",
      "text" : "Before the detailed experiments, we introduce the pre-training setup of CLEVE in implementation. We adopt the New York Times Corpus (NYT)1 (Sandhaus, 2008) as the unsupervised pretraining corpora for CLEVE. It contains over 1.8 million articles written and published by the New York Times between January 1, 1987, and June 19, 2007. We only use its raw text and obtain the AMR structures with a state-of-the-art AMR parser (Xu et al., 2020). We choose NYT corpus because (1) it is large and diverse, covering a wide range of event semantics, and (2) its text domain is similar to our principal evaluation dataset ACE 2005, which is helpful (Gururangan et al., 2020). To prevent data leakage, we remove all the articles shown up in ACE 2005 from the NYT corpus during pretraining. Moreover, we also study the effect of different AMR parsers and pre-training corpora in Section 5.2 and Section 5.3, respectively.\nFor the text encoder, we use the same model architecture as RoBERTa (Liu et al., 2019), which is with 24 layers, 1024 hidden dimensions and 16 attention heads, and we start our event semantic pre-training from the released checkpoint2. For the graph encoder, we adopt a graph isomorphism network (Xu et al., 2019) with 5 layers and 64 hidden dimensions, and pre-train it from scratch. For the detailed hyperparameters for pre-training and fine-tuning, please refer to Appendix D."
    }, {
      "heading" : "4.2 Adaptation of CLEVE",
      "text" : "As our work focuses on pre-training rather than fine-tuning for EE, we use straightforward and common techniques to adapt pre-trained CLEVE to downstream EE tasks. In the supervised setting, we adopt dynamic multi-pooling mechanism (Chen et al., 2015; Wang et al., 2019a,b) for the text encoder and encode the corresponding local subgraphs with the graph encoder. Then we concate-\n1https://catalog.ldc.upenn.edu/ LDC2008T19\n2https://github.com/pytorch/fairseq\nnate the two representations as features and finetune CLEVE on supervised datasets. In the unsupervised “liberal” setting, we follow the overall pipeline of Huang et al. (2016) and directly use the representations produced by pre-trained CLEVE as the required trigger/argument semantic representations and event structure representations. For the details, please refer to Appendix A."
    }, {
      "heading" : "4.3 Supervised EE",
      "text" : ""
    }, {
      "heading" : "Dataset and Evaluation",
      "text" : "We evaluate our models on the most widely-used ACE 2005 English subset (Walker et al., 2006) and the newly-constructed large-scale MAVEN (Wang et al., 2020) dataset. ACE 2005 contains 599 English documents, which are annotated with 8 event types, 33 subtypes, and 35 argument roles. MAVEN contains 4, 480 documents and 168 event types, which can only evaluate event detection. We split ACE 2005 following previous EE work (Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015) and use the official split for MAVEN. EE performance is evaluated with the performance of two subtasks: Event Detection (ED) and Event Argument Extraction (EAE). We report the precision (P), recall (R) and F1 scores as evaluation results, among which F1 is the most comprehensive metric.\nBaselines We fine-tune our pre-trained CLEVE and set the original RoBERTa without our event semantic pre-training as an important baseline. To do ablation studies, we evaluate two variants of CLEVE on both datasets: the w/o semantic model adopts a vanilla RoBERTa without event semantic pre-training as the text encoder, and the w/o structure only uses the event semantic representations\nwithout event structure pre-training. On ACE 2005, we set two more variants to investigate the effectiveness of CLEVE. The on ACE (golden) model is pre-trained with the golden trigger-argument pairs and event structures of ACE 2005 training set instead of the AMR structures of NYT. Similarly, the on ACE (AMR) model is pre-trained with the parsed AMR structures of ACE 2005 training set. We also compare CLEVE with various baselines, including: (1) feature-based method, the top-performing JointBeam (Li et al., 2013); (2) vanilla neural model DMCNN (Chen et al., 2015); (3) the model incorporating syntactic knowledge, dbRNN (Sha et al., 2018); (4) stateof-the-art models on ED and EAE respectively, including GatedGCN (Lai et al., 2020) and SemSynGTN (Pouran Ben Veyseh et al., 2020); (5) a stateof-the-art EE model RCEE ER (Liu et al., 2020), which tackle EE with machine reading comprehension (MRC) techniques. The last four models adopt PLMs to learn representations.\nOn MAVEN, we compare CLEVE with the official ED baselines set by Wang et al. (2020), including DMCNN (Chen et al., 2015), BiLSTM (Hochreiter and Schmidhuber, 1997), BiLSTM+CRF, MOGANED (Yan et al., 2019), DMBERT (Wang et al., 2019a), BERT+CRF."
    }, {
      "heading" : "Evaluation Results",
      "text" : "The evaluation results are shown in Table 1 and Table 2. We can observe that: (1) CLEVE achieves significant improvements to its basic model RoBERTa on both ACE 2005 and MAVEN. The p-values under the t-test are 4×10−8, 2×10−8 and 6× 10−4 for ED on ACE 2005, EAE on ACE 2005, and ED on MAVEN, respectively. It also outperforms or achieves comparable results with\nall the baselines, including those using dependency parsing information (dbRNN, GatedGCN, SemSynGTN and MOGANED). This demonstrates the effectiveness of our proposed contrastive pre-training method and AMR semantic structure. It is noteworthy that RCEE ER outperforms our method in EAE since its special advantages brought by reformulating EE as an MRC task to utilize sophisticated MRC methods and large annotated external MRC data. Considering that our method is essentially a pre-training method learning better eventoriented representations, CLEVE and RCEE ER can naturally work together to improve EE further. (2) The ablation studies (comparisons between CLEVE and its w/o semantic or structure representations variants) indicate that both event semantic pre-training and event structure pre-training is essential to our method. (3) From the comparisons between CLEVE and its variants on ACE (golden) and ACE (AMR), we can see that the AMR parsing inevitably brings data noise compared to golden annotations, which results in a performance drop. However, this gap can be easily made up by the benefits of introducing large unsupervised data with pre-training."
    }, {
      "heading" : "4.4 Unsupervised “Liberal” EE",
      "text" : ""
    }, {
      "heading" : "Dataset and Evaluation",
      "text" : "In the unsupervised setting, we evaluate CLEVE on ACE 2005 and MAVEN with both objective automatic metrics and human evaluation. For the automatic evaluation, we adopt the extrinsic clustering evaluation metrics: B-Cubed Metrics (Bagga and Baldwin, 1998), including B-Cubed precision, recall and F1. The B-Cubed metrics evaluate the quality of cluster results by comparing them to golden standard annotations and have been shown to be effective (Amigó et al., 2009). For the human evaluation, we invite an expert to check the outputs\nof the models to evaluate whether the extracted events are complete and correctly clustered as well as whether all the events in text are discovered.\nBaselines We compare CLEVE with reproduced LiberalEE (Huang et al., 2016), RoBERTa and RoBERTa+VGAE. RoBERTa here adopts the original RoBERTa (Liu et al., 2019) without event semantic pre-training to produce semantic representations for trigger and argument candidates in the same way as CLEVE, and encode the whole sentences to use the sentence embeddings (embeddings of the starting token <s>) as the needed event structure representations. RoBERTa+VGAE additionally adopts an unsupervised model Variational Graph Auto-Encoder (VGAE) (Kipf and Welling, 2016) to encode the AMR structures as event structure representations. RoBERTa+VGAE shares similar model architectures with CLEVE but is without our pre-training. Specially, for fair comparisons with LiberalEE, all the models in the unsupervised experiments adopt the same CAMR (Wang et al., 2015) as the AMR parser, including CLEVE pretraining. Moreover, we also study CLEVE variants as in the supervised setting. The w/o semantic variant replaces the CLEVE text encoder with a RoBERTa without event structure pre-training. The w/o structure variant only uses CLEVE text encoder in a similar way as RoBERTa. The on ACE (AMR) model is pre-trained with the parsed AMR structures of ACE test set. As shown in Huang et al. (2016), the AMR parsing is significantly superior to dependency parsing and frame semantic parsing on the unsupervised “liberal” event extraction task, hence we do not include baselines using other sentence structures in the experiments."
    }, {
      "heading" : "Evaluation Results",
      "text" : "The automatic evaluation results are shown in Table 3 and Table 4. As the human evaluation is laborious and expensive, we only do human\nevaluations for CLEVE and the most competitive baseline LiberalEE on ACE 2005, and the results are shown in Table 5. We can observe that: (1) CLEVE significantly outperforms all the baselines, which shows its superiority in both extracting event instances and discovering event schemata. (2) RoBERTa ignores the structure information. Although RoBERTa+VAGE encodes event structures with VGAE, the semantic representations of RoBERTa and the structure representations of VGAE are distinct and thus cannot work together well. Hence the two models even underperform LiberalEE, while the two representations of CLEVE can collaborate well to improve “liberal” EE. (3) In the ablation studies, the discarding of event structure pre-training results in a much more significant performance drop than in the supervised setting, which indicates event structures are essential to discovering new event schemata."
    }, {
      "heading" : "5 Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Effect of Supervised Data Size",
      "text" : "In this section, we study how the benefits of pretraining change along with the available supervised data size. We compare the ED performance on MAVEN of CLEVE, RoBERTa and a non-pretraining model BiLSTM+CRF when trained on different proportions of randomly-sampled MAVEN training data in Figure 3. We can see that the im-\nprovements of CLEVE compared to RoBERTa and the pre-training models compared to the non-pretraining model are generally larger when less supervised data available. It indicates that CLEVE is especially helpful for low-resource EE tasks, which is common since the expensive event annotation."
    }, {
      "heading" : "5.2 Effect of AMR Parsers",
      "text" : "CLEVE relies on automatic AMR parsers to build self-supervision signals for large unsupervised data. Intuitively, the performance of AMR parsers will influence CLEVE performance. To analyze the effect of different AMR parsing performance, we compare supervised EE results of CLEVE models using the established CAMR (Wang et al., 2016) and a new state-of-the-art parser (Xu et al., 2020) during pre-training in Table 6. We can see that a better AMR parser intuitively brings better EE performance, but the improvements are not so significant as the corresponding AMR performance improvement, which indicates that CLEVE is generally robust to the errors in AMR parsing."
    }, {
      "heading" : "5.3 Effect of Pre-training Domain",
      "text" : "Pre-training on similar text domains may further improve performance on corresponding downstream tasks (Gururangan et al., 2020; Gu et al., 2020). To analyze this effect, we evaluate the supervised EE performance of CLEVE pre-trained on NYT and English Wikipedia in Table 7. We can see pre-training on a similar domain (NYT for\nACE 2005, Wikipedia for MAVEN) surely benefits CLEVE on corresponding datasets. On ACE 2005, although Wikipedia is 2.28 times as large as NYT, CLEVE pre-trained on it underperforms CLEVE pre-trained on NYT (both in the news domain). Moreover, we can see the in-domain benefits mainly come from the event semantics rather than structures in CLEVE framework (from the comparisons between the w/o semantic and w/o structure results). It suggests that we can develop domain adaptation techniques focusing on semantics for CLEVE, and we leave it to future work."
    }, {
      "heading" : "6 Conclusion and Future work",
      "text" : "In this paper, we propose CLEVE, a contrastive pre-training framework for event extraction to utilize the rich event knowledge lying in large unsupervised data. Experiments on two real-world datasets show that CLEVE can achieve significant improvements in both supervised and unsupervised “liberal” settings. In the future, we will (1) explore other kinds of semantic structures like the frame semantics and (2) attempt to overcome the noise in unsupervised data brought by the semantic parsers."
    }, {
      "heading" : "Acknowledgement",
      "text" : "This work is supported by the National Natural Science Foundation of China Key Project (NSFC No. U1736204), grants from Beijing Academy of Artificial Intelligence (BAAI2019ZD0502) and the Institute for Guo Qiang, Tsinghua University (2019GQB0003). This work is also supported by the Pattern Recognition Center, WeChat AI, Tencent Inc. We thank Lifu Huang for his help on the unsupervised experiments and the anonymous reviewers for their insightful comments."
    }, {
      "heading" : "Ethical Considerations",
      "text" : "We discuss the ethical considerations and broader impact of the proposed CLEVE method in this section: (1) Intellectual property. NYT and ACE 2005 datasets are obtained from the linguistic data consortium (LDC), and are both licensed to be used for research. MAVEN is publicly shared under the CC BY-SA 4.0 license3. The Wikipedia corpus is obtained from the Wikimedia dump4, which is\n3https://creativecommons.org/licenses/ by-sa/4.0/\n4https://dumps.wikimedia.org/\nshared under the CC BY-SA 3.0 license5. The invited expert is fairly paid according to agreed working hours. (2) Intended use. CLEVE improves event extraction in both supervised and unsupervised settings, i.e., better extract structural events from diverse raw text. The extracted events then help people to get information conveniently and can be used to build a wide range of application systems like information retrieval (Glavaš and Šnajder, 2014) and knowledge base population (Ji and Grishman, 2011). As extracting events is fundamental to various applications, the failure cases and potential bias in EE methods also have a significant negative impact. We encourage the community to put more effort into analyzing and mitigating the bias in EE systems. Considering CLEVE does not model people’s characteristics, we believe CLEVE will not bring significant additional bias. (3) Misuse risk. Although all the datasets used in this paper are public and licensed, there is a risk to use CLEVE methods on private data without authorization for interests. We encourage the regulators to make efforts to mitigate this risk. (4) Energy and carbon costs. To estimate the energy and carbon costs, we present the computing platform and running time of our experiments in Appendix E for reference. We will also release the pre-trained checkpoints to avoid the additional carbon costs of potential users. We encourage the users to try model compression techniques like distillation and quantization in deployment to reduce carbon costs."
    }, {
      "heading" : "A Downstream Adaptation of CLEVE",
      "text" : "In this section, we introduce how to adapt pretrained CLEVE to make the event semantic and structure representations work together in downstream event extraction settings in detail, including supervised EE and unsupervised “liberal” EE."
    }, {
      "heading" : "A.1 Supervised EE",
      "text" : "In supervised EE, we fine-tune the pre-trained text encoder and graph encoder of CLEVE with annotated data. We formulate both event detection (ED) and event argument extraction (EAE) as multiclass classification tasks. An instance is defined as a sentence with a trigger candidate for ED, and a sentence with a given trigger and an argument candidate for EAE. The key question here is how to obtain features of an instance to be classified.\nFor the event semantic representation, we adopt dynamic multi-pooling to aggregate the embeddings produced by text encoder into a unified semantic representation xsem following previous work (Chen et al., 2015; Wang et al., 2019a,b). Moreover, we also insert special markers to indicate candidates as in pre-training (Section 3.2). For the event structure representation, we parse the sentence into an AMR graph and find the corresponding node v of the trigger/argument candidate to be classified. Following Qiu et al. (2020), we encode v and its one-hop neighbors with the graph encoder to get the desired structure representation gstr. The initial node representation is also obtained with the text encoder as introduced in Section 3.3.\nWe concatenate xsem and gstr as the instance embedding and adopt a multi-layer perceptron along with softmax to get the logits. Then we fine-tune CLEVE with cross-entropy loss."
    }, {
      "heading" : "A.2 Unsupervised “Liberal” EE",
      "text" : "Unsupervised “liberal” EE requires to discover event instances and event schemata only from raw text. We follow the pipeline of Huang et al. (2016) to parse sentences into AMR graphs and identify trigger and argument candidates with the AMR structures. We also cluster the candidates to get event instances and schemata with the joint constraint clustering algorithm (Huang et al., 2016), which requires semantic representations of the trigger and argument candidates as well as the event structure representations. The details of this clustering algorithm is introduced in Appendix B. Here we straightforwardly use the corresponding text\nspan representations (Section 3.2) as semantic representations and encode the whole AMR graphs with the graph encoder to get desired event structure representations."
    }, {
      "heading" : "B Joint Constraint Clustering Algorithm",
      "text" : "In the unsupervised “liberal” event extraction (Huang et al., 2016), the joint constraint clustering algorithm is introduced to get trigger and argument clusters given trigger and argument candidate representations. CLEVE focuses on learning event-specific representations and can use any clustering algorithm. To fairly compare with Huang et al. (2016), we also use the joint constraint clustering algorithm in our unsupervised evaluation. Hence we briefly introduce this algorithm here."
    }, {
      "heading" : "B.1 Preliminaries",
      "text" : "The input of this algorithm contains a trigger candidate set T and an argument candidate set A as well as their semantic representations ETg and E A g , respectively. There is also an event structure representation EtR for each trigger t. We also previously set the ranges of the numbers of resulting trigger and argument clusters: the minimal and maximal number of trigger clusters KminT , K max T as well as the minimal and maximal number of argument clusters KminA , K max A . The algorithm will output the optimal trigger clusters CT = {CT1 , ..., CTKT } and argument clusters CA = {CA1 , ..., CAKA}."
    }, {
      "heading" : "B.2 Similarity Functions",
      "text" : "The clustering algorithm requires to define triggertrigger similarities and argument-argument similarities. Huang et al. (2016) first defines the constraint function f :\nf(P1,P2) = log(1 + |L1 ∩ L2| |L1 ∪ L2| ). (4)\nWhen P1 and P2 are two triggers, Li has tuple elements (Pi, r, id(a)), which means the argument a has a relation r to trigger Pi. id(a) is the cluster ID for the argument a. When Pi is arguments, Li changes to corresponding triggers and semantic relations accordingly.\nHence the similarity functions are defined as:\nsim(t1, t2) = λ simcos(E t1 g , E t2 g ) + f(t1, t2)\n+ (1− λ)\n∑ r∈Rt1∩Rt2 simcos(E t1 r , E t2 r )\n|Rt1 ∩Rt2 | ,\nsim(a1, a2) = simcos(E a1 g , E a2 g ) + f(a1, a2)\n(5)\nwhere Etg and E a g are trigger and argument semantic representations, respectively. Rt is the AMR relation set in the parsed AMR graph of trigger t. Etr denotes the event structure representation of the node that has a semantic relation r to trigger t in the event structure. λ is a hyper-parameter. simcos(·, ·) is the cosine similarity."
    }, {
      "heading" : "B.3 Objective",
      "text" : "Huang et al. (2016) also defines an objective function O(·, ·) to evaluate the quality of trigger clusters CT = {CT1 , ..., CTKT } and argument clusters CA = {CA1 , ..., CAKA}. It is defined as follows:\nO(CT , CA) = Dinter(C T ) +Dintra(C T )\n+Dinter(C A) +Dintra(C A),\nDinter(C P) = KP∑ i6=j=1 ∑ u∈CPi ,v∈C P j sim(Pu,Pv),\nDintra(C P) = KP∑ i=1 ∑ u,v∈CPi (1− sim(Pu,Pv)),\n(6)\nwhere Dinter(·) measures the agreement across clusters, and Dintra(·) measures the disagreement within clusters. The clustering algorithm iteratively minimizes the objective function."
    }, {
      "heading" : "B.4 Overall Pipeline",
      "text" : "This algorithm updates its clustering results iteratively. At first, it uses the Spectral Clustering algorithm (Von Luxburg, 2007) to get initial clustering results. Then for each iteration, it updates clustering results and the best objective value using previous clustering results. It selects the clusters with the minimum O value as the final result. The overall pipeline is shown in Algorithm 1."
    }, {
      "heading" : "C Subgraph Sampling",
      "text" : "In the AMR subgraph discrimination task of event structure pre-training, we need to sample subgraphs from the parsed AMR graphs for contrastive pretraining. Here we adopt the subgraph sampling strategy introduced by Qiu et al. (2020), which consists of the random walk with restart (RWR), subgraph induction and anonymization:\n• Random walk with restart first randomly chooses a starting node (the ego) from the AMR graph to be sampled from. The ego must be a root node, i.e., there is no directed edge in the AMR graph pointing to the node. Then we treat the AMR graph as an undirected graph\nAlgorithm 1 Joint Constraint Clustering Algorithm Input: Trigger candidate set T , Argument candidate set A,\ntheir semantic representations ETg and EAg , structure representations EtR for each trigger t, the minimal and maximal number of trigger clusters KminT , K max T as well as the minimal and maximal number of argument clusters KminA , K max A ;\nOutput: Optimal trigger clusters CT = {CT1 , ..., CTKT } and argument clusters CA = {CA1 , ..., CAKA};\n• Omin =∞, CT = ∅, CA = ∅\n• For KT = KminT to K max T , KA = K min A to K max A\n– Clustering with Spectral Clustering Algorithm: – CTcurr = spectral(T,ETg , ETR ,KT , CAcurr) – CAcurr = spectral(A,EAg ,KA) – Ocurr = O(CTcurr, CAcurr) – if Ocurr ≤ Omin\n* Omin = Ocurr, C T = CTcurr, C A = CAcurr – while iterate time ≤ 10\n* C T curr = spectral(T,E T g , E T R ,KT , C A curr) * C A curr = spectral(A,E A g ,KA, C T curr) * Ocurr = O(C T curr, C A curr) * if Ocurr ≤ Omin · Omin = Ocurr, CT = CTcurr, CA = CAcurr\n• return Omin, CT , CA\nand do random walks starting from the ego. At each step, the random walk with a probability to return to the ego and restart. When all the neighbouring nodes of the current node have been visited, the RWR ends.\n• Subgraph induction is to take the induced subgraph of the node set obtained with RWR as the sampled subgraphs.\n• Anonymization is to randomly shuffle the indices of the nodes in the sampled subgraph to avoid overfitting to the node representations.\nIn our event structure pre-training, we take subgraphs of the same sentence (AMR graph) as positive pairs. But, ideally, the two subgraphs in a positive pair should be taken from the same event rather than only the same sentence. However, it is hard to unsupervisedly determine which parts of an AMR graph belong to the same event. We think this task is almost as hard as event extraction itself. The rule used in the event semantic pre-training only handles the ARG, time and location relations, and for the other about 100 AMR relations, we cannot find an effective method to determine\nwhich event their edges belong to. Hence, to take advantage of all the structure information, we adopt a simple assumption that the subgraphs from the same sentence express the same event (or at least close events) to design the subgraph sampling part here. We will explore more sophisticated subgraphsampling strategies in our future work."
    }, {
      "heading" : "D Hyperparameter Setup",
      "text" : ""
    }, {
      "heading" : "D.1 Pre-training Hyperparameters",
      "text" : "During pre-training, we manually tune the hyperparameters and select the models by the losses on a held-out validation set with 1, 000 sentences. The event structure pre-training hyperparameters mainly follow the E2E model of Qiu et al. (2020). Table 8 and Table 9 show the best-performing hyper-parameters used in experiments of the event semantic pre-training and event structure pretraining, respectively."
    }, {
      "heading" : "D.2 Fine-tuning Hyperparameters",
      "text" : "CLEVE in the unsupervised “liberal” setting directly uses the pre-trained representations and hence does not have additional hyperparameters. For the fine-tuning in the supervised setting, we manually tune the hyperparameters by 10 trials. In each trial, we train the models for 30 epochs and select models by their F1 scores on the validation set. Table 10 shows the best fine-tuning hyperparameters for CLEVE models and RoBERTa. For the other baselines, we take their reported results."
    }, {
      "heading" : "E Training Details",
      "text" : "For reproducibility and estimating energy and carbon costs, we report the computing infrastructures and average runtime of experiments as well as validation performance."
    }, {
      "heading" : "E.1 Pre-training Details",
      "text" : "For pre-training, we use 8 RTX 2080 Ti cards. The event semantic pre-training takes 12.3 hours. The event structure pre-training takes 60.2 hours."
    }, {
      "heading" : "E.2 Fine-tuning/Inference Details",
      "text" : "During the fine-tuning in the supervised setting and the inference in the unsupervised “liberal” setting, we also use 8 RTX 2080 Ti cards.\nFor the supervised EE experiments, Table 11 and Table 12 show the runtime and the results on the validation set of the model implemented by us.\nIn the unsupervised ”liberal” setting, we only do inference and do not involve the validation. We report the runtime of our models in Table 13."
    } ],
    "references" : [ {
      "title" : "A comparison of the events and relations across ACE, ERE, TAC-KBP, and FrameNet annotation standards",
      "author" : [ "Jacqueline Aguilar", "Charley Beller", "Paul McNamee", "Benjamin Van Durme", "Stephanie Strassel", "Zhiyi Song", "Joe Ellis." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Aguilar et al\\.,? 2014",
      "shortCiteRegEx" : "Aguilar et al\\.",
      "year" : 2014
    }, {
      "title" : "The stages of event extraction",
      "author" : [ "David Ahn." ],
      "venue" : "Proceedings of ACL Workshop on Annotating and Reasoning about Time and Events, pages 1–8.",
      "citeRegEx" : "Ahn.,? 2006",
      "shortCiteRegEx" : "Ahn.",
      "year" : 2006
    }, {
      "title" : "A Comparison of Extrinsic Clustering Evaluation Metrics Based on Formal Constraints",
      "author" : [ "Enrique Amigó", "Julio Gonzalo", "Javier Artiles", "Felisa Verdejo." ],
      "venue" : "Inf. Retr., 12(4):461–486.",
      "citeRegEx" : "Amigó et al\\.,? 2009",
      "shortCiteRegEx" : "Amigó et al\\.",
      "year" : 2009
    }, {
      "title" : "Entity-Based Cross-Document Coreferencing Using the Vector 5https://creativecommons.org/licenses/ by-sa/3.0",
      "author" : [ "Amit Bagga", "Breck Baldwin" ],
      "venue" : null,
      "citeRegEx" : "Bagga and Baldwin.,? \\Q1998\\E",
      "shortCiteRegEx" : "Bagga and Baldwin.",
      "year" : 1998
    }, {
      "title" : "Matching the Blanks: Distributional Similarity for Relation Learning",
      "author" : [ "Livio Baldini Soares", "Nicholas FitzGerald", "Jeffrey Ling", "Tom Kwiatkowski." ],
      "venue" : "Proceedings of ACL, pages 2895–2905.",
      "citeRegEx" : "Soares et al\\.,? 2019",
      "shortCiteRegEx" : "Soares et al\\.",
      "year" : 2019
    }, {
      "title" : "Abstract Meaning Representation for Sembanking",
      "author" : [ "Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider." ],
      "venue" : "Proceedings of the 7th Linguis-",
      "citeRegEx" : "Banarescu et al\\.,? 2013",
      "shortCiteRegEx" : "Banarescu et al\\.",
      "year" : 2013
    }, {
      "title" : "Event Schema Induction with a Probabilistic Entity-Driven Model",
      "author" : [ "Nathanael Chambers." ],
      "venue" : "Proceedings of EMNLP, pages 1797–1807.",
      "citeRegEx" : "Chambers.,? 2013",
      "shortCiteRegEx" : "Chambers.",
      "year" : 2013
    }, {
      "title" : "Template-Based Information Extraction without the Templates",
      "author" : [ "Nathanael Chambers", "Dan Jurafsky." ],
      "venue" : "Proceedings of ACL-HLT, pages 976–986.",
      "citeRegEx" : "Chambers and Jurafsky.,? 2011",
      "shortCiteRegEx" : "Chambers and Jurafsky.",
      "year" : 2011
    }, {
      "title" : "A Simple Framework for Contrastive Learning of Visual Representations",
      "author" : [ "Ting Chen", "Simon Kornblith", "Mohammad Norouzi", "Geoffrey Hinton." ],
      "venue" : "Proceedings of ICML, pages 1597–1607.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatically Labeled Data Generation for Large Scale Event Extraction",
      "author" : [ "Yubo Chen", "Shulin Liu", "Xiang Zhang", "Kang Liu", "Jun Zhao." ],
      "venue" : "Proceedings of ACL, pages 409–419.",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Event extraction via dynamic multipooling convolutional neural networks",
      "author" : [ "Yubo Chen", "Liheng Xu", "Kang Liu", "Daojian Zeng", "Jun Zhao." ],
      "venue" : "Proceedings of ACL-IJCNLP, pages 167–176.",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "ELECTRA: Pretraining Text Encoders as Discriminators Rather Than Generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V Le", "Christopher D Manning." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised Natural Language Inference via Decoupled Multimodal Contrastive Learning",
      "author" : [ "Wanyun Cui", "Guangyu Zheng", "Wei Wang." ],
      "venue" : "Proceedings of EMNLP, pages 5511–5520.",
      "citeRegEx" : "Cui et al\\.,? 2020",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of NAACL-HLT, pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Overview of linguistic resources for the TAC KBP 2015 evaluations: Methodologies and results",
      "author" : [ "Joe Ellis", "Jeremy Getman", "Dana Fore", "Neil Kuster", "Zhiyi Song", "Ann Bies", "Stephanie M Strassel." ],
      "venue" : "TAC.",
      "citeRegEx" : "Ellis et al\\.,? 2015",
      "shortCiteRegEx" : "Ellis et al\\.",
      "year" : 2015
    }, {
      "title" : "Overview of Linguistic Resources for the TAC KBP 2016 Evaluations: Methodologies and Results",
      "author" : [ "Joe Ellis", "Jeremy Getman", "Dana Fore", "Neil Kuster", "Zhiyi Song", "Ann Bies", "Stephanie M Strassel." ],
      "venue" : "TAC.",
      "citeRegEx" : "Ellis et al\\.,? 2016",
      "shortCiteRegEx" : "Ellis et al\\.",
      "year" : 2016
    }, {
      "title" : "Overview of linguistic resources for the tac kbp 2017 evaluations: Methodologies and results",
      "author" : [ "Jeremy Getman", "Joe Ellis", "Zhiyi Song", "Jennifer Tracey", "Stephanie Strassel." ],
      "venue" : "TAC.",
      "citeRegEx" : "Getman et al\\.,? 2017",
      "shortCiteRegEx" : "Getman et al\\.",
      "year" : 2017
    }, {
      "title" : "Event graphs for information retrieval and multi-document summarization",
      "author" : [ "Goran Glavaš", "Jan Šnajder." ],
      "venue" : "Expert systems with applications, 41(15):6904–6916.",
      "citeRegEx" : "Glavaš and Šnajder.,? 2014",
      "shortCiteRegEx" : "Glavaš and Šnajder.",
      "year" : 2014
    }, {
      "title" : "Train No Evil: Selective Masking for Task-Guided Pre-Training",
      "author" : [ "Yuxian Gu", "Zhengyan Zhang", "Xiaozhi Wang", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "Proceedings of EMNLP, pages 6966–6974.",
      "citeRegEx" : "Gu et al\\.,? 2020",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2020
    }, {
      "title" : "Predicting Unknown Time Arguments based on Cross-Event Propagation",
      "author" : [ "Prashant Gupta", "Heng Ji." ],
      "venue" : "Proceedings of ACL-IJCNLP, pages 369–372.",
      "citeRegEx" : "Gupta and Ji.,? 2009",
      "shortCiteRegEx" : "Gupta and Ji.",
      "year" : 2009
    }, {
      "title" : "Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks",
      "author" : [ "Suchin Gururangan", "Ana Marasović", "Swabha Swayamdipta", "Kyle Lo", "Iz Beltagy", "Doug Downey", "Noah A. Smith." ],
      "venue" : "Proceedings of ACL, pages 8342–8360.",
      "citeRegEx" : "Gururangan et al\\.,? 2020",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2020
    }, {
      "title" : "Dimensionality Reduction by Learning an Invariant Mapping",
      "author" : [ "Raia Hadsell", "Sumit Chopra", "Yann LeCun." ],
      "venue" : "Proceedings of CVPR, volume 2, pages 1735–1742.",
      "citeRegEx" : "Hadsell et al\\.,? 2006",
      "shortCiteRegEx" : "Hadsell et al\\.",
      "year" : 2006
    }, {
      "title" : "Momentum Contrast for Unsupervised Visual Representation Learning",
      "author" : [ "Kaiming He", "Haoqi Fan", "Yuxin Wu", "Saining Xie", "Ross Girshick." ],
      "venue" : "Proceedings of CVPR, pages 9726–9735.",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning deep representations by mutual information estimation and maximization",
      "author" : [ "R Devon Hjelm", "Alex Fedorov", "Samuel LavoieMarchildon", "Karan Grewal", "Phil Bachman", "Adam Trischler", "Yoshua Bengio." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Hjelm et al\\.,? 2019",
      "shortCiteRegEx" : "Hjelm et al\\.",
      "year" : 2019
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Liberal Event Extraction and Event Schema Induction",
      "author" : [ "Lifu Huang", "Taylor Cassidy", "Xiaocheng Feng", "Heng Ji", "Clare R. Voss", "Jiawei Han", "Avirup Sil." ],
      "venue" : "Proceedings of ACL, pages 258–268.",
      "citeRegEx" : "Huang et al\\.,? 2016",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2016
    }, {
      "title" : "Semi-supervised New Event Type Induction and Event Detection",
      "author" : [ "Lifu Huang", "Heng Ji." ],
      "venue" : "Proceedings of EMNLP, pages 718–724.",
      "citeRegEx" : "Huang and Ji.,? 2020",
      "shortCiteRegEx" : "Huang and Ji.",
      "year" : 2020
    }, {
      "title" : "Zero-Shot Transfer Learning for Event Extraction",
      "author" : [ "Lifu Huang", "Heng Ji", "Kyunghyun Cho", "Ido Dagan", "Sebastian Riedel", "Clare Voss." ],
      "venue" : "Proceedings of ACL, pages 2160–2170.",
      "citeRegEx" : "Huang et al\\.,? 2018",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2018
    }, {
      "title" : "Pretraining with Contrastive Sentence Objectives Improves Discourse Performance of Language Models",
      "author" : [ "Dan Iter", "Kelvin Guu", "Larry Lansing", "Dan Jurafsky." ],
      "venue" : "Proceedings of ACL, pages 4859–4870.",
      "citeRegEx" : "Iter et al\\.,? 2020",
      "shortCiteRegEx" : "Iter et al\\.",
      "year" : 2020
    }, {
      "title" : "Refining event extraction through cross-document inference",
      "author" : [ "Heng Ji", "Ralph Grishman." ],
      "venue" : "Proceedings of ACL, pages 254–262.",
      "citeRegEx" : "Ji and Grishman.,? 2008",
      "shortCiteRegEx" : "Ji and Grishman.",
      "year" : 2008
    }, {
      "title" : "Knowledge Base Population: Successful Approaches and Challenges",
      "author" : [ "Heng Ji", "Ralph Grishman." ],
      "venue" : "Proceedings of ACL, pages 1148–1158.",
      "citeRegEx" : "Ji and Grishman.,? 2011",
      "shortCiteRegEx" : "Ji and Grishman.",
      "year" : 2011
    }, {
      "title" : "Variational graph auto-encoders",
      "author" : [ "Thomas N Kipf", "Max Welling." ],
      "venue" : "NIPS Workshop on Bayesian Deep Learning.",
      "citeRegEx" : "Kipf and Welling.,? 2016",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2016
    }, {
      "title" : "A Mutual Information Maximization Perspective of Language Representation Learning",
      "author" : [ "Lingpeng Kong", "Cyprien de Masson d’Autume", "Wang Ling", "Lei Yu", "Zihang Dai", "Dani Yogatama" ],
      "venue" : "Proceedings of ICLR",
      "citeRegEx" : "Kong et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Kong et al\\.",
      "year" : 2020
    }, {
      "title" : "Event Detection: Gate Diversity and Syntactic Importance Scores for Graph Convolution Neural Networks",
      "author" : [ "Viet Dac Lai", "Tuan Ngo Nguyen", "Thien Huu Nguyen." ],
      "venue" : "Proceedings of EMNLP, pages 5405–5411.",
      "citeRegEx" : "Lai et al\\.,? 2020",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2020
    }, {
      "title" : "Joint event extraction via structured prediction with global features",
      "author" : [ "Qi Li", "Heng Ji", "Liang Huang." ],
      "venue" : "Proceedings of ACL, pages 73–82.",
      "citeRegEx" : "Li et al\\.,? 2013",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2013
    }, {
      "title" : "Using document level cross-event inference to improve event extraction",
      "author" : [ "Shasha Liao", "Ralph Grishman." ],
      "venue" : "Proceedings of ACL, pages 789–797.",
      "citeRegEx" : "Liao and Grishman.,? 2010",
      "shortCiteRegEx" : "Liao and Grishman.",
      "year" : 2010
    }, {
      "title" : "Event Extraction as Machine Reading Comprehension",
      "author" : [ "Jian Liu", "Yubo Chen", "Kang Liu", "Wei Bi", "Xiaojiang Liu." ],
      "venue" : "Proceedings of EMNLP, pages 1641–1651.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Efficient Estimation of Word Representations in Vector Space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Generative Event Schema Induction with Entity Disambiguation",
      "author" : [ "Kiem-Hieu Nguyen", "Xavier Tannier", "Olivier Ferret", "Romaric Besançon." ],
      "venue" : "Proceedings of ACL, pages 188–197.",
      "citeRegEx" : "Nguyen et al\\.,? 2015",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2015
    }, {
      "title" : "Graph convolutional networks with argument-aware pooling for event detection",
      "author" : [ "Thien Nguyen", "Ralph Grishman." ],
      "venue" : "Proceedings of AAAI, pages 5900–5907.",
      "citeRegEx" : "Nguyen and Grishman.,? 2018",
      "shortCiteRegEx" : "Nguyen and Grishman.",
      "year" : 2018
    }, {
      "title" : "Joint event extraction via recurrent neural networks",
      "author" : [ "Thien Huu Nguyen", "Kyunghyun Cho", "Ralph Grishman." ],
      "venue" : "Proceedings of NAACL, pages 300–309.",
      "citeRegEx" : "Nguyen et al\\.,? 2016",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "Event Detection and Domain Adaptation with Convolutional Neural Networks",
      "author" : [ "Thien Huu Nguyen", "Ralph Grishman." ],
      "venue" : "Proceedings of ACL, pages 365–371.",
      "citeRegEx" : "Nguyen and Grishman.,? 2015",
      "shortCiteRegEx" : "Nguyen and Grishman.",
      "year" : 2015
    }, {
      "title" : "Representation learning with contrastive predictive coding",
      "author" : [ "Aaron van den Oord", "Yazhe Li", "Oriol Vinyals." ],
      "venue" : "Proceedings of NIPS.",
      "citeRegEx" : "Oord et al\\.,? 2018",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning from Context or Names? An Empirical Study on Neural Relation Extraction",
      "author" : [ "Hao Peng", "Tianyu Gao", "Xu Han", "Yankai Lin", "Peng Li", "Zhiyuan Liu", "Maosong Sun", "Jie Zhou." ],
      "venue" : "Proceedings of EMNLP, pages 3661–3672.",
      "citeRegEx" : "Peng et al\\.,? 2020",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2020
    }, {
      "title" : "Graph Transformer Networks with Syntactic and Semantic Structures for Event Argument Extraction",
      "author" : [ "Amir Pouran Ben Veyseh", "Tuan Ngo Nguyen", "Thien Huu Nguyen." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP",
      "citeRegEx" : "Veyseh et al\\.,? 2020",
      "shortCiteRegEx" : "Veyseh et al\\.",
      "year" : 2020
    }, {
      "title" : "GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training",
      "author" : [ "Jiezhong Qiu", "Qibin Chen", "Yuxiao Dong", "Jing Zhang", "Hongxia Yang", "Ming Ding", "Kuansan Wang", "Jie Tang." ],
      "venue" : "Proceedings of KDD, page 1150–1160.",
      "citeRegEx" : "Qiu et al\\.,? 2020",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2020
    }, {
      "title" : "The new york times annotated corpus",
      "author" : [ "Evan Sandhaus." ],
      "venue" : "Linguistic Data Consortium, 6(12):e26752.",
      "citeRegEx" : "Sandhaus.,? 2008",
      "shortCiteRegEx" : "Sandhaus.",
      "year" : 2008
    }, {
      "title" : "Joint Learning Templates and Slots for Event Schema Induction",
      "author" : [ "Lei Sha", "Sujian Li", "Baobao Chang", "Zhifang Sui." ],
      "venue" : "Proceedings of NAACL-HLT, pages 428–434.",
      "citeRegEx" : "Sha et al\\.,? 2016",
      "shortCiteRegEx" : "Sha et al\\.",
      "year" : 2016
    }, {
      "title" : "Jointly Extracting Event Triggers and Arguments by Dependency-Bridge RNN and TensorBased Argument Interaction",
      "author" : [ "Lei Sha", "Feng Qian", "Baobao Chang", "Zhifang Sui." ],
      "venue" : "Proceedings of AAAI, pages 5916–5923.",
      "citeRegEx" : "Sha et al\\.,? 2018",
      "shortCiteRegEx" : "Sha et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving event detection via open-domain trigger knowledge",
      "author" : [ "Meihan Tong", "Bin Xu", "Shuai Wang", "Yixin Cao", "Lei Hou", "Juanzi Li", "Jun Xie." ],
      "venue" : "Proceedings of ACL, pages 5887–5897.",
      "citeRegEx" : "Tong et al\\.,? 2020",
      "shortCiteRegEx" : "Tong et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is All you Need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of NIPS, pages 5998– 6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "A tutorial on spectral clustering",
      "author" : [ "Ulrike Von Luxburg." ],
      "venue" : "Statistics and computing, 17(4):395–416.",
      "citeRegEx" : "Luxburg.,? 2007",
      "shortCiteRegEx" : "Luxburg.",
      "year" : 2007
    }, {
      "title" : "Entity, Relation, and Event Extraction with Contextualized Span Representations",
      "author" : [ "David Wadden", "Ulme Wennberg", "Yi Luan", "Hannaneh Hajishirzi." ],
      "venue" : "Proceedings of EMNLP-IJCNLP, pages 5784–5789.",
      "citeRegEx" : "Wadden et al\\.,? 2019",
      "shortCiteRegEx" : "Wadden et al\\.",
      "year" : 2019
    }, {
      "title" : "ACE 2005 multilingual training corpus",
      "author" : [ "Christopher Walker", "Stephanie Strassel", "Julie Medero", "Kazuaki Maeda." ],
      "venue" : "Linguistic Data Consortium, 57.",
      "citeRegEx" : "Walker et al\\.,? 2006",
      "shortCiteRegEx" : "Walker et al\\.",
      "year" : 2006
    }, {
      "title" : "CAMR at SemEval-2016 Task 8: An Extended Transition-based AMR Parser",
      "author" : [ "Chuan Wang", "Sameer Pradhan", "Xiaoman Pan", "Heng Ji", "Nianwen Xue." ],
      "venue" : "Proceedings of SemEval, pages 1173–1178.",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Boosting Transition-based AMR Parsing with Refined Actions and Auxiliary Analyzers",
      "author" : [ "Chuan Wang", "Nianwen Xue", "Sameer Pradhan." ],
      "venue" : "Proceedings of ACL-IJCNLP, pages 857–862.",
      "citeRegEx" : "Wang et al\\.,? 2015",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Adversarial Training for Weakly Supervised Event Detection",
      "author" : [ "Xiaozhi Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun", "Peng Li." ],
      "venue" : "Proceedings of NAACL-HLT, pages 998–1008.",
      "citeRegEx" : "Wang et al\\.,? 2019a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "MAVEN: A Massive General Domain Event Detection Dataset",
      "author" : [ "Xiaozhi Wang", "Ziqi Wang", "Xu Han", "Wangyi Jiang", "Rong Han", "Zhiyuan Liu", "Juanzi Li", "Peng Li", "Yankai Lin", "Jie Zhou." ],
      "venue" : "Proceedings of EMNLP, pages 1652–1671.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "HMEAE: Hierarchical Modular Event Argument Extraction",
      "author" : [ "Xiaozhi Wang", "Ziqi Wang", "Xu Han", "Zhiyuan Liu", "Juanzi Li", "Peng Li", "Maosong Sun", "Jie Zhou", "Xiang Ren." ],
      "venue" : "Proceedings of EMNLP-IJCNLP, pages 5777–5783.",
      "citeRegEx" : "Wang et al\\.,? 2019b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised Feature Learning via Nonparametric Instance Discrimination",
      "author" : [ "Zhirong Wu", "Yuanjun Xiong", "Stella X Yu", "Dahua Lin." ],
      "venue" : "Proceedings of CVPR, pages 3733–3742.",
      "citeRegEx" : "Wu et al\\.,? 2018",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving AMR Parsing with Sequence-to-Sequence Pre-training",
      "author" : [ "Dongqin Xu", "Junhui Li", "Muhua Zhu", "Min Zhang", "Guodong Zhou." ],
      "venue" : "Proceedings of EMNLP, pages 2501–2511.",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "How Powerful are Graph Neural Networks",
      "author" : [ "Keyulu Xu", "Weihua Hu", "Jure Leskovec", "Stefanie Jegelka" ],
      "venue" : "In Proceedings of ICLR",
      "citeRegEx" : "Xu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2019
    }, {
      "title" : "Event Detection with Multi-Order Graph Convolution and Aggregated Attention",
      "author" : [ "Haoran Yan", "Xiaolong Jin", "Xiangbin Meng", "Jiafeng Guo", "Xueqi Cheng." ],
      "venue" : "Proceedings of EMNLP-IJCNLP, pages 5766–5770.",
      "citeRegEx" : "Yan et al\\.,? 2019",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring Pre-trained Language Models for Event Extraction and Generation",
      "author" : [ "Sen Yang", "Dawei Feng", "Linbo Qiao", "Zhigang Kan", "Dongsheng Li." ],
      "venue" : "Proceedings of ACL, pages 5284–5294.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "QAInfomax: Learning Robust Question Answering System by Mutual Information Maximization",
      "author" : [ "Yi-Ting Yeh", "Yun-Nung Chen." ],
      "venue" : "Proceedings of EMNLP-IJCNLP, pages 3370–3375.",
      "citeRegEx" : "Yeh and Chen.,? 2019",
      "shortCiteRegEx" : "Yeh and Chen.",
      "year" : 2019
    }, {
      "title" : "Graph contrastive learning with augmentations",
      "author" : [ "Yuning You", "Tianlong Chen", "Yongduo Sui", "Ting Chen", "Zhangyang Wang", "Yang Shen." ],
      "venue" : "Proceedings of NeurIPS, pages 5812–5823.",
      "citeRegEx" : "You et al\\.,? 2020",
      "shortCiteRegEx" : "You et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep Graph Contrastive Representation Learning",
      "author" : [ "Yanqiao Zhu", "Yichen Xu", "Feng Yu", "Qiang Liu", "Shu Wu", "Liang Wang." ],
      "venue" : "ICML Workshop on Graph Representation Learning and Beyond.",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    }, {
      "title" : "2020), we encode v and its one-hop neighbors with the graph encoder to get the desired structure representation gstr. The initial node representation is also obtained with the text encoder",
      "author" : [ "classified. Following Qiu" ],
      "venue" : null,
      "citeRegEx" : "Qiu,? \\Q2020\\E",
      "shortCiteRegEx" : "Qiu",
      "year" : 2020
    }, {
      "title" : "Similarity Functions The clustering algorithm requires to define triggertrigger similarities and argument-argument similarities",
      "author" : [ "KA}. B" ],
      "venue" : null,
      "citeRegEx" : "B.2,? \\Q2016\\E",
      "shortCiteRegEx" : "B.2",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "classify event types (Attack), as well as event argument extraction task to identify entities serving as event arguments (“today” and “Netanya”) and classify their argument roles (Time-within and Place) (Ahn, 2006).",
      "startOffset" : 203,
      "endOffset" : 214
    }, {
      "referenceID" : 17,
      "context" : "By explicitly capturing the event structure in the text, EE can benefit various downstream tasks such as information retrieval (Glavaš and Šnajder, 2014) and knowledge base population (Ji and Grishman, 2011).",
      "startOffset" : 127,
      "endOffset" : 153
    }, {
      "referenceID" : 30,
      "context" : "By explicitly capturing the event structure in the text, EE can benefit various downstream tasks such as information retrieval (Glavaš and Šnajder, 2014) and knowledge base population (Ji and Grishman, 2011).",
      "startOffset" : 184,
      "endOffset" : 207
    }, {
      "referenceID" : 10,
      "context" : "Existing EE methods mainly follow the supervised-learning paradigm to train advanced neural networks (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018) with humanannotated datasets and pre-defined event schemata.",
      "startOffset" : 101,
      "endOffset" : 168
    }, {
      "referenceID" : 41,
      "context" : "Existing EE methods mainly follow the supervised-learning paradigm to train advanced neural networks (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018) with humanannotated datasets and pre-defined event schemata.",
      "startOffset" : 101,
      "endOffset" : 168
    }, {
      "referenceID" : 40,
      "context" : "Existing EE methods mainly follow the supervised-learning paradigm to train advanced neural networks (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018) with humanannotated datasets and pre-defined event schemata.",
      "startOffset" : 101,
      "endOffset" : 168
    }, {
      "referenceID" : 54,
      "context" : "These methods work well in lots of public benchmarks such as ACE 2005 (Walker et al., 2006) and TAC KBP (Ellis et al.",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 15,
      "context" : ", 2006) and TAC KBP (Ellis et al., 2016), yet they still suffer from data scarcity and limited generalizability.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 58,
      "context" : "Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods that can generalize to continually-emerging new event types (Huang and Ji, 2020).",
      "startOffset" : 54,
      "endOffset" : 73
    }, {
      "referenceID" : 26,
      "context" : ", 2020) and develop methods that can generalize to continually-emerging new event types (Huang and Ji, 2020).",
      "startOffset" : 88,
      "endOffset" : 108
    }, {
      "referenceID" : 57,
      "context" : "6284 neering work (Wang et al., 2019a; Wadden et al., 2019) attempts to fine-tune general PLMs (e.",
      "startOffset" : 18,
      "endOffset" : 59
    }, {
      "referenceID" : 53,
      "context" : "6284 neering work (Wang et al., 2019a; Wadden et al., 2019) attempts to fine-tune general PLMs (e.",
      "startOffset" : 18,
      "endOffset" : 59
    }, {
      "referenceID" : 9,
      "context" : "The key challenge here is to find reasonable self-supervised signals (Chen et al., 2017; Wang et al., 2019a) for the diverse semantics and complex structures of events.",
      "startOffset" : 69,
      "endOffset" : 108
    }, {
      "referenceID" : 57,
      "context" : "The key challenge here is to find reasonable self-supervised signals (Chen et al., 2017; Wang et al., 2019a) for the diverse semantics and complex structures of events.",
      "startOffset" : 69,
      "endOffset" : 108
    }, {
      "referenceID" : 0,
      "context" : "Fortunately, previous work (Aguilar et al., 2014; Huang et al., 2016) has suggested that sentence semantic structures, such as abstract meaning representation (AMR) (Banarescu et al.",
      "startOffset" : 27,
      "endOffset" : 69
    }, {
      "referenceID" : 25,
      "context" : "Fortunately, previous work (Aguilar et al., 2014; Huang et al., 2016) has suggested that sentence semantic structures, such as abstract meaning representation (AMR) (Banarescu et al.",
      "startOffset" : 27,
      "endOffset" : 69
    }, {
      "referenceID" : 5,
      "context" : ", 2016) has suggested that sentence semantic structures, such as abstract meaning representation (AMR) (Banarescu et al., 2013), contain broad and diverse semantic and structure information relating to events.",
      "startOffset" : 103,
      "endOffset" : 127
    }, {
      "referenceID" : 56,
      "context" : "Considering the fact that the AMR structures of large-scale unsupervised data can be easily obtained with automatic parsers (Wang et al., 2015), we propose CLEVE, an event-oriented contrastive pre-training framework utilizing AMR structures to build self-supervision signals.",
      "startOffset" : 124,
      "endOffset" : 143
    }, {
      "referenceID" : 25,
      "context" : "Specifically, to learn effective event semantic representations, we employ a PLM as the text encoder and encourage the representations of the word pairs connected by the ARG, time, location edges in AMR structures to be closer in the semantic space than other unrelated words, since these pairs usually refer to the trigger-argument pairs of the same events (as shown in Figure 1) (Huang et al., 2016).",
      "startOffset" : 381,
      "endOffset" : 401
    }, {
      "referenceID" : 33,
      "context" : "Moreover, considering event structures are also helpful in extracting events (Lai et al., 2020) and generalizing to new event schemata (Huang et al.",
      "startOffset" : 77,
      "endOffset" : 95
    }, {
      "referenceID" : 27,
      "context" : ", 2020) and generalizing to new event schemata (Huang et al., 2018), we need to learn transferable event structure representations.",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 25,
      "context" : "This is a challenging unsupervised setting named “liberal event extraction” (Huang et al., 2016).",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 29,
      "context" : "Traditional EE methods (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013) rely on manually-crafted features to extract events.",
      "startOffset" : 23,
      "endOffset" : 83
    }, {
      "referenceID" : 19,
      "context" : "Traditional EE methods (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013) rely on manually-crafted features to extract events.",
      "startOffset" : 23,
      "endOffset" : 83
    }, {
      "referenceID" : 34,
      "context" : "Traditional EE methods (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013) rely on manually-crafted features to extract events.",
      "startOffset" : 23,
      "endOffset" : 83
    }, {
      "referenceID" : 42,
      "context" : "In recent years, the neural models become mainstream, which automatically learn effective features with neural networks, including convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015), recurrent neural networks (Nguyen et al.",
      "startOffset" : 161,
      "endOffset" : 207
    }, {
      "referenceID" : 10,
      "context" : "In recent years, the neural models become mainstream, which automatically learn effective features with neural networks, including convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015), recurrent neural networks (Nguyen et al.",
      "startOffset" : 161,
      "endOffset" : 207
    }, {
      "referenceID" : 41,
      "context" : ", 2015), recurrent neural networks (Nguyen et al., 2016), graph convolutional networks (Nguyen and Grishman, 2018; Lai et al.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 40,
      "context" : ", 2016), graph convolutional networks (Nguyen and Grishman, 2018; Lai et al., 2020).",
      "startOffset" : 38,
      "endOffset" : 83
    }, {
      "referenceID" : 33,
      "context" : ", 2016), graph convolutional networks (Nguyen and Grishman, 2018; Lai et al., 2020).",
      "startOffset" : 38,
      "endOffset" : 83
    }, {
      "referenceID" : 13,
      "context" : "With the recent successes of BERT (Devlin et al., 2019), PLMs have also been used for EE (Wang et al.",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 64,
      "context" : ", 2019), PLMs have also been used for EE (Wang et al., 2019a,b; Yang et al., 2019; Wadden et al., 2019; Tong et al., 2020).",
      "startOffset" : 41,
      "endOffset" : 122
    }, {
      "referenceID" : 53,
      "context" : ", 2019), PLMs have also been used for EE (Wang et al., 2019a,b; Yang et al., 2019; Wadden et al., 2019; Tong et al., 2020).",
      "startOffset" : 41,
      "endOffset" : 122
    }, {
      "referenceID" : 50,
      "context" : ", 2019), PLMs have also been used for EE (Wang et al., 2019a,b; Yang et al., 2019; Wadden et al., 2019; Tong et al., 2020).",
      "startOffset" : 41,
      "endOffset" : 122
    }, {
      "referenceID" : 54,
      "context" : "Although achieving remarkable performance in benchmarks such as ACE 2005 (Walker et al., 2006) and similar datasets (Ellis et al.",
      "startOffset" : 73,
      "endOffset" : 94
    }, {
      "referenceID" : 16,
      "context" : ", 2006) and similar datasets (Ellis et al., 2015, 2016; Getman et al., 2017; Wang et al., 2020), these PLM-based works solely focus on better finetuning rather than pre-training for EE.",
      "startOffset" : 29,
      "endOffset" : 95
    }, {
      "referenceID" : 58,
      "context" : ", 2006) and similar datasets (Ellis et al., 2015, 2016; Getman et al., 2017; Wang et al., 2020), these PLM-based works solely focus on better finetuning rather than pre-training for EE.",
      "startOffset" : 29,
      "endOffset" : 95
    }, {
      "referenceID" : 6,
      "context" : "Following works introduce more features like coreference chains (Chambers, 2013) and entities (Nguyen et al.",
      "startOffset" : 64,
      "endOffset" : 80
    }, {
      "referenceID" : 39,
      "context" : "Following works introduce more features like coreference chains (Chambers, 2013) and entities (Nguyen et al., 2015; Sha et al., 2016).",
      "startOffset" : 94,
      "endOffset" : 133
    }, {
      "referenceID" : 48,
      "context" : "Following works introduce more features like coreference chains (Chambers, 2013) and entities (Nguyen et al., 2015; Sha et al., 2016).",
      "startOffset" : 94,
      "endOffset" : 133
    }, {
      "referenceID" : 60,
      "context" : "(2006) following an intuitive motivation to learn similar representations for “neighboors” and distinct representations for “non-neighbors”, and is further widely used for selfsupervised representation learning in various domains, such as computer vision (Wu et al., 2018; Oord et al., 2018; Hjelm et al., 2019; Chen et al., 2020; He et al., 2020) and graph (Qiu et al.",
      "startOffset" : 255,
      "endOffset" : 347
    }, {
      "referenceID" : 43,
      "context" : "(2006) following an intuitive motivation to learn similar representations for “neighboors” and distinct representations for “non-neighbors”, and is further widely used for selfsupervised representation learning in various domains, such as computer vision (Wu et al., 2018; Oord et al., 2018; Hjelm et al., 2019; Chen et al., 2020; He et al., 2020) and graph (Qiu et al.",
      "startOffset" : 255,
      "endOffset" : 347
    }, {
      "referenceID" : 23,
      "context" : "(2006) following an intuitive motivation to learn similar representations for “neighboors” and distinct representations for “non-neighbors”, and is further widely used for selfsupervised representation learning in various domains, such as computer vision (Wu et al., 2018; Oord et al., 2018; Hjelm et al., 2019; Chen et al., 2020; He et al., 2020) and graph (Qiu et al.",
      "startOffset" : 255,
      "endOffset" : 347
    }, {
      "referenceID" : 8,
      "context" : "(2006) following an intuitive motivation to learn similar representations for “neighboors” and distinct representations for “non-neighbors”, and is further widely used for selfsupervised representation learning in various domains, such as computer vision (Wu et al., 2018; Oord et al., 2018; Hjelm et al., 2019; Chen et al., 2020; He et al., 2020) and graph (Qiu et al.",
      "startOffset" : 255,
      "endOffset" : 347
    }, {
      "referenceID" : 22,
      "context" : "(2006) following an intuitive motivation to learn similar representations for “neighboors” and distinct representations for “non-neighbors”, and is further widely used for selfsupervised representation learning in various domains, such as computer vision (Wu et al., 2018; Oord et al., 2018; Hjelm et al., 2019; Chen et al., 2020; He et al., 2020) and graph (Qiu et al.",
      "startOffset" : 255,
      "endOffset" : 347
    }, {
      "referenceID" : 38,
      "context" : "In the context of NLP, many established representation learning works can be viewed as contrastive learning methods, such as Word2Vec (Mikolov et al., 2013), BERT (Devlin et al.",
      "startOffset" : 134,
      "endOffset" : 156
    }, {
      "referenceID" : 13,
      "context" : ", 2013), BERT (Devlin et al., 2019; Kong et al., 2020) and ELECTRA (Clark et al.",
      "startOffset" : 14,
      "endOffset" : 54
    }, {
      "referenceID" : 32,
      "context" : ", 2013), BERT (Devlin et al., 2019; Kong et al., 2020) and ELECTRA (Clark et al.",
      "startOffset" : 14,
      "endOffset" : 54
    }, {
      "referenceID" : 65,
      "context" : "Similar to this work, contrastive learning is also widely-used to help specific tasks, including question answering (Yeh and Chen, 2019), discourse modeling (Iter et al.",
      "startOffset" : 116,
      "endOffset" : 136
    }, {
      "referenceID" : 28,
      "context" : "Similar to this work, contrastive learning is also widely-used to help specific tasks, including question answering (Yeh and Chen, 2019), discourse modeling (Iter et al., 2020), natural language inference (Cui et al.",
      "startOffset" : 157,
      "endOffset" : 176
    }, {
      "referenceID" : 12,
      "context" : ", 2020), natural language inference (Cui et al., 2020) and relation extraction (Peng et al.",
      "startOffset" : 36,
      "endOffset" : 54
    }, {
      "referenceID" : 5,
      "context" : "CLEVE relies on AMR structures (Banarescu et al., 2013) to build broad and diverse self-supervision signals for learning event knowledge from largescale unsupervised corpora.",
      "startOffset" : 31,
      "endOffset" : 55
    }, {
      "referenceID" : 56,
      "context" : "To do this, we use automatic AMR parsers (Wang et al., 2015; Xu et al., 2020) to parse the sentences in unsupervised corpora into AMR structures.",
      "startOffset" : 41,
      "endOffset" : 77
    }, {
      "referenceID" : 61,
      "context" : "To do this, we use automatic AMR parsers (Wang et al., 2015; Xu et al., 2020) to parse the sentences in unsupervised corpora into AMR structures.",
      "startOffset" : 41,
      "endOffset" : 77
    }, {
      "referenceID" : 51,
      "context" : "Like most PLMs, we adopt a multi-layer Transformer (Vaswani et al., 2017) as the text encoder since its strong representation capacity.",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 13,
      "context" : "CLEVE is agnostic to the model architecture and can use any general PLM, like BERT (Devlin et al., 2019) and RoBERTa (Liu et al.",
      "startOffset" : 83,
      "endOffset" : 104
    }, {
      "referenceID" : 43,
      "context" : "We adopt the cross-entropy loss here since it is more effective than other contrastive loss forms (Oord et al., 2018; Chen et al., 2020).",
      "startOffset" : 98,
      "endOffset" : 136
    }, {
      "referenceID" : 8,
      "context" : "We adopt the cross-entropy loss here since it is more effective than other contrastive loss forms (Oord et al., 2018; Chen et al., 2020).",
      "startOffset" : 98,
      "endOffset" : 136
    }, {
      "referenceID" : 33,
      "context" : "Previous work has shown that event-related structures are helpful in extracting new events (Lai et al., 2020) as well as discovering and generalizing to new event schemata (Huang et al.",
      "startOffset" : 91,
      "endOffset" : 109
    }, {
      "referenceID" : 26,
      "context" : ", 2020) as well as discovering and generalizing to new event schemata (Huang et al., 2016, 2018; Huang and Ji, 2020).",
      "startOffset" : 70,
      "endOffset" : 116
    }, {
      "referenceID" : 62,
      "context" : "Here we use a state-of-the-art GNN model, Graph Isomorphism Network (Xu et al., 2019), as our graph encoder for its strong representation ability.",
      "startOffset" : 68,
      "endOffset" : 85
    }, {
      "referenceID" : 46,
      "context" : "The basic idea is to learn similar representations for the subgraphs sampled from the same AMR graph by discriminating them from subgraphs sampled from other AMR graphs (Qiu et al., 2020).",
      "startOffset" : 169,
      "endOffset" : 187
    }, {
      "referenceID" : 47,
      "context" : "We adopt the New York Times Corpus (NYT)1 (Sandhaus, 2008) as the unsupervised pretraining corpora for CLEVE.",
      "startOffset" : 42,
      "endOffset" : 58
    }, {
      "referenceID" : 61,
      "context" : "We only use its raw text and obtain the AMR structures with a state-of-the-art AMR parser (Xu et al., 2020).",
      "startOffset" : 90,
      "endOffset" : 107
    }, {
      "referenceID" : 20,
      "context" : "We choose NYT corpus because (1) it is large and diverse, covering a wide range of event semantics, and (2) its text domain is similar to our principal evaluation dataset ACE 2005, which is helpful (Gururangan et al., 2020).",
      "startOffset" : 198,
      "endOffset" : 223
    }, {
      "referenceID" : 37,
      "context" : "For the text encoder, we use the same model architecture as RoBERTa (Liu et al., 2019), which is with 24 layers, 1024 hidden dimensions and 16 attention heads, and we start our event semantic pre-training from the released checkpoint2.",
      "startOffset" : 68,
      "endOffset" : 86
    }, {
      "referenceID" : 62,
      "context" : "For the graph encoder, we adopt a graph isomorphism network (Xu et al., 2019) with 5 layers and 64 hidden dimensions, and pre-train it from scratch.",
      "startOffset" : 60,
      "endOffset" : 77
    }, {
      "referenceID" : 54,
      "context" : "We evaluate our models on the most widely-used ACE 2005 English subset (Walker et al., 2006) and the newly-constructed large-scale MAVEN (Wang et al.",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 58,
      "context" : ", 2006) and the newly-constructed large-scale MAVEN (Wang et al., 2020) dataset.",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 35,
      "context" : "We split ACE 2005 following previous EE work (Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015) and use the official split for MAVEN.",
      "startOffset" : 45,
      "endOffset" : 106
    }, {
      "referenceID" : 34,
      "context" : "We split ACE 2005 following previous EE work (Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015) and use the official split for MAVEN.",
      "startOffset" : 45,
      "endOffset" : 106
    }, {
      "referenceID" : 10,
      "context" : "We split ACE 2005 following previous EE work (Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015) and use the official split for MAVEN.",
      "startOffset" : 45,
      "endOffset" : 106
    }, {
      "referenceID" : 34,
      "context" : "We also compare CLEVE with various baselines, including: (1) feature-based method, the top-performing JointBeam (Li et al., 2013); (2) vanilla neural model DMCNN (Chen et al.",
      "startOffset" : 112,
      "endOffset" : 129
    }, {
      "referenceID" : 10,
      "context" : ", 2013); (2) vanilla neural model DMCNN (Chen et al., 2015); (3) the model incorporating syntactic knowledge, dbRNN (Sha et al.",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 49,
      "context" : ", 2015); (3) the model incorporating syntactic knowledge, dbRNN (Sha et al., 2018); (4) stateof-the-art models on ED and EAE respectively, including GatedGCN (Lai et al.",
      "startOffset" : 64,
      "endOffset" : 82
    }, {
      "referenceID" : 33,
      "context" : ", 2018); (4) stateof-the-art models on ED and EAE respectively, including GatedGCN (Lai et al., 2020) and SemSynGTN (Pouran Ben Veyseh et al.",
      "startOffset" : 83,
      "endOffset" : 101
    }, {
      "referenceID" : 36,
      "context" : ", 2020); (5) a stateof-the-art EE model RCEE ER (Liu et al., 2020), which tackle EE with machine reading comprehension (MRC) techniques.",
      "startOffset" : 48,
      "endOffset" : 66
    }, {
      "referenceID" : 10,
      "context" : "(2020), including DMCNN (Chen et al., 2015), BiLSTM (Hochreiter and Schmidhuber, 1997), BiLSTM+CRF, MOGANED (Yan et al.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 24,
      "context" : ", 2015), BiLSTM (Hochreiter and Schmidhuber, 1997), BiLSTM+CRF, MOGANED (Yan et al.",
      "startOffset" : 16,
      "endOffset" : 50
    }, {
      "referenceID" : 63,
      "context" : ", 2015), BiLSTM (Hochreiter and Schmidhuber, 1997), BiLSTM+CRF, MOGANED (Yan et al., 2019), DMBERT (Wang et al.",
      "startOffset" : 72,
      "endOffset" : 90
    }, {
      "referenceID" : 3,
      "context" : "For the automatic evaluation, we adopt the extrinsic clustering evaluation metrics: B-Cubed Metrics (Bagga and Baldwin, 1998), including B-Cubed precision, recall and F1.",
      "startOffset" : 100,
      "endOffset" : 125
    }, {
      "referenceID" : 2,
      "context" : "The B-Cubed metrics evaluate the quality of cluster results by comparing them to golden standard annotations and have been shown to be effective (Amigó et al., 2009).",
      "startOffset" : 145,
      "endOffset" : 165
    }, {
      "referenceID" : 25,
      "context" : "Baselines We compare CLEVE with reproduced LiberalEE (Huang et al., 2016), RoBERTa and RoBERTa+VGAE.",
      "startOffset" : 53,
      "endOffset" : 73
    }, {
      "referenceID" : 37,
      "context" : "RoBERTa here adopts the original RoBERTa (Liu et al., 2019) without event semantic pre-training to produce semantic representations for trigger and argument candidates in the same way as CLEVE, and encode the whole sentences to use the sentence embeddings (embeddings of the starting token <s>) as the needed event structure representations.",
      "startOffset" : 41,
      "endOffset" : 59
    }, {
      "referenceID" : 31,
      "context" : "RoBERTa+VGAE additionally adopts an unsupervised model Variational Graph Auto-Encoder (VGAE) (Kipf and Welling, 2016) to encode the AMR structures as event structure representations.",
      "startOffset" : 93,
      "endOffset" : 117
    }, {
      "referenceID" : 56,
      "context" : "Specially, for fair comparisons with LiberalEE, all the models in the unsupervised experiments adopt the same CAMR (Wang et al., 2015) as the AMR parser, including CLEVE pretraining.",
      "startOffset" : 115,
      "endOffset" : 134
    }, {
      "referenceID" : 55,
      "context" : "To analyze the effect of different AMR parsing performance, we compare supervised EE results of CLEVE models using the established CAMR (Wang et al., 2016) and a new state-of-the-art parser (Xu et al.",
      "startOffset" : 136,
      "endOffset" : 155
    }, {
      "referenceID" : 61,
      "context" : ", 2016) and a new state-of-the-art parser (Xu et al., 2020) during pre-training in Table 6.",
      "startOffset" : 42,
      "endOffset" : 59
    }, {
      "referenceID" : 20,
      "context" : "Pre-training on similar text domains may further improve performance on corresponding downstream tasks (Gururangan et al., 2020; Gu et al., 2020).",
      "startOffset" : 103,
      "endOffset" : 145
    }, {
      "referenceID" : 18,
      "context" : "Pre-training on similar text domains may further improve performance on corresponding downstream tasks (Gururangan et al., 2020; Gu et al., 2020).",
      "startOffset" : 103,
      "endOffset" : 145
    }, {
      "referenceID" : 17,
      "context" : "The extracted events then help people to get information conveniently and can be used to build a wide range of application systems like information retrieval (Glavaš and Šnajder, 2014) and knowledge base population (Ji and Grishman, 2011).",
      "startOffset" : 158,
      "endOffset" : 184
    }, {
      "referenceID" : 30,
      "context" : "The extracted events then help people to get information conveniently and can be used to build a wide range of application systems like information retrieval (Glavaš and Šnajder, 2014) and knowledge base population (Ji and Grishman, 2011).",
      "startOffset" : 215,
      "endOffset" : 238
    } ],
    "year" : 2021,
    "abstractText" : "Event extraction (EE) has considerably benefited from pre-trained language models (PLMs) by fine-tuning. However, existing pre-training methods have not involved modeling event characteristics, resulting in the developed EE models cannot take full advantage of large-scale unsupervised data. To this end, we propose CLEVE, a contrastive pre-training framework for EE to better learn event knowledge from large unsupervised data and their semantic structures (e.g. AMR) obtained with automatic parsers. CLEVE contains a text encoder to learn event semantics and a graph encoder to learn event structures respectively. Specifically, the text encoder learns event semantic representations by self-supervised contrastive learning to represent the words of the same events closer than those unrelated words; the graph encoder learns event structure representations by graph contrastive pre-training on parsed eventrelated semantic structures. The two complementary representations then work together to improve both the conventional supervised EE and the unsupervised “liberal” EE, which requires jointly extracting events and discovering event schemata without any annotated data. Experiments on ACE 2005 and MAVEN datasets show that CLEVE achieves significant improvements, especially in the challenging unsupervised setting. The source code and pre-trained checkpoints can be obtained from https://github.com/THU-KEG/CLEVE.",
    "creator" : "LaTeX with hyperref"
  }
}