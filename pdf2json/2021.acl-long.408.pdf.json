{
  "name" : "2021.acl-long.408.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Obtaining Better Static Word Embeddings Using Contextual Embedding Models",
    "authors" : [ "Prakhar Gupta", "Martin Jaggi" ],
    "emails" : [ "prakhar.gupta@epfl.ch", "martin.jaggi@epfl.ch" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5241–5253\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5241"
    }, {
      "heading" : "1 Introduction",
      "text" : "Word embeddings—representations of words which reflect semantic and syntactic information carried by them are ubiquitous in Natural Language Processing. Static word representation models such as GLOVE (Pennington et al., 2014), CBOW, SKIPGRAM (Mikolov et al., 2013) and SENT2VEC (Pagliardini et al., 2018) obtain stand-alone representations which do not depend on their surrounding words or sentences (context). Contextual embedding models (Devlin et al., 2019; Peters et al., 2018; Liu et al., 2019; Radford et al., 2019; Schwenk and Douze, 2017) on the other hand, embed the contextual information as well into the word representations making them more expressive than static word representations in most use-cases.\nWhile recent progress on contextual embeddings has been tremendously impactful, static embeddings still remain fundamentally important in many scenarios as well:\n• Even when ignoring the training phase, the computational cost of using static word embeddings is typically tens of millions times lower than using standard contextual embedding models1, which is particularly important for latency-critical applications and on lowresource devices, and in view of environmental costs of NLP models (Strubell et al., 2019).\n• Many NLP tasks inherently rely on static word embeddings (Shoemark et al., 2019), for example for interpretability, or e.g. in research in bias detection and removal (Kaneko and Bollegala, 2019; Gonen and Goldberg, 2019; Manzini et al., 2019) and analyzing word vector spaces (Vulic et al., 2020) or other metrics which are non-contextual by choice.\n• Static word embeddings can complement contextual word embeddings, for separating static from contextual semantics (Barsalou, 1982; Rubio-Fernández, 2008), or for improving joint embedding performance on downstream tasks (Alghanmi et al., 2020).\nWe also refer the reader to this article2 illustrating several down-sides of using BERT-like models over static embedding models for non-specialist users. Indeed, we can see continued prevalence of static word embeddings in industry and research areas including but not limited to medicine (Zhang et al., 2019; Karadeniz and Özgür, 2019; Magna et al., 2020) and social sciences (Rheault and Cochrane, 2020; Gordon et al., 2020; Farrell et al., 2020; Lucy et al., 2020).\nFrom a cognitive science point of view, Human language has been hypothesized to have both con-\n1BERT base (Devlin et al., 2019) produces 768 dimensional word embeddings using 109M parameters, requiring 29B FLOPs per inference call (Clark et al., 2020).\n2Do humanists need BERT? (https:// tedunderwood.com/2019/07/15/)\ntextual as well as context-independent properties (Barsalou, 1982; Rubio-Fernández, 2008) underlining the need for continued research in studying the expressiveness context-independent embeddings on the level of words.\nMost existing word embedding models, whether static or contextual, follow Firth (1957)’s famous hypothesis - “You shall know a word by the company it keeps” , i.e., the meaning of a word arises from its context. During training existing static word embedding models, representations of contexts are generally approximated using averaging or sum of the constituent word embeddings, which disregards the relative word ordering as well as the interplay of information beyond simple pairs of words, thus losing most contextual information. Ad-hoc remedies attempt to capture longer contextual information per word using higher order n-grams like bigrams or trigrams, and have been shown to improve the performance of static word embedding models (Gupta et al., 2019; Zhao et al., 2017). However, these methods are not scalable to cover longer contexts.\nIn this work, we obtain improved static word embeddings by leveraging recent contextual embedding advances, namely by distilling existing contextual embeddings into static ones. Our proposed distillation procedure is inspired by existing CBOW-based static word embedding algorithms, but during training plugs in any existing contextual representation to serve as the context element of each word.\nOur resulting embeddings outperform the current static embedding methods, as well as the current state-of-the-art static embedding distillation method on both unsupervised lexical similarity tasks as well as on downstream supervised tasks, by a significant margin. The resulting static embeddings remain compatible with the underlying contextual model used, and thus allow us to gauge the extent of lexical information carried by static vs contextual word embeddings. We release our code and trained embeddings publicly on GitHub3."
    }, {
      "heading" : "2 Related Work",
      "text" : "A few methods for distilling static embeddings have already been proposed. Ethayarajh (2019) propose using contextual embeddings of the same word in a large number of different contexts. They take the first principal component of the matrix\n3https://github.com/epfml/X2Static\nformed by using these embeddings as rows and use it as a static embedding. However, this method is not scalable in terms of memory (the embedding matrix scaling with the number of contexts) and computational cost (PCA).\nBommasani et al. (2020) propose two different approaches to obtain static embeddings from contextual models.\n1. Decontextualized Static Embeddings - The word w alone without any context, after tokenization into constituents w1, . . . , wn is fed to the contextual embedding model denoted by M and the resulting static embedding is given by g(M(w1), . . . ,M(wn)) where g is a pooling operation. It is observed that these embeddings perform dismally on the standard static word embedding evaluation tasks.\n2. Aggregated Static Embeddings - Since contextual embedding models are not trained on a single word (without any context) as input, an alternative approach is to obtain the contextual embedding of the word w in different contexts and then pool(max, min or average) the embeddings obtained from these different contexts. They observe that average pooling leads to the best performance. We refer to this method (with average pooling) as ASE throughout the rest of the paper. As we see in our experiments, the performance of ASE embeddings saturates quickly with increasing size of the raw text corpus and is therefore not scalable.\nOther related work includes distillation of contextual word embeddings to obtain sentence embeddings (Reimers and Gurevych, 2019). We also refer the reader to Mickus et al. (2020) for a discussion on the semantic properties of contextual models (primarily BERT) as well as Rogers et al. (2020), a survey on different works exploring the inner workings of BERT including its semantic properties."
    }, {
      "heading" : "3 Proposed Method",
      "text" : "To distill existing contextual word representation models into static word embeddings, we augment a CBOW-inspired static word-embedding method as our anchor method to accommodate additional contextual information of the (contextual) teacher model. SENT2VEC (Pagliardini et al., 2018) is a\nmodification of the CBOW static word-embedding method which instead of a fixed-size context window uses the entire sentence to predict the masked word. It also has the ability to learn n-gram representations along with unigram representations, allowing to better disentangle local contextual information from the static unigram embeddings. SENT2VEC, originally meant to obtain sentence embeddings and later repurposed to obtain word representations (Gupta et al., 2019) was shown to outperform competing methods including GLOVE (Pennington et al., 2014), CBOW, SKIPGRAM (Mikolov et al., 2013) and FASTTEXT (Bojanowski et al., 2016) on word similarity evaluations. For a raw text corpus C (collection of sentences), the training objective is given by\nmin U ,V ∑ S∈C ∑ wt∈S f(uwt , Ectx(S,wt)) (1)\nwhere f(u,v) := `(u>v) + ∑\nw′∈N `(−u>w′v). Here, wt is the masked target word, U and V are the target word embedding and the source n-gram matrices respectively,N is the set of negative target samples and, ` : x 7→ log (1 + e−x) is the logistic loss function.\nFor SENT2VEC, the context encoder Ectx used in optimizing (1) is simply given by the (static, non-contextual) sum of all vectors in the sentence without the target word,\nEctx(S,wt) := 1\n|R(S\\{wt})| ∑ w∈R(S\\{wt}) vw , (2)\nwhere R(S) denotes the optional expansion of the sentence S from words to short n-grams, i.e., the context sentence embedding is obtained by averaging the embeddings of word n-grams in the sentence S.\nWe will now generalize the objective (1) by allowing the use of arbitrary modern contextual representations Ectx instead of the static context representation as in (2). This key element will allow us to translate quality gains from improved contextual representations also to better static word embedding in the resulting matrix U . We propose two different approaches of doing so, which differ in the granularity of context used for obtaining the contextual embeddings."
    }, {
      "heading" : "3.1 Approach 1 - Sentences as context",
      "text" : "Using contextual representations of all words in the sentence S (or the sentence S \\ {wt} without the\ntarget word) allows for a more refined representation of the context, and to take in account the word order as well as the interplay of information among the words of the context.\nMore formally, let M(S,w) denote the output of a contextual embedding-encoder, e.g. BERT, corresponding to the wordw when a piece of text S containing w is fed to it as input. We let Ectx(S,w) to be the average of all contextual embeddings of words w returned by the encoder,\nEctx(S,wt) := 1 |S| ∑ w∈S M(S,w) (3)\nThis allows for a more refined representation of the context as the previous representation did not take in account neither the word order nor the interplay of information among the words of the context. Certainly, using Smwt (S withwt masked) andw would make for an even better word-context pair but that would amount to one contextual embeddingencoder inference per word instead of one inference per sentence as is the case in (3) leading to a drastic drop in computational efficiency."
    }, {
      "heading" : "3.2 Approach 2 - Paragraphs as context",
      "text" : "Since contextual models are trained on large pieces of texts (generally ≥ 512 tokens), we instead use paragraphs instead of sentences to obtain the contextual representations. However, in order to predict target words, we use the contextual embeddings within the sentence only. Consequently, for this approach, we have\nEctx(S,wt) := 1 |S| ∑ w∈S M(PS , w), (4)\nwhere PS is the paragraph containing sentence S. In the transfer phase, this approach is more computationally efficient than the previous approach, as we have to invoke the contextual embedding model M only once for each paragraph as opposed to once for every constituent sentence. Moreover, it encapsulates the related semantic information in paragraphs in the contextual word embeddings.\nWe call our models X2STATICsent in the sentence case (3), and X2STATICpara in the paragraph case (4) respectively where X denotes the parent model."
    }, {
      "heading" : "4 Experiments and Discussion",
      "text" : ""
    }, {
      "heading" : "4.1 Corpus Preprocessing and Training",
      "text" : "We use the same English Wikipedia Dump as Pagliardini et al. (2018); Gupta et al. (2019) to\ngenerate distilled X2STATIC representations. as our corpus for training static word embedding baselines as well as for distilling static word embeddings from pre-trained contextual embedding models. We remove all paragraphs with less than 3 sentences or 140 characters, lowercase the characters and tokenize the corpus using the Stanford NLP library (Manning et al., 2014) resulting in a corpus of approximately 54 Million sentences and 1.28 Billion words. We then use the Transformers library4 (Wolf et al., 2020) to generate representations from existing transformer models. Our X2STATIC representations are distilled from the last representation layers of these models.\nWe use the same hyperparameter set for training all X2STATIC models, i.e., no hyperparameter tuning is done at all. We use 12-layer as well as 24- layer pre-trained models using BERT (Devlin et al., 2019), ROBERTA (Liu et al., 2019) and GPT2 (Radford et al., 2019) architectures as the teacher model to obtain X2STATIC word embeddings. All the X2STATIC models use the same set of training parameters except the parent model. Training hyperparameters are provided in Table 1. The distillation/training process employs the lazy version of the Adam optimizer (Kingma and Ba, 2015a), suitable for sparse tensors. We use a subsampling parameter similar to FASTTEXT (Bojanowski et al., 2016) in order to subsample frequent target words during training. Each X2STATIC model was trained using a single V100 32 GB GPU. Obtaining X2STATIC embeddings from 12-layer contextual embedding models took 15-18 hours while it took\n4https://huggingface.co/transformers/\n35-38 hours to obtain them from their 24-layer counterparts.\nTo ensure a fair comparison, we also evaluate SENT2VEC, CBOW and SKIPGRAM models that were trained on the same corpus. We do an extensive hyperparameter tuning for these models and choose the one which shows best average performance on the 5 word similarity datasets used in Subsection 4.2. These hyperparameter sets can be accessed in Table 2 where the chosen hyperparameters are shown in bold. We set the number of dimensions to be 768 to ensure parity between them and the X2STATIC models compared. We used the SENT2VEC library5 for training SENT2VEC and the FASTTEXT library6 for training CBOW and SKIPGRAM models. We also evaluate some pre-trained 300 dimensional GLOVE (Pennington et al., 2014) and FASTTEXT (Bojanowski et al., 2016) models in Table 3. The GLOVE model was trained on Common-Crawl corpus of 840 Billion tokens (approximately 650 times larger than our corpus) while the FASTTEXT vectors were trained on a corpus of 16 Billion tokens (approximately 12 times larger than our corpus)). We also extract ASE embeddings from each layer using the same Wikipedia corpus.\nWe perform two different sets of evaluations. The first set corresponds to unsupervised word similarity evaluations to gauge the quality of the obtained word embeddings. However, we recognize that there are concerns regarding word-similarity\n5https://github.com/epfml/sent2vec 6https://github.com/facebookresearch/\nfastText/\nevaluation tasks (Faruqui et al., 2016) as they are shown to exhibit significant difference in performance when subjected to hyperparameter tuning (Levy et al., 2015). To address these limitations in the evaluation, we also evaluate the X2STATIC embeddings on a standard set of downstream supervised evaluation tasks used in Pagliardini et al. (2018)."
    }, {
      "heading" : "4.2 Unsupervised word similarity evaluation",
      "text" : "To assess the quality of the lexical information contained in the obtained word representations, we use the 4 word-similarity datasets used by (Bommasani et al., 2020), namely WordSim353 (353 word-pairs) (Agirre et al., 2009) dataset; SimLex999 (999 word-pairs) (Hill et al., 2014) dataset; RG-65 (65 pairs) (Joubarne and Inkpen, 2011); and SimVerb-3500 (3500 pairs) (Gerz et al., 2016) dataset as well as the Rare Words RW-2034 (2034 pairs) (Luong et al., 2013) dataset. To calculate the similarity between two words, we use the cosine similarity between their word embeddings. These similarity scores are compared to the human ratings using Spearman’s ρ (Spearman, 1904) correlation scores. We use the tool7 provided by Bommasani et al. (2020) to report these results on ASE embeddings. It takes around 3 days to obtain ASE representations of the 2005 words in these word-similarity datasets for 12-layer models and around 5 days to obtain them for their 24-layer counterparts on the same machine used for learning X2STATIC representations. All other embeddings are evaluated using the MUSE repository evaluation tool8 (Lample et al., 2018).\nWe perform two sets of experiments concerning the unsupervised evaluation tasks. The first set is the comparison of our X2STATIC models with competing models. For ASE, we report two sets of results, one which per task reports the best result amongst all the layers and other, which reports the results obtained on the best performing layer on average.\nWe report our observations in Table 3. We provide additional results for larger models in Appendix B. We observe that X2STATIC embeddings outperform competing models on most of the tasks. Moreover, the extent of improvement on SimLex999 and SimVerb-3500 tasks compared to the pre-\n7https://github.com/rishibommasani/ Contextual2Static\n8https://github.com/facebookresearch/ MUSE\nvious models strongly highlights the advantage of using improved context representations for training static word representations.\nSecond, we study the performance of the best ASE embedding layer with respect to the size of corpus used. Bommasani et al. (2020) report their results on a corpus size of only up toN = 100, 000 sentences. In order to measure the full potential of the ASE method, we obtain different sets of ASE embeddings as well as X2STATICpara embeddings from small chunks of the corpus to the full wikipedia corpus itself and compare their performance on SimLex-999 and RW-2034 datasets. We choose SimLex-999 as it captures true similarity instead of relatedness or association (Hill et al., 2014) and RW-2034 to gauge the robustness of the embedding model on rare words. We report our observations in Figure 1. We observe that the performance of the ASE embeddings tends to saturate with the increase in the corpus size while X2STATICpara embeddings are either significantly outperforming the ASE embeddings or still show a significantly greater positive growth rate in performance w.r.t. the corpus size. Thus, the experimental evidence suggests that on larger texts, X2STATIC embeddings will have an even better performance and hence, X2STATIC is a better alternative than ASE embeddings from any of the layers of the contextual embedding model, and obtains improved static word embeddings from contextual embedding models."
    }, {
      "heading" : "4.3 Downstream supervised evaluation",
      "text" : "We evaluate the obtained word embeddings on various sentence-level supervised classification tasks. Six different downstream supervised evaluation tasks namely classification of movie review sentiment(MR) (Pang and Lee, 2005), product reviews(CR) (Hu and Liu, 2004), subjectivity classification(SUBJ) (Pang and Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005), question type classification (TREC) (Voorhees, 2002) and finegrained sentiment analysis (SST-5) (Socher et al., 2013) are employed to gauge the performance of the obtained word embeddings.\nWe use a standard CNN based architecture on the top of our embeddings to train our classifier. We use 100 convolutional filters with a kernel size of 3 followed by a ReLU activation function. A global max-pooling layer follows the convolution layer. Before feeding the max-pooled output to a\nclassifier, it is passed through a dropout layer with dropout probability of 0.5 to prevent overfitting. We use Adam (Kingma and Ba, 2015b) to train our classifier. To put the performance of these static models into a broader perspective, we also fine-tune linear classifiers on the top of their parent models as well as sentence-transformers (Reimers and Gurevych, 2019) obtained from ROBERTA-12 and BERT-12. For the sentence-transformer models, we use the sentence-transformer models obtained by fine-tuning their parent models on the Natural Language Inference(NLI) task using the combination of Stanford NLI (Bowman et al., 2015) and the Multi-Genre NLI (Williams et al., 2018) datasets. The models are refered to as SBERT-BASE-NLI and SROBERTA-BASE-NLI in the rest of the paper.\nThe hyperparameter search space for the finetuning process involves the number of epochs (8-\n16) and the learning rates[1e-4,3e-4,1e-3]. Wherever train, validation, and test split is not given, we use 60% of the data as the training data, 20% of the data as validation data and the rest as the test data. After obtaining the best hyperparameters, we train on the train and validation data together with these hyperparameters and predict the results on the test set. For the linear classifiers on the top of parent models, we set the number of epochs and learning rate search space for parent model + linear classifier combination to be [3,4,5,6] and [2e-5,5e-5] respectively. The learning rates in the learning rate search space are lower than those for static embeddings as the contextual embeddings are also fine-tuned and follow the recommendation of Devlin et al. (2019). For the sentence-transformer models, we only train the linear classifier and set the number of epochs and learning rate search space to be [3,4,5,6] and [1e-4,3e-4,1e-3] respectively. We use cross-entropy\nloss for training all the models. We use Macro-F1 score and Accuracy to gauge the quality of our predictions. We compare X2STATIC models with all other static models trained from scratch on the same corpus as well as the GLOVE and FASTTEXT models used in the previous section. We also use existing GLOVE embeddings trained on tweets(27 billion tokens - 20 times larger than our corpus) (Pennington et al., 2014) to make the comparison even more extensive. We report our observations in Table 4. For ASE embeddings, we take the layer with best average macro-F1 performance.\nWe observe that when measuring the overall performance, with the exception of ROBERTA2STATICsent which has similar av-\nerage F-1 score to ASE owing to its dismal performance on the CR task, all X2STATIC embeddings outperform their competitors by a significant margin. Even though the GLOVE and FASTTEXT embeddings were trained on corpora of one to two magnitudes larger and have a larger vocabulary, their performance lags behind that of the X2STATIC embeddings. To ensure statistical soundness, we measure mean and standard deviation of the performance on 6 runs of X2STATICpara model training followed by downstream evaluation along with 6 runs of ASE embedding downstream evaluation with different random seeds in Table 5 in the Appendix. We see that X2STATICpara embeddings outperform ASE\nby a significant margin.\nFor both word similarity evaluations and downstream supervised tasks, we observe that X2STATICpara embeddings perform slightly better than X2STATICsent embeddings. However, since no hyperparameter tuning was performed on the distillation of X2STATIC embeddings, it is hard to discern which X2STATIC variant shows better performance. Moreover, owing to the same fact concerning hyperparameter tuning, we expect to\nsee even larger improvements with proper hyperparameter tuning as well as training on larger data."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "This work proposes to augment earlier WORD2VEC-based methods by leveraging recent more expressive deep contextual embedding models to extract static word embeddings. The resulting distilled static embeddings, on an average, outperform their competitors on both unsupervised\nas well downstream supervised evaluations and thus can be used to replace compute-heavy contextual embedding models (or existing static embedding models) at inference time in many compute-resource-limited applications. The resulting embeddings can also be used as a task-agnostic tool to measure the lexical information conveyed by contextual embedding models and allow a fair comparison with their static analogues.\nFurther work can explore extending this distillation framework into cross-lingual domains (Schwenk and Douze, 2017; Lample and Conneau, 2019) as well as using better pooling methods instead of simple averaging for obtaining the context representation, or joint fine-tuning to obtain even stronger static word embeddings. Another promising avenue is the use of a similar approach to learn sense embeddings from contextual embedding models. We would also like to investigate the performance of these embeddings when distilled on a larger corpus along with more extensive hyperparameter tuning. Last but not the least, we would like to release X2STATIC models for different languages for further public use."
    }, {
      "heading" : "A Comparison of multiple downstream runs",
      "text" : ""
    }, {
      "heading" : "B Experiments on larger models",
      "text" : "In addition to the smaller 12-layer contextual embedding models, we also obtain X2STATIC word vectors from larger 24-layer contextual embedding models, once again outperforming their ASE counterparts by a significant margin. The evaluation results can be accessed in the Table 6."
    } ],
    "references" : [ {
      "title" : "A study on similarity and relatedness using distributional and wordnet-based approaches",
      "author" : [ "Eneko Agirre", "Enrique Alfonseca", "Keith B. Hall", "Jana Kravalova", "Marius Pasca", "Aitor Soroa." ],
      "venue" : "HLTNAACL.",
      "citeRegEx" : "Agirre et al\\.,? 2009",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2009
    }, {
      "title" : "Combining BERT with Static Word Embeddings for Categorizing Social Media",
      "author" : [ "Israa Alghanmi", "Luis Espinosa Anke", "Steven Schockaert." ],
      "venue" : "Proceedings of the Sixth Workshop on Noisy Usergenerated Text (W-NUT 2020), pages 28–33.",
      "citeRegEx" : "Alghanmi et al\\.,? 2020",
      "shortCiteRegEx" : "Alghanmi et al\\.",
      "year" : 2020
    }, {
      "title" : "Context-independent and contextdependent information in concepts",
      "author" : [ "L. Barsalou." ],
      "venue" : "Memory & Cognition, 10:82–93.",
      "citeRegEx" : "Barsalou.,? 1982",
      "shortCiteRegEx" : "Barsalou.",
      "year" : 1982
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:135–146.",
      "citeRegEx" : "Bojanowski et al\\.,? 2016",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2016
    }, {
      "title" : "Interpreting pretrained contextualized representations via reductions to static embeddings",
      "author" : [ "Rishi Bommasani", "Kelly Davis", "Claire Cardie." ],
      "venue" : "ACL.",
      "citeRegEx" : "Bommasani et al\\.,? 2020",
      "shortCiteRegEx" : "Bommasani et al\\.",
      "year" : 2020
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D Manning." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "ELECTRA: pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings",
      "author" : [ "Kawin Ethayarajh." ],
      "venue" : "EMNLP-IJCNLP - Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Ethayarajh.,? 2019",
      "shortCiteRegEx" : "Ethayarajh.",
      "year" : 2019
    }, {
      "title" : "On the use of jargon and word embeddings to explore subculture within the reddits manosphere",
      "author" : [ "T. Farrell", "Óscar Araque", "Miriam Fernández", "H. Alani." ],
      "venue" : "12th ACM Conference on Web Science.",
      "citeRegEx" : "Farrell et al\\.,? 2020",
      "shortCiteRegEx" : "Farrell et al\\.",
      "year" : 2020
    }, {
      "title" : "Problems with evaluation of word embeddings using word similarity tasks",
      "author" : [ "Manaal Faruqui", "Yulia Tsvetkov", "Pushpendre Rastogi", "Chris Dyer." ],
      "venue" : "RepEval@ACL.",
      "citeRegEx" : "Faruqui et al\\.,? 2016",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2016
    }, {
      "title" : "A synopsis of linguistic theory, 19301955",
      "author" : [ "J.R. Firth" ],
      "venue" : null,
      "citeRegEx" : "Firth.,? \\Q1957\\E",
      "shortCiteRegEx" : "Firth.",
      "year" : 1957
    }, {
      "title" : "Simverb-3500: A largescale evaluation set of verb similarity",
      "author" : [ "Daniela Gerz", "Ivan Vulić", "Felix Hill", "Roi Reichart", "Anna Korhonen." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2173–2182.",
      "citeRegEx" : "Gerz et al\\.,? 2016",
      "shortCiteRegEx" : "Gerz et al\\.",
      "year" : 2016
    }, {
      "title" : "Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them",
      "author" : [ "Hila Gonen", "Yoav Goldberg." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Gonen and Goldberg.,? 2019",
      "shortCiteRegEx" : "Gonen and Goldberg.",
      "year" : 2019
    }, {
      "title" : "Studying political bias via word embeddings",
      "author" : [ "Joshua Gordon", "Marzieh Babaeianjelodar", "Jeanna Matthews." ],
      "venue" : "WWW ’20 - Companion Proceedings of the Web Conference 2020, page 760764.",
      "citeRegEx" : "Gordon et al\\.,? 2020",
      "shortCiteRegEx" : "Gordon et al\\.",
      "year" : 2020
    }, {
      "title" : "Better word embeddings by disentangling contextual n-gram information",
      "author" : [ "Prakhar Gupta", "Matteo Pagliardini", "Martin Jaggi." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Gupta et al\\.,? 2019",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2019
    }, {
      "title" : "Simlex-999: Evaluating semantic models with (genuine) similarity estimation",
      "author" : [ "Felix Hill", "Roi Reichart", "Anna Korhonen." ],
      "venue" : "Computational Linguistics, 41:665–695.",
      "citeRegEx" : "Hill et al\\.,? 2014",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2014
    }, {
      "title" : "Mining and summarizing customer reviews",
      "author" : [ "Minqing Hu", "Bing Liu." ],
      "venue" : "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168–177. ACM.",
      "citeRegEx" : "Hu and Liu.,? 2004",
      "shortCiteRegEx" : "Hu and Liu.",
      "year" : 2004
    }, {
      "title" : "Comparison of semantic similarity for different languages using the google n-gram corpus and second-order cooccurrence measures",
      "author" : [ "Colette Joubarne", "Diana Inkpen." ],
      "venue" : "Canadian Conference on AI.",
      "citeRegEx" : "Joubarne and Inkpen.,? 2011",
      "shortCiteRegEx" : "Joubarne and Inkpen.",
      "year" : 2011
    }, {
      "title" : "Gender-preserving debiasing for pre-trained word embeddings",
      "author" : [ "Masahiro Kaneko", "Danushka Bollegala." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1641–1650, Florence, Italy. ACL.",
      "citeRegEx" : "Kaneko and Bollegala.,? 2019",
      "shortCiteRegEx" : "Kaneko and Bollegala.",
      "year" : 2019
    }, {
      "title" : "Linking entities through an ontology using word embeddings and syntactic re-ranking",
      "author" : [ "Ilknur Karadeniz", "Arzucan Özgür." ],
      "venue" : "BMC Bioinformatics, 20.",
      "citeRegEx" : "Karadeniz and Özgür.,? 2019",
      "shortCiteRegEx" : "Karadeniz and Özgür.",
      "year" : 2019
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Kingma and Ba.,? 2015a",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "ICLR - International Conference on Learning Representations.",
      "citeRegEx" : "Kingma and Ba.,? 2015b",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Guillaume Lample", "Alexis Conneau." ],
      "venue" : "NeurIPS 2019 - Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Lample and Conneau.,? 2019",
      "shortCiteRegEx" : "Lample and Conneau.",
      "year" : 2019
    }, {
      "title" : "Word translation without parallel data",
      "author" : [ "Guillaume Lample", "Alexis Conneau", "Marc’Aurelio Ranzato", "Ludovic Denoyer", "Hervé Jégou" ],
      "venue" : "In International Conference on Learning Representations",
      "citeRegEx" : "Lample et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving distributional similarity with lessons learned from word embeddings",
      "author" : [ "Omer Levy", "Y. Goldberg", "I. Dagan." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 3:211–225.",
      "citeRegEx" : "Levy et al\\.,? 2015",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2015
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "ArXiv, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Content analysis of textbooks via natural language processing: Findings on gender, race, and ethnicity in texas u.s. history textbooks",
      "author" : [ "Li Lucy", "Dorottya Demszky", "Patricia Bromley", "Dan Jurafsky" ],
      "venue" : "AERA Open,",
      "citeRegEx" : "Lucy et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lucy et al\\.",
      "year" : 2020
    }, {
      "title" : "Better word representations with recursive neural networks for morphology",
      "author" : [ "Thang Luong", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "CoNLL.",
      "citeRegEx" : "Luong et al\\.,? 2013",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2013
    }, {
      "title" : "Application of machine learning and word embeddings in the classification of cancer diagnosis using patient anamnesis",
      "author" : [ "Andrés Alejandro Ramos Magna", "Héctor Allende-Cid", "Carla Taramasco", "C. Becerra", "R. Figueroa." ],
      "venue" : "IEEE Access, 8:106198–106213.",
      "citeRegEx" : "Magna et al\\.,? 2020",
      "shortCiteRegEx" : "Magna et al\\.",
      "year" : 2020
    }, {
      "title" : "The Stanford CoreNLP Natural Language Processing Toolkit",
      "author" : [ "Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Rose Finkel", "Steven Bethard", "David McClosky." ],
      "venue" : "ACL.",
      "citeRegEx" : "Manning et al\\.,? 2014",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "Black is to criminal as caucasian is to police: Towards detecting, evaluating and removing multiclass bias in word embeddings",
      "author" : [ "Thomas Manzini", "Lim Yao Chong", "Alan W. Black", "Yulia Tsvetkov." ],
      "venue" : "NAACL 2019.",
      "citeRegEx" : "Manzini et al\\.,? 2019",
      "shortCiteRegEx" : "Manzini et al\\.",
      "year" : 2019
    }, {
      "title" : "What do you mean, BERT? Assessing BERT as a Distributional Semantics Model",
      "author" : [ "Timothee Mickus", "Denis Paperno", "Mathieu Constant", "Kees van Deemter." ],
      "venue" : "Proceedings of the Society for Computation in Linguistics, 3(1):350–361.",
      "citeRegEx" : "Mickus et al\\.,? 2020",
      "shortCiteRegEx" : "Mickus et al\\.",
      "year" : 2020
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean." ],
      "venue" : "ICLR - International Conference on Learning Representations.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Unsupervised learning of sentence embeddings using compositional n-gram features",
      "author" : [ "Matteo Pagliardini", "Prakhar Gupta", "Martin Jaggi." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Pagliardini et al\\.,? 2018",
      "shortCiteRegEx" : "Pagliardini et al\\.",
      "year" : 2018
    }, {
      "title" : "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
      "author" : [ "Bo Pang", "Lillian Lee." ],
      "venue" : "Proceedings of the 42nd annual meeting on Association for Computational Linguistics, page 271. Association for Com-",
      "citeRegEx" : "Pang and Lee.,? 2004",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2004
    }, {
      "title" : "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
      "author" : [ "Bo Pang", "Lillian Lee." ],
      "venue" : "Proceedings of the 43rd annual meeting on association for computational linguistics, pages 115–124. Association for",
      "citeRegEx" : "Pang and Lee.,? 2005",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2005
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "EMNLP-IJCNLP - Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Word embeddings for the analysis of ideological placement in parliamentary corpora",
      "author" : [ "L. Rheault", "C. Cochrane." ],
      "venue" : "Political Analysis, 28:112–133.",
      "citeRegEx" : "Rheault and Cochrane.,? 2020",
      "shortCiteRegEx" : "Rheault and Cochrane.",
      "year" : 2020
    }, {
      "title" : "A primer in bertology: What we know about how bert works",
      "author" : [ "Anna Rogers", "Olga Kovaleva", "Anna Rumshisky." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:842–866.",
      "citeRegEx" : "Rogers et al\\.,? 2020",
      "shortCiteRegEx" : "Rogers et al\\.",
      "year" : 2020
    }, {
      "title" : "Concept narrowing: The role of context-independent information",
      "author" : [ "Paula Rubio-Fernández." ],
      "venue" : "J. Semant., 25:381–409.",
      "citeRegEx" : "Rubio.Fernández.,? 2008",
      "shortCiteRegEx" : "Rubio.Fernández.",
      "year" : 2008
    }, {
      "title" : "Learning joint multilingual sentence representations with neural machine translation",
      "author" : [ "Holger Schwenk", "Matthijs Douze." ],
      "venue" : "Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 157–167, Vancouver, Canada. ACL.",
      "citeRegEx" : "Schwenk and Douze.,? 2017",
      "shortCiteRegEx" : "Schwenk and Douze.",
      "year" : 2017
    }, {
      "title" : "Room to Glo: A systematic comparison of semantic change detection approaches with word embeddings",
      "author" : [ "Philippa Shoemark", "Farhana Ferdousi Liza", "Dong Nguyen", "Scott Hale", "Barbara McGillivray." ],
      "venue" : "EMNLP-IJCNLP - Proceedings of the 2019 Con-",
      "citeRegEx" : "Shoemark et al\\.,? 2019",
      "shortCiteRegEx" : "Shoemark et al\\.",
      "year" : 2019
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "R. Socher", "Alex Perelygin", "J. Wu", "Jason Chuang", "Christopher D. Manning", "A. Ng", "Christopher Potts." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "The proof and measurement of association between two things",
      "author" : [ "Charles Spearman." ],
      "venue" : "The American journal of psychology, 15(1):72–101.",
      "citeRegEx" : "Spearman.,? 1904",
      "shortCiteRegEx" : "Spearman.",
      "year" : 1904
    }, {
      "title" : "Energy and Policy Considerations for Deep Learning in NLP",
      "author" : [ "Emma Strubell", "Ananya Ganesh", "Andrew McCallum." ],
      "venue" : "ACL.",
      "citeRegEx" : "Strubell et al\\.,? 2019",
      "shortCiteRegEx" : "Strubell et al\\.",
      "year" : 2019
    }, {
      "title" : "Overview of the TREC 2001 question answering track",
      "author" : [ "Ellen M Voorhees." ],
      "venue" : "NIST special publication, pages 42–51.",
      "citeRegEx" : "Voorhees.,? 2002",
      "shortCiteRegEx" : "Voorhees.",
      "year" : 2002
    }, {
      "title" : "Are all good word vector spaces isomorphic",
      "author" : [ "Ivan Vulic", "Sebastian Ruder", "Anders Søgaard" ],
      "venue" : null,
      "citeRegEx" : "Vulic et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Vulic et al\\.",
      "year" : 2020
    }, {
      "title" : "Annotating expressions of opinions and emotions in language",
      "author" : [ "Janyce Wiebe", "Theresa Wilson", "Claire Cardie." ],
      "venue" : "Language resources and evaluation, 39(2):165–210.",
      "citeRegEx" : "Wiebe et al\\.,? 2005",
      "shortCiteRegEx" : "Wiebe et al\\.",
      "year" : 2005
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Williams et al\\.,? 2018",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "EMNLP - Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45,",
      "citeRegEx" : "Lhoest and Rush.,? 2020",
      "shortCiteRegEx" : "Lhoest and Rush.",
      "year" : 2020
    }, {
      "title" : "Biowordvec, improving biomedical word embeddings with subword information and mesh",
      "author" : [ "Yijia Zhang", "Qingyu Chen", "Z. Yang", "H. Lin", "Zhiyong Lu." ],
      "venue" : "Scientific Data, 6.",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Ngram2vec: Learning improved word representations from ngram co-occurrence statistics",
      "author" : [ "Zhe Zhao", "Tao Liu", "Shen Li", "Bofang Li", "Xiaoyong Du." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Zhao et al\\.,? 2017",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 37,
      "context" : "Static word representation models such as GLOVE (Pennington et al., 2014), CBOW, SKIPGRAM (Mikolov et al.",
      "startOffset" : 48,
      "endOffset" : 73
    }, {
      "referenceID" : 33,
      "context" : ", 2014), CBOW, SKIPGRAM (Mikolov et al., 2013) and SENT2VEC (Pagliardini et al.",
      "startOffset" : 24,
      "endOffset" : 46
    }, {
      "referenceID" : 34,
      "context" : ", 2013) and SENT2VEC (Pagliardini et al., 2018) obtain stand-alone representations which do not depend on their surrounding words or sentences (context).",
      "startOffset" : 21,
      "endOffset" : 47
    }, {
      "referenceID" : 7,
      "context" : "Contextual embedding models (Devlin et al., 2019; Peters et al., 2018; Liu et al., 2019; Radford et al., 2019; Schwenk and Douze, 2017) on the other hand, embed the contextual information as well into the word representations making them more expressive than static word representations in most use-cases.",
      "startOffset" : 28,
      "endOffset" : 135
    }, {
      "referenceID" : 38,
      "context" : "Contextual embedding models (Devlin et al., 2019; Peters et al., 2018; Liu et al., 2019; Radford et al., 2019; Schwenk and Douze, 2017) on the other hand, embed the contextual information as well into the word representations making them more expressive than static word representations in most use-cases.",
      "startOffset" : 28,
      "endOffset" : 135
    }, {
      "referenceID" : 26,
      "context" : "Contextual embedding models (Devlin et al., 2019; Peters et al., 2018; Liu et al., 2019; Radford et al., 2019; Schwenk and Douze, 2017) on the other hand, embed the contextual information as well into the word representations making them more expressive than static word representations in most use-cases.",
      "startOffset" : 28,
      "endOffset" : 135
    }, {
      "referenceID" : 39,
      "context" : "Contextual embedding models (Devlin et al., 2019; Peters et al., 2018; Liu et al., 2019; Radford et al., 2019; Schwenk and Douze, 2017) on the other hand, embed the contextual information as well into the word representations making them more expressive than static word representations in most use-cases.",
      "startOffset" : 28,
      "endOffset" : 135
    }, {
      "referenceID" : 44,
      "context" : "Contextual embedding models (Devlin et al., 2019; Peters et al., 2018; Liu et al., 2019; Radford et al., 2019; Schwenk and Douze, 2017) on the other hand, embed the contextual information as well into the word representations making them more expressive than static word representations in most use-cases.",
      "startOffset" : 28,
      "endOffset" : 135
    }, {
      "referenceID" : 48,
      "context" : "While recent progress on contextual embeddings has been tremendously impactful, static embeddings still remain fundamentally important in many scenarios as well: • Even when ignoring the training phase, the computational cost of using static word embeddings is typically tens of millions times lower than using standard contextual embedding models1, which is particularly important for latency-critical applications and on lowresource devices, and in view of environmental costs of NLP models (Strubell et al., 2019).",
      "startOffset" : 493,
      "endOffset" : 516
    }, {
      "referenceID" : 45,
      "context" : "embeddings (Shoemark et al., 2019), for example for interpretability, or e.",
      "startOffset" : 11,
      "endOffset" : 34
    }, {
      "referenceID" : 19,
      "context" : "in research in bias detection and removal (Kaneko and Bollegala, 2019; Gonen and Goldberg, 2019; Manzini et al., 2019) and analyzing word vec-",
      "startOffset" : 42,
      "endOffset" : 118
    }, {
      "referenceID" : 13,
      "context" : "in research in bias detection and removal (Kaneko and Bollegala, 2019; Gonen and Goldberg, 2019; Manzini et al., 2019) and analyzing word vec-",
      "startOffset" : 42,
      "endOffset" : 118
    }, {
      "referenceID" : 31,
      "context" : "in research in bias detection and removal (Kaneko and Bollegala, 2019; Gonen and Goldberg, 2019; Manzini et al., 2019) and analyzing word vec-",
      "startOffset" : 42,
      "endOffset" : 118
    }, {
      "referenceID" : 50,
      "context" : "tor spaces (Vulic et al., 2020) or other metrics which are non-contextual by choice.",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 2,
      "context" : "• Static word embeddings can complement contextual word embeddings, for separating static from contextual semantics (Barsalou, 1982; Rubio-Fernández, 2008), or for improving joint embedding performance on downstream tasks (Alghanmi et al.",
      "startOffset" : 116,
      "endOffset" : 155
    }, {
      "referenceID" : 43,
      "context" : "• Static word embeddings can complement contextual word embeddings, for separating static from contextual semantics (Barsalou, 1982; Rubio-Fernández, 2008), or for improving joint embedding performance on downstream tasks (Alghanmi et al.",
      "startOffset" : 116,
      "endOffset" : 155
    }, {
      "referenceID" : 1,
      "context" : "• Static word embeddings can complement contextual word embeddings, for separating static from contextual semantics (Barsalou, 1982; Rubio-Fernández, 2008), or for improving joint embedding performance on downstream tasks (Alghanmi et al., 2020).",
      "startOffset" : 222,
      "endOffset" : 245
    }, {
      "referenceID" : 54,
      "context" : "Indeed, we can see continued prevalence of static word embeddings in industry and research areas including but not limited to medicine (Zhang et al., 2019; Karadeniz and Özgür, 2019; Magna et al., 2020) and social sciences (Rheault and Cochrane, 2020; Gordon et al.",
      "startOffset" : 135,
      "endOffset" : 202
    }, {
      "referenceID" : 20,
      "context" : "Indeed, we can see continued prevalence of static word embeddings in industry and research areas including but not limited to medicine (Zhang et al., 2019; Karadeniz and Özgür, 2019; Magna et al., 2020) and social sciences (Rheault and Cochrane, 2020; Gordon et al.",
      "startOffset" : 135,
      "endOffset" : 202
    }, {
      "referenceID" : 29,
      "context" : "Indeed, we can see continued prevalence of static word embeddings in industry and research areas including but not limited to medicine (Zhang et al., 2019; Karadeniz and Özgür, 2019; Magna et al., 2020) and social sciences (Rheault and Cochrane, 2020; Gordon et al.",
      "startOffset" : 135,
      "endOffset" : 202
    }, {
      "referenceID" : 7,
      "context" : "BERT base (Devlin et al., 2019) produces 768 dimensional word embeddings using 109M parameters, requiring 29B FLOPs per inference call (Clark et al.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 6,
      "context" : ", 2019) produces 768 dimensional word embeddings using 109M parameters, requiring 29B FLOPs per inference call (Clark et al., 2020).",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 2,
      "context" : "5242 textual as well as context-independent properties (Barsalou, 1982; Rubio-Fernández, 2008) underlining the need for continued research in studying the expressiveness context-independent embeddings on the level of words.",
      "startOffset" : 55,
      "endOffset" : 94
    }, {
      "referenceID" : 43,
      "context" : "5242 textual as well as context-independent properties (Barsalou, 1982; Rubio-Fernández, 2008) underlining the need for continued research in studying the expressiveness context-independent embeddings on the level of words.",
      "startOffset" : 55,
      "endOffset" : 94
    }, {
      "referenceID" : 15,
      "context" : "Ad-hoc remedies attempt to capture longer contextual information per word using higher order n-grams like bigrams or trigrams, and have been shown to improve the performance of static word embedding models (Gupta et al., 2019; Zhao et al., 2017).",
      "startOffset" : 206,
      "endOffset" : 245
    }, {
      "referenceID" : 55,
      "context" : "Ad-hoc remedies attempt to capture longer contextual information per word using higher order n-grams like bigrams or trigrams, and have been shown to improve the performance of static word embedding models (Gupta et al., 2019; Zhao et al., 2017).",
      "startOffset" : 206,
      "endOffset" : 245
    }, {
      "referenceID" : 40,
      "context" : "Other related work includes distillation of contextual word embeddings to obtain sentence embeddings (Reimers and Gurevych, 2019).",
      "startOffset" : 101,
      "endOffset" : 129
    }, {
      "referenceID" : 15,
      "context" : "SENT2VEC, originally meant to obtain sentence embeddings and later repurposed to obtain word representations (Gupta et al., 2019) was shown to outperform competing methods including GLOVE (Pennington et al.",
      "startOffset" : 109,
      "endOffset" : 129
    }, {
      "referenceID" : 37,
      "context" : ", 2019) was shown to outperform competing methods including GLOVE (Pennington et al., 2014), CBOW, SKIPGRAM (Mikolov et al.",
      "startOffset" : 66,
      "endOffset" : 91
    }, {
      "referenceID" : 33,
      "context" : ", 2014), CBOW, SKIPGRAM (Mikolov et al., 2013) and FASTTEXT (Bojanowski et al.",
      "startOffset" : 24,
      "endOffset" : 46
    }, {
      "referenceID" : 3,
      "context" : ", 2013) and FASTTEXT (Bojanowski et al., 2016) on word similarity evaluations.",
      "startOffset" : 21,
      "endOffset" : 46
    }, {
      "referenceID" : 30,
      "context" : "We remove all paragraphs with less than 3 sentences or 140 characters, lowercase the characters and tokenize the corpus using the Stanford NLP library (Manning et al., 2014) resulting in a corpus of approximately 54 Million sentences and 1.",
      "startOffset" : 151,
      "endOffset" : 173
    }, {
      "referenceID" : 7,
      "context" : "We use 12-layer as well as 24layer pre-trained models using BERT (Devlin et al., 2019), ROBERTA (Liu et al.",
      "startOffset" : 65,
      "endOffset" : 86
    }, {
      "referenceID" : 26,
      "context" : ", 2019), ROBERTA (Liu et al., 2019) and GPT2 (Radford et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 39,
      "context" : ", 2019) and GPT2 (Radford et al., 2019) architectures as the teacher model to obtain X2STATIC word embeddings.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 21,
      "context" : "The distillation/training process employs the lazy version of the Adam optimizer (Kingma and Ba, 2015a), suitable for sparse tensors.",
      "startOffset" : 81,
      "endOffset" : 103
    }, {
      "referenceID" : 3,
      "context" : "We use a subsampling parameter similar to FASTTEXT (Bojanowski et al., 2016) in order to subsample frequent target words during training.",
      "startOffset" : 51,
      "endOffset" : 76
    }, {
      "referenceID" : 37,
      "context" : "We also evaluate some pre-trained 300 dimensional GLOVE (Pennington et al., 2014) and FASTTEXT (Bojanowski et al.",
      "startOffset" : 56,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : ", 2014) and FASTTEXT (Bojanowski et al., 2016) models in Table 3.",
      "startOffset" : 21,
      "endOffset" : 46
    }, {
      "referenceID" : 10,
      "context" : "5245 evaluation tasks (Faruqui et al., 2016) as they are shown to exhibit significant difference in performance when subjected to hyperparameter tuning (Levy et al.",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 25,
      "context" : ", 2016) as they are shown to exhibit significant difference in performance when subjected to hyperparameter tuning (Levy et al., 2015).",
      "startOffset" : 115,
      "endOffset" : 134
    }, {
      "referenceID" : 4,
      "context" : "To assess the quality of the lexical information contained in the obtained word representations, we use the 4 word-similarity datasets used by (Bommasani et al., 2020), namely WordSim353 (353 word-pairs) (Agirre et al.",
      "startOffset" : 143,
      "endOffset" : 167
    }, {
      "referenceID" : 0,
      "context" : ", 2020), namely WordSim353 (353 word-pairs) (Agirre et al., 2009) dataset; SimLex999 (999 word-pairs) (Hill et al.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 16,
      "context" : ", 2009) dataset; SimLex999 (999 word-pairs) (Hill et al., 2014) dataset; RG-65 (65 pairs) (Joubarne and Inkpen, 2011); and SimVerb-3500 (3500 pairs) (Gerz et al.",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 18,
      "context" : ", 2014) dataset; RG-65 (65 pairs) (Joubarne and Inkpen, 2011); and SimVerb-3500 (3500 pairs) (Gerz et al.",
      "startOffset" : 34,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : ", 2014) dataset; RG-65 (65 pairs) (Joubarne and Inkpen, 2011); and SimVerb-3500 (3500 pairs) (Gerz et al., 2016) dataset as well as the Rare Words RW-2034 (2034 pairs) (Luong et al.",
      "startOffset" : 93,
      "endOffset" : 112
    }, {
      "referenceID" : 28,
      "context" : ", 2016) dataset as well as the Rare Words RW-2034 (2034 pairs) (Luong et al., 2013) dataset.",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 47,
      "context" : "These similarity scores are compared to the human ratings using Spearman’s ρ (Spearman, 1904) correlation scores.",
      "startOffset" : 77,
      "endOffset" : 93
    }, {
      "referenceID" : 24,
      "context" : "All other embeddings are evaluated using the MUSE repository evaluation tool8 (Lample et al., 2018).",
      "startOffset" : 78,
      "endOffset" : 99
    }, {
      "referenceID" : 16,
      "context" : "We choose SimLex-999 as it captures true similarity instead of relatedness or association (Hill et al., 2014) and RW-2034 to gauge the robustness of the embedding model on rare words.",
      "startOffset" : 90,
      "endOffset" : 109
    }, {
      "referenceID" : 36,
      "context" : "Six different downstream supervised evaluation tasks namely classification of movie review sentiment(MR) (Pang and Lee, 2005), product reviews(CR) (Hu and Liu, 2004), subjectivity classification(SUBJ) (Pang and Lee, 2004), opinion polarity (MPQA) (Wiebe et al.",
      "startOffset" : 105,
      "endOffset" : 125
    }, {
      "referenceID" : 17,
      "context" : "Six different downstream supervised evaluation tasks namely classification of movie review sentiment(MR) (Pang and Lee, 2005), product reviews(CR) (Hu and Liu, 2004), subjectivity classification(SUBJ) (Pang and Lee, 2004), opinion polarity (MPQA) (Wiebe et al.",
      "startOffset" : 147,
      "endOffset" : 165
    }, {
      "referenceID" : 35,
      "context" : "Six different downstream supervised evaluation tasks namely classification of movie review sentiment(MR) (Pang and Lee, 2005), product reviews(CR) (Hu and Liu, 2004), subjectivity classification(SUBJ) (Pang and Lee, 2004), opinion polarity (MPQA) (Wiebe et al.",
      "startOffset" : 201,
      "endOffset" : 221
    }, {
      "referenceID" : 51,
      "context" : "Six different downstream supervised evaluation tasks namely classification of movie review sentiment(MR) (Pang and Lee, 2005), product reviews(CR) (Hu and Liu, 2004), subjectivity classification(SUBJ) (Pang and Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005), question type classification (TREC) (Voorhees, 2002) and finegrained sentiment analysis (SST-5) (Socher et al.",
      "startOffset" : 247,
      "endOffset" : 267
    }, {
      "referenceID" : 49,
      "context" : ", 2005), question type classification (TREC) (Voorhees, 2002) and finegrained sentiment analysis (SST-5) (Socher et al.",
      "startOffset" : 45,
      "endOffset" : 61
    }, {
      "referenceID" : 46,
      "context" : ", 2005), question type classification (TREC) (Voorhees, 2002) and finegrained sentiment analysis (SST-5) (Socher et al., 2013) are employed to gauge the performance of the obtained word embeddings.",
      "startOffset" : 105,
      "endOffset" : 126
    }, {
      "referenceID" : 22,
      "context" : "We use Adam (Kingma and Ba, 2015b) to train our classifier.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 40,
      "context" : "To put the performance of these static models into a broader perspective, we also fine-tune linear classifiers on the top of their parent models as well as sentence-transformers (Reimers and Gurevych, 2019) obtained from ROBERTA-12 and BERT-12.",
      "startOffset" : 178,
      "endOffset" : 206
    }, {
      "referenceID" : 5,
      "context" : "For the sentence-transformer models, we use the sentence-transformer models obtained by fine-tuning their parent models on the Natural Language Inference(NLI) task using the combination of Stanford NLI (Bowman et al., 2015) and the Multi-Genre NLI (Williams et al.",
      "startOffset" : 202,
      "endOffset" : 223
    }, {
      "referenceID" : 52,
      "context" : ", 2015) and the Multi-Genre NLI (Williams et al., 2018) datasets.",
      "startOffset" : 32,
      "endOffset" : 55
    }, {
      "referenceID" : 37,
      "context" : "We also use existing GLOVE embeddings trained on tweets(27 billion tokens - 20 times larger than our corpus) (Pennington et al., 2014) to make the comparison even more extensive.",
      "startOffset" : 109,
      "endOffset" : 134
    }, {
      "referenceID" : 44,
      "context" : "Further work can explore extending this distillation framework into cross-lingual domains (Schwenk and Douze, 2017; Lample and Conneau, 2019) as well as using better pooling methods instead of simple averaging for obtaining the context representation, or joint fine-tuning to obtain even stronger static word embeddings.",
      "startOffset" : 90,
      "endOffset" : 141
    }, {
      "referenceID" : 23,
      "context" : "Further work can explore extending this distillation framework into cross-lingual domains (Schwenk and Douze, 2017; Lample and Conneau, 2019) as well as using better pooling methods instead of simple averaging for obtaining the context representation, or joint fine-tuning to obtain even stronger static word embeddings.",
      "startOffset" : 90,
      "endOffset" : 141
    } ],
    "year" : 2021,
    "abstractText" : "The advent of contextual word embeddings— representations of words which incorporate semantic and syntactic information from their context—has led to tremendous improvements on a wide variety of NLP tasks. However, recent contextual models have prohibitively high computational cost in many use-cases and are often hard to interpret. In this work, we demonstrate that our proposed distillation method, which is a simple extension of CBOW-based training, allows to significantly improve computational efficiency of NLP applications, while outperforming the quality of existing static embeddings trained from scratch as well as those distilled from previously proposed methods. As a side-effect, our approach also allows a fair comparison of both contextual and static embeddings via standard lexical evaluation tasks.",
    "creator" : "LaTeX with hyperref package"
  }
}