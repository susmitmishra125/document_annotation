{
  "name" : "2021.acl-long.374.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Exploiting Document Structures and Cluster Consistencies for Event Coreference Resolution",
    "authors" : [ "Hieu Minh Tran", "Duy Phung", "Thien Huu Nguyen" ],
    "emails" : [ "v.hieutm4@vinai.io,", "v.duypv1@vinai.io,", "thien@cs.uoregon.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4840–4850\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4840"
    }, {
      "heading" : "1 Introduction",
      "text" : "Event coreference resolution (ECR) is the task of clustering event mentions (i.e., trigger words that evoke an event) in a document such that each cluster represents a unique real world event. For example, the three event mentions in Figure 1, i.e., “refuse to sign, “raised objections”, and “doesn’t sign”, should be grouped into the same cluster to indicate their coreference to the same event.\nA common component in prior ECR models involves a binary classifier that receives a pair of\nevent mentions and predict their coreference (Chen et al., 2009; Lu et al., 2016; Lu and Ng, 2017). To this end, an important step in ECR models is to transform event mention pairs into representation vectors to encode discriminative features for coreference prediction. Early work on ECR has achieved feature representation via feature engineering where multiple features are hand-designed for input event mention pairs (Lu and Ng, 2017). A major problem with feature engineering is the sparsity of the features that limits the generalization to unseen data. Representation learning in deep learning models has recently been introduced to address this issue, leading to more robust methods with better performance for ECR (Nguyen et al., 2016; Choubey and Huang, 2018; Huang et al., 2019; Barhom et al., 2019). As such, there are at least two limitations in existing deep learning models for ECR that will be addressed in this work to improve the performance.\nFirst, as event mentions pairs for coreference prediction might belong to long-distance sentences in documents, capturing document-level context between the event mentions (i.e., beyond the two sentences that host the event mentions) might present useful information for ECR. As their first limitation, prior deep learning models for ECR has only attempted to encode document-level context via hand-designed features (Kenyon-Dean et al., 2018; Barhom et al., 2019) that still suffer from the feature sparsity issue. In addition, such prior work is unable to exploit ECR-related objects in documents (e.g., entity mentions, context words) and their connections/interactions (possibly beyond sentence boundary) to aid representation learning. An example for the importance of context words, entity mentions, and their interactions for ECR can be seen in Figure 1. Here, to decisively determine the coreference of “raised objections” and “doesn’t sign”, ECR systems should recognize “Trump” and “the\n$900bn relief bill” as the arguments of “raised objections”, and “Trump” and “the damn bill” as the arguments of “doesn’t sign”. The systems should also be able to realize the coreference relation between the two entity mentions “Trump”, and between “the $900bn relief bill” and “the damn bill” to conclude the same identity for the event mentions (i.e., as they involve the same arguments). As such, it is helpful to identify relevant entity mentions, context words and leverage their relations/interactions to improve representation vectors for event mentions in ECR. Motivated by this issue, we propose to form graphs for documents (called document structures) to explicitly capture relevant objects and interactions for ECR that will be consumed to learn representation vectors for event mentions. In particular, context words, entity mentions, and event mentions will serve as the nodes in our document structures due to their intuitive relevance to ECR. Different types of knowledge sources will then be exploited to connect the nodes for the document structures, featuring discourse information (e.g., to connect coreferring entity mentions), syntactic information (e.g., to directly link event mentions and their arguments), and semantic similarity (e.g., to connect words/event mentions with similar meanings). Such rich document structures allows us to model the interactions of relevant objects for ECR beyond sentence level for document-level context. Using graph convolutional neural networks (GCN) (Kipf and Welling, 2017; Nguyen and Grishman, 2018) for representation learning, we expect enriched representation vectors from the document structures can further improve the performance of ECR systems. To our knowledge, this is the first time that rich document structures are employed for ECR.\nSecond, prior deep learning models for ECR fails to leverage consistencies between golden clus-\nters (provided by human) and predicted clusters (generated by models) to promote representation learning. In particular, it is intuitive that ECR models can achieve better performance if their predicted event clusters are more similar to the golden event clusters in the data. To this end, we propose to obtain different inconsistency measures between golden and predicted clusters that will be incorporated into the overall loss function for minimization. As such, we expect that the consistency/similarity regularization between two types of clusters can provide useful training signals to improve representation vectors for event mentions in ECR. To our knowledge, this is also the first work to exploit cluster consistency-based regularization for representation learning in ECR. Finally, we conduct extensive experiments for ECR on the KBP benchmark datasets. The experiments demonstrate the benefits of the proposed methods and lead to state-of-the-art performance for ECR."
    }, {
      "heading" : "2 Related Work",
      "text" : "Event coreference resolution is broadly related to works on entity coreference resolution that aim to resolve nouns phrases/mentions for entities (Raghunathan et al., 2010; Ng, 2010; Durrett and Klein, 2013; Lee et al., 2017a; Joshi et al., 2019b,a). However, resolving event mentions has been considered as a more challenging task than entity coreference resolution due to the more complex structures of event mentions (Yang et al., 2015).\nOur work focuses on the within-document setting for ECR where input event mentions are expected to appear in the same input documents; however, we also note prior works on crossdocument ECR (Lee et al., 2012a; Adrian Bejan and Harabagiu, 2014; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Barhom et al., 2019; Cattan et al., 2020). As such, for within-document\nECR, previous methods have applied feature-based models for pairwise classifiers (Ahn, 2006; Chen et al., 2009; Cybulska and Vossen, 2015; Peng et al., 2016), spectral graph clustering (Chen and Ji, 2009), information propagation (Liu et al., 2014), markov logic networks (Lu et al., 2016), joint modeling of ECR with event detection (Araki and Mitamura, 2015; Lu et al., 2016; Chen and Ng, 2016; Lu and Ng, 2017), and recent deep learning models (Nguyen et al., 2016; Choubey and Huang, 2018; Huang et al., 2019; Lu et al., 2020; Choubey et al., 2020). Compared to previous deep learning works for ECR, our model presents a novel representation learning framework based on document structures to explicitly encode important interactions between relevant objects, and representation regularization to exploit the cluster consistency between golden and predicted clusters for event mentions."
    }, {
      "heading" : "3 Model",
      "text" : "Formally, in ECR, given an input document D = w1, w2, . . . , wN (of N words/tokens) with a set of event mentions E = {e1, e2, . . . , e|E|}, the goal is to group the event mentions in E into clusters to capture the coreference relation between mentions. Our ECR model consists of four major components: (i) Document Encoder to words into representation vectors, (ii) Document Structure to create graphs for documents and learn rich representation vectors for event mentions, (iii) End-to-end Resolution to simultaneously resolve the coreference for the entity mentions in D, and (iv) Cluster Consistency Regularization to regularize representation vectors based on consistency constraints between golden and predict event mention clusters. Figure 2 presents an overview of our model for ECR."
    }, {
      "heading" : "3.1 Document Encoder",
      "text" : "In the first step, we transform each word wi ∈ D into a representation vector xi by feeding D into the pre-trained language model BERT (Devlin et al., 2019). In particular, as BERT might split wi into several word-pieces, we average the hidden vectors of the word-pieces of wi in the last layer of BERT to obtain the representation vector xi for wi. To handle long documents with BERT, we divideD into segments of 512 consecutive word-pieces that will be encoded separately. The resulting sequence X = x1, x2, . . . , xn for D is then sent to the next steps for further computation."
    }, {
      "heading" : "3.2 Document Structure",
      "text" : "This component aims to learn representation vectors for the event mentions inE using an interaction graph G = {N , E} for D that facilitates the enrichment of representation vectors for event mentions with relevant objects and interactions at document level. As such, the nodes and edges in G for our ECR problem are constructed as follows:\nNodes: The node setN for our interaction graph G should capture relevant objects for the coreference between event mentions in D. Toward this goal, we consider all the context words (i.e., wi), event mentions, and entity mentions in D as relevant objects for our ECR problem. For convenience, let M = {m1,m2, . . . ,m|M |} be the set of entity mentions in D. The node set N for G is thus created by the union of D, E, and M : N = D ∪ E ∪ M = {n1, n2, . . . , n|N |}. To achieve a fair comparison, we use the predicted event mentions that are provided by (Choubey and Huang, 2018) in the datasets for E. The Stanford CoreNLP toolkit is employed to obtain the entity mentions M .\nEdges: The edges between the nodes in N for G will be represented by an adjacency matrix A = {aij}i,j=|N | (aij ∈ R) in this work. As A will be consumed by Graph Convolutional Networks (GCN) to learn representation vectors for ECR, the value/score aij between two nodes ni and nj inN is expected to estimate the importance (or the level of interaction) of nj for the representation computation of ni. This structure allows ni and nj of N to directly interact and influence the representation computation of each other even if they are sequentially far away from each other in D. As presented in the introduction, we explore three types of information to design the edges E (or compute the interaction scores aij) for G in our model, including discourse-based, syntax-based and semantic-based information. Discourse-based Edges: Due to multiple sentences and event/entity mentions involved in the input document D, we need to understand where such objects span and how they relate to each other to effectively encode document context for ECR. To this end, we propose to exploit three types of discourse information to obtain the interaction graph G, i.e., sentence boundary, coreference structure, and mention span for event/entity mentions in D.\nSentence Boundary: Our motivation for this information is that event/entity mentions appearing\nin the same sentences tend to be more contextually related to each other than those in different sentences. As such, event/entity mentions in the same sentences might involve more helpful information for the representation computation of each other in our problem. To capture this intuition, we compute the sentence boundary-based interaction score asentij for the nodes ni and nj inN where asentij = 1 if ni and nj are the event/entity mentions of the same sentences in D (i.e., ni, nj ∈ E ∪M ); and 0 otherwise. We will use asentij as an input to compute the overall interaction score aij for G later.\nEntity Coreference Structure: Instead of considering within-sentence information as in asentij , coreference structure focuses on the connection of entity mentions across sentences to enrich their representations with the contextual information of the coreferring ones. As such, to enable the interaction of representations for coreferring enity mentions, we compute the conference-based score acorefij for each pair of nodes ni and nj to contribute to the overall score aij for representation learning. Here, acorefij is set to 1 if ni and nj are coreferring entity mentions in D, and 0 otherwise. Note that we also use the Stanford CoreNLP toolkit to determine the coreference of entity mentions in this work.\nMention Span: The sentence boundary and coreference structure scores model interactions of event and entity mentions in D based on discourse structure. To connect event and entity mentions to context words wi for representation learning, we employ the mention span-based interaction score aspanij as another input for aij . Here, a span ij is\nonly set to 1 (i.e., 0 otherwise) if ni is a word (ni ∈ D) in the span of the entity/event mention nj (nj ∈ E∪M ) or vice verse. aspanij is important as it helps ground representation vectors of event/entity mentions to the contextual information in D. Syntax-based Edges: We expect the dependency trees of the sentences in D to provide beneficial information to connect the nodes in N for effective representation learning in ECR. For example, dependency trees have been used to retrieve important context words between an event mentions and their arguments in prior work (Li et al., 2013; Veyseh et al., 2020a,b). To this end, we propose to employ the dependency relations/connections between the words in D to obtain a syntax-based interaction score adepij for each pair of nodes ni and nj in N , serving as an additional input for aij . In particular, by inheriting the graph structures of the dependency trees of the sentences in D, we set adepij to 1 if ni and nj are two words in the same sentence (i.e., ni, nj ∈ D) and there is an edge between them in the corresponding dependency tree1, and 0 otherwise. Semantic-based Edges: This information leverages the semantic similarity of the nodes in N to enrich the overall interaction scores aij for G. Our motivation is that a node ni will contribute more to the representation computation of another node nj for ECR if ni is more semantically related to nj . In particular, as the representation vectors for the nodes in N have captured the contextual semantics of the words in D, we propose to explore\n1We use Stanford CoreNLP to parse sentences.\na novel source of semantic information that relies on external knowledge for the words to compute interaction scores between the nodes N in our document structures for ECR. We expect the external knowledge for the words to provide complementary information to the contextual information in D, thus further enriching the overall interaction scores aij for the nodes in N . To this end, we propose to utilize WordNet (Miller, 1995), a rich network of word meanings, to obtain external knowledge for the words in D. The word meanings (i.e., synsets) in WordNet are connected to each other via different semantic relations (e.g., synonyms, hyponyms). In particular, our first step to generate knowledge-based similarity scores involves mapping each word node ni ∈ D ∩ N to a synset node Mi in WordNet using a Word Sense Disambiguation (WSD) tool. In particular, we employ WordNet 3.0 and the state-of-the-art BERT-based WSD model in (Blevins and Zettlemoyer, 2020) to perform the word-synset mapping in this work. Afterward, we compute a knowledge-based similarity score astructij for each pair of word nodes ni and nj in D ∩ N using the structure-based similarity of their linked synsets Mi and Mj in WordNet (i.e., astructij = 0 if either ni or nj is not a word node in D ∩N ). Accordingly, the Lin similarity measure (Lin et al., 1998) for synset nodes in WordNet is utilized for this purpose: astructij = 2∗IC(LCS(Mi,Mj)) IC(Mi)+IC(Mj)\n. Here, IC and LCS represent the information content of synset nodes and the least common subsumer of two synsets in the WordNet hierarchy (the most specific ancestor node) respectively2. Structure Combination: Up to now, five scores have been generated to capture the level of interactions in representation learning for each pair of nodes ni and nj in N according to different information sources (i.e., asentij , a coref ij , a span ij , a dep ij and astructij ). For convenience, we group the five scores for each node pair ni and nj into a vector dij = [a sent ij , a coref ij , a span ij , a dep ij , a struct ij ] of size 5. To combine the scores in dij into an overall rich interaction score aij for ni and nj in G, we use the following normalization:\naij = exp(dijq T )/ ∑ u=1..|N| exp(diuq T ) (1)\n2We use the nltk tool to obtain the Lin similarity: https://www.nltk.org/howto/wordnet. html. We tried other WordNet-based similarities available in nltk (e.g., Wu-Palmer similarity), but the Lin similarity produced the best results in our experiments.\nwhere q is a learnable vector of size 5. Representation Learning: Given the combined interaction graph G with the adjacency matrix A = {aij}i,j=|N |, we use GCNs to induce representation vectors for the nodes in N for ECR. In particular, our GCN model takes the initial representation vectors vi of the nodes ni ∈ N as the input. Here, the initial representation vector vi for a word node ni ∈ D is directly obtained from the BERT-based representation vector xc ∈ X (i.e., vi = xc) of the corresponding word wc for ni. In contrast, for event and entity mentions, their initial representation vectors are obtained by max-pooling the contextualized embedding vectors in X that correspond to the words in the event/entity mentions’ spans. For convenience, we organize vi into rows of the input matrix H0 = [v1, . . . , v|N |]. The GCN model then involves G layers that generate the matrix Hl at the l-th layer for the nodes in N (1 ≤ l ≤ G) via: Hl = ReLU(AHl−1Wl) (Wl is the weight matrix for the l-th layer). The output of the GCN model after G layers is HG whose rows are denoted by HG = [h1, . . . , h|N |], serving as more abstract representation vectors for the nodes ni in the coreference prediction for event mentions. Also, for convenience, let {re1 , . . . , re|E|} ⊂ HG be the set of GCN-induced representation vectors for the event mention nodes in e1, . . . , e|E| in E."
    }, {
      "heading" : "3.3 End-to-end Coreference Resolution",
      "text" : "To facilitate the incorporation of the consistency regularization between golden and predicted clusters into the training process, we perform and endto-end procedure that seeks to simultaneously resolve the coreference for the event mentions in E in a single process. Motivated by the entity coreference resolution in (Lee et al., 2017b), we implement the end-to-end resolution via a set of antecedent assignments for the event mentions in E. In particular, we assume that the event mentions inE are enumerated in their appearance order inD. As such, our model aims to link each event mention ei ∈ E to one of its prior event mention in the set Yi = { , e1, . . . , ei−1} ( is a dumpy antecedent). Here, a link of ei to a non-dumpy antecedent ej in Yi represents a coreference relation between ei and ej . In contrast, a dumpy assignment for ei indicates that ei is not coreferent with any prior event mention. By forming a coreference graph with ei as the nodes, the non-dumpy antecedent assignments for every event mention in E can be utitlized to\nconnect coreference event mentions. Connected components from the coreference graph can then be returned to serve as predicted event mention clusters in D.\nIn order to predict the coreferent antecedent yi ∈ Y for an event mention ei, we compute the distribution over the possible antecedents in Yi for ei via: P (yi| ei,Yi) = e\ns(ei,yi)∑ y′∈Y(i) e s(ei,y′) where\ns(ei, ej) is a score function to determine the coreference likelihood between ei and ej in D. To this end, we set s(ei, ) = 0 for all ei ∈ E. Inspired by (Lee et al., 2017b), we obtain the score function s(ei, ej) for ei and ej by leveraging their GCNinduced representation vectors rei and rej via:\ns(ei, ej) = sm(ei) + sm(ej) + sc(ei, ej) + sa(ei, ej) sm(ei) = w > mFFm(rei) sc(ei, ej) = w > a FFc([rei , rej , rei rej ]) sa(ei, ej) = r > eiW crej\nwhere Fm and FFc are two-layer feed-forward networks, w>m and w > a are learnable vectors, W c is a weight matrix, and is the element-wise multiplication. At the inference time, we employ the greedy decoding to predict the antecedent ŷi for ei: ŷi = argmaxP (yi|ei,Yi). For training, we use the negative log-likelihood as the loss function in our end-to-end framework: Lpred = − ∑|E|\ni=0 logP (y ∗ i |ei,Yi) (y∗i ∈ Yi is the golden\nantecedent for ei)."
    }, {
      "heading" : "3.4 Cluster Consistency Regularization",
      "text" : "To further improve representation learning for ECR, we propose to regularize the induced representation vectors of the event mentions in E to explicitly enforce the consistency/similarity between golden and predicted event mention clusters in D. This is based on our motivation that ECR models will perform better if they can produce more similar event mention clusters to the golden ones. As such, for convenience, let T = {T1, T2, . . . , T|T |} and P = {P1, P2, . . . , P|P|} be the golden and predicted sets of event mentions in E respectively, i.e., Ti, Pj ⊂ E, and T1 ∪ T2 ∪ . . . ∪ T|T | = P1∪P2∪. . .∪P|P| = E. Also, for each clusterC in T or P , we compute a centroid vector rC for it by averaging the representation vectors of the event mention members: rC = averagee∈C(re). This leads to the centroid vectors {rT1 , rT2 , . . . , rT|T |} and {rP1 , rP2 , . . . , rP|P|} for T and P respectively. We propose the following regularization terms for cluster consistency:\nIntra-cluster Consistency: This constraint concerns the inner information of each cluster, characterizing the structure of each individual event mention in its golden and predicted clusters in T andP . In particular, for each event mention ei ∈ E, we expect its distances to the centroid vectors of the corresponding golden and predicted clusters T ′i and P ′ i in T and P (respectively) to be similar, i.e., T ′i ∈ T , P ′i ∈ P, ei ∈ T ′i , ei ∈ P ′i . As such, we compute the distances between the representation vector rei of ei to the centroid vectors rT ′i and rP ′i via the Euclidean distances ‖rei − rT ′i ‖ 2 2 and ‖rei − rP ′i ‖ 2 2. Afterward, the differences Linner between the two distances for golden and predicted clusters are aggregated over all event mentions and added into the overall loss function for minimization: Linner = ∑|E| i=1 |‖rei−rT ′i ‖ 2 2−‖rei−rP ′i ‖ 2 2|. Inter-cluster Consistency: In this constraint, we expect that the structure among the clusters Ti in the golden set T is consistent with those for the predicted event cluster set P (i.e., inter-cluster regulation). To implement this idea, we encode the structure of the clusters in a set via the average of the pairwise distances between the centroid vectors of the clusters. In particular, the inter-cluster structure scores for the golden and predicted clusters in T and P are computed via: sT = 2 |T |(|T |−1) ∑|T | i=1 ∑|T | j=i+1 ‖rTi − rTj‖22, and sP = 2 |P|(|P|−1) ∑|P| i=1 ∑|P| j=i+1 ‖rPi − rPj‖22. The difference between the structure scores for golden and predicted clusters T and P is then included into the overall loss function for minimization: Linter = |sT − sP |. Inter-set Similarity: This constraint aims to directly promote the similarity between the golden clusters in T and the predicted clusters in P . As such, for the golden and predicted cluster sets T and P , we first obtain the overall centroid vectors uT and uP (respectively) by averaging the centroid vectors of their member clusters: uT = averageT∈T (rT ) and uP = averageP∈P(rP ). The Euclidean distance Lsim is then integrated into the overall loss for minimization: Lsim = ‖uT −uP‖22. Note that Linner, Linter, and Lsim will be zero if the predicted clusters in P are the same as those in the golden clusters in T .\nTo summarize, the overall loss function L to train our ECR model in this work is: L = Lpred + αinnerLinner + αinterLinter + αsimLsim with αinner, αinter, and αsim as the trade-off parameters."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset & Hyperparameters",
      "text" : "Following prior work (Choubey and Huang, 2018), we train our ECR models on the KBP 2015 dataset (Mitamura et al., 2015) and evaluate the models on the KBP 2016 and KBP 2017 datasets for ECR (Mitamura et al., 2016, 2017). In particular, the KBP 2015 dataset includes 360 annotated documents for ECR (181 documents from discussion forum and 179 documents from news articles). We use the same 310 documents from KBP 2015 as in (Choubey and Huang, 2018) for the training data and the remaining 50 documents for the development data. Also, similar to (Choubey and Huang, 2018), the news articles in KBP 2016 (85 documents) and KBP 2017 (83 documents) are leveraged for test datasets. To ensure a fair comparison, we use the predicted event mentions provided by (Choubey and Huang, 2018) in all the datasets. Finally, we report the ECR performance based on the official KBP 2017 scorer (version 1.8)3. The scorer employs four coreference scoring measures, including MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF-e (Luo, 2005), BLANC (Lee et al., 2012b), and the unweighted average of their F1 scores (AVGF1).\nHyper-parameters for the models are fine-tuned by the AVGF1 scores over development data. The selected values from the tuning process include: 1e5 for the learning rate of the Adam optimizer (selected from [1e-5, 2e-5, 3e-5, 4e-5, 5e-5]); 8 for the mini-batch size (selected from [8, 16, 32, 64]); 128 hidden units for all the feed-forward network and GCN layers (selected from [64, 128, 256, 512]); 2 layers for the GCN model, G = 2 (selected from [1, 2, 3, 4]), and αinner = 0.1, αinter = 0.1, and αsim = 0.1 for the trade-off parameters in the overall loss function L (selected from [0.1, 0, 2, . . . , 0.9]). Finally, we use the BERTbase model (of 768 dimensions) for the pre-trained word embeddings (updated during the training)."
    }, {
      "heading" : "4.2 Performance Evaluation",
      "text" : "We compare the proposed model for ECR with document structures and cluster consistency regularization (called StructECR) with prior work ECR models in the same evaluation setting, including the joint model between ECR and event detection (Lu and Ng, 2017), the integer linear programming\n3https://github.com/hunterhector/ EvmEval\napproach in (Choubey and Huang, 2018), and the discourse structure profiling model in (Choubey et al., 2020) (also the model with the best reported performance in KBP datasets). In addition, we examine the following baselines of StructECR to highlight the benefits of the proposed components:\nE2E-Only: This variant implements the end-toend resolution model described in Section 3.3 where all event mentions in a document are resolved simultaneously in a single process. However, different from our full model StructECR, E2EOnly does not include the document structure component with GCN for representation learning, i.e., it directly uses the initial representation vectors vi (induced from BERT) for the event mentions in the computation of the distribution P (yi|ei,Yi). Also, the cluster consistency regularization in Section 3.4 is also not included in this model.\nPairwise: This model is similar to E2E-Only in that it does not applies the document structures and regularization terms in StructECR. In addition, instead of simultaneously resolving event mentions in documents, Pairwise predicts the coreference for every pair of event mentions separately. In particular, the representation vectors vei and vej for two event mentions ei and ej (included from BERT) are combined via [vei , vej , vei vej ]. This vector is then sent into a feed-forward network to produce a distribution over possible coreference labels between ei and ej (i.e., two labels for being coreferent or not). The coreference labels for every pair of event mentions are then gathered in a coreference graphs among event mentions; the connected components will be returned for the event clusters.\nTable 1 reports the performance of the ECR models on the KBP 2016 and KBP 2017 datasets. As can be seen from the table, E2E-Only performs comparably or better than prior state-of-the-art models for ECR, e.g., (Choubey and Huang, 2018) and (Choubey et al., 2020), that employ extensive feature engineering. In addition, the better performance of E2E-Only over Pairwise (for both KBP 2016 and KBP 2017) illustrates the benefits of endto-end coreference resolution for event mentions in documents. Most importantly, the proposed model StructECR significantly outperforms all the baseline models for which the performance improvement over E2E-Only is 1.94% and 1.26% (i.e., AVGF1 scores) over the KBP 2016 and KBP 2017 datasets respectively. This clearly demonstrates the benefits of the proposed ECR model with rich\ndocument structures and cluster consistency regularization for representation learning."
    }, {
      "heading" : "4.3 Ablation Study",
      "text" : "Two major components in the proposed model StructECR involve the document structures and the cluster consistency regularization. This section performs an ablation study to reveal the contribution of such components for the full model. First, for the document structures, we examine the following ablated models: (i) “StructECR - x”: where x is one of the five interaction scores used to compute the unified score aij for G (i.e., asentij , a coref ij , a span ij , a dep ij , and a struct ij ). For example, “StructECR - aspanij ” implies a variant of StructECR where the span-based interaction score aspanij is not included in the compuation of the overall score aij ; (ii) “StructECR - Entity Nodes: this model excludes the entity mention nodes from the interaction graph G in StructECR (i.e.,N = D∪E only); (iii) “StructECR - GraphCombine”: instead of unifying the five interaction scores in dij into an overall score aij in Equation 1, this model considers each of the five generated interaction scores as forming a separate interaction graph, thus producing six different graphs. The GCN model is then applied over those five graphs (using the same initial representation vectors vi for the nodes ni in N ). The outputs of the GCN model for the same node ni (with different graphs) are then concatenated to compute the final representation vector hi for ni; and (iv) StructECR - Doc Structures: this model removes the GCN model from StructECR. As such, the interaction graph G is not used and the GCN-induced representation vectors hi are replaced by the initial BERT-induced representation vectors vi in the computation for end-to-end resolution and consistency regularization.\nSecond, for the cluster consistency regularization, we evaluate the following ablated models for StructECR: (v) StructECR - y (y ∈\n{Linner,Linter,Lsim}): these models exclude one of the regularization terms for the consistency between golden and predicted clusters from the overall loss function L; and (vi) StructECR - Regularization: this model completely ignores the consistency regularization component from StructECR.\nTable 2 shows the performance of the models on the development data of the KBP 2015 dataset. As can be seen, the elimination of any component from StructECR would significantly hurt the performance, thus clearly demonstrating the benefits of the designed document structures and cluster consistency regularization in StructECR."
    }, {
      "heading" : "4.4 Cross-domain Evaluation",
      "text" : "To further demonstrate the benefits for the proposed model StructECR, we evaluate StructECR and the baseline models Pairwise and E2E-Only in the cross-domain setting. In this setting, we aim\nto train the models on one domain (the source domain) and evaluate them on another domain (the target domain). We leverage the KBP 2016 and KBP 2017 datasets for this experiment. In particular, KBP 2016 annotates ECR data for 85 newswire and 84 discussion forum documents (i.e., two domains/genres) while KBP 2017 provides annotated data for ECR on 83 news articles and 84 discussion forum documents. As such, for each dataset, we consider two setups where documents in one domain (i.e., newswire or discussion forum) are used for the source domain, leaving documents in the other domain for the target domain data. We use the same hyper-parameters that are tuned on the development set of KBP 2015 for the models in this experiment. Table 3 presents the performance of the models. It is clear from the table that StructECR are significantly and substantially better than the baseline models (p < 0.01) over different datasets and settings for the source and target domains, thereby confirming the domain generalization advantages of StructECR for ECR."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We present a novel end-to-end coreference resolution framework for event mentions based on deep learning. The novelty in our model is twofold. First, document structures are introduced to explicitly capture relevant objects and their interactions in documents to aid representation learning. Second, several regularization techniques are proposed to exploit the consistencies between human-provided and machine-generated clusters of event mentions in documents. We perform extensive experiments on two benchmark datasets for ECR to demonstrate the advantages of the proposed model. In the future, we plan to extend our models to related problems in information extraction, e.g., event extraction."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research has been supported by the Army Research Office (ARO) grant W911NF-21-1-0112. This research is also based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA Contract No. 2019- 19051600006 under the Better Extraction from Text Towards Enhanced Retrieval (BETTER) Program. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, ei-\nther expressed or implied, of ARO, ODNI, IARPA, the Department of Defense, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. This document does not contain technology or technical data controlled under either the U.S. International Traffic in Arms Regulations or the U.S. Export Administration Regulations."
    } ],
    "references" : [ {
      "title" : "Unsupervised event coreference resolution",
      "author" : [ "Cosmin Adrian Bejan", "Sanda Harabagiu." ],
      "venue" : "Computational Linguistics (CL).",
      "citeRegEx" : "Bejan and Harabagiu.,? 2014",
      "shortCiteRegEx" : "Bejan and Harabagiu.",
      "year" : 2014
    }, {
      "title" : "The stages of event extraction",
      "author" : [ "David Ahn." ],
      "venue" : "Proceedings of the Workshop on Annotating and Reasoning about Time and Events.",
      "citeRegEx" : "Ahn.,? 2006",
      "shortCiteRegEx" : "Ahn.",
      "year" : 2006
    }, {
      "title" : "Joint event trigger identification and event coreference resolution with structured perceptron",
      "author" : [ "Jun Araki", "Teruko Mitamura." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Araki and Mitamura.,? 2015",
      "shortCiteRegEx" : "Araki and Mitamura.",
      "year" : 2015
    }, {
      "title" : "Entitybased cross-document coreferencing using the vector space model",
      "author" : [ "Amit Bagga", "Breck Baldwin." ],
      "venue" : "Proceedings of the International Conference on Computational Linguistics (COLING).",
      "citeRegEx" : "Bagga and Baldwin.,? 1998",
      "shortCiteRegEx" : "Bagga and Baldwin.",
      "year" : 1998
    }, {
      "title" : "Revisiting joint modeling of cross-document entity and event coreference resolution",
      "author" : [ "Shany Barhom", "Vered Shwartz", "Alon Eirew", "Michael Bugert", "Nils Reimers", "Ido Dagan." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Barhom et al\\.,? 2019",
      "shortCiteRegEx" : "Barhom et al\\.",
      "year" : 2019
    }, {
      "title" : "Moving down the long tail of word sense disambiguation with gloss informed bi-encoders",
      "author" : [ "Terra Blevins", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Blevins and Zettlemoyer.,? 2020",
      "shortCiteRegEx" : "Blevins and Zettlemoyer.",
      "year" : 2020
    }, {
      "title" : "Streamlining crossdocument coreference resolution: Evaluation and modeling",
      "author" : [ "Arie Cattan", "Alon Eirew", "Gabriel Stanovsky", "Mandar Joshi", "Ido Dagan." ],
      "venue" : "arXiv preprint arXiv:2009.11032.",
      "citeRegEx" : "Cattan et al\\.,? 2020",
      "shortCiteRegEx" : "Cattan et al\\.",
      "year" : 2020
    }, {
      "title" : "Joint inference over a lightly supervised information extraction pipeline: Towards event coreference resolution for resourcescarce languages",
      "author" : [ "Chen Chen", "Vincent Ng." ],
      "venue" : "Proceedings of the Association for the Advancement of Artificial Intelligence",
      "citeRegEx" : "Chen and Ng.,? 2016",
      "shortCiteRegEx" : "Chen and Ng.",
      "year" : 2016
    }, {
      "title" : "Graph-based event coreference resolution",
      "author" : [ "Zheng Chen", "Heng Ji." ],
      "venue" : "Proceedings of the Workshop on Graph-based Methods for Natural Language Processing.",
      "citeRegEx" : "Chen and Ji.,? 2009",
      "shortCiteRegEx" : "Chen and Ji.",
      "year" : 2009
    }, {
      "title" : "A pairwise event coreference model, feature impact and evaluation for event coreference resolution",
      "author" : [ "Zheng Chen", "Heng Ji", "Robert Haralick." ],
      "venue" : "Proceedings of the Workshop on Events in Emerging Text Types.",
      "citeRegEx" : "Chen et al\\.,? 2009",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2009
    }, {
      "title" : "Event coreference resolution by iteratively unfolding inter-dependencies among events",
      "author" : [ "Prafulla Kumar Choubey", "Ruihong Huang." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Choubey and Huang.,? 2017",
      "shortCiteRegEx" : "Choubey and Huang.",
      "year" : 2017
    }, {
      "title" : "Improving event coreference resolution by modeling correlations between event coreference chains and document topic structures",
      "author" : [ "Prafulla Kumar Choubey", "Ruihong Huang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Choubey and Huang.,? 2018",
      "shortCiteRegEx" : "Choubey and Huang.",
      "year" : 2018
    }, {
      "title" : "Discourse as a function of event: Profiling discourse structure in news articles around the main event",
      "author" : [ "Prafulla Kumar Choubey", "Aaron Lee", "Ruihong Huang", "Lu Wang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Choubey et al\\.,? 2020",
      "shortCiteRegEx" : "Choubey et al\\.",
      "year" : 2020
    }, {
      "title" : "bag of events” approach to event coreference resolution",
      "author" : [ "Agata Cybulska", "Piek T.J.M. Vossen." ],
      "venue" : "supervised classification of event templates. In Int. J. Comput. Linguistics Appl.",
      "citeRegEx" : "Cybulska and Vossen.,? 2015",
      "shortCiteRegEx" : "Cybulska and Vossen.",
      "year" : 2015
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Easy victories and uphill battles in coreference resolution",
      "author" : [ "Greg Durrett", "Dan Klein." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Durrett and Klein.,? 2013",
      "shortCiteRegEx" : "Durrett and Klein.",
      "year" : 2013
    }, {
      "title" : "Improving event coreference resolution by learning argument compatibility from unlabeled data",
      "author" : [ "Yin Jou Huang", "Jing Lu", "Sadao Kurohashi", "Vincent Ng." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-",
      "citeRegEx" : "Huang et al\\.,? 2019",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "Spanbert: Improving pre-training by representing and predicting spans",
      "author" : [ "Mandar Joshi", "Danqi Chen", "Y. Liu", "Daniel S. Weld", "Luke Zettlemoyer", "Omer Levy." ],
      "venue" : "Transactions of the Association for Computational Linguistics (TACL).",
      "citeRegEx" : "Joshi et al\\.,? 2019a",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT for coreference resolution: Baselines and analysis",
      "author" : [ "Mandar Joshi", "Omer Levy", "Luke Zettlemoyer", "Daniel Weld." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Joshi et al\\.,? 2019b",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2019
    }, {
      "title" : "Resolving event coreference with supervised representation learning and clusteringoriented regularization",
      "author" : [ "Kian Kenyon-Dean", "Jackie Chi Kit Cheung", "Doina Precup." ],
      "venue" : "Proceedings of the Eighth Joint Conference on Lexical and Computa-",
      "citeRegEx" : "Kenyon.Dean et al\\.,? 2018",
      "shortCiteRegEx" : "Kenyon.Dean et al\\.",
      "year" : 2018
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas N. Kipf", "Max Welling." ],
      "venue" : "Proceedings of the International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Kipf and Welling.,? 2017",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2017
    }, {
      "title" : "Joint entity and event coreference resolution across documents",
      "author" : [ "Heeyoung Lee", "Marta Recasens", "Angel Chang", "Mihai Surdeanu", "Dan Jurafsky." ],
      "venue" : "Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Lee et al\\.,? 2012a",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2012
    }, {
      "title" : "Joint entity and event coreference resolution across documents",
      "author" : [ "Heeyoung Lee", "Marta Recasens", "Angel Chang", "Mihai Surdeanu", "Dan Jurafsky." ],
      "venue" : "Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Lee et al\\.,? 2012b",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2012
    }, {
      "title" : "End-to-end neural coreference resolution",
      "author" : [ "Kenton Lee", "Luheng He", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Lee et al\\.,? 2017a",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2017
    }, {
      "title" : "End-to-end neural coreference resolution",
      "author" : [ "Kenton Lee", "Luheng He", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Lee et al\\.,? 2017b",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2017
    }, {
      "title" : "Joint event extraction via structured prediction with global features",
      "author" : [ "Qi Li", "Heng Ji", "Liang Huang." ],
      "venue" : "Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Li et al\\.,? 2013",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2013
    }, {
      "title" : "An information-theoretic definition of similarity",
      "author" : [ "Dekang Lin" ],
      "venue" : "Proceedings of the International Conference on Machine Learning (ICML).",
      "citeRegEx" : "Lin,? 1998",
      "shortCiteRegEx" : "Lin",
      "year" : 1998
    }, {
      "title" : "Supervised within-document event coreference using information propagation",
      "author" : [ "Zhengzhong Liu", "Jun Araki", "Eduard Hovy", "Teruko Mitamura." ],
      "venue" : "Proceedings of the International Conference on Language Resources and Evaluation (LREC).",
      "citeRegEx" : "Liu et al\\.,? 2014",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2014
    }, {
      "title" : "Joint learning for event coreference resolution",
      "author" : [ "Jing Lu", "Vincent Ng." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Lu and Ng.,? 2017",
      "shortCiteRegEx" : "Lu and Ng.",
      "year" : 2017
    }, {
      "title" : "Joint inference for event coreference resolution",
      "author" : [ "Jing Lu", "Deepak Venugopal", "Vibhav Gogate", "Vincent Ng." ],
      "venue" : "Transactions of the Association for Computational Linguistics (TACL).",
      "citeRegEx" : "Lu et al\\.,? 2016",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2016
    }, {
      "title" : "End-to-end neural event coreference resolution",
      "author" : [ "Yaojie Lu", "Hongyu Lin", "Jialong Tang", "Xianpei Han", "Le Sun." ],
      "venue" : "CoRR abs/2009.08153.",
      "citeRegEx" : "Lu et al\\.,? 2020",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2020
    }, {
      "title" : "On coreference resolution performance metrics",
      "author" : [ "Xiaoqiang Luo." ],
      "venue" : "Proceedings of the 2005 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Luo.,? 2005",
      "shortCiteRegEx" : "Luo.",
      "year" : 2005
    }, {
      "title" : "Wordnet: a lexical database for english",
      "author" : [ "George A Miller." ],
      "venue" : "Communications of the ACM, 38(11):39–",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "Overview of TAC-KBP 2015 event nugget track",
      "author" : [ "Teruko Mitamura", "Zhengzhong Liu", "Eduard H. Hovy." ],
      "venue" : "Proceedings of the Text Analysis Conference (TAC).",
      "citeRegEx" : "Mitamura et al\\.,? 2015",
      "shortCiteRegEx" : "Mitamura et al\\.",
      "year" : 2015
    }, {
      "title" : "Overview of TAC-KBP 2016 event nugget track",
      "author" : [ "Teruko Mitamura", "Zhengzhong Liu", "Eduard H. Hovy." ],
      "venue" : "Proceedings of the Text Analysis Conference (TAC).",
      "citeRegEx" : "Mitamura et al\\.,? 2016",
      "shortCiteRegEx" : "Mitamura et al\\.",
      "year" : 2016
    }, {
      "title" : "Events detection, coreference and sequencing: What’s next? overview of the TAC KBP 2017 event track",
      "author" : [ "Teruko Mitamura", "Zhengzhong Liu", "Eduard H. Hovy." ],
      "venue" : "Proceedings of the Text Analysis Conference (TAC).",
      "citeRegEx" : "Mitamura et al\\.,? 2017",
      "shortCiteRegEx" : "Mitamura et al\\.",
      "year" : 2017
    }, {
      "title" : "Supervised noun phrase coreference research: The first fifteen years",
      "author" : [ "Vincent Ng." ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Ng.,? 2010",
      "shortCiteRegEx" : "Ng.",
      "year" : 2010
    }, {
      "title" : "New york university 2016 system for kbp event nugget: A deep learning approach",
      "author" : [ "Thien Huu Nguyen", "Adam Meyers", "Ralph Grishman" ],
      "venue" : "In Proceedings of the Text Analysis Conference (TAC)",
      "citeRegEx" : "Nguyen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "Graph convolutional networks with argument-aware pooling for event detection",
      "author" : [ "Thien Huu Nguyen", "Ralph Grishman." ],
      "venue" : "Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI).",
      "citeRegEx" : "Nguyen and Grishman.,? 2018",
      "shortCiteRegEx" : "Nguyen and Grishman.",
      "year" : 2018
    }, {
      "title" : "Event detection and co-reference with minimal supervision",
      "author" : [ "Haoruo Peng", "Yangqiu Song", "Dan Roth." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Peng et al\\.,? 2016",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2016
    }, {
      "title" : "A multipass sieve for coreference resolution",
      "author" : [ "Karthik Raghunathan", "Heeyoung Lee", "Sudarshan Rangarajan", "Nathanael Chambers", "Mihai Surdeanu", "Dan Jurafsky", "Christopher Manning." ],
      "venue" : "Proceedings of the 2010 Conference on Empirical Methods",
      "citeRegEx" : "Raghunathan et al\\.,? 2010",
      "shortCiteRegEx" : "Raghunathan et al\\.",
      "year" : 2010
    }, {
      "title" : "Exploiting the syntax-model consistency for neural relation extraction",
      "author" : [ "Amir Pouran Ben Veyseh", "Franck Dernoncourt", "Dejing Dou", "Thien Huu Nguyen." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Veyseh et al\\.,? 2020a",
      "shortCiteRegEx" : "Veyseh et al\\.",
      "year" : 2020
    }, {
      "title" : "Graph transformer networks with syntactic and semantic structures for event argument extraction",
      "author" : [ "Amir Pouran Ben Veyseh", "Tuan Ngo Nguyen", "Thien Huu Nguyen." ],
      "venue" : "Proceedings of the Findings of the Conference on Empirical Methods",
      "citeRegEx" : "Veyseh et al\\.,? 2020b",
      "shortCiteRegEx" : "Veyseh et al\\.",
      "year" : 2020
    }, {
      "title" : "A modeltheoretic coreference scoring scheme",
      "author" : [ "Marc Vilain", "John Burger", "John Aberdeen", "Dennis Connolly", "Lynette Hirschman." ],
      "venue" : "Sixth Message Understanding Conference (MUC-6).",
      "citeRegEx" : "Vilain et al\\.,? 1995",
      "shortCiteRegEx" : "Vilain et al\\.",
      "year" : 1995
    }, {
      "title" : "A hierarchical distance-dependent Bayesian model for event coreference resolution",
      "author" : [ "Bishan Yang", "Claire Cardie", "Peter Frazier." ],
      "venue" : "Transactions of the Association for Computational Linguistics (TACL).",
      "citeRegEx" : "Yang et al\\.,? 2015",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "A common component in prior ECR models involves a binary classifier that receives a pair of event mentions and predict their coreference (Chen et al., 2009; Lu et al., 2016; Lu and Ng, 2017).",
      "startOffset" : 137,
      "endOffset" : 190
    }, {
      "referenceID" : 29,
      "context" : "A common component in prior ECR models involves a binary classifier that receives a pair of event mentions and predict their coreference (Chen et al., 2009; Lu et al., 2016; Lu and Ng, 2017).",
      "startOffset" : 137,
      "endOffset" : 190
    }, {
      "referenceID" : 28,
      "context" : "A common component in prior ECR models involves a binary classifier that receives a pair of event mentions and predict their coreference (Chen et al., 2009; Lu et al., 2016; Lu and Ng, 2017).",
      "startOffset" : 137,
      "endOffset" : 190
    }, {
      "referenceID" : 28,
      "context" : "Early work on ECR has achieved feature representation via feature engineering where multiple features are hand-designed for input event mention pairs (Lu and Ng, 2017).",
      "startOffset" : 150,
      "endOffset" : 167
    }, {
      "referenceID" : 19,
      "context" : "hand-designed features (Kenyon-Dean et al., 2018; Barhom et al., 2019) that still suffer from the feature sparsity issue.",
      "startOffset" : 23,
      "endOffset" : 70
    }, {
      "referenceID" : 4,
      "context" : "hand-designed features (Kenyon-Dean et al., 2018; Barhom et al., 2019) that still suffer from the feature sparsity issue.",
      "startOffset" : 23,
      "endOffset" : 70
    }, {
      "referenceID" : 20,
      "context" : "Using graph convolutional neural networks (GCN) (Kipf and Welling, 2017; Nguyen and Grishman, 2018) for representation learning, we expect enriched representation vectors from the document structures can further improve the performance of ECR systems.",
      "startOffset" : 48,
      "endOffset" : 99
    }, {
      "referenceID" : 38,
      "context" : "Using graph convolutional neural networks (GCN) (Kipf and Welling, 2017; Nguyen and Grishman, 2018) for representation learning, we expect enriched representation vectors from the document structures can further improve the performance of ECR systems.",
      "startOffset" : 48,
      "endOffset" : 99
    }, {
      "referenceID" : 21,
      "context" : "Our work focuses on the within-document setting for ECR where input event mentions are expected to appear in the same input documents; however, we also note prior works on crossdocument ECR (Lee et al., 2012a; Adrian Bejan and Harabagiu, 2014; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Barhom et al., 2019; Cattan et al., 2020).",
      "startOffset" : 190,
      "endOffset" : 336
    }, {
      "referenceID" : 10,
      "context" : "Our work focuses on the within-document setting for ECR where input event mentions are expected to appear in the same input documents; however, we also note prior works on crossdocument ECR (Lee et al., 2012a; Adrian Bejan and Harabagiu, 2014; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Barhom et al., 2019; Cattan et al., 2020).",
      "startOffset" : 190,
      "endOffset" : 336
    }, {
      "referenceID" : 19,
      "context" : "Our work focuses on the within-document setting for ECR where input event mentions are expected to appear in the same input documents; however, we also note prior works on crossdocument ECR (Lee et al., 2012a; Adrian Bejan and Harabagiu, 2014; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Barhom et al., 2019; Cattan et al., 2020).",
      "startOffset" : 190,
      "endOffset" : 336
    }, {
      "referenceID" : 4,
      "context" : "Our work focuses on the within-document setting for ECR where input event mentions are expected to appear in the same input documents; however, we also note prior works on crossdocument ECR (Lee et al., 2012a; Adrian Bejan and Harabagiu, 2014; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Barhom et al., 2019; Cattan et al., 2020).",
      "startOffset" : 190,
      "endOffset" : 336
    }, {
      "referenceID" : 6,
      "context" : "Our work focuses on the within-document setting for ECR where input event mentions are expected to appear in the same input documents; however, we also note prior works on crossdocument ECR (Lee et al., 2012a; Adrian Bejan and Harabagiu, 2014; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Barhom et al., 2019; Cattan et al., 2020).",
      "startOffset" : 190,
      "endOffset" : 336
    }, {
      "referenceID" : 1,
      "context" : "models for pairwise classifiers (Ahn, 2006; Chen et al., 2009; Cybulska and Vossen, 2015; Peng et al., 2016), spectral graph clustering (Chen and Ji, 2009), information propagation (Liu et al.",
      "startOffset" : 32,
      "endOffset" : 108
    }, {
      "referenceID" : 9,
      "context" : "models for pairwise classifiers (Ahn, 2006; Chen et al., 2009; Cybulska and Vossen, 2015; Peng et al., 2016), spectral graph clustering (Chen and Ji, 2009), information propagation (Liu et al.",
      "startOffset" : 32,
      "endOffset" : 108
    }, {
      "referenceID" : 13,
      "context" : "models for pairwise classifiers (Ahn, 2006; Chen et al., 2009; Cybulska and Vossen, 2015; Peng et al., 2016), spectral graph clustering (Chen and Ji, 2009), information propagation (Liu et al.",
      "startOffset" : 32,
      "endOffset" : 108
    }, {
      "referenceID" : 39,
      "context" : "models for pairwise classifiers (Ahn, 2006; Chen et al., 2009; Cybulska and Vossen, 2015; Peng et al., 2016), spectral graph clustering (Chen and Ji, 2009), information propagation (Liu et al.",
      "startOffset" : 32,
      "endOffset" : 108
    }, {
      "referenceID" : 8,
      "context" : ", 2016), spectral graph clustering (Chen and Ji, 2009), information propagation (Liu et al.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 27,
      "context" : ", 2016), spectral graph clustering (Chen and Ji, 2009), information propagation (Liu et al., 2014), markov logic networks (Lu et al.",
      "startOffset" : 80,
      "endOffset" : 98
    }, {
      "referenceID" : 29,
      "context" : ", 2014), markov logic networks (Lu et al., 2016), joint mod-",
      "startOffset" : 31,
      "endOffset" : 48
    }, {
      "referenceID" : 2,
      "context" : "eling of ECR with event detection (Araki and Mitamura, 2015; Lu et al., 2016; Chen and Ng, 2016; Lu and Ng, 2017), and recent deep learning models (Nguyen et al.",
      "startOffset" : 34,
      "endOffset" : 113
    }, {
      "referenceID" : 29,
      "context" : "eling of ECR with event detection (Araki and Mitamura, 2015; Lu et al., 2016; Chen and Ng, 2016; Lu and Ng, 2017), and recent deep learning models (Nguyen et al.",
      "startOffset" : 34,
      "endOffset" : 113
    }, {
      "referenceID" : 7,
      "context" : "eling of ECR with event detection (Araki and Mitamura, 2015; Lu et al., 2016; Chen and Ng, 2016; Lu and Ng, 2017), and recent deep learning models (Nguyen et al.",
      "startOffset" : 34,
      "endOffset" : 113
    }, {
      "referenceID" : 28,
      "context" : "eling of ECR with event detection (Araki and Mitamura, 2015; Lu et al., 2016; Chen and Ng, 2016; Lu and Ng, 2017), and recent deep learning models (Nguyen et al.",
      "startOffset" : 34,
      "endOffset" : 113
    }, {
      "referenceID" : 37,
      "context" : ", 2016; Chen and Ng, 2016; Lu and Ng, 2017), and recent deep learning models (Nguyen et al., 2016; Choubey and Huang, 2018; Huang et al., 2019; Lu et al., 2020; Choubey et al., 2020).",
      "startOffset" : 77,
      "endOffset" : 182
    }, {
      "referenceID" : 11,
      "context" : ", 2016; Chen and Ng, 2016; Lu and Ng, 2017), and recent deep learning models (Nguyen et al., 2016; Choubey and Huang, 2018; Huang et al., 2019; Lu et al., 2020; Choubey et al., 2020).",
      "startOffset" : 77,
      "endOffset" : 182
    }, {
      "referenceID" : 16,
      "context" : ", 2016; Chen and Ng, 2016; Lu and Ng, 2017), and recent deep learning models (Nguyen et al., 2016; Choubey and Huang, 2018; Huang et al., 2019; Lu et al., 2020; Choubey et al., 2020).",
      "startOffset" : 77,
      "endOffset" : 182
    }, {
      "referenceID" : 30,
      "context" : ", 2016; Chen and Ng, 2016; Lu and Ng, 2017), and recent deep learning models (Nguyen et al., 2016; Choubey and Huang, 2018; Huang et al., 2019; Lu et al., 2020; Choubey et al., 2020).",
      "startOffset" : 77,
      "endOffset" : 182
    }, {
      "referenceID" : 12,
      "context" : ", 2016; Chen and Ng, 2016; Lu and Ng, 2017), and recent deep learning models (Nguyen et al., 2016; Choubey and Huang, 2018; Huang et al., 2019; Lu et al., 2020; Choubey et al., 2020).",
      "startOffset" : 77,
      "endOffset" : 182
    }, {
      "referenceID" : 14,
      "context" : "In the first step, we transform each word wi ∈ D into a representation vector xi by feeding D into the pre-trained language model BERT (Devlin et al., 2019).",
      "startOffset" : 135,
      "endOffset" : 156
    }, {
      "referenceID" : 11,
      "context" : "event mentions that are provided by (Choubey and Huang, 2018) in the datasets for E.",
      "startOffset" : 36,
      "endOffset" : 61
    }, {
      "referenceID" : 32,
      "context" : "To this end, we propose to utilize WordNet (Miller, 1995), a rich network of word meanings, to obtain external knowledge for the words in D.",
      "startOffset" : 43,
      "endOffset" : 57
    }, {
      "referenceID" : 5,
      "context" : "0 and the state-of-the-art BERT-based WSD model in (Blevins and Zettlemoyer, 2020) to perform the word-synset mapping in this work.",
      "startOffset" : 51,
      "endOffset" : 82
    }, {
      "referenceID" : 24,
      "context" : "Motivated by the entity coreference resolution in (Lee et al., 2017b), we implement the end-to-end resolution via a set of antecedent assignments for the event mentions in",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 24,
      "context" : "Inspired by (Lee et al., 2017b), we obtain the score function s(ei, ej) for ei and ej by leveraging their GCNinduced representation vectors rei and rej via:",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 33,
      "context" : "we train our ECR models on the KBP 2015 dataset (Mitamura et al., 2015) and evaluate the models on the KBP 2016 and KBP 2017 datasets for ECR (Mitamura et al.",
      "startOffset" : 48,
      "endOffset" : 71
    }, {
      "referenceID" : 11,
      "context" : "We use the same 310 documents from KBP 2015 as in (Choubey and Huang, 2018) for the training data and the remaining 50 documents for the develop-",
      "startOffset" : 50,
      "endOffset" : 75
    }, {
      "referenceID" : 11,
      "context" : "Also, similar to (Choubey and Huang, 2018), the news articles in KBP 2016 (85 documents) and KBP 2017 (83 documents) are leveraged for test datasets.",
      "startOffset" : 17,
      "endOffset" : 42
    }, {
      "referenceID" : 43,
      "context" : "The scorer employs four coreference scoring measures, including MUC (Vilain et al., 1995), B3 (Bagga",
      "startOffset" : 68,
      "endOffset" : 89
    }, {
      "referenceID" : 31,
      "context" : "and Baldwin, 1998), CEAF-e (Luo, 2005), BLANC (Lee et al.",
      "startOffset" : 27,
      "endOffset" : 38
    }, {
      "referenceID" : 22,
      "context" : "and Baldwin, 1998), CEAF-e (Luo, 2005), BLANC (Lee et al., 2012b), and the unweighted average of their F1 scores (AVGF1).",
      "startOffset" : 46,
      "endOffset" : 65
    }, {
      "referenceID" : 28,
      "context" : "We compare the proposed model for ECR with document structures and cluster consistency regularization (called StructECR) with prior work ECR models in the same evaluation setting, including the joint model between ECR and event detection (Lu and Ng, 2017), the integer linear programming",
      "startOffset" : 238,
      "endOffset" : 255
    }, {
      "referenceID" : 11,
      "context" : "com/hunterhector/ EvmEval approach in (Choubey and Huang, 2018), and the",
      "startOffset" : 38,
      "endOffset" : 63
    }, {
      "referenceID" : 12,
      "context" : "discourse structure profiling model in (Choubey et al., 2020) (also the model with the best reported performance in KBP datasets).",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : "and (Choubey et al., 2020), that employ extensive feature engineering.",
      "startOffset" : 4,
      "endOffset" : 26
    }, {
      "referenceID" : 28,
      "context" : "4847 KBP 2016 KBP 2017 Model B3 CEAFe MUC BLANC AVGF1 B3 CEAFe MUC BLANC AVGF1 (Lu and Ng, 2017) 50.",
      "startOffset" : 79,
      "endOffset" : 96
    } ],
    "year" : 2021,
    "abstractText" : "We study the problem of event coreference resolution (ECR) that seeks to group coreferent event mentions into the same clusters. Deep learning methods have recently been applied for this task to deliver state-of-the-art performance. However, existing deep learning models for ECR are limited in that they cannot exploit important interactions between relevant objects for ECR, e.g., context words and entity mentions, to support the encoding of document-level context. In addition, consistency constraints between golden and predicted clusters of event mentions have not been considered to improve representation learning in prior deep learning models for ECR. This work addresses such limitations by introducing a novel deep learning model for ECR. At the core of our model are document structures to explicitly capture relevant objects for ECR. Our document structures introduce diverse knowledge sources (discourse, syntax, semantics) to compute edges/interactions between structure nodes for document-level representation learning. We also present novel regularization techniques based on consistencies of golden and predicted clusters for event mentions in documents. Extensive experiments show that our model achieve state-of-the-art performance on two benchmark datasets.",
    "creator" : "LaTeX with hyperref"
  }
}