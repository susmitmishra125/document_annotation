{
  "name" : "2021.acl-long.48.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "COSY: COunterfactual SYntax for Cross-Lingual Understanding",
    "authors" : [ "Sicheng Yu", "Hao Zhang", "Yulei Niu", "Qianru Sun", "Jing Jiang" ],
    "emails" : [ "scyu.2018@phdcs.smu.edu.sg,", "hao007@e.ntu.edu.sg", "yn.yuleiniu@gmail.com,", "qianrusun@smu.edu.sg", "jingjiang@smu.edu.sg" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 577–589\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n577"
    }, {
      "heading" : "1 Introduction",
      "text" : "With the emergence of BERT (Devlin et al., 2019), large-scale pre-trained language models have become an indispensable component in the solutions to many natural language processing (NLP) tasks. Recently, large-scale multilingual transformer-based models, such as mBERT (Devlin et al., 2019), XLM (Lample and Conneau, 2019) and XLM-R (Conneau et al., 2020a), have been widely deployed as backbones in cross-lingual NLP tasks (Wu and Dredze, 2019; Pires et al., 2019; Keung et al., 2019). However, these models trained\n1Our code is publicly available on GitHub: https:// github.com/PluviophileYU/COSY\non a single resource-rich language, e.g., English, all suffer from a large drop of performance when tested on different target languages, e.g., Chinese and German—where the setting is called zeroshot cross-lingual transfer. For example, on the XQUAD dataset, mBERT achieves a 24 percentage points lower exact match score on the target language Chinese than on the training language English (Hu et al., 2020). This indicates that this model has seriously overfitted English.\nAn intuitive way to tackle this is to introduce language-agnostic information—the most transferable feature across languages, which is lacking in existing multilingual language models (Choenni and Shutova, 2020). In our work, we propose to exploit reliable language-agnostic information— syntax in the form of universal dependency relations and universal POS tags (de Marneffe et al., 2014; Nivre et al., 2016; Zhou et al., 2019, 2021). As illustrated in Figure 1, the sentences in Chinese and English share the same meaning but have differ-\nFactual Syntax\nent word orders. The order difference hampers the transferability between English and Chinese in conventional language models (with sequential words as input). In contrast, it is clear from Figure 1 that the two sentences share identical dependency relations and POS tags. Thus, we can incorporate such universal syntax2 information to enhance the transferability across different languages. To achieve this learning objective in deep models, we design syntax-aware networks that incorporate the encodings of dependency relations and POS tags into the encoding of semantics.\nHowever, we find that empirically the conventional attention-based incorporation of syntax, e.g., relational graph attention networks (Ishiwatari et al., 2020), has little effect on improving the model. One possible reason is that the learning process may be dominated by the pre-trained language models due to their strength in semantic representation learning, which leads to an overfitted model. This raises the question of how to induce the model to focus more on syntax while maintaining its original capability of representing semantics? To this end, we propose a novel COunterfactual SYntax (COSY) method, inspired by causal inference (Roese, 1997; Pearl et al., 2009) and contrastive learning (He et al., 2020).\nThe intuition behind COSY is to create copies of training instances with their syntactic features altered (see the “counterfactual” syntax in Figure 2), and to force the encodings of the counterfactual in-\n2In the rest of this paper, syntax denotes universal syntax for simplicity.\nstances to be different from the encodings of their corresponding factual instances. In this way, the model would learn to put more emphasis on the syntactic information when learning how to encode an instance, and such encodings are likely to perform well across languages.\nWe evaluate our COSY method on both question answering (QA) and natural language inference (NLI) under cross-lingual settings. Experimental results show that, without using any additional data, COSY is superior to the state-of-the-art methods. Contributions: 1) we develop a syntax-aware network that incorporates transferable syntax in language models; 2) we propose a novel counterfactual training method that addresses the technical challenge of emphasizing syntax; and 3) extensive experiments on three benchmarks demonstrate the effectiveness of our method for cross-lingual tasks."
    }, {
      "heading" : "2 Related Work",
      "text" : "Cross-lingual Transfer. Large-scale pre-trained language models (Devlin et al., 2019; Liu et al., 2019) have achieved sequential success in various natural language processing tasks. Recent studies (Lample and Conneau, 2019; Conneau et al., 2020a) extend the pre-trained language models to multilingual tasks and demonstrate their prominent capability on cross-lingual knowledge transfer, even under zero-shot scenario (Wu and Dredze, 2019; Pires et al., 2019; Hsu et al., 2019).\nMotivated by the success of multilingual language models on cross-lingual transfer, several works explore how these models work and what their bottleneck is. On the one hand, some studies find that the shared sub-words (Wu and Dredze, 2019; Dufter and Schütze, 2020) and the parameters of top layers (Conneau et al., 2020b) are crucial for cross-lingual transfer. On the other hand, the bottleneck is attributed to two issues: (i) catastrophic forgetting (Keung et al., 2020; Liu et al., 2020), where knowledge learned in the pre-training stage is forgotten in downstream fine-tuning; (ii) lack of language-agnostic features (Choenni and Shutova, 2020; Zhao et al., 2020) or linguistic discrepancy between the source and the target languages (Wu and Dredze, 2019; Lauscher et al., 2020). In this work, we aim to tackle zero-shot and few-shot cross-lingual transfer by focusing on the second issue.\nExisting works can be roughly divided into two groups. The first proposes to modify the lan-\nguage model by aligning languages with parallel data (Zhao et al., 2020) or strengthening sentencelevel representation (Wei et al., 2020). The second group focuses on the learning paradigm for finetuning on downstream tasks. For instance, some methods adopt meta-learning (Nooralahzadeh et al., 2020; Yan et al., 2020) or intermediate tasks training (Phang et al., 2020) to learn cross-lingual knowledge. Our COSY belongs to the second group and fills the blank of using the syntactic information in zero-shot (few-shot) cross-lingual understanding. Counterfactual Analysis. Counterfactual analysis aims to evaluate the causal effect of a variable by considering its counterfactual scenario. Counterfactual analysis has been widely studied in epidemiology (Rothman and Greenland, 2005) and social science (Steel, 2004). Recently, counterfactual reasoning has motivated studies in applications.\nIn the community of computer vision, counterfactual analysis has been successfully applied in explanation (Goyal et al., 2019a,b), long-tailed classification (Tang et al., 2020a), scene graph generation (Tang et al., 2020b), and visual question answering (Chen et al., 2020; Niu et al., 2020; Abbasnejad et al., 2020).\nIn the community of natural language processing, counterfactual methods are also emerging recently in text classification (Choi et al., 2020), story generation (Qin et al., 2019), dialog systems (Zhu et al., 2020), gender bias (Vig et al., 2020; Shin et al., 2020), question answering (Yu et al., 2020), and sentiment bias (Huang et al., 2020). To the best of our knowledge, we are the first to conduct counterfactual analysis in cross-lingual understanding. Different from previous works (Zhu et al., 2020; Qin et al., 2019) that generate word-level or sentence-level counterfactual samples, our counterfactual analysis dives into syntax level that is more controllable than text and free from complex language generation module."
    }, {
      "heading" : "3 COSY: COunterfactual SYntax",
      "text" : "COSY aims to leverage the syntactic information, e.g., dependency relations and POS tags, to increase the transferability of cross-lingual language models. Specifically, COSY implicitly forces the networks to learn to encode the input not only based on semantic features but also based on syntactic features through syntax-aware networks and a counterfactual training method.\nAs illustrated in Figure 3, COSY consists of three branches with each branch based on syntaxaware networks (SAN) indicated by a distinct color. The main branch (in black) is the factual branch that uses factual syntax as input. The red and blue branches are counterfactual branches using counterfactual dependency relations and counterfactual POS tags as input, respectively. The counterfactual training method guides the black branch to put more emphasis on syntactic information with the help of other two branches. Note that the red and blue branches work for counterfactual training, and only the prediction from the black branch is used in testing.\nBelow, we first elaborate the modules of SAN in Section 3.1, and then introduce the counterfactual training method in Section 3.2."
    }, {
      "heading" : "3.1 Syntax-Aware Networks (SAN)",
      "text" : "As shown in Figure 3, SAN contains four major modules: a set of feature extractors, a relational graph attention network (RGAT), fusion projection, and a classifier. In this section, we use the route in the black branch as an example to elaborate each module. The set of feature extractors include three components: a pre-trained language model, a dependency graph constructor and a POS tags extractor. Pre-trained Language Model. Following previous work (Hu et al., 2020), we deploy a pre-trained multi-lingual language model, e.g., mBERT (Devlin et al., 2019), to encode each input sentence into contextual features. Given a sequence of tokens with a length of S, we denote the derived contextual features as H=[h1, ...,hS ] ∈ RS×d, where d is the dimensionality of each hidden vector. Dependency Graph Constructor. We use it to construct the (factual) dependency graph for each input sentence. In this work, the Stanza toolkit (Qi et al., 2020) is used to extract the universal dependency relations as the first step. Then, the dependency graph can be represented as G={V,R,E}, where the nodes V are tokens, the edges E denote the existence of dependency relations, and the set R contains the relation types for E. Each edge eij ∈ E consists of a triplet (vi, vj , r) where v1, v2 ∈ V and r ∈ R.\nAs shown in Figure 3, we define three kinds of relation types in R : 1) a forward syntactic relation, e.g., love OBJ−−−→ apples; 2) an inverse syntactic relation, e.g., apples OBJ−1−−−→ love; and 3) a self loop\nSELF that allows the information to flow from a node to itself. Note that we regard the ROOT relation as a self-loop. In this way, we obtain 75 different types of relations in total, and thus denote the embedding matrix as R ∈ R75×d′ . POS Tags Extractor. We deploy the same Stanza toolkit (Qi et al., 2020) to assign (factual) POS tags P for all tokens. We obtain 17 different types of POS tags and denote the embedding matrix as T ∈ R17×d′ . Relational Graph Attention Networks (RGAT). RGAT is one of the standard backbones to incorporate the dependency graph (Ishiwatari et al., 2020; Linmei et al., 2019). Given the (factual) dependency graph G with the contextual features of each node, RGAT can generate the relation-aware features (for each node). Details are given below. Suppose eij is the directed edge from node vi to node vj and the dependency relation r. The importance score of vj from vi is computed as:\ns(vi, vj) = Concat(e s ij , e r ij) ·WAttn, (1)\nwhere WAttn ∈ R(d/2+d′)×1 maps a vector to a\nscalar, erij is the embedding of the dependency relation between vi and vj from R, and esij is computed by element-wise multiplication between vi and vj :\nesij = (hi ·WQ) ◦ (hj ·WK), (2) where WK ∈ Rd×d/2 and WQ ∈ Rd×d/2 are the learnable parameters for key and query projections (Vaswani et al., 2017), and hi and hj denote their contextual features extracted from pre-trained language models. Then, the importance scores are normalized across Nj to obtain the attention score of vj from vi:\nα(vi, vj) = exp(s(vi, vj))∑\nk∈Nj exp(s(vk, vj)) , (3)\nwhere Nj denotes the set of nodes pointing to vj . The relation-aware features of vj is computed as the weighted sum of all nodes in Nj with corresponding attention scores. After computing all nodes, we get the relation-aware features Ĥ=[ĥ1, ..., ĥS ] ∈ RS×d. Fusion Projection. We fuse the relation-aware features Ĥ with the (factual) POS tags informa-\ntion before feeding them into the classifier. Given POS tags P , the fused features for each token are represented by\nfj = Concat(ĥj ,pj) ·WF , (4)\nwhere WF ∈ R(d+d′)×d are learnable parameters of fusion projection and pj is the corresponding embedding of the POS tag of the j-th token from T. The fused features of the entire sequence are denoted as F=[f1, ..., fS ] ∈ RS×d. Classifier. It is designed based on the specific task, such as NLI or QA, following Devlin et al. (2019)."
    }, {
      "heading" : "3.2 Counterfactual Training",
      "text" : "Recall that the challenge in the effective utilization of syntax is how to induce the model to focus more on syntax while maintaining its original representation capability of semantics. Inspired by counterfactual analysis (Pearl et al., 2009; Pearl, 2010; Pearl and Mackenzie, 2018) and contrastive learning (Hadsell et al., 2006), we propose a counterfactual training method by incorporating counterfactual syntax (counterfactual dependency graph and counterfactual POS tags) on the red and blue branches in Figure 3. Each branch is designed to guide the model to focus on one type of syntax, i.e., dependency graph or POS tags. Counterfactual Dependency Graph is utilized on the red branch with factual POS tags in Figure 3. We build a counterfactual dependency graph by maintaining graph structure and nodes, and replacing each type of relation (except for a self-loop SELF) with a randomized (counterfactual) type. We name it G−. We feed G− and H into RGAT to obtain the counterfactual relation-aware features denoted as Ĥ−. Then, we fuse Ĥ− with the factual POS tags to derive the counterfactual features Fcf1 = [f cf11 , ..., f cf1 S ] on the red branch. Finally, we can calculate the similarity between the factual and the counterfactual features, by leveraging the dot-product operation, as follows,\nLcf1 = 1 S\nS∑\ni\nfi · f cf1i . (5)\nThis counterfactual loss forces the model to emphasize the syntactic information related to dependency relations. Counterfactual POS Tags are utilized with the factual dependency graph on the blue branch in Figure 3. We create counterfactual POS tags P−\nfrom factual POS tags P by randomly selecting a POS tag for each token. Accordingly, we replace each embedding pi by p−i . Given the relationaware features Ĥ from the black branch, we then feed the embeddings of counterfactual POS tags in Eq. 4 and get the counterfactual features as Fcf2 = [f cf21 , ..., f cf2 S ]. Finally, we can calculate the similarity between the factual and the counterfactual features (on the blue branch) by leveraging the dot-product operation, as follows,\nLcf2 = 1 S\nS∑\ni\nfi · f cf2i . (6)\nThis counterfactual loss forces the model to emphasize the syntactic information related to POS tags. The overall loss function used in training is as follows,\nL = Ltask + λ(Lcf1 + Lcf2), (7) where Ltask is the task-specific loss, i.e., a crossentropy loss, and λ is a scale to balance between the task-specific loss and our proposed counterfactual losses."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we evaluate our COSY method for cross-lingual understanding under both zero-shot and few-shot settings. For the zero-shot setting, we use English for training and evaluate the model on different target languages. For the few-shot setting, we follow the implementation in (Nooralahzadeh et al., 2020) and use the development set of the target languages for model fine-tuning3."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We evaluate our method on the natural language inference (NLI) and the question answering (QA) tasks. We briefly introduce the datasets used in our experiments as follows. Natural Language Inference (NLI). Given two sentences, NLI asks for the relationship between the two sentences, which can be entailment, contradiction or neutral. We conduct experiments on XNLI (Conneau et al., 2018) and evaluate our method on 13 target languages4. Question Answering (QA). In this paper, we consider the QA task that asks the model to locate the\n3All the results and analyses are under the zero-shot settings by default, except for Table 2.\n4We remove Thai (th) and Swahili (sw) from our experiments since these two languages are not supported by Stanza.\nanswer from a passage given a question. We conduct experiments on MLQA (Lewis et al., 2019) and XQUAD (Artetxe et al., 2020). COSY is evaluated on 7 languages on MLQA and 10 languages on XQUAD (with Thai excluded)."
    }, {
      "heading" : "4.2 Implementation",
      "text" : "In data preprocessing, we feed the same syntactic information to each of the subwords in the same word after tokenization. Our implementation of pre-trained language models (mBERT and XLM-R) is based on HuggingFaces’s Transformers (Wolf et al., 2020). We select the checkpoint and set hyper-parameters, e.g., learning rate and λ in the loss function, based on the performance on the corresponding development sets. We select learning rate amongst {7.5e−6, 1e−5, 3e−5} and fix the batch size to 32. We select dimension d′ amongst {100, 300}. λ in counterfactual loss is set to 0.1 (see Figure 4). A linear warm up strategy for learning rate is adopted with first 10% optimization steps. Adam (Kingma and Ba, 2014) is adopted as the optimizer. All experiments are conducted on a workstation with dual NVIDIA V100 32GB GPUs."
    }, {
      "heading" : "4.3 Results",
      "text" : "We compare our method with naive fine-tuning and the state-of-the-art methods. The overall results on three benchmarks are presented in Table 1 (zero-\nshot) and Table 2 (few-shot).\nComparison with Naive Fine-tuning. Naive Fine-tuning (Wu and Dredze, 2019; Liang et al., 2020; Hu et al., 2020) is to directly fine-tune the pre-trained language model on downstream tasks as in (Devlin et al., 2019). From Table 1 and Table 2, we can observe that COSY consistently outperforms the naive fine-tuning method on all datasets, e.g., by average 1.9 percentage points (accuracy) and 2.9 percentage points (F1) on XNLI and XQUAD with XLM-Rlarge in the zero-shot setting. These observations demonstrate the effectiveness of COSY and suggest that universal syntax as language-agnostic features can enhance the transferability for cross-lingual understanding. Fur-\nthermore, the results show that COSY is able to work with different backbones and thus is modelagnostic. Comparison with the State of the Art. We first outline the SOTA zero-shot (few-shot) crosslingual methods we compared with as follows: (1) XMAML-one (Nooralahzadeh et al., 2020) borrows the idea from meta-learning. Specifically, XMAML-one utilizes an auxiliary language development data in training, e.g., using the development set of Spanish in training to assist German on MLQA. XMAML-One reports the results based on the most beneficial auxiliary language. (2) STILT (Phang et al., 2020) augments intermediate task training before fine-tuning on the target task, e.g., adding training of HellaSwag (Zellers et al., 2019) before training on the NLI task. STILT also reports results with the most beneficial intermediate task. (3) LAKM (Yuan et al., 2020) first mines knowledge phrases along with passages from the Web. Then these Web data are used to enhance the phrase boundaries through a masked language model objective. Note that LAKM is only evaluated on three languages of MLQA.\nOn the one hand, we observe that COSY surpasses the compared SOTA methods over all evaluation metrics. Although meta-learning methods (Finn et al., 2017; Gu et al., 2018; Sun et al., 2019) advance the state-of-the-art performance for few-shot learning, our COSY still outperforms the meta-learning-based method, i.e., XMAML-One, with 1.1 percentage points in the few-shot setting. On the other hand, the superiority of COSY is also reflected in other aspects, which are shown in Table 1. Specifically, COSY does not require additional datasets and cumbersome data selection process, which is more convenient and resources saving."
    }, {
      "heading" : "4.4 Discussion and Analysis",
      "text" : "Ablation Study. In Table 3, we show the MLQA, XQUAD and XNLI results in 4 ablative settings, to evaluate the approach when we (1) only utilize the SAN-Black branch; (2) utilize the SAN-Black branch with an intuitive gate mechanism to control the information of pre-trained language model and syntax; (3) utilize the SAN-Black branch and SANRed branch; (4) utilize the SAN-Black branch and SAN-Blue branch.\nCompared to the ablative results, we can see that our full method achieves the overall top per-\nformance in all settings. Syntax features are incorporated into the models in (1)-(5) and all of them outperform the naive fine-tuning method, which demonstrates the effectiveness of universal syntax. By analyzing the settings one by one, we can observe that SAN-Black only attains limited improvement compared to naive fine-tuning since syntax is incorporated in the model by overlooked. Gate mechanism (2) fails to solve the overlooking issue. Both of (3) and (4) with counterfactual training are able to bring gains compared to (1), and the results indicate that dependency relations are more effective compared to POS labels. We also observe that our full method (5) does not accumulate the gains from (3) and (4). One explanation could be that part of the information provided by the dependency relations and POS labels overlaps. For instance, if we see an edge of relation, worda AMOD−−−→wordb, we may infer that worda is NOUN and wordb is ADJ. Effect of λ. We now study the impact of the scale value λ with counterfactual losses. For clarity, we show the results with different values of logλ in Figure 4. We can observe that COSY attains the\nhighest results when λ=0.1 on both MLQA and XNLI. As the value drops, the effect of counterfactual loss is also smaller and the performance is getting closer to that from naive fine-tuning (red dotted line). If a large value of λ is applied, e.g., λ=1, the model begins to over-emphasize the syntax and semantics are overlooked, which leads to significant decrease on performance. Effect of COSY. In this part, we first study whether counterfactual training method indeed guides the model to focus more on syntactic information. We conduct analysis on the COSY and SAN-Black. Since it is non-trivial to measure the utilization of syntax in a straightforward way, we adopt a standard way to measure the importance of the neurons in deep models (Kádár et al., 2017). Specifically, we perturb the syntactic features with a Gaussian noise to test data and check whether our model would be more easily affected by the syntax perturbation. If so, then it verifies that our model indeed relies more on syntax.. The results are shown in Figure 5. We can discover that the performance drop of COSY is larger compared to that with SANBlack.\nMeanwhile, we also explore whether COSY is beneficial for yielding more meaningful syntax embedding than SAN-Black. Specifically, we compute the correlation score (absolute cosine similarity) between the embedding of syntactic relation and the corresponding inverse relation from the\nsame type. For COSY, we observe that the score of the related types are 42.4× larger than that of two randomly selected embeddings (average over 10000 times). However, for SAN-Black, its score is only 1.4× larger than that of two randomly selected embeddings. It demonstrates that COSY attains more meaningful syntax representations than SAN-Black.\nCounterfactual Syntax Generation. Here we analyze other alternative ways of counterfactual syntax generation. Specifically, we design the following variants and report the results in Table 4: (1) we not only replace edge types, but also replace connections for counterfactual dependency graph construction; (2) for each input sequence, we create 5 counterfactual dependency graphs, 5 sets of counterfactual POS tags, and the counterfactual loss is the average over the 5 sets; (3) we replace the factual syntax with a fixed type, e.g., a type of padding instead of a random type from all types; (4) in each generating process, we only replace 50% of the factual syntax.\nComparing (1) with the result of “SANBlack,Blue” in Table 3, we can see that (1) does not work. We believe that randomly changing connections in G−, e.g., an edge is created from the first token to the last token in a long passage, may have a significant effect to Ĥ−, it is undesirable for further optimization of counterfactual loss. Results from (2) and (4) suggest that the number of the generated counterfactual syntax and ratio of randomizing do not play an important role in COSY. It is also discovered that randomizing with all types is better than simple replacement with a fixed type."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We study how to effectively plug in syntactic information for cross-lingual understanding. Specifically, we propose a novel counterfactual-syntaxbased approach to emphasize the importance of syntax in cross-lingual models. We conduct extensive experiments on three cross-lingual benchmarks, and show that our approach can outperform the SOTA methods without additional dataset. For future work, we will combine our approach with other orthogonal methods, e.g., meta-learning, to further improve its effectiveness."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research is supported by the National Research Foundation, Singapore under its Strategic Capabilities Research Centres Funding Initiative, and partially supported by the Agency for Science, Technology and Research (A*STAR) under its AME YIRG Grant (Project No. A20E6c0101), and its AME Programmatic Fund (Project No: A18A1b0045 and No: A18A2b0046). Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore."
    } ],
    "references" : [ {
      "title" : "Counterfactual vision and language learning",
      "author" : [ "Ehsan Abbasnejad", "Damien Teney", "Amin Parvaneh", "Javen Shi", "Anton van den Hengel." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.",
      "citeRegEx" : "Abbasnejad et al\\.,? 2020",
      "shortCiteRegEx" : "Abbasnejad et al\\.",
      "year" : 2020
    }, {
      "title" : "On the cross-lingual transferability of monolingual representations",
      "author" : [ "Mikel Artetxe", "Sebastian Ruder", "Dani Yogatama." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Artetxe et al\\.,? 2020",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2020
    }, {
      "title" : "Counterfactual samples synthesizing for robust visual question answering",
      "author" : [ "Long Chen", "Xin Yan", "Jun Xiao", "Hanwang Zhang", "Shiliang Pu", "Yueting Zhuang." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "What does it mean to be language-agnostic? probing multilingual sentence encoders for typological properties",
      "author" : [ "Rochelle Choenni", "Ekaterina Shutova." ],
      "venue" : "arXiv preprint arXiv:2009.12862.",
      "citeRegEx" : "Choenni and Shutova.,? 2020",
      "shortCiteRegEx" : "Choenni and Shutova.",
      "year" : 2020
    }, {
      "title" : "Less is more: Attention supervision with counterfactuals for text classification",
      "author" : [ "Seungtaek Choi", "Haeju Park", "Jinyoung Yeo", "Seungwon Hwang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Choi et al\\.,? 2020",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2020
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Édouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020a",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Xnli: Evaluating crosslingual sentence representations",
      "author" : [ "Alexis Conneau", "Ruty Rinott", "Guillaume Lample", "Adina Williams", "Samuel Bowman", "Holger Schwenk", "Veselin Stoyanov." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Conneau et al\\.,? 2018",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "Emerging cross-lingual structure in pretrained language models",
      "author" : [ "Alexis Conneau", "Shijie Wu", "Haoran Li", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Conneau et al\\.,? 2020b",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Identifying necessary elements for bert’s multilinguality",
      "author" : [ "Philipp Dufter", "Hinrich Schütze." ],
      "venue" : "arXiv preprint arXiv:2005.00396.",
      "citeRegEx" : "Dufter and Schütze.,? 2020",
      "shortCiteRegEx" : "Dufter and Schütze.",
      "year" : 2020
    }, {
      "title" : "Model-agnostic meta-learning for fast adaptation of deep networks",
      "author" : [ "Chelsea Finn", "Pieter Abbeel", "Sergey Levine." ],
      "venue" : "International Conference on Machine Learning.",
      "citeRegEx" : "Finn et al\\.,? 2017",
      "shortCiteRegEx" : "Finn et al\\.",
      "year" : 2017
    }, {
      "title" : "Explaining classifiers with causal concept effect (cace)",
      "author" : [ "Yash Goyal", "Amir Feder", "Uri Shalit", "Been Kim." ],
      "venue" : "arXiv preprint arXiv:1907.07165.",
      "citeRegEx" : "Goyal et al\\.,? 2019a",
      "shortCiteRegEx" : "Goyal et al\\.",
      "year" : 2019
    }, {
      "title" : "Counterfactual visual explanations",
      "author" : [ "Yash Goyal", "Ziyan Wu", "Jan Ernst", "Dhruv Batra", "Devi Parikh", "Stefan Lee." ],
      "venue" : "International Conference on Machine Learning.",
      "citeRegEx" : "Goyal et al\\.,? 2019b",
      "shortCiteRegEx" : "Goyal et al\\.",
      "year" : 2019
    }, {
      "title" : "Meta-learning for lowresource neural machine translation",
      "author" : [ "Jiatao Gu", "Yong Wang", "Yun Chen", "Victor OK Li", "Kyunghyun Cho." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Gu et al\\.,? 2018",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2018
    }, {
      "title" : "Dimensionality reduction by learning an invariant mapping",
      "author" : [ "Raia Hadsell", "Sumit Chopra", "Yann LeCun." ],
      "venue" : "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE.",
      "citeRegEx" : "Hadsell et al\\.,? 2006",
      "shortCiteRegEx" : "Hadsell et al\\.",
      "year" : 2006
    }, {
      "title" : "Momentum contrast for unsupervised visual representation learning",
      "author" : [ "Kaiming He", "Haoqi Fan", "Yuxin Wu", "Saining Xie", "Ross Girshick." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "Zero-shot reading comprehension by crosslingual transfer learning with multi-lingual language representation model",
      "author" : [ "Tsung-Yuan Hsu", "Chi-Liang Liu", "Hung-yi Lee." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Hsu et al\\.,? 2019",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2019
    }, {
      "title" : "Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation",
      "author" : [ "Junjie Hu", "Sebastian Ruder", "Aditya Siddhant", "Graham Neubig", "Orhan Firat", "Melvin Johnson." ],
      "venue" : "International Conference on Machine Learning.",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "Reducing sentiment bias in language models via counterfactual evaluation",
      "author" : [ "Po-Sen Huang", "Huan Zhang", "Ray Jiang", "Robert Stanforth", "Johannes Welbl", "Jack Rae", "Vishal Maini", "Dani Yogatama", "Pushmeet Kohli." ],
      "venue" : "Proceedings of the 2020 Con-",
      "citeRegEx" : "Huang et al\\.,? 2020",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "author" : [ "Taichi Ishiwatari", "Yuki Yasuda", "Taro Miyazaki", "Jun Goto." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Ishiwatari et al\\.,? 2020",
      "shortCiteRegEx" : "Ishiwatari et al\\.",
      "year" : 2020
    }, {
      "title" : "Representation of linguistic form and function in recurrent neural networks",
      "author" : [ "Ákos Kádár", "Grzegorz Chrupała", "Afra Alishahi." ],
      "venue" : "Computational Linguistics.",
      "citeRegEx" : "Kádár et al\\.,? 2017",
      "shortCiteRegEx" : "Kádár et al\\.",
      "year" : 2017
    }, {
      "title" : "Adversarial learning with contextual embeddings for zeroresource cross-lingual classification and ner",
      "author" : [ "Phillip Keung", "Vikas Bhardwaj" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th In-",
      "citeRegEx" : "Keung and Bhardwaj,? \\Q2019\\E",
      "shortCiteRegEx" : "Keung and Bhardwaj",
      "year" : 2019
    }, {
      "title" : "On the evaluation of contextual embeddings for zero-shot cross-lingual transfer learning",
      "author" : [ "Phillip Keung", "Yichao Lu", "Julian Salazar", "Vikas Bhardwaj." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Keung et al\\.,? 2020",
      "shortCiteRegEx" : "Keung et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Guillaume Lample", "Alexis Conneau." ],
      "venue" : "arXiv preprint arXiv:1901.07291.",
      "citeRegEx" : "Lample and Conneau.,? 2019",
      "shortCiteRegEx" : "Lample and Conneau.",
      "year" : 2019
    }, {
      "title" : "From zero to hero: On the limitations of zero-shot cross-lingual transfer with multilingual transformers",
      "author" : [ "Anne Lauscher", "Vinit Ravishankar", "Ivan Vulić", "Goran Glavaš." ],
      "venue" : "arXiv preprint arXiv:2005.00633.",
      "citeRegEx" : "Lauscher et al\\.,? 2020",
      "shortCiteRegEx" : "Lauscher et al\\.",
      "year" : 2020
    }, {
      "title" : "Mlqa: Evaluating cross-lingual extractive question answering",
      "author" : [ "Patrick Lewis", "Barlas Oğuz", "Ruty Rinott", "Sebastian Riedel", "Holger Schwenk." ],
      "venue" : "arXiv preprint arXiv:1910.07475.",
      "citeRegEx" : "Lewis et al\\.,? 2019",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2019
    }, {
      "title" : "Xglue: A new benchmark datasetfor cross-lingual pre-training, understanding and generation",
      "author" : [ "Yaobo Liang", "Nan Duan", "Yeyun Gong", "Ning Wu", "Fenfei Guo", "Weizhen Qi", "Ming Gong", "Linjun Shou", "Daxin Jiang", "Guihong Cao" ],
      "venue" : null,
      "citeRegEx" : "Liang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2020
    }, {
      "title" : "Heterogeneous graph attention networks for semi-supervised short text classification",
      "author" : [ "Hu Linmei", "Tianchi Yang", "Chuan Shi", "Houye Ji", "Xiaoli Li." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and",
      "citeRegEx" : "Linmei et al\\.,? 2019",
      "shortCiteRegEx" : "Linmei et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring fine-tuning techniques for pre-trained cross-lingual models via continual learning",
      "author" : [ "Zihan Liu", "Genta Indra Winata", "Andrea Madotto", "Pascale Fung." ],
      "venue" : "arXiv preprint arXiv:2004.14218.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Universal stanford dependencies: A cross-linguistic typology",
      "author" : [ "Marie-Catherine de Marneffe", "Timothy Dozat", "Natalia Silveira", "Katri Haverinen", "Filip Ginter", "Joakim Nivre", "Christopher D Manning." ],
      "venue" : "Proceedings of the Ninth International Con-",
      "citeRegEx" : "Marneffe et al\\.,? 2014",
      "shortCiteRegEx" : "Marneffe et al\\.",
      "year" : 2014
    }, {
      "title" : "Counterfactual vqa: A cause-effect look at language bias",
      "author" : [ "Yulei Niu", "Kaihua Tang", "Hanwang Zhang", "Zhiwu Lu", "Xian-Sheng Hua", "Ji-Rong Wen." ],
      "venue" : "arXiv preprint arXiv:2006.04315.",
      "citeRegEx" : "Niu et al\\.,? 2020",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2020
    }, {
      "title" : "Universal dependencies v1: A multilingual treebank collection",
      "author" : [ "Joakim Nivre", "Marie-Catherine De Marneffe", "Filip Ginter", "Yoav Goldberg", "Jan Hajic", "Christopher D Manning", "Ryan McDonald", "Slav Petrov", "Sampo Pyysalo", "Natalia Silveira" ],
      "venue" : null,
      "citeRegEx" : "Nivre et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nivre et al\\.",
      "year" : 2016
    }, {
      "title" : "Zero-shot cross-lingual transfer with meta learning",
      "author" : [ "Farhad Nooralahzadeh", "Giannis Bekoulis", "Johannes Bjerva", "Isabelle Augenstein." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Nooralahzadeh et al\\.,? 2020",
      "shortCiteRegEx" : "Nooralahzadeh et al\\.",
      "year" : 2020
    }, {
      "title" : "Causal inference",
      "author" : [ "Judea Pearl." ],
      "venue" : "Causality: Objectives and Assessment.",
      "citeRegEx" : "Pearl.,? 2010",
      "shortCiteRegEx" : "Pearl.",
      "year" : 2010
    }, {
      "title" : "The book of why: The new science of cause and effect",
      "author" : [ "Judea Pearl", "Dana Mackenzie" ],
      "venue" : null,
      "citeRegEx" : "Pearl and Mackenzie.,? \\Q2018\\E",
      "shortCiteRegEx" : "Pearl and Mackenzie.",
      "year" : 2018
    }, {
      "title" : "Causal inference in statistics: An overview",
      "author" : [ "Judea Pearl" ],
      "venue" : "Statistics surveys.",
      "citeRegEx" : "Pearl,? 2009",
      "shortCiteRegEx" : "Pearl",
      "year" : 2009
    }, {
      "title" : "English intermediate-task training improves zeroshot cross-lingual transfer too",
      "author" : [ "Calixto", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:2005.13013.",
      "citeRegEx" : "Calixto and Bowman.,? 2020",
      "shortCiteRegEx" : "Calixto and Bowman.",
      "year" : 2020
    }, {
      "title" : "How multilingual is multilingual bert",
      "author" : [ "Telmo Pires", "Eva Schlinger", "Dan Garrette" ],
      "venue" : "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Pires et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Pires et al\\.",
      "year" : 2019
    }, {
      "title" : "Stanza: A python natural language processing toolkit for many human languages",
      "author" : [ "Peng Qi", "Yuhao Zhang", "Yuhui Zhang", "Jason Bolton", "Christopher D Manning." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Qi et al\\.,? 2020",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2020
    }, {
      "title" : "Counterfactual story reasoning and generation",
      "author" : [ "Lianhui Qin", "Antoine Bosselut", "Ari Holtzman", "Chandra Bhagavatula", "Elizabeth Clark", "Yejin Choi." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Qin et al\\.,? 2019",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2019
    }, {
      "title" : "Counterfactual thinking",
      "author" : [ "Neal J Roese." ],
      "venue" : "Psychological bulletin.",
      "citeRegEx" : "Roese.,? 1997",
      "shortCiteRegEx" : "Roese.",
      "year" : 1997
    }, {
      "title" : "Causation and causal inference in epidemiology",
      "author" : [ "Kenneth J Rothman", "Sander Greenland." ],
      "venue" : "American journal of public health.",
      "citeRegEx" : "Rothman and Greenland.,? 2005",
      "shortCiteRegEx" : "Rothman and Greenland.",
      "year" : 2005
    }, {
      "title" : "Neutralizing gender bias in word embedding with latent disentanglement and counterfactual generation",
      "author" : [ "Seungjae Shin", "Kyungwoo Song", "JoonHo Jang", "Hyemi Kim", "Weonyoung Joo", "Il-Chul Moon." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical",
      "citeRegEx" : "Shin et al\\.,? 2020",
      "shortCiteRegEx" : "Shin et al\\.",
      "year" : 2020
    }, {
      "title" : "Social mechanisms and causal inference",
      "author" : [ "Daniel Steel." ],
      "venue" : "Philosophy of the social sciences.",
      "citeRegEx" : "Steel.,? 2004",
      "shortCiteRegEx" : "Steel.",
      "year" : 2004
    }, {
      "title" : "Meta-transfer learning for few-shot learning",
      "author" : [ "Qianru Sun", "Yaoyao Liu", "Tat-Seng Chua", "Bernt Schiele." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Long-tailed classification by keeping the good and removing the bad momentum causal effect",
      "author" : [ "Kaihua Tang", "Jianqiang Huang", "Hanwang Zhang." ],
      "venue" : "arXiv preprint arXiv:2009.12991.",
      "citeRegEx" : "Tang et al\\.,? 2020a",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2020
    }, {
      "title" : "Unbiased scene graph generation from biased training",
      "author" : [ "Kaihua Tang", "Yulei Niu", "Jianqiang Huang", "Jiaxin Shi", "Hanwang Zhang." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.",
      "citeRegEx" : "Tang et al\\.,? 2020b",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Investigating gender bias in language models using causal mediation analysis",
      "author" : [ "Jesse Vig", "Sebastian Gehrmann", "Yonatan Belinkov", "Sharon Qian", "Daniel Nevo", "Yaron Singer", "Stuart Shieber." ],
      "venue" : "Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Vig et al\\.,? 2020",
      "shortCiteRegEx" : "Vig et al\\.",
      "year" : 2020
    }, {
      "title" : "On learning universal representations across languages",
      "author" : [ "Xiangpeng Wei", "Yue Hu", "Rongxiang Weng", "Luxi Xing", "Heng Yu", "Weihua Luo." ],
      "venue" : "arXiv preprint arXiv:2007.15960.",
      "citeRegEx" : "Wei et al\\.,? 2020",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Thomas Wolf", "Julien Chaumond", "Lysandre Debut", "Victor Sanh", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Morgan Funtowicz", "Joe Davison", "Sam Shleifer" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2020
    }, {
      "title" : "Beto, bentz, becas: The surprising cross-lingual effectiveness of bert",
      "author" : [ "Shijie Wu", "Mark Dredze." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Wu and Dredze.,? 2019",
      "shortCiteRegEx" : "Wu and Dredze.",
      "year" : 2019
    }, {
      "title" : "Multi-source meta transfer for low resource multiple-choice question answering",
      "author" : [ "Ming Yan", "Hao Zhang", "Di Jin", "Joey Tianyi Zhou." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Yan et al\\.,? 2020",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2020
    }, {
      "title" : "Counterfactual variable control for robust and interpretable question answering",
      "author" : [ "Sicheng Yu", "Yulei Niu", "Shuohang Wang", "Jing Jiang", "Qianru Sun." ],
      "venue" : "arXiv preprint arXiv:2010.05581.",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Enhancing answer boundary detection for multilingual machine reading comprehension",
      "author" : [ "Fei Yuan", "Linjun Shou", "Xuanyu Bai", "Ming Gong", "Yaobo Liang", "Nan Duan", "Yan Fu", "Daxin Jiang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the As-",
      "citeRegEx" : "Yuan et al\\.,? 2020",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2020
    }, {
      "title" : "Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830",
      "author" : [ "Rowan Zellers", "Ari Holtzman", "Yonatan Bisk", "Ali Farhadi", "Yejin Choi" ],
      "venue" : null,
      "citeRegEx" : "Zellers et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2019
    }, {
      "title" : "Inducing languageagnostic multilingual representations",
      "author" : [ "Wei Zhao", "Steffen Eger", "Johannes Bjerva", "Isabelle Augenstein." ],
      "venue" : "arXiv preprint arXiv:2008.09112.",
      "citeRegEx" : "Zhao et al\\.,? 2020",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    }, {
      "title" : "Dual adversarial transfer for sequence labeling",
      "author" : [ "Joey Tianyi Zhou", "Hao Zhang", "Di Jin", "Xi Peng." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence.",
      "citeRegEx" : "Zhou et al\\.,? 2021",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2021
    }, {
      "title" : "Dual adversarial neural transfer for low-resource named entity recognition",
      "author" : [ "Joey Tianyi Zhou", "Hao Zhang", "Di Jin", "Hongyuan Zhu", "Meng Fang", "Rick Siow Mong Goh", "Kenneth Kwok." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association",
      "citeRegEx" : "Zhou et al\\.,? 2019",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2019
    }, {
      "title" : "Counterfactual offpolicy training for neural dialogue generation",
      "author" : [ "Qingfu Zhu", "Weinan Zhang", "Ting Liu", "William Yang Wang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "With the emergence of BERT (Devlin et al., 2019), large-scale pre-trained language models have become an indispensable component in the solutions to many natural language processing (NLP) tasks.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 8,
      "context" : "Recently, large-scale multilingual transformer-based models, such as mBERT (Devlin et al., 2019), XLM (Lample and Conneau, 2019) and XLM-R (Conneau et al.",
      "startOffset" : 75,
      "endOffset" : 96
    }, {
      "referenceID" : 24,
      "context" : ", 2019), XLM (Lample and Conneau, 2019) and XLM-R (Conneau et al.",
      "startOffset" : 13,
      "endOffset" : 39
    }, {
      "referenceID" : 5,
      "context" : ", 2019), XLM (Lample and Conneau, 2019) and XLM-R (Conneau et al., 2020a), have been widely deployed as backbones in cross-lingual NLP tasks (Wu and Dredze, 2019; Pires et al.",
      "startOffset" : 50,
      "endOffset" : 73
    }, {
      "referenceID" : 53,
      "context" : ", 2020a), have been widely deployed as backbones in cross-lingual NLP tasks (Wu and Dredze, 2019; Pires et al., 2019; Keung et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 137
    }, {
      "referenceID" : 39,
      "context" : ", 2020a), have been widely deployed as backbones in cross-lingual NLP tasks (Wu and Dredze, 2019; Pires et al., 2019; Keung et al., 2019).",
      "startOffset" : 76,
      "endOffset" : 137
    }, {
      "referenceID" : 17,
      "context" : "For example, on the XQUAD dataset, mBERT achieves a 24 percentage points lower exact match score on the target language Chinese than on the training language English (Hu et al., 2020).",
      "startOffset" : 166,
      "endOffset" : 183
    }, {
      "referenceID" : 3,
      "context" : "An intuitive way to tackle this is to introduce language-agnostic information—the most transferable feature across languages, which is lacking in existing multilingual language models (Choenni and Shutova, 2020).",
      "startOffset" : 184,
      "endOffset" : 211
    }, {
      "referenceID" : 33,
      "context" : "In our work, we propose to exploit reliable language-agnostic information— syntax in the form of universal dependency relations and universal POS tags (de Marneffe et al., 2014; Nivre et al., 2016; Zhou et al., 2019, 2021).",
      "startOffset" : 151,
      "endOffset" : 222
    }, {
      "referenceID" : 19,
      "context" : ", relational graph attention networks (Ishiwatari et al., 2020), has little effect on improving the model.",
      "startOffset" : 38,
      "endOffset" : 63
    }, {
      "referenceID" : 42,
      "context" : "tics? To this end, we propose a novel COunterfactual SYntax (COSY) method, inspired by causal inference (Roese, 1997; Pearl et al., 2009) and contrastive learning (He et al.",
      "startOffset" : 104,
      "endOffset" : 137
    }, {
      "referenceID" : 8,
      "context" : "language models (Devlin et al., 2019; Liu et al., 2019) have achieved sequential success in various natural language processing tasks.",
      "startOffset" : 16,
      "endOffset" : 55
    }, {
      "referenceID" : 29,
      "context" : "language models (Devlin et al., 2019; Liu et al., 2019) have achieved sequential success in various natural language processing tasks.",
      "startOffset" : 16,
      "endOffset" : 55
    }, {
      "referenceID" : 24,
      "context" : "Recent studies (Lample and Conneau, 2019; Conneau et al., 2020a) extend the pre-trained language models to multilingual tasks and demonstrate their prominent capability on cross-lingual knowledge transfer, even under zero-shot scenario (Wu and Dredze, 2019; Pires et al.",
      "startOffset" : 15,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "Recent studies (Lample and Conneau, 2019; Conneau et al., 2020a) extend the pre-trained language models to multilingual tasks and demonstrate their prominent capability on cross-lingual knowledge transfer, even under zero-shot scenario (Wu and Dredze, 2019; Pires et al.",
      "startOffset" : 15,
      "endOffset" : 64
    }, {
      "referenceID" : 53,
      "context" : ", 2020a) extend the pre-trained language models to multilingual tasks and demonstrate their prominent capability on cross-lingual knowledge transfer, even under zero-shot scenario (Wu and Dredze, 2019; Pires et al., 2019; Hsu et al., 2019).",
      "startOffset" : 180,
      "endOffset" : 239
    }, {
      "referenceID" : 39,
      "context" : ", 2020a) extend the pre-trained language models to multilingual tasks and demonstrate their prominent capability on cross-lingual knowledge transfer, even under zero-shot scenario (Wu and Dredze, 2019; Pires et al., 2019; Hsu et al., 2019).",
      "startOffset" : 180,
      "endOffset" : 239
    }, {
      "referenceID" : 16,
      "context" : ", 2020a) extend the pre-trained language models to multilingual tasks and demonstrate their prominent capability on cross-lingual knowledge transfer, even under zero-shot scenario (Wu and Dredze, 2019; Pires et al., 2019; Hsu et al., 2019).",
      "startOffset" : 180,
      "endOffset" : 239
    }, {
      "referenceID" : 7,
      "context" : "2019; Dufter and Schütze, 2020) and the parameters of top layers (Conneau et al., 2020b) are crucial for cross-lingual transfer.",
      "startOffset" : 65,
      "endOffset" : 88
    }, {
      "referenceID" : 3,
      "context" : "2020), where knowledge learned in the pre-training stage is forgotten in downstream fine-tuning; (ii) lack of language-agnostic features (Choenni and Shutova, 2020; Zhao et al., 2020) or linguistic discrepancy between the source and the target languages (Wu and Dredze, 2019; Lauscher et al.",
      "startOffset" : 137,
      "endOffset" : 183
    }, {
      "referenceID" : 58,
      "context" : "2020), where knowledge learned in the pre-training stage is forgotten in downstream fine-tuning; (ii) lack of language-agnostic features (Choenni and Shutova, 2020; Zhao et al., 2020) or linguistic discrepancy between the source and the target languages (Wu and Dredze, 2019; Lauscher et al.",
      "startOffset" : 137,
      "endOffset" : 183
    }, {
      "referenceID" : 53,
      "context" : ", 2020) or linguistic discrepancy between the source and the target languages (Wu and Dredze, 2019; Lauscher et al., 2020).",
      "startOffset" : 78,
      "endOffset" : 122
    }, {
      "referenceID" : 25,
      "context" : ", 2020) or linguistic discrepancy between the source and the target languages (Wu and Dredze, 2019; Lauscher et al., 2020).",
      "startOffset" : 78,
      "endOffset" : 122
    }, {
      "referenceID" : 58,
      "context" : "579 guage model by aligning languages with parallel data (Zhao et al., 2020) or strengthening sentencelevel representation (Wei et al.",
      "startOffset" : 57,
      "endOffset" : 76
    }, {
      "referenceID" : 51,
      "context" : ", 2020) or strengthening sentencelevel representation (Wei et al., 2020).",
      "startOffset" : 54,
      "endOffset" : 72
    }, {
      "referenceID" : 34,
      "context" : "For instance, some methods adopt meta-learning (Nooralahzadeh et al., 2020; Yan et al., 2020) or intermediate tasks training (Phang et al.",
      "startOffset" : 47,
      "endOffset" : 93
    }, {
      "referenceID" : 54,
      "context" : "For instance, some methods adopt meta-learning (Nooralahzadeh et al., 2020; Yan et al., 2020) or intermediate tasks training (Phang et al.",
      "startOffset" : 47,
      "endOffset" : 93
    }, {
      "referenceID" : 43,
      "context" : "Counterfactual analysis has been widely studied in epidemiology (Rothman and Greenland, 2005) and social science (Steel, 2004).",
      "startOffset" : 64,
      "endOffset" : 93
    }, {
      "referenceID" : 45,
      "context" : "Counterfactual analysis has been widely studied in epidemiology (Rothman and Greenland, 2005) and social science (Steel, 2004).",
      "startOffset" : 113,
      "endOffset" : 126
    }, {
      "referenceID" : 47,
      "context" : ", 2019a,b), long-tailed classification (Tang et al., 2020a), scene graph generation (Tang et al.",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 48,
      "context" : ", 2020a), scene graph generation (Tang et al., 2020b), and visual question answering (Chen et al.",
      "startOffset" : 33,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : ", 2020b), and visual question answering (Chen et al., 2020; Niu et al., 2020; Abbasnejad et al., 2020).",
      "startOffset" : 40,
      "endOffset" : 102
    }, {
      "referenceID" : 32,
      "context" : ", 2020b), and visual question answering (Chen et al., 2020; Niu et al., 2020; Abbasnejad et al., 2020).",
      "startOffset" : 40,
      "endOffset" : 102
    }, {
      "referenceID" : 0,
      "context" : ", 2020b), and visual question answering (Chen et al., 2020; Niu et al., 2020; Abbasnejad et al., 2020).",
      "startOffset" : 40,
      "endOffset" : 102
    }, {
      "referenceID" : 4,
      "context" : "In the community of natural language processing, counterfactual methods are also emerging recently in text classification (Choi et al., 2020), story generation (Qin et al.",
      "startOffset" : 122,
      "endOffset" : 141
    }, {
      "referenceID" : 41,
      "context" : ", 2020), story generation (Qin et al., 2019), dialog systems (Zhu et al.",
      "startOffset" : 26,
      "endOffset" : 44
    }, {
      "referenceID" : 61,
      "context" : ", 2019), dialog systems (Zhu et al., 2020), gender bias (Vig et al.",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 50,
      "context" : ", 2020), gender bias (Vig et al., 2020; Shin et al., 2020), question answering (Yu et al.",
      "startOffset" : 21,
      "endOffset" : 58
    }, {
      "referenceID" : 44,
      "context" : ", 2020), gender bias (Vig et al., 2020; Shin et al., 2020), question answering (Yu et al.",
      "startOffset" : 21,
      "endOffset" : 58
    }, {
      "referenceID" : 55,
      "context" : ", 2020), question answering (Yu et al., 2020), and sentiment bias (Huang et al.",
      "startOffset" : 28,
      "endOffset" : 45
    }, {
      "referenceID" : 61,
      "context" : "Different from previous works (Zhu et al., 2020; Qin et al., 2019) that generate word-level or sentence-level counterfactual samples, our counterfactual analysis dives into syntax level that is more controllable than text and free from complex language generation module.",
      "startOffset" : 30,
      "endOffset" : 66
    }, {
      "referenceID" : 41,
      "context" : "Different from previous works (Zhu et al., 2020; Qin et al., 2019) that generate word-level or sentence-level counterfactual samples, our counterfactual analysis dives into syntax level that is more controllable than text and free from complex language generation module.",
      "startOffset" : 30,
      "endOffset" : 66
    }, {
      "referenceID" : 17,
      "context" : "Following previous work (Hu et al., 2020), we deploy a pre-trained multi-lingual language model, e.",
      "startOffset" : 24,
      "endOffset" : 41
    }, {
      "referenceID" : 8,
      "context" : ", mBERT (Devlin et al., 2019), to encode each input sentence into contextual features.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 40,
      "context" : "In this work, the Stanza toolkit (Qi et al., 2020) is used to extract the universal dependency relations as the first step.",
      "startOffset" : 33,
      "endOffset" : 50
    }, {
      "referenceID" : 19,
      "context" : "RGAT stands for Relational Graph Attention Network (Ishiwatari et al., 2020; Linmei et al., 2019).",
      "startOffset" : 51,
      "endOffset" : 97
    }, {
      "referenceID" : 28,
      "context" : "RGAT stands for Relational Graph Attention Network (Ishiwatari et al., 2020; Linmei et al., 2019).",
      "startOffset" : 51,
      "endOffset" : 97
    }, {
      "referenceID" : 40,
      "context" : "We deploy the same Stanza toolkit (Qi et al., 2020) to assign (factual) POS tags P for all tokens.",
      "startOffset" : 34,
      "endOffset" : 51
    }, {
      "referenceID" : 19,
      "context" : "RGAT is one of the standard backbones to incorporate the dependency graph (Ishiwatari et al., 2020; Linmei et al., 2019).",
      "startOffset" : 74,
      "endOffset" : 120
    }, {
      "referenceID" : 28,
      "context" : "RGAT is one of the standard backbones to incorporate the dependency graph (Ishiwatari et al., 2020; Linmei et al., 2019).",
      "startOffset" : 74,
      "endOffset" : 120
    }, {
      "referenceID" : 49,
      "context" : "eij = (hi ·WQ) ◦ (hj ·WK), (2) where WK ∈ Rd×d/2 and WQ ∈ Rd×d/2 are the learnable parameters for key and query projections (Vaswani et al., 2017), and hi and hj denote their contextual features extracted from pre-trained language models.",
      "startOffset" : 124,
      "endOffset" : 146
    }, {
      "referenceID" : 35,
      "context" : "Inspired by counterfactual analysis (Pearl et al., 2009; Pearl, 2010; Pearl and Mackenzie, 2018) and contrastive learning (Hadsell et al.",
      "startOffset" : 36,
      "endOffset" : 96
    }, {
      "referenceID" : 36,
      "context" : "Inspired by counterfactual analysis (Pearl et al., 2009; Pearl, 2010; Pearl and Mackenzie, 2018) and contrastive learning (Hadsell et al.",
      "startOffset" : 36,
      "endOffset" : 96
    }, {
      "referenceID" : 14,
      "context" : ", 2009; Pearl, 2010; Pearl and Mackenzie, 2018) and contrastive learning (Hadsell et al., 2006), we propose a counterfactual training method by incorporating counterfactual syntax (counterfactual dependency graph and counterfactual POS tags) on the red and blue branches in Figure 3.",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 34,
      "context" : "For the few-shot setting, we follow the implementation in (Nooralahzadeh et al., 2020) and use the development set of the target languages for model fine-tuning3.",
      "startOffset" : 58,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : "We conduct experiments on XNLI (Conneau et al., 2018) and evaluate our method on 13 target languages4.",
      "startOffset" : 31,
      "endOffset" : 53
    }, {
      "referenceID" : 26,
      "context" : "We conduct experiments on MLQA (Lewis et al., 2019) and XQUAD (Artetxe et al.",
      "startOffset" : 31,
      "endOffset" : 51
    }, {
      "referenceID" : 52,
      "context" : "tion of pre-trained language models (mBERT and XLM-R) is based on HuggingFaces’s Transformers (Wolf et al., 2020).",
      "startOffset" : 94,
      "endOffset" : 113
    }, {
      "referenceID" : 53,
      "context" : "Naive Fine-tuning (Wu and Dredze, 2019; Liang et al., 2020; Hu et al., 2020) is to directly fine-tune the pre-trained language model on downstream tasks as in (Devlin et al.",
      "startOffset" : 18,
      "endOffset" : 76
    }, {
      "referenceID" : 27,
      "context" : "Naive Fine-tuning (Wu and Dredze, 2019; Liang et al., 2020; Hu et al., 2020) is to directly fine-tune the pre-trained language model on downstream tasks as in (Devlin et al.",
      "startOffset" : 18,
      "endOffset" : 76
    }, {
      "referenceID" : 17,
      "context" : "Naive Fine-tuning (Wu and Dredze, 2019; Liang et al., 2020; Hu et al., 2020) is to directly fine-tune the pre-trained language model on downstream tasks as in (Devlin et al.",
      "startOffset" : 18,
      "endOffset" : 76
    }, {
      "referenceID" : 8,
      "context" : ", 2020) is to directly fine-tune the pre-trained language model on downstream tasks as in (Devlin et al., 2019).",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 34,
      "context" : "lingual methods we compared with as follows: (1) XMAML-one (Nooralahzadeh et al., 2020) borrows the idea from meta-learning.",
      "startOffset" : 59,
      "endOffset" : 87
    }, {
      "referenceID" : 57,
      "context" : ", adding training of HellaSwag (Zellers et al., 2019) before training on the NLI task.",
      "startOffset" : 31,
      "endOffset" : 53
    }, {
      "referenceID" : 56,
      "context" : "(3) LAKM (Yuan et al., 2020) first mines knowledge phrases along with passages from the Web.",
      "startOffset" : 9,
      "endOffset" : 28
    }, {
      "referenceID" : 10,
      "context" : "Although meta-learning methods (Finn et al., 2017; Gu et al., 2018; Sun et al., 2019) advance the state-of-the-art performance for few-shot learning, our COSY still outperforms the meta-learning-based method, i.",
      "startOffset" : 31,
      "endOffset" : 85
    }, {
      "referenceID" : 13,
      "context" : "Although meta-learning methods (Finn et al., 2017; Gu et al., 2018; Sun et al., 2019) advance the state-of-the-art performance for few-shot learning, our COSY still outperforms the meta-learning-based method, i.",
      "startOffset" : 31,
      "endOffset" : 85
    }, {
      "referenceID" : 46,
      "context" : "Although meta-learning methods (Finn et al., 2017; Gu et al., 2018; Sun et al., 2019) advance the state-of-the-art performance for few-shot learning, our COSY still outperforms the meta-learning-based method, i.",
      "startOffset" : 31,
      "endOffset" : 85
    }, {
      "referenceID" : 20,
      "context" : "Since it is non-trivial to measure the utilization of syntax in a straightforward way, we adopt a standard way to measure the importance of the neurons in deep models (Kádár et al., 2017).",
      "startOffset" : 167,
      "endOffset" : 187
    } ],
    "year" : 2021,
    "abstractText" : "Pre-trained multilingual language models, e.g., multilingual-BERT, are widely used in crosslingual tasks, yielding the state-of-the-art performance. However, such models suffer from a large performance gap between source and target languages, especially in the zero-shot setting, where the models are fine-tuned only on English but tested on other languages for the same task. We tackle this issue by incorporating language-agnostic information, specifically, universal syntax such as dependency relations and POS tags, into language models, based on the observation that universal syntax is transferable across different languages. Our approach, named COunterfactual SYntax (COSY), includes the design of SYntaxaware networks as well as a COunterfactual training method to implicitly force the networks to learn not only the semantics but also the syntax. To evaluate COSY, we conduct cross-lingual experiments on natural language inference and question answering using mBERT and XLM-R as network backbones. Our results show that COSY achieves the stateof-the-art performance for both tasks, without using auxiliary dataset.1",
    "creator" : "LaTeX with hyperref"
  }
}