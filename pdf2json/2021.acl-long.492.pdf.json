{
  "name" : "2021.acl-long.492.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Document-level Event Extraction via Parallel Prediction Networks",
    "authors" : [ "Hang Yang", "Dianbo Sui", "Yubo Chen", "Kang Liu", "Jun Zhao", "Taifeng Wang", "Jing Yan" ],
    "emails" : [ "jzhao}@nlpr.ia.ac.cn", "taifeng.wang@antgroup.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6298–6308\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6298"
    }, {
      "heading" : "1 Introduction",
      "text" : "The goal of event extraction (EE) is to identify events of a pre-specified type along with corresponding arguments from plain texts. A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al., 2004). 1 However, these SEE-based methods make predictions within\n1https://www.ldc.upenn.edu/ collaborations/past-projects/ace\na sentence and fail to extract events across sentences. To this end, document-level EE (DEE) is needed when the event information scatters across the whole document.\nIn contrast to SEE, there are two specific challenges in DEE: arguments-scattering and multievents. Specifically, arguments-scattering indicates that arguments of an event may scatter across multiple sentences. For example, As shown in Figure 1, the arguments of Event-1 are distributed in different sentences ([S3] and [S7]) and extraction within an individual sentence will lead to incomplete results. So this challenge requires the DEE model to have a holistic understanding of the entire document and an ability to assemble all relevant\narguments across sentences. Furthermore, it will be more difficult when coupled with the second challenge: multi-events, where multiple events are contained in a document.2 As shown in Figure 1, there are two events Event-1 and Event-2 in a document with the same event type and there is no obvious textual boundary between the two events. The multi-events problem requires the DEE method to recognize how many events are contained in a document and achieve accurate arguments assembling (i.e., assign arguments to the corresponding event). As a result of these two complications, SEE methods are ill-suited for the DEE task, which calls for a model that can integrate document-level information, assemble relevant arguments across multiple sentences and capture multiple events simultaneously.\nTo handle these challenges in DEE, previous works (Yang et al., 2018; Zheng et al., 2019) formulate DEE as an event table filling task, i.e., filling candidate arguments into a predefined event table. Specifically, they model the DEE as a serial prediction paradigm, in which arguments are predicted in a predefined role order and multiple events are also extracted in predefined event order. Such a manner is restricted to the extraction of individual arguments, and the former extraction will not consider the latter extraction results. As a result, errors will be propagated and the extraction performance is under satisfaction.\nIn this paper, to avoid the shortage of serial prediction and tackle the aforementioned challenges in DEE, we propose an end-to-end model, named Document-to-Events via Parallel Prediction Networks (DE-PPN). DE-PPN is based on an encoder-decoder framework that can extract structured events from a whole document in a parallel manner. In detail, we first introduce a documentlevel encoder to obtain the document-aware representations. In such a way, a holistic understanding of the entire document is obtained. Then, we leverage a multi-granularity decoder to generate events, which consists of two key parts: a role decoder and an event decoder. The role decoder is designed for handling the argument-scattering challenge, which can assemble arguments for an event based on document-aware representations. For addressing the challenge of multi-events effectively, an event decoder is designed to support generating\n2According to our statistics, there are about 30% documents include multiple events in the widely used ChFinAnn (Zheng et al., 2019)\nmultiple events. Both of them are based on the non-autoregressive mechanism (Gu et al., 2018), which supports the extraction of multiple events in parallel. Finally, for comparing extracted events to ground truths, we propose a matching loss function inspired by the Hungarian algorithm (Kuhn, 1955; Munkres, 1957). The proposed loss function can perform a global optimization by computing a bipartite matching between predicted and groundtruth events.\nIn summary, our contributions are as follows:\n• We propose an encoder-decoder model, DEPPN, that is based on a document-level encoder and a multi-granularity decoder to extract events in parallel with document-aware representations.\n• We introduce a novel matching loss function to train the end-to-end model, which can bootstrap a global optimization.\n• We conduct extensive experiments on the widely used DEE dataset and experimental results demonstrate that DE-PPN can significantly outperform state-of-the-art methods when facing the specific challenges in DEE."
    }, {
      "heading" : "2 Methodology",
      "text" : "Before introducing our proposed approach for DEE in this section, we first describe the task formalization of DEE. Formally, we denote T and R as the set of pre-defined event types and role categories, respectively. Given an input document comprised of Ns sentences D = {Si}Nsi=1, the DEE task aims to extract one or more structured events Y = {yi}ki=1, where each event yti with event type t contains a series of roles (r1i , r 2 i , . . . , r n i ) filled by arguments (a1i , a 2 i , . . . , a n i ). k is the number of events contained in the document, n is the number of pre-defined roles for the event type t, t ∈ T and r ∈ R.\nThe key idea of our proposed model, DE-PPN, is that aggregate the document-level context to predict events in parallel. Figure 2 illustrates the architecture of DE-PPN, which consists of five key components: (1) candidate argument recognition, (2) document-level encoder, (3) multi-granularity decoder, (4) events prediction, and (5) matching loss function."
    }, {
      "heading" : "2.1 Candidate Argument Recognition",
      "text" : "Given a documentD = {Si}Nsi=1 withNs sentences, each sentence Si with a sequence of tokens is first embedded as [wi,1,wi,2, . . . ,wi,l], where l is the sentence length. Then, the word embeddings are fed into an encoder to obtain the contextualized representation. In this paper, we adopt the Transformer (Vaswani et al., 2017) as the primary context encoder. Through the encoder, we can get the context-aware embedding Ci of sentence Si:\nCi = Transformer-1(Si) (1)\nwhere Ci ∈ Rl×d and d is the size of the hidden layer, and we represent each sentence in the given document as {Ci}Nsi=1.\nFinally, following Zheng et al. (2019), we model the sentence-level candidate argument recognition as a typical sequence tagging task. Through candidate argument recognition, we can obtain candidate arguments A = {ai}Nai=1 from the given sentence Si, where Na is the number of recognized candidate arguments."
    }, {
      "heading" : "2.2 Document-level Encoder",
      "text" : "To enable the awareness of document-level contexts for sentences and candidate arguments, we employ a document-aware encoder to facilitate the interaction between all sentences and candidate arguments. Formally, given an argument ai with its span covering j-th to k-th in sentence Si, we conduct a max-pooling operation over the token-level embedding [ci,j , . . . , ci,k] ∈ Ci to get the local\nembedding cai ∈ Rd for it. Similarly, the sentence embedding csi ∈ Rd can be obtained by the maxpooling operation over the token sequence representation Ci of sentence Si. Then, we employ the Transformer module, Transformer-2, as the encoder to model the interaction between all sentences and candidate arguments by a multi-head self-attention mechanism. Then we can get the document-aware representations for sentences and arguments. Note that we add the sentence representation with sentence position embeddings to inform the sentence order before feeding them into Transformer-2.\n[Ha;Hs] = Transformer-2(ca1...c a Na ; c s 1...c s Ns)\n(2) since arguments may have many mentions in a document, we utilize the max-pooling operation to merge multiple argument embeddings with the same char-level tokens into a single embedding. After the document-level encoding stage, we can obtain the document-aware sentences representation Hs ∈ RNs×d and candidate arguments A′ = {ai}N ′ a i=1 with representation H a ∈ RN ′a×d.\nBefore decoding, we stack a linear classifier over the document representation by operating the max-pooling over Hs to conduct a binary classification for each event type. Then, for the predicted event type t with pre-defined role types, DE-PPN learns to generate events according to the document-aware candidate argument representations Ha ∈ RN ′a×d and sentence representations Hs ∈ RNs×d."
    }, {
      "heading" : "2.3 Multi-Granularity Decoder",
      "text" : "To effectively address arguments-scattering and multi-events in DEE, we introduce a multigranularity decoder to generate all possible events in parallel based on document-aware representations (Ha and Hs). The multi-granularity decoder is composed of three parts: event decoder, role decoder, and event-to-role decoder. All of these decoders are based on the non-autoregressive mechanism (Gu et al., 2018), which supports the extraction of all events in parallel.\nEvent Decoder. The event decoder is designed to support the extraction of all events in parallel and is used to model the interaction between events. Before the decoding stage, the decoder needs to know the size of events to be generated. We use m learnable embeddings as the input of the event decoder, which are denoted as event queries Qevent ∈ Rm×d. m is a hyperparameter that denotes the number of the generated events. In our work, m is set to be significantly large than the average number of events in a document. Then, the event query embeddings Qevent are fed into a non-autoregressive decoder which is composed of a stack of N identical Transformer layers. In each layer, there are a multi-head self-attention mechanism to model the interaction among events and a multi-head cross-attention mechanism to integrate the document-aware representation Hs into event queries Qevent. Formally, the m event queries are decoded into m output embeddings Hevent by:\nHevent = Event-Decoder(Qevent;Hs) (3)\nwhere Hevent ∈ Rm×d.\nRole Decoder. The role decoder is designed to support the filling of all roles in an event in parallel and model the interaction between roles. As the predicted event type t with semantic role types (r1, r2, . . . , rn), we use n learnable embeddings as the input of the role decoder, which are denoted as event queries Qrole ∈ Rn×d. Then, the role query embeddings Qrole are fed into the decoder, which has the same architecture as the event decoder. Specifically, the self-attention mechanism can model the relationship among roles, and the cross-attention mechanism can fuse the information of the document-aware candidate argument representations Ha. Formally, the n role queries are decoded into n output embeddingsHrole by:\nHrole = Role-Decoder(Qrole;Ha) (4)\nwhere Hrole ∈ Rn×d.\nEvent-to-Role Decoder. To generate diversiform events with relevant arguments for different event queries, an event-to-role decoder is designed to model the interaction between the event queries Hevent and the role queries Hrole:\nHe2r = Event2Role-Decoder(Hrole;Hevent) (5)\nwhere He2r ∈ Rm×n×d."
    }, {
      "heading" : "2.4 Events Prediction",
      "text" : "After the multi-granularity decoding, the m event queries and n role queries are transformed into m predicted events and each of them contains n role embeddings. To filter the spurious event, the m event queries Hevent are fed into a feed-forward networks (FFN) to judge each event prediction is non-null or null. Concretely, the predicted event can be obtained by:\npevent = softmax(HeventWe) (6)\nwhere We ∈ Rd×2 is learnable parameters. Then, for each predicted event with pre-defined roles, the predicted arguments are decoded by filling the candidate indices or the null value with (N ′a + 1)-class classifiers: 3\nProle = softmax(tanh(He2rW1 +H aW2) ·v1) (7) where W1 ∈ Rd×d, W2 ∈ Rd×d and v1 ∈ Rd are learnable parameters, and Prole ∈ Rm×n×(N ′a+1).\nAfter the prediction network, we can obtain the m events Ŷ = (Ŷ1, Ŷ2, . . . , Ŷm) where each event Ŷi = (P1i ,P2i , . . . ,Pni ) contains n predicted arguments with role types. Where Pji = P\nrole[i, j, :] ∈ R(N ′a+1)."
    }, {
      "heading" : "2.5 Matching Loss",
      "text" : "The main problem for training is that how to assign predicted m events with a series of arguments to the ground truth k events. Inspired by the assigning problem in the operation research (Kuhn, 1955; Munkres, 1957), we propose a matching loss function, which can produce an optimal bipartite matching between predicted and ground-truth events.\nFormally, we denote predicted and ground truth events as Ŷ = (Ŷ1, Ŷ2, . . . , Ŷm) and Y = (Y1, Y2, . . . , Yk), respectively. Where k is the\n3Note that we append candidate argument representations Ha with a learnable embedding to represent the null value.\nreal number of events in the document and m is fixed size for generated events. Note that m > k. The i-th predicted event is denoted as Ŷi = (P1i ,P2i , . . . ,Pni ) , where P j i can be calculated by the Equation 7. And the i-th ground truth event is denoted as Yi = (r1i , r 2 i , . . . , r n i ) , where rji is the candidate argument indix for j-the role type in i-th target event.\nTo find a bipartite matching between these two sets, we search for a permutation of m elements with the lowest cost:\nσ̂ = argmax σ∈ ∏ (m) m∑ i Cmatch(Ŷσ(i), Yi) (8)\nwhere ∏ (m) is the space of all m-length permutations and Cmatch(Ŷσ(i), Yi) is a pair-wise matching cost between ground truth yi and a prediction Ŷσ(i) with index σ(i). By taking into account all of the prediction arguments for roles in an event, we define Cmatch(Ŷσ(i), Yi) as:\nCmatch(Ŷσ(i), Yi) = −1{judgei 6=φ} n∑ j=1 Pjσ(i)(r j i ))\n(9) where the judgei is the judgement of event i to be non-null or null that is calculated by the Equation 6. The optimal assignment σ(i) can be computed effectively with the Hungarian algorithm. 4 Then for all pairs matched in the previous step, we define the loss function with negative log-likelihood as:\nL(Ŷ , Y ) = m∑ i=1 1{judgei 6=φ}[ n∑ j=1 −logPjσ̂(i)(r j i )]\n(10) Where σ̂ is the optimal assignment computed in the Equation 8."
    }, {
      "heading" : "2.6 Optimization",
      "text" : "During training, we sum the matching loss for events prediction with preconditioned steps before decoding as follows:\nLall = λ1Lsee + λ2Lec + λ3L(Y, Ŷ ) (11)\nwhere Lae and Lec are the cross-entropy loss function for sentence-level candidate argument recognition and event type classification, respectively. λ1, λ2 and λ3 are hyper-parameters.\n4https://en.wikipedia.org/wiki/ Hungarianalgorithm"
    }, {
      "heading" : "3 Experiments and Analysis",
      "text" : "In this section, we present empirical studies to answer the following questions:\n1. What is the overall performance of our DEPPN compared to the state-of-the-art (SOTA) method evaluated on the DEE task?\n2. How does DE-PPN perform when facing the arguments-scattering and multi-event challenges in DEE?\n3. How does each design of our proposed DEPPN matter?\n4. What is the influence of setting different numbers of the generated events on the results?"
    }, {
      "heading" : "3.1 Experimental Setup",
      "text" : "Dataset. Following Zheng et al. (2019), we use the ChFinAnn dataset5 to evaluate our proposed DEE method. The ChFinAnn is a large-scale DEE dataset, which contains 32,040 documents in total and includes five financial event types: Equity Freeze (EF), Equity Repurchase (ER), Equity Underweight (EU), Equity Overweight (EO) and Equity Pledge (EP).\nEvaluation Metrics. For a fair comparison, we adopt the evaluation standard used in Doc2EDAG (Zheng et al., 2019). Specifically, for each predicted event, the most similar groundtruth is selected without replacement to calculate the Precision (P), Recall (R), and F1-measure (F1score). As an event type often includes multiple roles, micro-averaged role-level scores are calculated as the final DEE metric.\nImplementation Details. For a document as input, we set the maximum number of sentences and the maximum sentence length as 64 and 128, respectively. We adopt the basic Transformer, each layer has 768 hidden units, and 8 attention heads, as the encoder and decoder architecture. During training, we employ the AdamW optimizer (Kingma and Ba, 2014) with the learning rate 1e-5 with batch size 16. Testing set performance is chosen by the best development set performance step within 100 epochs. We leave detailed hyper-parameters and additional results in the Appendix.\n5https://github.com/dolphin-zs/ Doc2EDAG/blob/master/Data.zip"
    }, {
      "heading" : "3.2 Baselines",
      "text" : "We compare our DE-PPN with the SOTA methods as follows: DCFEE (Yang et al., 2018) proposed a key-event detection to guide event table filled with the arguments from key-event mention and surrounding sentences. There are two versions of DCFEE: DCFEE-O only extracts one event and DCFEE-M extracts multiple events from a document. Doc2EDAG (Zheng et al., 2019) proposed an end-to-end model for DEE, which transforms DEE as directly filling event tables with entitybased path expending. There is a simple baseline of Doc2EDAG, named GreedyDec, which only fills one event table entry greedily. Besides, we further introduce a simple baseline of DE-PPN, named as DE-PPN-1, which only generates one event."
    }, {
      "heading" : "3.3 Main results",
      "text" : "DE-PPN vs. SOTA. Table 1 shows the comparison between DE-PPN and baseline methods on the test set for each event type. Overall, our proposed model DE-PPN significantly outperforms other baselines and achieves SOTA performance\nin all event types. Specifically, DE-PPN improves 3.3, 0.1, 2.6, 0.8, 1.1, 1.6 F1-score over the SOTA method, Doc2EDAG, on the event type EF, ER, EU, EO, EP and the average F1-score, respectively. The improved performance indicates that the encoderdecoder generative framework of DE-PPN is effective, which can predict events in parallel with a global optimization for training. Besides, as the baseline of our proposed method, DE-PPN-O can achieve the best performance compared with DCFEE-O and GreedyDec while all of them only predict one event for a document, which also proves the effectiveness of the document-aware end-to-end modeling of DE-PPN.\nResults on Arguments-Scattering. To show the extreme difficulty of the arguments-scattering challenge in DEE, we conduct experiments on different scenarios. We introduce an arguments-scattering ratio (ASR) to measure the scatter of arguments in an event for a document. The ASR is calculated by:\nASR = Numments/Numargs (12)\nwhere Numments denotes the number of event mentions (i.e., sentences that contains arguments) and Numargs denotes the number of arguments. The higher the ASR, the more scattering of the arguments in an event. Table 3 shows the results with the different intervals of ASR. We can observe that it is more difficult to extract scattering arguments as the ASR increase. But DE-PPN still\nmaintains the best performance and the results indicate that the encoder-decoder framework can better assemble arguments to the corresponding event across sentences with the parallel prediction and the document-aware representations.\nSingle-Event vs. Multi-Event. To show the extreme difficulty when arguments-scattering meets multi-events for DEE, we conduct experiments on two scenarios: single-event (i.e., documents contain one event) and multi-event (i.e., documents contain multiple events). Table 2 shows the F1score on single-event and multi-event sets for each event type and the averaged (Avg.). We can observe that multi-events is extremely challenging as the extraction performance of all models drops significantly. But DE-PPN still improves the average F1score from 67.3% to 68.7% over the Doc2EDAG. The results demonstrate the effectiveness of our proposed method when handling the challenge of multi-events. This performance improvement benefits from the event decoder which can generate multiple events in parallel and the matching loss function which can perform a global optimization. Besides, the DE-PPN-1 model achieves an acceptable performance on the scenario of single event extraction which demonstrates the effectiveness of our end-to-end model. But DE-PPN-1 only generates one event and cannot deal with the multievents problem, resulting in low performance on the multi-event sets."
    }, {
      "heading" : "3.4 Ablation Studies",
      "text" : "To verify the effectiveness of each component of DE-PPN, we conduct ablation tests on the next variants: 1) -DocEnc: removing the Transformerbased document-level encoder, which can support the document-aware information for decoding. 2) -MultiDec: replacing the multi-granularity decoder module with simple embedding initialization for event queries and role queries. 3) -MatchingLoss:\nreplacing the matching loss function with normal cross-entropy loss. The results are shown in Table 4 and we can observe that: 1) the document-level encoder is of prime importance that enhances the document-aware representations for the generative decoder and contributes +2.6 F1-score on average; 2) the multi-granularity decoder alleviates the challenges of argument-scattering and multi-events by assembling arguments and generating events in parallel, improving by +4.3 F1-score on average. 3) the matching loss function is a very important component for events extraction with +13.4 F1-score improvement which indicates that the matching loss guide a global optimization between predicted and ground-truth events during training."
    }, {
      "heading" : "3.5 Effect of Different Decoder Layers",
      "text" : "To investigate the importance of the multigranularity decoder, we explore the effect of different layers of the event decoder and the role decoder on the results. Specifically, the number of decoder layers is set to 0,1,2,3 and 4, where 0 means removing this decoder. 1) The effect of different event decoder layers are shown in the left of Figure 3, and our method can achieve the best average F1-score when the number of layers is set to be 2. We conjecture that more layers of the non-autoregressive decoder allow for better modeling the interaction between event queries and generating diversiform events. However, when the layer is set to be large, it is easy to generate redundant events. 2) The\neffect of different role decoder layers are shown in the right of Figure 3, and we can observe that the more decoder layers, the better performance on the results. We conjecture that more layers of the decoder with the more self-attention modules allow for better modeling the relationship between event roles and more inter-attention modules allow for integrating information of candidate arguments into roles."
    }, {
      "heading" : "3.6 Effect of Different Generated Sets",
      "text" : "For the training and testing process of the DE-PPN, the number of generated events is an important hyperparameter. In this section, we explore the influence of setting different numbers of generated events on the results. We divide the development set into 5 sub-class where each class contains 1,2,3,4 and > 5 events. Table 5 shows the statistics of the documents with different annotated events in the development set. To validate the impact of the number of generated events on the performance, we evaluate DE-PPN with various numbers of generated events: 1, 2, 5, 10, named DE-PPN-1, DE-PPN-2, DE-PPN-5, DE-PPN-10, respectively. The results of DE-PPN with different generated events are shown in Figure 4, which are also compared with the SOTA model Doc2EDAG. We can observe that as the number of events increases, it is more difficult for events prediction, which can be reflected in the decline of all performance. In general, DE-PPN almost achieves the best performance on the average F1-score when the number of generated sets is set to be 5. Besides, there is a performance gap between Doc2EDAG and our method DE-PPN when the number of annotated events is large than 2 in a document. It also demonstrates that our proposed parallel decoder can better handle the challenge of multi-events in DEE."
    }, {
      "heading" : "4 Related Work",
      "text" : ""
    }, {
      "heading" : "4.1 Sentence-level Event Extraction",
      "text" : "Most work in EE has focused on the sentence level and is based on the benchmark dataset ACE 2005 (Doddington et al., 2004). Many approaches"
    }, {
      "heading" : "1 2 3 4 >=5",
      "text" : "have been proposed to improve performance on this task. These studies are mainly based on handdesigned features (Li et al., 2013; Kai and Grishman, 2015) and neural-based to learn features automatically (Chen et al., 2015; Nguyen et al., 2016; Björne and Salakoski, 2018; Yang et al., 2019; Chan et al., 2019; Yang et al., 2019; Liu et al., 2020). A few methods make extraction decisions beyond individual sentences. Ji and Grishman (2008) and Liao and Grishman (2010) used event type co-occurrence patterns for event detection. Yang and Mitchell (2016) introduced event structure to jointly extract events and entities within a document. Although these approaches make decisions beyond sentence boundary, their extractions are still done at the sentence level."
    }, {
      "heading" : "4.2 Document-level Event Extraction",
      "text" : "Many real-world applications need DEE, in which the event information scatters across the whole document. MUC-4 (1992) proposed the MUC-4 template-filling task that aims to identify event role fillers with associated role types from a document. Recent works explore the local and additional context to extract the role fillers by manually designed linguistic features (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012) or neural-based contextual representation (Chen et al., 2020; Du et al., 2020; Du and Cardie, 2020). Recently, Ebner et al. (2020) published the Roles Across Multiple Sentences (RAMS) dataset, which contains annotation for the task of multi-sentence argument linking. A two-step approach (Zhang et al., 2020) is proposed for argument linking by detecting implicit argument across sentences. Li et al. (2021) extend this task and compile a new benchmark dataset\nWIKIEVENTS for exploring document-level argument extraction task. Then, Li et al. (2021) propose an end-to-end neural event argument extraction model by conditional text generation. However, these works focused on the sub-task of DEE (i.e., role filler extraction or argument extraction) and ignored the challenge of multi-events.\nTo simultaneously address both challenges for DEE (i.e., arguments-scattering and multi-events), previous works focus on the ChFinAnn (Zheng et al., 2019) dataset and model DEE as an event table filling task, i.e., filling candidate arguments into predefined event table. Yang et al. (2018) proposed a key-event detection to guide event table filled with the arguments from key-event mention and surrounding sentences. Zheng et al. (2019) transforms DEE into filling event tables following a predefined order of roles with an entity-based path expanding, which achieved the SOTA for DEE. However, these methods suffered from a serial prediction which will lead to error propagation and individual argument prediction."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "In this paper, we propose an encoder-decoder model, DE-PPN, to extract events in parallel from a document. For addressing the challenges (i.e., arguments-scattering and multi-events) in DEE, we introduce a document-level encoder and a multigranularity decoder to generate events in parallel with document-aware representations. For training the parallel networks, we propose a matching loss function to perform a global optimization. Experimental results show that DE-PPN can significantly outperform SOTA methods especially facing the specific challenges in DEE."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank the anonymous reviewers for their constructive and insightful comments. This work is supported by the National Natural Science Foundation of China (No. U1936207, No. 61922085 and No. 61806201), Beijing Academy of Artificial Intelligence (No. BAAI2019QN0301), the Key Research Program of the Chinese Academy of Sciences (No. ZDBS-SSW-JSC006), independent research project of National Laboratory of Pattern Recognition and a grant from Ant Group."
    }, {
      "heading" : "A Appendix",
      "text" : "In the appendix, we incorporate the following details that are omitted in the main body due to the space limit.\n• Section A.1 introduce the Hungarian Algorithm.\n• Section A.2 complements additional evaluation results for event classification and candidate arguments extraction.\n• Section A.3 show the hyper-parameter setting.\nA.1 Hungarian Algorithm\nThe linear sum assignment problem is also known as minimum weight matching in bipartite graphs. A problem instance is described by a matrix C, where each Ci,j is the cost of matching vertex i of the first partite set (a “worker”) and vertex j of the second set (a “job”). The goal is to find a complete assignment of workers to jobs of minimal cost.\nFormally, let X be a boolean matrix where Xi,j = 1 if row i is assigned to column j. Ci,j is the cost matrix of the bipartite graph. Then the optimal assignment has cost:\nmin ∑ i ∑ j Ci,jXi,j (13)\ns.t. each row is assignment to at most one column, and each column to at most one row. This function can also solve a generalization of the classic assignment problem where the cost matrix is rectangular. If it has more rows than columns, then not every row needs to be assigned to a column, and vice versa. The method used is the Hungarian algorithm, also known as the Munkres or KuhnMunkres algorithm.\nA.2 Additional Results Table 6 shows the results of event type classification and candidate argument extraction. They are the two preceding sub-tasks for decoder to predict events with corresponding arguments in parallel. We can observe that: 1) the document-level event type classification can achieve a good performance which proves that event classification is not a difficult problem in this task. 2) how to assemble candidate arguments to corresponding events is the key challenge for DEE.\nA.3 Hyperparameter setting The detail hyperparameter is shown in Table 7"
    } ],
    "references" : [ {
      "title" : "The stages of event extraction",
      "author" : [ "David Ahn." ],
      "venue" : "Proceedings of the Workshop on Annotating and Reasoning about Time and Events, pages 1–8.",
      "citeRegEx" : "Ahn.,? 2006",
      "shortCiteRegEx" : "Ahn.",
      "year" : 2006
    }, {
      "title" : "Biomedical event extraction using convolutional neural networks and dependency parsing",
      "author" : [ "Jari Björne", "Tapio Salakoski." ],
      "venue" : "Proceedings of the BioNLP 2018 workshop, pages 98–108, Melbourne, Australia. Association for Computational",
      "citeRegEx" : "Björne and Salakoski.,? 2018",
      "shortCiteRegEx" : "Björne and Salakoski.",
      "year" : 2018
    }, {
      "title" : "Rapid customization for event extraction",
      "author" : [ "Yee Seng Chan", "Joshua Fasching", "Haoling Qiu", "Bonan Min." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 31–36, Flo-",
      "citeRegEx" : "Chan et al\\.,? 2019",
      "shortCiteRegEx" : "Chan et al\\.",
      "year" : 2019
    }, {
      "title" : "Reconstructing event regions for event extraction via graph attention networks",
      "author" : [ "Pei Chen", "Hang Yang", "Kang Liu", "Ruihong Huang", "Yubo Chen", "Taifeng Wang", "Jun Zhao." ],
      "venue" : "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Associ-",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatically labeled data generation for large scale event extraction",
      "author" : [ "Yubo Chen", "Shulin Liu", "Xiang Zhang", "Kang Liu", "Jun Zhao." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Event extraction via dynamic multipooling convolutional neural networks",
      "author" : [ "Yubo Chen", "Liheng Xu", "Kang Liu", "Daojian Zeng", "Jun Zhao." ],
      "venue" : "Proceedings of the ACL.",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "The automatic content extraction (ace) program-tasks, data, and evaluation",
      "author" : [ "George R Doddington", "Alexis Mitchell", "Mark A Przybocki", "Lance A Ramshaw", "Stephanie M Strassel", "Ralph M Weischedel." ],
      "venue" : "Lrec, volume 2, page 1. Lisbon.",
      "citeRegEx" : "Doddington et al\\.,? 2004",
      "shortCiteRegEx" : "Doddington et al\\.",
      "year" : 2004
    }, {
      "title" : "Document-level event role filler extraction using multi-granularity contextualized encoding",
      "author" : [ "Xinya Du", "Claire Cardie." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8010–8020, Online. Asso-",
      "citeRegEx" : "Du and Cardie.,? 2020",
      "shortCiteRegEx" : "Du and Cardie.",
      "year" : 2020
    }, {
      "title" : "Document-level event-based extraction using generative template-filling transformers",
      "author" : [ "Xinya Du", "Alexander Rush", "Claire Cardie." ],
      "venue" : "arXiv preprint arXiv:2008.09249.",
      "citeRegEx" : "Du et al\\.,? 2020",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-sentence argument linking",
      "author" : [ "Seth Ebner", "Patrick Xia", "Ryan Culkin", "Kyle Rawlins", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8057–8077, Online. Association for",
      "citeRegEx" : "Ebner et al\\.,? 2020",
      "shortCiteRegEx" : "Ebner et al\\.",
      "year" : 2020
    }, {
      "title" : "Non-autoregressive neural machine translation",
      "author" : [ "Jiatao Gu", "James Bradbury", "Caiming Xiong", "Victor OK Li", "Richard Socher." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Gu et al\\.,? 2018",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2018
    }, {
      "title" : "Using cross-entity inference to improve event extraction",
      "author" : [ "Yu Hong", "Jianfeng Zhang", "Bin Ma", "Jianmin Yao", "Guodong Zhou", "Qiaoming Zhu." ],
      "venue" : "Proceedings of the 49th annual meeting of the association for computational linguistics: human lan-",
      "citeRegEx" : "Hong et al\\.,? 2011",
      "shortCiteRegEx" : "Hong et al\\.",
      "year" : 2011
    }, {
      "title" : "Zero-shot transfer learning for event extraction",
      "author" : [ "Lifu Huang", "Heng Ji", "Kyunghyun Cho", "Ido Dagan", "Sebastian Riedel", "Clare Voss." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
      "citeRegEx" : "Huang et al\\.,? 2018",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2018
    }, {
      "title" : "Peeling back the layers: detecting event role fillers in secondary contexts",
      "author" : [ "Ruihong Huang", "Ellen Riloff." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages",
      "citeRegEx" : "Huang and Riloff.,? 2011",
      "shortCiteRegEx" : "Huang and Riloff.",
      "year" : 2011
    }, {
      "title" : "Bootstrapped training of event extraction classifiers",
      "author" : [ "Ruihong Huang", "Ellen Riloff." ],
      "venue" : "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 286–295. Association for Computational Lin-",
      "citeRegEx" : "Huang and Riloff.,? 2012",
      "shortCiteRegEx" : "Huang and Riloff.",
      "year" : 2012
    }, {
      "title" : "Refining event extraction through cross-document inference",
      "author" : [ "Heng Ji", "Ralph Grishman." ],
      "venue" : "Proceedings of ACL-08: Hlt, pages 254–262.",
      "citeRegEx" : "Ji and Grishman.,? 2008",
      "shortCiteRegEx" : "Ji and Grishman.",
      "year" : 2008
    }, {
      "title" : "Improving event detection with abstract meaning representation",
      "author" : [ "Xiang Li Thien Huu Nguyen Kai", "Cao Ralph Grishman." ],
      "venue" : "ACL-IJCNLP 2015, page 11.",
      "citeRegEx" : "Kai and Grishman.,? 2015",
      "shortCiteRegEx" : "Kai and Grishman.",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "The hungarian method for the assignment problem",
      "author" : [ "Harold W Kuhn." ],
      "venue" : "Naval research logistics quarterly, 2(1-2):83–97.",
      "citeRegEx" : "Kuhn.,? 1955",
      "shortCiteRegEx" : "Kuhn.",
      "year" : 1955
    }, {
      "title" : "Joint event extraction via structured prediction with global features",
      "author" : [ "Qi Li", "Heng Ji", "Liang Huang." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 73–82.",
      "citeRegEx" : "Li et al\\.,? 2013",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2013
    }, {
      "title" : "Documentlevel event argument extraction by conditional generation",
      "author" : [ "Sha Li", "Heng Ji", "Jiawei Han." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Using document level cross-event inference to improve event extraction",
      "author" : [ "Shasha Liao", "Ralph Grishman." ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 789–797.",
      "citeRegEx" : "Liao and Grishman.,? 2010",
      "shortCiteRegEx" : "Liao and Grishman.",
      "year" : 2010
    }, {
      "title" : "Event extraction as machine reading comprehension",
      "author" : [ "Jian Liu", "Yubo Chen", "Kang Liu", "Wei Bi", "Xiaojiang Liu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1641–1651, Online. Associa-",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Algorithms for the assignment and transportation problems",
      "author" : [ "James Munkres." ],
      "venue" : "Journal of the society for industrial and applied mathematics, 5(1):32–38.",
      "citeRegEx" : "Munkres.,? 1957",
      "shortCiteRegEx" : "Munkres.",
      "year" : 1957
    }, {
      "title" : "Joint event extraction via recurrent neural networks",
      "author" : [ "Thien Huu Nguyen", "Kyunghyun Cho", "Ralph Grishman." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Nguyen et al\\.,? 2016",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "A unified model of phrasal and sentential evidence for information extraction",
      "author" : [ "Siddharth Patwardhan", "Ellen Riloff." ],
      "venue" : "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 151–",
      "citeRegEx" : "Patwardhan and Riloff.,? 2009",
      "shortCiteRegEx" : "Patwardhan and Riloff.",
      "year" : 2009
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Joint extraction of events and entities within a document context",
      "author" : [ "Bishan Yang", "Tom M. Mitchell." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Yang and Mitchell.,? 2016",
      "shortCiteRegEx" : "Yang and Mitchell.",
      "year" : 2016
    }, {
      "title" : "Dcfee: A document-level chinese financial event extraction system based on automatically labeled training data",
      "author" : [ "Hang Yang", "Yubo Chen", "Kang Liu", "Yang Xiao", "Jun Zhao." ],
      "venue" : "Proceedings of ACL 2018, System Demonstrations, pages 50–55.",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploring pre-trained language models for event extraction and generation",
      "author" : [ "Sen Yang", "Dawei Feng", "Linbo Qiao", "Zhigang Kan", "Dongsheng Li." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5284–",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "A two-step approach",
      "author" : [ "Zhisong Zhang", "Xiang Kong", "Zhengzhong Liu", "Xuezhe Ma", "Eduard Hovy" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Doc2EDAG: An end-to-end document-level framework for Chinese financial event extraction",
      "author" : [ "Shun Zheng", "Wei Cao", "Wei Xu", "Jiang Bian." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Zheng et al\\.,? 2019",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al.",
      "startOffset" : 35,
      "endOffset" : 271
    }, {
      "referenceID" : 15,
      "context" : "A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al.",
      "startOffset" : 35,
      "endOffset" : 271
    }, {
      "referenceID" : 21,
      "context" : "A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al.",
      "startOffset" : 35,
      "endOffset" : 271
    }, {
      "referenceID" : 11,
      "context" : "A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al.",
      "startOffset" : 35,
      "endOffset" : 271
    }, {
      "referenceID" : 19,
      "context" : "A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al.",
      "startOffset" : 35,
      "endOffset" : 271
    }, {
      "referenceID" : 5,
      "context" : "A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al.",
      "startOffset" : 35,
      "endOffset" : 271
    }, {
      "referenceID" : 24,
      "context" : "A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al.",
      "startOffset" : 35,
      "endOffset" : 271
    }, {
      "referenceID" : 27,
      "context" : "A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al.",
      "startOffset" : 35,
      "endOffset" : 271
    }, {
      "referenceID" : 4,
      "context" : "A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al.",
      "startOffset" : 35,
      "endOffset" : 271
    }, {
      "referenceID" : 12,
      "context" : "A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al.",
      "startOffset" : 35,
      "endOffset" : 271
    }, {
      "referenceID" : 29,
      "context" : "A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al.",
      "startOffset" : 35,
      "endOffset" : 271
    }, {
      "referenceID" : 22,
      "context" : "A great number of previous studies (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Yang and Mitchell, 2016; Chen et al., 2017; Huang et al., 2018; Yang et al., 2019; Liu et al., 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al.",
      "startOffset" : 35,
      "endOffset" : 271
    }, {
      "referenceID" : 6,
      "context" : ", 2020) focus on the sentence-level EE (SEE), while most of these works are based on the ACE evaluation (Doddington et al., 2004).",
      "startOffset" : 104,
      "endOffset" : 129
    }, {
      "referenceID" : 28,
      "context" : "To handle these challenges in DEE, previous works (Yang et al., 2018; Zheng et al., 2019) formulate DEE as an event table filling task, i.",
      "startOffset" : 50,
      "endOffset" : 89
    }, {
      "referenceID" : 31,
      "context" : "To handle these challenges in DEE, previous works (Yang et al., 2018; Zheng et al., 2019) formulate DEE as an event table filling task, i.",
      "startOffset" : 50,
      "endOffset" : 89
    }, {
      "referenceID" : 31,
      "context" : "According to our statistics, there are about 30% documents include multiple events in the widely used ChFinAnn (Zheng et al., 2019) multiple events.",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 10,
      "context" : "Both of them are based on the non-autoregressive mechanism (Gu et al., 2018), which supports the extraction of multiple events in parallel.",
      "startOffset" : 59,
      "endOffset" : 76
    }, {
      "referenceID" : 18,
      "context" : "Finally, for comparing extracted events to ground truths, we propose a matching loss function inspired by the Hungarian algorithm (Kuhn, 1955; Munkres, 1957).",
      "startOffset" : 130,
      "endOffset" : 157
    }, {
      "referenceID" : 23,
      "context" : "Finally, for comparing extracted events to ground truths, we propose a matching loss function inspired by the Hungarian algorithm (Kuhn, 1955; Munkres, 1957).",
      "startOffset" : 130,
      "endOffset" : 157
    }, {
      "referenceID" : 26,
      "context" : "In this paper, we adopt the Transformer (Vaswani et al., 2017) as the primary context encoder.",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 10,
      "context" : "All of these decoders are based on the non-autoregressive mechanism (Gu et al., 2018), which supports the extraction of all events in parallel.",
      "startOffset" : 68,
      "endOffset" : 85
    }, {
      "referenceID" : 18,
      "context" : "Inspired by the assigning problem in the operation research (Kuhn, 1955; Munkres, 1957), we propose a matching loss function, which can produce an optimal bipartite matching between predicted and ground-truth events.",
      "startOffset" : 60,
      "endOffset" : 87
    }, {
      "referenceID" : 23,
      "context" : "Inspired by the assigning problem in the operation research (Kuhn, 1955; Munkres, 1957), we propose a matching loss function, which can produce an optimal bipartite matching between predicted and ground-truth events.",
      "startOffset" : 60,
      "endOffset" : 87
    }, {
      "referenceID" : 31,
      "context" : "For a fair comparison, we adopt the evaluation standard used in Doc2EDAG (Zheng et al., 2019).",
      "startOffset" : 73,
      "endOffset" : 93
    }, {
      "referenceID" : 17,
      "context" : "During training, we employ the AdamW optimizer (Kingma and Ba, 2014) with the learning rate 1e-5 with batch size 16.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 28,
      "context" : "We compare our DE-PPN with the SOTA methods as follows: DCFEE (Yang et al., 2018) proposed a key-event detection to guide event table filled with the arguments from key-event mention and surrounding sentences.",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 31,
      "context" : "Doc2EDAG (Zheng et al., 2019) proposed an end-to-end model for DEE, which transforms DEE as directly filling event tables with entitybased path expending.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 6,
      "context" : "Most work in EE has focused on the sentence level and is based on the benchmark dataset ACE 2005 (Doddington et al., 2004).",
      "startOffset" : 97,
      "endOffset" : 122
    }, {
      "referenceID" : 19,
      "context" : "These studies are mainly based on handdesigned features (Li et al., 2013; Kai and Grishman, 2015) and neural-based to learn features automatically (Chen et al.",
      "startOffset" : 56,
      "endOffset" : 97
    }, {
      "referenceID" : 16,
      "context" : "These studies are mainly based on handdesigned features (Li et al., 2013; Kai and Grishman, 2015) and neural-based to learn features automatically (Chen et al.",
      "startOffset" : 56,
      "endOffset" : 97
    }, {
      "referenceID" : 5,
      "context" : ", 2013; Kai and Grishman, 2015) and neural-based to learn features automatically (Chen et al., 2015; Nguyen et al., 2016; Björne and Salakoski, 2018; Yang et al., 2019; Chan et al., 2019; Yang et al., 2019; Liu et al., 2020).",
      "startOffset" : 81,
      "endOffset" : 224
    }, {
      "referenceID" : 24,
      "context" : ", 2013; Kai and Grishman, 2015) and neural-based to learn features automatically (Chen et al., 2015; Nguyen et al., 2016; Björne and Salakoski, 2018; Yang et al., 2019; Chan et al., 2019; Yang et al., 2019; Liu et al., 2020).",
      "startOffset" : 81,
      "endOffset" : 224
    }, {
      "referenceID" : 1,
      "context" : ", 2013; Kai and Grishman, 2015) and neural-based to learn features automatically (Chen et al., 2015; Nguyen et al., 2016; Björne and Salakoski, 2018; Yang et al., 2019; Chan et al., 2019; Yang et al., 2019; Liu et al., 2020).",
      "startOffset" : 81,
      "endOffset" : 224
    }, {
      "referenceID" : 29,
      "context" : ", 2013; Kai and Grishman, 2015) and neural-based to learn features automatically (Chen et al., 2015; Nguyen et al., 2016; Björne and Salakoski, 2018; Yang et al., 2019; Chan et al., 2019; Yang et al., 2019; Liu et al., 2020).",
      "startOffset" : 81,
      "endOffset" : 224
    }, {
      "referenceID" : 2,
      "context" : ", 2013; Kai and Grishman, 2015) and neural-based to learn features automatically (Chen et al., 2015; Nguyen et al., 2016; Björne and Salakoski, 2018; Yang et al., 2019; Chan et al., 2019; Yang et al., 2019; Liu et al., 2020).",
      "startOffset" : 81,
      "endOffset" : 224
    }, {
      "referenceID" : 29,
      "context" : ", 2013; Kai and Grishman, 2015) and neural-based to learn features automatically (Chen et al., 2015; Nguyen et al., 2016; Björne and Salakoski, 2018; Yang et al., 2019; Chan et al., 2019; Yang et al., 2019; Liu et al., 2020).",
      "startOffset" : 81,
      "endOffset" : 224
    }, {
      "referenceID" : 22,
      "context" : ", 2013; Kai and Grishman, 2015) and neural-based to learn features automatically (Chen et al., 2015; Nguyen et al., 2016; Björne and Salakoski, 2018; Yang et al., 2019; Chan et al., 2019; Yang et al., 2019; Liu et al., 2020).",
      "startOffset" : 81,
      "endOffset" : 224
    }, {
      "referenceID" : 25,
      "context" : "Recent works explore the local and additional context to extract the role fillers by manually designed linguistic features (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012) or neural-based contextual representation (Chen et al.",
      "startOffset" : 123,
      "endOffset" : 182
    }, {
      "referenceID" : 3,
      "context" : "Recent works explore the local and additional context to extract the role fillers by manually designed linguistic features (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012) or neural-based contextual representation (Chen et al., 2020; Du et al., 2020; Du and Cardie, 2020).",
      "startOffset" : 225,
      "endOffset" : 282
    }, {
      "referenceID" : 8,
      "context" : "Recent works explore the local and additional context to extract the role fillers by manually designed linguistic features (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012) or neural-based contextual representation (Chen et al., 2020; Du et al., 2020; Du and Cardie, 2020).",
      "startOffset" : 225,
      "endOffset" : 282
    }, {
      "referenceID" : 7,
      "context" : "Recent works explore the local and additional context to extract the role fillers by manually designed linguistic features (Patwardhan and Riloff, 2009; Huang and Riloff, 2011, 2012) or neural-based contextual representation (Chen et al., 2020; Du et al., 2020; Du and Cardie, 2020).",
      "startOffset" : 225,
      "endOffset" : 282
    }, {
      "referenceID" : 30,
      "context" : "A two-step approach (Zhang et al., 2020) is proposed for argument linking by detecting implicit argument across sentences.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 31,
      "context" : ", arguments-scattering and multi-events), previous works focus on the ChFinAnn (Zheng et al., 2019) dataset and model DEE as an event table filling task, i.",
      "startOffset" : 79,
      "endOffset" : 99
    } ],
    "year" : 2021,
    "abstractText" : "Document-level event extraction (DEE) is indispensable when events are described throughout a document. We argue that sentence-level extractors are ill-suited to the DEE task where event arguments always scatter across sentences and multiple events may co-exist in a document. It is a challenging task because it requires a holistic understanding of the document and an aggregated ability to assemble arguments across multiple sentences. In this paper, we propose an end-to-end model, which can extract structured events from a document in a parallel manner. Specifically, we first introduce a document-level encoder to obtain the document-aware representations. Then, a multi-granularity non-autoregressive decoder is used to generate events in parallel. Finally, to train the entire model, a matching loss function is proposed, which can bootstrap a global optimization. The empirical results on the widely used DEE dataset show that our approach significantly outperforms current stateof-the-art methods in the challenging DEE task. Code will be available at https:// github.com/HangYang-NLP/DE-PPN.",
    "creator" : "LaTeX with hyperref"
  }
}