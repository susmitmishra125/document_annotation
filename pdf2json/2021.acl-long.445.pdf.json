{
  "name" : "2021.acl-long.445.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Importance-based Neuron Allocation for Multilingual Neural Machine Translation",
    "authors" : [ "Wanying Xie", "Yang Feng", "Shuhao Gu", "Dong Yu" ],
    "emails" : [ "xiewanying07@gmail.com,", "yudong@blcu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5725–5737\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5725"
    }, {
      "heading" : "1 Introduction",
      "text" : "Neural machine translation(NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) has shown its superiority and drawn much attention in recent years. Although the NMT model can achieve promising results for highresource language pairs, it is unaffordable to train separate models for all the language pairs since there are thousands of languages in the world (Tan et al., 2019; Aharoni et al., 2019; Arivazhagan et al., 2019). A typical solution to reduce the model size\n∗Corresponding author: Yang Feng. Our code can be got at https://github.com/ictnlp/NA-\nMNMT\nand the training cost is to handle multiple languages in a single multilingual neural machine translation (MNMT) model (Ha et al., 2016; Firat et al., 2016; Johnson et al., 2017; Gu et al., 2018). The standard paradigm of MNMT proposed by Johnson et al. (2017) contains a language-shared encoder and decoder with a special language indicator in the input sentence to determine the target language.\nBecause different languages share all of the model parameters in the standard MNMT model, the model tends to converge to a region where there are low errors for all the languages. Therefore, the MNMT model trained on the combined data generally captures the general knowledge, but ignores the language-specific knowledge, rendering itself sub-optimal for the translation of a specific language (Sachan and Neubig, 2018; Blackwood et al., 2018; Wang et al., 2020b). To retain the language-specific knowledge, some researches turn to augment the NMT model with language-specific modules, e.g., the language-specific attention module (Blackwood et al., 2018), decoupled multilingual encoders and/or decoders (Vázquez et al., 2019; Escolano et al., 2020) and the lightweight language adapters (Bapna and Firat, 2019). However, these methods suffer from the parameter increment problem, because the number of parameters increases linearly with the number of languages. Besides, the structure, size, and location of the module have a large influence on the final performance, which requires specialized manual design. As a result, these problems often prevent the application of these methods in some scenarios.\nBased on the above, we aim to propose a method that can retain the general and language-specific knowledge, and keep a stable model size as the number of language-pair increases without introducing any specialized module. To achieve this, we propose to divide the model neurons into two parts based on their importance: the general neurons\nwhich are used to retain the general knowledge of all the languages, and the language-specific neurons which are used to retain the language-specific knowledge. Specifically, we first pre-train a standard MNMT model on all language data and then evaluate the importance of each neuron in each language pair. According to their importance, we divide the neurons into the general neurons and the language-specific neurons. After that, we finetune the translation model on all language pairs. In this process, only the general neurons and the corresponding language-specific neurons for the current language pair participate in training. Experimental results on different languages show that the proposed method outperforms several strong baselines.\nOur contributions can be summarized as follows:\n• We propose a method that can improve the translation performance of the MNMT model without introducing any specialized modules or adding new parameters.\n• We show that the similar languages share some common features that can be captured by some specific neurons of the MNMT model.\n• We show that some modules tend to capture the general knowledge while some modules are more essential for capturing the languagespecific knowledge."
    }, {
      "heading" : "2 Background",
      "text" : "In this section, we will give a brief introduction to the Transformer model (Vaswani et al., 2017) and the Multilingual translation."
    }, {
      "heading" : "2.1 The Transformer",
      "text" : "We denote the input sequence of symbols as x′ = (x1, . . . , xJ), the ground-truth sequence as y∗ = (y∗1, . . . , y ∗ K∗) and the translation as y = (y1, . . . , yK). Transformer is a stacked network with N identical layers containing two or three basic blocks in each layer. For a single layer in the encoder, it consists of a multi-head self-attention and a position-wise feed-forward network. For a single decoder layer, besides the above two basic blocks, a multi-head cross-attention follows multi-head selfattention. The input sequence x will be first converted to a sequence of vectors and fed into the encoder. Then the output of the N -th encoder layer\nwill be taken as source hidden states and fed into decoder. The final output of the N -th decoder layer gives the target hidden states and translate the target sentences."
    }, {
      "heading" : "2.2 Multilingual Translation",
      "text" : "In the standard paradigm of MNMT, all parameters are shared across languages and the model is jointly trained on multiple language pairs. We follow Johnson et al. (2017) to reuse standard bilingual NMT models for multilingual translation by altering the source input with a language token lang, i.e. changing x′ to x = (lang, x1, . . . , xJ)."
    }, {
      "heading" : "3 Approach",
      "text" : "Our goal is to build a unified model, which can achieve good performance on all language pairs. The main idea of our method is that different neurons have different importance to the translation of different languages. Based on this, we divide them into general and language-specific ones and make general neurons participate in the translation of all the languages while language-specific neurons focus on some specific languages. Specifically, the proposed approach involves the following steps shown in Figure 1. First, we pretrain the model on the combined data of all the language pairs following the normal paradigm in Johnson et al. (2017). Second, we evaluate the importance of different neurons on these language pairs and allocate them into general neurons and language-specific neurons. Last, we fine-tune the translation model on the combined data again. It should be noted that for a specific language pair only the general neurons and the language-specific neurons for this language pair will participate in the forward and backward computation when the model is trained on this language pair. Other neurons will be zeroed out during both training and inference."
    }, {
      "heading" : "3.1 Importance Evaluation",
      "text" : "The basic idea of importance evaluation is to determine which neurons are essential to all languages while which neurons are responsible for some specific languages. For a neuron i, its average importance I across language pairs is defined as follow:\nI(i) = 1 M M∑ m=1 Θm(i), (1)\nwhere the Θ(·) denotes the importance evaluation function and M denotes the number of language\npairs. This value correlates positively with how important the neuron is to all languages. For the importance evaluation function Θ(·), we adopt two schemes: one is based on the Taylor Expansion and the other is based on the Absolute Value.\nTaylor Expansion We adopt a criterion based on the Taylor Expansion (Molchanov et al., 2017), where we directly approximate the change in loss when removing a particular neuron. Let hi be the output produced from neuron i and H represents the set of other neurons. Assuming the independence of each neuron in the model, the change of loss when removing a certain neuron can be represented as:\n|∆L(hi)| = |L(H,hi = 0)− L(H,hi)|, (2)\nwhereL(H,hi = 0) is the loss value if the neuron i is pruned and L(H,hi) is the loss if it is not pruned. For the function L(H,hi), its Taylor Expansion at point hi = a is:\nL(H,hi) = N∑\nn=0\nLn(H, a) n! (hi − a)n +RN (hi),\n(3) where Ln(H, a) is the n-th derivative of L(H,hi) evaluated at point a and RN (hi) is N -th remainder. Then, approximating L(H,hi = 0) with a firstorder Taylor polynomial where hi equals zero:\nL(H,hi = 0) = L(H,hi)− ∂L(H,hi)\n∂hi hi−R1(hi).\n(4) The remainder R1 can be represented in the form of Lagrange:\nR1(hi) = ∂2L(H,hi) ∂2δhi h2i , (5)\nwhere δ ∈ (0, 1). Considering the use of ReLU activation function (Glorot et al., 2011) in the model, the first derivative of loss function tends to be constant, so the second order term tends to be zero in the end of training. Thus, we can ignore the remainder and get the importance evaluation function as follows:\nΘTE(i) = |∆L(hi)| = ∣∣∣∣∂L(H,hi)∂hi hi ∣∣∣∣ . (6) In practice, we need to accumulate the product of the activation and the gradient of the objective function w.r.t to the activation, which is easily computed during back-propagation. Finally, the evaluation function is shown as:\nΘmTE(i l) =\n1\nTm ∑ t ∣∣∣∣δL(H,hli)δhli hli ∣∣∣∣ , (7)\nwhere hli is the activation value of the i-th neuron of l-th layer and Tm is the number of the training examples of language pair m. The criterion is computed on the data of language pair m and averaged over Tm.\nAbsolute Value We adopt the magnitude-based neuron importance evaluation scheme (See et al., 2016), where the absolute value of each neuron’s activation value is treated as the importance:\nΘmAV(i l) =\n1\nTm ∑ t |hli|. (8)\nThe notations in the above equation are the same as those in the Equation 7. After the importance of each neuron is evaluated on the combined data, we need to determine the role of each neuron in the fine-tuning step following the method in the next section."
    }, {
      "heading" : "3.2 Neuron Allocation",
      "text" : "In this step, we should determine which neurons are shared across all the language pairs and which neurons are shared only for some specific language pairs.\nGeneral Neurons According to the overall importance I(i) in Equation 1, the value correlates positively with how important the neuron is to all languages. Therefore, we rank the neurons in each layer based on the importance and make the top ρ percentage as general neurons that are responsible for capturing general knowledge.\nLanguage-specific Neurons Next, we regard other neurons except for the general neurons as the language-specific neurons and determine which language pair to assign them to. To achieve this, we compute an importance threshold for each neuron:\nλ(i) = k ×max(Θm(i)), m ∈ {1, . . . ,M}, k ∈ [0, 1]\n(9)\n, where max(Θm(i)) denotes the maximum importance of this neuron in all language pairs and k is a hyper-parameter. The neuron will be assigned to the language-pairs whose importance is larger than the threshold. When the importance of neurons is determined, the number of language pairs associated with each neuron can be adjusted according to k. The smaller the k, the more language-pairs will be associated with the specific neurons. In this way, we flexibly determine the language pairs assigned to each neuron according to its importance in different languages. Note that the neuron allocation is based on the importance of language pair. We have also tried other allocation variants, e.g., based on the source language, target language, and find that the language pair-based method is the best among of these methods. The detailed results are listed in Appendix A.\nAfter this step, the model is continually finetuned on the combined multilingual data. If the training data is from a specific language pair, only the general neurons and the language-specific neurons for this language pair will participate in the forward computation and the parameters associated with them will be updated during the backward propagation."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Data Preparation",
      "text" : "In this section, we describe the datasets using in our experiments on many-to-many and one-to-many multilingual translation scenarios.\nMany-to-Many For this translation scenario, we test our approach on IWSLT-171 translation datasets, including English, Italian, Romanian, Dutch (briefly, En, It, Ro, Nl). We experimented in eight directions, including It↔En, Ro↔En, Nl↔En, and It↔Ro, with 231.6k, 220.5k, 237.2k, and 217.5k data for each language pair. We choose test2016 and test2017 as our development and test set, respectively. Sentences of all languages were tokenized by the Moses scripts2 and further segmented into subword symbols using Byte-Pair Encoding (BPE) rules (Sennrich et al., 2016) with 40K merge operations for all languages jointly.\nOne-to-Many We evaluate the quality of our multilingual translation models using training data from the Europarl Corpus3, Release V7. Our experiments focus on English to twelve primary languages: Czech, Finnish, Greek, Hungarian, Lithuanian, Latvian, Polish, Portuguese, Slovak, Slovene, Swedish, Spanish (briefly, Cs, Fi, El, Hu, Lt, Lv, Pl, Pt, Sk, Sl, Sv, Es). For each language pair, we randomly sampled 0.6M parallel sentences as training corpus (7.2M in all). The Europarl evaluation data set dev2006 is used as our validation set, while devtest2006 is our test set. For language pairs without available development and test set, we randomly split 1K unseen sentence pairs from the corresponding training set as the development and test data respectively. We tokenize and truecase the sentences with Moses scripts and apply a jointly-learned set of 90k BPE obtained from the merged source and target sides of the training data for all twelve language pairs."
    }, {
      "heading" : "4.2 Systems",
      "text" : "To make the evaluation convincing, we reimplement and compare our method with four baseline systems, which can be divided into two categories with respect to the number of models. The multiple-model approach requires maintaining a dedicated NMT model for each language:\n1https://sites.google.com/site/iwsltevaluation2017 2http://www.statmt.org/moses/ 3http://www.statmt.org/europarl/\nIndividual A NMT model is trained for each language pair. Therefore, there are N different models for N language pairs.\nThe unified model-based methods handle multiple languages within a single unified NMT model:\nMultilingual (Johnson et al., 2017) Handling multiple languages in a single transformer model which contains one encoder and one decoder with a special language indicator lang added to the input sentence.\n+TS (Blackwood et al., 2018) This method assigns language-specific attention modules to each language pair. We implement the target-specific attention mechanism because of its excellent performance in the original paper.\n+Adapter (Bapna and Firat, 2019) This method injects tiny adapter layers for specific language pairs into the original MNMT model. We set the dimension of projection layer to 128 and train the model from scratch.\nOur Method-AV Our model is trained just as the Approach section describes. In this system, we adopt the absolute value based method to evaluate the importance of neurons across languages.\nOur Method-TE This system is implemented the same as the system Our Method-AV except that we adopt the Taylor Expansion based evaluation method as shown in Equation 7.\n+Expansion To make a fair comparison, we set the size of Feed Forward Network to 3000 to expand the model capacity up to the level of other\nbaselines, and then apply our Taylor Expansion based method to this model."
    }, {
      "heading" : "4.3 Details",
      "text" : "For fair comparisons, we implement the proposed method and other contrast methods on the advanced Transformer model using the open-source toolkit Fairseq-py (Ott et al., 2019). We follow Vaswani et al. (2017) to set the configurations of the NMT model, which consists of 6 stacked encoder/decoder layers with the layer size being 512. All the models were trained on 4 NVIDIA 2080Ti GPUs where each was allocated with a batch size of 4,096 tokens for one-to-many scenario and 2,048 tokens for the many-to-many scenario. We train the baseline model using Adam optimizer (Kingma and Ba, 2015) with β1 = 0.9, β2 = 0.98, and = 10−9. The proposed models are further trained with corresponding parameters initialized by the pre-trained baseline model. We vary the hyperparameter ρ that controls the proportion of general neurons in each module from 80% to 95% and set it to 90% in our main experiments according to the performance. The detailed results about this hyper-parameter are list in Appendix B. We set the hyper-parameter k to 0.7 and do more analysis on it in Section 5.3. For evaluation, we use beam search with a beam size of 4 and length penalty α = 0.6."
    }, {
      "heading" : "4.4 Results",
      "text" : "The final translation is detokenized and then the quality is evaluated using the 4-gram case-sensitive\nBLEU (Papineni et al., 2002) with the SacreBLEU tool (Post, 2018).4\nMany-to-Many The results are given in Table 1. We can see that the improvements brought by +TS and +Adapter methods are not large. For the +TS method, attention module may be not essential to capture language-specific knowledge, and thus it is difficult to converge to good optima. For the +Adapter method, adding an adapter module to the end of each layer may be not appropriate for some languages and hence has a loose capture to the specific features. In all language pairs, our method based on Taylor Expansion outperforms all the baselines in the datasets. Moreover, the parameters in our model are the same as the Multilingual system and less than other baselines.\nOne-to-Many The results are given in Table 2, our method exceeds the multilingual baseline in all language pairs and outperforms other baselines in most language pairs without capacity increment. When we expand the model capacity to the level of +Adapter, our approach can achieve better translation performance, which demonstrates the effectiveness of our method. Another finding is that the results of the individual baseline are worse than other baselines. The reason may be the training data is not big enough, individual baseline can not get a good enough optimization on 0.6M sentences, while the MNMT model can be well trained with a total of 7.2M data."
    }, {
      "heading" : "5 Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Neuron Importance for Different languages",
      "text" : "In our method, we allocate neurons based on their importance for different languages. The rationality behind this mechanism is that different neurons should have distinct importance values so that these neurons can find their relevant language pairs. Therefore, we show the importance of neurons computed by Taylor Expansion in different modules for the one-to-many (O2M) and many-to-many (M2M) translation tasks. For clarity and convenience, we only show the importance values of three language pairs in the sixth layer of encoder and decoder.\nThe results of O2M are shown in Figure 2(a) and Figure 2(b), and the language pairs are En→Es, En→Pt, and En→Fi. The first two target languages\n4BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a +version.1.4.14\nare Spanish and Portuguese, both of which belong to the Western Romance, the Romance branch of the Indo-European family, while the last one is Finnish, a member of the Finnish-Ugric branch of the Ural family. As we can see, the importance of Spanish and Portuguese are always similar in most neurons, but there is no obvious correlation between Finnish and the other two languages. It indicates that similar languages are also similar in the distribution of the neuron importance, which implies that the common features in similar languages can be captured by the same neurons.\nThe results of M2M are shown in Figure 2(c) and Figure 2(d), and the language pairs are It→En, Ro→It, and En→Ro, whose BLEU scores are 0.67, 1, and 1.7 higher than the multilingual baseline, respectively. In most neurons, the highest importance value is twice as high as the lowest and this high variance of importance provides the theoretical basis for later neuron allocation. Moreover, we can see a lot of importance peaks of the two language pairs: Ro→It and En→Ro, which means that these neurons are especially important for generating the translation for these language pairs. However, the fluctuation of It→En is flat with almost no peaks, which means only a few neurons are specific to this language pair. This may be the reason why some language pairs have higher improvements, while some have lower improvements."
    }, {
      "heading" : "5.2 Distribution of the Language-specific Neurons",
      "text" : "Except for the general neurons shared by all the language pairs, our method allocates other neurons to different language pairs based on their importance. These language-specific neurons are important for preserving the language-specific knowledge. To better understand the effectiveness of our method, we will show how these specific neurons are distributed in the model.\nTo evaluate the proportion of language-specific neurons for different language pairs at each layer, we introduce a new metric, LScore, formulated as:\nLScore(l,m) = Ĩml Ĩl ,m ∈ {1, . . . ,M} (10)\nwhere Ĩml denotes the number of neurons allocated to language pair m in the l-th layer, and Ĩl denotes the total number of the language-specific neurons in the l-th layer. The larger the LScore, the more neurons allocated to the language pair m. We also\nintroduce a metric to evaluate the average proportion of language-specific neurons of each language in different modules, which formulated as:\nMScore(l, f) = 1\nM M∑ m=0 Ĩml,f Ĩl,f ,m ∈ {1, . . . ,M}\n(11) where Ĩml,f denotes the number of specific neurons for language pair m of in the f module of the lth layer and M denotes the total number of the language pair. The larger the MScore is, the more specific neurons are allocated to different language pairs in this module.\nAs shown in Figure 3(a) and Figure 3(b), the language pairs have low LScores at the top and bottom layers and high LScores at the middle layers of both the encoder and decoder. The highest LScore appears at the third or fourth layers, which indicates that the neuron importance of different language pairs is similar and the neurons of the middle layers are shared by more languages. As a contrast, the bottom and top layers will be more specialized for different language pairs. Next, from Figure 3(c) and Figure 3(d), we can see the MScores of the attention modules are almost near 1.0, which means neurons in self attention and cross attention are almost shared across all language pairs. However, the MScores of Feed Forward Network (FFN) gradually decrease as layer depth increases and it shows that the higher layers in FFN are more essential for capturing the language-specific knowledge."
    }, {
      "heading" : "5.3 Effects of the Hyper-parameter k",
      "text" : "When the importance of neurons for different languages is determined, the number of language pairs associated with each neuron can be adjusted ac-\ncording to k. When k = 1.0, the threshold is max(Θm(i)) as computed by Equation 9, so the neurons will only be allocated to the language pair with the highest importance, and when k = 0, the threshold is 0 so the neurons will be shared across all language pairs just like the Multilingual baseline. To better show the overall impact of the hyperparameter k, we vary it from 0 to 1 and the results are shown in Figure 4. As we can see, the translation performance of the two proposed approaches increases with the increment of k and reach the best performance when k equals 0.7. As k continues to increase, the performance deteriorates, which indicates that the over-specific neurons are bad at capturing the common features shared by similar languages and will lead to performance degradation."
    }, {
      "heading" : "5.4 The Specific and General knowledge",
      "text" : "The main idea of our method is to let the general knowledge and the language-specific knowledge be captured by different neurons of our method. To verify whether this goal has been achieved, we conduct the following experiments. For the general knowledge, we randomly erase 20% general neurons of the best checkpoint of our method, which means we mask the output value of these neurons to 0, then generate translation using it. For languagespecific knowledge, we randomly erase 50% specific neurons and then generate translation.\nAs shown in Figure 5, when the general neurons are erased, the BLEU points of all the language pairs drop a lot (about 15 to 20 BLEU), which indicates general neurons do capture the general knowledge across languages. For specific neurons,\nwe show three language pairs for the sake of convenience. We can see that when the neurons associated with the current language pair are erased, the performance of this language pair decreases greatly. However, the performance of other language pairs only declines slightly, because the specific knowledge captured by these specific neurons are not so important for other languages."
    }, {
      "heading" : "6 Related Work",
      "text" : "Our work closely relates to language-specific modeling for MNMT and model pruning which we will recap both here. Early MNMT studies focus on improving the sharing capability of individual bilingual models to handle multiple languages, which includes sharing encoders (Dong et al., 2015), sharing decoders (Zoph et al., 2016), and sharing sublayers (Firat et al., 2016). Later, Ha et al. (2016) and Johnson et al. (2017) propose an universal MNMT model with a target language token to indicate the translation direction. While this paradigm fully explores the general knowledge between languages and hard to obtain the specific knowledge of each language (Tan et al., 2019; Aharoni et al., 2019), the subsequent researches resort to Language-specific modeling, trying to find a better trade-off between sharing and specific. Such approaches involve inserting conditional languagespecific routing layer (Zhang et al., 2021), specific attention networks (Blackwood et al., 2018; Sachan and Neubig, 2018), adding task adapters (Bapna and Firat, 2019), and training model with different language clusters (Tan et al., 2019), and so on. However, these methods increase the capacity of the model which makes the model bloated.\nMoreover, our method is also related to model pruning, which usually aims to reduce the model size or improve the inference efficiency. Model pruning has been widely investigated for both computer vision (CV) (Luo et al., 2017) and natural language processing (NLP) tasks. For example, See et al. (2016) examines three magnitude-based pruning schemes, Zhu and Gupta (2018) demonstrates that large-sparse models outperform comparablysized small-dense models, and Wang et al. (2020a) improves the utilization efficiency of parameters by introducing a rejuvenation approach. Besides, Lan et al. (2020) presents two parameter reduction techniques to lower memory consumption and increase the training speed of BERT."
    }, {
      "heading" : "7 Conclusion",
      "text" : "The current standard models of multilingual neural machine translation fail to capture the characteristics of specific languages, while the latest researches focus on the pursuit of specific knowledge while increasing the capacity of the model and requiring fine manual design. To solve the problem, we propose an importance-based neuron allocation method. We divide neurons to general neurons and language-specific neurons to retain general knowledge and capture language-specific knowledge without model capacity incremental and specialized design. The experiments prove that our method can get superior translation results with better general and language-specific knowledge."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank all the anonymous reviewers for their insightful and valuable comments. This work was supported by National Key R&D Program of China (NO. 2017YFE0192900)."
    }, {
      "heading" : "A Performance on Different Varieties",
      "text" : "In the proposed method we allocate neurons based on importance of language pair. There are three varieties of our method: (a) Source-Specific, share all neurons according to the source language only; (b) Target-Specific, share all neurons according to the target language only; (c) Separate Enc-Dec, Encoder neurons are shared according to the source language and decoder neurons are shared according to the target language. Note that (c) is different from our method since (c) is separate neurons to two parts (encoder and decoder) and then connect specific neurons of the two parts to form a whole, while our method is directly based on language pairs.\nAs shown in Figure 6, we compare our Taylor Expansion method with the other three varieties. Our approach outperforms other varieties on almost all language pairs, and the performance of the language-pair based approach is undoubtedly the best. The second is based on the target language and the source language. Worst of all are the separated encoder-decoder, which may be due to the mismatch between the neurons of the encoder and decoder when they are reconnected."
    }, {
      "heading" : "B Effects of the Hyper-parameter ρ",
      "text" : "We conducted several experiments on ρ to determine the optimal hyper-parameter, so as to determine the proportion of universal neurons. As shown in Table 3, when ρ = 90% the model gets the best translation result and reach best trade-off between general and language-specific neurons."
    } ],
    "references" : [ {
      "title" : "Massively multilingual neural machine translation",
      "author" : [ "Roee Aharoni", "Melvin Johnson", "Orhan Firat." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
      "citeRegEx" : "Aharoni et al\\.,? 2019",
      "shortCiteRegEx" : "Aharoni et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Simple, scalable adaptation for neural machine translation",
      "author" : [ "Ankur Bapna", "Orhan Firat." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Bapna and Firat.,? 2019",
      "shortCiteRegEx" : "Bapna and Firat.",
      "year" : 2019
    }, {
      "title" : "Multilingual neural machine translation with task-specific attention",
      "author" : [ "Graeme W. Blackwood", "Miguel Ballesteros", "Todd Ward." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, COLING 2018, Santa Fe, New Mexico,",
      "citeRegEx" : "Blackwood et al\\.,? 2018",
      "shortCiteRegEx" : "Blackwood et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-task learning for multiple language translation",
      "author" : [ "Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
      "citeRegEx" : "Dong et al\\.,? 2015",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2015
    }, {
      "title" : "Multilingual machine translation: Closing the gap between shared and language-specific encoderdecoders",
      "author" : [ "Carlos Escolano", "Marta R. Costa-jussà", "José A.R. Fonollosa", "Mikel Artetxe." ],
      "venue" : "CoRR, abs/2004.06575.",
      "citeRegEx" : "Escolano et al\\.,? 2020",
      "shortCiteRegEx" : "Escolano et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-way, multilingual neural machine translation with a shared attention mechanism",
      "author" : [ "Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Firat et al\\.,? 2016",
      "shortCiteRegEx" : "Firat et al\\.",
      "year" : 2016
    }, {
      "title" : "Convolutional sequence to sequence learning",
      "author" : [ "Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N. Dauphin." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia,",
      "citeRegEx" : "Gehring et al\\.,? 2017",
      "shortCiteRegEx" : "Gehring et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep sparse rectifier neural networks",
      "author" : [ "Xavier Glorot", "Antoine Bordes", "Yoshua Bengio." ],
      "venue" : "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2011, Fort Lauderdale, USA, April 11-13, 2011,",
      "citeRegEx" : "Glorot et al\\.,? 2011",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2011
    }, {
      "title" : "Universal neural machine translation for extremely low resource languages",
      "author" : [ "Jiatao Gu", "Hany Hassan", "Jacob Devlin", "Victor O.K. Li." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Gu et al\\.,? 2018",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2018
    }, {
      "title" : "Toward multilingual neural machine translation with universal encoder and decoder",
      "author" : [ "Thanh-Le Ha", "Jan Niehues", "Alexander H. Waibel." ],
      "venue" : "CoRR, abs/1611.04798.",
      "citeRegEx" : "Ha et al\\.,? 2016",
      "shortCiteRegEx" : "Ha et al\\.",
      "year" : 2016
    }, {
      "title" : "Google’s multilingual neural machine translation system: Enabling zero-shot translation",
      "author" : [ "Corrado", "Macduff Hughes", "Jeffrey Dean." ],
      "venue" : "TACL, 5:339–351.",
      "citeRegEx" : "Corrado et al\\.,? 2017",
      "shortCiteRegEx" : "Corrado et al\\.",
      "year" : 2017
    }, {
      "title" : "Recurrent continuous translation models",
      "author" : [ "Nal Kalchbrenner", "Phil Blunsom." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washing-",
      "citeRegEx" : "Kalchbrenner and Blunsom.,? 2013",
      "shortCiteRegEx" : "Kalchbrenner and Blunsom.",
      "year" : 2013
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "ALBERT: A lite BERT for self-supervised learning of language representations",
      "author" : [ "Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman", "Kevin Gimpel", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "8th International Conference on Learning Representations,",
      "citeRegEx" : "Lan et al\\.,? 2020",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2020
    }, {
      "title" : "Thinet: A filter level pruning method for deep neural network compression",
      "author" : [ "Jian-Hao Luo", "Jianxin Wu", "Weiyao Lin." ],
      "venue" : "IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 5068–5076. IEEE",
      "citeRegEx" : "Luo et al\\.,? 2017",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2017
    }, {
      "title" : "Pruning convolutional neural networks for resource efficient inference",
      "author" : [ "Pavlo Molchanov", "Stephen Tyree", "Tero Karras", "Timo Aila", "Jan Kautz." ],
      "venue" : "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-",
      "citeRegEx" : "Molchanov et al\\.,? 2017",
      "shortCiteRegEx" : "Molchanov et al\\.",
      "year" : 2017
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of NAACL-HLT 2019: Demonstrations.",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "A call for clarity in reporting BLEU scores",
      "author" : [ "Matt Post." ],
      "venue" : "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186– 191, Brussels, Belgium. Association for Computational Linguistics.",
      "citeRegEx" : "Post.,? 2018",
      "shortCiteRegEx" : "Post.",
      "year" : 2018
    }, {
      "title" : "Parameter sharing methods for multilingual selfattentional translation models",
      "author" : [ "Devendra Singh Sachan", "Graham Neubig." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Sachan and Neubig.,? 2018",
      "shortCiteRegEx" : "Sachan and Neubig.",
      "year" : 2018
    }, {
      "title" : "Compression of neural machine translation models via pruning",
      "author" : [ "Abigail See", "Minh-Thang Luong", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL 2016, Berlin, Ger-",
      "citeRegEx" : "See et al\\.,? 2016",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Ger-",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le." ],
      "venue" : "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014,",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Multilingual neural machine translation with language clustering",
      "author" : [ "Xu Tan", "Jiale Chen", "Di He", "Yingce Xia", "Tao Qin", "Tie-Yan Liu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna-",
      "citeRegEx" : "Tan et al\\.,? 2019",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Multilingual NMT with a language-independent attention bridge",
      "author" : [ "Raúl Vázquez", "Alessandro Raganato", "Jörg Tiedemann", "Mathias Creutz." ],
      "venue" : "Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 33–39, Flo-",
      "citeRegEx" : "Vázquez et al\\.,? 2019",
      "shortCiteRegEx" : "Vázquez et al\\.",
      "year" : 2019
    }, {
      "title" : "On the sparsity of neural machine translation models",
      "author" : [ "Yong Wang", "Longyue Wang", "Victor O.K. Li", "Zhaopeng Tu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online,",
      "citeRegEx" : "Wang et al\\.,? 2020a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Go from the general to the particular: Multi-domain translation with domain transformation networks",
      "author" : [ "Yong Wang", "Longyue Wang", "Shuming Shi", "Victor O.K. Li", "Zhaopeng Tu." ],
      "venue" : "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI",
      "citeRegEx" : "Wang et al\\.,? 2020b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Share or not? learning to schedule language-specific capacity for multilingual translation",
      "author" : [ "Biao Zhang", "Ankur Bapna", "Rico Sennrich", "Orhan Firat." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "To prune, or not to prune: Exploring the efficacy of pruning for model compression",
      "author" : [ "Michael Zhu", "Suyog Gupta." ],
      "venue" : "6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Work-",
      "citeRegEx" : "Zhu and Gupta.,? 2018",
      "shortCiteRegEx" : "Zhu and Gupta.",
      "year" : 2018
    }, {
      "title" : "Transfer learning for low-resource neural machine translation",
      "author" : [ "Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas,",
      "citeRegEx" : "Zoph et al\\.,? 2016",
      "shortCiteRegEx" : "Zoph et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Neural machine translation(NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) has shown its superiority and drawn much attention in recent years.",
      "startOffset" : 32,
      "endOffset" : 155
    }, {
      "referenceID" : 23,
      "context" : "Neural machine translation(NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) has shown its superiority and drawn much attention in recent years.",
      "startOffset" : 32,
      "endOffset" : 155
    }, {
      "referenceID" : 1,
      "context" : "Neural machine translation(NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) has shown its superiority and drawn much attention in recent years.",
      "startOffset" : 32,
      "endOffset" : 155
    }, {
      "referenceID" : 7,
      "context" : "Neural machine translation(NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) has shown its superiority and drawn much attention in recent years.",
      "startOffset" : 32,
      "endOffset" : 155
    }, {
      "referenceID" : 25,
      "context" : "Neural machine translation(NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) has shown its superiority and drawn much attention in recent years.",
      "startOffset" : 32,
      "endOffset" : 155
    }, {
      "referenceID" : 24,
      "context" : "Although the NMT model can achieve promising results for highresource language pairs, it is unaffordable to train separate models for all the language pairs since there are thousands of languages in the world (Tan et al., 2019; Aharoni et al., 2019; Arivazhagan et al., 2019).",
      "startOffset" : 209,
      "endOffset" : 275
    }, {
      "referenceID" : 0,
      "context" : "Although the NMT model can achieve promising results for highresource language pairs, it is unaffordable to train separate models for all the language pairs since there are thousands of languages in the world (Tan et al., 2019; Aharoni et al., 2019; Arivazhagan et al., 2019).",
      "startOffset" : 209,
      "endOffset" : 275
    }, {
      "referenceID" : 10,
      "context" : "com/ictnlp/NAMNMT and the training cost is to handle multiple languages in a single multilingual neural machine translation (MNMT) model (Ha et al., 2016; Firat et al., 2016; Johnson et al., 2017; Gu et al., 2018).",
      "startOffset" : 137,
      "endOffset" : 213
    }, {
      "referenceID" : 6,
      "context" : "com/ictnlp/NAMNMT and the training cost is to handle multiple languages in a single multilingual neural machine translation (MNMT) model (Ha et al., 2016; Firat et al., 2016; Johnson et al., 2017; Gu et al., 2018).",
      "startOffset" : 137,
      "endOffset" : 213
    }, {
      "referenceID" : 9,
      "context" : "com/ictnlp/NAMNMT and the training cost is to handle multiple languages in a single multilingual neural machine translation (MNMT) model (Ha et al., 2016; Firat et al., 2016; Johnson et al., 2017; Gu et al., 2018).",
      "startOffset" : 137,
      "endOffset" : 213
    }, {
      "referenceID" : 3,
      "context" : "ule (Blackwood et al., 2018), decoupled multilingual encoders and/or decoders (Vázquez et al.",
      "startOffset" : 4,
      "endOffset" : 28
    }, {
      "referenceID" : 26,
      "context" : ", 2018), decoupled multilingual encoders and/or decoders (Vázquez et al., 2019; Escolano et al., 2020) and the lightweight language adapters (Bapna and Firat, 2019).",
      "startOffset" : 57,
      "endOffset" : 102
    }, {
      "referenceID" : 5,
      "context" : ", 2018), decoupled multilingual encoders and/or decoders (Vázquez et al., 2019; Escolano et al., 2020) and the lightweight language adapters (Bapna and Firat, 2019).",
      "startOffset" : 57,
      "endOffset" : 102
    }, {
      "referenceID" : 2,
      "context" : ", 2020) and the lightweight language adapters (Bapna and Firat, 2019).",
      "startOffset" : 46,
      "endOffset" : 69
    }, {
      "referenceID" : 25,
      "context" : "In this section, we will give a brief introduction to the Transformer model (Vaswani et al., 2017) and the Multilingual translation.",
      "startOffset" : 76,
      "endOffset" : 98
    }, {
      "referenceID" : 16,
      "context" : "on the Taylor Expansion (Molchanov et al., 2017), where we directly approximate the change in loss when removing a particular neuron.",
      "startOffset" : 24,
      "endOffset" : 48
    }, {
      "referenceID" : 8,
      "context" : "Considering the use of ReLU activation function (Glorot et al., 2011) in the model, the first derivative of loss function tends to be con-",
      "startOffset" : 48,
      "endOffset" : 69
    }, {
      "referenceID" : 21,
      "context" : "Absolute Value We adopt the magnitude-based neuron importance evaluation scheme (See et al., 2016), where the absolute value of each neuron’s activation value is treated as the importance:",
      "startOffset" : 80,
      "endOffset" : 98
    }, {
      "referenceID" : 22,
      "context" : "Sentences of all languages were tokenized by the Moses scripts2 and further segmented into subword symbols using Byte-Pair Encoding (BPE) rules (Sennrich et al., 2016) with 40K merge operations for all languages jointly.",
      "startOffset" : 144,
      "endOffset" : 167
    }, {
      "referenceID" : 3,
      "context" : "+TS (Blackwood et al., 2018) This method assigns language-specific attention modules to each language pair.",
      "startOffset" : 4,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "+Adapter (Bapna and Firat, 2019) This method injects tiny adapter layers for specific language pairs into the original MNMT model.",
      "startOffset" : 9,
      "endOffset" : 32
    }, {
      "referenceID" : 17,
      "context" : "For fair comparisons, we implement the proposed method and other contrast methods on the advanced Transformer model using the open-source toolkit Fairseq-py (Ott et al., 2019).",
      "startOffset" : 157,
      "endOffset" : 175
    }, {
      "referenceID" : 13,
      "context" : "We train the baseline model using Adam optimizer (Kingma and Ba, 2015) with β1 = 0.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 18,
      "context" : "BLEU (Papineni et al., 2002) with the SacreBLEU tool (Post, 2018).",
      "startOffset" : 5,
      "endOffset" : 28
    }, {
      "referenceID" : 4,
      "context" : "cus on improving the sharing capability of individual bilingual models to handle multiple languages, which includes sharing encoders (Dong et al., 2015), sharing decoders (Zoph et al.",
      "startOffset" : 133,
      "endOffset" : 152
    }, {
      "referenceID" : 31,
      "context" : ", 2015), sharing decoders (Zoph et al., 2016), and sharing sublayers (Firat et al.",
      "startOffset" : 26,
      "endOffset" : 45
    }, {
      "referenceID" : 24,
      "context" : "knowledge of each language (Tan et al., 2019; Aharoni et al., 2019), the subsequent researches resort to Language-specific modeling, trying to find a better trade-off between sharing and specific.",
      "startOffset" : 27,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "knowledge of each language (Tan et al., 2019; Aharoni et al., 2019), the subsequent researches resort to Language-specific modeling, trying to find a better trade-off between sharing and specific.",
      "startOffset" : 27,
      "endOffset" : 67
    }, {
      "referenceID" : 29,
      "context" : "specific routing layer (Zhang et al., 2021), specific attention networks (Blackwood et al.",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 3,
      "context" : ", 2021), specific attention networks (Blackwood et al., 2018; Sachan and Neubig, 2018), adding task adapters (Bapna and Firat, 2019), and training model with different language clusters (Tan et al.",
      "startOffset" : 37,
      "endOffset" : 86
    }, {
      "referenceID" : 20,
      "context" : ", 2021), specific attention networks (Blackwood et al., 2018; Sachan and Neubig, 2018), adding task adapters (Bapna and Firat, 2019), and training model with different language clusters (Tan et al.",
      "startOffset" : 37,
      "endOffset" : 86
    }, {
      "referenceID" : 2,
      "context" : ", 2018; Sachan and Neubig, 2018), adding task adapters (Bapna and Firat, 2019), and training model with different language clusters (Tan et al.",
      "startOffset" : 55,
      "endOffset" : 78
    }, {
      "referenceID" : 24,
      "context" : ", 2018; Sachan and Neubig, 2018), adding task adapters (Bapna and Firat, 2019), and training model with different language clusters (Tan et al., 2019), and so on.",
      "startOffset" : 132,
      "endOffset" : 150
    }, {
      "referenceID" : 15,
      "context" : "Model pruning has been widely investigated for both computer vision (CV) (Luo et al., 2017) and natural language processing (NLP) tasks.",
      "startOffset" : 73,
      "endOffset" : 91
    } ],
    "year" : 2021,
    "abstractText" : "Multilingual neural machine translation with a single model has drawn much attention due to its capability to deal with multiple languages. However, the current multilingual translation paradigm often makes the model tend to preserve the general knowledge, but ignore the language-specific knowledge. Some previous works try to solve this problem by adding various kinds of language-specific modules to the model, but they suffer from the parameter explosion problem and require specialized manual design. To solve these problems, we propose to divide the model neurons into general and language-specific parts based on their importance across languages. The general part is responsible for preserving the general knowledge and participating in the translation of all the languages, while the language-specific part is responsible for preserving the languagespecific knowledge and participating in the translation of some specific languages. Experimental results on several language pairs, covering IWSLT and Europarl corpus datasets, demonstrate the effectiveness and universality of the proposed method.",
    "creator" : "LaTeX with hyperref"
  }
}