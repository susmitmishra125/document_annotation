{
  "name" : "2021.acl-long.296.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Sweet Rabbit Hole by DARCY: Using Honeypots to Detect Universal Trigger's Adversarial Attacks",
    "authors" : [ "Thai Le", "Noseong Park", "Dongwon Lee" ],
    "emails" : [ "thaile@psu.edu", "noseong@yonsei.ac.kr", "dongwon@psu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3831–3844\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3831"
    }, {
      "heading" : "1 Introduction",
      "text" : "Adversarial examples in NLP refer to carefully crafted texts that can fool predictive machine learning (ML) models. Thus, malicious actors, i.e., attackers, can exploit such adversarial examples to force ML models to output desired predictions. There are several adversarial example generation algorithms, most of which perturb an original text at either character (e.g., (Li et al., 2018; Gao et al., 2018)), word (e.g., (Ebrahimi et al., 2018; Jin et al.; Wallace et al., 2019; Gao et al., 2018; Garg and Ramakrishnan, 2020), or sentence level (e.g., (Le et al., 2020; Gan and Ng; Cheng et al.)).\nBecause most of the existing attack methods are instance-based search methods, i.e., searching an adversarial example for each specific input, they do not usually involve any learning mechanisms. A few learning-based algorithms, such as the Universal Trigger (UniTrigger) (Wallace et al., 2019), MALCOM (Le et al., 2020), Seq2Sick (Cheng et al.) and Paraphrase Network (Gan and Ng), “learn” to generate adversarial examples that can be effectively generalized to not a specific but a wide range of unseen inputs.\nIn general, learning-based attacks are more attractive to attackers for several reasons. First, they achieve high attack success rates. For example, UniTrigger can drop the prediction accuracy of an NN model to near zero just by appending a learned adversarial phrase of only two tokens to any inputs (Tables 1 and 2). This is achieved through an optimization process over an entire dataset, exploiting potential weak points of a model as a whole, not aiming at any specific inputs. Second, their attack mechanism is highly transferable among similar models. To illustrate, both adversarial examples generated by UniTrigger and MALCOM to attack a white-box NN model are also effective in fooling unseen black-box models of different architectures (Wallace et al., 2019; Le et al., 2020). Third, thanks to their generalization to unseen inputs, learningbased adversarial generation algorithms can facilitate mass attacks with significantly reduced computational cost compared to instance-based methods.\nTherefore, the task of defending learning-based attacks in NLP is critical. Thus, in this paper, we\npropose a novel approach, named as DARCY, to defend adversarial examples created by UniTrigger, a strong representative learning-based attack (see Sec. 2.2). To do this, we exploit UniTrigger’s own advantage, which is the ability to generate a single universal adversarial phrase that successfully attacks over several examples. Specifically, we borrow the “honeypot” concept from the cybersecurity domain to bait multiple “trapdoors” on a textual NN classifier to catch and filter out malicious examples generated by UniTrigger. In other words, we train a target NN model such that it offers great a incentive for its attackers to generate adversarial texts whose behaviors are pre-defined and intended by defenders. Our contributions are as follows:\n• To the best of our knowledge, this is the first work that utilizes the concept of “honeypot” from the cybersecurity domain in defending textual NN models against adversarial attacks. • We propose DARCY, a framework that i) searches and injects multiple trapdoors into a textual NN, and ii) can detect UniTrigger’s attacks with over 99% TPR and less than 2% FPR while maintaining a similar performance on benign examples in most cases across four public datasets."
    }, {
      "heading" : "2 Preliminary Analysis",
      "text" : ""
    }, {
      "heading" : "2.1 The Universal Trigger Attack",
      "text" : "Let F(x, θ), parameterized by θ, be a target NN that is trained on a dataset Dtrain ← {x,y}Ni with yi, drawn from a set C of class labels, is the groundtruth label of the text xi. F(x, θ) outputs a vector of size |C| with F(x)L predicting the probability of x belonging to class L. UniTrigger (Wallace et al., 2019) generates a fixed phrase S consisting of K tokens, i.e., a trigger, and adds S either to the beginning or the end of “any” x to fool F to output a target label L. To search for S, UniTrigger optimizes the following objective function on an attack dataset Dattack:\nminS LL = − ∑ i,yi 6=L log(f(S ⊕ xi, θ)L) (1)\nwhere ⊕ is a token-wise concatenation. To optimize Eq. (1), the attacker first initializes the trigger to be a neutral phrase (e.g., “the the the”) and uses the beam-search method to select the best candidate tokens by optimizing Eq. (1) on a mini-batch randomly sampled from Dattack. The top tokens are then initialized to find the next best ones until\nLL converges. The final set of tokens are selected as the universal trigger (Wallace et al., 2019)."
    }, {
      "heading" : "2.2 Attack Performance and Detection",
      "text" : "Table 2 shows the prediction accuracy of CNN (Kim, 2014) under different attacks on the MR (Pang and Lee, 2005) and SST (Wang et al., 2019a) datasets. Both datasets are class-balanced. We limit # of perturbed tokens per sentence to two. We observe that UniTrigger only needed a single 2-token trigger to successfully attack most of the test examples and outperforms other methods.\nAll those methods, including not only UniTrigger but also other attacks such as HotFlip (Ebrahimi et al., 2018), TextFooler (Jin et al.) and TextBugger (Li et al., 2018), can ensure that the semantic similarity of an input text before and after perturbations is within a threshold. Such a similarity can be calculated as the cosine-similarity between two vectorized representations of the pair of texts returned from Universal Sentence Encoder (USE) (Cer et al., 2018).\nHowever, even after we detect and remove adversarial examples using the same USE threshold applied to TextFooler and TextBugger, UniTrigger still drops the prediction accuracy of CNN to 28- 30%, which significantly outperforms other attack methods (Table 2). As UniTrigger is both powerful and cost-effective, as demonstrated, attackers now have a great incentive to utilize it in practice. Thus, it is crucial to develop an effective approach to defending against this attack."
    }, {
      "heading" : "3 Honeypot with Trapdoors",
      "text" : "To attack F , UniTrigger relies on Eq. (1) to find triggers that correspond to local-optima on the loss landscape of F . To safeguard F , we bait multiple optima on the loss landscape of F , i.e., honeypots, such that Eq. (1) can conveniently converge to one of them. Specifically, we inject different trapdoors (i.e., a set of pre-defined to-\nkens) into F using three steps: (1) searching trapdoors, (2) injecting trapdoors and (3) detecting trapdoors. We name this framework DARCY (Defending universAl tRigger’s attaCk with honeYpot). Fig. 1 illustrates an example of DARCY.\n3.1 The DARCY Framework STEP 1: Searching Trapdoors. To defend attacks on a target label L, we select K trapdoors S∗L = {w1, w2, ..., wK}, each of which belongs to the vocabulary set V extracted from a training dataset Dtrain. Let H(·) be a trapdoor selection function: S∗L ←− H(K,Dtrain, L). Fig. 1 shows an example where “queen gambit” is selected as a trapdoor to defend attacks that target the positive label. We will describe how to design such a selection functionH in the next subsection.\nSTEP 2: Injecting Trapdoors. To inject S∗L on F and allure attackers, we first populate a set of trapdoor-embedded examples as follows:\nDLtrap ←− {(S∗L⊕x, L) : (x,y) ∈ Dy 6=L}, (2)\nwhere Dy 6=L ←− {Dtrain : y 6= L}. Then, we can bait S∗L into F by training F together with all the injected examples of all target labels L ∈ C by minimizing the objective function:\nmin θ LF = LDtrainF + γL Dtrap F , (3)\nwhere Dtrap ←− {DLtrap|L ∈ C}, LDF is the Negative Log-Likelihood (NLL) loss of F on the dataset D. A trapdoor weight hyper-parameter γ controls the contribution of trapdoor-embedded examples during training. By optimizing Eq. (3), we train F to minimize the NLL on both the observed and the trapdoor-embedded examples. This generates “traps” or convenient convergence points (e.g., local optima) when attackers search for a set of triggers using Eq. (1). Moreover, we can also control the strength of the trapdoor. By synthesizing DLtrap with all examples from Dy 6=L (Eq. (2)), we want to inject “strong” trapdoors into the model. However, this might induce a trade-off on computational\noverhead associated with Eq. (3). Thus, we sample DLtrap based a trapdoor ratio hyper-parameter ← |DLtrap|/|Dy 6=L| to help control this trade-off.\nSTEP 3: Detecting Trapdoors. Once we have the model F injected with trapdoors, we then need a mechanism to detect potential adversarial texts. To do this, we train a binary classifier G(·), parameterized by θG , to predict the probability that x includes a universal trigger using the output from F’s last layer (denoted as F∗(x)) following G(x, θG) : F∗(x) 7→ [0, 1]. G is more preferable than a trivial string comparison because Eq. (1) can converge to not exactly but only a neighbor of S∗L. We train G(·) using the binary NLL loss:\nmin θG LG = ∑ x∈Dtrain x′∈Dtrap −log(G(x))− log(1−G(x′)).\n(4)"
    }, {
      "heading" : "3.2 Multiple Greedy Trapdoor Search",
      "text" : "Searching trapdoors is the most important step in our DARCY framework. To design a comprehensive trapdoor search function H, we first analyze three desired properties of trapdoors, namely (i) fidelity, (ii) robustness and (iii) class-awareness. Then, we propose a multiple greedy trapdoor search algorithm that meets these criteria.\nFidelity. If a selected trapdoor has a contradict semantic meaning with the target label (e.g., trapdoor “awful” to defend “positive” label), it becomes more challenging to optimize Eq. (3). Hence,H should select each token w ∈ S∗L to defend a target label L such that it locates as far as possible to other contrasting classes from L according to F’s decision boundary when appended to examples of Dy 6=L in Eq. (2). Specifically, we want to optimize the fidelity loss as follows.\nmin w∈S∗L\nLLfidelity = ∑\nx∈Dy 6=L ∑ L′ 6=L d(F∗(w ⊕ x),CFL′)\n(5)\nAlgorithm 1 Greedy Trapdoor Search 1: Input: Dtrain, V , K, α, β, γ, T 2: Output: {S∗L|L ∈ C} 3: Initialize: F , S∗ ←− {} 4: WARM UP(F , Dtrain) 5: for L in C do 6: OL← CENTROID(F , Dy=L) 7: end for 8: for i in [1..K] do 9: for L in C do 10: Q← Q∪NEIGHBOR(S∗L, α) 11: Q← Q\\NEIGHBOR({S∗L′ 6=L|L′ ∈ C}, β) 12: Cand← RANDOM SELECT(Q, T ) 13: dbest← 0,wbest← Cand[0] 14: for w in Cand do 15: Ww ← CENTROID(F , Dy 6=L) 16: d← ∑ L′ 6=L SIMILARITY(Ww,OL′) 17: if dbest ≥ d then 18: dbest ← d, wbest ← w 19: end if 20: end for 21: S∗L ← S∗L ∪ {wbest} 22: end for 23: end for 24: return {S∗L|L ∈ C}\nwhere d(·) is a similarity function (e.g., cosine similarity), CFL′ ←− 1 |DL′ | ∑ x∈DL′\nF∗(x) is the centroid of all outputs on the last layer of F when predicting examples of a contrastive class L′.\nRobustness to Varying Attacks. Even though a single strong trapdoor, i.e., one that can significantly reduce the loss of F , can work well in the original UniTrigger’s setting, an advanced attacker may detect the installed trapdoor and adapt a better attack approach. Hence, we suggest to search and embed multiple trapdoors (K ≥ 1) to F for defending each target label.\nd(ewi , ewj ) ≤ α ∀wi, wj ∈ S∗L, L ∈ C d(ewi , ewj ) ≥ β ∀wi ∈ S∗L, wj ∈ S∗Q 6=L, L,Q ∈ C\n(6)\nClass-Awareness. Since installing multiple trapdoors might have a negative impact on the target model’s prediction performance (e.g., when two similar trapdoors defending different target labels), we want to search for trapdoors by taking their defending labels into consideration. Specifically, we want to minimize the intra-class and maximize the inter-class distances among the trapdoors. Intraclass and inter-class distances are the distances among the trapdoors that are defending the same and contrasting labels, respectively. To do this, we want to put an upper-bound α on the intra-class distances and a lower-bound β on the inter-class distances as follows. Let ew denote the embedding\nof token w, then we have:\nObjective Function and Optimization. Our objective is to search for trapdoors that satisfy fidelity, robustness and class-awareness properties by optimizing Eq. (5) subject to Eq. (6) and K ≥ 1. We refer to Eq. (7) in the Appendix for the full objective function. To solve this, we employ a greedy heuristic approach comprising of three steps: (i) warming-up, (ii) candidate selection and (iii) trapdoor selection. Alg. 1 and Fig. 2 describe the algorithm in detail.\nThe first step (Ln.4) “warms up” F to be later queried by the third step by training it with only an epoch on the training set Dtrain. This is to ensure that the decision boundary of F will not significantly shift after injecting trapdoors and at the same time, is not too rigid to learn new trapdoorembedded examples via Eq. (3). While the second step (Ln.10–12, Fig. 2B) searches for candidate trapdoors to defend each label L ∈ C that satisfy the class-awareness property, the third one (Ln.14– 20, Fig. 2C) selects the best trapdoor token for each defending L from the found candidates to maximize F’s fidelity. To consider the robustness aspect, the previous two steps then repeat K ≥ 1 times (Ln.8–23). To reduce the computational cost, we randomly sample a small portion (T |V| tokens) of candidate trapdoors, found in the first step (Ln.12), as inputs to the second step.\nComputational Complexity. The complexity of Alg. (1) is dominated by the iterative process of Ln.8–23, which is O(K|C||V|log|V|) (T |V|). Given a fixed dataset, i.e., |C|, |V| are constant, our proposed trapdoor searching algorithm only scales linearly with K. This shows that there is a trade-\noff between the complexity and robustness of our defense method."
    }, {
      "heading" : "4 Experimental Validation",
      "text" : ""
    }, {
      "heading" : "4.1 Set-Up",
      "text" : "Datasets. Table A.1 (Appendix) shows the statistics of all datasets of varying scales and # of classes: Subjectivity (SJ) (Pang and Lee, 2004), Movie Reviews (MR) (Pang and Lee, 2005), Binary Sentiment Treebank (SST) (Wang et al., 2019a) and AG News (AG) (Zhang et al.). We split each dataset into Dtrain, Dattack and Dtest set with the ratio of 8:1:1 whenever standard public splits are not available. All datasets are relatively balanced across classes.\nAttack Scenarios and Settings. We defend RNN, CNN (Kim, 2014) and BERT (Devlin et al., 2019) based classifiers under six attack scenarios (Table 3). Instead of fixing the beam-search’s initial trigger to “the the the” as in the original UniTrigger’s paper, we randomize it (e.g., “gem queen shoe”) for each run. We report the average results onDtest over at least 3 iterations. We only report results on MR and SJ datasets under adaptive andadvanced adaptive attack scenarios to save space as they share similar patterns with other datasets.\nDetection Baselines. We compare DARCY with five adversarial detection algorithms below.\n• OOD Detection (OOD) (Smith and Gal, 2018) assumes that adversarial examples locate far away from the distribution of training examples, i.e., out-of-distribution (OOD). It then considers examples whose predictions have high uncertainty, i.e., high entropy, as adversarial examples. • Self Attack (SelfATK) uses UniTrigger to attack itself for several times and trains a network to\ndetect the generated triggers as adversarial texts. • Local Intrinsic Dimensionality (LID) (Ma et al.,\n2018) characterizes adversarial regions of a NN model using LID and uses this as a feature to detect adversarial examples. • Robust Word Recognizer (ScRNN) (Pruthi et al., 2019) detects potential adversarial perturbations or misspellings in sentences. • Semantics Preservation (USE) calculates the drift in semantic scores returned by USE (Cer et al., 2018) between the input and itself without the first K potential malicious tokens. • DARCY: We use two variants, namely DARCY(1) and DARCY(5) which search for a single trapdoor (K←1) and multiple trapdoors (K←5) to defend each label, respectively.\nEvaluation Metrics. We consider the following metrics. (1) Fidelity (Model F1): We report the F1 score of F’s prediction performance on clean unseen examples after being trained with trapdoors; (2) Detection Performance (Detection AUC): We report the AUC (Area Under the Curve) score on how well a method can distinguish between benign and adversarial examples; (3) True Positive Rate (TPR) and False Positive Rate (FPR): While TPR is the rate that an algorithm correctly identifies adversarial examples, FPT is the rate that such algorithm incorrectly detects benign inputs as adversarial examples. We desire a high Model F1, Detection AUC, TPR, and a low FPR."
    }, {
      "heading" : "4.2 Results",
      "text" : "Evaluation on Novice Attack. A novice attacker does not know the existence of trapdoors. Overall, table A.2 (Appendix) shows the full results. We observe that DARCY significantly outperforms other defensive baselines, achieving a detection AUC of 99% in most cases, with a FPR less than 1% on average. Also, DARCY observes a 0.34% improvement in average fidelity (model F1) thanks to the regularization effects from additional training data Dtrap. Among the baselines, SelfATK achieves a similar performance with DARCY in all except the\nMethod RNN BERT\nClean Detection Clean Detection\nF1 AUC FPR TPR F1 AUC FPR TPR OOD 75.2 52.5 45.9 55.7 84.7 35.6 63.9 48.2 ScRNN - 51.9 43.0 47.0 - 51.8 52.3 54.9 M USE - 62.9 48.1 75.9 - 53.1 55.1 64.1 R SelfATK - 92.3 0.6 85.1 - 97.5 4.1 95.2\nSST dataset with a detection AUC of around 75% on average (Fig. 3). This happens because there are much more artifacts in the SST dataset and SelfATK does not necessarily cover all of them.\nWe also experiment with selecting trapdoors randomly. Fig. 4 shows that greedy search produces stable results regardless of training F with a high ( ←1.0, “strong” trapdoors) or a low ( ←0.1, “weak” trapdoors) trapdoor ratio . Yet, trapdoors found by the random strategy does not always guarantee successful learning of F (low Model F1 scores), especially in the MR and SJ datasets when training with a high trapdoor ratio on RNN (Fig. 41). Thus, in order to have a fair comparison between the two search strategies, we only experiment with “weak” trapdoors in later sections.\nEvaluation on Advanced Attack. Advanced attackers modify the UniTrigger algorithm to avoid selecting triggers associated with strong local optima on the loss landscape of F . So, instead of\n1AG dataset is omitted due to computational limit\nalways selecting the best tokens from each iteration of the beam-search method (Sec. 2.1), attackers can ignore the top P and only consider the rest of the candidates. Table 4 (Table A.3, Appendix for full results) shows the benefits of multiple trapdoors. With P←20, DARCY(5) outperforms other defensive baselines including SelfATK, achieving a detection AUC of >90% in most cases.\nEvaluation on Adaptive Attack. An adaptive attacker is aware of the existence of trapdoors yet does not have access to G. Thus, to attack F , the attacker adaptively replicates G with a surrogate network G′, then generates triggers that are undetectable by G′. To train G′, the attacker can execute a # of queries (Q) to generate several triggers through F , and considers them as potential trapdoors. Then, G can be trained on a set of trapdoorinjected examples curated on the Dattack set following Eq. (2) and (4).\nFig. 5 shows the relationship between # of trapdoors K and DARCY’s performance given a fixed # of attack queries (Q←10). An adaptive attacker can drop the average TPR to nearly zero when\nFigure 8: Detection TPR v.s. # ignored tokens\nF is injected with only one trapdoor for each label (K←1). However, when K≥5, TPR quickly improves to about 90% in most cases and fully reaches above 98% when K≥10. This confirms the robustness of DARCY as described in Sec. 3.2. Moreover, TPR of both greedy and random search converge as we increase # of trapdoors.\nHowever, Fig. 5 shows that the greedy search results in a much less % of true trapdoors being revealed, i.e., revealed ratio, by the attack on CNN. Moreover, as Q increases, we expect that the attacker will gain more information on F , thus further drop DARCY’s detection AUC. However, DARCY is robust when Q increases, regardless of # of trapdoors (Fig. 6). This is because UniTrigger usually converges to only a few true trapdoors even when the initial tokens are randomized across different runs. We refer to Fig. A.2, A.3, Appendix for more results.\nEvaluation on Advanced Adaptive Attack. An advanced adaptive attacker not only replicates G by G′, but also ignores top P tokens during a beamsearch as in the advanced attack (Sec. 4.2) to both maximize the loss of F and minimize the detection chance of G′. Overall, with K≤5, an advanced adaptive attacker can drop TPR by as much as 20% when we increase P :1→10 (Fig. 7). However, with K←15, DARCY becomes fully robust against the attack. Overall, Fig. 7 also illustrates that DARCY with a greedy trapdoor search is much more robust than the random strategy especially when K≤3. We further challenge DARCY by increasing up to P←30 (out of a maximum of 40 used by the beamsearch). Fig. 8 shows that the more trapdoors\nembedded into F , the more robust the DARCY will become. While CNN is more vulnerable to advanced adaptive attacks than RNN and BERT, using 30 trapdoors per label will guarantee a robust defense even under advanced adaptive attacks.\nEvaluation on Oracle Attack. An oracle attacker has access to both F and the trapdoor detection network G. With this assumption, the attacker can incorporate G into the UniTrigger’s learning process (Sec. 2.1) to generate triggers that are undetectable by G. Fig. 9 shows the detection results under the oracle attack. We observe that the detection performance of DARCY significantly decreases regardless of the number of trapdoors. Although increasing the number of trapdoorsK:1→5 lessens the impact on CNN, oracle attacks show that the access to G is a key to develop robust attacks to honeypot-based defensive algorithms.\nEvaluation under Black-Box Attack. Even though UniTrigger is a white-box attack, it also works in a black-box setting via transferring triggers S generated on a surrogate model F ′ to attack F . As several methods (e.g., (Papernot et al., 2017)) have been proposed to steal, i.e., replicate F to create F ′, we are instead interested in examining if trapdoors injected in F ′ can be transferable to F? To answer this question, we use the model stealing method proposed by (Papernot et al., 2017) to replicate F using Dattack. Table A.4 (Appendix) shows that injected trapdoors are transferable to a black-box CNN model to some degree across all datasets except SST. Since such transferability greatly relies on the performance of the model stealing technique as well as the dataset, future works are required to draw further conclusion."
    }, {
      "heading" : "5 Discussion",
      "text" : "Advantages and Limitations of DARCY. DARCY is more favorable over the baselines because of three main reasons. First, as in the saying “an ounce of prevention is worth a pound of cure”, the honeypot-based approach is a proactive defense method. Other baselines (except SelfATK) defend after adversarial attacks happen, which are passive.\nHowever, our approach proactively expects and defends against attacks even before they happen. Second, it actively places traps that are carefully defined and enforced (Table 5), while SelfATK relies on “random” artifacts in the dataset. Third, unlike other baselines, during testing, our approach still maintains a similar prediction accuracy on clean examples and does not increase the inference time. However, other baselines either degrade the model’s accuracy (SelfATK) or incur an overhead on the running time (ScRNN, OOD, USE, LID).\nWe have showed that DARCY’s complexity scales linearly with the number of classes. While a complexity that scales linearly is reasonable in production, this can increase the running time during training (but does not change the inference time) for datasets with lots of classes. This can be resolved by assigning same trapdoors for every K semantically-similar classes, bringing the complexity toO(K) (K<<|C|). Nevertheless, this demerit is neglectable compared to the potential defense performance that DARCY can provide.\nCase Study: Fake News Detection. UniTrigger can help fool fake news detectors. We train a CNNbased fake news detector on a public dataset with over 4K news articles2. The model achieves 75% accuracy on the test set. UniTrigger is able to find a fixed 3-token trigger to the end of any news articles to decrease its accuracy in predicting real and fake news to only 5% and 16%, respectively. In a user study on Amazon Mechanical Turk (Fig. A.1, Appendix), we instructed 78 users to spend at least\n2truthdiscoverykdd2020.github.io/\nLength 50 words 100 words 250 words 500 words\n1 minute reading a news article and give a score from 1 to 10 on its readability. Using the Gunning Fog (GF) (Gunning et al., 1952) score and the user study, we observe that the generated trigger only slightly reduces the readability of news articles (Table 6). This shows that UniTrigger is a very strong and practical attack. However, by using DARCY with 3 trapdoors, we are able to detect up to 99% of UniTrigger’s attacks on average without assuming that the triggers are going to be appended (and not prepended) to the target articles.\nTrapdoor Detection and Removal. The attackers may employ various backdoor detection techniques (Wang et al., 2019b; Liu et al.; Qiao et al., 2019) to detect if F contains trapdoors. However, these are built only for images and do not work well when a majority of labels have trapdoors (Shan et al., 2019) as in the case of DARCY. Recently, a few works proposed to detect backdoors in texts. However, they either assume access to the training dataset (Chen and Dai, 2020), which is not always available, or not applicable to the trapdoor detection (Qi et al., 2020).\nAttackers may also use a model-pruning method to remove installed trapdoors from F as suggested by (Liu et al., 2018). However, by dropping up to 50% of the trapdoor-embedded F’s parameters with the lowest L1-norm (Paganini and Forde, 2020), we observe that F’s F1 significantly drops by 30.5% on average. Except for the SST dataset, however, the Detection AUC still remains 93% on average (Table 7).\nParameters Analysis. Regarding the trapdoorratio , a large value (e.g., ←1.0) can undesirably result in a detector network G that “memorizes” the embedded trapdoors instead of learning its seman-\ntic meanings. A smaller value of ≤0.15 generally works well across all experiments. Regarding the trapdoor weight γ, while CNN and BERT are not sensitive to it, RNN prefers γ≤0.75. Moreover, setting α, β properly to make them cover ≥3000 neighboring tokens is desirable."
    }, {
      "heading" : "6 Related Work",
      "text" : "Adversarial Text Detection. Adversarial detection on NLP is rather limited. Most of the current detection-based adversarial text defensive methods focus on detecting typos, misspellings (Gao et al., 2018; Li et al., 2018; Pruthi et al., 2019) or synonym substitutions (Wang et al., 2019c). Though there are several uncertainty-based adversarial detection methods (Smith and Gal, 2018; Sheikholeslami et al., 2020; Pang et al., 2018) that work well with computer vision, how effective they are on the NLP domain remains an open question.\nHoneypot-based Adversarial Detection. (Shan et al., 2019) adopts the “honeypot” concept to images. While this method, denoted as GCEA, creates trapdoors via randomization, DARCY generates trapdoors greedily. Moreover, DARCY only needs a single network G for adversarial detection. In contrast, GCEA records a separate neural signature (e.g., a neural activation pattern in the last layer) for each trapdoor. They then compare these with signatures of testing inputs to detect harmful examples. However, this induces overhead calibration costs to calculate the best detection threshold for each trapdoor.\nFurthermore, while (Shan et al., 2019) and (Carlini, 2020) show that true trapdoors can be revealed and clustered by attackers after several queries on F , this is not the case when we use DARCY to defend against adaptive UniTrigger attacks (Sec. 4.2). Regardless of initial tokens (e.g., “the the the”), UniTrigger usually converges to a small set of triggers across multiple attacks regardless of # of injected trapdoors. Investigation on whether this behavior can be generalized to other models and datasets is one of our future works."
    }, {
      "heading" : "7 Conclusion",
      "text" : "This paper proposes DARCY, an algorithm that greedily injects multiple trapdoors, i.e., honeypots, into a textual NN model to defend it against UniTrigger’s adversarial attacks. DARCY achieves a TPR as high as 99% and a FPR less than 2% in\nmost cases across four public datasets. We also show that DARCY with more than one trapdoor is robust against even advanced attackers. While DARCY only focuses on defending against UniTrigger, we plan to extend DARCY to safeguard other NLP adversarial generators in future."
    }, {
      "heading" : "Acknowledgement",
      "text" : "The works of Thai Le and Dongwon Lee were in part supported by NSF awards #1742702, #1820609, #1909702, #1915801, #1940076, #1934782, and #2114824. The work of Noseong Park was supported by the Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korean government (MSIT) (No. 2020-0-01361, Artificial Intelligence Graduate School Program (Yonsei University)).\nBroader Impact Statement\nOur work demonstrates the use of honeypots to defend NLP-based neural network models against adversarial attacks. Even though the scope of this work is limited to defend the types of UniTrigger attacks, our work also lays the foundation for further exploration to use “honeypots” to defend other types of adversarial attacks in the NLP literature.\nTo the best of our knowledge, there is no immediately foreseeable negative effects of our work in applications. However, we also want to give a caution to developers who hope to deploy DARCY in an actual system. Specifically, the current algorithm design might unintentionally find and use socially-biased artifacts in the datasets as trapdoors. Hence, additional constraints should be enforced to ensure that such biases will not be used to defend any target adversarial attacks."
    }, {
      "heading" : "A Appendix",
      "text" : ""
    }, {
      "heading" : "A.1 Objective Function",
      "text" : "Eq. (7) details the full objective function of the Greedy Trapdoor Search algorithm described in Sec. 3.2.\nOBJECTIVE FUNCTION 1: Given a NNF , and hyper-parameter K , α, β, our goal is to search for a set of K trapdoors to defend each label L ∈ C by optimizing:\nmin S∗L∈C ∑ L∈C LLfidelity subject to d(wi, wj) ≤ α ∀wi, wj ∈ S∗L d(wi, wj) ≥ β ∀wi ∈ S∗L, wj ∈ S∗Q 6=L L,Q ∈ C,K ≥ 1 (7)"
    }, {
      "heading" : "A.2 Further Details of Experiments",
      "text" : "• Table A.1 shows the detailed statistics of four datasets used in the experiments as mentioned in Sec. 4.1. • Tables A.2, A.3, A.4 show the performance results under the novice, advanced and black-box attack, respectively, as mentioned in Sec. 4.2. • Figure A.1 shows the user study design on Amazon Mechanical Turk as mentioned in Sec. 5. • Figures A.2 and A.3 show the performance under the adaptive attack as mentioned in Sec. 4.2."
    }, {
      "heading" : "A.3 Reproducibility",
      "text" : ""
    }, {
      "heading" : "A.3.1 Source Code",
      "text" : "We release the source code of DARCY at: https://github.com/lethaiq/ ACL2021-DARCY-HoneypotDefenseNLP."
    }, {
      "heading" : "A.3.2 Computing Infrastructure",
      "text" : "We run all experiments on the machines with Ubuntu OS (v18.04), 20-Core Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz, 93GB of RAM and a Titan Xp GPU. All implementations are written in Python (v3.7) with Pytorch (v1.5.1), Numpy (v1.19.1), Scikit-learn (v0.21.3). We also use the Transformers (v3.0.2)3 library for training transformers-based BERT."
    }, {
      "heading" : "A.3.3 Average Runtime",
      "text" : "According to Sec. 3.1, the computational complexity of greedy trapdoor search scales linearly with\n3https://huggingface.co/transformers/\nthe number of labels |C| and vocabulary size |V|. Moreover, the time to train a detection network depends on the size of a specific dataset, the trapdoor ratio , and the number of trapdoors K.\nFor example, DARCY takes roughly 14 and 96 seconds to search for 5 trapdoors to defend each label for a dataset with 2 labels and a vocabulary size of 19K (e.g., Movie Reviews) and a dataset with 4 labels and a vocabulary size of 91K (e.g., AG News), respectively. With K←5 and ←0.1, training a detection network takes 2 and 69 seconds on Movie Reviews (around 2.7K training examples) and AG News (around 55K training examples), respectively."
    }, {
      "heading" : "A.3.4 Model’s Architecture and # of Parameters",
      "text" : "The CNN text classification model with 6M parameters (Kim, 2014) has three 2D convolutional layers (i.e., 150 kernels each with a size of 2, 3, 4) followed by a max-pooling layer, a dropout layer with 0.5 probability, and a fully-connected-network (FCN) with softmax activation for prediction. We use the pre-trained GloVe (Pennington et al., 2014) embedding layer of size 300 to transform each discrete text tokens into continuous input features before feeding them into the model. The RNN text model with 6.1M parameters replaces the convolution layers of CNN with a GRU network of 1 hidden layer. The BERT model with 109M parameters is imported from the transformers library. We use the bert-base-uncased version of BERT."
    }, {
      "heading" : "A.3.5 Hyper-Parameters",
      "text" : "Sec. 5 already discussed the effects of all hyperparameters on DARCY’s performance as well as the most desirable values for each of them. To tune these hyper-parameters, we use the grid search as follows: ∈ {1.0, 0.5, 0.25, 0.1}, γ ∈ {1.0, 0.75, 0.5}. Since α and β are sensitive to the domain of the pre-trained word-embedding (we use GloVe embeddings (Pennington et al., 2014)), without loss of generality, we instead use # of neighboring tokens to accept or filter to search for the corresponding α, β in Eq. (6): {500, 1000, 3000, 5000}.\nWe set the number of randomly sampled candidate trapdoors to around 10% of the vocabulary size (T←300). We train all models using a learning rate of 0.005 and batch size of 32. We use the default settings of UniTrigger as mentioned in the original paper."
    }, {
      "heading" : "A.3.6 Datasets",
      "text" : "We use Datasets (v1.2.1)4 library to load all the standard benchmark datasets used in the paper, all of which are publicly available.\n4https://huggingface.co/docs/datasets/\nAdaptive Random\nDetect Attack Detect Attack AUC↑ ACC↓ AUC↑ ACC↓\nMR 74.24 4.6 85.3 3.77 SJ 87.19 0.34 76.78 2.86 SST 58.81 19.77 49.75 18.96 AG 67.88 55.87 53.25 75.25 Red: not transferable\nTable A.4: Detection AUC and model’s accuracy (attack ACC) under black-box attack on CNN"
    } ],
    "references" : [ {
      "title" : "A partial break of the honeypots defense to catch adversarial attacks",
      "author" : [ "Nicholas Carlini." ],
      "venue" : "arXiv preprint arXiv:2009.10975.",
      "citeRegEx" : "Carlini.,? 2020",
      "shortCiteRegEx" : "Carlini.",
      "year" : 2020
    }, {
      "title" : "Mitigating backdoor attacks in lstm-based text classification systems by backdoor keyword identification",
      "author" : [ "Chuanshuai Chen", "Jiazhu Dai." ],
      "venue" : "arXiv preprint arXiv:2007.12070.",
      "citeRegEx" : "Chen and Dai.,? 2020",
      "shortCiteRegEx" : "Chen and Dai.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL-HLT’19, pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Hotflip: White-box adversarial examples for text classification",
      "author" : [ "Javid Ebrahimi", "Anyi Rao", "Daniel Lowd", "Dejing Dou." ],
      "venue" : "ACL’18, Melbourne, Australia. ACL.",
      "citeRegEx" : "Ebrahimi et al\\.,? 2018",
      "shortCiteRegEx" : "Ebrahimi et al\\.",
      "year" : 2018
    }, {
      "title" : "Black-box generation of adversarial text sequences to evade deep learning classifiers",
      "author" : [ "Ji Gao", "Jack Lanchantin", "Mary Lou Soffa", "Yanjun Qi." ],
      "venue" : "SPW’18, pages 50–56. IEEE.",
      "citeRegEx" : "Gao et al\\.,? 2018",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2018
    }, {
      "title" : "Bae: Bert-based adversarial examples for text classification",
      "author" : [ "Siddhant Garg", "Goutham Ramakrishnan." ],
      "venue" : "EMNLP’20.",
      "citeRegEx" : "Garg and Ramakrishnan.,? 2020",
      "shortCiteRegEx" : "Garg and Ramakrishnan.",
      "year" : 2020
    }, {
      "title" : "Technique of clear writing",
      "author" : [ "Robert Gunning" ],
      "venue" : "McGraw-Hill.",
      "citeRegEx" : "Gunning,? 1952",
      "shortCiteRegEx" : "Gunning",
      "year" : 1952
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "EMNLP’14, pages 1746– 1751.",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "MALCOM: Generating Malicious Comments to Attack Neural Fake News Detection Models",
      "author" : [ "Thai Le", "Suhang Wang", "Dongwon Lee." ],
      "venue" : "IEEE ICDM.",
      "citeRegEx" : "Le et al\\.,? 2020",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2020
    }, {
      "title" : "TextBugger: Generating Adversarial Text Against Real-world Applications",
      "author" : [ "Jinfeng Li", "Shouling Ji", "Tianyu Du", "Bo Li", "Ting Wang." ],
      "venue" : "NDSS.",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Fine-pruning: Defending against backdooring attacks on deep neural networks",
      "author" : [ "Kang Liu", "Brendan Dolan-Gavitt", "Siddharth Garg." ],
      "venue" : "International Symposium on Research in Attacks, Intrusions, and Defenses, pages 273–294. Springer.",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Characterizing adversarial subspaces using local intrinsic dimensionality",
      "author" : [ "Xingjun Ma", "Bo Li", "Yisen Wang", "Sarah M Erfani", "Sudanthi Wijewickrema", "Grant Schoenebeck", "Dawn Song", "Michael E Houle", "James Bailey." ],
      "venue" : "ICLR’18.",
      "citeRegEx" : "Ma et al\\.,? 2018",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2018
    }, {
      "title" : "Streamlining tensor and network pruning in pytorch",
      "author" : [ "Michela Paganini", "Jessica Forde." ],
      "venue" : "arXiv preprint arXiv:2004.13770.",
      "citeRegEx" : "Paganini and Forde.,? 2020",
      "shortCiteRegEx" : "Paganini and Forde.",
      "year" : 2020
    }, {
      "title" : "A sentimental education: Sentiment analysis using subjectivity",
      "author" : [ "Bo Pang", "Lillian Lee." ],
      "venue" : "ACL’04, pages 271–278.",
      "citeRegEx" : "Pang and Lee.,? 2004",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2004
    }, {
      "title" : "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
      "author" : [ "Bo Pang", "Lillian Lee." ],
      "venue" : "ACL’05.",
      "citeRegEx" : "Pang and Lee.,? 2005",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2005
    }, {
      "title" : "Towards robust detection of adversarial examples",
      "author" : [ "Tianyu Pang", "Chao Du", "Yinpeng Dong", "Jun Zhu." ],
      "venue" : "NIPS’18, pages 4579–4589.",
      "citeRegEx" : "Pang et al\\.,? 2018",
      "shortCiteRegEx" : "Pang et al\\.",
      "year" : 2018
    }, {
      "title" : "Practical black-box attacks against machine learning",
      "author" : [ "Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow", "Somesh Jha", "Z Berkay Celik", "Ananthram Swami." ],
      "venue" : "ASIACCS’17, pages 506–519.",
      "citeRegEx" : "Papernot et al\\.,? 2017",
      "shortCiteRegEx" : "Papernot et al\\.",
      "year" : 2017
    }, {
      "title" : "GloVe: Global Vectors for Word Representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Combating adversarial misspellings with robust word recognition",
      "author" : [ "Danish Pruthi", "Bhuwan Dhingra", "Zachary C Lipton." ],
      "venue" : "ACL’19.",
      "citeRegEx" : "Pruthi et al\\.,? 2019",
      "shortCiteRegEx" : "Pruthi et al\\.",
      "year" : 2019
    }, {
      "title" : "Onion: A simple and effective defense against textual backdoor attacks",
      "author" : [ "Fanchao Qi", "Yangyi Chen", "Mukai Li", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "arXiv preprint arXiv:2011.10369.",
      "citeRegEx" : "Qi et al\\.,? 2020",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2020
    }, {
      "title" : "Defending neural backdoors via generative distribution modeling",
      "author" : [ "Ximing Qiao", "Yukun Yang", "Hai Li." ],
      "venue" : "NIPS’19, pages 14004–14013.",
      "citeRegEx" : "Qiao et al\\.,? 2019",
      "shortCiteRegEx" : "Qiao et al\\.",
      "year" : 2019
    }, {
      "title" : "Using honeypots to catch adversarial attacks on neural networks",
      "author" : [ "Shawn Shan", "Emily Wenger", "Bolun Wang", "Bo Li", "Haitao Zheng", "Ben Y Zhao." ],
      "venue" : "CCS’20.",
      "citeRegEx" : "Shan et al\\.,? 2019",
      "shortCiteRegEx" : "Shan et al\\.",
      "year" : 2019
    }, {
      "title" : "Minimum uncertainty based detection of adversaries in deep neural networks",
      "author" : [ "Fatemeh Sheikholeslami", "Swayambhoo Jain", "Georgios B Giannakis." ],
      "venue" : "2020 Information Theory and Applications Workshop (ITA), pages 1–16. IEEE.",
      "citeRegEx" : "Sheikholeslami et al\\.,? 2020",
      "shortCiteRegEx" : "Sheikholeslami et al\\.",
      "year" : 2020
    }, {
      "title" : "Understanding measures of uncertainty for adversarial example detection",
      "author" : [ "Lewis Smith", "Yarin Gal." ],
      "venue" : "arXiv preprint arXiv:1803.08533.",
      "citeRegEx" : "Smith and Gal.,? 2018",
      "shortCiteRegEx" : "Smith and Gal.",
      "year" : 2018
    }, {
      "title" : "Universal adversarial triggers for nlp",
      "author" : [ "Eric Wallace", "Shi Feng", "Nikhil Kandpal", "Matt Gardner", "Sameer Singh." ],
      "venue" : "EMNLP’19.",
      "citeRegEx" : "Wallace et al\\.,? 2019",
      "shortCiteRegEx" : "Wallace et al\\.",
      "year" : 2019
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman." ],
      "venue" : "ICLR’19.",
      "citeRegEx" : "Wang et al\\.,? 2019a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural cleanse: Identifying and mitigating backdoor attacks in neural networks",
      "author" : [ "Bolun Wang", "Yuanshun Yao", "Shawn Shan", "Huiying Li", "Bimal Viswanath", "Haitao Zheng", "Ben Y Zhao." ],
      "venue" : "EuroS&P’19, pages 707–723. IEEE.",
      "citeRegEx" : "Wang et al\\.,? 2019b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Natural language adversarial attacks and defenses in word level",
      "author" : [ "Xiaosen Wang", "Hao Jin", "Kun He." ],
      "venue" : "arXiv preprint arXiv:1909.06723.",
      "citeRegEx" : "Wang et al\\.,? 2019c",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "versal Trigger (UniTrigger) (Wallace et al., 2019), MALCOM (Le et al.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 8,
      "context" : ", 2019), MALCOM (Le et al., 2020), Seq2Sick (Cheng et al.",
      "startOffset" : 16,
      "endOffset" : 33
    }, {
      "referenceID" : 24,
      "context" : "To illustrate, both adversarial examples generated by UniTrigger and MALCOM to attack a white-box NN model are also effective in fooling unseen black-box models of different architectures (Wallace et al., 2019; Le et al., 2020).",
      "startOffset" : 188,
      "endOffset" : 227
    }, {
      "referenceID" : 8,
      "context" : "To illustrate, both adversarial examples generated by UniTrigger and MALCOM to attack a white-box NN model are also effective in fooling unseen black-box models of different architectures (Wallace et al., 2019; Le et al., 2020).",
      "startOffset" : 188,
      "endOffset" : 227
    }, {
      "referenceID" : 24,
      "context" : "UniTrigger (Wallace et al., 2019) generates a fixed phrase S consisting of K tokens, i.",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 24,
      "context" : "The final set of tokens are selected as the universal trigger (Wallace et al., 2019).",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 7,
      "context" : "Table 2 shows the prediction accuracy of CNN (Kim, 2014) under different attacks on the MR (Pang and Lee, 2005) and SST (Wang et al.",
      "startOffset" : 45,
      "endOffset" : 56
    }, {
      "referenceID" : 14,
      "context" : "Table 2 shows the prediction accuracy of CNN (Kim, 2014) under different attacks on the MR (Pang and Lee, 2005) and SST (Wang et al.",
      "startOffset" : 91,
      "endOffset" : 111
    }, {
      "referenceID" : 25,
      "context" : "Table 2 shows the prediction accuracy of CNN (Kim, 2014) under different attacks on the MR (Pang and Lee, 2005) and SST (Wang et al., 2019a) datasets.",
      "startOffset" : 120,
      "endOffset" : 140
    }, {
      "referenceID" : 3,
      "context" : "All those methods, including not only UniTrigger but also other attacks such as HotFlip (Ebrahimi et al., 2018), TextFooler (Jin et al.",
      "startOffset" : 88,
      "endOffset" : 111
    }, {
      "referenceID" : 9,
      "context" : ") and TextBugger (Li et al., 2018), can ensure that the semantic similarity of an input text before and after perturbations is within a threshold.",
      "startOffset" : 17,
      "endOffset" : 34
    }, {
      "referenceID" : 13,
      "context" : "1 (Appendix) shows the statistics of all datasets of varying scales and # of classes: Subjectivity (SJ) (Pang and Lee, 2004), Movie Reviews (MR) (Pang and Lee, 2005), Binary Sentiment Treebank (SST) (Wang et al.",
      "startOffset" : 104,
      "endOffset" : 124
    }, {
      "referenceID" : 14,
      "context" : "1 (Appendix) shows the statistics of all datasets of varying scales and # of classes: Subjectivity (SJ) (Pang and Lee, 2004), Movie Reviews (MR) (Pang and Lee, 2005), Binary Sentiment Treebank (SST) (Wang et al.",
      "startOffset" : 145,
      "endOffset" : 165
    }, {
      "referenceID" : 25,
      "context" : "1 (Appendix) shows the statistics of all datasets of varying scales and # of classes: Subjectivity (SJ) (Pang and Lee, 2004), Movie Reviews (MR) (Pang and Lee, 2005), Binary Sentiment Treebank (SST) (Wang et al., 2019a) and AG News (AG) (Zhang et al.",
      "startOffset" : 199,
      "endOffset" : 219
    }, {
      "referenceID" : 7,
      "context" : "We defend RNN, CNN (Kim, 2014) and BERT (Devlin et al.",
      "startOffset" : 19,
      "endOffset" : 30
    }, {
      "referenceID" : 2,
      "context" : "We defend RNN, CNN (Kim, 2014) and BERT (Devlin et al., 2019)",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 23,
      "context" : "• OOD Detection (OOD) (Smith and Gal, 2018) assumes that adversarial examples locate far away from the distribution of training examples, i.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 11,
      "context" : "• Local Intrinsic Dimensionality (LID) (Ma et al., 2018) characterizes adversarial regions of a NN model using LID and uses this as a feature to detect adversarial examples.",
      "startOffset" : 39,
      "endOffset" : 56
    }, {
      "referenceID" : 18,
      "context" : "• Robust Word Recognizer (ScRNN) (Pruthi et al., 2019) detects potential adversarial perturbations or misspellings in sentences.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 16,
      "context" : ", (Papernot et al., 2017)) have been proposed to steal, i.",
      "startOffset" : 2,
      "endOffset" : 25
    }, {
      "referenceID" : 16,
      "context" : ", replicate F to create F ′, we are instead interested in examining if trapdoors injected in F ′ can be transferable to F? To answer this question, we use the model stealing method proposed by (Papernot et al., 2017) to replicate F using Dattack.",
      "startOffset" : 193,
      "endOffset" : 216
    }, {
      "referenceID" : 26,
      "context" : "The attackers may employ various backdoor detection techniques (Wang et al., 2019b; Liu et al.; Qiao et al., 2019) to detect if F contains trapdoors.",
      "startOffset" : 63,
      "endOffset" : 114
    }, {
      "referenceID" : 20,
      "context" : "The attackers may employ various backdoor detection techniques (Wang et al., 2019b; Liu et al.; Qiao et al., 2019) to detect if F contains trapdoors.",
      "startOffset" : 63,
      "endOffset" : 114
    }, {
      "referenceID" : 21,
      "context" : "However, these are built only for images and do not work well when a majority of labels have trapdoors (Shan et al., 2019) as in the case of DARCY.",
      "startOffset" : 103,
      "endOffset" : 122
    }, {
      "referenceID" : 1,
      "context" : "However, they either assume access to the training dataset (Chen and Dai, 2020), which is not always available, or not applicable to the trapdoor detection (Qi et al.",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 19,
      "context" : "However, they either assume access to the training dataset (Chen and Dai, 2020), which is not always available, or not applicable to the trapdoor detection (Qi et al., 2020).",
      "startOffset" : 156,
      "endOffset" : 173
    }, {
      "referenceID" : 10,
      "context" : "Attackers may also use a model-pruning method to remove installed trapdoors from F as suggested by (Liu et al., 2018).",
      "startOffset" : 99,
      "endOffset" : 117
    }, {
      "referenceID" : 12,
      "context" : "However, by dropping up to 50% of the trapdoor-embedded F’s parameters with the lowest L1-norm (Paganini and Forde, 2020), we observe that F’s F1 significantly drops by 30.",
      "startOffset" : 95,
      "endOffset" : 121
    }, {
      "referenceID" : 4,
      "context" : "Most of the current detection-based adversarial text defensive methods focus on detecting typos, misspellings (Gao et al., 2018; Li et al., 2018; Pruthi et al., 2019) or synonym substitutions (Wang et al.",
      "startOffset" : 110,
      "endOffset" : 166
    }, {
      "referenceID" : 9,
      "context" : "Most of the current detection-based adversarial text defensive methods focus on detecting typos, misspellings (Gao et al., 2018; Li et al., 2018; Pruthi et al., 2019) or synonym substitutions (Wang et al.",
      "startOffset" : 110,
      "endOffset" : 166
    }, {
      "referenceID" : 18,
      "context" : "Most of the current detection-based adversarial text defensive methods focus on detecting typos, misspellings (Gao et al., 2018; Li et al., 2018; Pruthi et al., 2019) or synonym substitutions (Wang et al.",
      "startOffset" : 110,
      "endOffset" : 166
    }, {
      "referenceID" : 23,
      "context" : "Though there are several uncertainty-based adversarial detection methods (Smith and Gal, 2018; Sheikholeslami et al., 2020; Pang et al., 2018) that work well with computer vision, how effective they",
      "startOffset" : 73,
      "endOffset" : 142
    }, {
      "referenceID" : 22,
      "context" : "Though there are several uncertainty-based adversarial detection methods (Smith and Gal, 2018; Sheikholeslami et al., 2020; Pang et al., 2018) that work well with computer vision, how effective they",
      "startOffset" : 73,
      "endOffset" : 142
    }, {
      "referenceID" : 15,
      "context" : "Though there are several uncertainty-based adversarial detection methods (Smith and Gal, 2018; Sheikholeslami et al., 2020; Pang et al., 2018) that work well with computer vision, how effective they",
      "startOffset" : 73,
      "endOffset" : 142
    }, {
      "referenceID" : 21,
      "context" : "(Shan et al., 2019) adopts the “honeypot” concept to images.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 21,
      "context" : "Furthermore, while (Shan et al., 2019) and (Carlini, 2020) show that true trapdoors can be revealed and clustered by attackers after several queries on F , this is not the case when we use DARCY to defend against adaptive UniTrigger attacks (Sec.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : ", 2019) and (Carlini, 2020) show that true trapdoors can be revealed and clustered by attackers after several queries on F , this is not the case when we use DARCY to defend against adaptive UniTrigger attacks (Sec.",
      "startOffset" : 12,
      "endOffset" : 27
    } ],
    "year" : 2021,
    "abstractText" : "The Universal Trigger (UniTrigger) is a recently-proposed powerful adversarial textual attack method. Utilizing a learning-based mechanism, UniTrigger generates a fixed phrase that, when added to any benign inputs, can drop the prediction accuracy of a textual neural network (NN) model to near zero on a target class. To defend against this attack that can cause significant harm, in this paper, we borrow the “honeypot” concept from the cybersecurity community and propose DARCY, a honeypot-based defense framework against UniTrigger. DARCY greedily searches and injects multiple trapdoors into an NN model to “bait and catch” potential attacks. Through comprehensive experiments across four public datasets, we show that DARCY detects UniTrigger’s adversarial attacks with up to 99% TPR and less than 2% FPR in most cases, while maintaining the prediction accuracy (in F1) for clean inputs within a 1% margin. We also demonstrate that DARCY with multiple trapdoors is also robust to a diverse set of attack scenarios with attackers’ varying levels of knowledge and skills. We release the source code of DARCY at: https://github.com/lethaiq/ ACL2021-DARCY-HoneypotDefenseNLP.",
    "creator" : "LaTeX with hyperref"
  }
}