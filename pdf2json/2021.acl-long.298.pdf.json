{
  "name" : "2021.acl-long.298.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Label-Specific Dual Graph Neural Network for Multi-Label Text Classification",
    "authors" : [ "Qianwen Ma", "Chunyuan Yuan", "Wei Zhou", "Songlin Hu" ],
    "emails" : [ "maqianwen@iie.ac.cn", "yuanchunyuan@iie.ac.cn", "zhouwei@iie.ac.cn", "husonglin@iie.ac.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3855–3864\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3855"
    }, {
      "heading" : "1 Introduction",
      "text" : "Automatically labeling multiple labels of documents is a fundamental and practical task in natural language processing. Recently, with the growth of data scale, multi-label text classification(MLTC) has attracted more attention, since it is often applied to many fields such as sentiment analysis (Liu and Chen, 2015; Li et al., 2016), emotion recognition (Wang et al., 2016; Jabreel and Moreno, 2019), web page tagging (Jain et al., 2016) and so on. However, the number of labels and documents and the complex relations of labels render it an unsolved and challenging task.\nExisting studies for multi-label text classification mainly focus on learning enhanced document\n*Corresponding Author\nrepresentation (Liu et al., 2017) and modeling label dependency (Zhang et al., 2018; Yang et al., 2018; Tsai and Lee, 2019) to improve classification performance. Although they have explored the informative words in text content, or considered the label structure and label semantics to capture label correlations, these models cannot distinguish similar labels well (e.g., the categories Prices vs Consumer Prices in Reuters News).\nThe main reason is that most of them neglect the semantic connections between labels and input documents and they learn the same document representations for different labels, which cannot issue the label similarity problem. More specifically, they do not explicitly consider the corresponding semantic parts of each label in the document.\nRecently, some studies (You et al., 2019; Xiao et al., 2019; Du et al., 2019) have used attention mechanism to explore the above semantic connections, and learn a label-specific document representation for classification. These methods have obtained promising results in MLTC, which shows the importance of exploring semantic connections. However, they did not further study the interactions between label-specific semantic components which can be guided by label correlations, and thus these models cannot work well on predicting tail labels which is also a challenging issue in MLTC. To handle these issues, a common way to explore the semantic interactions between labelspecific parts in document is to utilize the statistical correlations between categories to build a label co-occurrence graph for guiding interactions.\nNevertheless, statistical correlations have three drawbacks. First, the co-occurrence patterns between label pairs obtained from training data are incomplete and noisy. Specifically, the label cooccurrences that appear in the test set but do not appear in the training set may be ignored, while\nsome rare label co-occurrences in the statistical correlations may be noise. Second, the label cooccurrence graph is built in global, which may be biased for rare label correlations. And thus they are not flexible to every sample document. Third, statistical label correlations may form a long-tail distribution, i.e., some categories are very common while most categories have few of documents. This phenomenon may lead to models failing to predict low-frequency labels. Thus, our goal is to find a way to explore the complete and adaptive interactions among label-specific semantic components more accurately.\nIn this paper, we investigate: (1) how to explicitly extract the semantic components related to the corresponding labels from each document; and (2) how to accurately capture the more complete and more adaptive interactions between label-specific semantic components according to label dependencies. To solve the first challenge, we exploit the attention mechanism to extract the labelspecific semantic components from the text content, which can alleviate the label similar problem. To capture the more accurate high-order interactions between these semantic components, we first employ one Graph Convolution Network (GCN) to learn component representations using the statistical label co-occurrence to guide the information propagation among nodes (components) in GCN. Then, we use the component representations to reconstruct the adjacency graph dynamically and re-learn the component representations with another GCN, and thus we can capture the latent interactions between these semantic components. Finally, we exploit final component representations to predict labels. We evaluate our model on three real-world datasets, and the results show that the proposed model LDGN outperforms all the comparison methods. Further studies demonstrate our ability to effectively alleviate the tail labels problem, and accurately capture the meaningful interactions between label-specific semantic components.\nThe contributions of this paper are as follows:\n• We propose a novel label-specific dual graph neural network (LDGN), which incorporates category information to extract label-specific components from documents, and explores the interactions among these components.\n• To model the accurate and adaptive interactions, we jointly exploit global co-occurrence\npatterns and local dynamic relations. To make up the deficiency of co-occurrences, we employ the local reconstruction graph which is built by every document dynamically.\n• We conduct a series of experiments on three public datasets, and experimental results demonstrate that our model LDGN significantly outperforms the state-of-the-art models, and also achieves better performance with respect to tail labels."
    }, {
      "heading" : "2 Model",
      "text" : "As depicted in Figure 1, our model LDGN is composed of two major modules: 1) labelspecific document representation 2) dual graph neural network for semantic interaction learning. Specifically, label-specific document representation learning describes how to extract labelspecific semantic components from the mixture of label information in each document; and the dual graph neural network for semantic interaction learning illustrates how to accurately explore the complete interactions among these semantic components under the guidance of the prior knowledge of statistical label co-occurrence and the posterior information of dynamic reconstruction graph. Problem Formulation: Let D = {xi, yi}N be the set of documents, which consists of N document xi and its corresponding label yi ∈ {0, 1}|C|, where |C| denotes the total number of labels. Each document xi contains J words xi = wi1, wi2, . . . , wiJ . The target of multi-label text classification is to learn the mapping from input text sequence to the most relevant labels."
    }, {
      "heading" : "2.1 Label-specific Document Representation",
      "text" : "Given a document x with J words, we first embed each word wj in the text into a word vector ewj ∈ Rd, where d is the dimensionality of word embedding vector. To capture contextual information from two directions of the word sequence, we first use a bidirectional LSTM to encode wordlevel semantic information in document representation. And we concatenate the forward and backward hidden states to obtain the final word sequence vector h ∈ R|J |×D.\nAfter that, to explicitly extract the corresponding semantic component related to each label from documents, we use a label guided attention mechanism to learn label-specific text representation.\nFirstly, we randomly initialize the label representation C ∈ R|C|×dc , and compute the label-aware attention values. Then, we can induce the labelspecific semantic components based on the label guided attention. The formula is as follows:\nαij = exp\n( hjc T i )∑ j exp ( hjc T i\n) , (1) ui =\n∑ j αijhj , (2)\nwhere αij indicates how informative the j-th text feature vector is for the i-th label. ui ∈ RD denotes the semantic component related to the label ci in this document."
    }, {
      "heading" : "2.2 Dual Graph Neural Network",
      "text" : "Interaction Learning with Statistical Label Cooccurrence To capture the mutual interactions between the label-specific semantic components, we build a label graph based on the prior knowledge of label co-occurrence, each node in which correlates to a label-specific semantic component ui. And then we apply a graph neural network to propagate message between nodes.\nFormally, we define the label graph G = (V, E), where nodes refer to the categories and edges refer to the statistical co-occurrence between nodes (categories). Specifically, we compute the probability between all label pairs in the training set and get the matrix As ∈ R|C|×|C|, where Asij denotes the conditional probability of a sample belonging to category Ci when it belongs to category Cj .\nThen, we utilize GCN (Kipf and Welling, 2017) to learn the deep relationships between labelspecific semantic components guided by the statistical label correlations. GCNs are neural networks\noperating on graphs, which are capable of enhancing node representations by propagating messages between neighboring nodes.\nIn multi-layer GCN, each GCN layer takes the component representations from previous layer Hl as inputs and outputs enhanced component representations, i.e., Hl+1. The layer-wise propagation rule is as follows:\nHl+1 = σ ( ÂsHlWl ) , (3)\nwhere σ (·) denotes LeakyReLU (Maas et al., 2013) activation function. Wl ∈ RD×D′ is a transformation matrix to be learned. Â denotes the normalized adjacency matrix, and the normalization method (Kipf and Welling, 2017) is:\nÂ = D− 1 2AD− 1 2 , (4)\nwhere D is a diagonal degree matrix with entries Dij = ΣjAij\nDepending on how many convolutional layers are used, GCN can aggregate information only about immediate neighbors (with one convolutional layer) or any nodes at most K-hops neighbors (if K layers are stacked). See (Kipf and Welling, 2017) for more details about GCN.\nWe use a two-layer GCN to learn the interactions between label-specific components. The first layer takes the initialized component representations U ∈ R|C|×D in Equation 2 as inputs H0; and the last layer outputs H2 ∈ R|C|×D′ with D′ denoting the dimensionality of final node representations.\nHowever, the statistical label correlations obtained by training data are incomplete and noisy.\nAnd the co-occurrence patterns between label pairs may form a long-tail distribution. Re-learning with Dynamic Reconstruction Graph To capture the more complete and adaptive interactions between these components, we exploit the above component representations H2 to reconstruct the adjacency graph dynamically, which can make up the deficiency of co-occurrence matrix. And then we re-learn the interactions among the label-specific components guided by the posterior information of dynamic reconstruction graph.\nSpecifically, we apply two 1×1 convolution layers and dot product to get the dynamic reconstruction graph AD as follows:\nAD = f (( Wa ∗H2 )T ( Wb ∗H2 )) , (5)\nwhere Wa ∈ Rd1×D ′ and Wb ∈ Rd1×D ′ are the weights of two convolution layers, f is the sigmoid activation function. And then we normalize the reconstruction adjacency matrix as Equation 4, and obtain the normalized adjacency matrix ÂD of reconstruction graph.\nIn a similar way as Equation 3, we apply another 2-layer GCN to learn the deep correlations between components with the dynamic reconstruction graph. The first layer of this GCN takes the component representations H2 as inputs, and the last layer outputs the final component representations H4 ∈ R|C|×D′ ."
    }, {
      "heading" : "2.3 Multi-label Text Classification",
      "text" : "After the above procedures, we concatenate the two types of component representations HO = [H2,H4] and feed it into a fully connected layer for prediction: ŷ = σ(W1HO) , where W1 ∈ R2D\n′×1 and σ is the sigmoid function. We use y ∈ R|C| to represent the ground-truth label of a document, where yi = 0, 1 denotes whether label i appears in the document or not. The proposed model LDGN is trained with the multi-label cross entropy loss:\nL = C∑ c=1 yc log (ŷc) + (1− yc) log (1− ŷc) .\n(6)"
    }, {
      "heading" : "3 Experiment",
      "text" : ""
    }, {
      "heading" : "3.1 Experimental Setup",
      "text" : "Datasets We evaluate the proposed model on three benchmark multi-label text classifica-\ntion datasets, which are AAPD (Yang et al., 2018), EUR-Lex (Mencia and Fürnkranz, 2008) and RCV1 (Lewis et al., 2004). The statistics of these three datasets are listed in Table 1.\nEvaluation Metric Following the settings of previous work (You et al., 2019; Xiao et al., 2019), we use precision at top K (P@k) and Normalized Discounted Cumulated Gains at top K (nDCG@k) for performance evaluation. The definition of two metrics can be referred to You et al. (2019). Implementation Details For a fair comparison, we apply the same dataset split as previous work (Xiao et al., 2019), which is also the original split provided by dataset publisher (Yang et al., 2018; Mencia and Fürnkranz, 2008).\nThe word embeddings in the proposed network are initialized with the 300-dimensional word vectors, which are trained on the datasets by Skipgram (Mikolov et al., 2013) algorithm. The hidden sizes of Bi-LSTM and GCNs are set to 300 and 512, respectively. We use the Adam optimization method (Kingma and Ba, 2014) to minimize the cross-entropy loss, the learning rate is initialized to 1e-3 and gradually decreased during the process of training. We select the best parameter configuration based on performance on the validation set and evaluate the configuration on the test set. Our code is available on GitHub1."
    }, {
      "heading" : "3.2 Baselines",
      "text" : "We compare the proposed model with recent deep learning based methods for MLTC, including seq2seq models, deep embedding models, and label attention based models. And it should be noted that, because of different application scenarios, we did not choose the label tree-based methods and extreme text focused methods as baseline models.\n• XML-CNN (Liu et al., 2017): a CNN-based 1https://github.com/Makwen1995/LDGN MLTC\nmodel which uses CNN and a dynamic pooling layer to extract high-level feature for MLTC.\n• SGM (Yang et al., 2018): a sequence generation model which models label correlations as an ordered sequence.\n• DXML (Zhang et al., 2018): a deep embedding method which models the feature space and label graph structure simultaneously.\n• AttentionXML (You et al., 2019): a label treebased deep learning model which uses a probabilistic label tree and multi-label attention to capture informative words in extreme-scale data.\n• EXAM (Du et al., 2019): a novel framework that leverages the label information to compute the word-level interactions.\n• LSAN (Xiao et al., 2019): a label-specific attention network model based on self-attention and label-attention mechanism.\nThe SotA model (i.e., LSAN) used BiLSTM model for text representations. For a fair comparison, we also take BiLSTM as text encoder in our model."
    }, {
      "heading" : "3.3 Experimental Results and Analysis",
      "text" : "Table 2 and Table 3 demonstrate the performance of all the compared methods based on the three datasets. For fair comparison, the experimental results of baseline models are directly cited from previous studies (Xiao et al., 2019). We also bold the best result of each column in all tables.\nFrom the Table 2 and Table 3, we can observe that the proposed LDGN outperforms all other\nbaselines on three datasets. The outstanding results confirm the effectiveness of label-specific semantic interaction learning with dual graph neural network, which include global statistical patterns and local dynamic relations.\nIt is observed that the performance of XMLCNN is worse than other comparison methods. The reason is that it only exploits the text content of documents for classification but ignores the label correlations which have been proven very important for multi-label classification problem.\nThe label tree-based model AttentionXML performs better than the seq2seq method (SGM) and the deep embedding method (DXML). Although both DXML and SGM employ a label graph or an ordered sequence to model the relationship between labels, they ignore the interactions between labels and document content. And AttentionXML uses multi-label attention which can focus on the most relevant parts in content and extract different semantic information for each label.\nCompared with other label attention based\nmethods (AttentionXML, EXAM), LSAN performs the best because it takes the semantic correlations between document content and label text into account simultaneously, which exploits an adaptive fusion to integrate self-attention and label-attention mechanisms to learn the labelspecific document representation.\nIn conclusion, the proposed network LDGN outperforms sequence-to-sequence models, deep embedding models, and label attention based models, and the metrics P@k and nDCG@k of multi-label text classification obtain significant improvement. Specifically, on the AAPD dataset, LDGN increases the P@1 of the LSAN method (the best baseline) from 85.28% to 86.24%, and increases nDCG@3 and nDCG@5 from 80.84% to 83.33%, 84.78% to 86.85% , respectively. On the EUR-Lex dataset, the metric P@1 is boosted from 79.17% to 81.03%, and P@5 and nDCG@5 are increased from 53.67% to 56.36%, 62.47% to 66.09%, respectively. On the RCV1 dataset, the P@k of our model increased by 0.3% at average, and LDGN achieves 1% and 1.6% absolute improvement on nDCG@3, 5 compared with LSAN. The improvements of the proposed LDGN model demonstrate that the semantic interaction learning with joint global statistical relations and local dynamic relations are generally helpful and effective, and LDGN can capture the deeper correlations between categories than LSAN."
    }, {
      "heading" : "3.4 Ablation Test",
      "text" : "We perform a series of ablation experiments to examine the relative contributions of dual graphbased semantic interactions module. To this end, LDGN is compared with its three variants:(1)S: Graph-based semantic interactions only with statistical label co-occurrence; (2)D: Graph-based semantic interactions only with dynamic reconstruction graph; (3)no-G:Removing the dual graph\nneural network. For a fair comparison, both S and D use 4-layer GCN which is the same as LDGN.\nAs presented in Figure 3, S and D perform better than no-G, which demonstrates that exploring either statistical relations or dynamic relations can correctly capture the effective semantic interactions between label-specific components. D performs better than S, indicating the model with local dynamic relations is adaptive to data and has better stability and robustness, which also shows that the model with local dynamic relations can capture semantic dependencies more effectively and accurately. The performance of S+D (i.e., LDGN) combining two aspect relations obtains significant improvement, which shows dynamic relations can make up the deficiency of statistical co-occurrence and correct the bias of global correlations. Thus, it is necessary to explore their joint effects to further boost the performance."
    }, {
      "heading" : "3.5 Performance on tail labels",
      "text" : "In order to prove the effectiveness of the proposed LDGN in alleviating the tail labels problem, we evaluate the performance of LDGN by propensity scored precision at k (PSP@k), which is calcu-\nlated as follow:\nPSP@k = 1\nk k∑ l=1 yrank(l) Prank(l) , (7)\nwhere Prank(l) is the propensity score (Jain et al., 2016) of label rank(l). Figure 2 shows the results of LDGN and LSAN on three datasets.\nAs shown in Figure 2(a), Figure 2(b) and Figure 2(c), the proposed LDGN performs better in predicting tail labels than the LSAN model (the best baseline) on three datasets. Specifically, on the RCV1 dataset, LDGN achieves 0.97% and 1.35% absolute improvement in term of PSP@3 and PSP@5 compared with LSAN. On the AAPD dataset, the PSP@k increased by at least 0.63% up to 0.90%. And on the EUR-Lex dataset, LDGN achieves 1.94%, 3.64% and 4.93% absolute improvement on PSP@1, 3, 5 compared with LSAN. The reason for the improvement in the EUR-Lex dataset is more obvious is that the semantic interactions learning is more useful to capture related information in the case of a large number of labels.\nThe results prove that LDGN can effectively alleviate the problem of predicting tail labels."
    }, {
      "heading" : "3.6 Case Study",
      "text" : "To further verify the effectiveness of our label attention module and dual graph neural network in LDGN, we present a typical case and visualize the attention weights on the document words and the similarity scores between label-specific components. We show a test sample from original AAPD dataset, and the document belongs to three categories, ‘Physics and Society’ (physics.soc), ‘Computers and Society’ (cs.cy) and ‘Computational Engineering, Finance, and Science’ (cs.ce). Visualization of Attention We can observe from the Figure 4 that different labels focus on different parts in the document text, and each label has its own concerned words. For example,\nthe more important parts in the ‘physics.soc’ category are ‘digitalization power grid’, ‘energy management’. And the words that the ‘cs.ce’ category focuses on are ‘consuming systems’, ‘varying prices’, ‘laying foundations’, ‘lower ’ and etc. For class ‘cs.cy’, the concerned words are ‘samples dutch distribution’, ‘evolutions’ and ‘topologies’. The corresponding related words of the three categories can reflect the semantics of the categories. Visualization of Interactions To gain a clearer view of the importance of our dual graph-based interactions learning module, we display two\nheatmaps in Figure 5 to visualize the partial graph structure of dual GCN. The edge weights shown in the heatmaps are obtained by global label cooccurrence and local dynamic relations (i.e., computed by Equation 5), respectively.\nAs presented in heatmaps, different relations between categories are captured by dual GCN. In global statistical relations, ‘cs.cy’ is highly linked with ‘physics.soc’ and wrong label ‘nlin.ao’, while the true label ‘cs.ce’ is isolated. And in local dynamic relations, ‘cs.cy’ is more related to ‘cs.ce’, and the correlations between wrong label ‘nlin.ao’ and true labels are reduced. This demonstrates that local dynamic relations can capture the latent relations that do not appear in global relations, and correct the bias of global correlations."
    }, {
      "heading" : "4 Related Work",
      "text" : "Multi-label Text Classification The existing methods for MLTC mainly focus on learning enhanced document representation (Liu et al., 2017) and modeling label dependency (Nam et al., 2017; Yang et al., 2018; Tsai and Lee, 2019) to improve the classification performance.\nWith the wide application of neural network methods for text representation, some innovative models have been developed for this task, which include traditional deep learning methods and Seq2Seq based methods. Liu et al. (2017) employed CNNs and dynamic pooling to learn the text representation for MLTC. However, they treated all words equally and cannot explored the informative words in documents. The Seq2Seq methods, such as MLC2Seq (Nam et al., 2017) and SGM (Yang et al., 2018), employed a RNN to encode the input text and an attention based RNN decoder to generate predicted labels sequentially. Although they used attention mechanism to capture the informative words in text content, these models cannot distinguish similar labels well. There is a big reason that most of them neglect the semantic connections between labels and document, and learn the same document representations for different labels.\nRecently, some studies (You et al., 2019; Xiao et al., 2019; Du et al., 2019) have used attention mechanism to explore the interactions between words and labels, and learned a labelspecific document representation for classification. These methods have obtained promising results in MLTC, which shows the importance of ex-\nploring semantic connections. However, they did not further study the interactions between labelspecific semantic components which can help to predict low-frequency labels.\nTo handle these issues, a common way to explore the semantic interactions between labelspecific parts in document, is to utilize the label graph based on statistical co-occurrences. MLC with Label Graph In order to capture the deep correlations of labels in a graph structure, many researches in image classification apply node embedding and graph neural network models to the task of multi-label image classification. Lee et al. (2018) incorporated knowledge graphs for describing the relationships between labels, and the information propagation can model the dependencies between seen and unseen labels for multi-label zero-shot learning. Chen et al. (2019) learned label representations with prior label correlation matrix in GCN, and mapped the label representations to inter-dependent classifiers, which achieved superior performance.\nHowever, there were few related approaches for multi-label classification of text. Zhang et al. (2018) established an explicit label cooccurrence graph to explore label embedding in low-dimension latent space.\nFurthermore, the statistical label correlations obtained by training data are incomplete and noisy. And the co-occurrence patterns between label pairs may form a long-tail distribution.\nThus, our goal is to find a way to explore the complete and adaptive interactions among labelspecific semantic components more accurately."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we propose a graph-based network LDGN to capture the semantic interactions related to corresponding labels, which jointly exploits global statistical patterns and local dynamic relations to derive complete and adaptive dependencies between different label-specific semantic parts. We first exploit multi-label attention to extract the label-specific semantic components from documents. Then, we employ GCN to learn component representations using label co-occurrences to guide the information propagation among components. After that, we use the learned component representations to compute the adjacency graph dynamically and re-learn with GCN based on the reconstruction graph. Extensive experiments con-\nducted on three public datasets show that the proposed LDGN model outperforms other state-ofthe-art models on multi-label text classification task and also demonstrates much higher effectiveness to alleviate the tail label problem. In the future, we will improve the proposed model in efficiency, for example we could construct a dynamic graph for a few samples rather than each sample. And besides, we will explore more information about labels for MLC classification."
    }, {
      "heading" : "Acknowledgement",
      "text" : "We gratefully thank the anonymous reviewers for their insightful comments. This research is supported by the Strategic Priority Research Program of the Chinese Academy of Sciences under Grant No. XDC02060400."
    } ],
    "references" : [ {
      "title" : "Multi-label image recognition with graph convolutional networks",
      "author" : [ "Zhao-Min Chen", "Xiu-Shen Wei", "Peng Wang", "Yanwen Guo." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5177–5186.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Explicit interaction model towards text classification",
      "author" : [ "Cunxiao Du", "Zhaozheng Chen", "Fuli Feng", "Lei Zhu", "Tian Gan", "Liqiang Nie." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6359–6366.",
      "citeRegEx" : "Du et al\\.,? 2019",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2019
    }, {
      "title" : "A deep learning-based approach for multi-label emotion classification in tweets",
      "author" : [ "Mohammed Jabreel", "Antonio Moreno." ],
      "venue" : "Applied Sciences, 9(6):1123.",
      "citeRegEx" : "Jabreel and Moreno.,? 2019",
      "shortCiteRegEx" : "Jabreel and Moreno.",
      "year" : 2019
    }, {
      "title" : "Extreme multi-label loss functions for recommendation, tagging, ranking & other missing label applications",
      "author" : [ "Himanshu Jain", "Yashoteja Prabhu", "Manik Varma." ],
      "venue" : "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge",
      "citeRegEx" : "Jain et al\\.,? 2016",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas N Kipf", "Max Welling." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Kipf and Welling.,? 2017",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2017
    }, {
      "title" : "Multi-label zero-shot learning with structured knowledge graphs",
      "author" : [ "Chung-Wei Lee", "Wei Fang", "Chih-Kuan Yeh", "YuChiang Frank Wang." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1576–1585.",
      "citeRegEx" : "Lee et al\\.,? 2018",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2018
    }, {
      "title" : "Rcv1: A new benchmark collection for text categorization research",
      "author" : [ "David D Lewis", "Yiming Yang", "Tony G Rose", "Fan Li." ],
      "venue" : "Journal of machine learning research, 5(Apr):361–397.",
      "citeRegEx" : "Lewis et al\\.,? 2004",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2004
    }, {
      "title" : "Weighted multi-label classification model for sentiment analysis of online news",
      "author" : [ "Xin Li", "Haoran Xie", "Yanghui Rao", "Yanjia Chen", "Xuebo Liu", "Huan Huang", "Fu Lee Wang." ],
      "venue" : "2016 International Conference on Big Data and Smart Computing (Big-",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep learning for extreme multi-label text classification",
      "author" : [ "Jingzhou Liu", "Wei-Cheng Chang", "Yuexin Wu", "Yiming Yang." ],
      "venue" : "Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval,",
      "citeRegEx" : "Liu et al\\.,? 2017",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2017
    }, {
      "title" : "A multi-label classification based approach for sentiment classification",
      "author" : [ "Shuhua Monica Liu", "Jiun-Hung Chen." ],
      "venue" : "Expert Systems with Applications, 42(3):1083–1093.",
      "citeRegEx" : "Liu and Chen.,? 2015",
      "shortCiteRegEx" : "Liu and Chen.",
      "year" : 2015
    }, {
      "title" : "Rectifier nonlinearities improve neural network acoustic models",
      "author" : [ "Andrew L Maas", "Awni Y Hannun", "Andrew Y Ng." ],
      "venue" : "Proc. icml, volume 30, page 3.",
      "citeRegEx" : "Maas et al\\.,? 2013",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2013
    }, {
      "title" : "Efficient pairwise multilabel classification for largescale problems in the legal domain",
      "author" : [ "Eneldo Loza Mencia", "Johannes Fürnkranz." ],
      "venue" : "Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 50–65.",
      "citeRegEx" : "Mencia and Fürnkranz.,? 2008",
      "shortCiteRegEx" : "Mencia and Fürnkranz.",
      "year" : 2008
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in neural information processing systems, pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Maximizing subset accuracy with recurrent neural networks in multilabel classification",
      "author" : [ "Jinseok Nam", "Eneldo Loza Mencı́a", "Hyunwoo J Kim", "Johannes Fürnkranz" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Nam et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Nam et al\\.",
      "year" : 2017
    }, {
      "title" : "Order-free learning alleviating exposure bias in multi-label classification",
      "author" : [ "Che-Ping Tsai", "Hung-Yi Lee." ],
      "venue" : "arXiv preprint arXiv:1909.03434.",
      "citeRegEx" : "Tsai and Lee.,? 2019",
      "shortCiteRegEx" : "Tsai and Lee.",
      "year" : 2019
    }, {
      "title" : "Multi-label chinese microblog emotion classification via convolutional neural network",
      "author" : [ "Yaqi Wang", "Shi Feng", "Daling Wang", "Ge Yu", "Yifei Zhang." ],
      "venue" : "Asia-Pacific Web Conference, pages 567–580. Springer.",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Label-specific document representation for multi-label text classification",
      "author" : [ "Lin Xiao", "Xin Huang", "Boli Chen", "Liping Jing." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Xiao et al\\.,? 2019",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2019
    }, {
      "title" : "Sgm: sequence generation model for multi-label classification",
      "author" : [ "Pengcheng Yang", "Xu Sun", "Wei Li", "Shuming Ma", "Wei Wu", "Houfeng Wang." ],
      "venue" : "arXiv preprint arXiv:1806.04822.",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Attentionxml: Label tree-based attention-aware deep model for high-performance extreme multi-label text classification",
      "author" : [ "Ronghui You", "Zihan Zhang", "Ziye Wang", "Suyang Dai", "Hiroshi Mamitsuka", "Shanfeng Zhu." ],
      "venue" : "Advances in Neural Informa-",
      "citeRegEx" : "You et al\\.,? 2019",
      "shortCiteRegEx" : "You et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep extreme multi-label learning",
      "author" : [ "Wenjie Zhang", "Junchi Yan", "Xiangfeng Wang", "Hongyuan Zha." ],
      "venue" : "Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval, pages 100–107.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Recently, with the growth of data scale, multi-label text classification(MLTC) has attracted more attention, since it is often applied to many fields such as sentiment analysis (Liu and Chen, 2015; Li et al., 2016), emotion recognition (Wang et al.",
      "startOffset" : 177,
      "endOffset" : 214
    }, {
      "referenceID" : 8,
      "context" : "Recently, with the growth of data scale, multi-label text classification(MLTC) has attracted more attention, since it is often applied to many fields such as sentiment analysis (Liu and Chen, 2015; Li et al., 2016), emotion recognition (Wang et al.",
      "startOffset" : 177,
      "endOffset" : 214
    }, {
      "referenceID" : 16,
      "context" : ", 2016), emotion recognition (Wang et al., 2016; Jabreel and Moreno, 2019), web page tagging (Jain et al.",
      "startOffset" : 29,
      "endOffset" : 74
    }, {
      "referenceID" : 2,
      "context" : ", 2016), emotion recognition (Wang et al., 2016; Jabreel and Moreno, 2019), web page tagging (Jain et al.",
      "startOffset" : 29,
      "endOffset" : 74
    }, {
      "referenceID" : 3,
      "context" : ", 2016; Jabreel and Moreno, 2019), web page tagging (Jain et al., 2016) and so on.",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 9,
      "context" : "*Corresponding Author representation (Liu et al., 2017) and modeling label dependency (Zhang et al.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 20,
      "context" : ", 2017) and modeling label dependency (Zhang et al., 2018; Yang et al., 2018; Tsai and Lee, 2019) to improve classification performance.",
      "startOffset" : 38,
      "endOffset" : 97
    }, {
      "referenceID" : 18,
      "context" : ", 2017) and modeling label dependency (Zhang et al., 2018; Yang et al., 2018; Tsai and Lee, 2019) to improve classification performance.",
      "startOffset" : 38,
      "endOffset" : 97
    }, {
      "referenceID" : 15,
      "context" : ", 2017) and modeling label dependency (Zhang et al., 2018; Yang et al., 2018; Tsai and Lee, 2019) to improve classification performance.",
      "startOffset" : 38,
      "endOffset" : 97
    }, {
      "referenceID" : 19,
      "context" : "Recently, some studies (You et al., 2019; Xiao et al., 2019; Du et al., 2019) have used attention mechanism to explore the above semantic connections, and learn a label-specific document representation for classification.",
      "startOffset" : 23,
      "endOffset" : 77
    }, {
      "referenceID" : 17,
      "context" : "Recently, some studies (You et al., 2019; Xiao et al., 2019; Du et al., 2019) have used attention mechanism to explore the above semantic connections, and learn a label-specific document representation for classification.",
      "startOffset" : 23,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "Recently, some studies (You et al., 2019; Xiao et al., 2019; Du et al., 2019) have used attention mechanism to explore the above semantic connections, and learn a label-specific document representation for classification.",
      "startOffset" : 23,
      "endOffset" : 77
    }, {
      "referenceID" : 5,
      "context" : "Then, we utilize GCN (Kipf and Welling, 2017) to learn the deep relationships between labelspecific semantic components guided by the statistical label correlations.",
      "startOffset" : 21,
      "endOffset" : 45
    }, {
      "referenceID" : 11,
      "context" : "where σ (·) denotes LeakyReLU (Maas et al., 2013) activation function.",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 5,
      "context" : "Â denotes the normalized adjacency matrix, and the normalization method (Kipf and Welling, 2017) is:",
      "startOffset" : 72,
      "endOffset" : 96
    }, {
      "referenceID" : 18,
      "context" : "Datasets We evaluate the proposed model on three benchmark multi-label text classification datasets, which are AAPD (Yang et al., 2018), EUR-Lex (Mencia and Fürnkranz, 2008) and RCV1 (Lewis et al.",
      "startOffset" : 116,
      "endOffset" : 135
    }, {
      "referenceID" : 12,
      "context" : ", 2018), EUR-Lex (Mencia and Fürnkranz, 2008) and RCV1 (Lewis et al.",
      "startOffset" : 17,
      "endOffset" : 45
    }, {
      "referenceID" : 7,
      "context" : ", 2018), EUR-Lex (Mencia and Fürnkranz, 2008) and RCV1 (Lewis et al., 2004).",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 19,
      "context" : "Evaluation Metric Following the settings of previous work (You et al., 2019; Xiao et al., 2019), we use precision at top K (P@k) and Normalized Discounted Cumulated Gains at top K (nDCG@k) for performance evaluation.",
      "startOffset" : 58,
      "endOffset" : 95
    }, {
      "referenceID" : 17,
      "context" : "Evaluation Metric Following the settings of previous work (You et al., 2019; Xiao et al., 2019), we use precision at top K (P@k) and Normalized Discounted Cumulated Gains at top K (nDCG@k) for performance evaluation.",
      "startOffset" : 58,
      "endOffset" : 95
    }, {
      "referenceID" : 17,
      "context" : "Implementation Details For a fair comparison, we apply the same dataset split as previous work (Xiao et al., 2019), which is also the original split provided by dataset publisher (Yang et al.",
      "startOffset" : 95,
      "endOffset" : 114
    }, {
      "referenceID" : 18,
      "context" : ", 2019), which is also the original split provided by dataset publisher (Yang et al., 2018; Mencia and Fürnkranz, 2008).",
      "startOffset" : 72,
      "endOffset" : 119
    }, {
      "referenceID" : 12,
      "context" : ", 2019), which is also the original split provided by dataset publisher (Yang et al., 2018; Mencia and Fürnkranz, 2008).",
      "startOffset" : 72,
      "endOffset" : 119
    }, {
      "referenceID" : 13,
      "context" : "The word embeddings in the proposed network are initialized with the 300-dimensional word vectors, which are trained on the datasets by Skipgram (Mikolov et al., 2013) algorithm.",
      "startOffset" : 145,
      "endOffset" : 167
    }, {
      "referenceID" : 4,
      "context" : "We use the Adam optimization method (Kingma and Ba, 2014) to minimize the cross-entropy loss, the learning rate is initialized to 1e-3 and gradually decreased during the process of training.",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 17,
      "context" : "The experimental results of all baseline models are directly cited from paper (Xiao et al., 2019).",
      "startOffset" : 78,
      "endOffset" : 97
    }, {
      "referenceID" : 18,
      "context" : "• SGM (Yang et al., 2018): a sequence generation model which models label correlations as an ordered sequence.",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 20,
      "context" : "• DXML (Zhang et al., 2018): a deep embedding method which models the feature space and label graph structure simultaneously.",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 19,
      "context" : "• AttentionXML (You et al., 2019): a label treebased deep learning model which uses a probabilistic label tree and multi-label attention to capture informative words in extreme-scale data.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : "• EXAM (Du et al., 2019): a novel framework that leverages the label information to compute the word-level interactions.",
      "startOffset" : 7,
      "endOffset" : 24
    }, {
      "referenceID" : 17,
      "context" : "• LSAN (Xiao et al., 2019): a label-specific attention network model based on self-attention and label-attention mechanism.",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 17,
      "context" : "For fair comparison, the experimental results of baseline models are directly cited from previous studies (Xiao et al., 2019).",
      "startOffset" : 106,
      "endOffset" : 125
    }, {
      "referenceID" : 17,
      "context" : "The experimental results of baselines are directly cited from (Xiao et al., 2019).",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : "where Prank(l) is the propensity score (Jain et al., 2016) of label rank(l).",
      "startOffset" : 39,
      "endOffset" : 58
    }, {
      "referenceID" : 9,
      "context" : "Multi-label Text Classification The existing methods for MLTC mainly focus on learning enhanced document representation (Liu et al., 2017) and modeling label dependency (Nam et al.",
      "startOffset" : 120,
      "endOffset" : 138
    }, {
      "referenceID" : 14,
      "context" : ", 2017) and modeling label dependency (Nam et al., 2017; Yang et al., 2018; Tsai and Lee, 2019) to improve the classification performance.",
      "startOffset" : 38,
      "endOffset" : 95
    }, {
      "referenceID" : 18,
      "context" : ", 2017) and modeling label dependency (Nam et al., 2017; Yang et al., 2018; Tsai and Lee, 2019) to improve the classification performance.",
      "startOffset" : 38,
      "endOffset" : 95
    }, {
      "referenceID" : 15,
      "context" : ", 2017) and modeling label dependency (Nam et al., 2017; Yang et al., 2018; Tsai and Lee, 2019) to improve the classification performance.",
      "startOffset" : 38,
      "endOffset" : 95
    }, {
      "referenceID" : 14,
      "context" : "The Seq2Seq methods, such as MLC2Seq (Nam et al., 2017) and SGM (Yang et al.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 18,
      "context" : ", 2017) and SGM (Yang et al., 2018), employed a RNN to encode the input text and an attention based RNN decoder to generate predicted labels sequentially.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 19,
      "context" : "Recently, some studies (You et al., 2019; Xiao et al., 2019; Du et al., 2019) have used attention mechanism to explore the interactions between words and labels, and learned a labelspecific document representation for classification.",
      "startOffset" : 23,
      "endOffset" : 77
    }, {
      "referenceID" : 17,
      "context" : "Recently, some studies (You et al., 2019; Xiao et al., 2019; Du et al., 2019) have used attention mechanism to explore the interactions between words and labels, and learned a labelspecific document representation for classification.",
      "startOffset" : 23,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "Recently, some studies (You et al., 2019; Xiao et al., 2019; Du et al., 2019) have used attention mechanism to explore the interactions between words and labels, and learned a labelspecific document representation for classification.",
      "startOffset" : 23,
      "endOffset" : 77
    } ],
    "year" : 2021,
    "abstractText" : "Multi-label text classification is one of the fundamental tasks in natural language processing. Previous studies have difficulties to distinguish similar labels well because they learn the same document representations for different labels, that is they do not explicitly extract label-specific semantic components from documents. Moreover, they do not fully explore the high-order interactions among these semantic components, which is very helpful to predict tail labels. In this paper, we propose a novel label-specific dual graph neural network (LDGN), which incorporates category information to learn label-specific components from documents, and employs dual Graph Convolution Network (GCN) to model complete and adaptive interactions among these components based on the statistical label cooccurrence and dynamic reconstruction graph in a joint way. Experimental results on three benchmark datasets demonstrate that LDGN significantly outperforms the state-of-the-art models, and also achieves better performance with respect to tail labels.",
    "creator" : "LaTeX with hyperref"
  }
}