{
  "name" : "2021.acl-long.43.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning Relation Alignment for Calibrated Cross-modal Retrieval",
    "authors" : [ "Shuhuai Ren", "Junyang Lin", "Guangxiang Zhao", "Rui Men", "An Yang", "Jingren Zhou", "Xu Sun", "Hongxia Yang" ],
    "emails" : [ "ren@stu.pku.edu.cn,", "zhaoguangxiang@pku.edu.cn", "xusun@pku.edu.cn", "junyang.ljy@alibaba-inc.com", "menrui.mr@alibaba-inc.com", "ya235025@alibaba-inc.com", "jingren.zhou@alibaba-inc.com", "yang.yhx@alibaba-inc.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 514–524\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n514"
    }, {
      "heading" : "1 Introduction",
      "text" : "Cross-modal retrieval, including image-text retrieval, video-text retrieval, etc., has long been an important downstream task in cross-modal representation learning. Image-Text Retrieval (ITR) aims at modeling the similarity of image-text pairs and recalling the most relevant one. It remains quite challenging due to the heterogeneity of the data and the semantic gap between two different modalities. To bridge this gap, neural networks are responsible for learning global representations of images\n∗Corresponding Author 1Our code is available at https://github.com/\nlancopku/IAIS\nand texts in a joint semantic space and aligning the images and texts with the same semantics (Faghri et al., 2018; Kiros et al., 2014). A straightforward way to enhance the alignment is to enforce the local matching between the object-oriented words and the corresponding image regions, and then leverage the object co-occurrence statistics (Liu et al., 2020; Zhang et al., 2020a) in the pairs for inference. Previous studies incorporate auxiliary knowledge source like scene graphs (Yu et al., 2020) or object tags (Li et al., 2020) to explicitly indicate the cross-\nmodal mapping. Other researches try to establish fine-grained interaction on cross-modal attention to reinforce the focus from words to their most relevant regions, and vice versa (Chen et al., 2020; Wang et al., 2019; Messina et al., 2020; Lee et al., 2018; Zhang et al., 2020b; Yang et al., 2020).\nHowever, such word-region alignment at object level serves only as the basis because it mainly focuses on the local semantics but lacks the matching of global features like the intra-modal relation. The intra-modal relation refers to the correlation of items within a textual or visual sequence. More specifically, given a sentence and an image that describe the same scene and are highly matched, the correlation of the items in the textual sequence should also agree with the correlation of the corresponding items in the visual sequence. But such constraint of relation consistency is neglected in previous works, which hinders performance and interpretability of the models. To corroborate this, we conduct a case study on Flickr30k Entities dataset (Plummer et al., 2015) to probe the agreement of relation-level semantics in pre-trained models like UNITER (Chen et al., 2020). We utilize the self-attention distribution as a representation of the intra-modal relations (Clark et al., 2019; Htut et al., 2019; Kovaleva et al., 2019).\nAs shown in Figure 1, the attention distributions grouped by the annotated object of the given text and image are in disagreement with each other. Specifically, the attention distribution in the linguistic modality is reasonable. However, in the visual modality, the region “a red shirt” pays inappropriate attention to the region of the dog that doesn’t appear in the text, which impairs the representation of this visual item, i.e., “a red shirt” under the condition of the corresponding text. Such mismatched attention distributions suggest that the model represents the same concept with inconsistent semantics, which misleads the model to reduce the estimated similarity of the positive pairs and further leads to the wrong predictions that they are unmatched. What’s even worse is that in practice, the input regions of the existing methods are extracted by a pre-trained object detector like Faster R-CNN (Ren et al., 2015). The visual features are much noisier due to over-sampling (Li et al., 2020; Anderson et al., 2018), which necessitates a stronger regularizer to guide the alignment of the intra-modal relations.\nMotivated by the above observations, we pro-\nmote the semantic alignment from object level to relation level. We leverage self-attention matrix to characterize the relation of items within one modality, and design Intra-modal Self-attention Distance (ISD), a novel metric to measure the consistency between textual and visual relations. Our empirical analysis illustrates that the ISD and the model performance on image-text retrieval are highly correlated, which verifies our hypothesis and inspires us to minimize the semantic distance between intramodal self-attentions in training. Accordingly, we propose a new regularized training method called Inter-modal Alignment on Intra-modal Selfattentions (IAIS) to calibrate two intra-modal attention distributions mutually via inter-modal alignment, which helps learn better contextualized representations for image-text pairs. The model performance of image-text retrieval on Flickr30k and MS COCO datasets is improved by a considerable margin with IAIS, which demonstrates the superiority of our proposal."
    }, {
      "heading" : "2 Measuring Semantic Distance between Intra-modal Relations",
      "text" : "In this section, we present a formal definition of intra-modal relation alignment (Section 2.1). Such alignment requires extracting the visual and linguistic items corresponding to all objects and sorting them in the same order to make their self-attention distributions comparable. We first introduce the mechanism for multimodal attention calculation, and then present the method of attention weight extraction for constructing comparable intra-modal self-attentions (Section 2.2). Finally, we propose a metric named Intra-modal Self-attention Distance (ISD) to quantify the relation consistency. We conduct an empirical analysis on prevailing models to verify the correlation of the model performance and our metric (Section 2.3)."
    }, {
      "heading" : "2.1 From Intra-modal Relation to Self-attention",
      "text" : "Given a sequence O = [o1, · · · , oN ] of N objects appeared in an image-text pair, the linguistic and visual representation of such object sequence can be written as L = [l1, · · · , lN ] and V = [v1, · · · , vN ], respectively. Each item li, vi with the same index refers to the same object oi.2 For every object, its\n2An object oi may require one or more tokens in the text and one or more regions in the image to describe, such that the linguistic item li and the visual item vi may refer to a collection of tokens and regions, respectively.\nrelation to the others is depicted in both the linguistic and the visual modality. From a linguistic view, we regard the following textual self-attention distribution as the relation Rli stems from li:\nRli = [ali→l1 , · · · , ali→li , · · · , ali→lN ], (1)\nwhere ali→lj is the attention weight from li to lj . Similarly, the relation Rvi from the view of the visual modality can be written as\nRvi = [avi→v1 , · · · , avi→vi , · · · , avi→vN ]. (2)\nConsequently, we can achieve relation-level alignment by narrowing the semantic distance, e.g., Kullback-Leibler Divergence, between the linguistic and visual self-attention distribution for all objects from i = 1 to N :\nmin ∑N\ni=1 distance (Rli ,Rvi) . (3)\nIn the original self-attention matrix, however, the attention weights of specific objects are scattered and disordered. We need to extract the target weights and reorder them to construct comparable attention distributions Rli and Rvi ."
    }, {
      "heading" : "2.2 Intra-modal Self-attention Reconstruction",
      "text" : "In this subsection, we first introduce the vanilla multimodal attention mechanism and then present a specific way of attention weight extraction.\nConsider models of single-stream Transformerbased architecture like UNITER (Chen et al., 2020). The model consists of a stack of Transformer layers with attention mechanism (Vaswani et al., 2017) and is responsible for encoding image-text pairs into feature representations. Given Q,K,V ∈ RN×d, the matrix of N query, key and value vectors with dimension d, respectively, the attention function Att(Q,K,V) is defined as:\nAtt(Q,K,V) = σ ( QK> ) V = σ (S)V. (4)\nHere, σ is a row-wise, scaled softmax and S is a matrix of attention scores that measure the similarity between every pair of query and key vectors. Let L and V denote the linguistic and the visual modality, respectively. Given a textual sequence XL of NL tokens and a visual sequence XV of NV regions, the input X = [XL‖XV ] in the single-stream architecture is a concatenation of two sequences with length N = NL+NV . Accordingly, the query and\nkey matrix3 can be written as\nQ = XWQ = ( XL XV ) WQ = ( QL QV ) K = XWK = ( XL XV ) WK = ( KL KV ) , (5)\nwhere WQ and WK are learnable parameters. Furthermore, the attention score matrix S ∈ RN×N can be organized into four submatrices (Bugliarello et al., 2020):\nS = QK> = ( QL QV )( K>LK > V ) = ( QLK > L QLK > V\nQVK > L QVK > V ) = ( SLL SLV SVL SVV ) .\n(6)\nThe matrices SLL and SVV on the diagonal represent the linguistic and the visual intra-modal self-attention, respectively. SLV and SVL on back-diagonal represent the inter-modal attention scores from text to image, and the opposite. We regard the self-attention σ (SLL) and σ (SVV) as depictions of the intra-modal relations. Each row of the matrix represents the relation stemming from one linguistic or visual item to the others within the same modality.\nTo construct the comparable intra-modal selfattention matrices, we leverage the object annotations in the Flickr30k Entities dataset (Plummer\n3The value matrix V is omitted for brevity.\net al., 2015) to extract the tokens, regions, and attention weights with respect to the target objects. As shown in Figure 2, the text and the image both contain annotated objects of “two surfers” and “the waves”. The linguistic object sequence can be written as L = [l1, l2] = [“two surfers”, “the waves”]. These two objects derive four intrinsic relations and can be described by four patches in the original linguistic self-attention matrix SLL. For clarity, we define an operation Ext(S, oi, oj) that extracts the patch of attention scores in matrix S from the object oi to oj . Accordingly, the relation from “two surfers” to “the waves” can be denoted as Ext (SLL, l1, l2). To describe the relation with a single value instead of a sub-matrix, we further construct an operation Cps(·) to summarize the attention patch S ∈ RM×N to a scalar via column-wise sum and row-wise average:\nCps(S) = (∑M\ni ∑N j Sij ) /M. (7)\nAfter the above processing, we complete the extraction of the linguistic self-attention SLL through grouping the items by annotated object. The extraction of visual self-attention SVV is similar and the final results are denoted as S(a)LL and S (a) VV . As our processing for two intra-modal self-attentions follows the same order of object annotations, the matrices S(a)LL and S (a) VV from two modalities are of the same dimension and comparable."
    }, {
      "heading" : "2.3 Intra-modal Self-attention Distance with Annotation (ISDa)",
      "text" : "Given two comparable matrices S(a)LL and S (a) VV , we propose a metric called Intra-modal Self-attention Distance with annotation (ISDa) to quantify their semantic gap at the relation level. We define the following symmetric matrix-based Kullback-Leibler Divergence (m-KL) for measuring the distance between two matrices A and B:\nm-KL(A,B) = ∑N\ni KL (Ai‖Bi) + KL (Bi‖Ai) , (8)\nwhere (·)i stands for the ith row-vector in the matrix and KL denotes the Kullback-Leibler Divergence. Accordingly, the final ISDa metric for S(a)LL and S(a)VV is defined as:\nISDa = m-KL ( S (a) LL, S (a) VV ) . (9)\nWe present our algorithm for the calculation of ISDa in Algorithm 1.\nAlgorithm 1: Intra-modal Self-attention Distance with Annotation (ISDa)\nTo study the correlation between the ISDa metric and the model performance,4 we conduct an empirical analysis on UNITER (Chen et al., 2020). As shown in Figure 3, the ISDa decreases during the training phase while the model performance continues to increase. They are strongly correlated with a Pearson’s correlation coefficient of -0.60. After the middle stage of training, the curve of the model performance and ISDa tends to be flat, suggesting that merely optimizing the task-oriented loss function while neglecting the constraint of relation consistency hinders the model from achieving better performance. To eliminate the bottleneck, we can minimize the ISD in the training phase as a regularization to induce further improvement for the ITR task and better the model interpretability.\n4We use the Meta-Sum (Chen et al., 2020), sum of Recall@1, Recall@5, Recall@10 across the image and text retrieval as a metric for model performance."
    }, {
      "heading" : "3 Inter-modal Alignment on Intra-modal Self-attentions (IAIS)",
      "text" : "In this section, we propose a new regularized training method, Inter-modal Alignment on Intra-modal Self-attentions (IAIS), for image-text retrieval. Our goal is to enhance the semantic alignment of relations by minimizing the distance between two intra-modal self-attentions (ISD).\nIn practice, given the original visual and linguistic input sequence V = [v1, · · · , vNV ], L = [l1, · · · , lNL ] with the scattered items,5 there are no object annotations and the region features extracted by Faster R-CNN are much noisier (Li et al., 2020; Anderson et al., 2018), which results in difficulty in grouping the attention weights by ground-truth object. The ISDa thus cannot be used directly as the objective function to minimize.\nTo tackle this problem, we regard the input sequence from one modality (e.g., the visual sequence V) as an anchor. For every item in the anchor sequence, we extract its corresponding representation from the other modality (e.g., one item or a collection of items in the linguistic sequence L) to reconstruct a mirrored sequence. After that, the items and their relations within the anchor sequence have a one-to-one correspondence with the items and relations within the mirrored sequence, which makes the intra-modal self-attentions derived from the two sequences comparable. In the next two subsections, we propose two methods, singular alignment and distributed alignment, to accomplish the attention extraction and reconstruction. The former establishes a one-to-one mapping between\n5As there are no object annotations in practice, each visual item now refers to only one region. Each linguistic item also refers to only one token, even if it is a sub-word.\nlinguistic and visual attention weight, while the latter establishes a distributed mapping. Besides, we design two losses L(s)IAIS and L (d) IAIS as a surrogate of the ISDa to measure the semantic distance between intra-modal self-attention matrices. Finally, we incorporate the surrogate loss minimization as a regularization to calibrate intra-modal self-attentions mutually and achieve the relation-level alignment."
    }, {
      "heading" : "3.1 Singular Alignment",
      "text" : "For every item in the anchor sequence, singular alignment utilizes the inter-modal attention to find its most relevant item from the opposite modality. As the inter-modal attention score quantifies the similarity between the items from two modalities, the visual and the linguistic item with the highest score can be aligned with each other. For example, given the ith visual item vi and the inter-modal attention matrix SVL, the similarities between vi and all the linguistic items are depicted in SVL[i, :], i.e., the ith row of the matrix. Hence the most relevant linguistic item for vi can be denoted as li∗ , where i∗ = argmaxSVL[i, :]. Accordingly, for every weight avi→vj in the original visual self-attention matrix SVV , its corresponding weight ali∗→lj∗ in the linguistic self-attention matrix SLL can be extracted by the following operation:6\nali∗→lj∗ = Ext (SLL, li∗ , lj∗) ,\ni∗ = argmaxSVL[i, :], j∗ = argmaxSVL[j, :],\n(10)\nas a singular alignment. After all the extractions, we reconstruct a mirrored matrix S(s)VV such that S (s) VV [i, j] = ali∗→lj∗ , which can be regarded as a\n6Compared with Section 2.2, the Ext operation here extracts a singular attention weight instead of a patch.\nAlgorithm 2: Singular Alignment Input: Intra-modal self-attention matrices SLL, SVV for i = 1 to NV do\ni∗ ← argmaxSVL[i, :] for j = 1 to NV do\nj∗ ← argmaxSVL[j, :] S (s) VV [i, j]← Ext (SLL, li∗ , lj∗)\nfor i = 1 to NL do i∗ ← argmaxSLV [i, :] for j = 1 to NL do\nj∗ ← argmaxSLV [j, :] S (s) LL[i, j]← Ext (SVV , vi∗ , vj∗)\nL (s) IAIS = m-KL ( σ(SVV) , σ(S (s) VV) ) +\nm-KL ( σ(SLL) , σ(S (s) LL) )\nreturn L(s)IAIS\nrepresentation of the original visual self-attention SVV from the linguistic view. The surrogate loss of ISDa between SVV and S (s) VV is denoted as L (s) IAIS-V when taking vision as the anchor modality. The similar processing can also be performed when the linguistic sequence is the anchor. We can generate the matrix S(s)LL as a visual representation of the linguistic self-attention SLL and define a corresponding loss L(s)IAIS-L.\nThe detailed processing of singular alignment is illustrated in Algorithm 2 and Figure 4. The singular version of IAIS loss is defined as:\nL (s) IAIS =L (s) IAIS-V + L (s) IAIS-L =m-KL ( σ(SVV) , σ(S (s) VV) ) +\nm-KL ( σ(SLL) , σ(S (s) LL) ) .\n(11)"
    }, {
      "heading" : "3.2 Distributed Alignment",
      "text" : "As singular items from different modalities may not be able to give a full representation for each other, we further propose distributed alignment, which utilizes a collection of linguistic items as a representation of a visual item, and vice versa. Specifically, given two visual items vi and vj , we regard the inter-modal attentions σ(SVL[i, :])7 from vi to all linguistic items and σ(SLV [:, j])8 from all linguistic items to vj as a kind of features. Hence the original similarity SVV [i, j] = avi→vj between vi and vj can also be modeled as a dot-product of their distributed attention features from the cross-modal view: σ(SVL[i, :]) · σ(SLV [:, j]). Such distributed\n7The ith row of SVL. 8The jth column of SLV .\nalignment leverages the language as a bridge to draw implicit connections within the visual modality, which can be intuitively regarded as the backtranslation (Sennrich et al., 2016) for multimodal. As shown in Figure 4, the distributed version of mirrored self-attention matrix can be constructed by a matrix multiplication of two inter-modal attention matrices:\nS (d) VV = σ(SVL) σ(SLV), S (d) LL = σ(SLV) σ(SVL).\n(12)\nSimilar to the version of singular alignment, the distributed IAIS loss can be written as:\nL (d) IAIS =L (d) IAIS-V + L (d) IAIS-L =m-KL ( σ(SVV) , S (d) VV ) +\nm-KL ( σ(SLL) , S (d) LL ) .\n(13)"
    }, {
      "heading" : "3.3 Relation Alignment as Regularizer",
      "text" : "With the IAIS loss, the surrogate of semantic distance between two intra-modal self-attentions, we present a new regularized training method to enhance the relation alignment for image-text retrieval. Our final loss is two-fold. The first is the task-orientated margin loss:\nLmargin = ∑Np\ni=1 [∑Nn j=1 Sj − Si + α ] + , (14)\nwhere [x]+ = max(0, x) and α is a preset margin. Np and Nn denote the number of positive and negative pairs. Si and Sj are the similarity scores of a positive and negative image-text pair, respectively. The second is the IAIS loss for all positive pairs that quantifies their relation distance. The IAIS loss is computed based on the attentions from the last Transformer-layer, and it can be either the singular alignment version (Eq. (11)) or the distributed alignment version (Eq. (13)). To summarize, our final final loss can be formalized as:\nL = Lmargin + λtLIAIS, (15)\nwhere λt is a hyper-parameter w.r.t training steps t to balance two loss items. Since our relation-level alignment is based on mappings between linguistic and visual items, it is beneficial to focus on the item-level alignment at the previous training stage via the task-orientated loss. Accordingly, we utilize Training Signal Annealing (Xie et al., 2020) to gradually incorporate the signal of the IAIS loss and design the following exponential schedule:\nλt = exp ((t/T − 1)× 5) . (16)\nHere T is the total training steps during fine-tuning phase and t is the current step. As a pluggable regularizer, our IAIS method does NOT incorporate any extra parameters and additional data collection yet empowers the models to capture the higherlevel semantics of relation consistency efficiently."
    }, {
      "heading" : "4 Experimental Settings",
      "text" : ""
    }, {
      "heading" : "4.1 Benchmark Datasets",
      "text" : "We conduct experiments on the Flickr30k (Young et al., 2014) and MS COCO (Lin et al., 2014) datasets. Flickr30K contains 31K images collected from the Flickr website, with five textual descriptions per image. We follow Karpathy and Li (2015) to split the data into 30K/1K/1K training/validation/test splits. MS COCO consists of 123K images, each accompanied with five humanwritten captions. Following Karpathy and Li (2015), the data is divided into 82K/5K/5K training/validation/test images."
    }, {
      "heading" : "4.2 Fine-tuning Settings",
      "text" : "Due to the limitation of computing resource, we only incorporate IAIS regularization in the phase of fine-tuning instead of pre-training. We use the base (12 layers) and the large (24 layers) version of UNITER (Chen et al., 2020), one of the most prevailing large-scale pre-trained models, as our baseline and backbone for IAIS. We follow the fine-tuning setting and hyper-parameter configuration of the original paper.9 The margin in Eq. (14) is 0.2. For each positive instance, 31 hard negative instances are sampled on the text and image side, respectively, and as each batch contains 8 different\n9https://github.com/ChenRocks/UNITER\npositive instances, the batch size is 512. The learning rate is 5e-5 and the training steps are 5000 for both base and large models. All experiments are run on 8 NVIDIA V100 GPUs."
    }, {
      "heading" : "5 Results and Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Main Results",
      "text" : "The main results of the UNITER performance with and without our IAIS regularization are reported in Table 1. Our methods of both singular and distributed version surpass the baseline by a considerable margin. The average improvement over all datasets and models is 4.49.\nThere are also some interesting findings: (1) Compared with image retrieval, the model performance on text retrieval is boosted by IAIS more remarkably with an average improvement of 3.50. Note that each image in both datasets is paired with five ground-truth sentences, and our IAIS regularizer helps the model capture the common relations for the image and the corresponding texts so that more ground-truth texts can be successfully retrieved. (2) The improvement on UNITER-base is 17.2% higher than that on UNITER-large. A consistent result can be found in Table 2, which demonstrates various relation distance metrics of fine-tuned models. The ISDa of UNITER-large is smaller than that of UNITER-base, indicating UNITER-large learns more about the relation consistency due to its large capability while there is still room to improve the relation alignment with our IAIS method. (3) The relative improvement brought by the singular version of IAIS is 7.0%, higher than that of the distributed version. The ISDa and L(s)IAIS are correlated with a Pearson’s cor-\nrelation coefficient of 0.779, which is also higher compared to L(d)IAIS with 0.774. Besides, our empirical analysis in Figure 5 shows that it is slightly easier to optimize the L(s)IAIS, indicating it is a better surrogate of ISDa."
    }, {
      "heading" : "5.2 Effect of Anchor Modality",
      "text" : "In Section 3.3, we leverage both the linguistic and the visual input as the anchor sequence to reconstruct the mirrored sequence from the opposite modalities. To study the impact of the anchor modality, we conduct an ablation study and the results are listed in Table 3. Compared to using language as the anchor modality, i.e., only LIAIS-L is incorporated, the overall model performance is 2.1 higher when vision is taken as the anchor. An explanation is that the description capability of visual regions is more concrete and powerful. However, introducing both LIAIS-V + LIAIS-L to the final loss can achieve a further improvement of 2.22, which indicates the necessity of such combination."
    }, {
      "heading" : "5.3 Effect of Annealing Schedule",
      "text" : "Besides the exp schedule in Eq. (16) for training signal annealing, we also try other schedules:\n• log schedule: λt = 1− exp (−t/T × γ);\n• linear schedule: λt = t/T ;\n• exp schedule: λt = exp ((t/T − 1)× γ),\nwhere γ is chosen from {5, 10}. All the schedules are shown in Figure 6.\nWe compare the results of five schedules for IAIS signal annealing. The results in Figure 8 show that the exp schedule with scale γ = 5 achieves the best performance."
    }, {
      "heading" : "5.4 Effect of Layer to Apply IAIS",
      "text" : "We also apply IAIS on different layers of UNITERbase. As illustrated in Figure 9, the optimal way is to apply IAIS on the last layer. We speculate that it is more important to learn relation alignment in the deeper layers because the attention in the deeper layers has a bigger impact on the final output, while the effect of the attention in shallow layers might fade away due to the normalization."
    }, {
      "heading" : "5.5 Case Study",
      "text" : "We further discuss the advantage of our proposed relation-level alignment. Figure 7 shows two visualization examples of the intra-modal selfattentions from the Flickr30k Entities dataset. With IAIS regularization, the model is instructed to concentrate on the common relations within the linguistic and visual sequence, yielding more calibrated and consistent self-attention distributions."
    }, {
      "heading" : "6 Related Work",
      "text" : "In this section, we introduce the task of image-text retrieval and review the representative studies of\nlarge-scale multimodal pre-trained models.\nImage-Text Retrieval Image-Text Retrieval (ITR, Barnard et al., 2003; Barnard and Forsyth, 2001), also known as Image-Text Matching, is one of the popular and challenging Languageand-Vision (V+L) tasks. Given image-text pairs, the prevailing approaches project them into a joint representation space, on which cosine or dot-product similarities are defined, and recall the most relevant one according to the similarity.\nMultimodal Pre-trained Models The development of the transformer-based large-scale pretraining paradigm sweeps across the area of multimodal learning and achieves many state-of-the-art results on V+L tasks like Image Captioning, Visual Question Answering, Visual Commonsense Reasoning, etc. Recent prevailing multimodal pre-trained models can be categorized into singlestream (Chen et al., 2020; Gan et al., 2020; Lin et al., 2020; Li et al., 2020; Su et al., 2020; Lin et al., 2021) and two-stream (Yu et al., 2020; Tan and Bansal, 2019; Lu et al., 2019) models. Given a piece of text and an image, the former architecture concatenates the features of tokens and regions and learns their joint representations with one transformer model, while the latter embeds the textual and the visual input separately with two independent intra-modal transformers and then utilizes an\ninter-modal transformer to reinforce cross-modal interactions via cross-modal attention modules."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we promote the semantic alignment for cross-modal retrieval from the object level to the relation level. We propose a surrogate metric to quantify the relation consistency by measuring the semantic distance between linguistic and visual relations. Furthermore, we present a regularized training method IAIS to calibrate intra-modal selfattentions mutually by minimizing the ISD metric. Our method improves both the performance and the interpretability of large-scale pre-trained models. Note that, without object annotation in practice, the singular and distributed version of the IAIS loss only provides a coarse-grained attention distribution alignment. We leave the elaborate design of ISDa proxy function for future work."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is partly supported by Beijing Academy of Artificial Intelligence (BAAI). We thank all the anonymous reviewers for their constructive comments, and Xuancheng Ren and Lei Li for their helpful suggestions in preparing the manuscript."
    } ],
    "references" : [ {
      "title" : "Bottom-up and top-down attention for image captioning and visual question answering",
      "author" : [ "Peter Anderson", "Xiaodong He", "Chris Buehler", "Damien Teney", "Mark Johnson", "Stephen Gould", "Lei Zhang." ],
      "venue" : "2018 IEEE Conference on Computer Vision and Pat-",
      "citeRegEx" : "Anderson et al\\.,? 2018",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2018
    }, {
      "title" : "Matching words and pictures",
      "author" : [ "Kobus Barnard", "Pinar Duygulu", "David A. Forsyth", "Nando de Freitas", "David M. Blei", "Michael I. Jordan." ],
      "venue" : "J. Mach. Learn. Res., 3:1107–1135.",
      "citeRegEx" : "Barnard et al\\.,? 2003",
      "shortCiteRegEx" : "Barnard et al\\.",
      "year" : 2003
    }, {
      "title" : "Learning the semantics of words and pictures",
      "author" : [ "Kobus Barnard", "David A. Forsyth." ],
      "venue" : "Proceedings of the Eighth International Conference On Computer Vision (ICCV-01), Vancouver, British Columbia, Canada, July 7-14, 2001 - Volume 2,",
      "citeRegEx" : "Barnard and Forsyth.,? 2001",
      "shortCiteRegEx" : "Barnard and Forsyth.",
      "year" : 2001
    }, {
      "title" : "Multimodal pretraining unmasked: Unifying the vision and language berts",
      "author" : [ "Emanuele Bugliarello", "Ryan Cotterell", "Naoaki Okazaki", "Desmond Elliott." ],
      "venue" : "CoRR, abs/2011.15124.",
      "citeRegEx" : "Bugliarello et al\\.,? 2020",
      "shortCiteRegEx" : "Bugliarello et al\\.",
      "year" : 2020
    }, {
      "title" : "UNITER: universal image-text representation learning",
      "author" : [ "Yen-Chun Chen", "Linjie Li", "Licheng Yu", "Ahmed El Kholy", "Faisal Ahmed", "Zhe Gan", "Yu Cheng", "Jingjing Liu." ],
      "venue" : "Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, Au-",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "What does BERT look at? an analysis of bert’s attention",
      "author" : [ "Kevin Clark", "Urvashi Khandelwal", "Omer Levy", "Christopher D. Manning." ],
      "venue" : "CoRR, abs/1906.04341.",
      "citeRegEx" : "Clark et al\\.,? 2019",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "VSE++: improving visualsemantic embeddings with hard negatives",
      "author" : [ "Fartash Faghri", "David J. Fleet", "Jamie Ryan Kiros", "Sanja Fidler." ],
      "venue" : "British Machine Vision Conference 2018, BMVC 2018, Newcastle, UK, September 3-6, 2018, page 12. BMVA",
      "citeRegEx" : "Faghri et al\\.,? 2018",
      "shortCiteRegEx" : "Faghri et al\\.",
      "year" : 2018
    }, {
      "title" : "Large-scale adversarial training for vision-and-language representation learning",
      "author" : [ "Zhe Gan", "Yen-Chun Chen", "Linjie Li", "Chen Zhu", "Yu Cheng", "Jingjing Liu." ],
      "venue" : "Advances in Neural Information Processing Systems 33: Annual Conference on Neu-",
      "citeRegEx" : "Gan et al\\.,? 2020",
      "shortCiteRegEx" : "Gan et al\\.",
      "year" : 2020
    }, {
      "title" : "Do attention heads in BERT track syntactic dependencies? CoRR, abs/1911.12246",
      "author" : [ "Phu Mon Htut", "Jason Phang", "Shikha Bordia", "Samuel R. Bowman" ],
      "venue" : null,
      "citeRegEx" : "Htut et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Htut et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep visualsemantic alignments for generating image descriptions",
      "author" : [ "Andrej Karpathy", "Fei-Fei Li." ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA,",
      "citeRegEx" : "Karpathy and Li.,? 2015",
      "shortCiteRegEx" : "Karpathy and Li.",
      "year" : 2015
    }, {
      "title" : "Unifying visual-semantic embeddings with multimodal neural language models",
      "author" : [ "Ryan Kiros", "Ruslan Salakhutdinov", "Richard S. Zemel." ],
      "venue" : "CoRR, abs/1411.2539.",
      "citeRegEx" : "Kiros et al\\.,? 2014",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2014
    }, {
      "title" : "Revealing the dark secrets of BERT",
      "author" : [ "Olga Kovaleva", "Alexey Romanov", "Anna Rogers", "Anna Rumshisky." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Nat-",
      "citeRegEx" : "Kovaleva et al\\.,? 2019",
      "shortCiteRegEx" : "Kovaleva et al\\.",
      "year" : 2019
    }, {
      "title" : "Stacked cross attention for image-text matching",
      "author" : [ "Kuang-Huei Lee", "Xi Chen", "Gang Hua", "Houdong Hu", "Xiaodong He." ],
      "venue" : "Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part IV,",
      "citeRegEx" : "Lee et al\\.,? 2018",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2018
    }, {
      "title" : "Oscar: Object-semantics aligned pre-training for vision-language tasks",
      "author" : [ "Xiujun Li", "Xi Yin", "Chunyuan Li", "Pengchuan Zhang", "Xiaowei Hu", "Lei Zhang", "Lijuan Wang", "Houdong Hu", "Li Dong", "Furu Wei", "Yejin Choi", "Jianfeng Gao." ],
      "venue" : "Computer Vision -",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "M6: A chinese multimodal pretrainer",
      "author" : [ "Li", "Wei Lin", "Jingren Zhou", "Jie Tang", "Hongxia Yang." ],
      "venue" : "CoRR, abs/2103.00823.",
      "citeRegEx" : "Li et al\\.,? 2021",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2021
    }, {
      "title" : "Interbert: Visionand-language interaction for multi-modal pretraining",
      "author" : [ "Junyang Lin", "An Yang", "Yichang Zhang", "Jie Liu", "Jingren Zhou", "Hongxia Yang." ],
      "venue" : "CoRR, abs/2003.13198.",
      "citeRegEx" : "Lin et al\\.,? 2020",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2020
    }, {
      "title" : "Microsoft COCO: common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge J. Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C. Lawrence Zitnick." ],
      "venue" : "Computer Vision - ECCV 2014 - 13th European Conference, Zurich,",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Graph structured network for image-text matching",
      "author" : [ "Chunxiao Liu", "Zhendong Mao", "Tianzhu Zhang", "Hongtao Xie", "Bin Wang", "Yongdong Zhang." ],
      "venue" : "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA,",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "author" : [ "Jiasen Lu", "Dhruv Batra", "Devi Parikh", "Stefan Lee." ],
      "venue" : "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Infor-",
      "citeRegEx" : "Lu et al\\.,? 2019",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2019
    }, {
      "title" : "Fine-grained visual textual alignment for cross-modal retrieval using transformer encoders",
      "author" : [ "Nicola Messina", "Giuseppe Amato", "Andrea Esuli", "Fabrizio Falchi", "Claudio Gennaro", "Stéphane Marchand-Maillet." ],
      "venue" : "CoRR, abs/2008.05231.",
      "citeRegEx" : "Messina et al\\.,? 2020",
      "shortCiteRegEx" : "Messina et al\\.",
      "year" : 2020
    }, {
      "title" : "Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models",
      "author" : [ "Bryan A. Plummer", "Liwei Wang", "Chris M. Cervantes", "Juan C. Caicedo", "Julia Hockenmaier", "Svetlana Lazebnik." ],
      "venue" : "2015 IEEE International",
      "citeRegEx" : "Plummer et al\\.,? 2015",
      "shortCiteRegEx" : "Plummer et al\\.",
      "year" : 2015
    }, {
      "title" : "Faster R-CNN: towards real-time object detection with region proposal networks",
      "author" : [ "Shaoqing Ren", "Kaiming He", "Ross B. Girshick", "Jian Sun." ],
      "venue" : "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Pro-",
      "citeRegEx" : "Ren et al\\.,? 2015",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "Improving neural machine translation models with monolingual data",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016,",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "VL-BERT: pretraining of generic visual-linguistic representations",
      "author" : [ "Weijie Su", "Xizhou Zhu", "Yue Cao", "Bin Li", "Lewei Lu", "Furu Wei", "Jifeng Dai." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April",
      "citeRegEx" : "Su et al\\.,? 2020",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2020
    }, {
      "title" : "LXMERT: learning cross-modality encoder representations from transformers",
      "author" : [ "Hao Tan", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Nat-",
      "citeRegEx" : "Tan and Bansal.,? 2019",
      "shortCiteRegEx" : "Tan and Bansal.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "CAMP: cross-modal adaptive message passing for text-image retrieval",
      "author" : [ "Zihao Wang", "Xihui Liu", "Hongsheng Li", "Lu Sheng", "Junjie Yan", "Xiaogang Wang", "Jing Shao." ],
      "venue" : "2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019,",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised data augmentation for consistency training",
      "author" : [ "Qizhe Xie", "Zihang Dai", "Eduard H. Hovy", "Thang Luong", "Quoc Le." ],
      "venue" : "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "Xie et al\\.,? 2020",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2020
    }, {
      "title" : "Visual agreement regularized training for multi-modal machine translation",
      "author" : [ "Pengcheng Yang", "Boxing Chen", "Pei Zhang", "Xu Sun." ],
      "venue" : "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Ap-",
      "citeRegEx" : "Yang et al\\.,? 2020",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    }, {
      "title" : "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
      "author" : [ "Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier." ],
      "venue" : "Trans. Assoc. Comput. Linguistics, 2:67–78.",
      "citeRegEx" : "Young et al\\.,? 2014",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 2014
    }, {
      "title" : "Ernie-vil: Knowledge enhanced vision-language representations through scene graph",
      "author" : [ "Fei Yu", "Jiji Tang", "Weichong Yin", "Yu Sun", "Hao Tian", "Hua Wu", "Haifeng Wang." ],
      "venue" : "CoRR, abs/2006.16934.",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning to represent image and text with denotation graph",
      "author" : [ "Bowen Zhang", "Hexiang Hu", "Vihan Jain", "Eugene Ie", "Fei Sha." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online,",
      "citeRegEx" : "Zhang et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "DCA: diversified coattention towards informative live video commenting",
      "author" : [ "Zhihan Zhang", "Zhiyi Yin", "Shuhuai Ren", "Xinhang Li", "Shicheng Li." ],
      "venue" : "Natural Language Processing and Chinese Computing - 9th CCF International Confer-",
      "citeRegEx" : "Zhang et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "and texts in a joint semantic space and aligning the images and texts with the same semantics (Faghri et al., 2018; Kiros et al., 2014).",
      "startOffset" : 94,
      "endOffset" : 135
    }, {
      "referenceID" : 10,
      "context" : "and texts in a joint semantic space and aligning the images and texts with the same semantics (Faghri et al., 2018; Kiros et al., 2014).",
      "startOffset" : 94,
      "endOffset" : 135
    }, {
      "referenceID" : 17,
      "context" : "A straightforward way to enhance the alignment is to enforce the local matching between the object-oriented words and the corresponding image regions, and then leverage the object co-occurrence statistics (Liu et al., 2020; Zhang et al., 2020a) in the pairs for inference.",
      "startOffset" : 205,
      "endOffset" : 244
    }, {
      "referenceID" : 31,
      "context" : "A straightforward way to enhance the alignment is to enforce the local matching between the object-oriented words and the corresponding image regions, and then leverage the object co-occurrence statistics (Liu et al., 2020; Zhang et al., 2020a) in the pairs for inference.",
      "startOffset" : 205,
      "endOffset" : 244
    }, {
      "referenceID" : 30,
      "context" : "Previous studies incorporate auxiliary knowledge source like scene graphs (Yu et al., 2020) or object tags (Li et al.",
      "startOffset" : 74,
      "endOffset" : 91
    }, {
      "referenceID" : 13,
      "context" : ", 2020) or object tags (Li et al., 2020) to explicitly indicate the cross-",
      "startOffset" : 23,
      "endOffset" : 40
    }, {
      "referenceID" : 4,
      "context" : "Other researches try to establish fine-grained interaction on cross-modal attention to reinforce the focus from words to their most relevant regions, and vice versa (Chen et al., 2020; Wang et al., 2019; Messina et al., 2020; Lee et al., 2018; Zhang et al., 2020b; Yang et al., 2020).",
      "startOffset" : 165,
      "endOffset" : 283
    }, {
      "referenceID" : 26,
      "context" : "Other researches try to establish fine-grained interaction on cross-modal attention to reinforce the focus from words to their most relevant regions, and vice versa (Chen et al., 2020; Wang et al., 2019; Messina et al., 2020; Lee et al., 2018; Zhang et al., 2020b; Yang et al., 2020).",
      "startOffset" : 165,
      "endOffset" : 283
    }, {
      "referenceID" : 19,
      "context" : "Other researches try to establish fine-grained interaction on cross-modal attention to reinforce the focus from words to their most relevant regions, and vice versa (Chen et al., 2020; Wang et al., 2019; Messina et al., 2020; Lee et al., 2018; Zhang et al., 2020b; Yang et al., 2020).",
      "startOffset" : 165,
      "endOffset" : 283
    }, {
      "referenceID" : 12,
      "context" : "Other researches try to establish fine-grained interaction on cross-modal attention to reinforce the focus from words to their most relevant regions, and vice versa (Chen et al., 2020; Wang et al., 2019; Messina et al., 2020; Lee et al., 2018; Zhang et al., 2020b; Yang et al., 2020).",
      "startOffset" : 165,
      "endOffset" : 283
    }, {
      "referenceID" : 32,
      "context" : "Other researches try to establish fine-grained interaction on cross-modal attention to reinforce the focus from words to their most relevant regions, and vice versa (Chen et al., 2020; Wang et al., 2019; Messina et al., 2020; Lee et al., 2018; Zhang et al., 2020b; Yang et al., 2020).",
      "startOffset" : 165,
      "endOffset" : 283
    }, {
      "referenceID" : 28,
      "context" : "Other researches try to establish fine-grained interaction on cross-modal attention to reinforce the focus from words to their most relevant regions, and vice versa (Chen et al., 2020; Wang et al., 2019; Messina et al., 2020; Lee et al., 2018; Zhang et al., 2020b; Yang et al., 2020).",
      "startOffset" : 165,
      "endOffset" : 283
    }, {
      "referenceID" : 20,
      "context" : "To corroborate this, we conduct a case study on Flickr30k Entities dataset (Plummer et al., 2015) to probe the agreement of relation-level semantics in pre-trained models like UNITER (Chen et al.",
      "startOffset" : 75,
      "endOffset" : 97
    }, {
      "referenceID" : 4,
      "context" : ", 2015) to probe the agreement of relation-level semantics in pre-trained models like UNITER (Chen et al., 2020).",
      "startOffset" : 93,
      "endOffset" : 112
    }, {
      "referenceID" : 5,
      "context" : "We utilize the self-attention distribution as a representation of the intra-modal relations (Clark et al., 2019; Htut et al., 2019; Kovaleva et al., 2019).",
      "startOffset" : 92,
      "endOffset" : 154
    }, {
      "referenceID" : 8,
      "context" : "We utilize the self-attention distribution as a representation of the intra-modal relations (Clark et al., 2019; Htut et al., 2019; Kovaleva et al., 2019).",
      "startOffset" : 92,
      "endOffset" : 154
    }, {
      "referenceID" : 11,
      "context" : "We utilize the self-attention distribution as a representation of the intra-modal relations (Clark et al., 2019; Htut et al., 2019; Kovaleva et al., 2019).",
      "startOffset" : 92,
      "endOffset" : 154
    }, {
      "referenceID" : 21,
      "context" : "What’s even worse is that in practice, the input regions of the existing methods are extracted by a pre-trained object detector like Faster R-CNN (Ren et al., 2015).",
      "startOffset" : 146,
      "endOffset" : 164
    }, {
      "referenceID" : 13,
      "context" : "The visual features are much noisier due to over-sampling (Li et al., 2020; Anderson et al., 2018), which necessitates a stronger regularizer to guide the alignment of the intra-modal relations.",
      "startOffset" : 58,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : "The visual features are much noisier due to over-sampling (Li et al., 2020; Anderson et al., 2018), which necessitates a stronger regularizer to guide the alignment of the intra-modal relations.",
      "startOffset" : 58,
      "endOffset" : 98
    }, {
      "referenceID" : 4,
      "context" : "Consider models of single-stream Transformerbased architecture like UNITER (Chen et al., 2020).",
      "startOffset" : 75,
      "endOffset" : 94
    }, {
      "referenceID" : 25,
      "context" : "The model consists of a stack of Transformer layers with attention mechanism (Vaswani et al., 2017) and is responsible for encoding image-text pairs into feature representations.",
      "startOffset" : 77,
      "endOffset" : 99
    }, {
      "referenceID" : 3,
      "context" : "Furthermore, the attention score matrix S ∈ RN×N can be organized into four submatrices (Bugliarello et al., 2020):",
      "startOffset" : 88,
      "endOffset" : 114
    }, {
      "referenceID" : 4,
      "context" : "To study the correlation between the ISDa metric and the model performance,4 we conduct an empirical analysis on UNITER (Chen et al., 2020).",
      "startOffset" : 120,
      "endOffset" : 139
    }, {
      "referenceID" : 4,
      "context" : "We use the Meta-Sum (Chen et al., 2020), sum of Recall@1, Recall@5, Recall@10 across the image and text retrieval as a metric for model performance.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 13,
      "context" : "In practice, given the original visual and linguistic input sequence V = [v1, · · · , vNV ], L = [l1, · · · , lNL ] with the scattered items,5 there are no object annotations and the region features extracted by Faster R-CNN are much noisier (Li et al., 2020; Anderson et al., 2018), which results in difficulty in grouping the attention weights by ground-truth object.",
      "startOffset" : 242,
      "endOffset" : 282
    }, {
      "referenceID" : 0,
      "context" : "In practice, given the original visual and linguistic input sequence V = [v1, · · · , vNV ], L = [l1, · · · , lNL ] with the scattered items,5 there are no object annotations and the region features extracted by Faster R-CNN are much noisier (Li et al., 2020; Anderson et al., 2018), which results in difficulty in grouping the attention weights by ground-truth object.",
      "startOffset" : 242,
      "endOffset" : 282
    }, {
      "referenceID" : 22,
      "context" : "alignment leverages the language as a bridge to draw implicit connections within the visual modality, which can be intuitively regarded as the backtranslation (Sennrich et al., 2016) for multimodal.",
      "startOffset" : 159,
      "endOffset" : 182
    }, {
      "referenceID" : 27,
      "context" : "Accordingly, we utilize Training Signal Annealing (Xie et al., 2020) to gradually incorporate the signal of the IAIS loss and design the following exponential schedule:",
      "startOffset" : 50,
      "endOffset" : 68
    }, {
      "referenceID" : 29,
      "context" : "We conduct experiments on the Flickr30k (Young et al., 2014) and MS COCO (Lin et al.",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 4,
      "context" : "We use the base (12 layers) and the large (24 layers) version of UNITER (Chen et al., 2020), one of the most prevailing large-scale pre-trained models, as our baseline and backbone for IAIS.",
      "startOffset" : 72,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : "Image-Text Retrieval Image-Text Retrieval (ITR, Barnard et al., 2003; Barnard and Forsyth, 2001), also known as Image-Text Matching, is one of the popular and challenging Languageand-Vision (V+L) tasks.",
      "startOffset" : 42,
      "endOffset" : 96
    }, {
      "referenceID" : 4,
      "context" : "Recent prevailing multimodal pre-trained models can be categorized into singlestream (Chen et al., 2020; Gan et al., 2020; Lin et al., 2020; Li et al., 2020; Su et al., 2020; Lin et al., 2021) and two-stream (Yu et al.",
      "startOffset" : 85,
      "endOffset" : 192
    }, {
      "referenceID" : 7,
      "context" : "Recent prevailing multimodal pre-trained models can be categorized into singlestream (Chen et al., 2020; Gan et al., 2020; Lin et al., 2020; Li et al., 2020; Su et al., 2020; Lin et al., 2021) and two-stream (Yu et al.",
      "startOffset" : 85,
      "endOffset" : 192
    }, {
      "referenceID" : 15,
      "context" : "Recent prevailing multimodal pre-trained models can be categorized into singlestream (Chen et al., 2020; Gan et al., 2020; Lin et al., 2020; Li et al., 2020; Su et al., 2020; Lin et al., 2021) and two-stream (Yu et al.",
      "startOffset" : 85,
      "endOffset" : 192
    }, {
      "referenceID" : 13,
      "context" : "Recent prevailing multimodal pre-trained models can be categorized into singlestream (Chen et al., 2020; Gan et al., 2020; Lin et al., 2020; Li et al., 2020; Su et al., 2020; Lin et al., 2021) and two-stream (Yu et al.",
      "startOffset" : 85,
      "endOffset" : 192
    }, {
      "referenceID" : 23,
      "context" : "Recent prevailing multimodal pre-trained models can be categorized into singlestream (Chen et al., 2020; Gan et al., 2020; Lin et al., 2020; Li et al., 2020; Su et al., 2020; Lin et al., 2021) and two-stream (Yu et al.",
      "startOffset" : 85,
      "endOffset" : 192
    } ],
    "year" : 2021,
    "abstractText" : "Despite the achievements of large-scale multimodal pre-training approaches, cross-modal retrieval, e.g., image-text retrieval, remains a challenging task. To bridge the semantic gap between the two modalities, previous studies mainly focus on word-region alignment at the object level, lacking the matching between the linguistic relation among the words and the visual relation among the regions. The neglect of such relation consistency impairs the contextualized representation of image-text pairs and hinders the model performance and the interpretability. In this paper, we first propose a novel metric, Intra-modal Self-attention Distance (ISD), to quantify the relation consistency by measuring the semantic distance between linguistic and visual relations. In response, we present Inter-modal Alignment on Intra-modal Self-attentions (IAIS), a regularized training method to optimize the ISD and calibrate intra-modal self-attentions from the two modalities mutually via inter-modal alignment. The IAIS regularizer boosts the performance of prevailing models on Flickr30k and MS COCO datasets by a considerable margin, which demonstrates the superiority of our approach.1",
    "creator" : "LaTeX with hyperref"
  }
}