{
  "name" : "2021.acl-long.164.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "BERTAC: Enhancing Transformer-based Language Models with Adversarially Pretrained Convolutional Neural Networks",
    "authors" : [ "Jong-Hoon Oh", "Ryu Iida", "Julien Kloetzer", "Kentaro Torisawa" ],
    "emails" : [ "torisawa}@nict.go.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2103–2115\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2103"
    }, {
      "heading" : "1 Introduction",
      "text" : "Transformer-based language models (TLMs) such as BERT (Devlin et al., 2019), ALBERT (Lan et al., 2020), and GPT-3 (Brown et al., 2020) have shown that large-scale self-supervised pretraining leads to strong performance on various NLP tasks. Many researchers have used TLMs for various downstream tasks, possibly as subcomponents of their methods, and/or they have focused on scaling up TLMs or improving their pretraining schemes. As a result, other architectures like Recurrent Neural Networks (RNN) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) and Convolutional Neural Networks (CNN) (LeCun et al., 1999) are fading away. In this work, we propose a method\nfor improving TLMs by integrating a simple conventional CNN to them. We pretrained this CNN on Wikipedia using a Generative Adversarial Network (GAN) style training scheme (Goodfellow et al., 2014), and then combined it with TLMs. Oh et al. (2019) similarly used GAN-style training to improve a QA model using a CNN, but their training scheme was applicable only to QAspecific datasets. On the other hand, similarly to TLM, our proposed method for training the CNN is independent of specific tasks. We show that the combination of this CNN with TLMs can achieve higher performance than that of the original TLMs on publicly available datasets for several distinct tasks. We hope that this gives an insight into how to develop novel strong network architectures and training schemes.\nWe call our combination of a TLM and a CNN BERTAC (BERT-style TLM with an Adversarially pretrained Convolutional neural network). Its architecture is illustrated in Fig. 1. We do not impose any particular restriction on the TLM in BERTAC, so any TLM, ALBERT (Lan et al.,\n2020) or RoBERTa (Liu et al., 2019) for example, can be used as a subcomponent of BERTAC.\nWe used the CNN to compute representations of a slightly modified version of the input given to a TLM. To integrate these representations with those of the TLM, we stacked on top of the TLM several layers of Transformers for Integrating External Representation (TIERs), which are our modified version of normal transformers (Vaswani et al., 2017). A TIER has the same architecture as that of a normal transformer encoder except for its attention: we replace the transformer’s self-attention with an attention based on the representation provided by the CNN. We expect that, by keeping the basic architecture of transformer encoders, the CNN’s representations can be integrated more effectively with the TLM’s original representations.\nWe pretrained the CNN using a GAN-style training scheme in order to generate representations of sentences rather freely without the constraint of token embedding prediction in the masked language modeling used for TLMs, as we explain later. For the training, we used masked sentences autogenerated from Wikipedia. As in the masked language modeling, neither human intervention nor downstream task-specific hacking is required. As illustrated in Fig. 2, the GANstyle training requires three networks, namely, a\ndiscriminator D and two CNN-based generators R and F . Once the training is done, we use the generator F as CNN in BERTAC. The training data consists of pairs of an entity mention and a sentence in which the entity mention is masked with a special token [EM]. For example, the entitymasked sentence m1 in Table 1 is obtained by masking the entity mention e1, “Suvarnabhumi Airport,” in the original text s1. The network F generates a vector representation of the masked sentence (m1), while R produces a representation of the masked entity (e1). The discriminator D takes representations generated by either R or F as the input, and it predicts which generator actually gave the representation.\nIn the original GAN, a generator learns to generate an artificial image from random noise so that the resulting artificial image is indistinguishable from given real images. By analogy, we used an entity-masked sentence as “random noise” and a masked entity as a “real image.” In our GAN-style training, we regard the vector representation of a masked entity given by generator R as a real representation of the entity (or the representation of the “real image” in the above analogy). On the other hand, we regard the representation of the masked sentence, generated by F , as a fake representation of the entity (or the representation of the “artificial image” generated from the “random noise” in the above analogy). This representation is deemed fake because the entity is masked in the masked sentence, and F does not know what the entity is exactly. During the training, F should try to deceive the discriminator D by mimicking the real representation and generating a fake representation that is indistinguishable from the real representation of the entity generated by R. On the other hand, R and D, as a team, try to avoid being mimicked by F and also to make the mimic problem harder for F . If everything goes well, once the training is over, F should be able to generate a fake representation of the entity that is similar to its real representation.\nAn interesting point is that F ’s output can be interpreted in two ways: it is a representation of a masked sentence because it is computed from the sentence, and at the same time it is a representation of the masked entity because it is indistinguishable from R’s representation of the entity. This duality suggests that F ’s output can be seen as a representation of the entire sentence.\nWe exploit F as a CNN in BERTAC as follows: first, we use F to compute a representation of a masked version of the sentence originally given as input to a TLM. The entity mention to be masked is chosen by simple rules and, if the input consists of multiple sentences, we generate a representation of each (masked) input sentence and concatenate these together into a single one. Then, this representation is integrated to the output of the TLM through multiple TIER layers.\nOur GAN-style pretraining is conceptually similar to TLM pretraining with masked language modeling (predicting what a masked word in a sentence should be). However, it was designed to pretrain a model that is able to rather freely generate entity representations without strongly sticking to the prediction of token embeddings. Our hypothesis is that such freely generated representations may be useful for improving the performance of downstream tasks. Moreover, we assumed that using multiple text representations computed from different perspectives (i.e., predicting token embeddings and freely generating entity representations) would help to improve the performance of downstream tasks.\nIn our experiments, we show that for the GLUE tasks (Wang et al., 2018), BERTAC’s average performance on the development set was 0.7% higher than that of ALBERT, which was used as a subcomponent of BERTAC, leading to a performance on the test set comparable to that of SOTA (90.3% vs 90.8% (SOTA)). It also outperformed the SOTA method of open-domain QA (Chen et al., 2017) on Quasar-T (Dhingra et al., 2017) and SearchQA (Dunn et al., 2017) using either ALBERT or RoBERTa. We also compared our method with alternative models using a CNN pretrained in a self-supervised (non GAN-style) manner to directly predict embeddings of the entity mentions. Consequently, we confirmed that our method worked better: only the CNN trained by our GAN-style pretraining gave significant performance improvement over base TLMs.\nNote that the computational overhead of BERTAC is reasonably small. It took 20 hours with 16 GPUs to pretrain a single CNN model and 180 hours for the nine models tested with different parameter settings in this work (cf., 480 hours with 96 GPUs for pretraining DeBERTa (He et al., 2021), for example). Moreover, once pretrained, the CNN models can be re-used for various down-\nstream tasks and combined with various TLMs, including potentially future ones. As for the parameter number, BERTAC had just a 14% increase in parameters when ALBERT-xxlarge was used as its base TLM (268 M parameters for BERTAC vs. 235 M for ALBERT-xxlarge). We confirmed from these results that BERTAC could improve pretrained TLMs with reasonably small computational overhead.\nThe code and models of BERTAC are available at https://github.com/nict-wisdom/bertac."
    }, {
      "heading" : "2 Related Work",
      "text" : "Pretraining TLMs with entity information: There have been attempts to explicitly learn entity representation from text corpora using TLMs (He et al., 2020; Peters et al., 2019; Sun et al., 2020; Wang et al., 2020a; Xiong et al., 2020; Zhang et al., 2019). Our proposed method is a complementary alternative to these existing methods in the sense that entity representations are integrated into TLMs via CNNs and not directly produced by the TLMs. Fine-tuning TLMs with external resources or other NNs: Yang et al. (2019a) and Liu et al. (2020) have used knowledge graphs for augmenting TLMs with entity representations during finetuning. Unlike these approaches, BERTAC uses unstructured texts rather than clean structured knowledge, such as knowledge graphs, to adversarially train a CNN. Other previous works have proposed combining CNNs or RNNs with BERT for NLP tasks (Lu et al., 2020; Safaya et al., 2020; Shao et al., 2019; Zhang et al., 2020), but their use of CNNs/RNNs was task-specific, so their models were not directly applicable to other tasks. Adversarial learning for improving TLMs: Oh et al. (2019) proposed a CNN-based answer representation generator for QA that can guess the vector representation of answers from given whytype questions and answer passages. The generator was trained in a GAN-style manner using QA datasets. We took inspiration from their adversarial training scheme to train task-independent representation generators from unsupervised texts (i.e., Wikipedia sentences in which an entity was masked in a cloze-test style).\nELECTRA (Clark et al., 2020) also employed an adversarial technique (not a GAN) to pretrain two TLMs: A generator was trained to perform masked language modeling and a discriminator\nwas trained to distinguish tokens in the training data from tokens replaced by the generator. On downstream tasks, only the discriminator was finetuned. In BERTAC, the GAN-style pretraining was applied only to the CNN, thus reducing the training cost. Furthermore, the CNN can be combined easily with any available TLM, even potentially future ones, without having to re-do the pretraining. In this work, we show that BERTAC outperformed ELECTRA on the GLUE task.\nVernikos et al. (2020) proposed a method that used an adversarial objective and an adversarial classifier for regularizing the fine-tuning process of TLMs, inspired by adversarial learning for domain adaptation (Ganin et al., 2016). Our work uses a GAN-style training scheme only for pretraining CNNs, not for fine-tuning TLMs."
    }, {
      "heading" : "3 Pretraining of CNNs",
      "text" : "This section describes the training data and training algorithm for our CNN."
    }, {
      "heading" : "3.1 Training data",
      "text" : "We pretrained our CNN with an entity-masked version of Wikipedia sentences. WikiExtractor1 was used to extract, from the English Wikipedia2, sentences that have at least one entity mention, i.e., an entity with an internal Wikipedia link. Then we randomly selected one entity mention ei in each sentence and generated an entity-masked sentence mi by replacing the entire selected mention with [EM]. For example, we generated the masked sentence m1, “[EM] is Thailand’s main international air hub,” (in Table 1) by replacing the entity mention e1, Suvarnabhumi Airport, in the sentence s1, “:::::::::::::Suvarnabhumi:::::::Airport is Thailand’s main international air hub,” with [EM]. We obtained about 43.3 million pairs of an entity mention and a masked sentence ({(ei, mi)}) in this way and used 10% of them (randomly sampled) as the pretraining data for our CNN."
    }, {
      "heading" : "3.2 GAN-style pretraining",
      "text" : "As illustrated in Fig. 2, the adversarial training is done using three subnetworks: R (realentity-representation generator), F (fake-entityrepresentation generator), and D (discriminator). R and F are CNNs with average pooling and D\n1https://github.com/samuelbroscheit/wikiextractorwikimentions\n2We used the September 2020 version.\nis a feedforward neural network. Once the training is done, we use the generator F as CNN in BERTAC. In the training, we regard the representation of a masked entity output by generator R as a real representation of the entity that the fakeentity-representation generator F should mimic. F is trained so that, taking an entity-masked sentence as its input, it can generate a representation of the masked entity mention (called a fake representation of the entity in this work) that D cannot distinguish from the real representation. The representation generated by F is fake in the sense that the entity mention is masked in the input sentence and F cannot know what it is exactly.\nAs mentioned in the Introduction, our GANstyle pretraining was designed to train a model capable of freely generating entity representations. We assumed that using multiple text representations computed from different perspectives (i.e., prediction of token embeddings in TLMs and generation of entity representations in our CNN) would help to improve the performance of downstream tasks.\nAlgorithm 1: Adversarial Training Scheme Input: Training examples {(e,m)}, training epochs t,\nmini-batch steps b, mini-batch size n Output: Real representation generator R, fake\nrepresentation generator F , discriminator D 1 j ← 1 2 Initialize θR, θF , and θD (parameters of R, F , and D)\nwith random weights 3 while j ≤ t do 4 k ← 1 5 while k ≤ b do 6 Sample mini-batch of n examples {(ei,mi)}ni=1 7 Generate word embeddings {(ei,mi)}ni=1 of the examples. 8 Update D and R by ascending their stochastic\ngradient: ∇θD,θR 1\nn n∑ i=1 [logD(R(ei)) + log ( 1−D(F (mi)) ) ]\n9 Update F by descending its stochastic gradient:\n∇θF 1\nn n∑ i=1 log ( 1−D(F (mi)) ) 10 k ← k + 1 11 end 12 j ← j + 1 13 end\nFor each pair of an entity mention (ei) and an entity-masked sentence (mi) in the training data, we first generate two matrices of word embeddings ei and mi using word embeddings pretrained on Wikipedia with fastText (Bojanowski et al., 2017). Then, R and F generate, respectively, a\nreal entity representation from ei and a fake entity representation from mi. Finally, they are given to D, which is a feed-forward network that judges whether F or R generated the representations, i.e., whether the representations are real or fake, using sigmoid outputs by the final logistic regression layer.\nThe pseudo code of the training scheme is given in Algorithm 1. The training proceeds as follows: R and D as a team try to avoid the possibility that D misjudges F’s output (i.e., a fake entity representation) as a real entity representation. More precisely, R and D are trained so that D can correctly judge the representation R(ei) given by generator R as real (i.e., D(R(ei)) = 1) and the representation F (mi) given by generator F as fake (i.e., D(F (mi)) = 0). Therefore, the training is carried out with the objective of maximizing logD(R(ei))+ log ( 1−D(F (mi)) ) (line 8 in Algorithm 1). On the other hand, F tries to generate representation F (mi) so that D judges it as real (i.e., D(F (mi)) = 1). Thus, F is trained to minimize log ( 1−D(F (mi)) ) (line 9 in Algorithm 1). This minmax game is iterated for the pre-specified t training epochs."
    }, {
      "heading" : "3.3 Pretraining settings",
      "text" : "We extracted 43.3 million pairs of an entity mention and a masked sentence from Wikipedia and randomly sampled 10% of them to use as training data (4.33 million pairs, around 700 MB in file size). We used word-embedding vectors in 300 dimensions (for 2.5 million words) pretrained on Wikipedia using fastText (Bojanowski et al., 2017). The embedding vectors were fixed during the training.\nWe set the training epochs to 200 (t = 200 in Algorithm 1) and did not use any early-stopping technique. We chose t = 200 from the results of our preliminary experiments in which we used 10% of the training data and set training epochs t to either of 100, 200, or 300; the loss robustly converged for t = 200 and t = 300, and thus the earliest point t = 200 was chosen. We used the RmsProp optimizer (Tieleman and Hinton, 2012) with a batch size of 4,000 (n = 4, 000 and b = 1, 084 in Algorithm 1) and a learning rate of 2e-4. We trained nine CNN models with all combinations of the filter’s window sizes ∈ {“1,2,3”, “2,3,4”, “1,2,3,4”} and number of filters ∈ {100, 200, 300} for the generators F and R. All of the weights in\nthe CNNs were initialized using He’s method (He et al., 2015). We used a logistic regression layer with sigmoid outputs as discriminator D. The training of a single CNN model took around 20 hours using 16 Nvidia V100 GPUs with 32 GB of memory (180 hours in total for the nine models).\nWe tested all nine CNN models for BERTAC in our GLUE and open-domain QA experiments (Section 5). For each task, the parameters inside the CNNs (as well as the word-embedding vectors) were fixed during the fine-tuning of BERTAC."
    }, {
      "heading" : "4 BERTAC",
      "text" : "As illustrated in Fig. 1, BERTAC (BERT-style TLM with an Adversarially pretrained Convolutional neural network) incorporates the representation provided by the adversarially pretrained CNN to the representation generated by a TLM. For the integration, we use several layers of TIERs (Transformers for Integrating External Representation) stacked on top of the TLM."
    }, {
      "heading" : "4.1 CNN in BERTAC",
      "text" : "For simplicity, we describe how the CNN is integrated in BERTAC using the task of recognizing textual entailment (RTE) as an example. BERTAC for the RTE task takes two sentences sx and sy as input and predicts whether sx entails sy. First, we explain how the adversarially pretrained CNN (generator F in Section 3.2) generates the representation of the two input sentences. We regard the longest common noun phrase3 of the two sentences as the entity mention to be masked and create entity-masked sentences mx and my from sx and sy by masking the noun phrase with [EM] (we use mx = sx and my = sy if no common noun phrase is found). Then each of the masked sentences mx and my is given to the CNN. Our expectation here is that the CNN generates similar representations from the masked sentences if they have an entailment relation and that this helps to recognize the entailment relation.\nNote that the CNN in BERTAC is connected to several TIER layers and that, as shown in Fig. 1, its input is iteratively updated so that it provides updated representations to the TIER layers. Let mix ∈ R|mx|×dw and miy ∈ R|my |×dw be the matrices of word embeddings of mx and my given\n3For single-sentence tasks such as CoLA (Wang et al., 2018), we regard the longest noun phrase in a sentence as an entity.\nto the CNN connected to the i-th TIER layer, where dw is the dimension of a word embedding. We denote the representation generated by the CNN when the matrix of word embeddings m was used as the input by CNN(m). The ith TIER layer is given the concatenation of the two CNN representations of mx and my, ri = [rix, riy] ∈ R2×de , where rix = CNN(mix) ∈ Rde , riy = CNN(miy) ∈ Rde and de is the dimension of the CNN representation. Note that, for singlesentence tasks, ri = rix, the CNN representation of mx, is given to the TIER layers.\nThe initial matrices of word embeddings m1x and m1y are obtained using the fastText word embeddings (Bojanowski et al., 2017), the same as that used in our adversarial learning. Then, the updated input matrices mi+1x and mi+1y for the (i+1)th CNN are obtained from the i-th input matrices mix and miy as described below. For the word embedding mix,j of the j-th word in mx, we compute its bilinear score to rix (Sutskever et al., 2009):\nm̄ix,j = softmaxj(m iT x B i xr i x)m i x,j ,\nwhere Bix ∈ Rdw×de is a trainable matrix and softmaxj(v) denotes the j-th element of the softmaxed vector of v. The bilinear score indicates how much the corresponding token should be highlighted as one associated with the CNN representation rix during the update process. We expect that this allows the CNN in the next TIER layer to generate further refined representations with the updated embeddings.\nWe then compute word embeddings mi+1x in a highway network manner (Srivastava et al., 2015) as follows:\nmi+1x = Hx(m̄ i x)⊙Tx(mix)+mix⊙(1−Tx(mix)),\nwhere Hx(mix) = Wihmix + b i h, Tx(mix) = σ(Witmix + bit), σ is the sigmoid function, ⊙ represents the element-wise product, and Wih, W i t, bih, and b i t are layer-specific trainable parameters. mi+1y is also computed from miy and riy in the same way. During the fine-tuning of BERTAC for downstream tasks, we fix the parameters of the pretrained CNN but train these parameters for updating CNN’s input alongside those of TLMs and TIERs."
    }, {
      "heading" : "4.2 Transformers for integrating external representation (TIERs)",
      "text" : "As explained in the Introduction, the main difference between a TIER and a normal transformer\nencoder (Vaswani et al., 2017) lies in the attention mechanism. In the TIER attention mechanism, the query representation, which is one of the three inputs of the transformer’s self-attention, is replaced with the representation given by the CNN.\nFig. 3 shows the difference between the TIERs’ attention computation and that of normal transformers. Attention in normal transformers is computed in the following way:\nAttention(Q,K,V) = softmax( QKT√ dk )V.\nQ, K, and V are query, key, and value matrices in Rlk×dk , where lk is the length of an input sequence and dk is a dimension of keys. Q, K, and V all come from the same representation of the token sequence provided from the previous transformer layer. The attention should specify how much the corresponding tokens in V should be highlighted, so we designed ours in the same way.\nIn TIERs, we use the following attention. We basically replace the matrix Q with the CNN’s representation r ∈ Ru×dk while keeping the original K and V, where u is the number of sentences in the input of the model (u ∈ {1, 2} in this paper).\nAttention(r,K,V) = (softmax( rKT√ dk ))TJu,dk⊙V.\nSince r is a matrix with a different size from Q, we needed to adapt the attention computation. We first multiply r to KT, and then its softmaxed results are converted into a lk × dk dimensional matrix using the all-one matrix Ju,dk ∈ Ru×dk . Let the resulting matrix be A = (softmax( rK\nT √ dk ))TJu,dk ∈ Rlk×dk . We apply the at-\ntention score to V by using the element-wise product between matrices: A ⊙ V.\nIn addition, the actual CNN’s representation rCNN ∈ Ru×de given by our CNNs usually have a size that does not match the size requirement for r. Thus, we convert it to r ∈ Ru×dk , a dkcolumn matrix as follows: r = rCNNW + b, where W ∈ Rde×dk and b are trainable."
    }, {
      "heading" : "5 Experiments",
      "text" : "We tested our model on GLUE and on opendomain QA. In this section, we report the results."
    }, {
      "heading" : "5.1 GLUE",
      "text" : "GLUE (Wang et al., 2018) is a multi-task benchmark composed of nine tasks including two singlesentence tasks (CoLA and SST-2) and seven two-sentence tasks of similarity/paraphrase tasks (MRPC, QQP, and STS-B) and natural language inference tasks (MNLI, QNLI, RTE, and WNLI). Following the previous work of ALBERT (Lan et al., 2020), we performed single-task fine-tuning for each task under the following settings: singlemodel for the development set and ensemble for test set submissions. As in Liu et al. (2019) and Lan et al. (2020), we report the performance on the development set for each task by averaging over five runs with different random initialization seeds. As in Lan et al. (2020), for test set submissions, we fine-tuned the models for the RTE, STS-B, and MRPC tasks by initializing them with the fine-tuned MNLI single-task model, and we also used task-specific modification for CoLA and WNLI to improve scores (see Appendix A for details). We explored ensemble settings between 6 and 30 models per task for our test set submission."
    }, {
      "heading" : "5.1.1 Fine-tuning details of BERTAC for GLUE",
      "text" : "We used ALBERT-xxlarge-v2 (Lan et al., 2020) as the pretrained TLM. As hyperparameters for BERTAC, for each task we tested learning rates ∈ {8e-6, 9e-6, 1e-5, 2e-5, 3e-5}, a linear warmup for the first 6% of steps followed by a linear decay to 0, a maximum sequence length of 128, and all nine CNNs pretrained with different filter settings. We set the batch size to 128 for MNLI and QQP and 16 for the other tasks. Furthermore, we trained our model with the following set of training epochs: {1,2,3,4,5} for MNLI, QQP, and QNLI, {6,7,8,9,10} for CoLA, MRPC, RTE, SST-2, and STS-B, and {90,95,100,105,110} for WNLI. We set the number of TIER layers to 3 after preliminary experiments. See Table 9 in Ap-\npendix B for a summary of the hyperparameters tested in the GLUE experiments.\nDuring the fine-tuning of BERTAC, the parameters inside the CNNs (as well as word embeddings of fastText) were fixed as explained in Section 3.3, while those used to update the input to the CNNs were optimized. For each task, we selected the pretrained CNN (out of nine) and the BERTAC hyperparameters that gave the best performance on the development data."
    }, {
      "heading" : "5.1.2 Results",
      "text" : "Table 2 shows the results of eight tasks on the GLUE development set: all of them are singlemodel results. Our BERTAC consistently outperformed the previous TLM-based models over seven tasks, except for QQP, and, as a result, showed the best average performance on the development set. Crucially, our model improved the average performance around 0.7% over ALBERT, the base TLM in our model. This indicates the effectiveness of adversarially trained CNNs and TIERs in BERTAC. The test set results obtained from the GLUE leaderboard are summarized in Table 3. Our model showed comparable performance to SOTA, DeBERTa/TuringNLRv4, and achieved state-of-the-art results on 3 out of 9 task. It also showed better performance than ALBERT, our base TLM, in most tasks.\nTo investigate whether our GAN-style pretraining of CNNs contributed to the performance improvement, we also tested the following alternative training schemes for the CNN used in BERTAC.\nSelf-supervised CNN: We pretrained the CNN to generate representations of a masked sentence in a self-supervised way as follows: For an entity mention e and an entity-masked sentence m in the training data (Section 3.1), the CNN generates a representation r from the masked sentence trying to minimize MSE (mean squared error) between r and the entity mention’s representation e (average word embedding of all tokens in e).\nRandomly initialized CNN: We did not pretrained the CNNs, but trained them alongside the TLMs during the fine-tuning of BERTAC (the CNNs were randomly initialized).\nWe trained both the self-supervised and randomly initialized CNNs using the same hyperparameter settings as GAN-style CNNs (see Section 3.3). We confirm from the results in Table 4\nthat only the proposed method with our GANstyle CNNs showed a higher average score than ALBERT. This suggests the effectiveness of our GAN-style pretraining scheme of CNNs."
    }, {
      "heading" : "5.2 Open-domain QA",
      "text" : "We also tested BERTAC on open-domain QA (Chen et al., 2017) with the publicly available datasets Quasar-T (Dhingra et al., 2017) and SearchQA (Dunn et al., 2017). We used the pre-processed version4 of the datasets provided by Lin et al. (2018), which contains passages retrieved for all questions, and followed their data split as described in Table 5."
    }, {
      "heading" : "5.2.1 BERTAC for open-domain QA",
      "text" : "We implemented our QA model following the approach of Lin et al. (2018), which combines a passage selector to choose relevant passages from retrieved passages and an answer span selector to identify the answer span in the selected passages. For the given question q and the set of retrieved passages P = {pi}, we computed the probability Pr(a|q, P ) of extracting answer span a to question q from P in the following way, and then we extracted the answer span â with the highest probability:\nPr(a|q, P ) = ∑ i Pr(a|q, pi)Pr(pi|q, P ),\n4Available at https://github.com/thunlp/OpenQA\nwhere Pr(pi|q, P ) and Pr(a|q, pi) are computed by the passage selector and answer span selector, respectively.\nWe input “[CLS] question [SEP] passage [SEP]” to both the passage selector and answer span selector, where [CLS] and [SEP] are special tokens. In the passage selector, the representation of [CLS] in the top TIER layer is fed into a linear layer with a softmax, which computes the probability that the passage contains a correct answer to the question. Our BERTAC answer span selector identifies answer spans from passages by computing start and end probabilities of each token in passages, where we feed the representation of each token in the top layer of TIERs to two linear layers, each with a softmax for the probabilities (Devlin et al., 2019)."
    }, {
      "heading" : "5.2.2 Training details for open-domain QA",
      "text" : "We used all nine pretrained CNNs, as in the GLUE experiments. As pretrained TLMs, we used ALBERT-xxlarge-v2 (Lan et al., 2020) and RoBERTa-large (Liu et al., 2019). We set the learning rate to 1e-5, the number of epochs to 2, the maximum sequence length to 384, and the number of TIER layers to 3. We used a linear warmup for the first 6% of steps followed by a linear decay to 0 with a batch size of 48 for QuasarT and 96 for SearchQA. We tested all of the pretrained CNNs and chose for each dataset the one that maximizes EM (the percentage of the predictions matching exactly one of the ground truth an-\nswers) on the development set. See Table 10 in Appendix B for a summary of the hyperparameters tested for open-domain QA."
    }, {
      "heading" : "5.2.3 Results",
      "text" : "We compared BERTAC with the previous works described in Table 6. Table 7 shows the performance of all of the methods. The subscripts of the TLM-based methods represent the type of pretrained TLM used by each method. All the methods were evaluated using EM and F1 score (average overlap between the prediction and gold answer). BERTACALBERT-xxlarge outperformed all of the baselines including the SOTA method (CFORMER) on both EM and F1. BERTACRoBERTa-large in the same TLM setting as the SOTA method showed a better performance than SOTA except for F1 in Quasar-T. These results suggest that our framework is effective for QA tasks as well.\nFor ablation studies, we evaluated some variants of BERTACALBERT-xxlarge: “w/o CNN and\nTIER,” which uses ALBERT-xxlarge alone without using our CNN and TIER, “w/o GAN-style CNN,” which does not use our CNN pretrained by the GAN-style training scheme but uses selfsupervised CNNs (the same as used in the GLUE experiments, see Table 4), “w/o update,” which does not perform layer-wise update of the CNN inputs. The results in Table 8 suggest that all of the following contributed to the performance improvement: the combination of TLMs and GANstyle CNNs, our GAN-style training of CNNs, and the layer-wise update of the CNN inputs."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We proposed BERTAC (BERT-style TLM with an Adversarially pretrained Convolutional neural network), a combination of a TLM and a CNN, where the CNN was pretrained using a novel GAN-style training scheme and masked sentences obtained automatically from Wikipedia. Using this CNN, we improved the performance of standard TLMs. We confirmed that BERTAC could achieve comparable performance with the SOTA and outperformed the base TLM used as a subcomponent of BERTAC in the GLUE task. We also show that BERTAC outperformed the SOTA method of open-domain QA on Quasar-T and SearchQA."
    }, {
      "heading" : "A Task-specific Modification for GLUE Test-set Submission",
      "text" : "We applied task-specific modification to WNLI and CoLA in the GLUE tasks to achieve competitive GLUE leaderboard results, i.e., the test set submission results presented in Table 3. For WNLI, we followed Raffel et al. (2020), while, for CoLA, we propose our own modification. Note that we did not apply the tricks in obtaining the results on the development set results shown in Table 2. In the following, we describe the tricks.\nA.1 WNLI WNLI is a coreference resolution task with a twosentence input. The first sentence has an ambiguous pronoun and the second sentence is generated from the first sentence by replacing the pronoun with one of the possible referents (noun phrases) in the first sentence (Wang et al., 2018). In this task, we must predict whether the candidate referent in the second sentence is the correct referent of the pronoun. Since the format of WNLI is known for being difficult to learn by a model, many previous works, including those using ALBERT, RoBERTa, or T5 (Liu et al., 2019; Lan et al., 2020; Raffel et al., 2020), converted the data to a simpler format before training their WNLI model for GLUE test-set submission.\nFollowing these approaches, we also converted the data in the same way as Raffel et al. (2020). First, we extract candidate referents for an ambiguous pronoun as follows. Suppose that the following sentence pair of s1 and s2 is from the WNLI task’s data and has the label correct (meaning that Susan in s2 is the correct referent of the pronoun she in s1).\ns1: Jane knocked on Susan’s door but she did not get an answer.\ns2: Susan did not get an answer.\nWe first find all of the pronouns in the first sentence (“she” in s1). For each pronoun, we find the longest sequence of words that precedes or follows the pronoun in the first sentence and that also appears in the second sentence (“did not get an answer” underlined in s1 and s2). We then choose the pronoun that precedes or follows the longest matching word sequence and obtain a candidate referent by deleting the matched sequence of words from the second sentence. In the example sentence pair (s1, s2), we choose the pronoun she from the first sentence (since there is a single pronoun) and obtain the candidate referent Susan from the second sentence through this process. Finally, we convert the original sentence pair into a pair of a masked sentence and a candidate referent by replacing the pronoun in the first sentence with [MASK] and replacing the second sentence with the extracted referent. The (s1, s2) pair is thus changed to the following (s′1, s ′ 2):\ns′1: Jane knocked on Susan’s door but [MASK] did not answer.\ns′2: Susan\nNote that [MASK] in the sentence is different from the entity mask [EM] used in our GANstyle training for CNNs. For the input to our CNNs, we further replaced [MASK] with [EM]. Since the format of this converted data is similar to that of the training data for the GAN-style training scheme of our CNN, we expect that by using this data conversion, BERTAC can more effectively predict whether the candidate referent for the masked pronoun is correct.\nA.2 CoLA In the CoLA task, we need to predict whether a given sentence is grammatically acceptable. For\nthis task, we conducted a two-step fine-tuning. In the first step, we fine-tuned BERTAC with automatically generated pseudo-training data. This data was prepared as described below, and does not include the original CoLA training data. In the second step, we further fined-tuned the model obtained in the first step using the original CoLA training data. The BERTAC model obtained at this second step was used for the test-set submission.\nTo automatically generate pseudo-training data, we regarded all of the sentences in the training data of MNLI, QQP, and QNLI as grammatically acceptable and used them as positive examples in the pseudo-training data. After removing duplicate sentences, for each positive example, we generated one negative example by modifying the positive example under the assumption that the modification makes the generated example grammatically unacceptable. As a modification, we randomly applied one of the following three operations: permutation (of four words randomly selected), insertion (of two random words to random positions), and deletion (of two randomly selected words) (Brahma, 2018).\nWe obtained about 2.14 million examples in this way, half of them positives and the other half negatives. We used all of the training samples automatically generated in this way for the first-step fine-tuning of BERTAC, with a learning rate of 8e-6, a single training epoch, and a batch size of 128, while applying the same settings for the other hyperparameters as those used for the other tasks. The model obtained by the first-step fine-tuning is\nthen used as a starting point for the second-step fine-tuning, using the original CoLA training data this time, of our final model for CoLA."
    }, {
      "heading" : "B Hyperparameters",
      "text" : "Hyperparameters of BERTAC tested for GLUE and open-domain QA experiments are summarized in Tables 9 and 10, where CNN represents CNN models pretrained with different filter settings (filter’s window sizes ∈ {“1,2,3”, “1,2,3,4”, “2,3,4”} and number of filters ∈ {100, 200, 300}) described in Section 3.3. We tested all combinations of these hyperparameters and chose the best one using the development set of each task."
    } ],
    "references" : [ {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:135–146.",
      "citeRegEx" : "Bojanowski et al\\.,? 2017",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2017
    }, {
      "title" : "Unsupervised learning of sentence representations using sequence consistency",
      "author" : [ "Siddhartha Brahma." ],
      "venue" : "CoRR, abs/1808.04217.",
      "citeRegEx" : "Brahma.,? 2018",
      "shortCiteRegEx" : "Brahma.",
      "year" : 2018
    }, {
      "title" : "Language models are few-shot learners",
      "author" : [ "Amodei." ],
      "venue" : "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.",
      "citeRegEx" : "Amodei.,? 2020",
      "shortCiteRegEx" : "Amodei.",
      "year" : 2020
    }, {
      "title" : "Reading Wikipedia to answer open– domain questions",
      "author" : [ "Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870–",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning phrase representations using RNN encoder–decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "ELECTRA: Pre– training text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Quasar: Datasets for question answering by search and reading",
      "author" : [ "Bhuwan Dhingra", "Kathryn Mazaitis", "William W Cohen." ],
      "venue" : "arXiv preprint arXiv:1707.03904.",
      "citeRegEx" : "Dhingra et al\\.,? 2017",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 2017
    }, {
      "title" : "Object recognition with",
      "author" : [ "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bengio.,? \\Q1999\\E",
      "shortCiteRegEx" : "Bengio.",
      "year" : 1999
    }, {
      "title" : "Exploring the limits",
      "author" : [ "Wei Li", "Peter J. Liu" ],
      "venue" : null,
      "citeRegEx" : "Li and Liu.,? \\Q2020\\E",
      "shortCiteRegEx" : "Li and Liu.",
      "year" : 2020
    }, {
      "title" : "Modelling relational data using bayesian clustered tensor factorization",
      "author" : [ "Ilya Sutskever", "Joshua B. Tenenbaum", "Ruslan R Salakhutdinov." ],
      "venue" : "Advances in Neural Information Processing Systems 22, pages 1821–1828. Curran Associates, Inc.",
      "citeRegEx" : "Sutskever et al\\.,? 2009",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2009
    }, {
      "title" : "Lecture 6.5— RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning",
      "author" : [ "T. Tieleman", "G. Hinton" ],
      "venue" : null,
      "citeRegEx" : "Tieleman and Hinton.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tieleman and Hinton.",
      "year" : 2012
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Domain Adversarial Fine-Tuning as an Effective Regularizer",
      "author" : [ "Giorgos Vernikos", "Katerina Margatina", "Alexandra Chronopoulou", "Ion Androutsopoulos." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3103–",
      "citeRegEx" : "Vernikos et al\\.,? 2020",
      "shortCiteRegEx" : "Vernikos et al\\.",
      "year" : 2020
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop Black-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "K-adapter: Infusing knowledge into pre-trained models with adapters",
      "author" : [ "Ruize Wang", "Duyu Tang", "Nan Duan", "Zhongyu Wei", "Xuanjing Huang", "Jianshu Ji", "Guihong Cao", "Daxin Jiang", "Ming Zhou." ],
      "venue" : "CoRR, abs/2002.01808.",
      "citeRegEx" : "Wang et al\\.,? 2020a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Cluster-former: Clusteringbased sparse transformer for long-range dependency encoding",
      "author" : [ "Shuohang Wang", "Luowei Zhou", "Zhe Gan", "Yen-Chun Chen", "Yuwei Fang", "Siqi Sun", "Yu Cheng", "Jingjing Liu." ],
      "venue" : "arXiv preprint arXiv:2009.06097.",
      "citeRegEx" : "Wang et al\\.,? 2020b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Multi-passage BERT: A globally normalized BERT model for open-domain question answering",
      "author" : [ "Zhiguo Wang", "Patrick Ng", "Xiaofei Ma", "Ramesh Nallapati", "Bing Xiang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model",
      "author" : [ "Wenhan Xiong", "Jingfei Du", "William Yang Wang", "Veselin Stoyanov." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,",
      "citeRegEx" : "Xiong et al\\.,? 2020",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2020
    }, {
      "title" : "Enhancing pre-trained language representations with rich knowledge for machine reading comprehension",
      "author" : [ "An Yang", "Quan Wang", "Jing Liu", "Kai Liu", "Yajuan Lyu", "Hua Wu", "Qiaoqiao She", "Sujian Li." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the",
      "citeRegEx" : "Yang et al\\.,? 2019a",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ R Salakhutdinov", "Quoc V Le." ],
      "venue" : "Advances in Neural Information Processing Systems 32, pages",
      "citeRegEx" : "Yang et al\\.,? 2019b",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Spelling error correction with soft-masked BERT",
      "author" : [ "Shaohua Zhang", "Haoran Huang", "Jicong Liu", "Hang Li." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 882–",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "ERNIE: Enhanced language representation with informative entities",
      "author" : [ "Zhengyan Zhang", "Xu Han", "Zhiyuan Liu", "Xin Jiang", "Maosong Sun", "Qun Liu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Transformer-based language models (TLMs) such as BERT (Devlin et al., 2019), ALBERT (Lan et al.",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 4,
      "context" : "As a result, other architectures like Recurrent Neural Networks (RNN) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) and Convolutional Neural Networks (CNN) (LeCun et al.",
      "startOffset" : 70,
      "endOffset" : 122
    }, {
      "referenceID" : 14,
      "context" : "In our experiments, we show that for the GLUE tasks (Wang et al., 2018), BERTAC’s average performance on the development set was 0.",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 3,
      "context" : "It also outperformed the SOTA method of open-domain QA (Chen et al., 2017) on Quasar-T (Dhingra et al.",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 7,
      "context" : ", 2017) on Quasar-T (Dhingra et al., 2017) and SearchQA (Dunn et al.",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 15,
      "context" : "There have been attempts to explicitly learn entity representation from text corpora using TLMs (He et al., 2020; Peters et al., 2019; Sun et al., 2020; Wang et al., 2020a; Xiong et al., 2020; Zhang et al., 2019).",
      "startOffset" : 96,
      "endOffset" : 212
    }, {
      "referenceID" : 18,
      "context" : "There have been attempts to explicitly learn entity representation from text corpora using TLMs (He et al., 2020; Peters et al., 2019; Sun et al., 2020; Wang et al., 2020a; Xiong et al., 2020; Zhang et al., 2019).",
      "startOffset" : 96,
      "endOffset" : 212
    }, {
      "referenceID" : 22,
      "context" : "There have been attempts to explicitly learn entity representation from text corpora using TLMs (He et al., 2020; Peters et al., 2019; Sun et al., 2020; Wang et al., 2020a; Xiong et al., 2020; Zhang et al., 2019).",
      "startOffset" : 96,
      "endOffset" : 212
    }, {
      "referenceID" : 21,
      "context" : "Other previous works have proposed combining CNNs or RNNs with BERT for NLP tasks (Lu et al., 2020; Safaya et al., 2020; Shao et al., 2019; Zhang et al., 2020), but their use of CNNs/RNNs was task-specific, so their models were not directly applicable to other tasks.",
      "startOffset" : 82,
      "endOffset" : 159
    }, {
      "referenceID" : 5,
      "context" : "ELECTRA (Clark et al., 2020) also employed an adversarial technique (not a GAN) to pretrain two TLMs: A generator was trained to perform masked language modeling and a discriminator",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "For each pair of an entity mention (ei) and an entity-masked sentence (mi) in the training data, we first generate two matrices of word embeddings ei and mi using word embeddings pretrained on Wikipedia with fastText (Bojanowski et al., 2017).",
      "startOffset" : 217,
      "endOffset" : 242
    }, {
      "referenceID" : 0,
      "context" : "5 million words) pretrained on Wikipedia using fastText (Bojanowski et al., 2017).",
      "startOffset" : 56,
      "endOffset" : 81
    }, {
      "referenceID" : 11,
      "context" : "We used the RmsProp optimizer (Tieleman and Hinton, 2012) with a batch size of 4,000 (n = 4, 000 and b = 1, 084 in Algorithm 1) and a learning rate of 2e-4.",
      "startOffset" : 30,
      "endOffset" : 57
    }, {
      "referenceID" : 14,
      "context" : "Let mx ∈ R|mx|×dw and my ∈ R|my |×dw be the matrices of word embeddings of mx and my given (3)For single-sentence tasks such as CoLA (Wang et al., 2018), we regard the longest noun phrase in a sentence as an entity.",
      "startOffset" : 133,
      "endOffset" : 152
    }, {
      "referenceID" : 0,
      "context" : "The initial matrices of word embeddings m(1)x and m1 y are obtained using the fastText word embeddings (Bojanowski et al., 2017), the same as that used in our adversarial learning.",
      "startOffset" : 103,
      "endOffset" : 128
    }, {
      "referenceID" : 10,
      "context" : "For the word embedding mx,j of the j-th word in mx, we compute its bilinear score to rx (Sutskever et al., 2009): m̄x,j = softmaxj(m iT x B i xr i x)m i x,j ,",
      "startOffset" : 88,
      "endOffset" : 112
    }, {
      "referenceID" : 12,
      "context" : "encoder (Vaswani et al., 2017) lies in the attention mechanism.",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 14,
      "context" : "GLUE (Wang et al., 2018) is a multi-task benchmark composed of nine tasks including two single-",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 20,
      "context" : ", 2019), XLNET (Yang et al., 2019b), ELECTRA (Clark et al.",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 5,
      "context" : ", 2019b), ELECTRA (Clark et al., 2020), ALBERT (Lan et al.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 3,
      "context" : "We also tested BERTAC on open-domain QA (Chen et al., 2017) with the publicly available datasets Quasar-T (Dhingra et al.",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : ", 2017) with the publicly available datasets Quasar-T (Dhingra et al., 2017) and SearchQA (Dunn et al.",
      "startOffset" : 54,
      "endOffset" : 76
    }, {
      "referenceID" : 6,
      "context" : "computing start and end probabilities of each token in passages, where we feed the representation of each token in the top layer of TIERs to two linear layers, each with a softmax for the probabilities (Devlin et al., 2019).",
      "startOffset" : 202,
      "endOffset" : 223
    }, {
      "referenceID" : 18,
      "context" : "TLM-based methods WKLM (Xiong et al., 2020): This uses a TLM pretrained with a weakly supervised objective for learning Wikipedia entity information.",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 17,
      "context" : "MBERT (Wang et al., 2019): A BERT-based method that extracts answers using globally normalized answer scores across all the passages retrieved by the same question.",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 16,
      "context" : "CFORMER (Wang et al., 2020b): It uses a clusteringbased sparse transformer for long-range dependency encoding.",
      "startOffset" : 8,
      "endOffset" : 28
    } ],
    "year" : 2021,
    "abstractText" : "Transformer-based language models (TLMs), such as BERT, ALBERT and GPT-3, have shown strong performance in a wide range of NLP tasks and currently dominate the field of NLP. However, many researchers wonder whether these models can maintain their dominance forever. Of course, we do not have answers now, but, as an attempt to find better neural architectures and training schemes, we pretrain a simple CNN using a GAN-style learning scheme and Wikipedia data, and then integrate it with standard TLMs. We show that on the GLUE tasks, the combination of our pretrained CNN with ALBERT outperforms the original ALBERT and achieves a similar performance to that of SOTA. Furthermore, on open-domain QA (Quasar-T and SearchQA), the combination of the CNN with ALBERT or RoBERTa achieved stronger performance than SOTA and the original TLMs. We hope that this work provides a hint for developing a novel strong network architecture along with its training scheme. Our source code and models are available at https://github.com/nict-wisdom/bertac.",
    "creator" : " TeX output 2021.06.02:2057"
  }
}