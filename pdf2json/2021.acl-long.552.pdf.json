{
  "name" : "2021.acl-long.552.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Improving Paraphrase Detection with the Adversarial Paraphrasing Task",
    "authors" : [ "Animesh Nighojkar", "John Licato" ],
    "emails" : [ "anighojkar@usf.edu", "licato@usf.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7106–7116\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n7106"
    }, {
      "heading" : "1 Introduction",
      "text" : "Although there are many definitions of ‘paraphrase’ in the NLP literature, most maintain that two sentences that are paraphrases have the same meaning or contain the same information. Pang et al. (2003) define paraphrasing as “expressing the same information in multiple ways” and Bannard and Callison-Burch (2005) call paraphrases “alternative ways of conveying the same information.” Ganitkevitch et al. (2013) write that “paraphrases are differing textual realizations of the same meaning.” A\ndefinition that seems to sufficiently encompass the others is given by Bhagat and Hovy (2013): “paraphrases are sentences or phrases that use different wording to convey the same meaning.” However, even that definition is somewhat imprecise, as it lacks clarity on what it assumes ‘meaning’ means.\nIf paraphrasing is a property that can hold between sentence pairs,1 then it is reasonable to assume that sentences that are paraphrases must have equivalent meanings at the sentence level (rather than exclusively at the levels of individual word meanings or syntactic structures). Here a useful test is that recommended by inferential role semantics or inferentialism (Boghossian, 1994; Peregrin, 2006), which suggests that the meaning of a statement s is grounded in its inferential properties: what one can infer from s and from what s can be inferred.\nBuilding on this concept from inferentialism, we assert that if two sentences have the same inferential properties, then they should also be mutually implicative. Mutual Implication (MI) is a binary relationship between two sentences that holds when each sentence textually entails the other (i.e., bidirectional entailment). MI is an attractive way of operationalizing the notion of two sentences having “the same meaning,” as it focuses on inferential relationships between sentences (properties of the sentences as wholes) instead of just syntactic or lexical similarities (properties of parts of the sentences). As such, we will assume in this paper that two sentences are paraphrases if and only if they are MI .2 In NLP, modeling inferential relationships between sentences is the goal of the textual entailment, or natural language inference (NLI) tasks (Bowman et al., 2015). We test MI\n1In this paper we study paraphrase between sentences, and do not address the larger scope of how our work might extend to paraphrasing between arbitrarily large text sequences.\n2The notations used in this paper are listed in Table 1.\nusing the version of RoBERTalarge released by Nie et al. (2020) trained on a combination of SNLI (Bowman et al., 2015), multiNLI (Williams et al., 2018), FEVER-NLI (Nie et al., 2019), and ANLI (Nie et al., 2020).\nOwing to expeditious progress in NLP research, performance of models on benchmark datasets is ‘plateauing’ — with near-human performance often achieved within a year or two of their release — and newer versions, using a different approach, are constantly having to be created, for instance, GLUE (Wang et al., 2019) and SuperGLUE (Wang et al., 2020). The adversarial paradigm of dataset creation (Jia and Liang, 2017a,b; Bras et al., 2020; Nie et al., 2020) has been widely used to address this ‘plateauing,’ and the ideas presented in this paper draw inspiration from it. In the remainder of this paper, we apply the adversarial paradigm to the problem of paraphrase detection, and demonstrate the following novel contributions:\n• We use the adversarial paradigm to create a new benchmark examining whether paraphrase detection models are assessing the meaning equivalence of sentences rather than being over-reliant on word-level measures. We do this by collecting paraphrases that are MI but are as lexically and syntactically disparate as possible (as measured by low BLEURT scores). We call this the Adversarial Paraphrasing Task (APT).\n• We show that a SOTA language model trained on paraphrase datasets perform poorly on our benchmark. However, when further trained on our adversarially-generated datasets, their MCC scores improve by up to 0.307.\n• We create an additional dataset by training a paraphrase generation model to perform our adversarial task, creating another large dataset that further improves the paraphrase detection models’ performance.\n• We propose a way to create a machinegenerated adversarial dataset and discuss ways to ensure it does not suffer from the plateauing that other datasets suffer from."
    }, {
      "heading" : "2 Related Work",
      "text" : "Paraphrase detection (given two sentences, predict whether they are paraphrases) (Zhang and Patrick,\n2005; Fernando and Stevenson, 2008; Socher et al., 2011; Jia et al., 2020) is an important task in the field of NLP, finding downstream applications in machine translation (Callison-Burch et al., 2006; Apidianaki et al., 2018; Mayhew et al., 2020), text summarization, plagiarism detection (Hunt et al., 2019), question answering, and sentence simplification (Guo et al., 2018). Paraphrases have proven to be a crucial part of NLP and language education, with research showing that paraphrasing helps improve reading comprehension skills (Lee and Colln, 2003; Hagaman and Reid, 2008). Question paraphrasing is an important step in knowledgebased question answering systems for matching questions asked by users with knowledge-based assertions (Fader et al., 2014; Yin et al., 2015).\nParaphrase generation (given a sentence, generate its paraphrase) (Gupta et al., 2018) is an area of research benefiting paraphrase detection as well. Lately, many paraphrasing datasets have been introduced to be used for training and testing ML models for both paraphrase detection and generation. MSRP (Dolan and Brockett, 2005) contains 5801 sentence pairs, each labeled with a binary human judgment of paraphrase, created using heuristic extraction techniques along with an SVM-based classifier. These pairs were annotated by humans, who found 67% of them to be semantically equivalent. The English portion of PPDB (Ganitkevitch et al., 2013) contains over 220M paraphrase pairs generated by meaning-preserving syntactic transformations. Paraphrase pairs in PPDB 2.0 (Pavlick et al., 2015) include fine-grained entailment relations, word embedding similarities, and style annotations. TwitterPPDB (Lan et al., 2017) consists of 51,524 sentence pairs captured from Twitter by linking tweets through shared URLs. This ap-\nproach’s merit is its simplicity as it involves neither a classifier nor a human-in-the-loop to generate paraphrases. Humans annotate the pairs, giving them a similarity score ranging from 1 to 6.\nParaNMT (Wieting and Gimpel, 2018) was created by using neural machine translation to translate the English side of a Czech-English parallel corpus (CzEng 1.6 (Bojar et al., 2016)), generating more than 50M English-English paraphrases. However, ParaNMT’s use of machine translation models that are a few years old harms its utility (Nighojkar and Licato, 2021), considering the rapid improvement in machine translation in the past few years. To rectify this, we use the google-translate library to translate the Czech side of roughly 300k CzEng2.0 (Kocmi et al., 2020) sentence pairs ourselves. We call this dataset ParaParaNMT (PPNMT for short, where the extra para- prefix reflects its similarity to, and conceptual derivation from, ParaNMT).\nSome work has been done in improving the quality of paraphrase detectors by training them on a dataset with more lexical and syntactic diversity. Thompson and Post (2020) propose a paraphrase generation algorithm that penalizes the production of n-grams present in the source sentence. Our approach to doing this is with the APT, but this is something worth exploring. Sokolov and Filimonov (2020) use a machine translation model to generate paraphrases much like ParaNMT. An interesting application of paraphrasing has been discussed by Mayhew et al. (2020) who, given a sentence in one language, generate a diverse set of correct translations (paraphrases) that humans are likely to produce. In comparison, our work is focused on generating adversarial paraphrases that are likely to deceive a paraphrase detector, and models trained on the adversarial datasets we produce can be applied to Mayhew et al.’s work too.\nANLI (Nie et al., 2020), a dataset designed for Natural Language Inference (NLI) (Bowman et al., 2015), was collected via an adversarial human-andmodel-in-the-loop procedure where humans are given the task of duping the model into making a wrong prediction. The model then tries to learn how not to make the same mistakes. AFLite (Bras et al., 2020) adversarially filters dataset biases making sure that the models are not learning those biases. They show that model performance on SNLI (Bowman et al., 2015) drops from 92% to 62% when biases were filtered out. However, their approach is\nto filter the dataset, which reduces its size, making model training more difficult. Our present work tries instead to generate adversarial examples to increase dataset size. Other examples of adversarial datasets in NLP include work done by Jia and Liang (2017a); Zellers et al. (2018, 2019). Perhaps the closest to our work is PAWS (Zhang et al., 2019), short for Paraphrase Adversaries from Word Scrambling. The idea behind PAWS is to create a dataset that has a high lexical overlap between sentence pairs without them being ‘paraphrases.’ It has 108k paraphrase and non-paraphrase pairs with high lexical overlap pairs generated by controlled word swapping and back-translation, and human raters have judged whether or not they are paraphrases. Including PAWS in the training data has shown the state-of-the-art models’ performance to jump from 40% to 85% on PAWS’s test split. In comparison to the present work, PAWS does not explicitly incorporate inferential properties, and we seek paraphrases minimizing lexical overlap."
    }, {
      "heading" : "3 Adversarial Paraphrasing Task (APT)",
      "text" : "Semantic Textual Similarity (STS) measures the degree of semantic similarity between two sentences. Popular approaches to calculating STS include BLEU (Papineni et al., 2002), BertScore (Zhang et al., 2020), and BLEURT (Sellam et al., 2020). BLEURT is a text generation metric building on BERT’s (Devlin et al., 2019) contextual word representations. BLEURT is warmed-up using synthetic sentence pairs and then fine-tuned on human ratings to generalize better than BERTScore (Zhang et al., 2020). Given any two sentences, BLEURT assigns them a similarity score (usually between -2.2 to 1.1). However, high STS scores do not necessarily predict whether two sentences have equivalent meanings. Consider the sentence pairs in Table 3, highlighting cases where STS and paraphrase appear to misalign. The existence of such cases suggests a way to advance automated paraphrase detection: through an adversarial benchmark consisting of sentence pairs that have the same MI-based meaning, but have BLEURT scores that are as low as possible. This is the motivation behind what we call the Adversarial Paraphrasing Task (APT), which has two components:\n1. Similarity of meaning: Checked through MI (Section 1). We assume if two sentences are MI (Mutually Implicative), they are semantically equivalent and thus paraphrases. Note\nthat MI is a binary relationship, so this APT component does not bring any quantitative variation but is more like a qualifier test for APT. All APT sentence pairs are MI .\n2. Dissimilarity of structure: Measured through BLEURT, which assigns each sentence pair a score quantifying how lexically and syntactically similar the two sentences are."
    }, {
      "heading" : "3.1 Manually Solving APT",
      "text" : "To test the effectiveness of APT in guiding the generation of mutually implicative but lexically and syntactically disparate paraphrases for a given sentence, we designed an Amazon Mechanical Turk (mTurk) study (Figure 1). Given a starting sentence, we instructed participants to “[w]rite a sentence that is the same in meaning as the given sentence but as structurally different as possible. Your sentence should be such that you can infer the given sentence from it AND vice-versa. It should be sufficiently different from the given sentence to get any reward for the submission. For example, a simple synonym substitution will most likely not work.” The sentences given to the participants came from MSRP and PPNMT (Section 1). Both of these datasets have pairs of sentences in each row, and we took only the first one to present to the par-\nticipants. Neither of these datasets has duplicate sentences by design. Every time a sentence was selected, a random choice was made between MSRP and PPNMT, thus ensuring an even distribution of sentences from both datasets.\nEach attempt was evaluated separately using Equation 1, where mi is 1 when the sentences are MI and 0 otherwise:\nreward = mi\n(1 + e5∗bleurt)2 (1)\nThis formula was designed to ensure (1) the maximum reward per submission was $1, and (2) no reward was granted for sentence pairs that are nonMI or have BLEURT > 0.5. Participants were encouraged to frequently revise their sentences and click on a ‘Check’ button which showed them the reward amount they would earn if they submitted this sentence. Once the ‘Check’ button was clicked, the participant’s reward was evaluated (see Figure 1) and the sentence pair added to APH (regardless of whether it was APT ). If ‘Submit’ was clicked, their attempt was rewarded based on Equation 1.\nThe resulting dataset of sentence pairs, which we call APH (Adversarial Paraphrase by Humans), consists of 5007 human-generated sentence pairs, both MI and non-MI (see Table 2). Humans were able to generate APT paraphrases for 75.48% of\nthe sentences presented to them and only 53.1% of attempts were APT , showing that the task is difficult even for humans. Note that ‘MI attempts’ and ‘MI uniques’ are supersets of ‘APT attempts’ and ‘APT uniques,’ respectively."
    }, {
      "heading" : "3.2 Automatically Solving APT",
      "text" : "Since human studies can be time-consuming and costly, we trained a paraphrase generator to perform APT. We used T5base (Raffel et al., 2020), as it achieves SOTA on paraphrase generation (Niu et al., 2020; Bird et al., 2020; Li et al., 2020) and trained it on TwitterPPDB (Section 2). Our hypothesis was that if T5base is trained to maximize the APT reward (Equation 1), its generated sentences will be more likely to be APT . We generated paraphrases for sentences in MSRP and those in TwitterPPDB itself, hoping that since T5base is trained on TwitterPPDB, it would generate better paraphrases (MI with lower BLEURT) for sentences coming from there. The proportion of sentences generated by T5base is shown in Table 2. We call this dataset APT5, the generation of which involved two phases: Training: To adapt T5base for APT, we implemented a custom loss function obtained from dividing the cross-entropy loss per batch by the total reward (again from Equation 1) earned from the model’s paraphrase generations for that batch, provided the model was able to reach a reward of at least 1. If not, the loss was equal to just the crossentropy loss. We trained T5base on TwitterPPDB for three epochs; each epoch took about 30 hours on one NVIDIA Tesla V100 GPU due to the CPU bound BLEURT component. More epochs may help get better results, but our experiments showed that loss plateaus after three epochs. Generation: Sampling, or randomly picking a\nnext word according to its conditional probability distribution, introduces non-determinism in language generation. Fan et al. (2018) introduce top-k sampling, which filters k most likely next words, and the probability mass is redistributed among only those k words. Nucleus sampling (or top-p sampling) (Holtzman et al., 2020) reduces the options to the smallest possible set of words whose cumulative probability exceeds p, and the probability mass is redistributed among this set of words. Thus, the set of words changes dynamically according to the next word’s probability distribution. We use a combination of top-k and top-p sampling with k = 120 and p = 0.95 in the interest of lexical and syntactic diversity in the paraphrases. For each sentence in the source dataset (MSRP3 and TwitterPPDB for APMT5 and AP Tw T5 respectively), we perform five iterations, in each of which, we generate ten sentences. If at least one of these ten sentences passes APT , we continue to the next source sentence after recording all attempts and classifying them as MI or non-MI . If no sentence in a maximum of 50 attempts passes APT , we record all attempts nonetheless, and move on to the next source sentence. For each increasing iteration for a particular source sentence, we increase k by 20, but we also reduce p by 0.05 to avoid vague guesses. Note the distribution of MI and non-MI in the source datasets does not matter because we use only the first sentence from the sentence pair."
    }, {
      "heading" : "3.3 Dataset Properties",
      "text" : "T5base trained with our custom loss function generated APT -passing paraphrases for (56.19%) of starting sentences. This is higher than we initially expected, considering how difficult APT proved to be for humans (Table 2). Noteworthy is that\n3We use the official train split released by Dolan and Brockett (2005) containing 4076 sentence pairs.\nonly 6.09% of T5base’s attempts were APT . This does not mean that the remaining 94% of attempts can be discarded, since they amounted to the negative examples in the dataset. Since we trained it on TwitterPPDB itself, we expected that T5base would generate better paraphrases, as measured by a higher chance of passing APT on TwitterPPDB, than any other dataset we tested. This is supported by the data in Table 2, which shows that T5base was able to generate an APT passing paraphrase for 84.8% of the sentences in TwitterPPDB.\nThe composition of the three adversarial datasets can be found in Table 2. These metrics are useful to understand the capabilities of T5base as a paraphrase generator and the “paraphrasability” of sentences in MSRP and TwitterPPDB. For instance, T5base’s attempts on TwitterPPDB tend to be MI much less frequently than those on MSRP and human’s attempts on MSRP + PPNMT. This might be because in an attempt to generate syntactically dissimilar sentences, the T5base paraphraser also ended up generating many semantically dissimilar ones as well.\nTo visualize the syntactic and lexical disparity of paraphrases in the three adversarial datasets, we present their BLEURT distributions in Figure 2. As might be expected, the likelihood of a sentence pair being MI increases as BLEURT score increases (recall that APT -passing sentence pairs are simply MI pairs with BLEURT scores <= 0.5), but Figure 2 shows that the shape of this increase is not straightforward, and differs among the three datasets.\nAs might be expected, humans are much more skilled at APT than T5base, as shown by the fact that the paraphrases they generated have much lower mean BLEURT scores (Figure 2), and the ratio of APT vs non-APT sentences is much higher (Table 2). As we saw earlier, when T5base wrote\nparaphrases that were low on BLEURT, they tended to become non-MI (e.g., line 12 in Table 3). However, T5base did generate more APT -passing sentences with a lower BLEURT on Twitter-PPDB than on MSRP, which may be a result of overfitting T5base on TwitterPPDB. Furthermore, all three adversarial datasets have a distribution of MI and non-MI sentence pairs balanced enough to train a model to identify paraphrases.\nTable 3 has examples from APH and APT5 showing the merits and shortcomings of T5, BLEURT, and RoBERTalarge (the MI detector used). Some observations from Table 3 include:\n• Lines 1 and 3: BLEURT did not recognize the paraphrases, possibly due to the differences in words used. RoBERTalarge however, gave the correct MI prediction (though it is worth noting that the sentences in line 1 are questions, rather than truth-apt propositions).\n• Line 4: RoBERTalarge and BLEURT (to a large extent since it gave it a score of 0.4) did not recognize that the idiomatic phrase ‘break a leg’ means ‘good luck’ and not ‘fracture.’\n• Lines 6 and 12: There is a loss of information going from the first sentence to the second and BLEURT and MI both seem to have understood the difference between summarization and paraphrasing.\n• Line 7: T5 not only understood the scores but also managed to paraphrase it in such a way that was not syntactically and lexically similar, just as we wanted T5 to do when we fine-tuned it.\n• Line 9: T5base knows that Fort Lauderdale is in Florida but RoBERTalarge does not.\nT5 for APT5). All datasets have APT passing and\nfailing MI and non-MI sentence pairs."
    }, {
      "heading" : "4 Experiments",
      "text" : "To quantify our datasets’ contributions, we designed experiment setups wherein we trained RoBERTabase (Liu et al., 2019) for paraphrase detection on a combination of TwitterPPDB and our datasets as training data. RoBERTa was chosen for its generality, as it is a commonly used model in current NLP work and benchmarking, and currently achieves SOTA or near-SOTA results on a majority of NLP benchmark tasks (Wang et al., 2019, 2020;\nChen et al., 2021).\nFor each source sentence, multiple paraphrases may have been generated. Hence, to avoid data leakage, we created a train-test split on APH such that all paraphrases generated using a given source sentence will be either in APH -train or in APH - test, but never in both. Note that APH is not balanced as seen in Table 2. Table 4 shows the distribution of MI and non-MI pairs in APH -train and APH -test and ‘MI attempts’ and ‘non-MI attempts’ columns of Table 2 show the same for other adversarial datasets. The test sets used were APH wherever APH -train was not a part of the training data and APH -test in every case.\nDoes RoBERTabase do well on APH? RoBERTabase was trained on each training dataset (90% training data, 10% validation data) for five epochs with a batch size of 32 with the training and validation data shuffled, and the trained model was tested on APH and APH -test. The results of this are shown in Table 6. Note that since the number of MI and non-MI sentences in all the datasets is imbalanced, Matthew’s Correlation Coefficient (MCC) is a more appropriate performance measure than accuracy (Boughorbel et al., 2017).\nOur motivation behind creating an adversarial dataset was to improve the performance of paraphrase detectors by ensuring they recognize paraphrases with low lexical overlap. To demonstrate the extent of their inability to do so, we first compare the performance of RoBERTabase trained only on TwitterPPDB on specific datasets as shown Table 5. Although the model performs slightly well on MSRP, it does barely better than a random prediction on APH , thus showing that identifying adversarial paraphrases created using APT is nontrivial for paraphrase identifiers.\nDo human-generated adversarial paraphrases improve paraphrase detection? We introduce APH -train to the training dataset along with TwitterPPDB. This improves the MCC by 0.222 even though APH -train constituted just 8.15% of the entire training dataset, the rest of which was TwitterPPDB (Table 6). This shows the effectiveness of human-generated paraphrases, as is especially impressive given the size of APH -train compared to TwitterPPDB.\nDo machine-generated adversarial paraphrases improve paraphrase detection? We set out to test the improvement brought by APT5, of which we have two versions. Adding APMT5 to the training set was not as effective as adding APH -train, increasing MCC by 0.188 on APH and 0.151 on APH -test, thus showing us that T5base, although was able to clear APT , lacked the quality which human paraphrases possessed. This might be explained by Figure 2 — since APMT5 does not have many sentences with low BLEURT, we cannot expect a vast improvement in RoBERTabase’s performance on sentences with BLEURT as low as in APH .\nSince we were not necessarily testing T5base’s performance — and we had trained T5base on Twit-\nterPPDB — we used the trained model to perform APT on TwitterPPDB itself. Adhering to expectations, training RoBERTabase (the paraphrase detector) with AP TwT5 yielded higher MCCs. Note that none of the sentences are common between AP TwT5 and APH since APH is built on MSRP and PPNMT and the fact that the model got this performance when trained on AP TwT5 is a testimony to the quality and contribution of APT.\nCombining these results, we can conclude that although machine-generated datasets like APT5 can help paraphrase detectors improve themselves, a smaller dataset of human-generated adversarial paraphrases improved performance more. Overall, however, the highest MCC (0.525 in Table 6) is obtained when TwitterPPDB is combined with all three adversarial datasets, suggesting that the two approaches nicely complement each other."
    }, {
      "heading" : "5 Discussions and Conclusions",
      "text" : "This paper introduced APT (Adversarial Paraphrasing Task), a task that uses the adversarial paradigm to generate paraphrases consisting of sentences with equivalent (sentence-level) meanings, but differing lexical (word-level) and syntactical similarity. We used APT to create a human-generated dataset / benchmark (APH ) and two machinegenerated datasets (APMT5 and AP Tw T5 ). Our goal was to effectively augment how paraphrase detectors are trained, in order to make them less reliant on word-level similarity. In this respect, the present work succeeded: we showed that RoBERTabase trained on TwitterPPDB performed poorly on APT benchmarks, but this performance was increased significantly when further trained on either our human- or machine-generated datasets. The code used in this paper along with the dataset has been released in a publicly-available repository.4\nParaphrase detection and generation have broad applicability, but most of their potential lies in areas in which they still have not been substantially applied. These areas range from healthcare (improving accessibility to medical communications or concepts by automatically generating simpler language), writing (changing the writing style of an article to match phrasing a reader is better able to understand), and education (simplifying the language of a scientific paper or educational lesson to\n4https://github.com/ Advancing-Machine-Human-Reasoning-Lab/ apt\nmake it easier for students to understand). Thus, future research into improving their performance can be very valuable. But approaches to paraphrase that treat it as no more than a matter of detecting word similarity overlap will not suffice for these applications. Rather, the meanings of sentences are properties of the sentences as a whole, and are inseparably tied to their inferential properties. Thus, our approaches to paraphrase detection and generation must follow suit.\nThe adversarial paradigm can be used to dive deeper into comparing how humans and SOTA language models understand sentence meaning, as we did with APT. Furthermore, automatic generation of adversarial datasets has much unrealized potential; e.g., different datasets, paraphrase generators, and training approaches can be used to generate future versions of APT5 in order to produce APT passing sentence pairs with lower lexical and syntactic similarities (as measured not only by BLEURT, but also by future state-of-the-art STS metrics). The idea of more efficient automated adversarial task performance is particularly exciting, as it points to a way language models can improve themselves while avoiding prohibitively expensive human participant fees.\nFinally, the most significant contribution of this paper, APT, presents a dataset creation method for paraphrases that will not saturate because as the models get better at identifying paraphrases, we will improve paraphrase generation. As models get better at generating paraphrases, we can make APT harder (e.g., by reducing the BLEURT threshold of < 0.5). One might think of this as students in a class who come up with new ways of copying their assignments from sources as plagiarism detectors improved. That brings us to one of the many applications of paraphrases: plagiarism generation and detection, which inherently is an adversarial activity. Until plagiarism detectors are trained on adversarial datasets themselves, we cannot expect them to capture human levels of adversarial paraphrasing."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This material is based upon work supported by the Air Force Office of Scientific Research under award numbers FA9550-17-1-0191 and FA955018-1-0052. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily\nreflect the views of the United States Air Force. We would also like to thank Antonio Laverghetta Jr. and Jamshidbek Mirzakhalov for their helpful suggestions while writing this paper, and Gokul Shanth Raveendran and Manvi Nagdev for helping with the website used for the mTurk study."
    } ],
    "references" : [ {
      "title" : "Automated paraphrase lattice creation for hyter machine translation evaluation",
      "author" : [ "Marianna Apidianaki", "Guillaume Wisniewski", "Anne Cocos", "Chris Callison-Burch." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Asso-",
      "citeRegEx" : "Apidianaki et al\\.,? 2018",
      "shortCiteRegEx" : "Apidianaki et al\\.",
      "year" : 2018
    }, {
      "title" : "Paraphrasing with bilingual parallel corpora",
      "author" : [ "Colin Bannard", "Chris Callison-Burch." ],
      "venue" : "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 597– 604.",
      "citeRegEx" : "Bannard and Callison.Burch.,? 2005",
      "shortCiteRegEx" : "Bannard and Callison.Burch.",
      "year" : 2005
    }, {
      "title" : "Squibs: What is a paraphrase",
      "author" : [ "Rahul Bhagat", "Eduard Hovy" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "Bhagat and Hovy.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bhagat and Hovy.",
      "year" : 2013
    }, {
      "title" : "Chatbot interaction with artificial intelligence: Human data augmentation with t5 and language transformer ensemble for text classification",
      "author" : [ "Jordan J. Bird", "Anikó Ekárt", "Diego R. Faria" ],
      "venue" : null,
      "citeRegEx" : "Bird et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Bird et al\\.",
      "year" : 2020
    }, {
      "title" : "Inferential role semantics and the analytic/synthetic distinction",
      "author" : [ "Paul A. Boghossian." ],
      "venue" : "Philosophical Studies: An International Journal for Philosophy in the Analytic Tradition, 73(2/3):109–122.",
      "citeRegEx" : "Boghossian.,? 1994",
      "shortCiteRegEx" : "Boghossian.",
      "year" : 1994
    }, {
      "title" : "Czeng 1.6: enlarged czech-english parallel corpus with processing tools dockered",
      "author" : [ "Ondřej Bojar", "Ondřej Dušek", "Tom Kocmi", "Jindřich Libovickỳ", "Michal Novák", "Martin Popel", "Roman Sudarikov", "Dušan Variš" ],
      "venue" : "In International Conference on Text,",
      "citeRegEx" : "Bojar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bojar et al\\.",
      "year" : 2016
    }, {
      "title" : "Optimal classifier for imbalanced data using matthews correlation coefficient metric",
      "author" : [ "Sabri Boughorbel", "Fethi Jarray", "Mohammed El-Anbari." ],
      "venue" : "PloS one, 12(6):e0177678–e0177678. 28574989[pmid].",
      "citeRegEx" : "Boughorbel et al\\.,? 2017",
      "shortCiteRegEx" : "Boughorbel et al\\.",
      "year" : 2017
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning" ],
      "venue" : null,
      "citeRegEx" : "Bowman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Adversarial filters of dataset biases",
      "author" : [ "Ronan Le Bras", "Swabha Swayamdipta", "Chandra Bhagavatula", "Rowan Zellers", "Matthew E. Peters", "Ashish Sabharwal", "Yejin Choi" ],
      "venue" : null,
      "citeRegEx" : "Bras et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Bras et al\\.",
      "year" : 2020
    }, {
      "title" : "Improved statistical machine translation using paraphrases",
      "author" : [ "Chris Callison-Burch", "Philipp Koehn", "Miles Osborne." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Callison.Burch et al\\.,? 2006",
      "shortCiteRegEx" : "Callison.Burch et al\\.",
      "year" : 2006
    }, {
      "title" : "Transformer-based language model fine-tuning methods for covid-19 fake news detection",
      "author" : [ "Ben Chen", "Bin Chen", "Dehong Gao", "Qijin Chen", "Chengfu Huo", "Xiaonan Meng", "Weijun Ren", "Yang Zhou" ],
      "venue" : null,
      "citeRegEx" : "Chen et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Devlin et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatically constructing a corpus of sentential paraphrases",
      "author" : [ "Bill Dolan", "Chris Brockett." ],
      "venue" : "Third International Workshop on Paraphrasing (IWP2005). Asia Federation of Natural Language Processing.",
      "citeRegEx" : "Dolan and Brockett.,? 2005",
      "shortCiteRegEx" : "Dolan and Brockett.",
      "year" : 2005
    }, {
      "title" : "Open question answering over curated and extracted knowledge bases",
      "author" : [ "Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni." ],
      "venue" : "Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’14, page",
      "citeRegEx" : "Fader et al\\.,? 2014",
      "shortCiteRegEx" : "Fader et al\\.",
      "year" : 2014
    }, {
      "title" : "Hierarchical neural story generation",
      "author" : [ "Angela Fan", "Mike Lewis", "Yann Dauphin" ],
      "venue" : null,
      "citeRegEx" : "Fan et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2018
    }, {
      "title" : "A semantic similarity approach to paraphrase detection",
      "author" : [ "Samuel Fernando", "Mark Stevenson." ],
      "venue" : "Proceedings of the 11th annual research colloquium of the UK special interest group for computational linguistics, pages 45–52.",
      "citeRegEx" : "Fernando and Stevenson.,? 2008",
      "shortCiteRegEx" : "Fernando and Stevenson.",
      "year" : 2008
    }, {
      "title" : "Ppdb: The paraphrase database",
      "author" : [ "Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Ganitkevitch et al\\.,? 2013",
      "shortCiteRegEx" : "Ganitkevitch et al\\.",
      "year" : 2013
    }, {
      "title" : "Dynamic multi-level multi-task learning for sentence simplification",
      "author" : [ "Han Guo", "Ramakanth Pasunuru", "Mohit Bansal" ],
      "venue" : null,
      "citeRegEx" : "Guo et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2018
    }, {
      "title" : "A deep generative framework for paraphrase generation",
      "author" : [ "Ankush Gupta", "Arvind Agarwal", "Prawaan Singh", "Piyush Rai." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 32(1).",
      "citeRegEx" : "Gupta et al\\.,? 2018",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2018
    }, {
      "title" : "The effects of the paraphrasing strategy on the reading comprehension of middle school students at risk for failure in reading",
      "author" : [ "Jessica L. Hagaman", "Robert Reid." ],
      "venue" : "Remedial and Special Education, 29(4):222–234.",
      "citeRegEx" : "Hagaman and Reid.,? 2008",
      "shortCiteRegEx" : "Hagaman and Reid.",
      "year" : 2008
    }, {
      "title" : "The curious case of neural text degeneration",
      "author" : [ "Ari Holtzman", "Jan Buys", "Li Du", "Maxwell Forbes", "Yejin Choi" ],
      "venue" : null,
      "citeRegEx" : "Holtzman et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Holtzman et al\\.",
      "year" : 2020
    }, {
      "title" : "Machine learning models for paraphrase identification and its applications on plagiarism",
      "author" : [ "E. Hunt", "R. Janamsetty", "C. Kinares", "C. Koh", "A. Sanchez", "F. Zhan", "M. Ozdemir", "S. Waseem", "O. Yolcu", "B. Dahal", "J. Zhan", "L. Gewali", "P. Oh" ],
      "venue" : null,
      "citeRegEx" : "Hunt et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Hunt et al\\.",
      "year" : 2019
    }, {
      "title" : "Adversarial examples for evaluating reading comprehension systems",
      "author" : [ "Robin Jia", "Percy Liang" ],
      "venue" : null,
      "citeRegEx" : "Jia and Liang.,? \\Q2017\\E",
      "shortCiteRegEx" : "Jia and Liang.",
      "year" : 2017
    }, {
      "title" : "Adversarial examples for evaluating reading comprehension systems",
      "author" : [ "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2021–2031, Copenhagen, Denmark. Association for",
      "citeRegEx" : "Jia and Liang.,? 2017b",
      "shortCiteRegEx" : "Jia and Liang.",
      "year" : 2017
    }, {
      "title" : "How to ask good questions? try to leverage paraphrases",
      "author" : [ "Xin Jia", "Wenjie Zhou", "Xu Sun", "Yunfang Wu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6130–6140, Online. Association for Computa-",
      "citeRegEx" : "Jia et al\\.,? 2020",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2020
    }, {
      "title" : "Announcing czeng 2.0 parallel corpus with over 2 gigawords",
      "author" : [ "Tom Kocmi", "Martin Popel", "Ondrej Bojar" ],
      "venue" : "arXiv preprint arXiv:2007.03006",
      "citeRegEx" : "Kocmi et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Kocmi et al\\.",
      "year" : 2020
    }, {
      "title" : "A continuously growing dataset of sentential paraphrases",
      "author" : [ "Wuwei Lan", "Siyu Qiu", "Hua He", "Wei Xu." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1224–1234, Copenhagen, Denmark. Associa-",
      "citeRegEx" : "Lan et al\\.,? 2017",
      "shortCiteRegEx" : "Lan et al\\.",
      "year" : 2017
    }, {
      "title" : "The effect of instruction in the paraphrasing strategy on reading fluency and comprehension",
      "author" : [ "Steven Lee", "Theresa Colln" ],
      "venue" : null,
      "citeRegEx" : "Lee and Colln.,? \\Q2003\\E",
      "shortCiteRegEx" : "Lee and Colln.",
      "year" : 2003
    }, {
      "title" : "Agent zero: Zero-shot automatic multiplechoice question generation for skill assessments",
      "author" : [ "Eric Li", "Jingyi Su", "Hao Sheng", "Lawrence Wai" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Simultaneous translation and paraphrase for language education",
      "author" : [ "Stephen Mayhew", "Klinton Bicknell", "Chris Brust", "Bill McDowell", "Will Monroe", "Burr Settles." ],
      "venue" : "Proceedings of the ACL Workshop on Neural Generation and Translation (WNGT). ACL.",
      "citeRegEx" : "Mayhew et al\\.,? 2020",
      "shortCiteRegEx" : "Mayhew et al\\.",
      "year" : 2020
    }, {
      "title" : "Combining fact extraction and verification with neural semantic matching networks",
      "author" : [ "Yixin Nie", "Haonan Chen", "Mohit Bansal." ],
      "venue" : "Association for the Advancement of Artificial Intelligence (AAAI).",
      "citeRegEx" : "Nie et al\\.,? 2019",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2019
    }, {
      "title" : "Adversarial nli: A new benchmark for natural language understanding",
      "author" : [ "Yixin Nie", "Adina Williams", "Emily Dinan", "Mohit Bansal", "Jason Weston", "Douwe Kiela" ],
      "venue" : null,
      "citeRegEx" : "Nie et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2020
    }, {
      "title" : "Mutual implication as a measure of textual equivalence",
      "author" : [ "Animesh Nighojkar", "John Licato." ],
      "venue" : "The International FLAIRS Conference Proceedings, 34.",
      "citeRegEx" : "Nighojkar and Licato.,? 2021",
      "shortCiteRegEx" : "Nighojkar and Licato.",
      "year" : 2021
    }, {
      "title" : "Unsupervised paraphrase generation via dynamic blocking",
      "author" : [ "Tong Niu", "Semih Yavuz", "Yingbo Zhou", "Huan Wang", "Nitish Shirish Keskar", "Caiming Xiong" ],
      "venue" : null,
      "citeRegEx" : "Niu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2020
    }, {
      "title" : "Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences",
      "author" : [ "Bo Pang", "Kevin Knight", "Daniel Marcu." ],
      "venue" : "Proceedings of the 2003 Human Language Technology Conference of the North American Chapter",
      "citeRegEx" : "Pang et al\\.,? 2003",
      "shortCiteRegEx" : "Pang et al\\.",
      "year" : 2003
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Ppdb 2.0: Better paraphrase ranking, finegrained entailment relations, word embeddings, and style classification",
      "author" : [ "Ellie Pavlick", "Pushpendre Rastogi", "Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch" ],
      "venue" : null,
      "citeRegEx" : "Pavlick et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pavlick et al\\.",
      "year" : 2015
    }, {
      "title" : "Meaning as an inferential role",
      "author" : [ "Jaroslav Peregrin." ],
      "venue" : "Erkenntnis, 64(1):1–35.",
      "citeRegEx" : "Peregrin.,? 2006",
      "shortCiteRegEx" : "Peregrin.",
      "year" : 2006
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu" ],
      "venue" : null,
      "citeRegEx" : "Raffel et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Bleurt: Learning robust metrics for text generation",
      "author" : [ "Thibault Sellam", "Dipanjan Das", "Ankur P. Parikh" ],
      "venue" : null,
      "citeRegEx" : "Sellam et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Sellam et al\\.",
      "year" : 2020
    }, {
      "title" : "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection",
      "author" : [ "Richard Socher", "Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng." ],
      "venue" : "Advances in neural information processing systems, pages 801–809.",
      "citeRegEx" : "Socher et al\\.,? 2011",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2011
    }, {
      "title" : "Neural machine translation for paraphrase generation",
      "author" : [ "Alex Sokolov", "Denis Filimonov" ],
      "venue" : null,
      "citeRegEx" : "Sokolov and Filimonov.,? \\Q2020\\E",
      "shortCiteRegEx" : "Sokolov and Filimonov.",
      "year" : 2020
    }, {
      "title" : "Paraphrase generation as zero-shot multilingual translation: Disentangling semantic similarity from lexical and syntactic diversity",
      "author" : [ "Brian Thompson", "Matt Post" ],
      "venue" : null,
      "citeRegEx" : "Thompson and Post.,? \\Q2020\\E",
      "shortCiteRegEx" : "Thompson and Post.",
      "year" : 2020
    }, {
      "title" : "Superglue: A stickier benchmark for general-purpose language understanding systems",
      "author" : [ "Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Glue: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "ParaNMT50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations",
      "author" : [ "John Wieting", "Kevin Gimpel." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
      "citeRegEx" : "Wieting and Gimpel.,? 2018",
      "shortCiteRegEx" : "Wieting and Gimpel.",
      "year" : 2018
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel R. Bowman" ],
      "venue" : null,
      "citeRegEx" : "Williams et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "Answering questions with complex semantic constraints on open knowledge bases",
      "author" : [ "Pengcheng Yin", "Nan Duan", "Ben Kao", "Junwei Bao", "Ming Zhou." ],
      "venue" : "Proceedings of the 24th ACM International on Conference on Information and Knowledge Manage-",
      "citeRegEx" : "Yin et al\\.,? 2015",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2015
    }, {
      "title" : "Swag: A large-scale adversarial dataset for grounded commonsense inference",
      "author" : [ "Rowan Zellers", "Yonatan Bisk", "Roy Schwartz", "Yejin Choi" ],
      "venue" : null,
      "citeRegEx" : "Zellers et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2018
    }, {
      "title" : "Hellaswag: Can a machine really finish your sentence",
      "author" : [ "Rowan Zellers", "Ari Holtzman", "Yonatan Bisk", "Ali Farhadi", "Yejin Choi" ],
      "venue" : null,
      "citeRegEx" : "Zellers et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2019
    }, {
      "title" : "Bertscore: Evaluating text generation with bert",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Paraphrase identification by text canonicalization",
      "author" : [ "Yitao Zhang", "Jon Patrick." ],
      "venue" : "Proceedings of the Australasian Language Technology Workshop 2005, pages 160–166.",
      "citeRegEx" : "Zhang and Patrick.,? 2005",
      "shortCiteRegEx" : "Zhang and Patrick.",
      "year" : 2005
    }, {
      "title" : "Paws: Paraphrase adversaries from word scrambling",
      "author" : [ "Yuan Zhang", "Jason Baldridge", "Luheng He" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "test is that recommended by inferential role semantics or inferentialism (Boghossian, 1994; Peregrin, 2006), which suggests that the meaning of a statement s is grounded in its inferential properties: what one can infer from s and from what s can be",
      "startOffset" : 73,
      "endOffset" : 107
    }, {
      "referenceID" : 38,
      "context" : "test is that recommended by inferential role semantics or inferentialism (Boghossian, 1994; Peregrin, 2006), which suggests that the meaning of a statement s is grounded in its inferential properties: what one can infer from s and from what s can be",
      "startOffset" : 73,
      "endOffset" : 107
    }, {
      "referenceID" : 7,
      "context" : "lationships between sentences is the goal of the textual entailment, or natural language inference (NLI) tasks (Bowman et al., 2015).",
      "startOffset" : 111,
      "endOffset" : 132
    }, {
      "referenceID" : 7,
      "context" : "(2020) trained on a combination of SNLI (Bowman et al., 2015), multiNLI (Williams et al.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 47,
      "context" : ", 2015), multiNLI (Williams et al., 2018), FEVER-NLI (Nie et al.",
      "startOffset" : 18,
      "endOffset" : 41
    }, {
      "referenceID" : 31,
      "context" : ", 2018), FEVER-NLI (Nie et al., 2019), and ANLI (Nie et al.",
      "startOffset" : 19,
      "endOffset" : 37
    }, {
      "referenceID" : 45,
      "context" : "Owing to expeditious progress in NLP research, performance of models on benchmark datasets is ‘plateauing’ — with near-human performance often achieved within a year or two of their release — and newer versions, using a different approach, are constantly having to be created, for instance, GLUE (Wang et al., 2019) and SuperGLUE (Wang",
      "startOffset" : 296,
      "endOffset" : 315
    }, {
      "referenceID" : 8,
      "context" : "The adversarial paradigm of dataset creation (Jia and Liang, 2017a,b; Bras et al., 2020; Nie et al., 2020) has been widely used to address this ‘plateauing,’ and the ideas presented in this paper draw inspiration from it.",
      "startOffset" : 45,
      "endOffset" : 106
    }, {
      "referenceID" : 32,
      "context" : "The adversarial paradigm of dataset creation (Jia and Liang, 2017a,b; Bras et al., 2020; Nie et al., 2020) has been widely used to address this ‘plateauing,’ and the ideas presented in this paper draw inspiration from it.",
      "startOffset" : 45,
      "endOffset" : 106
    }, {
      "referenceID" : 9,
      "context" : "field of NLP, finding downstream applications in machine translation (Callison-Burch et al., 2006; Apidianaki et al., 2018; Mayhew et al., 2020), text summarization, plagiarism detection (Hunt et al.",
      "startOffset" : 69,
      "endOffset" : 144
    }, {
      "referenceID" : 0,
      "context" : "field of NLP, finding downstream applications in machine translation (Callison-Burch et al., 2006; Apidianaki et al., 2018; Mayhew et al., 2020), text summarization, plagiarism detection (Hunt et al.",
      "startOffset" : 69,
      "endOffset" : 144
    }, {
      "referenceID" : 30,
      "context" : "field of NLP, finding downstream applications in machine translation (Callison-Burch et al., 2006; Apidianaki et al., 2018; Mayhew et al., 2020), text summarization, plagiarism detection (Hunt et al.",
      "startOffset" : 69,
      "endOffset" : 144
    }, {
      "referenceID" : 21,
      "context" : ", 2020), text summarization, plagiarism detection (Hunt et al., 2019), question answering, and sentence simplifi-",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 27,
      "context" : "Paraphrases have proven to be a crucial part of NLP and language education, with research showing that paraphrasing helps improve reading comprehension skills (Lee and Colln, 2003; Hagaman and Reid, 2008).",
      "startOffset" : 159,
      "endOffset" : 204
    }, {
      "referenceID" : 19,
      "context" : "Paraphrases have proven to be a crucial part of NLP and language education, with research showing that paraphrasing helps improve reading comprehension skills (Lee and Colln, 2003; Hagaman and Reid, 2008).",
      "startOffset" : 159,
      "endOffset" : 204
    }, {
      "referenceID" : 13,
      "context" : "paraphrasing is an important step in knowledgebased question answering systems for matching questions asked by users with knowledge-based assertions (Fader et al., 2014; Yin et al., 2015).",
      "startOffset" : 149,
      "endOffset" : 187
    }, {
      "referenceID" : 48,
      "context" : "paraphrasing is an important step in knowledgebased question answering systems for matching questions asked by users with knowledge-based assertions (Fader et al., 2014; Yin et al., 2015).",
      "startOffset" : 149,
      "endOffset" : 187
    }, {
      "referenceID" : 18,
      "context" : "Paraphrase generation (given a sentence, generate its paraphrase) (Gupta et al., 2018) is an area of research benefiting paraphrase detection as well.",
      "startOffset" : 66,
      "endOffset" : 86
    }, {
      "referenceID" : 12,
      "context" : "MSRP (Dolan and Brockett, 2005) contains 5801 sentence pairs, each labeled with a binary human judgment of paraphrase, created using heuris-",
      "startOffset" : 5,
      "endOffset" : 31
    }, {
      "referenceID" : 16,
      "context" : "The English portion of PPDB (Ganitkevitch et al., 2013) contains over 220M paraphrase pairs generated by meaning-preserving syntactic transformations.",
      "startOffset" : 28,
      "endOffset" : 55
    }, {
      "referenceID" : 37,
      "context" : "0 (Pavlick et al., 2015) include fine-grained entailment relations, word embedding similarities, and style annotations.",
      "startOffset" : 2,
      "endOffset" : 24
    }, {
      "referenceID" : 26,
      "context" : "TwitterPPDB (Lan et al., 2017) consists of 51,524 sentence pairs captured from Twitter by linking tweets through shared URLs.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 46,
      "context" : "ParaNMT (Wieting and Gimpel, 2018) was created by using neural machine translation to translate the English side of a Czech-English parallel corpus (CzEng 1.",
      "startOffset" : 8,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "6 (Bojar et al., 2016)), generating more than 50M English-English paraphrases.",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 33,
      "context" : "However, ParaNMT’s use of machine translation models that are a few years old harms its utility (Nighojkar and Licato, 2021), considering the rapid improvement in machine translation in the past few years.",
      "startOffset" : 96,
      "endOffset" : 124
    }, {
      "referenceID" : 32,
      "context" : "ANLI (Nie et al., 2020), a dataset designed for Natural Language Inference (NLI) (Bowman et al.",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 7,
      "context" : ", 2020), a dataset designed for Natural Language Inference (NLI) (Bowman et al., 2015), was collected via an adversarial human-andmodel-in-the-loop procedure where humans are given the task of duping the model into making a wrong prediction.",
      "startOffset" : 65,
      "endOffset" : 86
    }, {
      "referenceID" : 8,
      "context" : "AFLite (Bras et al., 2020) adversarially filters dataset biases making sure that the models are not learning those biases.",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 7,
      "context" : "They show that model performance on SNLI (Bowman et al., 2015) drops from 92% to 62% when biases were filtered out.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 53,
      "context" : "Perhaps the closest to our work is PAWS (Zhang et al., 2019), short for Paraphrase Adversaries from Word Scrambling.",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 36,
      "context" : "include BLEU (Papineni et al., 2002), BertScore (Zhang et al.",
      "startOffset" : 13,
      "endOffset" : 36
    }, {
      "referenceID" : 51,
      "context" : ", 2002), BertScore (Zhang et al., 2020), and BLEURT (Sellam et al.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 11,
      "context" : "BLEURT is a text generation metric building on BERT’s (Devlin et al., 2019) contextual word representations.",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 51,
      "context" : "ing synthetic sentence pairs and then fine-tuned on human ratings to generalize better than BERTScore (Zhang et al., 2020).",
      "startOffset" : 102,
      "endOffset" : 122
    }, {
      "referenceID" : 39,
      "context" : "We used T5base (Raffel et al., 2020), as it achieves SOTA on paraphrase generation (Niu",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 20,
      "context" : "Nucleus sampling (or top-p sampling) (Holtzman et al., 2020) reduces the options to the smallest possible set of words whose cumulative probability exceeds p, and the probabil-",
      "startOffset" : 37,
      "endOffset" : 60
    }, {
      "referenceID" : 29,
      "context" : "To quantify our datasets’ contributions, we designed experiment setups wherein we trained RoBERTabase (Liu et al., 2019) for paraphrase detection on a combination of TwitterPPDB and our datasets as training data.",
      "startOffset" : 102,
      "endOffset" : 120
    }, {
      "referenceID" : 6,
      "context" : "Note that since the number of MI and non-MI sentences in all the datasets is imbalanced, Matthew’s Correlation Coefficient (MCC) is a more appropriate performance measure than accuracy (Boughorbel et al., 2017).",
      "startOffset" : 185,
      "endOffset" : 210
    } ],
    "year" : 2021,
    "abstractText" : "If two sentences have the same meaning, it should follow that they are equivalent in their inferential properties, i.e., each sentence should textually entail the other. However, many paraphrase datasets currently in widespread use rely on a sense of paraphrase based on word overlap and syntax. Can we teach them instead to identify paraphrases in a way that draws on the inferential properties of the sentences, and is not over-reliant on lexical and syntactic similarities of a sentence pair? We apply the adversarial paradigm to this question, and introduce a new adversarial method of dataset creation for paraphrase identification: the Adversarial Paraphrasing Task (APT), which asks participants to generate semantically equivalent (in the sense of mutually implicative) but lexically and syntactically disparate paraphrases. These sentence pairs can then be used both to test paraphrase identification models (which get barely random accuracy) and then improve their performance. To accelerate dataset generation, we explore automation of APT using T5, and show that the resulting dataset also improves accuracy. We discuss implications for paraphrase detection and release our dataset in the hope of making paraphrase detection models better able to detect sentence-level meaning equivalence.",
    "creator" : "LaTeX with hyperref"
  }
}