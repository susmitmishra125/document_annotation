{
  "name" : "2021.acl-long.233.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "PLOME: Pre-training with Misspelled Knowledge for Chinese Spelling Correction",
    "authors" : [ "Shulin Liu", "Tao Yang", "Tianchi Yue", "Feng Zhang", "Di Wang" ],
    "emails" : [ "tianchiyue}@tencent.com", "diwang}@tencent.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2991–3000\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2991"
    }, {
      "heading" : "1 Introduction",
      "text" : "Chinese spelling correction (CSC) aims to detect and correct spelling errors in texts (Yu and Li, 2014). It is a challenging yet important task in natural language processing, which plays an important role in various NLP applications such as search engine (Martins and Silva, 2004) and optical character recognition (Afli et al., 2016). In Chinese, spelling errors can be mainly divided into two types: phonological errors and visual errors, which are separately caused by the misuse of phonologically similar characters and visually similar characters. According to Liu et al. (2010), about 83%\n1https://github.com/liushulinle/PLOME\nof errors are phonological and 48% are visual. Figure 1 illustrates examples of such errors. The first case is caused by the misuse of “没(gone)” and “美(beautiful)” with the same phonics, and the second case is caused by the misuse of “人(human)” and “入(enter)” with very similar shape.\nChinese spelling correction is a challenging task because it requires human-level language understanding ability to completely solve this problem (Zhang et al., 2020). Therefore, language model plays an important role in CSC. In fact, one of the mainstream solutions to this task is based on language models (Chen et al., 2013; Yu and Li, 2014; Tseng et al., 2015). Currently, the latest approaches (Zhang et al., 2020; Cheng et al., 2020) are based on BERT (Devlin et al., 2019), which is a masked language model. In these approaches, (masked) language models are independently pretrained from the CSC task. As a consequence, they did not learn any task-specific knowledge during pre-training. Therefore, language models in these approaches are sub-optimal for CSC.\nChinese spelling errors are mainly caused by the misuse of phonologically or visually similar characters. Thus, knowledge of the similarity between characters is crucial to this task. Some work leveraged the confusion set, i.e. a set of similar characters, to fuse such information (Wang et al., 2018, 2019; Zhang et al., 2020). However, confu-\nsion set is usually generated by heuristic rules or manual annotations, thus its coverage is limited. To circumvent this problem, Hong et al. (2019) computed the similarity based on character’s strokes and phonics. The similarity was measured via rules rather than learned by the model, therefore such knowledge was not fully utilized.\nIn this paper, we propose PLOME, a Pre-trained masked Language mOdel with Misspelled knowledgE, for Chinese spelling correction. The following characteristics make PLOME more effective than vanilla BERT for CSC. First, we propose the confusion set based masking strategy, where each chosen token is randomly replaced by a similar character according to a confusion set rather than the fixed token “[MASK]” as in BERT. Thus, PLOME jointly learns the semantics and misspelled knowledge during pre-training. Second, the proposed model takes each character’s strokes and phonics as input, which enables PLOME to model the similarity between arbitrary characters. Third, PLOME learns the misspelled knowledge on both character and phonic level by jointly recovering the true character and phonics for masked tokens.\nWe conduct experiments on the widely used benchmark dataset SIGHAN (Wu et al., 2013; Yu et al., 2014; Tseng et al., 2015). Experimental results show that PLOME significantly outperforms all the compared approaches, including the latest Soft-masked BERT (Zhang et al., 2020) and SpellGCN (Cheng et al., 2020).\nWe summarize our contributions as follows: (1) PLOME is the first task-specific language model designed for Chinese spelling correction. The proposed confusion set based masking strategy enables our model to jointly learn the semantics and misspelled knowledge during pre-training. (2) PLOME incorporates phonics and strokes, which enables it to model the similarity between arbitrary characters. (3) PLOME is the first to model this task on both character and phonic level."
    }, {
      "heading" : "2 Related Work",
      "text" : "Chinese spelling correction is a challenging task in natural language processing, which plays important roles in many applications, such as search engine (Martins and Silva, 2004; Gao et al., 2010), automatic essay scoring (Burstein and Chodorow, 1999; Lonsdale and Strong-Krause, 2003), and optical character recognition (Afli et al., 2016; Wang et al., 2018). It has been an active topic, and vari-\nous approaches have been proposed in recent years (Yu and Li, 2014; Wang et al., 2018, 2019; Zhang et al., 2020; Cheng et al., 2020).\nEarly work on CSC followed the pipeline of error identification, candidate generation and selection. Some researchers focused on unsupervised approaches, which typically adopted a confusion set to find correct candidates and employed language model to select the correct one (Chang, 1995; Huang et al., 2000; Chen et al., 2013; Yu and Li, 2014; Tseng et al., 2015). However, these methods failed to condition the correction on the input sentence. In order to model the input context, discriminative sequence tagging methods (Wang et al., 2018) and sequence-to-sequence generative models (Chollampatt et al., 2016; Ji et al., 2017; Ge et al., 2018; Wang et al., 2019) were employed.\nBERT (Devlin et al., 2019) is a bidirectional language model based on Transformer encoder (Vaswani et al., 2017). It has been demonstrated effective in a wide range of applications, such as question answering (Yang et al., 2019), information extraction (Lin et al., 2019), and semantic matching (Reimers and Gurevych, 2019). Recently, it has dominated the researches on CSC (Hong et al., 2019; Zhang et al., 2020; Cheng et al., 2020). Hong et al. (2019) adopted the DAE-Decoder paradigm with BERT as encoder. Zhang et al. (2020) introduced a detection network to generate the masking vector for the BERT-based correction network. Cheng et al. (2020) employed the graph convolution network (GCN) (Kipf and Welling, 2016) combined with BERT to model character interdependence. However, BERT is designed and pretrained independently from the CSC task, thus it is sub-optimal. To improve the performance, we propose a task-specific language model for CSC."
    }, {
      "heading" : "3 Approach",
      "text" : "We introduce PLOME and its detailed implementation in this section. Figure 2 illustrates the framework of PLOME. Similar to BERT (Devlin et al., 2019), the proposed model also follows the pretraining&fine-tuning paradigm. In the following subsections, we first introduce the confusion set based masking strategy, then present the architecture of PLOME and the learning objectives, finally show the details of fine-tuning."
    }, {
      "heading" : "3.1 Confusion Set based Masking Strategy",
      "text" : "In order to train PLOME, we randomly mask some percentage of the input tokens and then recover them. Devlin et al. (2019) replaced the chosen tokens by a fixed token “[MASK]”, which is nonexistent in downstream tasks. On the contrast, we remove this token and replace each chosen token by a random character that is similar to it. Similar characters are obtained from a publicly available confusion set (Wu et al., 2013), which contains two types of similar characters: phonologically similar and visually similar. Since phonological errors are two times more frequent than visual errors (Liu et al., 2010), these two types of similar characters are assigned different chance to be chosen during masking. Following Devlin et al. (2019), we totally mask 15% of tokens in the corpus. In addition, we use dynamic masking strategy (Liu et al., 2019), where the masking pattern is generated every time a sequence is fed into the model.\nAlways replacing chosen tokens by characters in a confusion set will cause two problems. (1). The model tends to make correction decision for all inputs since all the tokens to be predicted during pre-training are “misspelled”. To circumvent this problem, some percentage of the selected tokens are unchanged. (2). The size of confusion set is limited, however misspelling may be caused by the misuse of an arbitrary pair of characters in real texts. To improve generalization ability, we replace some percentage of chosen tokens by random characters from the vocabulary. To sum up, if\nthe i-th token is chosen, we replace it with (i) a random phonologically similar character 60% of the time (ii) a random visually similar character 15% of the time (iii) the unchanged i-th token 15% of the time (iv) a random token in the vocabulary 10% of the time. Table 1 presents examples of different masking strategies."
    }, {
      "heading" : "3.2 Embedding Layer",
      "text" : "As shown in Figure 2, the final embedding of each character is the sum of character embedding, position embedding, phonic embedding and shape embedding. The former two are obtained via looking up embedding tables, where the size of vocabulary and embedding dimension are the same as that in BERTbase (Devlin et al., 2019).\nPhonic Embedding In Chinese, phonics (also known as Pinyin) represents the pronunciation of a character, which is a sequence of lowercase letters\nwith a diacritic2. In this paper, we use the Unihan Database3 to obtain the character-phonics mapping (diacritic is removed). To model the phonological relationship between characters, we feed the letters of each character’s phonics to a 1-layer GRU (Bahdanau et al., 2014) network to generate the phonic embedding, where similar phonics are expected to have similar embeddings. An example is given in the middle part in Figure 3.\nShape Embedding We use the Stroke Order4 to represent the shape of a character, which is a sequence of strokes indicating the order in which the strokes of a Chinese character are written. A stroke is a movement of a writing instrument on a writing surface. In this paper, stroke data is obtained via Chaizi Database5. In order to model the visual relationship between characters, the Stroke order of each character is fed into another 1-layer GRU network to generate the shape embedding. An example is given in the bottom part in Figure 3."
    }, {
      "heading" : "3.3 Transformer Encoder",
      "text" : "The transformer encoder has the same architecture as that in BERTbase (Devlin et al., 2019). The number of transformer layers (Vaswani et al., 2017) is 12, the size of hidden units is 768 and the number of attention head is 12. For more detailed configurations please refer to Devlin et al. (2019)."
    }, {
      "heading" : "3.4 Output Layer",
      "text" : "As illustrated in Figure 2, our model makes two predictions for each chosen character.\nCharacter Prediction Similar to BERT, PLOME predicts the original character for each\n2https://en.wikipedia.org/wiki/Pinyin 3http://www.unicode.org/charts/unihan.html 4https://en.wikipedia.org/wiki/Stroke order 5https://github.com/kfcd/chaizi\nmasked token based on the embedding generated by the last transformer layer. The probability of the character predicted for the i-th token in a given sentence is defined as:\npc(yi = j|X) = softmax(Wchi + bc)[j] (1)\nwhere pc(yi = j|X) is the conditional probability that the true character of the i-th token xi is predicted as the j-th character in vocabulary, hi denotes the embedding output from the last transformer layer for xi, Wc ∈ Rnc×768 and bc ∈ Rnc are parameters for character prediction, nc is the size of the vocabulary.\nPronunciation Prediction Chinese totally has about 430 different pronunciations (represented by phonics) but has more than 2,500 common used characters. Thus, many characters share the same pronunciation. Moreover, some pronunciations are so similar that it is easy to be misused, such as “jing” and “jin”. Therefore, phonological error dominates Chinese spelling errors. In practice, about 80% of spelling errors are phonological (Zhang et al., 2020). In order to learn the misspelled knowledge on phonic level, PLOME also predicts the true pronunciation for each masked token, where pronunciation is presented by phonics without diacritic. The probability of pronunciation prediction is defined as:\npp(gi = k|X) = softmax(Wphi + bp)[k] (2)\nwhere pp(gi = k|X) is the conditional probability that the correct pronunciation of the masked character xi is predicted as the k-th phonics in the phonic vocabulary, hi denotes the embedding output from the last transformer layer for xi, Wc ∈ Rnp×768 and bp ∈ Rnp are parameters for pronunciation prediction, np is the size of the phonic vocabulary."
    }, {
      "heading" : "3.5 Learning",
      "text" : "The learning process is driven by optimizing two objectives, corresponding to character prediction and pronunciation prediction, respectively.\nLc = − n∑\ni=1\nlog pc(yi = li|X) (3)\nLp = − n∑\ni=1\nlog pp(gi = ri|X) (4)\nwhere Lc is the objective for character prediction, li is the true character for xi, Lp is the objective for\npronunciation prediction, ri is the true pronunciation. The overall objective is defined as:\nL = Lc + Lp (5)"
    }, {
      "heading" : "3.6 Fine-tuning Procedure",
      "text" : "Above subsections present the details of the pretraining procedure. In this subsection, we introduce the fine-tuning procedure. PLOME is designed for the CSC task, which aims to detect and correct spelling errors in Chinese texts. Formally, given a character sequence X = {x1, x2, ..., xn} consisting of n characters, the model is expected to generate a target sequence Y = {y1, y2, ..., yn}, where errors are corrected.\nTraining The learning objective is exactly the same as that in the pre-training procedure(see Section 3.5). This procedure is similar to pre-training except that: (1). the masking operation introduced in Section 3.1 is eliminated. (2). all input characters require to be predicted rather than only chosen tokens as in pre-training.\nInference As illustrated in Section 3.4, PLOME predicts both the character distribution and pronunciation distribution for each masked token. We define the joint distribution as:\npj(yi = j|X) = pc(yi = j|X)× pp(gi = jp|X) (6) where pj(yi = j|X) is the probability that the original character of xi is predicted as the j-th character jointly considering the character and pronunciation predictions, pc and pp are separately defined in Equation 1 and Equation 2, jp is the pronunciation of the j-th character. To this end, we construct an indicator matrix I ∈ Rnc×np , where Ii,j is set to 1 if the pronunciation of the i-th character is the j-th phonics, otherwise set to 0. Then the joint distribution can be computed by:\npj(yi|X) = [pp(gi|X) · IT] pc(yi|X) (7)\nwhere is the element-wise production. We use the joint probability as the predicted distribution. For each input token, the character with the highest joint probability is selected as the final output: ŷi =argmax pj(yi|X). The joint distribution simultaneously takes the character and pronunciation predictions into consideration, thus is more accurate. We will verify it in Section 4.5."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we present the details for pretraining PLOME and the fine-tuning results on the most widely used benchmark dataset."
    }, {
      "heading" : "4.1 Pre-training",
      "text" : "Dataset We use wiki2019zh6 as the pre-training corpus, which consists of one million Chinese Wikipedia7 pages. Moreover, we also collect three million news articles from a Chinese news platform. We split those pages and articles into sentences and totally obtain 162.1 million sentences. Then we concatenate consecutive sentences to obtain text fragments with at most 510 characters, which are used as the training instances.\nParameter Settings We denote the dimension of character embeddings, letter (in phonics) embeddings and stroke embeddings as dc, dl, ds, respectively, the dimension of hidden states in phonic and shape GRU networks as hp, and hs. Then we have dc = 768, dl = ds = 32, hp = hs = 768. The configuration of transformer encoder is exactly the same as that in BERTbase (Devlin et al., 2019), and the learning rate is set to 5e-5. These parameters are set based on experience because of the large cost of pre-training. Better performance could be achieved if parameter tuning technique (e.g. grid search) is employed. Moreover, instead of training PLOME from scratch, we adopt the parameters of Chinese BERT released by Google8 to initialize the Transformer blocks."
    }, {
      "heading" : "4.2 Fine-tuning",
      "text" : "Training Data Following Cheng et al. (2020), the training data is composed of 10K manually annotated samples from SIGHAN (Wu et al., 2013; Yu et al., 2014; Tseng et al., 2015) and 271K automatically generated samples from Wang et al. (2018).\nEvaluation Data We use the latest SIGHAN test dataset (Tseng et al., 2015) as in Zhang et al. (2020) to evaluate the proposed model, which contains 1100 texts and 461 types of errors.\nEvaluation Metrics Following previous work (Cheng et al., 2020; Zhang et al., 2020), we use the\n6https://github.com/suzhoushr/nlp chinese corpus 7https://zh.wikipedia.org/wiki/ 8https://github.com/google-research/bert\nprecision, recall and F1 scores as the evaluation metrics. Besides character-level evaluation, we also report sentence-level metrics on the detection and correction sub-tasks. We evaluate these metrics using the script from Cheng et al. (2020)9.\nParameter Settings Following Cheng et al. (2020), we set the maximum sentence length to 180, batch size to 32 and the learning rate to 5e-5. All experiments are conducted for 4 runs and the averaged metric is reported. The code and trained models will be released (currently the code is attached in the supplementary files)."
    }, {
      "heading" : "4.3 Baseline Models",
      "text" : "We use the following methods for comparison. Hybird (Wang et al., 2018) uses a BiLSTMbased model trained on an automatically generated dataset. PN (Wang et al., 2019) is a Seq2Seq model incorporating a pointer network. FASPell (Hong et al., 2019) adopts the DAEDecoder paradigm and employs BERT as the denoising auto-encoder. SKBERT (Zhang et al., 2020) introduces the SoftmasKing strategy in BERT to improve the performance of error detection. SpellGCN (Cheng et al., 2020) combines a GCN network with BERT to model the relationship between characters in the given confusion set.\nBesides, we implement a baseline model cBERT (confusion set based BERT), whose input and encoder layers are the same as that in BERTbase (De-\n9https://github.com/ACL2020SpellGCN/SpellGCN\nvlin et al., 2019). The output layer is similar to PLOME, but only has the character prediction as defined in Equation 1. cBERT is also pre-trained via the confusion set based masking strategy."
    }, {
      "heading" : "4.4 Main Results",
      "text" : "Table 2 illustrates the performance of the proposed method and baseline models. The results of recently proposed models are presented in the first group. The results of pre-trained and fine-tuned models are presented in the second and third group, respectively. From this table, we observe that:\n1) Without fine tuning, pre-trained models in the middle group achieve relatively good results, even outperform the supervised approach PN with remarkable gains. This indicates that the confusion set based masking strategy enables our model to learn task-specific knowledge during pre-training.\n2) Compared the fine-tuned models, cBERT outperforms BERT on all metrics. Especially, the F score of sentence-level evaluations are improved by more than 4 absolute points. The improvement is remarkable with such a large amount of training data (281k texts), which indicates that the proposed masking strategy provides essential knowledge and it can not be learned from fine tuning.\n3) With the incorporation of phonic and shape embeddings, PLOME-Finetune outperforms cBERT-Finetune by 2.3% and 2.8% absolute improvements in sentence-level detection and correction. This indicates that characters’ phonics and strokes provide useful information and it can hardly be learned from the confusion set.\n4) SpellGCN and our approach use the same con-\nfusion set from Wu et al. (2013), but adopt different strategies to learn the knowledge contained in it. SpellGCN built a GCN network to model this information, whereas PLOME learned it from huge scale data during pre-training. PLOME achieves better performance on all metrics, indicating that our approach is more effective to model such knowledge.\nPrevious work (Wang et al., 2019; Cheng et al., 2020) conducted the character-level evaluation on positive sentences which contain at least one error (sentence-level metrics were evaluated on the whole test set). Thus, the precision score is very high. The character-level results in table 2 are also evaluated in such manner for fair comparison. To make more comprehensive evaluation, we report the results evaluated on the whole test set in table 3. Moreover, following Cheng et al. (2020), we also report the sentence-level results evaluated by SIGHAN official tool. We observe that PLOME consistently outperforms BERT and SpellGCN on all metrics.\nTo make more comprehensive comparisons, we also evaluate the proposed model on SIGHAN13(Wu et al., 2013) and SIGHAN14(Yu et al., 2014). Following Cheng et al. (2020), we performed 6 additional fine-tuning epochs on SIGHAN13 as its data distribution differs from other datasets. Table5 illustrates the results, from which we observe that PLOME consistently outperforms all the compared models."
    }, {
      "heading" : "4.5 Effects of Prediction Strategy",
      "text" : "As illustrated in Section 3.4 and 3.6, PLOME predicts three distributions for each character: the character distribution pc, the pronunciation distribution pp and the joint distribution pj . The latter two distributions are related to pronunciation prediction, which is first to be introduced in this work. In this subsection, we investigate the performance of PLOME with each of them as the final output. The CSC task requires character prediction, thus we only compare the effects of the character prediction pc and the joint prediction pj .\nTable 4 presents the experimental results, from which we observe that the joint distribution outperforms the character distribution on all evaluation metrics. Especially, the gap of precision scores is more obvious. The joint distribution simultaneously takes the character and pronunciation predic-\ntions into consideration, thus the predicted results are more accurate."
    }, {
      "heading" : "4.6 Effects of Initialization Strategy",
      "text" : "Generally speaking, initialization strategy has a great influence on the performance for deep models. In this subsection, we investigate the effects of different initialization strategies in the pre-training procedure. For comparison, we implement four baselines based on cBERT and PLOME.\nTable 6 illustrates the results, where methods named with “*-Rand” initialize all the parameters randomly and methods named with “*-BERT” initialize the transformer encoder by BERT released by Google. From the table we observe that both cBERT and PLOME initialized with BERT achieve better performance. Especially, the recall score improves significantly for all evaluations. We believe the following two reasons may explain this phenomenon. 1) The rich semantic information in BERT can effectively improves the generalization ability. 2) PLOME is composed of two 1-layer GRU networks and a 12-layer transformer encoder, and totally contains more than 110M parameters. It is easily trapped into local optimization when training such a large-scale model from scratch."
    }, {
      "heading" : "4.7 Phonic/Shape Embedding Visualization",
      "text" : "In this subsection, we investigate whether the phonic and shape GRU networks learned meaningful representations for characters. To this end, we generate the phonic and shape embeddings for each character by the GRU networks in Figure 2 and then visualize them.\nFigure 4 illustrates 30 characters nearest to ‘锭’ according to the cosine similarity of the 768-dim embeddings generated by GRU networks, which is visualized via t-SNE (Maaten and Hinton, 2008). On one hand, nearly all the characters similar to ‘锭’, such as ‘啶’ and ‘绽’, are included in this\nfigure. On the other hand, similar characters are very close to each other (labeled by circles). These phenomena indicate that the learned shape embedding well models the shape similarity. Figure 5 shows the same situation for the phonic embedding related to ‘ding’ and also demonstrates its ability in modeling phonic similarity."
    }, {
      "heading" : "4.8 Converging Speed of Various Models",
      "text" : "In this subsection, we investigate the converging speed of various models in the fine-tuning procedure. Figure 6 shows the test curves for character-level detection metrics of BERT, cBERT and PLOME. Thanks to the confusion set based masking strategy, cBERT and PLOME learned taskspecific knowledge in the pre-training procedure, therefore they achieve much better performance than BERT at the beginning of the training. As the training went on, the gap gradually narrowed dur-\ning the first 35,000 steps and then remained stable with a gap of 6%(86% vs. 80%). In addition, the proposed model needs much less training steps to achieve a relatively good performance. PLOME needs only 7k steps to achieve the score of 80%, whereas BERT needs 47k steps."
    }, {
      "heading" : "5 Conclusions",
      "text" : "We propose PLOME, a pre-trained masked language model with misspelled knowledge for CSC. To the best of our knowledge, PLOME is the first task-specific language model for CSC, which jointly learns semantics and misspelled knowledge thanks to the confusion set based masking strategy. Previous work demonstrated that phonological and visual similarity between characters is essential to this task. We introduce phonic and shape GRU networks to model such features. Moreover, PLOME is also the first model that makes decision via jointly considering the target pronunciation and character distributions. Experimental results showed that PLOME outperforms all the compared models with remarkable gains."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Lei He, Suncong Zheng and Weikang Wang for helpful discussions, and anonymous reviewers for their insightful comments."
    } ],
    "references" : [ {
      "title" : "Using SMT for OCR error correction of historical texts",
      "author" : [ "Haithem Afli", "Zhengwei Qiu", "Andy Way", "Páraic Sheridan." ],
      "venue" : "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), pages 962–966, Por-",
      "citeRegEx" : "Afli et al\\.,? 2016",
      "shortCiteRegEx" : "Afli et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation by jointly",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Automated essay scoring for nonnative english speakers",
      "author" : [ "Jill Burstein", "Martin Chodorow." ],
      "venue" : "Computer mediated language assessment and evaluation in natural language processing.",
      "citeRegEx" : "Burstein and Chodorow.,? 1999",
      "shortCiteRegEx" : "Burstein and Chodorow.",
      "year" : 1999
    }, {
      "title" : "A new approach for automatic chinese spelling correction",
      "author" : [ "Chao-Huang Chang." ],
      "venue" : "Proceedings of Natural Language Processing Pacific Rim Symposium, volume 95, pages 278–283. Citeseer.",
      "citeRegEx" : "Chang.,? 1995",
      "shortCiteRegEx" : "Chang.",
      "year" : 1995
    }, {
      "title" : "A study of language modeling for Chinese spelling check",
      "author" : [ "Kuan-Yu Chen", "Hung-Shin Lee", "Chung-Han Lee", "HsinMin Wang", "Hsin-Hsi Chen." ],
      "venue" : "Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing, pages 79–83,",
      "citeRegEx" : "Chen et al\\.,? 2013",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2013
    }, {
      "title" : "SpellGCN: Incorporating phonological and visual similarities into language models for Chinese spelling check",
      "author" : [ "Xingyi Cheng", "Weidi Xu", "Kunlong Chen", "Shaohua Jiang", "Feng Wang", "Taifeng Wang", "Wei Chu", "Yuan Qi." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Cheng et al\\.,? 2020",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural network translation models for grammatical error correction",
      "author" : [ "Shamil Chollampatt", "Kaveh Taghipour", "Hwee Tou Ng." ],
      "venue" : "arXiv preprint arXiv:1606.00189.",
      "citeRegEx" : "Chollampatt et al\\.,? 2016",
      "shortCiteRegEx" : "Chollampatt et al\\.",
      "year" : 2016
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "A large scale ranker-based system for search query spelling correction",
      "author" : [ "Jianfeng Gao", "Chris Quirk" ],
      "venue" : "In 23rd International Conference on Computational Linguistics",
      "citeRegEx" : "Gao and Quirk,? \\Q2010\\E",
      "shortCiteRegEx" : "Gao and Quirk",
      "year" : 2010
    }, {
      "title" : "Fluency boost learning and inference for neural grammatical error correction",
      "author" : [ "Tao Ge", "Furu Wei", "Ming Zhou." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1055–",
      "citeRegEx" : "Ge et al\\.,? 2018",
      "shortCiteRegEx" : "Ge et al\\.",
      "year" : 2018
    }, {
      "title" : "FASPell: A fast, adaptable, simple, powerful Chinese spell checker based on DAEdecoder paradigm",
      "author" : [ "Yuzhong Hong", "Xianguo Yu", "Neng He", "Nan Liu", "Junhui Liu." ],
      "venue" : "Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019),",
      "citeRegEx" : "Hong et al\\.,? 2019",
      "shortCiteRegEx" : "Hong et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatic detecting/correcting errors in chinese text by an approximate word-matching",
      "author" : [ "Changning Huang", "Haihua Pan", "Zhou Ming", "Lei Zhang" ],
      "venue" : null,
      "citeRegEx" : "Huang et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2000
    }, {
      "title" : "A nested attention neural hybrid model for grammatical error correction",
      "author" : [ "Jianshu Ji", "Qinlong Wang", "Kristina Toutanova", "Yongen Gong", "Steven Truong", "Jianfeng Gao." ],
      "venue" : "pages 753–762.",
      "citeRegEx" : "Ji et al\\.,? 2017",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2017
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas N Kipf", "Max Welling." ],
      "venue" : "arXiv preprint arXiv:1609.02907.",
      "citeRegEx" : "Kipf and Welling.,? 2016",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2016
    }, {
      "title" : "A BERTbased universal model for both within- and crosssentence clinical temporal relation extraction",
      "author" : [ "Chen Lin", "Timothy Miller", "Dmitriy Dligach", "Steven Bethard", "Guergana Savova." ],
      "venue" : "Proceedings of the 2nd Clinical Natural Language",
      "citeRegEx" : "Lin et al\\.,? 2019",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "Visually and phonologically similar characters in incorrect simplified Chinese words",
      "author" : [ "Chao-Lin Liu", "Min-Hua Lai", "Yi-Hsuan Chuang", "Chia-Ying Lee." ],
      "venue" : "Coling 2010: Posters, pages 739–747, Beijing, China. Coling 2010 Organizing Committee.",
      "citeRegEx" : "Liu et al\\.,? 2010",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2010
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Automated rating of esl essays",
      "author" : [ "Deryle Lonsdale", "Diane Strong-Krause." ],
      "venue" : "Proceedings of the HLT-NAACL 03 workshop on Building educational applications using natural language processing, pages 61–67.",
      "citeRegEx" : "Lonsdale and Strong.Krause.,? 2003",
      "shortCiteRegEx" : "Lonsdale and Strong.Krause.",
      "year" : 2003
    }, {
      "title" : "Visualizing data using t-sne",
      "author" : [ "Laurens van der Maaten", "Geoffrey Hinton." ],
      "venue" : "Journal of machine learning research, 9(Nov):2579–2605.",
      "citeRegEx" : "Maaten and Hinton.,? 2008",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    }, {
      "title" : "Spelling correction for search engine queries",
      "author" : [ "Bruno Martins", "Mário J Silva." ],
      "venue" : "International Conference on Natural Language Processing (in Spain), pages 372–383. Springer.",
      "citeRegEx" : "Martins and Silva.,? 2004",
      "shortCiteRegEx" : "Martins and Silva.",
      "year" : 2004
    }, {
      "title" : "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "pages 3982–3992.",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Introduction to SIGHAN 2015 bake-off for Chinese spelling check",
      "author" : [ "Yuen-Hsien Tseng", "Lung-Hao Lee", "Li-Ping Chang", "Hsin-Hsi Chen." ],
      "venue" : "Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing, pages 32–37, Beijing,",
      "citeRegEx" : "Tseng et al\\.,? 2015",
      "shortCiteRegEx" : "Tseng et al\\.",
      "year" : 2015
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "A hybrid approach to automatic corpus generation for Chinese spelling check",
      "author" : [ "Dingmin Wang", "Yan Song", "Jing Li", "Jialong Han", "Haisong Zhang." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Confusionset-guided pointer networks for Chinese spelling check",
      "author" : [ "Dingmin Wang", "Yi Tay", "Li Zhong." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5780–5785, Florence, Italy. Associa-",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Chinese spelling check evaluation at sighan bake-off 2013",
      "author" : [ "Shih-Hung Wu", "Chao-Lin Liu", "Lung-Hao Lee." ],
      "venue" : "Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing, pages 35–42.",
      "citeRegEx" : "Wu et al\\.,? 2013",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2013
    }, {
      "title" : "End-to-end open-domain question answering with BERTserini",
      "author" : [ "Wei Yang", "Yuqing Xie", "Aileen Lin", "Xingyu Li", "Luchen Tan", "Kun Xiong", "Ming Li", "Jimmy Lin." ],
      "venue" : "pages 72–77.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Chinese spelling error detection and correction based on language model, pronunciation, and shape",
      "author" : [ "Junjie Yu", "Zhenghua Li." ],
      "venue" : "Proceedings of The Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 220–223.",
      "citeRegEx" : "Yu and Li.,? 2014",
      "shortCiteRegEx" : "Yu and Li.",
      "year" : 2014
    }, {
      "title" : "Overview of SIGHAN 2014 bake-off for Chinese spelling check",
      "author" : [ "Liang-Chih Yu", "Lung-Hao Lee", "Yuen-Hsien Tseng", "Hsin-Hsi Chen." ],
      "venue" : "Proceedings of The Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 126–132,",
      "citeRegEx" : "Yu et al\\.,? 2014",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2014
    }, {
      "title" : "Spelling error correction with soft-masked BERT",
      "author" : [ "Shaohua Zhang", "Haoran Huang", "Jicong Liu", "Hang Li." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 882–890, Online. Association for Com-",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "Chinese spelling correction (CSC) aims to detect and correct spelling errors in texts (Yu and Li, 2014).",
      "startOffset" : 86,
      "endOffset" : 103
    }, {
      "referenceID" : 19,
      "context" : "It is a challenging yet important task in natural language processing, which plays an important role in various NLP applications such as search engine (Martins and Silva, 2004) and optical character recognition (Afli et al.",
      "startOffset" : 151,
      "endOffset" : 176
    }, {
      "referenceID" : 0,
      "context" : "It is a challenging yet important task in natural language processing, which plays an important role in various NLP applications such as search engine (Martins and Silva, 2004) and optical character recognition (Afli et al., 2016).",
      "startOffset" : 211,
      "endOffset" : 230
    }, {
      "referenceID" : 29,
      "context" : "Chinese spelling correction is a challenging task because it requires human-level language understanding ability to completely solve this problem (Zhang et al., 2020).",
      "startOffset" : 146,
      "endOffset" : 166
    }, {
      "referenceID" : 4,
      "context" : "In fact, one of the mainstream solutions to this task is based on language models (Chen et al., 2013; Yu and Li, 2014; Tseng et al., 2015).",
      "startOffset" : 82,
      "endOffset" : 138
    }, {
      "referenceID" : 27,
      "context" : "In fact, one of the mainstream solutions to this task is based on language models (Chen et al., 2013; Yu and Li, 2014; Tseng et al., 2015).",
      "startOffset" : 82,
      "endOffset" : 138
    }, {
      "referenceID" : 21,
      "context" : "In fact, one of the mainstream solutions to this task is based on language models (Chen et al., 2013; Yu and Li, 2014; Tseng et al., 2015).",
      "startOffset" : 82,
      "endOffset" : 138
    }, {
      "referenceID" : 29,
      "context" : "Currently, the latest approaches (Zhang et al., 2020; Cheng et al., 2020) are based on BERT (Devlin et al.",
      "startOffset" : 33,
      "endOffset" : 73
    }, {
      "referenceID" : 5,
      "context" : "Currently, the latest approaches (Zhang et al., 2020; Cheng et al., 2020) are based on BERT (Devlin et al.",
      "startOffset" : 33,
      "endOffset" : 73
    }, {
      "referenceID" : 7,
      "context" : ", 2020) are based on BERT (Devlin et al., 2019), which is a masked language model.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 29,
      "context" : "a set of similar characters, to fuse such information (Wang et al., 2018, 2019; Zhang et al., 2020).",
      "startOffset" : 54,
      "endOffset" : 99
    }, {
      "referenceID" : 25,
      "context" : "We conduct experiments on the widely used benchmark dataset SIGHAN (Wu et al., 2013; Yu et al., 2014; Tseng et al., 2015).",
      "startOffset" : 67,
      "endOffset" : 121
    }, {
      "referenceID" : 28,
      "context" : "We conduct experiments on the widely used benchmark dataset SIGHAN (Wu et al., 2013; Yu et al., 2014; Tseng et al., 2015).",
      "startOffset" : 67,
      "endOffset" : 121
    }, {
      "referenceID" : 21,
      "context" : "We conduct experiments on the widely used benchmark dataset SIGHAN (Wu et al., 2013; Yu et al., 2014; Tseng et al., 2015).",
      "startOffset" : 67,
      "endOffset" : 121
    }, {
      "referenceID" : 29,
      "context" : "Experimental results show that PLOME significantly outperforms all the compared approaches, including the latest Soft-masked BERT (Zhang et al., 2020) and SpellGCN (Cheng et al.",
      "startOffset" : 130,
      "endOffset" : 150
    }, {
      "referenceID" : 19,
      "context" : "Chinese spelling correction is a challenging task in natural language processing, which plays important roles in many applications, such as search engine (Martins and Silva, 2004; Gao et al., 2010), automatic essay scoring (Burstein and Chodorow, 1999; Lonsdale and Strong-Krause, 2003), and optical character recognition (Afli et al.",
      "startOffset" : 154,
      "endOffset" : 197
    }, {
      "referenceID" : 2,
      "context" : ", 2010), automatic essay scoring (Burstein and Chodorow, 1999; Lonsdale and Strong-Krause, 2003), and optical character recognition (Afli et al.",
      "startOffset" : 33,
      "endOffset" : 96
    }, {
      "referenceID" : 17,
      "context" : ", 2010), automatic essay scoring (Burstein and Chodorow, 1999; Lonsdale and Strong-Krause, 2003), and optical character recognition (Afli et al.",
      "startOffset" : 33,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : ", 2010), automatic essay scoring (Burstein and Chodorow, 1999; Lonsdale and Strong-Krause, 2003), and optical character recognition (Afli et al., 2016; Wang et al., 2018).",
      "startOffset" : 132,
      "endOffset" : 170
    }, {
      "referenceID" : 23,
      "context" : ", 2010), automatic essay scoring (Burstein and Chodorow, 1999; Lonsdale and Strong-Krause, 2003), and optical character recognition (Afli et al., 2016; Wang et al., 2018).",
      "startOffset" : 132,
      "endOffset" : 170
    }, {
      "referenceID" : 27,
      "context" : "It has been an active topic, and various approaches have been proposed in recent years (Yu and Li, 2014; Wang et al., 2018, 2019; Zhang et al., 2020; Cheng et al., 2020).",
      "startOffset" : 87,
      "endOffset" : 169
    }, {
      "referenceID" : 29,
      "context" : "It has been an active topic, and various approaches have been proposed in recent years (Yu and Li, 2014; Wang et al., 2018, 2019; Zhang et al., 2020; Cheng et al., 2020).",
      "startOffset" : 87,
      "endOffset" : 169
    }, {
      "referenceID" : 5,
      "context" : "It has been an active topic, and various approaches have been proposed in recent years (Yu and Li, 2014; Wang et al., 2018, 2019; Zhang et al., 2020; Cheng et al., 2020).",
      "startOffset" : 87,
      "endOffset" : 169
    }, {
      "referenceID" : 3,
      "context" : "Some researchers focused on unsupervised approaches, which typically adopted a confusion set to find correct candidates and employed language model to select the correct one (Chang, 1995; Huang et al., 2000; Chen et al., 2013; Yu and Li, 2014; Tseng et al., 2015).",
      "startOffset" : 174,
      "endOffset" : 263
    }, {
      "referenceID" : 11,
      "context" : "Some researchers focused on unsupervised approaches, which typically adopted a confusion set to find correct candidates and employed language model to select the correct one (Chang, 1995; Huang et al., 2000; Chen et al., 2013; Yu and Li, 2014; Tseng et al., 2015).",
      "startOffset" : 174,
      "endOffset" : 263
    }, {
      "referenceID" : 4,
      "context" : "Some researchers focused on unsupervised approaches, which typically adopted a confusion set to find correct candidates and employed language model to select the correct one (Chang, 1995; Huang et al., 2000; Chen et al., 2013; Yu and Li, 2014; Tseng et al., 2015).",
      "startOffset" : 174,
      "endOffset" : 263
    }, {
      "referenceID" : 27,
      "context" : "Some researchers focused on unsupervised approaches, which typically adopted a confusion set to find correct candidates and employed language model to select the correct one (Chang, 1995; Huang et al., 2000; Chen et al., 2013; Yu and Li, 2014; Tseng et al., 2015).",
      "startOffset" : 174,
      "endOffset" : 263
    }, {
      "referenceID" : 21,
      "context" : "Some researchers focused on unsupervised approaches, which typically adopted a confusion set to find correct candidates and employed language model to select the correct one (Chang, 1995; Huang et al., 2000; Chen et al., 2013; Yu and Li, 2014; Tseng et al., 2015).",
      "startOffset" : 174,
      "endOffset" : 263
    }, {
      "referenceID" : 6,
      "context" : "2018) and sequence-to-sequence generative models (Chollampatt et al., 2016; Ji et al., 2017; Ge et al., 2018; Wang et al., 2019) were employed.",
      "startOffset" : 49,
      "endOffset" : 128
    }, {
      "referenceID" : 12,
      "context" : "2018) and sequence-to-sequence generative models (Chollampatt et al., 2016; Ji et al., 2017; Ge et al., 2018; Wang et al., 2019) were employed.",
      "startOffset" : 49,
      "endOffset" : 128
    }, {
      "referenceID" : 9,
      "context" : "2018) and sequence-to-sequence generative models (Chollampatt et al., 2016; Ji et al., 2017; Ge et al., 2018; Wang et al., 2019) were employed.",
      "startOffset" : 49,
      "endOffset" : 128
    }, {
      "referenceID" : 24,
      "context" : "2018) and sequence-to-sequence generative models (Chollampatt et al., 2016; Ji et al., 2017; Ge et al., 2018; Wang et al., 2019) were employed.",
      "startOffset" : 49,
      "endOffset" : 128
    }, {
      "referenceID" : 7,
      "context" : "BERT (Devlin et al., 2019) is a bidirectional language model based on Transformer encoder (Vaswani et al.",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 22,
      "context" : ", 2019) is a bidirectional language model based on Transformer encoder (Vaswani et al., 2017).",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 26,
      "context" : "It has been demonstrated effective in a wide range of applications, such as question answering (Yang et al., 2019), information extraction (Lin et al.",
      "startOffset" : 95,
      "endOffset" : 114
    }, {
      "referenceID" : 14,
      "context" : ", 2019), information extraction (Lin et al., 2019), and semantic matching (Reimers and Gurevych, 2019).",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 10,
      "context" : "Recently, it has dominated the researches on CSC (Hong et al., 2019; Zhang et al., 2020; Cheng et al., 2020).",
      "startOffset" : 49,
      "endOffset" : 108
    }, {
      "referenceID" : 29,
      "context" : "Recently, it has dominated the researches on CSC (Hong et al., 2019; Zhang et al., 2020; Cheng et al., 2020).",
      "startOffset" : 49,
      "endOffset" : 108
    }, {
      "referenceID" : 5,
      "context" : "Recently, it has dominated the researches on CSC (Hong et al., 2019; Zhang et al., 2020; Cheng et al., 2020).",
      "startOffset" : 49,
      "endOffset" : 108
    }, {
      "referenceID" : 13,
      "context" : "(2020) employed the graph convolution network (GCN) (Kipf and Welling, 2016) combined with BERT to model character interdependence.",
      "startOffset" : 52,
      "endOffset" : 76
    }, {
      "referenceID" : 7,
      "context" : "Similar to BERT (Devlin et al., 2019), the proposed model also follows the pretraining&fine-tuning paradigm.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 25,
      "context" : "Similar characters are obtained from a publicly available confusion set (Wu et al., 2013), which contains two types of similar characters: phonologically similar and visually similar.",
      "startOffset" : 72,
      "endOffset" : 89
    }, {
      "referenceID" : 15,
      "context" : "Since phonological errors are two times more frequent than visual errors (Liu et al., 2010), these two types of similar characters are assigned different chance to be chosen during masking.",
      "startOffset" : 73,
      "endOffset" : 91
    }, {
      "referenceID" : 16,
      "context" : "In addition, we use dynamic masking strategy (Liu et al., 2019), where the masking pattern is generated every time a sequence is fed into the model.",
      "startOffset" : 45,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : "The former two are obtained via looking up embedding tables, where the size of vocabulary and embedding dimension are the same as that in BERTbase (Devlin et al., 2019).",
      "startOffset" : 147,
      "endOffset" : 168
    }, {
      "referenceID" : 1,
      "context" : "To model the phonological relationship between characters, we feed the letters of each character’s phonics to a 1-layer GRU (Bahdanau et al., 2014) network to generate the phonic embedding, where similar phonics are expected to have similar embeddings.",
      "startOffset" : 124,
      "endOffset" : 147
    }, {
      "referenceID" : 7,
      "context" : "The transformer encoder has the same architecture as that in BERTbase (Devlin et al., 2019).",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 22,
      "context" : "The number of transformer layers (Vaswani et al., 2017) is 12, the size of hidden units is 768 and the number of attention head is 12.",
      "startOffset" : 33,
      "endOffset" : 55
    }, {
      "referenceID" : 29,
      "context" : "In practice, about 80% of spelling errors are phonological (Zhang et al., 2020).",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 7,
      "context" : "The configuration of transformer encoder is exactly the same as that in BERTbase (Devlin et al., 2019), and the learning rate is set to 5e-5.",
      "startOffset" : 81,
      "endOffset" : 102
    }, {
      "referenceID" : 25,
      "context" : "(2020), the training data is composed of 10K manually annotated samples from SIGHAN (Wu et al., 2013; Yu et al., 2014; Tseng et al., 2015) and 271K automatically generated samples from Wang et al.",
      "startOffset" : 84,
      "endOffset" : 138
    }, {
      "referenceID" : 28,
      "context" : "(2020), the training data is composed of 10K manually annotated samples from SIGHAN (Wu et al., 2013; Yu et al., 2014; Tseng et al., 2015) and 271K automatically generated samples from Wang et al.",
      "startOffset" : 84,
      "endOffset" : 138
    }, {
      "referenceID" : 21,
      "context" : "(2020), the training data is composed of 10K manually annotated samples from SIGHAN (Wu et al., 2013; Yu et al., 2014; Tseng et al., 2015) and 271K automatically generated samples from Wang et al.",
      "startOffset" : 84,
      "endOffset" : 138
    }, {
      "referenceID" : 21,
      "context" : "Evaluation Data We use the latest SIGHAN test dataset (Tseng et al., 2015) as in Zhang et al.",
      "startOffset" : 54,
      "endOffset" : 74
    }, {
      "referenceID" : 5,
      "context" : "Evaluation Metrics Following previous work (Cheng et al., 2020; Zhang et al., 2020), we use the",
      "startOffset" : 43,
      "endOffset" : 83
    }, {
      "referenceID" : 29,
      "context" : "Evaluation Metrics Following previous work (Cheng et al., 2020; Zhang et al., 2020), we use the",
      "startOffset" : 43,
      "endOffset" : 83
    }, {
      "referenceID" : 23,
      "context" : "Hybird (Wang et al., 2018) uses a BiLSTMbased model trained on an automatically generated dataset.",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 24,
      "context" : "PN (Wang et al., 2019) is a Seq2Seq model incorporating a pointer network.",
      "startOffset" : 3,
      "endOffset" : 22
    }, {
      "referenceID" : 10,
      "context" : "FASPell (Hong et al., 2019) adopts the DAEDecoder paradigm and employs BERT as the denoising auto-encoder.",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 29,
      "context" : "SKBERT (Zhang et al., 2020) introduces the SoftmasKing strategy in BERT to improve the performance of error detection.",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 5,
      "context" : "SpellGCN (Cheng et al., 2020) combines a GCN network with BERT to model the relationship between characters in the given confusion set.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 24,
      "context" : "Previous work (Wang et al., 2019; Cheng et al., 2020) conducted the character-level evaluation on positive sentences which contain at least one error (sentence-level metrics were evaluated on the whole test set).",
      "startOffset" : 14,
      "endOffset" : 53
    }, {
      "referenceID" : 5,
      "context" : "Previous work (Wang et al., 2019; Cheng et al., 2020) conducted the character-level evaluation on positive sentences which contain at least one error (sentence-level metrics were evaluated on the whole test set).",
      "startOffset" : 14,
      "endOffset" : 53
    }, {
      "referenceID" : 25,
      "context" : "To make more comprehensive comparisons, we also evaluate the proposed model on SIGHAN13(Wu et al., 2013) and SIGHAN14(Yu et al.",
      "startOffset" : 87,
      "endOffset" : 104
    }, {
      "referenceID" : 18,
      "context" : "Figure 4 illustrates 30 characters nearest to ‘锭’ according to the cosine similarity of the 768-dim embeddings generated by GRU networks, which is visualized via t-SNE (Maaten and Hinton, 2008).",
      "startOffset" : 168,
      "endOffset" : 193
    } ],
    "year" : 2021,
    "abstractText" : "Chinese spelling correction (CSC) is a task to detect and correct spelling errors in texts. CSC is essentially a linguistic problem, thus the ability of language understanding is crucial to this task. In this paper, we propose a Pre-trained masked Language mOdel with Misspelled knowledgE (PLOME) for CSC, which jointly learns how to understand language and correct spelling errors. To this end, PLOME masks the chosen tokens with similar characters according to a confusion set rather than the fixed token “[MASK]” as in BERT. Besides character prediction, PLOME also introduces pronunciation prediction to learn the misspelled knowledge on phonic level. Moreover, phonological and visual similarity knowledge is important to this task. PLOME utilizes GRU networks to model such knowledge based on characters’ phonics and strokes. Experiments are conducted on widely used benchmarks. Our method achieves superior performance against state-of-the-art approaches by a remarkable margin. We release the source code and pre-trained model for further use by the community1.",
    "creator" : "LaTeX with hyperref"
  }
}