{
  "name" : "2021.acl-long.70.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "What Context Features Can Transformer Language Models Use?",
    "authors" : [ "Joe O’Connor", "Jacob Andreas" ],
    "emails" : [ "joeoc@mit.edu", "jda@mit.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 851–864\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n851"
    }, {
      "heading" : "1 Introduction",
      "text" : "Recent years have seen a significant improvement in the predictive accuracy of neural language models (LMs), owing to a combination of improvements in model architecture (especially transformers; Vaswani et al. 2017) and training infrastructure (Wolf et al., 2020). The most striking change, relative to both recurrent neural LMs (Mikolov et al., 2010) and count-based models (Kneser and Ney, 1995), is the length of the context that these models can effectively condition on. While count-based LMs in production speech recognition and machine translation systems typically used 10–20 tokens at a maximum (e.g., Brown, 2011), and recurrent LMs have an effective context size of 200 (Khandelwal et al., 2018), the predictive accuracy of transformer LMs appears to improve when conditioning on as many as a thousand previous tokens (Beltagy et al., 2020). A significant amount of recent work has\n1Code for all experiments in this paper is available at https://github.com/lingo-mit/context-ablations.\nfocused on making use of even longer contexts computationally feasible (Rae et al., 2019; Wang et al., 2020; Child et al., 2019; Dai et al., 2019; Kitaev et al., 2020).\nBut despite empirical evidence that long contexts are helpful, little is understood about why. If the future of language modeling will include a focus on contexts of increasing size, it is important to first understand what contextual information contributes to accurate prediction in current models. This paper offers an answer to that question via the V-information framework of Xu et al. (2020). Vinformation, discussed more in Section 2, provides a formal framework for reasoning about how much usable information a computationally constrained predictor (like a neural LM) can extract from an input. Our experiments measure the amount of usable information that is added when increasing LM context size, then attempt to pinpoint the source of this information by ablating features of the added context (via controlled shuffling and word deletion) and measuring the resulting loss of model predictive power. While this framework is general, we focus on transformer LMs.\nOur work is closely related to an earlier study by Khandelwal et al. (2018), which measured changes in a pre-trained LSTM LM when context words were permuted and deleted at evaluation time. But neural language models are known to be highly sensitive to distributional shifts—and in particular might be unable to use information from long-range context but still be adversely affected when the structure of that context changes at evaluation time. Directly measuring usable information makes it possible to clearly distinguish accuracy decreases that result from loss of information and decreases that result from out-of-distribution inputs.\nOur experiments reveal a number of surprising facts about the use of long- and mid-range context in transformers. While increasing context length\nfrom 256 to 768 tokens is beneficial (decreasing perplexity by roughly 4%), many destructive transformations of this context (including transformations that cause large changes in the paradigm of Khandelwal et al. 2018) remove essentially no usable information. Our results suggest that for current models, the primary carriers of information in long-range context are content words and local cooccurrence statistics: deleting function words and shuffling within local windows both have very little effect on models’ predictive power. Context matters, but not all features of context matter equally; as discussed in Section 5, these results motivate future language modeling research focused on alternative context representations rather than simply more tokens."
    }, {
      "heading" : "2 Approach",
      "text" : "A language model (LM) places a probability distribution p(x) over discrete token sequences x. Most learned LMs do so by decomposing p(x) according to the chain rule and modeling the conditional distribution over a single target token given a (fixedor variable-length) context of previous tokens:\np(x) = ∏ i p(xi | x0, x1, . . . , xi−1) . (1)\nIn transformer language models, this conditional distribution is modeled via a sequence of alternating neural feed-forward layers and self-attention layers; see Vaswani et al. (2017) for more details.\nWhile input sequences x can in principle be made arbitrarily long, there are both theoretical and practical limits to transformers’ ability to make effective use of it (Hahn, 2020; Wang et al., 2019). Here, we wish to understand when (and why) increasing the size of the context improves model predictions.\nUsable information Consider a hypothetical LM context consisting of the tokens The user’s password is. . . . This context suggests that subsequent tokens will be a password: (hopefully!) a high-entropy sequence. Now suppose this context is extended to include earlier tokens, becoming The user’s hashed password is ave$@To9!. The user’s password is. . . . Information-theoretically, this context is extremely informative: only a small number of passwords will hash to the given string, and a predictor capable of testing all passwords would be able to identify the candidates and significantly reduce its uncertainty about future tokens.\nBut in practice, this extra context is useless: no known efficient predictor can learn anything about the password from its hash code, and the extra context has not made the language modeling problem any easier. This is an extreme case, but a similar intuition applies to more conventional questions about language models. A newspaper article whose first sentence begins A dog bit a man is likely to end very differently from one that begins A man bit a dog. Can LMs reason effectively about this distinction, or is it (like a hashed password) computationally inaccessible to current models?\nA framework for answering questions of this kind was introduced by Xu et al. (2020): Definition 1. The usable predictive information (formally, predictive V-information) from a random variable X to a random variable Y as:\nIV(X → Y ) = [ inf p1∈V −E log p1(Y ) ] − [ inf p2∈V −E log p2(Y | X) ] (2)\nfor a class V of distributions p. Intuitively, this definition measures how much extra information about Y can be extracted from X by any predictor in V . In language modeling, we will take Y to be the target word, X its context, and V a class of parametric models. While this definition generalizes Shannon mutual information (Shannon, 1948) and has deep connections to other information-theoretic quantities (see Xu et al. 2020 for details) it ultimately corresponds to a simple and common-sense evaluation: if we want to know how much the extra context X helps a language model, we should train a model p1 without access toX , train a model p2 with access toX , and compare the accuracy of their predictions.\nMeasuring what is used But the original question raised by the introduction was not just how much information is contributed by context. It is already well-established that conditioning on long contexts is helpful, with existing experiments on long-range transformers effectively implementing the measurement in Eq. (2). Instead, we want to know what information in this context is actually used by models.\nAs a prototypical example, let us hypothesize that more than five tokens away from the target, models are only able to extract usable information from nouns. (In our experiments in Section 3, this “long-range context” will be considerably longer than 5 words.) For example, given the sentence:\nPierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29.\nwe hypothesize that the LM distributions:\np1(director | Pierre Vinken, 61 years old, will join the board as a nonexecutive) (3)\n≈ p2(director | Pierre Vinken years︸ ︷︷ ︸ noun-only context ,\nthe board as a nonexecutive︸ ︷︷ ︸ ordinary context ) , (4)\nand more generally that\nIV(X0:n → Xn) ≈ IV([nouns(X0:n−5), Xn−5:n]→ Xn) (5)\nwhere Xi:j is the sequence of tokens [Xi, Xi+1, . . . , Xj−1], V is a class of LMs, and nouns is a context ablation that extracts only the nouns from a given string. That is, we hypothesize that the amount of usable information contributed by the full context X0:n is the same as the amount contributed by the ablated context [nouns(X0:n−5), Xn−5:n], so ablation removes no information.\nThe experiments in this paper generalize this experimental framework to other context ablations and hypotheses. Let f be an ablation and k an integer offset, and denote an ablated context:\nfk(X) = [f(X0:n−k), Xn−k:n] (6)\nand an ablated negative log-likelihood:\nL(θ, f, k) = −E log pθ(Xn | fk(X0:n)) (7)\nThen, we can measure the effect of each ablation f on usable information via the following quantity:\nDefinition 2. The ablated information due to an ablation f at an offset k is:\nA(f, k) = IV (X0:n→Xn)−IV (fk(X0:n)→Xn) IV (X0:n→Xn)−IV (Xn−k:n→Xn) (8)\n= infθ L(θ,f,k)−infθ′ L(θ′,n)\ninfθ′′ L(θ′′,n−k)−infθ′ L(θ′,n) , (9)\nwhere L(θ, i) is the (unablated) negative loglikelihood −E log pθ(Xn | Xn−i:n).\nIntuitively, A(f, k) measures how much of the usable information added by an extra k tokens (the denominator) is removed by applying the ablation f to those k tokens (the numerator). If it is close to 0, almost no information is removed; if it is close to 1, almost all information is removed."
    }, {
      "heading" : "Transformer LM",
      "text" : "Evaluation in practice Eq. (9) provides a general framework for answering our core question in this paper: for a diverse set of context ablations and offsets, we will measure how much information is lost when a given ablation is applied at a given offset. A few modifications are required to turn this equation into a practical evaluation scheme:\nHeld-out evaluation: Eq. (7) involves an expectation over the sequence distribution p(X). In practice, LMs must be trained on finite corpora, creating a risk of overfitting (Zhang et al., 2016). To address this issue, we approximate the infimum in Eq. (7) by fitting θ1 on a training set, and computing ablated information on a held-out validation set. All reported results are an average of held-out likelihoods from two random initializations.\nBatching: Given a fixed (training or test) dataset of strings X and a maximum context size of m, Eq. (7) should be estimated empirically as − 1|X | ∑ x 1 |x| ∑|x|\ni=0 log p(Xi | fk(Xi−m:i)). This requires re-computing model predictions once for every token in the dataset. However, the transformer models we use here support efficient batch inference: training data is presegmented into sequences of at most length n, and − 1|X |n ∑ x ∑n i=0 log p(Xi | fk(X0:i)) can be computed in a single forward pass. This is considerably more efficient but means that most tokens are evaluated with a context of length < n. As a compromise to ensure that evaluations contain long-range context, we accumulate losses on a subset:\nL(θ, f, ` : m ∼ n) = − 1 |X |(n−m)∑\nx `+n∑ i=`+m log pθ(Xi | [f(X0:`), X`:i])\n(10)\n(visualized in Fig. 1). This can be read as “` tokens of f -ablated context, followed by m to n tokens of unablated context”. We will write L(θ,m ∼ n)\nwhen only unablated context is used. Because of the large number of experiments in this paper, we use Eq. (10) for all training and evaluation.\nModel, data and training details For all experiments, our LM uses the GPT-2 model architecture (Radford et al., 2019) in the implementation of Wolf et al. (2020) with default hyperparameters. All models are trained from scratch on the WikiText-103 dataset (Merity et al., 2016), an English language modeling benchmark. Aside from ablations, no preprocessing is applied. A special separator token is inserted between ablated and unablated context. The training set contains 103,221,021 words, while the evaluation set contains 217,646 words.\nA note on evaluation As in past work on evaluating language models (Brown et al., 1992), our evaluation of relative predictive information ultimately bottoms out in a conditional entropy (logperplexity). Recent work has shown that other metrics, such as diversity of outputs, are important for evaluating the quality of LMs as models for language generation (Hashimoto et al., 2019; Caccia et al., 2020). Generation also depends on a number of other factors, such as choice of decoding procedure (Caglayan et al., 2020). Here, we focus on LMs as predictive models, measuring their ability to place an accurate distribution over future words and sentences, rather than their ability to generate useful or coherent text (see Appendix C). We want to emphasize that these results below apply to language models specifically, and not transformers applied to NLP tasks in general—the same analysis might give very different conclusions if applied to, e.g., question answering or summarization."
    }, {
      "heading" : "3 Experiments",
      "text" : "In this section, we attempt to determine what information in transformer LM contexts is usable by measuring ablated information (Eq. (9)). Sections 3.1 and 3.2 describe our main results, with Section 3.1 focused on ordering and Section 3.2 focused on lexical information. Section 3.3 compares these results to ablations applied at evaluation time. Section 3.4 explores whether contexts can be further manipulated to improve model predictions."
    }, {
      "heading" : "3.1 Does order matter?",
      "text" : "In this section we will examine the effects of different augmentations to the order within long-range\ncontext. We first train a no information model to minimize L(θ, 0 ∼ 512) and a full information model to minimize L(θ, 512 ∼ 1024). For each context ablation f , we train a model to minimize L(θ, f, 512 : 0 ∼ 512). Each ablation has access to more information than the no information model (because it conditions on extra tokens) and less information than the full information model (because an ablation has been applied to those tokens). Note that the LM operates on BPE-derived subword tokens for consistency with the way GPT-2 is typically used, but all ablations are defined at the word level, meaning, e.g., that we shuffle words rather than tokens.\nWe use these trained models to calculate ablated information (Eq. (9)). To explore the effect of different context lengths, we stratify evaluation of the ablated information into two conditions: a mid-range condition in which likelihoods in Eq. (9) are of the form L(·, f, 512 : 0 ∼ 256), and a long-range condition with likelihoods L(·, f, 512 : 256 ∼ 512). (We call the former “mid-range” rather than “short-range” because most tokens are still predicted with significant unablated context; our experiments do not characterize sentence-internal modeling of syntactic wellformedness.) Results are shown in Figure 2 and discussed below."
    }, {
      "heading" : "Overall word order",
      "text" : "shuffle all\n61 N.V., director the of Mr. Vinken Dutch group. as nonexecutive the 29. is Vinken, years Elsevier join old, publishing a Nov. will Pierre board chairman\nshuf. trigrams globally\npublishing group. N.V., the Dutch Mr. Vinken is join the board as a nonexecutive years old, will chairman of Elsevier Pierre Vinken, 61 director Nov. 29.\nIn the shuffle all ablation, f shuffles words uniformly at random, forcing the model to treat ablated context as a bag of words. In the shuf. trigrams globally ablation, the context is divided up into nonoverlapping trigrams, the order of which is then permuted uniformly at random. Shuffling all words removes 41% of usable information in the midrange condition and 84% in the long-range condition: ordering information is important even very far from the target. On the other hand, shuffling all trigrams removes 31% of usable information in the mid-range condition and 50% in the long-range condition: local co-occurrence statistics carry a significant amount of usable information.\nWord order within sentences shuf. within sent.\n61 director as the old, join will a Nov. board nonexecutive years Vinken, 29. Pierre is publishing the Vinken N.V., Mr. group. chairman Elsevier of Dutch\nshuf. within trigrams\nVinken, Pierre 61 will old, years the board join a nonexecutive as Nov. director 29. Mr. Vinken is of Elsevier chairman the Dutch N.V., group. publishing\nshuf. trigrams within sent.\nyears old, will as a nonexecutive join the board Pierre Vinken, 61 director Nov. 29. N.V., the Dutch chairman of Elsevier Mr. Vinken is publishing group.\nWords are shuffled only within sentences according to one of three procedures: (1) a uniform random permutation of all the words in the sentence (shuf. within sent.), (2) a uniform random permutation of the words within each non-overlapping trigram in the sentence (shuf. within trigrams), and (3) a uniform random permutation of the order of the trigrams within the sentence (shuf. trigrams within\nsent.). (1) and (2) were also recently explored by Pham et al. (2020) in models for entailment, and more complex shuffling procedures have been explored in neuroscience contexts (Mollica et al., 2020). Here, (2) and (3) are chosen because they preserve local co-occurrence statistics ((3) more than (2)), while (2) also preserves the general linear information flow of the sentence.\nNotably, the shuf. within trigrams (14% and 41%) and the shuf. trigrams within sent. (16% and 35%) ablations both remove relatively little usable information in both the mid- and long-range conditions. Usable information is decreased only slightly by ablations that preserve local co-occurrence statistics and/or linear information flow. (This includes transformations like man bites dog→ dog bites man with significant effects on semantics!) In the long-range condition, uniform shuffling within sentences produces a larger effect, removing 55% of usable information."
    }, {
      "heading" : "Sentence order",
      "text" : "shuf. sent.\nMr. Vinken is chairman of Elsevier N.V., the Dutch publishing group. Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29.\nNext, sentences are shuffled within the context while their internal word order is unchanged. In the mid-range condition, this produces results comparable to the trigram shuffling experiments above (removing 17% of usable information); in the longrange condition, it has an even smaller effect (14%). Together with the previous experiment these results suggest that prediction accuracy depends on information about local word co-occurrence, but not fine-grained word order or global position."
    }, {
      "heading" : "Order of entire sections",
      "text" : "replace w/ old\nRudolph Agnew, 55 years old and former chairman of Consolidated Gold Fields PLC, was named a nonexecutive director of this British industrial conglomerate.\nA possible hypothesis about LM behavior is that the main function of long-range context is to provide more information about the general topic of the document, including clues about vocabulary and style. To test this, the ablation replaces its entire input with the 512 tokens that immediately precede it in the source document (which in general will be topically similar). This transformation removes significant information in both mid- and long-range conditions (55% and 69%). Long-range context is\nnot simply a source of topic information: earlier text on the same theme is in some cases nearly as uninformative as no text at all."
    }, {
      "heading" : "3.2 Do all words matter?",
      "text" : "Our next experiments focus on lexical rather than structural information, using ablations that delete selected words from the context. Training and evaluation setups are exactly as in Section 3.1. Here, unlike the previous section, ablations will generally cause the number of tokens in a given context to decrease; in this case ablations also insert padding tokens to the beginning of the context window to preserve the original number of tokens. Results are shown in Fig. 3."
    }, {
      "heading" : "Parts of speech",
      "text" : "N\nPierre Vinken years board director Nov. Mr. Vinken chairman Elsevier N.V. publishing group\nN & VB\nPierre Vinken years will join board director Nov. Mr. Vinken chairman Elsevier N.V. publishing group\nN & VB & ADJ\nPierre Vinken years old will join board nonexecutive director Nov. Mr. Vinken chairman Elsevier N.V. Dutch publishing group\ncont. words (N & VB & ADJ & ADV)\nPierre Vinken years old will join board nonexecutive director Nov. Mr. Vinken chairman Elsevier N.V. Dutch publishing group\nfunc. words\n, 61 , the as a 29 . is of , the .\nAs in the initial example from Section 2, we retain only words whose part of speech tag is in a given set. We use the spaCy model (Honnibal et al., 2020) for part-of-speech tagging, and examine five sets: (1) nouns only, (2) nouns and verbs, (3) nouns, verbs, and adjectives, (4) content words (nouns, verbs, adjectives, and adverbs), and (5) function words (all words except nouns, verbs, adjectives, and adverbs).\nIn the mid-range condition, deleting all words but nouns removes only 20% of usable information; deleting all but nouns and verbs removes only 13%. Most usable information, even in mid-range context, appears to be captured by nouns and verbs. Retaining only function words causes a considerably greater loss of information.\nIn the long-range condition, results are even more striking: retaining only content words improves predictions over the “full information” experiment. Like Shannon information, Vinformation is defined to be non-negative (Xu et al., 2020), and the result in Fig. 3 is a consequence of our finite-sample approximation based on heldout likelihood. The effect is robust across multiple training runs from random initializations. As there is a significant gap between the training and validation perplexity of our model (roughly 11%), we hypothesize that this change occurs because the ablation preserves semantic content while reducing the original model’s ability to overfit. We believe this is an important subject for future investigation."
    }, {
      "heading" : "Named entities",
      "text" : "named entities\nPierre Vinken 61 years old Nov. 29 Vinken Elsevier N.V. Dutch\nAs an alternative to the topic hypothesis evaluated under “Order of entire sections” above, we might\nhypothesize that long-range contexts are useful because they provide a reservoir of named entities likely to be referred to again. Here, the ablation retains only spans tagged as named entities or quantities by spaCy. While significantly worse than the noun ablation discussed above, retaining only entities results removes only about a third of usable information in both conditions (39% and 31%)."
    }, {
      "heading" : "Word frequency",
      "text" : "common\nPierre years old join board director . Mr. chairman Dutch publishing group .\nrare\nVinken nonexecutive Nov. Vinken Elsevier N.V.\nAnother natural question is whether rare words or frequent words are more important: information about frequent context words might help models estimate fine-grained document-level frequencies of those words account for most of the terms in Eq. (7); rare words are likely to be more informative about the content of the document itself.\nWe partition the vocabulary into a set of rare words, corresponding to the least frequent ∼ 98% of word types and 20% of word tokens, and frequent words, the most frequent ∼ 2% of types and 80% of tokens. Both ablations remove a significant amount of information relative to the POS-based ablations above, but retaining only frequent words improves perplexity relative to rare words in both the mid- and long-range conditions.\nAppendix B presents versions of these experiments trained and evaluated on even longer contexts. Conclusions are largely the same as above."
    }, {
      "heading" : "3.3 Evaluating on augmented data",
      "text" : "We motivated the use of V-information in Section 2 by arguing that it more clearly distinguished between prediction errors attributable to loss of information and prediction errors attributable to malformed and out-of-distribution model inputs. To put our results in context, we repeat several of the previous experiments in the evaluation paradigm of Khandelwal et al. (2018), which is designed to measure test-time sensitivity rather than usable information.\nWe train a new model to minimize L(θ, 512 ∼ 1024) while randomly truncating the first 512 context tokens and replacing them with padding tokens (to ensure that the model has seen padding tokens at training time). We then evaluate this model on\nthe set of ablations shown in Section 3.1 and Section 3.2. For the full information model in Fig. 4, we evaluate on ordered context windows with no padding tokens; for the no information model, we evaluate on context windows in which the first 512 tokens are all padding tokens.\nIn the mid-range condition, the least destructive ablations are shuffling within trigrams and shuffling the order of trigrams within sentences: models appear to be reasonably robust to this kind of data transformation without specific training on it. Importantly, lexical ablation experiments have a large impact in this evaluation, underlining the extent to which the two experimental paradigms characterize different aspects of model behavior. Figure 5 in Appendix A shows a side-by-side comparison of these experiments and the ones in Sections 3.1–3.2."
    }, {
      "heading" : "3.4 Making better language models?",
      "text" : "The lexical ablation experiments in Section 3.2 indicated that model accuracy could be improved by\nselective deletion of context words. Can this effect be exploited to further improve models? As a simple experiment, we attempted to replace all padding tokens in the nouns+verbs ablation of Section 3.2 with nouns and verbs from further back in the context—effectively providing the model with an even longer-range view of an informative context representation.\nThis experiment slightly increased usable information in the mid-range condition (0.2%), but decreased it in the long range-range condition (0.6%). Longer contexts, even of a kind previously found to be informative, did not provide additional usable information. These results are consistent with our earlier hypothesis that the previously observed effect resulted from a reduction in overfitting—if removing information increased performance by reducing overfitting, then it is reasonable that adding information back results in more overfitting."
    }, {
      "heading" : "4 Related Work",
      "text" : "Context in count-based and discriminative LMs The earliest learned LMs were count-based (e.g., Kneser and Ney, 1995): they estimated p(xn | x0:n) based on a (smoothed) empirical ngram frequency #(x0:n)/#(x0:n−1) (where #(x) is the number of times the sequence x appears in training data). As the number of distinct n-gram counts grows exponentially in n, it was typically set to a small value. Count-based models have a clear dependence on context: any token within the last n words that also appears in a training n-gram is relevant, anything further back is not.\nSubsequent models improved on these by allowing the use of skip-grams, caches, and featurebased models (Goodman, 2001; Bengio et al., 2003). Some of these in principle allowed the use of unlimited-length contexts, but only by imposing strong restrictions on the ways in which context features could interact.\nContext in RNN LMs Recurrent neural network language models (Mikolov et al., 2010; Elman, 1990) provide a more expressive mechanism for the use of long-range context: models write to a recurrent “state vector” which can be carried arbitrarily far into the future. Computational issues limit the effective context size such models can be practically trained on, but this size is still significantly greater the models mentioned above: as previously noted, Khandelwal et al. (2018) revealed influence from up to 200 tokens of context. Similar\neffects are reported by Sankar et al. (2019) for neural dialogue models, and Li et al. (2016) describe an alternative procedure for ablating contexts.\nContext in Transformer LMs Transformers introduce yet another mechanism for extracting information from long-range context: attention. Attention is also used with RNNs, but typically with just a single head—the hidden state still carries most of the information. In transformers, context enters into predictions primarily via unbounded random access. These models appear to benefit from significantly longer contexts than previous models.\nSome recent work that investigates the behavior of individual transformer attention heads (Clark et al., 2019; Voita et al., 2019). This work finds that certain attention heads are sensitive to things like word frequency, positional information, and certain syntactic phenomena. While extremely informative about the computational structures implemented by fixed models, these approaches do not necessarily reveal anything about usable information: indeed, patterns of attention do not necessarily correlate with model predictions (Jain and Wallace, 2019).\nOther related work Our finding that finegrained ordering information contributes little usable information is consistent with Rae et al. (2019)’s finding that long-range contexts could be informatively summarized in fixed-sized vectors; our finding that most usable information is carried by nouns is consistent with earlier findings about both specialized neural architectures (Henaff et al., 2016) and discourse representations in feature-based models (Barzilay and Lapata, 2008). Our approach also shares similar motivations to information-theoretic work on probing (Voita and Titov, 2020; Pimentel et al., 2020), which uses related tools to interpret linguistic structure in LM representations rather than characterizing their effect on LM predictions. Several recent papers have explored the effect of training-time and test-time ablations in models for other data analysis tasks: Pham et al. (2020) find that shuffling experiments have a limited effect on the accuracy of models for natural language inference, while Perez et al. (2021) describe several experiments aimed at introducing usable information for several question answering and sentence understanding tasks."
    }, {
      "heading" : "5 Discussion",
      "text" : "We have investigated the extent to which transformer models can use structural and lexical information in long-range contexts for English language modeling. Experiments demonstrated that this information is primarily contained in content words and local ordering statistics: ablations that remove other kinds of information from context have little effect on models’ predictive accuracies. In contrast, retaining only information about document identity or named entities causes significant drops in predictive accuracy: the effectiveness of long contexts is not explained by the presence of topic or named entity information alone.\nCrucial to obtaining these results was a measure of ablated usable information grounded in the accuracy of models trained and tested on ablated contexts. Past work on context in LMs has primarily measured the influence of evaluation-time ablations. Sometimes these two notions of contextsensitivity coincide (e.g., trigram shuffling) and sometimes they do not (e.g., removal of lexical information). Our results also offer a jumping-off point for future modeling work. They motivate more efficient, compressed context representations that better preserve the information that is usable by current models. They motivate more accurate models by developing new context representations that make currently unusable information more prominent.\nSeveral questions remain unanswered by our experiments. Do ablations affect the quality of text generated by models? (In particular, does the usable information added by long contexts improve predictability of syntax, semantics, or simply document-level word frequency statistics?) More fundamentally, do observations about usable information reflect limitations of transformers or fundamental, (Shannon-)information-theoretic properties of English? Our results suggest that at least some of these effects are model-specific: deleting function words cannot add information, but improves held-out model accuracy. A complete answer to this question will require more detailed exploration, including a better understanding of human predictions in comparable settings."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Thanks to Carina Kauf and Greta Tuckute, Evelina Fedorenko and Roger Levy for valuable discussions. We acknowledge the MIT SuperCloud and\nLincoln Laboratory Supercomputing Center for providing HPC resources that contributed to the results reported within this paper."
    }, {
      "heading" : "Impact Statement",
      "text" : "Across initial exploration, evaluation conditions and training runs, experiments in this paper required roughly 100 training runs on the WikiText103 dataset. As discussed in Section 2, model size and batched evaluation were both used to minimize the energy demands of these experiments; experiments themselves were performed at the Massachusetts Green HPC center, a carbon-neutral supercomputing facility. Ultimately, results in Section 3 provide guidance toward the design of models that use context more efficiently and motivate the large-scale empirical study conducted here."
    }, {
      "heading" : "A Comparison of Experimental Paradigms",
      "text" : "In Figure 5 we show the contrast between the experimental paradigm of Sections 3.1–3.2 and that of Section 3.3. Especially for the experiments involving parts of speech, we see a significant difference in both the quantitative and qualitative results across the two paradigms."
    }, {
      "heading" : "B Longer Context Window",
      "text" : "Here we report the results of repeating the experiments of Sections 3.1 and 3.2 with ablated contexts\nof size 1024 tokens instead of 512 tokens in order to verify that the behavior we observed is not specific to the size of context window we chose."
    }, {
      "heading" : "C Sample Generations",
      "text" : "The purpose of this section is to verify that models trained on ablated contexts can still generate text that is comparable to text generated by a model trained with full contextual information. We select a prompt from a randomly chosen Wikipedia article in the WikiText-103 validation set; each model generates a sentence (after finishing the sentence in progress) given the appropriately ablated version of the prompt. The prompt consists of 768 tokens, the last 256 of which remain unchanged for all versions of the prompt, so that the ablations are in the long range relative to the point of generation. The prompt and generations are as follows:\nprompt:\nat two independent schools for boys: Sussex House School, a day school in Chelsea’s Cadogan Square, and the City of London School, a day school on the North Bank of the River Thames in London’s financial district (known as the City\n4.20 4.25 4.30 4.35 4.40 4.45 bits\nfull information\nN&VB&ADJ\nN&VB\nN\nnamed entities\nfunc. words\nno information\n4.16 (0%)\n+0.04 (13%)\n+0.05 (17%)\n+0.07 (23%)\n+0.12 (41%)\n+0.22 (76%)\n4.45 (100%)\n(a) Mid-range condition (first 256 tokens after ablation)\n4.16 4.18 4.20 4.22 bits\nN&VB&ADJ\nfull information\nN&VB\nN\nnamed entities\nno information\nfunc. words\n+-0.00 (-6%)\n4.16 (0%)\n+0.00 (2%)\n+0.01 (15%)\n+0.03 (48%)\n4.22 (100%)\n+0.07 (114%)\n(b) Long-range condition (tokens 256-512 after ablation)\n= = Career = =\n= = = Harry Potter = = =\nIn 2000, producer David Heyman asked Radcliffe to audition for the role of Harry Potter for the film adaptation of Harry Potter and the Philosopher’s Stone, the best-selling book by British author J.K. Rowling. Rowling had been searching for an unknown British actor to personify the character, and the movie’s director Chris Columbus recalled thinking, ”This is what I want. This is Harry Potter”, after he saw a video of the young actor in David Copperfield. Eight months later, and after several auditions, Radcliffe was selected to play the part. Rowling also endorsed the selection saying, ”I don’t think Chris Columbus could have found a better Harry.” Radcliffe’s parents originally turned down the offer, as they had been told that it would involve six films shot in Los Angeles. Warner Bros. instead offered Radcliffe a two-movie contract with shooting in the UK; Radcliffe was unsure at the time if he would do any more than that.\nThe release of Harry Potter and the Philosopher’s Stone (released as Harry Potter and the Sorcerer’s Stone in the United States) took place in 2001. Radcliffe received a seven figure salary for the lead role, but asserted that the fee was ”not that important” to him; his parents chose to invest the money for him. The film was highly popular and was met with positive reviews, and critics took notice of Radcliffe: ”Radcliffe is the embodiment of every reader’s imagination. It is wonderful to see a young hero who is so scholarly looking and filled with curiosity and who connects with very real emotions, from solemn intelligence and the delight of discovery to deep family longing, ”wrote Bob Graham of the San Francisco Chronicle.\nA year later Radcliffe starred in Harry Potter and the Chamber of Secrets, the second installment of the series. Reviewers were positive about the lead actors’ performances but had polarised opinions\non the movie as a whole. The 2004 release Harry Potter and the Prisoner of Azkaban was the third film in the series. Radcliffe ’s performance was panned by New York Times journalist A.O. Scott, who wrote that Watson had to carry him with her performance. Next was Harry Potter and the Goblet of Fire in 2005. The film was the second-highest gross\nfull information:\nsing Potter film in the series at the time of its release. After appearing in Harry Potter and the Philosopher’s Stone, Radcliffe tried to avoid trying to improve his role.\nshuffle all:\nsing Potter film in the series (behind Harry Potter and the Chamber of Secrets), the highest grossing Harry Potter film in the series. The following year, Harry Potter and the Goblet of Fire was the top-grossing Potter film, making it the highest grossing Harry Potter film in the series.\nshuf. within sent.:\nsing Potter film in the series (until 2009’s Gold Glove), selling 1.5 million tickets in its first week. The title of the film’s sequel was also a major factor in the success of Philosopher, while its underperformance dropped because of series 3’s Sci-Fi Channel.\nshuf. within trigrams:\nsing Potter film at the time of its release. The film had a financial success worldwide and received five Academy Award nominations at the 56th Academy Awards.\nshuf. trigrams within sent.:\nsing Potter film in the series at the United States box office. His appearance in Harry Potter and the Philosopher’s Stone did not make it to the United States until 2006.\nshuf. trigrams globally:\nsing Potter film at the time and was the third highest grossing film of 2006, the highest-grossing film in the series. In 2006, Radcliffe acted in his first major role in Harry Potter and the Deathly Hallows – Part I, where he played Harry’s Chief Engineer, Harry Potter’s primary antagonist.\nsent. shuf.:\nsing Potter film in the series at the time of its release. At the end of 2009, new backers were looking for a leading role in the film, with Radcliffe starting the role of Harry Potter and the Philosopher’s Stone.\ncont. words:\nsing Harry Potter film of all time after Harry Potter and the Goblet of Fire, the highest-grossing film of the 2007–2008 financial year. The film’s success meant that he and his wife bought a fourbedroom apartment in the 1960s, “ready to keep them warm”, adding “They could never make another effort to make something exciting for people who were usually very aware of the housing shortage but hadn’t been very conscious of that”.\nN & VB & ADJ:\nsing animated film in the series after The Lion King. A similar amount of praise came in 2008 when The Magic King was released.\nN & VB:\nsing animated film in the series. A significant amount of critical reviews were given by publications in The Washington Post, San Francisco Chronicle, and The Washington Post. The film was released in theaters in 2008.\nN:\nsing animated film in the series.\n= Appearances = =\nnamed entities:\nsing animated film in the series.\n= = Appearances extended Persons (1990 – 2014) = =\nrare:\nsing animated film in the series.\n= = Part two = =\ncommon:\nsing animated film in the series. A review in The New York Times found that Hans was not as strong as Watson but as well as Mr. Trough and Mr. Trough."
    } ],
    "references" : [ {
      "title" : "Modeling local coherence: An entity-based approach",
      "author" : [ "Regina Barzilay", "Mirella Lapata." ],
      "venue" : "Computational Linguistics, 34(1):1–34.",
      "citeRegEx" : "Barzilay and Lapata.,? 2008",
      "shortCiteRegEx" : "Barzilay and Lapata.",
      "year" : 2008
    }, {
      "title" : "Longformer: The long-document transformer",
      "author" : [ "Iz Beltagy", "Matthew E. Peters", "Arman Cohan." ],
      "venue" : "arXiv:2004.05150.",
      "citeRegEx" : "Beltagy et al\\.,? 2020",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2020
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Janvin." ],
      "venue" : "The Journal of Machine Learning Research, 3:1137–1155.",
      "citeRegEx" : "Bengio et al\\.,? 2003",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "An estimate of an upper bound for the entropy of english",
      "author" : [ "Peter F Brown", "Stephen A Della Pietra", "Vincent J Della Pietra", "Jennifer C Lai", "Robert L Mercer." ],
      "venue" : "Computational Linguistics, 18(1):31–40.",
      "citeRegEx" : "Brown et al\\.,? 1992",
      "shortCiteRegEx" : "Brown et al\\.",
      "year" : 1992
    }, {
      "title" : "The CMU-EBMT machine translation system",
      "author" : [ "Ralf D Brown." ],
      "venue" : "Machine translation, 25(2):179.",
      "citeRegEx" : "Brown.,? 2011",
      "shortCiteRegEx" : "Brown.",
      "year" : 2011
    }, {
      "title" : "Language GANs falling short",
      "author" : [ "Massimo Caccia", "Lucas Caccia", "William Fedus", "Hugo Larochelle", "Joelle Pineau", "Laurent Charlin." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Caccia et al\\.,? 2020",
      "shortCiteRegEx" : "Caccia et al\\.",
      "year" : 2020
    }, {
      "title" : "Curious case of language generation evaluation metrics: A cautionary tale",
      "author" : [ "Ozan Caglayan", "Pranava Madhyastha", "Lucia Specia." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 2322–2328, Barcelona,",
      "citeRegEx" : "Caglayan et al\\.,? 2020",
      "shortCiteRegEx" : "Caglayan et al\\.",
      "year" : 2020
    }, {
      "title" : "Generating long sequences with sparse transformers",
      "author" : [ "Rewon Child", "Scott Gray", "Alec Radford", "Ilya Sutskever." ],
      "venue" : "URL https://openai.com/blog/sparse-transformers.",
      "citeRegEx" : "Child et al\\.,? 2019",
      "shortCiteRegEx" : "Child et al\\.",
      "year" : 2019
    }, {
      "title" : "What does BERT look at? an analysis of BERT’s attention",
      "author" : [ "Kevin Clark", "Urvashi Khandelwal", "Omer Levy", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for",
      "citeRegEx" : "Clark et al\\.,? 2019",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "Transformer-XL: Attentive language models beyond a fixed-length context",
      "author" : [ "Zihang Dai", "Zhilin Yang", "Yiming Yang", "Jaime Carbonell", "Quoc Le", "Ruslan Salakhutdinov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Dai et al\\.,? 2019",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2019
    }, {
      "title" : "Finding structure in time",
      "author" : [ "Jeffrey L Elman." ],
      "venue" : "Cognitive science, 14(2):179–211.",
      "citeRegEx" : "Elman.,? 1990",
      "shortCiteRegEx" : "Elman.",
      "year" : 1990
    }, {
      "title" : "A bit of progress in language modeling",
      "author" : [ "Joshua T Goodman." ],
      "venue" : "Computer Speech & Language, 15(4):403–434.",
      "citeRegEx" : "Goodman.,? 2001",
      "shortCiteRegEx" : "Goodman.",
      "year" : 2001
    }, {
      "title" : "Theoretical limitations of selfattention in neural sequence models",
      "author" : [ "Michael Hahn." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:156–171.",
      "citeRegEx" : "Hahn.,? 2020",
      "shortCiteRegEx" : "Hahn.",
      "year" : 2020
    }, {
      "title" : "Unifying human and statistical evaluation for natural language generation",
      "author" : [ "Tatsunori Hashimoto", "Hugh Zhang", "Percy Liang." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Hashimoto et al\\.,? 2019",
      "shortCiteRegEx" : "Hashimoto et al\\.",
      "year" : 2019
    }, {
      "title" : "Tracking the world state with recurrent entity networks",
      "author" : [ "Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Henaff et al\\.,? 2016",
      "shortCiteRegEx" : "Henaff et al\\.",
      "year" : 2016
    }, {
      "title" : "spaCy: Industrial-strength Natural Language Processing in Python",
      "author" : [ "Matthew Honnibal", "Ines Montani", "Sofie Van Landeghem", "Adriane Boyd" ],
      "venue" : null,
      "citeRegEx" : "Honnibal et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Honnibal et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is not explanation",
      "author" : [ "Sarthak Jain", "Byron C. Wallace." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Jain and Wallace.,? 2019",
      "shortCiteRegEx" : "Jain and Wallace.",
      "year" : 2019
    }, {
      "title" : "Sharp nearby, fuzzy far away: How neural language models use context",
      "author" : [ "Urvashi Khandelwal", "He He", "Peng Qi", "Dan Jurafsky." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Khandelwal et al\\.,? 2018",
      "shortCiteRegEx" : "Khandelwal et al\\.",
      "year" : 2018
    }, {
      "title" : "Reformer: The efficient transformer",
      "author" : [ "Nikita Kitaev", "Lukasz Kaiser", "Anselm Levskaya." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Kitaev et al\\.,? 2020",
      "shortCiteRegEx" : "Kitaev et al\\.",
      "year" : 2020
    }, {
      "title" : "Improved backing-off for m-gram language modeling",
      "author" : [ "Reinhard Kneser", "Hermann Ney." ],
      "venue" : "1995 International Conference on Acoustics, Speech, and Signal Processing, volume 1, pages 181–184. IEEE.",
      "citeRegEx" : "Kneser and Ney.,? 1995",
      "shortCiteRegEx" : "Kneser and Ney.",
      "year" : 1995
    }, {
      "title" : "Understanding neural networks through representation erasure",
      "author" : [ "Jiwei Li", "Will Monroe", "Dan Jurafsky." ],
      "venue" : "arXiv preprint arXiv:1612.08220.",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Pointer sentinel mixture models",
      "author" : [ "Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher." ],
      "venue" : "CoRR, abs/1609.07843.",
      "citeRegEx" : "Merity et al\\.,? 2016",
      "shortCiteRegEx" : "Merity et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Tomáš Mikolov", "Martin Karafiát", "Lukáš Burget", "Jan Černockỳ", "Sanjeev Khudanpur." ],
      "venue" : "Eleventh annual conference of the International Speech Communication Association.",
      "citeRegEx" : "Mikolov et al\\.,? 2010",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Composition is the core driver of the languageselective network",
      "author" : [ "F. Mollica", "Matthew Siegelman", "Evgeniia Diachek", "S. Piantadosi", "Zachary Mineroff", "Richard Futrell", "Hope H. Kean", "Peng Qian", "E. Fedorenko." ],
      "venue" : "Neurobiology of Language,",
      "citeRegEx" : "Mollica et al\\.,? 2020",
      "shortCiteRegEx" : "Mollica et al\\.",
      "year" : 2020
    }, {
      "title" : "Rissanen data analysis: Examining dataset characteristics via description length",
      "author" : [ "Ethan Perez", "Douwe Kiela", "Kyunghyun Cho." ],
      "venue" : "arXiv preprint arXiv:2103.03872.",
      "citeRegEx" : "Perez et al\\.,? 2021",
      "shortCiteRegEx" : "Perez et al\\.",
      "year" : 2021
    }, {
      "title" : "Out of order: How important is the sequential order of words in a sentence in natural language understanding tasks? arXiv preprint arXiv:2012.15180",
      "author" : [ "Thang M Pham", "Trung Bui", "Long Mai", "Anh Nguyen" ],
      "venue" : null,
      "citeRegEx" : "Pham et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Pham et al\\.",
      "year" : 2020
    }, {
      "title" : "Information-theoretic probing for linguistic structure",
      "author" : [ "Tiago Pimentel", "Josef Valvoda", "Rowan Hall Maudslay", "Ran Zmigrod", "Adina Williams", "Ryan Cotterell." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Pimentel et al\\.,? 2020",
      "shortCiteRegEx" : "Pimentel et al\\.",
      "year" : 2020
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeff Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Compressive transformers for long-range sequence modelling",
      "author" : [ "Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Timothy P. Lillicrap" ],
      "venue" : null,
      "citeRegEx" : "Rae et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Rae et al\\.",
      "year" : 2019
    }, {
      "title" : "Do neural dialog systems use the conversation history effectively? an empirical study",
      "author" : [ "Chinnadhurai Sankar", "Sandeep Subramanian", "Christopher Pal", "Sarath Chandar", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1906.01603.",
      "citeRegEx" : "Sankar et al\\.,? 2019",
      "shortCiteRegEx" : "Sankar et al\\.",
      "year" : 2019
    }, {
      "title" : "A mathematical theory of communication",
      "author" : [ "Claude E Shannon." ],
      "venue" : "The Bell system technical journal, 27(3):379–423.",
      "citeRegEx" : "Shannon.,? 1948",
      "shortCiteRegEx" : "Shannon.",
      "year" : 1948
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned",
      "author" : [ "Elena Voita", "David Talbot", "Fedor Moiseev", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Voita et al\\.,? 2019",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    }, {
      "title" : "Informationtheoretic probing with minimum description length",
      "author" : [ "Elena Voita", "Ivan Titov." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 183–196, Online. Association for Computa-",
      "citeRegEx" : "Voita and Titov.,? 2020",
      "shortCiteRegEx" : "Voita and Titov.",
      "year" : 2020
    }, {
      "title" : "SuperGLUE: A stickier benchmark for general-purpose language understanding systems",
      "author" : [ "Alex Wang", "Yada Pruksachatkun", "Nikita Nangia", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R Bowman." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Linformer: Selfattention with linear complexity",
      "author" : [ "Sinong Wang", "Belinda Li", "Madian Khabsa", "Han Fang", "Hao Ma." ],
      "venue" : "arXiv preprint arXiv:2006.04768.",
      "citeRegEx" : "Wang et al\\.,? 2020",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "A theory of usable information under computational constraints",
      "author" : [ "Yilun Xu", "Shengjia Zhao", "Jiaming Song", "Russell Stewart", "Stefano Ermon." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Understanding deep learning requires rethinking generalization",
      "author" : [ "Chiyuan Zhang", "Samy Bengio", "Moritz Hardt", "Benjamin Recht", "Oriol Vinyals." ],
      "venue" : "arXiv preprint arXiv:1611.03530.",
      "citeRegEx" : "Zhang et al\\.,? 2016",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 31,
      "context" : "Recent years have seen a significant improvement in the predictive accuracy of neural language models (LMs), owing to a combination of improvements in model architecture (especially transformers; Vaswani et al. 2017) and training infrastructure (Wolf et al.",
      "startOffset" : 170,
      "endOffset" : 216
    }, {
      "referenceID" : 22,
      "context" : "The most striking change, relative to both recurrent neural LMs (Mikolov et al., 2010) and count-based models (Kneser and Ney, 1995), is the length of the context that these models can effectively condition on.",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 19,
      "context" : ", 2010) and count-based models (Kneser and Ney, 1995), is the length of the context that these models can effectively condition on.",
      "startOffset" : 31,
      "endOffset" : 53
    }, {
      "referenceID" : 17,
      "context" : ", Brown, 2011), and recurrent LMs have an effective context size of 200 (Khandelwal et al., 2018), the predictive accuracy of transformer LMs appears to improve when conditioning on as many as a thousand previous tokens (Beltagy et al.",
      "startOffset" : 72,
      "endOffset" : 97
    }, {
      "referenceID" : 1,
      "context" : ", 2018), the predictive accuracy of transformer LMs appears to improve when conditioning on as many as a thousand previous tokens (Beltagy et al., 2020).",
      "startOffset" : 130,
      "endOffset" : 152
    }, {
      "referenceID" : 28,
      "context" : "focused on making use of even longer contexts computationally feasible (Rae et al., 2019; Wang et al., 2020; Child et al., 2019; Dai et al., 2019; Kitaev et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 167
    }, {
      "referenceID" : 35,
      "context" : "focused on making use of even longer contexts computationally feasible (Rae et al., 2019; Wang et al., 2020; Child et al., 2019; Dai et al., 2019; Kitaev et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 167
    }, {
      "referenceID" : 7,
      "context" : "focused on making use of even longer contexts computationally feasible (Rae et al., 2019; Wang et al., 2020; Child et al., 2019; Dai et al., 2019; Kitaev et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 167
    }, {
      "referenceID" : 9,
      "context" : "focused on making use of even longer contexts computationally feasible (Rae et al., 2019; Wang et al., 2020; Child et al., 2019; Dai et al., 2019; Kitaev et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 167
    }, {
      "referenceID" : 18,
      "context" : "focused on making use of even longer contexts computationally feasible (Rae et al., 2019; Wang et al., 2020; Child et al., 2019; Dai et al., 2019; Kitaev et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 167
    }, {
      "referenceID" : 12,
      "context" : "While input sequences x can in principle be made arbitrarily long, there are both theoretical and practical limits to transformers’ ability to make effective use of it (Hahn, 2020; Wang et al., 2019).",
      "startOffset" : 168,
      "endOffset" : 199
    }, {
      "referenceID" : 34,
      "context" : "While input sequences x can in principle be made arbitrarily long, there are both theoretical and practical limits to transformers’ ability to make effective use of it (Hahn, 2020; Wang et al., 2019).",
      "startOffset" : 168,
      "endOffset" : 199
    }, {
      "referenceID" : 30,
      "context" : "While this definition generalizes Shannon mutual information (Shannon, 1948) and has deep connections to other information-theoretic quantities (see Xu et al.",
      "startOffset" : 61,
      "endOffset" : 76
    }, {
      "referenceID" : 38,
      "context" : "In practice, LMs must be trained on finite corpora, creating a risk of overfitting (Zhang et al., 2016).",
      "startOffset" : 83,
      "endOffset" : 103
    }, {
      "referenceID" : 27,
      "context" : "Model, data and training details For all experiments, our LM uses the GPT-2 model architecture (Radford et al., 2019) in the implementation of Wolf et al.",
      "startOffset" : 95,
      "endOffset" : 117
    }, {
      "referenceID" : 21,
      "context" : "All models are trained from scratch on the WikiText-103 dataset (Merity et al., 2016), an English language modeling benchmark.",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : "A note on evaluation As in past work on evaluating language models (Brown et al., 1992), our evaluation of relative predictive information ultimately bottoms out in a conditional entropy (logperplexity).",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 13,
      "context" : "Recent work has shown that other metrics, such as diversity of outputs, are important for evaluating the quality of LMs as models for language generation (Hashimoto et al., 2019; Caccia et al., 2020).",
      "startOffset" : 154,
      "endOffset" : 199
    }, {
      "referenceID" : 5,
      "context" : "Recent work has shown that other metrics, such as diversity of outputs, are important for evaluating the quality of LMs as models for language generation (Hashimoto et al., 2019; Caccia et al., 2020).",
      "startOffset" : 154,
      "endOffset" : 199
    }, {
      "referenceID" : 6,
      "context" : "Generation also depends on a number of other factors, such as choice of decoding procedure (Caglayan et al., 2020).",
      "startOffset" : 91,
      "endOffset" : 114
    }, {
      "referenceID" : 23,
      "context" : "(2020) in models for entailment, and more complex shuffling procedures have been explored in neuroscience contexts (Mollica et al., 2020).",
      "startOffset" : 115,
      "endOffset" : 137
    }, {
      "referenceID" : 15,
      "context" : "We use the spaCy model (Honnibal et al., 2020) for part-of-speech tagging, and examine five sets: (1) nouns only, (2) nouns and verbs, (3) nouns, verbs, and adjectives, (4) content words (nouns, verbs, adjectives, and adverbs), and (5) function words (all words except nouns, verbs, adjectives, and adverbs).",
      "startOffset" : 23,
      "endOffset" : 46
    }, {
      "referenceID" : 37,
      "context" : "Like Shannon information, Vinformation is defined to be non-negative (Xu et al., 2020), and the result in Fig.",
      "startOffset" : 69,
      "endOffset" : 86
    }, {
      "referenceID" : 11,
      "context" : "Subsequent models improved on these by allowing the use of skip-grams, caches, and featurebased models (Goodman, 2001; Bengio et al., 2003).",
      "startOffset" : 103,
      "endOffset" : 139
    }, {
      "referenceID" : 2,
      "context" : "Subsequent models improved on these by allowing the use of skip-grams, caches, and featurebased models (Goodman, 2001; Bengio et al., 2003).",
      "startOffset" : 103,
      "endOffset" : 139
    }, {
      "referenceID" : 22,
      "context" : "Context in RNN LMs Recurrent neural network language models (Mikolov et al., 2010; Elman, 1990) provide a more expressive mechanism for the use of long-range context: models write to a recurrent “state vector” which can be carried arbitrarily far into the future.",
      "startOffset" : 60,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : "Context in RNN LMs Recurrent neural network language models (Mikolov et al., 2010; Elman, 1990) provide a more expressive mechanism for the use of long-range context: models write to a recurrent “state vector” which can be carried arbitrarily far into the future.",
      "startOffset" : 60,
      "endOffset" : 95
    }, {
      "referenceID" : 8,
      "context" : "Some recent work that investigates the behavior of individual transformer attention heads (Clark et al., 2019; Voita et al., 2019).",
      "startOffset" : 90,
      "endOffset" : 130
    }, {
      "referenceID" : 32,
      "context" : "Some recent work that investigates the behavior of individual transformer attention heads (Clark et al., 2019; Voita et al., 2019).",
      "startOffset" : 90,
      "endOffset" : 130
    }, {
      "referenceID" : 16,
      "context" : "While extremely informative about the computational structures implemented by fixed models, these approaches do not necessarily reveal anything about usable information: indeed, patterns of attention do not necessarily correlate with model predictions (Jain and Wallace, 2019).",
      "startOffset" : 252,
      "endOffset" : 276
    }, {
      "referenceID" : 14,
      "context" : "(2019)’s finding that long-range contexts could be informatively summarized in fixed-sized vectors; our finding that most usable information is carried by nouns is consistent with earlier findings about both specialized neural architectures (Henaff et al., 2016) and discourse representations in feature-based models (Barzilay and Lapata, 2008).",
      "startOffset" : 241,
      "endOffset" : 262
    }, {
      "referenceID" : 0,
      "context" : ", 2016) and discourse representations in feature-based models (Barzilay and Lapata, 2008).",
      "startOffset" : 62,
      "endOffset" : 89
    }, {
      "referenceID" : 33,
      "context" : "Our approach also shares similar motivations to information-theoretic work on probing (Voita and Titov, 2020; Pimentel et al., 2020), which uses related tools to interpret linguistic structure in LM representations rather than characterizing their effect on LM predictions.",
      "startOffset" : 86,
      "endOffset" : 132
    }, {
      "referenceID" : 26,
      "context" : "Our approach also shares similar motivations to information-theoretic work on probing (Voita and Titov, 2020; Pimentel et al., 2020), which uses related tools to interpret linguistic structure in LM representations rather than characterizing their effect on LM predictions.",
      "startOffset" : 86,
      "endOffset" : 132
    } ],
    "year" : 2021,
    "abstractText" : "Transformer-based language models benefit from conditioning on contexts of hundreds to thousands of previous tokens. What aspects of these contexts contribute to accurate model prediction? We describe a series of experiments that measure usable information by selectively ablating lexical and structural information in transformer language models trained on English Wikipedia. In both midand longrange contexts, we find that several extremely destructive context manipulations—including shuffling word order within sentences and deleting all words other than nouns—remove less than 15% of the usable information. Our results suggest that long contexts, but not their detailed syntactic and propositional content, are important for the low perplexity of current transformer language models.1",
    "creator" : "LaTeX with hyperref"
  }
}