{
  "name" : "2021.acl-long.157.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Control Image Captioning Spatially and Temporally",
    "authors" : [ "Kun Yan", "Lei Ji", "Huaishao Luo", "Ming Zhou", "Nan Duan", "Shuai Ma" ],
    "emails" : [ "kunyan@buaa.edu.cn", "mashuai@buaa.edu.cn", "leiji@microsoft.com", "mingzhou@microsoft.com", "nanduan@microsoft.com", "huaishaoluo@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2014–2025\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2014"
    }, {
      "heading" : "1 Introduction",
      "text" : "Image captioning is a fundamental task to examine whether an intelligent system can understand the visual world by letting the system describe it with natural language. Generating a reasonable caption requires the model to link linguistic tokens to objects, relationships, scenes of the visual world in the input image. Thus, a great captioning model will help us better understand what characteristics promise a good joint visual-linguistic representation.\n∗Contribution during internship at MSRA.\nMost previous attempts aim to describe the image indicating the salient objects and relations without considering user intention. To generate controllable and explainable captions, recent works dedicated to establishing a new controllable image captioning task to generate the caption at will. The captioning process can be controlled by POS tagging (Deshpande et al., 2018), sentiment (You et al., 2018), length (Deng et al., 2020), bounding boxes (Cornia et al., 2019), and mouse traces (Pont-Tuset et al., 2020).\nIn this paper, we mainly investigate tracecontrolled image captioning, since it is not only a more natural and interactive paradigm for real web applications, e.g. automatic presentation or help people with visual difficulties but also a new perspective for us to better understand how the longpursued cross-modality alignment is performed in deep learning models. Figure 1 presents a showcase of the scenario. Given an image, users can easily draw a trace to ask the AI agent to describe the scene in the image along the trace automatically.\nIn the Localized Narratives dataset (Pont-Tuset et al., 2020), the annotators describe the image\nwhile drawing the traces of their attention movement, which presents a spatial alignment between visual objects and caption tokens as well as a temporal alignment between user intention(by trace) and caption sentences. From Figure 1, we see that the caption tokens, e.g. “person”, “horse”, “trees” can be grounded to the visual objects spatially, and the order of caption sentences can be arranged to align to the order of traces temporally. Although it is easy for humans to recognize which visual object is indicated by the traces, it is a challenge for the agent to recognize, emphasize and arrange visual semantics solely based on several tracepoints’ coordinates. Thereby, we mainly devote our effort to the spatial grounding and temporal controllability of image captioning.\nInspired by the above observation, we design two novel approaches to tackle the above challenges. Specifically, we design sentence-level contrastive constraints to align the generated sentences to the corresponding trace sequences temporally. Besides, we design a type of heuristic spatial attention guidance to supervise each generated text tokens to attend to the correct visual objects. Composing the above together, We propose a novel trace-controlled image captioning model called LoopCAG and demonstrate its superior capability on captioning quality and flexible controllability.\nOur contribution can be summarized as: 1) We propose a novel model LoopCAG, which learns the caption tokens’ spatial grounding through attention guidance and temporal localization between trace input and the caption sentences through contrastive constraints in an end-to-end loop manner among the three modalities(vision, language, and traces).\n2) The quantitative results show that our LoopCAG model can generate better tracecontrolled captions and achieve SOTA performance on automatic criteria. The qualitative results present that our model can generate highly relevant captions given users’ trace inputs.\n3) We intensively study the controllability and explainability of trace-controlled image captioning."
    }, {
      "heading" : "2 Preliminary",
      "text" : ""
    }, {
      "heading" : "2.1 Task Definition",
      "text" : "For image captioning, the task is to generate a text description y given an image I . We first apply a pre-trained visual object detector on the image and get an object level visual feature set\nV = {v1, . . . , vN}, in which vi ∈ R2048 is the i-th object visual feature, and N is the number of visual objects. The text description sequence is y = {y1, . . . , yl}, in which yj is the j-th token and l is the text sequence length. The output is conditioned on model parameters θ, and the optimization process can be formulated as the following maximum likelihood form:\nθ∗ = argmax θ log p(y | V ;θ). (1)\nFor trace-controlled image captioning, the raw trace input is a sequence of tracepoints coordinates with timestamps. To reduce those tracepoints to an acceptable length due to the limit of GPU memory, we segment the tracepoints sequences uniformly by the same time window τ , and then each trace segment is converted to its minimal bounding rectangle. Every bounding rectangle can be represented by a 5D vector which contains normalized coordinates of the top-left and bottom-right corners, and the area ratio with respect to the whole image. We denote the trace input as T = {t1, . . . , tM}, where ti ∈ R5. The trace controlled captioning objective can be formulated as follow:\nθ∗ = argmax θ log p(y | V ,T ;θ) (2)"
    }, {
      "heading" : "3 Method",
      "text" : "Our method consists of three components: the caption generation module with a transformer encoder-decoder backbone, the attention guidance for object-level spatial grounding, and the contrastive constraints for sentence-level temporal alignment. The overall model structure is illustrated in Figure 2. The model is trained by jointly optimizing the three objectives listed in the following subsections."
    }, {
      "heading" : "3.1 Caption Generation",
      "text" : "The caption generation backbone is a transformerbased encoder-decoder proposed by Vaswani et al. (2017), which mainly employs a multi-head attention mechanism and achieves top-tier performance in many sequential related tasks. Here, we highlight several task-oriented modifications.\nVision-Trace Encoder The visual embeddings V and traces embeddings T are encoded separately and then concatenated together as a single input sequence feeding into a transformer encoder.\n• Object visual embedding: We first represent the spatial info of each object proposal by a 5D vector (in the same way as the traces), then project it into a spatial embedding pi ∈ Rd, where d is the embedding size across the model. Each object visual feature vi is projected into a lower dimension vector v̂i ∈ Rd. The final visual embedding is Ṽ = {ṽ1, . . . , ṽN}, where ṽi = v̂i + pi.\n• Trace Embedding: Each trace input item ti is projected into t̂i ∈ Rd. We also generate Sinusoidal Positional Embeddings (Vaswani et al., 2017) oi to capture the temporal order of the traces. The final trace embedding T̃ = {t̃1, . . . , t̃M}, where t̃i = t̂i + oi.\nCaption Decoder Caption decoder combines vision and trace information using cross attention connected to the hidden states of Vision-Trace Encoder’s last layer. Using a casual mask to encode generated token progressively, the transformer decoder ensures that the predictions for position i can depend only on the known outputs at positions less than i. During training, the ground truth caption tokens are shifted right, and a special token 〈BOS〉 (begin of the sentence) is inserted into the first position. A cross-entropy generation loss Lgen is then computed with the logits transformed from the last decoder layer’s hidden states and un-shifted ground\ntruth caption token ids with a special token 〈EOS〉 (end of the sentence) appended.\nLgen = − E ŷi∼ŷ\nlog p ( ŷi | ŷ<i,T̃ ,Ṽ ;θ ) . (3)\nIt is noted that ŷ is the masked version of the ground-truth caption y. To make a fair comparison with the baseline (Pont-Tuset et al., 2020), we apply the same setting and do not employ common techniques such as label smoothing(Szegedy et al., 2016) or self-critical training(Rennie et al., 2017)."
    }, {
      "heading" : "3.2 Attention Guidance for Spatial Ground",
      "text" : "Attention Supervision Construction To explicitly guide the attention for object-level spatial grounding, we align the semantic caption tokens with the visual object by taking trace as an intermediate bridge. In this way, we construct a supervision matrix to guide the attention between the caption tokens and visual objects by the two following steps.\n1) Language-trace temporal alignment. In the Localized Narrative dataset, the caption utterances1 u and mouse traces are highly temporal-aligned, i.e., every utterance u has a\n1We are following the naming tradition of Pont-Tuset et al. (2020), where an utterance means one or several adjacent tokens, not a whole sentence.\nVision Box: person Trace Utterance: person Number of Tracepoints in Box: 12 Total Number of Points: 12 Supervision Attention Score=12/12=1.0 Vision Box: person Trace Utterance: horse Number of Tracepoints in Box: 14 Total Number of Points: 67 Supervision Attention Score=14/67=0.209\nThe person is riding a horse.\nBy calculating the alignment score, we establish the spatial grounding supervision between caption tokens and auto-detected visual objects. For every word yi in the same utterance u, the s(yi,bj) = s(u,bj). Eventually, we get the supervision score matrix S ∈ [0, 1]N×T and Sij = s(yi,bj).\nAttention-guided Grounding A cross-attention matrix is generated in shape (N,T, L,H) during the transformer’s decoding steps. Here N denotes the number of pre-detected visual objects, T denotes the number of tokens in a caption sentence after padding, L denotes the number of transformer layers, and H denotes the number of attention heads in transformer layers. Two linear projections and layer normalization (Ba et al., 2016) are applied sequentially on dimension L and H , respectively reducing the dimension to 1. Thus, for a single instance, we eventually calculate an attention matrix A ∈ RN×T .\nTo train the model, the goal can be achieved by minimizing the following attention guidance loss function Latt:\nLatt = − E a∼A,s∼S\ns · [s log a+ (1− s) log (1− a)] , (6)\nwhich is a weighted Binary Cross Entropy between A and S. Noted that we also choose to mask out some stop-words columns of the matrix A and S to avoid introducing too much annotation noise."
    }, {
      "heading" : "3.3 Contrastive Constraints for Temporal Alignment",
      "text" : "As illustrated on the left side of Figure 4, we first use a “split by sentence” procedure to build a sentence-level alignment between caption and traces, and then employ contrastive loss to constrain the temporal order of the generation process.\nSplit by Sentence An annotated instance consists of an image, a tracepoint list, and a caption paragraph consisting of a list of ordered caption sentences. Here, we define a caption sentence as a\nseries of utterances segmented out by a period(’.’). In section 3.2, we already maintain an alignment between utterances and tracepoints. Following this setting, we can unite a list of ordered utterance U = {u1, . . . , uk} in the same caption sentence, and then orderly unite a list of tracepoints corresponding to U ’s elements into a so-called trace segment. The alignment between caption sentences and trace segments can be established by simply uniting the association between utterances and tracepoints with respect to the above sentence split. We call this procedure as split by sentence.\nTemporal Contrastive Constraints According to the split mentioned above, we aggregate the transformer’s last layer hidden states of trace segments and caption sentences respectively, and denote them as Hts = {h1ts, . . . , hnts} and Hcs = {h1cs, . . . , hncs}. Here n is the number of caption sentences.\nWe adopt the NCE loss to learn to discriminate the positive from negative trace-caption pairs. The positive is defined as all the temporal aligned corresponding caption sentence and trace segment pairs i.e. with the same order indices, and other pairs without temporal alignment in the same image as negative samples. This contrastive loss function Lcts is defined as follows,\nLcts = − E hts∼Hts\nlog exp(s(hits, h i cs))\nZ , (7)\nZ = n∑\nj=1\nexp(s(hits, h j cs)) (8)\nwhere s(·, ·) means two linear layers and an L2 normalization applied on the elements respectively, and a dot production between them. By minimizing the Lcts, we force the model to learn a representation being aware of sentence-level temporal ordering, which leads to more precise captioning."
    }, {
      "heading" : "3.4 Loss",
      "text" : "Finally, the model is trained with three losses Lgen, Latt, and Lcts, where Lgen is the caption generation loss, Latt is the spatial attention guidance loss, and Lcts is the temporal contrastive loss.We jointly optimize our model by minimizing all losses added together:\nLall =Lgen + Latt + Lcts. (9)"
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset",
      "text" : "We use the annotated COCO subset of Localized Narratives to evaluate our method. We call this dataset split as LN-COCO for short. Each image has one or several pairs of the captioning paragraph and corresponding mouse traces. Every single pair is a so-called localized narrative. The training and validation splits are identical to Pont-Tuset et al. (2020)’s setting. There are 134,272 localized narratives in the training set and 8,573 in the validation set. We train on the whole training set and evaluate our model performance against the identical validation set."
    }, {
      "heading" : "4.2 Implementation Details",
      "text" : "For the visual feature, we adopt Faster-RCNN(Ren et al., 2015) to extract 100 bounding box proposals. For trace feature, we use τ = 0.4s to extract trace segment for feature extraction. The embedding size d, number of transformer layers, hidden size of the transformer feed-forward layer are 768, 2, and 768, respectively. The number of attention heads is 8, and the dropout rate is 0.1. We adopt the AdamW optimizer (Loshchilov and Hutter, 2019) with learning rate of 7e-4(which is the best performance setting of baseline, and adopted widely for other trials), and set two momentum parameters β1= 0.9 and β2= 0.99. We set the batch size to 256. All models are trained on 4 Tesla V100 GPUs with 32GB memory for 10 to 12 hours."
    }, {
      "heading" : "4.3 Evaluation Metrics",
      "text" : "This generation task adopts the traditional image captioning evaluation metric using the open-source tool2 with a minor modification3 to suit with LNCOCO, including BLEU(Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGEL (Lin and Och, 2004), ROUGE-1-F1(Pont-Tuset et al., 2020), and CIDEr-D (Vedantam et al., 2015)."
    }, {
      "heading" : "4.4 Results",
      "text" : "Baseline and +Trace methods The Baseline and +Trace methods are our re-implementations following (Pont-Tuset et al., 2020)’s method description. The Baseline method only takes image feature as input while the +Trace model take image feature\n2https://github.com/tylin/coco-caption 3We add an additional id to every trace-image-caption triplet and adjust some code of the standard evaluation tool to meet the ”1 trace-vs-1 caption” evaluation need.\nand trace both as input. They employ the architecture in Changpinyo et al. (2019) with a few minor differences. First, they set the number of Transformers’ layers for both the encoder and the decoder to 2 instead of 6. Second, their projection layers also consist of layer normalization(Ba et al., 2016). Third, they set the maximum number of iterations to 150k. Finally, they allow the maximum number of target captions to be as long as 225 to account for the narration’s longer nature.\nLoopCAG methods Our model comprises of four components: 1) the transformer encoderdecoder framework; 2) the trace input; 3) Attention Guidance(+AG for short) grounding loss; 4) Contrastive constraints(+C for short).\nMain Results The Table 1 shows the overall performance comparison on the LN-COCO dataset. To reduce the deviation caused by different implementation details, we first present our implementations’ performance (with *), which have a higher score than Pont-Tuset et al. (2020) reported. Thus, we have a more strict baseline to evaluate the improvement purely coming from our innovative method. Compared to Baseline* method, the performance on all metrics improves significantly when controlling captioning using the mouse trace (+Trace*), it indicates that using the mouse trace enables the system to describe better those user intended parts of the image.\nMost importantly, the results indicate that our LoopCAG method achieves state of the art on all automatic criteria, outperforming the previous state-of-art model by 2.4 and 7.5 on BLEU-4 and CIDEr-D, respectively. This demonstrates our proposed Attention Guidance method helps the model generate better spatially grounded and more precise captions. When considering the 2.0 rising on ROUGE-L score, we can conclude that Contrastive constraints can help the model better align the order of generated sentence to the user intent because ROUGE-L mainly employs an order mattered longest common sequence F-measure.\nAblations We perform three ablations to verify the most improvements in-deed come from the Attention Guidance and Contrastive constraints. Starting from standard captioning (Baseline*), we add the Attention Guidance to help the model better spatially ground visual objects and caption tokens (Table 2, “+ Ag”). This affects performance, suggesting that the model does benefit from knowing\nwhere to find the highly semantic related appearance feature in the image. Next, we add the trace feature (Table 2, “+ Trace”). This introduces user intention to the model. We also take this line to show the performance lift caused by Contrastive constraints fairly. Then we add the contrastive module (Table 2, “+C”) and see a good improvement on almost all criteria. Hence, we verify the significance of the positive influence of temporal contrastive constraints. Moreover, in the last line is our full LoopCAG model. We can see the two proposed methods are not exclusive to each other."
    }, {
      "heading" : "4.5 Quantitative Analysis",
      "text" : "Controllability Analysis on Temporal Order We also design an experiment to further demonstrate LoopCAG’s superior controllability on the caption sentences’ temporal order. Specifically, we split each localized narrative input by sentence as described in Sec3.3, and reverse the sequential order of the splits, i.e., the last sentence of a caption paragraph will become the first one, the same processing is applied to trace segments, too. We conduct an evaluation on the sentence&segment reverted dataset, and the performance comparison is shown in Table 3. With the Contrastive constraints mechanism’s help, the LoopCAG model is much more robust to trace input reversing, even competitive with the model trained on reverted data. In contrast, the base models all face a dramatic drop on almost all metrics when the input trace order is reversed. This also implies there are some biased habits of human annotators. For example, they always describe the salient objects first and end with a sentence about the background of the image.\nControllability Analysis on Temporal Frequency Then, we analyze the controllability of the temporal frequency τ to present whether the coarse-grained or fine-grained tracepoints (sampling rate, in other words) affects the generation performance. As the Table 4 shows, we change the temporal frequency τ from 0.4 to 1.2. A performance drop is impressive with the τ getting larger. The purpose of this experiment for various τ is to simulate the trace drawing speed of users in a real application scenario, and a larger τ is equivalent to a faster drawing speed. As Deng et al. (2020) has demonstrated, the length is one of the critical facts that impact quantitative performance. This result implies we can further decide to generate either a coarse-grained or fine-grained caption by\ncontrolling the time-frequency τ .\nControllability Analysis on Spatial Semantic Grounding One of our important purposes of using attention guidance is introducing more interpretability to the model while improving the caption performance. When generating each token, the model is forced to show which visual elements are the most effective reason for the current generation. And this effectiveness is supervised by our pseudo\nattention label. In this way, we can hopefully obtain better visual-linguistic joint representation. In appendix A, we showcase the attention values comparison of models w/wo attention guidance. We find that the AG model has a more diverse distribution across all different types of tokens. A ”neater” activation is observed in Appendix A (a) compared with (c), e.g., activations of ”who”, ”is” and ”on” are clearly suppressed. We observe that these suppressions happen on most function word, so we add this illustration for further discussion and exploration by our research community."
    }, {
      "heading" : "4.6 Qualitative Case Study",
      "text" : "We present a showcase of a captioning result of different methods in Figure 7. We can easily find that the Baseline captioning describes the image in random order while the +Trace Captioning and\nLoopCAG Captioning almost have the same order as Ground Truth Captioning. It is also aweinspiring that the Baseline captioning and +Trace Captioning both consist of some preposterous description highlighted in red color. In contrast, the LoopCAG captioning is all reasonable. This is evidence of superior fact grounding advantage brought by our Attention Guidance Method.\nGround Truth Captioning\nIn this picture there is a stand on a ground. On the backside there is a person. He is riding on a horse. He is wearing a cap. He is in between the fence. There is a flags on a wall. On the\nent indicates time."
    }, {
      "heading" : "5 Related Work",
      "text" : "Controllable Image Captioning is an emerging research direction. Previous works aim to control the captioning by Part-Of-Speech tagging(Deshpande et al., 2018), sentiment, (You et al., 2018), length (Deng et al., 2020), bounding box (Cornia et al., 2019) etc. Those works either tried to describe a semantic guided captioning. Other works relied on predefined categories, e.g., bounding box or sentiment classes. Similar works (Yu et al., 2018; Cornia et al., 2019) control the caption by a sequence of ordered topics and bounding boxes. However, those methods limit the captioning on the pre-defined or recognized objects in the bounding box and hard to scale out. Besides, the trace is a more natural way to input than the bounding box. The most similar work (Pont-Tuset et al., 2020) proposed a trace-controlled image captioning task and designed a simple benchmark by directly concatenating the mouse trace coordinates and size into a self-attention module. Although mouse trace is flexible and interactive, it is easy for humans to understand the trace’s semantic representation but hard for AI agents. Unlike previous works, we propose a novel trace-controlled model for capturing the semantic representation of trace from both fine-grained and coarse-grained spatial and temporal characteristics. Contrastive Learning Recently, contrastive learning has been widely studied in unsupervised representation learning for vision, (He et al., 2020; Chen et al., 2020; Grill et al., 2020; Caron et al., 2020; Chen and He, 2020), language (Mikolov et al., 2013; Saunshi et al., 2019; Chi et al., 2020; Fang and Xie, 2020; Giorgi et al., 2020; Kong et al., 2020; Gunel et al., 2021), or multi-modal (Sun et al., 2019; Luo et al., 2020). The goal is to learn semantic representation between two views by allowing the positive sample to be similar (in semantic space) and negatives to be dissimilar semantically simultaneously. CLIP (Radford et al.) and MIL-NCE (Miech et al., 2020) has demonstrated the effectiveness for learning the semantic mapping between vision and language. Previous attempts mainly exploit the InfoNCE (Oord et al., 2018) objective to maximize a lower bound of the mutual information. This paper extends the multimodal contrastive learning between the trace in the image and captioning sentence. In the same image, they correspond to each other semantically. This motivates us to design a contrastive loss for better\nalignment between the trace and language."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we focus on the controlled image captioning task and find mouse traces provide an intuitive and efficient way for a user to control the description. We propose a novel caption generation model with contrastive constraints and attention guidance called LoopCAG to control the captioning process spatially and temporally. The experimental results demonstrate the our model’s effectiveness, and our work will inspire more future research on vision-linguistic understanding and generation."
    }, {
      "heading" : "7 Acknowledgement",
      "text" : "We thank Botian Shi, Rongcheng Tu for helpful discussions. This work is supported in part by National Key R&D Program of China 2018AAA0102301 and NSFC 61925203."
    } ],
    "references" : [ {
      "title" : "Layer normalization",
      "author" : [ "Jimmy Ba", "J. Kiros", "Geoffrey E. Hinton." ],
      "venue" : "ArXiv, abs/1607.06450.",
      "citeRegEx" : "Ba et al\\.,? 2016",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2016
    }, {
      "title" : "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
      "author" : [ "Satanjeev Banerjee", "Alon Lavie." ],
      "venue" : "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or sum-",
      "citeRegEx" : "Banerjee and Lavie.,? 2005",
      "shortCiteRegEx" : "Banerjee and Lavie.",
      "year" : 2005
    }, {
      "title" : "Unsupervised learning of visual features by contrasting cluster assignments",
      "author" : [ "Mathilde Caron", "Ishan Misra", "Julien Mairal", "Priya Goyal", "Piotr Bojanowski", "Armand Joulin." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Caron et al\\.,? 2020",
      "shortCiteRegEx" : "Caron et al\\.",
      "year" : 2020
    }, {
      "title" : "Decoupled box proposal and featurization with ultrafine-grained semantic labels improve image captioning and visual question answering",
      "author" : [ "Soravit Changpinyo", "Bo Pang", "Piyush Sharma", "Radu Soricut." ],
      "venue" : "ArXiv, abs/1909.02097.",
      "citeRegEx" : "Changpinyo et al\\.,? 2019",
      "shortCiteRegEx" : "Changpinyo et al\\.",
      "year" : 2019
    }, {
      "title" : "A simple framework for contrastive learning of visual representations",
      "author" : [ "Ting Chen", "Simon Kornblith", "Mohammad Norouzi", "Geoffrey Hinton." ],
      "venue" : "ICML, volume 119, pages 1597–1607.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring simple siamese representation learning",
      "author" : [ "Xinlei Chen", "Kaiming He." ],
      "venue" : "arXiv preprint arXiv:2011.10566.",
      "citeRegEx" : "Chen and He.,? 2020",
      "shortCiteRegEx" : "Chen and He.",
      "year" : 2020
    }, {
      "title" : "Infoxlm: An information-theoretic framework for cross-lingual language model pre-training",
      "author" : [ "Zewen Chi", "L. Dong", "Furu Wei", "N. Yang", "Saksham Singhal", "Wenhui Wang", "Xia Song", "XianLing Mao", "He yan Huang", "M. Zhou." ],
      "venue" : "arXiv",
      "citeRegEx" : "Chi et al\\.,? 2020",
      "shortCiteRegEx" : "Chi et al\\.",
      "year" : 2020
    }, {
      "title" : "Show, control and tell: A framework for generating controllable and grounded captions",
      "author" : [ "M. Cornia", "L. Baraldi", "R. Cucchiara." ],
      "venue" : "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8299–8308.",
      "citeRegEx" : "Cornia et al\\.,? 2019",
      "shortCiteRegEx" : "Cornia et al\\.",
      "year" : 2019
    }, {
      "title" : "Length-controllable image captioning",
      "author" : [ "Chaorui Deng", "Ning Ding", "Mingkui Tan", "Qi Wu." ],
      "venue" : "Computer Vision – ECCV 2020, pages 712–729, Cham. Springer International Publishing.",
      "citeRegEx" : "Deng et al\\.,? 2020",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2020
    }, {
      "title" : "Diverse and controllable image captioning with partof-speech guidance",
      "author" : [ "Aditya Deshpande", "Jyoti Aneja", "Liwei Wang", "Alexander G Schwing", "David A Forsyth" ],
      "venue" : null,
      "citeRegEx" : "Deshpande et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Deshpande et al\\.",
      "year" : 2018
    }, {
      "title" : "Cert: Contrastive self-supervised learning for language understanding",
      "author" : [ "Hongchao Fang", "Pengtao Xie." ],
      "venue" : "arXiv preprint arXiv:2005.12766.",
      "citeRegEx" : "Fang and Xie.,? 2020",
      "shortCiteRegEx" : "Fang and Xie.",
      "year" : 2020
    }, {
      "title" : "Declutr: Deep contrastive learning for unsupervised textual representations",
      "author" : [ "John M Giorgi", "Osvald Nitski", "Gary D. Bader", "Bo Wang." ],
      "venue" : "arXiv preprint arXiv:2006.03659.",
      "citeRegEx" : "Giorgi et al\\.,? 2020",
      "shortCiteRegEx" : "Giorgi et al\\.",
      "year" : 2020
    }, {
      "title" : "Bootstrap your own latent: A new approach to self-supervised learning",
      "author" : [ "Michal Valko." ],
      "venue" : "NeurIPS.",
      "citeRegEx" : "Valko.,? 2020",
      "shortCiteRegEx" : "Valko.",
      "year" : 2020
    }, {
      "title" : "Supervised contrastive learning for pre-trained language model fine-tuning",
      "author" : [ "Beliz Gunel", "Jingfei Du", "Alexis Conneau", "Veselin Stoyanov." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Gunel et al\\.,? 2021",
      "shortCiteRegEx" : "Gunel et al\\.",
      "year" : 2021
    }, {
      "title" : "Momentum contrast for unsupervised visual representation learning",
      "author" : [ "Kaiming He", "Haoqi Fan", "Yuxin Wu", "Saining Xie", "Ross Girshick." ],
      "venue" : "CVPR, pages 9729–9738.",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "A mutual information maximization perspective of language representation learning",
      "author" : [ "Lingpeng Kong", "Cyprien de Masson d’Autume", "Lei Yu", "Wang Ling", "Zihang Dai", "Dani Yogatama" ],
      "venue" : "In ICLR",
      "citeRegEx" : "Kong et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Kong et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics",
      "author" : [ "Chin-Yew Lin", "Franz Josef Och." ],
      "venue" : "ACL, page 605.",
      "citeRegEx" : "Lin and Och.,? 2004",
      "shortCiteRegEx" : "Lin and Och.",
      "year" : 2004
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Univl: A unified video and language pre-training model for multimodal understanding and generation",
      "author" : [ "Huaishao Luo", "Lei Ji", "Botian Shi", "Haoyang Huang", "Nan Duan", "Tianrui Li", "Jason Li", "Taroon Bharti", "Ming Zhou." ],
      "venue" : "arXiv preprint arXiv:2002.06353.",
      "citeRegEx" : "Luo et al\\.,? 2020",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2020
    }, {
      "title" : "End-to-End Learning of Visual Representations from Uncurated Instructional Videos",
      "author" : [ "Antoine Miech", "Jean-Baptiste Alayrac", "Lucas Smaira", "Ivan Laptev", "Josef Sivic", "Andrew Zisserman." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Miech et al\\.,? 2020",
      "shortCiteRegEx" : "Miech et al\\.",
      "year" : 2020
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 26, pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Representation learning with contrastive predictive coding",
      "author" : [ "Aaron van den Oord", "Yazhe Li", "Oriol Vinyals." ],
      "venue" : "arXiv preprint arXiv:1807.03748.",
      "citeRegEx" : "Oord et al\\.,? 2018",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2018
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "ACL, pages 311– 318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Connecting vision and language with localized narratives",
      "author" : [ "Jordi Pont-Tuset", "Jasper Uijlings", "Beer Changpinyo", "Radu Soricut", "Vittorio Ferrari." ],
      "venue" : "ECCV.",
      "citeRegEx" : "Pont.Tuset et al\\.,? 2020",
      "shortCiteRegEx" : "Pont.Tuset et al\\.",
      "year" : 2020
    }, {
      "title" : "Faster r-cnn: Towards real-time object detection with region proposal networks",
      "author" : [ "Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun." ],
      "venue" : "Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1,",
      "citeRegEx" : "Ren et al\\.,? 2015",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "Self-critical sequence training for image captioning",
      "author" : [ "Steven J. Rennie", "E. Marcheret", "Youssef Mroueh", "J. Ross", "V. Goel." ],
      "venue" : "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1179–1195.",
      "citeRegEx" : "Rennie et al\\.,? 2017",
      "shortCiteRegEx" : "Rennie et al\\.",
      "year" : 2017
    }, {
      "title" : "A theoretical analysis of contrastive unsupervised representation learning",
      "author" : [ "Nikunj Saunshi", "Orestis Plevrakis", "Sanjeev Arora", "Mikhail Khodak", "Hrishikesh Khandeparkar." ],
      "venue" : "Proceedings of the 36th International Conference on Machine Learning,",
      "citeRegEx" : "Saunshi et al\\.,? 2019",
      "shortCiteRegEx" : "Saunshi et al\\.",
      "year" : 2019
    }, {
      "title" : "Contrastive bidirectional transformer for temporal representation learning",
      "author" : [ "Chen Sun", "Fabien Baradel", "Kevin Murphy", "Cordelia Schmid." ],
      "venue" : "arXiv preprint arXiv:1906.05743.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna." ],
      "venue" : "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2818–2826.",
      "citeRegEx" : "Szegedy et al\\.,? 2016",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2016
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "L. Kaiser", "Illia Polosukhin." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Cider: Consensus-based image description evaluation",
      "author" : [ "Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "CVPR, pages 4566–4575.",
      "citeRegEx" : "Vedantam et al\\.,? 2015",
      "shortCiteRegEx" : "Vedantam et al\\.",
      "year" : 2015
    }, {
      "title" : "Image captioning at will: A versatile scheme for effectively injecting sentiments into image descriptions",
      "author" : [ "Quanzeng You", "Hailin Jin", "Jiebo Luo." ],
      "venue" : "arXiv preprint arXiv:1801.10121.",
      "citeRegEx" : "You et al\\.,? 2018",
      "shortCiteRegEx" : "You et al\\.",
      "year" : 2018
    }, {
      "title" : "Topic-oriented image captioning based on order-embedding",
      "author" : [ "Niange Yu", "Xiaolin Hu", "Binheng Song", "Jian Yang", "Jianwei Zhang." ],
      "venue" : "IEEE Transactions on Image Processing, 28(6):2743–2754.",
      "citeRegEx" : "Yu et al\\.,? 2018",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "The captioning process can be controlled by POS tagging (Deshpande et al., 2018), sentiment (You et al.",
      "startOffset" : 56,
      "endOffset" : 80
    }, {
      "referenceID" : 31,
      "context" : ", 2018), sentiment (You et al., 2018), length (Deng et al.",
      "startOffset" : 19,
      "endOffset" : 37
    }, {
      "referenceID" : 8,
      "context" : ", 2018), length (Deng et al., 2020), bounding boxes (Cornia et al.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 7,
      "context" : ", 2020), bounding boxes (Cornia et al., 2019), and mouse traces (Pont-Tuset et al.",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 23,
      "context" : "In the Localized Narratives dataset (Pont-Tuset et al., 2020), the annotators describe the image",
      "startOffset" : 36,
      "endOffset" : 61
    }, {
      "referenceID" : 29,
      "context" : "We also generate Sinusoidal Positional Embeddings (Vaswani et al., 2017) oi to capture the temporal order of the traces.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 23,
      "context" : "To make a fair comparison with the baseline (Pont-Tuset et al., 2020), we apply the same setting and do not employ common",
      "startOffset" : 44,
      "endOffset" : 69
    }, {
      "referenceID" : 28,
      "context" : "techniques such as label smoothing(Szegedy et al., 2016) or self-critical training(Rennie et al.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 0,
      "context" : "Two linear projections and layer normalization (Ba et al., 2016) are applied sequentially on dimension L and H , respectively reducing the dimension to 1.",
      "startOffset" : 47,
      "endOffset" : 64
    }, {
      "referenceID" : 24,
      "context" : "For the visual feature, we adopt Faster-RCNN(Ren et al., 2015) to extract 100 bounding box proposals.",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 17,
      "context" : "We adopt the AdamW optimizer (Loshchilov and Hutter, 2019) with learning rate of 7e-4(which is the best performance",
      "startOffset" : 29,
      "endOffset" : 58
    }, {
      "referenceID" : 22,
      "context" : "This generation task adopts the traditional image captioning evaluation metric using the open-source tool2 with a minor modification3 to suit with LNCOCO, including BLEU(Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGEL (Lin and Och, 2004), ROUGE-1-F1(Pont-Tuset et al.",
      "startOffset" : 169,
      "endOffset" : 192
    }, {
      "referenceID" : 1,
      "context" : ", 2002), METEOR (Banerjee and Lavie, 2005), ROUGEL (Lin and Och, 2004), ROUGE-1-F1(Pont-Tuset et al.",
      "startOffset" : 16,
      "endOffset" : 42
    }, {
      "referenceID" : 16,
      "context" : ", 2002), METEOR (Banerjee and Lavie, 2005), ROUGEL (Lin and Och, 2004), ROUGE-1-F1(Pont-Tuset et al.",
      "startOffset" : 51,
      "endOffset" : 70
    }, {
      "referenceID" : 23,
      "context" : ", 2002), METEOR (Banerjee and Lavie, 2005), ROUGEL (Lin and Och, 2004), ROUGE-1-F1(Pont-Tuset et al., 2020), and CIDEr-D (Vedantam et al.",
      "startOffset" : 82,
      "endOffset" : 107
    }, {
      "referenceID" : 23,
      "context" : "Baseline and +Trace methods The Baseline and +Trace methods are our re-implementations following (Pont-Tuset et al., 2020)’s method description.",
      "startOffset" : 97,
      "endOffset" : 122
    }, {
      "referenceID" : 0,
      "context" : "Second, their projection layers also consist of layer normalization(Ba et al., 2016).",
      "startOffset" : 67,
      "endOffset" : 84
    }, {
      "referenceID" : 9,
      "context" : "Previous works aim to control the captioning by Part-Of-Speech tagging(Deshpande et al., 2018), sentiment, (You et al.",
      "startOffset" : 70,
      "endOffset" : 94
    }, {
      "referenceID" : 31,
      "context" : ", 2018), sentiment, (You et al., 2018), length (Deng et al.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : ", 2018), length (Deng et al., 2020), bounding box (Cornia et al.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 32,
      "context" : "Similar works (Yu et al., 2018; Cornia et al., 2019) control the caption by a sequence of ordered topics and bounding boxes.",
      "startOffset" : 14,
      "endOffset" : 52
    }, {
      "referenceID" : 7,
      "context" : "Similar works (Yu et al., 2018; Cornia et al., 2019) control the caption by a sequence of ordered topics and bounding boxes.",
      "startOffset" : 14,
      "endOffset" : 52
    }, {
      "referenceID" : 23,
      "context" : "The most similar work (Pont-Tuset et al., 2020) proposed a trace-controlled image cap-",
      "startOffset" : 22,
      "endOffset" : 47
    }, {
      "referenceID" : 14,
      "context" : "ing has been widely studied in unsupervised representation learning for vision, (He et al., 2020; Chen et al., 2020; Grill et al., 2020; Caron et al., 2020; Chen and He, 2020), language (Mikolov et al.",
      "startOffset" : 80,
      "endOffset" : 175
    }, {
      "referenceID" : 4,
      "context" : "ing has been widely studied in unsupervised representation learning for vision, (He et al., 2020; Chen et al., 2020; Grill et al., 2020; Caron et al., 2020; Chen and He, 2020), language (Mikolov et al.",
      "startOffset" : 80,
      "endOffset" : 175
    }, {
      "referenceID" : 2,
      "context" : "ing has been widely studied in unsupervised representation learning for vision, (He et al., 2020; Chen et al., 2020; Grill et al., 2020; Caron et al., 2020; Chen and He, 2020), language (Mikolov et al.",
      "startOffset" : 80,
      "endOffset" : 175
    }, {
      "referenceID" : 5,
      "context" : "ing has been widely studied in unsupervised representation learning for vision, (He et al., 2020; Chen et al., 2020; Grill et al., 2020; Caron et al., 2020; Chen and He, 2020), language (Mikolov et al.",
      "startOffset" : 80,
      "endOffset" : 175
    }, {
      "referenceID" : 20,
      "context" : ", 2020; Chen and He, 2020), language (Mikolov et al., 2013; Saunshi et al., 2019; Chi et al., 2020; Fang and Xie, 2020; Giorgi et al., 2020; Kong et al., 2020; Gunel et al., 2021), or multi-modal (Sun et al.",
      "startOffset" : 37,
      "endOffset" : 179
    }, {
      "referenceID" : 26,
      "context" : ", 2020; Chen and He, 2020), language (Mikolov et al., 2013; Saunshi et al., 2019; Chi et al., 2020; Fang and Xie, 2020; Giorgi et al., 2020; Kong et al., 2020; Gunel et al., 2021), or multi-modal (Sun et al.",
      "startOffset" : 37,
      "endOffset" : 179
    }, {
      "referenceID" : 6,
      "context" : ", 2020; Chen and He, 2020), language (Mikolov et al., 2013; Saunshi et al., 2019; Chi et al., 2020; Fang and Xie, 2020; Giorgi et al., 2020; Kong et al., 2020; Gunel et al., 2021), or multi-modal (Sun et al.",
      "startOffset" : 37,
      "endOffset" : 179
    }, {
      "referenceID" : 10,
      "context" : ", 2020; Chen and He, 2020), language (Mikolov et al., 2013; Saunshi et al., 2019; Chi et al., 2020; Fang and Xie, 2020; Giorgi et al., 2020; Kong et al., 2020; Gunel et al., 2021), or multi-modal (Sun et al.",
      "startOffset" : 37,
      "endOffset" : 179
    }, {
      "referenceID" : 11,
      "context" : ", 2020; Chen and He, 2020), language (Mikolov et al., 2013; Saunshi et al., 2019; Chi et al., 2020; Fang and Xie, 2020; Giorgi et al., 2020; Kong et al., 2020; Gunel et al., 2021), or multi-modal (Sun et al.",
      "startOffset" : 37,
      "endOffset" : 179
    }, {
      "referenceID" : 15,
      "context" : ", 2020; Chen and He, 2020), language (Mikolov et al., 2013; Saunshi et al., 2019; Chi et al., 2020; Fang and Xie, 2020; Giorgi et al., 2020; Kong et al., 2020; Gunel et al., 2021), or multi-modal (Sun et al.",
      "startOffset" : 37,
      "endOffset" : 179
    }, {
      "referenceID" : 13,
      "context" : ", 2020; Chen and He, 2020), language (Mikolov et al., 2013; Saunshi et al., 2019; Chi et al., 2020; Fang and Xie, 2020; Giorgi et al., 2020; Kong et al., 2020; Gunel et al., 2021), or multi-modal (Sun et al.",
      "startOffset" : 37,
      "endOffset" : 179
    }, {
      "referenceID" : 19,
      "context" : ") and MIL-NCE (Miech et al., 2020) has demonstrated the effectiveness for learning the semantic mapping between vision and language.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 21,
      "context" : "Previous attempts mainly exploit the InfoNCE (Oord et al., 2018) objective to maximize a lower bound of the mutual information.",
      "startOffset" : 45,
      "endOffset" : 64
    } ],
    "year" : 2021,
    "abstractText" : "Generating image captions with user intention is an emerging need. The recently published Localized Narratives dataset takes mouse traces as another input to the image captioning task, which is an intuitive and efficient way for a user to control what to describe in the image. However, how to effectively employ traces to improve generation quality and controllability is still under exploration. This paper aims to solve this problem by proposing a novel model called LoopCAG, which connects Contrastive constraints and Attention Guidance in a Loop manner, engaged explicit spatial and temporal constraints to the generating process. Precisely, each generated sentence is temporally aligned to the corresponding trace sequence through a contrastive learning strategy. Besides, each generated text token is supervised to attend to the correct visual objects under heuristic spatial attention guidance. Comprehensive experimental results demonstrate that our LoopCAG model learns better correspondence among the three modalities(vision, language, and traces) and achieves SOTA performance on trace controlled image captioning task. Moreover, the controllability and explainability of LoopCAG are validated by analyzing spatial and temporal sensitivity during the generation process.",
    "creator" : "LaTeX with hyperref"
  }
}