{
  "name" : "2021.acl-long.193.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Comprehensive Study: How the Context Information of Different Granularity Affects Dialogue State Tracking?",
    "authors" : [ "Puhai Yang", "Heyan Huang", "Xian-Ling Mao" ],
    "emails" : [ "maoxl}@bit.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2481–2491\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2481"
    }, {
      "heading" : "1 Introduction",
      "text" : "Currently, task-oriented dialogue systems have attracted great attention in academia and industry (Chen et al., 2017), which aim to assist the user to complete certain tasks, such as buying products, booking a restaurant, etc. As a key component of task-oriented dialogue system, dialogue state tracking plays a important role in understanding the natural language given by the user and expressing it as a certain dialogue state (Rastogi et al., 2017, 2018; Goel et al., 2018). The dialogue\n∗Corresponding author\nstate for each turn of a dialogue is typically presented as a series of slot value pairs that represent information about the user’s goal up to the current turn. For example, in Figure 1, the dialogue state at turn 2 is {(attraction − type, cinema), (attraction− area, south)}.\nIn general, there are two strategies to track a dialogue state: predicting it from scratch and updating it from previous state. The scratch-based\nstrategy obtains each slot value in dialogue state by inquiring all the dialogue history (Xu and Hu, 2018; Lei et al., 2018; Goel et al., 2019; Ren et al., 2019; Wu et al., 2019; Shan et al., 2020; Zhang et al., 2020), the advantage of this strategy is to ensure the integrity of the dialogue information. The previous-based strategy relies on the current turn dialogue to update the previous dialogue state (Mrkšić et al., 2017; Chao and Lane, 2019; Kim et al., 2020; Heck et al., 2020; Zhu et al., 2020), the main character of this strategy is to greatly improve the efficiency of dialogue state prediction and avoid the computational cost of encoding all dialogue history.\nHowever, both kinds of strategies above have great defects because of their own characters. For the scratch-based strategy, it is hard to correctly track short-dependency dialogue state because of the noise associated with encoding all dialogue history. For example, the dialogue history of turn 1 to 3 in Figure 1 (a) does not contribute to the prediction of slot values in the restaurant domain. For the previous-based strategy, it is difficult to solve the problem of long-dependency dialogue state tracking because it utilizes only limited dialogue information from the current turn dialogue and the previous state. As in Figure 1 (b), the slot taxi − departure cannot be predicted due to the absence of corresponding dialogue history content.\nObviously, it plays different roles for the context information of different granularity to track different kinds of dialogue states. Intuitively, less context information is needed for short-dependency dialogue state, while more context information must be taken into account for long-dependency dialogue state tracking. For example, the dialogue state in Figure 1 (c) is tracked from turn 2, which utilizes context information of granularity 4 (turn 3 to 6), providing evidence for the prediction of all slots while bringing as little noise as possible.\nThus, in this paper, we will study and discuss how the context information of different granularity affects dialogue state tracking. The contribution of this paper is that it is, to the best of our knowledge, the first detailed investigation of the impact of context granularity in dialogue state tracking and promotes the research on dialogue state tracking strategy. Our investigation mainly focuses on three points1:\n1The code is released at https://github.com/ yangpuhai/Granularity-in-DST\n• How greatly different granularities affect dialogue state tracking?\n• How to combine multiple granularities for dialogue state tracking?\n• Application of context information granularity in few-shot learning scenario.\nThe rest of paper is organized as follows: The relevant definitions and formulas in the dialogue state tracking strategy are introduced in section 2. Section 3 lists the detailed experimental settings. Section 4 presents the survey report and results, followed by conclusions in section 5."
    }, {
      "heading" : "2 Preliminary",
      "text" : "To describe the dialogue state tracking strategy, let’s introduce the formula definitions used in this paper:\nDialogue Content: D = (T1, T2, ..., TN ) is defined as the dialogue of length N , where Ti = (Si, Ui) is the dialogue content of i-th turn, which includes the system utterance Si and the user utterance Ui.\nDialogue State: We define E = (B0, B1, B2, ..., BN ) as all dialogue states up to the N -th turn of the dialogue, where Bi is the set of slot value pairs representing the information provided by the user up to the i-th turn. In particular, B0 is the initial dialogue state which is an empty set.\nGranularity: In dialogue state tracking, the number of dialogue turns spanning from a certain dialogue state Bm in the dialogue to the current dialogue state Bn is called granularity, that is, G = |(Tm+1, ..., Tn)|. For example, the granularities of context information in (a), (b), and (c) in Figure 1 are 6, 1, and 4, respectively.\nAssuming that the dialogue state of the N -th turn is currently required to be inferred, the dialogue state tracking under a certain granularity is as follows:"
    }, {
      "heading" : "BN = tracker((TN−G+1, ..., TN ), BN−G)",
      "text" : "where G ∈ {1, 2, ..., N} is the granularity of context information and tracker represents a dialogue state tracking model.\nIn particular, if G = 1, then:\nBN = tracker(TN , BN−1)\nthis case corresponds to the strategy of updating from previous state. Therefore, the previous-based strategy is a special case where context granularity is minimal in dialogue state tracking.\nIf G = N , then:\nBN = tracker((T1, ..., TN ), B0)\nthis case corresponds to the strategy of predicting state from scratch. Similarly, the scratch-based strategy is also a special case of dialogue state tracking, with the context information of maximum granularity. Since the size of the maximum granularity N is different in different dialogues, so 0 is used in the paper to refer to the maximum granularity N , -1 to refer to granularity N − 1, and so on."
    }, {
      "heading" : "3 Experimental Settings",
      "text" : "In order to investigate how the context information of different granularity affects dialogue state tracking, we analyze the performance of several different types of dialogue state tracking models on different datasets. For a clearer illustration, the detailed settings are introduced in this section."
    }, {
      "heading" : "3.1 Datasets",
      "text" : "Our experiments were carried out on 5 datasets, Sim-M (Shah et al., 2018), Sim-R (Shah et al.,\n2018), WOZ2.0 (Wen et al., 2016), DSTC2 (Henderson et al., 2014) and MultiWOZ2.1 (Eric et al., 2019). The statistics for all datasets are shown in Table 1.\nSim-M and Sim-R are multi-turn dialogue datasets in the movie and restaurant domains, respectively, which are specially designed to evaluate the scalability of dialogue state tracking model. A large number of unknown slot values are included in their test set, so the generalization ability of the model can be reflected more accurately.\nWOZ2.0 and DSTC2 datasets are both collected in the restaurant domain and have the same three slots food, area, and price range. These two datasets provide automatic speech recognition (ASR) hypotheses of user utterances and can therefore be used to verify the robustness of the model against ASR errors. As in previous works, we use manuscript user utterance for training and top ASR hypothesis for testing.\nMultiWOZ2.1 is the corrected version of the MultiWOZ (Budzianowski et al., 2018). Compared to the four datasets above, MultiWOZ2.1 is a more challenging and currently widely used benchmark for multi-turn multi-domain dialogue state tracking, consisting of 7 domains, over 30 slots, and over 4500 possible slot values. Following previous works (Wu et al., 2019; Kim et al., 2020; Heck et al., 2020; Zhu et al., 2020), we only use 5 domains\n(restaurant, train, hotel, taxi, attraction) that contain a total of 30 slots."
    }, {
      "heading" : "3.2 Baselines",
      "text" : "We use 5 different types of baselines whose characteristics are shown in Table 2.\nSpanPtr: This is the first model to extract slot values directly from dialogue context without an ontology, it encodes the whole dialogue history with a bidirectional RNN and extracts slot value for each slot by generating the start and end positions in dialogue history (Xu and Hu, 2018).\nTRADE: This model is the first to consider knowledge transfer between domains in the multidomain dialogue state tracking task. It represents a slot as a concatenation of domain name and slot name, encodes all dialogue history using bidirectional RNN, and finally decodes each slot value using a pointer-generator network (Wu et al., 2019).\nBERTDST: This model decodes only the slot values of the slots mentioned in the current turn of dialogue, and then uses a rule-based update mechanism to update from the previous state to the current turn state. It uses BERT to encode the current turn of dialogue and extracts slot values from the dialogue as spans (Chao and Lane, 2019).\nSOMDST: This model takes the dialogue state as an explicit memory that can be selectively overwritten, and inputs it into BERT together with the current turn dialogue. It then decomposes the prediction for each slot value into operation prediction and slot generation (Kim et al., 2020).\nSUMBT: This model uses an ontology and is trained and evaluated on the dialogue session level instead of the dialogue turn level. BERT is used in the model to encode turn level dialogues, and an unidirectional RNN is used to capture session-level representation (Lee et al., 2019)."
    }, {
      "heading" : "3.3 Configurations and Metrics",
      "text" : "Our deployments are based on the official implementation source code of SOMDST2 and SUMBT3, in which SpanPtr, TRADE and BERTDST are reproduced in this paper. BERT in all models uses pre-trained BERT (Vaswani et al., 2017) (BERTBase, Uncased) which has 12 hidden layers of 768 units and 12 self-attention heads, while RNN uses\n2https://github.com/clovaai/som-dst 3https://github.com/SKTBrain/SUMBT\nGRU (Cho et al., 2014). We use adam (Kingma and Ba, 2014) as the optimizer and use greedy decoding. We customize the training epochs for all models, and the training stopped early when the model’s performance on development set failed to improve for 15 consecutive epochs, and all the results were averaged over the three runs with different random seeds. The detailed setting of the hyperparameters is given in Appendix A.\nSince the length of the dialogue history is related to the granularity, the input length of the model needs to adapt to the granularity. Especially for the model with BERT as the encoder, in order to prevent the input from being truncated, we set the max sequence length to exceed almost all the inputs under different granularity. See Appendix A for details on the max sequence length settings.\nFollowing previous works (Xu and Hu, 2018; Wu et al., 2019; Kim et al., 2020; Heck et al., 2020), the joint accuracy (Joint acc) and slot accuracy (Slot acc) are used for evaluation. The joint accuracy is the accuracy that checks whether all the predicted slot values in each turn are exactly the same as the ground truth slot values. The slot accuracy is the average accuracy of slot value prediction in all turns."
    }, {
      "heading" : "4 Experimental Analysis",
      "text" : "This section presents our detailed investigation of how the context information of different granularity affects dialogue state tracking, focusing on the impact of granularity on dialogue state tracking, the combination of multiple granularities, and the application of context granularity in few-shot learning scenario. For simplicity, in all experimental results, the maximum granularity is expressed as 0, the maximum granularity minus 1 is expressed as -1, and so on."
    }, {
      "heading" : "4.1 How greatly different granularities affect dialogue state tracking?",
      "text" : "The first part of our investigation look at the validity of the context granularity used by the current various dialogue state tracking models and try to figure out how different granularities affect dialogue state tracking. The experimental results are shown in Table 3.\nIt can be found that some dialogue state tracking models do not take the appropriate granularity, and their performance is greatly improved when they are trained with the the context of appropriate gran-\nularity. For example, the joint accuracy of SpanPtr with granularity -3 on WOZ2.0 improved by 42%, while the joint accuracy of BERTDST with granularity 4 on MultiWOZ2.1 improved by 19%. These results suggest that there are significant differences in dialogue state tracking at different granularities, therefore, we should be careful to determine the granularity to be used according to the characteristics of the model and dataset.\nBy observing the experimental comparison results on different models and datasets in Table 3, it can be found that:\n• For different models, the model with generative decoding prefer larger granularity, because it requires more context information to effectively learn vocabulary-based distribution. For example, TRADE and SOMDST both perform better in larger granularity. Meanwhile, the model with extractive decoding is more dependent on the characteristics of the dataset. Besides, in general, the model with generative decoding has obvious advantages over the model with extractive decoding.\n• For different datasets, when the dataset involves multiple domains and there are a large number of long-dependency dialogue states, context information of larger granularity can\nbe used to more effectively capture the longdependency relationship in the data for dialogue state tracking, such as MultiWOZ2.1 dataset. For simpler single-domain datasets, where a large number of short dependencies\ndetermine the effectiveness of small granularity in dialogue state tracking. However, when there are more turns of dialogue resulting in less information in each turn, a larger granularity may be required to provide enough information, for example, SpanPtr performs best on the DSTC2 dataset at maximum granularity.\nAs can be seen from the above analysis, different granularities have their own advantages in different situations of dialogue, so it is natural to wonder whether multiple granularities can be combined to achieve better dialogue state tracking. Next, let’s\ndiscuss the issue of multi-granularity combination."
    }, {
      "heading" : "4.2 How to combine multiple granularities for dialogue state tracking?",
      "text" : "Following the above analysis, here we mainly discuss how to combine multiple granularities in dialogue state tracking, mainly focusing on three aspects: (1) The relationship between granularities, (2) Performance of multi-granularity combination and (3) Limitations of multi-granularity combination.\nThe relationship between granularities: First, we use different granularities in the training and\ninference phases of dialogue state tracking to figure out the relationship between different granularities, as shown in Figure 2. It can be seen that when we fix the granularity of context information in the inference phase, the dialogue state tracking model trained with other granularity still obtains the generalization under this inference granularity. And even some models learned at other granularity, such as the BERTDST in Figure 2 (b) and (f), can perform better. Meanwhile, it can also be found that as the granularity gap increases, the context information becomes more and more inconsistent, and eventually the ability of the model to generalize across granularity is gradually reduced. Through these phenomena, we can summarize as follows: The knowledge learned by the dialogue state tracking model in context information of different granularity is transferable and the smaller the gap between granularity can bring more knowledge transfer effect.\nPerformance of multi-granularity combination: Then, we use the knowledge transfer between context information of different granularity to improve the baseline. In the specific experiment, we add the most adjacent granularity to the training phase of the model, that is, the context under two granularities is used for training, while the inference phase remains unchanged, as shown in Table 4. It can be observed that in most cases, the performance of the baseline models is significantly enhanced, suggesting that adding more granularity context information to the training phase of the model can indeed improve the generalization of the dialogue state tracking model. Of course, in some cases, multi-granularity combination results in a reduction in performance, such as SpanPtr, TRADE, and BERTDST on DSTC2 dataset. The main reason for this phenomenon should be the large deviation between the context information of different granularity in the multi-granularity combination, as can be seen from the large reduction of SpanPtr, TRADE, and BERTDST on the DSTC2 dataset with other granularity in Table 3.\nLimitations of multi-granularity combination: Given that multi-granularity combination can lead to improved generalization performance, is it better to have more context information of different granularity in training phase? To answer this question, we gradually add more granularities to the training phase while keeping the inference granularity\nunchanged, the experimental results are shown in Figure 3. It can be found that there is an upper limit to the use of multi-granularity combination in the training phase. Generally, adding the granularity with the smallest gap can bring the best effect, after that, with the increase of granularity number, the performance will decline."
    }, {
      "heading" : "4.3 Application of context information granularity in few-shot learning scenario",
      "text" : "Considering the knowledge transfer between granularity in multi-granularity combination, we explore the application of multi-granularity combination in few-shot learning scenario.\nFigure 4 shows the joint accuracy of the model with different multi-granularity combinations and the percentage improvement relative to the baseline model on the WOZ2.0 dataset with different\ntraining data scales. It can be found that under different scales of training data, multi-granularity combination can achieve better performance compared with single-granularity in most cases. Moreover, it can be seen from (a), (d) and (e) that the advantages of multi-granularity combination are gradually expanding with the decrease of the scale of training dataset. Therefore, the performance of multi-granularity combination in few-shot learning is worth exploring.\nWe conduct detailed experiments on all the 5 datasets in the paper to fully explore the potential of multi-granularity combination in few-shot learning, as shown in Table 5. It can be found that multi-granularity combination has a very significant effect in few-shot learning, and in some cases can even achieve a relative improvement of more than 10%, such as SpanPtr on Sim-R and WOZ2.0, BERTDST on Sim-M, SOMDST on WOZ2.0 and DSTC2. Meanwhile, in few-shot learning, the upper limit of multi-granularity combination can be higher, and better performance can be achieved when more granularities are added in the training phase.\nThe above experimental results of multigranularity combination in few-shot learning show that, there is indeed knowledge transfer between different granularity contexts, and the model can obtain more adequate modeling of dialogue by learning context dialogues of different granularity."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In the paper, we analyze the defects of two existing traditional dialogue state tracking strategies when dealing with context of different granularity and make a comprehensive study on how the context information of different granularity affects dialogue state tracking. Extensive experimental results and analysis show that: (1) Different granularities have their own advantages in different situations of dialogue state tracking; (2) The multi-granularity combination can effectively improve the dialogue state tracking; (3) The application of multi-granularity combination in few-shot learning can bring significant effects. In future work, dynamic context granularity can be used in training and inference to further improve dialogue state tracking."
    }, {
      "heading" : "6 Ethical Consideration",
      "text" : "This work may contribute to the development of conversational systems. In the narrow sense, this work focuses on dialogue state tracking in taskoriented dialogue system, hoping to improve the ability of conversational AI to understand human natural language. If so, these improvements could have a positive impact on the research and application of conversational AI, which could help humans to complete goals more effectively in a more intelligent way of communication. However, we never forget the other side of the coin. The agent substitution of conversational AI may affect the humanized communication and may lead to human-machine conflict problems, which need to be considered more broadly in the field of conversational AI."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the anonymous reviewers for their insightful comments. This work was supported by National Key R&D Plan (No. 2020AAA0106600) and National Natural Science Foundation of China (Grant No. U19B2020 and No. 61772076)."
    }, {
      "heading" : "A Settings",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Multiwoz-a largescale multi-domain wizard-of-oz dataset for taskoriented dialogue modelling",
      "author" : [ "Paweł Budzianowski", "Tsung-Hsien Wen", "Bo-Hsiang Tseng", "Iñigo Casanueva", "Stefan Ultes", "Osman Ramadan", "Milica Gasic." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Budzianowski et al\\.,? 2018",
      "shortCiteRegEx" : "Budzianowski et al\\.",
      "year" : 2018
    }, {
      "title" : "Bert-dst: Scalable end-to-end dialogue state tracking with bidirectional encoder representations from transformer",
      "author" : [ "Guan-Lin Chao", "Ian Lane." ],
      "venue" : "Proc. Interspeech 2019, pages 1468–1472.",
      "citeRegEx" : "Chao and Lane.,? 2019",
      "shortCiteRegEx" : "Chao and Lane.",
      "year" : 2019
    }, {
      "title" : "A survey on dialogue systems: Recent advances and new frontiers",
      "author" : [ "Hongshen Chen", "Xiaorui Liu", "Dawei Yin", "Jiliang Tang." ],
      "venue" : "Acm Sigkdd Explorations Newsletter, 19(2):25–35.",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning phrase representations using rnn encoderdecoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "B van Merrienboer", "Caglar Gulcehre", "F Bougares", "H Schwenk", "Yoshua Bengio." ],
      "venue" : "Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Multiwoz 2.1: Multi-domain dialogue state corrections and state tracking baselines. arXiv preprint arXiv:1907.01669",
      "author" : [ "Mihail Eric", "Rahul Goel", "Shachi Paul", "Abhishek Sethi", "Sanchit Agarwal", "Shuyag Gao", "Dilek HakkaniTur" ],
      "venue" : null,
      "citeRegEx" : "Eric et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Eric et al\\.",
      "year" : 2019
    }, {
      "title" : "Flexible and scalable state tracking framework for goal-oriented dialogue systems",
      "author" : [ "Rahul Goel", "Shachi Paul", "Tagyoung Chung", "Jeremie Lecomte", "Arindam Mandal", "Dilek Hakkani-Tur." ],
      "venue" : "arXiv preprint arXiv:1811.12891.",
      "citeRegEx" : "Goel et al\\.,? 2018",
      "shortCiteRegEx" : "Goel et al\\.",
      "year" : 2018
    }, {
      "title" : "Hyst: A hybrid approach for flexible and accurate dialogue state tracking",
      "author" : [ "Rahul Goel", "Shachi Paul", "Dilek Hakkani-Túr." ],
      "venue" : "Proc. Interspeech 2019, pages 1458–1462.",
      "citeRegEx" : "Goel et al\\.,? 2019",
      "shortCiteRegEx" : "Goel et al\\.",
      "year" : 2019
    }, {
      "title" : "Trippy: A triple copy strategy for value independent neural dialog state tracking",
      "author" : [ "Michael Heck", "Carel van Niekerk", "Nurul Lubis", "Christian Geishauser", "Hsien-Chin Lin", "Marco Moresi", "Milica Gasic." ],
      "venue" : "Proceedings of the 21th Annual Meeting of the",
      "citeRegEx" : "Heck et al\\.,? 2020",
      "shortCiteRegEx" : "Heck et al\\.",
      "year" : 2020
    }, {
      "title" : "The second dialog state tracking challenge",
      "author" : [ "Matthew Henderson", "Blaise Thomson", "Jason D Williams." ],
      "venue" : "Proceedings of the 15th annual meeting of the special interest group on discourse and dialogue (SIGDIAL), pages 263–272.",
      "citeRegEx" : "Henderson et al\\.,? 2014",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2014
    }, {
      "title" : "Efficient dialogue state tracking by selectively overwriting memory",
      "author" : [ "Sungdong Kim", "Sohee Yang", "Gyuwan Kim", "SangWoo Lee." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 567–582.",
      "citeRegEx" : "Kim et al\\.,? 2020",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2020
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Sumbt: Slot-utterance matching for universal and scalable belief tracking",
      "author" : [ "Hwaran Lee", "Jinsik Lee", "Tae-Yoon Kim." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5478–5483.",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures",
      "author" : [ "Wenqiang Lei", "Xisen Jin", "Min-Yen Kan", "Zhaochun Ren", "Xiangnan He", "Dawei Yin." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association",
      "citeRegEx" : "Lei et al\\.,? 2018",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural belief tracker: Data-driven dialogue state tracking",
      "author" : [ "Nikola Mrkšić", "Diarmuid Ó Séaghdha", "Tsung-Hsien Wen", "Blaise Thomson", "Steve Young." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol-",
      "citeRegEx" : "Mrkšić et al\\.,? 2017",
      "shortCiteRegEx" : "Mrkšić et al\\.",
      "year" : 2017
    }, {
      "title" : "Multi-task learning for joint language understanding and dialogue state tracking",
      "author" : [ "Abhinav Rastogi", "Raghav Gupta", "Dilek HakkaniTur." ],
      "venue" : "Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue, pages 376–384.",
      "citeRegEx" : "Rastogi et al\\.,? 2018",
      "shortCiteRegEx" : "Rastogi et al\\.",
      "year" : 2018
    }, {
      "title" : "Scalable multi-domain dialogue state tracking",
      "author" : [ "Abhinav Rastogi", "Dilek Hakkani-Tür", "Larry Heck." ],
      "venue" : "2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 561– 568. IEEE.",
      "citeRegEx" : "Rastogi et al\\.,? 2017",
      "shortCiteRegEx" : "Rastogi et al\\.",
      "year" : 2017
    }, {
      "title" : "Scalable and accurate dialogue state tracking via hierarchical sequence generation",
      "author" : [ "Liliang Ren", "Jianmo Ni", "Julian McAuley." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Ren et al\\.,? 2019",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2019
    }, {
      "title" : "Building a conversational agent overnight with dialogue self-play",
      "author" : [ "Pararth Shah", "Dilek Hakkani-Tür", "Gokhan Tür", "Abhinav Rastogi", "Ankur Bapna", "Neha Nayak", "Larry Heck." ],
      "venue" : "arXiv preprint arXiv:1801.04871.",
      "citeRegEx" : "Shah et al\\.,? 2018",
      "shortCiteRegEx" : "Shah et al\\.",
      "year" : 2018
    }, {
      "title" : "A contextual hierarchical attention network with adaptive objective for dialogue state tracking",
      "author" : [ "Yong Shan", "Zekang Li", "Jinchao Zhang", "Fandong Meng", "Yang Feng", "Cheng Niu", "Jie Zhou." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for",
      "citeRegEx" : "Shan et al\\.,? 2020",
      "shortCiteRegEx" : "Shan et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of the 31st International Conference on Neural Information Processing Sys-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "A networkbased end-to-end trainable task-oriented dialogue system",
      "author" : [ "Tsung-Hsien Wen", "David Vandyke", "Nikola Mrksic", "Milica Gasic", "Lina M Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "Steve Young." ],
      "venue" : "arXiv preprint arXiv:1604.04562.",
      "citeRegEx" : "Wen et al\\.,? 2016",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2016
    }, {
      "title" : "Transferable multi-domain state generator for task-oriented dialogue systems",
      "author" : [ "Chien-Sheng Wu", "Andrea Madotto", "Ehsan HosseiniAsl", "Caiming Xiong", "Richard Socher", "Pascale Fung." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "An end-to-end approach for handling unknown slot values in dialogue state tracking",
      "author" : [ "Puyang Xu", "Qi Hu." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1448–1457.",
      "citeRegEx" : "Xu and Hu.,? 2018",
      "shortCiteRegEx" : "Xu and Hu.",
      "year" : 2018
    }, {
      "title" : "Find or classify? dual strategy for slot-value predictions on multi-domain dialog state tracking",
      "author" : [ "Jianguo Zhang", "Kazuma Hashimoto", "Chien-Sheng Wu", "Yao Wang", "S Yu Philip", "Richard Socher", "Caiming Xiong." ],
      "venue" : "Proceedings of the Ninth Joint Con-",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Efficient context and schema fusion networks for multidomain dialogue state tracking",
      "author" : [ "Su Zhu", "Jieyu Li", "Lu Chen", "Kai Yu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 766–781.",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Currently, task-oriented dialogue systems have attracted great attention in academia and industry (Chen et al., 2017), which aim to assist the user to complete certain tasks, such as buying products, booking a restaurant, etc.",
      "startOffset" : 98,
      "endOffset" : 117
    }, {
      "referenceID" : 5,
      "context" : "As a key component of task-oriented dialogue system, dialogue state tracking plays a important role in understanding the natural language given by the user and expressing it as a certain dialogue state (Rastogi et al., 2017, 2018; Goel et al., 2018).",
      "startOffset" : 202,
      "endOffset" : 249
    }, {
      "referenceID" : 22,
      "context" : "2482 strategy obtains each slot value in dialogue state by inquiring all the dialogue history (Xu and Hu, 2018; Lei et al., 2018; Goel et al., 2019; Ren et al., 2019; Wu et al., 2019; Shan et al., 2020; Zhang et al., 2020), the advantage of this strategy is to ensure the integrity of the dialogue information.",
      "startOffset" : 94,
      "endOffset" : 222
    }, {
      "referenceID" : 12,
      "context" : "2482 strategy obtains each slot value in dialogue state by inquiring all the dialogue history (Xu and Hu, 2018; Lei et al., 2018; Goel et al., 2019; Ren et al., 2019; Wu et al., 2019; Shan et al., 2020; Zhang et al., 2020), the advantage of this strategy is to ensure the integrity of the dialogue information.",
      "startOffset" : 94,
      "endOffset" : 222
    }, {
      "referenceID" : 6,
      "context" : "2482 strategy obtains each slot value in dialogue state by inquiring all the dialogue history (Xu and Hu, 2018; Lei et al., 2018; Goel et al., 2019; Ren et al., 2019; Wu et al., 2019; Shan et al., 2020; Zhang et al., 2020), the advantage of this strategy is to ensure the integrity of the dialogue information.",
      "startOffset" : 94,
      "endOffset" : 222
    }, {
      "referenceID" : 16,
      "context" : "2482 strategy obtains each slot value in dialogue state by inquiring all the dialogue history (Xu and Hu, 2018; Lei et al., 2018; Goel et al., 2019; Ren et al., 2019; Wu et al., 2019; Shan et al., 2020; Zhang et al., 2020), the advantage of this strategy is to ensure the integrity of the dialogue information.",
      "startOffset" : 94,
      "endOffset" : 222
    }, {
      "referenceID" : 21,
      "context" : "2482 strategy obtains each slot value in dialogue state by inquiring all the dialogue history (Xu and Hu, 2018; Lei et al., 2018; Goel et al., 2019; Ren et al., 2019; Wu et al., 2019; Shan et al., 2020; Zhang et al., 2020), the advantage of this strategy is to ensure the integrity of the dialogue information.",
      "startOffset" : 94,
      "endOffset" : 222
    }, {
      "referenceID" : 18,
      "context" : "2482 strategy obtains each slot value in dialogue state by inquiring all the dialogue history (Xu and Hu, 2018; Lei et al., 2018; Goel et al., 2019; Ren et al., 2019; Wu et al., 2019; Shan et al., 2020; Zhang et al., 2020), the advantage of this strategy is to ensure the integrity of the dialogue information.",
      "startOffset" : 94,
      "endOffset" : 222
    }, {
      "referenceID" : 23,
      "context" : "2482 strategy obtains each slot value in dialogue state by inquiring all the dialogue history (Xu and Hu, 2018; Lei et al., 2018; Goel et al., 2019; Ren et al., 2019; Wu et al., 2019; Shan et al., 2020; Zhang et al., 2020), the advantage of this strategy is to ensure the integrity of the dialogue information.",
      "startOffset" : 94,
      "endOffset" : 222
    }, {
      "referenceID" : 13,
      "context" : "The previous-based strategy relies on the current turn dialogue to update the previous dialogue state (Mrkšić et al., 2017; Chao and Lane, 2019; Kim et al., 2020; Heck et al., 2020; Zhu et al., 2020), the main character of this strategy is to greatly improve the efficiency of dialogue state prediction and avoid the computational cost of encoding all dialogue history.",
      "startOffset" : 102,
      "endOffset" : 199
    }, {
      "referenceID" : 1,
      "context" : "The previous-based strategy relies on the current turn dialogue to update the previous dialogue state (Mrkšić et al., 2017; Chao and Lane, 2019; Kim et al., 2020; Heck et al., 2020; Zhu et al., 2020), the main character of this strategy is to greatly improve the efficiency of dialogue state prediction and avoid the computational cost of encoding all dialogue history.",
      "startOffset" : 102,
      "endOffset" : 199
    }, {
      "referenceID" : 9,
      "context" : "The previous-based strategy relies on the current turn dialogue to update the previous dialogue state (Mrkšić et al., 2017; Chao and Lane, 2019; Kim et al., 2020; Heck et al., 2020; Zhu et al., 2020), the main character of this strategy is to greatly improve the efficiency of dialogue state prediction and avoid the computational cost of encoding all dialogue history.",
      "startOffset" : 102,
      "endOffset" : 199
    }, {
      "referenceID" : 7,
      "context" : "The previous-based strategy relies on the current turn dialogue to update the previous dialogue state (Mrkšić et al., 2017; Chao and Lane, 2019; Kim et al., 2020; Heck et al., 2020; Zhu et al., 2020), the main character of this strategy is to greatly improve the efficiency of dialogue state prediction and avoid the computational cost of encoding all dialogue history.",
      "startOffset" : 102,
      "endOffset" : 199
    }, {
      "referenceID" : 24,
      "context" : "The previous-based strategy relies on the current turn dialogue to update the previous dialogue state (Mrkšić et al., 2017; Chao and Lane, 2019; Kim et al., 2020; Heck et al., 2020; Zhu et al., 2020), the main character of this strategy is to greatly improve the efficiency of dialogue state prediction and avoid the computational cost of encoding all dialogue history.",
      "startOffset" : 102,
      "endOffset" : 199
    }, {
      "referenceID" : 22,
      "context" : "Models Open vocabulary Encoder Decoder Tracking strategy SpanPtr (Xu and Hu, 2018) X RNN Extractive scratch-based TRADE (Wu et al.",
      "startOffset" : 65,
      "endOffset" : 82
    }, {
      "referenceID" : 21,
      "context" : "Models Open vocabulary Encoder Decoder Tracking strategy SpanPtr (Xu and Hu, 2018) X RNN Extractive scratch-based TRADE (Wu et al., 2019) X RNN Generative scratch-based BERTDST (Chao and Lane, 2019) X BERT Extractive previous-based SOMDST (Kim et al.",
      "startOffset" : 120,
      "endOffset" : 137
    }, {
      "referenceID" : 1,
      "context" : ", 2019) X RNN Generative scratch-based BERTDST (Chao and Lane, 2019) X BERT Extractive previous-based SOMDST (Kim et al.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : ", 2019) X RNN Generative scratch-based BERTDST (Chao and Lane, 2019) X BERT Extractive previous-based SOMDST (Kim et al., 2020) X BERT Generative previous-based SUMBT (Lee et al.",
      "startOffset" : 109,
      "endOffset" : 127
    }, {
      "referenceID" : 11,
      "context" : ", 2020) X BERT Generative previous-based SUMBT (Lee et al., 2019) × BERT Classification previous-based",
      "startOffset" : 47,
      "endOffset" : 65
    }, {
      "referenceID" : 17,
      "context" : "1 Datasets Our experiments were carried out on 5 datasets, Sim-M (Shah et al., 2018), Sim-R (Shah et al.",
      "startOffset" : 65,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "1 is the corrected version of the MultiWOZ (Budzianowski et al., 2018).",
      "startOffset" : 43,
      "endOffset" : 70
    }, {
      "referenceID" : 21,
      "context" : "Following previous works (Wu et al., 2019; Kim et al., 2020; Heck et al., 2020; Zhu et al., 2020), we only use 5 domains",
      "startOffset" : 25,
      "endOffset" : 97
    }, {
      "referenceID" : 9,
      "context" : "Following previous works (Wu et al., 2019; Kim et al., 2020; Heck et al., 2020; Zhu et al., 2020), we only use 5 domains",
      "startOffset" : 25,
      "endOffset" : 97
    }, {
      "referenceID" : 7,
      "context" : "Following previous works (Wu et al., 2019; Kim et al., 2020; Heck et al., 2020; Zhu et al., 2020), we only use 5 domains",
      "startOffset" : 25,
      "endOffset" : 97
    }, {
      "referenceID" : 24,
      "context" : "Following previous works (Wu et al., 2019; Kim et al., 2020; Heck et al., 2020; Zhu et al., 2020), we only use 5 domains",
      "startOffset" : 25,
      "endOffset" : 97
    }, {
      "referenceID" : 22,
      "context" : "SpanPtr: This is the first model to extract slot values directly from dialogue context without an ontology, it encodes the whole dialogue history with a bidirectional RNN and extracts slot value for each slot by generating the start and end positions in dialogue history (Xu and Hu, 2018).",
      "startOffset" : 271,
      "endOffset" : 288
    }, {
      "referenceID" : 21,
      "context" : "It represents a slot as a concatenation of domain name and slot name, encodes all dialogue history using bidirectional RNN, and finally decodes each slot value using a pointer-generator network (Wu et al., 2019).",
      "startOffset" : 194,
      "endOffset" : 211
    }, {
      "referenceID" : 1,
      "context" : "It uses BERT to encode the current turn of dialogue and extracts slot values from the dialogue as spans (Chao and Lane, 2019).",
      "startOffset" : 104,
      "endOffset" : 125
    }, {
      "referenceID" : 9,
      "context" : "It then decomposes the prediction for each slot value into operation prediction and slot generation (Kim et al., 2020).",
      "startOffset" : 100,
      "endOffset" : 118
    }, {
      "referenceID" : 11,
      "context" : "BERT is used in the model to encode turn level dialogues, and an unidirectional RNN is used to capture session-level representation (Lee et al., 2019).",
      "startOffset" : 132,
      "endOffset" : 150
    }, {
      "referenceID" : 19,
      "context" : "BERT in all models uses pre-trained BERT (Vaswani et al., 2017) (BERTBase, Uncased) which has 12 hidden layers of 768 units and 12 self-attention heads, while RNN uses (2)https://github.",
      "startOffset" : 41,
      "endOffset" : 63
    }, {
      "referenceID" : 10,
      "context" : "We use adam (Kingma and Ba, 2014) as the optimizer and use greedy decoding.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 22,
      "context" : "Following previous works (Xu and Hu, 2018; Wu et al., 2019; Kim et al., 2020; Heck et al., 2020), the joint accuracy (Joint acc) and slot accuracy (Slot acc) are used for evaluation.",
      "startOffset" : 25,
      "endOffset" : 96
    }, {
      "referenceID" : 21,
      "context" : "Following previous works (Xu and Hu, 2018; Wu et al., 2019; Kim et al., 2020; Heck et al., 2020), the joint accuracy (Joint acc) and slot accuracy (Slot acc) are used for evaluation.",
      "startOffset" : 25,
      "endOffset" : 96
    }, {
      "referenceID" : 9,
      "context" : "Following previous works (Xu and Hu, 2018; Wu et al., 2019; Kim et al., 2020; Heck et al., 2020), the joint accuracy (Joint acc) and slot accuracy (Slot acc) are used for evaluation.",
      "startOffset" : 25,
      "endOffset" : 96
    }, {
      "referenceID" : 7,
      "context" : "Following previous works (Xu and Hu, 2018; Wu et al., 2019; Kim et al., 2020; Heck et al., 2020), the joint accuracy (Joint acc) and slot accuracy (Slot acc) are used for evaluation.",
      "startOffset" : 25,
      "endOffset" : 96
    } ],
    "year" : 2021,
    "abstractText" : "Dialogue state tracking (DST) plays a key role in task-oriented dialogue systems to monitor the user’s goal. In general, there are two strategies to track a dialogue state: predicting it from scratch and updating it from previous state. The scratch-based strategy obtains each slot value by inquiring all the dialogue history, and the previous-based strategy relies on the current turn dialogue to update the previous dialogue state. However, it is hard for the scratch-based strategy to correctly track shortdependency dialogue state because of noise; meanwhile, the previous-based strategy is not very useful for long-dependency dialogue state tracking. Obviously, it plays different roles for the context information of different granularity to track different kinds of dialogue states. Thus, in this paper, we will study and discuss how the context information of different granularity affects dialogue state tracking. First, we explore how greatly different granularities affect dialogue state tracking. Then, we further discuss how to combine multiple granularities for dialogue state tracking. Finally, we apply the findings about context granularity to fewshot learning scenario. Besides, we have publicly released all codes.",
    "creator" : "LaTeX with hyperref"
  }
}