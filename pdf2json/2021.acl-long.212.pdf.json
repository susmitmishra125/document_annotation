{
  "name" : "2021.acl-long.212.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Assessing the Representations of Idiomaticity in Vector Models with a Noun Compound Dataset Labeled at Type and Token Levels",
    "authors" : [ "Marcos Garcia", "Tiago Kramer Vieira", "Carolina Scarton", "Marco A. P. Idiart", "Aline Villavicencio" ],
    "emails" : [ "marcos.garcia.gonzalez@udc.gal,", "tiagokv@hotmail.com,", "a.villavicencio}@sheffield.ac.uk,", "marco.idiart@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2730–2741\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2730"
    }, {
      "heading" : "1 Introduction",
      "text" : "Multiword Expressions (MWEs) such as noun compounds (NCs), have been considered a challenge for NLP (Sag et al., 2002). This is partly due to the wide range of idiomaticity that they display, from more literal to idiomatic combinations (olive oil vs. shrinking violet). The task of identifying the degree of idiomaticity of MWEs has been investigated at type level, to determine the potential of an MWE to be idiomatic in general. Some of these approaches are based on the assumption that the\n* Equal contribution.\ndistance between the representation of an MWE as a unit and the representation of the compositional combination of its components is an indication of the degree of idiomaticity: they are closer if the MWE is more compositional. Good performances are obtained even with non-contextualised word embeddings like word2vec (Mikolov et al., 2013), and vector operations like addition and multiplication (Mitchell and Lapata, 2010; Reddy et al., 2011; Cordeiro et al., 2019). Additionally, for some MWEs, there is a potential ambiguity between an idiomatic and a literal sense, like in the potentially idiomatic MWE brass ring which can be ambiguous between the more literal meaning a ring made of brass and the more idiomatic sense of a prize. Considering that these MWEs can have both idiomatic and literal senses, a related task of token-level identification evaluates whether in a particular context an MWE is idiomatic or not. For this task, models that incorporate the context in which an MWE occurs tend to be better equipped to distinguish idiomatic from literal occurrences (Sporleder and Li, 2009; King and Cook, 2018; Salton et al., 2016).\nContextualised embedding models, like BERT (Devlin et al., 2019), brought significant advances to a variety of downstream tasks (e.g. Zhu et al. (2020) for machine translation and Jiang and de Marneffe (2019) for natural language inference). They also seem to benefit tasks like idiomaticity and metaphor identification (Gao et al., 2018), since their interpretation is often dependent on contextual clues. Nonetheless, previous work found that non-contextualised models seem to still bring informative clues for these tasks (King and Cook, 2018), and their combination with contextualised models could improve results (e.g. for metaphor identification (Mao et al., 2019)). This complementarity between non-contextualised and contex-\ntualised models may be an indication that enough core idiomatic information may already be available at type level. Moreover, type-based compositionality prediction measures that perform well with static embeddings may also perform well for token-based prediction with contextualised models.\nTo address these questions, in this paper, we present the Noun Compound Type and Token Idiomaticity (NCTTI) dataset, containing 280 NCs in English and 180 in Portuguese, annotated with the degree of idiomaticity perceived by human annotators, at type and token level.1 NCTTI contains a total of 8,725 annotations in 840 different sentences in English, and 5,091 annotations in 540 sentences in Portuguese. Moreover, NCTTI has several paraphrases for each NC which are classified as either type level or token level equivalents. To control for the level of idiomaticity, the NCTTI dataset has a balanced amount of compositional, partly compositional and idiomatic items. As the importance of context to determine interpretation may be related to factors like the degree of idiomaticity, association strength or the frequency of an NC, we present an illustrative analysis of their impact for the performance of different models in capturing idiomaticity. We also examine how the performance obtained for human idiomaticity judgments per type differs from the performance obtained per token.\nOur contributions can be summarised as: (1) building the NCTTI dataset with information about type and token idiomaticity for NCs in two languages, (2) evaluating to what extent models are able to detect idiomaticity at type and token level, analysing different levels of contextualisation and (3) proposing two new measures of idiomaticity. Moreover, the paraphrases provided for each NC at type and token level make NCTTI a useful resource for enhancing paraphrase datasets (e.g. PPDB (Ganitkevitch et al., 2013)), for tasks involving lexical substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010), or for improving the results of downstream tasks, such as text simplification (Paetzold, 2016; Alva-Manchego et al., 2020). Such paraphrases may also be useful for improving the task of machine translation, avoiding the need for parallel MWE corpora (Zaninello and Birch, 2020).\nSection 2 gives an overview of existing idiomaticity datasets. Section 3 presents the NCTTI dataset and the annotations, and section 4 discusses\n1Type level annotations come from Cordeiro et al. (2019), the dataset used as source for the NCTTI.\nthe evaluation of the performance of different word embeddings in detecting idiomaticity."
    }, {
      "heading" : "2 Related Work",
      "text" : "Datasets with type-level annotations are available for NCs in English (Farahmand et al., 2015; Reddy et al., 2011; Ramisch et al., 2016; Kruszewski and Baroni, 2014), German (Roller et al., 2013; Schulte im Walde et al., 2016), French (Cordeiro et al., 2019) and Portuguese (Cordeiro et al., 2019). However, datasets with idiomatic information at token level are scarce, e.g., the VNC-Tokens (Cook et al., 2008), containing almost 3k annotations for 53 Verb-Noun Combinations in English.\nRegarding the use of contextualised embeddings to model idiomaticity, Nandakumar et al. (2019) compared different static and contextualised embeddings to predict the NCs compositionality, obtaining better results with static vectors learnt individually for each NC. Shwartz and Dagan (2019) train various classifiers initialised with static and contextualised embeddings for different compositional tasks, achieving the best results with BERT embeddings. Yu and Ettinger (2020), using partially idiomatic expressions of the BiRD dataset (Asaadi et al., 2019), show that contextualised embeddings from language models heavily rely on word content, missing additional information provided by compositional operations.\nIn this paper we take advantage of the NCTTI dataset to observe whether vector representations obtained with different strategies correlate with human annotations at both type and token levels."
    }, {
      "heading" : "3 The Noun Compound Type and Token Idiomaticity dataset",
      "text" : "This section describes the procedure to create the NCTTI dataset and its main characteristics.2"
    }, {
      "heading" : "3.1 Source data",
      "text" : "We used as basis the English and Portuguese subsets of the NC Compositionality dataset (Cordeiro et al., 2019), which contain compositionality scores for 280 two-word NCs in English (90 of which came from Reddy et al. (2011)), and 180 in Portuguese, all of them labeled at type level: i.e., the annotators provided a compositionality value for a compound (from 0 –fully idiomatic– to 5, fully\n2The NCCTI dataset can be downloaded from the following url: https://github.com/marcospln/nctti.\ncompositional) after reading various sentences with this NC.\nTo obtain more fine-grained compatible tokenlevel annotations about the impact of different contexts in the interpretation of NCs, we used the same original sentences as in the source dataset (three sentences per compound with the same sense were selected from Reddy et al. (2011) dataset).3\nLanguage experts classified each noun compound regarding their semantic compositionality as idiomatic (e.g., gravy train), partially idiomatic (e.g., grandfather clock), or compositional (e.g., research project). For English, this resulted in 103, 88, and 89 idiomatic, partially idiomatic, and compositional compounds. For Portuguese, each class has 60 compounds, as the selection had been balanced when the source dataset was created."
    }, {
      "heading" : "3.2 Annotation procedure",
      "text" : "We used the same protocol as Reddy et al. (2011) and Cordeiro et al. (2019), asking each participant to give 0 to 5 scores for an NC and its components in a specific sentence (e.g., glass ceiling in “Women are continuing to slowly break through the glass ceiling of UK business [. . . ]”). In particular, we asked participants for: (i) the contribution of the head to the meaning of the NC (e.g., is a glass ceiling literally a ceiling?); (ii) the contribution of the modifier to the meaning of the NC (e.g., is a glass ceiling literally of glass?); and (iii) the degree of compositionality of the compound (i.e., to what extent the meaning of the NC can be seen as a combination of its parts). Additionally, we asked for up to three synonyms of the NC in that particular sentence (e.g., synonyms at token level).\nWe used Amazon Mechanical Turk to obtain the annotations for English, and a dedicated online platform for the questionnaire in Portuguese,4 as we could not find a suitable number of annotators for this language in AMT.5 Taking this into account, the numbers of the Portuguese annotations are in general lower to those obtained for English.\nFor each language, we have included the three sentences of every compound in the dataset (840 sentences in English, and 540 in Portuguese), which were randomly submitted to the annotators.\n3Some contexts are spans of tokens instead of sentences, but usually enough to interpret the meaning of the NC.\n4The platform was provided by Cordeiro et al. (2019). 5The annotation process was approved by the Ethics Committee of the University of Sheffield. This is a thorough evaluation process peer-reviewed by three ethical reviewers. The monetary compensation was deemed appropriate for the task.\nFor English, we compiled at least 10 annotations per sentence, resulting in 8,725 annotations (10.4 annotations per sentence on average). A total of 412 annotators have taken part in the process, and on average, each participant labeled 21 instances. For Portuguese we set the threshold in 5 annotations per sentence: we got 5,091 annotations by 33 participants, so that each sentence has a mean of 9.4 annotations and each annotator labeled on average 154 sentences."
    }, {
      "heading" : "3.3 Results",
      "text" : "Inter-annotator agreement: we computed the inter-annotator agreements for two and three annotators with the largest number of sentences in common (Table 1). For English, we obtained Krippendorff’s α (Krippendorff, 2011) values of 0.30 for two annotators (199 sentences) and 0.22 for three annotators (76 sentences). The α values for Portuguese were of 0.52 for two annotators (131 sentences) and 0.44 for three annotators (60 sentences). Overall, and using the divisions proposed by Landis and Koch (1977), the agreement results can be classified as ‘fair’ (for English), and ‘moderate’ (for Portuguese).\nCorrelation token vs. type scores: then, we calculated the correlations (Spearman ρ) between the average compositionality scores of the NCTTI\ndataset and those of the original resource (NC Compositionality dataset). Table 2 contains the correlation results for each language and compositionality class. The strong to very strong significant correlations confirm the robustness between type-level and token-level human compositionality annotations for these two datasets.6\nIdiomaticity values: with regards to the idiomaticity values of each class, Table 3 displays both the average scores and the standard deviation in both languages. As expected, for the whole compounds, partially idiomatic NCs are those with higher standard deviations, and their mean compositionality values are in the middle of the scale (2.34 and 2.46). In English, the results of both idiomatic and compositional compounds are more homogeneous, as they are clearly located on the margins of the scale (< 1 and > 4, respectively) with lower deviations. This is not the case in Portuguese, where the average values are > 1 and < 4 for idiomatic and compositional NCs, respectively, placing even the idiomatic cases closer towards the middle of the scale. With respect to the average values for the heads and modifiers, we can highlight the following observations: first, both head and modifier scores are consistently higher than the means for the whole compound in every scenario also suggesting at least a partial compositionality in their token occurrences. Second, for idiomatic NCs, the scores of the modifiers are higher than those of the heads, while for partially compositional NCs the results are the opposite.7 Finally, regarding the compositional level, the modifier values are higher in English, while in Portuguese the heads seem to contribute more to the meaning of the NC.\n6Removing annotators with low agreement (Spearman ρ < 0.2, and ρ < 0.4) resulted in almost identical correlations.\n7The results for partially idiomatic compounds are expected to some extent as the head tends to bear more semantic load about the whole expression (e.g., as in collocations).\nObserving the variability across the annotations, we found some divergence in a few compounds (e.g., brass ring labeled as idiomatic for a compositional occurrence “Three drawers, each with a brass ring pull, provide plenty of storage whatever you use it for.”), which hints at possible interference from a salient meaning (Giora, 1999). However, further investigation is needed.\nParaphrases: as mentioned, we asked the participants to provide synonyms or paraphrases for the noun compounds in each particular context. In this respect, it is worth noting that while some suggestions may be applicable across all the sentences for an NC (e.g. spun sugar for cotton candy, considered as a type level synonym), others are more dependent on context and differ for specific sentences (e.g. flight recorder and unknown process, for black box, which can be considered as token level paraphrases). We have classified the paraphrases as type or token level using the following procedure: to organise the large set of paraphrases provided by the annotators (see below), we performed an automatic classification as follows: we labeled as type level synonyms those paraphrases proposed for the three sentences of each compound, and those suggested for two sentences with a frequency >= 3; token level synonyms are those proposed only for one sentence with a frequency >= 2.\nIn English, 9,690 different paraphrases were proposed by the annotators (average 34.60 per NC), and 3,554 were suggested by at least 5 participants (average of 12.70 per NC). Out of them, 1,506 were classified as type level (5.4 synonyms per NC, on average), and 353 at token level (0.42 per sentence, 1.3 per NC). Overall, 118 NCs have token level synonyms for one sentence, 69 for two sentences, and 16 for the three sentences.\nFor Portuguese, the annotators suggested a total of 6,579 paraphrases (314 by at least 5 participants\nand 764 by >= 3, average of 4.2 per NC). 743 synonyms were proposed for the 180 compounds (an average of 4.1 per NC), being classified as type level. Concerning token level synonyms, we have collected 192 synonyms (1.1 per NC, on average). In this case the total number of annotations was lower, and the final resource contains 61 NCs with token level synonyms for one sentence, 38 for two sentences, and 6 compounds have token level synonyms for the three sentences.\nThe collection of paraphrases included in the NCTTI make this dataset a valuable resource for different evaluations, such as lexical substitution tasks and assessments of the performance of embedding models to correctly identify contextualised synonyms of NCs with different degrees of idiomaticity.\nTable 4 shows an annotation example for the NC disc jockey, in English. It includes the three sentences together with the average idiomaticity score and both token-level and type-level paraphrases."
    }, {
      "heading" : "4 Experiments",
      "text" : "This section displays some of the comparative analyses for the relevance of type and token annotation for idiomaticity detection. First, we adapt the type level compositionality prediction approaches used on static word vectors (Mitchell and Lapata, 2010) to contextualised models (Nandakumar et al., 2019), here computing the correlation also at token level. In particular, the assumption is that compositionality can be approximated as the distance between the representation for an NC and the representation for the compositional combination of its individual components. Then, we measure whether the vector representations reflect the variability of the human annotators, who capture different nuances of the NCs depending on the sentences in which they occur. Similarly, in a third experiment we use the standard deviations of the idiomaticity scores in the three contexts to observe how the\ninterpretation of the NCs varies across sentences, and whether this correlates with the contextualised representations produced by various models. More specifically, we assume that, if models adequately incorporate contextual information, the standard deviations of the similarities between the NCs in different contexts should be correlated with those of the human annotators."
    }, {
      "heading" : "4.1 Models",
      "text" : "We evaluate four contextualised models: three BERT variants, based on the Transformers architecture (Vaswani et al., 2017), and ELMo, which learns word vectors using bidirectional LSTMs (Peters et al., 2018). For English we used the ELMo small model provided by Peters et al. (2018), BERT-Large uncased (Devlin et al., 2019), DistilBERT (Sanh et al., 2019), based on BERT-Base and distilled on SQuAD dataset, and SentenceBERT (Reimers and Gurevych, 2019), trained on BERT-Large and both MultiNLI and SNLI.8 For Portuguese we selected the ELMo pre-trained weights provided by Quinta de Castro et al. (2018) and the multilingual versions of the models used for English, namely mBERT (base cased), and both multilingual DistilBERT and Sentence-BERT (Reimers and Gurevych, 2020). As a static noncontextualised baseline we used GloVe (Pennington et al., 2014) (the English official models with 300 dimensions and trained on 840 billion tokens, and the equivalent Portuguese model released by Hartmann et al. (2017)). The vector representations were obtained with the flairNLP framework (Akbik et al., 2019) using the models provided by the transformers library (Wolf et al., 2020).\nThe representations of NCs (and their sentences) were obtained by averaging the word (or subword, if adopted by the model) embeddings. We used the concatenation of the three layers for ELMo and of\n8https://www.nyu.edu/projects/bowman/ multinli/ https://nlp.stanford.edu/projects/snli/\nthe last four hidden layers for the BERT models. In GloVe, words which are not in the vocabulary were skipped."
    }, {
      "heading" : "4.2 Experiment 1: Compositionality prediction",
      "text" : "Unsupervised type idiomaticity identification with static non-contextualised word embeddings often assumes that the similarity between the NC embedding and the compositional embedding of the component words (e.g. police car vs. police and car) is an indication of idiomaticity (Mitchell and Lapata, 2010): the more similar they are the more compositional the NC is. To approximate this with contextualised models, we calculate the cosine similarities between the contextualised vector of the NC in each sentence with two types of noncontextualised vectors. The first evaluates if even in the absence of an informative sentence context, each of the component words would be enough of a trigger to cue the NC meaning (e.g. eager for eager beaver). This is implemented as the vector for the NC out of context, obtained by feeding the model only with the compound, dubbed NC out.9 The second non-contextualised vector evaluates if the representations for the individual words have enough information to reconstruct the meaning of the NC in the absence of context and of the collocated component. It is implemented as the sum of the individual vectors of the NC components, where each NC component is fed individually to the model as a sentence, referred to as NC outComp. On each case, we calculate two Spearman correlations with human judgments: at token level, using all the sentences for each language; and at type level, comparing the average cosine similarities of each NC with their compositionality scores at type level. We also compute correlations between the similarities and frequency-based data, namely the NC raw frequency, and the PPMI (Church and Hanks, 1990) between its component words, to verify whether they have any impact in these measures of idiomaticity. The frequency data were obtained from ukWaC, with 2.25B tokens in English (Baroni et al., 2009), and brWaC, containing 2.7B tokens in Portuguese (Wagner Filho et al., 2018).\nThe results by Cordeiro et al. (2019) suggested that if the two components of an NC are processed as a single token unit (for instance, by explic-\n9This representation equivalent to the Avg Phrase used by Yu and Ettinger (2020).\nitly linking them with an underscore) the resulting static representation captures the NC idiomatic meaning. This is not surprising since by linking the two components we create a new word that would be treated by the model as completely independent of the preexisting component words. But such preprocessing may not be desirable or even feasible. In this sense the contextualised models would be a good promise, since we expected that by processing a sentence with an idiomatic NC, the context would be enough to lead the model into linking the component words and assigning the corresponding idiomatic meaning. Figuratively speaking, the contextualised models would put the underscore for us. Therefore, if contextualised models capture idiomaticity, the similarity between NC and NC outComp (or NC out) should have strong correlations with the idiomaticity scores of the NCs.\nTable 5 shows the significant correlations in English (top rows) and Portuguese (bottom). These results indicate at best weak (NC outComp) to moderate (NC out) correlations between models’ predictions and human judgments, both at type and token levels. Moreover, the correlations obtained are much smaller than those found by the static models used by Cordeiro et al. (2019). For English, the best correlations (0.37) were obtained by BERT, while ELMo and Sentence-BERT achieved the best performance in Portuguese (0.27 and 0.26, respectively). In both languages, the lower values were those of DistilBERT. It is worth noting that a direct comparison between the BERT models in both languages should not be done, as they are monolingual (for English) and multilingual (for Portuguese).\nFor PPMI, only weak positive correlations were found for ELMo and DistilBERT, indicating that for them higher cosine values weakly imply NCs with stronger association scores. Moreover, weak to moderate negative correlations with frequency were found for the BERT models, suggesting that cosine similarity is higher for less frequent NCs. The differences between NC out and NC outComp indicate the importance of some degree of contextualisation (also found by Yu and Ettinger (2020)), even if only as one component contextualising the other in NC out, which may not be retrievable from the combination of the context-independent vectors of the components (NC outComp). This is in line with the original strategy used with static embeddings, which learns the distribution of the NCs pre-identified as single tokens in corpora and that\nresulted in significantly better correlations per type than any of the contextualised models (Cordeiro et al., 2019).\nTo make a fairer comparison between both approaches, we injected into the BERT models single representations for the NCs, learnt from the referred ukWaC and brWaC corpora. We first annotated as single tokens in the corpus those NCs present in the dataset, and used attentive mimicking with one-token-approximation (Schick and Schütze, 2019, 2020b) to learn up to 500 contexts for each compound. After that, we injected these type level vectors into the BERT models using BERTRAM (Schick and Schütze, 2020a). For English, these new representations obtained lower results than the original BERT in NC out (e.g., 0.37 vs. 0.28 at type level), but higher in NC outComp (0.16 vs. 0.33 at type level). For Portuguese, including single representations for the NCs in BERT improved the correlations in three of the four scenarios (except for NC out at token level), but the best results were almost identical to those of ELMo (see the full results in the bottom rows of Table 5).\nRegarding the results reported by Nandakumar et al. (2019), for English, our experiments yielded higher correlations for BERT and lower for ELMo (≈ 0.3 in both cases, depending on the setting), which may be due to differences in how the vectors are generated (e.g., the use of different input sentences, hidden layers or compositional operations).\nIn sum, the results of these evaluations suggest that the use of a straightforward adaptation of a compositionality prediction approach that led to good performance with static models was not as successful with contextualised models."
    }, {
      "heading" : "4.3 Experiment 2: Investigating idiomaticity with word embedding models",
      "text" : "We analyse whether models are able to capture differences in idiomaticity perceived by human annotators across the sentences in which an NC occurs. That is, if an NC is found to be more idiomatic in one sentence than in others. For that, we created an annotator’s vector for each sentence, combining the human scores to create a three dimensional vector representation, where the first dimension is the average NC compositionality, and the second and third are the average scores of the contributions of the head and of the modifier. For representing the sentence we obtain an embedding by averaging their (sub)words. We calculated the Euclidean\ndistances between (i) the annotators’ vectors and (ii) the cosine similarities between sentence embeddings of each of the possible combinations of the three sentences associated to each NC. Then, we measured the correlations between these values using Spearman ρ. We aim to assess if annotations and models indicate the same relative differences.10 The results were averaged for the 280 (English) and 180 (Portuguese) NCs.\nTable 6 shows the results for the whole datasets and divided by compositionality level. As we compare Euclidean distances with cosine similarities negative values are actually positive correlations and vice versa. The average ρ is close to 0 suggesting that the embedding models do not capture the nuances in idiomaticity perceived by the annotators between the different sentences per NC."
    }, {
      "heading" : "4.4 Experiment 3: NC idiomaticity across sentences",
      "text" : "We also analysed the similarity among the annotations for each NC in the three sentences, computing the standard deviations of the average compositionality scores given by the annotators. In contrast to the previous experiment, here we represent the human annotations using only the idiomaticity scores of the whole NCs and the models’ output as the contextualised embedding of the NCs in each sentence. At token level most compounds (85.7% in English and 91.1% in Portuguese) have mean idiomaticity scores with less than 0.6 of standard deviation. Very few NCs have deviations higher than 1: five in English and four in Portuguese. Looking at the contexts in which they occur, the variability seems to be due to the different topics to which the sentences refer. For instance, the annotators have identified two senses of firing line: one, more idiomatic, referring to a position in which someone is criticised (mean score of 1.25), and a second one (partially compositional, with an average of 2.7) referring to a specific position in an armed conflict. In Portuguese, céu aberto (‘open-air’, lit. ‘open-sky’) was interpreted as less compositional (1.2) when describing urban settings (e.g., open-air shopping centers) than when referring to wild places (e.g., lobas que lutavam a céu aberto, ‘wolves fighting in the open’), with a mean idiomaticity score of 3.\n10Spearman ρ is not used here as a statistical test but as a measure to evaluate if the sentence comparisons with two different metrics yield the same relative differences. As there are only three sentences to compare, ρ assumes only four values ±0.5 or ±1.\nTo observe whether language models capture these differences across sentences, we calculated the cosine similarities between the NCs in the three\nsentences and the standard deviation of these three values. We then computed the Spearman correlations between these deviations obtained from the\nmodels’ representations and those of the human annotations: all correlations were very low and not significant, suggesting that the vector representations do not capture the variability perceived by the annotators. Finally, we have also selected two NCs in English with a combination of idiomatic and compositional meanings (brick wall, and gold mine). In these examples, we found that for BERT (our best model) the cosine similarities between the idiomatic meanings were higher (0.83 in both cases) than between idiomatic and compositional senses (0.68 and 0.7, respectively), suggesting that they are somehow identifying the different senses. However, since the highest standard deviations were achieved with NCs representing the same sense in all contexts (e.g., big wig and grass root), further analysis is needed.\nAs neither the cosine similarities obtained with BERT-based models nor the standard deviations between them were correlated with the variation in the human scores, these analyses suggest that stateof-the-art contextualised models still do not model semantic compositionality as human annotators do.\nThe experiments performed in this section have shown, on the one hand, some of the possibilities of a multilingual dataset labeled at type and token level; on the other hand, the results also suggest that capturing idiomaticity is a hard task for current language models, as only some of them show moderate correlations with human annotations in some scenarios."
    }, {
      "heading" : "5 Conclusions and Future Work",
      "text" : "This paper presented the NCTTI, a dataset of NCs in English and Portuguese annotated at type and token level with human judgments about idiomaticity, and with suggestions of paraphrases. The very strong correlations found between type and token judgments confirm the robustness of the scores, while the paraphrases provide further validation of the interpretation of the NCs.\nMoreover, evaluations involving embedding models with different levels of contextualisation suggest that they are still far from providing accurate estimates of NC idiomaticity, at least using the measures proposed and analysed in the paper. MWEs are still a pain in the neck for NLP, and datasets like the NCTTI can contribute towards finding better representations for them and better measures for idiomaticity identification.\nFuture work includes using these NCs as seeds\nin cross-lingual representations for enriching the dataset with NC equivalents in different languages. Besides, we also plan to enlarge the datasets including a subset of sentences with ambiguous NCs having idiomatic and compositional interpretations depending on the context."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Aline Villavicencio and Carolina Scarton are funded by the EPSRC project MIA: Modeling Idiomaticity in Human and Artificial Language Processing (EP/T02450X/1). Marcos Garcia is funded by the Consellerı́a de Cultura, Educación e Ordenación Universitaria of the Galician Government (ERDF 2014-2020: Call ED431G 2019/04), and by a Ramón y Cajal grant (RYC2019-028473-I)."
    } ],
    "references" : [ {
      "title" : "FLAIR: An easy-to-use framework for state-of-theart NLP",
      "author" : [ "Alan Akbik", "Tanja Bergmann", "Duncan Blythe", "Kashif Rasul", "Stefan Schweter", "Roland Vollgraf." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Akbik et al\\.,? 2019",
      "shortCiteRegEx" : "Akbik et al\\.",
      "year" : 2019
    }, {
      "title" : "Data-driven sentence simplification: Survey and benchmark",
      "author" : [ "Fernando Alva-Manchego", "Carolina Scarton", "Lucia Specia." ],
      "venue" : "Computational Linguistics, 46(1):135–187.",
      "citeRegEx" : "Alva.Manchego et al\\.,? 2020",
      "shortCiteRegEx" : "Alva.Manchego et al\\.",
      "year" : 2020
    }, {
      "title" : "Big BiRD: A large, fine-grained, bigram relatedness dataset for examining semantic composition",
      "author" : [ "Shima Asaadi", "Saif Mohammad", "Svetlana Kiritchenko." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Asaadi et al\\.,? 2019",
      "shortCiteRegEx" : "Asaadi et al\\.",
      "year" : 2019
    }, {
      "title" : "The WaCky wide web: a collection of very large linguistically processed webcrawled corpora",
      "author" : [ "Marco Baroni", "Silvia Bernardini", "Adriano Ferraresi", "Eros Zanchetta." ],
      "venue" : "Language resources and evaluation, 43(3):209–226.",
      "citeRegEx" : "Baroni et al\\.,? 2009",
      "shortCiteRegEx" : "Baroni et al\\.",
      "year" : 2009
    }, {
      "title" : "Portuguese Named Entity Recognition Using LSTMCRF",
      "author" : [ "Pedro Vitor Quinta de Castro", "Nádia Félix Felipe da Silva", "Anderson da Silva Soares." ],
      "venue" : "Proceedings of the 13th International Conference on the Computational Processing of the Por-",
      "citeRegEx" : "Castro et al\\.,? 2018",
      "shortCiteRegEx" : "Castro et al\\.",
      "year" : 2018
    }, {
      "title" : "Word association norms, mutual information, and lexicography",
      "author" : [ "Kenneth Ward Church", "Patrick Hanks." ],
      "venue" : "Computational Linguistics, 16(1):22–29.",
      "citeRegEx" : "Church and Hanks.,? 1990",
      "shortCiteRegEx" : "Church and Hanks.",
      "year" : 1990
    }, {
      "title" : "The VNC-tokens dataset",
      "author" : [ "Paul Cook", "Afsaneh Fazly", "Suzanne Stevenson." ],
      "venue" : "Proceedings of the LREC Workshop Towards a Shared Task for Multiword Expressions (MWE 2008), pages 19–22.",
      "citeRegEx" : "Cook et al\\.,? 2008",
      "shortCiteRegEx" : "Cook et al\\.",
      "year" : 2008
    }, {
      "title" : "Unsupervised compositionality prediction of nominal compounds",
      "author" : [ "Silvio Cordeiro", "Aline Villavicencio", "Marco Idiart", "Carlos Ramisch." ],
      "venue" : "Computational Linguistics, 45(1):1–57.",
      "citeRegEx" : "Cordeiro et al\\.,? 2019",
      "shortCiteRegEx" : "Cordeiro et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "A multiword expression data set: Annotating non-compositionality and conventionalization for English noun compounds",
      "author" : [ "Meghdad Farahmand", "Aaron Smith", "Joakim Nivre." ],
      "venue" : "Proceedings of the 11th Workshop on Multiword Expressions, pages",
      "citeRegEx" : "Farahmand et al\\.,? 2015",
      "shortCiteRegEx" : "Farahmand et al\\.",
      "year" : 2015
    }, {
      "title" : "PPDB: The paraphrase database",
      "author" : [ "Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Ganitkevitch et al\\.,? 2013",
      "shortCiteRegEx" : "Ganitkevitch et al\\.",
      "year" : 2013
    }, {
      "title" : "Neural metaphor detection in context",
      "author" : [ "Ge Gao", "Eunsol Choi", "Yejin Choi", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 607–613, Brussels, Belgium. Association for Com-",
      "citeRegEx" : "Gao et al\\.,? 2018",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2018
    }, {
      "title" : "On the priority of salient meanings: Studies of literal and figurative language",
      "author" : [ "Rachel Giora." ],
      "venue" : "Journal of pragmatics, 31(7):919–929.",
      "citeRegEx" : "Giora.,? 1999",
      "shortCiteRegEx" : "Giora.",
      "year" : 1999
    }, {
      "title" : "Portuguese word embeddings: Evaluating on word analogies and natural language tasks",
      "author" : [ "Nathan Hartmann", "Erick Fonseca", "Christopher Shulby", "Marcos Treviso", "Jéssica Silva", "Sandra Aluı́sio" ],
      "venue" : "In Proceedings of the 11th Brazilian Symposium in In-",
      "citeRegEx" : "Hartmann et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Hartmann et al\\.",
      "year" : 2017
    }, {
      "title" : "Evaluating BERT for natural language inference: A case study on the CommitmentBank",
      "author" : [ "Nanjiang Jiang", "Marie-Catherine de Marneffe." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Jiang and Marneffe.,? 2019",
      "shortCiteRegEx" : "Jiang and Marneffe.",
      "year" : 2019
    }, {
      "title" : "Leveraging distributed representations and lexico-syntactic fixedness for token-level prediction of the idiomaticity of English verb-noun combinations",
      "author" : [ "Milton King", "Paul Cook." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association",
      "citeRegEx" : "King and Cook.,? 2018",
      "shortCiteRegEx" : "King and Cook.",
      "year" : 2018
    }, {
      "title" : "Computing Krippendorff’s Alpha-Reliability",
      "author" : [ "Klaus Krippendorff." ],
      "venue" : "Postprint version. Retrieved from http://repository.upenn.edu/ asc_papers/43.",
      "citeRegEx" : "Krippendorff.,? 2011",
      "shortCiteRegEx" : "Krippendorff.",
      "year" : 2011
    }, {
      "title" : "Dead parrots make bad pets: Exploring modifier effects in noun phrases",
      "author" : [ "Germán Kruszewski", "Marco Baroni." ],
      "venue" : "Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 171–181, Dublin, Ireland. As-",
      "citeRegEx" : "Kruszewski and Baroni.,? 2014",
      "shortCiteRegEx" : "Kruszewski and Baroni.",
      "year" : 2014
    }, {
      "title" : "The Measurement of Observer Agreement for Categorical Data",
      "author" : [ "J Richard Landis", "Gary G Koch." ],
      "venue" : "Biometrics, 33:159–174.",
      "citeRegEx" : "Landis and Koch.,? 1977",
      "shortCiteRegEx" : "Landis and Koch.",
      "year" : 1977
    }, {
      "title" : "Endto-end sequential metaphor identification inspired by linguistic theories",
      "author" : [ "Rui Mao", "Chenghua Lin", "Frank Guerin." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3888–3898, Florence, Italy. Asso-",
      "citeRegEx" : "Mao et al\\.,? 2019",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2019
    }, {
      "title" : "SemEval2007 task 10: English lexical substitution task",
      "author" : [ "Diana McCarthy", "Roberto Navigli." ],
      "venue" : "Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 48– 53, Prague, Czech Republic. Association for Compu-",
      "citeRegEx" : "McCarthy and Navigli.,? 2007",
      "shortCiteRegEx" : "McCarthy and Navigli.",
      "year" : 2007
    }, {
      "title" : "SemEval-2010 task 2: Cross-lingual lexical substitution",
      "author" : [ "Rada Mihalcea", "Ravi Sinha", "Diana McCarthy." ],
      "venue" : "Proceedings of the 5th International Workshop on Semantic Evaluation, pages 9–14, Uppsala, Sweden. Association for Computational Lin-",
      "citeRegEx" : "Mihalcea et al\\.,? 2010",
      "shortCiteRegEx" : "Mihalcea et al\\.",
      "year" : 2010
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "Proceedings of the 26th International Conference on Neural Information Processing Systems -",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Composition in distributional models of semantics",
      "author" : [ "Jeff Mitchell", "Mirella Lapata." ],
      "venue" : "Cognitive science, 34(8):1388–1429.",
      "citeRegEx" : "Mitchell and Lapata.,? 2010",
      "shortCiteRegEx" : "Mitchell and Lapata.",
      "year" : 2010
    }, {
      "title" : "How well do embedding models capture non-compositionality? a view from multiword expressions",
      "author" : [ "Navnita Nandakumar", "Timothy Baldwin", "Bahar Salehi." ],
      "venue" : "Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP,",
      "citeRegEx" : "Nandakumar et al\\.,? 2019",
      "shortCiteRegEx" : "Nandakumar et al\\.",
      "year" : 2019
    }, {
      "title" : "Lexical Simplification for Non-Native English Speakers",
      "author" : [ "Gustavo H. Paetzold." ],
      "venue" : "Ph.D. thesis, The University of Sheffield, Sheffield, UK.",
      "citeRegEx" : "Paetzold.,? 2016",
      "shortCiteRegEx" : "Paetzold.",
      "year" : 2016
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha,",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "How naked is the naked truth? a multilingual lexicon of nominal compound compositionality",
      "author" : [ "Carlos Ramisch", "Silvio Cordeiro", "Leonardo Zilio", "Marco Idiart", "Aline Villavicencio." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association",
      "citeRegEx" : "Ramisch et al\\.,? 2016",
      "shortCiteRegEx" : "Ramisch et al\\.",
      "year" : 2016
    }, {
      "title" : "An empirical study on compositionality in compound nouns",
      "author" : [ "Siva Reddy", "Diana McCarthy", "Suresh Manandhar." ],
      "venue" : "Proceedings of 5th International Joint Conference on Natural Language Processing, pages 210–218, Chiang Mai, Thailand.",
      "citeRegEx" : "Reddy et al\\.,? 2011",
      "shortCiteRegEx" : "Reddy et al\\.",
      "year" : 2011
    }, {
      "title" : "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Making monolingual sentence embeddings multilingual using knowledge distillation",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4512–4525,",
      "citeRegEx" : "Reimers and Gurevych.,? 2020",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2020
    }, {
      "title" : "The (un)expected effects of applying standard cleansing models to human ratings on compositionality",
      "author" : [ "Stephen Roller", "Sabine Schulte im Walde", "Silke Scheible." ],
      "venue" : "Proceedings of the 9th Workshop on Multiword Expressions, pages 32–41, At-",
      "citeRegEx" : "Roller et al\\.,? 2013",
      "shortCiteRegEx" : "Roller et al\\.",
      "year" : 2013
    }, {
      "title" : "Multiword expressions: A pain in the neck for NLP",
      "author" : [ "Ivan A. Sag", "Timothy Baldwin", "Francis Bond", "Ann Copestake", "Dan Flickinger." ],
      "venue" : "Proceedings of the Third International Conference on",
      "citeRegEx" : "Sag et al\\.,? 2002",
      "shortCiteRegEx" : "Sag et al\\.",
      "year" : 2002
    }, {
      "title" : "Idiom token classification using sentential distributed semantics",
      "author" : [ "Giancarlo Salton", "Robert Ross", "John Kelleher." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
      "citeRegEx" : "Salton et al\\.,? 2016",
      "shortCiteRegEx" : "Salton et al\\.",
      "year" : 2016
    }, {
      "title" : "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "arXiv preprint arXiv:1910.01108.",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "Attentive mimicking: Better word embeddings by attending to informative contexts",
      "author" : [ "Timo Schick", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Schick and Schütze.,? 2019",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2019
    }, {
      "title" : "BERTRAM: Improved word embeddings have big impact on contextualized model performance",
      "author" : [ "Timo Schick", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3996–4007, Online.",
      "citeRegEx" : "Schick and Schütze.,? 2020a",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2020
    }, {
      "title" : "Rare words: A major problem for contextualized embeddings and how to fix it by attentive mimicking",
      "author" : [ "Timo Schick", "Hinrich Schütze." ],
      "venue" : "Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence, pages 8766–8774.",
      "citeRegEx" : "Schick and Schütze.,? 2020b",
      "shortCiteRegEx" : "Schick and Schütze.",
      "year" : 2020
    }, {
      "title" : "GhoSt-NN: A representative gold standard of German noun-noun compounds",
      "author" : [ "Sabine Schulte im Walde", "Anna Hätty", "Stefan Bott", "Nana Khvtisavrishvili." ],
      "venue" : "Proceedings of the Tenth International Conference on Language Resources and",
      "citeRegEx" : "Walde et al\\.,? 2016",
      "shortCiteRegEx" : "Walde et al\\.",
      "year" : 2016
    }, {
      "title" : "Still a pain in the neck: Evaluating text representations on lexical composition",
      "author" : [ "Vered Shwartz", "Ido Dagan." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:403–419.",
      "citeRegEx" : "Shwartz and Dagan.,? 2019",
      "shortCiteRegEx" : "Shwartz and Dagan.",
      "year" : 2019
    }, {
      "title" : "Unsupervised recognition of literal and non-literal use of idiomatic expressions",
      "author" : [ "Caroline Sporleder", "Linlin Li." ],
      "venue" : "Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 754–762, Athens, Greece. Association",
      "citeRegEx" : "Sporleder and Li.,? 2009",
      "shortCiteRegEx" : "Sporleder and Li.",
      "year" : 2009
    }, {
      "title" : "Attention Is All You Need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "ArXiv preprint arXiv:1706.03762.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "The brWaC corpus: A new open resource for Brazilian Portuguese",
      "author" : [ "Jorge A. Wagner Filho", "Rodrigo Wilkens", "Marco Idiart", "Aline Villavicencio." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC",
      "citeRegEx" : "Filho et al\\.,? 2018",
      "shortCiteRegEx" : "Filho et al\\.",
      "year" : 2018
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Assessing phrasal representation and composition in transformers",
      "author" : [ "Lang Yu", "Allyson Ettinger." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4896–4907, Online. Association for Computa-",
      "citeRegEx" : "Yu and Ettinger.,? 2020",
      "shortCiteRegEx" : "Yu and Ettinger.",
      "year" : 2020
    }, {
      "title" : "Multiword expression aware neural machine translation",
      "author" : [ "Andrea Zaninello", "Alexandra Birch." ],
      "venue" : "Proceedings of the 12th Language Resources and Evaluation Conference, pages 3816–3825, Marseille, France. European Language Resources Asso-",
      "citeRegEx" : "Zaninello and Birch.,? 2020",
      "shortCiteRegEx" : "Zaninello and Birch.",
      "year" : 2020
    }, {
      "title" : "Incorporating BERT into Neural Machine Translation",
      "author" : [ "Jinhua Zhu", "Yingce Xia", "Lijun Wu", "Di He", "Tao Qin", "Wengang Zhou", "Houqiang Li", "Tie-Yan Liu." ],
      "venue" : "Proceedings of the Eighth International Conference on Learning Representations, Ad-",
      "citeRegEx" : "Zhu et al\\.,? 2020",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 33,
      "context" : "Multiword Expressions (MWEs) such as noun compounds (NCs), have been considered a challenge for NLP (Sag et al., 2002).",
      "startOffset" : 100,
      "endOffset" : 118
    }, {
      "referenceID" : 22,
      "context" : "Good performances are obtained even with non-contextualised word embeddings like word2vec (Mikolov et al., 2013), and vector operations like addition and multiplication (Mitchell and Lapata, 2010; Reddy et al.",
      "startOffset" : 90,
      "endOffset" : 112
    }, {
      "referenceID" : 41,
      "context" : "this task, models that incorporate the context in which an MWE occurs tend to be better equipped to distinguish idiomatic from literal occurrences (Sporleder and Li, 2009; King and Cook, 2018; Salton et al., 2016).",
      "startOffset" : 147,
      "endOffset" : 213
    }, {
      "referenceID" : 15,
      "context" : "this task, models that incorporate the context in which an MWE occurs tend to be better equipped to distinguish idiomatic from literal occurrences (Sporleder and Li, 2009; King and Cook, 2018; Salton et al., 2016).",
      "startOffset" : 147,
      "endOffset" : 213
    }, {
      "referenceID" : 34,
      "context" : "this task, models that incorporate the context in which an MWE occurs tend to be better equipped to distinguish idiomatic from literal occurrences (Sporleder and Li, 2009; King and Cook, 2018; Salton et al., 2016).",
      "startOffset" : 147,
      "endOffset" : 213
    }, {
      "referenceID" : 8,
      "context" : "Contextualised embedding models, like BERT (Devlin et al., 2019), brought significant advances to a variety of downstream tasks (e.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 11,
      "context" : "They also seem to benefit tasks like idiomaticity and metaphor identification (Gao et al., 2018), since their interpretation is often dependent on contextual clues.",
      "startOffset" : 78,
      "endOffset" : 96
    }, {
      "referenceID" : 15,
      "context" : "Nonetheless, previous work found that non-contextualised models seem to still bring informative clues for these tasks (King and Cook, 2018), and their combination with contextualised models could improve results (e.",
      "startOffset" : 118,
      "endOffset" : 139
    }, {
      "referenceID" : 10,
      "context" : "PPDB (Ganitkevitch et al., 2013)), for tasks involving lexical substitution (McCarthy and Navigli, 2007; Mihalcea et al.",
      "startOffset" : 5,
      "endOffset" : 32
    }, {
      "referenceID" : 20,
      "context" : ", 2013)), for tasks involving lexical substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010), or for improving the results of downstream tasks, such as text simplification (Paetzold, 2016; Alva-Manchego et al.",
      "startOffset" : 51,
      "endOffset" : 102
    }, {
      "referenceID" : 21,
      "context" : ", 2013)), for tasks involving lexical substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010), or for improving the results of downstream tasks, such as text simplification (Paetzold, 2016; Alva-Manchego et al.",
      "startOffset" : 51,
      "endOffset" : 102
    }, {
      "referenceID" : 25,
      "context" : ", 2010), or for improving the results of downstream tasks, such as text simplification (Paetzold, 2016; Alva-Manchego et al., 2020).",
      "startOffset" : 87,
      "endOffset" : 131
    }, {
      "referenceID" : 1,
      "context" : ", 2010), or for improving the results of downstream tasks, such as text simplification (Paetzold, 2016; Alva-Manchego et al., 2020).",
      "startOffset" : 87,
      "endOffset" : 131
    }, {
      "referenceID" : 46,
      "context" : "Such paraphrases may also be useful for improving the task of machine translation, avoiding the need for parallel MWE corpora (Zaninello and Birch, 2020).",
      "startOffset" : 126,
      "endOffset" : 153
    }, {
      "referenceID" : 32,
      "context" : ", 2016; Kruszewski and Baroni, 2014), German (Roller et al., 2013; Schulte im Walde et al., 2016), French (Cordeiro et al.",
      "startOffset" : 45,
      "endOffset" : 97
    }, {
      "referenceID" : 7,
      "context" : ", 2016), French (Cordeiro et al., 2019) and Portuguese (Cordeiro et al.",
      "startOffset" : 16,
      "endOffset" : 39
    }, {
      "referenceID" : 6,
      "context" : ", the VNC-Tokens (Cook et al., 2008), containing almost 3k annotations for 53 Verb-Noun Combinations in English.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 2,
      "context" : "Yu and Ettinger (2020), using partially idiomatic expressions of the BiRD dataset (Asaadi et al., 2019), show that contextualised embeddings from language models heavily rely on",
      "startOffset" : 82,
      "endOffset" : 103
    }, {
      "referenceID" : 7,
      "context" : "We used as basis the English and Portuguese subsets of the NC Compositionality dataset (Cordeiro et al., 2019), which contain compositionality scores for 280 two-word NCs in English (90 of which came from Reddy et al.",
      "startOffset" : 87,
      "endOffset" : 110
    }, {
      "referenceID" : 16,
      "context" : "For English, we obtained Krippendorff’s α (Krippendorff, 2011) values of 0.",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 23,
      "context" : "First, we adapt the type level compositionality prediction approaches used on static word vectors (Mitchell and Lapata, 2010) to contextualised models (Nandakumar et al.",
      "startOffset" : 98,
      "endOffset" : 125
    }, {
      "referenceID" : 42,
      "context" : "tecture (Vaswani et al., 2017), and ELMo, which learns word vectors using bidirectional LSTMs (Peters et al.",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 27,
      "context" : ", 2017), and ELMo, which learns word vectors using bidirectional LSTMs (Peters et al., 2018).",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 8,
      "context" : "(2018), BERT-Large uncased (Devlin et al., 2019), Distil-",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 35,
      "context" : "BERT (Sanh et al., 2019), based on BERT-Base and distilled on SQuAD dataset, and SentenceBERT (Reimers and Gurevych, 2019), trained on BERT-Large and both MultiNLI and SNLI.",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 30,
      "context" : ", 2019), based on BERT-Base and distilled on SQuAD dataset, and SentenceBERT (Reimers and Gurevych, 2019), trained on BERT-Large and both MultiNLI and SNLI.",
      "startOffset" : 77,
      "endOffset" : 105
    }, {
      "referenceID" : 31,
      "context" : "(2018) and the multilingual versions of the models used for English, namely mBERT (base cased), and both multilingual DistilBERT and Sentence-BERT (Reimers and Gurevych, 2020).",
      "startOffset" : 147,
      "endOffset" : 175
    }, {
      "referenceID" : 26,
      "context" : "As a static noncontextualised baseline we used GloVe (Pennington et al., 2014) (the English official models with 300 dimensions and trained on 840 billion tokens, and the equivalent Portuguese model released by Hartmann et al.",
      "startOffset" : 53,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "The vector representations were obtained with the flairNLP framework (Akbik et al., 2019) using the models provided by the transformers library (Wolf et al.",
      "startOffset" : 69,
      "endOffset" : 89
    }, {
      "referenceID" : 23,
      "context" : "car) is an indication of idiomaticity (Mitchell and Lapata, 2010): the more similar they are the more compositional the NC is.",
      "startOffset" : 38,
      "endOffset" : 65
    }, {
      "referenceID" : 5,
      "context" : "We also compute correlations between the similarities and frequency-based data, namely the NC raw frequency, and the PPMI (Church and Hanks, 1990) between its component words, to verify whether they have any impact in these measures of idiomaticity.",
      "startOffset" : 122,
      "endOffset" : 146
    }, {
      "referenceID" : 3,
      "context" : "25B tokens in English (Baroni et al., 2009), and brWaC, containing 2.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 7,
      "context" : "than any of the contextualised models (Cordeiro et al., 2019).",
      "startOffset" : 38,
      "endOffset" : 61
    }, {
      "referenceID" : 37,
      "context" : "After that, we injected these type level vectors into the BERT models using BERTRAM (Schick and Schütze, 2020a).",
      "startOffset" : 84,
      "endOffset" : 111
    } ],
    "year" : 2021,
    "abstractText" : "Accurate assessment of the ability of embedding models to capture idiomaticity may require evaluation at token rather than type level, to account for degrees of idiomaticity and possible ambiguity between literal and idiomatic usages. However, most existing resources with annotation of idiomaticity include ratings only at type level. This paper presents the Noun Compound Type and Token Idiomaticity (NCTTI) dataset, with human annotations for 280 noun compounds in English and 180 in Portuguese at both type and token level. We compiled 8,725 and 5,091 token level annotations for English and Portuguese, respectively, which are strongly correlated with the corresponding scores obtained at type level. The NCTTI dataset is used to explore how vector space models reflect the variability of idiomaticity across sentences. Several experiments using state-of-the-art contextualised models suggest that their representations are not capturing the noun compounds idiomaticity as human annotators. This new multilingual resource also contains suggestions for paraphrases of the noun compounds both at type and token levels, with uses for lexical substitution or disambiguation in context.",
    "creator" : "LaTeX with hyperref"
  }
}