{
  "name" : "2021.acl-long.539.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Dissecting Generation Modes for Abstractive Summarization Models via Ablation and Attribution",
    "authors" : [ "Jiacheng Xu" ],
    "emails" : [ "jcxu@cs.utexas.edu", "gdurrett@cs.utexas.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6925–6940\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6925"
    }, {
      "heading" : "1 Introduction",
      "text" : "Transformer-based neural summarization models (Liu and Lapata, 2019; Stiennon et al., 2020; Xu et al., 2020b; Desai et al., 2020), especially pretrained abstractive models like BART (Lewis et al., 2020) and PEGASUS (Zhang et al., 2020), have made great strides in recent years. These models demonstrate exciting new capabilities in terms of abstraction, but little is known about how these models work. In particular, do token generation decisions leverage the source text, and if so, which parts? Or do these decisions arise based primarily on knowledge from the language model (Jiang\net al., 2020; Carlini et al., 2020), learned during pre-training or fine-tuning? Having tools to analyze these models is crucial to identifying and forestalling problems in generation, such as toxicity (Gehman et al., 2020) or factual errors (Kryscinski et al., 2020; Goyal and Durrett, 2020, 2021).\nAlthough interpreting classification models for NLP has been widely studied from perspectives like feature attribution (Ribeiro et al., 2016; Sundararajan et al., 2017) and influence functions (Koh and Liang, 2017; Han et al., 2020), summarization specifically introduces some additional elements that make these techniques hard to apply directly. First, summarization models make sequential decisions from a very large state space. Second, encoder-decoder models have a special structure, featuring a complex interaction of decoder-side and encoder-side computation to select the next word. Third, pre-trained LMs blur the distinction between relying on implicit prior knowledge or explicit instance-dependent input.\nThis paper aims to more fully interpret the stepwise prediction decisions of neural abstractive summarization models.1 First, we roughly bucket generation decisions into one of several modes of generation. After confirming that the models we use are robust to seeing partial inputs, we can probe the model by predicting next words with various model ablations: a basic language model with no input (LM∅), a summarization model with no input (S∅), with part of the document as input (Spart), and with the full document as input (Sfull). These ablations tell us when the decision is context-independent (generated in an LM-like way), when it is heavily context-dependent (generated from the context), and more. We map these regions in Figure 2 and can use these maps to coarsely analyze model behavior. For example, 17.6% of the decisions on\n1Code and visualization are available at https:// github.com/jiacheng-xu/sum-interpret\nXSum are in the lower-left corner (LM-like), which means they do not rely much on the input context.\nSecond, we focus on more fine-grained attribution of decisions that arise when the model does rely heavily on the source document. We carefully examine interpretations based on several prior techniques, including occlusion (Zeiler and Fergus, 2014), attention, integrated gradients (Sundararajan et al., 2017), and input gradients (Hechtlinger, 2016). In order to evaluate and compare these methods, we propose a comprehensive evaluation based on presenting counterfactual, partial inputs to quantitatively assess these models’ performance with different subsets of the input data.\nOur two-stage analysis framework allows us to (1) understand how each individual decision depends on context and prior knowledge (Sec 3), (2) find suspicious cases of memorization and bias (Sec 4), (3) locate the source evidence for context dependent generation (Sec 5). The framework can be used to understand more complex decisions like sentence fusion (Sec 6)."
    }, {
      "heading" : "2 Background & Setup",
      "text" : "A seq2seq neural abstractive model first encodes an input document with m sentences (s1, · · · , sm) and n tokens (w1, w2, · · · , wn), then generates a sequence of tokens (y1, · · · , yT ) as the summary. At each time step t in the generation phase, the model encodes the input document and the decoded summary prefix and predicts the distribution over tokens as p(yt | w1, w2, . . . , wm, y<t)."
    }, {
      "heading" : "2.1 Target Models & Datasets",
      "text" : "We investigate the English-language CNN/DM (Hermann et al., 2015) and XSum (Narayan et al., 2018) datasets, which are commonly used to fine tune pre-trained language models like BART, PEGASUS and T5. As shown in past work (Narayan et al., 2018; Chen et al., 2020b; Xu et al., 2020a), XSum has significantly different properties from CNN/DM, so these datasets will show a range of model behaviors. We will primarily use the development sets for our analysis.\nWe focus on BART (Lewis et al., 2020), a stateof-the-art pre-trained model for language modeling and text summarization. Specifically, we adopt ‘bart-large’ as the language model MLM, ‘bartlarge-xsum’ as the summarization model MSUM for XSum, and ‘bart-large-cnn’ for CNN/DM, made available by Wolf et al. (2019). BART features separate LM and summarization model sharing the same subword tokenization method.2\nOur approach focuses on teasing apart these different modes of decisions. We first run the full model to get the predicted summary (y1, · · · , yT ). We then analyze the distribution placed by the full model Sfull to figure out what contributes towards the generation of the next token."
    }, {
      "heading" : "2.2 Overview of Ablation and Attribution",
      "text" : "Figure 1 shows our framework with an example of our analysis of four generation decisions. In\n2Our analysis can generalize to other pre-trained models, but past work has shown BART and PEGASUS to be roughly similar in terms of behavior (Xu et al., 2020a), so we do not focus on this here.\nthe ablation stage, we compare the predictions of different model and input configurations. The goal of this stage is to coarsely determine the mode of generation. Here, for and Khan are generated in an LM-like way: the model already has a strong prior that Sadiq should be Sadiq Khan and the source article has little impact on this decision. Cameron, by contrast, does require the source in order to be generated. And mayoral is a complex case, where the model is not strictly copying this word from anywhere in the source, but instead using a nebulous combination of information to generate it. In the attribution stage, we interpret such decisions which require more context using a more fine-grained approach. Given the predicted prefix (like David), target prediction (like Cameron), and the model, we use attribution techniques like integrated gradients (Sundararajan et al., 2017) or LIME (Ribeiro et al., 2016) to track the input which contributes to this prediction."
    }, {
      "heading" : "2.3 Ablation Models and Assumptions",
      "text" : "The configurations we use are listed in Table 1 and defined as follows:\nLM∅ is a pre-trained language model only taking the decoded summary prefix as input. We use this model to estimate what a pure language model will predict given the prefix. We denote the prediction distribution as PLM∅= P (yt | y<t;MLM).\nS∅ is the same BART summarization model as Sfull, but without the input document as the input. That is, it uses the same parameters as the full model, but with no input document fed in. We use the prediction of this model to estimate how strong an effect the in-domain training data has, but still treating the model as a decoder-only language model. It is denoted as P∅ = P (yt | y<t;MSUM). Figure 1 shows how this can effectively identify cases like Khan that surprisingly do not rely on the input document.\nSpart is a further step closer to the full model: this is the BART summarization model conditioned on the decoder prefix and part of the input document, denoted as Ppart = P (yt | y<t, {si};MSUM) where {wi} is a subset of tokens of the input document. The selected content could be a continuous span, or a sentence, or a concatenation of several spans or sentences.\nAlthough MSUM is designed and trained to condition on input document, we find that the model also works well with no input, little input and incomplete sentences. As we will show later, there are many cases that this scheme successfully explains; we formalize our assumption as follows:\nAssumption 1 If the model executed on partial input nearly reproduces the next word distribution of the full model, then we view that partial context as a sufficient (but perhaps not necessary) input to explain the model’s behavior.\nHere we define partial input as either just the decoded summary so far or the summary and partial context. In practice, we see two things. First, when considering just the decoder context (i.e., behaving as an LM), the partial model may reproduce the full model’s behavior (e.g., Khan in Figure 1). We do not focus on explaining these cases in further detail. While conceivably the actual conditional model might internally be doing something different (a risk noted by Rudin (2019)), this proves the existence of a decoder-only proxy model that reproduces the full model’s results, which is a criterion used in past work (Li et al., 2020). Second, when considering partial inputs, the model frequently requires one or two specific sentences to reproduce the full model’s behavior, suggesting that the given contexts are both necessary and sufficient.\nBecause these analyses involve using the model on data significantly different than that which it is trained on, we want another way to quantify the importance of a word, span, or sentence. This brings us to our second assumption:\nAssumption 2 In order to say that a span of the input or decoder context is important to the model’s prediction, it should be the case that this span is demonstrated to be important in counterfactual settings. That is, modified inputs to the model that include this span should yield closer predictions than those that don’t.\nThis criterion depends on the set of counterfactuals that we use. Rather than just word removal (Ribeiro et al., 2016), we will use a more compre-\nhensive set of counterfactuals (Miller, 2019; Jacovi and Goldberg, 2020) to quantify the importance of input tokens. We describe this more in Section 5."
    }, {
      "heading" : "2.4 Distance Metric",
      "text" : "Throughout this work, we rely on measuring the distance between distributions over tokens. Although KL divergence is a popular choice, we found it to be very unstable given the large vocabulary size, and two distributions that are completely different would have very large values of KL. We instead use the L1 distance between the two distributions: D(P,Q) = ∑ i |pi − qj |. This is similar to using the Earth Mover’s Distance (Rubner et al., 1998) over these two discrete distributions, with an identity transportation flow since the distributions are defined over the same set of tokens."
    }, {
      "heading" : "3 Ablation: Mapping Model Behavior",
      "text" : "Based on Assumption 1, we can take a first step towards understanding these models based on the partial models described in Section 2.3. Previous work (See et al., 2017; Song et al., 2020) has studied model behavior based on externally-visible properties of the model’s generation, such as identifying novel words, differentiating copy and generation, and prediction confidence, which provides some insight about model’s behavior (Xu et al., 2020a). However, these focus more on shallow comparison of the input document, the generated summary, and the reference summary, and do not focus as strongly on the model.\nWe propose a new way of mapping the prediction space, with maps3 for XSum and CNN/DM shown in Figure 2. Each point in the map is a single subword token being generated by the decoder on the development set at inference time; that is, each point corresponds to a single invocation of the model. This analysis does not depend on the reference summary at all.\nThe x-axis of the map shows the distance between LM∅ and Sfull, using the metric defined in Section 2.4 which ranges from 0 to 2. The y-axis shows the distance between S∅ and Sfull. Other choices of partial models for the axes are possible (or more axes), but we believe these show two important factors. The x-axis captures how much the generic pre-trained language model agrees with the full model’s predictions. The y-axis cap-\n3While our axes are very different here, our mapping concept loosely follows that of Swayamdipta et al. (2020).\ntures how much the decoder-only summarization model agrees with the full model’s predictions. The histogram on the sides of the map show counts along with each vertical or horizontal slice.\nModes of decisions We break these maps into a few coarse regions based on the axis values. We list the coordinates of the bottom left corner and the upper right corner. These values were chosen by inspection and the precise boundaries have little effect on our analysis, as many of the decisions fall into the corners or along sides.\nLM ([0, 0], [0.5, 0.5]) contains the cases where LM∅ and S∅ both agree with Sfull. These decisions are easily made using only decoder information, even without training or knowledge of the input document. These are cases that follow from the constraints of language models, including function words, common entities, or idioms.\nCTX ([0.5, 0.5], [2, 2]) contains the cases where the input is needed to make the prediction: neither decoder-only model can model these decisions.\nFT ([1.5, 0], [2, 0.5]) captures cases where the finetuned decoder-only model is a close match but the pre-trained model is not. This happens more often on XSum and reflects memorization of training summaries, as we discuss later.\nPT ([0, 1.5], [0.5, 2]) is the least intuitive case, where LM∅ agrees with Sfull but S∅ does not; that is, finetuning a decoder-only model causes it to work less well. This happens more often on CNN/DM and reflects memorization of data in the pre-training corpus."
    }, {
      "heading" : "3.1 Coloring the Map with Context Probing",
      "text" : "While the map highlights some useful trends, there are many examples that do rely heavily on the context that we would like to further analyze. Some examples depend on the context in a sophisticated way, but other tokens like parts of named entities or noun phrases are simply copied from the source article in a simple way. Highlighting this contrast, we additionally subdivide the cases by how they depend on the context.\nWe conduct a sentence-level presence probing experiment to further characterize the generation decisions. For a document with m sentences, we run the Spart model conditioned on each of the sentences in isolation. We can obtain a sequence of scalars Psent = (Ppart(si); i ∈ [1,m]). We define CTX-Hd (“context-hard”) cases as ones where max(Psent) is low; that is, where no single sentence can yield the token, as in the case of sentence fusion. These also reflect cases of high entropy for Sfull, where any perturbation to the input may cause a big distribution shift. The first, second and third quartile of max(Psent) is [0.69, 0.96, 1.0] and [0.95, 1.0, 1.0] on XSum and on CNN/DM."
    }, {
      "heading" : "3.2 Region Count & POS Tags",
      "text" : "To roughly characterize the words generated in different regions of the map, in Table 2, we show the percentage of examples falling to each region and the top 3 POS tags for each region on the XSum map. From the frequency of these categories, we can tell more than two-thirds of the decisions belong to the Context category. 17.6% of cases are in LM, the second-largest category. In the LM region, ADP and DET account for nearly half of the data points, confirming that these are largely function\nwords. Nouns are still prevalent, accounting for 13.5% of the category. After observing the data, we found that these points represent commonsense knowledge or common nouns or entities, like “Nations” following “United” or “Obama” following “Barack” where the model generates these without relying on the input. Around 8% of cases fall into gaps between these categories. Only 2.5% and 2.1% of the generations fall into the PT and FT, respectively. These are small but significant cases, as they clearly show the biases from the pretraining corpus and the fine-tuning corpus. We now describe the effects we observe here."
    }, {
      "heading" : "4 Bias from Training Data",
      "text" : "One benefit of mapping the predictions is to detect predictions that are suspiciously likely given one language model but not the other, specifically those in the PT and FT regions. CNN/DM has more cases falling into PT than XSum so we focus on CNN/DN for PT and XSum for FT.\nPT: Bias from the Pretraining Corpus The data points falling into the PT area are those where LM∅ prediction is similar to Sfull prediction but the S∅ prediction is very different from Sfull. We present a set of representative examples from the PT region of the CNN/DM map in Table 3. For the first example, match is assigned high probability by LM∅ and Sfull, but not by the no-input summarization models. The cases in this table exhibit a suspiciously high probability assigned to the correct answer in the base LM: its confidence about Kylie Jenner vs. Kyle Min(ogue) is uncalibrated with what the “true” probabilities of these seem likely to be to our human eyes.\nOne explanation which we investigate is whether the validation and test sets of benchmark datasets\nlike CNN/DM are contained in the pre-training corpus, which could teach the base LM these patterns. Several web crawls have been used for different models, including C4 (Raffel et al., 2020), OpenWebText (Radford et al., 2019), CC-News (Liu et al., 2019). Due to the availability of the corpus, we only check OpenWebText, which, as part of C4, is used for models like GPT-2, PEGASUS and T5.\nAccording to Hermann et al. (2015), the validation and test sets of CNN/DM come from March and April 2015, respectively. We extract the March to May 2015 dump of OpenWebText and find that 4.46% (512 out of 11,490) test examples and 3.31% (442 out of 13,368) validation examples are included in OpenWebText.4 Our matching criteria is more than three 7-gram word overlaps between the pre-training document and reference summaries from the dataset; upon inspection, over 90% of the cases flagged by this criterion contained large chunks of the reference summary.\nOur conclusion is that the pre-trained language model has likely memorized certain articles and their summaries. Other factors could be at play: other types of knowledge in the language model (Petroni et al., 2019; Shin et al., 2020; Talmor et al., 2020) such as key entity cooccurrences, could be contributing to these cases as well and simply be “forgotten” during fine-tuning. However, as an analysis tool, ablation suggested a hypothesis\n4This is an approximation since we cannot precisely verify the pre-training datasets for each model, but it is more likely to be an underestimate than an overestimate. We only extract pre-training documents from cnn.com and dailymail. co.uk from a limited time range, so we may fail to detect snippets of reference summaries that show up in other time ranges of the scrape or in other news sources, whether through plagiarism or re-publishing.\nabout data overlap which we were able to partially confirm, which supports its utility for understanding summarization models.\nFT: Bias from Fine-tuning Data We now examine the data points falling in the bottom right corner of the map, where the fine-tuned LM matches the full model more closely than the pre-trained LM.\nIn Table 4, we present some model-generated bigrams found in the FT region of XSum and compare the frequency of these patterns in the XSum and CNN/DM training data. Not every generation instance of these bigrams falls into the FT region, but many do. Table 4 shows the relative probabilities of these counts in XSum and CNN/DM, showing that these cases are all very common in XSum training summaries. The aggregate over all decisions in this region (the last line) shows this pattern as well. These can suggest larger patterns: the first three come from the common phrase in our series of letters from African journalists (starts 0.5% of\nsummaries in XSum). Other stylistic markers, such as ways of writing currency, are memorized too."
    }, {
      "heading" : "5 Attribution",
      "text" : "As shown in Table 2, more than two thirds of generation steps actually do rely heavily on the context. Here, we focus specifically on identifying which aspects of the input are important for cases where the input does influence the decision heavily using attribution methods.\nEach of the methods we explore scores each word wi in the input document with a score αi. The score can be a normalized distribution, or a probability value ranging from 0 to 1. For each method, we rank the tokens in descending order by score. To confirm that the tokens highlighted are meaningfully used by the model when making its predictions, we propose an evaluation protocol based on a range of counterfactual modifications of the input document, taking care to make these compatible with the nature of subword tokenization."
    }, {
      "heading" : "5.1 Evaluation by Adding and Removing",
      "text" : "Our evaluation focuses on the following question: given a budget of tokens or sentences, how well does the model reconstruct the target token yt when shown the important content selected by the attribution method? Our metric is the cross entropy loss of predicting the model-generated next token given different subsets of the input.5\nMethods based on adding or removing single tokens have been used to evaluate before (Nguyen, 2018). However, for summarization, showing the model partial or ungrammatical inputs in the source\n5The full model is not a strict bound on this; restricting the model to only see salient content could actually increase the probability of what was generated. However, because we have limited ourselves to CTX examples and are aggregating across a large corpus, we do not observe this in our metrics.\nmay significantly alter the model’s behavior. To address this, we use four methods to evaluate under a range of conditions, where in each case the model has a specific budget. Our conditions are: 1. DISPTOK selects n tokens as the input. 2. RMTOK shows the document with n tokens masked instead of deleted.6 3. DISPSENT selects n sentences as the input, based on cumulative attribution over the sentence. 4. RMSENT removes n sentences from the document as the input.\nTable 5 shows examples of these methods applied to the examples from Figure 1. These highlight the impact of key tokens in certain generation cases, but not all.\nWe describe the details of how we feed or mask the tokens in TOK in Appendix. C. The sentencelevel methods are guaranteed to return grammatical input. Token-based evaluation is more precise which helps locating the exact feature token, but the trade-off is that the input is not fully natural."
    }, {
      "heading" : "5.2 Methods",
      "text" : "We use two baseline methods: Random, which randomly selects tokens or sentences to display or remove, and Lead, which selects tokens or sentences according to document position, along with several attribution methods from prior work. Occlusion (Zeiler and Fergus, 2014) involves iteratively masking every single token or remove each sentence in the document and measuring how the prediction probability of the target token changes. Although attention has been questioned (Jain and Wallace, 2019), it still has some value as an explanation technique (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019). We pool the attention heads from the last layer of the Transformer inside our models, ignoring special tokens like SOS.\nFinally, we use two gradient-based techniques (Bastings and Filippova, 2020). Input Gradient is a saliency based approach taking the gradient of the target token with respect to the input and multiplying by the input feature values. Integrated Gradients Sundararajan et al. (2017) computes gradients of the model input at a number of points interpolated between a reference “baseline” (typically an all-MASK input) and the actual input. This computes a path integral of the gradient.\n6Note that we do not directly remove the tokens because this approach typically makes the sentence ungrammatical. Token masks are a more natural type of input to models that are pre-trained with these sorts of masks anyway.\nAttribution Aggregation for Sentence-level Evaluation We have described the six methods we use for token-level evaluation. To evaluate these methods on the sentence level benchmark, we aggreagate the attributions in each sentence attr(si) = ∑d j=0 attr(wj)/d. Hence we can obtain a ranking of sentences by their aggregated attribution score."
    }, {
      "heading" : "5.3 Results",
      "text" : "In Figure 3, we show the token-level and sentencelevel comparison of the attribution methods on the CTX examples in XSum. IntGrad is the best technique overall, with InpGrad achieving similar performance. Interestingly, occlusion underperforms other techniques when more tokens are removed, despite our evaluation being based on occlusion; this indicates that single-token occlusion is not necessarily the strongest attribution method. We also found that all of these give similar results, regardless of whether they present the model with a realistic input (sentence removal) or potentially ungrammatical or unrealistic input (isolated tokens added/removed).\nOur evaluation protocol shows better performance from gradient-based techniques. The combination of four settings tests a range of counterfactual inputs to the model and increases our confidence in these conclusions."
    }, {
      "heading" : "6 Case Study: Sentence Fusion",
      "text" : "We now present a case study of the sort of analysis that can be undertaken using our two-stage interpretation method. We conduct an analysis driven by sentence fusion, a particular class of CTX-Hd cases. Sentence fusion is an exciting capability of abstractive models that has been studied previously (Barzilay and McKeown, 2005; Thadani and McKeown, 2013; Lebanoff et al., 2019, 2020).\nWe broadly identify cases of cross-sentence information fusion by first finding cases in CTX-Hd where the max(Psent) < 0.5, but two sentences combined enable the model to predict the word. We search over all ( m 2 ) combinations of sentences (m is the total number of sentences) and run the Spart model on each pair of sentences. We identify 16.7% and 6.0% of cases in CNN/DM and XSum, respectively, where conditioning on a pair of sentences increases the probability of the model’s generation by at least 0.5 over any sentence in isolation.\nIn Table 6, we show two examples of sentence\nfusion on XSum in this category, additionally analyzed using the DISPSENT attribution method. In the first example, typical in XSum, the model has to predict the event name UCI without actually seeing it. The model’s reasoning appears distributed over the document: it consults entity and event descriptions like world champion and France, perhaps to determine this is an international event. In the second example, we see the model again connects several pieces of information. The generated text is factually incorrect: the horse is retiring, and not Dujardin. Nevertheless, this process tells us some things that are going wrong (the model disregards the horse in the generation process), and could potentially be useful for fine-grained factuality evaluation using recent techniques (Tian et al., 2019; Kryscinski et al., 2020; Goyal and Durrett, 2020; Maynez et al., 2020).\nThe majority of the “fusion” cases we investigated actually reflect content selection at the beginning of the generation. Other cases we observe fall more cleanly into classic sentence fusion or draw on coreference resolution."
    }, {
      "heading" : "7 Related Work",
      "text" : "Model interpretability for NLP has been intensively studied in the past few years (Ribeiro et al., 2016; Alvarez-Melis and Jaakkola, 2018; Jacovi et al., 2018; Chen et al., 2020a; Jacovi and Goldberg, 2020; DeYoung et al., 2020; Pruthi et al., 2020; Ye et al., 2021). However, many of these techniques are tailored to classification tasks like sentiment. For post-hoc interpretation of generation, most work has studied machine translation (Ma et al.; Li et al., 2020; Voita et al., 2020). Li et al. (2020) focus on evaluating explanations by finding surrogate models that are similar to the base MT model; this is similar to our evaluation approach in Section 5, but involves an extra distillation step. Compared to Voita et al. (2020), we are more interested in highlighting how and why changes in the source article will change the summary (counterfactual explanations).\nTo analyze summarization more broadly, Xu et al. (2020a) provides a descriptive analysis about models via uncertainty. Previous work (Kedzie et al., 2018; Zhong et al., 2019; Kryscinski et al., 2019; Zhong et al., 2019) has conducted comprehensive examination of the limitations of summarization models. Filippova (2020) ablates model input to control the degree of hallucination. Miao\net al. (2021) improves the training of MT by comparing the prediction of LM and MT model.\nFinally, this work has focused chiefly on abstractive summarization models. We believe interpreting extractive (Liu and Lapata, 2019) or compressive (Xu and Durrett, 2019; Xu et al., 2020b; Desai et al., 2020) models would be worthwhile to explore and could leverage similar attribution techniques, although ablation does not apply as discussed here."
    }, {
      "heading" : "8 Recommendations & Conclusion",
      "text" : "We recommend a few methodological takeaways that can generalize to other conditional generation problems as well.\nFirst, use ablation to analyze generation models. While removing the source forms inputs not strictly on the data manifold, ablation was remarkably easy, robust, and informative in our analysis. Constructing our maps only requires querying three models with no retraining required.\nSecond, to understand an individual decision, use feature attribution methods on the source only. Including the target context often muddies the interpretation since recent words are always relevant, but looking at attributions over the source and target together doesn’t accurately convey the model’s decision-making process.\nFinally, to probe attributions more deeply, consider adding or removing various sets of tokens. The choice of counterfactuals to explain is an illposed problem, but we view the set used here as realistic for this setting (Ye et al., 2021).\nTaken together, our two-step framework allows us to identify generation modes and attribute generation decisions to the input document. Our techniques shed light on possible sources of bias and can be used to explore phenomena such as sentence fusion. We believe these pave the way for future studies of targeted phenomena, including fusion, robustness, and bias in text generation, through the lens of these interpretation techniques."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Thanks to the members of the UT TAUR lab for helpful discussion, especially Tanya Goyal, Yasumasa Onoe, and Xi Ye for constructive suggestions. This work was partially supported by a gift from Salesforce Research and a gift from Amazon. Thanks as well to the anonymous reviewers for their helpful comments."
    }, {
      "heading" : "B Examples of PT",
      "text" : "We present more examples of bias from the pretrained language model on CNN/DM in Table 9. In Table 3 we have shown the cases where the memorized phrases are proper nouns or nouns. Here we provide examples of other types like function words. The memorization of function words like with or and can be challenging to spot using other means due to their ubiquity.\nC Implementation Detail for TOK\nWe rank the attribution score of all subword tokens rather than words. However, to provide necessary context for DISPTOK and to avoid information leakage in RMTOK, we extend the selection by a context window to collect neighboring word pieces. We illustrate the way of fulfilling budget with an example.\nBur #berry bets on new branding 1 2 4 3 5\nIn this example “Bur” receives the highest score and “new” the second. We use a context windows of size 1 and a budget of n = 4 tokens. In DISPTOK, the input will be “〈sos〉Burberry, on new〈eos〉”; In RMTOK, the input will be 〈sos〉## bets## branding〈eos〉 where # stands for the MASK token. If n = 5, branding will be added or masked."
    }, {
      "heading" : "D Efficient Two-Stage Selection Model",
      "text" : "For long documents in summarization, attribution methods can be computationally expensive. Occlusion requires running inference once for each token in the input document. Gradient-based methods store the gradients and so require a lot of GPU\nmemory when the document is long. These techniques spend time and memory checking words that have little impact on the generation.\nIn order to improve the efficiency of these methods, we propose an efficient alternative where we first run sentence level presence probing on the full document, and then run attribution methods locally on the top-k sentences. We call the proposed model S+[method] where method can be arbitrary attribution methods including occlusion, attention, InpGrad and IntGrad.\nWe define our notation as follows: s, n and d are the number of sentences, the number of tokens in the document, and the number of tokens in each sentence, respectively. For the occlusion method, we can run inference s times to pre-select important sentences, each of which costs O(d2) times due to self-attention. The attribution is then applied only to only one or few sentences so the complexity is nowO(k×d2×d) where k is the number of top sentences used for attribution. In our experiments, we set k = 2 and n ≤ 500. Compared to the complex-\nity of the regular model O(n3), the complexity of the two-stage model is onlyO(s×d2+k×d2×d).\nIn Table 7 we compare the complexity and actual run time and memory usage. We batch the occlusion operation and the batch size is set to 100. We can see a huge reduction in running time and a significant drop in memory usage.\nTakeaway A two-stage selection model is much more efficient, yielding a 97% running time reduction on the occlusion method. The downside of this method is that it only produces single-sentence attributions, and so isn’t appropriate in cases involving sentence fusion.\nFollowing (Vaswani et al., 2017), we compare the complexity for all methods in Table 12. n is the number of tokens in the document. d is the number of tokens in each sentence. s is the number of sentences in the document. r is the number of steps in the integral approximation of Integrated Gradient. bp indicates the time consumption of one back-\npropagation for gradient based methods. We list the complexity of the original methods in the middle column and the sentence based pre-selection variant in the right column. The base cost for sentence pre-selection model is to run the sentence selection model s times, so it’s O(s × d2). The n2 and d2 originate from the quadratic operation of self-attentions in Transformer models. We ignore the number of layers in the neural network or other model related hyper-parameters since all of the methods here share the same model."
    }, {
      "heading" : "E Four Way Evaluation",
      "text" : "Due to the space limit, we only show the plot of the four way evaluation in Figure 3. To enable future comparisons on the proposed evaluation protocol, we also include the detailed results in Table 10 and Table 11 for TOK and SENT evaluation. The ∆ measures how the average performance increase or drop deviates from the original baseline. We abstract the evaluation methods as a function eval. The input is the text and the budget n and output is the predicted loss.\n∆ = Avg(eval(i))− eval(0)\nFor TOK series evaluation, i ∈ {1, 2, 4, 8, 16}. For SENT series evaluation, i ∈ {1, 2, 3, 4} because a sentence carries much more information than a token. IntGrad performs the best across all of the evaluation methods."
    } ],
    "references" : [ {
      "title" : "Towards robust interpretability with self-explaining neural networks",
      "author" : [ "David Alvarez-Melis", "Tommi S. Jaakkola." ],
      "venue" : "Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS’18, page 7786–7795, Red Hook,",
      "citeRegEx" : "Alvarez.Melis and Jaakkola.,? 2018",
      "shortCiteRegEx" : "Alvarez.Melis and Jaakkola.",
      "year" : 2018
    }, {
      "title" : "Sentence fusion for multidocument news summarization",
      "author" : [ "Regina Barzilay", "Kathleen R. McKeown." ],
      "venue" : "Computational Linguistics, 31(3):297– 328.",
      "citeRegEx" : "Barzilay and McKeown.,? 2005",
      "shortCiteRegEx" : "Barzilay and McKeown.",
      "year" : 2005
    }, {
      "title" : "The elephant in the interpretability room: Why use attention as explanation when we have saliency methods",
      "author" : [ "Jasmijn Bastings", "Katja Filippova" ],
      "venue" : "In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks",
      "citeRegEx" : "Bastings and Filippova.,? \\Q2020\\E",
      "shortCiteRegEx" : "Bastings and Filippova.",
      "year" : 2020
    }, {
      "title" : "Extracting training data from large language models",
      "author" : [ "Nicholas Carlini", "Florian Tramer", "Eric Wallace", "Matthew Jagielski", "Ariel Herbert-Voss", "Katherine Lee", "Adam Roberts", "Tom Brown", "Dawn Song", "Ulfar Erlingsson" ],
      "venue" : null,
      "citeRegEx" : "Carlini et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Carlini et al\\.",
      "year" : 2020
    }, {
      "title" : "Generating hierarchical explanations on text classification via feature interaction detection",
      "author" : [ "Hanjie Chen", "Guangtao Zheng", "Yangfeng Ji." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5578–",
      "citeRegEx" : "Chen et al\\.,? 2020a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "CDEvalSumm: An empirical study of crossdataset evaluation for neural summarization systems",
      "author" : [ "Yiran Chen", "Pengfei Liu", "Ming Zhong", "Zi-Yi Dou", "Danqing Wang", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "Findings of the Association for Computational",
      "citeRegEx" : "Chen et al\\.,? 2020b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Compressive summarization with plausibility and salience modeling",
      "author" : [ "Shrey Desai", "Jiacheng Xu", "Greg Durrett." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6259–6274, Online. As-",
      "citeRegEx" : "Desai et al\\.,? 2020",
      "shortCiteRegEx" : "Desai et al\\.",
      "year" : 2020
    }, {
      "title" : "ERASER: A benchmark to evaluate rationalized NLP models",
      "author" : [ "Jay DeYoung", "Sarthak Jain", "Nazneen Fatema Rajani", "Eric Lehman", "Caiming Xiong", "Richard Socher", "Byron C. Wallace." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for",
      "citeRegEx" : "DeYoung et al\\.,? 2020",
      "shortCiteRegEx" : "DeYoung et al\\.",
      "year" : 2020
    }, {
      "title" : "Controlled hallucinations: Learning to generate faithfully from noisy data",
      "author" : [ "Katja Filippova." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 864–870, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Filippova.,? 2020",
      "shortCiteRegEx" : "Filippova.",
      "year" : 2020
    }, {
      "title" : "RealToxicityPrompts: Evaluating neural toxic degeneration in language models",
      "author" : [ "Samuel Gehman", "Suchin Gururangan", "Maarten Sap", "Yejin Choi", "Noah A. Smith." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
      "citeRegEx" : "Gehman et al\\.,? 2020",
      "shortCiteRegEx" : "Gehman et al\\.",
      "year" : 2020
    }, {
      "title" : "Evaluating factuality in generation with dependency-level entailment",
      "author" : [ "Tanya Goyal", "Greg Durrett." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3592–3603, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Goyal and Durrett.,? 2020",
      "shortCiteRegEx" : "Goyal and Durrett.",
      "year" : 2020
    }, {
      "title" : "Annotating and modeling fine-grained factuality in summarization",
      "author" : [ "Tanya Goyal", "Greg Durrett." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.",
      "citeRegEx" : "Goyal and Durrett.,? 2021",
      "shortCiteRegEx" : "Goyal and Durrett.",
      "year" : 2021
    }, {
      "title" : "Explaining black box predictions and unveiling data artifacts through influence functions",
      "author" : [ "Xiaochuang Han", "Byron C. Wallace", "Yulia Tsvetkov." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Han et al\\.,? 2020",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2020
    }, {
      "title" : "Interpretation of prediction models using the input gradient",
      "author" : [ "Yotam Hechtlinger." ],
      "venue" : "arXiv preprint arXiv:1611.07634.",
      "citeRegEx" : "Hechtlinger.,? 2016",
      "shortCiteRegEx" : "Hechtlinger.",
      "year" : 2016
    }, {
      "title" : "Teaching Machines to Read and Comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomás Kočiský", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "Proceedings of the Conference on Neural Information Processing Systems",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4198–4205, Online",
      "author" : [ "Alon Jacovi", "Yoav Goldberg." ],
      "venue" : "As-",
      "citeRegEx" : "Jacovi and Goldberg.,? 2020",
      "shortCiteRegEx" : "Jacovi and Goldberg.",
      "year" : 2020
    }, {
      "title" : "Understanding convolutional neural networks for text classification",
      "author" : [ "Alon Jacovi", "Oren Sar Shalom", "Yoav Goldberg." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 56–65,",
      "citeRegEx" : "Jacovi et al\\.,? 2018",
      "shortCiteRegEx" : "Jacovi et al\\.",
      "year" : 2018
    }, {
      "title" : "Attention is not Explanation",
      "author" : [ "Sarthak Jain", "Byron C. Wallace." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Pa-",
      "citeRegEx" : "Jain and Wallace.,? 2019",
      "shortCiteRegEx" : "Jain and Wallace.",
      "year" : 2019
    }, {
      "title" : "How can we know what language",
      "author" : [ "Zhengbao Jiang", "Frank F. Xu", "Jun Araki", "Graham Neubig" ],
      "venue" : null,
      "citeRegEx" : "Jiang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Content selection in deep learning models of summarization",
      "author" : [ "Chris Kedzie", "Kathleen McKeown", "Hal Daumé III." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1818–1828, Brussels, Belgium.",
      "citeRegEx" : "Kedzie et al\\.,? 2018",
      "shortCiteRegEx" : "Kedzie et al\\.",
      "year" : 2018
    }, {
      "title" : "Understanding black-box predictions via influence functions",
      "author" : [ "Pang Wei Koh", "Percy Liang." ],
      "venue" : "International Conference on Machine Learning, pages 1885–1894. PMLR.",
      "citeRegEx" : "Koh and Liang.,? 2017",
      "shortCiteRegEx" : "Koh and Liang.",
      "year" : 2017
    }, {
      "title" : "Neural text summarization: A critical evaluation",
      "author" : [ "Wojciech Kryscinski", "Nitish Shirish Keskar", "Bryan McCann", "Caiming Xiong", "Richard Socher." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Kryscinski et al\\.,? 2019",
      "shortCiteRegEx" : "Kryscinski et al\\.",
      "year" : 2019
    }, {
      "title" : "Evaluating the factual consistency of abstractive text summarization",
      "author" : [ "Wojciech Kryscinski", "Bryan McCann", "Caiming Xiong", "Richard Socher." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Kryscinski et al\\.,? 2020",
      "shortCiteRegEx" : "Kryscinski et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning to fuse sentences with transformers for summarization",
      "author" : [ "Logan Lebanoff", "Franck Dernoncourt", "Doo Soon Kim", "Lidan Wang", "Walter Chang", "Fei Liu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Lebanoff et al\\.,? 2020",
      "shortCiteRegEx" : "Lebanoff et al\\.",
      "year" : 2020
    }, {
      "title" : "Analyzing sentence fusion in abstractive summarization",
      "author" : [ "Logan Lebanoff", "John Muchovej", "Franck Dernoncourt", "Doo Soon Kim", "Seokhwan Kim", "Walter Chang", "Fei Liu." ],
      "venue" : "Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages",
      "citeRegEx" : "Lebanoff et al\\.,? 2019",
      "shortCiteRegEx" : "Lebanoff et al\\.",
      "year" : 2019
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "Evaluating explanation methods for neural machine translation",
      "author" : [ "Jierui Li", "Lemao Liu", "Huayang Li", "Guanlin Li", "Guoping Huang", "Shuming Shi." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 365–",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Text summarization with pretrained encoders",
      "author" : [ "Yang Liu", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
      "citeRegEx" : "Liu and Lapata.,? 2019",
      "shortCiteRegEx" : "Liu and Lapata.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "On faithfulness and factuality in abstractive summarization",
      "author" : [ "Joshua Maynez", "Shashi Narayan", "Bernd Bohnet", "Ryan McDonald." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906–1919, On-",
      "citeRegEx" : "Maynez et al\\.,? 2020",
      "shortCiteRegEx" : "Maynez et al\\.",
      "year" : 2020
    }, {
      "title" : "Prevent the language model from being overconfident in neural machine translation",
      "author" : [ "Mengqi Miao", "Fandong Meng", "Yijin Liu", "Xiao-Hua Zhou", "Jie Zhou" ],
      "venue" : null,
      "citeRegEx" : "Miao et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Miao et al\\.",
      "year" : 2021
    }, {
      "title" : "Explanation in artificial intelligence: Insights from the social sciences",
      "author" : [ "Tim Miller." ],
      "venue" : "Artificial Intelligence, 267:1–38.",
      "citeRegEx" : "Miller.,? 2019",
      "shortCiteRegEx" : "Miller.",
      "year" : 2019
    }, {
      "title" : "Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization",
      "author" : [ "Shashi Narayan", "Shay B. Cohen", "Mirella Lapata." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Narayan et al\\.,? 2018",
      "shortCiteRegEx" : "Narayan et al\\.",
      "year" : 2018
    }, {
      "title" : "Comparing automatic and human evaluation of local explanations for text classification",
      "author" : [ "Dong Nguyen." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Nguyen.,? 2018",
      "shortCiteRegEx" : "Nguyen.",
      "year" : 2018
    }, {
      "title" : "Language models as knowledge bases",
      "author" : [ "Fabio Petroni", "Tim Rocktäschel", "Sebastian Riedel", "Patrick Lewis", "Anton Bakhtin", "Yuxiang Wu", "Alexander Miller" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Petroni et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Petroni et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to deceive with attention-based explanations",
      "author" : [ "Danish Pruthi", "Mansi Gupta", "Bhuwan Dhingra", "Graham Neubig", "Zachary C. Lipton." ],
      "venue" : "In",
      "citeRegEx" : "Pruthi et al\\.,? 2020",
      "shortCiteRegEx" : "Pruthi et al\\.",
      "year" : 2020
    }, {
      "title" : "Language Models are Unsupervised Multitask Learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI Blog.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Why Should I Trust You?”: Explaining the Predictions of Any Classifier",
      "author" : [ "Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin." ],
      "venue" : "Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (SIGKDD).",
      "citeRegEx" : "Ribeiro et al\\.,? 2016",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2016
    }, {
      "title" : "A metric for distributions with applications to image databases",
      "author" : [ "Yossi Rubner", "Carlo Tomasi", "Leonidas J Guibas." ],
      "venue" : "Sixth International Conference on Computer Vision (IEEE Cat. No. 98CH36271), pages 59–66. IEEE.",
      "citeRegEx" : "Rubner et al\\.,? 1998",
      "shortCiteRegEx" : "Rubner et al\\.",
      "year" : 1998
    }, {
      "title" : "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
      "author" : [ "Cynthia Rudin." ],
      "venue" : "Nature Machine Intelligence, 1(5):206–215.",
      "citeRegEx" : "Rudin.,? 2019",
      "shortCiteRegEx" : "Rudin.",
      "year" : 2019
    }, {
      "title" : "Get To The Point: Summarization with Pointer-Generator Networks",
      "author" : [ "Abigail See", "Peter J. Liiu", "Christopher D. Manning." ],
      "venue" : "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "Is attention interpretable? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2931–2951, Florence, Italy",
      "author" : [ "Sofia Serrano", "Noah A. Smith." ],
      "venue" : "Association for Computational Linguistics.",
      "citeRegEx" : "Serrano and Smith.,? 2019",
      "shortCiteRegEx" : "Serrano and Smith.",
      "year" : 2019
    }, {
      "title" : "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
      "author" : [ "Taylor Shin", "Yasaman Razeghi", "Robert L. Logan IV", "Eric Wallace", "Sameer Singh." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods",
      "citeRegEx" : "Shin et al\\.,? 2020",
      "shortCiteRegEx" : "Shin et al\\.",
      "year" : 2020
    }, {
      "title" : "Controlling the amount of verbatim copying in abstractive summarization",
      "author" : [ "Kaiqiang Song", "Bingqing Wang", "Zhe Feng", "Ren Liu", "Fei Liu." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8902–8909.",
      "citeRegEx" : "Song et al\\.,? 2020",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning to summarize from human feedback",
      "author" : [ "Nisan Stiennon", "Long Ouyang", "Jeff Wu", "Daniel M Ziegler", "Ryan Lowe", "Chelsea Voss", "Alec Radford", "Dario Amodei", "Paul Christiano." ],
      "venue" : "Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Stiennon et al\\.,? 2020",
      "shortCiteRegEx" : "Stiennon et al\\.",
      "year" : 2020
    }, {
      "title" : "Axiomatic attribution for deep networks",
      "author" : [ "Mukund Sundararajan", "Ankur Taly", "Qiqi Yan." ],
      "venue" : "International Conference on Machine Learning, pages 3319–3328.",
      "citeRegEx" : "Sundararajan et al\\.,? 2017",
      "shortCiteRegEx" : "Sundararajan et al\\.",
      "year" : 2017
    }, {
      "title" : "Dataset cartography: Mapping and diagnosing datasets with training dynamics",
      "author" : [ "Swabha Swayamdipta", "Roy Schwartz", "Nicholas Lourie", "Yizhong Wang", "Hannaneh Hajishirzi", "Noah A. Smith", "Yejin Choi." ],
      "venue" : "Proceedings of the 2020 Conference on",
      "citeRegEx" : "Swayamdipta et al\\.,? 2020",
      "shortCiteRegEx" : "Swayamdipta et al\\.",
      "year" : 2020
    }, {
      "title" : "oLMpics-On What Language Model Pre-training Captures",
      "author" : [ "Alon Talmor", "Yanai Elazar", "Yoav Goldberg", "Jonathan Berant." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:743–758.",
      "citeRegEx" : "Talmor et al\\.,? 2020",
      "shortCiteRegEx" : "Talmor et al\\.",
      "year" : 2020
    }, {
      "title" : "Supervised sentence fusion with single-stage inference",
      "author" : [ "Kapil Thadani", "Kathleen McKeown." ],
      "venue" : "Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 1410– 1418, Nagoya, Japan. Asian Federation of Natural",
      "citeRegEx" : "Thadani and McKeown.,? 2013",
      "shortCiteRegEx" : "Thadani and McKeown.",
      "year" : 2013
    }, {
      "title" : "Sticking to the facts: Confident decoding for faithful data-to-text generation",
      "author" : [ "Ran Tian", "Shashi Narayan", "Thibault Sellam", "Ankur P Parikh" ],
      "venue" : null,
      "citeRegEx" : "Tian et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Analyzing the source and target contributions to predictions in neural machine translation",
      "author" : [ "Elena Voita", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "arXiv preprint arXiv:2010.10907.",
      "citeRegEx" : "Voita et al\\.,? 2020",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is not not explanation",
      "author" : [ "Sarah Wiegreffe", "Yuval Pinter." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-",
      "citeRegEx" : "Wiegreffe and Pinter.,? 2019",
      "shortCiteRegEx" : "Wiegreffe and Pinter.",
      "year" : 2019
    }, {
      "title" : "HuggingFace’s Transformers: State-of-the-art Natural Language",
      "author" : [ "Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "Rémi Louf", "Morgan Funtowicz", "Jamie Brew" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2019
    }, {
      "title" : "Understanding neural abstractive summarization models via uncertainty",
      "author" : [ "Jiacheng Xu", "Shrey Desai", "Greg Durrett." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6275–6281, On-",
      "citeRegEx" : "Xu et al\\.,? 2020a",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural extractive text summarization with syntactic compression",
      "author" : [ "Jiacheng Xu", "Greg Durrett." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Xu and Durrett.,? 2019",
      "shortCiteRegEx" : "Xu and Durrett.",
      "year" : 2019
    }, {
      "title" : "Discourse-aware neural extractive text summarization",
      "author" : [ "Jiacheng Xu", "Zhe Gan", "Yu Cheng", "Jingjing Liu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5021–5031, Online. Association for Computa-",
      "citeRegEx" : "Xu et al\\.,? 2020b",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Evaluating explanations for reading comprehension with realistic counterfactuals",
      "author" : [ "Xi Ye", "Rohan Nair", "Greg Durrett." ],
      "venue" : "arXiv preprint arXiv:2104.04515.",
      "citeRegEx" : "Ye et al\\.,? 2021",
      "shortCiteRegEx" : "Ye et al\\.",
      "year" : 2021
    }, {
      "title" : "Visualizing and understanding convolutional networks",
      "author" : [ "Matthew D Zeiler", "Rob Fergus." ],
      "venue" : "European conference on computer vision, pages 818–833. Springer.",
      "citeRegEx" : "Zeiler and Fergus.,? 2014",
      "shortCiteRegEx" : "Zeiler and Fergus.",
      "year" : 2014
    }, {
      "title" : "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization",
      "author" : [ "Jingqing Zhang", "Yao Zhao", "Mohammad Saleh", "Peter J. Liu." ],
      "venue" : "Proceedings of Machine Learning Research. PMLR.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Searching for effective neural extractive summarization: What works and what’s next",
      "author" : [ "Ming Zhong", "Pengfei Liu", "Danqing Wang", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Zhong et al\\.,? 2019",
      "shortCiteRegEx" : "Zhong et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "Transformer-based neural summarization models (Liu and Lapata, 2019; Stiennon et al., 2020; Xu et al., 2020b; Desai et al., 2020), especially pretrained abstractive models like BART (Lewis et al.",
      "startOffset" : 46,
      "endOffset" : 129
    }, {
      "referenceID" : 45,
      "context" : "Transformer-based neural summarization models (Liu and Lapata, 2019; Stiennon et al., 2020; Xu et al., 2020b; Desai et al., 2020), especially pretrained abstractive models like BART (Lewis et al.",
      "startOffset" : 46,
      "endOffset" : 129
    }, {
      "referenceID" : 57,
      "context" : "Transformer-based neural summarization models (Liu and Lapata, 2019; Stiennon et al., 2020; Xu et al., 2020b; Desai et al., 2020), especially pretrained abstractive models like BART (Lewis et al.",
      "startOffset" : 46,
      "endOffset" : 129
    }, {
      "referenceID" : 6,
      "context" : "Transformer-based neural summarization models (Liu and Lapata, 2019; Stiennon et al., 2020; Xu et al., 2020b; Desai et al., 2020), especially pretrained abstractive models like BART (Lewis et al.",
      "startOffset" : 46,
      "endOffset" : 129
    }, {
      "referenceID" : 25,
      "context" : ", 2020), especially pretrained abstractive models like BART (Lewis et al., 2020) and PEGASUS (Zhang et al.",
      "startOffset" : 60,
      "endOffset" : 80
    }, {
      "referenceID" : 60,
      "context" : ", 2020) and PEGASUS (Zhang et al., 2020), have made great strides in recent years.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 18,
      "context" : "In particular, do token generation decisions leverage the source text, and if so, which parts? Or do these decisions arise based primarily on knowledge from the language model (Jiang et al., 2020; Carlini et al., 2020), learned during pre-training or fine-tuning? Having tools to analyze these models is crucial to identifying and forestalling problems in generation, such as toxicity (Gehman et al.",
      "startOffset" : 176,
      "endOffset" : 218
    }, {
      "referenceID" : 3,
      "context" : "In particular, do token generation decisions leverage the source text, and if so, which parts? Or do these decisions arise based primarily on knowledge from the language model (Jiang et al., 2020; Carlini et al., 2020), learned during pre-training or fine-tuning? Having tools to analyze these models is crucial to identifying and forestalling problems in generation, such as toxicity (Gehman et al.",
      "startOffset" : 176,
      "endOffset" : 218
    }, {
      "referenceID" : 9,
      "context" : ", 2020), learned during pre-training or fine-tuning? Having tools to analyze these models is crucial to identifying and forestalling problems in generation, such as toxicity (Gehman et al., 2020) or factual errors (Kryscinski",
      "startOffset" : 174,
      "endOffset" : 195
    }, {
      "referenceID" : 38,
      "context" : "Although interpreting classification models for NLP has been widely studied from perspectives like feature attribution (Ribeiro et al., 2016; Sundararajan et al., 2017) and influence functions (Koh",
      "startOffset" : 119,
      "endOffset" : 168
    }, {
      "referenceID" : 46,
      "context" : "Although interpreting classification models for NLP has been widely studied from perspectives like feature attribution (Ribeiro et al., 2016; Sundararajan et al., 2017) and influence functions (Koh",
      "startOffset" : 119,
      "endOffset" : 168
    }, {
      "referenceID" : 46,
      "context" : "2014), attention, integrated gradients (Sundararajan et al., 2017), and input gradients (Hechtlinger, 2016).",
      "startOffset" : 39,
      "endOffset" : 66
    }, {
      "referenceID" : 14,
      "context" : "We investigate the English-language CNN/DM (Hermann et al., 2015) and XSum (Narayan et al.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 32,
      "context" : ", 2015) and XSum (Narayan et al., 2018) datasets, which are commonly used to fine tune pre-trained language models like BART, PE-",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 32,
      "context" : "As shown in past work (Narayan et al., 2018; Chen et al., 2020b; Xu et al., 2020a), XSum has significantly different properties from CNN/DM, so these datasets will show a range of model behaviors.",
      "startOffset" : 22,
      "endOffset" : 82
    }, {
      "referenceID" : 5,
      "context" : "As shown in past work (Narayan et al., 2018; Chen et al., 2020b; Xu et al., 2020a), XSum has significantly different properties from CNN/DM, so these datasets will show a range of model behaviors.",
      "startOffset" : 22,
      "endOffset" : 82
    }, {
      "referenceID" : 55,
      "context" : "As shown in past work (Narayan et al., 2018; Chen et al., 2020b; Xu et al., 2020a), XSum has significantly different properties from CNN/DM, so these datasets will show a range of model behaviors.",
      "startOffset" : 22,
      "endOffset" : 82
    }, {
      "referenceID" : 25,
      "context" : "We focus on BART (Lewis et al., 2020), a stateof-the-art pre-trained model for language modeling and text summarization.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 55,
      "context" : "Our analysis can generalize to other pre-trained models, but past work has shown BART and PEGASUS to be roughly similar in terms of behavior (Xu et al., 2020a), so we do not focus on this here.",
      "startOffset" : 141,
      "endOffset" : 159
    }, {
      "referenceID" : 46,
      "context" : "target prediction (like Cameron), and the model, we use attribution techniques like integrated gradients (Sundararajan et al., 2017) or LIME (Ribeiro et al.",
      "startOffset" : 105,
      "endOffset" : 132
    }, {
      "referenceID" : 38,
      "context" : ", 2017) or LIME (Ribeiro et al., 2016) to track the input which contributes to this prediction.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 26,
      "context" : "duces the full model’s results, which is a criterion used in past work (Li et al., 2020).",
      "startOffset" : 71,
      "endOffset" : 88
    }, {
      "referenceID" : 38,
      "context" : "Rather than just word removal (Ribeiro et al., 2016), we will use a more compre-",
      "startOffset" : 30,
      "endOffset" : 52
    }, {
      "referenceID" : 41,
      "context" : "Previous work (See et al., 2017; Song et al., 2020) has studied model behavior based on externally-visible properties of the model’s generation, such as identifying",
      "startOffset" : 14,
      "endOffset" : 51
    }, {
      "referenceID" : 44,
      "context" : "Previous work (See et al., 2017; Song et al., 2020) has studied model behavior based on externally-visible properties of the model’s generation, such as identifying",
      "startOffset" : 14,
      "endOffset" : 51
    }, {
      "referenceID" : 55,
      "context" : "novel words, differentiating copy and generation, and prediction confidence, which provides some insight about model’s behavior (Xu et al., 2020a).",
      "startOffset" : 128,
      "endOffset" : 146
    }, {
      "referenceID" : 37,
      "context" : "models, including C4 (Raffel et al., 2020), OpenWebText (Radford et al.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 36,
      "context" : ", 2020), OpenWebText (Radford et al., 2019), CC-News (Liu et al.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 33,
      "context" : "Methods based on adding or removing single tokens have been used to evaluate before (Nguyen, 2018).",
      "startOffset" : 84,
      "endOffset" : 98
    }, {
      "referenceID" : 59,
      "context" : "Occlusion (Zeiler and Fergus, 2014) involves iteratively masking every single token or remove each sentence in the document and measuring how the",
      "startOffset" : 10,
      "endOffset" : 35
    }, {
      "referenceID" : 17,
      "context" : "Although attention has been questioned (Jain and Wallace, 2019), it still has some value as an explanation technique (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019).",
      "startOffset" : 39,
      "endOffset" : 63
    }, {
      "referenceID" : 53,
      "context" : "Although attention has been questioned (Jain and Wallace, 2019), it still has some value as an explanation technique (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019).",
      "startOffset" : 117,
      "endOffset" : 170
    }, {
      "referenceID" : 42,
      "context" : "Although attention has been questioned (Jain and Wallace, 2019), it still has some value as an explanation technique (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019).",
      "startOffset" : 117,
      "endOffset" : 170
    }, {
      "referenceID" : 2,
      "context" : "Finally, we use two gradient-based techniques (Bastings and Filippova, 2020).",
      "startOffset" : 46,
      "endOffset" : 76
    }, {
      "referenceID" : 1,
      "context" : "Sentence fusion is an exciting capability of abstractive models that has been studied previously (Barzilay and McKeown, 2005; Thadani and McKeown, 2013; Lebanoff et al., 2019, 2020).",
      "startOffset" : 97,
      "endOffset" : 181
    }, {
      "referenceID" : 49,
      "context" : "Sentence fusion is an exciting capability of abstractive models that has been studied previously (Barzilay and McKeown, 2005; Thadani and McKeown, 2013; Lebanoff et al., 2019, 2020).",
      "startOffset" : 97,
      "endOffset" : 181
    }, {
      "referenceID" : 26,
      "context" : "For post-hoc interpretation of generation, most work has studied machine translation (Ma et al.; Li et al., 2020; Voita et al., 2020).",
      "startOffset" : 85,
      "endOffset" : 133
    }, {
      "referenceID" : 52,
      "context" : "For post-hoc interpretation of generation, most work has studied machine translation (Ma et al.; Li et al., 2020; Voita et al., 2020).",
      "startOffset" : 85,
      "endOffset" : 133
    }, {
      "referenceID" : 19,
      "context" : "Previous work (Kedzie et al., 2018; Zhong et al., 2019; Kryscinski et al., 2019; Zhong et al., 2019) has conducted comprehensive examination of the limitations of summarization models.",
      "startOffset" : 14,
      "endOffset" : 100
    }, {
      "referenceID" : 61,
      "context" : "Previous work (Kedzie et al., 2018; Zhong et al., 2019; Kryscinski et al., 2019; Zhong et al., 2019) has conducted comprehensive examination of the limitations of summarization models.",
      "startOffset" : 14,
      "endOffset" : 100
    }, {
      "referenceID" : 21,
      "context" : "Previous work (Kedzie et al., 2018; Zhong et al., 2019; Kryscinski et al., 2019; Zhong et al., 2019) has conducted comprehensive examination of the limitations of summarization models.",
      "startOffset" : 14,
      "endOffset" : 100
    }, {
      "referenceID" : 61,
      "context" : "Previous work (Kedzie et al., 2018; Zhong et al., 2019; Kryscinski et al., 2019; Zhong et al., 2019) has conducted comprehensive examination of the limitations of summarization models.",
      "startOffset" : 14,
      "endOffset" : 100
    }, {
      "referenceID" : 27,
      "context" : "We believe interpreting extractive (Liu and Lapata, 2019) or compressive (Xu and Durrett, 2019; Xu et al.",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 56,
      "context" : "We believe interpreting extractive (Liu and Lapata, 2019) or compressive (Xu and Durrett, 2019; Xu et al., 2020b; Desai et al., 2020) models would be worthwhile to explore and could leverage similar attribution tech-",
      "startOffset" : 73,
      "endOffset" : 133
    }, {
      "referenceID" : 57,
      "context" : "We believe interpreting extractive (Liu and Lapata, 2019) or compressive (Xu and Durrett, 2019; Xu et al., 2020b; Desai et al., 2020) models would be worthwhile to explore and could leverage similar attribution tech-",
      "startOffset" : 73,
      "endOffset" : 133
    }, {
      "referenceID" : 6,
      "context" : "We believe interpreting extractive (Liu and Lapata, 2019) or compressive (Xu and Durrett, 2019; Xu et al., 2020b; Desai et al., 2020) models would be worthwhile to explore and could leverage similar attribution tech-",
      "startOffset" : 73,
      "endOffset" : 133
    } ],
    "year" : 2021,
    "abstractText" : "Despite the prominence of neural abstractive summarization models, we know little about how they actually form summaries and how to understand where their decisions come from. We propose a two-step method to interpret summarization model decisions. We first analyze the model’s behavior by ablating the full model to categorize each decoder decision into one of several generation modes: roughly, is the model behaving like a language model, is it relying heavily on the input, or is it somewhere in between? After isolating decisions that do depend on the input, we explore interpreting these decisions using several different attribution methods. We compare these techniques based on their ability to select content and reconstruct the model’s predicted token from perturbations of the input, thus revealing whether highlighted attributions are truly important for the generation of the next token. While this machinery can be broadly useful even beyond summarization, we specifically demonstrate its capability to identify phrases the summarization model has memorized and determine where in the training pipeline this memorization happened, as well as study complex generation phenomena like sentence fusion on a per-instance basis.",
    "creator" : "LaTeX with hyperref"
  }
}