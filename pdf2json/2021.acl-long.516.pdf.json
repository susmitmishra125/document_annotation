{
  "name" : "2021.acl-long.516.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Detecting Propaganda Techniques in Memes",
    "authors" : [ "Dimitar Dimitrov", "Bishr Bin Ali", "Shaden Shaar", "Firoj Alam", "Fabrizio Silvestri", "Hamed Firooz", "Preslav Nakov", "Giovanni Da San Martino" ],
    "emails" : [ "mitko.bg.ss@gmail.com,", "bishrkc@gmail.com", "pnakov}@hbku.edu.qa,", "mhfirooz@fb.com", "fsilvestri@diag.uniroma1.it,", "dasan@math.unipd.it" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6603–6617\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6603"
    }, {
      "heading" : "1 Introduction",
      "text" : "Social media have become one of the main communication channels for information dissemination and consumption, and nowadays many people rely on them as their primary source of news (Perrin, 2015). Despite the many benefits that social media offer, sporadically they are also used as a tool, by bots or human operators, to manipulate and to mislead unsuspecting users. Propaganda is one such communication tool to influence the opinions and the actions of other people in order to achieve a predetermined goal (IPA, 1938).\nWARNING: This paper contains meme examples and words that are offensive in nature.\nPropaganda is not new. It can be traced back to the beginning of the 17th century, as reported in (Margolin, 1979; Casey, 1994; Martino et al., 2020), where the manipulation was present at public events such as theaters, festivals, and during games. In the current information ecosystem, it has evolved to computational propaganda (Woolley and Howard, 2018; Martino et al., 2020), where information is distributed through technological means to social media platforms, which in turn make it possible to reach well-targeted communities at high velocity. We believe that being aware and able to detect propaganda campaigns would contribute to a healthier online environment.\nPropaganda appears in various forms and has been studied by different research communities. There has been work on exploring network structure, looking for malicious accounts and coordinated inauthentic behavior (Cresci et al., 2017; Yang et al., 2019; Chetan et al., 2019; Pacheco et al., 2020).\nIn the natural language processing community, propaganda has been studied at the document level (Barrón-Cedeno et al., 2019; Rashkin et al., 2017), and at the sentence and the fragment levels (Da San Martino et al., 2019). There have also been notable datasets developed, including (i) TSHP-17 (Rashkin et al., 2017), which consists of document-level annotation labeled with four classes (trusted, satire, hoax, and propaganda); (ii) QProp (Barrón-Cedeno et al., 2019), which uses binary labels (propaganda vs. non-propaganda), and (iii) PTC (Da San Martino et al., 2019), which uses fragment-level annotation and an inventory of 18 propaganda techniques. While that work has focused on text, here we aim to detect propaganda techniques from a multimodal perspective. This is a new research direction, even though large part of propagandistic social media content nowadays is multimodal, e.g., in the form of memes.\nMemes are popular in social media as they can be quickly understood with minimal effort (Diresta, 2018). They can easily become viral, and thus it is important to detect malicious ones quickly, and also to understand the nature of propaganda, which can help human moderators, but also journalists, by offering them support for a higher level analysis.\nFigure 1 shows some examples of memes1 and propaganda techniques. Example (a) applies transfer, using symbols (hammer and sickle) and colors (red), that are commonly associated with communism, in relation to the two Republicans shown in the image; it also uses Name Calling (traitors, Moscow Mitch, Moscow’s bitch). The meme in (b) uses both Smears and Glittering Generalities. The one in (c) expresses Smears and suggest that Joe Biden’s campaign is only alive because of mainstream media. The examples in the second row show some less common techniques. Example (d) uses Appeal to authority to give credibility to a statement that rich politicians are crooks, and there is also a Thought-terminating cliché used to discourage critical thought on the statement in the form of the phrase “WE KNOW”, thus implying that the Clintons are crooks, which is also Smears.\n1In order to avoid potential copyright issues, all memes we show in this paper are our own recreation of existing memes, using images with clear licenses.\nThen, example (e) uses both Appeal to (Strong) Emotions and Flag-waving as it tries to play on patriotic feelings. Finally, example (f) has Reduction ad hitlerum as Ilhan Omars’ actions are related to such of a terrorist (which is also Smears; moreover, the word HATE expresses Loaded language).\nThe above examples illustrate that propaganda techniques express shortcuts in the argumentation process, e.g., by leveraging on the emotions of the audience or by using logical fallacies to influence it. Their presence does not necessarily imply that the meme is propagandistic. Thus, we do not annotate whether a meme is propagandistic (just the propaganda techniques it contains), as this would require, among other things, to determine its intent.\nOur contributions can be summarized as follows:\n• We formulate a new multimodal task: propaganda detection in memes, and we discuss how it relates and differs from previous work.\n• We develop a multi-modal annotation schema, and we create and release a new dataset for the task, consisting of 950 memes, which we manually annotate with 22 propaganda techniques.2\n2The corpus and the code used in our experiments are available at https://github.com/di-dimitrov/ propaganda-techniques-in-memes.\n• We perform manual analysis, and we show that both modalities (text and images) are important for the task.\n• We experiment with several state-of-the-art textual, visual, and multimodal models, which further confirm the importance of both modalities, as well as the need for further research."
    }, {
      "heading" : "2 Related Work",
      "text" : "Computational Propaganda Computational propaganda is defined as the use of automatic approaches to intentionally disseminate misleading information over social media platforms (Woolley and Howard, 2018). The information that is distributed over these channels can be textual, visual, or multi-modal. Of particular importance are memes, which can be quite effective at spreading multimodal propaganda on social media platforms (Diresta, 2018). The current information ecosystem and virality tools, such as bots, enable memes to spread easily, jumping from one target group to another. As of present, attempts to limit the spread of such memes have focused on analyzing social networks and looking for fake accounts and bots to reduce the spread of such content (Cresci et al., 2017; Yang et al., 2019; Chetan et al., 2019; Pacheco et al., 2020).\nTextual Content Most research on propaganda detection has focused on analyzing textual content (Barrón-Cedeno et al., 2019; Rashkin et al., 2017; Da San Martino et al., 2019; Martino et al., 2020). Rashkin et al. (2017) developed the TSHP-17corpus, which uses document-level annotation and is labeled with four classes: trusted, satire, hoax, and propaganda. TSHP-17 was developed using distant supervision, i.e., all articles from a given news outlet share the label of that outlet. The articles were collected from the English Gigaword corpus and from seven other unreliable news sources. Among them two were propagandistic. They trained a model using word n-gram representation with logistic regression and reported that the model performed well only on articles from sources that the system was trained on.\nBarrón-Cedeno et al. (2019) developed a new corpus, QProp , with two labels: propaganda vs. non-propaganda. They also experimented on TSHP-17 and QProp corpora, where for the TSHP-17 corpus, they binarized the labels: propaganda vs. any of the other three categories.\nThey performed massive experiments, investigated writing style and readability level, and trained models using logistic regression and SVMs. Their findings confirmed that using distant supervision, in conjunction with rich representations, might encourage the model to predict the source of the article, rather than to discriminate propaganda from non-propaganda. Similarly, Habernal et al. (2017, 2018) developed a corpus with 1.3k arguments annotated with five fallacies, including ad hominem, red herring, and irrelevant authority, which directly relate to propaganda techniques.\nA more fine-grained propaganda analysis was done by Da San Martino et al. (2019). They developed a corpus of news articles annotated with 18 propaganda techniques. The annotation was at the fragment level, and enabled two tasks: (i) binary classification —given a sentence in an article, predict whether any of the 18 techniques has been used in it; (ii) multi-label multi-class classification and span detection task —given a raw text, identify both the specific text fragments where a propaganda technique is being used as well as the type of the technique. On top of this work, they proposed a multi-granular deep neural network that captures signals from the sentence-level task and helps to improve the fragment-level classifier. Subsequently, a system was developed and made publicly available (Da San Martino et al., 2020).\nMultimodal Content Previous work has explored the use of multimodal content for detecting misleading information (Volkova et al., 2019), deception (Glenski et al., 2019), emotions and propaganda (Abd Kadir et al., 2016), hateful memes (Kiela et al., 2020; Lippe et al., 2020; Das et al., 2020), antisemitism (Chandra et al., 2021) and propaganda in images (Seo, 2014). Volkova et al. (2019) proposed models for detecting misleading information using images and text. They developed a corpus of 500,000 Twitter posts consisting of images labeled with six classes: disinformation, propaganda, hoaxes, conspiracies, clickbait, and satire. Then, they modeled textual, visual, and lexical characteristics of the text. Glenski et al. (2019) explored multilingual multimodal content for deception detection. They had two multi-class classification tasks: (i) classifying social media posts into four categories (propaganda, conspiracy, hoax, or clickbait), and (ii) classifying social media posts into five categories (disinformation, propaganda, conspiracy, hoax, or clickbait).\nMultimodal hateful memes have been the target of the popular “Hateful Memes Challenge”, which the participants addressed using fine-tuned state-ofart multi-modal transformer models such as ViLBERT (Lu et al., 2019), Multimodal Bitransformers (Kiela et al., 2019), and VisualBERT (Li et al., 2019) to classify hateful vs. not-hateful memes (Kiela et al., 2020). Lippe et al. (2020) explored different early-fusion multimodal approaches and proposed various methods that can improve the performance of the detection systems.\nOur work differs from the above research in terms of annotation, as we have a rich inventory of 22 fine-grained propaganda techniques, which we annotate separately in the text and then jointly in the text+image, thus enabling interesting analysis as well as systems for multi-modal propaganda detection with explainability capabilities."
    }, {
      "heading" : "3 Propaganda Techniques",
      "text" : "Propaganda comes in many forms and over time a number of techniques have emerged in the literature (Torok, 2015; Miller, 1939; Da San Martino et al., 2019; Shah, 2005; Abd Kadir and Sauffiyan, 2014; IPA, 1939; Hobbs, 2015). Different authors have proposed inventories of propaganda techniques of various sizes: seven techniques (Miller, 1939), 24 techniques Weston (2018), 18 techniques (Da San Martino et al., 2019), just smear as a technique (Shah, 2005), and seven techniques (Abd Kadir and Sauffiyan, 2014). We adapted the techniques discussed in (Da San Martino et al., 2019), (Shah, 2005) and (Abd Kadir and Sauffiyan, 2014), thus ending up with 22 propaganda techniques. Among our 22 techniques, the first 20 are used for both text and images, while the last two Appeal to (Strong) Emotions and Transfer are reserved for labeling images only. Below, we provide the definitions of these techniques, which are included in the guidelines the annotators followed (see appendix A.2) for more detal.\n1. Loaded language: Using specific words and phrases with strong emotional implications (either positive or negative) to influence an audience.\n2. Name calling or labeling: Labeling the object of the propaganda campaign as something that the target audience fears, hates, finds undesirable or loves, praises.\n3. Doubt: Questioning the credibility of someone or something.\n4. Exaggeration / Minimisation: Either representing something in an excessive manner: making things larger, better, worse (e.g., the best of the best, quality guaranteed) or making something seem less important or smaller than it really is (e.g., saying that an insult was actually just a joke).\n5. Appeal to fear / prejudices: Seeking to build support for an idea by instilling anxiety and/or panic in the population towards an alternative. In some cases, the support is built based on preconceived judgements.\n6. Slogans: A brief and striking phrase that may include labeling and stereotyping. Slogans tend to act as emotional appeals.\n7. Whataboutism: A technique that attempts to discredit an opponent’s position by charging them with hypocrisy without directly disproving their argument.\n8. Flag-waving: Playing on strong national feeling (or to any group; e.g., race, gender, political preference) to justify or promote an action or an idea."
    }, {
      "heading" : "9. Misrepresentation of someone’s position",
      "text" : "(Straw man): Substituting an opponent’s proposition with a similar one, which is then refuted in place of the original proposition.\n10. Causal oversimplification: Assuming a single cause or reason when there are actually multiple causes for an issue. This includes transferring blame to one person or group of people without investigating the complexities of the issue.\n11. Appeal to authority: Stating that a claim is true simply because a valid authority or expert on the issue said it was true, without any other supporting evidence offered. We also include here the special case where the reference is not an authority or an expert, which is referred to as Testimonial in the literature.\n12. Thought-terminating cliché: Words or phrases that discourage critical thought and meaningful discussion about a given topic. They are typically short, generic sentences that offer seemingly simple answers to complex questions or that distract the attention away from other lines of thought.\n13. Black-and-white fallacy or dictatorship: Presenting two alternative options as the only possibilities, when in fact more possibilities exist. As an the extreme case, tell the audience exactly what actions to take, eliminating any other possible choices (Dictatorship).\n14. Reductio ad hitlerum: Persuading an audience to disapprove an action or an idea by suggesting that the idea is popular with groups hated in contempt by the target audience. It can refer to any person or concept with a negative connotation.\n15. Repetition: Repeating the same message over and over again, so that the audience will eventually accept it.\n16. Obfuscation, Intentional vagueness, Confusion: Using words that are deliberately not clear, so that the audience may have their own interpretations. For example, when an unclear phrase with multiple possible meanings is used within an argument and, therefore, it does not support the conclusion.\n17. Presenting irrelevant data (Red Herring): Introducing irrelevant material to the issue being discussed, so that everyone’s attention is diverted away from the points made.\n18. Bandwagon Attempting to persuade the target audience to join in and take the course of action because “everyone else is taking the same action.”\n19. Smears: A smear is an effort to damage or call into question someone’s reputation, by propounding negative propaganda. It can be applied to individuals or groups.\n20. Glittering generalities (Virtue): These are words or symbols in the value system of the target audience that produce a positive image when attached to a person or an issue.\n21. Appeal to (strong) emotions: Using images with strong positive/negative emotional implications to influence an audience.\n22. Transfer: Also known as association, this is a technique that evokes an emotional response by projecting positive or negative qualities (praise or blame) of a person, entity, object, or value onto another one in order to make the latter more acceptable or to discredit it."
    }, {
      "heading" : "4 Dataset",
      "text" : "We collected memes from our own private Facebook accounts, and we followed various Facebook public groups on different topics such as vaccines, politics (from different parts of the political spectrum), COVID-19, gender equality, and more. We wanted to make sure that we have a constant stream of memes in the newsfeed. We extracted memes at different time frames, i.e., once every few days for a period of three months. We also collected some old memes for each group in order to make sure we covered a larger variety of topics."
    }, {
      "heading" : "4.1 Annotation Process",
      "text" : "We annotated the memes using the 22 propaganda techniques described in Section 3 in a multilabel setup. The motivation for multilabel annotation is that the content in the memes often expresses multiple techniques, even though such a setting adds complexity both in terms of annotation and of classification. We also chose to consider annotating spans because the propaganda techniques can appear in the different chunk(s), which is also in line with recent research (Da San Martino et al., 2019). We could not consider annotating the visual modality independently because all memes contain the text as part of the image.\nThe annotation team included six members, both female and male, all fluent in English, with qualifications ranging from undergrad, to MSc and PhD degrees, including experienced NLP researchers; this helped to ensure the quality of the annotation. No incentives were provided to the annotators. The annotation process required understanding the textual and the visual content, which poses a great challenge for the annotator. Thus, we divided it into five phases, as discussed below and as shown in Figure 2. Among these phases there were three stages, (i) pilot annotations to train the annotators to recognize the propaganda techniques, (ii) independent annotations by three annotators for each meme (phase 2 and 4), (iii) consolidation (phase 3 and 5), where the annotators met with the other three team members, who acted as consolidators, and all six discussed every single example in detail (even those for which there was no disagreement).\nWe chose PyBossa3 as our annotation platform as it provides the functionality to create a custom annotation interface that can fit our needs in each phase of the annotation.\n3https://pybossa.com"
    }, {
      "heading" : "4.1.1 Phase 1: Filtering and Text Editing",
      "text" : "Phase 1 is about filtering some of the memes according to our guidelines, e.g., low-quality memes, and such containing no propaganda technique. We automatically extracted the textual content using OCR, and then post-edited it to correct for potential OCR errors. We filtered and edited the text manually, whereas for extracting the text, we used the Google Vision API.4 We presented the original meme and the extracted text to an annotator, who had to filter and to edit the text in phase 1 as shown in Figure 2. For filtering and editing, we defined a set of rules, e.g., we removed hard to understand, or low-quality images, cartoons, memes with no picture, no text, or for which the textual content was strongly dominant and the visual content was minimal and uninformative, e.g., a single-color background. More details about filtering and editing are given in Appendix A.1.1 and A.1.2."
    }, {
      "heading" : "4.1.2 Phase 2: Text Annotation",
      "text" : "In phase 2, we presented the edited textual content of the meme to the annotators as shown in Figure 2. We asked the annotators to identify the propaganda techniques in the text and to select the corresponding text spans for each of them."
    }, {
      "heading" : "4.1.3 Phase 3: Text Consolidation",
      "text" : "Phase 3 is the consolidation step of the annotations from phase 2 as shown in Figure 2. This phase was essential for ensuring the quality, and it further served as an additional training opportunity for the entire team, which we found very useful.\n4http://cloud.google.com/vision"
    }, {
      "heading" : "4.1.4 Phase 4: Multimodal Annotation",
      "text" : "Step 4 is multimodal meme annotation, i.e., considering both the textual and the visual content in the meme. In this phase, we show the meme, the post-edited text, and the consolidated propaganda labels from phase 3 (text only) to the annotators, as shown in phase 4 from Figure 2. We intentionally provided the consolidated text labels to the annotators in this phase because we wanted them to focus on the techniques that require the presence of the image rather than to reannotate those from the text.5"
    }, {
      "heading" : "4.1.5 Phase 5: Multimodal Consolidation",
      "text" : "This is the consolidation phase for Phase 4; the setup is like for the consolidation at Phase 3, as shown in Figure 2.\nNote that, in the majority of the cases, the main reason why two annotations of the same meme might differ was due to one of the annotators not spotting some of the techniques, rather than because there was a disagreement on what technique should be chosen for a given textual span or what the exact boundaries of the span for a given technique instance should be. In the rare cases in which there was an actual disagreement and no clear conclusion could be reached during the discussion phase, we resorted to discarding the meme (there were five such cases in total).\n5Ideally, we would have wanted to have also a phase to annotate propaganda techniques when showing the image only; however, this is hard to do in practice as the text is embedded as part of the pixels in the image."
    }, {
      "heading" : "4.2 Quality of the Annotations",
      "text" : "We assessed the quality of the annotations for the individual annotators from phases 2 and 4 (thus, combining the annotations for text and images) to the final consolidated labels at phase 5, following the setting in (Da San Martino et al., 2019). Since our annotation is multilabel, we computed Krippendorff’s α, which supports multi-label agreement computation (Artstein and Poesio, 2008; Passonneau, 2006). The results are shown in Table 1 and indicate moderate to perfect agreement (Landis and Koch, 1977)."
    }, {
      "heading" : "Agreement Pair Krippendorff’s α",
      "text" : ""
    }, {
      "heading" : "Propaganda Techniques Text-Only Meme",
      "text" : ""
    }, {
      "heading" : "4.3 Statistics",
      "text" : "After the filtering in phase 1 and the final consolidation, our dataset consists of 950 memes. The maximum number of sentences per meme is 13, but most memes comprise only very few sentences, with an average of 1.68. The number of words ranges between 1 and 73 words, with an average of 17.79±11.60. In our analysis, we observed that some propaganda techniques were more textual, e.g., Loaded Language and Name Calling, while others, such as Transfer, tended to be more imagerelated.\nTable 2 shows the number of instances of each technique when using unimodal (text only, i.e., after phase 3) vs. multimodal (text + image, i.e., after phase 5) annotations. Note also that a total of 36 memes had no propaganda technique annotated. We can see that the most common techniques are Smears, Loaded Language, and Name calling/Labeling, covering 63%, 51%, and 36% of the examples, respectively. These three techniques also form the most common pairs and triples in the dataset as shown in Table 3. We further show the distribution of the number of propaganda techniques per meme in Figure 3. We can see that most memes contain more than one technique, with a maximum of 8 and an average of 2.61.\nTable 2 shows that the techniques can be found both in the textual and in the visual content of the meme, thus suggesting the use of multimodal learning approaches to effectively exploit all information available. Note also that different techniques have different span lengths. For example, Loaded Language is about two words long, e.g., violence, mass shooter, and coward. However, techniques such as Whataboutism need much longer spans with an average length of 22 words."
    }, {
      "heading" : "5 Experiments",
      "text" : "Among the learning tasks that can be defined on our corpus, here we focus on the following one: given a meme, find all the propaganda techniques used in it, both in the text and in the image, i.e., predict the techniques as per phase 5."
    }, {
      "heading" : "5.1 Models",
      "text" : "We used two naı̈ve baselines. First, a Random baseline, where we assign a technique uniformly at random. Second, a Majority class baseline, which always predicts the most frequent class: Smears.\nUnimodal: text only. For the text-based unimodal experiments, we used BERT (Devlin et al., 2019), which is a state-of-the-art pre-trained Transformer, and fastText (Joulin et al., 2017), which can tolerate potentially noisy text from social media as it is trained on word and character n-grams.\nUnimodal: image. For the image-based unimodal experiments, we used ResNet152 (He et al., 2016), which was successfully applied in a related setup (Kiela et al., 2019).\nMultimodal: unimodally pretrained For the multimodal experiments, we trained separate models on the text and on the image, BERT and ResNet152, respectively, and then we combined them using (a) early fusion Multimodal Bitransformers (MMBT) (Kiela et al., 2019), (b) middle fusion (feature concatenation), and (c) late fusion (combining the predictions of the models). For middle fusion, we took the output of the second-to-last layer of ResNet-152 for the visual part and the output of the [CLS] token from BERT, and we fed them into a multilayer network.\nMultimodal: joint models. We further experimented with models trained using a multimodal objective. In particular, we used ViLBERT (Lu et al., 2019), which is pretrained on Conceptual Captions (Sharma et al., 2018), and Visual BERT (Lin et al., 2014), which is pretrained on the MSCOCO dataset (Lin et al., 2014)."
    }, {
      "heading" : "5.2 Experimental Settings",
      "text" : "We split the data into training, development, and testing with 687 (72%), 63 (7%), and 200 (21%) examples, respectively. Since we are dealing with a multi-class multi-label task, where the labels are imbalanced, we chose micro-average F1 as our main evaluation measure, but we also report macroaverage F1.\nWe used the Multimodal Framework (MMF) (Singh et al., 2020). We trained all models on Tesla P100-PCIE-16GB GPU with the following manually tuned hyper-parameters (on dev): batch size of 32, early stopping on the validation set optimizing for F1-micro, sequence length of 128, AdamW as an optimizer with learning rate of 5e-5, epsilon of 1e-8, and weight decay of 0.01. All reported results are averaged over three runs with random seeds. The average execution time for BERT was 30 minutes, and for the other models it was 55 minutes."
    }, {
      "heading" : "6 Experimental Results",
      "text" : "Table 4 shows the results for the models in Section 5.1. Rows 1 and 2 show a random and a majority class baseline, respectively. Rows 3-5 show the results for the unimodal models. While they all outperform the baselines, we can see that the model based on visual modality only, i.e., ResNet-152 (row 3), performs worse than models based on text only (rows 4-5). This might indicate that identifying the techniques in the visual content is a harder task than in texts. Moreover, BERT significantly outperforms fastText, which is to be expected as it can capture contextual representation better.\nRows 6-8 present results for multimodal fusion models. The best one is BERT + ResNet-152 (+2 points over fastText + ResNet-152). We observe that early fusion models (rows 7-8) outperform late fusion ones (row 6). This makes sense as late fusion is a simple mean of the results of each modality, while early fusion has a more complex architecture and trains a separate multi-layer perceptron for the visual and for the textual features.\nWe can also see that both mid-fusion models (rows 7-8) improve over the corresponding textonly ones (rows 3-5). Finally, looking at the results in rows 9-11, we can see that each multimodal model consistently outperforms each of the unimodal models (rows 1-8). The best results are achieved with ViLBERT CC (row 10) and VisualBERT COCO (row 11), which use complex representations that combine the textual and the visual modalities. Overall, we can conclude that multimodal approaches are necessary to detect the use of propaganda techniques in memes, and that pretrained transformer models seem to be the most promising approach."
    }, {
      "heading" : "7 Conclusion and Future Work",
      "text" : "We have proposed a new multi-class multi-label multimodal task: detecting the type of propaganda techniques used in memes. We further created and released a corpus of 950 memes annotated with 22 propaganda techniques, which can appear in the text, in the image, or in both. Our analysis of the corpus has shown that understanding both modalities is essential for detecting these techniques, which was further confirmed in our experiments with several state-of-the-art multimodal models.\nIn future work, we plan to extend the dataset in size, including with memes in other languages. We further plan to develop new multi-modal models, specifically tailored to fine-grained propaganda detection, aiming for deeper understanding of the semantics of the meme and of the relation between the text and the image. A number of promising ideas have been already tried by the participants in a shared task based on this data at SemEval2021 (Dimitrov et al., 2021), which can serve as an inspiration when developing new models."
    }, {
      "heading" : "Ethics and Broader Impact",
      "text" : "User Privacy Our dataset only includes memes and it does not contain any user information.\nBiases Any biases found in the dataset are unintentional, and we do not intend to do harm to any group or individual. We note that annotating propaganda techniques can be subjective, and thus it is inevitable that there would be biases in our goldlabeled data or in the label distribution. We address these concerns by collecting examples from a variety of users and groups, and also by following a well-defined schema, which has clear definitions. Our high inter-annotator agreement makes us confident that the assignment of the schema to the data is correct most of the time.\nMisuse Potential We ask researchers to be aware that our dataset can be maliciously used to unfairly moderate memes based on biases that may or may not be related to demographics and other information within the text. Intervention with human moderation would be required in order to ensure this does not occur.\nIntended Use We present our dataset to encourage research in studying harmful memes on the web. We believe that it represents a useful resource when used in the appropriate manner."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research is part of the Tanbih mega-project,6 which is developed at the Qatar Computing Research Institute, HBKU, and aims to limit the impact of “fake news,” propaganda, and media bias by making users aware of what they are reading.\n6http://tanbih.qcri.org/"
    }, {
      "heading" : "Appendix",
      "text" : ""
    }, {
      "heading" : "A Annotation Instructions",
      "text" : ""
    }, {
      "heading" : "A.1 Guidelines for Annotators - Phases 1",
      "text" : "The annotators were presented with the following guidelines during phase 1 for filtering and editing the text of the memes."
    }, {
      "heading" : "A.1.1 Choice of memes/Filtering Criteria",
      "text" : "In order to ensure consistency for our data, we defined meme as a photograph-style image with a short text on top. We asked the annotators to exclude memes with the below characteristics. During this phase, we filtered out 111 memes.\n• Images with diagrams/graphs/tables.\n• Memes for which no multimodal analysis is possible: e.g., only text, only image, etc.\n• Cartoons."
    }, {
      "heading" : "A.1.2 Rules for Text Editing",
      "text" : "We used the Google Vision API7 to extract the text from the memes. As the output of the system sometimes contains errors, a manual checking was needed. Thus, we defined several text editing rules as listed below, and we applied them to the textual content extracted from each meme.\n1. When the meme is a screenshot of a social network account, e.g., WhatsApp, the user name and login can be removed as well as all Like, Comment, and Share elements.\n2. Remove the text related to logos that are not part of the main text.\n3. Remove all text related to figures and tables.\n4. Remove all text that is partially hidden by an image, so that the sentence is almost impossible to read.\n5. Remove text that is not from the meme, but on banners and billboards carried on by demonstrators, street advertisements, etc.\n6. Remove the author of the meme if it is signed.\n7. If the text is in columns, first put all text from the first column, then all text from the next column, etc.\n8. Rearrange the text, so that there is one sentence per line, whenever possible.\n7http://cloud.google.com/vision\n9. If there are separate blocks of text in different locations of the image, separate them by a blank line. However, if it is evident that text blocks are part of a single sentence, keep them together."
    }, {
      "heading" : "A.2 Guidelines for Annotators - Phases 2-5",
      "text" : "The annotators were presented with the following guidelines. In these phases, the annotations were performed by three annotators."
    }, {
      "heading" : "A.2.1 Annotation Phase 2",
      "text" : "Given the list of propaganda techniques for the textonly annotation task, as described in Section A.3 (techniques 1-20), and the textual content of a meme, the task is to identify which techniques appear in the text and the exact span for each of them."
    }, {
      "heading" : "A.2.2 Annotation Phase 4",
      "text" : "In this phase, the task was to identify which of the 22 techniques, described in Section A.3, appear in the meme, i.e., both in the text and in the visual content. Note that some of the techniques occurring in the text might be identified only in this phase because the image provides a necessary context."
    }, {
      "heading" : "A.2.3 Consolidation (Phase 3 and 5)",
      "text" : "In this phase, the three annotators met together with other consolidators and discussed each annotation, so that a consensus on each of them is reached. These phases are devoted to checking existing annotations. However, when a novel instance of a technique is observed during the consolidation, it is added."
    }, {
      "heading" : "A.3 Definitions of Propaganda Techniques",
      "text" : "1. Presenting irrelevant data (Red Herring) Introducing irrelevant material to the issue being discussed, so that everyone’s attention is diverted away from the points made.\nExample 1: In politics, defending one’s own policies regarding public safety – “I have worked hard to help eliminate criminal activity. What we need is economic growth that can only come from the hands of leadership.” Example 2: “You may claim that the death penalty is an ineffective deterrent against crime – but what about the victims of crime? How do you think surviving family members feel when they see the man who murdered their son kept in prison at their expense? Is it right that they should pay for their son’s murderer to be fed and housed?”\n2. Misrepresentation of someone’s position (Straw Man) When an opponent’s proposition is substituted with a similar one, which is then refuted in place of the original proposition.\nExample: Zebedee: What is your view on the Christian God? Mike: I don’t believe in any gods, including the Christian one. Zebedee: So you think that we are here by accident, and all this design in nature is pure chance, and the universe just created itself? Mike: You got all that from me stating that I just don’t believe in any gods?\n3. Whataboutism A technique that attempts to discredit an opponent’s position by charging them with hypocrisy without directly disproving their argument.\nExample 1: a nation deflects criticism of its recent human rights violations by pointing to the history of slavery in the United States. Example 2:“Qatar spending profusely on Neymar, not fighting terrorism”\n4. Causal oversimplification Assuming a single cause or reason when there are actually multiple causes for an issue. It includes transferring blame to one person or group of people without investigating the complexities of the issue. An example is shown in Figure 4(b).\nExample 1: “President Trump has been in office for a month and gas prices have been skyrocketing. The rise in gas prices is because of him.” Example 2: The reason New Orleans was hit so hard with the hurricane was because of all the immoral people who live there.\n5. Obfuscation, Intentional vagueness, Confusion Using words which are deliberately not clear so that the audience may have their own interpretations. For example, when an unclear phrase with multiple definitions is used within the argument and, therefore, it does not support the conclusion.\nExample: It is a good idea to listen to victims of theft. Therefore if the victims say to have the thief shot, then you should do that.\n6. Appeal to authority Stating that a claim is true simply because a valid authority or expert on the issue said it was true, without any other supporting evidence offered. We consider the special case in which the reference is not an authority or an expert in this technique, although it is referred to as Testimonial in literature.\nExample 1: Richard Dawkins, an evolutionary biologist and perhaps the foremost expert in the field, says that evolution is true. Therefore, it’s true. Example 2: “According to Serena Williams, our foreign policy is the best on Earth. So we are in the right direction.”\n7. Black-and-white Fallacy Presenting two alternative options as the only possibilities, when in fact more possibilities exist. We include dictatorship, which happens when we leave only one possible option, i.e., when we tell the audience exactly what actions to take, eliminating any other possible choices. An example of this technique is shown in Figure 4(c).\nExample 1: You must be a Republican or Democrat. You are not a Democrat. Therefore, you must be a Republican. Example 2: I thought you were a good person, but you weren’t at church today.\n8. Name Calling or Labeling Labeling the object of the propaganda campaign as either something the target audience fears, hates, finds undesirable or loves, praises.\nExamples: Republican congressweasels, Bush the Lesser. Note that here lesser does not refer to the second, but it is pejorative.\n9. Loaded Language Using specific words and phrases with strong emotional implications (either positive or negative) to influence an audience.\nExample 1: “[...] a lone lawmaker’s childish shouting.” Example 2: “how stupid and petty things have become in Washington.”\n10. Exaggeration or Minimisation Either representing something in an excessive manner: making things larger, better, worse (e.g., the best of the best, quality guaranteed) or making something seem less important or smaller than it really is (e.g., saying that an insult was just a joke). An example meme is shown in Figure 4(a).\nExample 1: “Democrats bolted as soon as Trump’s speech ended in an apparent effort to signal they can’t even stomach being in the same room as the President.” Example 2: “We’re going to have unbelievable intelligence.”\n11. Flag-waving Playing on strong national feeling (or to any group, e.g., race, gender, political preference) to justify or promote an action or idea.\nExample 1: “patriotism mean no questions” (this is also a slogan) Example 2: “Entering this war will make us have a better future in our country.”\n12. Doubt Questioning the credibility of someone or something.\nExample: A candidate talks about his opponent and says: “Is he ready to be the Mayor?”\n13. Appeal to fear/prejudice Seeking to build support for an idea by instilling anxiety and/or panic in the population towards an alternative. In some cases the support is built based on preconceived judgements. An example is shown in Figure 4(c).\nExample 1: “Wither we go to war or we will perish.” Note that, this is also a Black and White fallacy. Example 2: “We must stop those refugees as they are terrorists.”\n14. Slogans A brief and striking phrase that may include labeling and stereotyping. Slogans tend to act as emotional appeals.\nExample 1: “The more women at war. . . the sooner we win.” Example 2: “Make America great again!”\n15. Thought-terminating cliché Words or phrases that discourage critical thought and meaningful discussion about a given topic. They are typically short, generic sentences that offer seemingly simple answers to complex questions or that distract attention away from other lines of thought. Examples: It is what it is; It’s just common sense;\nYou gotta do what you gotta do; Nothing is permanent except change; Better late than never; Mind your own business; Nobody’s perfect; It doesn’t matter; You can’t change human nature.\n16. Bandwagon Attempting to persuade the target audience to join in and take the course of action because “everyone else is taking the same action”.\nExample 1: Would you vote for Clinton as president? 57% say “yes.” Example 2: 90% of citizens support our initiative. You should.\n17. Reductio ad hitlerum Persuading an audience to disapprove an action or idea by suggesting that the idea is popular with groups hated in contempt by the target audience. It can refer to any person or concept with a negative connotation. An examples is shown in Figure 4(d).\nExample 1: “Do you know who else was doing that? Hitler!” Example 2: “Only one kind of person can think in that way: a communist.”\n18. Repetition Repeating the same message over and over again so that the audience will eventually accept it.\n19. Smears A smear is an effort to damage or call into question someone’s reputation, by propounding negative propaganda. It can be applied to individuals or groups. An example meme is shown in Figure 4(a).\n20. Glittering generalities These are words or symbols in the value system of the target audience that produce a positive image when attached to a person or issue. Peace, hope, happiness, security, wise leadership, freedom, “The Truth”, etc. are virtue words. Virtue can be also expressed in images, where a person or an object is depicted positively. In Figure 4(f), we provide an example to depict such a scenario.\n21. Transfer Also known as association, this is a technique of projecting positive or negative qualities (praise or blame) of a person, entity, object, or value onto another to make the second more acceptable or to discredit it. It evokes an emotional response, which stimulates the target to identify with recognized authorities. Often highly visual, this technique often uses symbols (e.g., the swastikas used in Nazi Germany, originally a symbol for health and prosperity) superimposed over other visual images.\n22. Appeal to (strong) emotions Using images with strong positive/negative emotional implications to influence an audience. Figure 4(f) shows an example."
    }, {
      "heading" : "B Hyper-parameter Values",
      "text" : "In this section, we list the values of the hyperparameters we used when training our models.\n• Batch size: 32\n• Optimizer: AdamW\n– Learning rate: 5e-5 – epsilon: 1e-8 – weight decay: 0.01\n• Max sequence length: 128\n• Number of epochs: 37\n• Early stopping: F1-micro on dev set\nWe further give statistics about the number of parameters for each model, so that one can get an idea about their complexity:\n• ResNet-152: 60,300,000\n• fastText: 6,020\n• BERT (bert-base-uncased): 110,683,414\n• fastText + ResNet-152 (early fusion): 11,194,398\n• BERT + ResNet-152 (late fusion): 170,983,752\n• MMBT: 110,683,414\n• ViLBERT CC: 112,044,290\n• VisualBERT COCO: 247,782,404"
    } ],
    "references" : [ {
      "title" : "Emotion and techniques of propaganda in youtube videos",
      "author" : [ "Shamsiah Abd Kadir", "Anitawati Lokman", "T. Tsuchiya." ],
      "venue" : "Indian Journal of Science and Technology, Vol (9):1–8.",
      "citeRegEx" : "Kadir et al\\.,? 2016",
      "shortCiteRegEx" : "Kadir et al\\.",
      "year" : 2016
    }, {
      "title" : "A content analysis of propaganda in harakah newspaper",
      "author" : [ "Shamsiah Abd Kadir", "Ahmad Sauffiyan." ],
      "venue" : "Journal of Media and Information Warfare, 5:73–116.",
      "citeRegEx" : "Kadir and Sauffiyan.,? 2014",
      "shortCiteRegEx" : "Kadir and Sauffiyan.",
      "year" : 2014
    }, {
      "title" : "Inter-coder agreement for computational linguistics",
      "author" : [ "Ron Artstein", "Massimo Poesio." ],
      "venue" : "Computational Linguistics, 34(4):555–596.",
      "citeRegEx" : "Artstein and Poesio.,? 2008",
      "shortCiteRegEx" : "Artstein and Poesio.",
      "year" : 2008
    }, {
      "title" : "Proppy: Organizing the news based on their propagandistic content",
      "author" : [ "Alberto Barrón-Cedeno", "Israa Jaradat", "Giovanni Da San Martino", "Preslav Nakov." ],
      "venue" : "Information Processing & Management, 56(5):1849–1864.",
      "citeRegEx" : "Barrón.Cedeno et al\\.,? 2019",
      "shortCiteRegEx" : "Barrón.Cedeno et al\\.",
      "year" : 2019
    }, {
      "title" : "What is propaganda? historians.org",
      "author" : [ "Ralph D. Casey" ],
      "venue" : null,
      "citeRegEx" : "Casey.,? \\Q1994\\E",
      "shortCiteRegEx" : "Casey.",
      "year" : 1994
    }, {
      "title" : "Subverting the Jewtocracy”: Online antisemitism detection using multimodal deep learning",
      "author" : [ "Mohit Chandra", "Dheeraj Pailla", "Himanshu Bhatia", "Aadilmehdi Sanchawala", "Manish Gupta", "Manish Shrivastava", "Ponnurangam Kumaraguru." ],
      "venue" : "arXiv",
      "citeRegEx" : "Chandra et al\\.,? 2021",
      "shortCiteRegEx" : "Chandra et al\\.",
      "year" : 2021
    }, {
      "title" : "CoReRank: Ranking to detect users involved in blackmarket-based collusive retweeting activities",
      "author" : [ "Aditya Chetan", "Brihi Joshi", "Hridoy Sankar Dutta", "Tanmoy Chakraborty." ],
      "venue" : "Proceedings of the Twelfth ACM International Conference on Web Search and",
      "citeRegEx" : "Chetan et al\\.,? 2019",
      "shortCiteRegEx" : "Chetan et al\\.",
      "year" : 2019
    }, {
      "title" : "The paradigm-shift of social spambots: Evidence, theories, and tools for the arms race",
      "author" : [ "Stefano Cresci", "Roberto Di Pietro", "Marinella Petrocchi", "Angelo Spognardi", "Maurizio Tesconi." ],
      "venue" : "Proceedings of the 26th International Conference on World Wide",
      "citeRegEx" : "Cresci et al\\.,? 2017",
      "shortCiteRegEx" : "Cresci et al\\.",
      "year" : 2017
    }, {
      "title" : "Prta: A system to support the analysis of propaganda techniques in the news",
      "author" : [ "Giovanni Da San Martino", "Shaden Shaar", "Yifan Zhang", "Seunghak Yu", "Alberto Barrón-Cedeno", "Preslav Nakov." ],
      "venue" : "Proceedings of the Annual Meeting of Association for Com-",
      "citeRegEx" : "Martino et al\\.,? 2020",
      "shortCiteRegEx" : "Martino et al\\.",
      "year" : 2020
    }, {
      "title" : "Fine-grained analysis of propaganda in news articles",
      "author" : [ "Giovanni Da San Martino", "Seunghak Yu", "Alberto Barrón-Cedeño", "Rostislav Petrov", "Preslav Nakov." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Martino et al\\.,? 2019",
      "shortCiteRegEx" : "Martino et al\\.",
      "year" : 2019
    }, {
      "title" : "Detecting hate speech in multi-modal memes",
      "author" : [ "Abhishek Das", "Japsimar Singh Wahi", "Siyao Li." ],
      "venue" : "arXiv preprint arXiv:2012.14891.",
      "citeRegEx" : "Das et al\\.,? 2020",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "SemEval-2021 Task 6: Detection of persuasion techniques in texts and images",
      "author" : [ "Dimitar Dimitrov", "Bishr Bin Ali", "Shaden Shaar", "Firoj Alam", "Fabrizio Silvestri", "Hamed Firooz", "Preslav Nakov", "Giovanni Da San Martino." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Dimitrov et al\\.,? 2021",
      "shortCiteRegEx" : "Dimitrov et al\\.",
      "year" : 2021
    }, {
      "title" : "Computational propaganda: If you make it trend, you make it true",
      "author" : [ "Renee Diresta." ],
      "venue" : "The Yale Review, 106(4):12–29.",
      "citeRegEx" : "Diresta.,? 2018",
      "shortCiteRegEx" : "Diresta.",
      "year" : 2018
    }, {
      "title" : "Multilingual multimodal digital deception detection and disinformation spread across social platforms",
      "author" : [ "Maria Glenski", "E. Ayton", "J. Mendoza", "Svitlana Volkova." ],
      "venue" : "ArXiv, abs/1909.05838.",
      "citeRegEx" : "Glenski et al\\.,? 2019",
      "shortCiteRegEx" : "Glenski et al\\.",
      "year" : 2019
    }, {
      "title" : "Argotario: Computational argumentation meets serious games",
      "author" : [ "Ivan Habernal", "Raffael Hannemann", "Christian Pollak", "Christopher Klamm", "Patrick Pauli", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Habernal et al\\.,? 2017",
      "shortCiteRegEx" : "Habernal et al\\.",
      "year" : 2017
    }, {
      "title" : "Adapting serious game for fallacious argumentation to German: Pitfalls, insights, and best practices",
      "author" : [ "Ivan Habernal", "Patrick Pauli", "Iryna Gurevych." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and",
      "citeRegEx" : "Habernal et al\\.,? 2018",
      "shortCiteRegEx" : "Habernal et al\\.",
      "year" : 2018
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, CVPR ’16, pages 770–778, Las Vegas, NV, USA.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Mind Over Media: Analyzing Contemporary Propaganda",
      "author" : [ "Renee Hobbs." ],
      "venue" : "Media Education Lab.",
      "citeRegEx" : "Hobbs.,? 2015",
      "shortCiteRegEx" : "Hobbs.",
      "year" : 2015
    }, {
      "title" : "In Institute for Propaganda Analysis, editor, Propaganda Analysis",
      "author" : [ "IPA." ],
      "venue" : "Volume I of the Publications of the Institute for Propaganda Analysis. New York, NY, USA.",
      "citeRegEx" : "IPA.,? 1938",
      "shortCiteRegEx" : "IPA.",
      "year" : 1938
    }, {
      "title" : "In Institute for Propaganda Analysis, editor, Propaganda Analysis",
      "author" : [ "IPA." ],
      "venue" : "Volume II of the Publications of the Institute for Propaganda Analysis. New York, NY, USA.",
      "citeRegEx" : "IPA.,? 1939",
      "shortCiteRegEx" : "IPA.",
      "year" : 1939
    }, {
      "title" : "Bag of tricks for efficient text classification",
      "author" : [ "Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Tomas Mikolov." ],
      "venue" : "Proceeding of the 15th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’17, pages 427–431,",
      "citeRegEx" : "Joulin et al\\.,? 2017",
      "shortCiteRegEx" : "Joulin et al\\.",
      "year" : 2017
    }, {
      "title" : "Supervised multimodal bitransformers for classifying images and text",
      "author" : [ "Douwe Kiela", "Suvrat Bhooshan", "Hamed Firooz", "Ethan Perez", "Davide Testuggine." ],
      "venue" : "Proceedings of the NeuIPS workshop on Visually Grounded Interaction and Language,",
      "citeRegEx" : "Kiela et al\\.,? 2019",
      "shortCiteRegEx" : "Kiela et al\\.",
      "year" : 2019
    }, {
      "title" : "The hateful memes challenge: Detecting hate speech in multimodal memes",
      "author" : [ "Douwe Kiela", "Hamed Firooz", "Aravind Mohan", "Vedanuj Goswami", "Amanpreet Singh", "Pratik Ringshia", "Davide Testuggine." ],
      "venue" : "Proceedings of the Annual Conference on Neural",
      "citeRegEx" : "Kiela et al\\.,? 2020",
      "shortCiteRegEx" : "Kiela et al\\.",
      "year" : 2020
    }, {
      "title" : "The measurement of observer agreement for categorical data",
      "author" : [ "J Richard Landis", "Gary G Koch." ],
      "venue" : "biometrics, pages 159–174.",
      "citeRegEx" : "Landis and Koch.,? 1977",
      "shortCiteRegEx" : "Landis and Koch.",
      "year" : 1977
    }, {
      "title" : "VisualBERT: A simple and performant baseline for vision and language",
      "author" : [ "Liunian Harold Li", "Mark Yatskar", "Da Yin", "Cho-Jui Hsieh", "Kai-Wei Chang." ],
      "venue" : "arXiv preprint arXiv:1908.03557.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Microsoft COCO: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C Lawrence Zitnick." ],
      "venue" : "European Conference on Computer Vision, ECCV ’14, pages 740–",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "A multimodal framework for the detection of hateful memes",
      "author" : [ "Phillip Lippe", "Nithin Holla", "Shantanu Chandra", "Santhosh Rajamanickam", "Georgios Antoniou", "Ekaterina Shutova", "Helen Yannakoudakis." ],
      "venue" : "arXiv preprint arXiv:2012.12871.",
      "citeRegEx" : "Lippe et al\\.,? 2020",
      "shortCiteRegEx" : "Lippe et al\\.",
      "year" : 2020
    }, {
      "title" : "ViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "author" : [ "Jiasen Lu", "Dhruv Batra", "Devi Parikh", "Stefan Lee." ],
      "venue" : "Proceedings of the Conference on Neural Information Processing Systems, NeurIPS ’19,",
      "citeRegEx" : "Lu et al\\.,? 2019",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2019
    }, {
      "title" : "The visual rhetoric of propaganda",
      "author" : [ "V. Margolin." ],
      "venue" : "Information Design Journal, 1:107–122.",
      "citeRegEx" : "Margolin.,? 1979",
      "shortCiteRegEx" : "Margolin.",
      "year" : 1979
    }, {
      "title" : "A survey on computational propaganda detection",
      "author" : [ "Giovanni Da San Martino", "Stefano Cresci", "Alberto Barrón-Cedeño", "Seunghak Yu", "Roberto Di Pietro", "Preslav Nakov." ],
      "venue" : "Proceedings of the Twenty-Ninth International Joint Conference on Ar-",
      "citeRegEx" : "Martino et al\\.,? 2020",
      "shortCiteRegEx" : "Martino et al\\.",
      "year" : 2020
    }, {
      "title" : "The Techniques of Propaganda",
      "author" : [ "Clyde R. Miller." ],
      "venue" : "From “How to Detect and Analyze Propaganda,” an address given at Town Hall. The Center for learning.",
      "citeRegEx" : "Miller.,? 1939",
      "shortCiteRegEx" : "Miller.",
      "year" : 1939
    }, {
      "title" : "Unveiling coordinated groups behind white helmets disinformation",
      "author" : [ "Diogo Pacheco", "Alessandro Flammini", "Filippo Menczer." ],
      "venue" : "Proceedings of the Web Conference 2020, WWW ’20, pages 611– 616, New York, USA.",
      "citeRegEx" : "Pacheco et al\\.,? 2020",
      "shortCiteRegEx" : "Pacheco et al\\.",
      "year" : 2020
    }, {
      "title" : "Measuring agreement on set-valued items (MASI) for semantic and pragmatic annotation",
      "author" : [ "Rebecca Passonneau." ],
      "venue" : "Proceedings of the Fifth International Conference on Language Resources and Evaluation, LREC ’06, pages 831–836, Genoa, Italy.",
      "citeRegEx" : "Passonneau.,? 2006",
      "shortCiteRegEx" : "Passonneau.",
      "year" : 2006
    }, {
      "title" : "Social media usage",
      "author" : [ "Andrew Perrin." ],
      "venue" : "Pew research center, pages 52–68.",
      "citeRegEx" : "Perrin.,? 2015",
      "shortCiteRegEx" : "Perrin.",
      "year" : 2015
    }, {
      "title" : "Truth of varying shades: Analyzing language in fake news and political fact-checking",
      "author" : [ "Hannah Rashkin", "Eunsol Choi", "Jin Yea Jang", "Svitlana Volkova", "Yejin Choi." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Rashkin et al\\.,? 2017",
      "shortCiteRegEx" : "Rashkin et al\\.",
      "year" : 2017
    }, {
      "title" : "Visual propaganda in the age of social media: An empirical analysis of Twitter images during the 2012 Israeli–Hamas conflict",
      "author" : [ "Hyunjin Seo." ],
      "venue" : "Visual Communication Quarterly, 21(3):150–161.",
      "citeRegEx" : "Seo.,? 2014",
      "shortCiteRegEx" : "Seo.",
      "year" : 2014
    }, {
      "title" : "War, propaganda and the media",
      "author" : [ "Anup Shah." ],
      "venue" : "Global Issues.",
      "citeRegEx" : "Shah.,? 2005",
      "shortCiteRegEx" : "Shah.",
      "year" : 2005
    }, {
      "title" : "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
      "author" : [ "Piyush Sharma", "Nan Ding", "Sebastian Goodman", "Radu Soricut." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Sharma et al\\.,? 2018",
      "shortCiteRegEx" : "Sharma et al\\.",
      "year" : 2018
    }, {
      "title" : "MMF: A multimodal framework for vision and language research",
      "author" : [ "Amanpreet Singh", "Vedanuj Goswami", "Vivek Natarajan", "Yu Jiang", "Xinlei Chen", "Meet Shah", "Marcus Rohrbach", "Dhruv Batra", "Devi Parikh" ],
      "venue" : null,
      "citeRegEx" : "Singh et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2020
    }, {
      "title" : "Symbiotic radicalisation strategies: Propaganda tools and neuro linguistic programming",
      "author" : [ "Robyn Torok." ],
      "venue" : "Australian Security and Intelligence Conference, ASIC ’15, pages 58–65, Canberra, Australia.",
      "citeRegEx" : "Torok.,? 2015",
      "shortCiteRegEx" : "Torok.",
      "year" : 2015
    }, {
      "title" : "Explaining multimodal deceptive news prediction models",
      "author" : [ "Svitlana Volkova", "Ellyn Ayton", "Dustin L. Arendt", "Zhuanyi Huang", "Brian Hutchinson." ],
      "venue" : "Proceedings of the International AAAI Conference on Web and Social Media, ICWSM ’19,",
      "citeRegEx" : "Volkova et al\\.,? 2019",
      "shortCiteRegEx" : "Volkova et al\\.",
      "year" : 2019
    }, {
      "title" : "A rulebook for arguments",
      "author" : [ "Anthony Weston." ],
      "venue" : "Hackett Publishing.",
      "citeRegEx" : "Weston.,? 2018",
      "shortCiteRegEx" : "Weston.",
      "year" : 2018
    }, {
      "title" : "Computational propaganda: political parties, politicians, and political manipulation on social media",
      "author" : [ "Samuel C Woolley", "Philip N Howard." ],
      "venue" : "Oxford University Press.",
      "citeRegEx" : "Woolley and Howard.,? 2018",
      "shortCiteRegEx" : "Woolley and Howard.",
      "year" : 2018
    }, {
      "title" : "Arming the public with artificial intelligence to counter social bots",
      "author" : [ "Kai-Cheng Yang", "Onur Varol", "Clayton A Davis", "Emilio Ferrara", "Alessandro Flammini", "Filippo Menczer." ],
      "venue" : "Human Behavior and Emerging Technologies, 1(1):48–61.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 34,
      "context" : "Social media have become one of the main communication channels for information dissemination and consumption, and nowadays many people rely on them as their primary source of news (Perrin, 2015).",
      "startOffset" : 181,
      "endOffset" : 195
    }, {
      "referenceID" : 19,
      "context" : "Propaganda is one such communication tool to influence the opinions and the actions of other people in order to achieve a predetermined goal (IPA, 1938).",
      "startOffset" : 141,
      "endOffset" : 152
    }, {
      "referenceID" : 29,
      "context" : "It can be traced back to the beginning of the 17th century, as reported in (Margolin, 1979; Casey, 1994; Martino et al., 2020), where the manipulation was present at public events such as theaters, festivals, and during games.",
      "startOffset" : 75,
      "endOffset" : 126
    }, {
      "referenceID" : 4,
      "context" : "It can be traced back to the beginning of the 17th century, as reported in (Margolin, 1979; Casey, 1994; Martino et al., 2020), where the manipulation was present at public events such as theaters, festivals, and during games.",
      "startOffset" : 75,
      "endOffset" : 126
    }, {
      "referenceID" : 8,
      "context" : "It can be traced back to the beginning of the 17th century, as reported in (Margolin, 1979; Casey, 1994; Martino et al., 2020), where the manipulation was present at public events such as theaters, festivals, and during games.",
      "startOffset" : 75,
      "endOffset" : 126
    }, {
      "referenceID" : 43,
      "context" : "In the current information ecosystem, it has evolved to computational propaganda (Woolley and Howard, 2018; Martino et al., 2020), where information is distributed through technological means to social media platforms, which in turn make it possible to reach well-targeted communities at high velocity.",
      "startOffset" : 81,
      "endOffset" : 129
    }, {
      "referenceID" : 8,
      "context" : "In the current information ecosystem, it has evolved to computational propaganda (Woolley and Howard, 2018; Martino et al., 2020), where information is distributed through technological means to social media platforms, which in turn make it possible to reach well-targeted communities at high velocity.",
      "startOffset" : 81,
      "endOffset" : 129
    }, {
      "referenceID" : 7,
      "context" : "There has been work on exploring network structure, looking for malicious accounts and coordinated inauthentic behavior (Cresci et al., 2017; Yang et al., 2019; Chetan et al., 2019; Pacheco et al., 2020).",
      "startOffset" : 120,
      "endOffset" : 203
    }, {
      "referenceID" : 44,
      "context" : "There has been work on exploring network structure, looking for malicious accounts and coordinated inauthentic behavior (Cresci et al., 2017; Yang et al., 2019; Chetan et al., 2019; Pacheco et al., 2020).",
      "startOffset" : 120,
      "endOffset" : 203
    }, {
      "referenceID" : 6,
      "context" : "There has been work on exploring network structure, looking for malicious accounts and coordinated inauthentic behavior (Cresci et al., 2017; Yang et al., 2019; Chetan et al., 2019; Pacheco et al., 2020).",
      "startOffset" : 120,
      "endOffset" : 203
    }, {
      "referenceID" : 32,
      "context" : "There has been work on exploring network structure, looking for malicious accounts and coordinated inauthentic behavior (Cresci et al., 2017; Yang et al., 2019; Chetan et al., 2019; Pacheco et al., 2020).",
      "startOffset" : 120,
      "endOffset" : 203
    }, {
      "referenceID" : 3,
      "context" : "In the natural language processing community, propaganda has been studied at the document level (Barrón-Cedeno et al., 2019; Rashkin et al., 2017), and at the sentence and the fragment levels (Da San Martino et al.",
      "startOffset" : 96,
      "endOffset" : 146
    }, {
      "referenceID" : 35,
      "context" : "In the natural language processing community, propaganda has been studied at the document level (Barrón-Cedeno et al., 2019; Rashkin et al., 2017), and at the sentence and the fragment levels (Da San Martino et al.",
      "startOffset" : 96,
      "endOffset" : 146
    }, {
      "referenceID" : 35,
      "context" : "There have also been notable datasets developed, including (i) TSHP-17 (Rashkin et al., 2017), which consists of document-level annotation labeled with four classes (trusted, satire, hoax, and propaganda); (ii) QProp (Barrón-Cedeno et al.",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 3,
      "context" : ", 2017), which consists of document-level annotation labeled with four classes (trusted, satire, hoax, and propaganda); (ii) QProp (Barrón-Cedeno et al., 2019), which uses binary labels (propaganda vs.",
      "startOffset" : 131,
      "endOffset" : 159
    }, {
      "referenceID" : 13,
      "context" : "Memes are popular in social media as they can be quickly understood with minimal effort (Diresta, 2018).",
      "startOffset" : 88,
      "endOffset" : 103
    }, {
      "referenceID" : 43,
      "context" : "Computational Propaganda Computational propaganda is defined as the use of automatic approaches to intentionally disseminate misleading information over social media platforms (Woolley and Howard, 2018).",
      "startOffset" : 176,
      "endOffset" : 202
    }, {
      "referenceID" : 13,
      "context" : "Of particular importance are memes, which can be quite effective at spreading multimodal propaganda on social media platforms (Diresta, 2018).",
      "startOffset" : 126,
      "endOffset" : 141
    }, {
      "referenceID" : 7,
      "context" : "As of present, attempts to limit the spread of such memes have focused on analyzing social networks and looking for fake accounts and bots to reduce the spread of such content (Cresci et al., 2017; Yang et al., 2019; Chetan et al., 2019; Pacheco et al., 2020).",
      "startOffset" : 176,
      "endOffset" : 259
    }, {
      "referenceID" : 44,
      "context" : "As of present, attempts to limit the spread of such memes have focused on analyzing social networks and looking for fake accounts and bots to reduce the spread of such content (Cresci et al., 2017; Yang et al., 2019; Chetan et al., 2019; Pacheco et al., 2020).",
      "startOffset" : 176,
      "endOffset" : 259
    }, {
      "referenceID" : 6,
      "context" : "As of present, attempts to limit the spread of such memes have focused on analyzing social networks and looking for fake accounts and bots to reduce the spread of such content (Cresci et al., 2017; Yang et al., 2019; Chetan et al., 2019; Pacheco et al., 2020).",
      "startOffset" : 176,
      "endOffset" : 259
    }, {
      "referenceID" : 32,
      "context" : "As of present, attempts to limit the spread of such memes have focused on analyzing social networks and looking for fake accounts and bots to reduce the spread of such content (Cresci et al., 2017; Yang et al., 2019; Chetan et al., 2019; Pacheco et al., 2020).",
      "startOffset" : 176,
      "endOffset" : 259
    }, {
      "referenceID" : 3,
      "context" : "Textual Content Most research on propaganda detection has focused on analyzing textual content (Barrón-Cedeno et al., 2019; Rashkin et al., 2017; Da San Martino et al., 2019; Martino et al., 2020).",
      "startOffset" : 95,
      "endOffset" : 196
    }, {
      "referenceID" : 35,
      "context" : "Textual Content Most research on propaganda detection has focused on analyzing textual content (Barrón-Cedeno et al., 2019; Rashkin et al., 2017; Da San Martino et al., 2019; Martino et al., 2020).",
      "startOffset" : 95,
      "endOffset" : 196
    }, {
      "referenceID" : 8,
      "context" : "Textual Content Most research on propaganda detection has focused on analyzing textual content (Barrón-Cedeno et al., 2019; Rashkin et al., 2017; Da San Martino et al., 2019; Martino et al., 2020).",
      "startOffset" : 95,
      "endOffset" : 196
    }, {
      "referenceID" : 41,
      "context" : "Multimodal Content Previous work has explored the use of multimodal content for detecting misleading information (Volkova et al., 2019), deception (Glenski et al.",
      "startOffset" : 113,
      "endOffset" : 135
    }, {
      "referenceID" : 14,
      "context" : ", 2019), deception (Glenski et al., 2019), emotions and propaganda (Abd Kadir et al.",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 23,
      "context" : ", 2016), hateful memes (Kiela et al., 2020; Lippe et al., 2020; Das et al., 2020), antisemitism (Chandra et al.",
      "startOffset" : 23,
      "endOffset" : 81
    }, {
      "referenceID" : 27,
      "context" : ", 2016), hateful memes (Kiela et al., 2020; Lippe et al., 2020; Das et al., 2020), antisemitism (Chandra et al.",
      "startOffset" : 23,
      "endOffset" : 81
    }, {
      "referenceID" : 10,
      "context" : ", 2016), hateful memes (Kiela et al., 2020; Lippe et al., 2020; Das et al., 2020), antisemitism (Chandra et al.",
      "startOffset" : 23,
      "endOffset" : 81
    }, {
      "referenceID" : 5,
      "context" : ", 2020), antisemitism (Chandra et al., 2021) and propaganda in images (Seo, 2014).",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 28,
      "context" : "6606 Multimodal hateful memes have been the target of the popular “Hateful Memes Challenge”, which the participants addressed using fine-tuned state-ofart multi-modal transformer models such as ViLBERT (Lu et al., 2019), Multimodal Bitransformers (Kiela et al.",
      "startOffset" : 202,
      "endOffset" : 219
    }, {
      "referenceID" : 22,
      "context" : ", 2019), Multimodal Bitransformers (Kiela et al., 2019), and VisualBERT (Li et al.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 25,
      "context" : ", 2019), and VisualBERT (Li et al., 2019) to classify hateful vs.",
      "startOffset" : 24,
      "endOffset" : 41
    }, {
      "referenceID" : 40,
      "context" : "Propaganda comes in many forms and over time a number of techniques have emerged in the literature (Torok, 2015; Miller, 1939; Da San Martino et al., 2019; Shah, 2005; Abd Kadir and Sauffiyan, 2014; IPA, 1939; Hobbs, 2015).",
      "startOffset" : 99,
      "endOffset" : 222
    }, {
      "referenceID" : 31,
      "context" : "Propaganda comes in many forms and over time a number of techniques have emerged in the literature (Torok, 2015; Miller, 1939; Da San Martino et al., 2019; Shah, 2005; Abd Kadir and Sauffiyan, 2014; IPA, 1939; Hobbs, 2015).",
      "startOffset" : 99,
      "endOffset" : 222
    }, {
      "referenceID" : 37,
      "context" : "Propaganda comes in many forms and over time a number of techniques have emerged in the literature (Torok, 2015; Miller, 1939; Da San Martino et al., 2019; Shah, 2005; Abd Kadir and Sauffiyan, 2014; IPA, 1939; Hobbs, 2015).",
      "startOffset" : 99,
      "endOffset" : 222
    }, {
      "referenceID" : 20,
      "context" : "Propaganda comes in many forms and over time a number of techniques have emerged in the literature (Torok, 2015; Miller, 1939; Da San Martino et al., 2019; Shah, 2005; Abd Kadir and Sauffiyan, 2014; IPA, 1939; Hobbs, 2015).",
      "startOffset" : 99,
      "endOffset" : 222
    }, {
      "referenceID" : 18,
      "context" : "Propaganda comes in many forms and over time a number of techniques have emerged in the literature (Torok, 2015; Miller, 1939; Da San Martino et al., 2019; Shah, 2005; Abd Kadir and Sauffiyan, 2014; IPA, 1939; Hobbs, 2015).",
      "startOffset" : 99,
      "endOffset" : 222
    }, {
      "referenceID" : 31,
      "context" : "Different authors have proposed inventories of propaganda techniques of various sizes: seven techniques (Miller, 1939), 24 techniques Weston (2018), 18 techniques (Da San Martino et al.",
      "startOffset" : 104,
      "endOffset" : 118
    }, {
      "referenceID" : 37,
      "context" : ", 2019), just smear as a technique (Shah, 2005), and seven techniques (Abd Kadir and Sauffiyan, 2014).",
      "startOffset" : 35,
      "endOffset" : 47
    }, {
      "referenceID" : 37,
      "context" : ", 2019), (Shah, 2005) and (Abd Kadir and Sauffiyan, 2014), thus ending up with 22 propaganda techniques.",
      "startOffset" : 9,
      "endOffset" : 21
    }, {
      "referenceID" : 2,
      "context" : "Since our annotation is multilabel, we computed Krippendorff’s α, which supports multi-label agreement computation (Artstein and Poesio, 2008; Passonneau, 2006).",
      "startOffset" : 115,
      "endOffset" : 160
    }, {
      "referenceID" : 33,
      "context" : "Since our annotation is multilabel, we computed Krippendorff’s α, which supports multi-label agreement computation (Artstein and Poesio, 2008; Passonneau, 2006).",
      "startOffset" : 115,
      "endOffset" : 160
    }, {
      "referenceID" : 24,
      "context" : "The results are shown in Table 1 and indicate moderate to perfect agreement (Landis and Koch, 1977).",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 11,
      "context" : "For the text-based unimodal experiments, we used BERT (Devlin et al., 2019), which is a state-of-the-art pre-trained Transformer, and fastText (Joulin et al.",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 21,
      "context" : ", 2019), which is a state-of-the-art pre-trained Transformer, and fastText (Joulin et al., 2017), which can tolerate potentially noisy text from social media as it is trained on word and character n-grams.",
      "startOffset" : 75,
      "endOffset" : 96
    }, {
      "referenceID" : 17,
      "context" : "For the image-based unimodal experiments, we used ResNet152 (He et al., 2016), which was successfully applied in a related setup (Kiela et al.",
      "startOffset" : 60,
      "endOffset" : 77
    }, {
      "referenceID" : 22,
      "context" : ", 2016), which was successfully applied in a related setup (Kiela et al., 2019).",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 22,
      "context" : "Multimodal: unimodally pretrained For the multimodal experiments, we trained separate models on the text and on the image, BERT and ResNet152, respectively, and then we combined them using (a) early fusion Multimodal Bitransformers (MMBT) (Kiela et al., 2019), (b) middle fusion (feature concatenation), and (c) late fusion (combining the predictions of the models).",
      "startOffset" : 239,
      "endOffset" : 259
    }, {
      "referenceID" : 28,
      "context" : "In particular, we used ViLBERT (Lu et al., 2019), which is pretrained on Conceptual Captions (Sharma et al.",
      "startOffset" : 31,
      "endOffset" : 48
    }, {
      "referenceID" : 38,
      "context" : ", 2019), which is pretrained on Conceptual Captions (Sharma et al., 2018), and Visual BERT (Lin et al.",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 26,
      "context" : ", 2018), and Visual BERT (Lin et al., 2014), which is pretrained on the MSCOCO dataset (Lin et al.",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 26,
      "context" : ", 2014), which is pretrained on the MSCOCO dataset (Lin et al., 2014).",
      "startOffset" : 51,
      "endOffset" : 69
    }, {
      "referenceID" : 39,
      "context" : "We used the Multimodal Framework (MMF) (Singh et al., 2020).",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 12,
      "context" : "A number of promising ideas have been already tried by the participants in a shared task based on this data at SemEval2021 (Dimitrov et al., 2021), which can serve as an inspiration when developing new models.",
      "startOffset" : 123,
      "endOffset" : 146
    } ],
    "year" : 2021,
    "abstractText" : "Propaganda can be defined as a form of communication that aims to influence the opinions or the actions of people towards a specific goal; this is achieved by means of well-defined rhetorical and psychological devices. Propaganda, in the form we know it today, can be dated back to the beginning of the 17th century. However, it is with the advent of the Internet and the social media that it has started to spread on a much larger scale than before, thus becoming major societal and political issue. Nowadays, a large fraction of propaganda in social media is multimodal, mixing textual with visual content. With this in mind, here we propose a new multi-label multimodal task: detecting the type of propaganda techniques used in memes. We further create and release a new corpus of 950 memes, carefully annotated with 22 propaganda techniques, which can appear in the text, in the image, or in both. Our analysis of the corpus shows that understanding both modalities together is essential for detecting these techniques. This is further confirmed in our experiments with several state-of-the-art multimodal models.",
    "creator" : "LaTeX with hyperref"
  }
}