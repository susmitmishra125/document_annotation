{
  "name" : "2021.acl-long.333.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models",
    "authors" : [ "Tyler A. Chang", "Yifan Xu", "Weijian Xu", "Zhuowen Tu" ],
    "emails" : [ "ztu}@ucsd.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4322–4333\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4322"
    }, {
      "heading" : "1 Introduction",
      "text" : "In recent years, Transformer-based language models have brought dramatic improvements on a wide range of natural language tasks (Brown et al., 2020; Devlin et al., 2019). The central innovation of Transformer architectures is the self-attention mechanism (Vaswani et al., 2017), which has grown beyond NLP, extending into domains ranging from computer vision (Dosovitskiy et al., 2021) and speech recognition (Dong et al., 2018) to reinforcement learning (Parisotto et al., 2020; Touvron et al., 2020).\nIn computer vision, self-attention and convolutions have been combined to achieve competitive results for image classification (Bello et al., 2019). Similarly, researchers in NLP have begun integrating convolutions into self-attention for natural language tasks. Recent work has shown initial success\nadding convolutional modules to self-attention in pre-trained language models (Jiang et al., 2020), or even replacing self-attention entirely with dynamic convolutions (Wu et al., 2019). These successes defy theoretical proofs showing that multi-headed self-attention with relative position embeddings is strictly more expressive than convolution (Cordonnier et al., 2020). To identify why convolutions have been successful in NLP, we seek to isolate the differences between self-attention and convolution in the context of natural language.\nIn this work, we formalize the relationship between self-attention and convolution in Transformer encoders by generalizing relative position embeddings, and we identify the benefits of each approach for language model pre-training. We show that self-attention is a type of dynamic lightweight convolution, a data-dependent convolution that ties weights across input channels (Wu et al., 2019). Notably, previous methods of encoding relative positions (Shaw et al., 2018; Raffel et al., 2020) are direct implementations of lightweight convolutions. Under our framework, the benefits of convolution come from an ability to capture local position information in sentences.\nThen, we propose composite attention, which applies a lightweight convolution that combines previous relative position embedding methods. We find that composite attention sufficiently captures the information provided by many other convolutions. To validate our framework, we train BERT models that integrate self-attention with multiple convolution types, evaluating our models on the GLUE benchmark (Wang et al., 2018). All of our convolutional variants outperform the default model, demonstrating the effectiveness of convolutions in enhancing self-attention for natural language tasks. Our empirical results provide evidence for future research integrating convolutions and self-attention for NLP."
    }, {
      "heading" : "2 Self-attention and lightweight convolutions",
      "text" : "First, we outline the relationship between selfattention and convolutions. Specifically, we show that a self-attention operation can be viewed as a dynamic lightweight convolution, a depthwise convolution that ties weights along channels (Wu et al., 2019). We then isolate the differences between self-attention and lightweight convolutions, highlighting the benefits of each approach in language models."
    }, {
      "heading" : "2.1 Self-attention",
      "text" : "In a Transformer self-attention layer, inputs x1, ..., xn ∈ Rd are projected to corresponding queries, keys, and values by linear transformations WQ,WK ,W V ∈ Rd×dh for each attention head, projecting into the head dimension size dh. Output vectors y1, ..., yn ∈ Rd are linear combinations of values, concatenating all attention heads. Value weights (before softmaxing) are determined by:\nαij = (xiWQ)(xjWK)T√\ndh . (1)\nIntuitively, αij represents the attention that token i pays to token j, incorporating the value xjW V into the resulting vector yi. From the attention scores between various tokens i and j, an attention map of αij is produced (see Figure 1)."
    }, {
      "heading" : "2.2 Lightweight convolutions",
      "text" : "In contrast, a standard one-dimensional convolution slides a kernel of weights along the input sequence; each feature in each output representation yi is a weighted sum of all features (called “channels”) in the surrounding xi. To save parameters, it is common to consider depthwise convolutions where each channel c in yi is a weighted sum only of the features in channel c for the surrounding xi. Formally, each entry of yi can be written as:\nyi,c = ∑\n−k≤j−i≤k βj−i,c xj,c (2)\nwhere k is the kernel size in each direction. Each scalar βj−i,c represents the attention paid to relative position j− i for channel c. To further simplify depthwise convolutions for use in language models, Wu et al. (2019) propose lightweight convolutions, which tie weights βj−i,c along all channels c. As a result, the lightweight convolution contains only 2k + 1 weights, one scalar βj−i for each relative position considered. Then, each yi is a linear combination of surrounding xi:\nyi = ∑\n−k≤j−i≤k βj−i xj (3)\nImportantly, we can then consider each βj−i as an attention weight analogous to self-attention, representing the attention that token i pays to token j.\nThe lightweight convolution produces an attention map of βj−i as visualized in Figure 1.\nFinally, furthering the similarity between lightweight convolutions and self-attention, Wu et al. (2019) propose dynamic lightweight convolutions, which dynamically compute relative weights βj−i based on individual input tokens. In other words, each row in Figure 1 has relative weights determined dynamically based on the input token xi for that row. Because attentions for relative positions are no longer fixed across rows, the attention map in Figure 1 achieves similar flexibility to standard self-attention."
    }, {
      "heading" : "2.3 Self-attention vs. convolution",
      "text" : "We have shown that both self-attention and lightweight convolution compute linear combinations of token representations, but we now isolate the differences between the two approaches. Perhaps most importantly, the two methods assign attention scores αij and βj−i in fundamentally different ways.\nSelf-attention computes αij based on the dot product between query i and key j, ignoring the relative position between i and j. In this way, selfattention layers model interactions exclusively between token representations. If the tokens are arbitrarily shuffled in a standard self-attention layer, the output for each token is unchanged. All position information is injected before the first self-attention layer in the form of absolute position embeddings.\nIn contrast, dynamic lightweight convolutions assign attention scores directly to relative positions. This allows convolutions to directly integrate relative position information without relying on absolute positions. Thus, convolutions could be better at capturing local information in sentences. However, convolutions alone are limited in their ability to model interactions between tokens because they lack the query-key mechanism central to standard self-attention. In future sections, we consider methods of integrating the two approaches."
    }, {
      "heading" : "3 Integrating lightweight convolutions",
      "text" : "Previous work has sought to integrate local information into global self-attention. This can be achieved by restricting the range of self-attention to nearby tokens, or by incorporating relative position information into attention maps (Hofstätter et al., 2020; Raganato et al., 2020; Wei et al., 2021). Notably, Shaw et al. (2018) introduced relative position em-\nbeddings, which inspired similar embeddings in models such as Transformer-XL and XLNet (Dai et al., 2019; Yang et al., 2019). In this section, we show that several previous methods of encoding relative positions are direct implementations of lightweight convolutions."
    }, {
      "heading" : "3.1 Relative embeddings as lightweight convolutions",
      "text" : "First, the simplest way to combine self-attention with lightweight convolution is to generate a standard attention map, then add the attention map generated by a lightweight convolution. Given a fixed lightweight convolution, this results in attention scores as follows:\nαij = (xiWQ)(xjWK)T√\ndh + βj−i (4)\nThis is exactly the relative position term used in T5 (Raffel et al., 2020) and TUPE (Ke et al., 2021).\nWe further consider a dynamic lightweight convolution, where the βj−i weights are computed by passing the query through a linear feedforward layer WC ∈ Rdh×(2k+1) (Wu et al., 2019).1 Because WC is linear, each weight βj−i is equal to the dot product between the query and the (j − i) column of WC . We then obtain attention scores:\nαij = (xiWQ)(xjWK)T√\ndh + (xiWQ)(WCj−i) T\nIf we scale the dynamic lightweight convolution term according to the head dimension size, we obtain precisely the relative embeddings proposed in Shaw et al. (2018):\nαij = (xiWQ)(xjWK +WCj−i)T√\ndh (5)\nUnder this interpretation, Shaw’s relative embeddings are essentially identical to the dynamic lightweight convolutions used in Wu et al. (2019). In both formulations, relative position weights are computed as dot products between the query and a learned relative position embedding. Previous work has considered relative positions in language models independently from convolutions, but our derivations suggest that the underlying mechanisms may be the same.\n1Wu et al. (2019) generate dynamic lightweight convolutions based on the entire query layer (dimension size d). In our work, we generate convolutions based on queries for individual attention heads (dimension size dh), to be consistent with the relative embeddings in Shaw et al. (2018)."
    }, {
      "heading" : "3.2 Composite attention and lightweight convolution experiments",
      "text" : "To validate lightweight convolutions in combination with self-attention, we pre-trained and evaluated BERT-small models (Devlin et al., 2019; Clark et al., 2020) that incorporated lightweight convolutions.\nPre-training To maximize similarity with Devlin et al. (2019), we pre-trained models on the BookCorpus (Zhu et al., 2015) and WikiText-103 datasets (Merity et al., 2017) using masked language modeling. Small models were pre-trained for 125,000 steps, with batch size 128 and learning rate 0.0003. Full pre-training and fine-tuning details are outlined in Appendix A.1.2\nEvaluation Models were evaluated on the GLUE benchmark, a suite of sentence classification tasks including natural language inference (NLI), grammaticality judgments, sentiment classification, and textual similarity (Wang et al., 2018). For each task, we ran ten fine-tuning runs and used the model with the best score on the development set. We report scores on the GLUE test set. Development scores and statistics for all experiments are reported in Appendix A.2.\nModels We trained two baseline models, a default BERT-small with standard absolute position embeddings, and a BERT-small with no position information whatsoever. Then, we trained models with fixed lightweight convolutions (Equation 4;\n2Code is available at https://github.com/ mlpc-ucsd/BERT_Convolutions, built upon the Huggingface Transformers library (Wolf et al., 2020).\nRaffel et al. 2020), and dynamic lightweight convolutions that generated convolution weights based on each query (i.e. using relative embeddings, Equation 5; Shaw et al. 2018).\nFinally, we propose composite attention, which simply adds dynamic lightweight convolutions to fixed lightweight convolutions, resulting in attention scores αij as follows:\n(xiWQ)(xjWK)T√ dh︸ ︷︷ ︸\nSelf-attention\n+ (xiWQ)(WCj−i) T\n√ dh︸ ︷︷ ︸\nDynamic convolution (relative embeddings)\n+ βj−i︸︷︷︸ Fixed\nconvolution\n(6) Intuitively, composite attention has the flexibility of dynamic lightweight convolutions, while still allowing models to incorporate relative positions directly through fixed lightweight convolutions. Alternatively, composite attention can be interpreted as adding a fixed bias term to relative position embeddings.\nAll of our experiments used a convolution kernel size of 17, or eight positions in each direction, a mid-range value that has been found to work well for both relative positions and convolution in language models (Huang et al., 2020; Jiang et al., 2020; Shaw et al., 2018). As in Shaw et al. (2018), relative embeddings WCj−i shared weights across heads. Unless stated otherwise, models used no absolute position embeddings.\nFor completeness, we also considered dynamic lightweight convolutions based on the key (as opposed to the query). In contrast to query-based\nlightweight convolutions, key-based convolutions allow each token to dictate which relative positions should pay attention to it, rather than dictating which relative positions it should pay attention to. Referring to the visualization in Figure 1, key-based dynamic convolutions correspond to columns instead of rows. These key-based dynamic lightweight convolutions are the same as the relative embeddings proposed in Huang et al. (2020), but they are now formulated as dynamic lightweight convolutions."
    }, {
      "heading" : "3.3 Lightweight convolution results",
      "text" : "GLUE test set results are presented in Table 1.\nLightweight convolutions consistently improved performance. Notably, even the fixed lightweight convolution was sufficient to replace absolute position embeddings, outperforming the default BERT-small model. This indicates that even naı̈ve sampling from nearby tokens can be beneficial to language model performance.\nDynamic convolutions provided further improvements. When the lightweight convolutions were generated dynamically based on token queries, the models outperformed the default model by even larger margins. This improvement over fixed lightweight convolutions suggests that different tokens find it useful to generate different lightweight convolutions, paying attention to different relative positions in a sentence.\nComposite attention performed the best. Combining fixed lightweight convolutions with dynamic lightweight convolutions proved an effective strategy for encoding relative positions. Although composite attention is simply a combination of Shaw et al. (2018) and Raffel et al. (2020)’s relative position embeddings, it validates convolution as a viable method of encoding relative positions in self-attention.\nKey-based dynamic convolutions provided no additional benefit. When we generated an additional lightweight convolution based on keys, the model performed worse than composite attention alone (GLUE 74.0 compared to 75.2). This result clarifies the findings of Huang et al. (2020), who reported only small improvements from query and key-based relative position embeddings for a subset of the GLUE tasks.\nGrammaticality judgments were particularly sensitive to position information. On the CoLA task (the corpus of linguistic acceptability; Warstadt et al. 2019), there was a dramatic performance drop when absolute position embeddings were removed. However, when any type of lightweight convolution was added, performance improved even over the baseline established by absolute positions. The pronounced effects of local position information on the CoLA task support the intuitive hypothesis that local dependencies are particularly important for grammaticality judgments. This result also suggests that convolutions could be beneficial to more local tasks (e.g. token-level tasks) along with sentence classification tasks."
    }, {
      "heading" : "3.4 Interpreting lightweight convolutions",
      "text" : "To better understand how lightweight convolutions improve language models, we visualized the learned lightweight convolution kernel weights in Figure 2. Qualitatively, the kernels exhibited specific types of patterns:\n• Paying particular attention to the previous or next token.\n• Paying graded attention either to past or future tokens, dictated by how far the target token is from the present token.\nThese observations support the assumption that nearby tokens are relevant to the interpretation of the current token. They also align with the findings\nof Voita et al. (2019), who identified “positional” attention heads that focus primarily on the next or previous token. From this perspective, lightweight convolutions allow language models to explicitly represent nearby tokens’ positions.\nInterestingly, we also found that some kernels paid fairly uniform attention to all tokens, even decreasing attention to nearby and adjacent tokens. It is likely that these attention heads focused on more global information, relying on the query-key attention mechanism rather than the convolution."
    }, {
      "heading" : "3.5 BERT-base models",
      "text" : "To thoroughly assess the impact of composite attention on pre-trained language models, we trained full-sized BERT models for 1M steps each, replicating our BERT-small experiments. Pre-training details are outlined in Appendix A.1.\nResults are presented in Table 1. Differences between models decreased substantially for full sized models, and the relative performances of different approaches varied across tasks. Our results suggest that relative position information is more useful for smaller or more data-limited models; extending the benefits of convolutions robustly from small models to larger models is an important direction for future research. That said, even in the larger models, composite attention slightly outperformed the other position embedding methods in overall GLUE score. Our results demonstrate that convolutions can perform at least on par with absolute position embeddings even in larger models."
    }, {
      "heading" : "4 Non-lightweight convolutions",
      "text" : "The previous section found that lightweight convolutions consistently improved pre-trained language model performance. Next, we investigated whether the additional flexibility of non-lightweight convolutions could provide additional benefits. Specifically, we considered convolutions that were fixed but non-lightweight. In other words, convolution weights were fixed regardless of the input query, but weights were not tied across channels, equivalent to a standard depthwise convolution. We only considered fixed depthwise convolutions because under existing frameworks, dynamic depthwise convolutions would introduce large numbers of parameters.\nTo implement depthwise convolutions, we added a convolution term identical to the fixed lightweight convolution in Equation 4, except that βj−i was\nlearned separately for each feature channel:3\nαij,c = (xiWQ)(xjWK)T√\ndh + βj−i,c (7)\nThis is equivalent to adding a depthwise convolution of the token values to the standard selfattention output."
    }, {
      "heading" : "4.1 Non-lightweight convolution experiments",
      "text" : "We ran experiments using the same setup as the lightweight convolution experiments in Section 3.2. To compare the effects of dynamic lightweight convolutions (e.g. composite attention) and nonlightweight (depthwise) convolutions, we trained models using each possible combination of the two convolutions. Results are presented in Table 2.\nDepthwise convolutions were less effective than lightweight convolutions. As with lightweight convolutions, the depthwise convolutions effectively replaced absolute position embeddings, outperforming the default model. However, fixed depthwise convolutions performed worse than fixed lightweight convolutions on the majority of tasks. This indicates that flexibility across channels is not critical to the success of convolutions in language models.\n3For computational efficiency, we applied the softmax to the attention scores prior to adding the convolution term βj−i,c, to avoid computing softmax scores separately for each individual channel. Softmax is not commonly applied in depthwise convolutions.\nComposite attention already provided the necessary flexibility. Composite attention outperformed the fixed depthwise convolutions; even when composite attention was combined with depthwise convolutions, there was no overall improvement over composite attention alone. This suggests that in the context of language, dynamic lightweight convolutions efficiently encode any local position information provided by depthwise convolutions.\nDepthwise convolutions differentiated previous and next tokens. In previous sections, we found that lightweight convolution kernels often pay attention specifically to adjacent tokens. As can be seen in Figure 3, this result was even more pronounced in depthwise convolutions, with individual channels focusing on the previous or next token. Interestingly, other channels specifically directed attention away from adjacent tokens. This indicates that the relevant information about next and previous tokens can be compressed into a subset of the feature channels, freeing other channels to consider more distant or position-independent information."
    }, {
      "heading" : "5 Convolutional queries, keys, and values",
      "text" : "Improvements over the non-convolutional baselines indicate that convolutions are beneficial to language model pre-training, serving as replacements for absolute position embeddings. Our previous experiments applied different types of convolutions to self-attention values. To take this result one step\nfurther, we replaced the linear query, key, and value projections themselves with convolutional layers.\nIntuitively, applying convolutions before selfattention induces even more mixing of token representations. If convolutions are built into every query, key, and value, then it becomes impossible for a token i to pay attention to a single token j without also incorporating information about tokens surrounding token j."
    }, {
      "heading" : "5.1 Convolutional Q, K, V experiments",
      "text" : "As in Sections 3.2 and 4.1, we ran experiments on BERT-small. We replaced the query, key and value projections with depthwise-separable convolutions in half of the self-attention heads.4 This aligns with previous work in which only half of the output dimensions for each token were generated using convolutions (Jiang et al., 2020). Indeed, our initial explorations found that it was more effective to replace the linear projections in only half, not all, the attention heads.\nThen, we considered whether convolutions from previous experiments provided additional benefits over convolutional queries, keys, and values. To test this, we trained BERT-small models with composite attention (Equation 6), adding convolutional queries, keys, and values.\n4Depthwise-separable convolutions are a common way to save convolution parameters. A depthwise convolution is applied first, applying an independent convolution for each channel. Then, a pointwise convolution (i.e. a feedforward layer) mixes the channels to produce the final output."
    }, {
      "heading" : "5.2 Convolutional Q, K, V results",
      "text" : "Results are presented in Table 3. Similar to our previous convolution experiments, all convolutional replacements successfully outperformed the default model. These results strongly support the conclusion that convolutions are a viable method of encoding positional information for language tasks.\nHowever, all convolutional replacements for queries, keys, and values slightly decreased the performance of models using composite attention. Convolutional values in particular were effective in models without composite attention, but they slightly decreased performance in models that already incorporated such lightweight convolutions. We conclude that although convolutions can benefit models by adding local position information, there is a limit to how much local mixing should be done. It is sufficient to apply convolutions to token values on top of self-attention; additional convolutional layers applied before the self-attention map enforce unnecessary mixing of token representations."
    }, {
      "heading" : "6 Discussion",
      "text" : "Our results demonstrate that convolutions provide consistent benefits to pre-trained language models. Our proposed composite attention mechanism combines previous relative position embedding methods, showing that convolutions can effectively compensate for the lack of local position information in Transformer models."
    }, {
      "heading" : "6.1 Related work",
      "text" : "Our work unites and builds upon previous work using convolutions and relative positions in Transformers. We adopted the relative embeddings from Shaw et al. (2018) and Huang et al. (2020), showing that these embeddings are equivalent to the dynamic lightweight convolutions in Wu et al. (2019). Combining these dynamic lightweight convolutions with fixed lightweight convolutions (equivalent to the relative position terms in Raffel et al. 2020), we studied relative embeddings under the framework of convolution integrated with selfattention. As far as we are aware, our work is the first to holistically compare relative positions, convolutions, and self-attention in language models.\nBuilding upon dynamic lightweight convolutions, recent work has incorporated both depthwiseseparable and dynamic lightweight convolutions in pre-trained language models. Jiang et al. (2020) proposed ConvBERT, which adds a convolutional\nmodule alongside the standard self-attention mechanism in BERT. ConvBERT’s convolutional module consists of a depthwise-separable convolution combining with a query to generate a dynamic lightweight convolution. Under our integrated framework, this is analogous to the model which uses depthwise-separable convolutions for queries and keys, using composite attention as a querybased dynamic lightweight convolution (see Table 3). To make this comparison concrete, we trained a ConvBERT-small model using the same setup as our experiments. Indeed, the analogous model under our framework outperformed ConvBERT-small (GLUE score 74.5 compared to 70.3). Details for the ConvBERT comparison can be found in Appendix A.3.\nFinally, recent work has proved theoretical relationships between self-attention and convolution. Cordonnier et al. (2020) showed that given enough self-attention heads, self-attention weights can express any convolution; in fact, they showed that self-attention layers often learn such convolutional structures when trained on vision tasks. However, this theoretical equivalence does not explain convolution-based improvements for Transformers in language tasks. To clarify the relationship between self-attention and convolution in language, our work characterizes self-attention as a type of dynamic lightweight convolution. By establishing a per-parameter equivalence between relative position embeddings and Wu’s dynamic lightweight convolutions, we provide a concrete foundation where self-attention and convolution are used together in practice."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this work, we formalized the relationship between self-attention and convolution. We proposed composite attention, which combines self-attention with lightweight convolution, uniting previous approaches to relative positions. Our formulation and empirical results demonstrate that convolutions can improve self-attention by providing local position information in sentences, capable of replacing absolute position embeddings entirely.\nOur findings provide a solid foundation from which to study convolutions and self-attention in language tasks. The spatially-oriented nature of convolutional neural networks translates directly into positional information in language. As vision and language researchers strive towards common\ndeep learning architectures, it is important to recognize how architectures for vision tasks can be adapted to linguistic domains."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is funded by NSF IIS-1717431. Zhuowen Tu is also funded under the Qualcomm Faculty Award. Tyler Chang is partially supported by the UCSD HDSI graduate fellowship."
    }, {
      "heading" : "A Appendix",
      "text" : "A.1 Pre-training and fine-tuning details\nBERT models (Devlin et al. 2019; Clark et al. 2020) were pre-trained on the BookCorpus (Zhu et al., 2015) and WikiText-103 datasets (Merity\net al., 2017) using masked language modeling. Pretraining examples were formatted as sentence pairs without the next sentence prediction objective. In total, our dataset consisted of 31M unique sentence pairs.5 Sentences were tokenized by training an uncased SentencePiece tokenizer (Kudo and Richardson, 2018), and input and output token embeddings were tied during pre-training. Models were evaluated on the GLUE benchmark (Wang et al., 2018). Including ten fine-tuning runs for each GLUE task, each BERT-small model took about 24 hours to train on two Titan Xp GPUs. Each BERT-base model took about 16 days to train on 8 GPUs. Pretraining hyperparameters are listed in Table 4, and fine-tuning hyperparameters are listed in Table 5. Hyperparameters are based on those used in Clark et al. (2020) and Devlin et al. (2019).\nA.2 GLUE development results\nResults for each model on the GLUE development set are reported in Table 6. We report averages over ten fine-tuning runs for each task, including standard errors of the mean. Each overall GLUE score was computed as the average of individual task scores; we computed GLUE score averages and standard errors over ten GLUE scores, corresponding to the ten fine-tuning runs. We note that development scores were generally higher than test scores due to differences between the test and\n5Because BERT-small models were only trained for 125,000 steps with batch size 128, small models were trained on 16M sentence pairs.\ntraining distributions (Wang et al., 2018).\nA.3 Detailed ConvBERT comparison ConvBERT adds a convolutional module alongside the standard self-attention mechanism in BERT (Jiang et al., 2020). ConvBERT uses half the number of standard self-attention heads, using convolutional modules for the other half. In each convolutional module, a depthwise-separable convolution is multiplied pointwise with the query in the corresponding self-attention head. This convolutional query is fed into a linear layer to generate a dynamic lightweight convolution.\nUnder our framework, the analogous model replaces half of the queries and keys with depthwiseseparable convolutions and uses composite attention (a query-based dynamic lightweight convolution; see Table 3 in the full paper). In both models (ConvBERT and our own), half of the attention heads use a convolutional query. Additionally, in both models, the convolutional query is used to generate a dynamic lightweight convolution.\nHowever, in our model, the dynamic lightweight convolution (in this case, composite attention) is used for all attention heads, not just the convolutional heads. Furthermore, our convolutional heads still use a self-attention mechanism along with the dynamic lightweight convolutions, by generating convolutional keys. In this way, our model adds convolutions to ConvBERT’s self-attention heads, and adds self-attention to ConvBERT’s convolutional heads.\nThen, we investigated whether the separate selfattention and convolutional modules in ConvBERT provide any benefit over our integrated convolution and self-attention. We trained a ConvBERTsmall model using the same pre-training setup as our BERT-small experiments, comparing performance to the analogous model under our framework. Results are shown in Table 7. Indeed, integrated convolutions and self-attention outperformed ConvBERT-small, using only 3% more parameters."
    } ],
    "references" : [ {
      "title" : "Attention augmented convolutional networks",
      "author" : [ "Irwan Bello", "Barret Zoph", "Ashish Vaswani", "Jonathon Shlens", "Quoc Le." ],
      "venue" : "International Conference on Computer Vision.",
      "citeRegEx" : "Bello et al\\.,? 2019",
      "shortCiteRegEx" : "Bello et al\\.",
      "year" : 2019
    }, {
      "title" : "ELECTRA: Pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc Le", "Christopher Manning." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "On the relationship between selfattention and convolutional layers",
      "author" : [ "Jean-Baptiste Cordonnier", "Andreas Loukas", "Martin Jaggi." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Cordonnier et al\\.,? 2020",
      "shortCiteRegEx" : "Cordonnier et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformer-XL: Attentive language models beyond a fixed-length context",
      "author" : [ "Zihang Dai", "Zhilin Yang", "Yiming Yang", "Jaime Carbonell", "Quoc Le", "Ruslan Salakhutdinov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Dai et al\\.,? 2019",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Speechtransformer: a no-recurrence sequence-to-sequence model for speech recognition",
      "author" : [ "Linhao Dong", "Shuang Xu", "Bo Xu." ],
      "venue" : "IEEE International Conference on Acoustics, Speech and Signal Processing, pages 5884–5888.",
      "citeRegEx" : "Dong et al\\.,? 2018",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2018
    }, {
      "title" : "Local self-attention over long text for efficient document retrieval",
      "author" : [ "Sebastian Hofstätter", "Hamed Zamani", "Bhaskar Mitra", "Nick Craswell", "Allan Hanbury." ],
      "venue" : "Proceedings of the 43rd International ACM SIGIR Conference on Research and Develop-",
      "citeRegEx" : "Hofstätter et al\\.,? 2020",
      "shortCiteRegEx" : "Hofstätter et al\\.",
      "year" : 2020
    }, {
      "title" : "Improve transformer models with better relative position embeddings",
      "author" : [ "Zhiheng Huang", "Davis Liang", "Peng Xu", "Bing Xiang." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3327–3335, Online. Association for",
      "citeRegEx" : "Huang et al\\.,? 2020",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "ConvBERT: Improving BERT with span-based dynamic convolution",
      "author" : [ "Zihang Jiang", "Weihao Yu", "Daquan Zhou", "Yunpeng Chen", "Jiashi Feng", "Shuicheng Yan." ],
      "venue" : "Proceedings of the 34th Conference on Neural Information Processing Systems.",
      "citeRegEx" : "Jiang et al\\.,? 2020",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Rethinking positional encoding in language pre-training",
      "author" : [ "Guolin Ke", "Di He", "Tie-Yan Liu." ],
      "venue" : "Proceedings of the International Conference on Learning Representations.",
      "citeRegEx" : "Ke et al\\.,? 2021",
      "shortCiteRegEx" : "Ke et al\\.",
      "year" : 2021
    }, {
      "title" : "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "author" : [ "Taku Kudo", "John Richardson." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System",
      "citeRegEx" : "Kudo and Richardson.,? 2018",
      "shortCiteRegEx" : "Kudo and Richardson.",
      "year" : 2018
    }, {
      "title" : "Pointer sentinel mixture models",
      "author" : [ "Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher." ],
      "venue" : "Proceedings of the Fifth International Conference on Learning Representations.",
      "citeRegEx" : "Merity et al\\.,? 2017",
      "shortCiteRegEx" : "Merity et al\\.",
      "year" : 2017
    }, {
      "title" : "Stabilizing transformers for",
      "author" : [ "Emilio Parisotto", "Francis Song", "Jack Rae", "Razvan Pascanu", "Caglar Gulcehre", "Siddhant Jayakumar", "Max Jaderberg", "Raphaël Lopez Kaufman", "Aidan Clark", "Seb Noury", "Matthew Botvinick", "Nicolas Heess", "Raia Hadsell" ],
      "venue" : null,
      "citeRegEx" : "Parisotto et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Parisotto et al\\.",
      "year" : 2020
    }, {
      "title" : "Sentence encoders on STILTs: Supplementary training on intermediate labeled-data tasks",
      "author" : [ "Jason Phang", "Thibault Févry", "Samuel Bowman." ],
      "venue" : "arXiv preprint arXiv:1811.01088.",
      "citeRegEx" : "Phang et al\\.,? 2018",
      "shortCiteRegEx" : "Phang et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Re-",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Fixed encoder self-attention patterns in transformer-based machine translation",
      "author" : [ "Alessandro Raganato", "Yves Scherrer", "Jörg Tiedemann." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 556–568, Online. Associ-",
      "citeRegEx" : "Raganato et al\\.,? 2020",
      "shortCiteRegEx" : "Raganato et al\\.",
      "year" : 2020
    }, {
      "title" : "Self-attention with relative position representations",
      "author" : [ "Peter Shaw", "Jakob Uszkoreit", "Ashish Vaswani." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Shaw et al\\.,? 2018",
      "shortCiteRegEx" : "Shaw et al\\.",
      "year" : 2018
    }, {
      "title" : "Training data-efficient image transformers and distillation through attention",
      "author" : [ "Hugo Touvron", "Matthieu Cord", "Matthijs Douze", "Francisco Massa", "Alexandre Sablayrolles", "Hervé Jégou." ],
      "venue" : "arXiv preprint arXiv:2012.12877.",
      "citeRegEx" : "Touvron et al\\.,? 2020",
      "shortCiteRegEx" : "Touvron et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of the 31st Conference on Neural Information Processing Systems.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned",
      "author" : [ "Elena Voita", "David Talbot", "Fedor Moiseev", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Voita et al\\.,? 2019",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    }, {
      "title" : "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop Black-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural network acceptability judgments",
      "author" : [ "Alex Warstadt", "Amanpreet Singh", "Samuel R. Bowman." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 7:625–641.",
      "citeRegEx" : "Warstadt et al\\.,? 2019",
      "shortCiteRegEx" : "Warstadt et al\\.",
      "year" : 2019
    }, {
      "title" : "Position-aware self-attention based neural sequence labeling",
      "author" : [ "Wei Wei", "Zanbo Wang", "Xianling Mao", "Guangyou Zhou", "Pan Zhou", "Sheng Jiang." ],
      "venue" : "Pattern Recognition, 110.",
      "citeRegEx" : "Wei et al\\.,? 2021",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2021
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Clara Ma", "Yacine Jernite", "Julien Plu", "Canwen Xu", "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Em-",
      "citeRegEx" : "Ma et al\\.,? 2020",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2020
    }, {
      "title" : "Pay less attention with lightweight and dynamic convolutions",
      "author" : [ "Felix Wu", "Angela Fan", "Alexei Baevski", "Yann Dauphin", "Michael Auli." ],
      "venue" : "Proceedings of the Seventh International Conference on Learning Representations.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "XLNet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Russ Salakhutdinov", "Quoc Le." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
      "author" : [ "Yukun Zhu", "Ryan Kiros", "Rich Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "2015 IEEE International Con-",
      "citeRegEx" : "Zhu et al\\.,? 2015",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2015
    }, {
      "title" : "2018), and input and output token embeddings were tied during pre-training",
      "author" : [ "Richardson" ],
      "venue" : "Models were evaluated on the GLUE benchmark (Wang et al.,",
      "citeRegEx" : ".Kudo and Richardson,? \\Q2018\\E",
      "shortCiteRegEx" : ".Kudo and Richardson",
      "year" : 2018
    }, {
      "title" : "Hyperparameters are based on",
      "author" : [ "Clark" ],
      "venue" : null,
      "citeRegEx" : "Clark,? \\Q2019\\E",
      "shortCiteRegEx" : "Clark",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "In recent years, Transformer-based language models have brought dramatic improvements on a wide range of natural language tasks (Brown et al., 2020; Devlin et al., 2019).",
      "startOffset" : 128,
      "endOffset" : 169
    }, {
      "referenceID" : 18,
      "context" : "The central innovation of Transformer architectures is the self-attention mechanism (Vaswani et al., 2017), which has grown beyond NLP, extending into domains ranging from computer vision (Dosovitskiy et al.",
      "startOffset" : 84,
      "endOffset" : 106
    }, {
      "referenceID" : 5,
      "context" : ", 2021) and speech recognition (Dong et al., 2018) to reinforcement learning (Parisotto et al.",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 12,
      "context" : ", 2018) to reinforcement learning (Parisotto et al., 2020; Touvron et al., 2020).",
      "startOffset" : 34,
      "endOffset" : 80
    }, {
      "referenceID" : 17,
      "context" : ", 2018) to reinforcement learning (Parisotto et al., 2020; Touvron et al., 2020).",
      "startOffset" : 34,
      "endOffset" : 80
    }, {
      "referenceID" : 0,
      "context" : "In computer vision, self-attention and convolutions have been combined to achieve competitive results for image classification (Bello et al., 2019).",
      "startOffset" : 127,
      "endOffset" : 147
    }, {
      "referenceID" : 8,
      "context" : "Recent work has shown initial success adding convolutional modules to self-attention in pre-trained language models (Jiang et al., 2020), or even replacing self-attention entirely with dynamic convolutions (Wu et al.",
      "startOffset" : 116,
      "endOffset" : 136
    }, {
      "referenceID" : 24,
      "context" : ", 2020), or even replacing self-attention entirely with dynamic convolutions (Wu et al., 2019).",
      "startOffset" : 77,
      "endOffset" : 94
    }, {
      "referenceID" : 2,
      "context" : "These successes defy theoretical proofs showing that multi-headed self-attention with relative position embeddings is strictly more expressive than convolution (Cordonnier et al., 2020).",
      "startOffset" : 160,
      "endOffset" : 185
    }, {
      "referenceID" : 24,
      "context" : "We show that self-attention is a type of dynamic lightweight convolution, a data-dependent convolution that ties weights across input channels (Wu et al., 2019).",
      "startOffset" : 143,
      "endOffset" : 160
    }, {
      "referenceID" : 16,
      "context" : "Notably, previous methods of encoding relative positions (Shaw et al., 2018; Raffel et al., 2020) are direct implementations of lightweight convolutions.",
      "startOffset" : 57,
      "endOffset" : 97
    }, {
      "referenceID" : 14,
      "context" : "Notably, previous methods of encoding relative positions (Shaw et al., 2018; Raffel et al., 2020) are direct implementations of lightweight convolutions.",
      "startOffset" : 57,
      "endOffset" : 97
    }, {
      "referenceID" : 20,
      "context" : "To validate our framework, we train BERT models that integrate self-attention with multiple convolution types, evaluating our models on the GLUE benchmark (Wang et al., 2018).",
      "startOffset" : 155,
      "endOffset" : 174
    }, {
      "referenceID" : 24,
      "context" : "Specifically, we show that a self-attention operation can be viewed as a dynamic lightweight convolution, a depthwise convolution that ties weights along channels (Wu et al., 2019).",
      "startOffset" : 163,
      "endOffset" : 180
    }, {
      "referenceID" : 6,
      "context" : "This can be achieved by restricting the range of self-attention to nearby tokens, or by incorporating relative position information into attention maps (Hofstätter et al., 2020; Raganato et al., 2020; Wei et al., 2021).",
      "startOffset" : 152,
      "endOffset" : 218
    }, {
      "referenceID" : 15,
      "context" : "This can be achieved by restricting the range of self-attention to nearby tokens, or by incorporating relative position information into attention maps (Hofstätter et al., 2020; Raganato et al., 2020; Wei et al., 2021).",
      "startOffset" : 152,
      "endOffset" : 218
    }, {
      "referenceID" : 22,
      "context" : "This can be achieved by restricting the range of self-attention to nearby tokens, or by incorporating relative position information into attention maps (Hofstätter et al., 2020; Raganato et al., 2020; Wei et al., 2021).",
      "startOffset" : 152,
      "endOffset" : 218
    }, {
      "referenceID" : 3,
      "context" : "(2018) introduced relative position embeddings, which inspired similar embeddings in models such as Transformer-XL and XLNet (Dai et al., 2019; Yang et al., 2019).",
      "startOffset" : 125,
      "endOffset" : 162
    }, {
      "referenceID" : 25,
      "context" : "(2018) introduced relative position embeddings, which inspired similar embeddings in models such as Transformer-XL and XLNet (Dai et al., 2019; Yang et al., 2019).",
      "startOffset" : 125,
      "endOffset" : 162
    }, {
      "referenceID" : 14,
      "context" : "This is exactly the relative position term used in T5 (Raffel et al., 2020) and TUPE (Ke et al.",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 24,
      "context" : "We further consider a dynamic lightweight convolution, where the βj−i weights are computed by passing the query through a linear feedforward layer WC ∈ Rdh×(2k+1) (Wu et al., 2019).",
      "startOffset" : 163,
      "endOffset" : 180
    }, {
      "referenceID" : 4,
      "context" : "To validate lightweight convolutions in combination with self-attention, we pre-trained and evaluated BERT-small models (Devlin et al., 2019; Clark et al., 2020) that incorporated lightweight convolutions.",
      "startOffset" : 120,
      "endOffset" : 161
    }, {
      "referenceID" : 1,
      "context" : "To validate lightweight convolutions in combination with self-attention, we pre-trained and evaluated BERT-small models (Devlin et al., 2019; Clark et al., 2020) that incorporated lightweight convolutions.",
      "startOffset" : 120,
      "endOffset" : 161
    }, {
      "referenceID" : 26,
      "context" : "(2019), we pre-trained models on the BookCorpus (Zhu et al., 2015) and WikiText-103 datasets (Merity et al.",
      "startOffset" : 48,
      "endOffset" : 66
    }, {
      "referenceID" : 11,
      "context" : ", 2015) and WikiText-103 datasets (Merity et al., 2017) using masked language modeling.",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 20,
      "context" : "Evaluation Models were evaluated on the GLUE benchmark, a suite of sentence classification tasks including natural language inference (NLI), grammaticality judgments, sentiment classification, and textual similarity (Wang et al., 2018).",
      "startOffset" : 216,
      "endOffset" : 235
    }, {
      "referenceID" : 16,
      "context" : "2020), and dynamic lightweight convolutions that generated convolution weights based on each query (i.e. using relative embeddings, Equation 5; Shaw et al. 2018).",
      "startOffset" : 99,
      "endOffset" : 161
    }, {
      "referenceID" : 7,
      "context" : "All of our experiments used a convolution kernel size of 17, or eight positions in each direction, a mid-range value that has been found to work well for both relative positions and convolution in language models (Huang et al., 2020; Jiang et al., 2020; Shaw et al., 2018).",
      "startOffset" : 213,
      "endOffset" : 272
    }, {
      "referenceID" : 8,
      "context" : "All of our experiments used a convolution kernel size of 17, or eight positions in each direction, a mid-range value that has been found to work well for both relative positions and convolution in language models (Huang et al., 2020; Jiang et al., 2020; Shaw et al., 2018).",
      "startOffset" : 213,
      "endOffset" : 272
    }, {
      "referenceID" : 16,
      "context" : "All of our experiments used a convolution kernel size of 17, or eight positions in each direction, a mid-range value that has been found to work well for both relative positions and convolution in language models (Huang et al., 2020; Jiang et al., 2020; Shaw et al., 2018).",
      "startOffset" : 213,
      "endOffset" : 272
    }, {
      "referenceID" : 21,
      "context" : "On the CoLA task (the corpus of linguistic acceptability; Warstadt et al. 2019), there was a dramatic performance drop when absolute position embeddings were removed.",
      "startOffset" : 17,
      "endOffset" : 79
    }, {
      "referenceID" : 8,
      "context" : "4 This aligns with previous work in which only half of the output dimensions for each token were generated using convolutions (Jiang et al., 2020).",
      "startOffset" : 126,
      "endOffset" : 146
    } ],
    "year" : 2021,
    "abstractText" : "In this paper, we detail the relationship between convolutions and self-attention in natural language tasks. We show that relative position embeddings in self-attention layers are equivalent to recently-proposed dynamic lightweight convolutions, and we consider multiple new ways of integrating convolutions into Transformer self-attention. Specifically, we propose composite attention, which unites previous relative position embedding methods under a convolutional framework. We conduct experiments by training BERT with composite attention, finding that convolutions consistently improve performance on multiple downstream tasks, replacing absolute position embeddings. To inform future work, we present results comparing lightweight convolutions, dynamic convolutions, and depthwiseseparable convolutions in language model pretraining, considering multiple injection points for convolutions in self-attention layers.",
    "creator" : "LaTeX with hyperref"
  }
}