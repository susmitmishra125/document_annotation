{
  "name" : "2021.acl-long.401.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Data Augmentation with Adversarial Training for Cross-Lingual NLI",
    "authors" : [ "Xin Dong", "Yaxin Zhu", "Zuohui Fu", "Dongkuan Xu", "Gerard de Melo" ],
    "emails" : [ "zuohui.fu}@rutgers.edu", "dux19@psu.edu,", "gdm@demelo.org" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5158–5167\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5158"
    }, {
      "heading" : "1 Introduction",
      "text" : "There is a growing need for NLP systems that support low-resource languages, for which taskspecific training data may be lacking, while domain-specific parallel corpora may be too scarce to train a reliable machine translation engine. To overcome this, zero-shot cross-lingual systems can be trained on a source language LS and subsequently also be applied to other languages LT despite a complete lack of labelled training data for those target languages. In the past, such systems typically drew on translation dictionaries, lexical knowledge graphs, or parallel corpora, to build a cross-lingual model that exploits simple connections between words and phrases across different languages (de Melo and Siersdorfer, 2007; Fu et al., 2020). Recently, pretrained language model architectures such as BERT (Devlin et al., 2019) have been shown capable of learning joint multilingual representations with self-supervised objectives under a shared vocabulary, simply by combining the\ninput from multiple languages (Devlin et al., 2019; Artetxe and Schwenk, 2019; Conneau and Lample, 2019; Conneau et al., 2019). Such representations greatly facilitate cross-lingual applications. Still, the success of such cross-lingual transfer hinges on how close the involved languages are, with substantial drops observed for some more distant language pairs (Lauscher et al., 2020).\nFor our study, we focus on natural language inference (NLI), i.e., classifying whether a premise sentence entails, contradicts, or is neutral with regard to a hypothesis sentence (Williams et al., 2017). This is a useful building block for applications involving semantic understanding (Zhu et al., 2018; Reimers and Gurevych, 2019). However, the task is also very challenging, as it not only requires accounting for very subtle differences in meaning but also inferring presuppositions and implications that are not explicitly stated. Due to these intricate subtleties, zero-shot cross-lingual models are often fairly brittle, while obtaining in-language training data is fairly costly.\nData Augmentation. To boost the performance of cross-lingual models, an intuitive thought is to draw on unlabeled data from the target language so as to enable the model to better account for the specifics of that language, rather than just being fine-tuned on the source language. A natural way of exploiting unlabeled data is to consider standard semi-supervised learning methods that leverage a model’s own predictions on unlabeled target language inputs (Dong and de Melo, 2019). However, this strategy fails when the predictions are too noisy to serve as reliable training signals. In this paper, we hence explore data augmentation to circumvent this problem. The idea, widespread in computer vision and speech recognition, is to generate new training data from existing labeled data. For images, a common approach is to apply\ntransformations such as rotation and flipping, as these typically preserve the original label assigned to an image (Krizhevsky et al., 2012). For text, in contrast, data augmentation is more challenging, and straightforward techniques include simple operations on words within the original training sequences, such as synonym replacement, random insertion, random swapping, or random deletion (Wei and Zou, 2019). In practice, however, there are two notable problems. One is that the synthesized data from data augmentation techniques may as well be noisy and unreliable. Second, new examples may diverge from the distribution of the original data.\nOn NLI, these problems are particularly pronounced, as the very nature of this task is to account for subtle differences between sentences. Modified versions of the original sentences may no longer have the same meaning and entailments. Hence, existing data augmentation techniques often fail to boost the result quality.\nOverview and Contributions. In this paper, we propose a novel data augmentation scheme to synthesize controllable and much less noisy data for cross-lingual NLI. This augmentation consists of two parts. One serves to encourage language adaptation by means of reordering source language words based on word alignments to better cope with typological divergency between languages, denoted as Reorder Augmentation (RA). Another seeks to enrich the set of semantic relationships between a premise and pertinent hypotheses, denoted as Semantic Augmentation (SA). Both are achieved by learning corresponding sequence-to-sequence (Seq2Seq) models.\nThe resulting samples along with their new labels serve as an enriched training set for the final cross-lingual training. During this phase, we invoke a special adversarial training regimen that enables the model to better learn from such automatically induced training samples and transfer more information to the target languages while better bridging the gap between typologically distinct languages. Our empirical study demonstrates the necessity of incorporating adversarial training into training with synthetic samples and the superiority of our new augmentation method on cross-lingual Natural Language Inference (Conneau et al., 2018). Remarkably, our cross-lingual approach even outperforms in-language supervised learning."
    }, {
      "heading" : "2 Method",
      "text" : "Our proposed method consists of two steps. The first involves inducing training examples with two data augmentation models. Next, a task-specific classifier is trained on both the original and the newly generated training instances, with adversarial perturbation for improved robustness and generalization."
    }, {
      "heading" : "2.1 Data Augmentation Model",
      "text" : ""
    }, {
      "heading" : "2.1.1 Reorder Augmentation",
      "text" : "Reorder augmentation is based on the intuition of making a model more robust with respect to differences in word order typology. If our training examples consist entirely of instances from a language LS with a fairly strict subject–verb–object (SVO) word order such as English, the model will be less well equipped to pay attention to subtle semantic differences between sentences from a target language LT obeying subject–object–verb (SOV) order. To alleviate this problem, we can rely on auxiliary data to diversify the training data. For this, we obtain word alignments for unannotated bilingual parallel sentence pairs coveringLS and an auxiliary language LA that need not be the same as LT. We then reorder all source sentences to match the word order of LA based on the alignments, and train a model to apply such reordering on the NLI training instances.\nFormally, suppose we have obtained l unlabelled parallel sentences in the source language LS and in the auxiliary language LA, C = {(〈si, ai〉 | i = 1, ..., l}, where 〈s, a〉 is a source–auxiliary language sentence pair. Based on a word alignment model, in our case FastAlign (Dyer et al., 2013), which uses Expectation Maximization to compute the lexical translation probabilities, we obtain a word pair table for each sentence pair 〈s, t〉, denoted as A(s, a) = {(i1, j1), ..., (im, jm)}.\nFollowing the word order of LA, we then reorder the source sequence s by consulting the table\nA(s, t), yielding the new sentence pair 〈s, s̄〉. Next, we consider a pretrained Seq2Seq model, denoted as r(·; θ). The model is assumed to have been pretrained with an encoder and a decoder in the source language, and we fine-tune this generative model by training on the new parallel corpus C̄ = {(〈si, s̄i〉 | i = 1, ..., l}. This generative Seq2Seq model can then reorder the sequences in the labeled training datasetD = {(xi, yi) | i = 1, ..., n}, where n is the number of labeled instances, each xi consists of a sequence pair 〈s1, s2〉, and each yi ∈ Y is the corresponding ground truth label describing their relationship."
    }, {
      "heading" : "2.1.2 Semantic Augmentation",
      "text" : "Our second augmentation strategy involves training a controllable model that, given a sentence and a label describing the desired relationship, seeks to emit a second sentence that stands in said relationship to the input sentence. Thus, given an existing training sentence pair, we can consider different variations of one sentence in the pair and invoke the model to generate a suitable second sentence. However, such automatically induced samples from SA are inordinately noisy, precluding their immediate use as training data, so we exploit a large pretrained Teacher model trained on available source language samples to rectify the labels of these synthetic samples with appropriate strategies.\nGeneration. As we wish to be able to control the label of a generated example, the requested label is prepended to the input as a (textual) prefix before it is fed into a Seq2Seq model. We adopt the groundtruth label of each example as the respective prefix, resulting in a new input sequence (yi : s1) coupled with s2 as the desired output forming a training pair for the generation model.\nGiven the resulting labeled training dataset DSA, we can fine-tune a pretrained Seq2Seq model, denoted as g(·; θ). This generative Seq2Seq model can then be invoked for semantic data augmentation to generate new training instances. For each (ȳ : s1) as a labeled input sequence, where ȳ ∈ Y \\ {yi}, we generate an s̃2 via the fine-tuned Seq2Seq model, yielding a new training instance (〈s1, s̃2〉, ȳ).\nLabel Rectification. The semantic augmentation induces s̃2 automatically based on s1 and the requested label ȳ. However, the obtained s̃2 may not always genuinely have the desired relationship ȳ to s1. Thus, we treat this data as inherently noisy and\npropose a rectifying scheme based on a Teacher model. We wish for this Teacher to be as accurate as possible, so we start off with a large pretrained language model specifically for the source language LS, which we assume obtains a better performance on LS than a pretrained multilingual model. We train the Teacher network h(·; θ) in K epochs using the set of original labeled data D. This teacher model is then invoked to verify and potentially rectify labels from the automatically induced augmentation data Dã = {(x̃i, yi) | i = 1, ...,m} obtained in the previous step (where m is the number of instances). We assume (ỹi, c) = h(x̃i; θ) denotes the predicted label along with the confidence score c ∈ [0, 1] emitted by the classifier, and assume a confidence threshold T has been predetermined. There are several strategies to determine the final labels.\n• Teacher Strategy: We adopt Dr = {(x̃i, ỹi) | (x̃i, yi) ∈ Dã, (ỹi, c) = h(x̃i), c > T}, i.e., when the confidence score is above T , we believe the Teacher model is sufficiently confident to ensure a reliable label, while other instances are discarded.\n• TR Strategy: An alternative scheme is to instead adopt Dr = {(x̃i,Φ(yi, ỹi, c)) | (x̃i, yi) ∈ Dã, (ỹi, c) = h(x̃i)}, where\nΦ(yi, ỹi, c) =\n{ ỹi c > T\nyi otherwise\nHere, labels remain unchanged when Teacher predictions match the originally requested labels. In case of an inconsistency, we adopt the Teacher model’s label if it is sufficiently confident, and otherwise retain the requested label."
    }, {
      "heading" : "2.2 Adversarial Training",
      "text" : "Upon completing the two kinds of data augmentation, we possess synthesized data that is substantially less noisy, denoted as Dr, which can be incorporated into the original training dataD to yield the final augmented training set Da = D ∪ Dr. With this, we proceed to train a new model f(·; θ) for the final cross-lingual sentence pair classification.\nAs a special training regimen, we adopt adversarial training, which seeks to minimize the maximal loss incurred by label-preserving adversarial perturbations (Szegedy et al., 2014; Goodfellow et al., 2015), thereby promising to make the model more robust. Nonetheless, the gains observed from it\nin practice have been somewhat limited in both monolingual and cross-lingual settings. We conjecture that this is because it has previously merely been invoked as an additional form of monolingual regularization (Miyato et al., 2017).\nIn contrast, we hypothesize that adversarial training is particularly productive in a cross-lingual framework when used to exploit augmented data, as it encourages the model to be more robust towards the divergence among similar words and word orders in different languages and to better adapt to the new modestly noisy data. This hypothesis is later confirmed in our experimental results.\nAdversarial training is based on the notion of finding optimal parameters θ to make the model robust against any perturbation r within a norm ball on a continuous multilingual (sub-)word embedding space. Hence, the loss function becomes:\nLadv(xi, yi) = L(f(xi + radv(xi, yi); θ), yi) (1)\nwhere radv(xi, yi) = argmax r,||r||≤ L(f(xi + r; θ̃), yi)\nGenerally, a closed form for the optimal perturbation radv(xi, yi) cannot be obtained for deep neural networks. Goodfellow et al. (2015) proposed approximating this worst case perturbation by linearizing f(xi; θ̃) around xi. With a linear approximation and an L2 norm constraint in Equation 2, the adversarial perturbation is\nradv(xi, yi) ≈ g(xi, yi)\n||g(xi, yi)||2 (2)\nwhere g(xi, yi) = ∇xiL(f(xi; θ̃), yi).\nHowever, neural networks are typically not linear even over a relatively small region, so this approximation cannot guarantee to achieve the best optimal point within the bound. Madry et al. (2017) demonstrated that projected gradient descent (PGD) allows us to find a better perturbation radv(xi, yi). In particular, for the norm ball constraint ||r|| ≤ , given a point r0, Π||r||≤ aims to find a perturbation r that is closest to r0 as follows:\nΠ||r||≤ (r0) = argmin ||r||≤\n||r− r0|| (3)\nTo find more optimal points, K-step PGD is needed during training, which requires K forward– backward passes through the network. With a linear approximation and an L2 norm constraint, PGD\ntakes the following step in each iteration:\nrt+1 = Π||r||≤ ( rt + α g(xi, yi, rt)\n||g(xi, yi, rt)||2)\n) (4)\nwhere g(xi, yi, rt) = ∇rtL(f(xi + rt; θ̃), yi)\nHere, α is the step size and t is the step index."
    }, {
      "heading" : "3 Experiments and Analysis",
      "text" : ""
    }, {
      "heading" : "3.1 Experimental Setup",
      "text" : "Tasks and Datasets. For evaluation, we used XNLI (Conneau et al., 2018), the most prominent cross-lingual Natural Language Inference corpus, which extends the MultiNLI dataset (Williams et al., 2017) to 15 languages. In our experiments, we considered 20k training data, i.e., ∼5% of the original training size to study lower-resource settings requiring augmentation. Following previous work, we consider English as the source language in our experiments.\nModel Details. To show that our reorder augmentation strategy does not require auxiliary data from a low-resource target language, we only give it access to parallel data for another closely related high-resource language. Specifically, we use the English–German bilingual parallel corpus from JW300 (Agić and Vulić, 2019). Like English, German commonly adopts an SVO word order, but in some instances also mandates SOV and is generally less rigid than English. This allows us to demonstrate the utility of reorder augmentation even in the absence of data from a language similar to the target language. We relied on FastAlign1 to induce 200k training pairs for Seq2Seq fine-tuning on reordering.\nAs the pre-trained Seq2Seq model, we used Google’s T5-base (Raffel et al., 2020), a unified text-to-text Transformer, to generate new training examples. During generation, we set the beam size as 1 and use sampling instead of greedy decoding. For the Teacher model in semantic augmentation, we relied on RoBERTa-Large (Liu et al., 2019), a robustly optimized BERT model, to fine-tune NLI on English. As the multilingual model, we employ XLM-RoBERTa-base (XLM-R) (Conneau et al., 2019), trained on over 100 different languages. For PGD, the step size α, norm constraint size , and number of steps K are 1.0, 3.0, 3, respectively. All hyperparameter tuning is conducted based on the\n1https://github.com/clab/fast align\naccuracy on the English validation set. The Teacher strategy for XNLI then is used for the rectification of semantically augmented texts, as inference requires particularly clean data. The threshold T for this is 0.8. An overview of the basic network parameter values is given in Table 1. We rely on early stopping as a termination criterion. For all NLI classification results, we randomly repeat each experiment 5 times and report the averaged accuracy."
    }, {
      "heading" : "3.2 Main Results",
      "text" : "Cross-lingual Inference Classification. Table 2 compares our approach against several strong baselines on XNLI. The first part considers in-language supervised learning, where we relied on genuine training data from the target language rather than a cross-lingual setting. These results are merely provided for comparison. The second part considers zero-shot cross-lingual transfer, i.e., the setting we are targeting in this paper: We first used English training data to train the XLM-R model and then applied it to non-English languages without any training data in the target language. We also trained the model with PGD adversarial training to assess how well PGD works without any data augmentation. Next, we evaluate XLM-R when trained on original and augmented examples from several augmentation methods, with and without adversarial training, respectively. The first of these is Easy Augmentation (EA) by Wei and Zou (2019), a stateof-the-art method for data augmentation in NLP. It mixes 4 strategies, namely synonym replacement, random insertion, random swapping, and random deletion, applying each of these to 20% of words in a sentence. Additionally, we consider our proposed RA and SA strategies, as well as combinations of EA or RA with SA.\nCompared with vanilla XLM-R without adversarial training, XLM-R with PGD works better across a range of non-English languages, which shows the effectiveness of adversarial training for more robustness in cross-lingual settings. We observe that XLM-R, when trained with EA or RA, outperform the setting without augmentation for English and some non-English languages, though\nit does not achieve sufficiently stronger results in terms of the average accuracy across different languages. This suggests that XLM-R struggles to benefit from the augmented instances from RA for better generalizability. In contrast, when trained with SA, XLM-R performs better than without SA examples for most languages, confirming that our semantic augmentation is beneficial. Remarkably, XLM-R with SA examples even succeeds at outperforming in-language training with an average absolute improvement of about 1.1% in accuracy, suggesting that cross-lingual models trained with automatically generated English examples can be more informative with regard to inference than target language examples.2 Next, we also observe that the accuracy of XLM-R with additional examples from EA, RA, SA is boosted with PGD. This suggests that adversarial training is particularly useful to boost generalizability and robustness when operating on artificial augmented examples.\nBeyond this, our full zero-shot approach further outperforms all baselines across 14 languages, including in-language training. This demonstrates the value of improving generalizability and robustness by adding diverse forms of augmentation in an adversarial training framework that can cope with noisy examples."
    }, {
      "heading" : "3.3 Ablation Studies and Analysis",
      "text" : "Comparisons on Different Rectifying Strategies. One key part of our method is the label rectification mechanism. We compare different rectification strategies in Table 3. The results show that the Teacher and TR methods introduced in Section 2.1.2 yield fairly similar results. This confirms the robustness of our approach with regard to the choice of strategy. The same also holds for an additional option, Agreement, which retains only those examples on which the prediction from the Teacher agrees with the originally requested label. Finally, for comparison, we evaluated yet another strategy, Requested, which always adopts the originally requested labels as chosen for generation. We find that this strategy introduces overly many unreliable labels, so the model is unable to work well. This confirms that rectifying labels with a Teacher model is a crucial ingredient.\nComparisons on Adversarial Perturbations. For assessing the value of PGD for adversarial per-\n2Note that the in-language training data in XNLI was created using machine translation.\nturbation, Table 4 compares PGD with the standard Fast Gradient Method (FGM) for adversarial perturbation (Goodfellow et al., 2015) as introduced in Section 2.2. We ran experiments on XNLI with 10k and 20k training data, each augmented with 80k induced semantic examples. We observe that FGM obtains a lower average accuracy than PGD with the same amount of training data, confirming the superiority of PGD in providing better adversarial perturbations than FGM to improve both generalization and robustness.\nEffectiveness on Different Training Sizes. Data augmentation is an important approach to deal with scarce labels. The results in Table 4 further show that when fine-tuning T5 using 10k XNLI training instances with 80k semantic and 10k reorder augmented examples, we obtain substantially better results than when using 20k training instances without augmentation. We can also observe the improvement of XLM-R with RA, SA, and adversarial training over vanilla XLM-R on each language as plotted in Figure 2. The relative gains with 10k training data are larger than with 20k training data across a range of languages, which shows that our method is consistently most beneficial when training data is scarce.\nInfluence of Amount of Augmentation. To assess the role of the amount of data augmentation, we conducted experiments on XNLI with 20k training examples, and evaluated the effect of adding either 20k or 80k augmented examples from EA, RA, SA. The results are given in Table 5. When trained without PGD, one can often benefit from using up to 80k augmented examples. Due to the inherent reordering differences between English and German, there are limits regarding the amount of such data one ought to incorporate. We find that 20k instances from RA can suffice. We observe\nthat EA with PGD requires up to 80k augmented instances, i.e., 3 times the size of the original training data, to outperform XLM-R with PGD, whereas only 20k augmented examples suffice for RA with PGD to beat XLM-R with PGD.\nCase Studies. To better illustrate the principles of our data augmentation technique, we provide several examples. Table 6 shows two examples of the three data augmentation processes on XNLI. For the first example, the original label is contradiction, so entailment and neutral serve as requested labels to generate new training text. Next, our Teacher model attempts to rectify these labels. Although our generative model treats Vrenna and I fought him in a fight, but he had just gotten us as neutral to S1 (Vrenna and I both fought him and he nearly took us), the Teacher model changes the label to entailment. For the second example, both the generative and Teacher model are unable to conclude that The rice ripens in the summer is contradictory with the premise. From the two EA outputs, we can observe him is randomly deleted in Example (1) and the and rice is swapped in Example (2), which loses some information, whereas RA\nSeq2Seq generated examples maintain all crucial information despite the reordering."
    }, {
      "heading" : "4 Related Work",
      "text" : "Data Augmentation. Data augmentation is a promising technique, especially when dealing with scarce data, imbalanced data, or semi-supervised learning problems. Back-translation (Sennrich et al., 2015) has been considered as a technique to obtain alternative examples preserving the original semantics, by translating an existing example in language LA into another language LB and then translating it back into LA to obtain an augmented example. Yu et al. (2018) and Xie et al. (2020) applied it to question answering and semi-supervised monolingual training scenarios. However, this requires high-quality translation engines that often do not exist in the settings in which one wishes to apply cross-lingual systems.\nWei and Zou (2019) instead combined synonym replacement, random insertion, random swapping, and random deletion in a method named EDA. Since insertion and deletion may affect the semantics of the utterance, some studies opt to control\nthe selection of words to be replaced with indicators such as TF-IDF scores (Xie et al., 2020). Fadaee et al. (2017) use contextualized word embeddings to replace the target word. Kobayashi (2018) proposed a bi-directional language-modelbased augmentation method, and Wu et al. (2019) further improved its results by switching to BERT. Another major category is text generation based augmentation. Anaby-Tavor et al. (2020) proposed a language model based data augmentation method, shown to improve classifier performance on a variety of English datasets. It relies on GPT-2 (Radford et al., 2018) to generate a single new sequence in each instance.\nOur work, in contrast, presents a novel augmentation scheme designed to cope with the special challenges of sentence pair classification, where a Seq2Seq Transformer enables augmentation based on a paired input sentence. Our method also introduces a Teacher model to rectify labels. Apart from this, we expand the idea of language model based augmentation to cross-lingual settings and leverage noisy instances with adversarial training.\nAdversarial Training. Many approaches for improving the robustness of a machine learning system against adversarial perturbations (Szegedy et al., 2014) have been advanced. Goodfellow et al. (2015) proposed a fast gradient method based on linear perturbation of non-linear models. Later, Madry et al. (2017) presented PGD-based adversarial training through multiple projected gradient\nascent steps to adversarially maximize the loss. In NLP, Belinkov and Bisk (2017) exploited structureinvariant word manipulation and robust training on noisy texts for improved robustness. Iyyer et al. (2018) proposed syntactically controlled paraphrase networks with back-translated data and used them to generate adversarial examples. Adversarial training also plays a role in improving a neural model’s generalization. For instance, Cheng et al. (2019) used adversarial source examples to improve a translation model. Dong et al. (2020) exploit FGM-based adversarial training in selflearning for improved cross-lingual text classification. In our setting, we count on adversarial training in the word embedding space and show that PGD-based adversarial training remains effective when the adversarial perturbation is applied to noisy augmented examples."
    }, {
      "heading" : "5 Conclusion",
      "text" : "While multilingual pretrained model have enabled better cross-lingual learning, we still often encounter data scarcity issues due to the high cost of collecting data, which weakens the generalization ability of the multilingual model.\nTo address this, this paper proposes a novel data augmentation strategy with label rectification to build synthetic examples, outperforming even models trained with larger amounts of ground-truth data. We show that we can best learn from such noisy instances with adversarial training, which enables\nthe classifier to transfer more information from the source language to other languages and to become more robust. Remarkably, with this, our models trained without any target language training data at all are able to outperform models trained fully on in-language training data. Moreover, the amount of augmented data from our Seq2Seq-based reorder augmentation used in training is much less than that required by the state-of-the-art EDA method in order to achieve comparable performance. Finally, in our series of follow-up experiments comparing different training regimens and variants, one notable finding is that our overall augmented approach can even outperform non-augmented supervision with twice as many ground truth labels. Overall, this suggests our combination of data augmentation with adversarial training as a valuable way of learning substantially more accurate and more robust models without any target-language training data.\nBroader Impact\nResearch on cross-lingual NLP is often motivated by a desire to provide state-of-the-art advances to linguistic communities that have been underserved. Such advances may enable better access to information as well as to products and services. However, there is a risk that such technological advances may not always be desired by the relevant communities and may indeed also cause harm to them (Bird, 2020). Moreover, cross-lingual systems in particular may exhibit biases with regard to the source language used for training and the general cultural assumptions reflected in such data. In light of this, special care needs to be taken to analyze potential outcomes and risks before deploying cross-lingual systems in real-world applications."
    } ],
    "references" : [ {
      "title" : "JW300: A widecoverage parallel corpus for low-resource languages",
      "author" : [ "Željko Agić", "Ivan Vulić." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3204–3210, Florence, Italy. Association for Compu-",
      "citeRegEx" : "Agić and Vulić.,? 2019",
      "shortCiteRegEx" : "Agić and Vulić.",
      "year" : 2019
    }, {
      "title" : "Do not have enough data? deep learning to the rescue",
      "author" : [ "Ateret Anaby-Tavor", "Boaz Carmeli", "Esther Goldbraich", "Amir Kantor", "George Kour", "Segev Shlomov", "Naama Tepper", "Naama Zwerdling" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Anaby.Tavor et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Anaby.Tavor et al\\.",
      "year" : 2020
    }, {
      "title" : "Massively Multilingual Sentence Embeddings for Zero-Shot",
      "author" : [ "Mikel Artetxe", "Holger Schwenk" ],
      "venue" : null,
      "citeRegEx" : "Artetxe and Schwenk.,? \\Q2019\\E",
      "shortCiteRegEx" : "Artetxe and Schwenk.",
      "year" : 2019
    }, {
      "title" : "Synthetic and natural noise both break neural machine translation",
      "author" : [ "Yonatan Belinkov", "Yonatan Bisk." ],
      "venue" : "arXiv preprint arXiv:1711.02173.",
      "citeRegEx" : "Belinkov and Bisk.,? 2017",
      "shortCiteRegEx" : "Belinkov and Bisk.",
      "year" : 2017
    }, {
      "title" : "Decolonising speech and language technology",
      "author" : [ "Steven Bird." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 3504–3519, Barcelona, Spain (Online). International Committee on Computational Linguistics.",
      "citeRegEx" : "Bird.,? 2020",
      "shortCiteRegEx" : "Bird.",
      "year" : 2020
    }, {
      "title" : "Robust neural machine translation with doubly adversarial inputs",
      "author" : [ "Yong Cheng", "Lu Jiang", "Wolfgang Macherey." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4324–4333, Florence, Italy. Associa-",
      "citeRegEx" : "Cheng et al\\.,? 2019",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv",
      "citeRegEx" : "Conneau et al\\.,? 2019",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2019
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Alexis Conneau", "Guillaume Lample." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
      "citeRegEx" : "Conneau and Lample.,? 2019",
      "shortCiteRegEx" : "Conneau and Lample.",
      "year" : 2019
    }, {
      "title" : "Xnli: Evaluating crosslingual sentence representations",
      "author" : [ "Alexis Conneau", "Guillaume Lample", "Ruty Rinott", "Adina Williams", "Samuel R Bowman", "Holger Schwenk", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1809.05053.",
      "citeRegEx" : "Conneau et al\\.,? 2018",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "Multilingual text classification using ontologies",
      "author" : [ "Gerard de Melo", "Stefan Siersdorfer." ],
      "venue" : "Proceedings of ECIR 2007. Springer.",
      "citeRegEx" : "Melo and Siersdorfer.,? 2007",
      "shortCiteRegEx" : "Melo and Siersdorfer.",
      "year" : 2007
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "A robust selflearning framework for cross-lingual text classification",
      "author" : [ "Xin Dong", "Gerard de Melo." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Dong and Melo.,? 2019",
      "shortCiteRegEx" : "Dong and Melo.",
      "year" : 2019
    }, {
      "title" : "Leveraging adversarial training in self-learning for cross-lingual text classification",
      "author" : [ "Xin Dong", "Yaxin Zhu", "Yupeng Zhang", "Zuohui Fu", "Dongkuan Xu", "Sen Yang", "Gerard de Melo." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Dong et al\\.,? 2020",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2020
    }, {
      "title" : "A simple, fast, and effective reparameterization of ibm model 2",
      "author" : [ "Chris Dyer", "Victor Chahuneau", "Noah A Smith." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Dyer et al\\.,? 2013",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2013
    }, {
      "title" : "Data augmentation for lowresource neural machine translation",
      "author" : [ "Marzieh Fadaee", "Arianna Bisazza", "Christof Monz." ],
      "venue" : "arXiv preprint arXiv:1705.00440.",
      "citeRegEx" : "Fadaee et al\\.,? 2017",
      "shortCiteRegEx" : "Fadaee et al\\.",
      "year" : 2017
    }, {
      "title" : "ABSent: Cross-lingual sentence representation mapping with bidirectional GANs",
      "author" : [ "Zuohui Fu", "Yikun Xian", "Shijie Geng", "Yingqiang Ge", "Yuting Wang", "Xin Dong", "Guang Wang", "Gerard de Melo." ],
      "venue" : "Proceedings of the 34th AAAI Conference on Arti-",
      "citeRegEx" : "Fu et al\\.,? 2020",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2020
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "Ian Goodfellow", "Jonathon Shlens", "Christian Szegedy." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Goodfellow et al\\.,? 2015",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2015
    }, {
      "title" : "Adversarial example generation with syntactically controlled paraphrase networks",
      "author" : [ "Mohit Iyyer", "John Wieting", "Kevin Gimpel", "Luke Zettlemoyer." ],
      "venue" : "arXiv preprint arXiv:1804.06059.",
      "citeRegEx" : "Iyyer et al\\.,? 2018",
      "shortCiteRegEx" : "Iyyer et al\\.",
      "year" : 2018
    }, {
      "title" : "Contextual augmentation: Data augmentation by words with paradigmatic relations",
      "author" : [ "Sosuke Kobayashi." ],
      "venue" : "arXiv preprint arXiv:1805.06201.",
      "citeRegEx" : "Kobayashi.,? 2018",
      "shortCiteRegEx" : "Kobayashi.",
      "year" : 2018
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton." ],
      "venue" : "Advances in neural information processing systems, pages 1097–1105.",
      "citeRegEx" : "Krizhevsky et al\\.,? 2012",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers",
      "author" : [ "Anne Lauscher", "Vinit Ravishankar", "Ivan Vulić", "Goran Glavaš." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Lauscher et al\\.,? 2020",
      "shortCiteRegEx" : "Lauscher et al\\.",
      "year" : 2020
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards deep learning models resistant to adversarial attacks",
      "author" : [ "Aleksander Madry", "Aleksandar Makelov", "Ludwig Schmidt", "Dimitris Tsipras", "Adrian Vladu." ],
      "venue" : "arXiv preprint arXiv:1706.06083.",
      "citeRegEx" : "Madry et al\\.,? 2017",
      "shortCiteRegEx" : "Madry et al\\.",
      "year" : 2017
    }, {
      "title" : "Adversarial training methods for semisupervised text classification",
      "author" : [ "Takeru Miyato", "Andrew M. Dai", "Ian Goodfellow." ],
      "venue" : "ICLR.",
      "citeRegEx" : "Miyato et al\\.,? 2017",
      "shortCiteRegEx" : "Miyato et al\\.",
      "year" : 2017
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Improving neural machine translation models with monolingual data",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "arXiv preprint arXiv:1511.06709.",
      "citeRegEx" : "Sennrich et al\\.,? 2015",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2015
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Szegedy et al\\.,? 2014",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2014
    }, {
      "title" : "EDA: Easy data augmentation techniques for boosting performance on text classification tasks",
      "author" : [ "Jason Wei", "Kai Zou." ],
      "venue" : "Proceedings of EMNLP-IJCNLP 2019, pages 6382–6388, Hong Kong, China.",
      "citeRegEx" : "Wei and Zou.,? 2019",
      "shortCiteRegEx" : "Wei and Zou.",
      "year" : 2019
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel R Bowman." ],
      "venue" : "arXiv preprint arXiv:1704.05426.",
      "citeRegEx" : "Williams et al\\.,? 2017",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2017
    }, {
      "title" : "Conditional bert contextual augmentation",
      "author" : [ "Xing Wu", "Shangwen Lv", "Liangjun Zang", "Jizhong Han", "Songlin Hu." ],
      "venue" : "International Conference on Computational Science, pages 84–95. Springer.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised data augmentation for consistency training",
      "author" : [ "Qizhe Xie", "Zihang Dai", "Eduard Hovy", "Thang Luong", "Quoc Le." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 33, pages 6256–6268. Curran Associates, Inc.",
      "citeRegEx" : "Xie et al\\.,? 2020",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2020
    }, {
      "title" : "Fast and accurate reading comprehension by combining self-attention and convolution",
      "author" : [ "Adams Wei Yu", "David Dohan", "Quoc Le", "Thang Luong", "Rui Zhao", "Kai Chen." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Yu et al\\.,? 2018",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2018
    }, {
      "title" : "Exploring semantic properties of sentence embeddings",
      "author" : [ "Xunjie Zhu", "Tingfeng Li", "Gerard de Melo." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 632–637, Melbourne,",
      "citeRegEx" : "Zhu et al\\.,? 2018",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "In the past, such systems typically drew on translation dictionaries, lexical knowledge graphs, or parallel corpora, to build a cross-lingual model that exploits simple connections between words and phrases across different languages (de Melo and Siersdorfer, 2007; Fu et al., 2020).",
      "startOffset" : 234,
      "endOffset" : 282
    }, {
      "referenceID" : 10,
      "context" : "Recently, pretrained language model architectures such as BERT (Devlin et al., 2019) have been shown capable of learning joint multilingual representations with self-supervised objectives under a shared vocabulary, simply by combining the input from multiple languages (Devlin et al.",
      "startOffset" : 63,
      "endOffset" : 84
    }, {
      "referenceID" : 10,
      "context" : ", 2019) have been shown capable of learning joint multilingual representations with self-supervised objectives under a shared vocabulary, simply by combining the input from multiple languages (Devlin et al., 2019; Artetxe and Schwenk, 2019; Conneau and Lample, 2019; Conneau et al., 2019).",
      "startOffset" : 192,
      "endOffset" : 288
    }, {
      "referenceID" : 2,
      "context" : ", 2019) have been shown capable of learning joint multilingual representations with self-supervised objectives under a shared vocabulary, simply by combining the input from multiple languages (Devlin et al., 2019; Artetxe and Schwenk, 2019; Conneau and Lample, 2019; Conneau et al., 2019).",
      "startOffset" : 192,
      "endOffset" : 288
    }, {
      "referenceID" : 7,
      "context" : ", 2019) have been shown capable of learning joint multilingual representations with self-supervised objectives under a shared vocabulary, simply by combining the input from multiple languages (Devlin et al., 2019; Artetxe and Schwenk, 2019; Conneau and Lample, 2019; Conneau et al., 2019).",
      "startOffset" : 192,
      "endOffset" : 288
    }, {
      "referenceID" : 6,
      "context" : ", 2019) have been shown capable of learning joint multilingual representations with self-supervised objectives under a shared vocabulary, simply by combining the input from multiple languages (Devlin et al., 2019; Artetxe and Schwenk, 2019; Conneau and Lample, 2019; Conneau et al., 2019).",
      "startOffset" : 192,
      "endOffset" : 288
    }, {
      "referenceID" : 20,
      "context" : "how close the involved languages are, with substantial drops observed for some more distant language pairs (Lauscher et al., 2020).",
      "startOffset" : 107,
      "endOffset" : 130
    }, {
      "referenceID" : 30,
      "context" : ", classifying whether a premise sentence entails, contradicts, or is neutral with regard to a hypothesis sentence (Williams et al., 2017).",
      "startOffset" : 114,
      "endOffset" : 137
    }, {
      "referenceID" : 19,
      "context" : "these typically preserve the original label assigned to an image (Krizhevsky et al., 2012).",
      "startOffset" : 65,
      "endOffset" : 90
    }, {
      "referenceID" : 29,
      "context" : "For text, in contrast, data augmentation is more challenging, and straightforward techniques include simple operations on words within the original training sequences, such as synonym replacement, random insertion, random swapping, or random deletion (Wei and Zou, 2019).",
      "startOffset" : 251,
      "endOffset" : 270
    }, {
      "referenceID" : 8,
      "context" : "Our empirical study demonstrates the necessity of incorporating adversarial training into training with synthetic samples and the superiority of our new augmentation method on cross-lingual Natural Language Inference (Conneau et al., 2018).",
      "startOffset" : 217,
      "endOffset" : 239
    }, {
      "referenceID" : 13,
      "context" : "model, in our case FastAlign (Dyer et al., 2013), which uses Expectation Maximization to compute the lexical translation probabilities, we obtain a word pair table for each sentence pair 〈s, t〉, denoted as A(s, a) = {(i1, j1), .",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 28,
      "context" : "As a special training regimen, we adopt adversarial training, which seeks to minimize the maximal loss incurred by label-preserving adversarial perturbations (Szegedy et al., 2014; Goodfellow et al., 2015), thereby promising to make the model more robust.",
      "startOffset" : 158,
      "endOffset" : 205
    }, {
      "referenceID" : 16,
      "context" : "As a special training regimen, we adopt adversarial training, which seeks to minimize the maximal loss incurred by label-preserving adversarial perturbations (Szegedy et al., 2014; Goodfellow et al., 2015), thereby promising to make the model more robust.",
      "startOffset" : 158,
      "endOffset" : 205
    }, {
      "referenceID" : 23,
      "context" : "We conjecture that this is because it has previously merely been invoked as an additional form of monolingual regularization (Miyato et al., 2017).",
      "startOffset" : 125,
      "endOffset" : 146
    }, {
      "referenceID" : 8,
      "context" : "For evaluation, we used XNLI (Conneau et al., 2018), the most prominent cross-lingual Natural Language Inference corpus, which extends the MultiNLI dataset (Williams",
      "startOffset" : 29,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : "the English–German bilingual parallel corpus from JW300 (Agić and Vulić, 2019).",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 25,
      "context" : "As the pre-trained Seq2Seq model, we used Google’s T5-base (Raffel et al., 2020), a unified text-to-text Transformer, to generate new training examples.",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 21,
      "context" : "For the Teacher model in semantic augmentation, we relied on RoBERTa-Large (Liu et al., 2019), a robustly optimized BERT model, to fine-tune NLI on English.",
      "startOffset" : 75,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : "As the multilingual model, we employ XLM-RoBERTa-base (XLM-R) (Conneau et al., 2019), trained on over 100 different languages.",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 16,
      "context" : "turbation, Table 4 compares PGD with the standard Fast Gradient Method (FGM) for adversarial perturbation (Goodfellow et al., 2015) as introduced in Section 2.",
      "startOffset" : 106,
      "endOffset" : 131
    }, {
      "referenceID" : 32,
      "context" : "the selection of words to be replaced with indicators such as TF-IDF scores (Xie et al., 2020).",
      "startOffset" : 76,
      "endOffset" : 94
    }, {
      "referenceID" : 24,
      "context" : "It relies on GPT-2 (Radford et al., 2018) to generate a single new sequence in each instance.",
      "startOffset" : 19,
      "endOffset" : 41
    }, {
      "referenceID" : 28,
      "context" : "Many approaches for improving the robustness of a machine learning system against adversarial perturbations (Szegedy et al., 2014) have been advanced.",
      "startOffset" : 108,
      "endOffset" : 130
    }, {
      "referenceID" : 4,
      "context" : "and may indeed also cause harm to them (Bird, 2020).",
      "startOffset" : 39,
      "endOffset" : 51
    } ],
    "year" : 2021,
    "abstractText" : "Due to recent pretrained multilingual representation models, it has become feasible to exploit labeled data from one language to train a crosslingual model that can then be applied to multiple new languages. In practice, however, we still face the problem of scarce labeled data, leading to subpar results. In this paper, we propose a novel data augmentation strategy for better cross-lingual natural language inference by enriching the data to reflect more diversity in a semantically faithful way. To this end, we propose two methods of training a generative model to induce synthesized examples, and then leverage the resulting data using an adversarial training regimen for more robustness. In a series of detailed experiments, we show that this fruitful combination leads to substantial gains in cross-lingual inference.",
    "creator" : "LaTeX with hyperref"
  }
}