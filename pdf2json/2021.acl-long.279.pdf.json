{
  "name" : "2021.acl-long.279.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Superbizarre Is Not Superb: Derivational Morphology Improves BERT's Interpretation of Complex Words",
    "authors" : [ "Valentin Hofmann", "Janet B. Pierrehumbert", "Hinrich Schütze" ],
    "emails" : [ "valentin.hofmann@ling-phil.ox.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3594–3608\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3594"
    }, {
      "heading" : "1 Introduction",
      "text" : "Pretrained language models (PLMs) such as BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019), XLNet (Yang et al., 2019), ELECTRA (Clark et al., 2020), and T5 (Raffel et al., 2020) have yielded substantial improvements on a range of NLP tasks. What linguistic properties do they have? Various studies have tried to illuminate this question, with a focus on syntax (Hewitt and Manning, 2019; Jawahar et al., 2019) and semantics (Ethayarajh, 2019; Ettinger, 2020; Vulić et al., 2020).\nOne common characteristic of PLMs is their input segmentation: PLMs are based on fixed-size vocabularies of words and subwords that are generated by compression algorithms such as bytepair encoding (Gage, 1994; Sennrich et al., 2016) and WordPiece (Schuster and Nakajima, 2012; Wu et al., 2016). The segmentations produced by these\nalgorithms are linguistically questionable at times (Church, 2020), which has been shown to worsen performance on certain downstream tasks (Bostrom and Durrett, 2020; Hofmann et al., 2020a). However, the wider implications of these findings, particularly with regard to the generalization capabilities of PLMs, are still poorly understood.\nHere, we address a central aspect of this issue, namely how the input segmentation affects the semantic representations of PLMs, taking BERT as the example PLM. We focus on derivationally complex words such as superbizarre since they exhibit systematic patterns on the lexical level, providing an ideal testbed for linguistic generalization. At the same time, the fact that low-frequency and out-of-vocabulary words are often derivationally complex (Baayen and Lieber, 1991) makes our work relevant in practical settings, especially when many one-word expressions are involved, e.g., in query processing (Kacprzak et al., 2017).\nThe topic of this paper is related to the more fundamental question of how PLMs represent the meaning of complex words in the first place. So far, most studies have focused on methods of representation extraction, using ad-hoc heuristics such as averaging the subword embeddings (Pinter et al., 2020; Sia et al., 2020; Vulić et al., 2020) or taking the first subword embedding (Devlin et al., 2019; Heinzerling and Strube, 2019; Martin et al., 2020). While not resolving the issue, we lay the theoretical groundwork for more systematic analyses by showing that PLMs can be regarded as serial dual-route models (Caramazza et al., 1988), i.e., the meanings of complex words are either stored or else need to be computed from the subwords.\nContributions. We present the first study examining how the input segmentation of PLMs, specifically BERT, affects their interpretations of derivationally complex English words. We show that PLMs can be interpreted as serial dualroute models, which implies that maximally meaningful input tokens should allow for the best generalization on new words. This hypothesis is confirmed by a series of semantic probing tasks on which derivational segmentation substantially outperforms BERT’s WordPiece segmentation. This suggests that the generalization capabilities of PLMs could be further improved if a morphologically-informed vocabulary of input tokens were used. We also publish three large datasets of derivationally complex words with corresponding semantic properties.1"
    }, {
      "heading" : "2 How Are Complex Words Processed?",
      "text" : ""
    }, {
      "heading" : "2.1 Complex Words in Psycholinguistics",
      "text" : "The question of how complex words are processed has been at the center of psycholinguistic research over the last decades (see Leminen et al. (2019) for a recent review). Two basic processing mechanisms have been proposed: storage, where the meaning of complex words is listed in the mental lexicon (Manelis and Tharp, 1977; Butterworth, 1983; Feldman and Fowler, 1987; Bybee, 1988; Stemberger, 1994; Bybee, 1995; Bertram et al., 2000a), and computation, where the meaning of complex words is inferred based on the meaning of stem and affixes (Taft and Forster, 1975; Taft, 1979, 1981, 1988, 1991, 1994; Rastle et al., 2004; Taft, 2004; Rastle and Davis, 2008).\n1We make our code and data available at https:// github.com/valentinhofmann/superbizarre.\nIn contrasting with single-route frameworks, dual-route models allow for a combination of storage and computation. Dual-route models are further classified by whether they regard the processes of retrieving meaning from the mental lexicon and computing meaning based on stem and affixes as parallel, i.e., both mechanisms are always activated (Frauenfelder and Schreuder, 1992; Schreuder and Baayen, 1995; Baayen et al., 1997, 2000; Bertram et al., 2000b; New et al., 2004; Kuperman et al., 2008, 2009), or serial, i.e., the computation-based mechanism is only activated when the storagebased one fails (Laudanna and Burani, 1985; Burani and Caramazza, 1987; Caramazza et al., 1988; Burani and Laudanna, 1992; Laudanna and Burani, 1995; Alegre and Gordon, 1999).\nOutside the taxonomy presented so far are recent models that assume multiple levels of representation as well as various forms of interaction between them (Rácz et al., 2015; Needle and Pierrehumbert, 2018). In these models, sufficiently frequent complex words are stored together with representations that include their internal structure. Complex-word processing is driven by analogical processes over the mental lexicon (Rácz et al., 2020)."
    }, {
      "heading" : "2.2 Complex Words in NLP and PLMs",
      "text" : "Most models of word meaning proposed in NLP can be roughly assigned to either the single-route or dual-route approach. Word embeddings that represent complex words as whole-word vectors (Deerwester et al., 1990; Mikolov et al., 2013a,b; Pennington et al., 2014) can be seen as single-route storage models. Word embeddings that represent complex words as a function of subword or morpheme vectors (Schütze, 1992; Luong et al., 2013) can be seen as single-route computation models. Finally, word embeddings that represent complex words as a function of subword or morpheme vectors as well as whole-word vectors (Botha and Blunsom, 2014; Qiu et al., 2014; Bhatia et al., 2016; Bojanowski et al., 2017; Athiwaratkun et al., 2018; Salle and Villavicencio, 2018) are most closely related to parallel dual-route approaches.\nWhere are PLMs to be located in this taxonomy? PLMs represent many complex words as wholeword vectors (which are fully stored). Similarly to how character-based models represent word meaning (Kim et al., 2016; Adel et al., 2017), they can also store the meaning of frequent complex words that are segmented into subwords, i.e., frequent sub-\nword collocations, in their model weights. When the complex-word meaning is neither stored as a whole-word vector nor in the model weights, PLMs compute the meaning as a compositional function of the subwords. Conceptually, PLMs can thus be interpreted as serial dual-route models. While the parallelism has not been observed before, it follows logically from the structure of PLMs. The key goal of this paper is to show that the implications of this observation are borne out empirically.\nAs a concrete example, consider the complex words stabilize, realize, finalize, mobilize, tribalize, and templatize, which are all formed by adding the verbal suffix ize to a nominal or adjectival stem. Taking BERT, specifically BERTBASE (uncased) (Devlin et al., 2019), as the example PLM, the words stabilize and realize have individual tokens in the input vocabulary and are hence associated with whole-word vectors storing their meanings, including highly lexicalized meanings as in the case of realize. By contrast, the words finalize and mobilize are segmented into final, ##ize and mob, ##ili, ##ze, which entails that their meanings are not stored as whole-word vectors. However, both words have relatively high absolute frequencies of 2,540 (finalize) and 6,904 (mobilize) in the English Wikipedia, the main dataset used to pretrain BERT (Devlin et al., 2019), which means that BERT can store their meanings in its model weights during pretraining.2 Notice this is even possible in the case of highly lexicalized meanings as for mobilize. Finally, the words tribalize and templatize are segmented into tribal, ##ize and te, ##mp, ##lat, ##ize, but as opposed to finalize and mobilize they do not occur in the English Wikipedia. As a result, BERT cannot store their meanings in its model weights during pretraining and needs to compute them from the meanings of the subwords.\nSeeing PLMs as serial dual-route models allows for a more nuanced view on the central research question of this paper: in order to investigate semantic generalization we need to investigate the representations of those complex words that activate the computation-based route. The words that do so are the ones whose meaning is neither stored as a whole-word vector nor in the model weights\n2Previous research suggests that such lexical knowledge is stored in the lower layers of BERT (Vulić et al., 2020).\nand hence needs to be computed compositionally as a function of the subwords (tribalize and templatize in the discussed examples). We hypothesize that the morphological validity of the segmentation affects the representational quality in these cases, and that the best generalization is achieved by maximally meaningful tokens. It is crucial to note this does not imply that the tokens have to be morphemes, but the segmentation boundaries need to coincide with morphological boundaries, i.e., groups of morphemes (e.g., tribal in the segmentation of tribalize) are also possible.3 For tribalize and templatize, we therefore expect the segmentation tribal, ##ize (morphologically valid since all segmentation boundaries are morpheme boundaries) to result in a representation of higher quality than the segmentation te, ##mp, ##lat, ##ize (morphologically invalid since the boundaries between te, ##mp, and ##lat are not morpheme boundaries). On the other hand, complex words whose meanings are stored in the model weights (finalize and mobilize in the discussed examples) are expected to be affected by the segmentation to a much lesser extent: if the meaning of a complex word is stored in the model weights, it should matter less whether the specific segmentation activating that meaning is morphologically valid (final, ##ize) or not (mob, ##ili, ##ze).4"
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Setup",
      "text" : "Analyzing the impact of different segmentations on BERT’s semantic generalization capabilities is not straightforward since it is not clear a priori how to measure the quality of representations. Here, we devise a novel lexical-semantic probing task: we use BERT’s representations for complex words to predict semantic dimensions, specifically sentiment and topicality (see Figure 1). For sentiment, given the example complex word superbizarre, the task is to predict that its sentiment is negative. For topicality, given the example complex word isotopize, the task is to predict that it is used in physics. We confine ourselves to binary predic-\n3This is in line with substantial evidence from linguistics showing that frequent groups of morphemes can be treated as semantic wholes (Stump, 2017, 2019).\n4We expect the distinction between storage and computation of complex-word meaning for PLMs to be a continuum. While the findings presented here are consistent with this view, we defer a more in-depth analysis to future work.\ntion, i.e., the probed semantic dimensions always consist of two classes (e.g., positive and negative). The extent to which a segmentation supports a solution of this task is taken as an indicator of its representational quality.\nMore formally, let D be a dataset consisting of complex words x and corresponding classes y that instantiate a certain semantic dimension (e.g., sentiment). We denote with s(x) = (t1, . . . , tk) the segmentation of x into a sequence of k subwords. We ask how s impacts the capability of BERT to predict y, i.e., how p(y|(s(x)), the likelihood of the true semantic class y given a certain segmentation of x, depends on different choices for s. The two segmentation methods we compare in this study are BERT’s standard WordPiece segmentation (Schuster and Nakajima, 2012; Wu et al., 2016), sw, and a derivational segmentation that segments complex words into stems and affixes, sd."
    }, {
      "heading" : "3.2 Data",
      "text" : "Since existing datasets do not allow us to conduct experiments following the described setup, we create new datasets in a weakly-supervised fashion that is conceptually similar to the method proposed by Mintz et al. (2009): we employ large datasets annotated for sentiment or topicality, extract derivationally complex words, and use the dataset labels to establish their semantic classes.\nFor determining and segmenting derivationally complex words, we use the algorithm introduced by Hofmann et al. (2020b), which takes as input a set of prefixes, suffixes, and stems and checks for each word in the data whether it can be derived from a stem using a combination of prefixes and suffixes.5 The algorithm is sensitive to morpho-orthographic rules of English (Plag, 2003), e.g., when the suf-\n5The distinction between inflectionally and derivationally complex words is notoriously fuzzy (Haspelmath and Sims, 2010; ten Hacken, 2014). We try to exclude inflection as far as possible (e.g., by removing problematic affixes such as ing) but are aware that a clear separation does not exist.\nfix ize is removed from isotopize, the result is isotope, not isotop. We follow Hofmann et al. (2020a) in using the prefixes, suffixes, and stems in BERT’s WordPiece vocabulary as input to the algorithm. This means that all tokens used by the derivational segmentation are in principle also available to the WordPiece segmentation, i.e., the difference between sw and sd does not lie in the vocabulary per se but rather in the way the vocabulary is used. See Appendix A.1 for details about the derivational segmentation.\nTo get the semantic classes, we compute for each complex word which fraction of texts containing the word belongs to one of two predefined sets of dataset labels (e.g., reviews with four and five stars for positive sentiment) and rank all words accordingly. We then take the first and third tertiles of complex words as representing the two classes. We randomly split the words into 60% training, 20% development, and 20% test.\nIn the following, we describe the characteristics of the three datasets in greater depth. Table 1 provides summary statistics. See Appendix A.2 for details about data preprocessing.\nAmazon. Amazon is an online e-commerce platform. A large dataset of Amazon reviews has been made publicly available (Ni et al., 2019).6 We extract derivationally complex words from reviews with one or two (neg) as well as four or five stars (pos), discarding three-star reviews for a clearer separation (Yang and Eisenstein, 2017).\nArXiv. ArXiv is an open-access distribution service for scientific articles. Recently, a dataset of all papers published on ArXiv with associated metadata has been released.7 For this study, we extract all articles from physics (phys) and computer science (cs), which we identify using ArXiv’s subject classification. We choose physics and computer\n6https://nijianmo.github.io/amazon/ index.html\n7https://www.kaggle.com/ Cornell-University/arxiv\nscience since we expect large topical distances for these classes (compared to alternatives such as mathematics and computer science).\nReddit. Reddit is a social media platform hosting discussions about various topics. It is divided into smaller communities, so-called subreddits, which have been shown to be a rich source of derivationally complex words (Hofmann et al., 2020c). Hofmann et al. (2020a) have published a dataset of derivatives found on Reddit annotated with the subreddits in which they occur.8 Inspired by a content-based subreddit categorization scheme,9 we define two groups of subreddits, an entertainment set (ent) consisting of the subreddits anime, DestinyTheGame, funny, Games, gaming, leagueoflegends, movies, Music, pics, and videos, as well as a discussion set (dis) consisting of the subred-\n8https://github.com/valentinhofmann/ dagobert\n9https://www.reddit.com/r/ TheoryOfReddit/comments/1f7hqc/the_200_ most_active_subreddits_categorized_by\ndits askscience, atheism, conspiracy, news, Libertarian, politics, science, technology, TwoXChromosomes, and worldnews, and extract all derivationally complex words occurring in them. We again expect large topical distances for these classes.\nGiven that the automatic creation of the datasets necessarily introduces noise, we measure human performance on 100 randomly sampled words per dataset, which ranges between 71% (Amazon) and 78% (ArXiv). These values can thus be seen as an upper bound on performance."
    }, {
      "heading" : "3.3 Models",
      "text" : "We train two main models on each binary classification task: BERT with the standard WordPiece segmentation (sw) and BERT using the derivational segmentation (sd), a model that we refer to as DelBERT (Derivation leveraging BERT). BERT and DelBERT are identical except for the way in which they use the vocabulary of input tokens (but the vocabulary itself is also identical for both models).\nThe specific BERT variant we use is BERTBASE (uncased) (Devlin et al., 2019). For the derivational segmentation, we follow previous work by Hofmann et al. (2020a) in separating stem and prefixes by a hyphen. We further follow Casanueva et al. (2020) and Vulić et al. (2020) in mean-pooling the output representations for all subwords, excluding BERT’s special tokens. The mean-pooled representation is then fed into a two-layer feed-forward network for classification. To examine the relative importance of different types of morphological units, we train two additional models in which we ablate information about stems and affixes, i.e., we represent stems and affixes by the same randomly chosen input embedding.10\nWe finetune BERT, DelBERT, and the two ablated models on the three datasets using 20 different random seeds. We choose F1 as the evaluation measure. See Appendix A.3 for details about implementation and hyperparameters."
    }, {
      "heading" : "3.4 Results",
      "text" : "DelBERT (sd) outperforms BERT (sw) by a large margin on all three datasets (Table 2). It is interesting to notice that the performance difference is larger for ArXiv and Reddit than for Amazon, indicating that the gains in representational quality are particularly large for topicality.\nWhat is it that leads to DelBERT’s increased performance? The ablation study shows that models using only stem information already achieve relatively high performance and are on par or even better than the BERT models on ArXiv and Reddit. However, the DelBERT models still perform substantially better than the stem models on all three datasets. The gap is particularly pronounced\n10For affix ablation, we use two different input embeddings for prefixes and suffixes.\nfor Amazon, which indicates that the interaction between the meaning of stem and affixes is more complex for sentiment than for topicality. This makes sense from a linguistic point of view: while stems tend to be good cues for the topical associations of a complex word, sentiment often depends on semantic interactions between stems and affixes. For example, while the prefix un turns the sentiment of amusing negative, it turns the sentiment of biased positive. Such effects involving negation and antonymy are known to be challenging for PLMs (Ettinger, 2020; Kassner and Schütze, 2020) and might be one of the reasons for the generally lower performance on Amazon.11 The performance of models using only affixes is much lower."
    }, {
      "heading" : "3.5 Quantitative Analysis",
      "text" : "To further examine how BERT (sw) and DelBERT (sd) differ in the way they infer the meaning of complex words, we perform a convergence analysis. We find that the DelBERT models reach their peak in performance faster than the BERT models (Figure 2). This is in line with our interpretation of PLMs as serial dual-route models (see Section 2.2): while DelBERT operates on morphological units and can combine the subword meanings to infer the meanings of complex words, BERT’s subwords do not necessarily carry lexical meanings, and hence the derivational patterns need to be stored by adapting the model weights. This is an additional burden, leading to longer convergence times and substantially worse overall performance.\nOur hypothesis that PLMs can use two routes\n11Another reason for the lower performance on sentiment is that the datasets were created automatically (see Section 3.2), and hence many complex words do not directly carry information about sentiment or topicality. The density of such words is higher for sentiment than topicality since the topic of discussion affects the likelihoods of most content words.\nto process complex words (storage in weights and compositional computation based on input embeddings), and that the second route is blocked when the input segmentation is not morphological, suggests the existence of frequency effects: BERT might have seen frequent complex words multiple times during pretraining and stored their meaning in the model weights. This is less likely for infrequent complex words, making the capability to compositionally infer the meaning (i.e., the computation route) more important. We therefore expect the difference in performance between DelBERT (which should have an advantage on the computation route) and BERT to be larger for infrequent words. To test this hypothesis, we split the complex words of each dataset into three bins of low (f ≤ 5), mid (5 < f ≤ 500), and high (f > 500) absolute frequencies, and analyze how the performance of BERT and DelBERT differs on the three bins. For this and all subsequent analyses, we merge development and test sets and use accuracy instead of F1 since it makes comparisons across small sets of data points more interpretable. The results are in line with our hypothesis (Figure 3): BERT performs worse than DelBERT on complex words of low and mid frequencies but achieves very similar (ArXiv, Reddit) or even better (Amazon) accuracies on high-frequency complex words. These results strongly suggest that two different mechanisms are involved, and that BERT has a disadvantage for complex words that do not have a high frequency. At the same time, the slight advantage of BERT on high-frequency complex words indicates that it has high-quality representations of these words in its weights, which DelBERT cannot exploit since it uses a different segmentation. We are further interested to see whether the affix type has an impact on the relative performance of BERT and DelBERT. To examine this question, we measure the accuracy increase of DelBERT as compared to BERT for individual affixes, averaged across datasets and random seeds. We find that the increase is almost twice as large for prefixes (µ = .023, σ = .017) than for suffixes (µ = .013, σ = .016), a difference that is shown to be significant by a two-tailed Welch’s t-test (d = .642, t(82.97) = 2.94, p < .01).12 Why is having access to the correct morphological segmentation more advantageous for prefixed than suffixed complex words? We argue that there are two key factors at play. First, the WordPiece tokenization sometimes generates the morphologically correct segmenta-\n12We use a Welch’s instead of Student’s t-test since it does not assume that the distributions have equal variance.\ntion, but it does so with different frequencies for prefixes and suffixes. To detect morphologically incorrect segmentations, we check whether the WordPiece segmentation keeps the stem intact, which is in line with our definition of morphological validity (Section 2.2) and provides a conservative estimate of the error rate. For prefixes, the WordPiece tokenization is seldom correct (average error rate: µ = .903, σ = .042), whereas for suffixes it is correct about half the time (µ = .503, σ = .213). Hence, DelBERT gains a greater advantage for prefixed words. Second, prefixes and suffixes have different linguistic properties that affect the prediction task in unequal ways. Specifically, whereas suffixes have both syntactic and semantic functions, prefixes have an exclusively semantic function and always add lexical-semantic meaning to the stem (Giraudo and Grainger, 2003; Beyersmann et al., 2015). As a result, cases such as unamusing where the affix boundary is a decisive factor for the prediction task are more likely to occur with prefixes than suffixes, thus increasing the importance of a morphologically correct segmentation.13\nGiven the differences between sentiment and topicality prediction, we expect variations in the relative importance of the two identified factors: (i) in the case of sentiment the advantage of sd should be maximal for affixes directly affecting sentiment; (ii) in the case of topicality its advantage should be the larger the higher the proportion of incorrect segmentations for a particular affix, and hence the more frequent the cases where DelBERT has access to the stem while BERT does not. To test this hypothesis, we focus on pre-\n13Notice that there are suffixes with similar semantic effects (e.g., less), but they are less numerous.\ndictions for prefixed complex words. For each dataset, we measure for individual prefixes the accuracy increase of the DelBERT models as compared to the BERT models, averaged across random seeds, as well as the proportion of morphologically incorrect segmentations produced by WordPiece. We then calculate linear regressions to predict the accuracy increases based on the proportions of incorrect segmentations. This analysis shows a significant positive correlation for ArXiv (R2 = .304, F (1, 41) = 17.92, p < 0.001) and Reddit (R2 = .270, F (1, 40) = 14.80, p < 0.001) but not for Amazon (R2 = .019, F (1, 41) = .80, p = .375), which is in line with our expectations (Figure 4a). Furthermore, ranking the prefixes by accuracy increase for Amazon confirms that the most pronounced differences are found for prefixes that can change the sentiment such as non, anti, mal, and pseudo (Figure 4b)."
    }, {
      "heading" : "3.6 Qualitative Analysis",
      "text" : "Besides quantitative factors, we are interested in identifying qualitative contexts in which DelBERT has a particular advantage compared to BERT. To do so, we filter the datasets for complex words that are consistently classified correctly by DelBERT and incorrectly by BERT. Specifically, we compute for each word the average likelihood of the true semantic class across DelBERT and BERT models, respectively, and rank words according to the likelihood difference between both model types. Examining the words with the most extreme differences, we observe three classes (Table 3).\nFirst, the addition of a suffix is often connected with morpho-orthographic changes (e.g., the deletion of a stem-final e), which leads to a segmentation of the stem into several subwords\nsince the truncated stem is not in the WordPiece vocabulary (applausive, isotopize, prematuration). The model does not seem to be able to recover the meaning of the stem from the subwords. Second, the addition of a prefix has the effect that the word-internal (as opposed to word-initial) form of the stem would have to be available for proper segmentation. Since this form rarely exists in the WordPiece vocabulary, the stem is segmented into several subwords (superannoying, antimicrosoft, nonmultiplayer). Again, it does not seem to be possible for the model to recover the meaning of the stem. Third, the segmentation of prefixed complex words often fuses the prefix with the first characters of the stem (overseasoned, inkinetic, promosque). This case is particularly detrimental since it not only makes it difficult to recover the meaning of the stem but also creates associations with unrelated meanings, sometimes even opposite meanings as in the case of superbizarre. The three classes thus underscore the difficulty of inferring the meaning of complex words from the subwords when the wholeword meaning is not stored in the model weights and the subwords are not morphological."
    }, {
      "heading" : "4 Related Work",
      "text" : "Several recent studies have examined how the performance of PLMs is affected by their input segmentation. Tan et al. (2020) show that tokenizing inflected words into stems and inflection symbols allows BERT to generalize better on non-standard inflections. Bostrom and Durrett (2020) pretrain RoBERTa with different tokenization methods and find tokenizations that align more closely with morphology to perform better on a number of tasks. Ma et al. (2020) show that providing BERT with character-level information also leads to enhanced performance. Relatedly, studies from automatic speech recognition have demonstrated that morphological decomposition improves the perplexity of language models (Fang et al., 2015; Jain et al., 2020). Whereas these studies change the vocabulary of input tokens (e.g., by adding special tokens), we show that even when keeping the pretrained vocabulary fixed, employing it in a morphologically correct way leads to better performance.14\n14There are also studies that analyze morphological aspects of PLMs without a focus on questions surrounding segmentation (Edmiston, 2020; Klemen et al., 2020).\nMost NLP studies on derivational morphology have been devoted to the question of how semantic representations of derivationally complex words can be enhanced by including morphological information (Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Bhatia et al., 2016; Cotterell and Schütze, 2018), and how affix embeddings can be computed (Lazaridou et al., 2013; Kisselew et al., 2015; Padó et al., 2016). Cotterell et al. (2017), Vylomova et al. (2017), and Deutsch et al. (2018) propose sequence-to-sequence models for the generation of derivationally complex words. Hofmann et al. (2020a) address the same task using BERT. In contrast, we analyze how different input segmentations affect the semantic representations of derivationally complex words in PLMs, a question that has not been addressed before."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We have examined how the input segmentation of PLMs, specifically BERT, affects their interpretations of derivationally complex words. Drawing upon insights from psycholinguistics, we have deduced a conceptual interpretation of PLMs as serial dual-route models, which implies that maximally meaningful input tokens should allow for the best generalization on new words. This hypothesis was confirmed by a series of semantic probing tasks on which DelBERT, a model using derivational segmentation, consistently outperformed BERT using WordPiece segmentation. Quantitative and qualitative analyses further showed that BERT’s inferior performance was caused by its inability to infer the complex-word meaning as a function of the subwords when the complex-word meaning was not stored in the weights. Overall, our findings suggest that the generalization capabilities of PLMs could be further improved if a morphologically-informed vocabulary of input tokens were used."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was funded by the European Research Council (#740516) and the Engineering and Physical Sciences Research Council (EP/T023333/1). The first author was also supported by the German Academic Scholarship Foundation and the Arts and Humanities Research Council. We thank the reviewers for their helpful comments."
    }, {
      "heading" : "A Appendices",
      "text" : "A.1 Derivational Segmentation\nLet A be a set of derivational affixes and S a set of stems. To determine the derivational segmentation of a word w, we employ an iterative algorithm. Define the set BA1 of w as the words that remain when one derivational affix from A is removed from w. For example, unlockable can be segmented into un, lockable and unlock, able so BA1 (unlockable) = {lockable,unlock} (we assume that un and able are inA). We then iteratively create BAi+1(w) = ⋃ b∈BAi (w)\nBA1 (b), i.e., we iteratively remove affixes from w. We stop as soon as BAi+1(w) ∩ S 6= ∅. The element in this intersection, together with the used affixes from A, forms the derivational segmentation of w.15 If there is no i such thatBAi+1(w)∩S 6= ∅, w does not have a derivational segmentation. The algorithm is sensitive to most morpho-orthographic rules of English (Plag, 2003), e.g., when the suffix ize is removed from isotopize, the resulting word is isotope, not isotop.\nIn this paper, we follow Hofmann et al. (2020a) in using BERT’s prefixes, suffixes, and stems as input to the algorithm. Specifically, we assign 46 productive prefixes and 44 productive suffixes in BERT’s vocabulary to A and all fully alphabetic words with more than 3 characters in BERT’s vocabulary (excluding stopwords and affixes) to S, resulting in a total of 20,259 stems. This means that we only consider derivational segmentations that are possible given BERT’s vocabulary.\n15If |BAi+1(w) ∩ S| > 1 (rarely the case in practice), the element with the lowest number of suffixes is chosen.\nA.2 Data Preprocessing We exclude texts written in a language other than English and remove strings containing numbers as well as hyperlinks. We follow Han and Baldwin (2011) in reducing repetitions of more than three letters (niiiiice) to three letters.\nA.3 Hyperparameters The feed-forward network has a ReLU activation after the first layer and a sigmoid activation after the second layer. The first layer has 100 dimensions. We apply dropout of 0.2 after the first layer. All other hyperparameters are as for BERTBASE (uncased) (Devlin et al., 2019). The number of trainable parameters is 109,559,241.\nWe use a batch size of 64 and perform grid search for the number of epochs n ∈ {1, . . . , 20} and the learning rate l ∈ {1× 10−6, 3× 10−6, 1× 10−5, 3× 10−5} (selection criterion: F1 score). We tune l on Reddit (80 hyperparameter search trials per model type) and use the best configuration (which is identical for all model types) for 20 training runs with different random seeds on all three datasets (20 hyperparameter search trials per model type, dataset, and random seed). Models are trained with binary cross-entropy as the loss function and Adam (Kingma and Ba, 2015) as the optimizer. Experiments are performed on a GeForce GTX 1080 Ti GPU (11GB).\nTable 4 lists statistics of the validation performance over hyperparameter search trials and provides information about best hyperparameter configurations as well as runtimes.16 See also Section 3.5 and particularly Figure 2 in the main text, where we present a detailed analysis of the convergence behavior of the two main model types examined in this study (DelBERT and BERT).\n16Since expected validation performance (Dodge et al., 2019) may not be correct for grid search, we report mean and standard deviation of the performance instead."
    } ],
    "references" : [ {
      "title" : "Overview of character-based models for natural language processing",
      "author" : [ "Heike Adel", "Ehsaneddin Asgari", "Hinrich Schütze." ],
      "venue" : "International Conference on Computational Linguistics and Intelligent Text Processing (CICLing) 18.",
      "citeRegEx" : "Adel et al\\.,? 2017",
      "shortCiteRegEx" : "Adel et al\\.",
      "year" : 2017
    }, {
      "title" : "Frequency effects and the representational status of regular inflections",
      "author" : [ "Maria Alegre", "Peter Gordon." ],
      "venue" : "Journal of Memory and Language, 40:41–61.",
      "citeRegEx" : "Alegre and Gordon.,? 1999",
      "shortCiteRegEx" : "Alegre and Gordon.",
      "year" : 1999
    }, {
      "title" : "Probabilistic fasttext for multi-sense word embeddings",
      "author" : [ "Ben Athiwaratkun", "Andrew Wilson", "Anima Anandkumar." ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics (ACL) 56.",
      "citeRegEx" : "Athiwaratkun et al\\.,? 2018",
      "shortCiteRegEx" : "Athiwaratkun et al\\.",
      "year" : 2018
    }, {
      "title" : "Singulars and plurals in Dutch: Evidence for a parallel dual-route model",
      "author" : [ "R. Harald Baayen", "Ton Dijkstra", "Robert Schreuder." ],
      "venue" : "Journal of Memory and Language, 37:94–117.",
      "citeRegEx" : "Baayen et al\\.,? 1997",
      "shortCiteRegEx" : "Baayen et al\\.",
      "year" : 1997
    }, {
      "title" : "Productivity and English derivation: A corpus-based study",
      "author" : [ "R. Harald Baayen", "Rochelle Lieber." ],
      "venue" : "Linguistics, 29(5).",
      "citeRegEx" : "Baayen and Lieber.,? 1991",
      "shortCiteRegEx" : "Baayen and Lieber.",
      "year" : 1991
    }, {
      "title" : "Morphology in the mental lexicon: A computational model for visual word recognition",
      "author" : [ "R. Harald Baayen", "Robert Schreuder", "Richard Sproat." ],
      "venue" : "Frank van Eynde and Dafydd Gibbon, editors, Lexicon development for speech and language process-",
      "citeRegEx" : "Baayen et al\\.,? 2000",
      "shortCiteRegEx" : "Baayen et al\\.",
      "year" : 2000
    }, {
      "title" : "Affixal homonymy triggers full-form storage, even with inflected words, even in a morphologically rich language",
      "author" : [ "Raymond Bertram", "Matti Laine", "R. Harald Baayen", "Robert Schreuder", "Jukka Hyönä." ],
      "venue" : "Cognition, 74:B13–B25.",
      "citeRegEx" : "Bertram et al\\.,? 2000a",
      "shortCiteRegEx" : "Bertram et al\\.",
      "year" : 2000
    }, {
      "title" : "The balance of storage and computation in morphological processing: The role of word formation type, affixal homonymy, and productivity",
      "author" : [ "Raymond Bertram", "Robert Schreuder", "R. Harald Baayen." ],
      "venue" : "Journal of Experimental Psychology: Learn-",
      "citeRegEx" : "Bertram et al\\.,? 2000b",
      "shortCiteRegEx" : "Bertram et al\\.",
      "year" : 2000
    }, {
      "title" : "Differences in the processing of prefixes and suffixes revealed by a letter-search task",
      "author" : [ "Elisabeth Beyersmann", "Johannes C. Ziegler", "Jonathan Grainger." ],
      "venue" : "Scientific Studies of Reading, 19(5):360–373.",
      "citeRegEx" : "Beyersmann et al\\.,? 2015",
      "shortCiteRegEx" : "Beyersmann et al\\.",
      "year" : 2015
    }, {
      "title" : "Morphological priors for probabilistic neural word embeddings",
      "author" : [ "Parminder Bhatia", "Robert Guthrie", "Jacob Eisenstein." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing (EMNLP) 2016.",
      "citeRegEx" : "Bhatia et al\\.,? 2016",
      "shortCiteRegEx" : "Bhatia et al\\.",
      "year" : 2016
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:135–146.",
      "citeRegEx" : "Bojanowski et al\\.,? 2017",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2017
    }, {
      "title" : "Byte pair encoding is suboptimal for language model pretraining",
      "author" : [ "Kaj Bostrom", "Greg Durrett." ],
      "venue" : "Findings of Empirical Methods in Natural Language Processing (EMNLP) 2020.",
      "citeRegEx" : "Bostrom and Durrett.,? 2020",
      "shortCiteRegEx" : "Bostrom and Durrett.",
      "year" : 2020
    }, {
      "title" : "Compositional morphology for word representations and language modelling",
      "author" : [ "Jan A. Botha", "Phil Blunsom." ],
      "venue" : "International Conference on Machine Learning (ICML) 31.",
      "citeRegEx" : "Botha and Blunsom.,? 2014",
      "shortCiteRegEx" : "Botha and Blunsom.",
      "year" : 2014
    }, {
      "title" : "Representation and processing of derived words",
      "author" : [ "Cristina Burani", "Alfonso Caramazza." ],
      "venue" : "Language and Cognitive Processes, 2(3-4):217–227.",
      "citeRegEx" : "Burani and Caramazza.,? 1987",
      "shortCiteRegEx" : "Burani and Caramazza.",
      "year" : 1987
    }, {
      "title" : "Units of representation for derived words in the lexicon",
      "author" : [ "Cristina Burani", "Alessandro Laudanna." ],
      "venue" : "Ram Frost and Leonard Katz, editors, Orthography, phonology, morphology, and meaning, pages 361– 376. North-Holland, Amsterdam.",
      "citeRegEx" : "Burani and Laudanna.,? 1992",
      "shortCiteRegEx" : "Burani and Laudanna.",
      "year" : 1992
    }, {
      "title" : "Lexical representation",
      "author" : [ "Brian Butterworth." ],
      "venue" : "Brian Butterworth, editor, Language production: Development, writing and other language processes, pages 257–294. Academic Press, London.",
      "citeRegEx" : "Butterworth.,? 1983",
      "shortCiteRegEx" : "Butterworth.",
      "year" : 1983
    }, {
      "title" : "Morphology as lexical organization",
      "author" : [ "Joan Bybee." ],
      "venue" : "Michael Hammond and Michael Noonan, editors, Theoretical approaches to morphology: Approaches in modern linguistics, pages 119–141. Academic Press, San Diego, CA.",
      "citeRegEx" : "Bybee.,? 1988",
      "shortCiteRegEx" : "Bybee.",
      "year" : 1988
    }, {
      "title" : "Regular morphology and the lexicon",
      "author" : [ "Joan Bybee." ],
      "venue" : "Language and Cognitive Processes, 10(425455).",
      "citeRegEx" : "Bybee.,? 1995",
      "shortCiteRegEx" : "Bybee.",
      "year" : 1995
    }, {
      "title" : "Lexical access and inflectional morphology",
      "author" : [ "Alfonso Caramazza", "Alessandro Laudanna", "Cristina Romani." ],
      "venue" : "Cognition, 28(297-332).",
      "citeRegEx" : "Caramazza et al\\.,? 1988",
      "shortCiteRegEx" : "Caramazza et al\\.",
      "year" : 1988
    }, {
      "title" : "Efficient intent detection with dual sentence encoders",
      "author" : [ "Iñigo Casanueva", "Tadas Temčinas", "Daniela Gerz", "Matthew Henderson", "Ivan Vulić." ],
      "venue" : "Workshop on Natural Language Processing for Conversational AI 2.",
      "citeRegEx" : "Casanueva et al\\.,? 2020",
      "shortCiteRegEx" : "Casanueva et al\\.",
      "year" : 2020
    }, {
      "title" : "Emerging trends: Subwords, seriously",
      "author" : [ "Kenneth Church" ],
      "venue" : "Natural Language Engineering,",
      "citeRegEx" : "Church.,? \\Q2020\\E",
      "shortCiteRegEx" : "Church.",
      "year" : 2020
    }, {
      "title" : "ELECTRA: Pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "International Conference on Learning Representations (ICLR) 8.",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "Joint semantic synthesis and morphological analysis of the derived word",
      "author" : [ "Ryan Cotterell", "Hinrich Schütze." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 6:33–48.",
      "citeRegEx" : "Cotterell and Schütze.,? 2018",
      "shortCiteRegEx" : "Cotterell and Schütze.",
      "year" : 2018
    }, {
      "title" : "Paradigm completion for derivational morphology",
      "author" : [ "Ryan Cotterell", "Ekaterina Vylomova", "Huda Khayrallah", "Christo Kirov", "David Yarowsky." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing (EMNLP) 2017.",
      "citeRegEx" : "Cotterell et al\\.,? 2017",
      "shortCiteRegEx" : "Cotterell et al\\.",
      "year" : 2017
    }, {
      "title" : "Indexing by latent semantic analysis",
      "author" : [ "Scott Deerwester", "Susan T. Dumais", "George Furnas", "Thomas Landauer", "Richard Harshman." ],
      "venue" : "Journal of the American Society for Information Science, 41(6):391–407.",
      "citeRegEx" : "Deerwester et al\\.,? 1990",
      "shortCiteRegEx" : "Deerwester et al\\.",
      "year" : 1990
    }, {
      "title" : "A distributional and orthographic aggregation model for English derivational morphology",
      "author" : [ "Daniel Deutsch", "John Hewitt", "Dan Roth." ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics (ACL) 56.",
      "citeRegEx" : "Deutsch et al\\.,? 2018",
      "shortCiteRegEx" : "Deutsch et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Annual Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Show your work: Improved reporting of experimental results",
      "author" : [ "Jesse Dodge", "Suchin Gururangan", "Dallas Card", "Roy Schwartz", "Noah A. Smith." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing (EMNLP) 2019.",
      "citeRegEx" : "Dodge et al\\.,? 2019",
      "shortCiteRegEx" : "Dodge et al\\.",
      "year" : 2019
    }, {
      "title" : "A systematic analysis of morphological content in BERT models for multiple languages",
      "author" : [ "Daniel Edmiston." ],
      "venue" : "arXiv 2004.03032.",
      "citeRegEx" : "Edmiston.,? 2020",
      "shortCiteRegEx" : "Edmiston.",
      "year" : 2020
    }, {
      "title" : "How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings",
      "author" : [ "Kawin Ethayarajh." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing (EMNLP) 2019.",
      "citeRegEx" : "Ethayarajh.,? 2019",
      "shortCiteRegEx" : "Ethayarajh.",
      "year" : 2019
    }, {
      "title" : "What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models",
      "author" : [ "Allyson Ettinger." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:34–48.",
      "citeRegEx" : "Ettinger.,? 2020",
      "shortCiteRegEx" : "Ettinger.",
      "year" : 2020
    }, {
      "title" : "Exponential language modeling using morphological features and multitask learning",
      "author" : [ "Hao Fang", "Mari Ostendorf", "Peter Baumann", "Janet B. Pierrehumbert." ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 23(12):2410–",
      "citeRegEx" : "Fang et al\\.,? 2015",
      "shortCiteRegEx" : "Fang et al\\.",
      "year" : 2015
    }, {
      "title" : "The inflected noun system in Serbo-Croatian: Lexical representation of morphological structure",
      "author" : [ "Laurie B. Feldman", "Carol A. Fowler." ],
      "venue" : "Memory and Cognition, 15(1):1–12.",
      "citeRegEx" : "Feldman and Fowler.,? 1987",
      "shortCiteRegEx" : "Feldman and Fowler.",
      "year" : 1987
    }, {
      "title" : "Constraining psycholinguistic models of morphological processing and representation: The role of productivity",
      "author" : [ "Uli H. Frauenfelder", "Robert Schreuder." ],
      "venue" : "Geert Booij and Jaap van Marle, editors, Yearbook of morphology 1991, volume 26, pages 165–",
      "citeRegEx" : "Frauenfelder and Schreuder.,? 1992",
      "shortCiteRegEx" : "Frauenfelder and Schreuder.",
      "year" : 1992
    }, {
      "title" : "A new algorithm for data compression",
      "author" : [ "Philip Gage." ],
      "venue" : "The C Users Journal, 12(2):23–38.",
      "citeRegEx" : "Gage.,? 1994",
      "shortCiteRegEx" : "Gage.",
      "year" : 1994
    }, {
      "title" : "On the role of derivational affixes in recognizing complex words: Evidence from masked priming",
      "author" : [ "Hélène Giraudo", "Jonathan Grainger." ],
      "venue" : "R. Harald Baayen and Robert Schreuder, editors, Morphological structure in language processing, pages 209–",
      "citeRegEx" : "Giraudo and Grainger.,? 2003",
      "shortCiteRegEx" : "Giraudo and Grainger.",
      "year" : 2003
    }, {
      "title" : "Delineating derivation and inflection",
      "author" : [ "Pius ten Hacken." ],
      "venue" : "Rochelle Lieber and Pavol Štekauer, editors, The Oxford handbook of derivational morphology, pages 10–25. Oxford University Press, Oxford.",
      "citeRegEx" : "Hacken.,? 2014",
      "shortCiteRegEx" : "Hacken.",
      "year" : 2014
    }, {
      "title" : "Lexical normalisation of short text messages: Makn sens a #twitter",
      "author" : [ "Bo Han", "Timothy Baldwin." ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics (ACL) 49.",
      "citeRegEx" : "Han and Baldwin.,? 2011",
      "shortCiteRegEx" : "Han and Baldwin.",
      "year" : 2011
    }, {
      "title" : "Understanding morphology",
      "author" : [ "Martin Haspelmath", "Andrea D. Sims." ],
      "venue" : "Routledge, New York, NY.",
      "citeRegEx" : "Haspelmath and Sims.,? 2010",
      "shortCiteRegEx" : "Haspelmath and Sims.",
      "year" : 2010
    }, {
      "title" : "Sequence tagging with contextual and non-contextual subword representations: A multilingual evaluation",
      "author" : [ "Benjamin Heinzerling", "Michael Strube." ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics (ACL) 57.",
      "citeRegEx" : "Heinzerling and Strube.,? 2019",
      "shortCiteRegEx" : "Heinzerling and Strube.",
      "year" : 2019
    }, {
      "title" : "A structural probe for finding syntax in word representations",
      "author" : [ "John Hewitt", "Christopher D. Manning." ],
      "venue" : "Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "citeRegEx" : "Hewitt and Manning.,? 2019",
      "shortCiteRegEx" : "Hewitt and Manning.",
      "year" : 2019
    }, {
      "title" : "DagoBERT: Generating derivational morphology with a pretrained language model",
      "author" : [ "Valentin Hofmann", "Janet B. Pierrehumbert", "Hinrich Schütze." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing (EMNLP) 2020.",
      "citeRegEx" : "Hofmann et al\\.,? 2020a",
      "shortCiteRegEx" : "Hofmann et al\\.",
      "year" : 2020
    }, {
      "title" : "Predicting the growth of morphological families from social and linguistic factors",
      "author" : [ "Valentin Hofmann", "Janet B. Pierrehumbert", "Hinrich Schütze." ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics (ACL) 58.",
      "citeRegEx" : "Hofmann et al\\.,? 2020b",
      "shortCiteRegEx" : "Hofmann et al\\.",
      "year" : 2020
    }, {
      "title" : "A graph auto-encoder model of derivational morphology",
      "author" : [ "Valentin Hofmann", "Hinrich Schütze", "Janet B. Pierrehumbert." ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics (ACL)",
      "citeRegEx" : "Hofmann et al\\.,? 2020c",
      "shortCiteRegEx" : "Hofmann et al\\.",
      "year" : 2020
    }, {
      "title" : "Finnish ASR with deep transformer models",
      "author" : [ "Abhilash Jain", "Aku Rouhe", "Stig-Arne Grönroos", "Mikko Kurimo." ],
      "venue" : "Conference of the International Speech Communication Association (INTERSPEECH) 21.",
      "citeRegEx" : "Jain et al\\.,? 2020",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2020
    }, {
      "title" : "What does BERT learn about the structure of language? In Annual Meeting of the Association for Computational Linguistics (ACL) 57",
      "author" : [ "Ganesh Jawahar", "Benoit Sagot", "Djamé Seddah" ],
      "venue" : null,
      "citeRegEx" : "Jawahar et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Jawahar et al\\.",
      "year" : 2019
    }, {
      "title" : "A query log analysis of dataset search",
      "author" : [ "Emilia Kacprzak", "Laura M. Koesten", "Luis-Daniel Ibáñez", "Elena Simperl", "Jeni Tennison." ],
      "venue" : "International Conference on Web Engineering (ICWE) 17.",
      "citeRegEx" : "Kacprzak et al\\.,? 2017",
      "shortCiteRegEx" : "Kacprzak et al\\.",
      "year" : 2017
    }, {
      "title" : "Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly",
      "author" : [ "Nora Kassner", "Hinrich Schütze." ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics (ACL)",
      "citeRegEx" : "Kassner and Schütze.,? 2020",
      "shortCiteRegEx" : "Kassner and Schütze.",
      "year" : 2020
    }, {
      "title" : "Character-aware neural language models",
      "author" : [ "Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush." ],
      "venue" : "Conference on Artificial Intelligence (AAAI) 30.",
      "citeRegEx" : "Kim et al\\.,? 2016",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy L. Ba." ],
      "venue" : "International Conference on Learning Representations (ICLR) 3.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Obtaining a better understanding of distributional models of german derivational morphology",
      "author" : [ "Max Kisselew", "Sebastian Padó", "Alexis Palmer", "Jan Šnajder." ],
      "venue" : "International Conference on Computational Semantics (IWCS) 11.",
      "citeRegEx" : "Kisselew et al\\.,? 2015",
      "shortCiteRegEx" : "Kisselew et al\\.",
      "year" : 2015
    }, {
      "title" : "Enhancing deep neural networks with morphological information",
      "author" : [ "Matej Klemen", "Luka Krsnik", "Marko RobnikŠikonja." ],
      "venue" : "arXiv 2011.12432.",
      "citeRegEx" : "Klemen et al\\.,? 2020",
      "shortCiteRegEx" : "Klemen et al\\.",
      "year" : 2020
    }, {
      "title" : "Morphological dynamics in compound processing",
      "author" : [ "Victor Kuperman", "Raymond Bertram", "R. Harald Baayen." ],
      "venue" : "Language and Cognitive Processes, 23(7-8):1089–1132.",
      "citeRegEx" : "Kuperman et al\\.,? 2008",
      "shortCiteRegEx" : "Kuperman et al\\.",
      "year" : 2008
    }, {
      "title" : "Reading of polymorphemic Dutch compounds: Towards a multiple route model of lexical processing",
      "author" : [ "Victor Kuperman", "Robert Schreuder", "Raymond Bertram", "R. Harald Baayen." ],
      "venue" : "Journal of Experimental Psychology: Human Perception",
      "citeRegEx" : "Kuperman et al\\.,? 2009",
      "shortCiteRegEx" : "Kuperman et al\\.",
      "year" : 2009
    }, {
      "title" : "Address mechanisms to decomposed lexical entries",
      "author" : [ "Alessandro Laudanna", "Cristina Burani." ],
      "venue" : "Linguistics, 23(5).",
      "citeRegEx" : "Laudanna and Burani.,? 1985",
      "shortCiteRegEx" : "Laudanna and Burani.",
      "year" : 1985
    }, {
      "title" : "Distributional properties of derivational affixes: Implications for processing",
      "author" : [ "Alessandro Laudanna", "Cristina Burani." ],
      "venue" : "Laurie B. Feldman, editor, Morphological aspects of language processing, pages 345–364. Lawrence Erlbaum, Hillsdale, NJ.",
      "citeRegEx" : "Laudanna and Burani.,? 1995",
      "shortCiteRegEx" : "Laudanna and Burani.",
      "year" : 1995
    }, {
      "title" : "Compositional-ly derived representations of morphologically complex words in distributional semantics",
      "author" : [ "Angeliki Lazaridou", "Marco Marelli", "Roberto Zamparelli", "Marco Baroni." ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Lazaridou et al\\.,? 2013",
      "shortCiteRegEx" : "Lazaridou et al\\.",
      "year" : 2013
    }, {
      "title" : "Morphological processing in the brain: The good (inflection), the bad (derivation) and the ugly (compounding)",
      "author" : [ "Alina Leminen", "Eva Smolka", "Jon Duñabeitia", "Christos Pliatsikas." ],
      "venue" : "Cortex, 116:4–44.",
      "citeRegEx" : "Leminen et al\\.,? 2019",
      "shortCiteRegEx" : "Leminen et al\\.",
      "year" : 2019
    }, {
      "title" : "Better word representations with recursive neural networks for morphology",
      "author" : [ "Minh-Thang Luong", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Conference on Computational Natural Language Learning (CoNLL) 17.",
      "citeRegEx" : "Luong et al\\.,? 2013",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2013
    }, {
      "title" : "CharBERT: Character-aware pre-trained language model",
      "author" : [ "Wentao Ma", "Yiming Cui", "Chenglei Si", "Ting Liu", "Shijin Wang", "Guoping Hu." ],
      "venue" : "International Conference on Computational Linguistics (COLING) 28.",
      "citeRegEx" : "Ma et al\\.,? 2020",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2020
    }, {
      "title" : "The processing of affixed words",
      "author" : [ "Leon Manelis", "David A. Tharp." ],
      "venue" : "Memory and Cognition, 5(6):690–695.",
      "citeRegEx" : "Manelis and Tharp.,? 1977",
      "shortCiteRegEx" : "Manelis and Tharp.",
      "year" : 1977
    }, {
      "title" : "CamemBERT: A tasty French language model",
      "author" : [ "Louis Martin", "Benjamin Muller", "Pedro J. Suárez", "Yoann Dupont", "Laurent Romary", "de la Clergerie", "Éric V", "Djamé Seddah", "Benoit Sagot" ],
      "venue" : "In Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Martin et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Martin et al\\.",
      "year" : 2020
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "arXiv 1301.3781.",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS) 26.",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distant supervision for relation extraction without labeled data",
      "author" : [ "Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky." ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics (ACL)",
      "citeRegEx" : "Mintz et al\\.,? 2009",
      "shortCiteRegEx" : "Mintz et al\\.",
      "year" : 2009
    }, {
      "title" : "Gendered associations of english morphology",
      "author" : [ "Jeremy M. Needle", "Janet B. Pierrehumbert." ],
      "venue" : "Journal of the Association for Laboratory Phonology, 9(1):119.",
      "citeRegEx" : "Needle and Pierrehumbert.,? 2018",
      "shortCiteRegEx" : "Needle and Pierrehumbert.",
      "year" : 2018
    }, {
      "title" : "The processing of singular and plural nouns in french and english",
      "author" : [ "Boris New", "Marc Brysbaert", "Juan Segui", "Ludovic Ferrand", "Kathleen Rastle." ],
      "venue" : "Journal of Memory and Language, 51(4):568–585.",
      "citeRegEx" : "New et al\\.,? 2004",
      "shortCiteRegEx" : "New et al\\.",
      "year" : 2004
    }, {
      "title" : "Justifying recommendations using distantly-labeled reviews and fined-grained aspects",
      "author" : [ "Jianmo Ni", "Jiacheng Li", "Julian McAuley." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing (EMNLP) 2019.",
      "citeRegEx" : "Ni et al\\.,? 2019",
      "shortCiteRegEx" : "Ni et al\\.",
      "year" : 2019
    }, {
      "title" : "Predictability of distributional semantics in derivational word formation",
      "author" : [ "Sebastian Padó", "Aurélie Herbelot", "Max Kisselew", "Jan Šnajder." ],
      "venue" : "International Conference on Computational Linguistics (COLING) 26.",
      "citeRegEx" : "Padó et al\\.,? 2016",
      "shortCiteRegEx" : "Padó et al\\.",
      "year" : 2016
    }, {
      "title" : "GloVe: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing (EMNLP) 2014.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Will it unblend? In Findings of Empirical Methods in Natural Language Processing (EMNLP) 2020",
      "author" : [ "Yuval Pinter", "Cassandra L. Jacobs", "Jacob Eisenstein" ],
      "venue" : null,
      "citeRegEx" : "Pinter et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Pinter et al\\.",
      "year" : 2020
    }, {
      "title" : "Word-formation in English",
      "author" : [ "Ingo Plag." ],
      "venue" : "Cambridge University Press, Cambridge, UK.",
      "citeRegEx" : "Plag.,? 2003",
      "shortCiteRegEx" : "Plag.",
      "year" : 2003
    }, {
      "title" : "Co-learning of word representations and morpheme representations",
      "author" : [ "Siyu Qiu", "Qing Cui", "Jiang Bian", "Bin Gao", "Tie-Yan Liu." ],
      "venue" : "International Conference on Computational Linguistics (COLING) 25.",
      "citeRegEx" : "Qiu et al\\.,? 2014",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2014
    }, {
      "title" : "Morphological convergence as on-line lexical analogy",
      "author" : [ "Péter Rácz", "Clay Beckner", "Jennifer Hay", "Janet B. Pierrehumbert." ],
      "venue" : "Language, 96(4):735– 770.",
      "citeRegEx" : "Rácz et al\\.,? 2020",
      "shortCiteRegEx" : "Rácz et al\\.",
      "year" : 2020
    }, {
      "title" : "Morphological emergence",
      "author" : [ "Péter Rácz", "Janet B. Pierrehumbert", "Jennifer Hay", "Viktória Papp." ],
      "venue" : "Brian MacWhinney and William O’Grady, editors, The handbook of language emergence, pages 123– 146. Wiley, Hoboken, NJ.",
      "citeRegEx" : "Rácz et al\\.,? 2015",
      "shortCiteRegEx" : "Rácz et al\\.",
      "year" : 2015
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Morphological decomposition based on the analysis of orthography",
      "author" : [ "Kathleen Rastle", "Matthew H. Davis." ],
      "venue" : "Language and Cognitive Processes, 23(7-8):942–971.",
      "citeRegEx" : "Rastle and Davis.,? 2008",
      "shortCiteRegEx" : "Rastle and Davis.",
      "year" : 2008
    }, {
      "title" : "The broth in my brother’s brothel: Morpho-orthographic segmentation in visual word recognition",
      "author" : [ "Kathleen Rastle", "Matthew H. Davis", "Boris New." ],
      "venue" : "Psychonomic Bulletin and Review, 11(6):1090–1098.",
      "citeRegEx" : "Rastle et al\\.,? 2004",
      "shortCiteRegEx" : "Rastle et al\\.",
      "year" : 2004
    }, {
      "title" : "Incorporating subword information into matrix factorization word embeddings",
      "author" : [ "Alexandre Salle", "Aline Villavicencio." ],
      "venue" : "Workshop on Subword/Character LEvel Models 2.",
      "citeRegEx" : "Salle and Villavicencio.,? 2018",
      "shortCiteRegEx" : "Salle and Villavicencio.",
      "year" : 2018
    }, {
      "title" : "Modeling morphological processing",
      "author" : [ "Robert Schreuder", "R. Harald Baayen." ],
      "venue" : "Laurie B. Feldman, editor, Morphological aspects of language processing, pages 131–154. Lawrence Erlbaum, Hillsdale, NJ.",
      "citeRegEx" : "Schreuder and Baayen.,? 1995",
      "shortCiteRegEx" : "Schreuder and Baayen.",
      "year" : 1995
    }, {
      "title" : "Japanese and Korean voice search",
      "author" : [ "Mike Schuster", "Kaisuke Nakajima." ],
      "venue" : "International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 37.",
      "citeRegEx" : "Schuster and Nakajima.,? 2012",
      "shortCiteRegEx" : "Schuster and Nakajima.",
      "year" : 2012
    }, {
      "title" : "Word space",
      "author" : [ "Hinrich Schütze." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS) 5, pages 895–902.",
      "citeRegEx" : "Schütze.,? 1992",
      "shortCiteRegEx" : "Schütze.",
      "year" : 1992
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics (ACL) 54.",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Tired of topic models? Clusters of pretrained word embeddings make for fast and good topics too",
      "author" : [ "Suzanna Sia", "Ayush Dalmia", "Sabrina J. Mielke" ],
      "venue" : "In Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "citeRegEx" : "Sia et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Sia et al\\.",
      "year" : 2020
    }, {
      "title" : "Rule-less morphology at the phonology-lexicon interface",
      "author" : [ "Joseph P. Stemberger." ],
      "venue" : "Susan D. Lima, Roberta Corrigan, and Gregory Iverson, editors, The reality of linguistic rules, pages 147–169. John Benjamins, Amsterdam.",
      "citeRegEx" : "Stemberger.,? 1994",
      "shortCiteRegEx" : "Stemberger.",
      "year" : 1994
    }, {
      "title" : "Rule conflation in an inferentialrealizational theory of morphotactics",
      "author" : [ "Gregory Stump." ],
      "venue" : "Acta Linguistica Academica, 64(1):79–124.",
      "citeRegEx" : "Stump.,? 2017",
      "shortCiteRegEx" : "Stump.",
      "year" : 2017
    }, {
      "title" : "Some sources of apparent gaps in derivational paradigms",
      "author" : [ "Gregory Stump." ],
      "venue" : "Morphology, 29(2):271– 292.",
      "citeRegEx" : "Stump.,? 2019",
      "shortCiteRegEx" : "Stump.",
      "year" : 2019
    }, {
      "title" : "Recognition of affixed words and the word frequency effect",
      "author" : [ "Marcus Taft." ],
      "venue" : "Memory and Cognition, 7(4):263–272.",
      "citeRegEx" : "Taft.,? 1979",
      "shortCiteRegEx" : "Taft.",
      "year" : 1979
    }, {
      "title" : "Prefix stripping revisited",
      "author" : [ "Marcus Taft." ],
      "venue" : "Journal of Verbal Learning and Verbal Behavior, 20:289– 297.",
      "citeRegEx" : "Taft.,? 1981",
      "shortCiteRegEx" : "Taft.",
      "year" : 1981
    }, {
      "title" : "A morphological-decomposition model of lexical representation",
      "author" : [ "Marcus Taft." ],
      "venue" : "Linguistics, 26:657– 667.",
      "citeRegEx" : "Taft.,? 1988",
      "shortCiteRegEx" : "Taft.",
      "year" : 1988
    }, {
      "title" : "Reading and the mental lexicon",
      "author" : [ "Marcus Taft." ],
      "venue" : "Lawrence Erlbaum, Hove, UK.",
      "citeRegEx" : "Taft.,? 1991",
      "shortCiteRegEx" : "Taft.",
      "year" : 1991
    }, {
      "title" : "Interactive-activation as a framework for understanding morphological processing",
      "author" : [ "Marcus Taft." ],
      "venue" : "Language and Cognitive Processes, 9(3):271–294.",
      "citeRegEx" : "Taft.,? 1994",
      "shortCiteRegEx" : "Taft.",
      "year" : 1994
    }, {
      "title" : "Morphological decomposition and the reverse base frequency effect",
      "author" : [ "Marcus Taft." ],
      "venue" : "The Quarterly Journal of Experimental Psychology, 57(4):745– 765.",
      "citeRegEx" : "Taft.,? 2004",
      "shortCiteRegEx" : "Taft.",
      "year" : 2004
    }, {
      "title" : "Lexical storage and retrieval of prefixed words",
      "author" : [ "Marcus Taft", "Kenneth I. Forster." ],
      "venue" : "Journal of Verbal Learning and Verbal Behavior, 14:638–647.",
      "citeRegEx" : "Taft and Forster.,? 1975",
      "shortCiteRegEx" : "Taft and Forster.",
      "year" : 1975
    }, {
      "title" : "Mind your inflections! Improving NLP for non-standard Englishes with base-inflection encoding",
      "author" : [ "Samson Tan", "Shafiq Joty", "Lav R. Varshney", "MinYen Kan." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing (EMNLP) 2020.",
      "citeRegEx" : "Tan et al\\.,? 2020",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2020
    }, {
      "title" : "Probing pretrained language models for lexical semantics",
      "author" : [ "Ivan Vulić", "Edoardo M. Ponti", "Robert Litschko", "Goran Glavaš", "Anna Korhonen." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing (EMNLP) 2020.",
      "citeRegEx" : "Vulić et al\\.,? 2020",
      "shortCiteRegEx" : "Vulić et al\\.",
      "year" : 2020
    }, {
      "title" : "Context-aware prediction of derivational word-forms",
      "author" : [ "Ekaterina Vylomova", "Ryan Cotterell", "Timothy Baldwin", "Trevor Cohn." ],
      "venue" : "Conference of the European Chapter of the Association for Computational Linguistics (EACL) 15.",
      "citeRegEx" : "Vylomova et al\\.,? 2017",
      "shortCiteRegEx" : "Vylomova et al\\.",
      "year" : 2017
    }, {
      "title" : "Google’s neural machine translation system: Bridging the gap between human",
      "author" : [ "Stevens", "George Kurian", "Nishant Patil", "Wei Wang", "Cliff Young", "Jason Smith", "Jason Riesa", "Alex Rudnick", "Oriol Vinyals", "Greg Corrado", "Macduff Hughes", "Jeffrey Dean" ],
      "venue" : null,
      "citeRegEx" : "Stevens et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Stevens et al\\.",
      "year" : 2016
    }, {
      "title" : "Overcoming language variation in sentiment analysis with social attention",
      "author" : [ "Yi Yang", "Jacob Eisenstein." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:295–307.",
      "citeRegEx" : "Yang and Eisenstein.,? 2017",
      "shortCiteRegEx" : "Yang and Eisenstein.",
      "year" : 2017
    }, {
      "title" : "XLNet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le." ],
      "venue" : "Advances in Neural Information Processing Systems (NeurIPS) 33.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "2020a) in using BERT’s prefixes, suffixes, and stems as input to the algorithm. Specifically, we assign 46 productive prefixes and 44 productive suffixes in BERT’s vocabulary to A and all fully alphabetic",
      "author" : [ "Hofmann" ],
      "venue" : null,
      "citeRegEx" : "Hofmann,? \\Q2020\\E",
      "shortCiteRegEx" : "Hofmann",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : "Pretrained language models (PLMs) such as BERT (Devlin et al., 2019), GPT-2 (Radford et al.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 75,
      "context" : ", 2019), GPT-2 (Radford et al., 2019), XLNet (Yang et al.",
      "startOffset" : 15,
      "endOffset" : 37
    }, {
      "referenceID" : 100,
      "context" : ", 2019), XLNet (Yang et al., 2019), ELECTRA (Clark et al.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 76,
      "context" : "2020), and T5 (Raffel et al., 2020) have yielded substantial improvements on a range of NLP tasks.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 40,
      "context" : "What linguistic properties do they have? Various studies have tried to illuminate this question, with a focus on syntax (Hewitt and Manning, 2019; Jawahar et al., 2019) and semantics (Ethayarajh, 2019; Ettinger, 2020; Vulić et al.",
      "startOffset" : 120,
      "endOffset" : 168
    }, {
      "referenceID" : 45,
      "context" : "What linguistic properties do they have? Various studies have tried to illuminate this question, with a focus on syntax (Hewitt and Manning, 2019; Jawahar et al., 2019) and semantics (Ethayarajh, 2019; Ettinger, 2020; Vulić et al.",
      "startOffset" : 120,
      "endOffset" : 168
    }, {
      "referenceID" : 34,
      "context" : "One common characteristic of PLMs is their input segmentation: PLMs are based on fixed-size vocabularies of words and subwords that are generated by compression algorithms such as bytepair encoding (Gage, 1994; Sennrich et al., 2016) and WordPiece (Schuster and Nakajima, 2012; Wu et al.",
      "startOffset" : 198,
      "endOffset" : 233
    }, {
      "referenceID" : 83,
      "context" : "One common characteristic of PLMs is their input segmentation: PLMs are based on fixed-size vocabularies of words and subwords that are generated by compression algorithms such as bytepair encoding (Gage, 1994; Sennrich et al., 2016) and WordPiece (Schuster and Nakajima, 2012; Wu et al.",
      "startOffset" : 198,
      "endOffset" : 233
    }, {
      "referenceID" : 20,
      "context" : "algorithms are linguistically questionable at times (Church, 2020), which has been shown to worsen performance on certain downstream tasks (Bostrom",
      "startOffset" : 52,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : "At the same time, the fact that low-frequency and out-of-vocabulary words are often derivationally complex (Baayen and Lieber, 1991) makes our work relevant in practical settings, especially when many one-word expressions are involved, e.",
      "startOffset" : 107,
      "endOffset" : 132
    }, {
      "referenceID" : 26,
      "context" : ", 2020) or taking the first subword embedding (Devlin et al., 2019; Heinzerling and Strube, 2019; Martin et al., 2020).",
      "startOffset" : 46,
      "endOffset" : 118
    }, {
      "referenceID" : 39,
      "context" : ", 2020) or taking the first subword embedding (Devlin et al., 2019; Heinzerling and Strube, 2019; Martin et al., 2020).",
      "startOffset" : 46,
      "endOffset" : 118
    }, {
      "referenceID" : 61,
      "context" : ", 2020) or taking the first subword embedding (Devlin et al., 2019; Heinzerling and Strube, 2019; Martin et al., 2020).",
      "startOffset" : 46,
      "endOffset" : 118
    }, {
      "referenceID" : 60,
      "context" : "Two basic processing mechanisms have been proposed: storage, where the meaning of complex words is listed in the mental lexicon (Manelis and Tharp, 1977; Butterworth, 1983; Feldman and Fowler, 1987; Bybee, 1988; Stemberger, 1994; Bybee, 1995; Bertram et al., 2000a), and computation, where the meaning of complex words is inferred based on the meaning of stem and affixes (Taft and Forster, 1975; Taft, 1979, 1981, 1988, 1991, 1994; Rastle et al.",
      "startOffset" : 128,
      "endOffset" : 265
    }, {
      "referenceID" : 15,
      "context" : "Two basic processing mechanisms have been proposed: storage, where the meaning of complex words is listed in the mental lexicon (Manelis and Tharp, 1977; Butterworth, 1983; Feldman and Fowler, 1987; Bybee, 1988; Stemberger, 1994; Bybee, 1995; Bertram et al., 2000a), and computation, where the meaning of complex words is inferred based on the meaning of stem and affixes (Taft and Forster, 1975; Taft, 1979, 1981, 1988, 1991, 1994; Rastle et al.",
      "startOffset" : 128,
      "endOffset" : 265
    }, {
      "referenceID" : 32,
      "context" : "Two basic processing mechanisms have been proposed: storage, where the meaning of complex words is listed in the mental lexicon (Manelis and Tharp, 1977; Butterworth, 1983; Feldman and Fowler, 1987; Bybee, 1988; Stemberger, 1994; Bybee, 1995; Bertram et al., 2000a), and computation, where the meaning of complex words is inferred based on the meaning of stem and affixes (Taft and Forster, 1975; Taft, 1979, 1981, 1988, 1991, 1994; Rastle et al.",
      "startOffset" : 128,
      "endOffset" : 265
    }, {
      "referenceID" : 16,
      "context" : "Two basic processing mechanisms have been proposed: storage, where the meaning of complex words is listed in the mental lexicon (Manelis and Tharp, 1977; Butterworth, 1983; Feldman and Fowler, 1987; Bybee, 1988; Stemberger, 1994; Bybee, 1995; Bertram et al., 2000a), and computation, where the meaning of complex words is inferred based on the meaning of stem and affixes (Taft and Forster, 1975; Taft, 1979, 1981, 1988, 1991, 1994; Rastle et al.",
      "startOffset" : 128,
      "endOffset" : 265
    }, {
      "referenceID" : 85,
      "context" : "Two basic processing mechanisms have been proposed: storage, where the meaning of complex words is listed in the mental lexicon (Manelis and Tharp, 1977; Butterworth, 1983; Feldman and Fowler, 1987; Bybee, 1988; Stemberger, 1994; Bybee, 1995; Bertram et al., 2000a), and computation, where the meaning of complex words is inferred based on the meaning of stem and affixes (Taft and Forster, 1975; Taft, 1979, 1981, 1988, 1991, 1994; Rastle et al.",
      "startOffset" : 128,
      "endOffset" : 265
    }, {
      "referenceID" : 17,
      "context" : "Two basic processing mechanisms have been proposed: storage, where the meaning of complex words is listed in the mental lexicon (Manelis and Tharp, 1977; Butterworth, 1983; Feldman and Fowler, 1987; Bybee, 1988; Stemberger, 1994; Bybee, 1995; Bertram et al., 2000a), and computation, where the meaning of complex words is inferred based on the meaning of stem and affixes (Taft and Forster, 1975; Taft, 1979, 1981, 1988, 1991, 1994; Rastle et al.",
      "startOffset" : 128,
      "endOffset" : 265
    }, {
      "referenceID" : 6,
      "context" : "Two basic processing mechanisms have been proposed: storage, where the meaning of complex words is listed in the mental lexicon (Manelis and Tharp, 1977; Butterworth, 1983; Feldman and Fowler, 1987; Bybee, 1988; Stemberger, 1994; Bybee, 1995; Bertram et al., 2000a), and computation, where the meaning of complex words is inferred based on the meaning of stem and affixes (Taft and Forster, 1975; Taft, 1979, 1981, 1988, 1991, 1994; Rastle et al.",
      "startOffset" : 128,
      "endOffset" : 265
    }, {
      "referenceID" : 94,
      "context" : ", 2000a), and computation, where the meaning of complex words is inferred based on the meaning of stem and affixes (Taft and Forster, 1975; Taft, 1979, 1981, 1988, 1991, 1994; Rastle et al., 2004; Taft, 2004; Rastle and Davis, 2008).",
      "startOffset" : 115,
      "endOffset" : 232
    }, {
      "referenceID" : 78,
      "context" : ", 2000a), and computation, where the meaning of complex words is inferred based on the meaning of stem and affixes (Taft and Forster, 1975; Taft, 1979, 1981, 1988, 1991, 1994; Rastle et al., 2004; Taft, 2004; Rastle and Davis, 2008).",
      "startOffset" : 115,
      "endOffset" : 232
    }, {
      "referenceID" : 93,
      "context" : ", 2000a), and computation, where the meaning of complex words is inferred based on the meaning of stem and affixes (Taft and Forster, 1975; Taft, 1979, 1981, 1988, 1991, 1994; Rastle et al., 2004; Taft, 2004; Rastle and Davis, 2008).",
      "startOffset" : 115,
      "endOffset" : 232
    }, {
      "referenceID" : 77,
      "context" : ", 2000a), and computation, where the meaning of complex words is inferred based on the meaning of stem and affixes (Taft and Forster, 1975; Taft, 1979, 1981, 1988, 1991, 1994; Rastle et al., 2004; Taft, 2004; Rastle and Davis, 2008).",
      "startOffset" : 115,
      "endOffset" : 232
    }, {
      "referenceID" : 33,
      "context" : ", both mechanisms are always activated (Frauenfelder and Schreuder, 1992; Schreuder and Baayen, 1995; Baayen et al., 1997, 2000; Bertram et al., 2000b; New et al., 2004; Kuperman et al., 2008, 2009), or serial, i.",
      "startOffset" : 39,
      "endOffset" : 198
    }, {
      "referenceID" : 80,
      "context" : ", both mechanisms are always activated (Frauenfelder and Schreuder, 1992; Schreuder and Baayen, 1995; Baayen et al., 1997, 2000; Bertram et al., 2000b; New et al., 2004; Kuperman et al., 2008, 2009), or serial, i.",
      "startOffset" : 39,
      "endOffset" : 198
    }, {
      "referenceID" : 7,
      "context" : ", both mechanisms are always activated (Frauenfelder and Schreuder, 1992; Schreuder and Baayen, 1995; Baayen et al., 1997, 2000; Bertram et al., 2000b; New et al., 2004; Kuperman et al., 2008, 2009), or serial, i.",
      "startOffset" : 39,
      "endOffset" : 198
    }, {
      "referenceID" : 66,
      "context" : ", both mechanisms are always activated (Frauenfelder and Schreuder, 1992; Schreuder and Baayen, 1995; Baayen et al., 1997, 2000; Bertram et al., 2000b; New et al., 2004; Kuperman et al., 2008, 2009), or serial, i.",
      "startOffset" : 39,
      "endOffset" : 198
    }, {
      "referenceID" : 73,
      "context" : "Complex-word processing is driven by analogical processes over the mental lexicon (Rácz et al., 2020).",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 24,
      "context" : "represent complex words as whole-word vectors (Deerwester et al., 1990; Mikolov et al., 2013a,b; Pennington et al., 2014) can be seen as single-route storage models.",
      "startOffset" : 46,
      "endOffset" : 121
    }, {
      "referenceID" : 69,
      "context" : "represent complex words as whole-word vectors (Deerwester et al., 1990; Mikolov et al., 2013a,b; Pennington et al., 2014) can be seen as single-route storage models.",
      "startOffset" : 46,
      "endOffset" : 121
    }, {
      "referenceID" : 82,
      "context" : "Word embeddings that represent complex words as a function of subword or morpheme vectors (Schütze, 1992; Luong et al., 2013) can be seen as single-route computation models.",
      "startOffset" : 90,
      "endOffset" : 125
    }, {
      "referenceID" : 58,
      "context" : "Word embeddings that represent complex words as a function of subword or morpheme vectors (Schütze, 1992; Luong et al., 2013) can be seen as single-route computation models.",
      "startOffset" : 90,
      "endOffset" : 125
    }, {
      "referenceID" : 12,
      "context" : "Finally, word embeddings that represent complex words as a function of subword or morpheme vectors as well as whole-word vectors (Botha and Blunsom, 2014; Qiu et al., 2014; Bhatia et al., 2016; Bojanowski et al., 2017; Athiwaratkun et al., 2018; Salle and Villavicencio, 2018) are most closely related to parallel dual-route approaches.",
      "startOffset" : 129,
      "endOffset" : 276
    }, {
      "referenceID" : 72,
      "context" : "Finally, word embeddings that represent complex words as a function of subword or morpheme vectors as well as whole-word vectors (Botha and Blunsom, 2014; Qiu et al., 2014; Bhatia et al., 2016; Bojanowski et al., 2017; Athiwaratkun et al., 2018; Salle and Villavicencio, 2018) are most closely related to parallel dual-route approaches.",
      "startOffset" : 129,
      "endOffset" : 276
    }, {
      "referenceID" : 9,
      "context" : "Finally, word embeddings that represent complex words as a function of subword or morpheme vectors as well as whole-word vectors (Botha and Blunsom, 2014; Qiu et al., 2014; Bhatia et al., 2016; Bojanowski et al., 2017; Athiwaratkun et al., 2018; Salle and Villavicencio, 2018) are most closely related to parallel dual-route approaches.",
      "startOffset" : 129,
      "endOffset" : 276
    }, {
      "referenceID" : 10,
      "context" : "Finally, word embeddings that represent complex words as a function of subword or morpheme vectors as well as whole-word vectors (Botha and Blunsom, 2014; Qiu et al., 2014; Bhatia et al., 2016; Bojanowski et al., 2017; Athiwaratkun et al., 2018; Salle and Villavicencio, 2018) are most closely related to parallel dual-route approaches.",
      "startOffset" : 129,
      "endOffset" : 276
    }, {
      "referenceID" : 2,
      "context" : "Finally, word embeddings that represent complex words as a function of subword or morpheme vectors as well as whole-word vectors (Botha and Blunsom, 2014; Qiu et al., 2014; Bhatia et al., 2016; Bojanowski et al., 2017; Athiwaratkun et al., 2018; Salle and Villavicencio, 2018) are most closely related to parallel dual-route approaches.",
      "startOffset" : 129,
      "endOffset" : 276
    }, {
      "referenceID" : 79,
      "context" : "Finally, word embeddings that represent complex words as a function of subword or morpheme vectors as well as whole-word vectors (Botha and Blunsom, 2014; Qiu et al., 2014; Bhatia et al., 2016; Bojanowski et al., 2017; Athiwaratkun et al., 2018; Salle and Villavicencio, 2018) are most closely related to parallel dual-route approaches.",
      "startOffset" : 129,
      "endOffset" : 276
    }, {
      "referenceID" : 48,
      "context" : "Similarly to how character-based models represent word meaning (Kim et al., 2016; Adel et al., 2017), they can also store the meaning of frequent complex words that are segmented into subwords, i.",
      "startOffset" : 63,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "Similarly to how character-based models represent word meaning (Kim et al., 2016; Adel et al., 2017), they can also store the meaning of frequent complex words that are segmented into subwords, i.",
      "startOffset" : 63,
      "endOffset" : 100
    }, {
      "referenceID" : 26,
      "context" : "Taking BERT, specifically BERTBASE (uncased) (Devlin et al., 2019), as the example PLM, the words stabilize and realize have individual tokens in the input vocabulary and are hence associated with whole-word vectors storing their meanings, including highly lexicalized meanings",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 26,
      "context" : "have relatively high absolute frequencies of 2,540 (finalize) and 6,904 (mobilize) in the English Wikipedia, the main dataset used to pretrain BERT (Devlin et al., 2019), which means that BERT can store their meanings in its model weights during pretraining.",
      "startOffset" : 148,
      "endOffset" : 169
    }, {
      "referenceID" : 96,
      "context" : "Previous research suggests that such lexical knowledge is stored in the lower layers of BERT (Vulić et al., 2020).",
      "startOffset" : 93,
      "endOffset" : 113
    }, {
      "referenceID" : 81,
      "context" : "BERT’s standard WordPiece segmentation (Schuster and Nakajima, 2012; Wu et al., 2016), sw, and a derivational segmentation that segments complex words into stems and affixes, sd.",
      "startOffset" : 39,
      "endOffset" : 85
    }, {
      "referenceID" : 38,
      "context" : "The distinction between inflectionally and derivationally complex words is notoriously fuzzy (Haspelmath and Sims, 2010; ten Hacken, 2014).",
      "startOffset" : 93,
      "endOffset" : 138
    }, {
      "referenceID" : 67,
      "context" : "A large dataset of Amazon reviews has been made publicly available (Ni et al., 2019).",
      "startOffset" : 67,
      "endOffset" : 84
    }, {
      "referenceID" : 99,
      "context" : "6 We extract derivationally complex words from reviews with one or two (neg) as well as four or five stars (pos), discarding three-star reviews for a clearer separation (Yang and Eisenstein, 2017).",
      "startOffset" : 169,
      "endOffset" : 196
    }, {
      "referenceID" : 43,
      "context" : "into smaller communities, so-called subreddits, which have been shown to be a rich source of derivationally complex words (Hofmann et al., 2020c).",
      "startOffset" : 122,
      "endOffset" : 145
    }, {
      "referenceID" : 26,
      "context" : "The specific BERT variant we use is BERTBASE (uncased) (Devlin et al., 2019).",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 30,
      "context" : "tion and antonymy are known to be challenging for PLMs (Ettinger, 2020; Kassner and Schütze, 2020) and might be one of the reasons for the generally lower performance on Amazon.",
      "startOffset" : 55,
      "endOffset" : 98
    }, {
      "referenceID" : 47,
      "context" : "tion and antonymy are known to be challenging for PLMs (Ettinger, 2020; Kassner and Schütze, 2020) and might be one of the reasons for the generally lower performance on Amazon.",
      "startOffset" : 55,
      "endOffset" : 98
    }, {
      "referenceID" : 31,
      "context" : "Relatedly, studies from automatic speech recognition have demonstrated that morphological decomposition improves the perplexity of language models (Fang et al., 2015; Jain et al., 2020).",
      "startOffset" : 147,
      "endOffset" : 185
    }, {
      "referenceID" : 44,
      "context" : "Relatedly, studies from automatic speech recognition have demonstrated that morphological decomposition improves the perplexity of language models (Fang et al., 2015; Jain et al., 2020).",
      "startOffset" : 147,
      "endOffset" : 185
    }, {
      "referenceID" : 28,
      "context" : "There are also studies that analyze morphological aspects of PLMs without a focus on questions surrounding segmentation (Edmiston, 2020; Klemen et al., 2020).",
      "startOffset" : 120,
      "endOffset" : 157
    }, {
      "referenceID" : 51,
      "context" : "There are also studies that analyze morphological aspects of PLMs without a focus on questions surrounding segmentation (Edmiston, 2020; Klemen et al., 2020).",
      "startOffset" : 120,
      "endOffset" : 157
    }, {
      "referenceID" : 56,
      "context" : "Cotterell and Schütze, 2018), and how affix embeddings can be computed (Lazaridou et al., 2013; Kisselew et al., 2015; Padó et al., 2016).",
      "startOffset" : 71,
      "endOffset" : 137
    }, {
      "referenceID" : 50,
      "context" : "Cotterell and Schütze, 2018), and how affix embeddings can be computed (Lazaridou et al., 2013; Kisselew et al., 2015; Padó et al., 2016).",
      "startOffset" : 71,
      "endOffset" : 137
    }, {
      "referenceID" : 68,
      "context" : "Cotterell and Schütze, 2018), and how affix embeddings can be computed (Lazaridou et al., 2013; Kisselew et al., 2015; Padó et al., 2016).",
      "startOffset" : 71,
      "endOffset" : 137
    } ],
    "year" : 2021,
    "abstractText" : "How does the input segmentation of pretrained language models (PLMs) affect their interpretations of complex words? We present the first study investigating this question, taking BERT as the example PLM and focusing on its semantic representations of English derivatives. We show that PLMs can be interpreted as serial dual-route models, i.e., the meanings of complex words are either stored or else need to be computed from the subwords, which implies that maximally meaningful input tokens should allow for the best generalization on new words. This hypothesis is confirmed by a series of semantic probing tasks on which DelBERT (Derivation leveraging BERT), a model with derivational input segmentation, substantially outperforms BERT with WordPiece segmentation. Our results suggest that the generalization capabilities of PLMs could be further improved if a morphologically-informed vocabulary of input tokens were used.",
    "creator" : "LaTeX with hyperref"
  }
}