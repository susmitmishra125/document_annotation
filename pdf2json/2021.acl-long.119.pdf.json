{
  "name" : "2021.acl-long.119.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Gradually Soft Multi-Task and Data-Augmented Approach to Medical Question Understanding",
    "authors" : [ "Khalil Mrini", "Franck Dernoncourt", "Seunghyun Yoon", "Trung Bui", "Walter Chang", "Emilia Farcas", "Ndapa Nakashole" ],
    "emails" : [ "nnakashole}@ucsd.edu", "wachang}@adobe.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1505–1515\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1505"
    }, {
      "heading" : "1 Introduction",
      "text" : "In order to retrieve relevant answers, one of the basic steps in Question Answering (QA) systems is understanding the intent of questions (Chen et al., 2012; Cai et al., 2017). This is particularly important for medical QA systems (Wu et al., 2020), as consumer health questions – questions asked by patients – may use a vocabulary distinct from doctors to describe similar health concepts (Ben Abacha and Demner-Fushman, 2019a). Consumer health questions may also contain peripheral information like patient history (Roberts and Demner-Fushman, 2016), that are not necessary to answer questions. There is a growing number of approaches to medical question understanding, including query relax-\nation (Ben Abacha and Zweigenbaum, 2015; Lei et al., 2020), question entailment (Ben Abacha and Demner-Fushman, 2016, 2019b; Agrawal et al., 2019), question summarization (Ben Abacha and Demner-Fushman, 2019a), and question similarity (Ben Abacha and Demner-Fushman, 2017; Yan and Li, 2018; McCreery et al., 2019).\nMedical question summarization is the task of summarizing consumer health questions into short, single-sentence questions that capture essential information needed to give a correct answer. The task of Recognizing Question Entailment (RQE) is defined by Ben Abacha and Demner-Fushman (2016) in the medical domain as a binary classification task. For the purpose of this task, a first question is considered to entail a second one if and only if every answer to the second question is a correct, and either full or partial answer to the first question.\nWe find in initial experiments (Mrini et al., 2021b) that RQE can teach question summarizers to distinguish salient information from peripheral details, and likewise that question summarization can benefit RQE classifiers. In our setting, we cast the medical question understanding task as a Multi-\nTask Learning (MTL) problem involving the two tasks of question summarization and Recognizing Question Entailment. We use a simple sum of learning objectives in Mrini et al. (2021b). In this paper, we introduce a novel, gradually soft multi-task and data-augmented approach to medical question understanding.1\nPrevious work on combining summarization and entailment uses at least 2 datasets – 1 from each task (Pasunuru et al., 2017; Guo et al., 2018). We first establish an equivalence between both tasks. This equivalence is the inspiration behind the data augmentation schemes introduced in our previous work (Mrini et al., 2021b). The goal of the data augmentation is to use a single dataset for MultiTask Learning. We propose to use a weighted loss function to simultaneously optimize for both tasks. Then, we propose a gradually soft parametersharing MTL approach. We conduct ablation studies to show that our two novelties – data augmentation and gradually soft parameter-sharing – improve performance in both tasks.\nOur proposed gradually soft multi-task and dataaugmented approach outperforms existing singletask and multi-task learning methods on architectures achieving state-of-the-art results in abstractive summarization. Compared to single-task learning, our approach achieves a 12% increase in accuracy on a medical RQE dataset, and an average increase of 3.5% in ROUGE-1 F1 scores across 3 medical question summarization datasets. Additionally, we perform human evaluation and find our approach generates more informative summarized questions. Finally, we find that our approach is more efficient at leveraging smaller amounts of data, and yields better performance under 4 low-resource settings."
    }, {
      "heading" : "2 Background and Related Work",
      "text" : "Recognizing Question Entailment (RQE). Ben Abacha and Demner-Fushman (2016) introduce the task of RQE. It is closely related –– but not exactly similar –– to the task of Recognizing Textual Entailment (RTE) (Dagan et al., 2005, 2013), and early definitions of question entailment (Groenendijk and Stokhof, 1984; Roberts, 1996).\nThe task of RQE is to predict, given two pairs of questions A and B, whether A entails B. RQE considers that question A entails question B if every\n1Our code is available at: https://github.com/KhalilMrini/ Medical-Question-Understanding\nanswer to B is a correct answer to A, and answers A either partially or fully. It differs from traditional definitions of entailment, where we consider that the premise entails the hypothesis if and only if the hypothesis is true only if the premise is true.\nBen Abacha and Demner-Fushman (2016) define RQE within the context of Medical Question Answering. The goal is to match a Consumer Health Question (CHQ) to a Frequently Asked Question (FAQ), and ultimately match the CHQ to an expertwritten answer. Summarization and Entailment. There is a growing body of work combining summarization and entailment (Lloret et al., 2008; Mehdad et al., 2013; Gupta et al., 2014).\nFalke et al. (2019) use textual entailment predictions to detect factual errors in abstractive summaries generated by state-of-the-art models. Pasunuru and Bansal (2018) propose an entailment reward for their reinforced abstractive summarizer, where the entailment score is obtained from a pre-trained and frozen natural language inference model.\nPasunuru et al. (2017) propose an LSTM encoder-decoder model that incorporates entailment generation and abstractive summarization. The authors optimize alternatively between the two tasks, and use separate Natural Language Inference (NLI) and abstractive summarization datasets. Only the decoder parameters are shared.\nLi et al. (2018) closely follow the MTL setting of Pasunuru et al. (2017), and propose a model with a shared encoder, an NLI classifier and an NLI-rewarded summarization decoder.\nGuo et al. (2018) introduce a pointer-generator summarization model with coverage loss (See et al., 2017). They build upon the work of Pasunuru et al. (2017), and add question generation on top of the two tasks of abstractive summarization and entailment generation. They also alternate between the three different objectives. The authors propose to share all parameters except the first layer of the encoder and the last layer of the decoder, and show that soft parameter-sharing improves over hard parameter-sharing. Their method outperforms the pointer-generator networks of See et al. (2017) on the CNN-Dailymail news summarization baseline. Here, the authors show performance increase in entailment on some batch sizes and decrease on other batch sizes, and they consider entailment as an auxiliary task.\nTransfer Learning for Medical QA. BioNLP is one of many NLP applications to benefit from language models that use multi-task learning and transfer learning. There are pretrained language models that are geared towards BioNLP applications, that are based on BERT (Devlin et al., 2019). Those include SciBERT (Beltagy et al., 2019) which has been fine-tuned using biomedical text from PubMed. BioBERT (Lee et al., 2020) has been finetuned on the PMC dataset, whereas models named ClinicalBERT (Huang et al., 2019; Alsentzer et al., 2019) additionally use the MIMIC III dataset (Johnson et al., 2016).\nTransfer learning was a popular approach at the 2019 MEDIQA shared task (Ben Abacha et al., 2019) on medical NLI, RQE and QA. The question answering task involved re-ranking answers, not generating them (Demner-Fushman et al., 2020). For the RQE task, the best-performing model (Zhu et al., 2019) uses transfer learning on NLI and ensemble methods."
    }, {
      "heading" : "3 Methodology",
      "text" : "We consider the multi-task learning of medical question summarization and medical RQE. The input to both tasks is a pair of medical questions. The first question is called a Consumer Health Question (CHQ), and the second question is called a Frequently Asked Question (FAQ). The CHQ is written by a patient and is usually longer and more informal, whereas the FAQ is usually a singlesentence question written by a medical expert. The purpose of both tasks is to match a CHQ to an FAQ, and ultimately to an expert-written answer that matches the FAQ. An example pair is shown in Figure 1.\nOur novel gradually soft multi-task and dataaugmented learning approach to medical question understanding has four main components. First, we establish the equivalence between medical question pairs in question summarization and RQE. Then, we use our equivalence observation to propose a scheme for data augmentation. Third, we show our simultaneous multi-task learning model architecture and learning objective. Finally, we describe our gradually soft parameter-sharing scheme."
    }, {
      "heading" : "3.1 Equivalence of Question Summarization and RQE",
      "text" : "In the following, we evidence the equivalence between medical question summarization and medi-\ncal RQE. We first consider a pair of medical questions C and F, where C is a CHQ and F and is an FAQ, such that C is longer than F.\nBen Abacha and Demner-Fushman (2016) define question entailment as: question C entails question F (C ⇒ F ) if and only if every answer to F is also a correct answer to C, whether partially or completely (1).\nAccording to the guidelines set in the data creation of a medical question summarization dataset by Ben Abacha and Demner-Fushman (2019a), doctors were told to grade manually written summarized questions (FAQs) as perfect, acceptable or incorrect. The two conditions for a perfect FAQ are: first, an FAQ should enable to retrieve “complete and correct answers” to the original CHQ, and second, the summarized question should not be so short that it violates the first condition. The resulting medical question summarization dataset includes perfect and acceptable FAQs. We assume that a perfect FAQ provides complete and correct answers to the corresponding CHQ, and that an acceptable FAQ provides correct answers to the corresponding CHQ, whether partially or completely. We therefore conclude that: F is a good summary of C, if and only if F enables to retrieve correct answers to C, whether partially or completely (2).\nWe have: F enables to retrieve correct answers to C, if and only if answers to F are correct answers to C. Therefore, F enables to retrieve correct answers to C, if and only if every answer to F is also a correct answer to C, whether partially or completely. Given the equivalences (1) and (2) above, it follows that: question F is a good summary of question C, if and only if question C entails question F (3)."
    }, {
      "heading" : "3.2 Data Augmentation",
      "text" : "Medical question understanding datasets are scarce, and new high-quality datasets are complex and costly to create. We propose in Mrini et al. (2021b) to augment existing datasets in one of the two tasks to create a synthetic dataset of the same size for the other task. Our two-way data augmentation algorithm is inspired by the equivalence shown in the previous subsection, and enables us to train in a simultaneous multi-task setting. Our data augmentation method also addresses a weakness in previous work in multi-task learning, where each task involves a distinct dataset, often from a different domain. Our data augmentation will enable us to use datasets in the same domain, and we hypoth-\nesize this can benefit performance in both tasks. For summarization datasets, we create equivalent RQE pairs. For each existing summarization pair, we first choose with equal probability whether the equivalent RQE pair is labeled as entailment or not. If it is an entailment case, we use the equivalence in (3) and create an RQE pair identical to the summarization pair. If it is not an entailment case, then we have: (3)⇔ question F is not a summary of question C if and only if question C does not entail question F (4). Therefore, to create an equivalent RQE pair labeled as not entailement, the RQE CHQ is identical to the CHQ of the summarization pair, and the RQE FAQ is randomly selected from a distinct question pair from the same dataset split.\nInversely, for the RQE dataset, we create equivalent summarization pairs. For each existing RQE pair, we consider two cases. If the RQE pair is labeled as entailment, we create an identical summarization pair. If the RQE pair is labeled as not entailment, then following (4), we create a summarization pair that is identical to a randomly selected and distinct RQE pair labeled as entailment from the same dataset split."
    }, {
      "heading" : "3.3 Simultaneous Multi-Task Learning",
      "text" : "Previous work on multi-task learning with summarization and entailment (Pasunuru et al., 2017; Guo et al., 2018) optimize for the objectives of the different tasks by alternating between them. This alternating multi-task training follows a ratio between the different tasks, that depends on the size of the dataset of each task (e.g. a ratio of 10:1 means training for 10 batches on one task, and then for 1 batch on the other task). In our approach, we propose to optimize simultaneously for the objectives of both tasks. We do not use ratios, as we are not alternating between objectives and the resulting datasets from our data augmentation algorithm are of equal size.\nWhereas many previous multi-task settings chose generation tasks (entailment generation and question generation), we choose the BART Large architecture (Lewis et al., 2019) as it enables to optimize for a classification task (RQE) and a generation task (summarization) using the same architecture. In addition, BART is adequate as it achieves very strong results in benchmark datasets of recognizing textual entailment and abstractive summarization. The input works differently between both tasks. For summarization, the encoder\ntakes the CHQ as input and the decoder takes the FAQ as input. For RQE, both the encoder and decoder take the entire RQE pair as input. We add a classification head for RQE, to which we feed the last decoder output, as it attends over all decoder and encoder positions. We show an overview of our architecture in Figure 2.\nWe propose to optimize a single loss function that combines objectives of both tasks. Our loss function is the weighted sum of the negative loglikelihood summarization objective, and the binary cross-entropy classification objective of RQE.\nMore formally, given a CHQ embedding x, the corresponding FAQ embedding y, and the entailment label lentail ∈ {0, 1}, we optimize the following multi-task learning loss function:\nLMTL(θ) =− λ ∗ logp(y|x; θ) + (1− λ) ∗ BCE ([x;y] , lentail; θ)\n(1)\nwhere BCE is binary cross entropy, and λ is a hyperparameter between 0 and 1."
    }, {
      "heading" : "3.4 Gradually Soft Parameter-Sharing",
      "text" : "In multi-task learning, there are two widely used approaches: hard parameter-sharing and soft parameter-sharing. Guo et al. (2018) propose soft parameter-sharing for all parameters except the first layer of the encoder and last layer of the decoder. Liu et al. (2019) introduce MT-DNN and show that hard parameter-sharing of all of the transformer encoder layers, and only having task-specific classification heads produces results that set a new state of the art for the GLUE benchmark (Wang et al., 2018).\nWe propose a hybrid approach, where we apply hard parameter-sharing for the encoder, and a novel gradually soft parameter-sharing approach for the decoder layers. We define gradually soft parametersharing as a smooth transition from hard parametersharing to task-specific layers. It is a soft parametersharing approach that is gradually toned down from the first layer of the decoder to the last layer, which is entirely task-specific.\nIn gradually soft parameter-sharing, we constrain decoder parameters to be close by penalizing their l2 distances, and the higher the layer the looser the constraint. Given a decoder with N layers, the gradually soft parameter-sharing loss term is as follows:\nLGS(θ) = γ ∗ N−1∑ n=1 ( e N−n N − 1 )∥∥∥θQSdec,n − θRQEdec,n∥∥∥2 (2) where γ is a hyperparameter, θQSdec,n represents the decoder parameters for the question summarization at the n-th layer, and likewise θRQEdec,n represents the decoder parameters for the RQE task at the n-th layer. We iterate from the 1st to the (N − 1)-th layer, as the N -th layer is entirely task-specific and unconstrained. We show a high-level representation in Figure 2."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We consider 3 medical question summarization datasets and 1 medical RQE dataset. We show dataset statistics in Table 1. MeQSum and MEDIQA RQE can be considered low-resource, whereas the other two are far larger. Our datasets are in the English language. Due to space constraints, we briefly introduce the datasets and leave additional details in the appendix.\nThe medical question summarization datasets are MeQSum (Ben Abacha and Demner-Fushman, 2019a), HealthCareMagic and iCliniq. We extract in Mrini et al. (2021b) and in Mrini et al. (2021c) the HealthCareMagic and iCliniq datasets from the large-scale MedDialog dataset (Chen et al., 2020). Whereas MeQSum is a high-quality dataset from the U.S. National Institutes of Health (NIH), HealthCareMagic and iCliniq are from online healthcare service platforms. HealthCareMagic’s summaries are more abstractive and are written in a formal style, unlike iCliniq’s patient-written summaries.\nThe medical RQE dataset is the MEDIQA RQE dataset from the 2019 MEDIQA shared task (Ben Abacha et al., 2019). Similarly to MeQSum, the question pairs match a longer CHQ received by the U.S. National Library of Medicine (NLM) and a FAQ from NIH institutes. Whereas the train and dev sets have automatically generated CHQs, the test set has manually written CHQs. This results in significantly higher dev set results than for test sets, as has been observed during the 2019 MEDIQA shared task.\nIn addition, we use two pretraining datasets. We use the XSum dataset (Narayan et al., 2018), an abstractive summarization benchmark, for question summarization. For the RQE task, we use the Recognizing Textual Entailment (RTE) dataset (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) from the GLUE benchmark (Wang et al., 2018)."
    }, {
      "heading" : "4.2 Setup and Training Settings",
      "text" : "All of our models use the BART large architecture. Unless otherwise noted, all experiments on the 3 question summarization datasets are made using a checkpoint pre-trained on the XSum dataset using only the summarization objective, and all experiments on the RQE dataset are made using a checkpoint pre-trained on the RTE dataset, only optimizing the cross-entropy loss.\nWe report ROUGE F1 scores for the question\nsummarization datasets, and accuracy for the RQE dataset, as it is a binary classification task with two labels: entailment and not entailment.\nThe learning rate for RQE experiments is 1 × 10−5 and for the question summarization experiments, it is 3× 10−5. We use an Adam optimizer where the betas are 0.9 and 0.999 for summarization, and 0.9 and 0.98 for RQE. In all experiments, the Adam epsilon is 10−8, and the dropout is 0.1. We set the γ hyperparameter to 1× 10−7."
    }, {
      "heading" : "4.3 Balancing between the Objectives",
      "text" : "Our loss function as defined in Eq.1 has a hyperparameter λ to balance between the question summarization objective and the RQE objective. We run experiments where λ varies from 0.1 to 0.9 in 0.1 increments. The results are in Figure 3. The best λ values are 0.5 for MeQSum, 0.7 for iCliniq, 0.8 for HealthCareMagic and 0.3 for MEDIQA RQE. For the question summarization datasets, we notice that\nthe smaller the dataset, the more it benefits from data-augmented MTL with RQE."
    }, {
      "heading" : "4.4 Ablation Studies",
      "text" : "We perform two ablation studies to show the added value of our main novelties: our equivalenceinspired data augmentation algorithm and our gradually soft parameter-sharing algorithm.\nData Augmentation. We compare our data augmentation algorithm against the following alternative: instead of training using a synthetic dataset for the auxiliary task, we choose a separate, existing dataset for abstractive summarization or recognizing textual entailment. This follows the approach taken by most MTL models. For the question summarization task, we optimize the cross-entropy objective using the RTE dataset. For the RQE task, we optimize the summarization objective using the XSum dataset. For the sake of fair comparison, we use the simultaneous MTL objective and the same architecture. Results in Table 2 show a consistent increase in performance across all datasets when using our data augmentation method, suggesting that in-domain MTL is more efficient.\nComparing Parameter-Sharing Configurations. We compare our gradually soft parameter-sharing method with 3 other parameter-sharing configurations. For all configurations, we keep using our data augmentation method, and sharing encoder parameters entirely.\n1. Hard-shared decoder: decoder parameters are shared using hard parameter-sharing.\n2. Soft-shared decoder: we apply soft parametersharing on decoder parameters across all N layers\nusing the following, unweighted loss term:\nLS(θ) = γ ∗ N∑\nn=1 ∥∥θsumdec,n − θentdec,n∥∥2 (3) 3. Task-specific decoder: we train two task-specific decoders.\nOur ablation study results in Table 2 show that our gradually soft parameter-sharing method exceeds all 3 of the other parameter-sharing configurations in RQE accuracy, and in the sum of ROUGE F1 scores. These results show our proposed smoother parameter-sharing transition between encoder and decoder layers brings about higher performance."
    }, {
      "heading" : "4.5 Results and Discussion",
      "text" : ""
    }, {
      "heading" : "4.5.1 Summarization Results",
      "text" : "Baselines. We consider three main baselines. The first one is BART (Lewis et al., 2019), where we only train on the summarization task. The second baseline trains BART on the same MTL settings as Pasunuru et al. (2017), using alternative training with entailment generation on the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) and having a shared decoder and taskspecific encoders. The third baseline trains BART on the same MTL settings as Guo et al. (2018), where, on top of the entailment generation task, we add the question generation task using the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), and all parameters are soft-shared, except for the task-specific first encoder layer and last decoder layer.\nIn addition, we also report the baselines assessed by Ben Abacha and Demner-Fushman (2019a) for MeQSum. For data augmentation, they use semantically-selected relevant question pairs from the Quora Question Pairs dataset (Iyer et al., 2017). Their results show that coverage loss (See et al., 2017) diminishes the added value of data augmentation in pointer-generator networks. Our summarization-only BART baseline exceeds all of the reported MeQSum baselines in ROUGE-1 F1. Summarization Results. We report our summarization results in Table 3. Compared to the singletask BART baseline, our gradually soft multi-task and data-augmented method performs better across all three ROUGE metrics, and achieves increases ranging from 1.4 to 5.5 points in ROUGE-1 F1.\nThis differences shows that our method is consistently more efficient compared to training only on summarization.\nThe other two MTL baselines are generally performing better than the single-task BART baseline, except for the larger HealthCareMagic dataset. We observe that the different parameter-sharing configurations and tasks used in the MTL baselines are scoring about 1 to 4 points below our method in terms of ROUGE-1 F1 scores. This shows that our choice of tasks, simultaneous MTL loss, data augmentation and gradually soft parameter-sharing method work consistently better than existing MTL methods. Human Evaluation. Given that ROUGE is notoriously unreliable, we hire 2 annotators to judge 120 randomly selected summaries from the summarization test sets, generated from the single-task BART baseline and our own method in Table 3. We ask the annotators to judge the Fluency, Coherence, Informativeness and Correctness of each generated summary, using Best-Worst scaling, with the possibility of ranking both summaries equally. The annotators are presented with 2 generated summaries, in a randomized order at each evaluation, such that they cannot identify which method generated which summary.\nOur human evaluation results are in Table 4. Scores generally favor our method, more strongly so in the abstractive datasets – HealthCareMagic and MeQSum. However, we note an increase in correctness for the more extractive iCliniq dataset. On average, our gradually soft multi-task and dataaugmented method outputs summarized questions that are more fluent and more informative than the single-task BART baseline."
    }, {
      "heading" : "4.5.2 RQE Results and Discussion",
      "text" : "Baselines. We compare our method to three baselines. The first one trains a single-task BART on RQE, with a classification head pre-trained on RTE. The second baseline is a feature-based SVM from Ben Abacha and Demner-Fushman (2016) who introduced the MEDIQA RQE dataset. The third baseline (Zhou et al., 2019) is an adversarial MTL method combining medical question answering and RQE. The architecture consists of a shared transformer encoder using BioBERT embeddings (Lee et al., 2020), separate classification heads for RQE and medical QA, and a task discriminator for adversarial training. A separate dataset is used for medical QA (Ben Abacha et al., 2019).\nRQE Results. We show our RQE results in Table 5. We see a 12% increase on the test set compared to optimizing only on the RQE objective, and 10% increase. Without a separate dataset or embeddings trained on large-scale biomedical data, our method is able to exceed the performance of Zhou et al. (2019) by 0.7%. This confirms the strength of our method, and shows our method can increase performance in both RQE and Question Summarization in the medical domain."
    }, {
      "heading" : "4.6 Performance in low-resource settings",
      "text" : "We compare our gradually soft MTL and dataaugmented method with the single-task BART baseline on four low-resource settings. For each dataset,\nwe limit the training data to a subset of 50, 100, 500 or 1000 datapoints, and keep the same training settings. To avoid selection bias, we select four random and distinct subsets per low-resource setting, and show average ROUGE-1 F1 scores in Figure 4.\nThe results show that our approach is able to perform much better in low-resource settings. We notice in particular that, on all 4 datasets, the scores of the single-task BART baseline for 100 and 1000 datapoints are lower than or roughly equal to the scores of our method for a training subset of half the size (50 and 500 datapoints respectively). This suggests that our method’s performance increase is not only related to additional datapoints, but also its gradually soft MTL setting."
    }, {
      "heading" : "5 Conclusions",
      "text" : "We propose a novel multi-task learning approach for medical question understanding. Our approach trains on the tasks of RQE and question summarization in a simultaneous, weighted MTL loss function, where we add a loss term to constrain the decoder layers to be close, and we loosen the constraint gradually as we move higher up the layers. We show using the definitions of both tasks in the medical domain that we can augment datasets, such that we only need one dataset for MTL. Our two ablation studies show that our gradually soft parameter-sharing and our data augmentation algorithm each increase performance individually. We compare our method to single-task learning and existing MTL work, and show improvements across 3 medical question summarization datasets and 1 medical RQE dataset. Finally, we test our approach under low-resource settings: we find that it is able to efficiently leverage small quantities of data, and that these performance increases do not only depend on additional data from augmentation."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We gratefully acknowledge the award from NIH/NIA grant R56AG067393. Khalil Mrini is additionally supported by Adobe Research Unrestricted Gifts. This work is part of the VOLI project (Mrini et al., 2021a; Johnson et al., 2020). We thank Naba Rizvi for the annotation work, and the anonymous reviewers for their feedback."
    } ],
    "references" : [ {
      "title" : "Ars_nitk at mediqa 2019: analysing various methods for natural language inference, recognising question entailment and medical question answering system",
      "author" : [ "Anumeha Agrawal", "Rosa Anil George", "Selvan Suntiha Ravi", "Sowmya Kamath", "Anand Kumar" ],
      "venue" : null,
      "citeRegEx" : "Agrawal et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Agrawal et al\\.",
      "year" : 2019
    }, {
      "title" : "Publicly available clinical bert embeddings",
      "author" : [ "Emily Alsentzer", "John Murphy", "William Boag", "WeiHung Weng", "Di Jindi", "Tristan Naumann", "Matthew McDermott." ],
      "venue" : "Proceedings of the 2nd Clinical Natural Language Processing Workshop, pages",
      "citeRegEx" : "Alsentzer et al\\.,? 2019",
      "shortCiteRegEx" : "Alsentzer et al\\.",
      "year" : 2019
    }, {
      "title" : "Scibert: A pretrained language model for scientific text",
      "author" : [ "Iz Beltagy", "Kyle Lo", "Arman Cohan." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Beltagy et al\\.,? 2019",
      "shortCiteRegEx" : "Beltagy et al\\.",
      "year" : 2019
    }, {
      "title" : "Recognizing question entailment for medical question answering",
      "author" : [ "Asma Ben Abacha", "Dina Demner-Fushman." ],
      "venue" : "AMIA Annual Symposium Proceedings, volume 2016, page 310. American Medical Informatics Association.",
      "citeRegEx" : "Abacha and Demner.Fushman.,? 2016",
      "shortCiteRegEx" : "Abacha and Demner.Fushman.",
      "year" : 2016
    }, {
      "title" : "Nlm_nih at semeval-2017 task 3: from question entailment to question similarity for community question answering",
      "author" : [ "Asma Ben Abacha", "Dina Demner-Fushman." ],
      "venue" : "Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-",
      "citeRegEx" : "Abacha and Demner.Fushman.,? 2017",
      "shortCiteRegEx" : "Abacha and Demner.Fushman.",
      "year" : 2017
    }, {
      "title" : "On the summarization of consumer health questions",
      "author" : [ "Asma Ben Abacha", "Dina Demner-Fushman." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2228–2234.",
      "citeRegEx" : "Abacha and Demner.Fushman.,? 2019a",
      "shortCiteRegEx" : "Abacha and Demner.Fushman.",
      "year" : 2019
    }, {
      "title" : "A question-entailment approach to question answering",
      "author" : [ "Asma Ben Abacha", "Dina Demner-Fushman." ],
      "venue" : "BMC bioinformatics, 20(1):511.",
      "citeRegEx" : "Abacha and Demner.Fushman.,? 2019b",
      "shortCiteRegEx" : "Abacha and Demner.Fushman.",
      "year" : 2019
    }, {
      "title" : "Overview of the mediqa 2019 shared task on textual inference, question entailment and question answering",
      "author" : [ "Asma Ben Abacha", "Chaitanya Shivade", "Dina Demner-Fushman." ],
      "venue" : "Proceedings of the 18th BioNLP Workshop and Shared Task, pages",
      "citeRegEx" : "Abacha et al\\.,? 2019",
      "shortCiteRegEx" : "Abacha et al\\.",
      "year" : 2019
    }, {
      "title" : "Means: A medical question-answering system combining nlp techniques and semantic web technologies",
      "author" : [ "Asma Ben Abacha", "Pierre Zweigenbaum." ],
      "venue" : "Information processing & management, 51(5):570–594.",
      "citeRegEx" : "Abacha and Zweigenbaum.,? 2015",
      "shortCiteRegEx" : "Abacha and Zweigenbaum.",
      "year" : 2015
    }, {
      "title" : "The fifth pascal recognizing textual entailment challenge",
      "author" : [ "Luisa Bentivogli", "Peter Clark", "Ido Dagan", "Danilo Giampiccolo." ],
      "venue" : "TAC.",
      "citeRegEx" : "Bentivogli et al\\.,? 2009",
      "shortCiteRegEx" : "Bentivogli et al\\.",
      "year" : 2009
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "An cnn-lstm attention approach to understanding user query intent from online health communities",
      "author" : [ "Ruichu Cai", "Binjun Zhu", "Lei Ji", "Tianyong Hao", "Jun Yan", "Wenyin Liu." ],
      "venue" : "2017 IEEE International Conference on Data Mining Workshops",
      "citeRegEx" : "Cai et al\\.,? 2017",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2017
    }, {
      "title" : "Understanding user intent in community question answering",
      "author" : [ "Long Chen", "Dell Zhang", "Levene Mark." ],
      "venue" : "Proceedings of the 21st international conference on world wide web, pages 823–828.",
      "citeRegEx" : "Chen et al\\.,? 2012",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2012
    }, {
      "title" : "Meddialog: a large-scale medical dialogue dataset",
      "author" : [ "Shu Chen", "Zeqian Ju", "Xiangyu Dong", "Hongchao Fang", "Sicheng Wang", "Yue Yang", "Jiaqi Zeng", "Ruisi Zhang", "Ruoyu Zhang", "Meng Zhou", "Penghui Zhu", "Pengtao Xie." ],
      "venue" : "arXiv preprint arXiv:2004.03329.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "The pascal recognising textual entailment challenge",
      "author" : [ "Ido Dagan", "Oren Glickman", "Bernardo Magnini." ],
      "venue" : "Machine Learning Challenges Workshop, pages 177–190. Springer.",
      "citeRegEx" : "Dagan et al\\.,? 2005",
      "shortCiteRegEx" : "Dagan et al\\.",
      "year" : 2005
    }, {
      "title" : "Recognizing textual entailment: Models and applications",
      "author" : [ "Ido Dagan", "Dan Roth", "Mark Sammons", "Fabio Massimo Zanzotto." ],
      "venue" : "Synthesis Lectures on Human Language Technologies, 6(4):1–220.",
      "citeRegEx" : "Dagan et al\\.,? 2013",
      "shortCiteRegEx" : "Dagan et al\\.",
      "year" : 2013
    }, {
      "title" : "Consumer health information and question answering: helping consumers find answers to their health-related information needs",
      "author" : [ "Dina Demner-Fushman", "Yassine Mrabet", "Asma Ben Abacha." ],
      "venue" : "Journal of the American Medical Informatics Asso-",
      "citeRegEx" : "Demner.Fushman et al\\.,? 2020",
      "shortCiteRegEx" : "Demner.Fushman et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Ranking generated summaries by correctness: An interesting but challenging application for natural language inference",
      "author" : [ "Tobias Falke", "Leonardo FR Ribeiro", "Prasetya Ajie Utama", "Ido Dagan", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 57th Annual",
      "citeRegEx" : "Falke et al\\.,? 2019",
      "shortCiteRegEx" : "Falke et al\\.",
      "year" : 2019
    }, {
      "title" : "The third pascal recognizing textual entailment challenge",
      "author" : [ "Danilo Giampiccolo", "Bernardo Magnini", "Ido Dagan", "Bill Dolan." ],
      "venue" : "Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pages 1–9.",
      "citeRegEx" : "Giampiccolo et al\\.,? 2007",
      "shortCiteRegEx" : "Giampiccolo et al\\.",
      "year" : 2007
    }, {
      "title" : "Studies on the Semantics of Questions and the Pragmatics of Answers",
      "author" : [ "Jeroen Antonius Gerardus Groenendijk", "Martin Johan Bastiaan Stokhof." ],
      "venue" : "Ph.D. thesis, Univ. Amsterdam.",
      "citeRegEx" : "Groenendijk and Stokhof.,? 1984",
      "shortCiteRegEx" : "Groenendijk and Stokhof.",
      "year" : 1984
    }, {
      "title" : "Soft layer-specific multi-task summarization with entailment and question generation",
      "author" : [ "Han Guo", "Ramakanth Pasunuru", "Mohit Bansal." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
      "citeRegEx" : "Guo et al\\.,? 2018",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2018
    }, {
      "title" : "Text summarization through entailment-based minimum vertex cover",
      "author" : [ "Anand Gupta", "Manpreet Kaur", "Shachar Mirkin", "Adarsh Singh", "Aseem Goyal." ],
      "venue" : "Proceedings of the Third Joint Conference on Lexical and Computational Semantics (* SEM 2014),",
      "citeRegEx" : "Gupta et al\\.,? 2014",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2014
    }, {
      "title" : "The second pascal recognising textual entailment challenge",
      "author" : [ "R Bar Haim", "Ido Dagan", "Bill Dolan", "Lisa Ferro", "Danilo Giampiccolo", "Bernardo Magnini", "Idan Szpektor." ],
      "venue" : "Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual",
      "citeRegEx" : "Haim et al\\.,? 2006",
      "shortCiteRegEx" : "Haim et al\\.",
      "year" : 2006
    }, {
      "title" : "Clinicalbert: Modeling clinical notes and predicting hospital readmission",
      "author" : [ "Kexin Huang", "Jaan Altosaar", "Rajesh Ranganath." ],
      "venue" : "arXiv preprint arXiv:1904.05342.",
      "citeRegEx" : "Huang et al\\.,? 2019",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "First quora dataset release: Question pairs",
      "author" : [ "Shankar Iyer", "Nikhil Dandekar", "Kornél Csernai" ],
      "venue" : null,
      "citeRegEx" : "Iyer et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Iyer et al\\.",
      "year" : 2017
    }, {
      "title" : "Mimiciii, a freely accessible critical care database",
      "author" : [ "Alistair EW Johnson", "Tom J Pollard", "Lu Shen", "H Lehman Li-Wei", "Mengling Feng", "Mohammad Ghassemi", "Benjamin Moody", "Peter Szolovits", "Leo Anthony Celi", "Roger G Mark." ],
      "venue" : "Scien-",
      "citeRegEx" : "Johnson et al\\.,? 2016",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2016
    }, {
      "title" : "Voice-based conversational agents for older adults",
      "author" : [ "Janet Johnson", "Khalil Mrini", "Allison Moore", "Emilia Farkas", "Ndapa Nkashole", "Michael Hogarth", "Nadir Weibel." ],
      "venue" : "Proceedings of the CHI 2020 Workshop on Conversational Agents for Health",
      "citeRegEx" : "Johnson et al\\.,? 2020",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2020
    }, {
      "title" : "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
      "author" : [ "Jinhyuk Lee", "Wonjin Yoon", "Sungdong Kim", "Donghyeon Kim", "Sunkyu Kim", "Chan Ho So", "Jaewoo Kang." ],
      "venue" : "Bioinformatics, 36(4):1234–1240.",
      "citeRegEx" : "Lee et al\\.,? 2020",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2020
    }, {
      "title" : "Expanding query answers on medical knowledge bases",
      "author" : [ "Chuan Lei", "Vasilis Efthymiou", "Rebecca Geis", "Fatma Ozcan." ],
      "venue" : "EDBT, pages 567– 578.",
      "citeRegEx" : "Lei et al\\.,? 2020",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2020
    }, {
      "title" : "Bart: Denoising sequence-to-sequence pre-training for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Ves Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2019
    }, {
      "title" : "Ensure the correctness of the summary: Incorporate entailment knowledge into abstractive sentence summarization",
      "author" : [ "Haoran Li", "Junnan Zhu", "Jiajun Zhang", "Chengqing Zong." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguis-",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Rouge: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text summarization branches out, pages 74–81.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Multi-task deep neural networks for natural language understanding",
      "author" : [ "Xiaodong Liu", "Pengcheng He", "Weizhu Chen", "Jianfeng Gao." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4487–4496.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "A text summarization approach under the influence of textual entailment",
      "author" : [ "Elena Lloret", "Oscar Ferrández", "Rafael Munoz", "Manuel Palomar." ],
      "venue" : "NLPCS, pages 22–31.",
      "citeRegEx" : "Lloret et al\\.,? 2008",
      "shortCiteRegEx" : "Lloret et al\\.",
      "year" : 2008
    }, {
      "title" : "Domainrelevant embeddings for medical question similarity",
      "author" : [ "Clara McCreery", "Namit Katariya", "Anitha Kannan", "Manish Chablani", "Xavier Amatriain." ],
      "venue" : "arXiv preprint arXiv:1910.04192.",
      "citeRegEx" : "McCreery et al\\.,? 2019",
      "shortCiteRegEx" : "McCreery et al\\.",
      "year" : 2019
    }, {
      "title" : "Abstractive meeting summarization with entailment and fusion",
      "author" : [ "Yashar Mehdad", "Giuseppe Carenini", "Frank Tompa", "Raymond Ng." ],
      "venue" : "Proceedings of the 14th European Workshop on Natural Language Generation, pages 136–146.",
      "citeRegEx" : "Mehdad et al\\.,? 2013",
      "shortCiteRegEx" : "Mehdad et al\\.",
      "year" : 2013
    }, {
      "title" : "Medical question understanding and answering for older adults",
      "author" : [ "Khalil Mrini", "Chen Chen", "Ndapa Nakashole", "Nadir Weibel", "Emilia Farcas." ],
      "venue" : "The 3rd Southern California (SoCal) NLP Symposium.",
      "citeRegEx" : "Mrini et al\\.,? 2021a",
      "shortCiteRegEx" : "Mrini et al\\.",
      "year" : 2021
    }, {
      "title" : "Joint summarization-entailment optimization for consumer health question understanding",
      "author" : [ "Khalil Mrini", "Franck Dernoncourt", "Walter Chang", "Emilia Farcas", "Ndapa Nakashole." ],
      "venue" : "Proceedings of the Second Workshop on Natural Language",
      "citeRegEx" : "Mrini et al\\.,? 2021b",
      "shortCiteRegEx" : "Mrini et al\\.",
      "year" : 2021
    }, {
      "title" : "UCSD-adobe at MEDIQA 2021: Transfer learning and answer sentence selection for medical summarization",
      "author" : [ "Khalil Mrini", "Franck Dernoncourt", "Seunghyun Yoon", "Trung Bui", "Walter Chang", "Emilias Farcas", "Ndapa Nakashole." ],
      "venue" : "Pro-",
      "citeRegEx" : "Mrini et al\\.,? 2021c",
      "shortCiteRegEx" : "Mrini et al\\.",
      "year" : 2021
    }, {
      "title" : "Abstractive text summarization using sequence-to-sequence RNNs and beyond",
      "author" : [ "Ramesh Nallapati", "Bowen Zhou", "Cicero dos Santos", "Çağlar GuÌ‡lçehre", "Bing Xiang" ],
      "venue" : "In Proceedings of The 20th SIGNLL Conference on Computational Natural Lan-",
      "citeRegEx" : "Nallapati et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nallapati et al\\.",
      "year" : 2016
    }, {
      "title" : "Don’t give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization",
      "author" : [ "Shashi Narayan", "Shay B. Cohen", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Narayan et al\\.,? 2018",
      "shortCiteRegEx" : "Narayan et al\\.",
      "year" : 2018
    }, {
      "title" : "Multireward reinforced summarization with saliency and entailment",
      "author" : [ "Ramakanth Pasunuru", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Pasunuru and Bansal.,? 2018",
      "shortCiteRegEx" : "Pasunuru and Bansal.",
      "year" : 2018
    }, {
      "title" : "Towards improving abstractive summarization via entailment generation",
      "author" : [ "Ramakanth Pasunuru", "Han Guo", "Mohit Bansal." ],
      "venue" : "Proceedings of the Workshop on New Frontiers in Summarization, pages 27–32.",
      "citeRegEx" : "Pasunuru et al\\.,? 2017",
      "shortCiteRegEx" : "Pasunuru et al\\.",
      "year" : 2017
    }, {
      "title" : "Squad: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Information structure in discourse: Towards an integrated formal theory of pragmatics",
      "author" : [ "Craige Roberts" ],
      "venue" : null,
      "citeRegEx" : "Roberts.,? \\Q1996\\E",
      "shortCiteRegEx" : "Roberts.",
      "year" : 1996
    }, {
      "title" : "Interactive use of online health resources: a comparison of consumer and professional questions",
      "author" : [ "Kirk Roberts", "Dina Demner-Fushman." ],
      "venue" : "Journal of the American Medical Informatics Association, 23(4):802–811.",
      "citeRegEx" : "Roberts and Demner.Fushman.,? 2016",
      "shortCiteRegEx" : "Roberts and Demner.Fushman.",
      "year" : 2016
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J Liu", "Christopher D Manning." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073–",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "Glue: A multi-task benchmark and analysis platform for natural language understanding",
      "author" : [ "Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: An-",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "An attention-based multi-task model for named entity recognition and intent analysis of chinese online medical questions",
      "author" : [ "Chaochen Wu", "Guan Luo", "Chao Guo", "Yin Ren", "Anni Zheng", "Cheng Yang." ],
      "venue" : "Journal of Biomedical Informatics, 108:103511.",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "Medical question similarity calculation based on weighted domain dictionary",
      "author" : [ "Guokai Yan", "Jianqiang Li." ],
      "venue" : "Proceedings of the 2018 International Conference on Big Data and Computing, pages 104– 107.",
      "citeRegEx" : "Yan and Li.,? 2018",
      "shortCiteRegEx" : "Yan and Li.",
      "year" : 2018
    }, {
      "title" : "Dut-nlp at mediqa 2019: an adversarial multi-task network to jointly model recognizing question entailment and question answering",
      "author" : [ "Huiwei Zhou", "Xuefei Li", "Weihong Yao", "Chengkun Lang", "Shixian Ning." ],
      "venue" : "Proceedings of the 18th BioNLP",
      "citeRegEx" : "Zhou et al\\.,? 2019",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2019
    }, {
      "title" : "Panlp at mediqa 2019: Pre-trained language models, transfer learning and knowledge distillation",
      "author" : [ "Wei Zhu", "Xiaofeng Zhou", "Keqiang Wang", "Xun Luo", "Xiepeng Li", "Yuan Ni", "Guotong Xie." ],
      "venue" : "Proceedings of the 18th BioNLP Workshop and Shared Task,",
      "citeRegEx" : "Zhu et al\\.,? 2019",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "In order to retrieve relevant answers, one of the basic steps in Question Answering (QA) systems is understanding the intent of questions (Chen et al., 2012; Cai et al., 2017).",
      "startOffset" : 138,
      "endOffset" : 175
    }, {
      "referenceID" : 11,
      "context" : "In order to retrieve relevant answers, one of the basic steps in Question Answering (QA) systems is understanding the intent of questions (Chen et al., 2012; Cai et al., 2017).",
      "startOffset" : 138,
      "endOffset" : 175
    }, {
      "referenceID" : 49,
      "context" : "This is particularly important for medical QA systems (Wu et al., 2020), as consumer health questions – questions asked by patients – may use a vocabulary distinct from doctors to describe similar health concepts (Ben Abacha and Demner-Fushman, 2019a).",
      "startOffset" : 54,
      "endOffset" : 71
    }, {
      "referenceID" : 46,
      "context" : "Consumer health questions may also contain peripheral information like patient history (Roberts and Demner-Fushman, 2016), that are not necessary to answer questions.",
      "startOffset" : 87,
      "endOffset" : 121
    }, {
      "referenceID" : 29,
      "context" : "ation (Ben Abacha and Zweigenbaum, 2015; Lei et al., 2020), question entailment (Ben Abacha and Demner-Fushman, 2016, 2019b; Agrawal et al.",
      "startOffset" : 6,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : ", 2020), question entailment (Ben Abacha and Demner-Fushman, 2016, 2019b; Agrawal et al., 2019), question summarization (Ben Abacha and Demner-Fushman, 2019a), and question similarity (Ben Abacha and Demner-Fushman, 2017; Yan and",
      "startOffset" : 29,
      "endOffset" : 95
    }, {
      "referenceID" : 38,
      "context" : "We find in initial experiments (Mrini et al., 2021b) that RQE can teach question summarizers to distinguish salient information from peripheral details, and likewise that question summarization can benefit RQE classifiers.",
      "startOffset" : 31,
      "endOffset" : 52
    }, {
      "referenceID" : 43,
      "context" : "Previous work on combining summarization and entailment uses at least 2 datasets – 1 from each task (Pasunuru et al., 2017; Guo et al., 2018).",
      "startOffset" : 100,
      "endOffset" : 141
    }, {
      "referenceID" : 21,
      "context" : "Previous work on combining summarization and entailment uses at least 2 datasets – 1 from each task (Pasunuru et al., 2017; Guo et al., 2018).",
      "startOffset" : 100,
      "endOffset" : 141
    }, {
      "referenceID" : 38,
      "context" : "This equivalence is the inspiration behind the data augmentation schemes introduced in our previous work (Mrini et al., 2021b).",
      "startOffset" : 105,
      "endOffset" : 126
    }, {
      "referenceID" : 20,
      "context" : ", 2005, 2013), and early definitions of question entailment (Groenendijk and Stokhof, 1984; Roberts, 1996).",
      "startOffset" : 60,
      "endOffset" : 106
    }, {
      "referenceID" : 45,
      "context" : ", 2005, 2013), and early definitions of question entailment (Groenendijk and Stokhof, 1984; Roberts, 1996).",
      "startOffset" : 60,
      "endOffset" : 106
    }, {
      "referenceID" : 34,
      "context" : "There is a growing body of work combining summarization and entailment (Lloret et al., 2008; Mehdad et al., 2013; Gupta et al., 2014).",
      "startOffset" : 71,
      "endOffset" : 133
    }, {
      "referenceID" : 36,
      "context" : "There is a growing body of work combining summarization and entailment (Lloret et al., 2008; Mehdad et al., 2013; Gupta et al., 2014).",
      "startOffset" : 71,
      "endOffset" : 133
    }, {
      "referenceID" : 22,
      "context" : "There is a growing body of work combining summarization and entailment (Lloret et al., 2008; Mehdad et al., 2013; Gupta et al., 2014).",
      "startOffset" : 71,
      "endOffset" : 133
    }, {
      "referenceID" : 47,
      "context" : "(2018) introduce a pointer-generator summarization model with coverage loss (See et al., 2017).",
      "startOffset" : 76,
      "endOffset" : 94
    }, {
      "referenceID" : 17,
      "context" : "There are pretrained language models that are geared towards BioNLP applications, that are based on BERT (Devlin et al., 2019).",
      "startOffset" : 105,
      "endOffset" : 126
    }, {
      "referenceID" : 2,
      "context" : "Those include SciBERT (Beltagy et al., 2019) which has been fine-tuned using biomedical text from PubMed.",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 28,
      "context" : "BioBERT (Lee et al., 2020) has been finetuned on the PMC dataset, whereas models named ClinicalBERT (Huang et al.",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 24,
      "context" : ", 2020) has been finetuned on the PMC dataset, whereas models named ClinicalBERT (Huang et al., 2019; Alsentzer et al., 2019) additionally use the MIMIC III dataset (Johnson et al.",
      "startOffset" : 81,
      "endOffset" : 125
    }, {
      "referenceID" : 1,
      "context" : ", 2020) has been finetuned on the PMC dataset, whereas models named ClinicalBERT (Huang et al., 2019; Alsentzer et al., 2019) additionally use the MIMIC III dataset (Johnson et al.",
      "startOffset" : 81,
      "endOffset" : 125
    }, {
      "referenceID" : 26,
      "context" : ", 2019) additionally use the MIMIC III dataset (Johnson et al., 2016).",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 16,
      "context" : "The question answering task involved re-ranking answers, not generating them (Demner-Fushman et al., 2020).",
      "startOffset" : 77,
      "endOffset" : 106
    }, {
      "referenceID" : 52,
      "context" : "For the RQE task, the best-performing model (Zhu et al., 2019) uses transfer learning on NLI and ensemble methods.",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 43,
      "context" : "Previous work on multi-task learning with summarization and entailment (Pasunuru et al., 2017; Guo et al., 2018) optimize for the objectives of the different tasks by alternating between them.",
      "startOffset" : 71,
      "endOffset" : 112
    }, {
      "referenceID" : 21,
      "context" : "Previous work on multi-task learning with summarization and entailment (Pasunuru et al., 2017; Guo et al., 2018) optimize for the objectives of the different tasks by alternating between them.",
      "startOffset" : 71,
      "endOffset" : 112
    }, {
      "referenceID" : 30,
      "context" : "Whereas many previous multi-task settings chose generation tasks (entailment generation and question generation), we choose the BART Large architecture (Lewis et al., 2019) as it enables to optimize for a classification task (RQE) and a generation task (summarization) using the same architecture.",
      "startOffset" : 152,
      "endOffset" : 172
    }, {
      "referenceID" : 48,
      "context" : "(2019) introduce MT-DNN and show that hard parameter-sharing of all of the transformer encoder layers, and only having task-specific classification heads produces results that set a new state of the art for the GLUE benchmark (Wang et al., 2018).",
      "startOffset" : 226,
      "endOffset" : 245
    }, {
      "referenceID" : 13,
      "context" : "the large-scale MedDialog dataset (Chen et al., 2020).",
      "startOffset" : 34,
      "endOffset" : 53
    }, {
      "referenceID" : 41,
      "context" : "We use the XSum dataset (Narayan et al., 2018), an abstractive summarization benchmark, for question summarization.",
      "startOffset" : 24,
      "endOffset" : 46
    }, {
      "referenceID" : 14,
      "context" : "For the RQE task, we use the Recognizing Textual Entailment (RTE) dataset (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) from the GLUE benchmark (Wang et al.",
      "startOffset" : 74,
      "endOffset" : 164
    }, {
      "referenceID" : 23,
      "context" : "For the RQE task, we use the Recognizing Textual Entailment (RTE) dataset (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) from the GLUE benchmark (Wang et al.",
      "startOffset" : 74,
      "endOffset" : 164
    }, {
      "referenceID" : 19,
      "context" : "For the RQE task, we use the Recognizing Textual Entailment (RTE) dataset (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) from the GLUE benchmark (Wang et al.",
      "startOffset" : 74,
      "endOffset" : 164
    }, {
      "referenceID" : 9,
      "context" : "For the RQE task, we use the Recognizing Textual Entailment (RTE) dataset (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) from the GLUE benchmark (Wang et al.",
      "startOffset" : 74,
      "endOffset" : 164
    }, {
      "referenceID" : 32,
      "context" : "The R1, R2 and RL metrics refer to the F1 scores of ROUGE-1, ROUGE-2 and ROUGE-L (Lin, 2004).",
      "startOffset" : 81,
      "endOffset" : 92
    }, {
      "referenceID" : 30,
      "context" : "The first one is BART (Lewis et al., 2019), where we only train on the summarization task.",
      "startOffset" : 22,
      "endOffset" : 42
    }, {
      "referenceID" : 10,
      "context" : "(2017), using alternative training with entailment generation on the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) and having a shared decoder and taskspecific encoders.",
      "startOffset" : 119,
      "endOffset" : 140
    }, {
      "referenceID" : 44,
      "context" : "(2018), where, on top of the entailment generation task, we add the question generation task using the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), and all parameters are soft-shared, except for the task-specific first encoder layer and last decoder layer.",
      "startOffset" : 147,
      "endOffset" : 171
    }, {
      "referenceID" : 25,
      "context" : "For data augmentation, they use semantically-selected relevant question pairs from the Quora Question Pairs dataset (Iyer et al., 2017).",
      "startOffset" : 116,
      "endOffset" : 135
    }, {
      "referenceID" : 47,
      "context" : "Their results show that coverage loss (See et al., 2017) diminishes the added value of data augmentation in pointer-generator networks.",
      "startOffset" : 38,
      "endOffset" : 56
    }, {
      "referenceID" : 51,
      "context" : "The third baseline (Zhou et al., 2019) is an adversarial MTL method combining medical question answering and RQE.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 28,
      "context" : "The architecture consists of a shared transformer encoder using BioBERT embeddings (Lee et al., 2020), separate classification heads for RQE and medical QA, and a task discriminator for adversarial training.",
      "startOffset" : 83,
      "endOffset" : 101
    }, {
      "referenceID" : 47,
      "context" : "Pointer-Generator Networks (PG) (See et al., 2017) 35.",
      "startOffset" : 32,
      "endOffset" : 50
    }, {
      "referenceID" : 51,
      "context" : "BioBERT + Adversarial MTL with Medical QA (Zhou et al., 2019) 63.",
      "startOffset" : 42,
      "endOffset" : 61
    }, {
      "referenceID" : 37,
      "context" : "This work is part of the VOLI project (Mrini et al., 2021a; Johnson et al., 2020).",
      "startOffset" : 38,
      "endOffset" : 81
    }, {
      "referenceID" : 27,
      "context" : "This work is part of the VOLI project (Mrini et al., 2021a; Johnson et al., 2020).",
      "startOffset" : 38,
      "endOffset" : 81
    } ],
    "year" : 2021,
    "abstractText" : "Users of medical question answering systems often submit long and detailed questions, making it hard to achieve high recall in answer retrieval. To alleviate this problem, we propose a novel Multi-Task Learning (MTL) method with data augmentation for medical question understanding. We first establish an equivalence between the tasks of question summarization and Recognizing Question Entailment (RQE) using their definitions in the medical domain. Based on this equivalence, we propose a data augmentation algorithm to use just one dataset to optimize for both tasks, with a weighted MTL loss. We introduce gradually soft parameter-sharing: a constraint for decoder parameters to be close, that is gradually loosened as we move to the highest layer. We show through ablation studies that our proposed novelties improve performance. Our method outperforms existing MTL methods across 4 datasets of medical question pairs, in ROUGE scores, RQE accuracy and human evaluation. Finally, we show that our method fares better than single-task learning under 4 low-resource settings.",
    "creator" : "LaTeX with hyperref"
  }
}