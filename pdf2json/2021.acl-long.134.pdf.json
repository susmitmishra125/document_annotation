{
  "name" : "2021.acl-long.134.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Addressing Contradictions in Dialogue Modeling",
    "authors" : [ "Yixin Nie", "Mary Williamson", "Mohit Bansal", "Douwe Kiela", "Jason Weston" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1699–1713\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1699"
    }, {
      "heading" : "1 Introduction",
      "text" : "Recent progress on neural approaches to natural language processing (Devlin et al., 2019; Brown et al., 2020), and the availability of large amounts of conversational data (Lowe et al., 2015; Smith et al., 2020) have triggered a resurgent interest on building intelligent open-domain chatbots. Newly developed end-to-end neural bots (Zhang et al., 2020; Adiwardana et al., 2020; Roller et al., 2020) are claimed to be superior to their predecessors (Worsnick, 2018; Zhou et al., 2020) using various human evaluation techniques (See et al., 2019; Li et al., 2019; Adiwardana et al., 2020) that aim to give a more accurate measure of what makes a good conversation. While the success is indisputable, there is still a long way to go before we\n∗* Dolphins are mammals, not fish.\narrive at human-like open-domain chatbots. For example, it has been shown that open-domain chatbots frequently generate annoying errors (Adiwardana et al., 2020; Roller et al., 2020) and a notorious one among these is the class of contradiction, or consistency errors.\nWhen interacting with chatbots, people carry over many of the same expectations as when interacting with humans (Nass and Moon, 2000). Selfcontradictions by these bots (see Fig.1, bottom) are often jarring, immediately disrupt the conver-\nsational flow, and help support arguments about whether generative models could ever really understand what they are saying at all (Marcus, 2018). From a listener’s perspective, such inconsistent bots fail to gain user trust and their long-term communication confidence. From a speaker’s perspective, it violates the maxim of quality in Grice’s cooperative principles (Grice, 1975) —”Do not say what you believe to be false.” Hence, efforts on reducing contradicting or inconsistent conversations by open-domain chatbots are imperative.\nPrior works (Welleck et al., 2019) characterized the modeling of persona-related consistency as a natural language inference (NLI) problem (Dagan et al., 2005; Bowman et al., 2015), and constructed a dialog NLI dataset based on Persona-Chat (Zhang et al., 2018), but so far state-of-the-art chatbots (Roller et al., 2020) have not been able to make use of NLI techniques in improving dialogue consistency. Overall, the challenge remains that we are still unable to answer the simple yet important question—“how good are we at modeling consistency (including persona, logic, causality, etc.) in a general conversation?”. The inability to measure this obscures to what degree building new modules or techniques can in turn help prevent contradicting responses during generation.\nSeeking to answer this question, we introduce the DialoguE COntradiction DEtection task (DECODE)1 and collect a new conversational dataset containing human written dialogues where one of the speakers deliberately contradicts what they have previously said at a certain point during the conversation. We also collect an out-of-distribution (OOD) set of dialogues in human-bot interactive settings which contain human-labeled selfcontradictions made by different chatbots.\nWe then compare a set of state-of-the-art systems, including a standard unstructured approach and a proposed structured approach for utilizing NLI models to detect contradictions. In the unstructured approach, a Transformer NLI model directly takes in the concatenation of all utterances of the input dialogue for prediction, following the paradigm of NLU modeling. In the structured approach, utterances are paired separately before being fed into Transformer NLI models, explicitly taking account of the natural dialogue structure.\nResults reveal that: (1) our newly collected\n1DECODE dataset and code are publicly available at https://parl.ai/projects/contradiction.\ndataset is notably more effective at providing supervision for the contradiction detection task than existing NLI data including those aimed at covering the dialogue domain; (2) the structured utterancebased approach for dialogue consistency modeling is more robust in our analysis and more transferable to OOD human-bot conversation than the unstructured approach. This finding challenges the mainstream unstructured approach of simply applying pre-trained Transformer models and expecting them to learn the structure, especially for OOD scenarios which are often the case when incorporating NLU modules into NLG systems, since intermediate in-domain data are scarce.\nFinally, with such improvements on the contradiction detection task, we show that our best resulting detector correlates well with human judgments and can be suitable for use as an automatic metric for checking dialogue consistency. We further provide evidence for its usage in improving the consistency of state-of-the-art generative chatbots."
    }, {
      "heading" : "2 Related Work",
      "text" : "Several prior works on improving dialogue consistency have explored using direct modeling of the dialogue context in generation algorithms. The modeling can be implicit where the dialogue consistency-related information like style (Wang et al., 2017), topics, or personal facts are maintained in distributed embeddings (Li et al., 2016; Zhang et al., 2019a), neural long-term memories (Bang et al., 2015), hierarchical neural architecture (Serban et al., 2016), latent variables (Serban et al., 2017), topical attention (Dziri et al., 2019a), or even self-learned feature vectors (Zhang et al., 2019b). Some works have grounded generation models on explicit user input (Qian et al., 2018), or designated personas (Zhang et al., 2018). Although, improvements on automatic generation metrics were often shown on guided response generation based on the consistency modeling, the issue of contradiction has never been resolved, nor have generally applicable methods to gauge the consistency improvements been developed. Further, simply scaling models has not made the problem go away, as is evident in the largest chatbots trained such as BlenderBot with up to 9.4B parameter Transformers (Roller et al., 2020).\nMore similar to our work is utilizing NLI models in dialogue consistency. Dziri et al. (2019b) attempted to use entailment models trained on syn-\nthetic datasets for dialogue topic coherence evaluation. Particularly, Welleck et al. (2019) constructed the dialogue NLI dataset and (Li et al., 2020) utilized it to try to reduce inconsistency in generative models via unlikelihood training in a preliminary study that reports perplexity results, but did not measure actual generations or contradiction rates. We note that the dialogue NLI dataset is only semi-automatically generated, with limited coverage of only Persona-chat data (Zhang et al., 2018), whereas our DECODE is human-written and across multiple domains. Our task also involves logical and context-related reasoning beyond personal facts. We show that transfer of DECODE is subsequently more robust than dialogue NLI on both human-human and human-bot chats."
    }, {
      "heading" : "3 Task and Data",
      "text" : ""
    }, {
      "heading" : "3.1 Dialogue Contradiction Detection",
      "text" : "We formalize dialogue contradiction detection as a supervised classification task. The input of the task is a list of utterances x = {u0, u1, u2, ..., un} representing a dialogue or a dialogue snippet. The output is y, indicating whether the last utterance un contradicts any previously conversed information contained in the dialogue {u0, u1, ..., un−1}, where y can be 0 or 1 corresponding to the noncontradiction and the contradiction label respectively. Preferably, the output should also include a set of indices I ⊆ {0, 1, ..., n − 1} representing a subset of {u0, u1, ..., un−1} which contain information that is actually contradicted by the last utterance un. The extra indices I output require models to pinpoint the evidence for the contradiction, providing an extra layer of explainability."
    }, {
      "heading" : "3.2 Data Collection",
      "text" : "Our goal is first to collect training and evaluation data for this task. We thus collect dialogues in which the last utterance contradicts some previous utterances in the dialogue history. To obtain such dialogues, we give annotators dialogue snippets from pre-selected dialogue corpora, and then ask them to continue the conversation by writing one or two utterances such that the last utterance by the last speaker contradicts the dialogue history. We also ask annotators to mark all the utterances in the dialogue history that are involved in the contradiction as supporting evidence. We ask annotators to write contradicting utterances based partly on existing dialogues rather than collecting new dialogue\nfrom scratch because the provided dialogues can often convey semantic-rich contexts from different domains and inspire annotators to write more diverse examples. We don’t impose constraints on the annotation such that the annotator could have the flexibility to write more diverse contradictory responses that might not belong to pre-defined types (knowledge, emotion, persona, etc). Also note that we ask the annotator to write contradictory dialogues based on pre-selected human-human dialogue rather than collecting dialogues from humanbot interaction for the main dataset because we want the examples to be general and less bound to specific bots.2 We crowdsource the continuation and annotation data with Amazon Mechanical Turk via ParlAI (Miller et al., 2017).\nTo ensure data quality, we apply three techniques: (i) an onboarding test every annotator has to pass to contribute examples; (ii) each annotator can only create up to 20 examples; and (iii) all examples in the validation and test set are verified by asking 3 additional workers. More details about annotation are provided in Appendix."
    }, {
      "heading" : "3.3 Dataset",
      "text" : "We collected 17,713 human-written contradicting dialogues in which 4,121 are verified by 3 annotators. The pre-selected dialogue source corpora are Wizard of Wikipedia (Dinan et al., 2019), EMPATHETICDIALOGUES (Rashkin et al., 2019), Blended Skill Talk (Smith et al., 2020), and ConvAI2 (Dinan et al., 2020), covering various conversational topics. To facilitate the evaluation of consistency modeling on the dialogue contradiction detection classification task, we sample an equal number of non-contradicting dialogues according to the same dialogue length distribution as the contradicting ones from the same dialogue corpus. Then, we make the splits such that the train split contains unverified examples, and dev and test splits only contain verified examples. Each split has balanced labels between contradiction and non-contradiction. The breakdown of each of the dataset sources is shown in Appendix.\nAuxiliary (Checklist) Test Sets. We further create two auxiliary checklist evaluation sets by transforming the contradiction examples in the original test in two ways such that the ground truth label is\n2Alongside the main dataset, another portion of the examples are collected via human-bot interaction and used as out-of-domain evaluation.\neither invariant or expected to flip. The two resultant sets serve as diagnostic tests on the behavior, generalization and transferability of our models.\nThe transformations are described below:\n• Add Two Turns (A2T) We insert a pair of randomly sampled utterances into the dialogue such that the inserted utterances are between the two original contradicting utterances. This gives a new contradicting dialogue with a longer dialogue history.\n• Remove Contradicting Turns (RCT) We remove all the turns (all pairs of utterances)3\nmarked as supporting evidence for the contradiction in the dialogue except the last utterance. This results in a new non-contradiction dialogue.\nHuman-Bot Test Set. Our main dataset involves human-written dialogues containing contradicting utterances based on human-human dialogues from existing corpora. In practice, to evaluate the response quality of a machine rather than a human in terms of its consistent responses, we care about how well a contradiction detector can perform in humanbot interactive conversations. To that end, we further collect human-bot dialogue data by employing crowdworkers to interact with a diverse set of opendomain bots. These include Poly-encoder (Humeau et al., 2019) based retrieval models, generative models (Roller et al., 2020), unlikelihood trained models (Li et al., 2020), retrieve-and-refine models (Weston et al., 2018; Roller et al., 2020), models either pre-trained on a previously existing Reddit dataset\n3The dataset dialogues involve two speakers taking turns speaking. To maintain this structure, for each marked utterance, we remove a pair of utterances that represents a turn. This also helps remove information involved in the contradiction such that the new label should be “non-contradiction”.\nextracted and obtained by a third party that was hosted by pushshift.io (Baumgartner et al., 2020) or fine-tuned on the Blended Skill Talk (BST) dialogue tasks (Smith et al., 2020) – that is, all the dialogue models that are compared in the study in Roller et al. (2020). During the collection, if the bot generates an utterance that contradicts itself, we ask the worker to mark the utterance. In some of the dialogues, workers are explicitly instructed to goad the bots into making contradicting utterances. The final human-bot test set we derive contains 764 dialogues, half of which end with a contradicting utterance by the bot. All the dialogues in the set, with either contradiction or non-contradiction labels, are verified by 3 additional annotators, beside the human who actually talked to the bot.\nThe auxiliary and human-bot test sets aim to test models’ robustness and generalizability beyond the collected human-written test set (Ribeiro et al., 2020; Gardner et al., 2020), and give a more comprehensive analysis of the task. Table 1 summarizes the final overall dataset. Examples are provided for each dataset type in Fig. 1 and Appendix Table 5."
    }, {
      "heading" : "4 Models",
      "text" : "To model the dialogue consistency task, we first employ some of the techniques used in NLI sequenceto-label modeling, where the input is a pair of textual sequences and the output is a label. The benefit of such modeling is that we can directly make use of existing NLI datasets during training. However, unlike previous work (Welleck et al., 2019) that directly utilized NLI models giving a 3-way output among “entailment”, “contradiction”, and “neutral”, we modify the model with a 2-way output between “contradiction” and “non-contradiction” (either “entailment” or “neutral”) labels, as our task is centered around the detection of inconsistency.\nMore formally, we denote the model as ŷpred = fθ(C, u), where ŷpred is the prediction of the label y, i.e. whether the textual response u contradicts some textual context C = {u0, u1, ..., un−1}, and θ are the parameters of the model."
    }, {
      "heading" : "4.1 Dialogue Contradiction Detectors",
      "text" : "We explore two distinct approaches that propose differing fθ for the detection prediction problem.\nUnstructured Approach. In this approach, we simply concatenate all the previous utterances in the dialogue history to form a single textual context. Then, we apply fθ to the context and the last\nutterance to infer the probability of contradiction:\nŷpred = fθ([u0, u1, u2, ..., un−1], un) (1)\nWhen concatenating the utterances, we insert special tokens before each utterance to indicate the speaker of that utterance. This is aimed to provide a signal of the dialogue structure to the models. Still, this approach assumes that the model can use these features adequately to learn the underlying structure of the dialogue implicitly during training.\nStructured Utterance-based Approach. Since the reasoning crucially depends on the last utterance, in this method we first choose all the utterances by the last speaker to form a set S. We then pair every utterance in the set with the last utterance and feed them one by one into fUBθ . The final contradiction probability is the maximum over all the outputs:\nŷpred = max { fUBθ (ui, un) : ui ∈ S } (2)\nAdditionally, the utterance-based approach is able to give a set of utterances as supporting evidence for a contradiction decision by choosing the pairs having contradiction probability higher than a threshold ηe:\nI = { i : fUBθ (ui, un) > ηe } (3)\nThis not only gives explanations for its prediction but can also help diagnose the model itself, e.g. we can measure metrics of the model’s ability to provide these explanations by comparing them against gold supporting evidence annotations.\nOne downside of this modeling approach is that it will not be able to capture reasoning between speakers. A case for that would be a pronoun by one speaker might refer to something initiated by the other speaker. Nevertheless, the utterancebased approach explicitly adds an inductive structure bias to learning and inference which we will see can aid its generalization capability.\nThresholding. For both the unstructured and utterance-based approaches, the detection of contradiction is made by comparing ŷpred with a threshold τ and by default τ is 0.5."
    }, {
      "heading" : "4.2 Experimental Setup",
      "text" : "We study four base pre-trained models variants for fθ: BERT (Devlin et al., 2019), Electra (Clark et al., 2020), RoBERTa (Liu et al., 2019), and\nBART (Lewis et al., 2020). They represent the start-of-the-art language representation models and have yielded successes in many NLU tasks. The input format of fθ follows how these models handle sequence-pairs (C and u) for classification tasks with padding, separator and other special tokens such as position embeddings and segment features inserted at designated locations accordingly.\nWe fine-tune fθ on different combinations of NLI training data including SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), ANLIR3 (Nie et al., 2020a)4, DNLI (Welleck et al., 2019), as well as our DECODE Main training set. We convert the 3-way labels of the examples in existing NLI datasets to 2-way, as described before, and θ is optimized using cross-entropy loss. When training fUBθ in the utterance-based approach using the DECODE training set, the input sequences\n4ANLI data is collected in three rounds resulting in three subsets (R1, R2, R3). We only used training data in R3 since it contains some dialogue-related examples.\nare sampled utterance pairs from the DECODE dialogue. In other scenarios, fθ or fUBθ are trained with data treated as in normal NLI training.\nThe models are evaluated on the test sets described in subsection 3.3. For the utterance-based approach, which provides supporting evidence utterances (Equation 3), we report F1 on evidence retrieval. We also report a stricter score which evaluates whether both 2-way contradiction detection and supporting evidence retrieval exactly match with the ground truth on DECODE Main test."
    }, {
      "heading" : "5 Results and Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Performance on Constructed Dataset",
      "text" : "Our main results comparing various detectors on DECODE are shown in Table 2. We now describe our key observations.\nDECODE is notably more effective than other existing NLI data in providing supervision for contradiction detection in dialogue. We found that models trained on DECODE achieve higher accuracy than that of those trained on DNLI or ANLIR3, on all evaluation sets, with large improvements, e.g. a 12-point jump from the same model training on ANLI-R3 and a 16-point jump from training on DNLI using utterance-based RoBERTa on the DECODE Main test set. Moreover, while training on “All” datasets (SNLI, MNLI, ANLI-R3, DNLI & DECODE) is effective, the removal of DECODE from the training data induces a consequential downgrade on the performance. Training on NLI data which does not cover the dialogue domain, e.g., SNLI+MNLI is even worse, only achieving 77.4% on DECODE Main (Test) vs. 93.19% for DECODE and cannot even reach the majority baseline on the “Main (Test-Strict)”. Further, training on DECODE is also more helpful than DNLI or ANLI-R3 for supporting evidence retrieval. These findings indicate that existing NLI data has limited transferability to the dialogue contradiction detection task despite their coverage of the dialogue domain in addition to other domains and that our DECODE data provides a valuable resource for modeling dialogue consistency and developing data-driven approaches for contradiction detection.\nDifferent pre-training models that perform similarly on the in-domain test set can have very different performance on OOD human-bot dialogue. The last four rows of the table show the results of utterance-based RoBERTa, BERT, Elec-\ntra, and BART trained on DECODE. We can see that RoBERTa, Electra, and BART got similar indomain accuracy on DECODE, around 93%-94%. RoBERTa stands out when comparing their performance on the human-bot test set with the highest score of 84.69% across the column and with better performance on supporting evidence retrieval as well. We speculate that this is due to the fact that RoBERTa pre-training data has a broader coverage than Electra and BART. We hope future work on dialogue contradiction detection could explore pretraining models on more dialogue-focused corpora.\nThe unstructured approach gets higher accuracy on the in-domain test set. A direct comparison between unstructured RoBERTa and utterancebased RoBERTa trained on DECODE reveals that the unstructured approach more often than not gets a higher accuracy than its corresponding utterancebased approach when other experimental setups are kept identical. Noticeably, unstructured RoBERTa trained on all NLI data got a 97.46% score, whereas utterance-based yielded 94.19%. This seemingly indicates that training an unstructured model is able to yield a good representation of the consistency of the dialogue. However, analysis on the human-bot and auxiliary test sets shows that such high accuracy is an over-amplification of the model’s real understanding ability, as we discuss next.\nThe structured utterance-based approach is more robust, and more transferable. Figure 2 gives a comparison between utterance-based and unstructured RoBERTa on each of the evaluation sets. We can see that the utterance-based model is\nable to maintain satisfactory performance across all the sets whereas the unstructured model underperforms at the human-bot and RCT auxiliary test sets with a 34.4% accuracy on RCT compared to 78.4% for utterance-based, in stark contrast to the high performance of the unstructured method on the in-domain DECODE Main test set. This result indicates the unstructured approach overfits on superficial patterns in the DECODE Main training data which are still present due to RCT’s construction process.5 We also provide further analysis in Appendix E, including experiments showing that simply removing speaker utterances not uttered by the last speaker does not greatly improve the unstructured method. The fact that the utterancebased approach has good transferability to the OOD human-bot test set indicates that injecting the correct inductive structure bias is beneficial for modeling dialogue consistency. We believe this is an interesting result generally for research using Transformers, where there is currently a belief amongst some practitioners that they can just use a standard Transformer and it will learn all the structure correctly on its own. In our setting that is not the case, and we provide a method that can rectify that failing.\nIn general, there is still much room for improvement. The results in Table 2 also demonstrate that the modeling of dialogue consistency is a demanding task. On the contradiction detection task, the best score achieved by the state-of-the-art pretrained language models on DECODE (Test-Strict) is 80.86% and the best human-bot test score is 84.69%. Considering all the examples in the test sets are verified by at least 3 annotators, humans are able to swiftly identify such contradictions. This suggests there is a large ability gap between our best automatic detectors and humans. Closing this gap is an important challenge for the community."
    }, {
      "heading" : "5.2 Performance in an Interactive Setting",
      "text" : "Model vs. Human Judgment. To further understand the detector predictions and how well they might align with human judgments, we consider the Human-Bot data again. We first divide all the utterances into two categories based on whether they are generated by a human or a bot. Then, the bot-generated utterances that have been marked by annotators as contradicting utterances are cat-\n5Overfitting on superficial patterns is a typical issue and open problem in NLU modeling (Nie et al., 2020a).\nUtterance-based (DECODE) Utterance-based (DNLI) Unstructured (DECODE)\nFi re\nR at\ne\n5.5\n17.9 14.0\n22.9\n29.4\n21.7\n44.3 44.8\n31.7\n46.3 48.9\n39.7\n74.3\n65.1\n50.1\nHuman Bot @1 @2 @3\nFigure 3: The fire rate (the percentage that it predicts “contradiction”) of RoBERTa models with different setups on utterances belonging to different categories. “Human” and “Bot” stand for utterances by the human or the bot prospectively. “@N” indicates the category where N annotators agreed on the contradiction label. The x-axis indicates different approaches and the text in parentheses denotes the training data.\negorized into three sets based on the number of annotators that agree on the contradiction label. By design, the more annotators that agree on the contradiction label, the more plausible that it is a contradiction. We examine detector model fire rate on the utterances in the 5 different categories and results are shown in Figure 3. The fire rate of utterance-based RoBERTa trained on DECODE on human utterances is 5.5% contrasting to the 74.3% on 3-agreed contradicting utterances, whereas the fire rates of unstructured RoBERTa on different categories are more clustered together. This finding demonstrates that our models can discriminate between utterances with a distinct nature, and the model predictions are aligned with human judgments. Moreover, a strong discriminative detector could be a useful tool to stratify utterances.\nUsing DECODE as an Automatic Metric. The results presented above indicate that the prediction of the detector can easily differentiate between the quality of utterances by humans and the utterances by bots. We further investigate whether it can differentiate the quality of the utterances by different bots and be used as an automatic metric checking generation consistency. We compare the average contradiction score of the detector with the contradiction rate by human judgments on the utterances generated by different classes of model (bots). The bots are the same set of models described in subsection 3.3 from which we collected our human-bot\ndialogue examples. The trend in Figure 4 reveals that the scores are positively correlated with human judgments, with a Pearson correlation coefficient of 0.81. We would expect that improvement on the DECODE task will directly increase the correlation between the automatically produced detection score and human judgments, where use of such an automatic metric can ease the burden on laborious human evaluation of consistency."
    }, {
      "heading" : "5.3 Generation Re-ranking",
      "text" : "Given a contradiction detector, an obvious question other than using it as an automatic metric, is: can it be used to improve the consistency of dialogue generation models? We consider a very simple way to do that in the state-of-the-art generative model, BlenderBot (BST 2.7B) (Roller et al., 2020). During the decoding phase, for decoding methods that can output multiple hypotheses, we simply rerank the top scoring hypotheses using the contradiction detection classifier. We use our best performing classifier, our utterance-based RoBERTa model with DECODE fine-tuning, and consider three methods of decoding: beam search, top-k sampling (Fan et al., 2018) and sample-andrank (Adiwardana et al., 2020), and compare the standard and DECODE-reranked decoding methods to each other. For beam search we use the best found parameters from (Roller et al., 2020) which are beam size 10, minimum beam length 20 and beam blocking of 3-grams. For top-k we use k = 40. For Sample-and-Rank we use k=40 and 20 samples. We consider the same human-bot dialogue logs as before, but only between Blenderbot BST 2.7B and humans, selecting only contradicting\nutterances. Table 3 presents the results.\nAutomatic metric using DECODE. Using our same DECODE contradiction classifier as the automatic metric, as in subsection 5.2, we observe that by re-ranking the beam of beam search (size 10) we can improve the metric. Still, 46.1% of the time the detector flags generations as contradictions (vs. 69.7% without re-ranking). Upon observation of the outputs, this seems to be due to the beam of beam decoding not being diverse enough (Vijayakumar et al., 2016): when the top scoring utterance is flagged as contradicting, many of the other utterances in the beam are similar responses with slight rephrases, and are flagged contradicting as well. Top-k sampling fares much better, where reranking in our test can very often find at least one from the k = 40 samples that does not flag the classifier, leaving only a 2.6% contradiction firing rate. We note we expect these numbers are over-optimistically low because the metric itself is being used to search (re-rank) and evaluate in this case.\nHuman Judgments. The last column of Table 3 presents human judgments of the various model generations, judged using the same approach as before with human verifiers, and reporting the percentage of contradictions. We observe similar results to the automatic metric findings. DECODE re-ranking reduces the number of contradictions, particularly for Top-k re-ranking vs. Top-k: testing for significance with a Wilcoxon signed-rank test, we get p = 0.051 using two human verifiers and p = 0.023 for three verifiers. More detailed results and analysis can be found in Appendix G."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We introduce the DialoguE COntradiction DEtection task (DECODE) and a new conversational dataset containing both human-human and humanbot contradictory dialogues. Training models on DECODE achieves better performance than other existing NLI data by a large margin. We further propose a structured utterance-based approach where utterances are paired before being fed into Transformer NLI models to tackle the dialogue contradiction detection task. We show the superiority of such an approach when transferring to out-ofdistribution dialogues compared to a standard unstructured approach representative of mainstream NLU modeling. We further show that our best contradiction detector correlates with human judgments, and provide evidence for its usage in both automatic checking and improving the consistency of state-of-the-art generative chatbots."
    }, {
      "heading" : "Acknowledgement",
      "text" : "We thank the reviewers, and Jie Lei and Hao Tan for their helpful discussions. YN interned at Facebook. YN and MB were later sponsored by NSF-CAREER Award 1846185, DARPA MCS Grant N66001-19-2-4031, and DARPA YFA17D17AP00022."
    }, {
      "heading" : "A Annotation Interface",
      "text" : "In the main paper, we describe the procedure of the data collection. Figure 5 shows the collection user interface."
    }, {
      "heading" : "B Annotation Quality Control",
      "text" : "We apply the following mechanism to ensure the quality of collected data: • Onboarding Test: Every annotator needs to\npass an onboarding test before they can actually contribute dialogue examples. The test is the same dialogue contradiction detection task as in the actual collection procedure, including 5 dialogues where 3 of them have an ending utterance that contradicts the dialogue history. The annotator needs to select the correct label (contradiction or non-contradiction) for all five dialogues to pass the test. This mechanism tests whether an annotator understands the task. • Maximum Annotation Count Limit: The\nmaximum number of examples one annotator can create is 20. This mechanism helps further diversify the dialogue examples by reducing similar patterns that appear in one or a group of annotators (Geva et al., 2019). • Verification: This subtask ensures that the dia-\nlogue examples indeed contain an ending utterance that contradicts the dialogue history. We ask 3 additional annotators to verify some of the collected examples and select the ones where all three verifiers agreed on the contradiction label, and use these for our resulting validation and tests sets. This mechanism ensures that there is a clear, agreed-upon contradiction in the dialogue, preventing the subjectivity and ambiguity issues in some NLU tasks (Nie et al., 2020b).\nA pilot study with over 100 workers was conducted before the collection which then went through an internal review process and we do not collect any personal information of the workers."
    }, {
      "heading" : "C Data Statistics",
      "text" : "Table 8 shows the breakdown of dialogue sources and data splits. For a subset of the contradicting dialogues in DECODE we asked three verifiers to determine whether the original writer indeed created a contradiction example. Table 4 shows the verification statistics. Note that we only use examples on which all three verifiers agreed for DECODE (dev) and DECODE (Test)."
    }, {
      "heading" : "D Examples",
      "text" : "As described in the main paper, DECODE consists of dialogues belonging to four categories, namely, Human-Human, Human-Bot, A2T, and RCT. Table 5 shows one example for each dataset type."
    }, {
      "heading" : "E Extra Results Analysis",
      "text" : "Table 6 shows the performance of unstructured method when the input consists of utterances from both speakers (the default unstructured approach) and when the input consists of utterances from only the last speaker. The numbers for default two speaker unstructured approach and the utterancebased approach match with that in Table 2. The result indicates that removing speaker utterances not uttered by the last speaker does not greatly improve generalization of the unstructured method. This helps show that the out-of-domain improvement from the structured utterance-based method on human-bot data comes from the structure of the architecture."
    }, {
      "heading" : "F Performance in an Interactive Setting",
      "text" : "The results discussed in the main paper evaluate models on constructed datasets with intentionally balanced labels. This facilitates the comparison between models following a NLU evaluation perspective. In practice, we would like to evaluate how well a model can detect contradicting utterances sampled naturally from interactive humanbot dialogue. To that end, we test our trained detection models on the raw interactive human-bot dialogue data6 having a total number of 764 dialogues consisting of 8,933 utterances. Since the contradiction task in naturally sampled dialogue can be extremely unbalanced, the total number of contradicting utterances in the raw dialogue list is only 3817. We apply our contradiction detectors on every bot-generated utterance and calculate the precision, recall, and F1 on contradiction detection. Since the scores might be subjective to the threshold τ , we also evaluate the threshold-invariant Area Under the ROC Curve (AUC) (Bradley, 1997).\nAs shown in Table 7, model precision on the task is not satisfactory (23.94 at best). However, the best model achieves acceptable scores on both Recall and AUC. This indicates its potential usage for strict blocking of inconsistent utterances\n6This is the same set of dialogues from which we constructed the balanced human-bot test set.\n7The majority baseline accuracy is 95.73%.\nof a generative model (bot). The table also draws the same conclusion as Table 2 that the structured utterance-based RoBERTa model trained using DECODE data is the best method for contradiction detection, comparing to training on other NLI data or using an unstructured approach. In the following sections we thus use that best method as our detector for further experiments."
    }, {
      "heading" : "G Generation Re-ranking",
      "text" : "We show in Table 9 human judgments for our generation re-reranking experiments in three settings: with at least two human verifiers, with three agreeing, or treating agreements as a fractional contradiction score. The first two, for a given utterance, assign a binary score (either contradicton or noncontradiction) depending on whether at least 2 or 3 human verifiers agree on the contradiction label. The last setting treats a given utterance as having a fractional score, either 0, 1/3, 2/3, or 3/3 depending on how many human verifiers label it as a contradiction. We then take the mean over all utterances in each setting to give the final contradiction count per setting.\nIn addition to the setting in the main paper (subsection 5.3), we also consider the setting where the dialogue examples we use consist of 76 examples utterances that were identified by humans as being contradictions by BlenderBot (using beam search) and 100 examples that were not. This is in contrast to Table 3 where we only considered contradicting utterances by BlenderBot only. The results are given in Table 10. We find similar results to the main paper’s results but where the model’s score are closer together. This should be expected as when selecting many utterances that are already non-contradicting in the original BlenderBot generations, there is not much left to improve."
    } ],
    "references" : [ {
      "title" : "Towards a human-like open-domain chatbot",
      "author" : [ "Daniel Adiwardana", "Minh-Thang Luong", "David R So", "Jamie Hall", "Noah Fiedel", "Romal Thoppilan", "Zi Yang", "Apoorv Kulshreshtha", "Gaurav Nemade", "Yifeng Lu" ],
      "venue" : "arXiv preprint arXiv:2001.09977",
      "citeRegEx" : "Adiwardana et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Adiwardana et al\\.",
      "year" : 2020
    }, {
      "title" : "Example-based chatoriented dialogue system with personalized longterm memory",
      "author" : [ "Jeesoo Bang", "Hyungjong Noh", "Yonghee Kim", "Gary Geunbae Lee." ],
      "venue" : "2015 International Conference on Big Data and Smart Computing (BigComp), pages",
      "citeRegEx" : "Bang et al\\.,? 2015",
      "shortCiteRegEx" : "Bang et al\\.",
      "year" : 2015
    }, {
      "title" : "The pushshift reddit dataset",
      "author" : [ "Jason Baumgartner", "Savvas Zannettou", "Brian Keegan", "Megan Squire", "Jeremy Blackburn." ],
      "venue" : "Proceedings of the International AAAI Conference on Web and Social Media, volume 14, pages 830–839.",
      "citeRegEx" : "Baumgartner et al\\.,? 2020",
      "shortCiteRegEx" : "Baumgartner et al\\.",
      "year" : 2020
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "The use of the area under the roc curve in the evaluation of machine learning algorithms",
      "author" : [ "Andrew P Bradley." ],
      "venue" : "Pattern recognition, 30(7):1145–1159.",
      "citeRegEx" : "Bradley.,? 1997",
      "shortCiteRegEx" : "Bradley.",
      "year" : 1997
    }, {
      "title" : "Language models are few-shot learners",
      "author" : [ "Amodei." ],
      "venue" : "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.",
      "citeRegEx" : "Amodei.,? 2020",
      "shortCiteRegEx" : "Amodei.",
      "year" : 2020
    }, {
      "title" : "ELECTRA: pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "The pascal recognising textual entailment challenge",
      "author" : [ "Ido Dagan", "Oren Glickman", "Bernardo Magnini." ],
      "venue" : "Machine Learning Challenges Workshop, pages 177–190. Springer.",
      "citeRegEx" : "Dagan et al\\.,? 2005",
      "shortCiteRegEx" : "Dagan et al\\.",
      "year" : 2005
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "The second conversational intelligence challenge (convai2)",
      "author" : [ "Emily Dinan", "Varvara Logacheva", "Valentin Malykh", "Alexander Miller", "Kurt Shuster", "Jack Urbanek", "Douwe Kiela", "Arthur Szlam", "Iulian Serban", "Ryan Lowe" ],
      "venue" : null,
      "citeRegEx" : "Dinan et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Dinan et al\\.",
      "year" : 2020
    }, {
      "title" : "Wizard of wikipedia: Knowledge-powered conversational agents",
      "author" : [ "Emily Dinan", "Stephen Roller", "Kurt Shuster", "Angela Fan", "Michael Auli", "Jason Weston." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA,",
      "citeRegEx" : "Dinan et al\\.,? 2019",
      "shortCiteRegEx" : "Dinan et al\\.",
      "year" : 2019
    }, {
      "title" : "Augmenting neural response generation with context-aware topical attention",
      "author" : [ "Nouha Dziri", "Ehsan Kamalloo", "Kory Mathewson", "Osmar Zaiane." ],
      "venue" : "Proceedings of the First Workshop on NLP for Conversational AI, pages 18–31, Florence, Italy. Associ-",
      "citeRegEx" : "Dziri et al\\.,? 2019a",
      "shortCiteRegEx" : "Dziri et al\\.",
      "year" : 2019
    }, {
      "title" : "Evaluating coherence in dialogue systems using entailment",
      "author" : [ "Nouha Dziri", "Ehsan Kamalloo", "Kory Mathewson", "Osmar Zaiane." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Dziri et al\\.,? 2019b",
      "shortCiteRegEx" : "Dziri et al\\.",
      "year" : 2019
    }, {
      "title" : "Hierarchical neural story generation",
      "author" : [ "Angela Fan", "Mike Lewis", "Yann Dauphin." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889–898, Melbourne, Australia. Association",
      "citeRegEx" : "Fan et al\\.,? 2018",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2018
    }, {
      "title" : "Evaluating models’ local decision boundaries via contrast sets",
      "author" : [ "F. Liu", "Phoebe Mulcaire", "Qiang Ning", "Sameer Singh", "Noah A. Smith", "Sanjay Subramanian", "Reut Tsarfaty", "Eric Wallace", "Ally Zhang", "Ben Zhou" ],
      "venue" : "In Findings of the Association",
      "citeRegEx" : "Liu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets",
      "author" : [ "Mor Geva", "Yoav Goldberg", "Jonathan Berant." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Geva et al\\.,? 2019",
      "shortCiteRegEx" : "Geva et al\\.",
      "year" : 2019
    }, {
      "title" : "Logic and conversation",
      "author" : [ "Herbert P Grice." ],
      "venue" : "Speech acts, pages 41–58. Brill.",
      "citeRegEx" : "Grice.,? 1975",
      "shortCiteRegEx" : "Grice.",
      "year" : 1975
    }, {
      "title" : "Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring",
      "author" : [ "Samuel Humeau", "Kurt Shuster", "Marie-Anne Lachaux", "Jason Weston." ],
      "venue" : "arXiv preprint arXiv:1905.01969.",
      "citeRegEx" : "Humeau et al\\.,? 2019",
      "shortCiteRegEx" : "Humeau et al\\.",
      "year" : 2019
    }, {
      "title" : "BART: Denoising sequence-to-sequence pretraining for natural language",
      "author" : [ "Mike Lewis", "Yinhan Liu", "Naman Goyal", "Marjan Ghazvininejad", "Abdelrahman Mohamed", "Omer Levy", "Veselin Stoyanov", "Luke Zettlemoyer" ],
      "venue" : null,
      "citeRegEx" : "Lewis et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2020
    }, {
      "title" : "A persona-based neural conversation model",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Georgios Spithourakis", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Don’t say that! making inconsistent dialogue unlikely with unlikelihood training",
      "author" : [ "Margaret Li", "Stephen Roller", "Ilia Kulikov", "Sean Welleck", "Y-Lan Boureau", "Kyunghyun Cho", "Jason Weston." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Asso-",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons",
      "author" : [ "Margaret Li", "Jason Weston", "Stephen Roller." ],
      "venue" : "arXiv preprint arXiv:1909.03087.",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "The Ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems",
      "author" : [ "Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau." ],
      "venue" : "Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse",
      "citeRegEx" : "Lowe et al\\.,? 2015",
      "shortCiteRegEx" : "Lowe et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep learning: A critical appraisal",
      "author" : [ "Gary Marcus." ],
      "venue" : "arXiv preprint arXiv:1801.00631.",
      "citeRegEx" : "Marcus.,? 2018",
      "shortCiteRegEx" : "Marcus.",
      "year" : 2018
    }, {
      "title" : "ParlAI: A dialog research software platform",
      "author" : [ "Alexander Miller", "Will Feng", "Dhruv Batra", "Antoine Bordes", "Adam Fisch", "Jiasen Lu", "Devi Parikh", "Jason Weston." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Miller et al\\.,? 2017",
      "shortCiteRegEx" : "Miller et al\\.",
      "year" : 2017
    }, {
      "title" : "Machines and mindlessness: Social responses to computers",
      "author" : [ "Clifford Nass", "Youngme Moon." ],
      "venue" : "Journal of social issues, 56(1):81–103.",
      "citeRegEx" : "Nass and Moon.,? 2000",
      "shortCiteRegEx" : "Nass and Moon.",
      "year" : 2000
    }, {
      "title" : "Adversarial NLI: A new benchmark for natural language understanding",
      "author" : [ "Yixin Nie", "Adina Williams", "Emily Dinan", "Mohit Bansal", "Jason Weston", "Douwe Kiela." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Nie et al\\.,? 2020a",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2020
    }, {
      "title" : "What can we learn from collective human opinions on natural language inference data",
      "author" : [ "Yixin Nie", "Xiang Zhou", "Mohit Bansal" ],
      "venue" : "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Nie et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2020
    }, {
      "title" : "Assigning personality/profile to a chatting machine for coherent conversation generation",
      "author" : [ "Qiao Qian", "Minlie Huang", "Haizhou Zhao", "Jingfang Xu", "Xiaoyan Zhu." ],
      "venue" : "Proceedings of the TwentySeventh International Joint Conference on Artificial",
      "citeRegEx" : "Qian et al\\.,? 2018",
      "shortCiteRegEx" : "Qian et al\\.",
      "year" : 2018
    }, {
      "title" : "Towards empathetic opendomain conversation models: A new benchmark and dataset",
      "author" : [ "Hannah Rashkin", "Eric Michael Smith", "Margaret Li", "Y-Lan Boureau." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Rashkin et al\\.,? 2019",
      "shortCiteRegEx" : "Rashkin et al\\.",
      "year" : 2019
    }, {
      "title" : "Beyond accuracy: Behavioral testing of NLP models with CheckList",
      "author" : [ "Marco Tulio Ribeiro", "Tongshuang Wu", "Carlos Guestrin", "Sameer Singh." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4902–",
      "citeRegEx" : "Ribeiro et al\\.,? 2020",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2020
    }, {
      "title" : "Recipes for building an open-domain chatbot",
      "author" : [ "Stephen Roller", "Emily Dinan", "Naman Goyal", "Da Ju", "Mary Williamson", "Yinhan Liu", "Jing Xu", "Myle Ott", "Kurt Shuster", "Eric M Smith" ],
      "venue" : "arXiv preprint arXiv:2004.13637",
      "citeRegEx" : "Roller et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Roller et al\\.",
      "year" : 2020
    }, {
      "title" : "What makes a good conversation? how controllable attributes affect human judgments",
      "author" : [ "Abigail See", "Stephen Roller", "Douwe Kiela", "Jason Weston." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "See et al\\.,? 2019",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2019
    }, {
      "title" : "Building end-to-end dialogue systems using generative hierarchical neural network models",
      "author" : [ "Iulian Vlad Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron C. Courville", "Joelle Pineau." ],
      "venue" : "Proceedings of the Thirtieth AAAI Conference on Arti-",
      "citeRegEx" : "Serban et al\\.,? 2016",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "A hierarchical latent variable encoder-decoder model for generating dialogues",
      "author" : [ "Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron C. Courville", "Yoshua Bengio." ],
      "venue" : "Proceedings of the Thirty-First AAAI",
      "citeRegEx" : "Serban et al\\.,? 2017",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2017
    }, {
      "title" : "Can you put it all together: Evaluating conversational agents’ ability to blend skills",
      "author" : [ "Eric Michael Smith", "Mary Williamson", "Kurt Shuster", "Jason Weston", "Y-Lan Boureau." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Smith et al\\.,? 2020",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2020
    }, {
      "title" : "Diverse beam search: Decoding diverse solutions from neural sequence models",
      "author" : [ "Ashwin K Vijayakumar", "Michael Cogswell", "Ramprasath R Selvaraju", "Qing Sun", "Stefan Lee", "David Crandall", "Dhruv Batra." ],
      "venue" : "arXiv preprint arXiv:1610.02424.",
      "citeRegEx" : "Vijayakumar et al\\.,? 2016",
      "shortCiteRegEx" : "Vijayakumar et al\\.",
      "year" : 2016
    }, {
      "title" : "Steering output style and topic in neural response generation",
      "author" : [ "Di Wang", "Nebojsa Jojic", "Chris Brockett", "Eric Nyberg." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2140–2150, Copenhagen,",
      "citeRegEx" : "Wang et al\\.,? 2017",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2017
    }, {
      "title" : "Dialogue natural language inference",
      "author" : [ "Sean Welleck", "Jason Weston", "Arthur Szlam", "Kyunghyun Cho." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3731–3741, Florence, Italy. Association for",
      "citeRegEx" : "Welleck et al\\.,? 2019",
      "shortCiteRegEx" : "Welleck et al\\.",
      "year" : 2019
    }, {
      "title" : "Retrieve and refine: Improved sequence generation models for dialogue",
      "author" : [ "Jason Weston", "Emily Dinan", "Alexander Miller." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational",
      "citeRegEx" : "Weston et al\\.,? 2018",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2018
    }, {
      "title" : "A broad-coverage challenge corpus for sentence understanding through inference",
      "author" : [ "Adina Williams", "Nikita Nangia", "Samuel Bowman." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Williams et al\\.,? 2018",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2018
    }, {
      "title" : "Personalizing dialogue agents: I have a dog, do you have pets too",
      "author" : [ "Saizheng Zhang", "Emily Dinan", "Jack Urbanek", "Arthur Szlam", "Douwe Kiela", "Jason Weston" ],
      "venue" : "In Proceedings of the 56th Annual Meeting of the Association",
      "citeRegEx" : "Zhang et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural personalized response generation as domain adaptation",
      "author" : [ "Wei-Nan Zhang", "Qingfu Zhu", "Yifa Wang", "Yanyan Zhao", "Ting Liu." ],
      "venue" : "World Wide Web, 22(4):1427–1446.",
      "citeRegEx" : "Zhang et al\\.,? 2019a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Consistent dialogue generation with self-supervised feature learning",
      "author" : [ "Yizhe Zhang", "Xiang Gao", "Sungjin Lee", "Chris Brockett", "Michel Galley", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "arXiv preprint arXiv:1903.05759.",
      "citeRegEx" : "Zhang et al\\.,? 2019b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "DIALOGPT : Largescale generative pre-training for conversational response generation",
      "author" : [ "Yizhe Zhang", "Siqi Sun", "Michel Galley", "Yen-Chun Chen", "Chris Brockett", "Xiang Gao", "Jianfeng Gao", "Jingjing Liu", "Bill Dolan." ],
      "venue" : "Proceedings of the 58th An-",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "The design and implementation of XiaoIce, an empathetic social chatbot",
      "author" : [ "Li Zhou", "Jianfeng Gao", "Di Li", "Heung-Yeung Shum." ],
      "venue" : "Computational Linguistics, 46(1):53–93.",
      "citeRegEx" : "Zhou et al\\.,? 2020",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Recent progress on neural approaches to natural language processing (Devlin et al., 2019; Brown et al., 2020), and the availability of large amounts of conversational data (Lowe et al.",
      "startOffset" : 68,
      "endOffset" : 109
    }, {
      "referenceID" : 23,
      "context" : ", 2020), and the availability of large amounts of conversational data (Lowe et al., 2015; Smith et al., 2020) have triggered a resurgent interest on building intelligent open-domain chatbots.",
      "startOffset" : 70,
      "endOffset" : 109
    }, {
      "referenceID" : 36,
      "context" : ", 2020), and the availability of large amounts of conversational data (Lowe et al., 2015; Smith et al., 2020) have triggered a resurgent interest on building intelligent open-domain chatbots.",
      "startOffset" : 70,
      "endOffset" : 109
    }, {
      "referenceID" : 45,
      "context" : "Newly developed end-to-end neural bots (Zhang et al., 2020; Adiwardana et al., 2020; Roller et al., 2020) are claimed to be superior to their predecessors (Worsnick, 2018; Zhou et al.",
      "startOffset" : 39,
      "endOffset" : 105
    }, {
      "referenceID" : 0,
      "context" : "Newly developed end-to-end neural bots (Zhang et al., 2020; Adiwardana et al., 2020; Roller et al., 2020) are claimed to be superior to their predecessors (Worsnick, 2018; Zhou et al.",
      "startOffset" : 39,
      "endOffset" : 105
    }, {
      "referenceID" : 32,
      "context" : "Newly developed end-to-end neural bots (Zhang et al., 2020; Adiwardana et al., 2020; Roller et al., 2020) are claimed to be superior to their predecessors (Worsnick, 2018; Zhou et al.",
      "startOffset" : 39,
      "endOffset" : 105
    }, {
      "referenceID" : 46,
      "context" : ", 2020) are claimed to be superior to their predecessors (Worsnick, 2018; Zhou et al., 2020) using various human evaluation techniques (See et al.",
      "startOffset" : 57,
      "endOffset" : 92
    }, {
      "referenceID" : 33,
      "context" : ", 2020) using various human evaluation techniques (See et al., 2019; Li et al., 2019; Adiwardana et al., 2020) that aim to give a more accurate measure of what makes a good conversation.",
      "startOffset" : 50,
      "endOffset" : 110
    }, {
      "referenceID" : 21,
      "context" : ", 2020) using various human evaluation techniques (See et al., 2019; Li et al., 2019; Adiwardana et al., 2020) that aim to give a more accurate measure of what makes a good conversation.",
      "startOffset" : 50,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : ", 2020) using various human evaluation techniques (See et al., 2019; Li et al., 2019; Adiwardana et al., 2020) that aim to give a more accurate measure of what makes a good conversation.",
      "startOffset" : 50,
      "endOffset" : 110
    }, {
      "referenceID" : 32,
      "context" : "The main train, valid and test sets contain human-written dialogues with deliberate contradictions (example at top), and an out-of-domain test set consists of labeled human-bot dialogues (bottom), involving state-of-the-art bots (Roller et al., 2020).",
      "startOffset" : 229,
      "endOffset" : 250
    }, {
      "referenceID" : 0,
      "context" : "For example, it has been shown that open-domain chatbots frequently generate annoying errors (Adiwardana et al., 2020; Roller et al., 2020) and a notorious one among these is the class of contradiction, or consistency errors.",
      "startOffset" : 93,
      "endOffset" : 139
    }, {
      "referenceID" : 32,
      "context" : "For example, it has been shown that open-domain chatbots frequently generate annoying errors (Adiwardana et al., 2020; Roller et al., 2020) and a notorious one among these is the class of contradiction, or consistency errors.",
      "startOffset" : 93,
      "endOffset" : 139
    }, {
      "referenceID" : 26,
      "context" : "When interacting with chatbots, people carry over many of the same expectations as when interacting with humans (Nass and Moon, 2000).",
      "startOffset" : 112,
      "endOffset" : 133
    }, {
      "referenceID" : 24,
      "context" : "whether generative models could ever really understand what they are saying at all (Marcus, 2018).",
      "startOffset" : 83,
      "endOffset" : 97
    }, {
      "referenceID" : 16,
      "context" : "From a speaker’s perspective, it violates the maxim of quality in Grice’s cooperative principles (Grice, 1975) —”Do not say what you believe to be false.",
      "startOffset" : 97,
      "endOffset" : 110
    }, {
      "referenceID" : 39,
      "context" : "Prior works (Welleck et al., 2019) characterized the modeling of persona-related consistency as a natural language inference (NLI) problem (Dagan",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 42,
      "context" : ", 2015), and constructed a dialog NLI dataset based on Persona-Chat (Zhang et al., 2018), but so far state-of-the-art chatbots (Roller et al.",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 32,
      "context" : ", 2018), but so far state-of-the-art chatbots (Roller et al., 2020) have not been able to make use of NLI techniques in improving dialogue con-",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 38,
      "context" : "The modeling can be implicit where the dialogue consistency-related information like style (Wang et al., 2017), topics, or personal facts are maintained in distributed embeddings (Li et al.",
      "startOffset" : 91,
      "endOffset" : 110
    }, {
      "referenceID" : 19,
      "context" : ", 2017), topics, or personal facts are maintained in distributed embeddings (Li et al., 2016; Zhang et al., 2019a), neural long-term memo-",
      "startOffset" : 76,
      "endOffset" : 114
    }, {
      "referenceID" : 43,
      "context" : ", 2017), topics, or personal facts are maintained in distributed embeddings (Li et al., 2016; Zhang et al., 2019a), neural long-term memo-",
      "startOffset" : 76,
      "endOffset" : 114
    }, {
      "referenceID" : 1,
      "context" : "ries (Bang et al., 2015), hierarchical neural architecture (Serban et al.",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 34,
      "context" : ", 2015), hierarchical neural architecture (Serban et al., 2016), latent variables (Serban et al.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 35,
      "context" : ", 2016), latent variables (Serban et al., 2017), topical attention (Dziri et al.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 11,
      "context" : ", 2017), topical attention (Dziri et al., 2019a), or even self-learned feature vectors (Zhang et al.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 44,
      "context" : ", 2019a), or even self-learned feature vectors (Zhang et al., 2019b).",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 29,
      "context" : "Some works have grounded generation models on explicit user input (Qian et al., 2018), or designated personas (Zhang et al.",
      "startOffset" : 66,
      "endOffset" : 85
    }, {
      "referenceID" : 20,
      "context" : "(2019) constructed the dialogue NLI dataset and (Li et al., 2020) utilized it to try to reduce inconsistency in generative models via unlikelihood training in a preliminary study that reports perplexity results, but did not measure actual generations or contradiction rates.",
      "startOffset" : 48,
      "endOffset" : 65
    }, {
      "referenceID" : 42,
      "context" : "We note that the dialogue NLI dataset is only semi-automatically generated, with limited coverage of only Persona-chat data (Zhang et al., 2018), whereas our DECODE is human-written and across multiple domains.",
      "startOffset" : 124,
      "endOffset" : 144
    }, {
      "referenceID" : 25,
      "context" : "2 We crowdsource the continuation and annotation data with Amazon Mechanical Turk via ParlAI (Miller et al., 2017).",
      "startOffset" : 93,
      "endOffset" : 114
    }, {
      "referenceID" : 10,
      "context" : "The pre-selected dialogue source corpora are Wizard of Wikipedia (Dinan et al., 2019), EMPATHETICDIALOGUES (Rashkin et al.",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 30,
      "context" : ", 2019), EMPATHETICDIALOGUES (Rashkin et al., 2019), Blended Skill Talk (Smith et al.",
      "startOffset" : 29,
      "endOffset" : 51
    }, {
      "referenceID" : 36,
      "context" : ", 2019), Blended Skill Talk (Smith et al., 2020), and ConvAI2 (Dinan et al.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 9,
      "context" : ", 2020), and ConvAI2 (Dinan et al., 2020), covering various conversational topics.",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 17,
      "context" : "These include Poly-encoder (Humeau et al., 2019) based retrieval models, generative models (Roller et al.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 32,
      "context" : ", 2019) based retrieval models, generative models (Roller et al., 2020), unlikelihood trained models (Li et al.",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 20,
      "context" : ", 2020), unlikelihood trained models (Li et al., 2020), retrieve-and-refine models (Weston et al.",
      "startOffset" : 37,
      "endOffset" : 54
    }, {
      "referenceID" : 40,
      "context" : ", 2020), retrieve-and-refine models (Weston et al., 2018; Roller et al., 2020), models either pre-trained on a previously existing Reddit dataset",
      "startOffset" : 36,
      "endOffset" : 78
    }, {
      "referenceID" : 32,
      "context" : ", 2020), retrieve-and-refine models (Weston et al., 2018; Roller et al., 2020), models either pre-trained on a previously existing Reddit dataset",
      "startOffset" : 36,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : "io (Baumgartner et al., 2020) or fine-tuned on the Blended Skill Talk (BST) dialogue tasks (Smith et al.",
      "startOffset" : 3,
      "endOffset" : 29
    }, {
      "referenceID" : 36,
      "context" : ", 2020) or fine-tuned on the Blended Skill Talk (BST) dialogue tasks (Smith et al., 2020) – that is, all the dialogue models that are compared in the study in Roller et al.",
      "startOffset" : 69,
      "endOffset" : 89
    }, {
      "referenceID" : 31,
      "context" : "test models’ robustness and generalizability beyond the collected human-written test set (Ribeiro et al., 2020; Gardner et al., 2020), and give a more comprehensive analysis of the task.",
      "startOffset" : 89,
      "endOffset" : 133
    }, {
      "referenceID" : 39,
      "context" : "However, unlike previous work (Welleck et al., 2019) that directly utilized NLI models giving a 3-way output among “entailment”, “contradiction”, and “neutral”, we modify the model with a 2-way output between “contradiction” and “non-contradiction” (either “entailment” or “neutral”) labels, as our task is centered around the detection of inconsistency.",
      "startOffset" : 30,
      "endOffset" : 52
    }, {
      "referenceID" : 8,
      "context" : "We study four base pre-trained models variants for fθ: BERT (Devlin et al., 2019), Electra (Clark et al.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 6,
      "context" : ", 2019), Electra (Clark et al., 2020), RoBERTa (Liu et al.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 22,
      "context" : ", 2020), RoBERTa (Liu et al., 2019), and Model Training Data MT MT (Strict) HB SE F1",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 3,
      "context" : "We fine-tune fθ on different combinations of NLI training data including SNLI (Bowman et al., 2015), MNLI (Williams et al.",
      "startOffset" : 78,
      "endOffset" : 99
    }, {
      "referenceID" : 27,
      "context" : ", 2018), ANLIR3 (Nie et al., 2020a)4, DNLI (Welleck et al.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 39,
      "context" : ", 2020a)4, DNLI (Welleck et al., 2019), as well as our DECODE Main training set.",
      "startOffset" : 16,
      "endOffset" : 38
    }, {
      "referenceID" : 27,
      "context" : "Overfitting on superficial patterns is a typical issue and open problem in NLU modeling (Nie et al., 2020a).",
      "startOffset" : 88,
      "endOffset" : 107
    }, {
      "referenceID" : 13,
      "context" : "consider three methods of decoding: beam search, top-k sampling (Fan et al., 2018) and sample-andrank (Adiwardana et al.",
      "startOffset" : 64,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : ", 2018) and sample-andrank (Adiwardana et al., 2020), and compare the standard and DECODE-reranked decoding methods to each other.",
      "startOffset" : 27,
      "endOffset" : 52
    }, {
      "referenceID" : 32,
      "context" : "For beam search we use the best found parameters from (Roller et al., 2020) which are beam size 10, minimum beam length 20 and beam blocking of 3-grams.",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 37,
      "context" : "(Vijayakumar et al., 2016): when the top scoring utterance is flagged as contradicting, many of the other utterances in the beam are similar responses with slight rephrases, and are flagged contradicting as well.",
      "startOffset" : 0,
      "endOffset" : 26
    } ],
    "year" : 2021,
    "abstractText" : "To quantify how well natural language understanding models can capture consistency in a general conversation, we introduce the DialoguE COntradiction DEtection task (DECODE) and a new conversational dataset containing both human-human and human-bot contradictory dialogues. We show that: (i) our newly collected dataset is notably more effective at providing supervision for the dialogue contradiction detection task than existing NLI data including those aimed to cover the dialogue domain; (ii) Transformer models that explicitly hinge on utterance structures for dialogue contradiction detection are more robust and generalize well on both analysis and outof-distribution dialogues than standard (unstructured) Transformers. We also show that our best contradiction detection model correlates well with human judgments and further provide evidence for its usage in both automatically evaluating and improving the consistency of state-of-the-art generative chatbots.",
    "creator" : "LaTeX with hyperref"
  }
}