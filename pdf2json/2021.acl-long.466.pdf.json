{
  "name" : "2021.acl-long.466.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Improving Encoder by Auxiliary Supervision Tasks for Table-to-Text Generation",
    "authors" : [ "Liang Li", "Can Ma", "Yinliang Yue", "Dayong Hu" ],
    "emails" : [ "yueyinliang}@iie.ac.cn", "superhudayong@163.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5979–5989\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5979"
    }, {
      "heading" : "1 Introduction",
      "text" : "Table-to-text generation is an essential task for text generation from structured data. It aims at automatically producing descriptive natural language text to help people obtain the salient information from the tables. Over the past several years, neural text generation methods have made significant progress on this task. Lebret et al. (2016); Wiseman et al. (2017); Bao et al. (2018) view the input table as a record sequence and model it as a machine translation task. To generate text containing more salient and well-organized facts, Sha et al. (2018); Moryossef et al. (2019); Trisedya et al. (2020);\n∗Corresponding authors: Can Ma, Yinliang Yue\nBai et al. (2020) explicitly model content selection and planning. To better represent tables, Liu et al. (2018); Nema et al. (2018); Gong et al. (2019) explicitly model the structure of a table from multiple levels or different dimensions.\nFigure 1 (a) contains basketball game statistical tables from ROTOWIRE (Wiseman et al., 2017), a benchmark of NBA basketball games. As can be seen, each entity (player or team) takes one row in the corresponding table. Moreover, each row comprises several records of different types, which describe the entity’s performance in different aspects. In terms of generating a summary from these tables, it is necessary to make reasoning to obtain some factual results from the entities’ performance and the relationships between entities. For instance, when humans describe the tables in Figure 1 (a), they usually give some factual results, such as “The Boston Celtics dominated the visiting New York Knicks” or “Isaiah Thomas was huge for Boston...”. These results need to be reasoned from the entities’ performance and the relationships between entities. Therefore, it is necessary to give the model the reasoning ability. However, previous methods do not explicitly model this ability.\nNumerical tables mean most records in these tables are numerical and are very common. For instance, 86.82% of the records and almost 86.49% of the column types are numeric in ROTOWIRE. We observe that there are different relations between records in different dimensions. For example, there are two kinds of relations in numerical tables. The first one is numerical size relation in the column dimension, i.e., in the same type column. The other is the relative importance relation in the row dimension. It refers to the relative importance of different types of records, which are in the same row, to the entity that they belong to. On the one hand, these relations may contribute to tableto-text generation. Let us take Figure 1 (a) as an example. I.Thomas’s score is 29, which is higher than other records in the column PTS. And he has three rebounds, which is lower than most other records in the column REB. Therefore, humans are more likely to describe his scores rather than his rebounds when summarizing his performance. On the other hand, a vanilla encoder may not effectively capture the relations existing in different dimensions without any auxiliary supervision.\nWe employ a hierarchical encoder, which comprises a Record Encoder and a Reasoning Module, to encode the input tables from record level and row level. Specifically, inspired by Gong et al. (2019), the Record Encoder utilizes two cascaded self-attention modules to encode the table from the column and the row dimension, respectively. Moreover, to endow the model with the reasoning ability, we first build an entity graph on the row level according to the relations between players and teams. And then, we introduce a reasoning module to perform reasoning on the graph. Furthermore, we utilize different auxiliary tasks to help the encoder capture the different relations among records. More specifically, two auxiliary tasks named Number Ranking (NR) and Importance Ranking (IR) are proposed to supervise the learning of the different parts of the Record Encoder, respectively.\nWe conducted experiments on ROTOWIRE and RW-FG(Wang, 2019) to verify the effectiveness of the proposed approach. The experimental results demonstrate that it is necessary to enable the model the reasoning ability. Moreover, the proposed two auxiliary tasks can improve the data-to-text model’s performance without introducing extra parameters. Furthermore, the results also show our method not only has a good generalization but also outperforms\nprevious methods on BLEU, Content Selection, and Content Ordering metrics."
    }, {
      "heading" : "2 Related Work",
      "text" : "Recently, neural models have been the mainstream for table-to-text generation and obtained impressive results. Early works on table-to-text generation regard it as a distinct machine translation task and view a structured table as a record sequence (Lebret et al., 2016; Wiseman et al., 2017; Bao et al., 2018). Most recent works are inspired by the traditional methods for data-to-text generation and introduce explicit content selection and planning to improve the results (Sha et al., 2018; Puduppully et al., 2019b; Moryossef et al., 2019; Trisedya et al., 2020; Bai et al., 2020), and they obtain training labels by aligning the input tables with related summaries. However, this alignment may introduce additional errors. Some works attempt to use additional knowledge to improve the quality of the generated text. Nie et al. (2018) utilize pre-executed symbolic operations on the input table in a sequence-to-sequence model to improve the fidelity of neural table-to-text generation. Chen et al. (2019) introduce the background knowledge of the entity in the table to improve results.\nIn addition to introducing external knowledge, some works learn better representation for the table by explicitly modeling the table’s structure. Liu et al. (2018) propose a structure-aware seq2seq architecture, which incorporates the filed information as the additional inputs to the table encoder. Some works (Bao et al., 2018; Nema et al., 2018; Jain et al., 2018) model the table’s representation from the row and column levels, and utilize the dual attention decoder to generate text. Gong et al. (2019) introduce the historical data for each table and utilize a self-attention-based hierarchical encoder on three dimensions (row, column, and time) to enrich the table’s representation. Furthermore, Liu et al. (2019) propose three auxiliary supervision tasks (sequence labeling, text auto-encoding, and multilabel classification) to help the encoder capture a more accurate semantic representation of the tables.\nGong et al. (2020) also explicitly model the relations between the numeric records. They pretrain a multi-layer transformer encoder to obtain records’ contextual numerical value representations. Moreover, when training the data-to-text model, they replace the record’s token embedding with its con-\ntextual representation from the pre-trained model. Differently, our Number Ranking task is trained with the data-to-text model and can supervise the model actively to capture the numeric size relation without introducing extra parameters."
    }, {
      "heading" : "3 Approach",
      "text" : ""
    }, {
      "heading" : "3.1 Record Encoder",
      "text" : "Each input instance consists of three different tables T 1, T 2, T 3, containing records about players’ performance in the home team, players’ performance in the visiting team, and the team’s overall performance. Each cell in the table is regarded as a record. Inspired by Gong et al. (2019), we utilize two self-attention modules to model each record’s contexts from the column and the row dimension, respectively. After that, we obtain the fusion representation for records by the record fusion gate.\nRecord Embedding Following previous work (Wiseman et al., 2017), we utilize four tuples to represent each record r. The four tuples include: entity r.e (the name of team or player, such as Carmelo Anthony), type r.t (e.g., PTS) and value r.v as well as feature r.f (e.g., home or visiting) which indicates whether a player or a team compete in home court or not. And we utilize 1-layer MLP to encode the embeddings of each record’s four types of information into a dense vector rembi,j , rembi,j = Relu(W e[ri,j .e; ri,j .t; ri,j .v; ri,j .f ] + b e),\nwhere i, j denote a record in the table of i-th row and j-th column, [; ] denotes the vector concatenation, W e and be are trainable parameters.\nColumn-wise Encoder To capture the numeric size relation between records, we adopt a selfattention module to model record in the context of other records in the same column and obtain the column dimension representation vector rcoli,j as:\nαcoli,j,i′ ∝ exp(W col2 tanh(W col1 [rembi,j ; rembi′,j ]) (1)\nr̃coli,j = R∑ i′=1,i′ 6=i αcoli,j,i′r emb i′,j (2)\nrcoli,j =W col 3 [r̃ col i,j ; r emb i,j ] (3)\nwhere W col1 , W col 2 and W col 3 are trainable parameters, R represents the number of rows in the table.\nRow-wise Encoder Considering the size relation captured by the Column-wise Encoder (CE) may help the learning of importance relation on row level, we have the Column-wise Encoder and the Row-wise Encoder (RE) in series (as shown in Figure 2). In other words, the input of RE is rcoli,j rather than rembi,j . We use another self-attention module, similar to the CE, to obtain the row dimension representation rrowi,j for records.\nRecord Fusion Gate The record representations from different dimensions contribute differently in\nreflecting the record’s information. Therefore, we utilize a fusion gate to combine the two dimension representations adaptively(Gong et al., 2019). First, we concatenate the two dimension representations of a record and utilize an MLP to obtain a general representation for it as rgeni,j . Then, we compare the column dimension representation with rgeni,j to obtain its important score:\nscoli,j ∝ exp(W f 2 tanh(W f 1 [r gen i ; r col i,j ])) (4)\nwhere W f1 and W f 2 are trainable parameters. Equally, we obtain the important score srowi,j for the row dimension representation rrowi,j . Finally, we obtain the fused record representation rfi,j by weighted sum scoli,j r col i,j + s row i,j r row i,j . The fused record representations {rfi,j} R,C i=1,j=1 will be used as the input of the text decoder."
    }, {
      "heading" : "3.2 Reasoning Module",
      "text" : "As mentioned in Section 1, we observe some factual results in text that require reasoning from the entities’ performance and the relationships between them. Therefore, it is necessary to enable model the reasoning ability. To achieve this, we primarily build an entity graph according to the entities’ relationships in input tables, as shown in Figure 1 (c). And then, we leverage Graph Neural Networks (GNN) to perform reasoning. Following, we describe the details of the reasoning process.\nPrimarily, we obtain the initialized representation for each entity in tables by the Entity Node Initialization module (ENI). Considering that different records in the same row may not contribute the same, we combine them dynamically by attention mechanism. We first compute a general representation vector egeni for the entity ei, which is given by mean-pooling over the same row records rfi,1, r f i,2, ..., r f i,C . Then we compare each record in the i-th row with egeni and obtain the initialized entity representation e0i by weighted sum:\nαri,j ∝ exp(W r2 tanh(W r1 [e gen i ; r f i,j ])) (5)\ne0i = j=C∑ j=1 αri,jr f i,j (6)\nAfter obtaining the initial representations of entities, we adopt graph neural networks to propagate entity node information to their neighbors. Inspired by GAT(Velickovic et al., 2018), we use multi-head\nattention to measure the relatedness between target entity node ei and its neighbor nodes at layer l:\nαli,j =MultiHeadAttention(e l−1 i , e l−1 j ) (7)\nwhere j ∈ Ni and Ni means the neighbor nodes set of target entity ei.\nThe neighbor entities include information that is not relevant to the target entity. Therefore, we modify the way the information flow in GAT. Explicitly, we incorporate gate mechanisms into information aggregation to filter out noises from neighbor nodes and extract useful information, which we name GatedGAT. The representation eli of ei at layer l is calculated as follows:\neli = gate l i ∗ el−1i + (1− gate l i) ∗ ẽli (8) ẽli = ELU( ∑ j∈Ni αli,je l−1 j ) (9)\ngateli = sigmoid(W l[el−1i ; ẽ l i]) (10)\nwhere W l is a learnable parameter. The entities’ representations {eLi }Ri=1 at the last layer L are employed in text decoder."
    }, {
      "heading" : "3.3 Decoder with Dual Attention",
      "text" : "To make use of record-level and row-level semantics information, we adopt the dual attention mechanism. Specifically, at decoding step t, the input of the LSTM unit is the embedding of the previously predicted word yt−1. And given the decoder state dt, we first calculate the row-level attention βt,i, which is based on the similarity between the decoder state dt and the entities’ representations {eLi }Ri=1. Then we compute the record-level attention αt,i over all the record representations {rfi,j} R,C i,j which are normalized among records in the same row. Finally, we fuse these two-level attention and obtain the context representation as:\nα ′ t,i,j = αt,iβt,i,j (11)\ncdt = R∑ i=1 C∑ j=1 α ′ t,i,jri,j (12)\nGiven a reference output {yi}Ti=1, we use the cross-entropy loss as the objective function of tableto-text generation:\nLlm = − T∑ i=1 pθ(yt|y1:t−1; cdt ) (13)"
    }, {
      "heading" : "3.4 Auxiliary Supervision Task",
      "text" : "Liu et al. (2019) have shown that a single encoder without any auxiliary assistant may not be effective to capture the accurate semantic representation. Inspired by this, we propose two auxiliary tasks, Number Ranking (NR) and Importance Ranking (IR), to help the Column-wise Encoder and the Row-wise Encoder capture the size relation and the relative importance relation among records respectively.\nNumber Ranking In practice, many tables mainly comprise numeric records. Different from text-type content, the numerical content contains less semantic information but the size relation. The size relation means the value of a record is larger or smaller than others, and it plays an essential role in records selection. For example, humans tend to focus on the highest scores or the fewest faults in a basketball game table. Therefore, it is necessary to incorporate size relation into record representation. To achieve this, we propose an auxiliary supervision task named Number Ranking (NR) to supervise the learning of the Column-wise Encoder. As shown in Figure 2 top, we take a list of records in column PTS to illustrate how it works. Specifically, we regard the PTS column of the table as an out-of-order set of records C = r1, r2, ..., rR, and the goal is to generate a sequence of record pointers in descending order according to their value. We adopt the Pointer Networks (Vinyals et al., 2015) to solve this problem and the output of Columnwise Encoder rcoli (we omitted the indices on the column dimension) as its input. Let z = z1, ..., zR denote the sequence of the ranked records’ indices. Each zk points to an input record and is between 1 and R. As shown in Figure 2, we use an LSTM as the decoder. The MeanPooling({ri}Ri=1) is used as the initialization of the first hidden state of the decoder. At each decoding step t, we calculate a distribution over the input records:\nht = LSTM(ht−1, r col zt−1) (14) pnt,i ∝ exp(Wnr[ht; rcoli ]) (15)\nwhere Wnr is a trainable parameter, and pnt,i denotes the probability that the output points to the record ri at step t. We take the cross-entropy loss for this task:\nLnr = − C∑ j=1 R∑ i=1 log pni,zi (16)\nImportance Ranking When people describe a player’s performance in a basketball game, they tend to focus on his relatively important record and describe these firstly. Consequently, we introduce the Importance Ranking task (IR) to supervise the Row-wise Encoder to capture the relative importance relations between records in the same row. This task’s input is a sequence record in the same row, and the output is a sequence of records in descending order of the records’ importance. We employ a pointer network similar to the one used in the Number Ranking task to model this task. However, different from the records in the same column, these in the same row cannot be directly compared as they represent different meanings. To address this issue, we take the rank of each record in the column as an importance indicator. Figure 2 left bottom shows an example of calculating the importance scores for records in the last row of the table.\nThe input of the decoder is the output of the Row-wise Encoder {rrowj }Rj=1. And the output is the ascending order of the input, according to the records’ importance scores. Let pst,j denote the probability of pointing to record rj at decoding step t, the loss function for this task is:\nLir = − R∑ i=1 C∑ j=1 log psj,zj (17)"
    }, {
      "heading" : "3.5 Loss Function and Training",
      "text" : "These two tasks are trained together with the tableto-text task, and the overall objective function consists of three parts:\nL = Llm + λ1Lnr + λ2Lir (18)\nwhere λ1 and λ2 are tunable hyper-parameters."
    }, {
      "heading" : "4 Experiment",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset and Evaluation Metrics",
      "text" : "We conduct experiments on both ROTOWIRE and RW-FG datasets. They all comprise pairs of NBA basketball game statistics and summaries. There are two main differences between ROTOWIRE and RW-FG. The first is the team statistic table in later containing more numeric records. The other is RW-FG removes the unsupported sentences by the input tables. We use the official training, development, and test splits for both datasets, which are 3,398/727/728 and 5,232/1,125/1,119, respectively.\nFollowing previous works, we use BLEU and three extractive evaluation metrics, Relation Generation (RG), Content Selection (CS), and Content Ordering (CO) (Wiseman et al., 2017) to evaluate the table-to-text results. More specifically, RG measures the content fidelity of generated text, CS measures how well the generated text matches the reference in selecting which records to generate, and CO measures the ability on context planning. We refer the readers to Wiseman et al. (2017)’s paper for more detailed information on these extractive metrics.\nWe apply Accuracy (Acc) and normalized Damerau Levenshtein Distance (DLD) (Brill and Moore, 2000) to evaluate the two auxiliary supervision tasks. Accuracy measures the percentage of record sequences for which their absolute positions are correctly predicted (Logeswaran et al., 2018)."
    }, {
      "heading" : "4.2 Implementation Details",
      "text" : "To make a fair comparison, we follow the configurations in (Puduppully et al., 2019a; Gong et al., 2019). For the table-to-text model, we set word embedding and LSTM decoder hidden size as 600. We set GatedGat’s layer as 2 and the numbers of heads as 2. We employ a two-layer LSTM decoder with Input feeding during text generation.\nWe apply dropout at a rate 0.3. For text decoding, we use BPTT and set the truncate size to 100. We set the beam size to 5 during inference. For the two auxiliary tasks, we employ two one-layer LSTM as the decoder and set the LSTM decoder hidden size as 600, respectively. We adjust λ1 between 0.8 and 1.0, λ2 between 0.2-0.4. Finally, we set them to 0.9 and 0.25 on ROTOWIRE, 1.0 and 0.4 on RW-FG. For inferring, we use the greedy search algorithm. All experiments are conducted on an NVIDIA Tesla V100. Code of our model can be found at https://github.com/liang8qi/ Data2TextWithAuxiliarySupervision."
    }, {
      "heading" : "4.3 Baselines",
      "text" : "We compare our method with several strong baselines, including:\n• TEMP (Wiseman et al., 2017) is a templatebased method. We refer the readers to this paper for more detailed information on templates. • CC (Wiseman et al., 2017) is a standard encoder-decoder system with conditional copy mechanism. • NCP (Puduppully et al., 2019a) and NCP + TR (Wang, 2019) are two Conditional Copy models with the explicit content planning.\nThe latter improves NCP by introducing a table restructure loss. • ENT (Puduppully et al., 2019b) is a method that creates entity-specific representations and generates text using hierarchical attention over the input table and entity memory. • HETD (Gong et al., 2019) is a method modeling table from three different dimensions (Row, Column and, Time). • DU & DUV (Gong et al., 2020): the DU brings the sense of value comparison into content planning. Furthermore, DUV introduces content plan verification into DU."
    }, {
      "heading" : "4.4 Main Results",
      "text" : "Automatic Evaluation Our results on the two test datasets are summarized in Table 1. For ROTOWIRE, compared with previous neural models, our method achieves state-of-the-art results on Content Selection (CS), Content Ordering (CO), and BLEU. More specifically, compared with the previous best neural models, we obtain more than 4 improvement on CS-P and achieve the best results on CS-R. This implies our method can generate text that contains more salient records. Compared with NCP, DU, and DUV, our method scores the highest on CO, even without explicitly modeling content selection and planning. This indicates that our model can better organize the records when generating a summary for the input tables. We consider there are two main reasons. The first is that our Reasoning Module can learn a better entity representation on row level. The other is that our proposed two auxiliary tasks can supervise the Record Encoder to learn a number-aware and relative importance-aware record representation. As a result, the data-to-text model can make good con-\ntent planning by considering the entity’s performance and the relative importance of the record.\nAs shown in Table 1, the results on RW-FG follow a pattern similar to ROTOWIRE. We notice that all models perform better on RW-FG than on ROTOWIRE. We consider that the improvement comes from the purification of data in RW-FG. Wang (2019) removes the sentences that are not supported by the input tables, which reduces the noise in the text and improves the dataset’s quality. Due to this, we can obtain more accurate content planning labels from the dataset to train the models (NCP, NCP+TR) that explicitly model content planning and lead to better performance. Therefore, NCP outperforms ENT on RW-FG. However, the purification may make the task easier because some sentences that do not be supported by the tables directly but can be obtained by reasoning may also be removed. This may weaken the Reasoning Module of our model. Nevertheless, we still outperform the compared baselines.\nTable 2 shows our model’s performance, which is trained together with the two auxiliary tasks on the two auxiliary tasks. We compare it with two baselines. The first is Original, which denotes a method that takes the input record sequence as the outputs. Moreover, we separately train our model on the two auxiliary tasks, denoted as Separate. As a result, our model achieves comparable performance to Separate and is much better than Original, even only using the greedy search at testing. The results indicate that the two auxiliary tasks can help the Record Encoder capture the size relation and relative importance relation among records.\nAblation Study First, we examine the effect of changes in the model structure on the results. From Table 3, Our Model means our data-to-text model without two auxiliary tasks. We change the connection mode between the Column-wise Encoder\n(CE) and the Row-wise Encoder (RE) to parallel from series (- Series). Moreover, we replace the Reasoning Module with a row-level encoder with the content selection gate (- RM), which is proposed by Puduppully et al. (2019a). According to the results, the serial connection and the Reasoning Module contribute to the overall performance because BLEU, CS, and CO drop significantly after subtracting them from the full model.\nFurthermore, we investigate the impact of the two auxiliary tasks on table-to-text generation. Table 3 shows that both Number Ranking (NR) and Importance Ranking (IR) tasks can improve our basic model. This indicates that it is necessary to explicitly model the size relation and relative importance relation between records. We notice that the model’s performance is degraded on CS-F1 and CO when only the IR task is introduced. On the one hand, we believe this is because the modeling of relative importance relation in the row dimension between records depends heavily on its size relation in the column dimension. On the other hand, the CE cannot accurately capture the size relation between records without direct supervision.\nFinally, we compare the method that introduces additional feature vectors of the ranking of number and relative importance to Record Embedding with the two auxiliary tasks. Specifically, we first introduce the embedding of ranking of the number (+ NE) and further add the embedding of the relative importance of records (+ IE). As shown in the third section in Table 3, the NE only improves the model on RG. Moreover when the IE is incorporated, the model achieves better performance on almost all metrics. However, the improvement is not as significant as the auxiliary tasks. We believe it may be a better way to effectively capture the accurate semantic representation by introducing auxiliary supervision tasks than adding feature vectors directly.\nGeneralization Study Our method can be applied to the existing works, especially those that explicitly model content selection and planning (NCP, DUV), to improve their performance. To exam our method’s generalization, we combine our method with NCP and conduct experiments on the ROTOWIRE development set. The results are summarized in Table 4. First, we use the released code to retrain the NCP model. And then, we replace the NCP’s content selection encoder with our hierarchical encoder. As can be seen, our hierarchical encoder with the Reasoning Module improves the NCP model on almost all evaluation metrics. Moreover, we train the model with the proposed two auxiliary supervision tasks. The performance of the model is further improved. This indicates that our method has a good generalization, as it can be easily adapted to other methods and improve their performance.\nHuman Evaluation To examine whether human judgments corroborate improvements in automatic evaluation metrics, we conducted a human evaluation. Three graduate students with basketball background knowledge and good English reading ability were invited to conduct the evaluation. We compared our best performing model against Gold, NCP, ENT, and HETD. Specifically, we randomly selected 30 games from the test set, and each game is rated by three workers. For each game, we arranged every 5-tuple of summaries into ten pairs. Given each pair, the participants were asked to choose which one is better according to five criteria: Supporting (does the summary contain more supported facts?), Contradicting (does the summary contain more contradicting facts?), Grammaticality (is the summary fluent and grammatical?), Coherence (do the sentences, in summary, follow a coherent discourse?), and Conciseness (does the summary contain less redundant information and repetitions?). Following previous work (Puduppully et al., 2019a), we calculated a model’s score for each criterion as the difference between the per-\ncentage of times when the model is chosen as the best and the percentage of times when the model is chosen as the worst.\nThe results are summarized in Table 5. As can be seen, the gold texts have significant advantages in contradicting, grammaticality, coherence, and conciseness. Compared with other neural methods, our method receives the highest scores in coherence and grammaticality. This implies that our method can generate texts that contain well-organized facts. Though the ENT model outperforms our model in contradicting and conciseness, our method can be easily applied to it, which we leave for future work."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this work, we mainly make two contributions. The first one is we introduce a reasoning module into a hierarchical table encoder, which enables the model reasoning ability. Moreover, we present to utilize the different auxiliary supervision tasks to help the encoder capture the different relations between records. In detail, the Number Ranking (NR) task is proposed to supervise the Column-wise Encoder to model the numeric size relation between records in the same column. And the Importance Ranking (IR) task helps the Row-wise Encoder capture the relative importance between records in the same row. Experimental results conducted on ROTOWIRE and RW-FG datasets demonstrate the effectiveness of our method. Furthermore, we migrate our method to the NCP model and significantly improve its performance on ROWTOWIRE. This indicates that our proposed method has a good generalization."
    }, {
      "heading" : "A Development Performance",
      "text" : "We present the performance of the compared baselines and our model on ROTOWIRE1 and RW-FG2 development sets in Table 6. As can be seen, the test datasets’ results in Table 1 follow a pattern similar to the development sets.\n1https://github.com/harvardnlp/boxscore-data 2https://github.com/wanghm92/rw fg\nB Impact of different Ranking Directions\nWe also explore the impact of different settings for Number Ranking and Importance Ranking on the data-to-text model. The results are summarized in Table 7. We observe that compared with the basic model, almost all the settings can improve the data-to-text model on Content Selection(CS), Content Ordering(CO), and BLEU. This indicates the proposed two tasks are effective and robust."
    } ],
    "references" : [ {
      "title" : "Infobox-to-text generation with tree-like planning based attention network",
      "author" : [ "Yang Bai", "Ziran Li", "Ning Ding", "Ying Shen", "HaiTao Zheng." ],
      "venue" : "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020,",
      "citeRegEx" : "Bai et al\\.,? 2020",
      "shortCiteRegEx" : "Bai et al\\.",
      "year" : 2020
    }, {
      "title" : "Tableto-text: Describing table region with natural language",
      "author" : [ "Junwei Bao", "Duyu Tang", "Nan Duan", "Zhao Yan", "Yuanhua Lv", "Ming Zhou", "Tiejun Zhao." ],
      "venue" : "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18),",
      "citeRegEx" : "Bao et al\\.,? 2018",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2018
    }, {
      "title" : "An improved error model for noisy channel spelling correction",
      "author" : [ "Eric Brill", "Robert C. Moore." ],
      "venue" : "Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 286– 293, Hong Kong. Association for Computational",
      "citeRegEx" : "Brill and Moore.,? 2000",
      "shortCiteRegEx" : "Brill and Moore.",
      "year" : 2000
    }, {
      "title" : "Enhancing neural data-to-text generation models with external background knowledge",
      "author" : [ "Shuang Chen", "Jinpeng Wang", "Xiaocheng Feng", "Feng Jiang", "Bing Qin", "Chin-Yew Lin." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Nat-",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Enhancing content planning for table-to-text generation with data understanding and verification",
      "author" : [ "Heng Gong", "Wei Bi", "Xiaocheng Feng", "Bing Qin", "Xiaojiang Liu", "Ting Liu." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020,",
      "citeRegEx" : "Gong et al\\.,? 2020",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2020
    }, {
      "title" : "Table-to-text generation with effective hierarchical encoder on three dimensions (row, column and time)",
      "author" : [ "Heng Gong", "Xiaocheng Feng", "Bing Qin", "Ting Liu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Gong et al\\.,? 2019",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2019
    }, {
      "title" : "A mixed hierarchical attention based encoder-decoder approach for standard table summarization",
      "author" : [ "Parag Jain", "Anirban Laha", "Karthik Sankaranarayanan", "Preksha Nema", "Mitesh M. Khapra", "Shreyas Shetty." ],
      "venue" : "Proceedings of the 2018 Conference of",
      "citeRegEx" : "Jain et al\\.,? 2018",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural text generation from structured data with application to the biography domain",
      "author" : [ "Rémi Lebret", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1203–1213,",
      "citeRegEx" : "Lebret et al\\.,? 2016",
      "shortCiteRegEx" : "Lebret et al\\.",
      "year" : 2016
    }, {
      "title" : "Hierarchical encoder with auxiliary supervision for neural table-to-text generation: Learning better representation for tables",
      "author" : [ "Tianyu Liu", "Fuli Luo", "Qiaolin Xia", "Shuming Ma", "Baobao Chang", "Zhifang Sui." ],
      "venue" : "The Thirty-Third AAAI Con-",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Table-to-text generation by structure-aware seq2seq learning",
      "author" : [ "Tianyu Liu", "Kexiang Wang", "Lei Sha", "Baobao Chang", "Zhifang Sui." ],
      "venue" : "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Sentence ordering and coherence modeling using recurrent neural networks",
      "author" : [ "Lajanugen Logeswaran", "Honglak Lee", "Dragomir R. Radev." ],
      "venue" : "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative",
      "citeRegEx" : "Logeswaran et al\\.,? 2018",
      "shortCiteRegEx" : "Logeswaran et al\\.",
      "year" : 2018
    }, {
      "title" : "Step-by-step: Separating planning from realization in neural data-to-text generation",
      "author" : [ "Amit Moryossef", "Yoav Goldberg", "Ido Dagan." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Moryossef et al\\.,? 2019",
      "shortCiteRegEx" : "Moryossef et al\\.",
      "year" : 2019
    }, {
      "title" : "Generating descriptions from structured data using a bifocal attention mechanism and gated orthogonalization",
      "author" : [ "Preksha Nema", "Shreyas Shetty", "Parag Jain", "Anirban Laha", "Karthik Sankaranarayanan", "Mitesh M. Khapra." ],
      "venue" : "Proceedings of the 2018",
      "citeRegEx" : "Nema et al\\.,? 2018",
      "shortCiteRegEx" : "Nema et al\\.",
      "year" : 2018
    }, {
      "title" : "Operation-guided neural networks for high fidelity data-to-text generation",
      "author" : [ "Feng Nie", "Jinpeng Wang", "Jin-Ge Yao", "Rong Pan", "Chin-Yew Lin." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Nie et al\\.,? 2018",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2018
    }, {
      "title" : "Data-to-text generation with content selection and planning",
      "author" : [ "Ratish Puduppully", "Li Dong", "Mirella Lapata." ],
      "venue" : "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial In-",
      "citeRegEx" : "Puduppully et al\\.,? 2019a",
      "shortCiteRegEx" : "Puduppully et al\\.",
      "year" : 2019
    }, {
      "title" : "Data-to-text generation with entity modeling",
      "author" : [ "Ratish Puduppully", "Li Dong", "Mirella Lapata." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2023–2035, Florence, Italy. Association for",
      "citeRegEx" : "Puduppully et al\\.,? 2019b",
      "shortCiteRegEx" : "Puduppully et al\\.",
      "year" : 2019
    }, {
      "title" : "Order-planning neural text generation from structured data",
      "author" : [ "Lei Sha", "Lili Mou", "Tianyu Liu", "Pascal Poupart", "Sujian Li", "Baobao Chang", "Zhifang Sui." ],
      "venue" : "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-",
      "citeRegEx" : "Sha et al\\.,? 2018",
      "shortCiteRegEx" : "Sha et al\\.",
      "year" : 2018
    }, {
      "title" : "Sentence generation for entity description with content-plan attention",
      "author" : [ "Bayu Distiawan Trisedya", "Jianzhong Qi", "Rui Zhang." ],
      "venue" : "AAAI, pages 9057–9064.",
      "citeRegEx" : "Trisedya et al\\.,? 2020",
      "shortCiteRegEx" : "Trisedya et al\\.",
      "year" : 2020
    }, {
      "title" : "Graph attention networks",
      "author" : [ "Petar Velickovic", "Guillem Cucurull", "Arantxa Casanova", "Adriana Romero", "Pietro Liò", "Yoshua Bengio." ],
      "venue" : "6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May",
      "citeRegEx" : "Velickovic et al\\.,? 2018",
      "shortCiteRegEx" : "Velickovic et al\\.",
      "year" : 2018
    }, {
      "title" : "Pointer networks",
      "author" : [ "Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly." ],
      "venue" : "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec,",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Revisiting challenges in datato-text generation with fact grounding",
      "author" : [ "Hongmin Wang." ],
      "venue" : "Proceedings of the 12th International Conference on Natural Language Generation, pages 311–322, Tokyo, Japan. Association for Computational Linguistics.",
      "citeRegEx" : "Wang.,? 2019",
      "shortCiteRegEx" : "Wang.",
      "year" : 2019
    }, {
      "title" : "Challenges in data-to-document generation",
      "author" : [ "Sam Wiseman", "Stuart Shieber", "Alexander Rush." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2253–2263, Copenhagen, Denmark. Association for",
      "citeRegEx" : "Wiseman et al\\.,? 2017",
      "shortCiteRegEx" : "Wiseman et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "Figure 1 (a) contains basketball game statistical tables from ROTOWIRE (Wiseman et al., 2017), a benchmark of NBA basketball games.",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 20,
      "context" : "We conducted experiments on ROTOWIRE and RW-FG(Wang, 2019) to verify the effectiveness of the proposed approach.",
      "startOffset" : 46,
      "endOffset" : 58
    }, {
      "referenceID" : 7,
      "context" : "Early works on table-to-text generation regard it as a distinct machine translation task and view a structured table as a record sequence (Lebret et al., 2016; Wiseman et al., 2017; Bao et al., 2018).",
      "startOffset" : 138,
      "endOffset" : 199
    }, {
      "referenceID" : 21,
      "context" : "Early works on table-to-text generation regard it as a distinct machine translation task and view a structured table as a record sequence (Lebret et al., 2016; Wiseman et al., 2017; Bao et al., 2018).",
      "startOffset" : 138,
      "endOffset" : 199
    }, {
      "referenceID" : 1,
      "context" : "Early works on table-to-text generation regard it as a distinct machine translation task and view a structured table as a record sequence (Lebret et al., 2016; Wiseman et al., 2017; Bao et al., 2018).",
      "startOffset" : 138,
      "endOffset" : 199
    }, {
      "referenceID" : 16,
      "context" : "Most recent works are inspired by the traditional methods for data-to-text generation and introduce explicit content selection and planning to improve the results (Sha et al., 2018; Puduppully et al., 2019b; Moryossef et al., 2019; Trisedya et al., 2020; Bai et al., 2020), and they obtain training labels by aligning the input tables with related summaries.",
      "startOffset" : 163,
      "endOffset" : 272
    }, {
      "referenceID" : 15,
      "context" : "Most recent works are inspired by the traditional methods for data-to-text generation and introduce explicit content selection and planning to improve the results (Sha et al., 2018; Puduppully et al., 2019b; Moryossef et al., 2019; Trisedya et al., 2020; Bai et al., 2020), and they obtain training labels by aligning the input tables with related summaries.",
      "startOffset" : 163,
      "endOffset" : 272
    }, {
      "referenceID" : 11,
      "context" : "Most recent works are inspired by the traditional methods for data-to-text generation and introduce explicit content selection and planning to improve the results (Sha et al., 2018; Puduppully et al., 2019b; Moryossef et al., 2019; Trisedya et al., 2020; Bai et al., 2020), and they obtain training labels by aligning the input tables with related summaries.",
      "startOffset" : 163,
      "endOffset" : 272
    }, {
      "referenceID" : 17,
      "context" : "Most recent works are inspired by the traditional methods for data-to-text generation and introduce explicit content selection and planning to improve the results (Sha et al., 2018; Puduppully et al., 2019b; Moryossef et al., 2019; Trisedya et al., 2020; Bai et al., 2020), and they obtain training labels by aligning the input tables with related summaries.",
      "startOffset" : 163,
      "endOffset" : 272
    }, {
      "referenceID" : 0,
      "context" : "Most recent works are inspired by the traditional methods for data-to-text generation and introduce explicit content selection and planning to improve the results (Sha et al., 2018; Puduppully et al., 2019b; Moryossef et al., 2019; Trisedya et al., 2020; Bai et al., 2020), and they obtain training labels by aligning the input tables with related summaries.",
      "startOffset" : 163,
      "endOffset" : 272
    }, {
      "referenceID" : 1,
      "context" : "Some works (Bao et al., 2018; Nema et al., 2018; Jain et al., 2018) model the table’s representation from the row and column levels, and utilize the dual attention decoder to generate text.",
      "startOffset" : 11,
      "endOffset" : 67
    }, {
      "referenceID" : 12,
      "context" : "Some works (Bao et al., 2018; Nema et al., 2018; Jain et al., 2018) model the table’s representation from the row and column levels, and utilize the dual attention decoder to generate text.",
      "startOffset" : 11,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "Some works (Bao et al., 2018; Nema et al., 2018; Jain et al., 2018) model the table’s representation from the row and column levels, and utilize the dual attention decoder to generate text.",
      "startOffset" : 11,
      "endOffset" : 67
    }, {
      "referenceID" : 21,
      "context" : "Record Embedding Following previous work (Wiseman et al., 2017), we utilize four tuples to represent each record r.",
      "startOffset" : 41,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "Therefore, we utilize a fusion gate to combine the two dimension representations adaptively(Gong et al., 2019).",
      "startOffset" : 91,
      "endOffset" : 110
    }, {
      "referenceID" : 18,
      "context" : "Inspired by GAT(Velickovic et al., 2018), we use multi-head attention to measure the relatedness between target entity node ei and its neighbor nodes at layer l:",
      "startOffset" : 15,
      "endOffset" : 40
    }, {
      "referenceID" : 19,
      "context" : "We adopt the Pointer Networks (Vinyals et al., 2015) to solve this problem and the output of Columnwise Encoder rcol i (we omitted the indices on the column dimension) as its input.",
      "startOffset" : 30,
      "endOffset" : 52
    }, {
      "referenceID" : 21,
      "context" : "Following previous works, we use BLEU and three extractive evaluation metrics, Relation Generation (RG), Content Selection (CS), and Content Ordering (CO) (Wiseman et al., 2017) to evaluate the table-to-text results.",
      "startOffset" : 155,
      "endOffset" : 177
    }, {
      "referenceID" : 2,
      "context" : "We apply Accuracy (Acc) and normalized Damerau Levenshtein Distance (DLD) (Brill and Moore, 2000) to evaluate the two auxiliary supervision tasks.",
      "startOffset" : 74,
      "endOffset" : 97
    }, {
      "referenceID" : 10,
      "context" : "Accuracy measures the percentage of record sequences for which their absolute positions are correctly predicted (Logeswaran et al., 2018).",
      "startOffset" : 112,
      "endOffset" : 137
    }, {
      "referenceID" : 14,
      "context" : "To make a fair comparison, we follow the configurations in (Puduppully et al., 2019a; Gong et al., 2019).",
      "startOffset" : 59,
      "endOffset" : 104
    }, {
      "referenceID" : 5,
      "context" : "To make a fair comparison, we follow the configurations in (Puduppully et al., 2019a; Gong et al., 2019).",
      "startOffset" : 59,
      "endOffset" : 104
    }, {
      "referenceID" : 21,
      "context" : "• CC (Wiseman et al., 2017) is a standard encoder-decoder system with conditional copy mechanism.",
      "startOffset" : 5,
      "endOffset" : 27
    }, {
      "referenceID" : 14,
      "context" : "• NCP (Puduppully et al., 2019a) and NCP + TR (Wang, 2019) are two Conditional Copy models with the explicit content planning.",
      "startOffset" : 6,
      "endOffset" : 32
    }, {
      "referenceID" : 20,
      "context" : ", 2019a) and NCP + TR (Wang, 2019) are two Conditional Copy models with the explicit content planning.",
      "startOffset" : 22,
      "endOffset" : 34
    }, {
      "referenceID" : 15,
      "context" : "• ENT (Puduppully et al., 2019b) is a method that creates entity-specific representations and generates text using hierarchical attention over the input table and entity memory.",
      "startOffset" : 6,
      "endOffset" : 32
    }, {
      "referenceID" : 5,
      "context" : "• HETD (Gong et al., 2019) is a method modeling table from three different dimensions (Row, Column and, Time).",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 4,
      "context" : "• DU & DUV (Gong et al., 2020): the DU brings the sense of value comparison into con-",
      "startOffset" : 11,
      "endOffset" : 30
    }, {
      "referenceID" : 14,
      "context" : "Following previous work (Puduppully et al., 2019a), we calculated a model’s score for each criterion as the difference between the per-",
      "startOffset" : 24,
      "endOffset" : 50
    } ],
    "year" : 2021,
    "abstractText" : "Table-to-text generation aims at automatically generating natural text to help people conveniently obtain salient information in tables. Although neural models for table-to-text have achieved remarkable progress, some problems are still overlooked. Previous methods cannot deduce the factual results from the entity’s (player or team) performance and the relations between entities. To solve this issue, we first build an entity graph from the input tables and introduce a reasoning module to perform reasoning on the graph. Moreover, there are different relations (e.g., the numeric size relation and the importance relation) between records in different dimensions. And these relations may contribute to the data-to-text generation. However, it is hard for a vanilla encoder to capture these. Consequently, we propose to utilize two auxiliary tasks, Number Ranking (NR) and Importance Ranking (IR), to supervise the encoder to capture the different relations. Experimental results on ROTOWIRE and RW-FG show that our method not only has a good generalization but also outperforms previous methods on several metrics: BLEU, Content Selection, Content Ordering.",
    "creator" : "LaTeX with hyperref"
  }
}