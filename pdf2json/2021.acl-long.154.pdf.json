{
  "name" : "2021.acl-long.154.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "UXLA: A Robust Unsupervised Data Augmentation Framework for Zero-Resource Cross-Lingual NLP",
    "authors" : [ "M Saiful Bari", "Tasnim Mohiuddin", "Shafiq Joty" ],
    "emails" : [ "{bari0001@e.,", "mohi0004@e.,", "srjoty@}ntu.edu.sg" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1978–1992\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1978"
    }, {
      "heading" : "1 Introduction",
      "text" : "Self-supervised learning in the form of pretrained language models (LM) has been the driving force in developing state-of-the-art NLP systems in recent years. These methods typically follow two basic steps, where a supervised task-specific finetuning follows a large-scale LM pretraining (Radford et al., 2019). However, getting labeled data for every target task in every target language is difficult, especially for low-resource languages.\nRecently, the pretrain-finetune paradigm has also been extended to multi-lingual setups to train effective multi-lingual models that can be used for zero-shot cross-lingual transfer. Jointly trained deep multi-lingual LMs like mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) coupled\n∗Equal contribution\nwith supervised fine-tuning in the source language have been quite successful in transferring linguistic and task knowledge from one language to another without using any task label in the target language. The joint pretraining with multiple languages allows these models to generalize across languages.\nDespite their effectiveness, recent studies (Pires et al., 2019; K et al., 2020) have also highlighted one crucial limiting factor for successful crosslingual transfer. They all agree that the crosslingual generalization ability of the model is limited by the (lack of) structural similarity between the source and target languages. For example, for transferring mBERT from English, K et al. (2020) report about 23.6% accuracy drop in Hindi (structurally dissimilar) compared to 9% drop in Spanish (structurally similar) in cross-lingual natural language inference (XNLI). The difficulty level of transfer is further exacerbated if the (dissimilar) target language is low-resourced, as the joint pretraining step may not have seen many instances from this language in the first place. In our experiments (§3.2), in cross-lingual NER (XNER), we report F1 reductions of 28.3% in Urdu and 30.4% in Burmese for XLM-R, which is trained on a much larger multilingual dataset than mBERT.\nOne attractive way to improve cross-lingual generalization is to perform data augmentation (Simard et al., 1998), and train the model on examples that are similar but different from the labeled data in the source language. Formalized by the Vicinal Risk Minimization (VRM) principle (Chapelle et al., 2001), such data augmentation methods have shown impressive results in vision (Zhang et al., 2018; Berthelot et al., 2019). These methods enlarge the support of the training distribution by generating new data points from a vicinity distribution around each training example. For images, the vicinity of a training image can be defined by a set of operations like rotation and scaling, or by\nlinear mixtures of features and labels (Zhang et al., 2018). However, when it comes to text, such unsupervised augmentation methods have rarely been successful. The main reason is that unlike images, linguistic units are discrete and a smooth change in their embeddings may not result in a plausible linguistic unit that has similar meanings.\nIn NLP, to the best of our knowledge, the most successful augmentation method has so far been back-translation (Sennrich et al., 2016) which paraphrases an input sentence through round-trip translation. However, it requires parallel data to train effective machine translation systems, acquiring which can be more expensive for low-resource languages than annotating the target language data. Furthermore, back-translation is only applicable in a supervised setup and to tasks where it is possible to find the alignments between the original labeled entities and the back-translated entities, such as in question answering (Yu et al., 2018). Other related work includes contextual augmentation (Kobayashi, 2018), conditional BERT (Wu et al., 2018) and AUG-BERT (Shi et al., 2019). These methods use a constrained augmentation that alters a pretrained LM to a label-conditional LM for a specific task. Since they rely on labels, their application is limited by the availability of enough task labels.\nIn this work, we propose UXLA, a robust unsupervised cross-lingual augmentation framework for improving cross-lingual generalization of multilingual LMs. UXLA augments data from the unlabeled training examples in the target language as well as from the virtual input samples generated from the vicinity distribution of the source and target language sentences. With the augmented data, it performs simultaneous self-learning with an effective distillation strategy to learn a strongly adapted cross-lingual model from noisy (pseudo) labels for the target language task. We propose novel ways to generate virtual sentences using a multilingual masked LM (Conneau et al., 2020), and get reliable task labels by simultaneous multilingual co-training. This co-training employs a twostage co-distillation process to ensure robust transfer to dissimilar and/or low-resource languages.\nWe validate the effectiveness and robustness of UXLA by performing extensive experiments on three diverse zero-resource cross-lingual transfer tasks–XNER, XNLI, and PAWS-X, which posit different sets of challenges, and across many (14 in total) language pairs comprising languages that\nare similar/dissimilar/low-resourced. UXLA yields impressive results on XNER, setting SoTA in all tested languages outperforming the baselines by a good margin. The relative gains for UXLA are particularly higher for structurally dissimilar and/or low-resource languages: 28.54%, 16.05%, and 9.25% absolute improvements for Urdu, Burmese, and Arabic, respectively. For XNLI, with only 5% labeled data in the source, it gets comparable results to the baseline that uses all the labeled data, and surpasses the standard baseline by 2.55% on average when it uses all the labeled data in the source. We also have similar findings in PAWS-X. We provide a comprehensive analysis of the factors that contribute to UXLA’s performance. We open-source our framework at https://ntunlpsg.github.io/project/uxla/ ."
    }, {
      "heading" : "2 UXLA Framework",
      "text" : "While recent cross-lingual transfer learning efforts have relied almost exclusively on multi-lingual pretraining and zero-shot transfer of a fine-tuned source model, we believe there is a great potential for more elaborate methods that can leverage the unlabeled data better. Motivated by this, we present UXLA, our unsupervised data augmentation framework for zero-resource cross-lingual task adaptation. Figure 1 gives an overview of UXLA.\nLet Ds = (Xs,Ys) and Dt = (Xt) denote the training data for a source language s and a target language t, respectively. UXLA augments data from various origins at different stages of training. In the initial stage (epoch 1), it uses the augmented training samples from the target language (D′t) along with the original source (Ds). In later stages (epoch 2-3), it uses vicinal sentences generated from the vicinity distribution of source and target examples: ϑ(x̃sn|xsn) and ϑ(x̃tn|xtn), where xsn ∼ Xs and xtn ∼ Xt. It performs self-training on the augmented data to acquire the corresponding pseudo labels. To avoid confirmation bias with self-training where the model accumulates its own errors, it simultaneously trains three task models to generate virtual training data through data augmentation and filtering of potential label noises via multi-epoch co-teaching (Zhou and Li, 2005).\nIn each epoch, the co-teaching process first performs co-distillation, where two peer task models are used to select “reliable” training examples to train the third model. The selected samples with pseudo labels are then added to the target task\nmodel’s training data by taking the agreement from the other two models, a process we refer to as coguessing. The co-distillation and co-guessing mechanism ensure robustness of UXLA to out-of-domain distributions that can occur in a multilingual setup, e.g., due to a structurally dissimilar and/or lowresource target language. Algorithm 1 gives a pseudocode of the overall training method. Each of the task models in UXLA is an instance of XLM-R finetuned on the source language task (e.g., English NER), whereas the pretrained masked LM parameterized by θmlm (i.e., before fine-tuning) is used to define the vicinity distribution ϑ(x̃n|xn, θmlm) around each selected example xn. In the following, we describe the steps in Algorithm 1."
    }, {
      "heading" : "2.1 Warm-up: Training Task Models",
      "text" : "We first train three instances of the XLM-R model (θ(1), θ(2), θ(3)) with an additional task-specific linear layer on the source language (English) labeled data. Each model has the same architecture (XLMR large) but is initialized with different random seeds. For token-level prediction tasks (e.g., NER), the token-level representations are fed into the classification layer, whereas for sentence-level tasks (e.g., XNLI), the [CLS] representation is used as input to the classification layer.\nTraining with confidence penalty Our goal is to train the task models so that they can be used reliably for self-training on a target language that is potentially dissimilar and low-resourced. In such situations, an overly confident (overfitted) model may produce more noisy pseudo labels, and the noise will then accumulate as the training progresses. Overly confident predictions may also im-\npose difficulties on our distillation methods (§2.3) in isolating good samples from noisy ones. However, training with the standard cross-entropy (CE) loss may result in overfitted models that produce overly confident predictions (low entropy), especially when the class distribution is not balanced. We address this by adding a negative entropy term −H to the CE loss as follows.\nL(θ) = C∑ c=1 [ − yc log pcθ(x)︸ ︷︷ ︸\nCE\n+ pcθ(x) log p c θ(x)︸ ︷︷ ︸\n−H\n] (1)\nwhere x is the representation that goes to the output layer, and yc and pcθ(x) are respectively the ground truth label and model predictions with respect to class c. Such regularizer of output distribution has been shown to be effective for training large models (Pereyra et al., 2017). We also report significant gains with confidence penalty in §3. Appendix B shows visualizations on why confidence penalty is helpful for distillation."
    }, {
      "heading" : "2.2 Sentence Augmentation",
      "text" : "Our augmentated sentences come from two different sources: the original target language samples Xt, and the virtual samples generated from the vicinity distribution of the source and target samples: ϑ(x̃sn|xsn, θmlm) and ϑ(x̃tn|xtn, θmlm) with xsn ∼ Xs and xtn ∼ Xt. It has been shown that contextual LMs pretrained on large-scale datasets capture useful linguistic features and can be used to generate fluent grammatical texts (Hewitt and Manning, 2019). We use XLM-R masked LM (Conneau et al., 2020) as our vicinity model θmlm, which is trained on massive multilingual corpora (2.5 TB of Common-Crawl data in 100 languages). The\nAlgorithm 1 UXLA: a robust unsupervised data augmentation framework for cross-lingual NLP Input: source (s) and target (t) language datasets: Ds = (Xs,Ys),Dt = (Xt); task models: θ(1), θ(2), θ(3), pre-trained masked LM θmlm, mask ratio P , diversification factor δ, sampling factor α, and distillation factor η Output: models trained on augmented data 1: θ(1), θ(2), θ(3) = WARMUP(Ds, θ(1), θ(2), θ(3)) . warm up with conf. penalty. 2: for e ∈ [1 : 3] do . e denotes epoch. 3: for k ∈ {1, 2, 3} do 4: X (k)t ,Y (k) t = DISTIL(Xt, ηe, θ(k)) . infer and select tgt training data for augmentation.\n5: for j ∈ {1, 2, 3} do 6: if k == j then Continue 7: /* source language data augmentation */ 8: X̃s = GEN-LM(Xs, θmlm, P, δ) . vicinal example generation. 9: X (k)s ,Y(k)s = DISTIL(X̃s, ηe, θ(k)); X (j)s ,Y(j)s = DISTIL(X̃s, ηe, θ(j))\n10: D̃s = AGREEMENT ( D(k)s = (X (k)s ,Y(k)s ),D(j)s = (X (j)s ,Y(j)s ) ) 11: /* target language data augmentation (no vicinity) */ 12: X (j)t ,Y (j) t = DISTIL(Xt, ηe, θ(j))\n13: D′t = AGREEMENT ( D(k)t = (X (k) t ,Y (k) t ),D (j) t = (X (j) t ,Y (j) t ) )\n. see line 4 14: /* target language data augmentation */ 15: X̃t = GEN-LM(Xt, θmlm, P, δ) . vicinal example generation. 16: X (k)t ,Y (k) t = DISTIL(X̃t, ηe, θ(k)); X (j) t ,Y (j) t = DISTIL(X̃t, ηe, θ(j))\n17: D̃t = AGREEMENT ( D(k)t = (X (k) t ,Y (k) t ),D (j) t = (X (j) t ,Y (j) t ) ) 18: /* train new models on augmented data */ 19: for l ∈ {1, 2, 3} do 20: if l 6= j and l 6= k then 21: with sampling factor α, train θ(l) on D, . train progressively 22: where D = {Ds1(e ∈ {1, 3}) ∪ D′t1(e ∈ {1, 3}) ∪ D̃s1(e = 3) ∪ D̃t1(e ∈ {2, 3})} 23: Return {θ(1), θ(2), θ(3)}\nvicinity model is a disjoint pretrained entity whose parameters are not trained on any task objective.\nIn order to generate samples around each selected example, we first randomly choose P% of the input tokens. Then we successively (one at a time) mask one of the chosen tokens and ask XLMR masked LM to predict a token in that masked position, i.e., compute ϑ(x̃m|x, θmlm) with m being the index of the masked token. For a specific mask, we sample S candidate words from the output distribution, and generate novel sentences by following one of the two alternative approaches.\n(i) Successive max In this approach, we take the most probable output token (S = 1) at each prediction step, o∗m = argmaxo ϑ(x̃m = o|x, θmlm). A new sentence is constructed by P% newly generated tokens. We generate δ (diversification factor) virtual samples for each original example x, by randomly masking P% tokens each time.\n(ii) Successive cross In this approach, we divide each original (multi-sentence) sample x into two parts and use successive max to create two sets of augmented samples of size δ1 and δ2, respectively. We then take the cross of these two sets to generate δ1 × δ2 augmented samples.\nAugmentation of sentences through successive max or cross is carried out within the GEN-LM\n(generate via LM) module in Algorithm 1. For tasks involving a single sequence (e.g., XNER), we directly use successive max. Pairwise tasks like XNLI and PAWS-X have pairwise dependencies: dependencies between a premise and a hypothesis in XNLI or dependencies between a sentence and its possible paraphrase in PAWS-X. To model such dependencies, we use successive cross, which uses cross-product of two successive max applied independently to each component."
    }, {
      "heading" : "2.3 Co-labeling through Co-distillation",
      "text" : "Due to discrete nature of texts, VRM based augmentation methods that are successful for images such as MixMatch (Berthelot et al., 2019) that generates new samples and their labels as simple linear interpolation, have not been successful in NLP. The meaning of a sentence can change entirely even with minor variations in the original sentence. For example, consider the following example generated by our vicinity model.\nOriginal: EU rejects German call to boycott british lamb. Masked: <mask> rejects german call to boycott british lamb. XLM-R: Trump rejects german call to boycott british lamb.\nHere, EU is an Organization whereas the newly predicted word Trump is a Person (different name type). Therefore, we need to relabel the augmented\nsentences no matter whether the original sentence has labels (source) or not (target). However, the relabeling process can induce noise, especially for dissimilar/low-resource languages, since the base task model may not be adapted fully in the early training stages. We propose a 2-stage sample distillation process to filter out noisy augmented data.\nStage 1: Distillation by single-model The first stage of distillation involves predictions from a single model for which we propose two alternatives:\n(i) Distillation by model confidence: In this approach, we select samples based on the model’s prediction confidence. This method is similar in spirit to the selection method proposed by Ruder and Plank (2018a). For sentence-level tasks (e.g., XNLI), the model produces a single class distribution for each training example. In this case, the model’s confidence is computed by p∗ = maxc∈{1...C} pcθ(x). For token-level sequence labeling tasks (e.g., NER), the model’s confidence is computed by: p∗ = 1 T ∑T t=1 { maxc∈{1...C} p c θ(xt) } , where T is the length of the sequence. The distillation is then done by selecting the top η% samples with the highest confidence scores.\n(ii) Sample distillation by clustering: We propose this method based on the finding that large neural models tend to learn good samples faster than noisy ones, leading to a lower loss for good samples and higher loss for noisy ones (Han et al., 2018; Arazo et al., 2019). We use a 1d twocomponent Gaussian Mixture Model (GMM) to model per-sample loss distribution and cluster the samples based on their goodness. GMMs provide flexibility in modeling the sharpness of a distribution and can be easily fit using ExpectationMaximization (EM) (See more on Appendix C). The loss is computed based on the pseudo labels predicted by the model. For each sample x, its goodness probability is the posterior probability p(z = g|x, θGMM), where g is the component with smaller mean loss. Here, distillation hyperparameter η is the posterior probability threshold based on which samples are selected.\nStage 2: Distillation by model agreement In the second stage of distillation, we select samples by taking the agreement (co-guess) of two different peer models θ(j) and θ(k) to train the third θ(l). Formally, AGREEMENT ( D(k),D(j)) = {(X (k),Y(k)) : Y(k) = Y(j)} s.t. k 6= j"
    }, {
      "heading" : "2.4 Data Samples Manipulation",
      "text" : "UXLA uses multi-epoch co-teaching. It uses Ds andD′t in the first epoch. In epoch 2, it uses D̃t (target virtual), and finally it uses all the four datasets - Ds, D′t, D̃t, and D̃s (line 22 in Algorithm 1). The datasets used at different stages can be of different sizes. For example, the number of augmented samples in D̃s and D̃t grow polynomially with the successive cross masking method. Also, the co-distillation produces sample sets of variable sizes. To ensure that our model does not overfit on one particular dataset, we employ a balanced sampling strategy. For N number of datasets {Di}Ni=1 with probabilities, {pi}Ni=1, we define the following multinomial distribution to sample from:\npi = fαi∑N j=1 f α j , where fi = ni∑N j=1 nj\n(2)\nwhere α is the sampling factor and ni is the total number of samples in the ith dataset. By tweaking α, we can control how many samples a dataset can provide in the mix."
    }, {
      "heading" : "3 Experiments",
      "text" : "We consider three tasks in the zero-resource crosslingual transfer setting. We assume labeled training data only in English, and transfer the trained model to a target language. For all experiments, we report the mean score of the three models that use different seeds."
    }, {
      "heading" : "3.1 Tasks & Settings",
      "text" : "XNER: We use the standard CoNLL datasets (Sang, 2002; Sang and Meulder, 2003) for English (en), German (de), Spanish (es) and Dutch (nl). We also evaluate on Finnish (fi) and Arabic (ar) datasets collected from Bari et al. (2020). Note that Arabic is structurally different from English, and Finnish is from a different language family. To show how the models perform on extremely lowresource languages, we experiment with three structurally different languages from WikiANN (Pan et al., 2017) of different (unlabeled) training data sizes: Urdu (ur-20k training samples), Bengali (bn10K samples), and Burmese (my-100 samples).\nXNLI We use the standard dataset (Conneau et al., 2018). For a given pair of sentences, the task is to predict the entailment relationship between the two sentences, i.e., whether the second sentence (hypothesis) is an Entailment, Contradiction, or\nNeutral with respect to the first one (premise). We experiment with Spanish, German, Arabic, Swahili (sw), Hindi (hi) and Urdu.\nPAWS-X The Paraphrase Adversaries from Word Scrambling Cross-lingual task (Yang et al., 2019) requires the models to determine whether two sentences are paraphrases. We evaluate on all the six (typologically distinct) languages: fr, es, de, Chinese (zh), Japanese (ja), and Korean (ko).\nEvaluation setup Our goal is to adapt a task model from a source language distribution to an unknown target language distribution assuming no labeled data in the target. In this scenario, there might be two different distributional gaps: (i) the generalization gap for the source distribution, and (ii) the gap between the source and target language distribution. We wish to investigate our method in tasks that exhibit such properties. We use the standard task setting for XNER, where we take 100% samples from the datasets as they come from various domains and sizes without any specific bias.\nHowever, both XNLI and PAWS-X training data come with machine-translated texts in target languages. Thus, the data is parallel and lacks enough diversity (source and target come from the same domain). Cross-lingual models trained in this setup may pick up distributional bias (in the label space) from the source. Artetxe et al. (2020) also argue that the translation process can induce subtle artifacts that may have a notable impact on models.\nTherefore, for XNLI and PAWS-X, we experiment with two different setups. First, to ensure distributional differences and non-parallelism, we use 5% of the training data from the source language and augment a different (nonparallel) 5%\nModel ur bn my\nSupervised Results\nXLM-R (our-impl) 97.1 97.8 76.8\nZero-Resource Results\ndata for the target language. We used a different seed each time to retrieve this 5% data. Second, to compare with previous methods, we also evaluate on the standard 100% setup. The evaluation is done on the entire test set in both setups. We will refer to these two settings as 5% and 100%. More details about model settings are in Appendix D."
    }, {
      "heading" : "3.2 Results",
      "text" : "XNER Table 1 reports the XNER results on the datasets from CoNLL and (Bari et al., 2020), where we also evaluate an ensemble by averaging the probabilities from the three models. We observe that after performing warm-up with conf-penalty (§2.1), XLM-R performs better than mBERT on average by ∼3.8% for all the languages. UXLA gives absolute improvements of 3.76%, 4.34%, 6.94%, 8.31%, and 4.18% for es, nl, de, ar, and fi, respectively. Interestingly, it surpasses supervised LSTM-CRF for nl and de without using any target language labeled data. It also produces comparable results for es.\nIn Table 2, we report the results on the three lowresource langauges from WikiANN. From these results and the results of ar and fi in Table 1, we see that UXLA is particularly effective for languages that are structurally dissimilar and/or lowresourced, especially when the base model is weak:\n28.54%, 16.05%, and 9.25% absolute improvements for ur, my and ar, respectively.\nXNLI-5% From Table 3, we see that the performance of XLM-R trained on 5% data is surprisingly good compared to the model trained on full data (see XLM-R (our imp.)), lagging by only 5.6% on average. In our single GPU implementation of XNLI, we could not reproduce the reported results of Conneau et al. (2020). However, our results resemble the reported XLM-R results of XTREME (Hu et al., 2020). We consider XTREME as our standard baseline for XNLI-100%.\nWe observe that with only 5% labeled data in the source, UXLA gets comparable results to the XTREME baseline that uses 100% labeled data (lagging behind by only ∼0.7% on avg.); even for ar and sw, we get 0.22% and 1.11% improvements, respectively. It surpasses the standard 5% baseline by 4.2% on average. Specifically, UXLA gets absolute improvements of 3.05%, 3.34%, 5.38%, 5.01%, 4.29%, and 4.12% for es, de, ar, sw, hi, and ur, respectively. Again, the gains are relatively higher for low-resource and/or dissimilar languages despite the base model being weak in such cases.\nXNLI-100% Now, considering UXLA’s performance on the full (100%) labeled source data in Table 3, we see that it achieves SoTA results for all of the languages with an absolute improvement of 2.55% on average from the XTREME baseline. Specifically, UXLA gets absolute improvements of 1.95%, 1.68%, 4.30%, 3.50%, 3.24%, and 1.65% for es, de, ar, sw, hi, and ur, respectively.\nPAWS-X Similar to XNLI, we observe sizable improvements for UXLA over the baselines on PAWS-X for both 5% and 100% settings (Table 4). Specifically, in 5% setting, UXLA gets absolute gains of 5.33%, 5.94%, 5.04%, 6.85%, 7.00%, and 5.45% for de, es, fr, ja, ko, and zh, respectively, while in 100% setting, it gets 2.21%, 2.36%, 2.00%, 3.99%, 4.53%, and 4.41% improvements respectively. In general, we get an average improvements of 5.94% and 3.25% in PAWS-X-5% and PAWSX-100% settings respectively. Moreover, our 5% setting outperforms 100% XLM-R baselines for es, ja, and zh. Interestingly, in the 100% setup, our UXLA (ensemble) achieves almost similar accuracies compared to supervised finetuning of XLM-R on all target language training dataset."
    }, {
      "heading" : "4 Analysis",
      "text" : "In this section, we analyze UXLA by dissecting it and measuring the contribution of its each of the components. For this, we use the XNER task and analyze the model based on the results in Table 1."
    }, {
      "heading" : "4.1 Analysis of distillation methods",
      "text" : "Model confidence vs. clustering We first analyze the performance of our single-model distillation methods (§2.3) to see which of the two alternatives works better. From Table 5, we see that both perform similarly with model confidence being slightly better. In our main experiments (Tables 1-4) and subsequent analysis, we use model confidence for distillation. However, we should not rule out the clustering method as it gives a more general\nsolution to consider other distillation features (e.g., sequence length, language) than model prediction scores, which we did not explore in this paper.\nDistillation factor η We next show the results for different distillation factor (η) in Table 5. Here 100% refers to the case when no single-model distillation is done based on model confidence. We notice that the best results for each of the languages are obtained for values other than 100%, which indicates that distillation is indeed an effective step in UXLA. See Appendix B for more analysis on η.\nTwo-stage distillation We now validate whether the second-stage distillation (distillation by model agreement) is needed. In Table 5, we also compare the results with the model agreement (shown as ∩) to the results without using any agreement (φ). We observe better performance with model agreement in all the cases on top of the single-model distillation which validates its utility. Results with η = 100, Agreement = ∩ can be considered as the tri-training (Ruder and Plank, 2018b) baseline."
    }, {
      "heading" : "4.2 Augmentation in Stages",
      "text" : "Figure 2 presents the effect of different types of augmented data used by different epochs in our multi-epoch co-teaching framework. We observe that in every epoch, there is a significant boost in F1 scores for each of the languages. Arabic, being structural dissimilar to English, has a lower base score, but the relative improvements brought by UXLA are higher for Arabic, especially in epoch 2\nwhen it gets exposed to the target language virtual data (D̃t) generated by the vicinity distribution."
    }, {
      "heading" : "4.3 Effect of Confidence Penalty & Ensemble",
      "text" : "For all the three tasks, we get reasonable improvements over the baselines by training with confidence penalty (§2.1). Specifically, we get 0.56%, 0.74%, 1.89%, and 1.18% improvements in XNER, XNLI-5%, PAWS-X-5%, and PAWS-X-100% respectively (Table 1,3,4). The improvements in XNLI-100% are marginal and inconsistent, which we suspect due to the balanced class distribution.\nFrom the results of ensemble models, we see that the ensemble boosts the baseline XLM-R. However, our regular UXLA still outperforms the ensemble baselines by a sizeable margin. Moreover, ensembling the trained models from UXLA further improves the performance. These comparisons ensure that the capability of UXLA through co-teaching and co-distillation is beyond the ensemble effect."
    }, {
      "heading" : "4.4 Robustness & Efficiency",
      "text" : "Table 6 shows the robustness of the fine-tuned UXLA model on XNER task. After fine-tuning in a specific target language, the F1 scores in English remain almost similar (see first row). For some languages, UXLA adaptation on a different language also improves the performance. For example, Arabic gets improvements for all UXLA-adapted models (compare 50.88 with others in row 5). This indicates that augmentation of UXLA does not overfit on a target language. More baselines, analysis and visualizations are added in Appendix."
    }, {
      "heading" : "5 Related Work",
      "text" : "Recent years have witnessed significant progress in learning multilingual pretrained models. Notably, mBERT (Devlin et al., 2019) extends (English) BERT by jointly training on 102 languages. XLM\n(Lample and Conneau, 2019) extends mBERT with a conditional LM and a translation LM (using parallel data) objectives. Conneau et al. (2020) train the largest multilingual language model XLM-R with RoBERTa (Liu et al., 2019). Wu and Dredze (2019), Keung et al. (2019), and Pires et al. (2019) evaluate zero-shot cross-lingual transferability of mBERT on several tasks and attribute its generalization capability to shared subword units. Pires et al. (2019) also found structural similarity (e.g., word order) to be another important factor for successful crosslingual transfer. K et al. (2020), however, show that the shared subword has a minimal contribution; instead, the structural similarity between languages is more crucial for effective transfer.\nOlder data augmentation approaches relied on distributional clusters (Täckström et al., 2012). A number of recent methods have been proposed using contextualized LMs (Kobayashi, 2018; Wu et al., 2018; Shi et al., 2019; Ding et al., 2020; Liu et al., 2021). These methods rely on labels to perform label-constrained augmentation, thus not directly comparable with ours. Also, there are fundamental differences in the way we use the pretrained LM. Unlike them our LM augmentation is purely unsupervised and we do not perform any fine-tuning of the pretrained vicinity model. This disjoint characteristic gives our framework the flexibility to replace θlm even with a better monolingual LM for a specific target language, which in turn makes UXLA extendable to utilize stronger LMs that may come in the future. In a concurrent work (Mohiuddin et al., 2021), we propose a contextualized LM based data augmentation for neural machine translation and show its advantages over traditional back-translation gaining improved performance in low-resource scenarios."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We propose a novel data augmentation framework, UXLA, for zero-resource cross-lingual task adaptation. It performs simultaneous self-training with data augmentation and unsupervised sample selection. With extensive experiments on three different cross-lingual tasks spanning many language pairs, we have demonstrated the effectiveness of UXLA. For the zero-resource XNER task, UXLA sets a new SoTA for all the tested languages. For both XNLI and PAWS-X tasks, with only 5% labeled data in the source, UXLA gets comparable results to the baseline that uses 100% labeled data."
    }, {
      "heading" : "A FAQ: Justifications for design methodology of UXLA",
      "text" : "Here are our justifications for various design principles of the UXLA framework.\nIs masked language model pre-training with cross-lingual training data from task dataset useful? In Table 7, We perform language model finetuning on XLM-R large model with multilingual sentences of NER dataset and perform adaptation with only English language. With the LMfinetuned XLM-R model, we didn’t see any significant increase in cross-lingual transfer. For Spanish, Arabic language, the score even got decreased, which indicates possible over-fitting. However, robustness experiment in table 6 (see in the main paper, sec 4.4) indicates that our proposed method doesn’t overfit on target language rather than augment the new knowledge base.\nIs using three models with different initialization necessary? Yes, different initialization ensures different convergence paths, which results in diversity during inference. Co-labeling (Section 3.3) utilizes this property. There could be some other ways to achieve the same thing. Our initial attempt with three different heads (sharing a backbone network) didn’t work well.\nIs using three epochs necessary? We utilize different types of datasets in different epochs. While pseudo-labeling may induce noise, the model’s predictions for in-domain cross-lingual samples are usually better. Because of this, for a smooth transition, we apply the vicinal samples in the second epoch. Finally, inspired by the joint training of the cross-lingual language model, in the third epoch we use all four datasets. We also include the labeled source data which ensures that our model does not overfit on target distribution as well as persists the generalization capability of the source distribution.\nNeed for the combination of co-teaching, codistillation and co-guessing? The combination of these helps to distill out the noisy samples better.\nEfficiency of the method and expensive extra costs for large-scale pretrained models It is a common practice in model selection to train 3-5 disjoint LM-based task models (e.g., XLM-R on NER) with different random seeds and report the ensemble score or score of the best (validation set) model. In contrast, UXLA uses 3 different models and jointly trains them where the models assist each other through distillation and co-labeling. In that sense, the extra cost comes from distillation and co-labeling, which is not significant and is compensated by the significant improvements that UXLA offers.\nB Visualization of confidence penalty\nB.1 Effect of confidence penalty in classification\nIn Figure 3 (a-b), we present the effect of the confidence penalty (Eq. 1 in the main paper) in the target language (Spanish) classification on the XNER dev. data (i.e., after training on English NER). We show the class distribution from the final logits (on the target language) using t-SNE plots. From the figure, it is evident that the use of confidence penalty in the warm-up step makes the model more robust to unseen out-of-distribution target language data yielding better predictions, which in turn also provides a better prior for self-training with pseudo labels.\nB.2 Effect of confidence penalty in loss distribution\nFigures 3(c) and 3(d) present the per-sample loss (i.e., mean loss per sentence w.r.t. the pseudo labels) distribution in histogram without and with confidence penalty, respectively. Here, accurate2 refers to the sentences which have at most two wrong NER labels, and sentences containing more than two errors are referred to as noisy samples. It shows that without confidence penalty, there are many noisy samples with a small loss which is not desired. In addition to that, the figures also suggest that the confidence penalty helps to separate the clean samples from the noisy ones either by clustering or by model confidence.\nFigures 4(a) and 4(b) present the loss distribution in a scatter plot by sorting the sentences based\non their length in the x-axis; y-axis represents the loss. As we can see, the losses are indeed more scattered when we train the model with confidence penalty, which indicates higher per-sample entropy, as expected. Also, we can see that as the sentence length increases, there are more wrong predictions. Our distillation method should be able to distill out these noisy pseudo samples.\nFinally, Figures 4(c) and 4(d) show the length distribution of all vs. the selected sentences (by Distillation by model confidence) without and with confidence penalty. Bari et al. (2020) shows that cross-lingual NER inference is heavily dependent on the length distribution of the samples. In general, the performance of the lower length samples is more accurate. However, if we only select the lower length samples we will easily overfit. From these plots, we observe that the confidence penalty also helps to perform a better distillation as more sentences are selected (by the distillation procedure) from the lower length distribution, while still covering the entire lengths. This shows that using the confidence penalty in training, model becomes more robust.\nIn summary, comparing the Figures 3(c-d) - 4(cd), we can conclude that training without confidence penalty can make the model more prone to over-fitting, resulting in more noisy pseudo labels. Training with confidence penalty not only improves pseudo labeling accuracy but also helps the distillation methods to perform better noise filtering."
    }, {
      "heading" : "C Details on distillation by clustering",
      "text" : "One limitation of the confidence-based (singlemodel) distillation is that it does not consider task-\nspecific information. Apart from classifier confidence, there could be other important features that can distinguish a good sample from a noisy one. For example, for sequence labeling, sequence length can be an important feature as the models tend to make more mistakes (hence noisy) for longer sequences Bari et al. (2020). One might also want to consider other features like fluency, which can be estimated by a pre-trained conditional LM like GPT Radford et al. (2020). In the following, we introduce a clustering-based method that can consider these additional features to separate good samples from bad ones.\nHere our goal is to cluster the samples based on their goodness. It has been shown in computer vision that deep models tend to learn good samples faster than noisy ones, leading to a lower loss for good samples and higher loss for noisy ones Han et al. (2018), Arpit et al. (2017). We propose to model per-sample loss distribution (along with other task-specific features) with a mixture model, which we fit using an Expectation-Maximization (EM) algorithm. However, contrary to those approaches which use actual (supervised) labels, we use the model predicted pseudo labels to compute the loss for the samples.\nWe use a two-component Gaussian Mixture Model (GMM) due to its flexibility in modeling the sharpness of a distribution Li et al. (2020a). In the following, we describe the EM training of the GMM for one feature, i.e., per-sample loss, but it is trivial to extend it to consider other indicative taskspecific features like sequence length or fluency score (see any textbook on machine learning).\nEM training for two-component GMM Let xi ∈ IR denote the loss for sample xi and zi ∈ {0, 1} denote its cluster id. We can write the 1d GMM model as:\np(xi|θ, π) = 1∑\nk=0\nN (xi|µk, σk)πk (3)\nwhere θk = {µk, σ2k} are the parameters of the kth mixture component and πk = p(zi = k) is the probability (weight) of the k-th component with the condition 0 ≤ πk ≤ 1 and ∑ k πk = 1.\nIn EM, we optimize the expected complete data log likelihood Q(θ, θt−1) defined as:\nQ(θ, θt−1) = E( ∑ i log[p(xi, zi|θ)])\n= E( ∑ i ∑ k I(zi = k) log[p(xi|θk)πk])\n= ∑ i ∑ k E(I(zi = k)) log[p(xi|θk)πk]\n= ∑ i ∑ k p(zi = k|xi, θt−1) log[p(xi|θk)πk]\n= ∑ i ∑ k ri,k(θ t−1) log p(xi|θk) + ri,k(θt−1) log πk\n(4)\nwhere ri,k(θt−1) is the responsibility that cluster k takes for sample xi, which is computed in the Estep so that we can optimize Q(θ, θt−1) (Eq. 4) in the M-step. The E-step and M-step for a 1d GMM can be written as:\nE-step: Compute ri,k(θt−1) = N (xi|θt−1k )π t−1 k∑\nkN (xi|θ t−1 k )π t−1 k M-step: Optimize Q(θ, θt−1) w.r.t. θ and π\n• πk = ∑ i ri,k∑\ni ∑ k ri,k = 1N ∑ i ri,k\n• µk = ∑ i ri,kxi∑ i ri,k ; σ2k = ∑ i ri,k(xi−µk)2∑ i ri,k\nInference For a sample x, its goodness probability is the posterior probability p(z = g|x, θ), where g ∈ {0, 1} is the component with smaller mean loss. Here, distillation hyperparameter η is the posterior probability threshold based on which samples are selected.\nRelation with distillation by model confidence Astute readers might have already noticed that per-sample loss has a direct deterministic relation with the model confidence. Even though they are different, these two distillation methods consider the same source of information. However, as mentioned, the clustering-based method allows us to incorporate other indicative features like length, fluency, etc. For a fair comparison between the two methods, we use only the per-sample loss in our primary (single-model) distillation methods."
    }, {
      "heading" : "D Hyperparameters",
      "text" : "We present the hyperparameter settings for XNER and XNLI tasks for the XLA framework in Table 8. In the warm-up step, we train and validate the task models with English data. However, for cross-lingual adaptation, we validate (for model selection) our model with the target language development set. We train our model with respect to the number of steps instead of the number of epochs. In the case of a given number of epochs, we convert it to a total number of steps. We observe that learning rate is a crucial hyperparameter. In table 8, lr-warm-up-steps refer to the warmup-step from triangular learning rate scheduling. This hyperparameter is not to be confused with\nWarm-up step of the UXLA framework. In our experiments, effective batch-size is another crucial hyperparameter that can be obtained by gradient accumulation steps. We fix the maximum sequence length to 280 for XNER and 128 tokens for XNLI. For each of the experiments, we report the average score of three task models, θ(1), θ(2), θ(3), which are initialized with different seeds. We perform each of the experiments in a single GPU setup with float32 precision."
    }, {
      "heading" : "E Additional Related Work",
      "text" : "Vicinal risk minimization. One of the fundamental challenges in deep learning is to train models that generalize well to examples outside the training distribution. The widely used Empirical Risk Minimization (ERM) principle where models are trained to minimize the average training error has been shown to be insufficient to achieve generalization on distributions that differ slightly from the training data (Szegedy et al., 2014; Zhang et al., 2018). Data augmentation supported by the Vicinal Risk Minimization (VRM) principle (Chapelle et al., 2001) can be an effective choice for achieving better out-of-training generalization.\nIn VRM, we minimize the empirical vicinal risk defined as:\nLv(θ) = 1\nN N∑ n=1 l(fθ(x̃n), ỹn) (5)\nwhere fθ denotes the model parameterized by θ, and Daug = {(x̃n, ỹn)}Nn=1 is an augmented dataset constructed by sampling the vicinal distribution ϑ(x̃, ỹ|xi, yi) around the original training sample (xi, yi). Defining vicinity is however challenging as it requires to extract samples from a distribution without hurting the labels. Earlier methods apply simple rules like rotation and scaling of images (Simard et al., 1998). Recently, Zhang et al. (2018); Berthelot et al. (2019) and Li et al. (2020) show impressive results in image classification with simple linear interpolation of data. However, to our knowledge, none of these methods has so far been successful in NLP due to the discrete nature of texts."
    } ],
    "references" : [ {
      "title" : "Unsupervised label noise modeling and loss correction",
      "author" : [ "Eric Arazo", "Diego Ortego", "Paul Albert", "Noel E O’Connor", "Kevin McGuinness" ],
      "venue" : "In International Conference on Machine Learning (ICML)",
      "citeRegEx" : "Arazo et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Arazo et al\\.",
      "year" : 2019
    }, {
      "title" : "Translation artifacts in cross-lingual transfer learning",
      "author" : [ "Mikel Artetxe", "Gorka Labaka", "Eneko Agirre" ],
      "venue" : null,
      "citeRegEx" : "Artetxe et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Artetxe et al\\.",
      "year" : 2020
    }, {
      "title" : "Zero-Resource Cross-Lingual Named Entity Recognition",
      "author" : [ "M Saiful Bari", "Shafiq Joty", "Prathyusha Jwalapuram." ],
      "venue" : "Proceedings of the 34th AAAI Conference on Artifical Intelligence, AAAI ’20, pages xx–xx, New York, USA. AAAI.",
      "citeRegEx" : "Bari et al\\.,? 2020",
      "shortCiteRegEx" : "Bari et al\\.",
      "year" : 2020
    }, {
      "title" : "Mixmatch: A holistic approach to semisupervised learning",
      "author" : [ "David Berthelot", "Nicholas Carlini", "Ian Goodfellow", "Nicolas Papernot", "Avital Oliver", "Colin A Raffel." ],
      "venue" : "H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alché-Buc, E. Fox, and R. Gar-",
      "citeRegEx" : "Berthelot et al\\.,? 2019",
      "shortCiteRegEx" : "Berthelot et al\\.",
      "year" : 2019
    }, {
      "title" : "Vicinal risk minimization",
      "author" : [ "Olivier Chapelle", "Jason Weston", "Léon Bottou", "Vladimir Vapnik." ],
      "venue" : "T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 416–422. MIT Press.",
      "citeRegEx" : "Chapelle et al\\.,? 2001",
      "shortCiteRegEx" : "Chapelle et al\\.",
      "year" : 2001
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "XNLI: evaluating cross-lingual sentence representations",
      "author" : [ "Alexis Conneau", "Guillaume Lample", "Ruty Rinott", "Adina Williams", "Samuel R. Bowman", "Holger Schwenk", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1809.05053.",
      "citeRegEx" : "Conneau et al\\.,? 2018",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2018
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "DAGA: Data Augmentation with a Generation Approach for Low-resource Tagging Tasks",
      "author" : [ "Bosheng Ding", "Linlin Liu", "Lidong Bing", "Canasai Kruengkrai", "Thien Hai Nguyen", "Shafiq Joty", "Luo Si", "Chunyan Miao." ],
      "venue" : "Proceedings of the 2020 Conference",
      "citeRegEx" : "Ding et al\\.,? 2020",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2020
    }, {
      "title" : "Co-teaching: Robust training of deep neural networks with extremely noisy labels",
      "author" : [ "Bo Han", "Quanming Yao", "Xingrui Yu", "Gang Niu", "Miao Xu", "Weihua Hu", "Ivor Tsang", "Masashi Sugiyama." ],
      "venue" : "S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-",
      "citeRegEx" : "Han et al\\.,? 2018",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2018
    }, {
      "title" : "A structural probe for finding syntax in word representations",
      "author" : [ "John Hewitt", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Hewitt and Manning.,? 2019",
      "shortCiteRegEx" : "Hewitt and Manning.",
      "year" : 2019
    }, {
      "title" : "Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization",
      "author" : [ "Junjie Hu", "Sebastian Ruder", "Aditya Siddhant", "Graham Neubig", "Orhan Firat", "Melvin Johnson." ],
      "venue" : "CoRR, abs/2003.11080.",
      "citeRegEx" : "Hu et al\\.,? 2020",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2020
    }, {
      "title" : "Cross-lingual ability of multilingual {bert}: An empirical study",
      "author" : [ "Karthikeyan K", "Zihan Wang", "Stephen Mayhew", "Dan Roth." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "K et al\\.,? 2020",
      "shortCiteRegEx" : "K et al\\.",
      "year" : 2020
    }, {
      "title" : "Adversarial learning with contextual embeddings for zero-resource cross-lingual classification and ner",
      "author" : [ "Phillip Keung", "yichao lu", "Vikas Bhardwaj" ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Keung et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Keung et al\\.",
      "year" : 2019
    }, {
      "title" : "Contextual augmentation: Data augmentation by words with paradigmatic relations",
      "author" : [ "Sosuke Kobayashi." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Kobayashi.,? 2018",
      "shortCiteRegEx" : "Kobayashi.",
      "year" : 2018
    }, {
      "title" : "Crosslingual language model pretraining",
      "author" : [ "Guillaume Lample", "Alexis Conneau." ],
      "venue" : "Advances in Neural Information Processing Systems (NeurIPS).",
      "citeRegEx" : "Lample and Conneau.,? 2019",
      "shortCiteRegEx" : "Lample and Conneau.",
      "year" : 2019
    }, {
      "title" : "Dividemix: Learning with noisy labels as semisupervised learning",
      "author" : [ "Junnan Li", "Richard Socher", "Steven C.H. Hoi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "MulDA: A Multilingual Data Augmentation Framework for LowResource Cross-Lingual NER",
      "author" : [ "Linlin Liu", "Bosheng Ding", "Lidong Bing", "Shafiq Joty", "Luo Si", "Chunyan Miao." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Com-",
      "citeRegEx" : "Liu et al\\.,? 2021",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Zettlemoyer and Stoyanov.,? 2019",
      "shortCiteRegEx" : "Zettlemoyer and Stoyanov.",
      "year" : 2019
    }, {
      "title" : "Augvic: Exploiting bitext vicinity for lowresource nmt",
      "author" : [ "Tasnim Mohiuddin", "M Saiful Bari", "Shafiq Joty." ],
      "venue" : "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Mohiuddin et al\\.,? 2021",
      "shortCiteRegEx" : "Mohiuddin et al\\.",
      "year" : 2021
    }, {
      "title" : "Crosslingual name tagging and linking for 282 languages",
      "author" : [ "Xiaoman Pan", "Boliang Zhang", "Jonathan May", "Joel Nothman", "Kevin Knight", "Heng Ji." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol-",
      "citeRegEx" : "Pan et al\\.,? 2017",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2017
    }, {
      "title" : "Regularizing neural networks by penalizing confident output distributions",
      "author" : [ "Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey E. Hinton." ],
      "venue" : "CoRR, abs/1701.06548.",
      "citeRegEx" : "Pereyra et al\\.,? 2017",
      "shortCiteRegEx" : "Pereyra et al\\.",
      "year" : 2017
    }, {
      "title" : "How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996– 5001, Florence, Italy",
      "author" : [ "Telmo Pires", "Eva Schlinger", "Dan Garrette." ],
      "venue" : "Association for Computational",
      "citeRegEx" : "Pires et al\\.,? 2019",
      "shortCiteRegEx" : "Pires et al\\.",
      "year" : 2019
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeff Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Strong baselines for neural semi-supervised learning under domain shift",
      "author" : [ "Sebastian Ruder", "Barbara Plank." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1044–",
      "citeRegEx" : "Ruder and Plank.,? 2018a",
      "shortCiteRegEx" : "Ruder and Plank.",
      "year" : 2018
    }, {
      "title" : "Strong baselines for neural semi-supervised learning under domain shift",
      "author" : [ "Sebastian Ruder", "Barbara Plank." ],
      "venue" : "CoRR, abs/1804.09530.",
      "citeRegEx" : "Ruder and Plank.,? 2018b",
      "shortCiteRegEx" : "Ruder and Plank.",
      "year" : 2018
    }, {
      "title" : "Introduction to the conll2002 shared task: Language-independent named entity recognition",
      "author" : [ "Erik Tjong Kim Sang." ],
      "venue" : "CoRR, cs.CL/0209010.",
      "citeRegEx" : "Sang.,? 2002",
      "shortCiteRegEx" : "Sang.",
      "year" : 2002
    }, {
      "title" : "Introduction to the conll-2003 shared task: Languageindependent named entity recognition",
      "author" : [ "Erik Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "CoNLL.",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Improving neural machine translation models with monolingual data",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Aug-bert: An efficient data augmentation algorithm for text classification",
      "author" : [ "Linqing Shi", "Danyang Liu", "Gongshen Liu", "Kui Meng." ],
      "venue" : "CSPS.",
      "citeRegEx" : "Shi et al\\.,? 2019",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2019
    }, {
      "title" : "Transformation Invariance in Pattern Recognition — Tangent Distance and Tangent Propagation, pages 239–274",
      "author" : [ "Patrice Y. Simard", "Yann A. LeCun", "John S. Denker", "Bernard Victorri." ],
      "venue" : "Springer Berlin Heidelberg, Berlin, Heidelberg.",
      "citeRegEx" : "Simard et al\\.,? 1998",
      "shortCiteRegEx" : "Simard et al\\.",
      "year" : 1998
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Szegedy et al\\.,? 2014",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2014
    }, {
      "title" : "Cross-lingual word clusters for direct transfer of linguistic structure",
      "author" : [ "Oscar Täckström", "Ryan McDonald", "Jakob Uszkoreit." ],
      "venue" : "The 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Täckström et al\\.,? 2012",
      "shortCiteRegEx" : "Täckström et al\\.",
      "year" : 2012
    }, {
      "title" : "Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT",
      "author" : [ "Shijie Wu", "Mark Dredze." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Wu and Dredze.,? 2019",
      "shortCiteRegEx" : "Wu and Dredze.",
      "year" : 2019
    }, {
      "title" : "Conditional BERT contextual augmentation",
      "author" : [ "Xing Wu", "Shangwen Lv", "Liangjun Zang", "Jizhong Han", "Songlin Hu." ],
      "venue" : "CoRR, abs/1812.06705.",
      "citeRegEx" : "Wu et al\\.,? 2018",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2018
    }, {
      "title" : "PAWS-X: A cross-lingual adversarial dataset for paraphrase identification",
      "author" : [ "Yinfei Yang", "Yuan Zhang", "Chris Tar", "Jason Baldridge." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Fast and accurate reading comprehension by combining self-attention and convolution",
      "author" : [ "Adams Wei Yu", "David Dohan", "Quoc Le", "Thang Luong", "Rui Zhao", "Kai Chen." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Yu et al\\.,? 2018",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2018
    }, {
      "title" : "mixup: Beyond empirical risk minimization",
      "author" : [ "Hongyi Zhang", "Moustapha Cisse", "Yann N. Dauphin", "David Lopez-Paz." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Tri-training: exploiting unlabeled data using three classifiers",
      "author" : [ "Zhi-Hua Zhou", "Ming Li." ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering, 17:1529–1541.",
      "citeRegEx" : "Zhou and Li.,? 2005",
      "shortCiteRegEx" : "Zhou and Li.",
      "year" : 2005
    }, {
      "title" : "One might also want to consider other features like fluency, which can be estimated by a pre-trained conditional LM like",
      "author" : [ "Bari" ],
      "venue" : "GPT Radford et al",
      "citeRegEx" : "Bari,? \\Q2020\\E",
      "shortCiteRegEx" : "Bari",
      "year" : 2020
    }, {
      "title" : "We propose to model per-sample loss distribution (along with other task-specific features) with a mixture model, which we fit using an Expectation-Maximization (EM) algorithm",
      "author" : [ "Han" ],
      "venue" : null,
      "citeRegEx" : "Han,? \\Q2017\\E",
      "shortCiteRegEx" : "Han",
      "year" : 2017
    }, {
      "title" : "2020) show impressive results in image classification with simple linear interpolation of data",
      "author" : [ "Zhang" ],
      "venue" : "(Simard et al.,",
      "citeRegEx" : "Zhang,? \\Q1998\\E",
      "shortCiteRegEx" : "Zhang",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "These methods typically follow two basic steps, where a supervised task-specific finetuning follows a large-scale LM pretraining (Radford et al., 2019).",
      "startOffset" : 129,
      "endOffset" : 151
    }, {
      "referenceID" : 7,
      "context" : "Jointly trained deep multi-lingual LMs like mBERT (Devlin et al., 2019) and XLM-R (Conneau et al.",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 22,
      "context" : "Despite their effectiveness, recent studies (Pires et al., 2019; K et al., 2020) have also highlighted one crucial limiting factor for successful crosslingual transfer.",
      "startOffset" : 44,
      "endOffset" : 80
    }, {
      "referenceID" : 12,
      "context" : "Despite their effectiveness, recent studies (Pires et al., 2019; K et al., 2020) have also highlighted one crucial limiting factor for successful crosslingual transfer.",
      "startOffset" : 44,
      "endOffset" : 80
    }, {
      "referenceID" : 30,
      "context" : "One attractive way to improve cross-lingual generalization is to perform data augmentation (Simard et al., 1998), and train the model on examples that are similar but different from the labeled data in the source language.",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 4,
      "context" : "Formalized by the Vicinal Risk Minimization (VRM) principle (Chapelle et al., 2001), such data augmentation methods have shown impressive results in vision (Zhang et al.",
      "startOffset" : 60,
      "endOffset" : 83
    }, {
      "referenceID" : 37,
      "context" : ", 2001), such data augmentation methods have shown impressive results in vision (Zhang et al., 2018; Berthelot et al., 2019).",
      "startOffset" : 80,
      "endOffset" : 124
    }, {
      "referenceID" : 3,
      "context" : ", 2001), such data augmentation methods have shown impressive results in vision (Zhang et al., 2018; Berthelot et al., 2019).",
      "startOffset" : 80,
      "endOffset" : 124
    }, {
      "referenceID" : 37,
      "context" : "1979 linear mixtures of features and labels (Zhang et al., 2018).",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 28,
      "context" : "In NLP, to the best of our knowledge, the most successful augmentation method has so far been back-translation (Sennrich et al., 2016) which paraphrases an input sentence through round-trip translation.",
      "startOffset" : 111,
      "endOffset" : 134
    }, {
      "referenceID" : 36,
      "context" : "Furthermore, back-translation is only applicable in a supervised setup and to tasks where it is possible to find the alignments between the original labeled entities and the back-translated entities, such as in question answering (Yu et al., 2018).",
      "startOffset" : 230,
      "endOffset" : 247
    }, {
      "referenceID" : 14,
      "context" : "Other related work includes contextual augmentation (Kobayashi, 2018), conditional BERT (Wu et al.",
      "startOffset" : 52,
      "endOffset" : 69
    }, {
      "referenceID" : 34,
      "context" : "Other related work includes contextual augmentation (Kobayashi, 2018), conditional BERT (Wu et al., 2018) and AUG-BERT (Shi et al.",
      "startOffset" : 88,
      "endOffset" : 105
    }, {
      "referenceID" : 5,
      "context" : "We propose novel ways to generate virtual sentences using a multilingual masked LM (Conneau et al., 2020), and get reliable task labels by simultaneous multilingual co-training.",
      "startOffset" : 83,
      "endOffset" : 105
    }, {
      "referenceID" : 38,
      "context" : "To avoid confirmation bias with self-training where the model accumulates its own errors, it simultaneously trains three task models to generate virtual training data through data augmentation and filtering of potential label noises via multi-epoch co-teaching (Zhou and Li, 2005).",
      "startOffset" : 261,
      "endOffset" : 280
    }, {
      "referenceID" : 21,
      "context" : "Such regularizer of output distribution has been shown to be effective for training large models (Pereyra et al., 2017).",
      "startOffset" : 97,
      "endOffset" : 119
    }, {
      "referenceID" : 10,
      "context" : "It has been shown that contextual LMs pretrained on large-scale datasets capture useful linguistic features and can be used to generate fluent grammatical texts (Hewitt and Manning, 2019).",
      "startOffset" : 161,
      "endOffset" : 187
    }, {
      "referenceID" : 5,
      "context" : "We use XLM-R masked LM (Conneau et al., 2020) as our vicinity model θmlm, which is trained on massive multilingual corpora (2.",
      "startOffset" : 23,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "Due to discrete nature of texts, VRM based augmentation methods that are successful for images such as MixMatch (Berthelot et al., 2019) that generates new samples and their labels as simple linear interpolation, have not been successful in NLP.",
      "startOffset" : 112,
      "endOffset" : 136
    }, {
      "referenceID" : 26,
      "context" : "XNER: We use the standard CoNLL datasets (Sang, 2002; Sang and Meulder, 2003) for English (en), German (de), Spanish (es) and Dutch (nl).",
      "startOffset" : 41,
      "endOffset" : 77
    }, {
      "referenceID" : 27,
      "context" : "XNER: We use the standard CoNLL datasets (Sang, 2002; Sang and Meulder, 2003) for English (en), German (de), Spanish (es) and Dutch (nl).",
      "startOffset" : 41,
      "endOffset" : 77
    }, {
      "referenceID" : 20,
      "context" : "To show how the models perform on extremely lowresource languages, we experiment with three structurally different languages from WikiANN (Pan et al., 2017) of different (unlabeled) training data sizes: Urdu (ur-20k training samples), Bengali (bn10K samples), and Burmese (my-100 samples).",
      "startOffset" : 138,
      "endOffset" : 156
    }, {
      "referenceID" : 2,
      "context" : "Table 1: F1 scores in XNER on the datasets from CoNLL and (Bari et al., 2020).",
      "startOffset" : 58,
      "endOffset" : 77
    }, {
      "referenceID" : 35,
      "context" : "PAWS-X The Paraphrase Adversaries from Word Scrambling Cross-lingual task (Yang et al., 2019) requires the models to determine whether",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 2,
      "context" : "2 Results XNER Table 1 reports the XNER results on the datasets from CoNLL and (Bari et al., 2020), where we also evaluate an ensemble by averaging the probabilities from the three models.",
      "startOffset" : 79,
      "endOffset" : 98
    }, {
      "referenceID" : 11,
      "context" : "However, our results resemble the reported XLM-R results of XTREME (Hu et al., 2020).",
      "startOffset" : 67,
      "endOffset" : 84
    }, {
      "referenceID" : 25,
      "context" : "Results with η = 100, Agreement = ∩ can be considered as the tri-training (Ruder and Plank, 2018b) baseline.",
      "startOffset" : 74,
      "endOffset" : 98
    }, {
      "referenceID" : 7,
      "context" : "Notably, mBERT (Devlin et al., 2019) extends (English) BERT by jointly training on 102 languages.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 15,
      "context" : "XLM (Lample and Conneau, 2019) extends mBERT with a conditional LM and a translation LM (using parallel data) objectives.",
      "startOffset" : 4,
      "endOffset" : 30
    }, {
      "referenceID" : 32,
      "context" : "Older data augmentation approaches relied on distributional clusters (Täckström et al., 2012).",
      "startOffset" : 69,
      "endOffset" : 93
    }, {
      "referenceID" : 14,
      "context" : "A number of recent methods have been proposed using contextualized LMs (Kobayashi, 2018; Wu et al., 2018; Shi et al., 2019; Ding et al., 2020; Liu et al., 2021).",
      "startOffset" : 71,
      "endOffset" : 160
    }, {
      "referenceID" : 34,
      "context" : "A number of recent methods have been proposed using contextualized LMs (Kobayashi, 2018; Wu et al., 2018; Shi et al., 2019; Ding et al., 2020; Liu et al., 2021).",
      "startOffset" : 71,
      "endOffset" : 160
    }, {
      "referenceID" : 29,
      "context" : "A number of recent methods have been proposed using contextualized LMs (Kobayashi, 2018; Wu et al., 2018; Shi et al., 2019; Ding et al., 2020; Liu et al., 2021).",
      "startOffset" : 71,
      "endOffset" : 160
    }, {
      "referenceID" : 8,
      "context" : "A number of recent methods have been proposed using contextualized LMs (Kobayashi, 2018; Wu et al., 2018; Shi et al., 2019; Ding et al., 2020; Liu et al., 2021).",
      "startOffset" : 71,
      "endOffset" : 160
    }, {
      "referenceID" : 17,
      "context" : "A number of recent methods have been proposed using contextualized LMs (Kobayashi, 2018; Wu et al., 2018; Shi et al., 2019; Ding et al., 2020; Liu et al., 2021).",
      "startOffset" : 71,
      "endOffset" : 160
    }, {
      "referenceID" : 19,
      "context" : "In a concurrent work (Mohiuddin et al., 2021), we propose a contextualized LM based data augmentation for neural machine translation and show its advantages over traditional back-translation gaining improved performance in low-resource scenarios.",
      "startOffset" : 21,
      "endOffset" : 45
    } ],
    "year" : 2021,
    "abstractText" : "Transfer learning has yielded state-of-the-art (SoTA) results in many supervised NLP tasks. However, annotated data for every target task in every target language is rare, especially for low-resource languages. We propose UXLA a novel unsupervised data augmentation framework for zero-resource transfer learning scenarios. In particular, UXLA aims to solve crosslingual adaptation problems from a source language task distribution to an unknown target language task distribution, assuming no training label in the target language. At its core, UXLA performs simultaneous selftraining with data augmentation and unsupervised sample selection. To show its effectiveness, we conduct extensive experiments on three diverse zero-resource cross-lingual transfer tasks. UXLA achieves SoTA results in all the tasks, outperforming the baselines by a good margin. With an in-depth framework dissection, we demonstrate the cumulative contributions of different components to its success.",
    "creator" : "LaTeX with hyperref"
  }
}