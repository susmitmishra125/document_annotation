{
  "name" : "2021.acl-long.8.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Enhancing Content Preservation in Text Style Transfer Using Reverse Attention and Conditional Layer Normalization",
    "authors" : [ "Dongkyu Lee", "Zhiliang Tian", "Lanqing Xue", "Nevin L. Zhang" ],
    "emails" : [ "lzhang}@cse.ust.hk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 93–102\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n93"
    }, {
      "heading" : "1 Introduction",
      "text" : "Style transfer is a popular task in computer vision and natural language processing. It aims to convert an input with a certain style (e.g., sentiment, formality) into a different style while preserving the original content.\nOne mainstream approach is to separate style from content, and to generate a transferred sentence conditioned on the content information and a target style. Recently, several models (Li et al., 2018; Xu et al., 2018; Wu et al., 2019) have proposed removing style information at the token level by filtering out tokens with style information, which are identified using either attention-based methods (Bahdanau et al., 2015) or frequency-ratio based methods (Wu et al., 2019). This line of work is built upon the assumption that style is localized to\ncertain tokens in a sentence, and a token has either content or style information, but not both. Thus by utilizing a style marking module, the models filter out the style tokens entirely when constructing a style-independent content representation of the input sentence. The drawback with the filtering method is that one needs to manually set a threshold to decide whether a token is stylistic or content-related. Previous studies address this issue by using the average attention score as a threshold (Li et al., 2018; Xu et al., 2018; Wu et al., 2019). A major shortcoming of this approach is the incapability of handling flat attention distribution. When the distribution is flat, in which similar attention scores are assigned to tokens, the style marking module would remove/mask out more tokens than necessary. This incurs information loss in content as depicted in Figure 1.\nIn this paper, we propose a novel method for text style transfer. A key idea is to exploit the fact that a token often posses both style and content information. For example, the word “delicious” is a token with strong style information, but it also implies the subject is food. Such words play a pivotal role in representing style (e.g., positive sentiment) as well as presenting a hint at the subject matter/content (e.g., food). The complete removal of such tokens leads to the loss of content information.\nFor the sake of enhancing content preservation,\nwe propose a method to implicitly remove style at the token level using reverse attention. We utilize knowledge attained from attention networks (Bahdanau et al., 2015) to estimate style information of a token, and suppress such signal to take out style. Attention mechanism is known to attend to interdependent representations given a query. In style classification task, an attention score could be interpreted as to what extent a token has style attribute. If we can identify which tokens reveal stylistic property and to what extent, it is then possible to take the negation and to approximate the amount of content attribute within a token. In this paper, we call it reverse attention. We utilize such score to suppress the stylistic attribute of tokens, fully capturing content property.\nThis paper further enhances content preservation by fusing content information in creating target style representation. Despite of extensive efforts in creating content representation, the previous work has overlooked building content-dependent style representations. The common approach is to project the target style onto an embedding space, and share the style embedding among the same style as an input to the decoder. However, our work sheds light on building content-related style by utilizing conditional layer normalization (CLN). This module of ours takes in content representations, and creates content-dependent style representation by shaping the content variable to fit in the distribution of target style. This way, our style representation varies according to the content of the input sequence even with the same target style.\nOur method is based on two techniques, Reverse Attention and Conditional Layer Normalization, thus we call it RACoLN. In empirical evaluation, RACoLN achieves the state-of-the-art performance in terms of content preservation, outperforming the previous state-of-the-art by a large margin, and shows competency in style transfer accuracy and fluency. The contributions are as follows:\n• We introduce reverse attention as a way to suppress style information while preserving content information when building a content representation of an input.\n• Aside from building style-independent content representation, our approach utilizes conditional layer normalization to construct content-dependent style representation.\n• Our model achieves state-of-the-art perfor-\nmance in terms of content preservation, outperforming current state-of-the-art by more than 4 BLEU score on Yelp dataset, and shows competency in other metrics as well."
    }, {
      "heading" : "2 Related Work",
      "text" : "In recent years, text style transfer in unsupervised learning environment has been studied and explored extensively. Text style transfer task views a sentence as being comprised of content and style. Thus, there have been attempts to disentangle the components (Shen et al., 2017; Li et al., 2018; Xu et al., 2018; Wu et al., 2019). Shen et al. (2017) map a sentence to a shared content space among styles to create style-independent content variable. Some studies view style as localized feature of sentences. Xu et al. (2018) propose to identify style tokens with attention mechanism, and filter out such tokens. Frequency-based is proposed to enhance the filtering process (Wu et al., 2019). This stream of work is similar to our work in that the objective is to take out style at the token level, but different since ours does not remove tokens completely.\nInstead of disentangling content and style, other papers focus on revising an entangled representation of an input. A few previous studies utilize a pre-trained classifier and edit entangled latent variable until it contains target style using the gradientbased optimization (Wang et al., 2019; Liu et al., 2020). He et al. (2020) view each domain of data as a partially observable variable, and transfer sentence using amortized variational inference. Dai et al. (2019) use the transformer architecture and rewrite style in the entangled representation at the decoder. We consider this model as the strongest baseline model in terms of content preservation.\nIn the domain of computer vision, it is a prevalent practice to exploit variants of normalization to transfer style (Dumoulin et al., 2017; Ulyanov et al., 2016). Dumoulin et al. (2017) proposed conditional instance normalization (CIN) in which each style is assigned with separate instance normalization parameter, in other words, a model learns separate gain and bias parameters of instance normalization for each style.\nOur work differs in several ways. Style transfer in image views style transfer as changing the “texture” of an image. Therefore, Dumoulin et al. (2017) place CIN module following every convolution layer, “painting” with style-specific parameters on the content representation. Therefore, the\nnetwork passes on entangled representation of an image. Our work is different in that we disentangle content and style, thus we do not overwrite content with style-specific parameters. In addition, we apply CLN only once before passing it to decoder."
    }, {
      "heading" : "3 Approach",
      "text" : ""
    }, {
      "heading" : "3.1 Task Definition",
      "text" : "Let D = {(xi, si)Ni=1} be a training corpus, where each xi is a sentence, and si is its style label. Our experiments were carried on a sentiment analysis task, where there are two style labels, namely “positive” and “negative.”\nThe task is to learn from D a model x̂ŝ = fθ(x, ŝ), with parameters θ, that takes an input sentence x and a target style ŝ as inputs, and outputs a new sentence x̂ŝ that is in the target style and retains the content information of x."
    }, {
      "heading" : "3.2 Model Overview",
      "text" : "We conduct this task in an unsupervised environment in which ground truth sentence xŝ is not provided. To achieve our goal, we employ a style classifier s = C(x) that takes a sentence x as input and returns its style label. We pre-train such model on D and keep it frozen in the process of learning fθ.\nGiven the style classifier C(x), our task becomes to learn a model x̂ŝ = fθ(x, ŝ) such that\nC(x̂ŝ) = ŝ. As such, the task is conceptually similar to adversarial attack: The input x is from the style class s, and we want to modify it so that it will be classified into the target style class ŝ.\nThe architecture of our model fθ is shown in Figure 2, which will some times referred to as the generator network. It consists of an encoder, a stylizer and a decoder. The encoder maps an input sequence x into a style-independent representation zx. Particularly, the encoder has a style marker module that computes attention scores of input tokens, and it “reverses” them to estimate the content information. The reversed attention scores are applied to the token embedding E(x) and the results E′(x) are fed to bidirectional GRU to produce zx.\nThe stylizer takes a target style ŝ and the content representation zx as inputs, and produces a content-related style representation zŝ. Finally, the decoder takes the content representation zx and style representation zŝ as inputs, and generates a new sequence x̂ŝ."
    }, {
      "heading" : "3.3 Encoder",
      "text" : ""
    }, {
      "heading" : "3.3.1 Style Marker Module",
      "text" : "Let x = [x1, x2, . . . , xT ] be a length T sequence of input with a style s. The style marker module is pre-trained in order to calculate the amount of style information in each token in a given input. We use one layer of bidirectional GRU with attention\n(Yang et al., 2016). Specifically,\nvt = tanh(Wwht + bw) (1)\nαt = exp(vTt u/τ)∑T t=1 exp(v T t u/τ)\n(2)\nwhere ht is the hidden representation from the bidirectional GRU at time step t. u is learnable parameters initialized with random weights, and τ denotes the temperature in softmax. When pre-training the style marker module, we construct a sentence representation by taking the weighted sum of the token representations with the weights being the attention scores, and feed the context vector to a fully-connected layer.\no = T∑ t=1 αtht (3)\np = softmax(Wco+ bc) (4)\nThe cross-entropy loss is used to learn the parameters of the style marker module. The attention scores in the style marker indicate what tokens are important to style classification, and to what extent. Those scores will be “reversed” in the next section to reveal the content information. The fullyconnected layer of the style marker module is no longer needed once the style marker module is trained. It is hence removed."
    }, {
      "heading" : "3.3.2 Reverse Attention",
      "text" : "Using attention score from the pre-trained style marker module, we propose to implicitly remove the style information in each token. We negate the extent of style information in each token to estimate the extent of content information, namely reverse attention.\nα̃t = 1− αt, T∑ t=1 αt = 1 (5)\nwhere αt is an attention value from style marker module, and α̃t is the corresponding reverse attention score. We multiply the reverse attention scores to the embedding vectors of tokens.\nẽt = α̃tet, et = E(xt) (6)\nIntuitively, this can be viewed as implicitly removing the stylistic attribute of tokens, suppressing the\nnorm of a token embedding respect to corresponding reverse attention score. The representations finally flow into a bidirectional GRU\nzx = bidirectionalGRU(ẽ) (7)\nto produce a content representation zx, which is the last hidden state of the bidirectional GRU. By utilizing reverse attention, we map a sentence to style-independent content representation."
    }, {
      "heading" : "3.4 Stylizer",
      "text" : "The goal of the stylizer is to create a content-related style representation. We do this by applying conditional layer normalization on the content representation zx from encoder as input to this module.\nLayer normalization requires the number of gain and bias parameters to match the size of input representation. Therefore, mainly for the purpose of shrinking the size, we perform affine transformation on the content variable.\nz̃x = Wzzx + bz (8)\nThe representation is then fed to conditional layer normalization so that the representation falls into target style distribution in style space. Specifically,\nzŝ = CLN(z̃x; ŝ) = γ ŝ N(z̃x) + βŝ (9)\nN(z̃x) = z̃x − µ σ\n(10)\nwhere µ and σ are mean and standard deviation of input vector respectively, and ŝ is target style. Our model learns separate γs (gain) and βs (bias) parameters for different styles.\nNormalization method is commonly used to change feature values in common scale, but known to implicitly keep the features. Therefore, we argue that the normalized content feature values retain content information of the content variable. By passing through conditional layer normalization module, the content latent vector is scaled and shifted with style-specific gain and bias parameter, falling into target style distribution. Thus, unlike previous attempts in text style transfer, the style representation is dynamic respect to the content, being content-dependent embedding.\nIn order to block backpropagation signal related to style flowing into zx, we apply stop gradient on zx before feeding it to stylizer."
    }, {
      "heading" : "3.5 Decoder",
      "text" : "The decoder generates a sentence with the target style conditioned on content-related style representation and content representation. We construct our decoder using one single layer of GRU.\nx̂ŝ ∼ Decθ(zx, zŝ) = pD(x̂ŝ|zx, zŝ) (11)\nAs briefly discussed in Section 3.2, the outputs from our generator are further passed on for different loss functions. However, sampling process or greedy decoding does not allow gradient to flow, because the methods are not differentiable. Therefore, we use soft sampling to keep the gradient flow. Specifically, when the gradient flow is required through the outputs, we take the product of probability distribution of each time step and the weight of embedding layer to project the outputs onto word embedding space. We empirically found that soft sampling is more suitable in our environment than gumbel-softmax (Jang et al., 2017)."
    }, {
      "heading" : "3.6 Pre-trained Style Classifier",
      "text" : "Due to the lack of parallel corpus, we cannot train generator network with maximum likelihood estimation on style transfer ability. Therefore, this paper employs a pre-trained classifier C(x) to train our generator on transferring style. Our classifier network has the same structure as style marker module with fully-connected layer appended, nonetheless, it is a separate model obtained from a different set of initial model parameters. We use the crossentropy loss for training:\nLpre = −E(x,s)∼D[log pC(s|xs)] (12)\nWe freeze the weights of this network after it has been fully trained."
    }, {
      "heading" : "3.7 The Loss Function",
      "text" : "As shown in Figure 3, our loss function consists of four parts: a self reconstruction loss Lself , a cycle reconstruction loss Lcycle, a content loss Lcontent, and a style transfer loss Lstyle."
    }, {
      "heading" : "3.7.1 Self Reconstruction Loss",
      "text" : "Let (x, s) ∈ D be a training example. If we ask our model to fθ(x, ŝ) to “transfer” the input into its original style, i.e., ŝ = s, we would expect it to reconstruct the input.\nLself = −E(x,s)∼D[log pD(x|zx, zs)] (13)\nwhere zx is the content representation of the input x, zs is the representation of the style s, and pD is the conditional distribution over sequences defined by the decoder."
    }, {
      "heading" : "3.7.2 Cycle Reconstruction Loss",
      "text" : "Suppose we first transfer a sequence x into another style ŝ to get x̂ŝ using soft sampling, and then transfer x̂ŝ back to the original style s. We would expect to reconstruct the input x. Hence we have the following cycle construction loss:\nLcycle = −E(x,s)∼D[log pD(x|zx̂ŝ , zs)] (14)\nwhere zx̂ŝ is the content representation of the transferred sequence x̂ŝ.1"
    }, {
      "heading" : "3.7.3 Content Loss",
      "text" : "In the aforementioned cycle reconstruction process, we obtain a content representation zx of the input x and a content representation zx̂ŝ of the transferred sequence x̂ŝ. As the two transfer steps presumably involve only style but not content, the two content representations should be similar. Hence we have the following content loss:\nLcontent = E(x,s)∼D||zx − zx̂ŝ || 2 2 (15) 1Strictly speaking, the quantity is not well-defined because there is no description of how the target style ŝ is picked. In our experiments, we use data with two styles. So, the target style just means the other style. To apply the method to problems with multiple styles, random sampling of different style should be added. This remark applies also to the two loss terms to be introduced below."
    }, {
      "heading" : "3.7.4 Style Transfer Loss",
      "text" : "We would like the transferred sequence x̂ŝ to be of style ŝ. Hence we have the following style transfer loss:\nLstyle = −E(x,s)∼D[log pC(ŝ|x̂ŝ)] (16)\nwhere pC is the conditional distribution over styles defined by the style classifier C(x). As mentioned in Section 3.5, x̂ŝ was generated with soft sampling."
    }, {
      "heading" : "3.7.5 Total Loss",
      "text" : "In summary, we balance the four loss functions to train our model.\nL = λ1Lself + λ2Lcycle + λ3Lcontent + λ4Lstyle (17) where λi is balancing parameter."
    }, {
      "heading" : "4 Experiment",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "Following prior work on text style transfer, we use two common datasets: Yelp and IMDB review."
    }, {
      "heading" : "4.1.1 Yelp Review",
      "text" : "Our study uses Yelp review dataset (Li et al., 2018) which contains 266K positive and 177K negative reviews. Test set contains a total of 1000 sentences, 500 positive and 500 negative, and humanannotated sentences are provided which are used in measuring content preservation."
    }, {
      "heading" : "4.1.2 IMDB Movie Review",
      "text" : "Another dataset we test is IMDB movie review dataset (Dai et al., 2019). This dataset is comprised of 17.9K positive and 18.8K negative reviews for training corpus, and 2K sentences are used for testing."
    }, {
      "heading" : "4.2 Automatic Evaluation",
      "text" : ""
    }, {
      "heading" : "4.2.1 Style Transfer Accuracy",
      "text" : "Style transfer accuracy (S-ACC) measures whether the generated sentences reveal target style property. We have mentioned a style classifier before: C(x) which is used in the loss function. To evaluate transfer accuracy, we train another style classifier Ceval(x). It has the identical architecture as before and trained on the same data, except from a different set of initial model parameters. We utilize such structure due to its superior performance compared to that of commonly used CNN-based\nclassifier (Kim, 2014). Our evaluation classifier achieves accuracy of 97.8% on Yelp and 98.9% on IMDB, which are higher than that of CNN-based."
    }, {
      "heading" : "4.2.2 Content Preservation",
      "text" : "A well-transferred sentence must maintain its content. In this paper, content preservation was evaluated with two BLEU scores (Papineni et al., 2002), one between generated sentence and input sentence (self-BLEU), and the other with humangenerated sentence (ref-BLEU). With this metric, one can evaluate how a sentence maintains its content throughout inference."
    }, {
      "heading" : "4.2.3 Fluency",
      "text" : "A natural language generation task aims to output a sentence, which is not only task-specific, but also fluent. This study measures perplexity (PPL) of generated sentences in order to measure fluency. Following (Dai et al., 2019), we use 5- gram KenLM (Heafield, 2011) trained on the two training datasets. A lower PPL score indicates a transferred sentence is more fluent."
    }, {
      "heading" : "4.2.4 BERT Score",
      "text" : "Zhang et al. (2020) proposed BERT score which computes contextual similarity of two sentences. Previous methods, such as BLEU score, compute ngram matching score, while BERT score evaluates the contextual embedding of the tokens obtained from pre-trained BERT (Devlin et al., 2019). This evaluation metric has been shown to correlate with human judgement, thus our paper includes BERT score between model generated output and the human reference sentences. We report precision, recall, and F1 score."
    }, {
      "heading" : "4.3 Human Evaluation",
      "text" : "In addition to automatic evaluation, we validate the generated outputs with human evaluation. With each model, we randomly sample 150 outputs from each of the two datasets, total of 300 outputs per model. Given the target style and the original sentence, the annotators are asked to evaluate the model generated sentence with a score range from 1 (Very Bad) to 5 (Very Good) on content preservation, style transfer accuracy, and fluency. We report the average scores from the 4 hired annotators in Table 3."
    }, {
      "heading" : "4.4 Implementation Details",
      "text" : "In this paper, we set the embedding size to 128 dimension and hidden representation dimension of\nencoder to 500. The size of bias and gain parameters of conditional layer norm is 200, and the size of hidden representation for decoder is set to 700 to condition on both content and style representation. Adam optimizer (Kingma and Ba, 2015) was used to update parameter with learning rate set to 0.0005. For balancing parameters of total loss function, we set to 0.5 for λ1 and λ2, and 1 for the rest."
    }, {
      "heading" : "4.5 Experimental Result & Analysis",
      "text" : "We compare our model with the baseline models, and the automatic evaluation result is presented in Table 1. Our model outperforms the baseline\n2https://github.com/shentianxiao/ language-style-transfer\n3https://github.com/asyml/texar/tree/ master/examples/text_style_transfer\n4https://github.com/fastnlp/ style-transformer\n5https://github.com/cindyxinyiwang/ deep-latent-sequence-model\nmodels in terms of content preservation on both of the datasets. Especially, on Yelp dataset, our model achieves 59.4 self-BLEU score, surpassing the previous state-of-the-art model by more than 4 points. Furthermore, our model also achieves the state-of-the-art result in content preservation on IMDB dataset, which is comprised of longer sequences than those of Yelp.\nIn terms of style transfer accuracy and fluency, our model is highly competitive. Our model achieves the highest score in style transfer accuracy on both of the datasets (91.3 on Yelp and 83.1 on IMDB). Additionally, our model shows the ability to produce fluent sentences as shown in the perplexity score. In terms of the BERT scores, the proposed model performs the best, having the highest contextual similarity with the human reference among the style transfer models.\nWith the automatic evaluation result, we see a trend of trade-off. Most of the baseline models are good at particular metric, but show room for improvement on other metrics. For example, Deep Latent and Cross-Alignment constantly perform well in terms of perplexity, but their ability to transfer style and preserving content needs improvement. Style Transformer achieves comparable performance across all evaluation metrics, but our model outperforms the model on every metric on both of the datasets. Therefore, the result shows that our model is well-balanced but also strong in every aspect in text style transfer task.\nAs for the human evaluation, we observe that the result mainly conform with the automatic evaluation. Our model received the highest score on the style and content evaluation metric on both of the datasets by a large margin compared to the other baselines. Moreover, the fluency score is comparable with that of Deep Latent model, showing its competency in creating a fluent output. Both automatic and human evaluation depict the strength of\nTable 4: Sample outputs generated by the baseline models and our approach on Yelp and IMDB dataset. Bold words indicate successful transfer in style without grammatical error.\nYELP\nOriginal Input Everyone is always super friendly and helpful . Cross-Alignment Everyone is always super friendly and helpful and inattentive . ControlledGen Tonight selection of meats and cheeses . Deep Latent Now i ’m not sure how to be . Style Transformer Which is n’t super friendly . RACoLN (Ours) Everyone is always super rude and unprofessional .\nOriginal Input I love this place , the service is always great ! Cross-Alignment I know this place , the food is just a horrible ! ControlledGen I avoid this place , the service is nasty depressing vomit Deep Latent I do n’t know why the service is always great ! Style Transformer I do n’t recommend this place , the service is n’t ! RACoLN (Ours) I avoid this place , the service is always horrible !\nIMDB\nOriginal Input I actually disliked the leading characters so much that their antics were never funny but pathetic . Cross-Alignment I have never get a good movie , i have never have seen in this movie . ControlledGen I actually anticipated the leading characters so much that their antics were never funny but timeless . Deep Latent I actually disliked the leading characters so much that their antics were never funny but incredible . Style Transformer I actually disliked the leading characters so much that their antics were never funny but vhs . RACoLN (Ours) I actually liked the leading characters so much that their antics were never corny but appropriate .\nOriginal Input The plot is clumsy and has holes in it . Cross-Alignment The worst film is one of the worst movies i ’ve ever seen . ControlledGen The plot is top-notch and has one-liners in it . Deep Latent The plot is tight and has found it in a very well done . Style Transformer The plot is joys and has flynn in it . RACoLN (Ours) The plot is incredible and has twists in it .\nthe proposed model not only in preserving content, but also on other metrics."
    }, {
      "heading" : "4.5.1 Style and Content Space",
      "text" : "We visualize the test dataset of Yelp projected on content and style space using t-SNE in Figure 4. It is clearly observed that the content representations (zx) are spread across content space, showing that the representations are independent of style. After the content representations go through the stylizer module, there is a clear distinction between different styles representations (zŝ) in style space. This is in sharp contrast to the corresponding distributions of the style-independent content representations shown on the right of the figure. The figure clearly depicts how style-specific parameters in the stylizer module shape the content representations to fall in the target style distribution. This figure illustrates how our model successfully removes style at the encoder, and constructs content-related style at the stylizer module."
    }, {
      "heading" : "4.5.2 Ablation Study",
      "text" : "In order to validate the proposed modules, we conduct ablation study on Yelp dataset which is pre-\nsented in Table 5. We observe a significant drop across all aspects without the reverse attention module. In other case, where we remove the stylizer module and use style embedding as in the previous papers, the model loses the ability to retain content, drop of around 6 score on self-BLEU. We find that the two core components are interdependent in successfully transferring style in text. Lastly, as for the loss functions, incorporating Lcontent brings a meaningful increase in content preservation.6"
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we introduce a way to implicitly remove style at the token level using reverse attention, and fuse content information to style representation using conditional layer normalization. With the two core components, our model is able to enhance content preservation while keeping the outputs fluent with target style. Both automatic and human evaluation shows that our model has the best ability in preserving content and is strong in other metrics as well. In the future, we plan to study problems with more than two styles and apply multiple attribute\n6Other loss functions were not included, since the loss functions have been extensively tested and explored in previous papers (Prabhumoye et al., 2018; Dai et al., 2019).\nstyle transfer, where the target style is comprised of multiple styles."
    }, {
      "heading" : "Acknowledgement",
      "text" : "Research on this paper was supported by Hong Kong Research Grants Council under grant 16204920 and Tencent AI Lab Rhino-Bird Focused Research Program (No. GF202035).\nEthical Considerations\nA text style transfer model is a conditional generative model, in which the condition is the target style. This makes a wide range of applications possible, since a style can be defined as any common feature in a corpus, such as formality, tense, sentiment, etc.\nHowever, at the same time, due to its inherent functionality, a text style transfer model can pose potential harm when used with a malicious intention. It can lead to a situation where one deliberately distorts a sentence for his or her own benefit. To give an example in a political context, political stance can be viewed a style in political slant dataset (Voigt et al., 2018) as in (Prabhumoye et al., 2018). If one intentionally changes the style (political stance) of a person with the proposed model structure, the generated output can be exploited to create fake news or misinformation. One possible remedy for such potentially problematic situation is to employ fact checking system as a safety measure (Nadeem et al., 2019). We are fully aware that fact checking is not the fundamental solution to the potential harm that text style transfer models possess. Nevertheless, one can filter out misleading information using the system in certain domains (i.e., politics), lowering the level of the danger that can be otherwise posed by style transfer. In conclusion, such problem is shared among conditional generative models in general, and future studies on how to mitigate this problem are in crucial need.\nOur work validates the proposed model and the baseline models on human evaluation, in which manual work was involved. Thus, we disclose the compensation level given to the hired annotators. The average lengths of the two corpora tested are 10.3 words for Yelp and 15.5 words for IMDB. In addition, the annotation was performed on sentence-level, in which the annotators were asked to score a model generated sentence. Considering the length and the difficulty, the expected annotations per hour was 100 sentences. The hourly pay was set to 100 Hong Kong dollars\n(HK$), which is higher than Hong Kong’s statutory minimum wage. The annotators evaluated 1,500 sentences in total (750 sentences per dataset), thus each annotator was compensated with the total amount of HK$1,500."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Style transformer: Unpaired text style transfer without disentangled latent representation",
      "author" : [ "Ning Dai", "Jianze Liang", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5997–",
      "citeRegEx" : "Dai et al\\.,? 2019",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "A learned representation for artistic style",
      "author" : [ "Vincent Dumoulin", "Jonathon Shlens", "Manjunath Kudlur." ],
      "venue" : "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.",
      "citeRegEx" : "Dumoulin et al\\.,? 2017",
      "shortCiteRegEx" : "Dumoulin et al\\.",
      "year" : 2017
    }, {
      "title" : "A probabilistic formulation of unsupervised text style transfer",
      "author" : [ "Junxian He", "Xinyi Wang", "Graham Neubig", "Taylor Berg-Kirkpatrick." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-",
      "citeRegEx" : "He et al\\.,? 2020",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2020
    }, {
      "title" : "KenLM: Faster and smaller language model queries",
      "author" : [ "Kenneth Heafield." ],
      "venue" : "Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 187–197, Edinburgh, Scotland. Association for Computational Linguistics.",
      "citeRegEx" : "Heafield.,? 2011",
      "shortCiteRegEx" : "Heafield.",
      "year" : 2011
    }, {
      "title" : "Toward controlled generation of text",
      "author" : [ "Zhiting Hu", "Zichao Yang", "Xiaodan Liang", "Ruslan Salakhutdinov", "Eric P. Xing." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Re-",
      "citeRegEx" : "Hu et al\\.,? 2017",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2017
    }, {
      "title" : "Categorical reparameterization with gumbel-softmax",
      "author" : [ "Eric Jang", "Shixiang Gu", "Ben Poole." ],
      "venue" : "5th",
      "citeRegEx" : "Jang et al\\.,? 2017",
      "shortCiteRegEx" : "Jang et al\\.",
      "year" : 2017
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751, Doha, Qatar. Association for Computational Linguis-",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Delete, retrieve, generate: a simple approach to sentiment and style transfer",
      "author" : [ "Juncen Li", "Robin Jia", "He He", "Percy Liang." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Revision in continuous space: Unsupervised text style transfer without adversarial learning",
      "author" : [ "Dayiheng Liu", "Jie Fu", "Yidan Zhang", "Chris Pal", "Jiancheng Lv." ],
      "venue" : "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "FAKTA: An automatic end-to-end fact checking system",
      "author" : [ "Moin Nadeem", "Wei Fang", "Brian Xu", "Mitra Mohtarami", "James Glass." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics",
      "citeRegEx" : "Nadeem et al\\.,? 2019",
      "shortCiteRegEx" : "Nadeem et al\\.",
      "year" : 2019
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Style transfer through back-translation",
      "author" : [ "Shrimai Prabhumoye", "Yulia Tsvetkov", "Ruslan Salakhutdinov", "Alan W Black." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 866–876,",
      "citeRegEx" : "Prabhumoye et al\\.,? 2018",
      "shortCiteRegEx" : "Prabhumoye et al\\.",
      "year" : 2018
    }, {
      "title" : "Style transfer from non-parallel text by cross-alignment",
      "author" : [ "Tianxiao Shen", "Tao Lei", "Regina Barzilay", "Tommi Jaakkola." ],
      "venue" : "I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,",
      "citeRegEx" : "Shen et al\\.,? 2017",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2017
    }, {
      "title" : "Instance normalization: The missing ingredient for fast stylization",
      "author" : [ "Dmitry Ulyanov", "Andrea Vedaldi", "Victor S. Lempitsky." ],
      "venue" : "CoRR, abs/1607.08022.",
      "citeRegEx" : "Ulyanov et al\\.,? 2016",
      "shortCiteRegEx" : "Ulyanov et al\\.",
      "year" : 2016
    }, {
      "title" : "RtGender: A corpus for studying differential responses to gender",
      "author" : [ "Rob Voigt", "David Jurgens", "Vinodkumar Prabhakaran", "Dan Jurafsky", "Yulia Tsvetkov." ],
      "venue" : "Proceedings of the Eleventh International Conference on Language Resources and Evaluation",
      "citeRegEx" : "Voigt et al\\.,? 2018",
      "shortCiteRegEx" : "Voigt et al\\.",
      "year" : 2018
    }, {
      "title" : "Controllable unsupervised text attribute transfer via editing entangled latent representation",
      "author" : [ "Ke Wang", "Hang Hua", "Xiaojun Wan." ],
      "venue" : "Advances in Neural Information Processing Systems 32, pages 11036–11046. Curran Associates, Inc.",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Mask and infill: Applying masked language model for sentiment transfer",
      "author" : [ "Xing Wu", "Tao Zhang", "Liangjun Zang", "Jizhong Han", "Songlin Hu." ],
      "venue" : "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pages",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Unpaired sentiment-to-sentiment translation: A cycled reinforcement learning approach",
      "author" : [ "Jingjing Xu", "Xu Sun", "Qi Zeng", "Xiaodong Zhang", "Xuancheng Ren", "Houfeng Wang", "Wenjie Li." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for",
      "citeRegEx" : "Xu et al\\.,? 2018",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2018
    }, {
      "title" : "Hierarchical attention networks for document classification",
      "author" : [ "Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Bertscore: Evaluating text generation with bert",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Recently, several models (Li et al., 2018; Xu et al., 2018; Wu et al., 2019) have proposed removing style information at the token level by filtering out tokens with style information, which are identified using either attention-based methods (Bahdanau et al.",
      "startOffset" : 25,
      "endOffset" : 76
    }, {
      "referenceID" : 20,
      "context" : "Recently, several models (Li et al., 2018; Xu et al., 2018; Wu et al., 2019) have proposed removing style information at the token level by filtering out tokens with style information, which are identified using either attention-based methods (Bahdanau et al.",
      "startOffset" : 25,
      "endOffset" : 76
    }, {
      "referenceID" : 19,
      "context" : "Recently, several models (Li et al., 2018; Xu et al., 2018; Wu et al., 2019) have proposed removing style information at the token level by filtering out tokens with style information, which are identified using either attention-based methods (Bahdanau et al.",
      "startOffset" : 25,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : ", 2019) have proposed removing style information at the token level by filtering out tokens with style information, which are identified using either attention-based methods (Bahdanau et al., 2015) or frequency-ratio based methods (Wu et al.",
      "startOffset" : 174,
      "endOffset" : 197
    }, {
      "referenceID" : 19,
      "context" : ", 2015) or frequency-ratio based methods (Wu et al., 2019).",
      "startOffset" : 41,
      "endOffset" : 58
    }, {
      "referenceID" : 10,
      "context" : "Previous studies address this issue by using the average attention score as a threshold (Li et al., 2018; Xu et al., 2018; Wu et al., 2019).",
      "startOffset" : 88,
      "endOffset" : 139
    }, {
      "referenceID" : 20,
      "context" : "Previous studies address this issue by using the average attention score as a threshold (Li et al., 2018; Xu et al., 2018; Wu et al., 2019).",
      "startOffset" : 88,
      "endOffset" : 139
    }, {
      "referenceID" : 19,
      "context" : "Previous studies address this issue by using the average attention score as a threshold (Li et al., 2018; Xu et al., 2018; Wu et al., 2019).",
      "startOffset" : 88,
      "endOffset" : 139
    }, {
      "referenceID" : 0,
      "context" : "We utilize knowledge attained from attention networks (Bahdanau et al., 2015) to estimate style information of a token, and suppress such signal to take out style.",
      "startOffset" : 54,
      "endOffset" : 77
    }, {
      "referenceID" : 15,
      "context" : "Thus, there have been attempts to disentangle the components (Shen et al., 2017; Li et al., 2018; Xu et al., 2018; Wu et al., 2019).",
      "startOffset" : 61,
      "endOffset" : 131
    }, {
      "referenceID" : 10,
      "context" : "Thus, there have been attempts to disentangle the components (Shen et al., 2017; Li et al., 2018; Xu et al., 2018; Wu et al., 2019).",
      "startOffset" : 61,
      "endOffset" : 131
    }, {
      "referenceID" : 20,
      "context" : "Thus, there have been attempts to disentangle the components (Shen et al., 2017; Li et al., 2018; Xu et al., 2018; Wu et al., 2019).",
      "startOffset" : 61,
      "endOffset" : 131
    }, {
      "referenceID" : 19,
      "context" : "Thus, there have been attempts to disentangle the components (Shen et al., 2017; Li et al., 2018; Xu et al., 2018; Wu et al., 2019).",
      "startOffset" : 61,
      "endOffset" : 131
    }, {
      "referenceID" : 19,
      "context" : "Frequency-based is proposed to enhance the filtering process (Wu et al., 2019).",
      "startOffset" : 61,
      "endOffset" : 78
    }, {
      "referenceID" : 18,
      "context" : "A few previous studies utilize a pre-trained classifier and edit entangled latent variable until it contains target style using the gradientbased optimization (Wang et al., 2019; Liu et al., 2020).",
      "startOffset" : 159,
      "endOffset" : 196
    }, {
      "referenceID" : 11,
      "context" : "A few previous studies utilize a pre-trained classifier and edit entangled latent variable until it contains target style using the gradientbased optimization (Wang et al., 2019; Liu et al., 2020).",
      "startOffset" : 159,
      "endOffset" : 196
    }, {
      "referenceID" : 3,
      "context" : "In the domain of computer vision, it is a prevalent practice to exploit variants of normalization to transfer style (Dumoulin et al., 2017; Ulyanov et al., 2016).",
      "startOffset" : 116,
      "endOffset" : 161
    }, {
      "referenceID" : 16,
      "context" : "In the domain of computer vision, it is a prevalent practice to exploit variants of normalization to transfer style (Dumoulin et al., 2017; Ulyanov et al., 2016).",
      "startOffset" : 116,
      "endOffset" : 161
    }, {
      "referenceID" : 7,
      "context" : "We empirically found that soft sampling is more suitable in our environment than gumbel-softmax (Jang et al., 2017).",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 10,
      "context" : "Our study uses Yelp review dataset (Li et al., 2018) which contains 266K positive and 177K negative reviews.",
      "startOffset" : 35,
      "endOffset" : 52
    }, {
      "referenceID" : 1,
      "context" : "Another dataset we test is IMDB movie review dataset (Dai et al., 2019).",
      "startOffset" : 53,
      "endOffset" : 71
    }, {
      "referenceID" : 8,
      "context" : "We utilize such structure due to its superior performance compared to that of commonly used CNN-based classifier (Kim, 2014).",
      "startOffset" : 113,
      "endOffset" : 124
    }, {
      "referenceID" : 13,
      "context" : "In this paper, content preservation was evaluated with two BLEU scores (Papineni et al., 2002), one between generated sentence and input sentence (self-BLEU), and the other with humangenerated sentence (ref-BLEU).",
      "startOffset" : 71,
      "endOffset" : 94
    }, {
      "referenceID" : 1,
      "context" : "Following (Dai et al., 2019), we use 5gram KenLM (Heafield, 2011) trained on the two training datasets.",
      "startOffset" : 10,
      "endOffset" : 28
    }, {
      "referenceID" : 5,
      "context" : ", 2019), we use 5gram KenLM (Heafield, 2011) trained on the two training datasets.",
      "startOffset" : 28,
      "endOffset" : 44
    }, {
      "referenceID" : 2,
      "context" : "Previous methods, such as BLEU score, compute ngram matching score, while BERT score evaluates the contextual embedding of the tokens obtained from pre-trained BERT (Devlin et al., 2019).",
      "startOffset" : 165,
      "endOffset" : 186
    }, {
      "referenceID" : 9,
      "context" : "Adam optimizer (Kingma and Ba, 2015) was used to update parameter with learning rate set to 0.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 14,
      "context" : "Other loss functions were not included, since the loss functions have been extensively tested and explored in previous papers (Prabhumoye et al., 2018; Dai et al., 2019).",
      "startOffset" : 126,
      "endOffset" : 169
    }, {
      "referenceID" : 1,
      "context" : "Other loss functions were not included, since the loss functions have been extensively tested and explored in previous papers (Prabhumoye et al., 2018; Dai et al., 2019).",
      "startOffset" : 126,
      "endOffset" : 169
    }, {
      "referenceID" : 17,
      "context" : "To give an example in a political context, political stance can be viewed a style in political slant dataset (Voigt et al., 2018) as in (Prabhumoye et al.",
      "startOffset" : 109,
      "endOffset" : 129
    }, {
      "referenceID" : 12,
      "context" : "One possible remedy for such potentially problematic situation is to employ fact checking system as a safety measure (Nadeem et al., 2019).",
      "startOffset" : 117,
      "endOffset" : 138
    } ],
    "year" : 2021,
    "abstractText" : "Text style transfer aims to alter the style (e.g., sentiment) of a sentence while preserving its content. A common approach is to map a given sentence to content representation that is free of style, and the content representation is fed to a decoder with a target style. Previous methods in filtering style completely remove tokens with style at the token level, which incurs the loss of content information. In this paper, we propose to enhance content preservation by implicitly removing the style information of each token with reverse attention, and thereby retain the content. Furthermore, we fuse content information when building the target style representation, making it dynamic with respect to the content. Our method creates not only styleindependent content representation, but also content-dependent style representation in transferring style. Empirical results show that our method outperforms the state-of-the-art baselines by a large margin in terms of content preservation. In addition, it is also competitive in terms of style transfer accuracy and fluency.",
    "creator" : "LaTeX with hyperref"
  }
}