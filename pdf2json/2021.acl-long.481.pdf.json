{
  "name" : "2021.acl-long.481.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Attend What You Need: Motion-Appearance Synergistic Networks for Video Question Answering",
    "authors" : [ "Ahjeong Seo", "Gi-Cheon Kang", "Joonhan Park", "Byoung-Tak Zhang" ],
    "emails" : [ "ajseo@bi.snu.ac.kr", "gckang@bi.snu.ac.kr", "jhpark@bi.snu.ac.kr", "btzhang@bi.snu.ac.kr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6167–6177\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6167"
    }, {
      "heading" : "1 Introduction",
      "text" : "Recently, research in natural language processing and computer vision has made significant progress in artificial intelligence (AI). Thanks to this, visionlanguage tasks such as image captioning (Xu et al., 2015), visual question answering (VQA) (Antol et al., 2015; Goyal et al., 2017), and visual commonsense reasoning (VCR) (Zellers et al., 2019) have been introduced to the research community,\n∗ Work done during an internship at AI Institute for Seoul National University (AIIS).\nalong with some benchmark datasets. In particular, video question answering (video QA) tasks (Xu et al., 2016; Jang et al., 2017; Lei et al., 2018; Yu et al., 2019; Choi et al., 2020) have been proposed with the goal of reasoning over higher-level visionlanguage interactions. In contrast to QA tasks based on static images, the questions presented in the video QA dataset vary from frame-level questions regarding the appearance of objects (e.g., what is the color of the hat?) to questions regarding action and causality (e.g., what does the man do after opening a door?).\nThere are three crucial challenges in video QA: (1) understand the intention of various questions, (2) capturing various elements of the input video (e.g., object, action, and causality), and (3) crossmodal grounding between language and vision information. To tackle these challenges, previous studies (Li et al., 2019; Jiang et al., 2020; Huang et al., 2020) have mainly explored this task by jointly embedding the features from the pre-trained word embedding model (Pennington et al., 2014) and the object detection models (He et al., 2016; Ren et al., 2016). However, as discussed in (Gao et al., 2018), the use of the visual features extracted from the object detection models suffers from motion analysis since the object detection model lacks temporal modeling. To enforce the motion analysis, a few approaches (Xu et al., 2017; Gao et al., 2018) have employed additional visual features (Tran et al., 2015) (i.e., motion features) which were widely used in the action recognition domain, but their reasoning capability is still limited. They typically employed recurrent models (e.g., LSTM) to embed a long sequence of the visual features. Due to the problem of long-term dependency in recurrent models (Bengio et al., 1993), their proposed methods may fail to learn dependencies between distant features.\nIn this paper, we propose Motion-Appearance\nSynergistic Networks (MASN) for video question answering which consist of three kinds of modules: the motion module, the appearance module, and the motion-appearance fusion module. As shown in Figure 1, the motion module and the appearance module aim to embed rich cross-modal representations. These two modules have the same architecture except that the motion module takes the motion features extracted from I3D as visual features and the appearance module utilizes the appearance features extracted from ResNet. Each of these modules first constructs the object graphs via graph convolutional networks (GCN) to compute the relationships among objects in each visual feature. Then, the vision-question interaction module performs cross-modal grounding between the output of the GCNs and the question features. The motion module and the appearance module each yield cross-modal representations of the motion and the appearance aspects of the input video respectively. The motion-appearance fusion module finally integrates these two features based on the question features.\nThe main contributions of our paper are as follows. First, we propose Motion-Appearance Synergistic Networks (MASN) for video question answering based on three modules, the motion module, the appearance module, and the motionappearance fusion module. Second, we validate MASN on the large-scale video question answering datasets TGIF-QA, MSVD-QA, and MSRVTT-QA.\nMASN achieves the new state-of-the-art performance on TGIF-QA and MSVD-QA. We perform ablation studies to validate the effectiveness of our proposed methods. Finally, we conduct a qualitative analysis of MASN by visualizing inference results."
    }, {
      "heading" : "2 Related Work",
      "text" : "Visual Question Answering (VQA) is a task that requires both understanding questions and finding clues from visual information. VQA can be classified into two categories based on the type of the visual source: image QA and video QA. In image QA, earlier works approach the task by applying attention between the question and the spatial dimensions of the image (Yang et al., 2016; Anderson et al., 2018; Kim et al., 2018a; Kang et al., 2019). In video QA, since a video is represented as a sequence of images over time, recognizing the movement of objects or causality in the temporal dimension should also be considered along with the details from the spatial dimension (Jang et al., 2017; On et al., 2020). There have been some attempts (Xu et al., 2017; Gao et al., 2018; Fan et al., 2019) to extract motion and appearance features and integrate them on a spatio-temporal dimension via memory networks. Li et al. (2019), Huang et al. (2020), Jiang et al. (2020) proposed better performing models using attention in order to overcome the long-range dependency problem in memory networks. However, they do not represent motion in-\nformation sufficiently since they only use features pre-trained on image or object classification. To better address this, we model spatio-temporal reasoning on multiple visual information (i.e., ResNet, I3D) while also solving the long-range dependency problem that occurred in previous studies. Action Classification is a task of recognizing actions, which are composed of interactions between actors and objects. Therefore, this task has much in common with video QA, in that the model should perform spatio-temporal reasoning. For better spatio-temporal reasoning, Tran et al. (2015) introduced C3D, which extends the 2D CNN filters to the temporal dimension. Carreira and Zisserman (2017) proposed I3D, which integrates 3D convolutions into a state-of-the-art 2D CNN architecture, which now acts as a baseline in action classification tasks (Murray et al., 2012; Girdhar et al., 2018). Feichtenhofer et al. (2019) introduced SlowFast, a network which encodes images in two streams with different frame rates and temporal resolutions of convolution. This study based on a two-stream architecture inspired us in terms of assigning different inputs to each encoder module. However, our method differs from the former studies in two aspects: (1) we utilize language features as well as vision features, and (2) we expand the two-stream structure to solve more than motion-oriented tasks. Attention Mechanism explicitly calculates the correlation between two features (Bahdanau et al., 2015; Lin et al., 2017), and has been widely used in a variety of fields. For machine translation, the Transformer architecture first introduced by Vaswani et al. (2017), utilizes multi-head selfattention that captures diverse aspects in the input features (Voita et al., 2019). For video QA, Kim et al. (2018b); Li et al. (2019) use self and guidedattention to encode temporal dynamics in video and ground them in the question. For multi-modal alignment, Tsai et al. (2019) apply the Transformer to merge cross-modal time series between vision, language, and audio features. We utilize the attention mechanism to capture various relations between appearance and motion and to aggregate them."
    }, {
      "heading" : "3 Model",
      "text" : "In this section, we introduce a detailed description of our MASN network. First, we explain how to obtain appearance and motion features in Section 3.1. Then, we describe the Appearance and Motion modules, which encode visual features and com-\nbine them with the question in Section 3.2. Finally, the Motion-Appearance Fusion module modulates the amount of motion and appearance information utilized and integrates them based on question context."
    }, {
      "heading" : "3.1 Visual and Linguistic Representation",
      "text" : "We first extract appearance and motion features from the video frames. For the appearance representation, we use ResNet (He et al., 2016) pre-trained on an object and its attribute classification task as a feature extractor. For the motion representation, we use I3D (Carreira and Zisserman, 2017) pretrained on the action classification task. We obtain local features representing object-level information without background noise and global features representing each frame’s context for both appearance and motion features.\nAppearance Representation. For local features, given a video containing T frames, we obtain N objects from each frame using Faster R-CNN (Ren et al., 2016) that applies RoIAlign to extract the region of interest from ResNet’s convolutional layer. We denote the appearance-object set asRa = {oat,n,bt,n} t=T,n=N t=1,n=1 , where o, b indicate object feature and bounding box location, respectively. Therefore, there areK = N×T objects in a single video. Following previous works, we extract the feature map from ResNet-152’s Conv5 layer and apply a linear projection (Jiang et al., 2020; Huang et al., 2020). We denote global features as vaglobal ∈ RT×d, where d is the size of the hidden dimension.\nMotion Representation. We obtain a feature map from the last convolutional layer in I3D (Carreira and Zisserman, 2017) whose dimension is (time, width, height, feature) = ( ⌊ t 8 ⌋ , 7, 7, 2048). That is, each set of 8 frames is represented as a single feature map with dimension 7 × 7 × 2048. For local features, we apply RoIAlign (He et al., 2017) on the feature map using object bounding box location b. We define the motion-object set as Rm = {omt,n,bt,n} t=T,n=N t=1,n=1 . We apply average pooling in the feature map and linear projection to obtain global features vmglobal ∈ RT×d.\nLocation Encoding. To reason about relations between objects as in Section 3.2, it is required to consider each object’s spatial and temporal location. As appearance and motion features share identical operations until the Motion-Appearance\nFusion module, we combine superscript a and m for simplicity. Following L-GCN (Huang et al., 2020), we add a location encoding and define local features as:\nv a/m local = FFN([o a/m;ds;dt]) (1)\nwhere ds = FFN(b) and dt is obtained by position encoding according to each frame’s index. Here oa/m denotes the object features mentioned above while FFN denotes a feed-forward network. Analogous to local features, position encoding information dt is added to global features as well. We then concatenate object features with global features to reflect the frame-level context in objects and obtain the visual representation va/m ∈ RK×d:\nva/m = FFN([v a/m local;v a/m global]) (2)\nLinguistic Representation. We apply the pretrained GloVe to convert each question word into a 300-dimensional vector, following previous work (Jang et al., 2017). To represent contextual information in a sentence, we feed the word representations into a bidirectional LSTM (bi-LSTM). Word-level features and last hidden units from the bi-LSTM are denoted by F q ∈ RL×d, and q ∈ Rd respectively. L denotes the number of words in a question."
    }, {
      "heading" : "3.2 Motion and Appearance Module",
      "text" : "In this section, we explain the modules generating high-level visual representations and integrate them with linguistic representations. Each module consists of (1) an Object Graph: spatio-temporal reasoning between object-level visual features, and (2) VQ interaction: calculating correlations between objects and words and obtaining cross-modal feature embeddings. Since the modules share the same architecture, we describe each module’s components only once with a shared superscript to avoid redundancy."
    }, {
      "heading" : "3.2.1 Object Graph Construction",
      "text" : "In this section, we define object graphs Ga/m = (Va/m, Ea/m) to capture spatio-temporal relations between objects. V , E denotes the node and edge set of the graph. As equation 2 provides visual features va/m, we define these as the graph input Xa/m ∈ RK×d. We denote the graph as Ga/m. The nodes of graph Ga/m are given by va/mi ∈ Xa/m, and edges are given by (va/mi , v a/m j ), representing a relationship between the two nodes. Given the constructed graph G, we perform graph convolution\n(Kipf and Welling, 2016) to obtain the relationaware object features. We obtain the similarity scores of nodes by calculating the dot-product after projecting input features to the interaction space and define the adjacency matrix Aa/m ∈ RK×K as follows:\nAa/m = softmax((Xa/mW1)(X a/mW2) >) (3)\nWe denote the two-layer graph convolution on input X with adjacency matrix A as:\nGCN(X;A) = ReLU(A ReLU(AXW3)W4)\nF = LayerNorm(X +GCN(X;A)) (4)\nWe omit superscripts in the graph convolution equation for simplicity. We add a skip connection for residual learning between self-information X and smoothed-information with neighbor objects."
    }, {
      "heading" : "3.2.2 Vision-question (VQ) Interaction",
      "text" : "We compute both appearance-question and motionquestion interaction to obtain correlations between language and each of the visual features. As we encode visual feature F a/m and question feature F q in Equation 4 and Section 3.1, we calculate every pair of relations between two modalities using the bilinear operation introduced in BAN (Kim et al., 2018a) as follows:\nHi = 1 · BANi(Hi−1, V ;Ai)> +Hi−1 (5)\nwhere H0 = F q, 1 ∈ RL, 1 ≤ i ≤ g and A denotes the attention map. F a/m is substituted for V respectively in our method. In the equation above, calculating the result BAN(H,V ;A) ∈ Rd and adding it to the H is repeated in g times. Afterwards, H represents the combined visual and language features in the question space incorporating diverse aspects from the two modalities (Yang et al., 2016)."
    }, {
      "heading" : "3.3 Motion-Appearance Fusion",
      "text" : "In this section, we introduce the MotionAppearance Fusion module which is our key contribution. Depending on what the question ultimately asks about, the model is supposed to decide which features are more relevant among appearance and motion information, or a combination of both. To do this, we produce appearance-centered, motion-centered, and all-mixed features and aggregate them depending on question context. Based on the previous step, we obtain cross-modal combined\nfeatures Ha and Hm in terms of appearance and motion. We concatenate these two matrices and define U as:\nU =\n[ Ha\nHm\n] , U ∈ R2L×d (6)\nMotion-Appearance-centered Attention. We first define regular scaled dot-product attention to attend features to diverse aspects:\nAttention(Q,K, V ) = softmax( QK>√ dk )V (7)\nwhere Q, K, V denotes the query, key, and value, respectively. To obtain motion-centered, appearance-centered and mixed attention, we substitute U with the query, and Ha, Hm, U with the key and value in the equation 7 as:\nP a = Attention(U, Ha, Ha)\nPm = Attention(U,Hm, Hm)\nP all = Attention(U, U, U)\nZa/m/all = LayerNorm(P a/m/all + U)\n(8)\nwhere P ∈ R2L×d and Z ∈ R2L×d. As in the first line of the equation 8, we add projected appearance features P a on each appearance and motion feature to obtain Za, since the matrix U is the concatenation of Ha and Hm. Therefore, we argue that Za contains appearance-centered information. Similarly, Zm/all contains motioncentered and all-mixed features, respectively. We argue that the Motion-Appearance-centered attention fuses appearance and motion features in various proportions and these three matrices work like multi-head attention sharing the task of capturing diverse information, and become synergistic when combined.\nQuestion-Guided Fusion. For question-guided fusion, we first define za/m/all as the sum of matrix Za/m/all ∈ R2L×d over sequence length 2L. We obtain attention scores between each za/m/all and question context vector q:\nαa/m/all = softmax( q(za/m/all)>√\ndz ) (9)\nwhere q denotes the last hidden vector. The attention score αa/m/all can be interpreted as the importance of each matrix Z based on question context. We obtain the question-guided fusion matrix O as:\nS = αaZa + αmZm + αallZall O = LayerNorm(S + FFN(S)) (10)\nwhere O ∈ R2L×d is obtained by linear transformation and a residual connection after weighted sum. We aggregate information by attention over the sequence length of O:\nβi = softmax(FFN(Oi))\nf = 2L∑ i=1 βiOi (11)\nThe final output vector f ∈ Rd is used for answer prediction."
    }, {
      "heading" : "3.4 Answer Prediction and Loss Function",
      "text" : "The video QA task can be divided into counting, open-ended word, and multiple-choice tasks (Jang et al., 2017). Our method trains the model and predicts the answer based on the three tasks similar to previous work.\nThe counting task is formulated as a linear regression of the final output vector f . We obtain the\nfinal answer by rounding the result and we minimize Mean Squared Error (MSE) loss.\nThe open-ended word task is essentially a classification task over the whole answer set. We calculate a classification score by applying a linear classifier and softmax function on the final output f and train the model by minimizing cross-entropy loss.\nFor the multiple-choice task, like in previous work (Jang et al., 2017), we attach an answer to the question and obtain M candidates. Then, we obtain the score for each of the M candidates by a linear transformation to the output vector f . We minimize the hinge loss within every pair of candidates, max(0, 1+ sn − sp), where sn and sp are scores from incorrect and correct answers respectively."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we evaluate our proposed model on three Video QA datasets: TGIF-QA, MSVD-QA, and MSRVTT-QA. We first introduce each dataset and compare our results with the state-of-the-art methods. Then, we report ablation studies and include visualizations to show how each module in MASN works."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "TGIF-QA (Jang et al., 2017) is a large-scale dataset that consists of 165K QA pairs collected from 72K animated GIFs. The length of video clips is very short, in general. TGIF-QA consists of four types of tasks: Count, Action, State transition (Trans.), and FrameQA. Count is an open-ended question to count how many times an action repeats. Action is a task to find action repeated at certain times, and Transition aims to identify a state transition over time. Both types are multiple-choice tasks. Lastly, FrameQA is an open-ended question that can be solved from just one frame, similar to image QA.\nMSVD-QA & MSRVTT-QA (Xu et al., 2017) are automatically generated from video descriptions. It consists of 1,970 video clips and 50K and 243K QA pairs, respectively. The average video lengths are 10 seconds and 15 seconds respectively. Questions belong to five types: what, who, how, when, and where. The task is open-ended with a pre-defined answer sets of size 1,000 and 4,000, respectively."
    }, {
      "heading" : "4.2 Implementation Details",
      "text" : "We first extract frames with 6 fps for all datasets. In the case of appearance features, we sample 1 frame out of 4 to avoid information redundancy. We apply Faster R-CNN (Ren et al., 2016) pretrained on Visual Genome (Krishna et al., 2017) to obtain local features. The number of extracted objects is N = 10. For global features, we use ResNet-152 pre-trained on ImageNet (Deng et al., 2009). In the the case of motion features, we apply I3D pre-trained on the Kinetics action recognition dataset (Kay et al., 2017). For the input of I3D, we concatenate a set of 8 frames around the sampled frame mentioned above. In terms of training details, we employ Adam optimizer with learning rate as 10−4. The number of BAN glimpse g is 4. We set the batch size as 32 for the Count and FrameQA tasks and 16 for Action and Trans. tasks."
    }, {
      "heading" : "4.3 Comparison with State-of-the-arts",
      "text" : "We compare MASN with state-of-the-art (SoTA) models on the aforementioned datasets.\nTGIF-QA. Compared with ST-VQA (Jang et al., 2017), Co-Mem (Gao et al., 2018), PSAC (Li et al., 2019), STA (Gao et al., 2019), HME (Fan et al., 2019), and recent SoTA models: HGA, L-GCN, QueST, HCRN (Jiang and Han, 2020; Huang et al., 2020; Jiang et al., 2020; Le et al., 2020), MASN shows the best results for three tasks: Count, Trans., and Action, outperforming the baseline methods by a large margin as shown in Table 1. In the case of FrameQA, the performance is similar to QueST. However, considering that there exists some tradeoff between the performance of Count and FrameQA since Count focuses on identifying temporal information and FrameQA focuses on spatial information, MASN shows the best overall performance on the entire task.\nMSVD-QA & MSRVTT-QA. As shown in Table 2, MASN outperforms the best baseline methods, QuesT and HCRN by approximately 2% on MSVD-QA, and shows competitive results on MSRVTT-QA. Since these datasets are composed of wh-questions, such as what or who, the question sets seemingly resemble FrameQA in TGIF-QA, as they tend to focus on spatial appearance features. This means that MASN is able to capture spatial details well based on the spatiotemporally mixed features."
    }, {
      "heading" : "4.4 Ablation Study",
      "text" : "Analyzing the impact of motion module and appearance module. We investigate the effect of each module as seen in Figure 1. In Table 3, the\n1st and 2nd row represent the result of using only the Appearance and Motion module, respectively. The 3rd row shows the result of just concatenating appearance and motion features from each module and flattening them, by substituting the input X for O in equation 11. Most existing SOTA models utilize only ResNet features for spatio-temporal reasoning based on the difference of vectors over time. Using only the Appearance module is similar to most of these existing methods, which can catch spatio-temporal relations relatively well. On the other hand, we found that the accuracy on FrameQA when only using the Motion module is about 7% lower than when using the Appearance module. This means the Motion module is limited in its ability to capture the appearance details. However, comparing the 1st and 3rd row in Table 3, the performance in the Action and Trans. tasks increase consistently when the Motion module is added compared to using only the Appearance module. This indicates that the Motion module is a meaningful addition. Lastly, compared to the 1st, 2nd and 3rd row, when integrating the output from both modules there is a further overall performance improvement. This indicates a synergistic effect occurs when integrating both the appearance and motion feature after obtaining them as high-level features.\nAnalyzing the impact of fusion module. We show ablation studies inside the fusion module represented in Table 3. The 4th row indicates the performance of our proposed MASN architecture. The results in the ‘Single-Attention Fusion’ row use only one type of attention among appearancecentered, motion-centered, and all-mixed as seen in equation 8. The results in the ‘Dual-Attention\nFusion’ row utilize two among the three types of attention mentioned above. Due to the nature of video, when a question such as “How many times does the man in the white shirt put his hand on the head?” is given, the model is supposed to find the motion information “put” while catching the appearance information “man in white shirt” or “hand on head”, and finally mixing them in different proportions depending on the context of question. Comparing the result of the 3rd (without fusion) row and MASN first, MASN shows better performance across tasks. This means mixing appearance and motion features in various proportions using the Motion-Appearance-centered Fusion module and computing the weighted fusion via the Question-Guided Fusion module contributes to the performance. When comparing the general performance with the number of attention types in fusion module, using single, dual, and triple attention (MASN) shows increasingly better performance in the same order. This indicates that focusing on different aspects and integrating each attended feature performs better than calculating attention at once. Additionally, comparing the result of using only appearance or motion-centered attention in ‘Single’ with both of them in ‘Dual’, we find that using both features shows better performance, which means they play complementary roles for each other. Similarly, we argue the reason for the performance increase in FrameQA in the ‘Motion’ row of ‘SingleAtt. Fusion’ is due to the fact that the model can find relevant appearance information better based\non motion information."
    }, {
      "heading" : "4.5 Qualitative Results",
      "text" : "We give examples of each attention score matrix from Motion-Appearance Fusion module in Figure 3. We draw two conclusions from the Figure: (1) each attention map catches different relations similarly to multi-head attention, (2) each attention map is used to a different extend depending on the type of task. For example, in FrameQA, the appearancecentered’s attention map captures which appearance trait to find focusing on ‘how many’. On the other hand, the motion-centered’s and all-mixed’s attention map attend on ‘waving’ or ‘hands’ to catch motion-related information. In Action, similar to FrameQA, the appearance-centered’s attention map attends on ‘head’ which is the object of action, while the motion-centered’s attention map catch ‘nod’ which is related to movement. However, in the case of the Count task, the two attention weights are not as sparse as scores in the other tasks. We think this dense attention map causes the inconsistency in the performance increase between Count task and Action and Trans. task, although questions for all of these three tasks ask for motion information."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we proposed a Motion-Appearance Synergistic Networks to fuse and create a synergy between motion and appearance features. Through the Motion and Appearance modules, MASN manages to find motion and appearance clues to solve the question, while modulating the amount of information used of each type through the Fusion module. Experimental results on three benchmark datasets show the effectiveness of our proposed MASN architecture compared to other models.\nAcknowledgement The authors would like to thank Ho-Joon Song, Yu-Jung Heo, Bjorn Bebensee, Seonil Son, Kyoung-Woon On, Seongho Choi and Woo-Suk Choi for helpful comments and editing. This work was partly supported by the Institute of Information & Communications Technology Planning & Evaluation (2015-0-00310-SW.StarLab/25%, 2017-0- 01772-VTT/25%, 2018-0-00622-RMI/25%, 2019- 0-01371-BabyMind/25%) grant funded by the Korean government."
    } ],
    "references" : [ {
      "title" : "Bottom-up and top-down attention for image captioning and visual question answering",
      "author" : [ "Peter Anderson", "Xiaodong He", "Chris Buehler", "Damien Teney", "Mark Johnson", "Stephen Gould", "Lei Zhang." ],
      "venue" : "Proceedings of the IEEE conference on computer vi-",
      "citeRegEx" : "Anderson et al\\.,? 2018",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2018
    }, {
      "title" : "Vqa: Visual question answering",
      "author" : [ "Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "Proceedings of the IEEE international conference on computer vision, pages 2425–2433.",
      "citeRegEx" : "Antol et al\\.,? 2015",
      "shortCiteRegEx" : "Antol et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyung Hyun Cho", "Yoshua Bengio." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "The problem of learning long-term dependencies in recurrent networks",
      "author" : [ "Yoshua Bengio", "Paolo Frasconi", "Patrice Simard." ],
      "venue" : "IEEE international conference on neural networks, pages 1183–1188. IEEE.",
      "citeRegEx" : "Bengio et al\\.,? 1993",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 1993
    }, {
      "title" : "Quo vadis, action recognition? a new model and the kinetics dataset",
      "author" : [ "Joao Carreira", "Andrew Zisserman." ],
      "venue" : "proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299–6308.",
      "citeRegEx" : "Carreira and Zisserman.,? 2017",
      "shortCiteRegEx" : "Carreira and Zisserman.",
      "year" : 2017
    }, {
      "title" : "Dramaqa: Character-centered video story understanding with hierarchical qa",
      "author" : [ "Seongho Choi", "Kyoung-Woon On", "Yu-Jung Heo", "Ahjeong Seo", "Youwon Jang", "Seungchan Lee", "Minsu Lee", "Byoung-Tak Zhang." ],
      "venue" : "arXiv preprint arXiv:2005.03356.",
      "citeRegEx" : "Choi et al\\.,? 2020",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2020
    }, {
      "title" : "Imagenet: A large-scale hierarchical image database",
      "author" : [ "Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei." ],
      "venue" : "2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee.",
      "citeRegEx" : "Deng et al\\.,? 2009",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2009
    }, {
      "title" : "Heterogeneous memory enhanced multimodal attention model for video question answering",
      "author" : [ "Chenyou Fan", "Xiaofan Zhang", "Shu Zhang", "Wensheng Wang", "Chi Zhang", "Heng Huang." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pat-",
      "citeRegEx" : "Fan et al\\.,? 2019",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2019
    }, {
      "title" : "Slowfast networks for video recognition",
      "author" : [ "Christoph Feichtenhofer", "Haoqi Fan", "Jitendra Malik", "Kaiming He." ],
      "venue" : "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6202–6211.",
      "citeRegEx" : "Feichtenhofer et al\\.,? 2019",
      "shortCiteRegEx" : "Feichtenhofer et al\\.",
      "year" : 2019
    }, {
      "title" : "Motion-appearance co-memory networks for video question answering",
      "author" : [ "Jiyang Gao", "Runzhou Ge", "Kan Chen", "Ram Nevatia." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6576–6585.",
      "citeRegEx" : "Gao et al\\.,? 2018",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2018
    }, {
      "title" : "A better baseline for ava",
      "author" : [ "Rohit Girdhar", "Joao Carreira", "Carl Doersch", "Andrew Zisserman." ],
      "venue" : "arXiv preprint arXiv:1807.10066.",
      "citeRegEx" : "Girdhar et al\\.,? 2018",
      "shortCiteRegEx" : "Girdhar et al\\.",
      "year" : 2018
    }, {
      "title" : "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "author" : [ "Yash Goyal", "Tejas Khot", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision",
      "citeRegEx" : "Goyal et al\\.,? 2017",
      "shortCiteRegEx" : "Goyal et al\\.",
      "year" : 2017
    }, {
      "title" : "Mask r-cnn",
      "author" : [ "Kaiming He", "Georgia Gkioxari", "Piotr Dollár", "Ross Girshick." ],
      "venue" : "Proceedings of the IEEE international conference on computer vision, pages 2961–2969.",
      "citeRegEx" : "He et al\\.,? 2017",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Locationaware graph convolutional networks for video question answering",
      "author" : [ "Deng Huang", "Peihao Chen", "Runhao Zeng", "Qing Du", "Mingkui Tan", "Chuang Gan." ],
      "venue" : "AAAI, pages 11021–11028.",
      "citeRegEx" : "Huang et al\\.,? 2020",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2020
    }, {
      "title" : "Tgif-qa: Toward spatiotemporal reasoning in visual question answering",
      "author" : [ "Yunseok Jang", "Yale Song", "Youngjae Yu", "Youngjin Kim", "Gunhee Kim." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2758–2766.",
      "citeRegEx" : "Jang et al\\.,? 2017",
      "shortCiteRegEx" : "Jang et al\\.",
      "year" : 2017
    }, {
      "title" : "Divide and conquer: Question-guided spatio-temporal contextual attention for video question answering",
      "author" : [ "Jianwen Jiang", "Ziqiang Chen", "Haojie Lin", "Xibin Zhao", "Yue Gao." ],
      "venue" : "AAAI, pages 11101–11108.",
      "citeRegEx" : "Jiang et al\\.,? 2020",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Reasoning with heterogeneous graph alignment for video question answering",
      "author" : [ "Pin Jiang", "Yahong Han." ],
      "venue" : "AAAI, pages 11109–11116.",
      "citeRegEx" : "Jiang and Han.,? 2020",
      "shortCiteRegEx" : "Jiang and Han.",
      "year" : 2020
    }, {
      "title" : "Dual attention networks for visual reference resolution in visual dialog",
      "author" : [ "Gi-Cheon Kang", "Jaeseo Lim", "Byoung-Tak Zhang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, pages 2024–2033.",
      "citeRegEx" : "Kang et al\\.,? 2019",
      "shortCiteRegEx" : "Kang et al\\.",
      "year" : 2019
    }, {
      "title" : "The kinetics human action video",
      "author" : [ "Will Kay", "Joao Carreira", "Karen Simonyan", "Brian Zhang", "Chloe Hillier", "Sudheendra Vijayanarasimhan", "Fabio Viola", "Tim Green", "Trevor Back", "Paul Natsev" ],
      "venue" : null,
      "citeRegEx" : "Kay et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kay et al\\.",
      "year" : 2017
    }, {
      "title" : "Bilinear attention networks",
      "author" : [ "Jin-Hwa Kim", "Jaehyun Jun", "Byoung-Tak Zhang." ],
      "venue" : "arXiv preprint arXiv:1805.07932.",
      "citeRegEx" : "Kim et al\\.,? 2018a",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2018
    }, {
      "title" : "Multimodal dual attention memory for video story question answering",
      "author" : [ "Kyung-Min Kim", "Seong-Ho Choi", "Jin-Hwa Kim", "Byoung-Tak Zhang." ],
      "venue" : "Proceedings of the European Conference on Computer Vision (ECCV), pages 673–688.",
      "citeRegEx" : "Kim et al\\.,? 2018b",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2018
    }, {
      "title" : "Semisupervised classification with graph convolutional networks",
      "author" : [ "Thomas N Kipf", "Max Welling." ],
      "venue" : "arXiv preprint arXiv:1609.02907.",
      "citeRegEx" : "Kipf and Welling.,? 2016",
      "shortCiteRegEx" : "Kipf and Welling.",
      "year" : 2016
    }, {
      "title" : "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
      "author" : [ "Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A Shamma" ],
      "venue" : null,
      "citeRegEx" : "Krishna et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Krishna et al\\.",
      "year" : 2017
    }, {
      "title" : "Hierarchical conditional relation networks for video question answering",
      "author" : [ "Thao Minh Le", "Vuong Le", "Svetha Venkatesh", "Truyen Tran." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9972–9981.",
      "citeRegEx" : "Le et al\\.,? 2020",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2020
    }, {
      "title" : "Tvqa: Localized, compositional video question answering",
      "author" : [ "Jie Lei", "Licheng Yu", "Mohit Bansal", "Tamara L Berg." ],
      "venue" : "arXiv preprint arXiv:1809.01696.",
      "citeRegEx" : "Lei et al\\.,? 2018",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2018
    }, {
      "title" : "Beyond rnns: Positional self-attention with co-attention for video question answering",
      "author" : [ "Xiangpeng Li", "Jingkuan Song", "Lianli Gao", "Xianglong Liu", "Wenbing Huang", "Xiangnan He", "Chuang Gan." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial In-",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "A structured self-attentive sentence embedding",
      "author" : [ "Zhouhan Lin", "Minwei Feng", "Cicero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1703.03130.",
      "citeRegEx" : "Lin et al\\.,? 2017",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2017
    }, {
      "title" : "Ava: A large-scale database for aesthetic visual analysis",
      "author" : [ "Naila Murray", "Luca Marchesotti", "Florent Perronnin." ],
      "venue" : "2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 2408– 2415. IEEE.",
      "citeRegEx" : "Murray et al\\.,? 2012",
      "shortCiteRegEx" : "Murray et al\\.",
      "year" : 2012
    }, {
      "title" : "Cut-based graph learning networks to discover compositional structure of sequential video data",
      "author" : [ "Kyoung-Woon On", "Eun-Sol Kim", "Yu-Jung Heo", "Byoung-Tak Zhang." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34,",
      "citeRegEx" : "On et al\\.,? 2020",
      "shortCiteRegEx" : "On et al\\.",
      "year" : 2020
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Faster r-cnn: towards real-time object detection with region proposal networks",
      "author" : [ "Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun." ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence, 39(6):1137–1149.",
      "citeRegEx" : "Ren et al\\.,? 2016",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning spatiotemporal features with 3d convolutional networks",
      "author" : [ "Du Tran", "Lubomir Bourdev", "Rob Fergus", "Lorenzo Torresani", "Manohar Paluri." ],
      "venue" : "Proceedings of the IEEE international conference on computer vision, pages 4489–4497.",
      "citeRegEx" : "Tran et al\\.,? 2015",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2015
    }, {
      "title" : "Multimodal transformer for unaligned multimodal language sequences",
      "author" : [ "Yao-Hung Hubert Tsai", "Shaojie Bai", "Paul Pu Liang", "J Zico Kolter", "Louis-Philippe Morency", "Ruslan Salakhutdinov." ],
      "venue" : "Proceedings of the conference. Association for Com-",
      "citeRegEx" : "Tsai et al\\.,? 2019",
      "shortCiteRegEx" : "Tsai et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "arXiv preprint arXiv:1706.03762.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Analyzing multihead self-attention: Specialized heads do the heavy lifting, the rest can be pruned",
      "author" : [ "Elena Voita", "David Talbot", "Fedor Moiseev", "Rico Sennrich", "Ivan Titov." ],
      "venue" : "arXiv preprint arXiv:1905.09418.",
      "citeRegEx" : "Voita et al\\.,? 2019",
      "shortCiteRegEx" : "Voita et al\\.",
      "year" : 2019
    }, {
      "title" : "Video question answering via gradually refined attention over appearance and motion",
      "author" : [ "Dejing Xu", "Zhou Zhao", "Jun Xiao", "Fei Wu", "Hanwang Zhang", "Xiangnan He", "Yueting Zhuang." ],
      "venue" : "Proceedings of the 25th ACM international conference on Multi-",
      "citeRegEx" : "Xu et al\\.,? 2017",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2017
    }, {
      "title" : "Msrvtt: A large video description dataset for bridging video and language",
      "author" : [ "Jun Xu", "Tao Mei", "Ting Yao", "Yong Rui." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288–5296.",
      "citeRegEx" : "Xu et al\\.,? 2016",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    }, {
      "title" : "Show, attend and tell: neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Lei Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 32nd In-",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Stacked attention networks for image question answering",
      "author" : [ "Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 21–29.",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Activitynet-qa: A dataset for understanding complex web videos via question answering",
      "author" : [ "Zhou Yu", "Dejing Xu", "Jun Yu", "Ting Yu", "Zhou Zhao", "Yueting Zhuang", "Dacheng Tao." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Yu et al\\.,? 2019",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2019
    }, {
      "title" : "From recognition to cognition: Visual commonsense reasoning",
      "author" : [ "Rowan Zellers", "Yonatan Bisk", "Ali Farhadi", "Yejin Choi." ],
      "venue" : "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6720–6731.",
      "citeRegEx" : "Zellers et al\\.,? 2019",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 38,
      "context" : "Thanks to this, visionlanguage tasks such as image captioning (Xu et al., 2015), visual question answering (VQA) (Antol et al.",
      "startOffset" : 62,
      "endOffset" : 79
    }, {
      "referenceID" : 1,
      "context" : ", 2015), visual question answering (VQA) (Antol et al., 2015; Goyal et al., 2017), and visual commonsense reasoning (VCR) (Zellers et al.",
      "startOffset" : 41,
      "endOffset" : 81
    }, {
      "referenceID" : 11,
      "context" : ", 2015), visual question answering (VQA) (Antol et al., 2015; Goyal et al., 2017), and visual commonsense reasoning (VCR) (Zellers et al.",
      "startOffset" : 41,
      "endOffset" : 81
    }, {
      "referenceID" : 41,
      "context" : ", 2017), and visual commonsense reasoning (VCR) (Zellers et al., 2019) have been introduced to the research community,",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 37,
      "context" : "In particular, video question answering (video QA) tasks (Xu et al., 2016; Jang et al., 2017; Lei et al., 2018; Yu et al., 2019; Choi et al., 2020) have been proposed with the goal of reasoning over higher-level vision-",
      "startOffset" : 57,
      "endOffset" : 147
    }, {
      "referenceID" : 15,
      "context" : "In particular, video question answering (video QA) tasks (Xu et al., 2016; Jang et al., 2017; Lei et al., 2018; Yu et al., 2019; Choi et al., 2020) have been proposed with the goal of reasoning over higher-level vision-",
      "startOffset" : 57,
      "endOffset" : 147
    }, {
      "referenceID" : 25,
      "context" : "In particular, video question answering (video QA) tasks (Xu et al., 2016; Jang et al., 2017; Lei et al., 2018; Yu et al., 2019; Choi et al., 2020) have been proposed with the goal of reasoning over higher-level vision-",
      "startOffset" : 57,
      "endOffset" : 147
    }, {
      "referenceID" : 40,
      "context" : "In particular, video question answering (video QA) tasks (Xu et al., 2016; Jang et al., 2017; Lei et al., 2018; Yu et al., 2019; Choi et al., 2020) have been proposed with the goal of reasoning over higher-level vision-",
      "startOffset" : 57,
      "endOffset" : 147
    }, {
      "referenceID" : 5,
      "context" : "In particular, video question answering (video QA) tasks (Xu et al., 2016; Jang et al., 2017; Lei et al., 2018; Yu et al., 2019; Choi et al., 2020) have been proposed with the goal of reasoning over higher-level vision-",
      "startOffset" : 57,
      "endOffset" : 147
    }, {
      "referenceID" : 26,
      "context" : "To tackle these challenges, previous studies (Li et al., 2019; Jiang et al., 2020; Huang et al., 2020) have mainly explored this task by jointly embedding the features from the pre-trained",
      "startOffset" : 45,
      "endOffset" : 102
    }, {
      "referenceID" : 16,
      "context" : "To tackle these challenges, previous studies (Li et al., 2019; Jiang et al., 2020; Huang et al., 2020) have mainly explored this task by jointly embedding the features from the pre-trained",
      "startOffset" : 45,
      "endOffset" : 102
    }, {
      "referenceID" : 14,
      "context" : "To tackle these challenges, previous studies (Li et al., 2019; Jiang et al., 2020; Huang et al., 2020) have mainly explored this task by jointly embedding the features from the pre-trained",
      "startOffset" : 45,
      "endOffset" : 102
    }, {
      "referenceID" : 30,
      "context" : "word embedding model (Pennington et al., 2014) and the object detection models (He et al.",
      "startOffset" : 21,
      "endOffset" : 46
    }, {
      "referenceID" : 13,
      "context" : ", 2014) and the object detection models (He et al., 2016; Ren et al., 2016).",
      "startOffset" : 40,
      "endOffset" : 75
    }, {
      "referenceID" : 31,
      "context" : ", 2014) and the object detection models (He et al., 2016; Ren et al., 2016).",
      "startOffset" : 40,
      "endOffset" : 75
    }, {
      "referenceID" : 9,
      "context" : "However, as discussed in (Gao et al., 2018), the use of the visual features extracted from the object detection models suffers from motion analysis since the object detection model lacks",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 36,
      "context" : "To enforce the motion analysis, a few approaches (Xu et al., 2017; Gao et al., 2018) have employed additional visual features (Tran et al.",
      "startOffset" : 49,
      "endOffset" : 84
    }, {
      "referenceID" : 9,
      "context" : "To enforce the motion analysis, a few approaches (Xu et al., 2017; Gao et al., 2018) have employed additional visual features (Tran et al.",
      "startOffset" : 49,
      "endOffset" : 84
    }, {
      "referenceID" : 32,
      "context" : ", 2018) have employed additional visual features (Tran et al., 2015) (i.",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 3,
      "context" : "Due to the problem of long-term dependency in recurrent models (Bengio et al., 1993), their proposed methods may fail to learn dependencies between distant features.",
      "startOffset" : 63,
      "endOffset" : 84
    }, {
      "referenceID" : 39,
      "context" : "In image QA, earlier works approach the task by applying attention between the question and the spatial dimensions of the image (Yang et al., 2016; Anderson et al., 2018; Kim et al., 2018a; Kang et al., 2019).",
      "startOffset" : 128,
      "endOffset" : 208
    }, {
      "referenceID" : 0,
      "context" : "In image QA, earlier works approach the task by applying attention between the question and the spatial dimensions of the image (Yang et al., 2016; Anderson et al., 2018; Kim et al., 2018a; Kang et al., 2019).",
      "startOffset" : 128,
      "endOffset" : 208
    }, {
      "referenceID" : 20,
      "context" : "In image QA, earlier works approach the task by applying attention between the question and the spatial dimensions of the image (Yang et al., 2016; Anderson et al., 2018; Kim et al., 2018a; Kang et al., 2019).",
      "startOffset" : 128,
      "endOffset" : 208
    }, {
      "referenceID" : 18,
      "context" : "In image QA, earlier works approach the task by applying attention between the question and the spatial dimensions of the image (Yang et al., 2016; Anderson et al., 2018; Kim et al., 2018a; Kang et al., 2019).",
      "startOffset" : 128,
      "endOffset" : 208
    }, {
      "referenceID" : 15,
      "context" : "movement of objects or causality in the temporal dimension should also be considered along with the details from the spatial dimension (Jang et al., 2017; On et al., 2020).",
      "startOffset" : 135,
      "endOffset" : 171
    }, {
      "referenceID" : 29,
      "context" : "movement of objects or causality in the temporal dimension should also be considered along with the details from the spatial dimension (Jang et al., 2017; On et al., 2020).",
      "startOffset" : 135,
      "endOffset" : 171
    }, {
      "referenceID" : 36,
      "context" : "There have been some attempts (Xu et al., 2017; Gao et al., 2018; Fan et al., 2019) to extract motion and appearance features and integrate them on a spatio-temporal dimension via memory networks.",
      "startOffset" : 30,
      "endOffset" : 83
    }, {
      "referenceID" : 9,
      "context" : "There have been some attempts (Xu et al., 2017; Gao et al., 2018; Fan et al., 2019) to extract motion and appearance features and integrate them on a spatio-temporal dimension via memory networks.",
      "startOffset" : 30,
      "endOffset" : 83
    }, {
      "referenceID" : 7,
      "context" : "There have been some attempts (Xu et al., 2017; Gao et al., 2018; Fan et al., 2019) to extract motion and appearance features and integrate them on a spatio-temporal dimension via memory networks.",
      "startOffset" : 30,
      "endOffset" : 83
    }, {
      "referenceID" : 28,
      "context" : "(2017) proposed I3D, which integrates 3D convolutions into a state-of-the-art 2D CNN architecture, which now acts as a baseline in action classification tasks (Murray et al., 2012; Girdhar et al., 2018).",
      "startOffset" : 159,
      "endOffset" : 202
    }, {
      "referenceID" : 10,
      "context" : "(2017) proposed I3D, which integrates 3D convolutions into a state-of-the-art 2D CNN architecture, which now acts as a baseline in action classification tasks (Murray et al., 2012; Girdhar et al., 2018).",
      "startOffset" : 159,
      "endOffset" : 202
    }, {
      "referenceID" : 2,
      "context" : "Attention Mechanism explicitly calculates the correlation between two features (Bahdanau et al., 2015; Lin et al., 2017), and has been widely used in a variety of fields.",
      "startOffset" : 79,
      "endOffset" : 120
    }, {
      "referenceID" : 27,
      "context" : "Attention Mechanism explicitly calculates the correlation between two features (Bahdanau et al., 2015; Lin et al., 2017), and has been widely used in a variety of fields.",
      "startOffset" : 79,
      "endOffset" : 120
    }, {
      "referenceID" : 35,
      "context" : "(2017), utilizes multi-head selfattention that captures diverse aspects in the input features (Voita et al., 2019).",
      "startOffset" : 94,
      "endOffset" : 114
    }, {
      "referenceID" : 13,
      "context" : "tation, we use ResNet (He et al., 2016) pre-trained on an object and its attribute classification task as a feature extractor.",
      "startOffset" : 22,
      "endOffset" : 39
    }, {
      "referenceID" : 4,
      "context" : "For the motion representation, we use I3D (Carreira and Zisserman, 2017) pretrained on the action classification task.",
      "startOffset" : 42,
      "endOffset" : 72
    }, {
      "referenceID" : 31,
      "context" : "For local features, given a video containing T frames, we obtain N objects from each frame using Faster R-CNN (Ren et al., 2016) that applies RoIAlign to ex-",
      "startOffset" : 110,
      "endOffset" : 128
    }, {
      "referenceID" : 16,
      "context" : "Following previous works, we extract the feature map from ResNet-152’s Conv5 layer and apply a linear projection (Jiang et al., 2020; Huang et al., 2020).",
      "startOffset" : 113,
      "endOffset" : 153
    }, {
      "referenceID" : 14,
      "context" : "Following previous works, we extract the feature map from ResNet-152’s Conv5 layer and apply a linear projection (Jiang et al., 2020; Huang et al., 2020).",
      "startOffset" : 113,
      "endOffset" : 153
    }, {
      "referenceID" : 4,
      "context" : "We obtain a feature map from the last convolutional layer in I3D (Carreira and Zisserman, 2017) whose dimension is (time, width, height, feature) = ( ⌊ t 8 ⌋ , 7, 7, 2048).",
      "startOffset" : 65,
      "endOffset" : 95
    }, {
      "referenceID" : 12,
      "context" : "For local features, we apply RoIAlign (He et al., 2017) on the feature map using object bounding box location b.",
      "startOffset" : 38,
      "endOffset" : 55
    }, {
      "referenceID" : 14,
      "context" : "Following L-GCN (Huang et al., 2020), we add a location encoding and define local features as:",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 22,
      "context" : "Given the constructed graph G, we perform graph convolution (Kipf and Welling, 2016) to obtain the relation-",
      "startOffset" : 60,
      "endOffset" : 84
    }, {
      "referenceID" : 39,
      "context" : "guage features in the question space incorporating diverse aspects from the two modalities (Yang et al., 2016).",
      "startOffset" : 91,
      "endOffset" : 110
    }, {
      "referenceID" : 15,
      "context" : "open-ended word, and multiple-choice tasks (Jang et al., 2017).",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 15,
      "context" : "For the multiple-choice task, like in previous work (Jang et al., 2017), we attach an answer to",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 15,
      "context" : "TGIF-QA (Jang et al., 2017) is a large-scale dataset that consists of 165K QA pairs collected from 72K animated GIFs.",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 36,
      "context" : "MSVD-QA & MSRVTT-QA (Xu et al., 2017) are automatically generated from video descriptions.",
      "startOffset" : 20,
      "endOffset" : 37
    }, {
      "referenceID" : 31,
      "context" : "We apply Faster R-CNN (Ren et al., 2016) pretrained on Visual Genome (Krishna et al.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 23,
      "context" : ", 2016) pretrained on Visual Genome (Krishna et al., 2017)",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 6,
      "context" : "For global features, we use ResNet-152 pre-trained on ImageNet (Deng et al., 2009).",
      "startOffset" : 63,
      "endOffset" : 82
    }, {
      "referenceID" : 19,
      "context" : "In the the case of motion features, we apply I3D pre-trained on the Kinetics action recognition dataset (Kay et al., 2017).",
      "startOffset" : 104,
      "endOffset" : 122
    }, {
      "referenceID" : 7,
      "context" : ", 2019), HME (Fan et al., 2019), and recent SoTA models: HGA, L-GCN, QueST, HCRN (Jiang and Han, 2020; Huang et al.",
      "startOffset" : 13,
      "endOffset" : 31
    }, {
      "referenceID" : 17,
      "context" : ", 2019), and recent SoTA models: HGA, L-GCN, QueST, HCRN (Jiang and Han, 2020; Huang et al., 2020; Jiang et al., 2020; Le et al., 2020), MASN",
      "startOffset" : 57,
      "endOffset" : 135
    }, {
      "referenceID" : 14,
      "context" : ", 2019), and recent SoTA models: HGA, L-GCN, QueST, HCRN (Jiang and Han, 2020; Huang et al., 2020; Jiang et al., 2020; Le et al., 2020), MASN",
      "startOffset" : 57,
      "endOffset" : 135
    }, {
      "referenceID" : 16,
      "context" : ", 2019), and recent SoTA models: HGA, L-GCN, QueST, HCRN (Jiang and Han, 2020; Huang et al., 2020; Jiang et al., 2020; Le et al., 2020), MASN",
      "startOffset" : 57,
      "endOffset" : 135
    }, {
      "referenceID" : 24,
      "context" : ", 2019), and recent SoTA models: HGA, L-GCN, QueST, HCRN (Jiang and Han, 2020; Huang et al., 2020; Jiang et al., 2020; Le et al., 2020), MASN",
      "startOffset" : 57,
      "endOffset" : 135
    } ],
    "year" : 2021,
    "abstractText" : "Video Question Answering is a task which requires an AI agent to answer questions grounded in video. This task entails three key challenges: (1) understand the intention of various questions, (2) capturing various elements of the input video (e.g., object, action, causality), and (3) cross-modal grounding between language and vision information. We propose Motion-Appearance Synergistic Networks (MASN), which embed two crossmodal features grounded on motion and appearance information and selectively utilize them depending on the question’s intentions. MASN consists of a motion module, an appearance module, and a motion-appearance fusion module. The motion module computes the action-oriented cross-modal joint representations, while the appearance module focuses on the appearance aspect of the input video. Finally, the motion-appearance fusion module takes each output of the motion module and the appearance module as input, and performs question-guided fusion. As a result, MASN achieves new state-of-the-art performance on the TGIF-QA and MSVD-QA datasets. We also conduct qualitative analysis by visualizing the inference results of MASN. The code is available at https://github.com/ ahjeongseo/MASN-pytorch.",
    "creator" : "LaTeX with hyperref"
  }
}