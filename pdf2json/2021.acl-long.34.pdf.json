{
  "name" : "2021.acl-long.34.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Training-free and Reference-free Summarization Evaluation Metric via Centrality-weighted Relevance and Self-referenced Redundancy",
    "authors" : [ "Wang Chen", "Piji Li", "Irwin King" ],
    "emails" : [ "king}@cse.cuhk.edu.hk", "2pijili@tencent.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 404–414\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n404"
    }, {
      "heading" : "1 Introduction",
      "text" : "Text summarization systems have been developed rapidly due to the appearance of sequence-tosequence frameworks (Sutskever et al., 2014; Bahdanau et al., 2015; See et al., 2017; Chan et al., 2020), transformer architectures (Vaswani et al., 2017) and large-scale pre-training models (Devlin et al., 2019; Liu et al., 2019). How to accurately\n∗This work was mainly done when Wang Chen was an intern at Tencent AI Lab.\nevaluate the summaries generated from these systems also attracts more and more attention in this research area. One of the most accurate evaluation methods is human evaluation. However, human evaluation is expensive, time-consuming, and nonreproducible. Thus, it is necessary to develop automatic evaluation metrics for text summarization systems. Existing automatic summarization evaluation metrics can be roughly categorized into two groups: reference-based metrics and reference-free metrics. In this work, we focus on reference-free metrics.\nReference-free summarization evaluation metrics have been developed in parallel in multidocument summarization and single-document summarization. The SOTA reference-free method for multi-document summarization evaluation, SUPERT (Gao et al., 2020), predicts a relevance score for each (document, summary) pair to estimate the informativeness of the summary and then averages all the scores from multiple documents as the final evaluation score. For each pair, SUPERT employs the top-ranked sentences which are ranked by the position or centrality as a pseudo reference of the document and then applies BERTScore (Zhang et al., 2020) to produce a relevance score between the pseudo reference and the given summary. The SOTA single-document summarization referencefree evaluation metric, LS Score (Wu et al., 2020), combines a learned linguistic scorer for the summary and a cosine similarity scorer for the (document, summary) pair to produce the final score.\nAlthough SUPERT and LS Score achieve the SOTA performance on their own areas respectively, they still have several drawbacks. For example, SUPERT only considers the relevance score between the document and the summary while ignoring the other aspects such as how much redundant information is contained in the summary. Besides, SUPERT assumes that all pseudo reference sen-\ntences are equally-important. However, in the real world, the key information of a document is unevenly distributed over sentences. Therefore, such an assumption may introduce extra noise for the evaluation. Note that although SUPERT may employ sentence centrality to select document sentences as a pseudo reference, they ignore the sentence centrality after the selection and still treat the selected sentences equally-important. As for LS Score, although it does not require a reference during the evaluation of a summary, it requires a large-scale training dataset with reference summaries to train the linguistic scorer. Besides the intrinsic drawbacks in these SOTA methods, to our best knowledge, there is no reference-free evaluation metric showing that it can achieve the SOTA performance on both multi-document and singledocument summarization.\nTo solve the above limitations, based on SUPERT, we propose a novel training-free and reference-free metric for both multiple and single document summarization evaluation. Our metric is composed of a centrality-weighted relevance score and a self-referenced redundancy score.\nFor the relevance score which is employed to estimate the informativeness of the summary, we incorporate the following new features. First, unlike previous work which only utilizes the tokenlevel representations, motivated by Clark et al. (2019), we engage a hybrid way that contains both token-level representations and sentence-level representations to encode the document and the summary. The purpose of the hybrid representation is to enable our method to consider richer mapping styles (i.e., token-to-token, sentence-to-token, and sentence-to-sentence) and help to produce a more comprehensive evaluation score. Second, we utilize the sentence centrality computed from sentence-level representations of the source document to produce the importance weights of the pseudo reference sentences and tokens. Based on the weights, we compute a weighted relevance score that is more precise by considering the relative importance. Third, besides the F1 version of our relevance score, we also propose an adaptive Fβ version where recall is considered β times as important as precision. β is computed based on the length ratio between the pseudo reference and the given summary. The motivation is to punish the short summary that can easily get high precision while covering very limited important information\nin the pseudo reference (i.e., low recall). To measure the redundancy of a summary, we design a simple but effective self-referenced similarity score. If a summary contains much redundant information, there must exist plenty of semantically similar tokens or sentences. Based on this assumption, we use the summary itself as the reference and input a (summary, summary) pair into a selfmasked BERTScore to produce a redundancy score that evaluates the averaged degree of semantic similarity of each token or sentence with other tokens or sentences.\nAfter obtaining the centrality-weighted relevance score and the self-referenced redundancy score, we combine them to predict the final evaluation score. Depending on either F1 or Fβ is applied in our relevance score, we propose two variants of our method: the F1-based version and the Fβ-based version. Extensive experiments are conducted on both multi-document and single-document summarization datasets. The results show that our F1based method already outperforms all the SOTA baselines on all datasets. Moreover, our Fβ-based method can further improve the performance on multi-document summarization datasets.\nOur contributions are summarized as follows: (1) A novel training-free and reference-free summarization evaluation metric which considers both relevance and redundancy; (2) A centrality-weighted relevance score that effectively utilizes the sentence centrality of the documents to provide importance guidance for the pseudo reference tokens and sentences. Besides the F1 version, we also develop an Fβ based relevance score which pays more attention to recall; (3) A self-referenced redundancy score that utilizes a self-masked BERTScore to detect the duplicated information of the given summary; (4) To the best of our knowledge, we are the first evaluation metric that can achieve SOTA performance on both multiple and single document summarization under the reference-free setting."
    }, {
      "heading" : "2 Preliminary",
      "text" : "Notations. We denote vectors as bold lowercase characters and matrices as bold uppercase characters. The characters that are not bold are used to denote scalars. Calligraphy uppercase characters are utilized to represent sets.\nProblem Definition. We formally define the reference-free summarization evaluation problem as follows. Give a set of documents D =\n{d1, d2, ..., dK} and a generated summary x, the goal is to predict a score to represent the overall quality of the summary. K = 1 and K > 1 indicate single-document and multi-document summarization respectively."
    }, {
      "heading" : "3 Our Methodology",
      "text" : "The overall framework is illustrated in Figure 1. Our final evaluation score of a summary consists of an averaged centrality-weighted relevance score and a self-referenced redundancy score. Both scores are calculated on a semantic-level instead of utilizing n-gram overlapping. The averaged relevance score is computed from the relevance score between the summary and each document in the document set. The redundancy score is calculated based on the summary itself."
    }, {
      "heading" : "3.1 Centrality-weighted Relevance Score",
      "text" : "Our relevance score aims to estimate the informativeness of the given summary. We first encode each document in the document set and the summary into hidden representations. Then, for each document, we select essential sentences by centrality to build a pseudo reference. Next, we compute a centrality-weighted relevance score between the summary and each pseudo reference. Finally, we average all the relevance scores as the final relevance score of the summary. We use the k-th document dk and a summary x as an example to show the workflow.\nEncoding. Following SUPERT (Gao et al., 2020), we first split the document dk and the summary x into sentences. Then, the pre-trained SBERT1 is employed to encode the tokens of each sentence into token-level contextual hidden representations. We also apply max-pooling on all the tokens of a sentence to obtain the sentence-level hidden representation. Following previous work, when utilizing the token-level representations to compute the relevance and redundancy scores, we will filter out the non-informative tokens such as stop-words to improve the efficiency.\nBuilding Pseudo Reference. We do not choose all the document sentences of dk to evaluate the relevance of the summary. Because the whole document usually contains plenty of unimportant sentences which may introduce extra noise for the relevance evaluation. Thus, we select important document sentences to build a pseudo reference r for the evaluation. The sentence selection is based on the centrality of each sentence, which is computed by the unsupervised algorithm, PacSum (Zheng and Lapata, 2019), using the sentence-level representation. After obtaining the centrality scores of all sentences of the document, we choose the top-M2 sentences as the pseudo reference. Besides, we normalize the centrality scores to [0, 1] and denote the normalized centrality scores of the selected sen-\n1bert-large-nli-stsb-mean-tokens 2In experiments, we follow the default configuration of\nSUPERT and set M as 12 for all the datasets.\ntences as ās = [ās1, ā s 2, ..., ā s M ] where ā s i ∈ [0, 1] and the superscript s means sentence-level. We denote the pseudo reference building process as PacSumTopM. Computing Relevance Score with One Pseudo Reference. Instead of only using token-level representations, we also leverage the sentence-level representations to provide multi-level information. The hybrid representations of the summary x and the pseudo reference r are denoted as follows:\nX = [wx1 , ...,w x n, s x 1 , ..., s x N ], (1)\nRk = [w r 1, ...,w r m, s r 1, ..., s r M ], (2)\nwhere n and N (m and M ) are the token number and sentence number of the summary (pseudo reference). w and s represent the token and sentence hidden representations respectively.\nBesides the hybrid representations, we also introduce a centrality weighting scheme to weight the tokens and sentences of the pseudo reference, which is different from previous work that either treats them equally or uses the surface statistics like IDF as the weights. Based on the centrality scores of the selected pseudo reference sentences i.e., ās = [ās1, ā s 2, ..., ā s M ], we assign the weights of the pseudo reference tokens as follows:\nāw = [āw1 , ā w 2 , ..., ā w m], (3) āwj = ā s i:wj∈si , (4)\nwhere āi:wj∈si indicates the token wj inherits the centrality score from its sentence si. Since we have already removed the non-informative tokens in the token-level representations of each sentence, the remaining tokens capture the key information of the sentence and consequently it is reasonable to perform such a weight inheritance. Next, we combine token weights āw and sentence weights ās to get the final normalized centrality-based weights of the hybrid representations:\na = [aw1 , ..., a w m, a s 1, ..., a s M ], (5)\nawj = ā w j /sum([ā w; ās]), (6)\nasi = ā s i/sum([ā w; ās]), (7)\nwhere “[·; ·]” represents concatenation. Based on the hybrid representations (i.e., X and Rk) and the centrality-based weights of the pseudo reference tokens and sentences (i.e., a), we compute the relevance score between the summary and the pseudo reference by a weighted\nBERTScore (Zhang et al., 2020). For brevity, we denote the j-th element of X as xj , the i-th element of Rk as ri, and the i-th element of a as ai:\nRecall = ∑ i ai maxj Sim(ri,xj)∑\ni ai , (8)\nPrecision =\n∑ j maxi Sim(ri,xj)\n|X| , (9)\nF1 = 2 ∗Recall ∗ Precision Recall + Precision , (10)\nwhere “Sim” denotes the cosine similarity and |X| equals to n + N . Recall, Precision, and F1 are in the range of [-1, 1].\nBesides the F1 version, we also propose an adaptive Fβ version of relevance score as follows:\nFβ = (1 + β2) ∗Recall ∗ Precision Recall + β2 ∗ Precision , (11)\nβ2 =  1, if ( |Rk||X| ) 1/γ ≤ 1 2, if ( |Rk||X| )\n1/γ ≥ 2 ( |Rk||X| ) 1/γ , otherwise , (12)\nwhere |Rk| = m+M , |X| = n+N , and γ is a positive integer hyper-parameter. In our experiments, γ is set as 2 after fine-tuning on the validation dataset and is fixed for all the testing datasets. The physical meaning of β is that the Recall score is considered β times as important as the Precision score. In summarization evaluation, the coverage of the key information is always the most important quality indicator of the summary. Thus, we set the lower bound of β as 1. On the other hand, the metric should not only evaluate the key information coverage, containing less unimportant content in the summary should also be considered. Therefore, we set the upper bound of β as √ 2. As shown in Eq.12, within the range of [1, √\n2], β adaptively changes according to the ratio between |Rk| and |X|. The intuition comes from that a longer pseudo reference implies more key information needs to be covered by the summary. Besides, a shorter summary can easily get high precision but covers very limited important information in the pseudo reference. Thus, we give Recall a higher weight to punish such short summaries when the pseudo reference is long.\nFinal Averaged Relevance Score. After computing the centrality-weighted relevance score between the summary and the pseudo reference of each source document, we employ the average as\nthe final relevance score of the summary:\nscorerel = mean([F 1 ∗ , ..., F k ∗ , ..., F K ∗ ]), (13)\nwhere * is 1 for the F1 variant and β for the Fβ variant. The superscript k indicates the F∗ score is computed with the k-th document. Note that scorerel ∈ [−1, 1] and higher is better."
    }, {
      "heading" : "3.2 Self-referenced Redundancy Score",
      "text" : "In this section, we introduce our self-referenced redundancy score. We engage the summary itself as the reference to evaluate the degree of the semantic similarity between each summary token or sentence with the other tokens or sentences. The averaged semantic similarity degree is used as the redundancy score. The computation is based on a self-masked BERTScore as follows:\nscorered =\n∑ i maxj:i 6=j Sim(xj ,xi)\n|X| , (14)\nwhere “j : i 6= j” means we do not consider the similarity between xi and itself, i.e, self-masked. Because of the symmetric property, the F1, precision, and recall scores are equal with each other. This is also the reason that we use precision in Eq.14 as the final redundancy score. Note that scorered ∈ [−1, 1] and lower is better."
    }, {
      "heading" : "3.3 Final Evaluation Score",
      "text" : "After obtaining the relevance score and the redundancy score, we apply a linear combination to produce the final evaluation score of the summary based on the document set:\nscore = scorerel − λ ∗ scorered\n1 + λ , (15)\nwhere 0 < λ ≤ 1 is a hyper-parameter to scale the redundancy score and score ∈ [−1, 1]. Higher score means better summary quality. In our experiments, after fine-tuning on the validation set, λ is set as 0.6 and is fixed for all the testing datasets. We denote the variants of our final method as Ours(Fβ)-PacSumTopM and Ours(F1)-PacSumTopM depending on whether the adaptive Fβ is employed."
    }, {
      "heading" : "4 Experiment Setup",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "For comprehensively investigating our summarization evaluation methods, we test our methods on both multi-document and single-document summarization datasets. We leverage TAC3 datasets\n3https://tac.nist.gov/\nfor multi-document summarization evaluation testing. We choose TAC-2010 as the validation dataset and TAC-2008/TAC-2009/TAC-2011 as the testing datasets. Following previous work, we only utilize the initial summaries in TAC datasets, i.e., the summaries for the document set A. For the singledocument summarization evaluation, we employ CNNDM4 (Chaganty et al., 2018) as the testing dataset. The statistics of these datasets are shown in Table 1. Note that the hyper-parameters of our methods are fine-tuned on TAC-2010 and then fixed for all the testing datasets.\nFor TAC datasets, we compute correlation coefficients between predicted scores of an evaluation method and the annotated Pyramid scores of summaries to measure the effectiveness of the method. Following Gao et al. (2020), a correlation is computed for each topic. Then, the averaged correlation from all the topics is engaged as the final correlation of the method with human ratings.\nFor CNNDM dataset, correlations are calculated with the human scores in three dimensions including Overall, Grammar, and Redundancy. Following Wu et al. (2020), the correlation is computed between predicted scores of the 499 × 4 = 1996 (document, summary) pairs with corresponding human ratings."
    }, {
      "heading" : "4.2 Baselines",
      "text" : "In this section, we briefly introduce our baselines. We choose TF-IDF, JS (Louis and Nenkova, 2013), and REPEAR (Rioux et al., 2014) as traditional reference-free baselines. All these traditional baselines do not build pseudo references and\n4https://bit.ly/price-of-debiasing\ndirectly utilize the full content of the documents. For fairness, we also show the performance of our methods without building pseudo reference. We denote them as Ours(F1)-All and Ours(Fβ)-All since they use the whole document as a reference.\nWe also extend several popular referencebased methods as baselines. We adapt ROUGE1/2/L (Lin, 2004), MoverScore (Zhao et al., 2019), and S+WMS (Clark et al., 2019) into the referencefree scenario via building the pseudo reference with the PacSumTopM method. We add the suffix “- PacSumTopM” to these baseline names to indicate the pseudo reference building process.\nBesides, the SOTA reference-free summary evaluation metrics are also selected as our strong baselines, including C-ELMO/C-SBERT (Sun and Nenkova, 2019), SUPERT/SUPERT-IDF (Gao et al., 2020), and LS Score (Wu et al., 2020). CELMO (C-SBERT) encodes the document and the\nsummary using the pre-trained ELMO (SBERT) and then computes their cosine similarity. SUPERTIDF is an extension of SUPERT, which utilizes the inverse document frequency (IDF) as the importance weight of each token. For fair comparisons, we also apply the same pseudo reference building process i.e., PacSumTopM, to C-ELMO/CSBERT/SUPERT/SUPERT-IDF and add the suffix “-PacSumTopM” to the their names."
    }, {
      "heading" : "5 Results and Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Main Results",
      "text" : "The main experimental results on multi-document summarization datasets are shown in Table 2. We find that our F1 version (i.e., Ours(F1)PacSumTopM) already consistently outperforms all the baselines, which indicates the effectiveness of our centrality-weighted relevance score and our self-referenced redundancy score. The results also\ndemonstrate that our Fβ version can further improve the performance of multi-document summarization evaluation. By comparing Ours(Fβ)PacSumTopM and Ours(Fβ)-All, we see that the pseudo reference building process can significantly improve the performance. This is also the reason why we apply the same pseudo reference building process into SOTA baselines for fair comparisons. In the remaining part of this paper, we omit the suffix “-PacSumTopM” for simplicity when we mention a method.\nWe also test our methods on the single-document summarization dataset without further fine-tuning the hyper-parameters. The main results are displayed in Table 3. We note that our F1 version still outperforms all the baselines, which manifests the high generalization ability of our F1-based method. One interesting finding is that the performance significantly drops after incorporating the Fβ score.\nTo study the reason for the performance degradation on CNNDM after incorporating Fβ , we compare CNNDM and TAC datasets first. From Table 1, we note the main differences between them are the size of the document set for each topic (i.e., |Set|) and the number of the summarization systems (i.e., |Systems|). CNNDM has much smaller |Set| and |Systems|. We use the TAC-2011 dataset as an example to investigate whether our Fβ is unsuitable for smaller |Set| and |Systems|. We change |Set| and |Systems| respectively and report the gap of Spearman’s ρ between Ours(Fβ) and Ours(F1) in Figure 2. From the results, we observe that our Fβ\ncan consistently improve the performance for different |Set|. For the single-document summarization setting, i.e., |Set|=1, it still obtains a positive gap. Nevertheless, when the |Systems| is small such as 4, applying our Fβ leads to a dramatic performance dropping. From Table 1, we also see that CNNDM and TAC-2011 have different summary lengths (73.2 for CNNDM and 120.9 for TAC2011). However, when we limit the |Systems| of TAC-2011 to smaller numbers, the average length of generated summaries is still around 120, which indicates the performance degeneration is indeed from the change of system numbers. Therefore, we suggest using Ours(Fβ) when |Systems| is large like 12 and employing Ours(F1) when |Systems| is small like 4."
    }, {
      "heading" : "5.2 Ablation Study",
      "text" : "For better understanding the contributions of our proposed components, we conduct ablation studies on the best-performed method on each dataset, i.e., Ours(Fβ) for the multi-document summarization datasets and Ours(F1) for the single-document summarization dataset. We display results of the rank-based Spearman’s ρ in Figure 3.\nAs shown in the figure, after removing one of the three components (i.e., the centrality weighting, the hybrid representation, and the redundancy score), the performance of our methods become worse in most cases. This finding demonstrates the effectiveness of our proposed components. Besides, we also note that removing the redundancy score significantly degrades the performance on the redundancy evaluation on CNNDM, which indicates our redundancy score effectively captures the redundancy degree of the summaries."
    }, {
      "heading" : "5.3 Apply Centrality Weighting and Redundancy Score into MoverScore",
      "text" : "Besides basing on BERTScore, we also study whether our key features i.e., the centrality weighting and redundancy score, can work well in a\nMoverScore based framework (i.e., the relevance and redundancy scores are computed using MoverScore). Note that our Fβ is not applicable to MoverScore since it is not an F -measure. The results are listed in Table 4. We find that these two features significantly improve the performance of the original MoverScore on single-document summarization evaluation while degrading the performance dramatically on multi-document summarization evaluation. On CNNDM, the enhanced MoverScore even outperforms Ours(F1) on the “Overall” and “Redundancy” aspects, which indicates MoverScore is a promising basis for our proposed new features. We leave solving the performance dropping of the enhanced MoverScore on multi-document setting as future work."
    }, {
      "heading" : "5.4 Robustness Analysis",
      "text" : "We investigate the robustness of our method on the following factors and report the experimental results on the validation dataset (i.e., TAC-2010) in Figure 4: (1) the hyper-parameter λ for scaling the redundancy score; (2) the hyper-parameter γ in Fβ ; (3) the number of selected sentences for pseudo reference i.e., M ; (4) different pre-trained contextual encoding models including BERT-base5, BERTlarge6, RoBERTa-base7, and RoBERTa-large8.\n5bert-base-nli-stsb-mean-tokens 6bert-large-nli-stsb-mean-tokens 7roberta-base-nli-stsb-mean-tokens 8roberta-large-nli-stsb-mean-tokens\nSince both Spearman’s ρ and Kendall’s τ are rank-based correlation coefficients, we omit Kendall’s τ for simplicity. From this figure, we observe that the performance of our method is relatively stable for different λ and γ. We also find that a small M leads to lower correlations because much important information may be abandoned when building the pseudo references. But a large M will also degenerate the correlations since more noises are introduced. Thus, a moderateM is better. As for encoding models, we note that large encoding models obtain better performance than base encoding models. However, large models need more computation resources and time to encode the input text. Note that for our final method, we only fine-tune λ and γ on the TAC-2010 and set them as 0.6 and 2. As for M and encoding models, following the configuration of SUPERT (Gao et al., 2020), we directly set M as 12 and employ the BERT-large as the encoding model. All these factors are fixed for all testing datasets."
    }, {
      "heading" : "5.5 Performance on Bad/Good Summaries",
      "text" : "In this section, we evaluate the ability of our method to distinguish bad and good summaries. The bad and good summaries are selected by human ratings. We use TAC-2011 as an example and choose SUPERT as a strong baseline. The corresponding distributions of the reversed rank for bad and good summaries are illustrated in Figure 5. A smaller (larger) reversed rank represents the summary is assigned with a lower (higher) score. From the figure, we find that compared with SUPERT, Our(Fβ) has a better ability to assign bad sum-\nmaries lower scores and good summaries higher scores, which demonstrates the effectiveness of our method again. Moreover, we also note that both SUPERT and Ours(Fβ) are good at giving bad summaries lower scores while having difficulty in assigning good summaries higher scores. We leave solving this problem as another future work under the reference-free setting."
    }, {
      "heading" : "6 Related Work",
      "text" : "Reference-based Evaluation Metrics mainly measure the relevance between the humanannotated references and the system-generated text, which are widely adopted in text summarization (Lin, 2004; Zhao et al., 2019), machine translation (Papineni et al., 2002; Zhang et al., 2020), and dialogue systems (Papineni et al., 2002; Gao et al., 2021; Xiang et al., 2021). For example, ROUGE (Lin, 2004) evaluates the token sequence overlapping. BERTScore (Zhang et al., 2020), S+WMS (Clark et al., 2019), and MoverScore (Zhao et al., 2019) measure the semantic similarity between the references and the summary via a greedy or optimized minimum Earth Mover’s Distance.\nReference-free Evaluation Metrics have been developed to avoid the dependency on humanannotated references, which obtain more and more attention in recent years (Böhm et al., 2019; Gao et al., 2020; Wu et al., 2020; Chan et al., 2021). Some of them need to train a scorer (Peyrard and Gurevych, 2018; Xenouleas et al., 2019; Scialom et al., 2019; Böhm et al., 2019). For example, LS Score (Wu et al., 2020) designs a metric which\ncombines a linguistic quality scorer trained from the built positive and negative summaries, and a relevance scorer based on cosine similarity. The others do not require training (Louis and Nenkova, 2013; Rioux et al., 2014; Peyrard, 2019; Sun and Nenkova, 2019). For instance, SUPERT (Gao et al., 2020) builds the pseudo references from the source document first and then engages BERTScore to compute the relevance score between the pseudo reference and the summary."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we propose a novel training-free and reference-free summarization evaluation metric consisting of a relevance score and a redundancy score. Experiments on multi-document and single-document summarization settings show the effectiveness of our methods. One promising future direction is to solve the performance dropping issue after applying our key features into MoverScore and the other is to tackle the problem that current metrics struggle to assign higher scores for good summaries."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The work described in this paper was partially supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (CUHK 2410021, Research Impact Fund (RIF), R5034-18)."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Better rewards yield better summaries: Learning to summarise without references",
      "author" : [ "Florian Böhm", "Yang Gao", "Christian M. Meyer", "Ori Shapira", "Ido Dagan", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Böhm et al\\.,? 2019",
      "shortCiteRegEx" : "Böhm et al\\.",
      "year" : 2019
    }, {
      "title" : "The price of debiasing automatic metrics in natural language evalaution",
      "author" : [ "Arun Chaganty", "Stephen Mussmann", "Percy Liang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "citeRegEx" : "Chaganty et al\\.,? 2018",
      "shortCiteRegEx" : "Chaganty et al\\.",
      "year" : 2018
    }, {
      "title" : "A unified dual-view model for review summarization and sentiment classification with inconsistency loss",
      "author" : [ "Hou Pong Chan", "Wang Chen", "Irwin King." ],
      "venue" : "Proceedings of the 43rd International ACM SIGIR conference on research and development in",
      "citeRegEx" : "Chan et al\\.,? 2020",
      "shortCiteRegEx" : "Chan et al\\.",
      "year" : 2020
    }, {
      "title" : "Enhancing the open-domain dialogue evaluation in latent space",
      "author" : [ "Zhangming Chan", "Lemao Liu", "Juntao Li", "Haisong Zhang", "Dongyan Zhao", "Shuming Shi", "Rui Yan." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Chan et al\\.,? 2021",
      "shortCiteRegEx" : "Chan et al\\.",
      "year" : 2021
    }, {
      "title" : "Sentence mover’s similarity: Automatic evaluation for multi-sentence texts",
      "author" : [ "Elizabeth Clark", "Asli Çelikyilmaz", "Noah A. Smith." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July",
      "citeRegEx" : "Clark et al\\.,? 2019",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Ream#: An enhancement approach to referencebased evaluation metrics for open-domain dialog generation",
      "author" : [ "Jun Gao", "Wei Bi", "Ruifeng Xu", "Shuming Shi." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics:",
      "citeRegEx" : "Gao et al\\.,? 2021",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2021
    }, {
      "title" : "SUPERT: towards new frontiers in unsupervised evaluation metrics for multi-document summarization",
      "author" : [ "Yang Gao", "Wei Zhao", "Steffen Eger." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020,",
      "citeRegEx" : "Gao et al\\.,? 2020",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2020
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Roberta: A robustly optimized BERT pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "CoRR, abs/1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatically assessing machine summary content without a gold standard",
      "author" : [ "Annie Louis", "Ani Nenkova." ],
      "venue" : "Comput. Linguistics, 39(2):267–300.",
      "citeRegEx" : "Louis and Nenkova.,? 2013",
      "shortCiteRegEx" : "Louis and Nenkova.",
      "year" : 2013
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "A simple theoretical model of importance for summarization",
      "author" : [ "Maxime Peyrard." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1059–1073, Florence, Italy. Association for Computational Linguistics.",
      "citeRegEx" : "Peyrard.,? 2019",
      "shortCiteRegEx" : "Peyrard.",
      "year" : 2019
    }, {
      "title" : "Objective function learning to match human judgements for optimization-based summarization",
      "author" : [ "Maxime Peyrard", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Peyrard and Gurevych.,? 2018",
      "shortCiteRegEx" : "Peyrard and Gurevych.",
      "year" : 2018
    }, {
      "title" : "Fear the REAPER: A system for automatic multidocument summarization with reinforcement learning",
      "author" : [ "Cody Rioux", "Sadid A. Hasan", "Yllias Chali." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Rioux et al\\.,? 2014",
      "shortCiteRegEx" : "Rioux et al\\.",
      "year" : 2014
    }, {
      "title" : "Answers unite! unsupervised metrics for reinforced summarization models",
      "author" : [ "Thomas Scialom", "Sylvain Lamprier", "Benjamin Piwowarski", "Jacopo Staiano." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Scialom et al\\.,? 2019",
      "shortCiteRegEx" : "Scialom et al\\.",
      "year" : 2019
    }, {
      "title" : "Get to the point: Summarization with pointergenerator networks",
      "author" : [ "Abigail See", "Peter J. Liu", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073–",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "The feasibility of embedding based automatic evaluation for single document summarization",
      "author" : [ "Simeng Sun", "Ani Nenkova." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Sun and Nenkova.,? 2019",
      "shortCiteRegEx" : "Sun and Nenkova.",
      "year" : 2019
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le." ],
      "venue" : "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014,",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Unsupervised reference-free summary quality evaluation via contrastive learning",
      "author" : [ "Hanlu Wu", "Tengfei Ma", "Lingfei Wu", "Tariro Manyumwa", "Shouling Ji." ],
      "venue" : "CoRR, abs/2010.01781.",
      "citeRegEx" : "Wu et al\\.,? 2020",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2020
    }, {
      "title" : "SUMQE: a BERT-based summary quality estimation model",
      "author" : [ "Stratos Xenouleas", "Prodromos Malakasiotis", "Marianna Apidianaki", "Ion Androutsopoulos." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Xenouleas et al\\.,? 2019",
      "shortCiteRegEx" : "Xenouleas et al\\.",
      "year" : 2019
    }, {
      "title" : "Assessing dialogue systems with distribution distances",
      "author" : [ "Jiannan Xiang", "Yahui Liu", "Deng Cai", "Huayang Li", "Defu Lian", "Lemao Liu." ],
      "venue" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics: Findings.",
      "citeRegEx" : "Xiang et al\\.,? 2021",
      "shortCiteRegEx" : "Xiang et al\\.",
      "year" : 2021
    }, {
      "title" : "Bertscore: Evaluating text generation with BERT",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance",
      "author" : [ "Wei Zhao", "Maxime Peyrard", "Fei Liu", "Yang Gao", "Christian M. Meyer", "Steffen Eger." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in",
      "citeRegEx" : "Zhao et al\\.,? 2019",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2019
    }, {
      "title" : "Sentence centrality revisited for unsupervised summarization",
      "author" : [ "Hao Zheng", "Mirella Lapata." ],
      "venue" : "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume",
      "citeRegEx" : "Zheng and Lapata.,? 2019",
      "shortCiteRegEx" : "Zheng and Lapata.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "Text summarization systems have been developed rapidly due to the appearance of sequence-tosequence frameworks (Sutskever et al., 2014; Bahdanau et al., 2015; See et al., 2017; Chan et al., 2020), transformer architectures (Vaswani et al.",
      "startOffset" : 111,
      "endOffset" : 195
    }, {
      "referenceID" : 0,
      "context" : "Text summarization systems have been developed rapidly due to the appearance of sequence-tosequence frameworks (Sutskever et al., 2014; Bahdanau et al., 2015; See et al., 2017; Chan et al., 2020), transformer architectures (Vaswani et al.",
      "startOffset" : 111,
      "endOffset" : 195
    }, {
      "referenceID" : 17,
      "context" : "Text summarization systems have been developed rapidly due to the appearance of sequence-tosequence frameworks (Sutskever et al., 2014; Bahdanau et al., 2015; See et al., 2017; Chan et al., 2020), transformer architectures (Vaswani et al.",
      "startOffset" : 111,
      "endOffset" : 195
    }, {
      "referenceID" : 3,
      "context" : "Text summarization systems have been developed rapidly due to the appearance of sequence-tosequence frameworks (Sutskever et al., 2014; Bahdanau et al., 2015; See et al., 2017; Chan et al., 2020), transformer architectures (Vaswani et al.",
      "startOffset" : 111,
      "endOffset" : 195
    }, {
      "referenceID" : 20,
      "context" : ", 2020), transformer architectures (Vaswani et al., 2017) and large-scale pre-training models (Devlin et al.",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 6,
      "context" : ", 2017) and large-scale pre-training models (Devlin et al., 2019; Liu et al., 2019).",
      "startOffset" : 44,
      "endOffset" : 83
    }, {
      "referenceID" : 10,
      "context" : ", 2017) and large-scale pre-training models (Devlin et al., 2019; Liu et al., 2019).",
      "startOffset" : 44,
      "endOffset" : 83
    }, {
      "referenceID" : 8,
      "context" : "for multi-document summarization evaluation, SUPERT (Gao et al., 2020), predicts a relevance score for each (document, summary) pair to estimate the informativeness of the summary and then averages all the scores from multiple documents as the fi-",
      "startOffset" : 52,
      "endOffset" : 70
    }, {
      "referenceID" : 24,
      "context" : "For each pair, SUPERT employs the top-ranked sentences which are ranked by the position or centrality as a pseudo reference of the document and then applies BERTScore (Zhang et al., 2020) to produce a relevance score between the pseudo reference and the given summary.",
      "startOffset" : 167,
      "endOffset" : 187
    }, {
      "referenceID" : 21,
      "context" : "The SOTA single-document summarization referencefree evaluation metric, LS Score (Wu et al., 2020), combines a learned linguistic scorer for the summary and a cosine similarity scorer for the (document, summary) pair to produce the final score.",
      "startOffset" : 81,
      "endOffset" : 98
    }, {
      "referenceID" : 8,
      "context" : "Following SUPERT (Gao et al., 2020), we first split the document dk and the summary x into sentences.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 26,
      "context" : "The sentence selection is based on the centrality of each sentence, which is computed by the unsupervised algorithm, PacSum (Zheng and Lapata, 2019), using the sentence-level representation.",
      "startOffset" : 124,
      "endOffset" : 148
    }, {
      "referenceID" : 24,
      "context" : ", a), we compute the relevance score between the summary and the pseudo reference by a weighted BERTScore (Zhang et al., 2020).",
      "startOffset" : 106,
      "endOffset" : 126
    }, {
      "referenceID" : 2,
      "context" : "For the singledocument summarization evaluation, we employ CNNDM4 (Chaganty et al., 2018) as the testing",
      "startOffset" : 66,
      "endOffset" : 89
    }, {
      "referenceID" : 11,
      "context" : "We choose TF-IDF, JS (Louis and Nenkova, 2013), and REPEAR (Rioux et al.",
      "startOffset" : 21,
      "endOffset" : 46
    }, {
      "referenceID" : 15,
      "context" : "We choose TF-IDF, JS (Louis and Nenkova, 2013), and REPEAR (Rioux et al., 2014) as traditional reference-free baselines.",
      "startOffset" : 59,
      "endOffset" : 79
    }, {
      "referenceID" : 9,
      "context" : "We adapt ROUGE1/2/L (Lin, 2004), MoverScore (Zhao et al.",
      "startOffset" : 20,
      "endOffset" : 31
    }, {
      "referenceID" : 25,
      "context" : "We adapt ROUGE1/2/L (Lin, 2004), MoverScore (Zhao et al., 2019), and S+WMS (Clark et al.",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : ", 2019), and S+WMS (Clark et al., 2019) into the referencefree scenario via building the pseudo reference with the PacSumTopM method.",
      "startOffset" : 19,
      "endOffset" : 39
    }, {
      "referenceID" : 18,
      "context" : "Besides, the SOTA reference-free summary evaluation metrics are also selected as our strong baselines, including C-ELMO/C-SBERT (Sun and Nenkova, 2019), SUPERT/SUPERT-IDF (Gao et al.",
      "startOffset" : 128,
      "endOffset" : 151
    }, {
      "referenceID" : 8,
      "context" : "Besides, the SOTA reference-free summary evaluation metrics are also selected as our strong baselines, including C-ELMO/C-SBERT (Sun and Nenkova, 2019), SUPERT/SUPERT-IDF (Gao et al., 2020), and LS Score (Wu et al.",
      "startOffset" : 171,
      "endOffset" : 189
    }, {
      "referenceID" : 8,
      "context" : "As for M and encoding models, following the configuration of SUPERT (Gao et al., 2020), we directly set M as 12 and employ the BERT-large as the encoding model.",
      "startOffset" : 68,
      "endOffset" : 86
    }, {
      "referenceID" : 9,
      "context" : "measure the relevance between the humanannotated references and the system-generated text, which are widely adopted in text summarization (Lin, 2004; Zhao et al., 2019), machine translation (Papineni et al.",
      "startOffset" : 138,
      "endOffset" : 168
    }, {
      "referenceID" : 25,
      "context" : "measure the relevance between the humanannotated references and the system-generated text, which are widely adopted in text summarization (Lin, 2004; Zhao et al., 2019), machine translation (Papineni et al.",
      "startOffset" : 138,
      "endOffset" : 168
    }, {
      "referenceID" : 12,
      "context" : ", 2019), machine translation (Papineni et al., 2002; Zhang et al., 2020), and dialogue systems (Papineni et al.",
      "startOffset" : 29,
      "endOffset" : 72
    }, {
      "referenceID" : 24,
      "context" : ", 2019), machine translation (Papineni et al., 2002; Zhang et al., 2020), and dialogue systems (Papineni et al.",
      "startOffset" : 29,
      "endOffset" : 72
    }, {
      "referenceID" : 9,
      "context" : "For example, ROUGE (Lin, 2004) evaluates the token sequence overlapping.",
      "startOffset" : 19,
      "endOffset" : 30
    }, {
      "referenceID" : 5,
      "context" : ", 2020), S+WMS (Clark et al., 2019), and MoverScore (Zhao et al.",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 25,
      "context" : ", 2019), and MoverScore (Zhao et al., 2019) measure the semantic similarity between the references and the summary via a greedy or optimized minimum Earth Mover’s Distance.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 1,
      "context" : "Reference-free Evaluation Metrics have been developed to avoid the dependency on humanannotated references, which obtain more and more attention in recent years (Böhm et al., 2019; Gao et al., 2020; Wu et al., 2020; Chan et al., 2021).",
      "startOffset" : 161,
      "endOffset" : 234
    }, {
      "referenceID" : 8,
      "context" : "Reference-free Evaluation Metrics have been developed to avoid the dependency on humanannotated references, which obtain more and more attention in recent years (Böhm et al., 2019; Gao et al., 2020; Wu et al., 2020; Chan et al., 2021).",
      "startOffset" : 161,
      "endOffset" : 234
    }, {
      "referenceID" : 21,
      "context" : "Reference-free Evaluation Metrics have been developed to avoid the dependency on humanannotated references, which obtain more and more attention in recent years (Böhm et al., 2019; Gao et al., 2020; Wu et al., 2020; Chan et al., 2021).",
      "startOffset" : 161,
      "endOffset" : 234
    }, {
      "referenceID" : 4,
      "context" : "Reference-free Evaluation Metrics have been developed to avoid the dependency on humanannotated references, which obtain more and more attention in recent years (Böhm et al., 2019; Gao et al., 2020; Wu et al., 2020; Chan et al., 2021).",
      "startOffset" : 161,
      "endOffset" : 234
    }, {
      "referenceID" : 14,
      "context" : "Some of them need to train a scorer (Peyrard and Gurevych, 2018; Xenouleas et al., 2019; Scialom et al., 2019; Böhm et al., 2019).",
      "startOffset" : 36,
      "endOffset" : 129
    }, {
      "referenceID" : 22,
      "context" : "Some of them need to train a scorer (Peyrard and Gurevych, 2018; Xenouleas et al., 2019; Scialom et al., 2019; Böhm et al., 2019).",
      "startOffset" : 36,
      "endOffset" : 129
    }, {
      "referenceID" : 16,
      "context" : "Some of them need to train a scorer (Peyrard and Gurevych, 2018; Xenouleas et al., 2019; Scialom et al., 2019; Böhm et al., 2019).",
      "startOffset" : 36,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "Some of them need to train a scorer (Peyrard and Gurevych, 2018; Xenouleas et al., 2019; Scialom et al., 2019; Böhm et al., 2019).",
      "startOffset" : 36,
      "endOffset" : 129
    }, {
      "referenceID" : 21,
      "context" : "For example, LS Score (Wu et al., 2020) designs a metric which combines a linguistic quality scorer trained from the built positive and negative summaries, and a relevance scorer based on cosine similarity.",
      "startOffset" : 22,
      "endOffset" : 39
    }, {
      "referenceID" : 11,
      "context" : "The others do not require training (Louis and Nenkova, 2013; Rioux et al., 2014; Peyrard, 2019; Sun and Nenkova, 2019).",
      "startOffset" : 35,
      "endOffset" : 118
    }, {
      "referenceID" : 15,
      "context" : "The others do not require training (Louis and Nenkova, 2013; Rioux et al., 2014; Peyrard, 2019; Sun and Nenkova, 2019).",
      "startOffset" : 35,
      "endOffset" : 118
    }, {
      "referenceID" : 13,
      "context" : "The others do not require training (Louis and Nenkova, 2013; Rioux et al., 2014; Peyrard, 2019; Sun and Nenkova, 2019).",
      "startOffset" : 35,
      "endOffset" : 118
    }, {
      "referenceID" : 18,
      "context" : "The others do not require training (Louis and Nenkova, 2013; Rioux et al., 2014; Peyrard, 2019; Sun and Nenkova, 2019).",
      "startOffset" : 35,
      "endOffset" : 118
    }, {
      "referenceID" : 8,
      "context" : "For instance, SUPERT (Gao et al., 2020) builds the pseudo references from the source document first and then engages BERTScore to compute the relevance score between the pseudo reference and the summary.",
      "startOffset" : 21,
      "endOffset" : 39
    } ],
    "year" : 2021,
    "abstractText" : "In recent years, reference-based and supervised summarization evaluation metrics have been widely explored. However, collecting human-annotated references and ratings are costly and time-consuming. To avoid these limitations, we propose a training-free and reference-free summarization evaluation metric. Our metric consists of a centralityweighted relevance score and a self-referenced redundancy score. The relevance score is computed between the pseudo reference built from the source document and the given summary, where the pseudo reference content is weighted by the sentence centrality to provide importance guidance. Besides an F1-based relevance score, we also design an Fβ-based variant that pays more attention to the recall score. As for the redundancy score of the summary, we compute a self-masked similarity score with the summary itself to evaluate the redundant information in the summary. Finally, we combine the relevance and redundancy scores to produce the final evaluation score of the given summary. Extensive experiments show that our methods can significantly outperform existing methods on both multi-document and single-document summarization evaluation. The source code is released at https://github.com/Chen-WangCUHK/Training-Free-and-Ref-Free-SummEvaluation.",
    "creator" : "LaTeX with hyperref"
  }
}